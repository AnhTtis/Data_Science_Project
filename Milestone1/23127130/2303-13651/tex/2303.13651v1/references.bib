
@article{kievit_sensitive_2020,
	series = {Sensitive and critical periods},
	title = {Sensitive periods in cognitive development: a mutualistic perspective},
	volume = {36},
	issn = {2352-1546},
	shorttitle = {Sensitive periods in cognitive development},
	url = {https://www.sciencedirect.com/science/article/pii/S2352154620301522},
	doi = {10.1016/j.cobeha.2020.10.007},
	abstract = {The theory of mutualism posits that general cognitive ability emerges, at least in part, due to positive reciprocal interaction between distinct cognitive abilities during development. In other words, cognitive abilities at a given developmental time point govern the rate of growth in other cognitive abilities. Moreover, this emerging field of work finds that the strength as well as the nature of these interactions differs during developmental time: In other words, there are sensitive periods when small individual differences may have especially pronounced, long-lasting consequences for cognitive development. Here, I review the literature on mutualistic effects, and show how it can shed new light on sensitive periods. I do so by considering sensitive periods as periods where interactions between (cognitive) domains differ in strength and/or in kind.},
	language = {en},
	urldate = {2023-03-08},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Kievit, Rogier A},
	month = dec,
	year = {2020},
	pages = {144--149},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/B2CUHIXB/Kievit - 2020 - Sensitive periods in cognitive development a mutu.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/7UKN4H3H/S2352154620301522.html:text/html},
}

@book{hardt_patterns_2022,
	address = {Princeton},
	title = {Patterns, predictions, and actions: a story about machine learning},
	isbn = {978-0-691-23373-4},
	shorttitle = {Patterns, predictions, and actions},
	abstract = {"An authoritative, up-to-date graduate textbook on machine learning that highlights its historical context and societal impactsPatterns, Predictions, and actions introduces graduate students to the essentials of machine learning while offering invaluable perspective on its history and social implications. Beginning with the foundations of decision making, Moritz Hardt and Benjamin Recht explain how representation, optimization, and generalization are the constituents of supervised learning. They go on to provide self-contained discussions of causality, the practice of causal inference, sequential decision making, and reinforcement learning, equipping readers with the concepts and tools they need to assess the consequences that may arise from acting on statistical decisions. The text: provides a modern introduction to machine learning, showing how patterns in data support predictions and consequential actions, pays special attention to societal impacts and fairness in decision making, and traces the development of machine learning from its origins to today. Also features a novel chapter on machine learning benchmarks and datasets and invites readers from all backgrounds, requiring some experience with probability, calculus, and linear algebra. An essential textbook for students and a guide for researchers"--},
	publisher = {Princeton University Press},
	author = {Hardt, Moritz and Recht, Benjamin},
	year = {2022},
	keywords = {COMPUTERS / Data Science / Machine Learning, Machine learning, MATHEMATICS / Probability \& Statistics / General},
}

@misc{zador_toward_2023,
	title = {Toward {Next}-{Generation} {Artificial} {Intelligence}: {Catalyzing} the {NeuroAI} {Revolution}},
	shorttitle = {Toward {Next}-{Generation} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2210.08340},
	doi = {10.48550/arXiv.2210.08340},
	abstract = {Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities, inherited from over 500 million years of evolution, that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Zador, Anthony and Escola, Sean and Richards, Blake and Ölveczky, Bence and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and DiCarlo, James and Ganguli, Surya and Hawkins, Jeff and Koerding, Konrad and Koulakov, Alexei and LeCun, Yann and Lillicrap, Timothy and Marblestone, Adam and Olshausen, Bruno and Pouget, Alexandre and Savin, Cristina and Sejnowski, Terrence and Simoncelli, Eero and Solla, Sara and Sussillo, David and Tolias, Andreas S. and Tsao, Doris},
	month = feb,
	year = {2023},
	note = {arXiv:2210.08340 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: White paper, 10 pages + 8 pages of references, 1 figures},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/WCJIH6GF/Zador et al. - 2023 - Toward Next-Generation Artificial Intelligence Ca.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/YQNC88WP/2210.html:text/html},
}

@article{hassabis_neuroscience-inspired_2017,
	title = {Neuroscience-{Inspired} {Artificial} {Intelligence}},
	volume = {95},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627317305093},
	doi = {10.1016/j.neuron.2017.06.011},
	abstract = {The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields.},
	language = {en},
	number = {2},
	urldate = {2023-03-08},
	journal = {Neuron},
	author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
	month = jul,
	year = {2017},
	keywords = {artificial intelligence, brain, cognition, learning, neural network},
	pages = {245--258},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/EWF5UQ9I/Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/AZQGU5FV/S0896627317305093.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/S4SN2YEG/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/2FJEGS8Z/2005.html:text/html},
}

@misc{webb_emergent_2022,
	title = {Emergent {Analogical} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.09196},
	doi = {10.48550/arXiv.2212.09196},
	abstract = {The recent advent of large language models - large neural networks trained on a simple predictive objective over a massive corpus of natural language - has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training on those problems. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (GPT-3) on a range of analogical tasks, including a novel text-based matrix reasoning task closely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Webb, Taylor and Holyoak, Keith J. and Lu, Hongjing},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/2BK5DAF4/Webb et al. - 2022 - Emergent Analogical Reasoning in Large Language Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/J3IGKNVZ/2212.html:text/html},
}

@article{nelli_neural_2023,
	title = {Neural knowledge assembly in humans and neural networks},
	volume = {0},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(23)00118-6},
	doi = {10.1016/j.neuron.2023.02.014},
	language = {English},
	number = {0},
	urldate = {2023-03-10},
	journal = {Neuron},
	author = {Nelli, Stephanie and Braun, Lukas and Dumbalska, Tsvetomira and Saxe, Andrew and Summerfield, Christopher},
	month = mar,
	year = {2023},
	note = {Publisher: Elsevier},
	keywords = {abstraction, continual learning, decision making, generalization, neural networks, neuroimaging},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/GBQTLRWH/Nelli et al. - 2023 - Neural knowledge assembly in humans and neural net.pdf:application/pdf},
}


@article{rainer_selective_1998,
	title = {Selective representation of relevant information by neurons in the primate prefrontal cortex},
	volume = {393},
	copyright = {1998 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/31235},
	doi = {10.1038/31235},
	abstract = {The severe limitation of the capacity of working memory, the ability to store temporarily and manipulate information1, necessitates mechanisms that restrict access to it. Here we report tests to discover whether the activity of neurons in the prefrontal (PF)cortex, the putative neural correlate of working memory2,3,4,5,6,7,8, might reflect these mechanisms and preferentially represent behaviourally relevant information. Monkeys performed a ‘delayed-matching-to-sample’ task with an array of three objects. Only one of the objects in the array was relevant for task performance and the monkeys needed to find that object (the target) and remember its location. For many PF neurons, activity to physically identical arrays varied with the target location; the location of the non-target objects had little or no influence on activity. Information about the target location was present in activity as early as 140 ms after array onset. Also, information about which object was the target was reflected in the sustained activity of many PF neurons. These results suggest that the prefrontal cortex is involved in selecting and maintaining behaviourally relevant information.},
	language = {en},
	number = {6685},
	urldate = {2023-03-08},
	journal = {Nature},
	author = {Rainer, Gregor and Asaad, Wael F. and Miller, Earl K.},
	month = jun,
	year = {1998},
	note = {Number: 6685
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {577--579},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/UCZZCZMZ/Rainer et al. - 1998 - Selective representation of relevant information b.pdf:application/pdf},
}


@misc{fan_minedojo_2022,
	title = {{MineDojo}: {Building} {Open}-{Ended} {Embodied} {Agents} with {Internet}-{Scale} {Knowledge}},
	shorttitle = {{MineDojo}},
	url = {http://arxiv.org/abs/2206.08853},
	doi = {10.48550/arXiv.2206.08853},
	abstract = {Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
	month = nov,
	year = {2022},
	note = {arXiv:2206.08853 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Outstanding Paper Award at NeurIPS 2022. Project website: https://minedojo.org},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/UX9VI6EH/Fan et al. - 2022 - MineDojo Building Open-Ended Embodied Agents with.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/DSKRGR47/2206.html:text/html},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and others},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 27 pages, 17 figures + references and appendices, repo: https://github.com/google/BIG-bench},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/EMNVD738/Srivastava et al. - 2022 - Beyond the Imitation Game Quantifying and extrapo.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/X5AL7H3N/2206.html:text/html},
}

 % and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swedrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ramón Risco and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},

@misc{adaptive_agent_team_human-timescale_2023,
	title = {Human-{Timescale} {Adaptation} in an {Open}-{Ended} {Task} {Space}},
	url = {http://arxiv.org/abs/2301.07608},
	doi = {10.48550/arXiv.2301.07608},
	abstract = {Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {{Adaptive Agent Team} and Bauer, Jakob and Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Bhoopchand, Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael and Clay, Natalie and Collister, Adrian and Dasagi, Vibhavari and Gonzalez, Lucy and Gregor, Karol and Hughes, Edward and Kashem, Sheleem and Loks-Thompson, Maria and Openshaw, Hannah and Parker-Holder, Jack and Pathak, Shreya and Perez-Nieves, Nicolas and Rakicevic, Nemanja and Rocktäschel, Tim and Schroecker, Yannick and Sygnowski, Jakub and Tuyls, Karl and York, Sarah and Zacherl, Alexander and Zhang, Lei},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/Z5VM9TCQ/Adaptive Agent Team et al. - 2023 - Human-Timescale Adaptation in an Open-Ended Task S.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/AP24FZ7Y/2301.html:text/html},
}

@misc{xu_multimodal_2022,
	title = {Multimodal {Learning} with {Transformers}: {A} {Survey}},
	shorttitle = {Multimodal {Learning} with {Transformers}},
	url = {http://arxiv.org/abs/2206.06488},
	doi = {10.48550/arXiv.2206.06488},
	abstract = {Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Xu, Peng and Zhu, Xiatian and Clifton, David A.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.06488 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/VGPGIIKA/Xu et al. - 2022 - Multimodal Learning with Transformers A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/U359N36I/2206.html:text/html},
}

@misc{akkus_multimodal_2023,
	title = {Multimodal {Deep} {Learning}},
	url = {http://arxiv.org/abs/2301.04856},
	doi = {10.48550/arXiv.2301.04856},
	abstract = {This book is the result of a seminar in which we reviewed multimodal approaches and attempted to create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. Further, modeling frameworks are discussed where one modality is transformed into the other, as well as models in which one modality is utilized to enhance representation learning for the other. To conclude the second part, architectures with a focus on handling both modalities simultaneously are introduced. Finally, we also cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. One interesting application (Generative Art) eventually caps off this booklet.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Akkus, Cem and Chu, Luyang and Djakovic, Vladana and Jauch-Walser, Steffen and Koch, Philipp and Loss, Giacomo and Marquardt, Christopher and Moldovan, Marco and Sauter, Nadja and Schneider, Maximilian and Schulte, Rickmer and Urbanczyk, Karol and Goschenhofer, Jann and Heumann, Christian and Hvingelby, Rasmus and Schalk, Daniel and Aßenmacher, Matthias},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04856 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/PPIHJS2T/Akkus et al. - 2023 - Multimodal Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/4D7EF46G/2301.html:text/html},
}

@misc{xu_bridgetower_2023,
	title = {{BridgeTower}: {Building} {Bridges} {Between} {Encoders} in {Vision}-{Language} {Representation} {Learning}},
	shorttitle = {{BridgeTower}},
	url = {http://arxiv.org/abs/2206.08657},
	doi = {10.48550/arXiv.2206.08657},
	abstract = {Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance on various downstream vision-language tasks. In particular, on the VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73\%, outperforming the previous state-of-the-art model METER by 1.09\% with the same pre-training data and almost negligible additional parameters and computational costs. Notably, when further scaling the model, BridgeTower achieves an accuracy of 81.15\%, surpassing models that are pre-trained on orders-of-magnitude larger datasets. Code and checkpoints are available at https://github.com/microsoft/BridgeTower.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Xu, Xiao and Wu, Chenfei and Rosenman, Shachar and Lal, Vasudev and Che, Wanxiang and Duan, Nan},
	month = feb,
	year = {2023},
	note = {arXiv:2206.08657 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted by AAAI 2023, Oral},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/2MKDH7I4/Xu et al. - 2023 - BridgeTower Building Bridges Between Encoders in .pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/LT4DA84N/2206.html:text/html},
}

@article{goyal_inductive_2022,
	title = {Inductive biases for deep learning of higher-level cognition},
	volume = {478},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2021.0068},
	doi = {10.1098/rspa.2021.0068},
	abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
	number = {2266},
	urldate = {2023-03-08},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Goyal, Anirudh and Bengio, Yoshua},
	month = oct,
	year = {2022},
	note = {Publisher: Royal Society},
	keywords = {causality, deep learning, reasoning, system 2, systematic and out-of-distribution generalization},
	pages = {20210068},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/4GU47R8N/Goyal and Bengio - 2022 - Inductive biases for deep learning of higher-level.pdf:application/pdf},
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993},
	doi = {10.1017/S0140525X16001837},
	abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	language = {en},
	urldate = {2023-03-08},
	journal = {Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	year = {2017},
	note = {Publisher: Cambridge University Press},
	pages = {e253},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/I7G4RRNI/Lake et al. - 2017 - Building machines that learn and think like people.pdf:application/pdf},
}

@article{russin_deep_2020,
	title = {{DEEP} {LEARNING} {NEEDS} {A} {PREFRONTAL} {CORTEX}},
	abstract = {Research seeking to build artificial systems capable of reproducing elements of human intelligence may benefit from a deeper consideration of the architecture and learning mechanisms of the human brain. In this brief review, we note a connection between many current challenges facing artificial intelligence and the functions of a particular brain area —the prefrontal cortex (PFC). This brain area is known to be involved in executive functions such as reasoning, rule-learning, deliberate or controlled processing, and abstract planning. Motivated by the hypothesis that these functions provide a form of out-of-distribution robustness currently not available in state-of-the-art AI systems, we elaborate on this connection and highlight some computational principles thought to be at work in PFC, with the goal of enhancing the synergy between neuroscience and machine learning.},
	language = {en},
	journal = {“Bridging AI and Cognitive Science” (ICLR 2020)},
	author = {Russin, Jacob and O’Reilly, Randall C and Bengio, Yoshua},
	year = {2020},
	file = {Russin et al. - 2020 - DEEP LEARNING NEEDS A PREFRONTAL CORTEX.pdf:/Users/jaschaachterberg/Zotero/storage/CA2NVU48/Russin et al. - 2020 - DEEP LEARNING NEEDS A PREFRONTAL CORTEX.pdf:application/pdf},
}

@article{kaas_organization_2001,
	title = {The organization of sensory cortex},
	volume = {11},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438800002403},
	doi = {10.1016/S0959-4388(00)00240-3},
	abstract = {Recent studies of primary visual cortex (V1) redefine layers 3 and 4 of V1 in monkeys and show that monkeys, apes and humans have different laminar specializations. Projections from V1 define a smaller, but complete, third visual area, and a dorsomedial area. The middle temporal visual area has two types of motion-sensitive modules with inputs from cytochrome oxidase columns in V1. Second-level somatosensory areas have been described in humans, and a second-level auditory area is shown to respond to somatosensory stimuli.},
	language = {en},
	number = {4},
	urldate = {2023-03-08},
	journal = {Current Opinion in Neurobiology},
	author = {Kaas, Jon H and Collins, Christine E},
	month = aug,
	year = {2001},
	keywords = {auditory cortex, extrastriate cortex, primates, sensory cortex, somatosensory cortex, visual cortex},
	pages = {498--504},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/IS7VDSMZ/Kaas and Collins - 2001 - The organization of sensory cortex.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/X3YCSKI6/S0959438800002403.html:text/html},
}

@article{ralph_neural_2017,
	title = {The neural and computational bases of semantic cognition},
	volume = {18},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn.2016.150},
	doi = {10.1038/nrn.2016.150},
	abstract = {Semantic cognition refers to our ability to use, manipulate and generalize knowledge that is acquired over the lifespan to support innumerable verbal and non-verbal behaviours.Semantic cognition relies on two principal interacting neural systems: representation and control. We refer to this two-system view as the controlled semantic cognition framework.Coherent, generalizable concepts are formed through the hub-and-spoke representational system with the hub localised to the anterior temporal region (bilaterally) and spokes localised in modality-specific association cortices that are distributed across the cortex.Convergent clinical and cognitive neuroscience data show that the anterior temporal lobe hub has graded variations of semantic function that follow its pattern of connectivity.Category-specific differences in semantic function reflect the contributions of different parts of the connectivity-constrained version of the hub-and-spoke framework.Semantic control is implemented within a distributed frontal and temporoparietal neural network. Semantic control supports executive mechanisms that constrain how activation propagates through the network for semantic representation.},
	language = {en},
	number = {1},
	urldate = {2023-03-08},
	journal = {Nature Reviews Neuroscience},
	author = {Ralph, Matthew A. Lambon and Jefferies, Elizabeth and Patterson, Karalyn and Rogers, Timothy T.},
	month = jan,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cognitive neuroscience, Dementia, Stroke},
	pages = {42--55},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/V7NBHNMX/Ralph et al. - 2017 - The neural and computational bases of semantic cog.pdf:application/pdf},
}

@article{skeide_ontogeny_2016,
	title = {The ontogeny of the cortical language network},
	volume = {17},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn.2016.23},
	doi = {10.1038/nrn.2016.23},
	abstract = {The understanding of spoken language is mediated by bottom-up and top-down processing in the brain. In this Opinion article, Skeide and Friederici propose how changes in the structure and function of children's brains are associated with the development of language-processing skills.},
	language = {en},
	number = {5},
	urldate = {2023-03-08},
	journal = {Nature Reviews Neuroscience},
	author = {Skeide, Michael A. and Friederici, Angela D.},
	month = may,
	year = {2016},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Cognitive neuroscience, Development of the nervous system, Language, Sensory processing},
	pages = {323--332},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/JTZZJDHC/Skeide and Friederici - 2016 - The ontogeny of the cortical language network.pdf:application/pdf},
}

@article{atilgan_integration_2018,
	title = {Integration of {Visual} {Information} in {Auditory} {Cortex} {Promotes} {Auditory} {Scene} {Analysis} through {Multisensory} {Binding}},
	volume = {97},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627317311820},
	doi = {10.1016/j.neuron.2017.12.034},
	abstract = {How and where in the brain audio-visual signals are bound to create multimodal objects remains unknown. One hypothesis is that temporal coherence between dynamic multisensory signals provides a mechanism for binding stimulus features across sensory modalities. Here, we report that when the luminance of a visual stimulus is temporally coherent with the amplitude fluctuations of one sound in a mixture, the representation of that sound is enhanced in auditory cortex. Critically, this enhancement extends to include both binding and non-binding features of the sound. We demonstrate that visual information conveyed from visual cortex via the phase of the local field potential is combined with auditory information within auditory cortex. These data provide evidence that early cross-sensory binding provides a bottom-up mechanism for the formation of cross-sensory objects and that one role for multisensory binding in auditory cortex is to support auditory scene analysis.},
	language = {en},
	number = {3},
	urldate = {2023-03-08},
	journal = {Neuron},
	author = {Atilgan, Huriye and Town, Stephen M. and Wood, Katherine C. and Jones, Gareth P. and Maddox, Ross K. and Lee, Adrian K. C. and Bizley, Jennifer K.},
	month = feb,
	year = {2018},
	keywords = {attention, auditory cortex, auditory-visual, binding, cross-modal, ferret, multisensory, sensory cortex, visual cortex},
	pages = {640--655.e4},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/QUSMRGQZ/Atilgan et al. - 2018 - Integration of Visual Information in Auditory Cort.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/XCDNAA4T/S0896627317311820.html:text/html},
}

@article{steinmetz_distributed_2019,
	title = {Distributed coding of choice, action and engagement across the mouse brain},
	volume = {576},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1787-x},
	doi = {10.1038/s41586-019-1787-x},
	abstract = {Vision, choice, action and behavioural engagement arise from neuronal activity that may be distributed across brain regions. Here we delineate the spatial distribution of neurons underlying these processes. We used Neuropixels probes1,2 to record from approximately 30,000 neurons in 42 brain regions of mice performing a visual discrimination task3. Neurons in nearly all regions responded non-specifically when the mouse initiated an action. By contrast, neurons encoding visual stimuli and upcoming choices occupied restricted regions in the neocortex, basal ganglia and midbrain. Choice signals were rare and emerged with indistinguishable timing across regions. Midbrain neurons were activated before contralateral choices and were suppressed before ipsilateral choices, whereas forebrain neurons could prefer either side. Brain-wide pre-stimulus activity predicted engagement in individual trials and in the overall task, with enhanced subcortical but suppressed neocortical activity during engagement. These results reveal organizing principles for the distribution of neurons encoding behaviourally relevant variables across the mouse brain.},
	language = {en},
	number = {7786},
	urldate = {2023-03-08},
	journal = {Nature},
	author = {Steinmetz, Nicholas A. and Zatka-Haas, Peter and Carandini, Matteo and Harris, Kenneth D.},
	month = dec,
	year = {2019},
	note = {Number: 7786
Publisher: Nature Publishing Group},
	keywords = {Decision, Neural circuits},
	pages = {266--273},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/BR8NDERJ/Steinmetz et al. - 2019 - Distributed coding of choice, action and engagemen.pdf:application/pdf},
}

@article{xu_cortical_2022,
	title = {The cortical connectome of primate lateral prefrontal cortex},
	volume = {110},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627321007868},
	doi = {10.1016/j.neuron.2021.10.018},
	abstract = {The lateral prefrontal cortex (LPFC) of primates plays an important role in executive control, but how it interacts with the rest of the cortex remains unclear. To address this, we densely mapped the cortical connectome of LPFC, using electrical microstimulation combined with functional MRI (EM-fMRI). We found isomorphic mappings between LPFC and five major processing domains composing most of the cerebral cortex except early sensory and motor areas. An LPFC grid of ∼200 stimulation sites topographically mapped to separate grids of activation sites in the five domains, coarsely resembling how the visual cortex maps the retina. The temporal and parietal maps largely overlapped in LPFC, suggesting topographically organized convergence of the ventral and dorsal streams, and the other maps overlapped at least partially. Thus, the LPFC contains overlapping, millimeter-scale maps that mirror the organization of major cortical processing domains, supporting LPFC’s role in coordinating activity within and across these domains.},
	language = {en},
	number = {2},
	urldate = {2023-03-08},
	journal = {Neuron},
	author = {Xu, Rui and Bichot, Narcisse P. and Takahashi, Atsushi and Desimone, Robert},
	month = jan,
	year = {2022},
	keywords = {cortical organization, dense connectome, electrical microstimulation, executive control, fMRI, topographic connectivity},
	pages = {312--327.e7},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/AV4HPDU6/Xu et al. - 2022 - The cortical connectome of primate lateral prefron.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/JH6C8WKD/S0896627321007868.html:text/html},
}

@article{vanrullen_deep_2021,
	title = {Deep learning and the {Global} {Workspace} {Theory}},
	volume = {44},
	issn = {0166-2236},
	url = {https://www.sciencedirect.com/science/article/pii/S0166223621000771},
	doi = {10.1016/j.tins.2021.04.005},
	abstract = {Recent advances in deep learning have allowed artificial intelligence (AI) to reach near human-level performance in many sensory, perceptual, linguistic, and cognitive tasks. There is a growing need, however, for novel, brain-inspired cognitive architectures. The Global Workspace Theory (GWT) refers to a large-scale system integrating and distributing information among networks of specialized modules to create higher-level forms of cognition and awareness. We argue that the time is ripe to consider explicit implementations of this theory using deep-learning techniques. We propose a roadmap based on unsupervised neural translation between multiple latent spaces (neural networks trained for distinct tasks, on distinct sensory inputs and/or modalities) to create a unique, amodal Global Latent Workspace (GLW). Potential functional advantages of GLW are reviewed, along with neuroscientific implications.},
	language = {en},
	number = {9},
	urldate = {2023-03-08},
	journal = {Trends in Neurosciences},
	author = {VanRullen, Rufin and Kanai, Ryota},
	month = sep,
	year = {2021},
	keywords = {attention, broadcast, consciousness, grounding, latent space, multimodal translation},
	pages = {692--704},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/JFWLLC3P/VanRullen and Kanai - 2021 - Deep learning and the Global Workspace Theory.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/FA6V7JN2/S0166223621000771.html:text/html},
}

@article{duncan_integrated_2020,
	title = {Integrated {Intelligence} from {Distributed} {Brain} {Activity}},
	volume = {24},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661320301698},
	doi = {10.1016/j.tics.2020.06.012},
	abstract = {How does organized cognition arise from distributed brain activity? Recent analyses of fluid intelligence suggest a core process of cognitive focus and integration, organizing the components of a cognitive operation into the required computational structure. A cortical ‘multiple-demand’ (MD) system is closely linked to fluid intelligence, and recent imaging data define nine specific MD patches distributed across frontal, parietal, and occipitotemporal cortex. Wide cortical distribution, relative functional specialization, and strong connectivity suggest a basis for cognitive integration, matching electrophysiological evidence for binding of cognitive operations to their contents. Though still only in broad outline, these data suggest how distributed brain activity can build complex, organized cognition.},
	language = {en},
	number = {10},
	urldate = {2023-03-08},
	journal = {Trends in Cognitive Sciences},
	author = {Duncan, John and Assem, Moataz and Shashidhara, Sneha},
	month = oct,
	year = {2020},
	keywords = {attention, brain networks, cognitive control, intelligence, neural coding},
	pages = {838--852},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/7EIIGZNW/Duncan et al. - 2020 - Integrated Intelligence from Distributed Brain Act.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/27JGR6IQ/S1364661320301698.html:text/html},
}

@article{deco_revisiting_2021,
	title = {Revisiting the global workspace orchestrating the hierarchical organization of the human brain},
	volume = {5},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-01003-6},
	doi = {10.1038/s41562-020-01003-6},
	abstract = {A central challenge in neuroscience is how the brain organizes the information necessary to orchestrate behaviour. Arguably, this whole-brain orchestration is carried out by a core subset of integrative brain regions, a ‘global workspace’, but its constitutive regions remain unclear. We quantified the global workspace as the common regions across seven tasks as well as rest, in a common ‘functional rich club’. To identify this functional rich club, we determined the information flow between brain regions by means of a normalized directed transfer entropy framework applied to multimodal neuroimaging data from 1,003 healthy participants and validated in participants with retest data. This revealed a set of regions orchestrating information from perceptual, long-term memory, evaluative and attentional systems. We confirmed the causal significance and robustness of our results by systematically lesioning a generative whole-brain model. Overall, this framework describes a complex choreography of the functional hierarchical organization of the human brain.},
	language = {en},
	number = {4},
	urldate = {2023-03-08},
	journal = {Nature Human Behaviour},
	author = {Deco, Gustavo and Vidaurre, Diego and Kringelbach, Morten L.},
	month = apr,
	year = {2021},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Cognitive control, Neuroscience, Psychology},
	pages = {497--511},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/EK49RR8I/Deco et al. - 2021 - Revisiting the global workspace orchestrating the .pdf:application/pdf},
}

@misc{goyal_coordination_2022,
	title = {Coordination {Among} {Neural} {Modules} {Through} a {Shared} {Global} {Workspace}},
	url = {http://arxiv.org/abs/2103.01197},
	doi = {10.48550/arXiv.2103.01197},
	abstract = {Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions; object-centric architectures make use of graph neural networks to model interactions among entities. However, pairwise interactions may not achieve global coordination or a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Goyal, Anirudh and Didolkar, Aniket and Lamb, Alex and Badola, Kartikeya and Ke, Nan Rosemary and Rahaman, Nasim and Binas, Jonathan and Blundell, Charles and Mozer, Michael and Bengio, Yoshua},
	month = mar,
	year = {2022},
	note = {arXiv:2103.01197 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR'22 accepted paper},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/23ISZE9L/Goyal et al. - 2022 - Coordination Among Neural Modules Through a Shared.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/36QSUG2K/2103.html:text/html},
}

@misc{pfeiffer_modular_2023,
	title = {Modular {Deep} {Learning}},
	url = {http://arxiv.org/abs/2302.11529},
	doi = {10.48550/arXiv.2302.11529},
	abstract = {Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Pfeiffer, Jonas and Ruder, Sebastian and Vulić, Ivan and Ponti, Edoardo Maria},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11529 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/DXTWHZKS/Pfeiffer et al. - 2023 - Modular Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/HSC3ALZN/2302.html:text/html},
}

@article{grill-spector_human_2004,
	title = {The {Human} {Visual} {Cortex}},
	volume = {27},
	url = {https://doi.org/10.1146/annurev.neuro.27.070203.144220},
	doi = {10.1146/annurev.neuro.27.070203.144220},
	abstract = {The discovery and analysis of cortical visual areas is a major accomplishment of visual neuroscience. In the past decade the use of noninvasive functional imaging, particularly functional magnetic resonance imaging (fMRI), has dramatically increased our detailed knowledge of the functional organization of the human visual cortex and its relation to visual perception. The fMRI method offers a major advantage over other techniques applied in neuroscience by providing a large-scale neuroanatomical perspective that stems from its ability to image the entire brain essentially at once. This bird's eye view has the potential to reveal large-scale principles within the very complex plethora of visual areas. Thus, it could arrange the entire constellation of human visual areas in a unified functional organizational framework. Here we review recent findings and methods employed to uncover the functional properties of the human visual cortex focusing on two themes: functional specialization and hierarchical processing.},
	number = {1},
	urldate = {2023-03-08},
	journal = {Annual Review of Neuroscience},
	author = {Grill-Spector, Kalanit and Malach, Rafael},
	year = {2004},
	pmid = {15217346},
	note = {\_eprint: https://doi.org/10.1146/annurev.neuro.27.070203.144220},
	keywords = {functional magnetic resonance imaging (fMRI), object and face recognition, retinotopic mapping, visual perception},
	pages = {649--677},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/MW9L8FRQ/Grill-Spector and Malach - 2004 - The Human Visual Cortex.pdf:application/pdf},
}

@article{mashour_conscious_2020,
	title = {Conscious {Processing} and the {Global} {Neuronal} {Workspace} {Hypothesis}},
	volume = {105},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627320300520},
	doi = {10.1016/j.neuron.2020.01.026},
	abstract = {We review the central tenets and neuroanatomical basis of the global neuronal workspace (GNW) hypothesis, which attempts to account for the main scientific observations regarding the elementary mechanisms of conscious processing in the human brain. The GNW hypothesis proposes that, in the conscious state, a non-linear network ignition associated with recurrent processing amplifies and sustains a neural representation, allowing the corresponding information to be globally accessed by local processors. We examine this hypothesis in light of recent data that contrast brain activity evoked by either conscious or non-conscious contents, as well as during conscious or non-conscious states, particularly general anesthesia. We also discuss the relationship between the intertwined concepts of conscious processing, attention, and working memory.},
	language = {en},
	number = {5},
	urldate = {2023-03-08},
	journal = {Neuron},
	author = {Mashour, George A. and Roelfsema, Pieter and Changeux, Jean-Pierre and Dehaene, Stanislas},
	month = mar,
	year = {2020},
	pages = {776--798},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/8A478SSH/Mashour et al. - 2020 - Conscious Processing and the Global Neuronal Works.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/3M6ZZGXA/S0896627320300520.html:text/html},
}

@misc{schrimpf_brain-score_2018,
	title = {Brain-{Score}: {Which} {Artificial} {Neural} {Network} for {Object} {Recognition} is most {Brain}-{Like}?},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Brain-{Score}},
	url = {https://www.biorxiv.org/content/10.1101/407007v1},
	doi = {10.1101/407007},
	abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed Brain-Score – a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain’s mechanisms for core object recognition – and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at ≥ 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain’s network and thus drive next experiments. To facilitate both of these, we release Brain-Score.org: a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {bioRxiv},
	author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Schmidt, Kailyn and Yamins, Daniel L. K. and DiCarlo, James J.},
	month = sep,
	year = {2018},
	note = {bioRxiv: 407007
Section: New Results},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/5BK2P35R/Schrimpf et al. - 2018 - Brain-Score Which Artificial Neural Network for O.pdf:application/pdf},
}

@article{hackett_information_2011,
	series = {Auditory {Cortex}: {Current} {Concepts} in {Human} and {Animal} {Research}},
	title = {Information flow in the auditory cortical network},
	volume = {271},
	issn = {0378-5955},
	url = {https://www.sciencedirect.com/science/article/pii/S0378595510000249},
	doi = {10.1016/j.heares.2010.01.011},
	abstract = {Auditory processing in the cerebral cortex is comprised of an interconnected network of auditory and auditory-related areas distributed throughout the forebrain. The nexus of auditory activity is located in temporal cortex among several specialized areas, or fields, that receive dense inputs from the medial geniculate complex. These areas are collectively referred to as auditory cortex. Auditory activity is extended beyond auditory cortex via connections with auditory-related areas elsewhere in the cortex. Within this network, information flows between areas to and from countless targets, but in a manner that is characterized by orderly regional, areal and laminar patterns. These patterns reflect some of the structural constraints that passively govern the flow of information at all levels of the network. In addition, the exchange of information within these circuits is dynamically regulated by intrinsic neurochemical properties of projecting neurons and their targets. This article begins with an overview of the principal circuits and how each is related to information flow along major axes of the network. The discussion then turns to a description of neurochemical gradients along these axes, highlighting recent work on glutamate transporters in the thalamocortical projections to auditory cortex. The article concludes with a brief discussion of relevant neurophysiological findings as they relate to structural gradients in the network.},
	language = {en},
	number = {1},
	urldate = {2023-03-08},
	journal = {Hearing Research},
	author = {Hackett, Troy A.},
	month = jan,
	year = {2011},
	pages = {133--146},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/STBSZNTA/Hackett - 2011 - Information flow in the auditory cortical network.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/HKLGQAEA/S0378595510000249.html:text/html},
}

@article{lindsay_convolutional_2021,
	title = {Convolutional {Neural} {Networks} as a {Model} of the {Visual} {System}: {Past}, {Present}, and {Future}},
	volume = {33},
	issn = {0898-929X},
	shorttitle = {Convolutional {Neural} {Networks} as a {Model} of the {Visual} {System}},
	url = {https://doi.org/10.1162/jocn_a_01544},
	doi = {10.1162/jocn_a_01544},
	abstract = {Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNs in vision research beyond basic object recognition.},
	number = {10},
	urldate = {2023-03-08},
	journal = {Journal of Cognitive Neuroscience},
	author = {Lindsay, Grace W.},
	month = sep,
	year = {2021},
	pages = {2017--2031},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/WDZGHQ8A/Lindsay - 2021 - Convolutional Neural Networks as a Model of the Vi.pdf:application/pdf},
}

@article{kietzmann_recurrence_2019,
	title = {Recurrence is required to capture the representational dynamics of the human visual system},
	volume = {116},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1905544116},
	doi = {10.1073/pnas.1905544116},
	abstract = {The human visual system is an intricate network of brain regions that enables us to recognize the world around us. Despite its abundant lateral and feedback connections, object processing is commonly viewed and studied as a feedforward process. Here, we measure and model the rapid representational dynamics across multiple stages of the human ventral stream using time-resolved brain imaging and deep learning. We observe substantial representational transformations during the first 300 ms of processing within and across ventral-stream regions. Categorical divisions emerge in sequence, cascading forward and in reverse across regions, and Granger causality analysis suggests bidirectional information flow between regions. Finally, recurrent deep neural network models clearly outperform parameter-matched feedforward models in terms of their ability to capture the multiregion cortical dynamics. Targeted virtual cooling experiments on the recurrent deep network models further substantiate the importance of their lateral and top-down connections. These results establish that recurrent models are required to understand information processing in the human ventral stream.},
	number = {43},
	urldate = {2023-03-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kietzmann, Tim C. and Spoerer, Courtney J. and Sörensen, Lynn K. A. and Cichy, Radoslaw M. and Hauk, Olaf and Kriegeskorte, Nikolaus},
	month = oct,
	year = {2019},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {21854--21863},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/G4IZWSWT/Kietzmann et al. - 2019 - Recurrence is required to capture the representati.pdf:application/pdf},
}

@misc{schmidhuber_annotated_2022,
	title = {Annotated {History} of {Modern} {AI} and {Deep} {Learning}},
	url = {http://arxiv.org/abs/2212.11279},
	doi = {10.48550/arXiv.2212.11279},
	abstract = {Machine learning is the science of credit assignment: finding patterns in observations that predict the consequences of actions and help to improve future performance. Credit assignment is also required for human understanding of how the world works, not only for individuals navigating daily life, but also for academic professionals like historians who interpret the present in light of past events. Here I focus on the history of modern artificial intelligence (AI) which is dominated by artificial neural networks (NNs) and deep learning, both conceptually closer to the old field of cybernetics than to what's been called AI since 1956 (e.g., expert systems and logic programming). A modern history of AI will emphasize breakthroughs outside of the focus of traditional AI text books, in particular, mathematical foundations of today's NNs such as the chain rule (1676), the first NNs (linear regression, circa 1800), and the first working deep learners (1965-). From the perspective of 2022, I provide a timeline of the -- in hindsight -- most important relevant events in the history of NNs, deep learning, AI, computer science, and mathematics in general, crediting those who laid foundations of the field. The text contains numerous hyperlinks to relevant overview sites from my AI Blog. It supplements my previous deep learning survey (2015) which provides hundreds of additional references. Finally, to round it off, I'll put things in a broader historic context spanning the time since the Big Bang until when the universe will be many times older than it is now.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Schmidhuber, Juergen},
	month = dec,
	year = {2022},
	note = {arXiv:2212.11279 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 75 pages, over 500 references. arXiv admin note: substantial text overlap with arXiv:2005.05744},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/LWS26VDN/Schmidhuber - 2022 - Annotated History of Modern AI and Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/NYFE9RGM/2212.html:text/html},
}

@incollection{miller_rules_2007,
	title = {Rules through {Recursion}: {How} {Interactions} between the {Frontal} {Cortex} and {Basal} {Ganglia} {May} {Build} {Abstract}, {Complex} {Rules} from {Concrete}, {Simple} {Ones}},
	isbn = {978-0-19-531427-4},
	shorttitle = {Rules through {Recursion}},
	url = {https://doi.org/10.1093/acprof:oso/9780195314274.003.0022},
	abstract = {The understanding of abstract rules is necessary for the development of goal‐directed behavior, and generally has been linked to prefrontal function. However, this chapter puts forward the theory that complex thoughts and actions can actually be “bootstrapped” from simpler ones through the parallel interactions of the prefrontal cortex (PFC) and basal ganglia (specifically, the dorsal striatum) via the corticoganglia loops. The relationship between the two structures appears to be that, as the animal learns specific stimulus‐response associations, they are quickly represented in the striatum, which then slowly trains the PFC. The closed circuit loop between the two structures suggests an autoassociative network in which a key component is the ability to learn temporal sequences of patterns and thus make predictions.The understanding of abstract rules is necessary for the development of goal‐directed behavior, and generally has been linked to prefrontal function. However, this chapter puts forward the theory that complex thoughts and actions can actually be “bootstrapped” from simpler ones through the parallel interactions of the prefrontal cortex (PFC) and basal ganglia (specifically, the dorsal striatum) via the corticoganglia loops. The relationship between the two structures appears to be that, as the animal learns specific stimulus‐response associations, they are quickly represented in the striatum, which then slowly trains the PFC. The closed circuit loop between the two structures suggests an autoassociative network in which a key component is the ability to learn temporal sequences of patterns and thus make predictions.},
	urldate = {2023-03-08},
	booktitle = {Neuroscience of {Rule}-{Guided} {Behavior}},
	publisher = {Oxford University Press},
	author = {Miller, Earl K. and Buschman, Timothy J.},
	editor = {Bunge, Silvia A. and Wallis, Jonathan D.},
	month = nov,
	year = {2007},
	doi = {10.1093/acprof:oso/9780195314274.003.0022},
	pages = {0},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/QH6ERM8X/Miller and Buschman - 2007 - Rules through Recursion How Interactions between .pdf:application/pdf;Snapshot:/Users/jaschaachterberg/Zotero/storage/ZGYZEWR6/197819298.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {Conference Name: Neural Computation},
	pages = {1735--1780},
	file = {IEEE Xplore Abstract Record:/Users/jaschaachterberg/Zotero/storage/S4U8R68N/6795963.html:text/html},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-03-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/UQDG8JGH/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{fawaz_deep_2019,
	title = {Deep learning for time series classification: a review},
	volume = {33},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Deep learning for time series classification},
	url = {http://arxiv.org/abs/1809.04356},
	doi = {10.1007/s10618-019-00619-1},
	abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
	number = {4},
	urldate = {2023-03-08},
	journal = {Data Mining and Knowledge Discovery},
	author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	month = jul,
	year = {2019},
	note = {arXiv:1809.04356 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {917--963},
	annote = {Comment: Accepted at Data Mining and Knowledge Discovery},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/I89WQ4YU/Fawaz et al. - 2019 - Deep learning for time series classification a re.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/9BWHZ286/1809.html:text/html},
}

@article{tay_efficient_2022,
	title = {Efficient {Transformers}: {A} {Survey}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {Efficient {Transformers}},
	url = {https://doi.org/10.1145/3530811},
	doi = {10.1145/3530811},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of “X-former” models have been proposed—Reformer, Linformer, Performer, Longformer, to name a few—which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	number = {6},
	urldate = {2023-03-08},
	journal = {ACM Computing Surveys},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = dec,
	year = {2022},
	keywords = {attention, deep learning, neural networks, Transformers},
	pages = {109:1--109:28},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/YZE9SPH5/Tay et al. - 2022 - Efficient Transformers A Survey.pdf:application/pdf},
}

@article{munakata_unified_2011,
	title = {A unified framework for inhibitory control},
	volume = {15},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661311001562},
	doi = {10.1016/j.tics.2011.07.011},
	abstract = {Inhibiting unwanted thoughts, actions and emotions figures centrally in daily life, and the prefrontal cortex (PFC) is widely viewed as a source of this inhibitory control. We argue that the function of the PFC is best understood in terms of representing and actively maintaining abstract information, such as goals, which produces two types of inhibitory effects on other brain regions. Inhibition of some subcortical regions takes a directed global form, with prefrontal regions providing contextual information relevant to when to inhibit all processing in a region. Inhibition within neocortical (and some subcortical) regions takes an indirect competitive form, with prefrontal regions providing excitation of goal-relevant options. These distinctions are crucial for understanding the mechanisms of inhibition and how they can be impaired or improved.},
	language = {en},
	number = {10},
	urldate = {2023-03-08},
	journal = {Trends in Cognitive Sciences},
	author = {Munakata, Yuko and Herd, Seth A. and Chatham, Christopher H. and Depue, Brendan E. and Banich, Marie T. and O’Reilly, Randall C.},
	month = oct,
	year = {2011},
	pages = {453--459},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/PN98GAUV/Munakata et al. - 2011 - A unified framework for inhibitory control.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/PW4AR4TX/S1364661311001562.html:text/html},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/7THLLLJ3/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/X2TX4W8E/2112.html:text/html},
}

@misc{lecun_path_2022,
	title = {A {Path} {Towards} {Autonomous} {Machine} {Intelligence} {Version} 0.9.2, 2022-06-27},
	abstract = {How could machines learn as eﬃciently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as conﬁgurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.},
	language = {en},
	publisher = {OpenReview},
	author = {LeCun, Yann},
	month = jun,
	year = {2022},
	file = {LeCun - A Path Towards Autonomous Machine Intelligence Ver.pdf:/Users/jaschaachterberg/Zotero/storage/PR9PEBWC/LeCun - A Path Towards Autonomous Machine Intelligence Ver.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/JMH33V9V/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/253F3SDH/2203.html:text/html},
}

@article{assem_domain-general_2020,
	title = {A {Domain}-{General} {Cognitive} {Core} {Defined} in {Multimodally} {Parcellated} {Human} {Cortex}},
	volume = {30},
	issn = {1047-3211},
	url = {https://doi.org/10.1093/cercor/bhaa023},
	doi = {10.1093/cercor/bhaa023},
	abstract = {Numerous brain imaging studies identified a domain-general or “multiple-demand” (MD) activation pattern accompanying many tasks and may play a core role in cognitive control. Though this finding is well established, the limited spatial localization provided by traditional imaging methods precluded a consensus regarding the precise anatomy, functional differentiation, and connectivity of the MD system. To address these limitations, we used data from 449 subjects from the Human Connectome Project, with the cortex of each individual parcellated using neurobiologically grounded multimodal MRI features. The conjunction of three cognitive contrasts reveals a core of 10 widely distributed MD parcels per hemisphere that are most strongly activated and functionally interconnected, surrounded by a penumbra of 17 additional areas. Outside cerebral cortex, MD activation is most prominent in the caudate and cerebellum. Comparison with canonical resting-state networks shows MD regions concentrated in the fronto-parietal network but also engaging three other networks. MD activations show modest relative task preferences accompanying strong co-recruitment. With distributed anatomical organization, mosaic functional preferences, and strong interconnectivity, we suggest MD regions are well positioned to integrate and assemble the diverse components of cognitive operations. Our precise delineation of MD regions provides a basis for refined analyses of their functions.},
	number = {8},
	urldate = {2023-03-08},
	journal = {Cerebral Cortex},
	author = {Assem, Moataz and Glasser, Matthew F and Van Essen, David C and Duncan, John},
	month = jun,
	year = {2020},
	pages = {4361--4380},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/8SKLVJIL/Assem et al. - 2020 - A Domain-General Cognitive Core Defined in Multimo.pdf:application/pdf;Snapshot:/Users/jaschaachterberg/Zotero/storage/RYV4SKUK/5815289.html:text/html},
}

@article{seguin_inferring_2019,
	title = {Inferring neural signalling directionality from undirected structural connectomes},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-12201-w},
	doi = {10.1038/s41467-019-12201-w},
	abstract = {Neural information flow is inherently directional. To date, investigation of directional communication in the human structural connectome has been precluded by the inability of non-invasive neuroimaging methods to resolve axonal directionality. Here, we demonstrate that decentralized measures of network communication, applied to the undirected topology and geometry of brain networks, can infer putative directions of large-scale neural signalling. We propose the concept of send-receive communication asymmetry to characterize cortical regions as senders, receivers or neutral, based on differences between their incoming and outgoing communication efficiencies. Our results reveal a send-receive cortical hierarchy that recapitulates established organizational gradients differentiating sensory-motor and multimodal areas. We find that send-receive asymmetries are significantly associated with the directionality of effective connectivity derived from spectral dynamic causal modeling. Finally, using fruit fly, mouse and macaque connectomes, we provide further evidence suggesting that directionality of neural signalling is significantly encoded in the undirected architecture of nervous systems.},
	language = {en},
	number = {1},
	urldate = {2023-03-08},
	journal = {Nature Communications},
	author = {Seguin, Caio and Razi, Adeel and Zalesky, Andrew},
	month = sep,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computer science, Network models, Neural circuits},
	pages = {4289},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/ILCLGSTR/Seguin et al. - 2019 - Inferring neural signalling directionality from un.pdf:application/pdf},
}

@article{goni_resting-brain_2014,
	title = {Resting-brain functional connectivity predicted by analytic measures of network communication},
	volume = {111},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1315529111},
	doi = {10.1073/pnas.1315529111},
	abstract = {The complex relationship between structural and functional connectivity, as measured by noninvasive imaging of the human brain, poses many unresolved challenges and open questions. Here, we apply analytic measures of network communication to the structural connectivity of the human brain and explore the capacity of these measures to predict resting-state functional connectivity across three independently acquired datasets. We focus on the layout of shortest paths across the network and on two communication measures—search information and path transitivity—which account for how these paths are embedded in the rest of the network. Search information is an existing measure of information needed to access or trace shortest paths; we introduce path transitivity to measure the density of local detours along the shortest path. We find that both search information and path transitivity predict the strength of functional connectivity among both connected and unconnected node pairs. They do so at levels that match or significantly exceed path length measures, Euclidean distance, as well as computational models of neural dynamics. This capacity suggests that dynamic couplings due to interactions among neural elements in brain networks are substantially influenced by the broader network context adjacent to the shortest communication pathways.},
	number = {2},
	urldate = {2023-03-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Goñi, Joaquín and van den Heuvel, Martijn P. and Avena-Koenigsberger, Andrea and Velez de Mendizabal, Nieves and Betzel, Richard F. and Griffa, Alessandra and Hagmann, Patric and Corominas-Murtra, Bernat and Thiran, Jean-Philippe and Sporns, Olaf},
	month = jan,
	year = {2014},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {833--838},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/J6VXLNED/Goñi et al. - 2014 - Resting-brain functional connectivity predicted by.pdf:application/pdf},
}

@misc{betzel_multi-policy_2022,
	title = {Multi-policy models of interregional communication in the human connectome},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.05.08.490752v1},
	doi = {10.1101/2022.05.08.490752},
	abstract = {Network models of communication, e.g. shortest paths, diffusion, navigation, have become useful tools for studying structure-function relationships in the brain. These models generate estimates of communication efficiency between all pairs of brain regions, which can then be linked to the correlation structure of recorded activity, i.e. functional connectivity (FC). At present, however, communication models have a number of limitations, including difficulty adjudicating between models and the absence of a generic framework for modeling multiple interacting communication policies at the regional level. Here, we present a framework that allows us to incorporate multiple region-specific policies and fit them to empirical estimates of FC. Briefly, we show that many communication policies, including shortest paths and greedy navigation, can be modeled as biased random walks, enabling these policies to be incorporated into the same multi-policy communication model alongside unbiased processes, e.g. diffusion. We show that these multi-policy models outperform existing communication measures while yielding neurobiologically interpretable regional preferences. Further, we show that these models explain the majority of variance in time-varying patterns of FC. Collectively, our framework represents an advance in network-based communication models and establishes a strong link between these patterns and FC. Our findings open up many new avenues for future inquiries and present a flexible framework for modeling anatomically-constrained communication.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {bioRxiv},
	author = {Betzel, Richard F. and Faskowitz, Joshua and Mišić, Bratislav and Sporns, Olaf and Seguin, Caio},
	month = may,
	year = {2022},
	note = {bioRxiv: 2022.05.08.490752
Section: New Results},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/FRFBE9IR/Betzel et al. - 2022 - Multi-policy models of interregional communication.pdf:application/pdf},
}

@article{estrada_physics_2012,
	series = {The {Physics} of {Communicability} in {Complex} {Networks}},
	title = {The physics of communicability in complex networks},
	volume = {514},
	issn = {0370-1573},
	url = {https://www.sciencedirect.com/science/article/pii/S0370157312000154},
	doi = {10.1016/j.physrep.2012.01.006},
	abstract = {A fundamental problem in the study of complex networks is to provide quantitative measures of correlation and information flow between different parts of a system. To this end, several notions of communicability have been introduced and applied to a wide variety of real-world networks in recent years. Several such communicability functions are reviewed in this paper. It is emphasized that communication and correlation in networks can take place through many more routes than the shortest paths, a fact that may not have been sufficiently appreciated in previously proposed correlation measures. In contrast to these, the communicability measures reviewed in this paper are defined by taking into account all possible routes between two nodes, assigning smaller weights to longer ones. This point of view naturally leads to the definition of communicability in terms of matrix functions, such as the exponential, resolvent, and hyperbolic functions, in which the matrix argument is either the adjacency matrix or the graph Laplacian associated with the network. Considerable insight on communicability can be gained by modeling a network as a system of oscillators and deriving physical interpretations, both classical and quantum-mechanical, of various communicability functions. Applications of communicability measures to the analysis of complex systems are illustrated on a variety of biological, physical and social networks. The last part of the paper is devoted to a review of the notion of locality in complex networks and to computational aspects that by exploiting sparsity can greatly reduce the computational efforts for the calculation of communicability functions for large networks.},
	language = {en},
	number = {3},
	urldate = {2023-03-08},
	journal = {Physics Reports},
	author = {Estrada, Ernesto and Hatano, Naomichi and Benzi, Michele},
	month = may,
	year = {2012},
	pages = {89--119},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/ZGL4DY4X/Estrada et al. - 2012 - The physics of communicability in complex networks.pdf:application/pdf},
}

@article{crofts_weighted_2009,
	title = {A weighted communicability measure applied to complex brain networks},
	volume = {6},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2008.0484},
	doi = {10.1098/rsif.2008.0484},
	abstract = {Recent advances in experimental neuroscience allow non-invasive studies of the white matter tracts in the human central nervous system, thus making available cutting-edge brain anatomical data describing these global connectivity patterns. Through magnetic resonance imaging, this non-invasive technique is able to infer a snapshot of the cortical network within the living human brain. Here, we report on the initial success of a new weighted network communicability measure in distinguishing local and global differences between diseased patients and controls. This approach builds on recent advances in network science, where an underlying connectivity structure is used as a means to measure the ease with which information can flow between nodes. One advantage of our method is that it deals directly with the real-valued connectivity data, thereby avoiding the need to discretize the corresponding adjacency matrix, i.e. to round weights up to 1 or down to 0, depending upon some threshold value. Experimental results indicate that the new approach is able to extract biologically relevant features that are not immediately apparent from the raw connectivity data.},
	number = {33},
	urldate = {2023-03-08},
	journal = {Journal of The Royal Society Interface},
	author = {Crofts, Jonathan J and Higham, Desmond J},
	month = jan,
	year = {2009},
	note = {Publisher: Royal Society},
	keywords = {matrix functions, network science, neuroscience, unsupervised classification},
	pages = {411--414},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/ZAEDSDES/Crofts and Higham - 2009 - A weighted communicability measure applied to comp.pdf:application/pdf},
}

@article{avena-koenigsberger_communication_2018,
	title = {Communication dynamics in complex brain networks},
	volume = {19},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn.2017.149},
	doi = {10.1038/nrn.2017.149},
	abstract = {The topology of structural brain networks shapes patterns of interaction and signalling among neurons and brain regions, and the resulting communication dynamics is important for brain function.Different aspects of network topology imply different communication mechanisms, from routing of information through shortest paths to alternative models that involve spreading, diffusion and broadcasting.Different topological attributes promote different types of communication mechanisms.Communication dynamics are subject to competing constraints and demands (trade-offs) among efficiency, cost, versatility and resilience. One aspect of cost is the amount of information needed to implement network communication. This cost is high for routing and low for diffusion, and is likely to be an important factor for determining the biological feasibility of a given communication model.},
	language = {en},
	number = {1},
	urldate = {2023-03-08},
	journal = {Nature Reviews Neuroscience},
	author = {Avena-Koenigsberger, Andrea and Misic, Bratislav and Sporns, Olaf},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Dynamical systems, Network models},
	pages = {17--33},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/QHKAJXLU/Avena-Koenigsberger et al. - 2018 - Communication dynamics in complex brain networks.pdf:application/pdf},
}

@misc{griffa_evolution_2022,
	title = {The evolution of information transmission in mammalian brain networks},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.05.09.491115v1},
	doi = {10.1101/2022.05.09.491115},
	abstract = {Brain communication, defined as information transmission through white-matter connections, is at the foundation of the brain’s computational capacities that virtually subtend all aspects of behavior: from sensory perception shared across mammalian species, to complex cognitive functions in humans. How did communication strategies in macroscale brain networks adapted across evolution to accomplish increasingly complex functions? By applying a novel approach to measure information transmission in mouse, macaque and human brains, we found an evolutionary gradient from selective information processing, where brain regions share information through single polysynaptic pathways, to parallel information processing, where regions communicate through multiple parallel pathways. In humans, parallel processing acts as a major connector between unimodal and transmodal systems. Communication strategies are unique to individuals across different mammalian species, pointing at the individual-level specificity of information routing architecture. Our work provides compelling evidence that different communication strategies are tied to the evolutionary complexity of mammalian brain networks.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {bioRxiv},
	author = {Griffa, Alessandra and Mach, Mathieu and Dedelley, Julien and Gutierrez-Barragan, Daniel and Gozzi, Alessandro and Allali, Gilles and Grandjean, Joanes and Ville, Dimitri Van De and Amico, Enrico},
	month = may,
	year = {2022},
	note = {bioRxiv: 2022.05.09.491115
Section: New Results},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/P9LWRB2X/Griffa et al. - 2022 - The evolution of information transmission in mamma.pdf:application/pdf},
}

@article{avena-koenigsberger_spectrum_2019,
	title = {A spectrum of routing strategies for brain networks},
	volume = {15},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006833},
	doi = {10.1371/journal.pcbi.1006833},
	abstract = {Communication of signals among nodes in a complex network poses fundamental problems of efficiency and cost. Routing of messages along shortest paths requires global information about the topology, while spreading by diffusion, which operates according to local topological features, is informationally “cheap” but inefficient. We introduce a stochastic model for network communication that combines local and global information about the network topology to generate biased random walks on the network. The model generates a continuous spectrum of dynamics that converge onto shortest-path and random-walk (diffusion) communication processes at the limiting extremes. We implement the model on two cohorts of human connectome networks and investigate the effects of varying the global information bias on the network’s communication cost. We identify routing strategies that approach a (highly efficient) shortest-path communication process with a relatively small global information bias on the system’s dynamics. Moreover, we show that the cost of routing messages from and to hub nodes varies as a function of the global information bias driving the system’s dynamics. Finally, we implement the model to identify individual subject differences from a communication dynamics point of view. The present framework departs from the classical shortest paths vs. diffusion dichotomy, unifying both models under a single family of dynamical processes that differ by the extent to which global information about the network topology influences the routing patterns of neural signals traversing the network.},
	language = {en},
	number = {3},
	urldate = {2023-03-08},
	journal = {PLOS Computational Biology},
	author = {Avena-Koenigsberger, Andrea and Yan, Xiaoran and Kolchinsky, Artemy and Heuvel, Martijn P. van den and Hagmann, Patric and Sporns, Olaf},
	month = mar,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Brain mapping, Central nervous system, Centrality, Communications, Network analysis, Neural networks, Random walk, Signaling networks},
	pages = {e1006833},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/7IAFP3NV/Avena-Koenigsberger et al. - 2019 - A spectrum of routing strategies for brain network.pdf:application/pdf},
}

@misc{shrivastava_beyond_2017,
	title = {Beyond {Skip} {Connections}: {Top}-{Down} {Modulation} for {Object} {Detection}},
	shorttitle = {Beyond {Skip} {Connections}},
	url = {http://arxiv.org/abs/1612.06851},
	doi = {10.48550/arXiv.1612.06851},
	abstract = {In recent years, we have seen tremendous progress in the field of object detection. Most of the recent improvements have been achieved by targeting deeper feedforward networks. However, many hard object categories such as bottle, remote, etc. require representation of fine details and not just coarse, semantic representations. But most of these fine details are lost in the early convolutional layers. What we need is a way to incorporate finer details from lower layers into the detection architecture. Skip connections have been proposed to combine high-level and low-level features, but we argue that selecting the right features from low-level requires top-down contextual information. Inspired by the human visual pathway, in this paper we propose top-down modulations as a way to incorporate fine details into the detection framework. Our approach supplements the standard bottom-up, feedforward ConvNet with a top-down modulation (TDM) network, connected using lateral connections. These connections are responsible for the modulation of lower layer filters, and the top-down network handles the selection and integration of contextual information and low-level features. The proposed TDM architecture provides a significant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16, 35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any bells and whistles (e.g., multi-scale, iterative box refinement, etc.).},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Shrivastava, Abhinav and Sukthankar, Rahul and Malik, Jitendra and Gupta, Abhinav},
	month = sep,
	year = {2017},
	note = {arXiv:1612.06851 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/Y4ULCUHP/Shrivastava et al. - 2017 - Beyond Skip Connections Top-Down Modulation for O.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/YC72RXCV/1612.html:text/html},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 19 pages, 15 figures},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/S88QPN68/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/85SNYKSA/2001.html:text/html},
}

@misc{zaheer_big_2021,
	title = {Big {Bird}: {Transformers} for {Longer} {Sequences}},
	shorttitle = {Big {Bird}},
	url = {http://arxiv.org/abs/2007.14062},
	doi = {10.48550/arXiv.2007.14062},
	abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	month = jan,
	year = {2021},
	note = {arXiv:2007.14062 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/93SSV952/Zaheer et al. - 2021 - Big Bird Transformers for Longer Sequences.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/SJ5IB2GI/2007.html:text/html},
}

@misc{achterberg_spatially-embedded_2022,
	title = {Spatially-embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.11.17.516914v1},
	doi = {10.1101/2022.11.17.516914},
	abstract = {Brain networks exist within the confines of resource limitations. As a result, a brain network must overcome metabolic costs of growing and sustaining the network within its physical space, while simultaneously implementing its required information processing. To observe the effect of these processes, we introduce the spatially-embedded recurrent neural network (seRNN). seRNNs learn basic task-related inferences while existing within a 3D Euclidean space, where the communication of constituent neurons is constrained by a sparse connectome. We find that seRNNs, similar to primate cerebral cortices, naturally converge on solving inferences using modular small-world networks, in which functionally similar units spatially configure themselves to utilize an energetically-efficient mixed-selective code. As all these features emerge in unison, seRNNs reveal how many common structural and functional brain motifs are strongly intertwined and can be attributed to basic biological optimization processes. seRNNs can serve as model systems to bridge between structural and functional research communities to move neuroscientific understanding forward.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {bioRxiv},
	author = {Achterberg, Jascha and Akarca, Danyal and Strouse, D. J. and Duncan, John and Astle, Duncan E.},
	month = nov,
	year = {2022},
	note = {bioRxiv: 2022.11.17.516914
Section: New Results},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/XN5IMYBZ/Achterberg et al. - 2022 - Spatially-embedded recurrent neural networks revea.pdf:application/pdf},
}

@misc{falandays_potential_2023,
	title = {A {Potential} {Mechanism} for {Gibsonian} {Resonance}: {Behavioral} {Entrainment} {Emerges} from {Local} {Homeostasis} in an {Unsupervised} {Reservoir} {Network}},
	shorttitle = {A {Potential} {Mechanism} for {Gibsonian} {Resonance}},
	url = {https://psyarxiv.com/pt7bn/},
	doi = {10.31234/osf.io/pt7bn},
	abstract = {While the cognitivist school of thought holds that the mind is analogous to a computer, performing logical operations over internal representations, the tradition of ecological psychology contends that organisms can directly "resonate" to information for action and perception without the need for a representational intermediary. The concept of resonance has played an important role in ecological psychology, but it remains a metaphor. Supplying a mechanistic account of resonance requires a non-representational account of central nervous system (CNS) dynamics. We present a series of simple models in which a reservoir network with homeostatic nodes is used to control a simple agent embedded in an environment. This network spontaneously produces behaviors that are adaptive in each context, including (1) visually tracking a moving object, (2) substantially above-chance performance in the arcade game Pong, (2) and avoiding walls while controlling a mobile agent. These results may represent a useful step towards a mechanistic grounding of resonance and a view of the CNS that is compatible with ecological psychology.},
	language = {en-us},
	urldate = {2023-03-08},
	publisher = {PsyArXiv},
	author = {Falandays, J. Benjamin and Yoshimi, Jeff and Warren, William and Spivey, Michael},
	month = feb,
	year = {2023},
	keywords = {Action, computational cognitive neuroscience, ecological psychology, Perception, perception-action, reservoir computing, Social and Behavioral Sciences},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/85TLKT83/Falandays et al. - 2023 - A Potential Mechanism for Gibsonian Resonance Beh.pdf:application/pdf},
}

@article{duncan_adaptive_2001,
	title = {An adaptive coding model of neural function in prefrontal cortex},
	volume = {2},
	copyright = {2001 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/35097575},
	doi = {10.1038/35097575},
	abstract = {The prefrontal cortex is crucial for effective, organized behaviour. On the basis of data from functional neuroimaging in humans and single-cell electrophysiology in the behaving monkey, this paper proposes an adaptive coding model of prefrontal function. Functional imaging data show some specific associations between particular cognitive functions and local prefrontal activations. However, there is also strong evidence for common regions of recruitment in response to a wide range of different cognitive demands. These regions include the cortex in and around the posterior part of the inferior frontal sulcus, the frontal operculum/anterior insula and the dorsal part of the anterior cingulate. Converging data come from electrophysiology in the monkey. Over large regions of the lateral frontal cortex, many cells show activity related to whatever arbitrary task a monkey has been trained to perform. These cells code many aspects of task events, including information relevant to stimuli, responses, working memory delays, response rules and reward states. Cells of many different types are found closely intermingled and widely distributed across the lateral surface. Even individual cells show evidence for adaptability of function, coding different information in different task contexts. In the adaptive coding model, the central idea is that neurons throughout large regions of prefrontal cortex have the capacity to code many different types of information. In any given task context, neurons adapt to preserve only information of relevance to current behaviour. At the same time, they support the representation of related information elsewhere in the brain, including coding of relevant stimuli, responses, representations in semantic memory and reward states. This view links previous accounts of prefrontal function that are based on concepts of working memory, selective attention and control. The model implies that, within the prefrontal cortex, regional specializations will be statistical rather than absolute. Neurons with the capacity to contribute to any given function might be widely distributed across the prefrontal cortex, although possibly with different distributions for different functions. This view of quantitative rather than qualitative specialization is consistent with data from electrophysiological, imaging and lesion studies. It suggests that conclusions concerning regional specialization will depend on criteria for assessing selectivity and, in imaging experiments, on experimental demand and power. The adaptive coding model points to several key issues and approaches for future work. These include an assessment of long- and short-term adaptability, a quantitative comparison of cell properties between different prefrontal regions, and an investigation of how prefrontal adaptability differs from that in other cortical regions.},
	language = {en},
	number = {11},
	urldate = {2023-03-08},
	journal = {Nature Reviews Neuroscience},
	author = {Duncan, John},
	month = nov,
	year = {2001},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences},
	pages = {820--829},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/IM4M76JA/Duncan - 2001 - An adaptive coding model of neural function in pre.pdf:application/pdf},
}

@article{achterberg_one-shot_2022,
	title = {A {One}-{Shot} {Shift} from {Explore} to {Exploit} in {Monkey} {Prefrontal} {Cortex}},
	volume = {42},
	copyright = {Copyright © 2022 Achterberg et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/42/2/276},
	doi = {10.1523/JNEUROSCI.1338-21.2021},
	abstract = {Much animal learning is slow, with cumulative changes in behavior driven by reward prediction errors. When the abstract structure of a problem is known, however, both animals and formal learning models can rapidly attach new items to their roles within this structure, sometimes in a single trial. Frontal cortex is likely to play a key role in this process. To examine information seeking and use in a known problem structure, we trained monkeys in an explore/exploit task, requiring the animal first to test objects for their association with reward, then, once rewarded objects were found, to reselect them on further trials for further rewards. Many cells in the frontal cortex showed an explore/exploit preference aligned with one-shot learning in the monkeys' behavior: the population switched from an explore state to an exploit state after a single trial of learning but partially maintained the explore state if an error indicated that learning had failed. Binary switch from explore to exploit was not explained by continuous changes linked to expectancy or prediction error. Explore/exploit preferences were independent for two stages of the trial: object selection and receipt of feedback. Within an established task structure, frontal activity may control the separate processes of explore and exploit, switching in one trial between the two.
SIGNIFICANCE STATEMENT Much animal learning is slow, with cumulative changes in behavior driven by reward prediction errors. When the abstract structure a problem is known, however, both animals and formal learning models can rapidly attach new items to their roles within this structure. To address transitions in neural activity during one-shot learning, we trained monkeys in an explore/exploit task using familiar objects and a highly familiar task structure. When learning was rapid, many frontal neurons showed a binary, one-shot switch between explore and exploit. Within an established task structure, frontal activity may control the separate operations of exploring alternative objects to establish their current role, then exploiting this knowledge for further reward.},
	language = {en},
	number = {2},
	urldate = {2023-03-08},
	journal = {Journal of Neuroscience},
	author = {Achterberg, Jascha and Kadohisa, Mikiko and Watanabe, Kei and Kusunoki, Makoto and Buckley, Mark J. and Duncan, John},
	month = jan,
	year = {2022},
	pmid = {34782437},
	note = {Publisher: Society for Neuroscience
Section: Research Articles},
	keywords = {attention, exploit, explore, frontal cortex, one-shot learning, primate},
	pages = {276--287},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/G4Z9FD7E/Achterberg et al. - 2022 - A One-Shot Shift from Explore to Exploit in Monkey.pdf:application/pdf},
}

@article{stokes_activity-silent_2015,
	title = {‘{Activity}-silent’ working memory in prefrontal cortex: a dynamic coding framework},
	volume = {19},
	issn = {1364-6613},
	shorttitle = {‘{Activity}-silent’ working memory in prefrontal cortex},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661315001023},
	doi = {10.1016/j.tics.2015.05.004},
	abstract = {Working memory (WM) provides the functional backbone to high-level cognition. Maintenance in WM is often assumed to depend on the stationary persistence of neural activity patterns that represent memory content. However, accumulating evidence suggests that persistent delay activity does not always accompany WM maintenance but instead seems to wax and wane as a function of the current task relevance of memoranda. Furthermore, new methods for measuring and analysing population-level patterns show that activity states are highly dynamic. At first glance, these dynamics seem at odds with the very nature of WM. How can we keep a stable thought in mind while brain activity is constantly changing? This review considers how neural dynamics might be functionally important for WM maintenance.},
	language = {en},
	number = {7},
	urldate = {2023-03-08},
	journal = {Trends in Cognitive Sciences},
	author = {Stokes, Mark G.},
	month = jul,
	year = {2015},
	pages = {394--405},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/65WNWKWI/Stokes - 2015 - ‘Activity-silent’ working memory in prefrontal cor.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/X4SZWXE7/S1364661315001023.html:text/html},
}

@article{tang_prefrontal_2022,
	title = {Prefrontal cortical plasticity during learning of cognitive tasks},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-27695-6},
	doi = {10.1038/s41467-021-27695-6},
	abstract = {Training in working memory tasks is associated with lasting changes in prefrontal cortical activity. To assess the neural activity changes induced by training, we recorded single units, multi-unit activity (MUA) and local field potentials (LFP) with chronic electrode arrays implanted in the prefrontal cortex of two monkeys, throughout the period they were trained to perform cognitive tasks. Mastering different task phases was associated with distinct changes in neural activity, which included recruitment of larger numbers of neurons, increases or decreases of their firing rate, changes in the correlation structure between neurons, and redistribution of power across LFP frequency bands. In every training phase, changes induced by the actively learned task were also observed in a control task, which remained the same across the training period. Our results reveal how learning to perform cognitive tasks induces plasticity of prefrontal cortical activity, and how activity changes may generalize between tasks.},
	language = {en},
	number = {1},
	urldate = {2023-03-08},
	journal = {Nature Communications},
	author = {Tang, Hua and Riley, Mitchell R. and Singh, Balbir and Qi, Xue-Lian and Blake, David T. and Constantinidis, Christos},
	month = jan,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cognitive control, Working memory},
	pages = {90},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/TMZYPYC7/Tang et al. - 2022 - Prefrontal cortical plasticity during learning of .pdf:application/pdf},
}

@article{garcia-cabezas_mirror_2017,
	title = {Mirror trends of plasticity and stability indicators in primate prefrontal cortex},
	volume = {46},
	issn = {1460-9568},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.13706},
	doi = {10.1111/ejn.13706},
	abstract = {Research on plasticity markers in the cerebral cortex has largely focused on their timing of expression and role in shaping circuits during critical and normal periods. By contrast, little attention has been focused on the spatial dimension of plasticity–stability across cortical areas. The rationale for this analysis is based on the systematic variation in cortical structure that parallels functional specialization and raises the possibility of varying levels of plasticity. Here, we investigated in adult rhesus monkeys the expression of markers related to synaptic plasticity or stability in prefrontal limbic and eulaminate areas that vary in laminar structure. Our findings revealed that limbic areas are impoverished in three markers of stability: intracortical myelin, the lectin Wisteria floribunda agglutinin, which labels perineuronal nets, and parvalbumin, which is expressed in a class of strong inhibitory neurons. By contrast, prefrontal limbic areas were enriched in the enzyme calcium/calmodulin-dependent protein kinase II (CaMKII), known to enhance plasticity. Eulaminate areas have more elaborate laminar architecture than limbic areas and showed the opposite trend: they were enriched in markers of stability and had lower expression of the plasticity-related marker CaMKII. The expression of glial fibrillary acidic protein (GFAP), a marker of activated astrocytes, was also higher in limbic areas, suggesting that cellular stress correlates with the rate of circuit reshaping. Elevated markers of plasticity may endow limbic areas with flexibility necessary for learning and memory within an affective context, but may also render them vulnerable to abnormal structural changes, as seen in neurologic and psychiatric diseases.},
	language = {en},
	number = {8},
	urldate = {2023-03-08},
	journal = {European Journal of Neuroscience},
	author = {García-Cabezas, Miguel A. and Joyce, Mary Kate P. and John, Yohan J. and Zikopoulos, Basilis and Barbas, Helen},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.13706},
	keywords = {eulaminate, limbic, macaque monkey, plasticity, selective vulnerability},
	pages = {2392--2405},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/THVL9DNM/García-Cabezas et al. - 2017 - Mirror trends of plasticity and stability indicato.pdf:application/pdf;Snapshot:/Users/jaschaachterberg/Zotero/storage/4YVHFCBS/ejn.html:text/html},
}

@article{duncan_multiple-demand_2010,
	title = {The multiple-demand ({MD}) system of the primate brain: mental programs for intelligent behaviour},
	volume = {14},
	issn = {1364-6613},
	shorttitle = {The multiple-demand ({MD}) system of the primate brain},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661310000057},
	doi = {10.1016/j.tics.2010.01.004},
	abstract = {A common or multiple-demand (MD) pattern of frontal and parietal activity is associated with diverse cognitive demands, and with standard tests of fluid intelligence. In intelligent behaviour, goals are achieved by assembling a series of sub-tasks, creating structured mental programs. Single cell and functional magnetic resonance imaging (fMRI) data indicate a key role for MD cortex in defining and controlling the parts of such programs, with focus on the specific content of a current cognitive operation, rapid reorganization as mental focus is changed, and robust separation of successive task steps. Resembling the structured problem-solving of symbolic artificial intelligence, the mental programs of MD cortex appear central to intelligent thought and action.},
	language = {en},
	number = {4},
	urldate = {2023-03-08},
	journal = {Trends in Cognitive Sciences},
	author = {Duncan, John},
	month = apr,
	year = {2010},
	pages = {172--179},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/MX7ZWLHB/Duncan - 2010 - The multiple-demand (MD) system of the primate bra.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/WLCI3J3N/S1364661310000057.html:text/html},
}

@article{lindsay_attention_2020,
	title = {Attention in {Psychology}, {Neuroscience}, and {Machine} {Learning}},
	volume = {14},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2020.00029},
	abstract = {Attention is the important ability to flexibly control limited computational resources. It has been studied in conjunction with many other topics in neuroscience and psychology including awareness, vigilance, saliency, executive control, and learning. It has also recently been applied in several domains in machine learning. The relationship between the study of biological attention and its use as a tool to enhance artificial neural networks is not always clear. This review starts by providing an overview of how attention is conceptualized in the neuroscience and psychology literature. It then covers several use cases of attention in machine learning, indicating their biological counterparts where they exist. Finally, the ways in which artificial attention can be further inspired by biology for the production of complex and integrative systems is explored.},
	urldate = {2023-03-08},
	journal = {Frontiers in Computational Neuroscience},
	author = {Lindsay, Grace W.},
	year = {2020},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/PWUPKVBL/Lindsay - 2020 - Attention in Psychology, Neuroscience, and Machine.pdf:application/pdf},
}

@article{smolensky_tensor_1990,
	title = {Tensor product variable binding and the representation of symbolic structures in connectionist systems},
	volume = {46},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/000437029090007M},
	doi = {10.1016/0004-3702(90)90007-M},
	abstract = {A general method, the tensor product representation, is defined for the connectionist representation of value/variable bindings. The technique is a formalization of the idea that a set of value/variable pairs can be represented by accumulating activity in a collection of units each of which computes the product of a feature of a variable and a feature of its value. The method allows the fully distributed representation of bindings and symbolic structures. Fully and partially localized special cases of the tensor product representation reduce to existing cases of connectionist representations of structured data. The representation rests on a principled analysis of structure; it saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; and it enables analysis of the interference of symbolic structures stored in associative memories. It has also served as the basis for working connectionist models of high-level cognitive tasks.},
	language = {en},
	number = {1},
	urldate = {2023-03-08},
	journal = {Artificial Intelligence},
	author = {Smolensky, Paul},
	month = nov,
	year = {1990},
	pages = {159--216},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/LPI3DQRC/Smolensky - 1990 - Tensor product variable binding and the representa.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/ITAEXS5C/000437029090007M.html:text/html},
}

@article{botvinick_reinforcement_2019,
	title = {Reinforcement {Learning}, {Fast} and {Slow}},
	volume = {23},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30061-0},
	doi = {10.1016/j.tics.2019.02.006},
	language = {English},
	number = {5},
	urldate = {2023-03-08},
	journal = {Trends in Cognitive Sciences},
	author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
	month = may,
	year = {2019},
	pmid = {31003893},
	note = {Publisher: Elsevier},
	pages = {408--422},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/RQZ65M5E/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf:application/pdf},
}

@misc{von_oswald_transformers_2022,
	title = {Transformers learn in-context by gradient descent},
	url = {http://arxiv.org/abs/2212.07677},
	doi = {10.48550/arXiv.2212.07677},
	abstract = {Transformers have become the state-of-the-art neural network architecture across numerous domains of machine learning. This is partly due to their celebrated ability to transfer and to learn in-context based on few examples. Nevertheless, the mechanisms by which Transformers become in-context learners are not well understood and remain mostly an intuition. Here, we argue that training Transformers on auto-regressive tasks can be closely related to well-known gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers implement gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of optimized Transformers that learn in-context. Furthermore, we identify how Transformers surpass plain gradient descent by an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, João and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
	month = dec,
	year = {2022},
	note = {arXiv:2212.07677 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/9SG5PZ22/von Oswald et al. - 2022 - Transformers learn in-context by gradient descent.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/XDVIYSMN/2212.html:text/html},
}

@article{buschman_goal-direction_2014,
	title = {Goal-direction and top-down control},
	volume = {369},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2013.0471},
	doi = {10.1098/rstb.2013.0471},
	abstract = {We review the neural mechanisms that support top-down control of behaviour and suggest that goal-directed behaviour uses two systems that work in concert. A basal ganglia-centred system quickly learns simple, fixed goal-directed behaviours while a prefrontal cortex-centred system gradually learns more complex (abstract or long-term) goal-directed behaviours. Interactions between these two systems allow top-down control mechanisms to learn how to direct behaviour towards a goal but also how to guide behaviour when faced with a novel situation.},
	number = {1655},
	urldate = {2023-03-08},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Buschman, Timothy J. and Miller, Earl K.},
	month = nov,
	year = {2014},
	note = {Publisher: Royal Society},
	keywords = {basal ganglia, cognition, frontal lobe, goal direction, learning},
	pages = {20130471},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/JRYXGRIR/Buschman and Miller - 2014 - Goal-direction and top-down control.pdf:application/pdf},
}

@misc{macdowell_multiplexed_2023,
	title = {Multiplexed {Subspaces} {Route} {Neural} {Activity} {Across} {Brain}-wide {Networks}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.02.08.527772v1},
	doi = {10.1101/2023.02.08.527772},
	abstract = {Cognition is flexible. Behaviors can change on a moment-by-moment basis. Such flexibility is thought to rely on the brain’s ability to route information through different networks of brain regions in order to support different cognitive computations. However, the mechanisms that determine which network of brain regions is engaged are unknown. To address this, we combined cortex-wide calcium imaging with high-density electrophysiological recordings in eight cortical and subcortical regions of mice. Different dimensions within the population activity of each brain region were functionally connected with different cortex-wide ‘subspace networks’ of regions. These subspace networks were multiplexed, allowing a brain region to simultaneously interact with multiple independent, yet overlapping, networks. Alignment of neural activity within a region to a specific subspace network dimension predicted how neural activity propagated between regions. Thus, changing the geometry of the neural representation within a brain region could be a mechanism to selectively engage different brain-wide networks to support cognitive flexibility.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {bioRxiv},
	author = {MacDowell, Camden J. and Libby, Alexandra and Jahn, Caroline I. and Tafazoli, Sina and Buschman, Timothy J.},
	month = feb,
	year = {2023},
	note = {bioRxiv: 2023.02.08.527772
Section: New Results},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/PT9TZ9AF/MacDowell et al. - 2023 - Multiplexed Subspaces Route Neural Activity Across.pdf:application/pdf},
}

@article{whittington_tolman-eichenbaum_2020,
	title = {The {Tolman}-{Eichenbaum} {Machine}: {Unifying} {Space} and {Relational} {Memory} through {Generalization} in the {Hippocampal} {Formation}},
	volume = {183},
	issn = {0092-8674},
	shorttitle = {The {Tolman}-{Eichenbaum} {Machine}},
	url = {https://www.sciencedirect.com/science/article/pii/S009286742031388X},
	doi = {10.1016/j.cell.2020.10.024},
	abstract = {The hippocampal-entorhinal system is important for spatial and relational memory tasks. We formally link these domains, provide a mechanistic understanding of the hippocampal role in generalization, and offer unifying principles underlying many entorhinal and hippocampal cell types. We propose medial entorhinal cells form a basis describing structural knowledge, and hippocampal cells link this basis with sensory representations. Adopting these principles, we introduce the Tolman-Eichenbaum machine (TEM). After learning, TEM entorhinal cells display diverse properties resembling apparently bespoke spatial responses, such as grid, band, border, and object-vector cells. TEM hippocampal cells include place and landmark cells that remap between environments. Crucially, TEM also aligns with empirically recorded representations in complex non-spatial tasks. TEM also generates predictions that hippocampal remapping is not random as previously believed; rather, structural knowledge is preserved across environments. We confirm this structural transfer over remapping in simultaneously recorded place and grid cells.},
	language = {en},
	number = {5},
	urldate = {2023-03-08},
	journal = {Cell},
	author = {Whittington, James C. R. and Muller, Timothy H. and Mark, Shirley and Chen, Guifen and Barry, Caswell and Burgess, Neil and Behrens, Timothy E. J.},
	month = nov,
	year = {2020},
	keywords = {entorhinal cortex, generalization, grid cells, hippocampus, neural networks, non-spatial reasoning, place cells, representation learning},
	pages = {1249--1263.e23},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/KEX7L5Y6/Whittington et al. - 2020 - The Tolman-Eichenbaum Machine Unifying Space and .pdf:application/pdf},
}

@misc{dekker_determinants_2022,
	title = {Determinants of human compositional generalization},
	url = {https://psyarxiv.com/qnpw6/},
	doi = {10.31234/osf.io/qnpw6},
	abstract = {Generalisation (or transfer) is the ability to repurpose knowledge in novel settings. It is often asserted that generalisation is an important ingredient of human intelligence, but its extent, nature and determinants have proved controversial. Here, we re-examine this question with a new paradigm that formalises the transfer learning problem as one of recomposing existing functions to solve unseen problems. We find that people can generalise compositionally in ways that are elusive for standard neural networks, and that human generalisation benefits from training regimes in which items are axis-aligned and temporally correlated. We describe a neural network model based around a Hebbian gating process which can capture how human generalisation benefits from different training curricula. We additionally find that adult humans tend to learn composable functions asynchronously, exhibiting discontinuities in learning that resemble those seen in child development.},
	language = {en-us},
	urldate = {2023-03-08},
	publisher = {PsyArXiv},
	author = {Dekker, Ronald Boris and Otto, Fabian and Summerfield, Christopher},
	month = mar,
	year = {2022},
	keywords = {Cognitive Psychology, Computational Neuroscience, Concepts and Categories, generalisation, Learning, neural network, Neuroscience, Social and Behavioral Sciences},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/3SZ2U5P3/Dekker et al. - 2022 - Determinants of human compositional generalization.pdf:application/pdf},
}

@article{masse_circuit_2019,
	title = {Circuit mechanisms for the maintenance and manipulation of information in working memory},
	volume = {22},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-019-0414-3},
	doi = {10.1038/s41593-019-0414-3},
	abstract = {Recently it has been proposed that information in working memory (WM) may not always be stored in persistent neuronal activity but can be maintained in ‘activity-silent’ hidden states, such as synaptic efficacies endowed with short-term synaptic plasticity. To test this idea computationally, we investigated recurrent neural network models trained to perform several WM-dependent tasks, in which WM representation emerges from learning and is not a priori assumed to depend on self-sustained persistent activity. We found that short-term synaptic plasticity can support the short-term maintenance of information, provided that the memory delay period is sufficiently short. However, in tasks that require actively manipulating information, persistent activity naturally emerges from learning, and the amount of persistent activity scales with the degree of manipulation required. These results shed insight into the current debate on WM encoding and suggest that persistent activity can vary markedly between short-term memory tasks with different cognitive demands.},
	language = {en},
	number = {7},
	urldate = {2023-03-08},
	journal = {Nature Neuroscience},
	author = {Masse, Nicolas Y. and Yang, Guangyu R. and Song, H. Francis and Wang, Xiao-Jing and Freedman, David J.},
	month = jul,
	year = {2019},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Learning and memory, Neural circuits, Working memory},
	pages = {1159--1167},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/QP4R64FS/Masse et al. - 2019 - Circuit mechanisms for the maintenance and manipul.pdf:application/pdf},
}

@misc{whittington_relating_2022,
	title = {Relating transformers to models and neural representations of the hippocampal formation},
	url = {http://arxiv.org/abs/2112.04035},
	doi = {10.48550/arXiv.2112.04035},
	abstract = {Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Whittington, James C. R. and Warren, Joseph and Behrens, Timothy E. J.},
	month = mar,
	year = {2022},
	note = {arXiv:2112.04035 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/E2JJ387S/Whittington et al. - 2022 - Relating transformers to models and neural represe.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/H9YS9IR9/2112.html:text/html},
}

@misc{lowe_putting_2020,
	title = {Putting {An} {End} to {End}-to-{End}: {Gradient}-{Isolated} {Learning} of {Representations}},
	shorttitle = {Putting {An} {End} to {End}-to-{End}},
	url = {http://arxiv.org/abs/1905.11786},
	doi = {10.48550/arXiv.1905.11786},
	abstract = {We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Löwe, Sindy and O'Connor, Peter and Veeling, Bastiaan S.},
	month = jan,
	year = {2020},
	note = {arXiv:1905.11786 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Honorable Mention for Outstanding New Directions Paper Award at NeurIPS 2019},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/HNAMYB3F/Löwe et al. - 2020 - Putting An End to End-to-End Gradient-Isolated Le.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/RRS39PJ3/1905.html:text/html},
}

@misc{ren_scaling_2023,
	title = {Scaling {Forward} {Gradient} {With} {Local} {Losses}},
	url = {http://arxiv.org/abs/2210.03310},
	doi = {10.48550/arXiv.2210.03310},
	abstract = {Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Ren, Mengye and Kornblith, Simon and Liao, Renjie and Hinton, Geoffrey},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03310 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 31 pages, ICLR 2023},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/W6G4W3LJ/Ren et al. - 2023 - Scaling Forward Gradient With Local Losses.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/YK2IGVQ5/2210.html:text/html},
}

@misc{hinton_forward-forward_2022,
	title = {The {Forward}-{Forward} {Algorithm}: {Some} {Preliminary} {Investigations}},
	shorttitle = {The {Forward}-{Forward} {Algorithm}},
	url = {http://arxiv.org/abs/2212.13345},
	doi = {10.48550/arXiv.2212.13345},
	abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Hinton, Geoffrey},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13345 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jaschaachterberg/Zotero/storage/GDHH8RL5/Hinton - 2022 - The Forward-Forward Algorithm Some Preliminary In.pdf:application/pdf;arXiv.org Snapshot:/Users/jaschaachterberg/Zotero/storage/RER77J5R/2212.html:text/html},
}

@article{luppi_synergistic_2022,
	title = {A synergistic core for human brain evolution and cognition},
	volume = {25},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-022-01070-0},
	doi = {10.1038/s41593-022-01070-0},
	abstract = {How does the organization of neural information processing enable humans’ sophisticated cognition? Here we decompose functional interactions between brain regions into synergistic and redundant components, revealing their distinct information-processing roles. Combining functional and structural neuroimaging with meta-analytic results, we demonstrate that redundant interactions are predominantly associated with structurally coupled, modular sensorimotor processing. Synergistic interactions instead support integrative processes and complex cognition across higher-order brain networks. The human brain leverages synergistic information to a greater extent than nonhuman primates, with high-synergy association cortices exhibiting the highest degree of evolutionary cortical expansion. Synaptic density mapping from positron emission tomography and convergent molecular and metabolic evidence demonstrate that synergistic interactions are supported by receptor diversity and human-accelerated genes underpinning synaptic function. This information-resolved approach provides analytic tools to disentangle information integration from coupling, enabling richer, more accurate interpretations of functional connectivity, and illuminating how the human neurocognitive architecture navigates the trade-off between robustness and integration.},
	language = {en},
	number = {6},
	urldate = {2023-03-08},
	journal = {Nature Neuroscience},
	author = {Luppi, Andrea I. and Mediano, Pedro A. M. and Rosas, Fernando E. and Holland, Negin and Fryer, Tim D. and O’Brien, John T. and Rowe, James B. and Menon, David K. and Bor, Daniel and Stamatakis, Emmanuel A.},
	month = jun,
	year = {2022},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Cognitive control, Dynamical systems, Genetics of the nervous system, Network models},
	pages = {771--782},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/JKGBQWD5/Luppi et al. - 2022 - A synergistic core for human brain evolution and c.pdf:application/pdf},
}

@article{horvat_spatial_2016,
	title = {Spatial {Embedding} and {Wiring} {Cost} {Constrain} the {Functional} {Layout} of the {Cortical} {Network} of {Rodents} and {Primates}},
	volume = {14},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002512},
	doi = {10.1371/journal.pbio.1002512},
	abstract = {Mammals show a wide range of brain sizes, reflecting adaptation to diverse habitats. Comparing interareal cortical networks across brains of different sizes and mammalian orders provides robust information on evolutionarily preserved features and species-specific processing modalities. However, these networks are spatially embedded, directed, and weighted, making comparisons challenging. Using tract tracing data from macaque and mouse, we show the existence of a general organizational principle based on an exponential distance rule (EDR) and cortical geometry, enabling network comparisons within the same model framework. These comparisons reveal the existence of network invariants between mouse and macaque, exemplified in graph motif profiles and connection similarity indices, but also significant differences, such as fractionally smaller and much weaker long-distance connections in the macaque than in mouse. The latter lends credence to the prediction that long-distance cortico-cortical connections could be very weak in the much-expanded human cortex, implying an increased susceptibility to disconnection syndromes such as Alzheimer disease and schizophrenia. Finally, our data from tracer experiments involving only gray matter connections in the primary visual areas of both species show that an EDR holds at local scales as well (within 1.5 mm), supporting the hypothesis that it is a universally valid property across all scales and, possibly, across the mammalian class.},
	language = {en},
	number = {7},
	urldate = {2023-03-08},
	journal = {PLOS Biology},
	author = {Horvát, Szabolcs and Gămănuț, Răzvan and Ercsey-Ravasz, Mária and Magrou, Loïc and Gămănuț, Bianca and Essen, David C. Van and Burkhalter, Andreas and Knoblauch, Kenneth and Toroczkai, Zoltán and Kennedy, Henry},
	month = jul,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Central nervous system, Connectomics, Macaque, Mice, Network analysis, Network motifs, Neural networks, Neurons},
	pages = {e1002512},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/YY7VFQ6C/Horvát et al. - 2016 - Spatial Embedding and Wiring Cost Constrain the Fu.pdf:application/pdf},
}

@article{dehaene_neuronal_1998,
	title = {A neuronal model of a global workspace in effortful cognitive tasks},
	volume = {95},
	url = {https://www.pnas.org/doi/10.1073/pnas.95.24.14529},
	doi = {10.1073/pnas.95.24.14529},
	abstract = {A minimal hypothesis is proposed concerning the brain processes underlying effortful tasks. It distinguishes two main computational spaces: a unique global workspace composed of distributed and heavily interconnected neurons with long-range axons, and a set of specialized and modular perceptual, motor, memory, evaluative, and attentional processors. Workspace neurons are mobilized in effortful tasks for which the specialized processors do not suffice. They selectively mobilize or suppress, through descending connections, the contribution of specific processor neurons. In the course of task performance, workspace neurons become spontaneously coactivated, forming discrete though variable spatio-temporal patterns subject to modulation by vigilance signals and to selection by reward signals. A computer simulation of the Stroop task shows workspace activation to increase during acquisition of a novel task, effortful execution, and after errors. We outline predictions for spatio-temporal activation patterns during brain imaging, particularly about the contribution of dorsolateral prefrontal cortex and anterior cingulate to the workspace.},
	number = {24},
	urldate = {2023-03-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Dehaene, Stanislas and Kerszberg, Michel and Changeux, Jean-Pierre},
	month = nov,
	year = {1998},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {14529--14534},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/KL3V5RJN/Dehaene et al. - 1998 - A neuronal model of a global workspace in effortfu.pdf:application/pdf},
}

@article{power_evidence_2013,
	title = {Evidence for {Hubs} in {Human} {Functional} {Brain} {Networks}},
	volume = {79},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627313006594},
	doi = {10.1016/j.neuron.2013.07.035},
	abstract = {Hubs integrate and distribute information in powerful ways due to the number and positioning of their contacts in a network. Several resting-state functional connectivity MRI reports have implicated regions of the default mode system as brain hubs; we demonstrate that previous degree-based approaches to hub identification may have identified portions of large brain systems rather than critical nodes of brain networks. We utilize two methods to identify hub-like brain regions: (1) finding network nodes that participate in multiple subnetworks of the brain, and (2) finding spatial locations in which several systems are represented within a small volume. These methods converge on a distributed set of regions that differ from previous reports on hubs. This work identifies regions that support multiple systems, leading to spatially constrained predictions about brain function that may be tested in terms of lesions, evoked responses, and dynamic patterns of activity.},
	language = {en},
	number = {4},
	urldate = {2023-03-08},
	journal = {Neuron},
	author = {Power, Jonathan D. and Schlaggar, Bradley L. and Lessov-Schlaggar, Christina N. and Petersen, Steven E.},
	month = aug,
	year = {2013},
	pages = {798--813},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/E8FUCR6A/Power et al. - 2013 - Evidence for Hubs in Human Functional Brain Networ.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/AE2Q9K5J/S0896627313006594.html:text/html},
}

@article{crowe_rapid_2010,
	title = {Rapid {Sequences} of {Population} {Activity} {Patterns} {Dynamically} {Encode} {Task}-{Critical} {Spatial} {Information} in {Parietal} {Cortex}},
	volume = {30},
	copyright = {Copyright © 2010 the authors 0270-6474/10/3011640-14\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/30/35/11640},
	doi = {10.1523/JNEUROSCI.0954-10.2010},
	abstract = {We characterized the temporal dynamics of population activity in parietal cortex of monkeys as they solved a spatial cognitive problem posed by an object construction task. We applied pattern classification techniques to characterize patterns of activity coding object-centered side, a task-defined variable specifying whether an object component was located on the left or right side of a reference object, regardless of its retinocentric position. During a period in which the value of object-centered side, as defined by task events, remained constant, parietal cortex represented this variable using a dynamic neural code by activating neurons with the same spatial preference in rapid succession so that the pattern of active neurons changed dramatically while the spatial information they collectively encoded remained stable. Furthermore, if the neurons shared the same spatial preference, then their pretrial activity (measured before objects were shown) was correlated to a degree that scaled as a positive linear function of how close together in time the neurons would be activated later in the trial. Finally, we found that while parietal cortex represented task-critical spatial information using a dynamic neural code, it simultaneously represented task-irrelevant spatial information using a stationary neural code. These data demonstrate that dynamic spatial representations exist in parietal cortex, provide novel insight into the synaptic mechanisms that generate them, and suggest they may preferentially encode task-critical spatial information.},
	language = {en},
	number = {35},
	urldate = {2023-03-08},
	journal = {Journal of Neuroscience},
	author = {Crowe, David A. and Averbeck, Bruno B. and Chafee, Matthew V.},
	month = sep,
	year = {2010},
	pmid = {20810885},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	pages = {11640--11653},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/SMTG2QEC/Crowe et al. - 2010 - Rapid Sequences of Population Activity Patterns Dy.pdf:application/pdf},
}

@article{meyers_dynamic_2008,
	title = {Dynamic {Population} {Coding} of {Category} {Information} in {Inferior} {Temporal} and {Prefrontal} {Cortex}},
	volume = {100},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/full/10.1152/jn.90248.2008},
	doi = {10.1152/jn.90248.2008},
	abstract = {Most electrophysiology studies analyze the activity of each neuron separately. While such studies have given much insight into properties of the visual system, they have also potentially overlooked important aspects of information coded in changing patterns of activity that are distributed over larger populations of neurons. In this work, we apply a population decoding method to better estimate what information is available in neuronal ensembles and how this information is coded in dynamic patterns of neural activity in data recorded from inferior temporal cortex (ITC) and prefrontal cortex (PFC) as macaque monkeys engaged in a delayed match-to-category task. Analyses of activity patterns in ITC and PFC revealed that both areas contain “abstract” category information (i.e., category information that is not directly correlated with properties of the stimuli); however, in general, PFC has more task-relevant information, and ITC has more detailed visual information. Analyses examining how information coded in these areas show that almost all category information is available in a small fraction of the neurons in the population. Most remarkably, our results also show that category information is coded by a nonstationary pattern of activity that changes over the course of a trial with individual neurons containing information on much shorter time scales than the population as a whole.},
	number = {3},
	urldate = {2023-03-08},
	journal = {Journal of Neurophysiology},
	author = {Meyers, Ethan M. and Freedman, David J. and Kreiman, Gabriel and Miller, Earl K. and Poggio, Tomaso},
	month = sep,
	year = {2008},
	note = {Publisher: American Physiological Society},
	pages = {1407--1419},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/UYBNT82L/Meyers et al. - 2008 - Dynamic Population Coding of Category Information .pdf:application/pdf},
}

@article{wang_prefrontal_2018,
	title = {Prefrontal cortex as a meta-reinforcement learning system},
	volume = {21},
	copyright = {2018 The Author(s)},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-018-0147-8},
	doi = {10.1038/s41593-018-0147-8},
	abstract = {Over the past 20 years, neuroscience research on reward-based learning has converged on a canonical model, under which the neurotransmitter dopamine ‘stamps in’ associations between situations, actions and rewards by modulating the strength of synaptic connections between neurons. However, a growing number of recent findings have placed this standard model under strain. We now draw on recent advances in artificial intelligence to introduce a new theory of reward-based learning. Here, the dopamine system trains another part of the brain, the prefrontal cortex, to operate as its own free-standing learning system. This new perspective accommodates the findings that motivated the standard model, but also deals gracefully with a wider range of observations, providing a fresh foundation for future research.},
	language = {en},
	number = {6},
	urldate = {2023-03-08},
	journal = {Nature Neuroscience},
	author = {Wang, Jane X. and Kurth-Nelson, Zeb and Kumaran, Dharshan and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Hassabis, Demis and Botvinick, Matthew},
	month = jun,
	year = {2018},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Cognitive control, Learning algorithms},
	pages = {860--868},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/LM3G6GT8/Wang et al. - 2018 - Prefrontal cortex as a meta-reinforcement learning.pdf:application/pdf},
}

@article{vyas_computation_2020,
	title = {Computation {Through} {Neural} {Population} {Dynamics}},
	volume = {43},
	url = {https://doi.org/10.1146/annurev-neuro-092619-094115},
	doi = {10.1146/annurev-neuro-092619-094115},
	abstract = {Significant experimental, computational, and theoretical work has identified rich structure within the coordinated activity of interconnected neural populations. An emerging challenge now is to uncover the nature of the associated computations, how they are implemented, and what role they play in driving behavior. We term this computation through neural population dynamics. If successful, this framework will reveal general motifs of neural population activity and quantitatively describe how neural population dynamics implement computations necessary for driving goal-directed behavior. Here, we start with a mathematical primer on dynamical systems theory and analytical tools necessary to apply this perspective to experimental data. Next, we highlight some recent discoveries resulting from successful application of dynamical systems. We focus on studies spanning motor control, timing, decision-making, and working memory. Finally, we briefly discuss promising recent lines of investigation and future directions for the computation through neural population dynamics framework.},
	number = {1},
	urldate = {2023-03-08},
	journal = {Annual Review of Neuroscience},
	author = {Vyas, Saurabh and Golub, Matthew D. and Sussillo, David and Shenoy, Krishna V.},
	year = {2020},
	pmid = {32640928},
	note = {\_eprint: https://doi.org/10.1146/annurev-neuro-092619-094115},
	keywords = {dynamical systems, neural computation, neural population dynamics, state spaces},
	pages = {249--275},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/7SNVB4GK/Vyas et al. - 2020 - Computation Through Neural Population Dynamics.pdf:application/pdf},
}

@article{sakagami_encoding_1994,
	title = {Encoding of behavioral significance of visual stimuli by primate prefrontal neurons: relation to relevant task conditions},
	volume = {97},
	issn = {1432-1106},
	shorttitle = {Encoding of behavioral significance of visual stimuli by primate prefrontal neurons},
	url = {https://doi.org/10.1007/BF00241536},
	doi = {10.1007/BF00241536},
	abstract = {Single-unit activity was recorded from the inferior dorsolateral prefrontal cortex of two monkeys while they performed a symmetrically rewarded go/no-go discrimination task. Three different task conditions were used in which the monkeys had to base their response on (1) the color, or (2) the shape, or (3) the position of a cue that was presented during fixation of a light spot. The colors of the fixation spot informed the monkeys which condition was relevant. The monkeys had to make an immediate release (go) or a delayed release (no-go) at the time of the fixation color change (imperative stimulus) depending on the currently relevant condition and the discriminative cue previously presented. The effect of changing the relevant condition on neuronal responses to the discriminative cue was analyzed. Out of 328 neurons tested in two or three conditions, 249 responded differentially at the cue period depending on the particular behavioral meaning of the stimulus (go or no-go) in at least one of the task conditions. This differential cue-period activity was examined across the different task conditions: the majority of neurons (111/154, 72\%) showed such activity in all three conditions. In the remaining 43 neurons (28\%) the differential activity was observed in two conditions (27/154, 18\%) or in one condition (16/ 154, 10\%). A few neurons (n = 7) showed feature-specific cue-period activity. In addition, 27 neurons displayed condition-dependent anticipatory activity prior to the cue onset. It is suggested that neurons in the inferior dorsolateral prefrontal cortex may participate in the conversion of sensory information from different visual channels into behavioral information (information on the upcoming response).},
	language = {en},
	number = {3},
	urldate = {2023-03-08},
	journal = {Experimental Brain Research},
	author = {Sakagami, Masamichi and Niki, Hiroaki},
	month = jan,
	year = {1994},
	pages = {423--436},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/UXNEURCT/Sakagami and Niki - 1994 - Encoding of behavioral significance of visual stim.pdf:application/pdf},
}

@article{wallis_single_2001,
	title = {Single neurons in prefrontal cortex encode abstract rules},
	volume = {411},
	copyright = {2001 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/35082081},
	doi = {10.1038/35082081},
	abstract = {The ability to abstract principles or rules from direct experience allows behaviour to extend beyond specific circumstances to general situations. For example, we learn the ‘rules’ for restaurant dining from specific experiences and can then apply them in new restaurants. The use of such rules is thought to depend on the prefrontal cortex (PFC) because its damage often results in difficulty in following rules1. Here we explore its neural basis by recording from single neurons in the PFC of monkeys trained to use two abstract rules. They were required to indicate whether two successively presented pictures were the same or different depending on which rule was currently in effect. The monkeys performed this task with new pictures, thus showing that they had learned two general principles that could be applied to stimuli that they had not yet experienced. The most prevalent neuronal activity observed in the PFC reflected the coding of these abstract rules.},
	language = {en},
	number = {6840},
	urldate = {2023-03-08},
	journal = {Nature},
	author = {Wallis, Jonathan D. and Anderson, Kathleen C. and Miller, Earl K.},
	month = jun,
	year = {2001},
	note = {Number: 6840
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {953--956},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/AGKLWJ5U/Wallis et al. - 2001 - Single neurons in prefrontal cortex encode abstrac.pdf:application/pdf},
}

@incollection{norman_attention_1986,
	address = {Boston, MA},
	title = {Attention to {Action}},
	isbn = {978-1-4757-0629-1},
	url = {https://doi.org/10.1007/978-1-4757-0629-1_1},
	abstract = {Much effort has been made to understand the role of attention in perception; much less effort has been placed on the role attention plays in the control of action. Our goal in this chapter is to account for the role of attention in action, both when performance is automatic and when it is under deliberate conscious control. We propose a theoretical framework structured around the notion of a set of active schemas, organized according to the particular action sequences of which they are a part, awaiting the appropriate set of conditions so that they can become selected to control action. The analysis is therefore centered around actions, primarily external actions, but the same principles apply to internal actions—actions that involve only the cognitive processing mechanisms. One major emphasis in the study of attentional processes is the distinction between controlled and automatic processing of perceptual inputs (e.g., Shiffrin \& Schneider, 1977). Our work here can be seen as complementary to the distinction between controlled and automatic processes: we examine action rather than perception; we emphasize the situations in which deliberate, conscious control of activity is desired rather than those that are automatic.},
	language = {en},
	urldate = {2023-03-08},
	booktitle = {Consciousness and {Self}-{Regulation}: {Advances} in {Research} and {Theory} {Volume} 4},
	publisher = {Springer US},
	author = {Norman, Donald A. and Shallice, Tim},
	editor = {Davidson, Richard J. and Schwartz, Gary E. and Shapiro, David},
	year = {1986},
	doi = {10.1007/978-1-4757-0629-1_1},
	keywords = {Action Sequence, Attentional Resource, Dual Task, Human Information Processing, Psychological Refractory Period},
	pages = {1--18},
}

@article{srivastava_models_2020,
	title = {Models of communication and control for brain networks: distinctions, convergence, and future outlook},
	volume = {4},
	issn = {2472-1751},
	shorttitle = {Models of communication and control for brain networks},
	url = {https://doi.org/10.1162/netn_a_00158},
	doi = {10.1162/netn_a_00158},
	abstract = {Recent advances in computational models of signal propagation and routing in the
human brain have underscored the critical role of white-matter structure. A
complementary approach has utilized the framework of network control theory to
better understand how white matter constrains the manner in which a region or
set of regions can direct or control the activity of other regions. Despite the
potential for both of these approaches to enhance our understanding of the role
of network structure in brain function, little work has sought to understand the
relations between them. Here, we seek to explicitly bridge computational models
of communication and principles of network control in a conceptual review of the
current literature. By drawing comparisons between communication and control
models in terms of the level of abstraction, the dynamical complexity, the
dependence on network attributes, and the interplay of multiple spatiotemporal
scales, we highlight the convergence of and distinctions between the two
frameworks. Based on the understanding of the intertwined nature of
communication and control in human brain networks, this work provides an
integrative perspective for the field and outlines exciting directions for
future work.Models of communication in brain networks have been essential in building a
quantitative understanding of the relationship between structure and function.
More recently, control-theoretic models have also been applied to brain networks
to quantify the response of brain networks to exogenous and endogenous
perturbations. Mechanistically, both of these frameworks investigate the role of
interregional communication in determining the behavior and response of the
brain. Theoretically, both of these frameworks share common features, indicating
the possibility of combining the two approaches. Drawing on a large body of past
and ongoing works, this review presents a discussion of convergence and
distinctions between the two approaches, and argues for the development of
integrated models at the confluence of the two frameworks, with potential
applications to various topics in neuroscience.},
	number = {4},
	urldate = {2023-03-08},
	journal = {Network Neuroscience},
	author = {Srivastava, Pragya and Nozari, Erfan and Kim, Jason Z. and Ju, Harang and Zhou, Dale and Becker, Cassiano and Pasqualetti, Fabio and Pappas, George J. and Bassett, Danielle S.},
	month = nov,
	year = {2020},
	pages = {1122--1159},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/AL6TWFLQ/Srivastava et al. - 2020 - Models of communication and control for brain netw.pdf:application/pdf},
}

@article{buschman_synchronous_2012,
	title = {Synchronous {Oscillatory} {Neural} {Ensembles} for {Rules} in the {Prefrontal} {Cortex}},
	volume = {76},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627312008823},
	doi = {10.1016/j.neuron.2012.09.029},
	abstract = {Intelligent behavior requires acquiring and following rules. Rules define how our behavior should fit different situations. To understand its neural mechanisms, we simultaneously recorded from multiple electrodes in dorsolateral prefrontal cortex (PFC) while monkeys switched between two rules (respond to color versus orientation). We found evidence that oscillatory synchronization of local field potentials (LFPs) formed neural ensembles representing the rules: there were rule-specific increases in synchrony at “beta” (19–40 Hz) frequencies between electrodes. In addition, individual PFC neurons synchronized to the LFP ensemble corresponding to the current rule (color versus orientation). Furthermore, the ensemble encoding the behaviorally dominant orientation rule showed increased “alpha” (6–16 Hz) synchrony when preparing to apply the alternative (weaker) color rule. This suggests that beta-frequency synchrony selects the relevant rule ensemble, while alpha-frequency synchrony deselects a stronger, but currently irrelevant, ensemble. Synchrony may act to dynamically shape task-relevant neural ensembles out of larger, overlapping circuits.},
	language = {en},
	number = {4},
	urldate = {2023-03-08},
	journal = {Neuron},
	author = {Buschman, Timothy J. and Denovellis, Eric L. and Diogo, Cinira and Bullock, Daniel and Miller, Earl K.},
	month = nov,
	year = {2012},
	pages = {838--846},
	file = {ScienceDirect Full Text PDF:/Users/jaschaachterberg/Zotero/storage/44Q2Z7Q6/Buschman et al. - 2012 - Synchronous Oscillatory Neural Ensembles for Rules.pdf:application/pdf;ScienceDirect Snapshot:/Users/jaschaachterberg/Zotero/storage/6N53N6LK/S0896627312008823.html:text/html},
}

@book{cajal_cajals_1995,
	address = {Oxford, New York},
	series = {History of {Neuroscience}},
	title = {Cajal's {Histology} of the {Nervous} {System} of {Man} and {Vertebrates}},
	isbn = {978-0-19-507401-7},
	abstract = {In terms of breadth, depth, and originality, this work ranked Cajal with Pasteur and Darwin as giants of 19th century biology. Summarizing almost 20 years of intense research, Cajal systematically described the cellular organization of almost every part of the nervous system in all five classes of vertebrate, and provided a synthetic account of their embryogenesis as well. This revolutionary work laid a broad foundation for modern neuroscience.   Neuroscientists, neurologists, psychologists, computer and cognitive scientists, and nonspecialists will find this work of great use. Modern neuroanatomical terminology is used wherever possible, while attempting to preserve the style of the original text. Summarizing almost 20 years of intense research, Cajal systematically described the cellular organization of almost every part of the nervous system in all five classes of vertebrate, and provided a synthetic account of their embryogenesis as well. This work was revolutionary and laid a broad foundation for modern neuroscience because two new concepts - the neuron doctrine and the law of functional polarity - were used to interpret the data, and because the resulting interpretations opened vast new fields of research with profound clinical implications in neurology and psychiatry. In terms of breadth, depth and originality, this work is second only to that of Vesalius in the history of anatomy, and ranked Cajal with Pasteur and Darwin as the giants of 19th century biology. In many ways, the  Histology is as valuable today as when it was written, and these volumes will be of use to a broad spectrum of neuroscientists, neurologists, psychologists, and computer and cognitive scientists. To make this work accessible to non specialists, the translators have used modern neuroanatomical terminology wherever possible, while attempting to preserve the style of the original text. They have also provided extensive cross-referencing of synonyms in the index, and notes to clarify difficult passages.
             
             
              
            ,  
             In terms of breadth, depth, and originality, this work ranked Cajal with Pasteur and Darwin as giants of 19th century biology. Summarizing almost 20 years of intense research, Cajal systematically described the cellular organization of almost every part of the nervous system in all five classes of vertebrate, and provided a synthetic account of their embryogenesis as well. This revolutionary work laid a broad foundation for modern neuroscience.   Neuroscientists, neurologists, psychologists, computer and cognitive scientists, and nonspecialists will find this work of great use. Modern neuroanatomical terminology is used wherever possible, while attempting to preserve the style of the original text. Summarizing almost 20 years of intense research, Cajal systematically described the cellular organization of almost every part of the nervous system in all five classes of vertebrate, and provided a synthetic account of their embryogenesis as well. This work was revolutionary and laid a broad foundation for modern neuroscience because two new concepts - the neuron doctrine and the law of functional polarity - were used to interpret the data, and because the resulting interpretations opened vast new fields of research with profound clinical implications in neurology and psychiatry. In terms of breadth, depth and originality, this work is second only to that of Vesalius in the history of anatomy, and ranked Cajal with Pasteur and Darwin as the giants of 19th century biology. In many ways, the  Histology is as valuable today as when it was written, and these volumes will be of use to a broad spectrum of neuroscientists, neurologists, psychologists, and computer and cognitive scientists. To make this work accessible to non specialists, the translators have used modern neuroanatomical terminology wherever possible, while attempting to preserve the style of the original text. They have also provided extensive cross-referencing of synonyms in the index, and notes to clarify difficult passages.},
	publisher = {Oxford University Press},
	author = {Cajal, Santiago Ramon y and Swanson, Neely and Swanson, Larry W. and Cajal, Santiago Ramon y and Swanson, Neely and Swanson, Larry W.},
	month = apr,
	year = {1995},
	file = {Snapshot:/Users/jaschaachterberg/Zotero/storage/URHHN9A8/cajals-histology-of-the-nervous-system-of-man-and-vertebrates-9780195074017.html:text/html},
}

@article{raichle_appraising_2002,
	title = {Appraising the brain's energy budget},
	volume = {99},
	url = {https://www.pnas.org/doi/10.1073/pnas.172399499},
	doi = {10.1073/pnas.172399499},
	number = {16},
	urldate = {2023-03-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Raichle, Marcus E. and Gusnard, Debra A.},
	month = aug,
	year = {2002},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {10237--10239},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/4M9W4RQD/Raichle and Gusnard - 2002 - Appraising the brain's energy budget.pdf:application/pdf},
}

@article{tomasi_energetic_2013,
	title = {Energetic cost of brain functional connectivity},
	volume = {110},
	url = {https://www.pnas.org/doi/10.1073/pnas.1303346110},
	doi = {10.1073/pnas.1303346110},
	abstract = {The brain's functional connectivity is complex, has high energetic cost, and requires efficient use of glucose, the brain's main energy source. It has been proposed that regions with a high degree of functional connectivity are energy efficient and can minimize consumption of glucose. However, the relationship between functional connectivity and energy consumption in the brain is poorly understood. To address this neglect, here we propose a simple model for the energy demands of brain functional connectivity, which we tested with positron emission tomography and MRI in 54 healthy volunteers at rest. Higher glucose metabolism was associated with proportionally larger MRI signal amplitudes, and a higher degree of connectivity was associated with nonlinear increases in metabolism, supporting our hypothesis for the energy efficiency of the connectivity hubs. Basal metabolism (in the absence of connectivity) accounted for 30\% of brain glucose utilization, which suggests that the spontaneous brain activity accounts for 70\% of the energy consumed by the brain. The energy efficiency of the connectivity hubs was higher for ventral precuneus, cerebellum, and subcortical hubs than for cortical hubs. The higher energy demands of brain communication that hinges upon higher connectivity could render brain hubs more vulnerable to deficits in energy delivery or utilization and help explain their sensitivity to neurodegenerative conditions, such as Alzheimer’s disease.},
	number = {33},
	urldate = {2023-03-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Tomasi, Dardo and Wang, Gene-Jack and Volkow, Nora D.},
	month = aug,
	year = {2013},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {13642--13647},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/2PKJV8HC/Tomasi et al. - 2013 - Energetic cost of brain functional connectivity.pdf:application/pdf},
}

@article{bassett_small-world_2006,
	title = {Small-{World} {Brain} {Networks}},
	volume = {12},
	issn = {1073-8584},
	url = {https://doi.org/10.1177/1073858406293182},
	doi = {10.1177/1073858406293182},
	abstract = {Many complex networks have a small-world topology characterized by dense local clustering or cliquishness of connections between neighboring nodes yet a short path length between any (distant) pair of nodes due to the existence of relatively few long-range connections. This is an attractive model for the organization of brain anatomical and functional networks because a small-world topology can support both segregated/specialized and distributed/integrated information processing. Moreover, small-world networks are economical, tending to minimize wiring costs while supporting high dynamical complexity. The authors introduce some of the key mathematical concepts in graph theory required for small-world analysis and review how these methods have been applied to quantification of cortical connectivity matrices derived from anatomical tract-tracing studies in the macaque monkey and the cat. The evolution of small-world networks is discussed in terms of a selection pressure to deliver cost-effective information-processing systems. The authors illustrate how these techniques and concepts are increasingly being applied to the analysis of human brain functional networks derived from electroencephalography/magnetoencephalography and fMRI experiments. Finally, the authors consider the relevance of small-world models for understanding the emergence of complex behaviors and the resilience of brain systems to pathological attack by disease or aberrant development. They conclude that small-world models provide a powerful and versatile approach to understanding the structure and function of human brain systems.},
	language = {en},
	number = {6},
	urldate = {2023-03-08},
	journal = {The Neuroscientist},
	author = {Bassett, Danielle Smith and Bullmore, Ed},
	month = dec,
	year = {2006},
	note = {Publisher: SAGE Publications Inc STM},
	pages = {512--523},
	file = {SAGE PDF Full Text:/Users/jaschaachterberg/Zotero/storage/X3ET8UBI/Bassett and Bullmore - 2006 - Small-World Brain Networks.pdf:application/pdf},
}

@article{bassett_small-world_2017,
	title = {Small-{World} {Brain} {Networks} {Revisited}},
	volume = {23},
	issn = {1073-8584},
	url = {https://doi.org/10.1177/1073858416667720},
	doi = {10.1177/1073858416667720},
	abstract = {It is nearly 20 years since the concept of a small-world network was first quantitatively defined, by a combination of high clustering and short path length; and about 10 years since this metric of complex network topology began to be widely applied to analysis of neuroimaging and other neuroscience data as part of the rapid growth of the new field of connectomics. Here, we review briefly the foundational concepts of graph theoretical estimation and generation of small-world networks. We take stock of some of the key developments in the field in the past decade and we consider in some detail the implications of recent studies using high-resolution tract-tracing methods to map the anatomical networks of the macaque and the mouse. In doing so, we draw attention to the important methodological distinction between topological analysis of binary or unweighted graphs, which have provided a popular but simple approach to brain network analysis in the past, and the topology of weighted graphs, which retain more biologically relevant information and are more appropriate to the increasingly sophisticated data on brain connectivity emerging from contemporary tract-tracing and other imaging studies. We conclude by highlighting some possible future trends in the further development of weighted small-worldness as part of a deeper and broader understanding of the topology and the functional value of the strong and weak links between areas of mammalian cortex.},
	language = {en},
	number = {5},
	urldate = {2023-03-08},
	journal = {The Neuroscientist},
	author = {Bassett, Danielle S. and Bullmore, Edward T.},
	month = oct,
	year = {2017},
	note = {Publisher: SAGE Publications Inc STM},
	pages = {499--516},
	file = {SAGE PDF Full Text:/Users/jaschaachterberg/Zotero/storage/V69UD7F6/Bassett and Bullmore - 2017 - Small-World Brain Networks Revisited.pdf:application/pdf},
}

@article{suarez_connectomics-based_2022,
	title = {A connectomics-based taxonomy of mammals},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.78635},
	doi = {10.7554/eLife.78635},
	abstract = {Mammalian taxonomies are conventionally defined by morphological traits and genetics. How species differ in terms of neural circuits and whether inter-species differences in neural circuit organization conform to these taxonomies is unknown. The main obstacle to the comparison of neural architectures has been differences in network reconstruction techniques, yielding species-specific connectomes that are not directly comparable to one another. Here, we comprehensively chart connectome organization across the mammalian phylogenetic spectrum using a common reconstruction protocol. We analyse the mammalian MRI (MaMI) data set, a database that encompasses high-resolution ex vivo structural and diffusion MRI scans of 124 species across 12 taxonomic orders and 5 superorders, collected using a unified MRI protocol. We assess similarity between species connectomes using two methods: similarity of Laplacian eigenspectra and similarity of multiscale topological features. We find greater inter-species similarities among species within the same taxonomic order, suggesting that connectome organization reflects established taxonomic relationships defined by morphology and genetics. While all connectomes retain hallmark global features and relative proportions of connection classes, inter-species variation is driven by local regional connectivity profiles. By encoding connectomes into a common frame of reference, these findings establish a foundation for investigating how neural circuits change over phylogeny, forging a link from genes to circuits to behaviour.},
	urldate = {2023-03-08},
	journal = {eLife},
	author = {Suarez, Laura E and Yovel, Yossi and van den Heuvel, Martijn P and Sporns, Olaf and Assaf, Yaniv and Lajoie, Guillaume and Misic, Bratislav},
	editor = {Baker, Chris I and Jbabdi, Saad and Heuer, Katja},
	month = nov,
	year = {2022},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {connectomics, mammals, taxonomy},
	pages = {e78635},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/DQ4QQB5S/Suarez et al. - 2022 - A connectomics-based taxonomy of mammals.pdf:application/pdf},
}

@article{bullmore_economy_2012,
	title = {The economy of brain network organization},
	volume = {13},
	copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn3214},
	doi = {10.1038/nrn3214},
	abstract = {Cost control and complex topology are important aspects of the organization of human and other nervous systems.Efficient transfer of information between modules of brain networks confers functional advantages in terms of adaptive behaviour, but it imposes a premium in terms of wiring cost.Brain networks negotiate an economical trade-off between minimizing wiring cost and maximizing expensive but advantageous topological properties such as efficiency.Brain networks can renegotiate trade-offs between cost and efficiency dynamically over short and long timescales.High-cost components of human brain networks may be particularly vulnerable to abnormal development or pathological attack, leading to disorders of cognition or behaviour.},
	language = {en},
	number = {5},
	urldate = {2023-03-08},
	journal = {Nature Reviews Neuroscience},
	author = {Bullmore, Ed and Sporns, Olaf},
	month = may,
	year = {2012},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Cognitive neuroscience, Computational neuroscience, Psychiatric disorders, Social neuroscience},
	pages = {336--349},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/NBGWKKRY/Bullmore and Sporns - 2012 - The economy of brain network organization.pdf:application/pdf},
}

@article{levy_communication_2021,
	title = {Communication consumes 35 times more energy than computation in the human cortex, but both costs are needed to predict synapse number},
	volume = {118},
	url = {https://www.pnas.org/doi/10.1073/pnas.2008173118},
	doi = {10.1073/pnas.2008173118},
	abstract = {Darwinian evolution tends to produce energy-efficient outcomes. On the other hand, energy limits computation, be it neural and probabilistic or digital and logical. Taking a particular energy-efficient viewpoint, we define neural computation and make use of an energy-constrained computational function. This function can be optimized over a variable that is proportional to the number of synapses per neuron. This function also implies a specific distinction between adenosine triphosphate (ATP)-consuming processes, especially computation per se vs. the communication processes of action potentials and transmitter release. Thus, to apply this mathematical function requires an energy audit with a particular partitioning of energy consumption that differs from earlier work. The audit points out that, rather than the oft-quoted 20 W of glucose available to the human brain, the fraction partitioned to cortical computation is only 0.1 W of ATP [L. Sokoloff, Handb. Physiol. Sect. I Neurophysiol. 3, 1843–1864 (1960)] and [J. Sawada, D. S. Modha, “Synapse: Scalable energy-efficient neurosynaptic computing” in Application of Concurrency to System Design (ACSD) (2013), pp. 14–15]. On the other hand, long-distance communication costs are 35-fold greater, 3.5 W. Other findings include 1) a 
1
0
8
1
0
8
-fold discrepancy between biological and lowest possible values of a neuron’s computational efficiency and 2) two predictions of 
��
��
, the number of synaptic transmissions needed to fire a neuron (2,500 vs. 2,000).},
	number = {18},
	urldate = {2023-03-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Levy, William B and Calvert, Victoria G.},
	month = may,
	year = {2021},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2008173118},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/68UVSWDL/Levy and Calvert - 2021 - Communication consumes 35 times more energy than c.pdf:application/pdf},
}

@article{laughlin_communication_2003,
	title = {Communication in {Neuronal} {Networks}},
	volume = {301},
	url = {https://www.science.org/doi/10.1126/science.1089662},
	doi = {10.1126/science.1089662},
	abstract = {Brains perform with remarkable efficiency, are capable of prodigious computation, and are marvels of communication. We are beginning to understand some of the geometric, biophysical, and energy constraints that have governed the evolution of cortical networks. To operate efficiently within these constraints, nature has optimized the structure and function of cortical networks with design principles similar to those used in electronic networks. The brain also exploits the adaptability of biological systems to reconfigure in response to changing needs.},
	number = {5641},
	urldate = {2023-03-08},
	journal = {Science},
	author = {Laughlin, Simon B. and Sejnowski, Terrence J.},
	month = sep,
	year = {2003},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1870--1874},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/PPXY4F8J/Laughlin and Sejnowski - 2003 - Communication in Neuronal Networks.pdf:application/pdf},
}

@article{miller_integrative_2001,
	title = {An {Integrative} {Theory} of {Prefrontal} {Cortex} {Function}},
	volume = {24},
	url = {https://doi.org/10.1146/annurev.neuro.24.1.167},
	doi = {10.1146/annurev.neuro.24.1.167},
	abstract = {The prefrontal cortex has long been suspected to play an important role in cognitive control, in the ability to orchestrate thought and action in accordance with internal goals. Its neural basis, however, has remained a mystery. Here, we propose that cognitive control stems from the active maintenance of patterns of activity in the prefrontal cortex that represent goals and the means to achieve them. They provide bias signals to other brain structures whose net effect is to guide the flow of activity along neural pathways that establish the proper mappings between inputs, internal states, and outputs needed to perform a given task. We review neurophysiological, neurobiological, neuroimaging, and computational studies that support this theory and discuss its implications as well as further issues to be addressed},
	number = {1},
	urldate = {2023-03-08},
	journal = {Annual Review of Neuroscience},
	author = {Miller, Earl K. and Cohen, Jonathan D.},
	year = {2001},
	pmid = {11283309},
	note = {\_eprint: https://doi.org/10.1146/annurev.neuro.24.1.167},
	keywords = {attention, cognition, executive control, frontal lobes, working memory},
	pages = {167--202},
	file = {Full Text PDF:/Users/jaschaachterberg/Zotero/storage/N22Y9GSG/Miller and Cohen - 2001 - An Integrative Theory of Prefrontal Cortex Functio.pdf:application/pdf},
}
