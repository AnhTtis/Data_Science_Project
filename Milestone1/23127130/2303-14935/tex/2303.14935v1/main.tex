\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{pythontex} 
\usepackage{booktabs}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\IEEEoverridecommandlockouts
% \IEEEpubid{\makebox[\columnwidth]{978-1-6654-8045-1/22/\$31.00 \copyright 2022 IEEE \hfill}
% \hspace{\columnsep}\makebox[\columnwidth]{ }}

\title{TabIQA: Table Questions Answering on \\ Business Document Images
%*\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}
\author{
\IEEEauthorblockN{Phuc Nguyen\textsuperscript{\textsection}, Nam Tuan Ly\textsuperscript{\textsection}, Hideaki Takeda, Atsuhiro Takasu}
\IEEEauthorblockA{\textit{National Institute of Informatics, Japan}\\
\{phucnt, namly, takeda, takasu\}@nii.ac.jp}
}

\maketitle
\begingroup
\renewcommand\thefootnote{\textsection}
\footnotetext{Equal contribution}
\endgroup
\begin{abstract}
Table answering questions from business documents has many challenges that require understanding tabular structures, cross-document referencing, and additional numeric computations beyond simple search queries. This paper introduces a novel pipeline, named TabIQA, to answer questions about business document images. TabIQA combines state-of-the-art deep learning techniques 1) to extract table content and structural information from images and 2) to answer various questions related to numerical data, text-based information, and complex queries from structured tables. The evaluation results on VQAonBD 2023 dataset demonstrate the effectiveness of TabIQA in achieving promising performance in answering table-related questions. The TabIQA repository is available at \url{https://github.com/phucty/itabqa}. 
\end{abstract}

\begin{IEEEkeywords}
Visual Question Answering, Table Question Answering, Business Documents
\end{IEEEkeywords}

\section{Introduction} \label{sec:intro}
% Background:
Businesses generate and process vast amounts of information, and extracting valuable insights from this data is crucial for making informed decisions. Business documents, such as financial reports, invoices, and contracts, often contain valuable information in tabular form. However, answering questions based on these document images can be challenging due to their complex structures, cross-referencing, and numerical computations beyond simple search queries.

% Motivation
Traditional information retrieval methods, such as keyword search and regular expressions, are only sometimes effective in retrieving information from tables in business documents. Therefore, there is a growing need for automated approaches that can accurately extract relevant information from tables and answer various questions. Such approaches can save businesses time and effort and improve the extracted information's accuracy and reliability.

\begin{figure}[htbp]
\centering
\includegraphics{fig1.pdf}
\caption{Illustration of question answering from the business document image of \text{val\_table\_image\_7517\_\_GPC\_\_2013\_\_page\_55\_split\_0} from VQAonBD 2023 dataset}
\label{fig:fig1}
\end{figure}

% Objectives and methods
The main objective of this paper is to introduce TabIQA, a novel pipeline for answering questions on business document images. Figure \ref{fig:fig1} illustrates the question-answering task from the business document image in VQAonBD 2023 dataset. Given a business document image and a question about the image: ``What is the dollar value (in thousands) of foreign currency translation for the year 2013?" the output answer is  ``-37619". TabIQA utilizes the table recognition module to extract table structure information and the text content of each table cell and convert them into HTML format. Subsequently, the high-level table structure is extracted to identify the headers, data cells, and hierarchical structure with the post-structure extraction module. Once the table is structured, it is converted to a dataframe format for further processing. The question-answering module processes the input question and the table dataframe with an encoder and generates the final answer from a decoder.  

% Contributions
Overall, this study makes the following contributions:
\begin{itemize}
    \item Introducing TabIQA, a novel pipeline for answering questions about business document images: TabIQA is a comprehensive pipeline combining state-of-the-art deep learning techniques to extract relevant information from tables and answer various questions related to numerical data, text-based information, and complex queries. 
    \item Providing a publicly available repository: We have made the TabIQA repository publicly available to encourage the reproducibility of our results and enable other researchers to use and build upon our work.
    \item Demonstrating the effectiveness of TabIQA on the VQAonBD 2023 dataset: The evaluation results on VQAonBD 2023 dataset\footnote{VQAonBD 2023: \url{https://ilocr.iiit.ac.in/vqabd/dataset.html}} demonstrate the effectiveness of TabIQA in achieving promising performance in answering table-related questions.
\end{itemize}

The rest of the paper is structured as follows. Section \ref{sec2:framework} summarizes related work on question answering on business document images. We introduce the TabIQA method in Section \ref{sec1:approach}. Section \ref{sec1:experiments} presents the experimental settings and results. Finally, in Section \ref{sec1:conclusion}, we present conclusions and discuss future directions of the task table question answering on business document images. 

\section{Related Work} \label{sec1:related_work}
Image-based table recognition is one of the important parts of the document understanding system as well as the table questions answering system, which aims to recognize the table structure information and the text content of each table cell from an input image and represent them in a machine-readable format (HTML or CSV). Most of the previous works \cite{QiaoICDAR2021, Ye2021PingAnVCGroupsSF, NassarCVPR2022, ZhangPR2022} of table recognition focused on two-step approaches that divide the problem into two sub-problems: table structure recognition and table cell content recognition, and then attempt to solve each sub-problem independently by two separate systems. In recent years, due to the advantages of deep learning and the availability of large-scale table image datasets, some works \cite{Deng2019, Zhong2020, multitasktabnet, wstabnet} try to focus on end-to-end approaches which solve the table recognition problem using one end-to-end system. Ly et al. \cite{multitasktabnet} formulated the problem of table recognition as a multi-task learning problem. They proposed an end-to-end multi-task learning model for image-based table recognition, which consists of three separate decoders for three sub-tasks of table recognition: table structure recognition, cell detection, and cell-content recognition. The proposed model achieves state-of-the-art accuracies on PubTabNet and FinTabNet datasets. Ly et al. \cite{wstabnet} also proposed an end-to-end weakly supervised learning model named WSTabNet, which requires only table images and their annotations of table HTML code for training the model. WSTabNet achieves competitive accuracies compared to the fully supervised and two-step learning methods.

Information Retrieval from Business Documents Business documents such as invoices, receipts, and financial statements contain valuable information critical for decision-making and analysis. Traditional information retrieval methods from business documents rely on manual data entry or simple search queries, which can be time-consuming and error-prone. Recent advances in deep learning and natural language processing have enabled the development of automated systems that can extract information from business documents with high accuracy and efficiency.


Deep Learning Techniques for Table Extraction and Question Answering Table extraction and question answering are critical tasks in automated information retrieval from business documents. Deep learning techniques have shown great promise in addressing these challenges. Various approaches have been proposed for table extraction, including region-based, cell-based, and structure-based methods. For question answering, neural network-based models such as transformer-based models have achieved state-of-the-art results on various datasets.

While these studies have achieved promising results in automated information retrieval from business documents, there is still room for improvement in accuracy, efficiency, and scalability. This paper proposes a novel pipeline, TabIQA, for answering questions about business document tables that leverage state-of-the-art deep learning techniques for improved performance.

\section{Approach} \label{sec1:approach}
This section describes TabIQA's overall framework in Section \ref{sec2:framework}. The details of the table recognition module, post-structure extraction module, and question-answering module are described in Section \ref{sec2:table_recognition}, \ref{sec2:structure_extraction}, and Section \ref{sec2:qa} respectively. 

\subsection{Framework} \label{sec2:framework}
The overall framework of TabIQA, a system designed for question-answering using table images in business documents, is illustrated in Fig. \ref{fig:framework}. TabIQA utilizes a table recognition algorithm that extracts the table's structure and textual content of each cell and then converts them into HTML format. The system subsequently analyzes the HTML table to identify headers, data cells, and hierarchical structure and transforms it into a dataframe for further processing. Finally, the question-answering module processes the input question and the table dataframe with an encoder and generates the final answer through a decoder. 
\begin{figure}[htbp]
\centering
\includegraphics{tabqa.pdf}
\caption{Framework}
\label{fig:framework}
\end{figure}

\subsection{Table Recognition} \label{sec2:table_recognition}
This module aims to predict the table structure information and the text content of each table cell from a table image and represent them in a machine-readable format (HTML). This module consists of one shared encoder, one shared decoder, and three separate decoders for three sub-tasks of table recognition: table structure recognition, cell detection, and cell-content recognition.

First, we trained this model on the training set of VQAonBD 2023 and validated it on the validation set for model selection and choosing the hyperparameters. Finally, we used training and validation sets to train the final table recognition module.

\subsection{Post Structure Extraction} \label{sec2:structure_extraction}
The table post-structure extraction module plays a crucial role in the TabIQA system. The module's primary function is to predict table headers and extract the hierarchical rows from the HTML table.
\subsubsection{Header Prediction}
To predict table headers, the module uses a set of heuristics based on the characteristics of the input table. Specifically, the headers are identified as one of the first table rows that satisfy one or more of the following conditions:
\begin{itemize}
    \item Column spans: The header row contains cells that span multiple columns. 
    \item Nan cells: The header row contains cells with missing values.
    \item Duplicate value cells: The header row contains cells with identical values in the same row.
    \item If no headers are found using these heuristics, the first row is treated as the table header.
\end{itemize}
All the remaining rows are then classified as data rows.

\subsubsection{Hierarchical Row Prediction}
This module predicts the hierarchical information from the table HTML and then concatenates the value of each hierarchical cell to the lower-level cells in the same column. In this work, we propose two hierarchical row prediction algorithms: the first one is based on the predicted table HTML and the second one is based on both the predicted table HTML and the table cell bounding boxes. The two hierarchical row prediction algorithms are defined in the algorithm 1 and 2, respectively:

\begin{algorithm}
\caption{Hierarchical row prediction algorithm}
\begin{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE R (a list of rows of table cells)
\ENSURE  R' (a list of rows of table cells with hierarchical information)
\STATE $hierarchical\_cell \gets null$
\FOR{r in R}
    \IF{r[0] is a colspan cell OR r[1] is an empty cell}
        \STATE $hierarchical\_cell \gets r[0]$
    \ELSIF{r[0] is a rowspan cell OR r[0] is an empty cell}
        \STATE $hierarchical\_cell \gets null$
    \ELSIF{hierarchical\_cell is not null}
        \STATE $r[0].text \gets r[0].text + hierarchical\_cell.text$
    \ENDIF
\ENDFOR
\RETURN $R$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Hierarchical row prediction algorithm}
\begin{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE R (a list of rows of table cells with their bounding boxes)
\ENSURE  R' (a list of rows of table cells with hierarchical information)
\STATE $hierarchical\_cell \gets null$
\STATE $different\_bbox\_flag \gets False$
\FOR{r in R}
    \IF{r[0] is a colspan cell OR r[1] is an empty cell}
        \STATE $hierarchical\_cell \gets r[0]$
    \ELSIF{r[0] is a rowspan cell OR r[0] is an empty cell}
        \STATE $hierarchical\_cell \gets null$
    \ELSIF{$different\_bbox\_flag$ AND r[0].left\_bbox - $hierarchical\_cell$.left\_bbox < threshold}
        \STATE $hierarchical\_cell \gets null$
    \ELSIF{hierarchical\_cell is not null}
        \STATE $r[0].text \gets r[0].text + hierarchical\_cell.text$
    \ENDIF
\ENDFOR
\RETURN $R$
\end{algorithmic}
\end{algorithm}

\subsection{Question Answering} \label{sec2:qa}
We adopt the state-of-the-art table-based QA setting OmniTab \cite{omnitab}, based on TAPEX \cite{tapex} pre-training setting. It feeds the concatenating token sequences of natural language questions and linearized table dataframes into the bidirectional encoder. The table dataframes are linearized in the order of top-to-bottom and left-to-right. The final answers are generated with an autoregressive decoder.

We create a new fine-tuning training set from the training set of VQAonBD 2023 and the results of table recognization and post-structure extraction modules. Each sample consists of a table dataframe and a natural language question, the equivalent ground truth answer. We fine-tuned the OmniTab large pre-trained models using a training set of 100K samples sampled from the training set of VQAonBD 2023, with 20k samples of each question category. 

We developed multiple question-answering models using both raw HTML tables (results of table recognization in Section \ref{sec2:table_recognition}) and structured tables (results of post-structure extraction in Section \ref{sec2:structure_extraction}). In cases where the QA model returns no answer, we assign the answer as zero.

\section{Experiments} \label{sec1:experiments}


\subsection{Dataset} \label{sec2:data}
We evaluate TabIQA on VQAonDB 2023 dataset. This dataset contains document images from the FinTabNet dataset \cite{fintabnet} and relevant questions about these document images. The training and validation set's ground truth also contains table structure annotation information, i.e., bounding boxes of words, tokens, digitized text, and row and column identifiers.

Each document image may include up to 50 questions in five categories with their corresponding answers. The number of questions in each category for a single table varies based on its content and format. For Category 1, the number of questions falls within the range of 0 to 25, while for Category 2, it is between 0 and 10. Similarly, the number of questions in Category 3 ranges from 0 to 3, while for Category 4, it is between 0 and 7. Lastly, Category 5 has a range of 0 to 5 questions.

The detailed statistics of VQAonBD 2023 are described in the following sections.

\subsubsection{Document Images}
In this section, we analyze the document images of the VQAonBD 2023 dataset. The statistics of document images are reported in Table \ref{tab:doc_images}. ``Doc Images" are the number of document images, whereas the ``Blank Images" are the number of the blank page. For example, the sample ``val\_table\_image\_9684\_\_CL\_\_2014\_\_page\_54\_split\_0" in the validation set of VQAonBD 2023 is the blank page. 

The training contains 12\% blank images within the dataset, whereas validation and testing sets have blank images at less than 1\%. Despite the absence of content, related questions exist regarding these blank images. During TabIQA training phase, we exclude these samples containing blank images. In the testing phase, TabIQA returns a zero value for samples containing blank images.

\begin{table}[!ht]
\centering
\caption{Document Image Statistics on VQAonBD 2023}
\label{tab:doc_images}
\begin{tabular}{@{}lll@{}}
\toprule
           & Doc Images & Blank Images \\ \midrule
Train      & 39,999     & 5,025        \\
Validation & 4,535      & 6            \\
Test       & 4,361      & 13           \\ \bottomrule
\end{tabular}
\end{table}
 

\subsubsection{Questions}
The dataset used for training, validation, and testing contains 41,465, 1,254,165, and 135,825 questions, respectively. The average question length is 109.45 characters, and the average number of words in a question is 10.5. Some questions are longer than 1,500 characters. To identify named entities within the questions, we use the Spacy tool \cite{spacy}, which detects an average of 1.71 entities per question. Of these, 1.42 entities pertain to the time dimension, 0.18 are numerical values, and 0.12 have textual values.

\subsubsection{Tables}
Table \ref{tab:table_size} reports size statistics of annotated tables from the training and validation set of VQAonBD 2023. The training set comprises tables with  numbers of rows larger than those in the validation set, and the cells in the training set are also longer than those in the validation set.
\begin{table}[!ht]
\centering
\caption{Annotated Table Statistics on VQAonBD 2023}
\label{tab:table_size}
\begin{tabular}{@{}lrr@{}}
\toprule
          & Train (avg.)      & Val (avg.)         \\ \midrule
Row       & 2-77 (13.27)      & 2 -58 (12.15)      \\
Column       & 2-16 (4.57)       & 2-13 (4.44)        \\
Cell length & 3.2-161.08 (11.3) & 4.56-40.54 (11.17) \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Experiment Setup} \label{sec2:setup}
\subsubsection{Baselines}
To evaluate our results, we compared them against other baselines, including TAPAS \cite{tapas}, TAPEX \cite{tapex}, OmniTab \cite{omnitab}, and the Zero model. The Zero model always returns zero for any question. 
\subsubsection{Metrics}
The VQAonBD 2023 performance metric is determined depending on answer types. If the answer types are textual values, then the Averaged Normalised Levenshtein Similarity (ANLS) is used as the metric in DocVQA \cite{DocVQA}. ANLS is designed to respond softly to answer mismatches that may arise due to OCR imperfections. On the other hand, if the answer types are numerical values, then the metric is calculated by taking a scaled Euclidean norm of the ANLS score and the percentage of the absolute difference between the predicted answer and the ground truth answer.

\subsection{Results and Discussions} \label{sec2:results}
Table \ref{tab:result} compares TabIQA's question-answering performance against other baseline models using the VQAonBD 2023 dataset. We use the fine-tuned hugging face models\footnote{Huggingface models: \url{https://huggingface.co/models?pipeline_tag=table-question-answering}} of TAPAS \cite{tapas}, TAPEX \cite{tapex}, and OmniTab \cite{omnitab} on the WikiTableQuestion dataset \cite{wtd_dataset}. In addition, we included the Zero setting, a model that always returns zero for any question. TabIQA1 represents the setting where the question-answering model is fine-tuned directly on raw HTML tables. On the other hand, TabIQA2 refers to the setting where the QA model is fine-tuned on structured tables.


 
\begin{table}[!ht]
\centering
\caption{Question answering performance on the validate split VQAonBD 2023 dataset.}
\label{tab:result}
\begin{tabular}{@{}lr@{}}
\toprule
Model   & VQAonBD 2023 Score  \\ \midrule
TAPAS \cite{tapas}   & 0.4138 \\
TAPEX \cite{tapex}   & 0.4390  \\
OmniTab \cite{omnitab} & 0.4421 \\ \midrule
Zero    & 0.2616 \\
TabIQA1 & 0.8808 \\
TabIQA2 & \textbf{0.8997} \\ \bottomrule
\end{tabular}
\end{table}

Regarding question-answering performance, the scores indicate that TabIQA1 and TabIQA2 significantly outperform the other baseline models, i.e., TAPAS, TAPEX, OmniTab, and the Zero model. These results suggest that TabIQA's fine-tuning the question-answering model on raw HTML tables or structured tables can significantly improve question-answering performance compared to other baseline models. It also indicates that TabIQA2 outperforms TabIQA1, which suggests that fine-tuning the QA model on structured tables can lead to better performance than fine-tuning on raw HTML tables. Overall, these results demonstrate the effectiveness of TabIQA in achieving high accuracy in answering table-related questions.


\section{Conclusion and Future Work} \label{sec1:conclusion}
This paper aims to present a new pipeline called TabIQA to answer questions related to business document images. TabIQA employs cutting-edge deep learning methods in two stages. Firstly, it extracts both the content and structural information from images of tables. Secondly, it utilizes these features to answer questions about numerical data, text-based information, and complex queries from structured tables. Experimental results on VQAonBD 2023 dataset demonstrate that TabIQA can achieve promising performance in answering questions about tables.

We plan to extend the TabIQA pipeline to handle more complex queries that require reasoning over multiple tables or information from the document's non-tabular parts. Another area for future work is to investigate the generalization capabilities of TabIQA to handle tables from different domains or document layouts. These are all potential avenues for future research that could enhance the capabilities and performance of TabIQA in real-world scenarios.
 
\section*{Acknowledgements}
The research was supported by the Cross-ministerial Strategic Innovation Promotion Program (SIP) Second Phase, “Big-data and AI-enabled Cyberspace Technologies” by the New Energy and Industrial Technology Development Organization (NEDO).

% This document is a model and instructions for \LaTeX.
% Please observe the conference page limits. 

% \section{Ease of Use}

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,ref.bib}

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
