\section{Results}
\mypar{Implementation Details}
We implement mesh rasterization and fusion with Pytorch3D~\cite{ravi2020pytorch3d}.
As our text-to-image model $\mathcal{F}_{t2i}$, we utilize a Stable Diffusion~\cite{Rombach2021HighResolutionIS} model, that is finetuned on the image inpainting task, using additional mask input.
We generate a single inpainting proposal and employ a state-of-the-art guided diffusion sampler~\cite{lu2022dpm}.
As our monocular depth estimator $\mathcal{F}_{d}$, we employ an IronDepth~\cite{Bae2022} model, that is trained on indoor scenes from the ScanNet dataset~\cite{dai2017scannet} and augment it for depth inpainting according to Bae~\etal~\cite{Bae2022}.
We set $\delta_{edge}{=}0.1$ and $\delta_{sn}{=}0.1$ in all our experiments.
During generation, we use $20$ different trajectories with $10$ frames each sampled between the respective start and end poses.
We construct prompts using the guidelines suggested by Pierre~\cite{twitter2023}.
Creating one scene takes approximately 50 minutes on one RTX 3090 GPU.

\mypar{Baselines}
To the best of our knowledge, there are no direct baselines that generate textured 3D room geometry from text.
We compare against four related methods (please see the supplemental material for more details about baselines).
\begin{itemize}[leftmargin=*,topsep=0pt, noitemsep]
    \item \emph{PureClipNeRF}~\cite{lee2022understanding}: We compare against text-to-3D methods for generating objects~\cite{poole2022dreamfusion, lin2022magic3d, jain2021dreamfields, lee2022understanding, wang2022score} and choose Lee~\etal~\cite{lee2022understanding} as open-source representative.
    \item \emph{Outpainting}~\cite{Ramesh2022HierarchicalTI, OpenAI2022}: We combine outpainting from a Stable Diffusion~\cite{Rombach2021HighResolutionIS} model with depth estimation and triangulation to create a mesh from an enlarged viewpoint.
    \item \emph{Text2Light}~\cite{chen2022text2light}: We generate RGB panoramas from text using Chen~\etal~\cite{chen2022text2light}. Estimating 3D mesh structure from a panorama is difficult. Related approaches estimate room layout~\cite{xu2021layout}, perform view synthesis ~\cite{kulkarni2022360fusionnerf, hsu2021moving, hara2022enhancement, huang2022360roam} or predict $360^\circ$ depth~\cite{area2022360monodepth, jin2020geometric}. We perform depth prediction and subsequently apply our mesh fusion step.
    \item \emph{Blockade}~\cite{blockade}: We apply \emph{Blockade}~\cite{blockade}, which uses a text-to-image diffusion model to produce more expressive RGB panoramas. We then extract the mesh similarly.
\end{itemize}


\mypar{Evaluation Metrics}
The generated 3D geometry is evaluated both quantitatively and qualitatively. 
We calculate CLIP Score (CS)~\cite{radford2021learning} and Inception Score (IS)~\cite{salimans2016improved} on RGB renderings of the respective scenes.
Additionally, we conduct a user study and ask $n{=}61$ users to score Perceptual Quality (\emph{PQ}) and 3D Structure Completeness (\emph{3DS}) of the whole scene on a scale of $1{-}5$.

\subsection{Qualitative Results}

\input{iccv2023AuthorKit/tables/fig_ours_only}

We show top-down views into the scene and RGB renderings from within for our method and baselines in Figure~\ref{fig:ours-baselines}.
We show additional results of our method in Figure~\ref{fig:ours-only}.
\emph{PureClipNeRF}~\cite{lee2022understanding} creates the key objects of the given text prompt, but does not create a complete 3D structure with floor, walls and ceilings.
\emph{Outpainting}~\cite{Ramesh2022HierarchicalTI, OpenAI2022} creates high-detail textures, but projection from a single viewpoint creates holes due to occlusion and hinders the creation of complete 3D geometry.
\emph{Text2Light}~\cite{chen2022text2light} and \emph{Blockade}~\cite{blockade} both create a high-detail $360^\circ$ view of a complete scene, but occlusions that cannot be resolved from a single panoramic viewpoint lead to holes in the extracted 3D geometry.

In contrast, our approach creates high-detail textures and geometry, that are fused into a complete 3D scene mesh without holes.
The resulting scenes contain flat floors, walls and ceilings, as well as 3D object geometry distributed throughout the scene.
When specifying text prompts with a huge variety, the resulting scene contains a diverse set of objects.
Please see the supplemental material for more scenes, animated results, intermediate outputs of our baselines (such as the panoramic images) as well as top-down views of meshes, that contain the reconstructed ceilings.

\input{iccv2023AuthorKit/tables/fig_ours_baseline}

\subsection{Quantitative Results}

\input{iccv2023AuthorKit/tables/tab_ours_baseline.tex}

We show quantitative results averaged over multiple scenes in Table~\ref{tab:ours-baseline}.
We render 60 images from novel viewpoints for each scene to calculate the 2D metrics.
We present users with multiple top-down views and renderings for each scene and let them rate each method individually (no side-by-side comparison).
Stretched-out geometry and holes in the 3D geometry lead to lower scores for the baselines in all image-based metrics.
Our approach achieves the highest scores, because the renderings are complete from arbitrary novel poses, satisfy the given text-prompt and contain high-resolution image features.
Users prefer our method, which highlights the quality of our accurate and complete geometry, as well as the RGB texture.

\subsection{Ablations}

\input{iccv2023AuthorKit/tables/fig_ours_ablation}

The key ingredients of our method are depth alignment (Section~\ref{subsec:Depth Alignment}), mesh fusion (Section~\ref{subsec:3D Mesh Generation}) and the two-stage viewpoint selection (Section~\ref{subsec:Trajectory Generation}).
We demonstrate the importance of each component in Figure~\ref{fig:ours-ablation} and Table~\ref{tab:ours-baseline}.

\mypar{Depth alignment creates seamless scenes}
Monocular depth predictions from subsequent frames can be inconsistent in scale.
This leads to disconnected components in the mesh that are backprojected from multiple viewpoints (see Figure~\ref{fig:ours-ablation}a).
Our depth alignment strategy allows fusing multiple frames into a seamless mesh, eventually creating a complete scene with flat floors, walls, ceilings and no holes.

\mypar{Stretch removal creates undistorted scene geometry}
During mesh fusion, we update the scene geometry with the contents of the next frame.
Due to noisy depth prediction, the objects become stretched out, if they are observed from small grazing angles.
Thus, we propose two filters (edge length and surface normal thresholds) that alleviate this issue.
Instead of baking in stretched-out geometry (see Figure~\ref{fig:ours-ablation}b), we disregard the corresponding faces and let the object be completed from a more suitable, later viewpoint.

\mypar{Two-stage generation creates complete scenes}
Our approach chooses camera poses in two stages to create a complete scene without holes.
After generating the scene from predefined trajectories, the scene still contains some holes (see Figure~\ref{fig:ours-ablation}c).
Because the scene is built-up over time, it is impossible to choose camera poses \emph{a-priori}, that view all unobserved regions.
To this end, our completion stage samples poses \emph{a-posteriori} to refine those regions.
The resulting mesh is watertight and contains no holes (see Figure~\ref{fig:ours-ablation}d).

\subsection{Application: Controllable Scene Generation}
\input{iccv2023AuthorKit/tables/fig_ours_mix_appl}


Our method can be applied to generate a scene as the combination of multiple text prompts.
Specifically, we use separate text prompts for different poses, crafting a set of trajectories that spatially combines scene descriptions.
This can be desired to avoid repeating elements in a complete scene (e.g., multiple couches spread out over the whole room).
It can also be used to design a house comprised of multiple rooms, each with a different type (e.g., a living room that leads to a kitchen).
We show results that combine multiple text prompts in Figure~\ref{fig:ours-mix-appl}.
We note that the layout can only be partially controlled by the camera poses, since scene generation can create chunks with larger or smaller extent.
We think this demonstrates an exciting application of our method, that can be further explored in future work.

\subsection{Limitations}

Our approach allows to generate 3D room geometry from arbitrary text prompts that are highly detailed and contain consistent geometry.
Nevertheless, our method can still fail under certain conditions (see supplemental material).
First, our thresholding scheme (see Section ~\ref{subsec:3D Mesh Generation}) may not detect all stretched-out regions, which may lead to remaining distortions.
Additionally, some holes may still not be completed fully after the second stage (see Section ~\ref{subsec:Trajectory Generation}), which results in over-smoothed regions after applying poisson reconstruction.
Our scene representation does not decompose material from lighting, which bakes in shadows or bright lamps, that are generated from the diffusion model.

