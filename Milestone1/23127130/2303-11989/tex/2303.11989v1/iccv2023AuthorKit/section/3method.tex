\begin{figure*}[ht]
\centering
\setlength\tabcolsep{0pt}
\begin{tabular}{cc}
\includegraphics[width=0.712\textwidth]{iccv2023AuthorKit/figures/pipeline/pipe_left.pdf} &
\includegraphics[width=0.287\textwidth]{iccv2023AuthorKit/figures/pipeline/pipe_right.pdf} \\
(a) Scene Generation Stage & (b) Scene Completion Stage
\end{tabular}
\vspace{0.5mm}
\caption{\textbf{Method overview}. We iteratively create a textured 3D mesh in two stages. (a) First, we sample predefined poses and text to generate the complete scene layout and furniture. 
Each new pose (marked in green) adds newly generated geometry to the mesh (depicted by green triangles) in an iterative scene generation scheme (see Figure~\ref{fig:iterative-gen} for details).
Blue poses/triangles denote viewpoints that created geometry in a previous iteration.
(b) Second, we fill in the remaining unobserved regions by sampling additional poses (marked in red) after the scene layout is defined.}
\label{fig:pipeline}
\end{figure*}

\section{Method}

Our method creates a textured 3D mesh of a complete scene from text input.
To this end, we continuously fuse generated frames from a 2D text-to-image model at different poses into a joint 3D mesh, creating the scene over time.
The core idea of our approach is a two-stage tailored viewpoint selection, that first generates the scene layout and objects and then closes remaining holes in the 3D geometry (Section~\ref{subsec:Trajectory Generation}).
We visualize this workflow in Figure~\ref{fig:pipeline}.
For each pose in both stages, we apply an iterative scene generation scheme to update the mesh (Section~\ref{subsec:Perpetual Generation}).
We first align each frame with the existing geometry with a depth alignment strategy (Section~\ref{subsec:Depth Alignment}).
Next, we triangulate and filter the novel content to merge it into the mesh (Section~\ref{subsec:3D Mesh Generation}).

\subsection{Iterative 3D Scene Generation}
\label{subsec:Perpetual Generation}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{iccv2023AuthorKit/figures/pipeline/iterative_gen.pdf}
\caption{\textbf{Iterative scene generation}. For each new pose, we render the current mesh to obtain partial RGB and depth renderings. We complete both, utilizing respective inpainting models and the text prompt. Next, we perform depth alignment (see Section~\ref{subsec:Depth Alignment}) and mesh filtering (see Section~\ref{subsec:3D Mesh Generation}) to obtain an optimal next mesh patch, that is finally fused with the existing geometry.}
\label{fig:iterative-gen}
\end{figure}

Our scene is represented as a mesh $\mathcal{M}=(\mathcal{V}, \mathcal{C}, \mathcal{S})$ where the vertices $\mathcal{V} \in \mathbb{R}^{N \times 3}$, vertex colors $\mathcal{C} \in \mathbb{R}^{N \times 3}$ and the face set $\mathcal{S} \in \mathbb{N}_0^{M\times 3}$ are generated over time.
Input to our method is a set of arbitrary text prompts $\{P_t\}_{t=1}^T$ that corresponds to our selected poses $\{E_t z\}_{t=1}^T \in \mathbb{R}^{3\times 4}$ in both stages.
Inspired by recent methods~\cite{infinite_nature_2020, li2022_infinite_nature_zero}, we iteratively build up the scene, following a \textit{render-refine-repeat} pattern.
We summarize this iterative scene generation process in Figure~\ref{fig:iterative-gen}.
Formally, for each step of generation $t$, we first render the current scene from a novel viewpoint: 
\begin{equation}
\label{eq:step-1}
    I_t, d_t, m_t = \textit{r}(\mathcal{M}_t, E_t),
\end{equation}
where $\textit{r}$ is a classical rasterization function without shading, $I_t$ is the rendered image, $d_t$ the rendered depth and $m_t$ the image-space mask, that marks pixels without observed content.
We then use a fixed text-to-image model $\mathcal{F}_{t2i}$ to inpaint unobserved pixels according to the text prompt:
\begin{equation}
\label{eq:step-2}
    \hat{I}_t = \mathcal{F}_{t2i}(I_t, m_t, P_t).
\end{equation}
Next, we inpaint unobserved depth by applying a monocular depth estimator $\mathcal{F}_{d}$ in our depth alignment (see Section~\ref{subsec:Depth Alignment}):
\begin{equation}
\label{eq:step-3}
    \hat{d}_t = \textit{align}(\mathcal{F}_{d}, I_t, d_t, m_t).
\end{equation}
Finally, we combine the novel content $\{\hat{I}_t, \hat{d}_t, m_t\}$ with the existing mesh by our fusion scheme (see Section~\ref{subsec:3D Mesh Generation}):
\begin{equation}
\label{eq:step-4}
    \mathcal{M}_{t{+}1} = \textit{fuse}(\mathcal{M}_{t}, \hat{I}_t, \hat{d}_t, m_t, E_t).
\end{equation}
\subsection{Depth Alignment Step}
\label{subsec:Depth Alignment}

To lift a 2D image $I$ into 3D, we predict the per-pixel depth.
To correctly combine old and new content, it is necessary that both align with each other.
In other words, similar regions in a scene like walls or furniture should be placed at similar depth.
However, directly using the predicted depth for backprojection leads to hard cuts and discontinuities in the 3D geometry, since the depth is inconsistent in scale between subsequent viewpoints (see Figure~\ref{fig:ours-ablation}a).

To this end, we perform depth alignment in two-stages.
First, we use a state-of-the-art depth inpainting network~\cite{Bae2022} that takes ground-truth depth $d$ for known parts in the image as input and aligns the prediction to it: $\hat{d}_{p} = \mathcal{F}_d(I, d).$

Inspired by Liu~\etal~\cite{infinite_nature_2020} we then improve the result by optimizing for scale and shift parameters $\gamma, \beta \in \mathbb{R}$, aligning predicted and rendered disparity in the least squares sense:
\begin{equation}
\label{eq:align-step-2}
    \min_{\gamma, \beta} \quad \left \lVert m \odot \left( \frac{\gamma}{\hat{d}_{p}} + \beta - \frac{1}{d} \right) \right \rVert^2,
\end{equation}
where we mask out unobserved pixels via $m$.
We can then extract the aligned depth as $\hat{d} = \gamma \cdot \hat{d}_{p} + \beta$.
Finally, we smooth $\hat{d}$ by applying a $5\times5$ Gaussian kernel at the mask edges (see supplemental material for more details).

\subsection{Mesh Fusion Step}
\label{subsec:3D Mesh Generation}

\begin{figure}
 \centering
 \setlength\tabcolsep{1pt}
 \begin{tabular}{ccc}
 \includegraphics[width=0.1448\textwidth]{iccv2023AuthorKit/figures/fusion/fuse_part_1.pdf} &
 \includegraphics[width=0.1448\textwidth]{iccv2023AuthorKit/figures/fusion/fuse_part_2.pdf} &
 \includegraphics[width=0.1096\textwidth]{iccv2023AuthorKit/figures/fusion/fuse_part_3.pdf} \\
 (a) Pixel Triangulation & (b) Face Filtering & (c) Mesh Fusion
 \end{tabular}
 \vspace{1mm}
 \caption{\textbf{Visualization of our mesh fusion step.}
(a) We triangulate an image, such that 4 neighboring pixels (orange dots) create two faces.
(b) We filter a face (marked in red), if its surface normal forms a small grazing angle with the viewing direction or if any edge in world space is too long.
(c) We fuse the remaining faces (marked in green) with the existing geometry (marked in blue).
 }
 \label{fig:fusion}
\end{figure}


At each step, we insert new content $\{\hat{I}_t, \hat{d}_t, m_t\}$ into the scene.
For that, we first backproject the image-space pixels into a world-space point cloud:
\begin{equation}
\label{eq:unproj}
\mathcal{P}_t = \{E_t^{-1} K^{-1} (u, v, \hat{d}_t(u,v))^T\}_{u=0,v=0}^{W, H},
\end{equation}
where $K \in \mathbb{R}^{3\times 3}$ are the camera intrinsics and $W, H$ are image width and height, respectively.
We then use a simple triangulation scheme (Figure~\ref{fig:fusion}a), where each four neighboring pixels $\{(u, v), (u{+}1, v), (u, v{+}1), (u{+}1, v{+}1)\}$ in the image form two triangles.
Since the estimated depth is noisy, this na\"ive triangulation creates stretched out 3D geometry (see Figure~\ref{fig:ours-ablation}b).
To alleviate this problem, we propose two filters that remove stretched out faces (Figure~\ref{fig:fusion}b).

First, we filter faces based on their edge length.
We remove a face if the Euclidean distance of any face edge is larger than a threshold $\delta_{edge}$.
Second, we filter faces based on the angle between surface normal and viewing direction:
\begin{equation}
\label{eq:sn-filter}
\mathcal{S} = \{(i_0, i_1, i_2) | n^T v > \delta_{sn} \}
\end{equation}
where $\mathcal{S}$ is the face set, $(i_0, i_1, i_2)$ are the vertex indices of the triangle, $\delta_{sn}$ is the threshold, $n \in \mathbb{R}^3$ is the normalized face normal, and $v \in \mathbb{R}^3$ is the normalized view direction in world space from the camera center towards the average pixel location from which the triangle originated. 
This avoids creating texture for large regions of the mesh from a comparatively small number of pixels from an image.


Finally, we fuse together the newly generated mesh patch and the existing geometry (Figure~\ref{fig:fusion}c).
All faces that are backprojected from pixels falling into the inpainting mask $m_t$ are stitched together with their neighboring faces, which are already part of the mesh.
Precisely, we continue the triangulation scheme at all edges of $m_t$, but use the existing vertex positions of $\mathcal{M}_t$ to create the corresponding faces.

\subsection{Two-Stage Viewpoint Selection}
\label{subsec:Trajectory Generation}

A key part of our method is the choice of text prompts and camera poses from which the scene is synthesized.
Users can in principle choose these inputs arbitrarily to create any desired indoor scene.
However, the generated scene can degenerate and contain stretch and hole artifacts, if poses are chosen carelessly (see Figure~\ref{fig:ours-ablation} and supplemental material).
To this end, we propose a two-stage viewpoint selection strategy, that samples each next camera pose from optimal positions and refines empty regions subsequently.

\mypar{Generation Stage}
\label{para:gen-stage}
In the first stage, we create the main parts of the scene, including the general layout and furniture.
For that, we subsequently render multiple \emph{predefined} trajectories in different directions that eventually cover the whole room.
We found generation works best, if each trajectory starts off from a viewpoint with mostly unobserved regions.
This generates the outline of the next chunk, while still being connected to the rest of the scene (see Figure~\ref{fig:iterative-gen} as an example).
Then, we complete the 3D structure of that chunk by moving and rotating into it subsequently until the end of the trajectory.
Additionally, we ensure an optimal observation distance for each pose.
For example, the green pose in Figure~\ref{fig:pipeline}a is moved back as far as possible into the existing geometry such that it views most of the empty floor region.
We create closed room layouts following this principle, by choosing trajectories that generate the next chunks in a circular motion, roughly centered around the origin.
We found it helpful to discourage the text-to-image generator from generating furniture in unwanted regions by engineering the text prompts accordingly.
For example, for poses looking at the floor or ceiling, we choose text prompts that only contain the words ``floor'' or ``ceiling'', respectively.



\mypar{Completion Stage}
\label{para:complete-stage}
After the first stage, the scene layout and furniture is defined.
However, it is impossible to choose sufficient poses \emph{a-priori}.
Since the scene is generated on-the-fly, the mesh contains holes that were not observed by any camera (see Figure~\ref{fig:ours-ablation}c).
We complete the scene by sampling additional poses \emph{a-posteriori}, looking at those holes.

Inspired by trajectory optimization~\cite{hepp2018plan3d, roberts2017submodular}, we voxelize the scene into dense uniform cells.
We sample random poses in each cell, discarding those being too close to existing geometry.
We select one pose per cell that views most unobserved pixels (e.g., see the red poses in Figure~\ref{fig:pipeline}b).

Next, we inpaint the scene from all chosen camera poses following Section~\ref{subsec:Perpetual Generation}.
Similar to Fridman~\etal~\cite{fridman2023scenescape}, we observe it is important to clean the inpainting masks, because our text-to-image generator can generate better results for large connected regions.
Thus, we first inpaint small holes with a classical inpainting algorithm~\cite{telea2004image} and dilate the remaining holes.
We additionally remove all faces that fall into the dilated region and are close to the rendered depth.
Please see the supplemental material for more details.

Finally, we run Poisson surface reconstruction~\cite{kazhdan2006poisson} on the scene mesh.
This closes any remaining holes after completion and smoothens out discontinuities. 
The result is a watertight mesh of the generated scene, that can be rendered with classical rasterization.
