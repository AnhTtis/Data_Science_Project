\section{Conclusion}
We have shown a method to generate textured 3D meshes from only text input.
We use text-to-image 2D generators to create a sequence of images.
The core insight of our method is a tailored viewpoint selection, that allows to create a 3D mesh with seamless geometry and compelling textures.
Specifically, we lift the images into a 3D scene, by employing our alignment strategy that iteratively fuses all images into the mesh.
Our output meshes represent arbitrary indoor scenes that can be rendered with classical rasterization pipelines.
We believe our approach demonstrates an exciting application of large-scale 3D asset creation, that only requires text as input.