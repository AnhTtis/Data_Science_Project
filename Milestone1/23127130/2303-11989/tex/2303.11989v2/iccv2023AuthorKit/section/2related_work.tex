\section{Related Work}


\par \noindent{\bf Text-based Generation}
has seen significant advances due to large-scale image-text datasets~\cite{sharma2018conceptual, Schuhmann2021LAION400MOD, Desai2021RedCapsWI, Schuhmann2022LAION5BAO} and scalable generative model architectures~\cite{Esser2020TamingTF, Ronneberger2015UNetCN, Razavi2019GeneratingDH, Karras2017ProgressiveGO}, enabling synthesis of novel images from text~\cite{Gu2021VectorQD, Avrahami2021BlendedDF, Patashnik2021StyleCLIPTM}.

Recently, diffusion models~\cite{sohl2015deep, Ho2020DenoisingDP, Song2019GenerativeMB, Song2020ImprovedTF, Song2020ScoreBasedGM} achieved impressive results on image synthesis~\cite{Dhariwal2021DiffusionMB, Rombach2021HighResolutionIS, Saharia2022PhotorealisticTD, Nichol2021GLIDETP, Ramesh2022HierarchicalTI} through improvements like latent space denoising~\cite{Rombach2021HighResolutionIS, Vahdat2021ScorebasedGM}, faster sampling~\cite{Ho2020DenoisingDP, Song2020DenoisingDI, Nichol2021ImprovedDD, Kong2021OnFS}, and better guidance~\cite{Ho2022ClassifierFreeDG}. 

In particular, \emph{text-to-image} methods like Stable Diffusion~\cite{Rombach2021HighResolutionIS}, Imagen~\cite{Saharia2022PhotorealisticTD}, GLIDE~\cite{Nichol2021GLIDETP} and DALL$\cdot$E 2~\cite{Ramesh2022HierarchicalTI} yield diverse, high-fidelity, and controllable~\cite{Brooks2022InstructPix2PixLT,zhang2023adding} outputs.
Text-based generation has been extended to other modalities including audio~\cite{Kong2020DiffWaveAV, Forsgren_Martiros_2022, Huang2023Noise2MusicTM, Schneider2023MosaiTG}, video~\cite{Singer2022MakeAVideoTG, Wu2022TuneAVideoOT, villegas2022phenaki, Ho2022VideoDM}, and 4D fields~\cite{Singer2023TextTo4DDS}.
We use \emph{text-to-image} models by lifting their generated output into complete 3D scene meshes.

\rwpar{Text-to-3D.}
Several methods use 3D data for supervised training of text-to-3D models~\cite{Chen2018Text2ShapeGS, nichol2022point, Bautista2022GAUDIAN};
however this direction remains challenging due to the lack of large-scale aligned datasets of text and 3D.

Alternative approaches use 2D vision-language models like CLIP~\cite{radford2021learning} to create 3D content by formulating the generation as an optimization problem in the image domain~\cite{Wang2021CLIPNeRFTD, jain2021dreamfields, lee2022understanding, khalid2022clipmesh, jiang20223d} or as object alignment~\cite{Sanghi2021CLIPForgeTZ}.
Related methods refine existing 3D input through text guidance in a similar fashion~\cite{michel2022text2mesh, chen2022tango, wang2022nerf, richardson2023texture}.

Recent methods~\cite{poole2022dreamfusion, lin2022magic3d, metzer2022latent, wang2022score, MelasKyriazi2023RealFusion3R} combine large text-to-image diffusion models~\cite{Rombach2021HighResolutionIS, Saharia2022PhotorealisticTD} and neural radiance fields~\cite{Mildenhall2020NeRFRS} to generate 3D objects without training.
Other approaches train custom diffusion models on a similar text-to-3D task~\cite{li20223ddesigner, nam20223d, cheng2022sdfusion}.
In contrast, we use a fixed text-to-image model and extract a 3D mesh representing entire scenes of many objects and structural elements like walls.






\rwpar{3D-Consistent View Synthesis from a Single Image.}
Several methods have been proposed that perform novel-view-synthesis from a single image~\cite{Rockwell2021PixelSynthGA, Wiles2019SynSinEV, Shih20203DPU, Ren2022LookOT, Gu2023NerfDiffSV}.
Others optimize a neural 3D representation of an object, that can be viewed from arbitrary novel view points~\cite{Xu_2022_neuralLift, watson2022novel, anciukevicius2022renderdiffusion}.
Another line of work performs \emph{perpetual view generation}~\cite{Sivic2008InfiniteIC, infinite_nature_2020, li2022_infinite_nature_zero, cai2022diffdreamer}, synthesizing videos via a \emph{render-refine-repeat} pattern from a single RGB image that depict a scene along a forward-facing camera trajectory.
In very recent concurrent work, Fridman~\etal~\cite{fridman2023scenescape} create 3D scenes from text, but focus on this type of 3D-consistent ``zoom-out'' video generation.
Instead, we generate complete, textured 3D room geometry from arbitrary trajectories.











