\section{Introduction}

Mesh representations of 3D scenes are a crucial component for many applications, from AR/VR asset creation to computer graphics, yet creating these 3D assets remains a painstaking process that requires considerable expertise.
In the 2D domain, recent works have successfully created high-quality images from text using generative models, such as diffusion models~\cite{Rombach2021HighResolutionIS, Ramesh2022HierarchicalTI, Saharia2022PhotorealisticTD}.
These methods significantly reduce the barriers to creating images that contain a user's desired content, effectively helping towards the democratization of content creation.
An emerging line of work has sought to apply similar methods to create 3D models from text~\cite{Chen2018Text2ShapeGS,poole2022dreamfusion, jain2021dreamfields, lin2022magic3d, lee2022understanding}, yet existing approaches come with a number of significant limitations and lack the generality of 2D text-to-image models.

One of the core challenges of generating 3D models is coping with the lack of available 3D training data, as 3D datasets are vastly smaller than those available in many other applications, such as 2D image synthesis.
For example, methods that directly use 3D supervision, such as Chen~\etal~\cite{Chen2018Text2ShapeGS}, are often limited to datasets of simple shapes, such as ShapeNet~\cite{Chang2015ShapeNetAI}.
To address these data limitations, recent methods~\cite{poole2022dreamfusion, jain2021dreamfields, lin2022magic3d, lee2022understanding, wang2022score} lift the expressive power of 2D text-to-image models into 3D by formulating 3D generation as an iterative optimization problem in the image domain.
This allows them to generate 3D objects stored in a radiance field representation, demonstrating the ability to generate arbitrary (neural) shapes from text.
However, these methods cannot easily be extended to create room-scale 3D structure and texture.
The challenge of generating large scenes is ensuring that the generated output is dense and coherent across outward-facing viewpoints, and that these views contain all of the required structures, such as walls, floors, and furniture.
Additionally, a mesh remains a desired representation for many end-user tasks, such as rendering on commodity hardware (which requires an additional conversion step as presented in Lin~\etal~\cite{lin2022magic3d}).




To address these shortcomings, we propose a method that extracts scene-scale 3D meshes from off-the-shelf 2D text-to-image models. 
Our method iteratively generates a scene through inpainting and monocular depth estimation.
We produce an initial mesh by generating an image from text, and backproject it into 3D using a depth estimation model.
Then, we iteratively render the mesh from novel viewpoints.
From each one, we fill in holes in the rendered images via inpainting, then fuse the generated content into the mesh (Fig.~\ref{fig:teaser}a).

Our iterative generation scheme has two important design considerations: how we choose the viewpoints, and how we merge generated scene content with the existing mesh.
We first select viewpoints from predefined trajectories that will cover large amounts of scene content, then adaptively select viewpoints that close remaining holes.
When merging generated content with the mesh, we align the two depth maps to create smooth transitions, and remove parts of the mesh that contain distorted textures.
Together, these decisions lead to large, scene-scale 3D meshes with compelling textures and consistent geometry (Fig.~\ref{fig:teaser}b), that can represent a wide range of rooms.

\noindent To summarize, our contributions are:
\begin{itemize}[leftmargin=*,topsep=1pt, noitemsep]
    \item Generating 3D meshes of room-scale indoor scenes with compelling textures and geometry from any text input.
    \item A method that leverages 2D text-to-image models and monocular depth estimation to lift frames into 3D in an iterative scene generation. Our proposed depth alignment and mesh fusion steps, enable us to create seamless and undistorted geometry and textures.
    \item A two-stage tailored viewpoint selection that samples camera poses from optimal positions to first create the room layout and furniture and then close any remaining holes, creating a watertight mesh.
\end{itemize}
























