\appendix
\section{Supplemental Video}
Please watch our attached video~\footnote[1]{\url{https://youtu.be/fjRnFL91EZc}} for a comprehensive evaluation of the proposed method.
We include rendered videos of multiple generated scenes from novel trajectories, that showcase the quality of both generated texture and geometry (and also show the generated ceilings).
We also show an animation how the mesh is built up over time, that illustrates the usage of our two-stage pose sampling scheme (generation and completion).
We compare against baselines and ablations of our method by showing rendered videos.

\section{Societal Impact}

Our method leverages text-to-image models to generate a sequence of images from text, specifically we use the Stable Diffusion model~\cite{Rombach2021HighResolutionIS}.
Thus it inherits possible drawbacks of these 2D models.
First, our method could be exploited to generate harmful content, by forcing the text-to-image model to generate respective images.
Furthermore, our method is biased towards the cultural or stereotypical data distribution, that was used to train the text-to-image model.
Lastly, we note that text-to-image models are trained on large-scale text-image datasets~\cite{Schuhmann2022LAION5BAO}.
Thus, the model learns to reproduce and combine the style of artists, whose works are contained in these datasets.
This raises questions regarding the correct way to credit these artists or if it is ethical to benefit from their works in this way at all.

Our method can be used to generate meshes, that depict entire scenes, from only text as input.
This significantly reduces the required expertise to model and design such 3D assets.
Thus, we believe our work proposes a promising step towards the democratization of large-scale 3D content creation.

\section{Limitations}
\input{iccv2023AuthorKit/Supp_figures/sup_fig_limitations}

Given a text prompt, our approach allows to generate 3D room geometry that is highly detailed and contains consistent 3D geometry.
Nevertheless, our method can still fail under certain conditions (see Figure~\ref{fig:supp-limitation}).

First, our completion stage
(see Section~3.4)
might not be able to inpaint all holes (Figure~\ref{fig:supp-limitation}b).
For example this can happen, if an object contains holes that are close to a wall.
These angles are hard to see from additional cameras and thus might remain untouched.
We still close these holes by applying Poisson surface reconstruction~\cite{kazhdan2006poisson}.
However, this can results in overly smoothed geometry.

Second, our mesh fusion stage
(see Section~3.3)
might not remove all stretched-out faces.
Faces can appear stretched-out because of imperfect depth estimation and alignment.
Over time this can yield unusual room shapes such as the curved wall in Figure~\ref{fig:supp-limitation}c.
We apply two filtering schemes to remove stretched-out faces before fusing them with the existing geometry.
Both use thresholds $\delta_{sn}{=}0.1, \delta_{edge}{=}0.1$, that we fix during all our experiments.
It can happen that some faces are not removed by the filtering schemes, but are still stretched-out unnaturally.
However, we find that lowering the thresholds would also remove unstretched geometry.
This would make creating a complete scene harder, because more holes need to be inpainted in the completion stage.

\section{Details on User Study}
\input{iccv2023AuthorKit/Supp_figures/sup_fig_user_study}

We conduct a user study and ask $n{=}61$ users to score Perceptual Quality (\emph{PQ}) and 3D Structure Completeness (\emph{3DS}) of the whole scene on a scale of $1{-}5$.
We show an example of how we asked the users to score these two metrics in Figure~\ref{fig:sup_user_study}.
We present users with multiple images from each scene, that show it from multiple angles.
Then we ask them to rate the scene on a scale from $1{-}5$ by asking them about the 3D structure completeness and the overall perceptual quality.
In total, we received $1098$ datapoints from multiple scenes and report averaged results per method.

\section{Additional Implementation Details}
We give additional implementation details in the following subsections.

\subsection{Importance of Predefined Trajectories}
\input{iccv2023AuthorKit/Supp_figures/sup_fig_predefined_trajectory}

We create the complete scene layout and furniture in the first stage of our tailored two-stage viewpoint selection scheme
(see Section~3.4).
To this end, we sample multiple \emph{predefined} trajectories from which we iteratively generate the scene.
We fix the trajectories for our main results, as we found it already creates rooms with a variety of different layouts.
Users can modify them according to our guidelines as demonstrated in
Section~4.4 in the main paper.
Each trajectory consists of a start pose and an end pose and we linearly interpolate between both.
We found generation works best, if each trajectory starts off from a viewpoint with mostly unobserved regions.
This gives the text-to-image model enough freedom to create novel content with reasonable global structure.

Thus, we construct each trajectory with the following principle.
First, we select a start pose that views mostly unobserved content and generate the outline of the next scene chunk from it (Figure~\ref{fig:supp-predefined_traj}b).
Then, we subsequently translate and rotate into the chunk to refine its 3D structure until the end of the trajectory (Figure~\ref{fig:supp-predefined_traj}c).
This creates mesh patches with convincing 3D structure (Figure~\ref{fig:supp-predefined_traj}d).
In contrast, if we design trajectories that do not follow this principle, results can degenerate.
For example, if the viewpoint change is small, the text-to-image model creates novel content only for small portions of the image (Figure~\ref{fig:supp-predefined_traj}e-g).
Thus, locally the generated content looks reasonable, but it accumulates into inconsistent global structure (Figure~\ref{fig:supp-predefined_traj}h).

\subsection{Effect of Depth Smoothing in Alignment}
\input{iccv2023AuthorKit/Supp_figures/sup_fig_depth_smoothing}

For each camera pose in both stages, we follow an iterative scene generation scheme
(see Section~3.1).
After generating novel content, we predict its depth in our depth alignment stage
(see Section~3.2).
First, we predict the depth using a monocular depth inpainting network (Figure~\ref{fig:supp-depth-smoothing}b).
However, directly using this depth for mesh fusion results in unaligned mesh patches (Figure~\ref{fig:supp-depth-smoothing}g).
Thus, we improve the result by aligning rendered depth and inpainted depth in the least squares sense (Figure~\ref{fig:supp-depth-smoothing}c).
Finally, we smooth the aligned depth by applying a $5\times5$ gaussian blur kernel at the image edges between rendered and predicted depth (Figure~\ref{fig:supp-depth-smoothing}d).
This smoothens out remaining discontinuity artifacts between old and new content (Figure~\ref{fig:supp-depth-smoothing}e and f).
In practice, we found this can further reduce sharp borders between objects, leading to overall better alignment (Figure~\ref{fig:supp-depth-smoothing}h).

\subsection{Importance of Mask Dilation in Completion}
\input{iccv2023AuthorKit/Supp_figures/sup_fig_mask_dilation}

We complete the scene in the second stage of our tailored two-stage viewpoint selection scheme, by filling in remaining holes in the mesh
(see Section~3.4).
To this end, we first select suitable camera poses that look at these holes (Figure~\ref{fig:supp-mask-dilation}a).
We then follow the iterative scene generation scheme to fill in the holes in the mesh
(see Section~3.1).
The holes can have arbitrarily small or large sizes, depending on how the scene layout was generated in the first stage of our method (Figure~\ref{fig:supp-mask-dilation}b).
Similarly to Fridman~\etal~\cite{fridman2023scenescape}, we found that directly inpainting such holes can lead to sub-optimal results (Figure~\ref{fig:supp-mask-dilation}c).
This is because the text-to-image model needs to inpaint small regions and the direct neighborhood of the holes can be distorted.
To alleviate this issue, we inpaint small holes with a classical inpainting algorithm~\cite{telea2004image}.
We classify small holes by applying a morphological erosion operation with a $3\times3$ kernel on the inpainting mask.
Next, we increase the size of remaining holes, by repeating a morphological dilation operation with a $7\times7$ kernel on the eroded inpainting mask for five times (Figure~\ref{fig:supp-mask-dilation}d).
Finally, we inpaint the image using the dilated mask (Figure~\ref{fig:supp-mask-dilation}e).
This yields more convincing results because the text-to-image model can inpaint larger areas and create more meaningful global structure.
To combine the new content with the existing mesh, we apply our triangulation scheme
(see Section~3.3).
Additionally, we remove all faces that fall into the dilated region and are close to the rendered screen-space depth (since they are replaced by the novel content).

\section{Additional Discussion on Related Methods and Baselines}
\input{iccv2023AuthorKit/Supp_figures/sup_fig_dreamfusion}
\input{iccv2023AuthorKit/Supp_figures/sup_fig_baseline_results}

To the best of our knowledge, there are no direct baselines that generate textured 3D room geometry from text.
We compare against four related methods, that do not require supervision from 3D datasets.
In the following we give additional discussion on related methods and our selected baselines.

\rwpar{PureClipNeRF}~\cite{lee2022understanding}: We compare against text-to-3D methods for generating objects~\cite{poole2022dreamfusion, lin2022magic3d, jain2021dreamfields, lee2022understanding, wang2022score} and choose Lee~\etal~\cite{lee2022understanding} as open-source representative.
A common pattern in these text-to-3D methods is to sample inward-facing poses on a hemisphere, from which the object is iteratively optimized.
While the method of Lee~\etal~\cite{lee2022understanding} does not use a diffusion model to create high-fidelity images, it still uses the same pose sampling pattern.
This allows us to compare against these methods in general, by analyzing how well this pose sampling pattern can produce complete 3D scenes with structural elements like walls or floors.
We also run DreamFusion~\cite{poole2022dreamfusion} from the third-party implementation of Guo~\etal~\cite{threestudio2023}, see Figure~\ref{fig:sup_fig_dreamfusion}.
Similar to PureClipNeRF, object-centric cameras yield incomplete rooms.
Outward-facing cameras yield blurry $360^{\circ}$ surroundings, showing floaters when rendered out-of-distribution.

\rwpar{Outpainting}~\cite{Ramesh2022HierarchicalTI, OpenAI2022}: We compare against image outpainting. 
We combine outpainting from a Stable Diffusion~\cite{Rombach2021HighResolutionIS} model with depth estimation and triangulation to create a mesh from an enlarged viewpoint.
Starting off from a single generated image, we can synthesize novel content around it to create a complete scene in a single image plane (Figure~\ref{fig:supp-itermediate-baseline}a).
After creating the image, we then perform depth estimation and triangulation to lift the image into a 3D mesh.

\rwpar{Text2Light}~\cite{chen2022text2light}: We generate RGB panoramas from text using Chen~\etal~\cite{chen2022text2light}. 
We show example outputs in Figure~\ref{fig:supp-itermediate-baseline}b.
One can create immersive experiences by rendering a panorama onto a sphere, allowing to view the scene from arbitrary $360^\circ$ viewpoints.
However, it is not possible to simulate a true 3D environment directly (e.g., translating or rotating around objects), because the panorama only captures a single viewpoint.
Thus, related approaches estimate room layout~\cite{xu2021layout}, perform view synthesis ~\cite{kulkarni2022360fusionnerf, hsu2021moving, hara2022enhancement, huang2022360roam} or predict $360^\circ$ depth~\cite{area2022360monodepth, jin2020geometric} from one or multiple panoramas.
To compare to our method, we reconstruct the 3D mesh structure that can be obtained from a single panoramic image.
To this end, we perform depth prediction and subsequently apply our mesh fusion step.

\rwpar{Blockade}~\cite{blockade}: We compare against \emph{Blockade}~\cite{blockade}, which uses a text-to-image diffusion model to produce expressive RGB panoramas. We then extract the mesh similarly.

\rwpar{GAUDI}~\cite{Bautista2022GAUDIAN}: Bautista and Guo~\etal~\cite{Bautista2022GAUDIAN} present a method to generate large-scale 3D scenes encoded into a NeRF~\cite{Mildenhall2020NeRFRS} representation.
Their generative model can be conditioned to produce 3D indoor scenes from text as input.
In general, each scene allows for a different distribution of camera poses.
Walls and objects are placed at different positions in each scene, thus it depends on the scene to determine valid camera poses.
They model this joint latent distribution of scenes and cameras.
This allows to synthesize scenes that can be rendered from corresponding camera trajectories (e.g., a scene is rendered in a forward motion).
However, it requires training supervision from 3D datasets that contain ground-truth camera trajectories.
This restricts the method to the domain of a specific dataset of (synthetic, low-resolution) 3D scenes, which is limited in size and diversity.

In contrast, we choose another approach to represent the joint distribution of scenes and camera trajectories.
Our two-stage tailored viewpoint selection
(see Section~3.4)
first creates the general scene layout and furniture from predefined trajectories.
We choose these trajectories such that the camera poses do not intersect with generated geometry
(see Section~3.4 for more details).
Then we inpaint remaining holes by sampling additional poses.
This allows us to generate complete scenes with varying layouts.
Our resulting mesh can be rendered from arbitrary viewpoints, i.e., it is not bound to the specific trajectory used during generation.
Furthermore, our method can directly lift the generated images of a 2D text-to-image model into 3D, without requiring supervised training from 3D datasets.
This allows us to generate meshes, that can represent a much larger and more diverse set of indoor scenes with higher visual quality.

\section{Additional Qualitative Results}
\input{iccv2023AuthorKit/Supp_figures/sup_fig_ours_only_more_final}

We show additional qualitative results of our method in Figure~\ref{fig:supp-ours-only-final}.