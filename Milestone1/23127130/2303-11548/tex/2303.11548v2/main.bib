@inproceedings{ijcai2019-129,
  title     = {Talking Face Generation by Conditional Recurrent Adversarial Network},
  author    = {Song, Yang and Zhu, Jingwen and Li, Dawei and Wang, Andy and Qi, Hairong},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {919--925},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/129},
  url       = {https://doi.org/10.24963/ijcai.2019/129},
}
@inproceedings{wu2018reenactgan,
  title={Reenactgan: Learning to reenact faces via boundary transfer},
  author={Wu, Wayne and Zhang, Yunxuan and Li, Cheng and Qian, Chen and Loy, Chen Change},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={603--619},
  year={2018}
}
@inproceedings{chen2019hierarchical,
  title={Hierarchical cross-modal talking face generation with dynamic pixel-wise loss},
  author={Chen, Lele and Maddox, Ross K and Duan, Zhiyao and Xu, Chenliang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7832--7841},
  year={2019}
}
@InProceedings{Huang_2020_CVPR,
author = {Huang, Po-Hsiang and Yang, Fu-En and Wang, Yu-Chiang Frank},
title = {Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{Zhang_2020_CVPR,
author = {Zhang, Jiangning and Zeng, Xianfang and Wang, Mengmeng and Pan, Yusu and Liu, Liang and Liu, Yong and Ding, Yu and Fan, Changjie},
title = {FReeNet: Multi-Identity Face Reenactment},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}
@InProceedings{Zhang_2021_ICCV,
    author    = {Zhang, Chenxu and Zhao, Yifan and Huang, Yifei and Zeng, Ming and Ni, Saifeng and Budagavi, Madhukar and Guo, Xiaohu},
    title     = {FACIAL: Synthesizing Dynamic Talking Face With Implicit Attribute Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {3867-3876}
}
@inproceedings{prajwal2020lip,
  title={A lip sync expert is all you need for speech to lip generation in the wild},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={484--492},
  year={2020}
}
@article{wang2022attention,
  title={Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild},
  author={Wang, Ganglai and Zhang, Peng and Xie, Lei and Huang, Wei and Zha, Yufei},
  journal={arXiv preprint arXiv:2203.03984},
  year={2022}
}
@InProceedings{Ji_2021_CVPR,
    author    = {Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Wayne and Loy, Chen Change and Cao, Xun and Xu, Feng},
    title     = {Audio-Driven Emotional Video Portraits},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {14080-14089}
}
@inbook{10.1145/3394171.3413844,
author = {Zeng, Dan and Liu, Han and Lin, Hui and Ge, Shiming},
title = {Talking Face Generation with Expression-Tailored Generative Adversarial Network},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413844},
abstract = {A key of automatically generating vivid talking faces is to synthesize identity-preserving natural facial expressions beyond audio-lip synchronization, which usually need to disentangle the informative features from multiple modals and then fuse them together. In this paper, we propose an end-to-end Expression-Tailored Generative Adversarial Network (ET-GAN) to generate an expression enriched talking face video of arbitrary identity. Different from talking face generation based on identity image and audio, an expressional video of arbitrary identity serves as the expression source in our approach. Expression encoder is proposed to disentangle expression-tailored representation from the guiding expressional video, while audio encoder disentangles audio-lip representation. Instead of using single image as identity input, multi-image identity encoder is proposed by learning different views of faces and merging a unified representation. Multiple discriminators are exploited to keep both image-aware and the video-aware realistic details, including a spatial-temporal discriminator for visual continuity of expression synthesis and facial movements. We conduct extensive experimental evaluations on quantitative metrics, expression retention quality and audio-visual synchronization. The results show the effectiveness of our ET-GAN in generating high quality expressional talking face videos against existing state-of-the-arts.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1716–1724},
numpages = {9}
}
@InProceedings{10.1007/978-3-030-01249-6_50,
author="Pumarola, Albert
and Agudo, Antonio
and Martinez, Aleix M.
and Sanfeliu, Alberto
and Moreno-Noguer, Francesc",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="GANimation: Anatomically-Aware Facial Animation from a Single Image",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="835--851",
abstract="Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN, that conditions GANs' generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild.",
isbn="978-3-030-01249-6"
}
@article{ali2020efficient,
  title={An Efficient Integration of Disentangled Attended Expression and Identity FeaturesFor Facial Expression Transfer andSynthesis},
  author={Ali, Kamran and Hughes, Charles E},
  journal={arXiv preprint arXiv:2005.00499},
  year={2020}
}
@ARTICLE{9496264,
  author={Eskimez, Sefik Emre and Zhang, You and Duan, Zhiyao},
  journal={IEEE Transactions on Multimedia}, 
  title={Speech Driven Talking Face Generation from a Single Image and an Emotion Condition}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TMM.2021.3099900}}
@inproceedings{chung2016out,
  title={Out of time: automated lip sync in the wild},
  author={Chung, Joon Son and Zisserman, Andrew},
  booktitle={Asian conference on computer vision},
  pages={251--263},
  year={2016},
  organization={Springer}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{10.1145/3306346.3323028,
author = {Fried, Ohad and Tewari, Ayush and Zollh\"{o}fer, Michael and Finkelstein, Adam and Shechtman, Eli and Goldman, Dan B and Genova, Kyle and Jin, Zeyu and Theobalt, Christian and Agrawala, Maneesh},
title = {Text-Based Editing of Talking-Head Video},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3323028},
doi = {10.1145/3306346.3323028},
abstract = {Editing talking-head video to change the speech content or to remove filler words is challenging. We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame. To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. We demonstrate a large variety of edits, such as the addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {68},
numpages = {14},
keywords = {visemes, face tracking, text-based video editing, dubbing, neural rendering, face parameterization, talking heads}
}
@InProceedings{Chen_2019_CVPR_Workshops,
author = {Chen, Lele and Zheng, Haitian and Maddox, Ross and Duan, Zhiyao and Xu, Chenliang},
title = {Sound to Visual: Hierarchical Cross-Modal Talking Face Generation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2019}
}
@article{jamaludin2019you,
  title={You said that?: Synthesising talking faces from audio},
  author={Jamaludin, Amir and Chung, Joon Son and Zisserman, Andrew},
  journal={International Journal of Computer Vision},
  volume={127},
  number={11},
  pages={1767--1779},
  year={2019},
  publisher={Springer}
}
@inproceedings{thies2020neural,
  title={Neural voice puppetry: Audio-driven facial reenactment},
  author={Thies, Justus and Elgharib, Mohamed and Tewari, Ayush and Theobalt, Christian and Nie{\ss}ner, Matthias},
  booktitle={European conference on computer vision},
  pages={716--731},
  year={2020},
  organization={Springer}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@article{cao2014crema,
  title={Crema-d: Crowd-sourced emotional multimodal actors dataset},
  author={Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur, Ruben C and Nenkova, Ani and Verma, Ragini},
  journal={IEEE transactions on affective computing},
  volume={5},
  number={4},
  pages={377--390},
  year={2014},
  publisher={IEEE}
}
@article{livingstone2018ryerson,
  title={The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
  author={Livingstone, Steven R and Russo, Frank A},
  journal={PloS one},
  volume={13},
  number={5},
  pages={e0196391},
  year={2018},
  publisher={Public Library of Science}
}
@article{10.1145/3072959.3073640,
author = {Suwajanakorn, Supasorn and Seitz, Steven M. and Kemelmacher-Shlizerman, Ira},
title = {Synthesizing Obama: Learning Lip Sync from Audio},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073640},
doi = {10.1145/3072959.3073640},
abstract = {Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {95},
numpages = {13},
keywords = {videos, lip sync, audiovisual speech, big data, LSTM, face synthesis, uncanny valley, RNN, audio}
}
@article{yin2022styleheat,
  title={StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pretrained StyleGAN},
  author={Yin, Fei and Zhang, Yong and Cun, Xiaodong and Cao, Mingdeng and Fan, Yanbo and Wang, Xuan and Bai, Qingyan and Wu, Baoyuan and Wang, Jue and Yang, Yujiu},
  journal={arXiv preprint arXiv:2203.04036},
  year={2022}
}
@InProceedings{Zhang_2021_CVPR,
    author    = {Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
    title     = {Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3661-3670}
}
@incollection{cambria2012hourglass,
  title={The hourglass of emotions},
  author={Cambria, Erik and Livingstone, Andrew and Hussain, Amir},
  booktitle={Cognitive behavioural systems},
  pages={144--157},
  year={2012},
  publisher={Springer}
}
@article{susanto2020hourglass,
  title={The hourglass model revisited},
  author={Susanto, Yosephine and Livingstone, Andrew G and Ng, Bee Chin and Cambria, Erik},
  journal={IEEE Intelligent Systems},
  volume={35},
  number={5},
  pages={96--102},
  year={2020},
  publisher={IEEE}
}
@article{lewis2007neural,
  title={Neural correlates of processing valence and arousal in affective words},
  author={Lewis, Penelope A and Critchley, Hugo D and Rotshtein, Pia and Dolan, Raymond J},
  journal={Cerebral cortex},
  volume={17},
  number={3},
  pages={742--748},
  year={2007},
  publisher={Oxford University Press}
}
@article{nicolaou2011continuous,
  title={Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space},
  author={Nicolaou, Mihalis A and Gunes, Hatice and Pantic, Maja},
  journal={IEEE Transactions on Affective Computing},
  volume={2},
  number={2},
  pages={92--105},
  year={2011},
  publisher={IEEE}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@article{afouras2018deep,
  title={Deep audio-visual speech recognition},
  author={Afouras, Triantafyllos and Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2018},
  publisher={IEEE}
}
@inproceedings{magnusson2021invertable,
  title={Invertable Frowns: Video-to-Video Facial Emotion Translation},
  author={Magnusson, Ian and Sankaranarayanan, Aruna and Lippman, Andrew},
  booktitle={Proceedings of the 1st Workshop on Synthetic Multimedia-Audiovisual Deepfake Generation and Detection},
  pages={25--33},
  year={2021}
}
@inproceedings{berndt1994using,
  title={Using dynamic time warping to find patterns in time series.},
  author={Berndt, Donald J and Clifford, James},
  booktitle={KDD workshop},
  volume={10},
  number={16},
  pages={359--370},
  year={1994},
  organization={Seattle, WA, USA:}
}
@inproceedings{johnson2016perceptual,
  title={Perceptual losses for real-time style transfer and super-resolution},
  author={Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  booktitle={European conference on computer vision},
  pages={694--711},
  year={2016},
  organization={Springer}
}
@inproceedings{hosler2021deepfakes,
  title={Do Deepfakes Feel Emotions? A Semantic Approach to Detecting Deepfakes via Emotional Inconsistencies},
  author={Hosler, Brian and Salvi, Davide and Murray, Anthony and Antonacci, Fabio and Bestagini, Paolo and Tubaro, Stefano and Stamm, Matthew C},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1013--1022},
  year={2021}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@article{DBLP:journals/corr/abs-1906-06337,
  author    = {Konstantinos Vougioukas and
               Stavros Petridis and
               Maja Pantic},
  title     = {Realistic Speech-Driven Facial Animation with GANs},
  journal   = {CoRR},
  volume    = {abs/1906.06337},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.06337},
  eprinttype = {arXiv},
  eprint    = {1906.06337},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-06337.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{https://doi.org/10.48550/arxiv.2007.08547,
  doi = {10.48550/ARXIV.2007.08547},
  
  url = {https://arxiv.org/abs/2007.08547},
  
  author = {Chen, Lele and Cui, Guofeng and Liu, Celong and Li, Zhong and Kou, Ziyi and Xu, Yi and Xu, Chenliang},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Talking-head Generation with Rhythmic Head Motion},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{wen2020photorealistic,
  title={Photorealistic audio-driven video portraits},
  author={Wen, Xin and Wang, Miao and Richardt, Christian and Chen, Ze-Yin and Hu, Shi-Min},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={26},
  number={12},
  pages={3457--3466},
  year={2020},
  publisher={IEEE}
}

@inproceedings{wang2020mead,
  title={Mead: A large-scale audio-visual dataset for emotional talking-face generation},
  author={Wang, Kaisiyuan and Wu, Qianyi and Song, Linsen and Yang, Zhuoqian and Wu, Wayne and Qian, Chen and He, Ran and Qiao, Yu and Loy, Chen Change},
  booktitle={European Conference on Computer Vision},
  pages={700--717},
  year={2020},
  organization={Springer}
}
@article{sinha2022emotion,
  title={Emotion-Controllable Generalized Talking Face Generation},
  author={Sinha, Sanjana and Biswas, Sandika and Yadav, Ravindra and Bhowmick, Brojeshwar},
  journal={arXiv preprint arXiv:2205.01155},
  year={2022}
}
@inproceedings{das2020speech,
  title={Speech-driven facial animation using cascaded gans for learning of motion and texture},
  author={Das, Dipanjan and Biswas, Sandika and Sinha, Sanjana and Bhowmick, Brojeshwar},
  booktitle={European conference on computer vision},
  pages={408--424},
  year={2020},
  organization={Springer}
}
@article{zhou2020makelttalk,
  title={Makelttalk: speaker-aware talking-head animation},
  author={Zhou, Yang and Han, Xintong and Shechtman, Eli and Echevarria, Jose and Kalogerakis, Evangelos and Li, Dingzeyu},
  journal={ACM Transactions on Graphics (TOG)},
  volume={39},
  number={6},
  pages={1--15},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@inproceedings{zhang2021flow,
  title={Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset},
  author={Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3661--3670},
  year={2021}
}
@article{wang2021audio2head,
  title={Audio2head: Audio-driven one-shot talking-head generation with natural head motion},
  author={Wang, Suzhen and Li, Lincheng and Ding, Yu and Fan, Changjie and Yu, Xin},
  journal={arXiv preprint arXiv:2107.09293},
  year={2021}
}
@inproceedings{zhou2021pose,
  title={Pose-controllable talking face generation by implicitly modularized audio-visual representation},
  author={Zhou, Hang and Sun, Yasheng and Wu, Wayne and Loy, Chen Change and Wang, Xiaogang and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4176--4186},
  year={2021}
}
@article{song2022everybody,
  title={Everybody’s talkin’: Let me talk as you want},
  author={Song, Linsen and Wu, Wayne and Qian, Chen and He, Ran and Loy, Chen Change},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={17},
  pages={585--598},
  year={2022},
  publisher={IEEE}
}
@INPROCEEDINGS{1467314,
  author={Chopra, S. and Hadsell, R. and LeCun, Y.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Learning a similarity metric discriminatively, with application to face verification}, 
  year={2005},
  volume={1},
  number={},
  pages={539-546 vol. 1},
  doi={10.1109/CVPR.2005.202}}


@article{DBLP:journals/corr/abs-2103-00484,
  author    = {Momina Masood and
               Marriam Nawaz and
               Khalid Mahmood Malik and
               Ali Javed and
               Aun Irtaza},
  title     = {Deepfakes Generation and Detection: State-of-the-art, open challenges,
               countermeasures, and way forward},
  journal   = {CoRR},
  volume    = {abs/2103.00484},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00484},
  eprinttype = {arXiv},
  eprint    = {2103.00484},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00484.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{goyal2022emotional,
  title={Emotional Talking Faces: Making Videos More Expressive and Realistic},
  author={Goyal, Sahil and Uppal, Shagun and Bhagat, Sarthak and Goel, Dhroov and Mali, Sakshat and Yu, Yi and Yin, Yifang and Shah, Rajiv Ratn},
  booktitle={Proceedings of the 4th ACM International Conference on Multimedia in Asia},
  pages={1--3},
  year={2022}
}

@article{UPPAL2022149,
title = {Multimodal research in vision and language: A review of current and emerging trends},
journal = {Information Fusion},
volume = {77},
pages = {149-171},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001512},
author = {Shagun Uppal and Sarthak Bhagat and Devamanyu Hazarika and Navonil Majumder and Soujanya Poria and Roger Zimmermann and Amir Zadeh},
}

@article{vougioukas2018end,
  title={End-to-end speech-driven facial animation with temporal gans},
  author={Vougioukas, Konstantinos and Petridis, Stavros and Pantic, Maja},
  journal={arXiv preprint arXiv:1805.09313},
  year={2018}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}


@article{10.1145/3487891,
author = {Aldausari, Nuha and Sowmya, Arcot and Marcus, Nadine and Mohammadi, Gelareh},
title = {Video Generative Adversarial Networks: A Review},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3487891},
doi = {10.1145/3487891},
abstract = {With the increasing interest in the content creation field in multiple sectors such as media, education, and entertainment, there is an increased trend in the papers that use AI algorithms to generate content such as images, videos, audio, and text. Generative Adversarial Networks (GANs) is one of the promising models that synthesizes data samples that are similar to real data samples. While the variations of GANs models in general have been covered to some extent in several survey papers, to the best of our knowledge, this is the first paper that reviews the state-of-the-art video GANs models. This paper first categorizes GANs review papers into general GANs review papers, image GANs review papers, and special field GANs review papers such as anomaly detection, medical imaging, or cybersecurity. The paper then summarizes the main improvements in GANs that are not necessarily applied in the video domain in the first run but have been adopted in multiple video GANs variations. Then, a comprehensive review of video GANs models are provided under two main divisions based on existence of a condition. The conditional models are then further classified according to the provided condition into audio, text, video, and image. The paper concludes with the main challenges and limitations of the current video GANs models.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {30},
numpages = {25},
keywords = {multimodal data, Generative Adversarial Networks, conditional generation, video synthesis}
}

@inproceedings{Bhagat2020DisentanglingMF,
  title={Disentangling Multiple Features in Video Sequences Using Gaussian Processes in Variational Autoencoders},
  author={Sarthak Bhagat and Shagun Uppal and Vivian Yin and Nengli Lim},
  booktitle={European Conference on Computer Vision},
  year={2020}
}

@article{Shukla2019PrOSePO,
  title={PrOSe: Product of Orthogonal Spheres Parameterization for Disentangled Representation Learning},
  author={Ankita Shukla and Sarthak Bhagat and Shagun Uppal and Saket Anand and Pavan K. Turaga},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.09554}
}

@inproceedings{Mathieu2018DisentanglingDI,
  title={Disentangling Disentanglement in Variational Autoencoders},
  author={Emile Mathieu and Tom Rainforth and N. Siddharth and Yee Whye Teh},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@article{Bengio2012RepresentationLA,
  title={Representation Learning: A Review and New Perspectives},
  author={Yoshua Bengio and Aaron C. Courville and Pascal Vincent},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2012},
  volume={35},
  pages={1798-1828}
}

@article{Bhagat2020DisContSV,
  title={DisCont: Self-Supervised Visual Attribute Disentanglement using Context Vectors},
  author={Sarthak Bhagat and Vishaal Udandarao and Shagun Uppal},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.05895}
}

@inproceedings{
higgins2017betavae,
title={beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
author={Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Sy2fzU9gl}
}