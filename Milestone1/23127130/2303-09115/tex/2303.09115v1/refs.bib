@inproceedings{hirschfeld1935connection,
  title={A connection between correlation and contingency},
  author={Hirschfeld, Hermann O},
  booktitle={Mathematical Proceedings of the Cambridge Philosophical Society},
  volume={31},
  number={4},
  pages={520--524},
  year={1935},
  organization={Cambridge University Press}
}

@misc{kale2019supervised,
      title={Supervised Contextual Embeddings for Transfer Learning in Natural Language Processing Tasks}, 
      author={Mihir Kale and Aditya Siddhant and Sreyashi Nag and Radhika Parik and Matthias Grabmair and Anthony Tomasic},
      year={2019},
      eprint={1906.12039},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{NEURIPS2019_6048ff4e,
 author = {Lee, Joshua and Sattigeri, Prasanna and Wornell, Gregory},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/6048ff4e8cb07aa60b6777b6f7384d52-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inproceedings{Long_2015,
author = {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael I.},
title = {Learning Transferable Features with Deep Adaptation Networks},
year = {2015},
publisher = {JMLR.org},
abstract = {Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multikernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {97–105},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}


@inproceedings{Gupta_2008,
author = {Gupta, Rakesh and Ratinov, Lev},
title = {Text Categorization with Knowledge Transfer from Heterogeneous Data Sources},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Multi-category classification of short dialogues is a common task performed by humans. When assigning a question to an expert, a customer service operator tries to classify the customer query into one of N different classes for which experts are available. Similarly, questions on the web (for example questions at Yahoo Answers) can be automatically forwarded to a restricted group of people with a specific expertise. Typical questions are short and assume background world knowledge for correct classification.With exponentially increasing amount of knowledge available, with distinct properties (labeled vs unlabeled, structured vs unstructured), no single knowledge-transfer algorithm such as transfer learning, multi-task learning or selftaught learning can be applied universally. In this work we show that bag-of-words classifiers performs poorly on noisy short conversational text snippets. We present an algorithm for leveraging heterogeneous data sources and algorithms with significant improvements over any single algorithm, rivaling human performance. Using different algorithms for each knowledge source we use mutual information to aggressively prune features. With heterogeneous data sources including Wikipedia, Open Directory Project (ODP), and Yahoo Answers, we show 89.4% and 96.8% correct classification on Google Answers corpus and Switchboard corpus using only 200 features/class. This reflects a huge improvement over bag of words approaches and 48-65% error reduction over previously published state of art (Gabrilovich et. al. 2006).},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2},
pages = {842–847},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI'08}
}




















@inproceedings{chen-etal-2019-multi,
    title = "Multi-Source Cross-Lingual Model Transfer: Learning What to Share",
    author = "Chen, Xilun  and
      Awadallah, Ahmed Hassan  and
      Hassan, Hany  and
      Wang, Wei  and
      Cardie, Claire",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1299",
    doi = "10.18653/v1/P19-1299",
    pages = "3098--3112"
    }

@article{Ni_2017,
  author    = {Jian Ni and
               Georgiana Dinu and
               Radu Florian},
  title     = {Weakly Supervised Cross-Lingual Named Entity Recognition via Effective
               Annotation and Representation Projection},
  journal   = {CoRR},
  volume    = {abs/1707.02483},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.02483},
  archivePrefix = {arXiv},
  eprint    = {1707.02483},
  timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NiDF17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Xingjian_2020,
  author    = {Xingjian Li and
               Haoyi Xiong and
               Haozhe An and
               Chengzhong Xu and
               Dejing Dou},
  title     = {XMixup: Efficient Transfer Learning with Auxiliary Samples by Cross-domain
               Mixup},
  journal   = {CoRR},
  volume    = {abs/2007.10252},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.10252},
  archivePrefix = {arXiv},
  eprint    = {2007.10252},
  timestamp = {Tue, 28 Jul 2020 14:46:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-10252.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article {Guo_Pasunuru_Bansal_2020,
	title = {
		Multi - Source Domain Adaptation for Text Classification via DistanceNet - Bandits
	},
	volume = {
		34
	},
	number = {
		05
	},
	journal = {
		Proceedings of the AAAI Conference on Artificial Intelligence
	},
	author = {
		Guo, Han and Pasunuru, Ramakanth and Bansal, Mohit
	},
	year = {
		2020
	},
	month = {
		Apr.
	},
	pages = {
		7830 - 7838
	}
}


@inproceedings{ijcai2017-311,
  author    = {Zheng Li and Yu Zhang and Ying Wei and Yuxiang Wu and Qiang Yang},
  title     = {End-to-End Adversarial Memory Network for Cross-domain Sentiment Classification},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {2237--2243},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/311},
  url       = {https://doi.org/10.24963/ijcai.2017/311},
}

@inproceedings{mayhew-etal-2017-cheap,
    title = "Cheap Translation for Cross-Lingual Named Entity Recognition",
    author = "Mayhew, Stephen  and
      Tsai, Chen-Tse  and
      Roth, Dan",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1269",
    doi = "10.18653/v1/D17-1269",
    pages = "2536--2545"
}



@inproceedings{keung-etal-2019-adversarial,
    title = "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and {NER}",
    author = "Keung, Phillip  and
      Lu, Yichao  and
      Bhardwaj, Vikas",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1138",
    doi = "10.18653/v1/D19-1138",
    pages = "1355--1360"
}






























@inproceedings{liu_multi_domain_2018,
    title = "Learning Domain Representation for Multi-Domain Sentiment Classification",
    author = "Liu, Qi  and
      Zhang, Yue  and
      Liu, Jiangming",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1050",
    pages = "541--550",
    abstract = "Training data for sentiment analysis are abundant in multiple domains, yet scarce for other domains. It is useful to leveraging data available for all existing domains to enhance performance on different domains. We investigate this problem by learning domain-specific representations of input sentences using neural network. In particular, a descriptor vector is learned for representing each domain, which is used to map adversarially trained domain-general Bi-LSTM input representations into domain-specific representations. Based on this model, we further expand the input representation with exemplary domain knowledge, collected by attending over a memory network of domain training data. Results show that our model outperforms existing methods on multi-domain sentiment analysis significantly, giving the best accuracies on two different benchmarks.",
}


@inproceedings{li_cross_domain_2017,
  author    = {Zheng Li and Yu Zhang and Ying Wei and Yuxiang Wu and Qiang Yang},
  title     = {End-to-End Adversarial Memory Network for Cross-domain Sentiment Classification},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {2237--2243},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/311},
  url       = {https://doi.org/10.24963/ijcai.2017/311},
}






@inproceedings{glorot_domain_adaptation_2011,
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Y.},
year = {2011},
month = {06},
pages = {},
title = {Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach},
booktitle = {Proceedings of the 28th International Conference on Machine Learning, ICML 2011}
}


@inproceedings{pan_cross_domain_2010,
author = {Pan, Sinno and Ni, Xiaochuan and Sun, Jian-Tao and Yang, Qiang and Chen, Zheng},
year = {2010},
month = {01},
pages = {751-760},
title = {Cross-Domain Sentiment Classification via Spectral Feature Alignment},
booktitle = {Proceedings of the 19th International Conference on World Wide Web, WWW '10},
doi = {10.1145/1772690.1772767}
}

@INPROCEEDINGS{finetune_bert_vietnamese,
  author={Q. T. {Nguyen} and T. L. {Nguyen} and N. H. {Luong} and Q. H. {Ngo}},
  booktitle={2020 7th NAFOSTED Conference on Information and Computer Science (NICS)}, 
  title={Fine-Tuning BERT for Sentiment Analysis of Vietnamese Reviews}, 
  year={2020},
  volume={},
  number={},
  pages={302-307},
  doi={10.1109/NICS51282.2020.9335899}}
  
  

@inproceedings{meta_embeddings,
    title = "Learning Word Meta-Embeddings",
    author = {Yin, Wenpeng  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1128",
    pages = "1351--1360",
}


@inproceedings{FrenchBERT,
    title = "{C}amem{BERT}: a Tasty {F}rench Language Model",
    author = "Martin, Louis  and
      Muller, Benjamin  and
      Ortiz Suarez, Pedro Javier  and
      Dupont, Yoann  and
      Romary, Laurent  and
      de la Clergerie, Eric  and
      Seddah, Djame  and
      Sagot, Benoit",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.645",
    pages = "7203--7219",
    abstract = "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models {--}in all languages except English{--} very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",
}




@misc{DutchBERT,
      title={BERTje: A Dutch BERT Model}, 
      author={Wietse de Vries and Andreas van Cranenburgh and Arianna Bisazza and Tommaso Caselli and Gertjan van Noord and Malvina Nissim},
      year={2019},
      eprint={1912.09582},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{ChineseBERT,
      title={Pre-Training with Whole Word Masking for Chinese BERT}, 
      author={Yiming Cui and Wanxiang Che and Ting Liu and Bing Qin and Ziqing Yang and Shijin Wang and Guoping Hu},
      year={2019},
      eprint={1906.08101},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{AraBERT,
    title = "{A}ra{BERT}: Transformer-based Model for {A}rabic Language Understanding",
    author = "Antoun, Wissam  and
      Baly, Fady  and
      Hajj, Hazem",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resource Association",
    url = "https://www.aclweb.org/anthology/2020.osact-1.2",
    pages = "9--15",
    abstract = "The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.",
    language = "English",
    ISBN = "979-10-95546-51-1",
}




@inproceedings{UniLM,
 author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
 url = {https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf},
 volume = {32},
 year = {2019}
}






@inproceedings{XLNet,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
 volume = {32},
 year = {2019}
}





@InProceedings{gatedCNN,
author="Madasu, Avinash
and Rao, Vijjini Anvesh",
editor="M{\'e}tais, Elisabeth
and Meziane, Farid
and Vadera, Sunil
and Sugumaran, Vijayan
and Saraee, Mohamad",
title="Gated Convolutional Neural Networks for Domain Adaptation",
booktitle="Natural Language Processing and Information Systems",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="118--130",
abstract="Domain Adaptation explores the idea of how to maximize performance on a target domain, distinct from source domain, upon which the model was trained. This idea has been explored for the task of sentiment analysis extensively. The training of reviews pertaining to one domain and evaluation on another domain is widely studied for modeling a domain independent algorithm. This further helps in understanding corelation of information between domains. In this paper, we show that Gated Convolutional Neural Networks (GCN) perform effectively at learning sentiment analysis in a manner where domain dependant knowledge is filtered out using its gates. We perform our experiments on multiple gate architectures: Gated Tanh ReLU Unit (GTRU), Gated Tanh Unit (GTU) and Gated Linear Unit (GLU). Extensive experimentation on two standard datasets relevant to the task, reveal that training with Gated Convolutional Neural Networks give significantly better performance on target domains than regular convolution and recurrent based architectures. While complex architectures like attention, filter domain specific knowledge as well, their complexity order is remarkably high as compared to gated architectures. GCNs rely on convolution hence gaining an upper hand through parallelization.",
isbn="978-3-030-23281-8"
}




@inproceedings{ViWordSegmentor,
author={Dat Quoc, Nguyen and Dai Quoc, Nguyen and Thanh, Vu and Mark, Dras and Mark, Johnson},
title={{A Fast and Accurate Vietnamese Word Segmenter}},
booktitle={Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC 2018)},
pages={2582--2587},
year={2018}
}


@inproceedings{MGNC,
    title = "{MGNC}-{CNN}: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification",
    author = "Zhang, Ye  and
      Roller, Stephen  and
      Wallace, Byron C.",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N16-1178",
    pages = "1522--1527",
}


@inproceedings{XLM,
 author = {Conneau, Alexis and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Cross-lingual Language Model Pretraining},
 url = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
 volume = {32},
 year = {2019}
}





@article{AmazonReviewDataset,
   title={Ups and Downs},
   ISBN={9781450341431},
   url={http://dx.doi.org/10.1145/2872427.2883037},
   DOI={10.1145/2872427.2883037},
   journal={Proceedings of the 25th International Conference on World Wide Web - WWW  ’16},
   publisher={ACM Press},
   author={He, Ruining and McAuley, Julian},
   year={2016}
}

@inproceedings{MultiDomainDataset,
    title = "Biographies, {B}ollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",
    author = "Blitzer, John  and
      Dredze, Mark  and
      Pereira, Fernando",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P07-1056",
    pages = "440--447",
}

@inproceedings{Attention,
  title={Effective Approaches to Attention-based Neural Machine Translation},
  author={Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
  year={2015},
  url       =  {https://arxiv.org/abs/1508.04025},
  archivePrefix = {arXiv},
  eprint    = {1508.04025},
}

@inproceedings{GRU,
  title={Learning phrase representations using rnn encoder-decoder for statistical machine translation},
  author={Kyunghyun Cho and Bart Van Merrienboer and Caglar Gul- cehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  year={2014},
  url       =  {https://arxiv.org/abs/1508.04025},
  archivePrefix = {arXiv},
  eprint    = {1406.1078},
}

@article{LSTM,
author = {Hochreiter S. and Schmidhuber J.},
year = {1997},
month = {11},
pages = {1735–1780},
title = {Long short-term memory},
volume = {9},
journal = {Neural Computing},
doi = {https://doi.org/10.1162/neco.1997.9.8.1735}
}


@Article{app9245462,
AUTHOR = {Chakriswaran, Priya and Vincent, Durai Raj and Srinivasan, Kathiravan and Sharma, Vishal and Chang, Chuan-Yu and Reina, Daniel Gutiérrez},
TITLE = {Emotion AI-Driven Sentiment Analysis: A Survey, Future Research Directions, and Open Issues},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5462},
ISSN = {2076-3417},
DOI = {10.3390/app9245462}
}

@article{WANG201477,
title = "Sentiment classification: The contribution of ensemble learning",
journal = "Decision Support Systems",
volume = "57",
pages = "77 - 93",
year = "2014",
issn = "0167-9236",
url = "http://www.sciencedirect.com/science/article/pii/S0167923613001978",
author = "Gang Wang and Jianshan Sun and Jian Ma and Kaiquan Xu and Jibao Gu",
keywords = "Sentiment classification, Ensemble learning, Bagging, Boosting, Random Subspace",
abstract = "With the rapid development of information technologies, user-generated contents can be conveniently posted online. While individuals, businesses, and governments are interested in evaluating the sentiments behind this content, there are no consistent conclusions on which sentiment classification technologies are best. Recent studies suggest that ensemble learning methods may have potential applicability in sentiment classification. In this study, we conduct a comparative assessment of the performance of three popular ensemble methods (Bagging, Boosting, and Random Subspace) based on five base learners (Naive Bayes, Maximum Entropy, Decision Tree, K Nearest Neighbor, and Support Vector Machine) for sentiment classification. Moreover, ten public sentiment analysis datasets were investigated to verify the effectiveness of ensemble learning for sentiment analysis. Based on a total of 1200 comparative group experiments, empirical results reveal that ensemble methods substantially improve the performance of individual base learners for sentiment classification. Among the three ensemble methods, Random Subspace has the better comparative results, although it was seldom discussed in the literature. These results illustrate that ensemble learning methods can be used as a viable method for sentiment classification."
}

@InProceedings{10.1007/978-3-319-77712-2_12,
author="Contratres, Felipe G.
and Alves-Souza, Solange N.
and Filgueiras, Lucia Vilela Leite
and DeSouza, Luiz S.",
editor="Rocha, {\'A}lvaro
and Adeli, Hojjat
and Reis, Lu{\'i}s Paulo
and Costanzo, Sandra",
title="Sentiment Analysis of Social Network Data for Cold-Start Relief in Recommender Systems",
booktitle="Trends and Advances in Information Systems and Technologies",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="122--132",
abstract="Recommender systems have been used in e-commerce to increase conversion due to matching product offer and consumer preferences. Cold-start is the situation of a new user about whom there is no information to make suitable recommendations. Texts published by the user in social networks are a good source of information to reduce the cold-start issue. However, the valence of the emotion in a text must be considered in the recommendation so that no product is recommended based on a negative opinion. This paper proposes a recommendation process that includes sentiment analysis to textual data extracted from Facebook and Twitter and present results of an experiment in which this algorithm is used to reduce the cold-start issue.",
isbn="978-3-319-77712-2"
}

@inproceedings{Word2Vec,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {Distributed Representations of Words and Phrases and Their Compositionality},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3111–3119},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS’13}
}


@article{ZHANG20151857,
title = "Chinese comments sentiment classification based on word2vec and SVMperf",
journal = "Expert Systems with Applications",
volume = "42",
number = "4",
pages = "1857 - 1863",
year = "2015",
issn = "0957-4174",
url = "http://www.sciencedirect.com/science/article/pii/S0957417414005508",
author = "Dongwen Zhang and Hua Xu and Zengcai Su and Yunfeng Xu",
keywords = "Sentiment classification, Word2vec, SVM, Semantic features",
abstract = "Since the booming development of e-commerce in the last decade, the researchers have begun to pay more attention to extract the valuable information from consumers comments. Sentiment classification, which focuses on classify the comments into positive class and negative class according to the polarity of sentiment, is one of the studies. Machine learning-based method for sentiment classification becomes mainstream due to its outstanding performance. Most of the existing researches are centered on the extraction of lexical features and syntactic features, while the semantic relationships between words are ignored. In this paper, in order to get the semantic features, we propose a method for sentiment classification based on word2vec and SVMperf. Our research consists of two parts of work. First of all, we use word2vec to cluster the similar features for purpose of showing the capability of word2vec to capture the semantic features in selected domain and Chinese language. And then, we train and classify the comment texts using word2vec again and SVMperf. In the process, the lexicon-based and part-of-speech-based feature selection methods are respectively adopted to generate the training file. We conduct the experiments on the data set of Chinese comments on clothing products. The experimental results show the superior performance of our method in sentiment classification."
}


@article{MEDHAT20141093,
title = "Sentiment analysis algorithms and applications: A survey",
journal = "Ain Shams Engineering Journal",
volume = "5",
number = "4",
pages = "1093 - 1113",
year = "2014",
issn = "2090-4479",
url = "http://www.sciencedirect.com/science/article/pii/S2090447914000550",
author = "Walaa Medhat and Ahmed Hassan and Hoda Korashy",
keywords = "Sentiment analysis, Sentiment classification, Feature selection, Emotion detection, Transfer learning, Building resources",
abstract = "Sentiment Analysis (SA) is an ongoing field of research in text mining field. SA is the computational treatment of opinions, sentiments and subjectivity of text. This survey paper tackles a comprehensive overview of the last update in this field. Many recently proposed algorithms' enhancements and various SA applications are investigated and presented briefly in this survey. These articles are categorized according to their contributions in the various SA techniques. The related fields to SA (transfer learning, emotion detection, and building resources) that attracted researchers recently are discussed. The main target of this survey is to give nearly full image of SA techniques and the related fields with brief details. The main contributions of this paper include the sophisticated categorizations of a large number of recent articles and the illustration of the recent trend of research in the sentiment analysis and its related areas."
}


@article{RCNN,
author = {Siwei Lai and Liheng Xu andKang Liu andJun Zhao},
year = {2015},
month = {01},
pages = {2267–2273},
title = {Recurrent convolutional neural networks for text classification},
volume = {},
journal = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
doi = {}
}


@ARTICLE{MoE,
  author={R. A. {Jacobs} and M. I. {Jordan} and S. J. {Nowlan} and G. E. {Hinton}},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  abstract={We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.},
  keywords={},
  doi={10.1162/neco.1991.3.1.79},
  ISSN={0899-7667},
  month={March},}
  
  
@article{DPCNN,
author = {Rie Johnson and Tong Zhang},
year = {2017},
month = {07},
pages = {562–570},
title = {Deep Pyramid Convolutional Neural Networks for Text Categorization},
volume = {1},
journal = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/P17-1052}
}

@inproceedings{SelfAttention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{BERT,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}




@inproceedings{PhoBERT,
  title={PhoBERT: Pre-trained language models for Vietnamese},
  author={Dat Quoc Nguyen and Anh Tuan Nguyen},
  year={2020},
  url       =  {https://arxiv.org/abs/2003.00744},
  archivePrefix = {arXiv},
  eprint    = {2003.00744},
}

@inproceedings{GPT2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  url = {https://openai.com/blog/better-language-models/},
  archivePrefix = {},
  eprint    = {},
}

@inproceedings{TextCNN,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}







@inproceedings{VDCNN,
    title = "Very Deep Convolutional Networks for Text Classification",
    author = {Conneau, Alexis  and
      Schwenk, Holger  and
      Barrault, Lo{\"\i}c  and
      Lecun, Yann},
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-1104",
    pages = "1107--1116",
    abstract = "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.",
}






@article{Hung_2016,
author = {Vo, Hung and Lam, Hai and Nguyen, Duc Dung and Tuong, Nguyen},
year = {2016},
month = {05},
pages = {27-34},
title = {Topic classification and sentiment analysis for Vietnamese education survey system},
volume = {6},
journal = {Asian Journal of Computer Science and Information Technology},
doi = {10.15520/ajcsit.v6i3.44.g31}
}

@inbook{Son_2016,
author = {Trinh, Son and Nguyen, Luu and Vo, Minh and Do, Phuc},
year = {2016},
month = {02},
pages = {263-276},
title = {Lexicon-Based Sentiment Analysis of Facebook Comments in Vietnamese Language},
volume = {642},
isbn = {978-3-319-31276-7},
doi = {10.1007/978-3-319-31277-4_23}
}

@inproceedings{Hoang_2017,
author = {Vo, Quan-Hoang and Nguyen, Huy-Tien and Le, Bac and Nguyen, Minh-Le},
year = {2017},
month = {10},
pages = {24-29},
title = {Multi-channel LSTM-CNN model for Vietnamese sentiment analysis},
doi = {10.1109/KSE.2017.8119429}
}

@inproceedings{Phu_2018,
author = {Nguyen, Phu and Hong, Tham and Nguyen, Kiet and Nguyen, Ngan},
year = {2018},
month = {11},
pages = {75-80},
title = {Deep Learning versus Traditional Classifiers on Vietnamese Students’ Feedback Corpus},
doi = {10.1109/NICS.2018.8606837}
}

@inproceedings{Vu_2018,
author = {Nguyen, Vu and Nguyen, Kiet and Nguyen, Ngan},
year = {2018},
month = {11},
pages = {306-311},
title = {Variants of Long Short-Term Memory for Sentiment Analysis on Vietnamese Students’ Feedback Corpus},
doi = {10.1109/KSE.2018.8573351}
}


@inbook{Tuyen_2019,
author = {Nguyen-Thi, Bich-Tuyen and Duong, Huu-Thanh},
year = {2019},
month = {08},
pages = {240-249},
title = {A Vietnamese Sentiment Analysis System Based on Multiple Classifiers with Enhancing Lexicon Features},
isbn = {978-3-030-30148-4},
doi = {10.1007/978-3-030-30149-1_20}
}

@inproceedings{Thuy_2019,
author = {Nguyen-Thanh, Thuy and Tran, Giang},
year = {2019},
month = {12},
pages = {},
title = {Vietnamese Sentiment Analysis for Hotel Review based on Overfitting Training and Ensemble Learning},
isbn = {978-1-4503-7245-9},
doi = {10.1145/3368926.3369675}
}


@inbook{Long_2018,
author = {Mai, Long and Le, Bac},
year = {2018},
month = {01},
pages = {149-158},
title = {Aspect-Based Sentiment Analysis of Vietnamese Texts with Deep Learning},
isbn = {978-3-319-75416-1},
doi = {10.1007/978-3-319-75417-8_14}
}


@inproceedings{Quan_2018,
author = {Nguyen, Hoang-Quan and Nguyen, Quang-Uy},
year = {2018},
month = {11},
pages = {165-170},
title = {An Ensemble of Shallow and Deep Learning Algorithms for Vietnamese Sentiment Analysis},
doi = {10.1109/NICS.2018.8606880}
}


@inproceedings{10.1145/2676585.2676606,
author = {Phan, Dang-Hung and Cao, Tuan-Dung},
title = {Applying Skip-Gram Word Estimation and SVM-Based Classification for Opinion Mining Vietnamese Food Places Text Reviews},
year = {2014},
isbn = {9781450329309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676585.2676606},
doi = {10.1145/2676585.2676606},
booktitle = {Proceedings of the Fifth Symposium on Information and Communication Technology},
pages = {232–239},
numpages = {8},
keywords = {maximum entropy, support vector machine, part of speech, continuous Skip-gram, opinion mining},
location = {Hanoi, Viet Nam},
series = {SoICT ’14}
}


@article{HARNN,
author = {Zichao Yang and Diyi Yang and Chris Dyer and Xiaodong He and Alex Smola and Eduard Hovy},
year = {2016},
month = {06},
pages = {1480–1489},
title = {Hierarchical Attention Networks for Document Classification},
volume = {},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
doi = {10.18653/v1/N16-1174}
}

@inproceedings{UDA,
  title={Unsupervised Data Augmentation for Consistency Training},
  author={Qizhe Xie and  Zihang Dai and Eduard Hovy and Minh-Thang Luong and Quoc V. Le},
  year={2019},
  url       =  {https://arxiv.org/abs/1904.12848},
  archivePrefix = {arXiv},
  eprint    = {1904.12848},
}

@article{LCF,
author = {Biqing Zeng and Heng Yang and Ruyang Xu and Wu Zhou and Xuli Han},
year = {2019},
month = {August},
pages = {},
title = {LCF: A Local Context Focus Mechanism for Aspect-Based Sentiment Classification},
volume = {9(16)},
journal = {Applied Sciences},
doi = {https://doi.org/10.3390/app9163389}
}

@article{MGAN,
author = {Feifan Fan and Yansong Feng and Dongyan Zhao},
year = {2018},
month = {11},
pages = {3433–3442},
title = {Multi-grained Attention Network for Aspect-Level Sentiment Classification},
volume = {},
journal = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D18-1380}
}

@article{DRNN,
author = {Baoxin Wang},
year = {2018},
month = {7},
pages = {2311–2320},
title = {Disconnected Recurrent Neural Networks for Text Categorization},
volume = {},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
doi = {}
}

@article{Glove,
author = {Effrey Pennington and Richard Socher and Christopher Manning},
year = {2014},
month = {10},
pages = {1532–1543},
title = {Glove: Global Vectors for Word Representation},
volume = {},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)s},
doi = {10.3115/v1/D14-1162}
}

@inproceedings{DeepCharCNN,
  added-at = {2019-02-06T12:02:26.000+0100},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle = {Advances in neural information processing systems},
  keywords = {},
  pages = {649--657},
  timestamp = {2019-02-06T12:02:26.000+0100},
  title = {Character-level convolutional networks for text classification},
  year = 2015
}


@inproceedings{Joachims_2005,
author = {Joachims, Thorsten},
title = {A Support Vector Method for Multivariate Performance Measures},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102351.1102399},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {377–384},
numpages = {8},
location = {Bonn, Germany},
series = {ICML ’05}
}


@ARTICLE{8244338, author={Z. {Jianqiang} and G. {Xiaolin} and Z. {Xuejun}}, journal={IEEE Access}, title={Deep Convolution Neural Networks for Twitter Sentiment Analysis}, year={2018}, volume={6}, number={}, pages={23253-23260},}

@inproceedings{ShallowWordCNN,
  title={Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level
},
  author={Rie Johnson, Tong Zhang},
  year={2016},
  url       =  {https://arxiv.org/abs/1609.00718},
  archivePrefix = {arXiv},
  eprint    = {1609.00718},
}

@article{RNN,
author = {Ethan Wilcox and Roger Levy andTakashi Morita and Richard Futrell},
year = {2018},
month = {11},
pages = {211–221},
title = {What do RNN Language Models Learn about Filler–Gap Dependencies?},
volume = {},
journal = {Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
doi = {10.18653/v1/W18-5423}
}

@article{LSTMforSentiment,
author = {Duyu Tang and Bing Qin and Ting Liu},
year = {2015},
month = {9},
pages = {1422–1432},
title = {Document Modeling with Gated Recurrent Neural Network for Sentiment Classification},
volume = {},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D15-1167}
}

@article{CachedLSTM,
author = {Jiacheng Xu and Danlu Chen and Xipeng Qiu and Xuanjing Huang},
year = {2016},
month = {},
pages = {1660–1669},
title = {Cached long short-term memory neural networks for document-level sentiment classifi- cation},
volume = {},
journal = {In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
doi = {}
}

@article{RecurrentResidual,
author = {Yiren Wang and Fei Tian},
year = {2016},
month = {},
pages = {938–943},
title = {Recurrent residual learning for sequence classification},
volume = {},
journal = {In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
doi = {}
}

@article{hULMonA,
author = {Obeida ElJundi and Wissam Antoun and Nour El Droubi and Hazem Hajj and Wassim El-Hajj and Khaled Shaban},
year = {2019},
month = {8},
pages = {68–77},
title = {hULMonA: The Universal Language Model in Arabic},
volume = {},
journal = {Proceedings of the Fourth Arabic Natural Language Processing Workshop},
doi = {}
}

@article{ViFeedbackSentiment,
author = {Phu X. V. Nguyen and Tham T. T. Hong and Kiet Van Nguyen and Ngan Luu-Thuy Nguyen},
year = {2018},
month = {},
pages = {},
title = {Deep Learning versus Traditional Classifiers on Vietnamese Students’ Feedback Corpus},
volume = {},
journal = {2018 5th NAFOSTED Conference on Information and Computer Science (NICS)},
doi = {}
}

@article{ViAspectSentiment,
author = {Long Mai and Bac Le},
year = {2018},
month = {},
pages = {149–158},
title = {Aspect-Based Sentiment Analysis of Vietnamese Texts with Deep Learning},
volume = {},
journal = {Intelligent Information and Database Systems. ACIIDS 2018},
doi = {https://doi.org/10.1007/978-3-319-75417-8_14}
}








@misc{ChessVision2016,
  title={ChessVision: Chess Board and Piece Recognition},
  author={Jialin Ding},
  year={2016}
  
}

@article{SIFT2004,
author = {Lowe, David},
year = {2004},
month = {11},
pages = {91-},
title = {Distinctive Image Features from Scale-Invariant Keypoints},
volume = {60},
journal = {International Journal of Computer Vision},
doi = {10.1023/B:VISI.0000029664.99615.94}
}

@article{HOG2005,
author = {Dalal, Navneet and Triggs, Bill},
year = {2005},
month = {06},
pages = {},
title = {Histograms of Oriented Gradients for Human Detection},
volume = {2},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2005)}
}


@inproceedings{Czyzewski2017ChessboardAC,
  title={Chessboard and chess piece recognition with the support of neural networks.},
  author={Maciej A. Czyzewski and Artur Laskowski and Szymon Wasik},
  year={2017},
  url       =  {http://arxiv.org/abs/1708.03898},
  archivePrefix = {arXiv},
  eprint    = {1708.03898},
}

@article{VnCoreNLP,
author = {Thanh Vu and Dat Quoc Nguyen and Dai Quoc Nguyen and Mark Dras and Mark Johnson},
year = {2018},
month = {04},
pages = {},
title = {VnCoreNLP: A Vietnamese Natural Language Processing Toolkit}
}


@article{AttentiveNN,
author = {Kim Anh Nguyen and Ngan Dong and Cam-Tu Nguyen},
year = {2019},
month = {06},
pages = {},
title = {Attentive Neural Network for Named Entity
Recognition in Vietnamese}
}

@article{Transformer,
author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser},
year = {2017},
month = {},
pages = {},
title = {Attention Is All You Need}
}

@article{zhang2018deep,
  title={Deep learning for sentiment analysis: A survey},
  author={Zhang, Lei and Wang, Shuai and Liu, Bing},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={8},
  number={4},
  pages={e1253},
  year={2018},
  publisher={Wiley Online Library}
}

@inproceedings{lee2019learning,
  title={Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks},
  author={Lee, Joshua and Sattigeri, Prasanna and Wornell, Gregory},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4372--4382},
  year={2019}
}

@inproceedings{chen-etal-2019-multi-source,
    title = "Multi-Source Cross-Lingual Model Transfer: Learning What to Share",
    author = "Chen, Xilun  and
      Awadallah, Ahmed Hassan  and
      Hassan, Hany  and
      Wang, Wei  and
      Cardie, Claire",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1299",
    doi = "10.18653/v1/P19-1299",
    pages = "3098--3112",
    abstract = "Modern NLP applications have enjoyed a great boost utilizing neural networks models. Such deep neural models, however, are not applicable to most human languages due to the lack of annotated training data for various NLP tasks. Cross-lingual transfer learning (CLTL) is a viable method for building NLP models for a low-resource target language by leveraging labeled data from other (source) languages. In this work, we focus on the multilingual transfer setting where training data in multiple source languages is leveraged to further boost target language performance. Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at instance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. Moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource setting where neither target language training data nor cross-lingual resources are available. Our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale industry dataset.",
}

@article{joulin2016fasttext,
  title={FastText.zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}

@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}