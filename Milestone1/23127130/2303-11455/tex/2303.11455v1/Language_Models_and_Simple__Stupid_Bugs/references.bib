@inproceedings{vaithilingam2022expectation,
  title={Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models},
  author={Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L},
  booktitle={CHI Conference on Human Factors in Computing Systems Extended Abstracts},
  pages={1--7},
  year={2022}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{sobania2022choose,
  title={Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming},
  author={Sobania, Dominik and Briesch, Martin and Rothlauf, Franz},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={1019--1027},
  year={2022}
}


@inproceedings{fewshot2022ahmed,
author = {Ahmed, Toufique and Devanbu, Premkumar},
title = {Few-Shot Training LLMs for Project-Specific Code-Summarization},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559555},
doi = {10.1145/3551349.3559555},
abstract = {Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.},
booktitle = {37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {177},
numpages = {5},
keywords = {deep learning, code summarization, large language model},
location = {Rochester, MI, USA},
series = {ASE22}
}



@article{prenner2021automatic,
  title={Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs},
  author={Prenner, Julian Aron and Robbes, Romain},
  journal={arXiv preprint arXiv:2111.03922},
  year={2021}
}

 @misc{ziegler, title={https://github.blog/2021-06-30-github-copilot-research-recitation/}, url={https://github.blog/2021-06-30-github-copilot-research-recitation/}, journal={Github Copilot research recitation}, publisher={GitHub}, author={Ziegler, Albert}} 

 @article{sarkar2022like,
  title={What is it like to program with artificial intelligence?},
  author={Sarkar, Advait and Gordon, Andrew D and Negreanu, Carina and Poelitz, Christian and Ragavan, Sruti Srinivasa and Zorn, Ben},
  journal={arXiv preprint arXiv:2208.06213},
  year={2022}
}

@inproceedings{wang2022no,
  title={No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence},
  author={Wang, Chaozheng and Yang, Yuanhang and Gao, Cuiyun and Peng, Yun and Zhang, Hongyu and Lyu, Michael R},
  booktitle={Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={382--394},
  year={2022}
}

@article{rajkumar2022evaluating,
  title={Evaluating the Text-to-SQL Capabilities of Large Language Models},
  author={Rajkumar, Nitarshan and Li, Raymond and Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:2204.00498},
  year={2022}
}

@article{trummer2022codexdb,
  title={CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex},
  author={Trummer, Immanuel},
  journal={arXiv preprint arXiv:2204.08941},
  year={2022}
}

@inproceedings{yetistiren2022assessing,
  title={Assessing the quality of GitHub copilot’s code generation},
  author={Yetistiren, Burak and Ozsoy, Isik and Tuzun, Eray},
  booktitle={Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
  pages={62--71},
  year={2022}
}

@inproceedings{mosolygo2021rise,
  title={On the rise and fall of simple stupid bugs: a life-cycle analysis of sstubs},
  author={Mosolyg{\'o}, Bal{\'a}zs and V{\'a}ndor, Norbert and Antal, G{\'a}bor and Heged{\H{u}}s, P{\'e}ter},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={495--499},
  year={2021},
  organization={IEEE}
}

@article{elnaggar2021codetrans,
  title={CodeTrans: Towards Cracking the Language of Silicon's Code Through Self-Supervised Deep Learning and High Performance Computing},
  author={Elnaggar, Ahmed and Ding, Wei and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Severini, Silvia and Matthes, Florian and Rost, Burkhard},
  journal={arXiv preprint arXiv:2104.02443},
  year={2021}
}

@inproceedings{mashhadi2021applying,
  title={Applying codebert for automated program repair of java simple bugs},
  author={Mashhadi, Ehsan and Hemmati, Hadi},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={505--509},
  year={2021},
  organization={IEEE}
}

@inproceedings{peruma2021distribution,
  title={On the Distribution of" Simple Stupid Bugs" in Unit Test Files: An Exploratory Study},
  author={Peruma, Anthony and Newman, Christian D},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={525--529},
  year={2021},
  organization={IEEE}
}

@inproceedings{zhu2021mea,
  title={Mea culpa: How developers fix their own simple bugs differently from other developers},
  author={Zhu, Wenhan and Godfrey, Michael W},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={515--519},
  year={2021},
  organization={IEEE}
}

@inproceedings{hua2021effectiveness,
  title={On the effectiveness of deep vulnerability detectors to simple stupid bug detection},
  author={Hua, Jiayi and Wang, Haoyu},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={530--534},
  year={2021},
  organization={IEEE}
}

@inproceedings{kamienski2021pysstubs,
  title={Pysstubs: Characterizing single-statement bugs in popular open-source python projects},
  author={Kamienski, Arthur V and Palechor, Luisa and Bezemer, Cor-Paul and Hindle, Abram},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={520--524},
  year={2021},
  organization={IEEE}
}


@article{pearce2021can,
  title={Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?},
  author={Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan},
  journal={arXiv preprint arXiv:2112.02125},
  year={2021}
}

@inproceedings{nguyen2022empirical,
  title={An empirical evaluation of GitHub copilot's code suggestions},
  author={Nguyen, Nhan and Nadi, Sarah},
  booktitle={Proceedings of the 19th International Conference on Mining Software Repositories},
  pages={1--5},
  year={2022}
}

@inproceedings{karampatsis2020often,
  title={How often do single-statement bugs occur? the manysstubs4j dataset},
  author={Karampatsis, Rafael-Michael and Sutton, Charles},
  booktitle={Proceedings of the 17th International Conference on Mining Software Repositories},
  pages={573--577},
  year={2020}
}

@inproceedings{madeiral2021large,
  title={A large-scale study on human-cloned changes for automated program repair},
  author={Madeiral, Fernanda and Durieux, Thomas},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={510--514},
  year={2021},
  organization={IEEE}
}

@article{asare2022github,
  title={Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?},
  author={Asare, Owura and Nagappan, Meiyappan and Asokan, N},
  journal={arXiv preprint arXiv:2204.04741},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{richter2022tssb,
  title={TSSB-3M: Mining single statement bugs at massive scale},
  author={Richter, Cedric and Wehrheim, Heike},
  journal={arXiv preprint arXiv:2201.12046},
  year={2022}
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{svyatkovskiy2020intellicode,
  title={Intellicode compose: Code generation using transformer},
  author={Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1433--1443},
  year={2020}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@inproceedings{pearce2022asleep,
  title={Asleep at the keyboard? assessing the security of github copilot’s code contributions},
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={754--768},
  year={2022},
  organization={IEEE}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{allamanis2019adverse,
  title={The adverse effects of code duplication in machine learning models of code},
  author={Allamanis, Miltiadis},
  booktitle={Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
  pages={143--153},
  year={2019}
}

@article{fried2022incoder,
  title={Incoder: A generative model for code infilling and synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2204.05999},
  year={2022}
}

@article{perry2022users,
  title={Do Users Write More Insecure Code with AI Assistants?},
  author={Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
  journal={arXiv preprint arXiv:2211.03622},
  year={2022}
}

@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  pages={1--10},
  year={2022}
}

@book{tunstall2022natural,
  title={Natural language processing with transformers},
  author={Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},
  year={2022},
  publisher={" O'Reilly Media, Inc."}
}

@article{nijkamp2022conversational,
  title={A conversational paradigm for program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{sandoval2022security,
  title={Security Implications of Large Language Model Code Assistants: A User Study},
  author={Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:2208.09727},
  year={2022}
}

@inproceedings{pearce2022examining,
  title={Examining Zero-Shot Vulnerability Repair with Large Language Models},
  author={Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={1--18},
  year={2022},
  organization={IEEE Computer Society}
}

@article{joshi2022repair,
  title={Repair is nearly generation: Multilingual program repair with llms},
  author={Joshi, Harshit and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Radicek, Ivan and Verbruggen, Gust},
  journal={arXiv preprint arXiv:2208.11640},
  year={2022}
}

@article{karmakar2022codex,
  title={Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation},
  author={Karmakar, Anjan and Prenner, Julian Aron and D'Ambros, Marco and Robbes, Romain},
  journal={arXiv preprint arXiv:2212.02684},
  year={2022}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{tang2021solving,
  title={Solving probability and statistics problems by program synthesis},
  author={Tang, Leonard and Ke, Elizabeth and Singh, Nikhil and Verma, Nakul and Drori, Iddo},
  journal={arXiv preprint arXiv:2111.08267},
  year={2021}
}

@article{drori2022neural,
  title={A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level},
  author={Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={32},
  pages={e2123433119},
  year={2022},
  publisher={National Acad Sciences}
}


@article{hindle2016naturalness,
  title={On the naturalness of software},
  author={Hindle, Abram and Barr, Earl T and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
  journal={Communications of the ACM},
  volume={59},
  number={5},
  pages={122--131},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{latendresse2021effective,
  title={How Effective is Continuous Integration in Indicating Single-Statement Bugs?},
  author={Latendresse, Jasmine and Abdalkareem, Rabe and Costa, Diego Elias and Shihab, Emad},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={500--504},
  year={2021},
  organization={IEEE}
}



%for related work





@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}



@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}

@article{guo2020graphcodebert,
  title={Graphcodebert: Pre-training code representations with data flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  journal={arXiv preprint arXiv:2009.08366},
  year={2020}
}

@article{ahmad2021unified,
  title={Unified pre-training for program understanding and generation},
  author={Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2103.06333},
  year={2021}
}

@inproceedings{kanade2020learning,
  title={Learning and evaluating contextual embedding of source code},
  author={Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
  booktitle={International Conference on Machine Learning},
  pages={5110--5121},
  year={2020},
  organization={PMLR}
}

@inproceedings{karmakar2021pre,
  title={What do pre-trained code models know about code?},
  author={Karmakar, Anjan and Robbes, Romain},
  booktitle={2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={1332--1336},
  year={2021},
  organization={IEEE}
}

@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}


%prompting
@article{gu2021ppt,
  title={Ppt: Pre-trained prompt tuning for few-shot learning},
  author={Gu, Yuxian and Han, Xu and Liu, Zhiyuan and Huang, Minlie},
  journal={arXiv preprint arXiv:2109.04332},
  year={2021}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{liu2021gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}


@article{liu2021pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={arXiv preprint arXiv:2107.13586},
  year={2021}
}

@article{le2022coderl,
  title={Coderl: Mastering code generation through pretrained models and deep reinforcement learning},
  author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2207.01780},
  year={2022}
}


@article{chen2019sequencer,
  title={Sequencer: Sequence-to-sequence learning for end-to-end program repair},
  author={Chen, Zimin and Kommrusch, Steve and Tufano, Michele and Pouchet, Louis-No{\"e}l and Poshyvanyk, Denys and Monperrus, Martin},
  journal={IEEE Transactions on Software Engineering},
  volume={47},
  number={9},
  pages={1943--1959},
  year={2019},
  publisher={IEEE}
}

 @misc{openai_api, url={https://beta.openai.com/examples?category=code}, journal={OpenAI API}} 

 @article{petroni2020context,
  title={How context affects language models' factual predictions},
  author={Petroni, Fabio and Lewis, Patrick and Piktus, Aleksandra and Rockt{\"a}schel, Tim and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2005.04611},
  year={2020}
}


article{petroni2020context,
  title={How context affects language models' factual predictions},
  author={Petroni, Fabio and Lewis, Patrick and Piktus, Aleksandra and Rockt{\"a}schel, Tim and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2005.04611},
  year={2020}
}
@inproceedings{hu2018deep,
  title={Deep code comment generation},
  author={Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
  booktitle={2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)},
  pages={200--20010},
  year={2018},
  organization={IEEE}
}

@article{tenny1988program,
  title={Program readability: Procedures versus comments},
  author={Tenny, Ted},
  journal={IEEE Transactions on Software Engineering},
  volume={14},
  number={9},
  pages={1271},
  year={1988},
  publisher={IEEE Computer Society}
}

@inproceedings{woodfield1981effect,
  title={The effect of modularization and comments on program comprehension},
  author={Woodfield, Scott N and Dunsmore, Hubert E and Shen, Vincent Y},
  booktitle={Proceedings of the 5th international conference on Software engineering},
  pages={215--223},
  year={1981}
}




@inproceedings{de2005study,
  title={A study of the documentation essential to software maintenance},
  author={de Souza, Sergio Cozzetti B and Anquetil, Nicolas and de Oliveira, K{\'a}thia M},
  booktitle={Proceedings of the 23rd annual international conference on Design of communication: documenting \& designing for pervasive information},
  pages={68--75},
  year={2005}
}

@inproceedings{hartzman1993maintenance,
  title={Maintenance productivity: Observations based on an experience in a large system environment},
  author={Hartzman, Carl S and Austin, Charles F},
  booktitle={Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: software engineering-Volume 1},
  pages={138--170},
  year={1993}
}

@article{lientz1983issues,
  title={Issues in software maintenance},
  author={Lientz, Bennet P},
  journal={ACM Computing Surveys (CSUR)},
  volume={15},
  number={3},
  pages={271--278},
  year={1983},
  publisher={ACM New York, NY, USA}
}

@inproceedings{bavota2012does,
  title={When does a refactoring induce bugs? an empirical study},
  author={Bavota, Gabriele and De Carluccio, Bernardino and De Lucia, Andrea and Di Penta, Massimiliano and Oliveto, Rocco and Strollo, Orazio},
  booktitle={2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation},
  pages={104--113},
  year={2012},
  organization={IEEE}
}


@book{becker1999refactoring,
  title={Refactoring: improving the design of existing code},
  author={Becker, Paul and Fowler, Martin and Beck, Kent and Brant, John and Opdyke, William and Roberts, Don},
  year={1999},
  publisher={Addison-Wesley Professional}
}

@article{huang2019learning,
  title={Learning code context information to predict comment locations},
  author={Huang, Yuan and Hu, Xinyu and Jia, Nan and Chen, Xiangping and Xiong, Yingfei and Zheng, Zibin},
  journal={IEEE Transactions on Reliability},
  volume={69},
  number={1},
  pages={88--105},
  year={2019},
  publisher={IEEE}
}

@inproceedings{wen2019large,
  title={A large-scale empirical study on code-comment inconsistencies},
  author={Wen, Fengcai and Nagy, Csaba and Bavota, Gabriele and Lanza, Michele},
  booktitle={2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)},
  pages={53--64},
  year={2019},
  organization={IEEE}
}

@article{hellendoorn2021growing,
  title={The growing cost of deep learning for source code},
  author={Hellendoorn, Vincent J and Sawant, Anand Ashok},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={31--33},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{lachaux2020unsupervised,
  title={Unsupervised translation of programming languages},
  author={Lachaux, Marie-Anne and Roziere, Baptiste and Chanussot, Lowik and Lample, Guillaume},
  journal={arXiv preprint arXiv:2006.03511},
  year={2020}
}

@article{roy2007survey,
  title={A survey on software clone detection research},
  author={Roy, Chanchal Kumar and Cordy, James R},
  journal={Queen’s School of computing TR},
  volume={541},
  number={115},
  pages={64--68},
  year={2007}
}


@article{ahmed2023recommending,
  title={Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models},
  author={Ahmed, Toufique and Ghosh, Supriyo and Bansal, Chetan and Zimmermann, Thomas and Zhang, Xuchao and Rajmohan, Saravan},
  journal={arXiv preprint arXiv:2301.03797},
  year={2023}
}

@inproceedings{jain2022jigsaw,
  title={Jigsaw: Large language models meet program synthesis},
  author={Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={1219--1231},
  year={2022}
}

@article{ahmed2022few,
  title={Few-shot training LLMs for project-specific code-summarization},
  author={Ahmed, Toufique and Devanbu, Premkumar},
  journal={arXiv preprint arXiv:2207.04237},
  year={2022}
}

@inproceedings{casalnuovo2020theory,
  title={A theory of dual channel constraints},
  author={Casalnuovo, Casey and Barr, Earl T and Dash, Santanu Kumar and Devanbu, Prem and Morgan, Emily},
  booktitle={2020 IEEE/ACM 42nd International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)},
  pages={25--28},
  year={2020},
  organization={IEEE}
}