@inproceedings{10.1007/978-3-319-11179-7_6,
  title = {Memory Capacity of Input-Driven Echo State Networks at the Edge of Chaos},
  booktitle = {Artificial Neural Networks and Machine Learning \textendash{} {{ICANN}} 2014},
  author = {Baran{\v c}ok, Peter and Farka{\v s}, Igor},
  editor = {Wermter, Stefan and Weber, Cornelius and Duch, W{\l}odzis{\l}aw and Honkela, Timo and {Koprinkova-Hristova}, Petia and Magg, Sven and Palm, G{\"u}nther and Villa, Alessandro E. P.},
  year = {2014},
  pages = {41--48},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {Reservoir computing provides a promising approach to efficient training of recurrent neural networks, by exploiting the computational properties of the reservoir structure. Various approaches, ranging from suitable initialization to reservoir optimization by training have been proposed. In this paper we take a closer look at short-term memory capacity, introduced by Jaeger in case of echo state networks. Memory capacity has recently been investigated with respect to criticality, the so called edge of chaos, when the network switches from a stable regime to an unstable dynamic regime. We calculate memory capacity of the networks for various input data sets, both random and structured, and show how the data distribution affects the network performance. We also investigate the effect of reservoir sparsity in this context.},
  isbn = {978-3-319-11179-7}
}

@article{10.3389/fnbot.2019.00037,
  title = {{{SAE}}+{{LSTM}}: {{A}} New Framework for Emotion Recognition from Multi-Channel {{EEG}}},
  author = {Xing, Xiaofen and Li, Zhenqi and Xu, Tianyuan and Shu, Lin and Hu, Bin and Xu, Xiangmin},
  year = {2019},
  journal = {Frontiers in Neurorobotics},
  volume = {13},
  pages = {37},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2019.00037},
  abstract = {EEG-based automatic emotion recognition can help brain-inspired robots in improving their interactions with humans. This paper presents a novel framework for emotion recognition using multi-channel electroencephalogram (EEG). The framework consists of a linear EEG mixing model and an emotion timing model. Our proposed framework considerably decomposes the EEG source signals from the collected EEG signals and improves classification accuracy by using the context correlations of the EEG feature sequences. Specially, Stack AutoEncoder (SAE) is used to build and solve the linear EEG mixing model and the emotion timing model is based on the Long Short-Term Memory Recurrent Neural Network (LSTM-RNN). The framework was implemented on the DEAP dataset for an emotion recognition experiment, where the mean accuracy of emotion recognition achieved 81.10\% in valence and 74.38\% in arousal, and the effectiveness of our framework was verified. Our framework exhibited a better performance in emotion recognition using multi-channel EEG than the compared conventional approaches in the experiments.},
  keywords = {human_state_monitoring}
}

@article{10.3389/frai.2022.824655,
  title = {Catastrophic Forgetting in Deep Graph Networks: {{A}} Graph Classification Benchmark},
  author = {Carta, Antonio and Cossu, Andrea and Errica, Federico and Bacciu, Davide},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  issn = {2624-8212},
  doi = {10.3389/frai.2022.824655},
  abstract = {In this work, we study the phenomenon of catastrophic forgetting in the graph representation learning scenario. The primary objective of the analysis is to understand whether classical continual learning techniques for flat and sequential data have a tangible impact on performances when applied to graph data. To do so, we experiment with a structure-agnostic model and a deep graph network in a robust and controlled environment on three different datasets. The benchmark is complemented by an investigation on the effect of structure-preserving regularization techniques on catastrophic forgetting. We find that replay is the most effective strategy in so far, which also benefits the most from the use of regularization. Our findings suggest interesting future research at the intersection of the continual and graph representation learning fields. Finally, we provide researchers with a flexible software framework to reproduce our results and carry out further experiments.}
}

@article{10.3389/frai.2022.829842,
  title = {Is Class-Incremental Enough for Continual Learning?},
  author = {Cossu, Andrea and Graffieti, Gabriele and Pellegrini, Lorenzo and Maltoni, Davide and Bacciu, Davide and Carta, Antonio and Lomonaco, Vincenzo},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  issn = {2624-8212},
  doi = {10.3389/frai.2022.829842},
  abstract = {The ability of a model to learn continually can be empirically assessed in different continual learning scenarios. Each scenario defines the constraints and the opportunities of the learning environment. Here, we challenge the current trend in the continual learning literature to experiment mainly on class-incremental scenarios, where classes present in one experience are never revisited. We posit that an excessive focus on this setting may be limiting for future research on continual learning, since class-incremental scenarios artificially exacerbate catastrophic forgetting, at the expense of other important objectives like forward transfer and computational efficiency. In many real-world environments, in fact, repetition of previously encountered concepts occurs naturally and contributes to softening the disruption of previous knowledge. We advocate for a more in-depth study of alternative continual learning scenarios, in which repetition is integrated by design in the stream of incoming information. Starting from already existing proposals, we describe the advantages such class-incremental with repetition scenarios could offer for a more comprehensive assessment of continual learning models.}
}

@inproceedings{1570399,
  title = {Echo State Networks Used for Motor Control},
  booktitle = {Proceedings of the 2005 {{IEEE}} International Conference on Robotics and Automation},
  author = {Salmen, M. and Ploger, P. G.},
  year = {2005},
  pages = {1953--1958},
  doi = {10.1109/ROBOT.2005.1570399}
}

@article{1629106,
  title = {A Tighter Bound for the Echo State Property},
  author = {Buehner, M. and Young, P.},
  year = {2006},
  journal = {IEEE Transactions on Neural Networks},
  volume = {17},
  number = {3},
  pages = {820--824},
  doi = {10.1109/TNN.2006.872357}
}

@misc{190403137Learning,
  title = {[1904.03137] {{Learning}} to {{Remember}}: {{A Synaptic Plasticity Driven Framework}} for {{Continual Learning}}},
  howpublished = {https://arxiv.org/abs/1904.03137},
  keywords = {continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RN8AN6H3\\1904.html}
}

@misc{190808574RNNs,
  title = {[1908.08574] {{RNNs Evolving}} on an {{Equilibrium Manifold}}: {{A Panacea}} for {{Vanishing}} and {{Exploding Gradients}}?},
  howpublished = {https://arxiv.org/abs/1908.08574},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5U7WAK3C\\[1908.08574] RNNs Evolving on an Equilibrium Manif.pdf;C\:\\Users\\w-32\\Zotero\\storage\\Y58VIIPT\\1908.html}
}

@misc{191002718Continual,
  title = {[1910.02718] {{Continual Learning}} in {{Neural Networks}}},
  howpublished = {https://arxiv.org/abs/1910.02718},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KABV7YQB\\[1910.02718] Continual Learning in Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\X4Z3TBE2\\1910.html}
}

@misc{200200025Gating,
  title = {[2002.00025] {{Gating}} Creates Slow Modes and Controls Phase-Space Complexity in {{GRUs}} and {{LSTMs}}},
  howpublished = {https://arxiv.org/abs/2002.00025},
  keywords = {Dynamical systems,rnn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\N2LSN3FX\\[2002.00025] Gating creates slow modes and control.pdf;C\:\\Users\\w-32\\Zotero\\storage\\8N2MY8GM\\2002.html}
}

@misc{210202805How,
  title = {[2102.02805] {{How}} Do {{Quadratic Regularizers Prevent Catastrophic Forgetting}}: {{The Role}} of {{Interpolation}}},
  howpublished = {https://arxiv.org/abs/2102.02805},
  keywords = {cl-regularization,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EIV7XVWL\\[2102.02805] How do Quadratic Regularizers Prevent.pdf;C\:\\Users\\w-32\\Zotero\\storage\\Y5UR8CWU\\2102.html}
}

@misc{220409517Entropybased,
  title = {[2204.09517] {{Entropy-based Stability-Plasticity}} for {{Lifelong Learning}}},
  howpublished = {https://arxiv.org/abs/2204.09517},
  keywords = {continual,entropy},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GJLQFAYF\\2204.html}
}

@misc{220602916Remember,
  title = {[2206.02916] {{Remember}} the {{Past}}: {{Distilling Datasets}} into {{Addressable Memories}} for {{Neural Networks}}},
  howpublished = {https://arxiv.org/abs/2206.02916},
  keywords = {continual,synthetic-replay},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6B6KEXLK\\2206.html}
}

@misc{220709248Don,
  title = {[2207.09248] {{Don}}'t {{Stop Learning}}: {{Towards Continual Learning}} for the {{CLIP Model}}},
  howpublished = {https://arxiv.org/abs/2207.09248},
  keywords = {clip,continual,unread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XKHKTBBV\\2207.html}
}

@article{7428848,
  title = {High-Performance Personalized Heartbeat Classification Model for Long-Term {{ECG}} Signal},
  author = {Li, P. and Wang, Y. and He, J. and Wang, L. and Tian, Y. and Zhou, T. and Li, T. and Li, J.},
  year = {2017},
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {64},
  number = {1},
  pages = {78--86},
  doi = {10.1109/TBME.2016.2539421},
  keywords = {human_state_monitoring}
}

@article{8070966,
  title = {End-to-End Multimodal Emotion Recognition Using Deep Neural Networks},
  author = {Tzirakis, P. and Trigeorgis, G. and Nicolaou, M. A. and Schuller, B. W. and Zafeiriou, S.},
  year = {2017},
  month = dec,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {11},
  number = {8},
  pages = {1301--1309},
  issn = {1941-0484},
  doi = {10.1109/JSTSP.2017.2764438},
  abstract = {Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human-computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a convolutional neural network (CNN) to extract features from the speech, while for the visual modality a deep residual network of 50 layers is used. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, long short-term memory networks are utilized. The system is then trained in an end-to-end fashion where-by also taking advantage of the correlations of each of the streams-we manage to significantly outperform, in terms of concordance correlation coefficient, traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.},
  keywords = {CNN,emotion recognition,resnet,speech recognition}
}

@article{846741,
  ids = {atiyaNewResultsRecurrent2000},
  title = {New Results on Recurrent Network Training: Unifying the Algorithms and Accelerating Convergence},
  author = {Atiya, A. F. and Parlos, A. G.},
  year = {2000},
  journal = {IEEE Transactions on Neural Networks},
  volume = {11},
  number = {3},
  pages = {697--709},
  doi = {10.1109/72.846741},
  keywords = {RNN,sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YEF32Z2D\\Atiya, Parlos - 2000 - New results on recurrent network training unifying the algorithmsand accelerating convergence(2).pdf}
}

@article{8668730,
  ids = {jingGatedOrthogonalRecurrent2017},
  title = {Gated Orthogonal Recurrent Units: {{On}} Learning to Forget},
  author = {Jing, L. and Gulcehre, C. and Peurifoy, J. and Shen, Y. and Tegmark, M. and Soljacic, M. and Bengio, Y.},
  year = {2019},
  journal = {Neural Computation},
  volume = {31},
  number = {4},
  pages = {765--783},
  doi = {10.1162/neco_a_01174},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IN9QZEAR\\Jing et al. - 2017 - Gated Orthogonal Recurrent Units On Learning to Forget(2).pdf}
}

@article{8691755,
  title = {{{LSTM-Based ECG}} Classification for Continuous Monitoring on Personal Wearable Devices},
  author = {Saadatnejad, S. and Oveisi, M. and Hashemi, M.},
  year = {2020},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {24},
  number = {2},
  pages = {515--523},
  doi = {10.1109/JBHI.2019.2911367},
  keywords = {human_state_monitoring}
}

@inproceedings{9207550,
  title = {Continual {{Learning}} with {{Gated Incremental Memories}} for Sequential Data Processing},
  booktitle = {2020 International Joint Conference on Neural Networks ({{IJCNN}})},
  author = {Cossu, A. and Carta, A. and Bacciu, D.},
  year = {2020},
  pages = {1--8},
  doi = {10.1109/IJCNN48605.2020.9207550}
}

@article{9737321,
  ids = {yuSelfTrainingClassIncrementalSemantic2022},
  title = {Self-Training for Class-Incremental Semantic Segmentation},
  author = {Yu, Lu and Liu, Xialei and {van de Weijer}, Joost},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  eprint = {2012.03362},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1--12},
  doi = {10.1109/TNNLS.2022.3155746},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\42ALGAZU\\Yu et al_2022_Self-Training for Class-Incremental Semantic Segmentation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\A3IB3A5P\\2012.html}
}

@misc{99BottlesOOP,
  title = {99 {{Bottles}} of {{OOP A Practical Guide}} to {{Object-Oriented Design}} ({{JavaScript Edition}}) by {{Sandi Metz}}, {{Katrina Owen}}, {{TJ Stankus}} (z-Lib.Org).Pdf},
  keywords = {book,oop},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\59HL34JK\\99 Bottles of OOP A Practical Guide to Object-Oriented Design (JavaScript Edition) by Sandi Metz, Katrina Owen, TJ Stankus (z-lib.org).pdf}
}

@inproceedings{abadiDeepLearningDifferential2016,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  month = oct,
  series = {{{CCS}} '16},
  pages = {308--318},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2976749.2978318},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  isbn = {978-1-4503-4139-4},
  keywords = {deep learning,diff-privacy,differential privacy},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7T4ZY5EP\\Abadi et al_2016_Deep Learning with Differential Privacy.pdf}
}

@article{abadiTensorflowLargescaleMachine2016,
  title = {Tensorflow: {{Large-scale}} Machine Learning on Heterogeneous Distributed Systems},
  shorttitle = {Tensorflow},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
  year = {2016},
  journal = {arXiv preprint arXiv:1603.04467},
  eprint = {1603.04467},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {software},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VQANGVAC\\whitepaper tensorflow.pdf}
}

@article{abrahamMemoryRetentionSynaptic2005,
  title = {Memory Retention \textendash{} the Synaptic Stability versus Plasticity Dilemma},
  author = {Abraham, Wickliffe C. and Robins, Anthony},
  year = {2005},
  month = feb,
  journal = {Trends in Neurosciences},
  volume = {28},
  number = {2},
  pages = {73--78},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2004.12.003},
  abstract = {Memory maintenance is widely believed to involve long-term retention of the synaptic weights that are set within relevant neural circuits during learning. However, despite recent exciting technical advances, it has not yet proved possible to confirm experimentally this intuitively appealing hypothesis. Artificial neural networks offer an alternative methodology as they permit continuous monitoring of individual connection weights during learning and retention. In such models, ongoing alterations in connection weights are required if a network is to retain previously stored material while learning new information. Thus, the duration of synaptic change does not necessarily define the persistence of a memory; rather, it is likely that a regulated balance of synaptic stability and synaptic plasticity is required for optimal memory retention in real neuronal circuits.},
  langid = {english},
  keywords = {cl-neuroscience,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WV5V32RQ\\S0166223604003704.html}
}

@misc{AcceleratingContinualLearning,
  title = {Accelerating {{Continual Learning}} on {{Edge FPGA}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  howpublished = {https://ieeexplore.ieee.org/document/9556356},
  keywords = {continual,edge-computing,fpga},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YJ8Z94EC\\9556356.html}
}

@article{achilleCriticalLearningPeriods2017,
  title = {Critical {{Learning Periods}} in {{Deep Neural Networks}}},
  author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
  year = {2017},
  abstract = {Critical periods are phases in the early development of humans and animals during which experience can irreversibly affect the architecture of neuronal networks. In this work, we study the effects of visual stimulus deficits on the training of artificial neural networks (ANNs). Introducing well-characterized visual deficits, such as cataract-like blurring, in the early training phase of a standard deep neural network causes a permanent performance loss that closely mimics critical period behavior in humans and animal models. Deficits that do not affect low-level image statistics, such as vertical flipping of the images, have no lasting effect on the ANNs' performance and can be rapidly overcome with further training. In addition, the deeper the ANN is, the more pronounced the critical period. To better understand this phenomenon, we use Fisher Information as a measure of the strength of the network's connections during the training. Our information-theoretic analysis suggests that the first few epochs are critical for the creation of strong connections across different layers, optimal for processing the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial rapid learning phase of ANN training, under-scrutinized compared to its asymptotic behavior, plays a key role in defining the final performance of networks. Our results also show how critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.},
  keywords = {fisher-info,info-bottleneck,sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IJBZCIVZ\\Achille, Rovere, Soatto - 2017 - Critical Learning Periods in Deep Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\WMD4Y9P6\\1711.08856.pdf}
}

@article{achilleEmergenceInvarianceDisentangling2017,
  title = {Emergence of {{Invariance}} and {{Disentangling}} in {{Deep Representations}}},
  author = {Achille, Alessandro and Soatto, Stefano},
  year = {2017},
  volume = {18},
  pages = {1--34},
  abstract = {Using established principles from Information Theory and Statistics, we show that in a deep neural network invariance to nuisance factors is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then show that, in order to avoid memorization, we need to limit the quantity of information stored in the weights, which leads to a novel usage of the Information Bottleneck Lagrangian on the weights as a learning criterion. This also has an alternative interpretation as minimizing a PAC-Bayesian bound on the test error. Finally, we exploit a duality between weights and activations induced by the architecture, to show that the information in the weights bounds the minimality and Total Correlation of the layers, therefore showing that regularizing the weights explicitly or implicitly, using SGD, not only helps avoid overfitting, but also fosters invariance and disentangling of the learned representation. The theory also enables predicting sharp phase transitions between underfitting and overfitting random labels at precise information values, and sheds light on the relation between the geometry of the loss function, in particular so-called "flat minima," and generalization.},
  keywords = {disentangle,info-bottleneck,representation learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\W6FF4GBZ\\Achille, Soatto - 2017 - Emergence of Invariance and Disentangling in Deep Representations.pdf}
}

@article{achilleWhereInformationDeep2019,
  title = {Where Is the {{Information}} in a {{Deep Neural Network}}?},
  author = {Achille, Alessandro and Soatto, Stefano},
  year = {2019},
  month = jun,
  journal = {arXiv:1905.12213 [cs, math, stat]},
  eprint = {1905.12213},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Whatever information a Deep Neural Network has gleaned from past data is encoded in its weights. How this information affects the response of the network to future data is largely an open question. In fact, even how to define and measure information in a network is still not settled. We introduce the notion of Information in the Weights as the optimal trade-off between accuracy of the network and complexity of the weights, relative to a prior. Depending on the prior, the definition reduces to known information measures such as Shannon Mutual Information and Fisher Information, but affords added flexibility that enables us to relate it to generalization, via the PAC-Bayes bound, and to invariance. This relation hinges not only on the architecture of the model, but surprisingly on how it is trained. We then introduce a notion of effective information in the activations, which are deterministic functions of future inputs, resolving inconsistencies in prior work. We relate this to the Information in the Weights, and use this result to show that models of low complexity not only generalize better, but are bound to learn invariant representations of future inputs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {fisher-info,info-bottleneck,info-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\36GP7N8E\\Achille and Soatto - 2019 - Where is the Information in a Deep Neural Network.pdf}
}

@inproceedings{afoninModelAgnosticFederated2021,
  title = {Towards {{Model Agnostic Federated Learning Using Knowledge Distillation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Afonin, Andrei and Karimireddy, Sai Praneeth},
  year = {2021},
  month = sep,
  abstract = {Is it possible to design an universal API for federated learning using which an ad-hoc group of data-holders (agents) collaborate with each other and perform federated learning? Such an API would...},
  langid = {english},
  keywords = {continual,exmodel,federated,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\E9NKU924\\forum.html}
}

@inproceedings{agirreSemEval2012TaskPilot2012,
  title = {{{SemEval-2012 Task}} 6: {{A Pilot}} on {{Semantic Textual Similarity}}},
  booktitle = {{{SemEval}}@{{NAACL-HLT}}},
  author = {Agirre, Eneko and Cer, Daniel M and Diab, Mona T and {Gonzalez-Agirre}, Aitor},
  year = {2012},
  keywords = {nlp}
}

@article{aguilarKnowledgeDistillationInternal2020,
  title = {Knowledge {{Distillation}} from {{Internal Representations}}},
  author = {Aguilar, Gustavo and Ling, Yuan and Zhang, Yu and Yao, Benjamin and Fan, Xing and Guo, Chenlei},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {7350--7357},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i05.6229},
  abstract = {Knowledge distillation is typically conducted by training a small model (the student) to mimic a large and cumbersome model (the teacher). The idea is to compress the knowledge from the teacher by using its output probabilities as softlabels to optimize the student. However, when the teacher is considerably large, there is no guarantee that the internal knowledge of the teacher will be transferred into the student; even if the student closely matches the soft-labels, its internal representations may be considerably different. This internal mismatch can undermine the generalization capabilities originally intended to be transferred from the teacher to the student. In this paper, we propose to distill the internal representations of a large model such as BERT into a simplified version of it. We formulate two ways to distill such representations and various algorithms to conduct the distillation. We experiment with datasets from the GLUE benchmark and consistently show that adding knowledge distillation from internal representations is a more powerful method than only using soft-label distillation.},
  langid = {english},
  keywords = {bert,continual,latent-knowledge-distillation,transformer,unread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HK8BW7XC\\Aguilar et al. - 2020 - Knowledge Distillation from Internal Representatio.pdf}
}

@article{aguirreBirdEyeView2019,
  title = {A {{Bird}}'s {{Eye View}} of {{Nonlinear System Identification}}},
  author = {Aguirre, Luis Antonio},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.06803 [cs, eess]},
  eprint = {1907.06803},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {This text aims at providing a bird's eye view of system identification with special attention to nonlinear systems. The driving force is to give a feeling for the philosophical problems facing those that build mathematical models from data. Special attention will be given to grey-box approaches in nonlinear system identification. In this text, grey-box methods use auxiliary information such as the system steady-state data, possible symmetries, some bifurcations and the presence of hysteresis. The text ends with a sample of applications. No attempt is made to be thorough nor to survey such an extensive and mature field as system identification. In most parts references will be provided for a more detailed study.},
  archiveprefix = {arXiv},
  keywords = {book,LDS},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4748JMJQ\\Aguirre - 2019 - A Bird's Eye View of Nonlinear System Identificati.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MW64I3EV\\1907.html}
}

@inproceedings{ahnVariationalInformationDistillation2019,
  title = {Variational {{Information Distillation}} for {{Knowledge Transfer}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D. and Dai, Zhenwen},
  year = {2019},
  month = jun,
  pages = {9155--9163},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00938},
  abstract = {Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding handcrafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\924TIG7H\\Ahn et al. - 2019 - Variational Information Distillation for Knowledge.pdf}
}

@misc{ainsworthGitReBasinMerging2022,
  title = {Git {{Re-Basin}}: {{Merging Models}} modulo {{Permutation Symmetries}}},
  shorttitle = {Git {{Re-Basin}}},
  author = {Ainsworth, Samuel K. and Hayase, Jonathan and Srinivasa, Siddhartha},
  year = {2022},
  month = sep,
  number = {arXiv:2209.04836},
  eprint = {2209.04836},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.04836},
  abstract = {The success of deep learning is thanks to our ability to solve certain massive non-convex optimization problems with relative ease. Despite non-convex optimization being NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes contain (nearly) a single basin, after accounting for all possible permutation symmetries of hidden units. We introduce three algorithms to permute the units of one model to bring them into alignment with units of a reference model. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10 and CIFAR-100. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity across a variety of models and datasets. Finally, we discuss shortcomings of a single basin theory, including a counterexample to the linear mode connectivity hypothesis.},
  archiveprefix = {arXiv},
  keywords = {exmodel,model-patching},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\D3DBJKTK\\Ainsworth et al_2022_Git Re-Basin.pdf;C\:\\Users\\w-32\\Zotero\\storage\\48W9FKN4\\2209.html}
}

@article{akiyamaAnalysisCharacteristicsMultiStep2019,
  title = {Analysis on {{Characteristics}} of {{Multi-Step Learning Echo State Networks}} for {{Nonlinear Time Series Prediction}}},
  author = {Akiyama, Takanori},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {ESN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZLNR7B65\\Akiyama - 2019 - Analysis on Characteristics of Multi-Step Learning Echo State Networks for Nonlinear Time Series Prediction.pdf}
}

@article{alabiDifferentiallyPrivateSimple2020,
  title = {Differentially {{Private Simple Linear Regression}}},
  author = {Alabi, Daniel and McMillan, Audra and Sarathy, Jayshree and Smith, Adam and Vadhan, Salil},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.05157 [cs, stat]},
  eprint = {2007.05157},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Economics and social science research often require analyzing datasets of sensitive personal information at fine granularity, with models fit to small subsets of the data. Unfortunately, such fine-grained analysis can easily reveal sensitive individual information. We study algorithms for simple linear regression that satisfy differential privacy, a constraint which guarantees that an algorithm's output reveals little about any individual input data record, even to an attacker with arbitrary side information about the dataset. We consider the design of differentially private algorithms for simple linear regression for small datasets, with tens to hundreds of datapoints, which is a particularly challenging regime for differential privacy. Focusing on a particular application to small-area analysis in economics research, we study the performance of a spectrum of algorithms we adapt to the setting. We identify key factors that affect their performance, showing through a range of experiments that algorithms based on robust estimators (in particular, the Theil-Sen estimator) perform well on the smallest datasets, but that other more standard algorithms do better as the dataset size increases.},
  archiveprefix = {arXiv},
  keywords = {diff-privacy,project-dp-rnn-galli},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MTSUJUET\\2007.html}
}

@article{alemiTherMLThermodynamicsMachine2018,
  title = {{{TherML}}: {{Thermodynamics}} of {{Machine Learning}}},
  author = {Alemi, Alexander A. and Fischer, Ian},
  year = {2018},
  month = jul,
  abstract = {In this work we offer a framework for reasoning about a wide class of existing objectives in machine learning. We develop a formal correspondence between this work and thermodynamics and discuss its implications.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GRI6KPF7\\Alemi, Fischer - 2018 - TherML Thermodynamics of Machine Learning.pdf}
}

@article{Alhagry2017,
  title = {Emotion Recognition Based on {{EEG}} Using {{LSTM}} Recurrent Neural Network},
  author = {Alhagry, Salma and Fahmy, Aly Aly and {El-Khoribi}, Reda A.},
  year = {2017},
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {8},
  number = {10},
  publisher = {{The Science and Information Organization}},
  doi = {10.14569/IJACSA.2017.081046},
  keywords = {human_state_monitoring}
}

@article{aljundiContinualLearningNeural2019,
  title = {Continual {{Learning}} in {{Neural Networks}}},
  author = {Aljundi, Rahaf},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.02718 [cs, stat]},
  eprint = {1910.02718},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Artificial neural networks have exceeded human-level performance in accomplishing several individual tasks (e.g. voice recognition, object recognition, and video games). However, such success remains modest compared to human intelligence that can learn and perform an unlimited number of tasks. Humans' ability of learning and accumulating knowledge over their lifetime is an essential aspect of their intelligence. Continual machine learning aims at a higher level of machine intelligence through providing the artificial agents with the ability to learn online from a non-stationary and never-ending stream of data. A key component of such a never-ending learning process is to overcome the catastrophic forgetting of previously seen data, a problem that neural networks are well known to suffer from. The work described in this thesis has been dedicated to the investigation of continual learning and solutions to mitigate the forgetting phenomena in neural networks. To approach the continual learning problem, we first assume a task incremental setting where tasks are received one at a time and data from previous tasks are not stored. Since the task incremental setting can't be assumed in all continual learning scenarios, we also study the more general online continual setting. We consider an infinite stream of data drawn from a non-stationary distribution with a supervisory or self-supervisory training signal. The proposed methods in this thesis have tackled important aspects of continual learning. They were evaluated on different benchmarks and over various learning sequences. Advances in the state of the art of continual learning have been shown and challenges for bringing continual learning into application were critically identified.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\G9ZD7HIN\\Aljundi - 2019 - Continual Learning in Neural Networks.pdf}
}

@article{aljundiContinualNoveltyDetection2021,
  title = {Continual {{Novelty Detection}}},
  author = {Aljundi, Rahaf and Reino, Daniel Olmeda and Chumerin, Nikolay and Turner, Richard E.},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.12964 [cs]},
  eprint = {2106.12964},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Novelty Detection methods identify samples that are not representative of a model's training set thereby flagging misleading predictions and bringing a greater flexibility and transparency at deployment time. However, research in this area has only considered Novelty Detection in the offline setting. Recently, there has been a growing realization in the computer vision community that applications demand a more flexible framework - Continual Learning - where new batches of data representing new domains, new classes or new tasks become available at different points in time. In this setting, Novelty Detection becomes more important, interesting and challenging. This work identifies the crucial link between the two problems and investigates the Novelty Detection problem under the Continual Learning setting. We formulate the Continual Novelty Detection problem and present a benchmark, where we compare several Novelty Detection methods under different Continual Learning settings. We show that Continual Learning affects the behaviour of novelty detection algorithms, while novelty detection can pinpoint insights in the behaviour of a continual learner. We further propose baselines and discuss possible research directions. We believe that the coupling of the two problems is a promising direction to bring vision models into practice.},
  archiveprefix = {arXiv},
  keywords = {continual,notag,toread,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\25C7MCH9\\Aljundi et al_2021_Continual Novelty Detection.pdf;C\:\\Users\\w-32\\Zotero\\storage\\DFGW2V3Q\\2106.html}
}

@article{aljundiExpertGateLifelong2017,
  title = {Expert Gate: {{Lifelong}} Learning with a Network of Experts},
  author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  year = {2017},
  journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume = {2017-Janua},
  pages = {7120--7129},
  issn = {9781538604571},
  doi = {10.1109/CVPR.2017.753},
  abstract = {In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process,data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with finetuning or learning without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.},
  keywords = {autoencoders,CNN,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KJVSBJVN\\Aljundi, Chakravarty, Tuytelaars - 2017 - Expert gate Lifelong learning with a network of experts(2).pdf}
}

@article{aljundiOnlineContinualLearning,
  title = {Online {{Continual Learning}} with {{Maximally Interfered Retrieval}}},
  author = {Aljundi, Rahaf and Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Lin, Min and Charlin, Laurent and Tuytelaars, Tinne},
  pages = {12},
  abstract = {Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or "single-pass through the data" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https://github.com/optimass/Maximally\_Interfered\_Retrieval.},
  langid = {english},
  keywords = {continual,online learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZKNJAZMH\\Aljundi et al. - Online Continual Learning with Maximally Interfere.pdf}
}

@article{aljundiSelflessSequentialLearning2019,
  title = {Selfless {{Sequential Learning}}},
  author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
  year = {2019},
  month = apr,
  journal = {arXiv:1806.05421 [cs, stat]},
  eprint = {1806.05421},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them. To achieve Selfless Sequential Learning we study different regularization strategies and activation functions. We find that imposing sparsity at the level of the representation (i.e.\textasciitilde neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations, our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain. We combine our novel regularizer, with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network. We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement \%over alternative regularizers we studied on diverse datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\W6LEF3DM\\Aljundi et al. - 2019 - Selfless Sequential Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\82RA27T2\\1806.html}
}

@misc{allen-zhuUnderstandingEnsembleKnowledge2021,
  title = {Towards {{Understanding Ensemble}}, {{Knowledge Distillation}} and {{Self-Distillation}} in {{Deep Learning}}},
  author = {{Allen-Zhu}, Zeyuan and Li, Yuanzhi},
  year = {2021},
  month = jul,
  number = {arXiv:2012.09816},
  eprint = {2012.09816},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.09816},
  abstract = {We formally study how ensemble of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using knowledge distillation. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the SAME architecture, trained using the SAME algorithm on the SAME data set, and they only differ by the random seeds used in the initialization. We empirically show that ensemble/knowledge distillation in deep learning works very differently from traditional learning theory, especially differently from ensemble of random feature mappings or the neural-tangent-kernel feature mappings, and is potentially out of the scope of existing theorems. Thus, to properly understand ensemble and knowledge distillation in deep learning, we develop a theory showing that when data has a structure we refer to as "multi-view", then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model by training a single model to match the output of the ensemble instead of the true label. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the "dark knowledge" is hidden in the outputs of the ensemble -- that can be used in knowledge distillation -- comparing to the true data labels. In the end, we prove that self-distillation can also be viewed as implicitly combining ensemble and knowledge distillation to improve test accuracy.},
  archiveprefix = {arXiv},
  keywords = {knowledge-distillation,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UUQ3W64Y\\Allen-Zhu_Li_2021_Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VVVYTD7U\\2012.html}
}

@article{alpayLearningMultipleTimescales2016,
  title = {Learning Multiple Timescales in Recurrent Neural Networks},
  author = {Alpay, Tayfun and Heinrich, Stefan and Wermter, Stefan},
  year = {2016},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {9886 LNCS},
  pages = {132--139},
  issn = {9783319447773},
  doi = {10.1007/978-3-319-44778-0_16},
  abstract = {Recurrent Neural Networks (RNNs) are powerful architec-tures for sequence learning. Recent advances on the vanishing gradient problem have led to improved results and an increased research interest. Among recent proposals are architectural innovations that allow the emergence of multiple timescales during training. This paper explores a number of architectures for sequence generation and prediction tasks with long-term relationships. We compare the Simple Recurrent Network (SRN) and Long Short-Term Memory (LSTM) with the recently proposed Clockwork RNN (CWRNN), Structurally Constrained Recurrent Network (SCRN), and Recurrent Plausibility Network (RPN) with regard to their capabilities of learning multiple timescales. Our results show that partitioning hidden layers under distinct temporal constraints enables the learning of multiple timescales, which contributes to the understanding of the fundamental conditions that allow RNNs to self-organize to accurate temporal abstractions.},
  keywords = {Hierarchical RNN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6BCZB6QW\\Alpay, Heinrich, Wermter - 2016 - Learning multiple timescales in recurrent neural networks(2).pdf}
}

@article{alpayPreservingActivationsRecurrent2019,
  title = {Preserving Activations in Recurrent Neural Networks Based on Surprisal},
  author = {Alpay, Tayfun and Abawi, Fares and Wermter, Stefan},
  year = {2019},
  month = feb,
  journal = {Neurocomputing},
  doi = {10.1016/J.NEUCOM.2018.11.092},
  abstract = {Learning hierarchical abstractions from sequences is a challenging and open problem for recurrent neural networks (RNNs). This is mainly due to the difficulty of detecting features that span over long time distances with also different frequencies. In this paper, we address this challenge by introducing surprisal-based activation, a novel method to preserve activations and skip updates depending on encoding-based information content. The preserved activations can be considered as temporal shortcuts with perfect memory. We present a preliminary analysis by evaluating surprisal-based activation on language modeling with the Penn Treebank corpus and find that it can improve performance when compared to baseline RNNs and Long Short-Term Memory (LSTM) networks.},
  keywords = {RNN,Surprisal},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VIZLTEU9\\Alpay, Abawi, Wermter - 2019 - Preserving activations in recurrent neural networks based on surprisal.pdf}
}

@article{alpayQuestionAnsweringHierarchical2019,
  title = {Question {{Answering}} with {{Hierarchical Attention Networks}}},
  author = {Alpay, Tayfun and Heinrich, Stefan and Nelskamp, Michael and Wermter, Stefan},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {attention,nlp,QA},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IHQQ6FXF\\Alpay et al. - 2019 - Question Answering with Hierarchical Attention Networks(2).pdf}
}

@inproceedings{alpaySurprisalBasedActivation2018,
  title = {Surprisal-{{Based Activation}} In},
  booktitle = {{{ESANN}}},
  author = {Alpay, Tayfun and Abawi, Fares and Wermter, Stefan},
  year = {2018},
  pages = {25--27},
  isbn = {978-2-87587-047-6},
  keywords = {RNN,Surprisal}
}

@misc{andriushchenkoUnderstandingImprovingFast2020,
  title = {Understanding and {{Improving Fast Adversarial Training}}},
  author = {Andriushchenko, Maksym and Flammarion, Nicolas},
  year = {2020},
  month = oct,
  number = {arXiv:2007.02617},
  eprint = {2007.02617},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.02617},
  abstract = {A recent line of work focused on making adversarial training computationally efficient for deep learning models. In particular, Wong et al. (2020) showed that \$\textbackslash ell\_\textbackslash infty\$-adversarial training with fast gradient sign method (FGSM) can fail due to a phenomenon called "catastrophic overfitting", when the model quickly loses its robustness over a single epoch of training. We show that adding a random step to FGSM, as proposed in Wong et al. (2020), does not prevent catastrophic overfitting, and that randomness is not important per se -- its main role being simply to reduce the magnitude of the perturbation. Moreover, we show that catastrophic overfitting is not inherent to deep and overparametrized networks, but can occur in a single-layer convolutional network with a few filters. In an extreme case, even a single filter can make the network highly non-linear locally, which is the main reason why FGSM training fails. Based on this observation, we propose a new regularization method, GradAlign, that prevents catastrophic overfitting by explicitly maximizing the gradient alignment inside the perturbation set and improves the quality of the FGSM solution. As a result, GradAlign allows to successfully apply FGSM training also for larger \$\textbackslash ell\_\textbackslash infty\$-perturbations and reduce the gap to multi-step adversarial training. The code of our experiments is available at https://github.com/tml-epfl/understanding-fast-adv-training.},
  archiveprefix = {arXiv},
  keywords = {notag,robustness,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JCF9QC5I\\2007.html}
}

@article{andrychowiczLearningEfficientAlgorithms2016,
  title = {Learning {{Efficient Algorithms}} with {{Hierarchical Attentive Memory}}},
  author = {Andrychowicz, Marcin and Kurach, Karol},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1602.0},
  abstract = {In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in {$\Theta$}(log n) complexity, which is a significant improvement over the standard attention mechanism that requires {$\Theta$}(n) opera-tions, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time {$\Theta$}(n log n) and general-izes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data struc-tures: a stack, a FIFO queue and a priority queue.},
  keywords = {attention,ICML},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7YZX6H3C\\Andrychowicz, Kurach - 2016 - Learning Efficient Algorithms with Hierarchical Attentive Memory.pdf}
}

@inproceedings{anonymousGraphBasedContinualLearning2020,
  title = {Graph-{{Based Continual Learning}}},
  booktitle = {Submitted to {{International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2020},
  month = sep,
  abstract = {Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal...},
  langid = {english},
  keywords = {continual,DGN,similarity-graph},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NUQJRXSE\\Anonymous_2020_Graph-Based Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\E6B7RZZN\\forum.html}
}

@article{anonymousImprovingDifferentiableNeural2018,
  title = {Improving the {{Differentiable Neural Computer Through Memory Masking}}, {{De-allocaiton}}, and {{Link DIstribution Sharpness}}},
  author = {{Anonymous}},
  year = {2018},
  journal = {ICLR 2019 - Under Review},
  pages = {1--13},
  keywords = {ICLR,MANN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JD4KKP98\\Anonymous - 2018 - Improving the Differentiable Neural Computer Through Memory Masking, De-allocaiton, and Link DIstribution Sharpnes(2).pdf}
}

@inproceedings{anonymousLearningPrejudicesContinual2022,
  title = {Learning without {{Prejudices}}: {{Continual Unbiased Learning}} via {{Benign}} and {{Malignant Forgetting}}},
  shorttitle = {Learning without {{Prejudices}}},
  booktitle = {Submitted to {{The Eleventh International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2022},
  month = oct,
  abstract = {Although machine learning algorithms have achieved state-of-the-art status in image classification, recent studies have substantiated that the ability of the models to learn several tasks in sequence, termed continual learning (CL), often suffers from abrupt degradation of performance from previous tasks. A large body of CL frameworks has been devoted to alleviating this issue. However, we observe that forgetting phenomena in CL are not always unfavorable, especially when there is bias (spurious correlation) in training data. We term such type of forgetting benign forgetting, and categorize detrimental forgetting as malignant forgetting. Based on this finding, our objective in this study is twofold: (a) to discourage malignant forgetting by generating previous representations, and (b) encourage benign forgetting by employing contrastive learning in conjunction with feature-level augmentation. Extensive evaluations of biased experimental setups demonstrate that our proposed method, Learning without Prejudices, is effective for continual unbiased learning.},
  langid = {english},
  keywords = {continual,positive-forgetting},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3TW8MKIA\\forum.html}
}

@inproceedings{arjovskyUnitaryEvolutionRecurrent2015,
  title = {Unitary {{Evolution Recurrent Neural Networks}}},
  booktitle = {{{ICML}}},
  author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  year = {2015},
  month = nov,
  abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.},
  keywords = {ICML,orthogonal-nn,RNN}
}

@article{arjovskyWassersteinGenerativeAdversarial2017,
  title = {Wasserstein {{Generative Adversarial Networks}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  year = {2017},
  month = jul,
  pages = {214--223},
  keywords = {GAN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\V56J56YP\\Arjovsky, Chintala, Bottou - 2017 - Wasserstein Generative Adversarial Networks(2).pdf}
}

@article{aroraCompressedSensingView2018,
  title = {A {{Compressed Sensing View}} of {{Unsupervised Text Embeddings}}, {{Bag-of-N-Grams}}, and {{LSTMs}}},
  author = {Arora, Sanjeev and Khodak, Mikhail and Saunshi, Nikunj and Vodrahalli, Kiran},
  year = {2018},
  journal = {ICLR},
  pages = {1--19},
  abstract = {Low-dimensional vector embeddings, computed using LSTMs or simpler tech-niques, are a popular approach for capturing the " meaning " of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by look-ing at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice.},
  keywords = {nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\B7XWGUDS\\Arora et al. - 2018 - A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-N-Grams, and LSTMs(2).pdf}
}

@article{aroraLatentVariableModel2016,
  title = {A Latent Variable Model Approach to Pmi-Based Word Embeddings},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  year = {2016},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {4},
  pages = {385--399},
  doi = {10.1109/TKDE.2018.2807452},
  abstract = {Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods including Vector Space Methods (VSMs) such as Latent Semantic Analysis (LSA), generative text models such as topic models, matrix factorization, neural nets, and energy-based models. Many of these use nonlinear operations on co-occurrence statistics, such as computing Pairwise Mutual Information (PMI). Some use hand-tuned hyperparameters and term reweighting. Often a generative model can help provide theoretical insight into such modeling choices, but there appears to be no such model to explain the above nonlinear models. For example, we know of no generative model for which the correct solution is the usual (dimension-restricted) PMI model. This paper gives a new generative model, a dynamic version of the loglinear topic model of Mnih and Hinton (2007),, as well as a pair of training objectives called RAND-WALK to compute word embeddings. The methodological novelty is to use the prior to compute closed form expressions for word statistics. These provide an explanation for the PMI model and other recent models, as well as hyperparameter choices. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are spatially isotropic. The model also helps explain why linear algebraic structure arises in low-dimensional semantic embeddings. Such structure has been used to solve analogy tasks by Mikolov et al. (2013a) and many subsequent papers. This theoretical explanation is to give an improved analogy solving method that improves success rates on analogy solving by a few percent.},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\L7HFWVPL\\Arora et al. - 2016 - A latent variable model approach to pmi-based word embeddings(2).pdf}
}

@article{aroraRANDWALKLatentVariable2015,
  title = {{{RAND-WALK}}: {{A Latent Variable Model Approach}} to {{Word Embeddings}}},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  year = {2015},
  month = feb,
  journal = {arXiv preprint arXiv:1502.03520},
  eprint = {1502.03520},
  eprinttype = {arxiv},
  abstract = {Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of\textasciitilde\textbackslash citet\{mnih2007three\}. The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by\textasciitilde\textbackslash citet\{mikolov2013efficient\} and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.},
  archiveprefix = {arXiv},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RQXIHLRS\\Arora et al. - 2015 - RAND-WALK A Latent Variable Model Approach to Word Embeddings.pdf}
}

@article{aroraSimpleToughBeat2017,
  title = {A Simple but Tough to Beat Baseline for Sentence Embeddings},
  author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  year = {2017},
  journal = {ICLR},
  pages = {1--14},
  abstract = {The success of neural network methods for computing word embeddings has mo-tivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embed-dings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsuper-vised sentence embedding is a formidable baseline: Use word embeddings com-puted using one of the popular methods on unlabeled corpus like Wikipedia, rep-resent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10\% to 30\% in textual similarity tasks, and beats sophisticated supervised methods in-cluding RNN's and LSTM's. It even improves Wieting et al.'s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsu-pervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new " smoothing " terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts.},
  keywords = {ICLR,nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SFQPEUN6\\Arora, Liang, Ma - 2017 - A simple but tough to beat baseline for sentence embeddings(2).pdf}
}

@article{arpitCloserLookMemorization2017,
  title = {A {{Closer Look}} at {{Memorization}} in {{Deep Networks}}},
  author = {Arpit, Devansh and Jastrz, Stanis{\l}aw and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua},
  year = {2017},
  keywords = {generalization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\S9PFC86L\\Arpit et al. - 2017 - A Closer Look at Memorization in Deep Networks.pdf}
}

@misc{asanoCriticalAnalysisSelfsupervision2020,
  title = {A Critical Analysis of Self-Supervision, or What We Can Learn from a Single Image},
  author = {Asano, Yuki M. and Rupprecht, Christian and Vedaldi, Andrea},
  year = {2020},
  month = feb,
  number = {arXiv:1904.13132},
  eprint = {1904.13132},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.13132},
  abstract = {We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3AL2WZSH\\Asano et al_2020_A critical analysis of self-supervision, or what we can learn from a single.pdf;C\:\\Users\\w-32\\Zotero\\storage\\L7F6KSX3\\1904.html}
}

@article{asanoExtrapolatingSingleImage2022,
  title = {Extrapolating from a {{Single Image}} to a {{Thousand Classes}} Using {{Distillation}}},
  author = {Asano, Yuki M. and Saeed, Aaqib},
  year = {2022},
  month = jan,
  journal = {arXiv:2112.00725 [cs]},
  eprint = {2112.00725},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {What can neural networks learn about the visual world from a single image? While it obviously cannot contain the multitudes of possible objects, scenes and lighting conditions that exist - within the space of all possible 256\^(3x224x224) 224-sized square images, it might still provide a strong prior for natural images. To analyze this hypothesis, we develop a framework for training neural networks from scratch using a single image by means of knowledge distillation from a supervisedly pretrained teacher. With this, we find that the answer to the above question is: 'surprisingly, a lot'. In quantitative terms, we find top-1 accuracies of 94\%/74\% on CIFAR-10/100, 59\% on ImageNet, and by extending this method to video and audio, 77\% on UCF-101 and 84\% on SpeechCommands. In extensive analyses we disentangle the effect of augmentations, choice of source image and network architectures and also discover "panda neurons" in networks that have never seen a panda. This work shows that one image can be used to extrapolate to thousands of object classes and motivates a renewed research agenda on the fundamental interplay of augmentations and image.},
  archiveprefix = {arXiv},
  keywords = {exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EEGCRWU7\\Asano_Saeed_2022_Extrapolating from a Single Image to a Thousand Classes using Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\IZ2G4YVW\\2112.html}
}

@phdthesis{asanoLearningDeepNeural2021,
  type = {{{http://purl.org/dc/dcmitype/Text}}},
  title = {Learning Deep Neural Networks: Necessity and Scope of Prior Knowledge, Raw Data, and Labels},
  shorttitle = {Learning Deep Neural Networks},
  author = {Asano, Y. M.},
  year = {2021},
  abstract = {{$<$}p{$>$}The recent rise in machine learning has been largely made possible by novel algorithms, such as convolutional neural networks and large-scale labelled datasets. Yet obtaining labelled datasets is expensive, does not scale well, and should not be necessary for learning general representations of data such as images or videos. Thus, by shifting the training of neural networks to not require labelled data, it is possible to obtain more robust, diverse and generelizeable neural networks that can scale to the vast quantities of readily available unlabelled data. Learning transferable representations from raw data can thus drastically reduce the cost of solving new tasks and improve performance in many settings where supervisory signals are scarce. The field of self-supervised learning has therefore become an increasingly popular framework for learning without labels and in this thesis I provide several works that shed light on the workings of self-supervised learning, and that contributed to, and shaped the state of the art in this field. This thesis covers works that: i) analyze the role of image transformations; ii) develop methods for self-supervised clustering in the image and video domain; iii) develop novel representation learning methods for video-audio and video-text data and; iv) propose a new dataset better suited to self-supervised pretraining than the current standard. Put together, these works investigate and further the state of self-supervised learning from multiple dimensions and its insights continue to shape the future of deep learning without labels.{$<$}/p{$>$}},
  langid = {english},
  school = {University of Oxford},
  keywords = {self-supervised,thesis,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MY6XJND4\\uuid3afdf0e8-3239-436b-bd4e-c79a8b32d8cd.html}
}

@article{asgharProgressiveMemoryBanks2018,
  title = {Progressive {{Memory Banks}} for {{Incremental Domain Adaptation}}},
  author = {Asghar, Nabiha and Mou, Lili and Selby, Kira A. and Pantasdo, Kevin D. and Poupart, Pascal and Jiang, Xin},
  year = {2018},
  month = nov,
  abstract = {This paper addresses the problem of incremental domain adaptation (IDA). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is to build a unified model performing well on all the domains that we have encountered. We propose to augment a recurrent neural network (RNN) with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that our approach achieves significantly better performance than fine-tuning alone, which suffers from the catastrophic forgetting problem. Compared with expanding hidden states, our approach is more robust for old domains, shown by both empirical and theoretical results. Our model also outperforms previous work of IDA including elastic weight consolidation (EWC) and the progressive neural network.},
  keywords = {architectural,continual,Memory,nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PNYZ5GZN\\Asghar et al. - 2018 - Progressive Memory Banks for Incremental Domain Ad.pdf;C\:\\Users\\w-32\\Zotero\\storage\\SPCXPX8X\\1811.00239.pdf}
}

@inproceedings{attardiFA3LSemEval2017Task2017,
  title = {{{FA3L}} at {{SemEval-2017 Task}} 3: {{A ThRee Embeddings Recurrent Neural Network}} for {{Question Answering}}},
  booktitle = {Proceedings of the 11th {{International Workshop}} on {{Semantic Evaluation}}, {{SemEval}}@{{ACL}} 2017, {{Vancouver}}, {{Canada}}, {{August}} 3-4, 2017},
  author = {Attardi, Giuseppe and Carta, Antonio and Errica, Federico and Madotto, Andrea and Pannitto, Ludovica},
  editor = {Bethard, Steven and Carpuat, Marine and Apidianaki, Marianna and Mohammad, Saif M and Cer, Daniel M and Jurgens, David},
  year = {2017},
  pages = {299--304},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/S17-2048}
}

@article{aurisanoConvolutionalNeuralNetwork2016,
  ids = {aurisanoConvolutionalNeuralNetwork2016a},
  title = {A Convolutional Neural Network Neutrino Event Classifier},
  author = {Aurisano, A and Radovic, A and Rocco, D and Himmel, A and Messier, MD and Niner, E and Pawloski, G and Psihas, F and Sousa, A and Vahle, P},
  year = {2016},
  journal = {Journal of Instrumentation},
  volume = {11},
  number = {09},
  pages = {P09001},
  keywords = {CNN,high-energy-physics}
}

@book{axlerLinearAlgebraDone2015,
  title = {Linear {{Algebra Done Right}}},
  author = {Axler, Sheldon},
  year = {2015},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-11080-6},
  isbn = {978-3-319-11079-0 978-3-319-11080-6},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\V29XUHRF\\Axler - 2015 - Linear Algebra Done Right.pdf}
}

@article{aytekinApproximatingBinarizationNeural2019,
  title = {Approximating {{Binarization}} in {{Neural Networks}}},
  author = {Aytekin, Caglar and Cricri, Francesco and Lainema, Jani and Aksu, Emre and Hannuksela, Miska},
  year = {2019},
  number = {July},
  issn = {9781728120096},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UGAHVTX3\\Aytekin et al. - 2019 - Approximating Binarization in Neural Networks.pdf}
}

@article{bacciuAdoptingMachineLearning2016,
  title = {Adopting a {{Machine Learning Approach}} in the {{Design}} of {{Smart Transportation Systems}}},
  author = {Bacciu, Davide and Carta, Antonio and Gnesi, Stefania and Semini, Laura},
  year = {2016},
  journal = {\{ERCIM\} News},
  volume = {2016},
  number = {105}
}

@article{bacciuContextualGraphMarkov2018,
  title = {Contextual {{Graph Markov Model}}: {{A Deep}} and {{Generative Approach}} to {{Graph Processing}}},
  author = {Bacciu, Davide and Errica, Federico and Micheli, Alessio},
  year = {2018},
  month = may,
  abstract = {We introduce the Contextual Graph Markov Model, an approach combining ideas from generative models and neural networks for the processing of graph data. It founds on a constructive methodology to build a deep architecture comprising layers of probabilistic models that learn to encode the structured information in an incremental fashion. Context is diffused in an efficient and scalable way across the graph vertexes and edges. The resulting graph encoding is used in combination with discriminative models to address structure classification benchmarks.},
  keywords = {DGN,unipi},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EKU3RBEU\\Bacciu, Errica, Micheli - 2018 - Contextual Graph Markov Model A Deep and Generative Approach to Graph Processing(3).pdf}
}

@inproceedings{bacciuDeepTreeTransductions2019,
  title = {Deep {{Tree Transductions}} - {{A Short Survey}}},
  booktitle = {Proceedings of the 2019 {{INNS Big Data}} and {{Deep Learning}} ({{INNSBDDL}} 2019)},
  author = {Bacciu, Davide and Bruno, Antonio},
  year = {2019},
  keywords = {tree,unipi}
}

@article{bacciuExperienceUsingMachine2017,
  title = {An Experience in Using Machine Learning for Short-Term Predictions in Smart Transportation Systems},
  author = {Bacciu, Davide and Carta, Antonio and Gnesi, Stefania and Semini, Laura},
  year = {2017},
  journal = {Journal of Logical and Algebraic Methods in Programming},
  volume = {87},
  pages = {52--66},
  doi = {10.1016/j.jlamp.2016.11.002}
}

@inproceedings{bacciuFederatedReservoirComputing2021,
  title = {Federated {{Reservoir Computing Neural Networks}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Bacciu, Davide and Di Sarli, Daniele and Faraji, Pouria and Gallicchio, Claudio and Micheli, Alessio},
  year = {2021},
  month = jul,
  pages = {1--7},
  issn = {2161-4407},
  doi = {10.1109/IJCNN52387.2021.9534035},
  abstract = {A critical aspect in Federated Learning is the aggregation strategy for the combination of multiple models, trained on the edge, into a single model that incorporates all the knowledge in the federation. Common Federated Learning approaches for Recurrent Neural Networks (RNNs) do not provide guarantees on the predictive performance of the aggregated model. In this paper we show how the use of Echo State Networks (ESNs), which are efficient state-of-the-art RNN models for time-series processing, enables a form of federation that is optimal in the sense that it produces models mathematically equivalent to the corresponding centralized model. Furthermore, the proposed method is compliant with privacy constraints. The proposed method, which we denote as Incremental Federated Learning, is experimentally evaluated against an averaging strategy on two datasets for human state and activity recognition.},
  keywords = {Collaborative work,Computational modeling,esn,federated,Predictive models,Reservoirs,rnn,RNN,teaching,Training,Training data},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Bacciu et al_2021_Federated Reservoir Computing Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\CJ3Y823K\\9534035.html}
}

@inproceedings{bacciuFunctionalSafetyCompliance2021,
  title = {Towards {{Functional Safety Compliance}} of {{Recurrent Neural Networks}}},
  booktitle = {Proceedings of the 1st {{International Conference}} on {{AI}} for {{People}}: {{Towards Sustainable AI}}, {{CAIP}} 2021, 20-24 {{November}} 2021, {{Bologna}}, {{Italy}}},
  author = {Bacciu, Davide and Carta, Antonio and Sarli, Daniele Di and Gallicchio, Claudio and Lomonaco, Vincenzo and Petroni, Salvatore},
  year = {2021},
  month = dec,
  abstract = {Deploying Autonomous Driving systems requires facing some novel challenges for the Automotive industry. One of the most critical aspects that can severely compromise their deployment is Functional Safety. The ISO 26262 standard provides guidelines to ensure Functional Safety of road vehicles. Howeve},
  isbn = {978-1-63190-326-7},
  keywords = {dependability,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H3XJL2JT\\Bacciu et al_2021_Towards Functional Safety Compliance of Recurrent Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ICSMSDD8\\eai.20-11-2021.html}
}

@inproceedings{bacciuFunctionalSafetyCompliance2021a,
  title = {Towards {{Functional Safety Compliance}} of {{Recurrent Neural Networks}}},
  booktitle = {Proceedings of the 1st {{International Conference}} on {{AI}} for {{People}}: {{Towards Sustainable AI}}, {{CAIP}} 2021, 20-24 {{November}} 2021, {{Bologna}}, {{Italy}}},
  author = {Bacciu, Davide and Carta, Antonio and Sarli, Daniele Di and Gallicchio, Claudio and Lomonaco, Vincenzo and Petroni, Salvatore},
  year = {2021},
  month = dec,
  abstract = {Deploying Autonomous Driving systems requires facing some novel challenges for the Automotive industry. One of the most critical aspects that can severely compromise their deployment is Functional Safety. The ISO 26262 standard provides guidelines to ensure Functional Safety of road vehicles. Howeve},
  isbn = {978-1-63190-326-7},
  keywords = {dependability,RNN},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Bacciu et al_2021_Towards Functional Safety Compliance of Recurrent Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\8UPVP8DE\\eai.20-11-2021.html}
}

@inproceedings{bacciuLinearMemoryNetworks2019,
  title = {Linear {{Memory Networks}}},
  booktitle = {{{ICANN}}},
  author = {Bacciu, Davide and Carta, Antonio and Sperduti, Alessandro},
  year = {2019}
}

@article{bacciuTEACHINGTrustworthyAutonomous2021,
  title = {{{TEACHING}} -- {{Trustworthy}} Autonomous Cyber-Physical Applications through Human-Centred Intelligence},
  author = {Bacciu, Davide and Akarmazyan, Siranush and Armengaud, Eric and Bacco, Manlio and Bravos, George and Calandra, Calogero and Carlini, Emanuele and Carta, Antonio and Cassara, Pietro and Coppola, Massimo and Davalas, Charalampos and Dazzi, Patrizio and Degennaro, Maria Carmela and Di Sarli, Daniele and Dobaj, J{\"u}rgen and Gallicchio, Claudio and Girbal, Sylvain and Gotta, Alberto and Groppo, Riccardo and Lomonaco, Vincenzo and Macher, Georg and Mazzei, Daniele and Mencagli, Gabriele and Michail, Dimitrios and Micheli, Alessio and Peroglio, Roberta and Petroni, Salvatore and Potenza, Rosaria and Pourdanesh, Farank and Sardianos, Christos and Tserpes, Konstantinos and Tagliab{\`o}, Fulvio and Valtl, Jakob and Varlamis, Iraklis and Veledar, Omar},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.06543 [cs]},
  eprint = {2107.06543},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper discusses the perspective of the H2020 TEACHING project on the next generation of autonomous applications running in a distributed and highly heterogeneous environment comprising both virtual and physical resources spanning the edge-cloud continuum. TEACHING puts forward a human-centred vision leveraging the physiological, emotional, and cognitive state of the users as a driver for the adaptation and optimization of the autonomous applications. It does so by building a distributed, embedded and federated learning system complemented by methods and tools to enforce its dependability, security and privacy preservation. The paper discusses the main concepts of the TEACHING approach and singles out the main AI-related research challenges associated with it. Further, we provide a discussion of the design choices for the TEACHING system to tackle the aforementioned challenges},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SVR6LTDY\\h2020Teaching_conceptpaper.pdf}
}

@inproceedings{bacciuTEACHINGTrustworthyAutonomous2021a,
  title = {{{TEACHING}} - {{Trustworthy}} Autonomous Cyber-Physical Applications through Human-Centred Intelligence},
  booktitle = {2021 {{IEEE International Conference}} on {{Omni-Layer Intelligent Systems}} ({{COINS}})},
  author = {Bacciu, Davide and Akarmazyan, Siranush and Armengaud, Eric and Bacco, Manlio and Bravos, George and Calandra, Calogero and Carlini, Emanuele and Carta, Antonio and Cassar{\`a}, Pietro and Coppola, Massimo and Davalas, Charalampos and Dazzi, Patrizio and Degennaro, Maria Carmela and Di Sarli, Daniele and Dobaj, Juergen and Gallicchio, Claudio and Girbal, Sylvain and Gotta, Alberto and Groppo, Riccardo and Lomonaco, Vincenzo and Macher, Georg and Mazzei, Daniele and Mencagli, Gabriele and Michail, Dimitrios and Micheli, Alessio and Peroglio, Roberta and Petroni, Salvatore and Potenza, Rosaria and Pourdanesh, Farank and Sardianos, Christos and Tserpes, Konstantinos and Tagliab{\'o}, Fulvio and Valtl, Jakob and Varlamis, Iraklis and Veledar, Omar},
  year = {2021},
  month = aug,
  pages = {1--6},
  doi = {10.1109/COINS51742.2021.9524099},
  abstract = {This paper discusses the perspective of the H2020 TEACHING project on the next generation of autonomous applications running in a distributed and highly heterogeneous environment comprising both virtual and physical resources spanning the edge-cloud continuum. TEACHING puts forward a human-centred vision leveraging the physiological, emotional, and cognitive state of the users as a driver for the adaptation and optimization of the autonomous applications. It does so by building a distributed, embedded and federated learning system complemented by methods and tools to enforce its dependability, security and privacy preservation. The paper discusses the main concepts of the TEACHING approach and singles out the main AI-related research challenges associated with it. Further, we provide a discussion of the design choices for the TEACHING system to tackle the aforementioned challenges},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Bacciu et al_2021_TEACHING - Trustworthy autonomous cyber-physical applications through.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NITCFYVI\\9524099.html}
}

@techreport{BACKPACKPACKINGMORE,
  title = {{{BACKPACK}}: {{PACKING MORE INTO BACKPROP}}},
  abstract = {Automatic differentiation frameworks are optimized for exactly one thing: computing the average mini-batch gradient. Yet, other quantities such as the variance of the mini-batch gradients or many approximations to the Hessian can, in theory, be computed efficiently, and at the same time as the gradient. While these quantities are of great interest to researchers and practitioners, current deep-learning software does not support their automatic calculation. Manually implementing them is burdensome, inefficient if done na\textasciidieresis\i velyna\textasciidieresis\i vely, and the resulting code is rarely shared. This hampers progress in deep learning, and unnecessarily narrows research to focus on gradient descent and its variants; it also complicates repli-cation studies and comparisons between newly developed methods that require those quantities, to the point of impossibility. To address this problem, we introduce BACKPACK 1 , an efficient framework built on top of PYTORCH, that extends the backpropagation algorithm to extract additional information from first-and second-order derivatives. Its capabilities are illustrated by benchmark reports for computing additional quantities on deep neural networks, and an example application by testing several recent curvature approximations for optimization.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EUX2XC77\\Unknown - Unknown - BACKPACK PACKING MORE INTO BACKPROP(2).pdf}
}

@inproceedings{baekCommonalityNaturalImages2022,
  title = {Commonality in {{Natural Images Rescues GANs}}: {{Pretraining GANs With Generic}} and {{Privacy-Free Synthetic Data}}},
  shorttitle = {Commonality in {{Natural Images Rescues GANs}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Baek, Kyungjune and Shim, Hyunjung},
  year = {2022},
  pages = {7854--7864},
  langid = {english},
  keywords = {data-free,exmodel},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7D6BC6G8\\Baek_Commonality_in_Natural_Images_Rescues_GANs_Pretraining_GANs_With_Generic_CVPR_2022_paper.html}
}

@article{bahdanauNeuralMachineTranslation2014,
  ids = {bahdanauNeuralMachineTranslation2014a},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.0473},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {attention,LSTM,nlp,nlp-translation,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3QRC82RF\\Bahdanau, Cho, Bengio - 2014 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ECXXHP7B\\nmt jointly align and translate.pdf}
}

@article{bahuleyanVariationalAttentionSequencetoSequence2017,
  title = {Variational {{Attention}} for {{Sequence-to-Sequence Models}}},
  author = {Bahuleyan, Hareesh and Mou, Lili and Vechtomova, Olga and Poupart, Pascal},
  year = {2017},
  month = dec,
  abstract = {The variational encoder-decoder (VED) encodes source information as a set of random variables using a neural network, which in turn is decoded into target data using another neural network. In natural language processing, sequence-to-sequence (Seq2Seq) models typically serve as encoder-decoder networks. When combined with a traditional (deterministic) attention mechanism, the variational latent space may be bypassed by the attention model, and thus becomes ineffective. In this paper, we propose a variational attention mechanism for VED, where the attention vector is also modeled as Gaussian distributed random variables. Results on two experiments show that, without loss of quality, our proposed method alleviates the bypassing phenomenon as it increases the diversity of generated sentences.},
  keywords = {attention,RNN,seq2seq,vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ATTUFSFH\\Bahuleyan et al. - 2017 - Variational Attention for Sequence-to-Sequence Models(3).pdf}
}

@article{baiDeepEquilibriumModels2019,
  title = {Deep {{Equilibrium Models}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  year = {2019},
  month = oct,
  journal = {arXiv:1909.01377 [cs, stat]},
  eprint = {1909.01377},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective "depth" of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88\% memory reduction in our experiments. The code is available at https://github.com/locuslab/deq .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HQY6KPTX\\Bai et al_2019_Deep Equilibrium Models.pdf;C\:\\Users\\w-32\\Zotero\\storage\\73FHEE3U\\1909.html}
}

@article{baiEmpiricalEvaluationGeneric2018,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  year = {2018},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GXRGPDZI\\Bai, Kolter, Koltun - 2018 - An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf}
}

@article{balanLipschitzPropertiesDeep2017,
  title = {Lipschitz {{Properties}} for {{Deep Convolutional Networks}}},
  author = {Balan, Radu and Singh, Maneesh and Zou, Dongmian},
  year = {2017},
  month = jan,
  abstract = {In this paper we discuss the stability properties of convolutional neural networks. Convolutional neural networks are widely used in machine learning. In classification they are mainly used as feature extractors. Ideally, we expect similar features when the inputs are from the same class. That is, we hope to see a small change in the feature vector with respect to a deformation on the input signal. This can be established mathematically, and the key step is to derive the Lipschitz properties. Further, we establish that the stability results can be extended for more general networks. We give a formula for computing the Lipschitz bound, and compare it with other methods to show it is closer to the optimal value.},
  keywords = {CNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\J2WA76CI\\Balan, Singh, Zou - 2017 - Lipschitz Properties for Deep Convolutional Networks.pdf}
}

@article{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  journal = {CoRR},
  volume = {abs/1607.0},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  keywords = {regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UPJ5Z6QA\\Ba, Kiros, Hinton - 2016 - Layer Normalization.pdf}
}

@inproceedings{baldiBidirectionalDynamicsProtein2000,
  title = {Bidirectional {{Dynamics}} for {{Protein Secondary Structure Prediction}}},
  booktitle = {Sequence {{Learning}}},
  author = {Baldi, Pierre and Brunak, S{\o}ren and Frasconi, Paolo and Pollastri, Gianluca and Soda, Giovanni},
  year = {2000},
  keywords = {BIOINF}
}

@article{baldiJetSubstructureClassification2016,
  title = {Jet Substructure Classification in High-Energy Physics with Deep Neural Networks},
  author = {Baldi, Pierre and Bauer, Kevin and Eng, Clara and Sadowski, Peter and Whiteson, Daniel},
  year = {2016},
  journal = {Physical Review D},
  volume = {93},
  number = {9},
  pages = {94034--94034},
  keywords = {high-energy-physics}
}

@article{baldiNeuralNetworksPrincipal1989,
  title = {Neural Networks and Principal Component Analysis: {{Learning}} from Examples without Local Minima},
  author = {Baldi, Pierre and Hornik, Kurt},
  year = {1989},
  journal = {Neural Networks},
  volume = {2},
  number = {1},
  pages = {53--58},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90014-2},
  abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed. \textcopyright{} 1989.},
  keywords = {PCA}
}

@article{ballesGradientMatchingCoresetsRehearsalBased2022,
  title = {Gradient-{{Matching Coresets}} for {{Rehearsal-Based Continual Learning}}},
  author = {Balles, Lukas and Zappella, Giovanni and Archambeau, C{\'e}dric},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.14544 [cs]},
  eprint = {2203.14544},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The goal of continual learning (CL) is to efficiently update a machine learning model with new data without forgetting previously-learned knowledge. Most widely-used CL methods rely on a rehearsal memory of data points to be reused while training on new data. Curating such a rehearsal memory to maintain a small, informative subset of all the data seen so far is crucial to the success of these methods. We devise a coreset selection method for rehearsal-based continual learning. Our method is based on the idea of gradient matching: The gradients induced by the coreset should match, as closely as possible, those induced by the original training dataset. Inspired by the neural tangent kernel theory, we perform this gradient matching across the model's initialization distribution, allowing us to extract a coreset without having to train the model first. We evaluate the method on a wide range of continual learning scenarios and demonstrate that it improves the performance of rehearsal-based CL methods compared to competing memory management strategies such as reservoir sampling.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,continual,notag,sample-selectin,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3ZE2YJCC\\2203.html}
}

@misc{baoBEiTBERTPreTraining2021,
  title = {{{BEiT}}: {{BERT Pre-Training}} of {{Image Transformers}}},
  shorttitle = {{{BEiT}}},
  author = {Bao, Hangbo and Dong, Li and Wei, Furu},
  year = {2021},
  month = jun,
  number = {arXiv:2106.08254},
  eprint = {2106.08254},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.08254},
  abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
  archiveprefix = {arXiv},
  keywords = {notag,pretraining,vision},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FX8S2MEA\\Bao et al_2021_BEiT.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VMLBSGZW\\2106.html}
}

@inproceedings{baradadjurjoLearningSeeLooking2021,
  ids = {baradadLearningSeeLooking2021},
  title = {Learning to {{See}} by {{Looking}} at {{Noise}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Baradad Jurjo, Manel and Wulff, Jonas and Wang, Tongzhou and Isola, Phillip and Torralba, Antonio},
  year = {2021},
  volume = {34},
  eprint = {2106.05963},
  eprinttype = {arxiv},
  pages = {2556--2569},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Current vision systems are trained on huge datasets, and these datasets come with costs: curation is expensive, they inherit human biases, and there are concerns over privacy and usage rights. To counter these costs, interest has surged in learning from cheaper data sources, such as unlabeled images. In this paper we go a step further and ask if we can do away with real image datasets entirely, instead learning from procedural noise processes. We investigate a suite of image generation models that produce images from simple random processes. These are then used as training data for a visual representation learner with a contrastive loss. In particular, we study statistical image models, randomly initialized deep generative models, and procedural graphics models.Our findings show that it is important for the noise to capture certain structural properties of real data but that good performance can be achieved even with processes that are far from realistic. We also find that diversity is a key property to learn good representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,continual,data-free,exmodel},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Baradad et al_2021_Learning to See by Looking at Noise.pdf;C\:\\Users\\w-32\\Zotero\\storage\\2EM6NQ74\\2106.html}
}

@article{barberBayesianReasoningMachine2011,
  title = {Bayesian {{Reasoning}} and {{Machine Learning}}},
  author = {Barber, David},
  year = {2011},
  issn = {9780511804779},
  doi = {10.1017/CBO9780511804779},
  abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IGKLARCM\\Barber - 2011 - Bayesian Reasoning and Machine Learning(2).pdf}
}

@book{barberBayesianReasoningMachine2012,
  title = {Bayesian {{Reasoning}} and {{Machine Learning}}},
  author = {Barber, David},
  year = {2012},
  month = jun,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511804779},
  abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  isbn = {978-0-521-51814-7 978-0-511-80477-9},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9JGBXRDV\\Barber - 2012 - Bayesian Reasoning and Machine Learning.pdf}
}

@article{Bashivan2016LearningRF,
  title = {Learning Representations from {{EEG}} with Deep Recurrent-Convolutional Neural Networks},
  author = {Bashivan, Pouya and Rish, Irina and Yeasin, M. and Codella, N.},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1511.06448},
  keywords = {human_state_monitoring}
}

@article{bayerLearningStochasticRecurrent2014,
  title = {Learning {{Stochastic Recurrent Networks}}},
  author = {Bayer, Justin and Osendorfer, Christian},
  year = {2014},
  month = nov,
  abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
  keywords = {RNN,vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\73EIV8R3\\Bayer, Osendorfer - 2014 - Learning Stochastic Recurrent Networks(3).pdf}
}

@inproceedings{bayEVALUATIONMULTIPLEF0ESTIMATION2009,
  title = {{{EVALUATION OF MULTIPLE-F0 ESTIMATION AND TRACKING SYSTEMS}}},
  booktitle = {{{ISMIR}}},
  author = {Bay, Mert and Ehmann, Andreas F and Downie, J Stephen},
  year = {2009},
  abstract = {Multi-pitch estimation of sources in music is an ongoing research area that has a wealth of applications in music in-formation retrieval systems. This paper presents the sys-tematic evaluations of over a dozen competing methods and algorithms for extracting the fundamental frequencies of pitched sound sources in polyphonic music. The eval-uations were carried out as part of the Music Information Retrieval Evaluation eXchange (MIREX) over the course of two years, from 2007 to 2008. The generation of the dataset and its corresponding ground-truth, the methods by which systems can be evaluated, and the evaluation results of the different systems are presented and discussed.},
  keywords = {music}
}

@article{beerOneStepBack2019,
  title = {One Step Back, Two Steps Forward: Interference and Learning in Recurrent Neural Networks},
  shorttitle = {One Step Back, Two Steps Forward},
  author = {Beer, Chen and Barak, Omri},
  year = {2019},
  month = may,
  journal = {arXiv:1805.09603 [q-bio]},
  eprint = {1805.09603},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {Artificial neural networks, trained to perform cognitive tasks, have recently been used as models for neural recordings from animals performing these tasks. While some progress has been made in performing such comparisons, the evolution of network dynamics throughout learning remains unexplored. This is paralleled by an experimental focus on recording from trained animals, with few studies following neural activity throughout training. In this work, we address this gap in the realm of artificial networks by analyzing networks that are trained to perform memory and pattern generation tasks. The functional aspect of these tasks corresponds to dynamical objects in the fully trained network - a line attractor or a set of limit cycles for the two respective tasks. We use these dynamical objects as anchors to study the effect of learning on their emergence. We find that the sequential nature of learning has major consequences for the learning trajectory and its final outcome. Specifically, we show that Least Mean Squares (LMS), a simple gradient descent suggested as a biologically plausible version of the FORCE algorithm, is constantly obstructed by forgetting, which is manifested as the destruction of dynamical objects from previous trials. The degree of interference is determined by the correlation between different trials. We show which specific ingredients of FORCE avoid this phenomenon. Overall, this difference results in convergence that is orders of magnitude slower for LMS. Learning implies accumulating information across multiple trials to form the overall concept of the task. Our results show that interference between trials can greatly affect learning, in a learning rule dependent manner. These insights can help design experimental protocols that minimize such interference, and possibly infer underlying learning rules by observing behavior and neural activity throughout learning.},
  archiveprefix = {arXiv},
  keywords = {neuroscience,Quantitative Biology - Neurons and Cognition,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JXVGJ9T4\\Beer_Barak_2019_One step back, two steps forward.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EWCS8KSF\\1805.html}
}

@article{behrmannInvertibleResidualNetworks2018,
  title = {Invertible {{Residual Networks}}},
  author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, J{\"o}rn-Henrik},
  year = {2018},
  abstract = {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\I3CEU4UB\\Behrmann et al. - 2018 - Invertible Residual Networks.pdf}
}

@article{belangerLinearDynamicalSystem,
  title = {A {{Linear Dynamical System Model}} for {{Text}}},
  author = {Belanger, David and Kakade, Sham},
  pages = {10},
  abstract = {Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words' local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words' representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple cooccurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.},
  langid = {english},
  keywords = {ICML,Kalman,LDS,linear,nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\E94G6ULE\\Belanger and Kakade - A Linear Dynamical System Model for Text.pdf}
}

@article{bellavistaDecentralisedLearningFederated2021,
  title = {Decentralised {{Learning}} in {{Federated Deployment Environments}}: {{A System-Level Survey}}},
  shorttitle = {Decentralised {{Learning}} in {{Federated Deployment Environments}}},
  author = {Bellavista, Paolo and Foschini, Luca and Mora, Alessio},
  year = {2021},
  month = feb,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {1},
  pages = {15:1--15:38},
  issn = {0360-0300},
  doi = {10.1145/3429252},
  abstract = {Decentralised learning is attracting more and more interest because it embodies the principles of data minimisation and focused data collection, while favouring the transparency of purpose specification (i.e., the objective for which a model is built). Cloud-centric-only processing and deep learning are no longer strict necessities to train high-fidelity models; edge devices can actively participate in the decentralised learning process by exchanging meta-level information in place of raw data, thus paving the way for better privacy guarantees. In addition, these new possibilities can relieve the network backbone from unnecessary data transfer and allow it to meet strict low-latency requirements by leveraging on-device model inference. This survey provides a detailed and up-to-date overview of the most recent contributions available in the state-of-the-art decentralised learning literature. In particular, it originally provides the reader audience with a clear presentation of the peculiarities of federated settings, with a novel taxonomy of decentralised learning approaches, and with a detailed description of the most relevant and specific system-level contributions of the surveyed solutions for privacy, communication efficiency, non-IIDness, device heterogeneity, and poisoning defense.},
  keywords = {decentralized,federated,review,toread}
}

@article{belouadahIL2MClassIncremental,
  title = {{{IL2M}}: {{Class Incremental Learning With Dual Memory}}},
  author = {Belouadah, Eden and Popescu, Adrian},
  pages = {10},
  abstract = {This paper presents a class incremental learning (IL) method which exploits fine tuning and a dual memory to reduce the negative effect of catastrophic forgetting in image recognition. First, we simplify the current fine tuning based approaches which use a combination of classification and distillation losses to compensate for the limited availability of past data. We find that the distillation term actually hurts performance when a memory is allowed. Then, we modify the usual class IL memory component. Similar to existing works, a first memory stores exemplar images of past classes. A second memory is introduced here to store past class statistics obtained when they were initially learned. The intuition here is that classes are best modeled when all their data are available and that their initial statistics are useful across different incremental states. A prediction bias towards newly learned classes appears during inference because the dataset is imbalanced in their favor. The challenge is to make predictions of new and past classes more comparable. To do this, scores of past classes are rectified by leveraging contents from both memories. The method has negligible added cost, both in terms of memory and of inference complexity. Experiments with three large public datasets show that the proposed approach is more effective than a range of competitive state-of-the-art methods.},
  langid = {english},
  keywords = {cl-replay,class-incremental,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PQNV8Z6K\\Belouadah and Popescu - IL2M Class Incremental Learning With Dual Memory.pdf}
}

@article{belouadahInitialClassifierWeights2020,
  title = {Initial {{Classifier Weights Replay}} for {{Memoryless Class Incremental Learning}}},
  author = {Belouadah, Eden and Popescu, Adrian and Kanellos, Ioannis},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.13710 [cs]},
  eprint = {2008.13710},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Incremental Learning (IL) is useful when artificial systems need to deal with streams of data and do not have access to all data at all times. The most challenging setting requires a constant complexity of the deep model and an incremental model update without access to a bounded memory of past data. Then, the representations of past classes are strongly affected by catastrophic forgetting. To mitigate its negative effect, an adapted fine tuning which includes knowledge distillation is usually deployed. We propose a different approach based on a vanilla fine tuning backbone. It leverages initial classifier weights which provide a strong representation of past classes because they are trained with all class data. However, the magnitude of classifiers learned in different states varies and normalization is needed for a fair handling of all classes. Normalization is performed by standardizing the initial classifier weights, which are assumed to be normally distributed. In addition, a calibration of prediction scores is done by using state level statistics to further improve classification fairness. We conduct a thorough evaluation with four public datasets in a memoryless incremental learning setting. Results show that our method outperforms existing techniques by a large margin for large-scale datasets.},
  archiveprefix = {arXiv},
  keywords = {cl-regularization,class-incremental,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8TMZAFX4\\Belouadah et al_2020_Initial Classifier Weights Replay for Memoryless Class Incremental Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\C3MJBU2M\\2008.html}
}

@inproceedings{bendaleOpenWorldRecognition2015,
  title = {Towards {{Open World Recognition}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Bendale, Abhijit and Boult, Terrance},
  year = {2015},
  month = jun,
  pages = {1893--1902},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2015.7298799},
  abstract = {With the of advent rich classification models and high computational power visual recognition systems have found many operational applications. Recognition in the real world poses multiple challenges that are not apparent in controlled lab environments. The datasets are dynamic and novel categories must be continuously detected and then added. At prediction time, a trained system has to deal with myriad unseen categories. Operational systems require minimal downtime, even to learn. To handle these operational issues, we present the problem of Open World Recognition and formally define it. We prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance ``open space risk'' and empirical risk. Our theory extends existing algorithms for open world recognition. We present a protocol for evaluation of open world recognition systems. We present the Nearest Non-Outlier (NNO) algorithm that evolves model efficiently, adding object categories incrementally while detecting outliers and managing open space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to validate the effectiveness of our method on large scale visual recognition tasks. NNO consistently yields superior results on open world recognition.},
  keywords = {open-world},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8ESVK94B\\Bendale_Boult_2015_Towards Open World Recognition.pdf;C\:\\Users\\w-32\\Zotero\\storage\\FG2ZE5T2\\7298799.html}
}

@inproceedings{bengioCurriculumLearning2009,
  title = {Curriculum Learning},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  author = {Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  year = {2009},
  pages = {41--48},
  publisher = {{ACM}},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\47SYXF6J\\curriculum learning.pdf}
}

@article{bengioEstimatingPropagatingGradients2013,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  year = {2013},
  month = aug,
  journal = {arXiv preprint arXiv:1308.3432},
  eprint = {1308.3432},
  eprinttype = {arxiv},
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{\textbackslash em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  archiveprefix = {arXiv},
  keywords = {discrete-backprop},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\95QIFYD2\\Bengio, Lonard, Courville - 2013 - Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation(2).pdf}
}

@inproceedings{bengioGreedyLayerwiseTraining2007,
  title = {Greedy Layer-Wise Training of Deep Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  year = {2007},
  pages = {153--160},
  keywords = {autoencoders}
}

@article{bengioLearningLongtermDependencies1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  year = {1994},
  journal = {IEEE transactions on neural networks},
  volume = {5},
  number = {2},
  pages = {157--166},
  keywords = {learning,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IJMYCNQN\\Bengio, Simard, Frasconi - 1994 - Learning long-term dependencies with gradient descent is difficult(2).pdf}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  journal = {Journal of machine learning research},
  volume = {3},
  number = {Feb},
  pages = {1137--1155},
  keywords = {nlp}
}

@article{bergstraRandomSearchHyperparameter2012,
  ids = {bergstraRandomSearchHyperparameter2012a},
  title = {Random Search for Hyper-Parameter Optimization},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {13},
  number = {Feb},
  pages = {281--305},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QRHPEDBY\\Random Search for Hyperparameter - bergstra12a.pdf}
}

@article{bertugliFewShotUnsupervisedContinual2020,
  title = {Few-{{Shot Unsupervised Continual Learning}} through {{Meta-Examples}}},
  author = {Bertugli, Alessia and Vincenzi, Stefano and Calderara, Simone and Passerini, Andrea},
  year = {2020},
  month = nov,
  journal = {arXiv:2009.08107 [cs, stat]},
  eprint = {2009.08107},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In real-world applications, data do not reflect the ones commonly used for neural networks training, since they are usually few, unlabeled and can be available as a stream. Hence many existing deep learning solutions suffer from a limited range of applications, in particular in the case of online streaming data that evolve over time. To narrow this gap, in this work we introduce a novel and complex setting involving unsupervised meta-continual learning with unbalanced tasks. These tasks are built through a clustering procedure applied to a fitted embedding space. We exploit a meta-learning scheme that simultaneously alleviates catastrophic forgetting and favors the generalization to new tasks. Moreover, to encourage feature reuse during the meta-optimization, we exploit a single inner loop taking advantage of an aggregated representation achieved through the use of a self-attention mechanism. Experimental results on few-shot learning benchmarks show competitive performance even compared to the supervised case. Additionally, we empirically observe that in an unsupervised scenario, the small tasks and the variability in the clusters pooling play a crucial role in the generalization capability of the network. Further, on complex datasets, the exploitation of more clusters than the true number of classes leads to higher results, even compared to the ones obtained with full supervision, suggesting that a predefined partitioning into classes can miss relevant structural information.},
  archiveprefix = {arXiv},
  keywords = {continual,notag,toread,unsupervised},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Bertugli et al_2020_Few-Shot Unsupervised Continual Learning through Meta-Examples.pdf;C\:\\Users\\w-32\\Zotero\\storage\\8N9IA72Y\\2009.html}
}

@inproceedings{beyerKnowledgeDistillationGood2022,
  ids = {beyerKnowledgeDistillationGood2021},
  title = {Knowledge {{Distillation}}: {{A Good Teacher Is Patient}} and {{Consistent}}},
  shorttitle = {Knowledge {{Distillation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Beyer, Lucas and Zhai, Xiaohua and Royer, Am{\'e}lie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
  year = {2022},
  eprint = {2106.05237},
  eprinttype = {arxiv},
  pages = {10925--10934},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {exmodel,knowledge distillation,knowledge-distillation},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Beyer et al_2021_Knowledge distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\J9E68JIK\\2106.html;C\:\\Users\\w-32\\Zotero\\storage\\VRP4IZQL\\Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.html}
}

@article{bhardwajDreamDistillationDataIndependent2019,
  title = {Dream {{Distillation}}: {{A Data-Independent Model Compression Framework}}},
  shorttitle = {Dream {{Distillation}}},
  author = {Bhardwaj, Kartikeya and Suda, Naveen and Marculescu, Radu},
  year = {2019},
  month = may,
  journal = {arXiv:1905.07072 [cs, stat]},
  eprint = {1905.07072},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Model compression is eminently suited for deploying deep learning on IoT-devices. However, existing model compression techniques rely on access to the original or some alternate dataset. In this paper, we address the model compression problem when no real data is available, e.g., when data is private. To this end, we propose Dream Distillation, a data-independent model compression framework. Our experiments show that Dream Distillation can achieve 88.5\% accuracy on the CIFAR-10 test set without actually training on the original data!},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,data-distill,data-free,exmodel,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Bhardwaj et al_2019_Dream Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\89VTAZVH\\1905.html}
}

@misc{bhatConsistencyKeyFurther2022,
  title = {Consistency Is the Key to Further Mitigating Catastrophic Forgetting in Continual Learning},
  author = {Bhat, Prashant and Zonooz, Bahram and Arani, Elahe},
  year = {2022},
  month = jul,
  number = {arXiv:2207.04998},
  eprint = {2207.04998},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.04998},
  abstract = {Deep neural networks struggle to continually learn multiple sequential tasks due to catastrophic forgetting of previously learned tasks. Rehearsal-based methods which explicitly store previous task samples in the buffer and interleave them with the current task samples have proven to be the most effective in mitigating forgetting. However, Experience Replay (ER) does not perform well under low-buffer regimes and longer task sequences as its performance is commensurate with the buffer size. Consistency in predictions of soft-targets can assist ER in preserving information pertaining to previous tasks better as soft-targets capture the rich similarity structure of the data. Therefore, we examine the role of consistency regularization in ER framework under various continual learning scenarios. We also propose to cast consistency regularization as a self-supervised pretext task thereby enabling the use of a wide variety of self-supervised learning methods as regularizers. While simultaneously enhancing model calibration and robustness to natural corruptions, regularizing consistency in predictions results in lesser forgetting across all continual learning scenarios. Among the different families of regularizers, we find that stricter consistency constraints preserve previous task information in ER better.},
  archiveprefix = {arXiv},
  keywords = {continual,knowledge-distillation,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5CKANSS2\\2207.html}
}

@inproceedings{bhuniaDoodleItYourself2022,
  title = {Doodle {{It Yourself}}: {{Class Incremental Learning}} by {{Drawing}} a {{Few Sketches}}},
  shorttitle = {Doodle {{It Yourself}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Bhunia, Ayan Kumar and Gajjala, Viswanatha Reddy and Koley, Subhadeep and Kundu, Rohit and Sain, Aneeshan and Xiang, Tao and Song, Yi-Zhe},
  year = {2022},
  pages = {2293--2302},
  langid = {english},
  keywords = {Continual,domain-shift,knowledge-distillation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9BES2S8L\\Bhunia_Doodle_It_Yourself_Class_Incremental_Learning_by_Drawing_a_Few_CVPR_2022_paper.html}
}

@article{bianchiInvestigatingEchoStateNetworks2018,
  title = {Investigating {{Echo-State Networks Dynamics}} by {{Means}} of {{Recurrence Analysis}}},
  author = {Bianchi, Filippo Maria and Livi, Lorenzo and Alippi, Cesare},
  year = {2018},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {2},
  pages = {427--439},
  doi = {10.1109/TNNLS.2016.2630802},
  abstract = {In this paper, we elaborate over the well-known interpretability issue in echo-state networks (ESNs). The idea is to investigate the dynamics of reservoir neurons with time-series analysis techniques developed in complex systems research. Notably, we analyze time series of neuron activations with recurrence plots (RPs) and recurrence quantification analysis (RQA), which permit to visualize and characterize high-dimensional dynamical systems. We show that this approach is useful in a number of ways. First, the 2-D representation offered by RPs provides a visualization of the high-dimensional reservoir dynamics. Our results suggest that, if the network is stable, reservoir and input generate similar line patterns in the respective RPs. Conversely, as the ESN becomes unstable, the patterns in the RP of the reservoir change. As a second result, we show that an RQA measure, called \$L-\textbackslash mathrm \{max\}\$ , is highly correlated with the well-established maximal local Lyapunov exponent. This suggests that complexity measures based on RP diagonal lines distribution can quantify network stability. Finally, our analysis shows that all RQA measures fluctuate on the proximity of the so-called edge of stability, where an ESN typically achieves maximum computational capability. We leverage on this property to determine the edge of stability and show that our criterion is more accurate than two well-known counterparts, both based on the Jacobian matrix of the reservoir. Therefore, we claim that RPs and RQA-based analyses are valuable tools to design an ESN, given a specific problem.},
  keywords = {ESN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WDJR3RNB\\Bianchi, Livi, Alippi - 2018 - Investigating Echo-State Networks Dynamics by Means of Recurrence Analysis.pdf}
}

@incollection{bianUnsupervisedScaleconsistentDepth2019,
  title = {Unsupervised {{Scale-consistent Depth}} and {{Ego-motion Learning}} from {{Monocular Video}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Bian, Jiawang and Li, Zhichao and Wang, Naiyan and Zhan, Huangying and Shen, Chunhua and Cheng, Ming-Ming and Reid, Ian},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {35--45},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MPRNHXTU\\Bian et al. - 2019 - Unsupervised Scale-consistent Depth and Ego-motion.pdf;C\:\\Users\\w-32\\Zotero\\storage\\K732JK9U\\8299-unsupervised-scale-consistent-depth-and-ego-motion-learning-from-monocular-video.html}
}

@article{biniciPreventingCatastrophicForgetting2021,
  title = {Preventing {{Catastrophic Forgetting}} and {{Distribution Mismatch}} in {{Knowledge Distillation}} via {{Synthetic Data}}},
  author = {Binici, Kuluhan and Pham, Nam Trung and Mitra, Tulika and Leman, Karianto},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.05698 [cs]},
  eprint = {2108.05698},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {With the increasing popularity of deep learning on edge devices, compressing large neural networks to meet the hardware requirements of resource-constrained devices became a significant research direction. Numerous compression methodologies are currently being used to reduce the memory sizes and energy consumption of neural networks. Knowledge distillation (KD) is among such methodologies and it functions by using data samples to transfer the knowledge captured by a large model (teacher) to a smaller one(student). However, due to various reasons, the original training data might not be accessible at the compression stage. Therefore, data-free model compression is an ongoing research problem that has been addressed by various works. In this paper, we point out that catastrophic forgetting is a problem that can potentially be observed in existing data-free distillation methods. Moreover, the sample generation strategies in some of these methods could result in a mismatch between the synthetic and real data distributions. To prevent such problems, we propose a data-free KD framework that maintains a dynamic collection of generated samples over time. Additionally, we add the constraint of matching the real data distribution in sample generation strategies that target maximum information gain. Our experiments demonstrate that we can improve the accuracy of the student models obtained via KD when compared with state-of-the-art approaches on the SVHN, Fashion MNIST and CIFAR100 datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,data-free,exmodel,generative,knowledge distillation},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Binici et al_2021_Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge.pdf;C\:\\Users\\w-32\\Zotero\\storage\\HIW8SV6Y\\2108.html}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {book,Machine learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\36RF3E36\\Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{blazquez-garciaReviewOutlierAnomaly2020,
  title = {A Review on Outlier/Anomaly Detection in Time Series Data},
  author = {{Bl{\'a}zquez-Garc{\'i}a}, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.04236 [cs, stat]},
  eprint = {2002.04236},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
  archiveprefix = {arXiv},
  keywords = {anomaly-detection,esn},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Blzquez-Garca et al_2020_A review on outlier-anomaly detection in time series data.pdf;C\:\\Users\\w-32\\Zotero\\storage\\UFLAG73K\\2002.html}
}

@misc{blotGossipTrainingDeep2016,
  title = {Gossip Training for Deep Learning},
  author = {Blot, Michael and Picard, David and Cord, Matthieu and Thome, Nicolas},
  year = {2016},
  month = nov,
  number = {arXiv:1611.09726},
  eprint = {1611.09726},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.09726},
  abstract = {We address the issue of speeding up the training of convolutional networks. Here we study a distributed method adapted to stochastic gradient descent (SGD). The parallel optimization setup uses several threads, each applying individual gradient descents on a local variable. We propose a new way to share information between different threads inspired by gossip algorithms and showing good consensus convergence properties. Our method called GoSGD has the advantage to be fully asynchronous and decentralized. We compared our method to the recent EASGD in \textbackslash cite\{elastic\} on CIFAR-10 show encouraging results.},
  archiveprefix = {arXiv},
  keywords = {distributed},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4ZN4WYU5\\Blot et al_2016_Gossip training for deep learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MTTDCZ6U\\1611.html}
}

@misc{bonicelliEffectivenessLipschitzDrivenRehearsal2022,
  title = {On the {{Effectiveness}} of {{Lipschitz-Driven Rehearsal}} in {{Continual Learning}}},
  author = {Bonicelli, Lorenzo and Boschini, Matteo and Porrello, Angelo and Spampinato, Concetto and Calderara, Simone},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06443},
  eprint = {2210.06443},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.06443},
  abstract = {Rehearsal approaches enjoy immense popularity with Continual Learning (CL) practitioners. These methods collect samples from previously encountered data distributions in a small memory buffer; subsequently, they repeatedly optimize on the latter to prevent catastrophic forgetting. This work draws attention to a hidden pitfall of this widespread practice: repeated optimization on a small pool of data inevitably leads to tight and unstable decision boundaries, which are a major hindrance to generalization. To address this issue, we propose Lipschitz-DrivEn Rehearsal (LiDER), a surrogate objective that induces smoothness in the backbone network by constraining its layer-wise Lipschitz constants w.r.t.\textbackslash{} replay examples. By means of extensive experiments, we show that applying LiDER delivers a stable performance gain to several state-of-the-art rehearsal CL methods across multiple datasets, both in the presence and absence of pre-training. Through additional ablative experiments, we highlight peculiar aspects of buffer overfitting in CL and better characterize the effect produced by LiDER. Code is available at https://github.com/aimagelab/LiDER},
  archiveprefix = {arXiv},
  keywords = {cl-replay,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AU36XPYI\\2210.html}
}

@article{bontempsCollectiveAnomalyDetection2017,
  title = {Collective {{Anomaly Detection}} Based on {{Long Short Term Memory Recurrent Neural Network}}},
  author = {Bontemps, Loic and Cao, Van Loi and McDermott, James and {Le-Khac}, Nhien-An},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.09752 [cs]},
  eprint = {1703.09752},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Intrusion detection for computer network systems becomes one of the most critical tasks for network administrators today. It has an important role for organizations, governments and our society due to its valuable resources on computer networks. Traditional misuse detection strategies are unable to detect new and unknown intrusion. Besides, anomaly detection in network security is aim to distinguish between illegal or malicious events and normal behavior of network systems. Anomaly detection can be considered as a classification problem where it builds models of normal network behavior, which it uses to detect new patterns that significantly deviate from the model. Most of the cur- rent research on anomaly detection is based on the learning of normally and anomaly behaviors. They do not take into account the previous, re- cent events to detect the new incoming one. In this paper, we propose a real time collective anomaly detection model based on neural network learning and feature operating. Normally a Long Short Term Memory Recurrent Neural Network (LSTM RNN) is trained only on normal data and it is capable of predicting several time steps ahead of an input. In our approach, a LSTM RNN is trained with normal time series data before performing a live prediction for each time step. Instead of considering each time step separately, the observation of prediction errors from a certain number of time steps is now proposed as a new idea for detecting collective anomalies. The prediction errors from a number of the latest time steps above a threshold will indicate a collective anomaly. The model is built on a time series version of the KDD 1999 dataset. The experiments demonstrate that it is possible to offer reliable and efficient for collective anomaly detection.},
  archiveprefix = {arXiv},
  keywords = {anomaly-detection,RNN},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Bontemps et al_2017_Collective Anomaly Detection based on Long Short Term Memory Recurrent Neural.pdf;C\:\\Users\\w-32\\Zotero\\storage\\C7IX5RMH\\1703.html}
}

@article{borsosCoresetsBilevelOptimization2020,
  title = {Coresets via {{Bilevel Optimization}} for {{Continual Learning}} and {{Streaming}}},
  author = {Borsos, Zal{\'a}n and Mutn{\'y}, Mojm{\'i}r and Krause, Andreas},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.03875 [cs, stat]},
  eprint = {2006.03875},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Coresets are small data summaries that are sufficient for model training. They can be maintained online, enabling efficient handling of large data streams under resource constraints. However, existing constructions are limited to simple models such as k-means and logistic regression. In this work, we propose a novel coreset construction via cardinality-constrained bilevel optimization. We show how our framework can efficiently generate coresets for deep neural networks, and demonstrate its empirical benefits in continual learning and in streaming settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3S9WZE2M\\Borsos et al_2020_Coresets via Bilevel Optimization for Continual Learning and Streaming.pdf;C\:\\Users\\w-32\\Zotero\\storage\\8UKQ99IJ\\2006.html}
}

@article{boschiniClassIncrementalContinualLearning2022,
  title = {Class-{{Incremental Continual Learning}} into the {{eXtended DER-verse}}},
  author = {Boschini, Matteo and Bonicelli, Lorenzo and Buzzega, Pietro and Porrello, Angelo and Calderara, Simone},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2201.00766},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1--16},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3206549},
  abstract = {The staple of human intelligence is the capability of acquiring knowledge in a continuous fashion. In stark contrast, Deep Networks forget catastrophically and, for this reason, the sub-field of Class-Incremental Continual Learning fosters methods that learn a sequence of tasks incrementally, blending sequentially-gained knowledge into a comprehensive prediction. This work aims at assessing and overcoming the pitfalls of our previous proposal Dark Experience Replay (DER), a simple and effective approach that combines rehearsal and Knowledge Distillation. Inspired by the way our minds constantly rewrite past recollections and set expectations for the future, we endow our model with the abilities to i) revise its replay memory to welcome novel information regarding past data ii) pave the way for learning yet unseen classes. We show that the application of these strategies leads to remarkable improvements; indeed, the resulting method - termed eXtended-DER (X-DER) - outperforms the state of the art on both standard benchmarks (such as CIFAR-100 and miniImagenet) and a novel one here introduced. To gain a better understanding, we further provide extensive ablation studies that corroborate and extend the findings of our previous research (e.g. the value of Knowledge Distillation and flatter minima in continual learning setups).},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\A7SAZ3JE\\2201.html}
}

@inproceedings{boulanger-lewandowskiModelingTemporalDependencies2012,
  title = {Modeling {{Temporal Dependencies}} in {{High-Dimensional Sequences}}: {{Application}} to {{Polyphonic Music Generation}} and {{Transcription}}},
  booktitle = {{{ICML}}},
  author = {{Boulanger-Lewandowski}, Nicolas and Bengio, Yoshua and Vincent, Pascal},
  year = {2012},
  abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
  isbn = {978-1-4503-1285-1},
  keywords = {ICML,music,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ATF9CV75\\Boulanger-Lewandowski, Bengio, Vincent - 2012 - Modeling Temporal Dependencies in High-Dimensional Sequences Application to Polyphonic M.pdf}
}

@inproceedings{boydGossipAlgorithmsDesign2005,
  title = {Gossip Algorithms: Design, Analysis and Applications},
  shorttitle = {Gossip Algorithms},
  booktitle = {Proceedings {{IEEE}} 24th {{Annual Joint Conference}} of the {{IEEE Computer}} and {{Communications Societies}}.},
  author = {Boyd, S. and Ghosh, A. and Prabbakar, B. and Shah, D.},
  year = {2005},
  volume = {3},
  pages = {1653--1664},
  publisher = {{IEEE}},
  address = {{Miami, FL, USA}},
  doi = {10.1109/INFCOM.2005.1498447},
  isbn = {978-0-7803-8968-7},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KMQQH62N\\Boyd et al. - 2005 - Gossip algorithms design, analysis and applicatio.pdf}
}

@article{bradburyQuasiRecurrentNeuralNetworks2016,
  title = {Quasi-{{Recurrent Neural Networks}}},
  author = {Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  year = {2016},
  month = nov,
  abstract = {Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.},
  keywords = {efficient,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\22RWKI49\\Bradbury et al. - 2016 - Quasi-Recurrent Neural Networks(2).pdf}
}

@article{bronskillTaskNormRethinkingBatch2020,
  title = {{{TaskNorm}}: {{Rethinking Batch Normalization}} for {{Meta-Learning}}},
  shorttitle = {{{TaskNorm}}},
  author = {Bronskill, John and Gordon, Jonathan and Requeima, James and Nowozin, Sebastian and Turner, Richard E.},
  year = {2020},
  month = jun,
  journal = {arXiv:2003.03284 [cs, stat]},
  eprint = {2003.03284},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,notag,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HF4C54M9\\Bronskill et al_2020_TaskNorm.pdf;C\:\\Users\\w-32\\Zotero\\storage\\3YB7FDUL\\2003.html}
}

@article{burdaLargeScaleStudyCuriosityDriven2018,
  title = {Large-{{Scale Study}} of {{Curiosity-Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  year = {2018},
  month = aug,
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  keywords = {Surprisal},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WU9PJJTI\\Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning(2).pdf}
}

@article{burgessUnderstandingDisentanglingBeta2018,
  title = {Understanding Disentangling in \$\textbackslash beta\$-{{VAE}}},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  year = {2018},
  number = {Nips},
  abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in \$\textbackslash beta\$-VAE, as training progresses. From these insights, we propose a modification to the training regime of \$\textbackslash beta\$-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in \$\textbackslash beta\$-VAE, without the previous trade-off in reconstruction accuracy.},
  keywords = {vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\URUT2M3V\\Burgess et al. - 2018 - Understanding disentangling in $beta$-VAE(2).pdf}
}

@inproceedings{buzzegaDarkExperienceGeneral2020a,
  ids = {buzzegaDarkExperienceGeneral2020},
  title = {Dark {{Experience}} for {{General Continual Learning}}: A {{Strong}}, {{Simple Baseline}}},
  shorttitle = {Dark {{Experience}} for {{General Continual Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and CALDERARA, SIMONE},
  year = {2020},
  volume = {33},
  eprint = {2004.07211},
  eprinttype = {arxiv},
  pages = {15920--15930},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards General Continual Learning (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through mixing rehearsal with knowledge distillation and regularization; our simple baseline, Dark Experience Replay, matches the network's logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on both standard benchmarks and a novel GCL evaluation setting (MNIST-360), we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. We further explore the generalization capabilities of our objective, showing its regularization being beneficial beyond mere performance.},
  archiveprefix = {arXiv},
  keywords = {cl-distillation,cl-online,cl-replay,continual,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Q64VA24E\\2004.07211.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EGSXKY79\\2004.html}
}

@article{buzzegaRethinkingExperienceReplay2020,
  title = {Rethinking {{Experience Replay}}: A {{Bag}} of {{Tricks}} for {{Continual Learning}}},
  shorttitle = {Rethinking {{Experience Replay}}},
  author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Calderara, Simone},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.05595 [cs, stat]},
  eprint = {2010.05595},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In Continual Learning, a Neural Network is trained on a stream of data whose distribution shifts over time. Under these assumptions, it is especially challenging to improve on classes appearing later in the stream while remaining accurate on previous ones. This is due to the infamous problem of catastrophic forgetting, which causes a quick performance degradation when the classifier focuses on learning new categories. Recent literature proposed various approaches to tackle this issue, often resorting to very sophisticated techniques. In this work, we show that naive rehearsal can be patched to achieve similar performance. We point out some shortcomings that restrain Experience Replay (ER) and propose five tricks to mitigate them. Experiments show that ER, thus enhanced, displays an accuracy gain of 51.2 and 26.9 percentage points on the CIFAR-10 and CIFAR-100 datasets respectively (memory buffer size 1000). As a result, it surpasses current state-of-the-art rehearsal-based methods.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,continual,core50,split-cifar10,split-cifar100,split-mnist},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2GP8I6DD\\Buzzega et al_2020_Rethinking Experience Replay.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZJGATQMD\\2010.html}
}

@misc{cacciaAnytimeLearningMacroscale2022,
  title = {On {{Anytime Learning}} at {{Macroscale}}},
  author = {Caccia, Lucas and Xu, Jing and Ott, Myle and Ranzato, Marc'Aurelio and Denoyer, Ludovic},
  year = {2022},
  month = mar,
  number = {arXiv:2106.09563},
  eprint = {2106.09563},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.09563},
  abstract = {In many practical applications of machine learning data arrives sequentially over time in large chunks. Practitioners have then to decide how to allocate their computational budget in order to obtain the best performance at any point in time. Online learning theory for convex optimization suggests that the best strategy is to use data as soon as it arrives. However, this might not be the best strategy when using deep non-linear networks, particularly when these perform multiple passes over each chunk of data rendering the overall distribution non i.i.d.. In this paper, we formalize this learning setting in the simplest scenario in which each data chunk is drawn from the same underlying distribution, and make a first attempt at empirically answering the following questions: How long should the learner wait before training on the newly arrived chunks? What architecture should the learner adopt? Should the learner increase capacity over time as more data is observed? We probe this learning setting using convolutional neural networks trained on classic computer vision benchmarks as well as a large transformer model trained on a large-scale language modeling task. Code is available at \textbackslash url\{www.github.com/facebookresearch/ALMA\}.},
  archiveprefix = {arXiv},
  keywords = {anytime-inference,continual,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KG4NERW5\\2106.html}
}

@inproceedings{cacciaNewInsightsReducing2021,
  ids = {cacciaNewInsightsReducing2022},
  title = {New {{Insights}} on {{Reducing Abrupt Representation Change}} in {{Online Continual Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Caccia, Lucas and Aljundi, Rahaf and Asadi, Nader and Tuytelaars, Tinne and Pineau, Joelle and Belilovsky, Eugene},
  year = {2021},
  month = sep,
  eprint = {2104.05025},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In the online continual learning paradigm, agents must learn from a changing distribution while respecting memory and compute constraints. Experience Replay (ER), where a small subset of past data...},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {cl-replay,continual,ocl,online,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GFXLQ7V3\\Caccia et al_2022_New Insights on Reducing Abrupt Representation Change in Online Continual.pdf;C\:\\Users\\w-32\\Zotero\\storage\\5F6C9IX8\\2104.html;C\:\\Users\\w-32\\Zotero\\storage\\9WNEVEA4\\forum.html}
}

@article{cacciaOnlineFastAdaptation2020,
  title = {Online {{Fast Adaptation}} and {{Knowledge Accumulation}}: A {{New Approach}} to {{Continual Learning}}},
  shorttitle = {Online {{Fast Adaptation}} and {{Knowledge Accumulation}}},
  author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexandre and Vazquez, David and Charlin, Laurent},
  year = {2020},
  month = jul,
  journal = {arXiv:2003.05856 [cs]},
  eprint = {2003.05856},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Continual learning studies agents that learn from streams of tasks without forgetting previous ones while adapting to new ones. Two recent continual-learning scenarios have opened new avenues of research. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting of previous tasks. In continual-meta learning, the aim is to train agents for faster remembering of previous tasks through adaptation. In their original formulations, both methods have limitations. We stand on their shoulders to propose a more general scenario, OSAKA, where an agent must quickly solve new (out-of-distribution) tasks, while also requiring fast remembering. We show that current continual learning, meta-learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. We propose Continual-MAML, an online extension of the popular MAML algorithm as a strong baseline for this scenario. We empirically show that Continual-MAML is better suited to the new scenario than the aforementioned methodologies, as well as standard continual learning and meta-learning approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual,MAML,meta-learn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TNY3TU9T\\Caccia et al_2020_Online Fast Adaptation and Knowledge Accumulation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\FX6D7WS7\\2003.html}
}

@article{cacciaOnlineLearnedContinual2020,
  title = {Online {{Learned Continual Compression}} with {{Adaptive Quantization Modules}}},
  author = {Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Pineau, Joelle},
  year = {2020},
  month = mar,
  journal = {arXiv:1911.08019 [cs, stat]},
  eprint = {1911.08019},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4MC2SK6F\\Caccia et al_2020_Online Learned Continual Compression with Adaptive Quantization Modules.pdf;C\:\\Users\\w-32\\Zotero\\storage\\YC754TL3\\1911.html}
}

@misc{cacciaTaskAgnosticContinualReinforcement2022,
  title = {Task-{{Agnostic Continual Reinforcement Learning}}: {{In Praise}} of a {{Simple Baseline}}},
  shorttitle = {Task-{{Agnostic Continual Reinforcement Learning}}},
  author = {Caccia, Massimo and Mueller, Jonas and Kim, Taesup and Charlin, Laurent and Fakoor, Rasool},
  year = {2022},
  month = may,
  number = {arXiv:2205.14495},
  eprint = {2205.14495},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.14495},
  abstract = {We study task-agnostic continual reinforcement learning (TACRL) in which standard RL challenges are compounded with partial observability stemming from task agnosticism, as well as additional difficulties of continual learning (CL), i.e., learning on a non-stationary sequence of tasks. Here we compare TACRL methods with their soft upper bounds prescribed by previous literature: multi-task learning (MTL) methods which do not have to deal with non-stationary data distributions, as well as task-aware methods, which are allowed to operate under full observability. We consider a previously unexplored and straightforward baseline for TACRL, replay-based recurrent RL (3RL), in which we augment an RL algorithm with recurrent mechanisms to address partial observability and experience replay mechanisms to address catastrophic forgetting in CL. Studying empirical performance in a sequence of RL tasks, we find surprising occurrences of 3RL matching and overcoming the MTL and task-aware soft upper bounds. We lay out hypotheses that could explain this inflection point of continual and task-agnostic learning research. Our hypotheses are empirically tested in continuous control tasks via a large-scale study of the popular multi-task and continual learning benchmark Meta-World. By analyzing different training statistics including gradient conflict, we find evidence that 3RL's outperformance stems from its ability to quickly infer how new tasks relate with the previous ones, enabling forward transfer.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,continual,RL},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\X62XNEHV\\2205.html}
}

@article{camposSkipRNNLearning2017,
  title = {Skip {{RNN}}: {{Learning}} to {{Skip State Updates}} in {{Recurrent Neural Networks}}},
  author = {Campos, Victor and Jou, Brendan and {Giro-i-Nieto}, Xavier and Torres, Jordi and Chang, Shih-Fu},
  year = {2017},
  month = aug,
  journal = {Arxiv},
  volume = {abs/1708.0},
  abstract = {Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/ .},
  keywords = {Hierarchical RNN,LSTM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3PXYJFQB\\Campos et al. - 2017 - Skip RNN Learning to Skip State Updates in Recurrent Neural Networks(3).pdf}
}

@misc{CanImportPdfs,
  title = {Can't Import Pdfs into {{Zotero}}},
  journal = {Zotero Forums},
  abstract = {Zotero is a powerful, easy-to-use research tool that helps you gather, organize, and analyze sources and then share the results of your research., For some reason, I cannot drag and drop pdfs into Zotero. I tried with the standalone. I also tried from the Chrome extension and the firefox extension.},
  howpublished = {https://forums.zotero.org/discussion/67181/cant-import-pdfs-into-zotero},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XSL6FKPY\\cant-import-pdfs-into-zotero.html}
}

@inproceedings{carta2020shortterm,
  title = {Short-{{Term Memory Optimization}} in {{Recurrent Neural Networks}} by {{Autoencoder-based Initialization}}},
  booktitle = {{{NeurIPS}} 2020 Workshop "{{Beyond Backpropagation}}: {{Novel Ideas}} for {{Training Neural Architectures}}"},
  author = {Carta, Antonio and Sperduti, Alessandro and Bacciu, Davide},
  year = {2020},
  eprint = {2011.02886},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@article{CARTA2021,
  title = {Encoding-Based Memory for Recurrent Neural Networks},
  author = {Carta, Antonio and Sperduti, Alessandro and Bacciu, Davide},
  year = {2021},
  journal = {Neurocomputing},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.04.051},
  abstract = {Learning to solve sequential tasks with recurrent models requires the ability to memorize long sequences and to extract task-relevant features from them. In this paper, we study memorization from the point of view of the design and training of recurrent neural networks. We study how to maximize the short-term memory of recurrent units, an objective difficult to achieve using backpropagation. We propose a new model, the Linear Memory Network, which features an encoding-based memorization component built with a linear autoencoder for sequences. Additionally, we provide a specialized training algorithm that initializes the memory to efficiently encode the hidden activations of the network. Experimental results on synthetic and real-world datasets show that the chosen encoding mechanism is superior to static encodings such as orthogonal models and the delay line. The method also outperforms RNN and LSTM units trained using stochastic gradient descent. Experiments on symbolic music modeling show that the training algorithm specialized for the memorization component improves the final performance compared to stochastic gradient descent.},
  keywords = {autoencoders,linear dynamical systems,memory capacity,recurrent neural networks}
}

@inproceedings{cartaAutoencoderbasedInitializationRecurrent2019,
  title = {Autoencoder-Based {{Initialization}} for {{Recurrent Neural Networks}} with a {{Linear Memory}}},
  booktitle = {Under {{Review}}},
  author = {Carta, Antonio and Sperduti, Alessandro and Bacciu, Davide},
  year = {2019}
}

@article{cartaCatastrophicForgettingDeep2021,
  title = {Catastrophic {{Forgetting}} in {{Deep Graph Networks}}: An {{Introductory Benchmark}} for {{Graph Classification}}},
  shorttitle = {Catastrophic {{Forgetting}} in {{Deep Graph Networks}}},
  author = {Carta, Antonio and Cossu, Andrea and Errica, Federico and Bacciu, Davide},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.11750 [cs]},
  eprint = {2103.11750},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work, we study the phenomenon of catastrophic forgetting in the graph representation learning scenario. The primary objective of the analysis is to understand whether classical continual learning techniques for flat and sequential data have a tangible impact on performances when applied to graph data. To do so, we experiment with a structure-agnostic model and a deep graph network in a robust and controlled environment on three different datasets. The benchmark is complemented by an investigation on the effect of structure-preserving regularization techniques on catastrophic forgetting. We find that replay is the most effective strategy in so far, which also benefits the most from the use of regularization. Our findings suggest interesting future research at the intersection of the continual and graph representation learning fields. Finally, we provide researchers with a flexible software framework to reproduce our results and carry out further experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Carta et al_2021_Catastrophic Forgetting in Deep Graph Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\6NP8KNZE\\2103.html}
}

@inproceedings{cartaExModelContinualLearning2021,
  title = {Ex-{{Model}}: {{Continual Learning}} from a {{Stream}} of {{Trained Models}}},
  booktitle = {Under Review},
  author = {Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2021}
}

@article{cartaExModelContinualLearning2021a,
  title = {Ex-{{Model}}: {{Continual Learning}} from a {{Stream}} of {{Trained Models}}},
  shorttitle = {Ex-{{Model}}},
  author = {Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.06511 [cs]},
  eprint = {2112.06511},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning continually from non-stationary data streams is a challenging research topic of growing popularity in the last few years. Being able to learn, adapt, and generalize continually in an efficient, effective, and scalable way is fundamental for a sustainable development of Artificial Intelligent systems. However, an agent-centric view of continual learning requires learning directly from raw data, which limits the interaction between independent agents, the efficiency, and the privacy of current approaches. Instead, we argue that continual learning systems should exploit the availability of compressed information in the form of trained models. In this paper, we introduce and formalize a new paradigm named "Ex-Model Continual Learning" (ExML), where an agent learns from a sequence of previously trained models instead of raw data. We further contribute with three ex-model continual learning algorithms and an empirical setting comprising three datasets (MNIST, CIFAR-10 and CORe50), and eight scenarios, where the proposed algorithms are extensively tested. Finally, we highlight the peculiarities of the ex-model paradigm and we point out interesting future research directions.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Carta et al_2021_Ex-Model.pdf;C\:\\Users\\w-32\\Zotero\\storage\\WX3XB9J8\\2112.html}
}

@inproceedings{cartaExModelContinualLearning2022,
  title = {Ex-{{Model}}: {{Continual Learning From}} a {{Stream}} of {{Trained Models}}},
  shorttitle = {Ex-{{Model}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2022},
  pages = {3790--3799},
  langid = {english},
  keywords = {continual,exmodel},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AZGJKF6S\\Carta_Ex-Model_Continual_Learning_From_a_Stream_of_Trained_Models_CVPRW_2022_paper.html}
}

@inproceedings{cartaIncrementalTrainingRecurrent2020,
  title = {Incremental {{Training}} of a {{Recurrent Neural Network Exploiting}} a {{Multi-Scale Dynamic Memory}}},
  booktitle = {{{ECML}}},
  author = {Carta, Antonio and Sperduti, Alessandro and Bacciu, Davide},
  year = {2020},
  month = jun,
  eprint = {2006.16800},
  eprinttype = {arxiv},
  abstract = {The effectiveness of recurrent neural networks can be largely influenced by their ability to store into their dynamical memory information extracted from input sequences at different frequencies and timescales. Such a feature can be introduced into a neural architecture by an appropriate modularization of the dynamic memory. In this paper we propose a novel incrementally trained recurrent architecture targeting explicitly multi-scale learning. First, we show how to extend the architecture of a simple RNN by separating its hidden state into different modules, each subsampling the network hidden activations at different frequencies. Then, we discuss a training algorithm where new modules are iteratively added to the model to learn progressively longer dependencies. Each new module works at a slower frequency than the previous ones and it is initialized to encode the subsampled sequence of hidden activations. Experimental results on synthetic and real-world datasets on speech recognition and handwritten characters show that the modular architecture and the incremental training algorithm improve the ability of recurrent neural networks to capture long-term dependencies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\T3ANM3C7\\Carta et al_2020_Incremental Training of a Recurrent Neural Network Exploiting a Multi-Scale.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KPL2CS36\\2006.html}
}

@inproceedings{cartaLearningStyleAwareSymbolic2020,
  title = {Learning {{Style-Aware Symbolic Music Representations}} by {{Adversarial Autoencoders}}},
  booktitle = {{{ECAI}}},
  author = {Carta, Antonio and Bacciu, Davide},
  year = {2020},
  pages = {8},
  abstract = {We address the challenging open problem of learning an effective latent space for symbolic music data in generative music modeling. We focus on leveraging adversarial regularization as a flexible and natural mean to imbue variational autoencoders with context information concerning music genre and style. Through the paper, we show how Gaussian mixtures taking into account music metadata information can be used as an effective prior for the autoencoder latent space, introducing the first Music Adversarial Autoencoder (MusAE). The empirical analysis on a large scale benchmark shows that our model has a higher reconstruction accuracy than state-of-the-art models based on standard variational autoencoders. It is also able to create realistic interpolations between two musical sequences, smoothly changing the dynamics of the different tracks. Experiments show that the model can organise its latent space accordingly to low-level properties of the musical pieces, as well as to embed into the latent variables the high-level genre information injected from the prior distribution to increase its overall performance. This allows us to perform changes to the generated pieces in a principled way.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2WSHE5WR\\Carta and Bacciu - 2020 - Learning Style-Aware Symbolic Music Representation.pdf}
}

@inproceedings{cartaSequentialSentenceEmbeddings2019,
  title = {Sequential {{Sentence Embeddings}} for {{Semantic Similarity}}},
  booktitle = {{{SSCI}}},
  author = {Carta, Antonio and Bacciu, Davide},
  year = {2019}
}

@article{castellanaGeneralisingRecursiveNeural,
  title = {Generalising {{Recursive Neural Models}} by {{Tensor Decomposition}}},
  author = {Castellana, Daniele and Bacciu, Davide and Pontecorvo, Largo B},
  pages = {8},
  abstract = {Most machine learning models for structured data encode the structural knowledge of a node by leveraging simple aggregation functions (in neural models, typically a weighted sum) of the information in the node's neighbourhood. Nevertheless, the choice of simple context aggregation functions, such as the sum, can be widely sub-optimal. In this work we introduce a general approach to model aggregation of structural context leveraging a tensor-based formulation. We show how the exponential growth in the size of the parameter space can be controlled through an approximation based on the Tucker tensor decomposition. This approximation allows limiting the parameters space size, decoupling it from its strict relation with the size of the hidden encoding space. By this means, we can effectively regulate the trade-off between expressivity of the encoding, controlled by the hidden size, computational complexity and model generalisation, influenced by parameterisation. Finally, we introduce a new Tensorial Tree-LSTM derived as an instance of our framework and we use it to experimentally assess our working hypotheses on tree classification scenarios.},
  langid = {english},
  keywords = {notag,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DQUZQS7A\\Castellana et al. - Generalising Recursive Neural Models by Tensor Dec.pdf}
}

@inproceedings{cazenavetteDatasetDistillationMatching2022,
  title = {Dataset {{Distillation}} by {{Matching Training Trajectories}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cazenavette, George and Wang, Tongzhou and Torralba, Antonio and Efros, Alexei A. and Zhu, Jun-Yan},
  year = {2022},
  pages = {10718--10727},
  langid = {english},
  keywords = {dataset-distillation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FCVAP6FA\\Cazenavette_Dataset_Distillation_by_Matching_Training_Trajectories_CVPR_2022_paper.html}
}

@inproceedings{ceratiKalmanFilterbasedParticleTracking2015,
  ids = {ceratiKalmanFilterbasedParticleTracking2015a},
  title = {Kalman-{{Filter-based}} Particle Tracking on Parallel Architectures at {{Hadron Colliders}}},
  booktitle = {Nuclear {{Science Symposium}} and {{Medical Imaging Conference}} ({{NSS}}/{{MIC}}), 2015 {{IEEE}}},
  author = {Cerati, Giuseppe and Tadel, M and W{\"u}rthwein, F and Yagil, A and Lantz, S and McDermott, K and Riley, D and Wittich, P and Elmer, P},
  year = {2015},
  pages = {1--4},
  publisher = {{IEEE}},
  keywords = {high-energy-physics,Kalman}
}

@inproceedings{cerSemEval2017TaskSemantic2017,
  title = {{{SemEval-2017 Task}} 1: {{Semantic Textual Similarity Multilingual}} and {{Crosslingual Focused Evaluation}}},
  booktitle = {{{SemEval}}@{{ACL}}},
  author = {Cer, Daniel M and Diab, Mona T and Agirre, Eneko and {Lopez-Gazpio}, I{\~n}igo and Specia, Lucia},
  year = {2017},
  keywords = {nlp}
}

@article{cerUniversalSentenceEncoder2018,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St. and Constant, Noah and {Guajardo-Cespedes}, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  year = {2018},
  month = mar,
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GUMCA7IY\\Cer et al. - 2018 - Universal Sentence Encoder(3).pdf}
}

@misc{chaContinualLearningTruly2022,
  title = {Is {{Continual Learning Truly Learning Representations Continually}}?},
  author = {Cha, Sungmin and Shim, Dongsub and Kim, Hyunwoo and Lee, Moontae and Lee, Honglak and Moon, Taesup},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08101},
  eprint = {2206.08101},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.08101},
  abstract = {Continual learning (CL) aims to learn from sequentially arriving tasks without forgetting previous tasks. Whereas CL algorithms have tried to achieve higher average test accuracy across all the tasks learned so far, learning continuously useful representations is critical for successful generalization and downstream transfer. To measure representational quality, we re-train only the output layers using a small balanced dataset for all the tasks, evaluating the average accuracy without any biased predictions toward the current task. We also test on several downstream tasks, measuring transfer learning accuracy of the learned representations. By testing our new formalism on ImageNet-100 and ImageNet-1000, we find that using more exemplar memory is the only option to make a meaningful difference in learned representations, and most of the regularization- or distillation-based CL algorithms that use the exemplar memory fail to learn continuously useful representations in class-incremental learning. Surprisingly, unsupervised (or self-supervised) CL with sufficient memory size can achieve comparable performance to the supervised counterparts. Considering non-trivial labeling costs, we claim that finding more efficient unsupervised CL algorithms that minimally use exemplary memory would be the next promising direction for CL research.},
  archiveprefix = {arXiv},
  keywords = {Continual,continual-representation-learning,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FHKS6HWQ\\2206.html}
}

@article{chandarNonsaturatingRecurrentUnits2019,
  title = {Towards {{Non-saturating Recurrent Units}} for {{Modelling Long-term Dependencies}}},
  author = {Chandar, Sarath and Sankar, Chinnadhurai and Vorontsov, Eugene and Kahou, Samira Ebrahimi and Bengio, Yoshua},
  year = {2019},
  month = jan,
  abstract = {Modelling long-term dependencies is a challenge for recurrent neural networks. This is primarily due to the fact that gradients vanish during training, as the sequence length increases. Gradients can be attenuated by transition operators and are attenuated or dropped by activation functions. Canonical architectures like LSTM alleviate this issue by skipping information through a memory mechanism. We propose a new recurrent architecture (Non-saturating Recurrent Unit; NRU) that relies on a memory mechanism but forgoes both saturating activation functions and saturating gates, in order to further alleviate vanishing gradients. In a series of synthetic and real world tasks, we demonstrate that the proposed model is the only model that performs among the top 2 models across all tasks with and without long-term dependencies, when compared against a range of other architectures.},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AHQ96FJ6\\Chandar et al. - 2019 - Towards Non-saturating Recurrent Units for Modelling Long-term Dependencies(3).pdf}
}

@misc{chanDataDistributionalProperties2022,
  title = {Data {{Distributional Properties Drive Emergent In-Context Learning}} in {{Transformers}}},
  author = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, Jay and Hill, Felix},
  year = {2022},
  month = may,
  number = {arXiv:2205.05055},
  eprint = {2205.05055},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.05055},
  abstract = {Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.},
  archiveprefix = {arXiv},
  keywords = {large-scale,notag,toread,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XQRYIAWT\\2205.html}
}

@article{chandolaAnomalyDetectionSurvey2009,
  title = {Anomaly Detection: {{A}} Survey},
  shorttitle = {Anomaly Detection},
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  year = {2009},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {3},
  pages = {15:1--15:58},
  issn = {0360-0300},
  doi = {10.1145/1541880.1541882},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
  keywords = {anomaly-detection},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Chandola et al_2009_Anomaly detection.pdf}
}

@inproceedings{changAntisymmetricRNNDynamicalSystem2019,
  title = {{{AntisymmetricRNN}}: {{A Dynamical System View}} on {{Recurrent Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Chang, Bo and Chen, Minmin and Haber, Eldad and Chi, Ed H.},
  year = {2019},
  month = feb,
  abstract = {Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.},
  keywords = {Dynamical systems,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SN3MEGVS\\Chang et al. - 2019 - AntisymmetricRNN A Dynamical System View on Recurrent Neural Networks.pdf}
}

@article{changDilatedRecurrentNeural,
  title = {Dilated {{Recurrent Neural Networks}}},
  author = {Chang, Shiyu and Zhang, Yang and Han, Wei and Yu, Mo and Guo, Xiaoxiao and Tan, Wei and Cui, Xiaodong and Witbrock, Michael and {Hasegawa-Johnson}, Mark and Huang, Thomas S},
  abstract = {Learning with recurrent neural networks (RNNs) on long sequences is a notori-ously difficult task. There are three major challenges: 1) complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization. In this paper, we introduce a simple yet effective RNN connection structure, the DILATEDRNN, which simultaneously tackles all of these challenges. The proposed architecture is characterized by multi-resolution dilated recurrent skip connections, and can be combined flexibly with diverse RNN cells. Moreover, the DILATEDRNN reduces the number of parameters needed and enhances training efficiency significantly, while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies. To provide a theory-based quantifi-cation of the architecture's advantages, we introduce a memory capacity measure, the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures. We rigorously prove the advantages of the DILATEDRNN over other recurrent neural architectures. The code for our method is publicly available 1 .},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H8W9XDZM\\Chang et al. - Unknown - Dilated Recurrent Neural Networks(2).pdf}
}

@article{chanListenAttendSpell2015,
  title = {Listen, {{Attend}} and {{Spell}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  year = {2015},
  month = aug,
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  keywords = {attention,speech},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\A5998CIU\\Chan et al. - 2015 - Listen, Attend and Spell(2).pdf}
}

@article{chatrchyanCMSExperimentCERN2008,
  title = {The {{CMS}} Experiment at the {{CERN LHC}}},
  author = {Chatrchyan, Serguei and Hmayakyan, G. and Khachatryan, V. and Sirunyan, A. M. and Adam, Wolfgang and Bauer, T. and Bergauer, Thomas and Bergauer, H. and Dragicevic, M. and Er{\"o}, Janos and others},
  year = {2008},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FM5LFIJW\\The CMS Experiment.pdf}
}

@article{chatrchyanCMSExperimentCERN2008a,
  ids = {chatrchyanCMSExperimentCERN2008b},
  title = {The {{CMS}} Experiment at the {{CERN LHC}}. {{The Compact Muon Solenoid}} Experiment},
  author = {Chatrchyan, Serguei and Hmayakyan, G and Khachatryan, V and Sirunyan, AM and Adam, W and Bauer, T and Bergauer, Thomas and Bergauer, H and Dragicevic, M and Er{\"o}, Janos and others},
  year = {2008},
  journal = {J. Instrum.},
  volume = {3},
  pages = {S08004. 361 p},
  keywords = {high-energy-physics}
}

@article{chatrchyanObservationNewBoson2012,
  title = {Observation of a New Boson at a Mass of 125 {{GeV}} with the {{CMS}} Experiment at the {{LHC}}},
  author = {Chatrchyan, Serguei and Khachatryan, Vardan and Sirunyan, Albert M and Tumasyan, Armen and Adam, Wolfgang and Aguilo, Ernest and Bergauer, T and Dragicevic, M and Er{\"o}, J and Fabjan, C and others},
  year = {2012},
  journal = {Physics Letters B},
  volume = {716},
  number = {1},
  pages = {30--61},
  keywords = {high-energy-physics}
}

@article{chatterjeeLearningMemorization2018,
  title = {Learning and {{Memorization}}},
  author = {Chatterjee, Satrajit},
  year = {2018},
  month = jul,
  pages = {754--762},
  keywords = {generalization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JFGFE9T2\\Chatterjee - 2018 - Learning and Memorization(2).pdf}
}

@inproceedings{chaudhari2018stochastic,
  ids = {chaudhariStochasticGradientDescent2017},
  title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  year = {2018},
  keywords = {sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MUV8EUDB\\Chaudhari, Soatto - 2017 - Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks(2).pdf}
}

@article{chaudhariEntropySGDBiasingGradient2016,
  title = {Entropy-{{SGD}}: {{Biasing Gradient Descent Into Wide Valleys}}},
  author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  year = {2016},
  pages = {1--19},
  issn = {978-3-642-04273-7},
  abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
  keywords = {sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MVBLM5H4\\Chaudhari et al. - 2016 - Entropy-SGD Biasing Gradient Descent Into Wide Valleys.pdf}
}

@article{chaudhryContinualLearningLowrank,
  title = {Continual {{Learning}} in {{Low-rank Orthogonal Subspaces}}},
  author = {Chaudhry, Arslan and Khan, Naeemullah and Dokania, Puneet K and Torr, Philip H S},
  pages = {12},
  langid = {english},
  keywords = {cl-orthogonal,continual,exmodel},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XXVV7J65\\Chaudhry et al. - Continual Learning in Low-rank Orthogonal Subspace.pdf}
}

@article{chaudhryContinualLearningTiny2019,
  title = {Continual {{Learning}} with {{Tiny Episodic Memories}}},
  author = {Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K. and Torr, Philip H. S. and Ranzato, Marc'Aurelio},
  year = {2019},
  abstract = {Learning with less supervision is a major challenge in artificial intelligence. One sensible approach to decrease the amount of supervision is to leverage prior experience and transfer knowledge from tasks seen in the past. However, a necessary condition for a successful transfer is the ability to remember how to perform previous tasks. The Continual Learning (CL) setting, whereby an agent learns from a stream of tasks without seeing any example twice, is an ideal framework to investigate how to accrue such knowledge. In this work, we consider supervised learning tasks and methods that leverage a very small episodic memory for continual learning. Through an extensive empirical analysis across four benchmark datasets adapted to CL, we observe that a very simple baseline, which jointly trains on both examples from the current task as well as examples stored in the memory, outperforms state-of-the-art CL approaches with and without episodic memory. Surprisingly, repeated learning over tiny episodic memories does not harm generalization on past tasks, as joint training on data from subsequent tasks acts like a data dependent regularizer. We discuss and evaluate different approaches to write into the memory. Most notably, reservoir sampling works remarkably well across the board, except when the memory size is extremely small. In this case, writing strategies that guarantee an equal representation of all classes work better. Overall, these methods should be considered as a strong baseline candidate when benchmarking new CL approaches},
  keywords = {continual,episodic-mem,multi-head,multi-task},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LMRLRQJA\\Chaudhry et al. - 2019 - Continual Learning with Tiny Episodic Memories.pdf}
}

@article{chaudhryEfficientLifelongLearning2019,
  title = {Efficient {{Lifelong Learning}} with {{A-GEM}}},
  author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  year = {2019},
  month = jan,
  journal = {arXiv:1812.00420 [cs, stat]},
  eprint = {1812.00420},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz \& Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
  archiveprefix = {arXiv},
  keywords = {cl-regularization,cl-replay,continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Chaudhry et al_2019_Efficient Lifelong Learning with A-GEM.pdf;C\:\\Users\\w-32\\Zotero\\storage\\LMULAW7B\\1812.html}
}

@inproceedings{chaudhryRiemannianWalkIncremental2018,
  title = {Riemannian {{Walk}} for {{Incremental Learning}}: {{Understanding Forgetting}} and {{Intransigence}}},
  shorttitle = {Riemannian {{Walk}} for {{Incremental Learning}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Chaudhry, Arslan and Dokania, Puneet K. and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
  year = {2018},
  pages = {532--547},
  keywords = {continual,ewc,regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HUI69TBE\\Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VVQ44N7S\\Arslan_Chaudhry__Riemannian_Walk_ECCV_2018_paper.html}
}

@article{chenComprehensiveIdentificationAnnotation2012,
  title = {Comprehensive Identification and Annotation of Cell Type-Specific and Ubiquitous {{CTCF-binding}} Sites in the Human Genome},
  author = {Chen, Hebing and Tian, Yao and Shu, Wenjie and Bo, Xiaochen and Wang, Shengqi},
  year = {2012},
  journal = {PLoS ONE},
  volume = {7},
  number = {7},
  doi = {10.1371/journal.pone.0041374},
  abstract = {Chromatin insulators are DNA elements that regulate the level of gene expression either by preventing gene silencing through the maintenance of heterochromatin boundaries or by preventing gene activation by blocking interactions between enhancers and promoters. CCCTC-binding factor (CTCF), a ubiquitously expressed 11-zinc-finger DNA-binding protein, is the only protein implicated in the establishment of insulators in vertebrates. While CTCF has been implicated in diverse regulatory functions, CTCF has only been studied in a limited number of cell types across human genome. Thus, it is not clear whether the identified cell type-specific differences in CTCF-binding sites are functionally significant. Here, we identify and characterize cell type-specific and ubiquitous CTCF-binding sites in the human genome across 38 cell types designated by the Encyclopedia of DNA Elements (ENCODE) consortium. These cell type-specific and ubiquitous CTCF-binding sites show uniquely versatile transcriptional functions and characteristic chromatin features. In addition, we confirm the insulator barrier function of CTCF-binding and explore the novel function of CTCF in DNA replication. These results represent a critical step toward the comprehensive and systematic understanding of CTCF-dependent insulators and their versatile roles in the human genome.},
  keywords = {BIOINF},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SCWW36AN\\Chen et al. - 2012 - Comprehensive identification and annotation of cell type-specific and ubiquitous CTCF-binding sites in the human ge.pdf}
}

@article{chenDataFreeLearningStudent2019,
  title = {Data-{{Free Learning}} of {{Student Networks}}},
  author = {Chen, Hanting and Wang, Yunhe and Xu, Chang and Yang, Zhaohui and Liu, Chuanjian and Shi, Boxin and Xu, Chunjing and Xu, Chao and Tian, Qi},
  year = {2019},
  month = dec,
  journal = {arXiv:1904.01186 [cs, stat]},
  eprint = {1904.01186},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning portable neural networks is very essential for computer vision for the purpose that pre-trained heavy deep models can be well applied on edge devices such as mobile phones and micro sensors. Most existing deep neural network compression and speed-up methods are very effective for training compact deep models, when we can directly access the training dataset. However, training data for the given deep network are often unavailable due to some practice problems (e.g. privacy, legal issue, and transmission), and the architecture of the given network are also unknown except some interfaces. To this end, we propose a novel framework for training efficient deep neural networks by exploiting generative adversarial networks (GANs). To be specific, the pre-trained teacher networks are regarded as a fixed discriminator and the generator is utilized for derivating training samples which can obtain the maximum response on the discriminator. Then, an efficient network with smaller model size and computational complexity is trained using the generated data and the teacher network, simultaneously. Efficient student networks learned using the proposed Data-Free Learning (DAFL) method achieve 92.22\% and 74.47\% accuracies using ResNet-18 without any training data on the CIFAR-10 and CIFAR-100 datasets, respectively. Meanwhile, our student network obtains an 80.56\% accuracy on the CelebA benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,data-free,knowledge distillation,notag,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Chen et al_2019_Data-Free Learning of Student Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\W5TX8BCH\\1904.html}
}

@article{chenDynamicalIsometryMean2018,
  title = {Dynamical {{Isometry}} and a {{Mean Field Theory}} of {{RNNs}}: {{Gating Enables Signal Propagation}} in {{Recurrent Neural Networks}}},
  author = {Chen, Minmin and Pennington, Jeffrey and Schoenholz, Samuel S.},
  year = {2018},
  month = jun,
  abstract = {Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improvement in training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.},
  keywords = {Dynamical systems,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KGAPUR2I\\Chen, Pennington, Schoenholz - 2018 - Dynamical Isometry and a Mean Field Theory of RNNs Gating Enables Signal Propagation in Recurre(2).pdf}
}

@article{chenNet2NetAcceleratingLearning2016,
  title = {{{Net2Net}}: {{Accelerating Learning}} via {{Knowledge Transfer}}},
  shorttitle = {{{Net2Net}}},
  author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  year = {2016},
  month = apr,
  journal = {arXiv:1511.05641 [cs]},
  eprint = {1511.05641},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2WWHZFKX\\Chen et al. - 2016 - Net2Net Accelerating Learning via Knowledge Trans.pdf;C\:\\Users\\w-32\\Zotero\\storage\\TMF39EKR\\1511.html}
}

@article{chenRecurrentNeuralNetwork2019,
  title = {A Recurrent Neural Network Applied to Optimal Motion Control of Mobile Robots with Physical Constraints},
  author = {Chen, Dechao and Li, Shuai and Liao, Liefa},
  year = {2019},
  month = dec,
  journal = {Applied Soft Computing},
  volume = {85},
  pages = {105880},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2019.105880},
  abstract = {Conventional solutions, such as the conventional recurrent neural network (CRNN) and gradient recurrent neural network (GRNN), for the motion control of mobile robots in the unified framework of recurrent neural network (RNN) are difficult to simultaneously consider both criteria optimization and physical constraints. The limitation of the RNN solution may lead to the damage of mobile robots for exceeding physical constraints during the task execution. To overcome this limitation, this paper proposes a novel inequality and equality constrained optimization RNN (IECORNN) to handle the motion control of mobile robots. Firstly, the real-time motion control problem with both criteria optimization and physical constraints is skillfully converted to a real-time equality system by leveraging the Lagrange multiplier rule. Then, the detailed design process for the proposed IECORNN is presented together with the neural network architecture developed. Afterward, theoretical analyses on the motion control problem conversion equivalence, global stability, and exponential convergence property are rigorously provided. Finally, two numerical simulation verifications and extensive comparisons with other existing RNNs, e.g., the CRNN and the GRNN, based on the mobile robot for two different path-tracking applications sufficiently demonstrate the effectiveness and superiority of the proposed IECORNN for the real-time motion control of mobile robots with both criteria optimization and physical constraints. This work makes progresses in both theory as well as practice, and fills the vacancy in the unified framework of RNN in motion control of mobile robots.},
  langid = {english},
  keywords = {Criteria optimization,Mobile robots,Motion control,Physical constraints,Recurrent neural network (RNN)},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FLLRTP2U\\Chen et al_2019_A recurrent neural network applied to optimal motion control of mobile robots.pdf}
}

@inproceedings{chenReusingTaskSpecificClassifier2022,
  title = {Reusing the {{Task-Specific Classifier}} as a {{Discriminator}}: {{Discriminator-Free Adversarial Domain Adaptation}}},
  shorttitle = {Reusing the {{Task-Specific Classifier}} as a {{Discriminator}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Lin and Chen, Huaian and Wei, Zhixiang and Jin, Xin and Tan, Xiao and Jin, Yi and Chen, Enhong},
  year = {2022},
  pages = {7181--7190},
  langid = {english},
  keywords = {unsupervised-domain-adaptation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VIBM9K6R\\Chen_Reusing_the_Task-Specific_Classifier_as_a_Discriminator_Discriminator-Free_Adversarial_Dom.html}
}

@article{chenRNNDPNewDifferential2020,
  title = {{{RNN-DP}}: {{A}} New Differential Privacy Scheme Base on {{Recurrent Neural Network}} for {{Dynamic}} Trajectory Privacy Protection},
  shorttitle = {{{RNN-DP}}},
  author = {Chen, Si and Fu, Anmin and Shen, Jian and Yu, Shui and Wang, Huaqun and Sun, Huaijiang},
  year = {2020},
  month = oct,
  journal = {Journal of Network and Computer Applications},
  volume = {168},
  pages = {102736},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2020.102736},
  abstract = {Mobile devices furnish users with various services while on the move, but also raise public concerns about trajectory privacy. Unfortunately, traditional privacy protection methods, such as anonymity and generalization, are not secure because they cannot resist attackers with background knowledge. The emergence of differential privacy provides an effective solution to this problem. Still, the existing schemes are almost designed based on the collected aggregate historical data (so-called static trajectory privacy protection), which are not suitable for real-time dynamic trajectory privacy protection of mobile users. Furthermore, due to the complexity and redundancy features of the full trajectory data, the efficiency and accuracy of the privacy protection model are significantly limited by the existing schemes. In this paper, we propose a new differential privacy scheme base on the Recurrent Neural Network for Dynamic trajectory privacy Protection (RNN-DP). We firstly introduce a recurrent neural network model to handle the real-time data effectively instead of the full data. Secondly, we novelty leverage the dynamic velocity attribute to form a quaternion to indicate the status of the users. Moreover, we design a prejudgment mechanism to increase the availability of differential privacy technology. Compared with the current state-of-the-art mechanisms, the experimental results demonstrate that RNN-DP displays excellent performance in privacy protection and data availability for dynamic trajectory data.},
  langid = {english},
  keywords = {Data publishing,diff-privacy,Differential privacy,Dynamic trajectory,Neural network},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\U3Q4W7BC\\Chen et al_2020_RNN-DP.pdf;C\:\\Users\\w-32\\Zotero\\storage\\Z7INWPEQ\\S1084804520302101.html}
}

@article{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.05709 [cs, stat]},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NAQTVISE\\Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf;C\:\\Users\\w-32\\Zotero\\storage\\22UL8WTF\\2002.html}
}

@inproceedings{chenThisLooksThat2019,
  title = {This {{Looks Like That}}: {{Deep Learning}} for {{Interpretable Image Recognition}}},
  shorttitle = {This {{Looks Like That}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.},
  keywords = {interpretability,notag,prototype}
}

@article{chenThoroughExaminationCNN2016,
  title = {A {{Thorough Examination}} of the {{CNN}}/{{Daily Mail Reading Comprehension Task}}},
  author = {Chen, Danqi and Bolton, Jason and Manning, Christopher D.},
  year = {2016},
  issn = {9781510827585},
  abstract = {Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6\% and 76.6\% on these two datasets, exceeding current state-of-the-art results by 7-10\% and approaching what we believe is the ceiling for performance on this task.},
  keywords = {nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SMRBUEH9\\Chen, Bolton, Manning - 2016 - A Thorough Examination of the CNNDaily Mail Reading Comprehension Task.pdf}
}

@inproceedings{chenXgboostScalableTree2016,
  ids = {chenXgboostScalableTree2016a},
  title = {Xgboost: {{A}} Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  pages = {785--794},
  publisher = {{ACM}},
  keywords = {software}
}

@article{cheungSuperpositionManyModels2019,
  title = {Superposition of Many Models into One},
  author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  year = {2019},
  month = feb,
  abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
  langid = {english}
}

@inproceedings{chiMetaFSCILMetaLearningApproach2022,
  title = {{{MetaFSCIL}}: {{A Meta-Learning Approach}} for {{Few-Shot Class Incremental Learning}}},
  shorttitle = {{{MetaFSCIL}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chi, Zhixiang and Gu, Li and Liu, Huan and Wang, Yang and Yu, Yuanhao and Tang, Jin},
  year = {2022},
  pages = {14166--14175},
  langid = {english},
  keywords = {Continual,few-shot,meta-learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NW8DRPMF\\Chi_MetaFSCIL_A_Meta-Learning_Approach_for_Few-Shot_Class_Incremental_Learning_CVPR_2022_paper.html}
}

@inproceedings{chiochiaNovelTechniqueReconstruction2008,
  ids = {chiochiaNovelTechniqueReconstruction2008a},
  title = {A Novel Technique for the Reconstruction and Simulation of Hits in the {{CMS}} Pixel Detector},
  booktitle = {Nuclear {{Science Symposium Conference Record}}, 2008. {{NSS}}'08. {{IEEE}}},
  author = {Chiochia, V and Swartz, M and Fehling, D and Giurgiu, G and Maksimovic, P},
  year = {2008},
  pages = {1909--1912},
  publisher = {{IEEE}},
  keywords = {high-energy-physics}
}

@article{choiDualTeacherClassIncrementalLearning2021,
  title = {Dual-{{Teacher Class-Incremental Learning With Data-Free Generative Replay}}},
  author = {Choi, Yoojin and {El-Khamy}, Mostafa and Lee, Jungwon},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.09835 [cs]},
  eprint = {2106.09835},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper proposes two novel knowledge transfer techniques for class-incremental learning (CIL). First, we propose data-free generative replay (DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples from a generative model. In the conventional generative replay, the generative model is pre-trained for old data and shared in extra memory for later incremental learning. In our proposed DF-GR, we train a generative model from scratch without using any training data, based on the pre-trained classification model from the past, so we curtail the cost of sharing pre-trained generative models. Second, we introduce dual-teacher information distillation (DT-ID) for knowledge distillation from two teachers to one student. In CIL, we use DT-ID to learn new classes incrementally based on the pre-trained model for old classes and another model (pre-)trained on the new data for new classes. We implemented the proposed schemes on top of one of the state-of-the-art CIL methods and showed the performance improvement on CIFAR-100 and ImageNet datasets.},
  archiveprefix = {arXiv},
  keywords = {continual,data-distill,data-free,exmodel},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Choi et al_2021_Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay.pdf;C\:\\Users\\w-32\\Zotero\\storage\\7MU3AIUP\\2106.html}
}

@article{choiEncodingMusicalStyle2019,
  title = {Encoding {{Musical Style}} with {{Transformer Autoencoders}}},
  author = {Choi, Kristy and Hawthorne, Curtis and Simon, Ian and Dinculescu, Monica and Engel, Jesse},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.05537 [cs, eess, stat]},
  eprint = {1912.05537},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and a YouTube dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.},
  archiveprefix = {arXiv},
  keywords = {music},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CCAV5VX6\\Choi et al. - 2019 - Encoding Musical Style with Transformer Autoencode.pdf;C\:\\Users\\w-32\\Zotero\\storage\\B9ENDFBL\\1912.html}
}

@article{choLearningPhraseRepresentations2014,
  ids = {choLearningPhraseRepresentations2014a},
  title = {Learning Phrase Representations Using {{RNN}} Encoder-Decoder for Statistical Machine Translation},
  author = {Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  journal = {arXiv preprint arXiv:1406.1078},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {nlp,NMT,RNN,seq2seq},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GIACF5RR\\Learning Phrase Representations using RNN EncoderDecoder.pdf}
}

@inproceedings{choromanskiInitializationMattersOrthogonal2018,
  title = {Initialization Matters: {{Orthogonal Predictive State Recurrent Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Choromanski, Krzysztof and Downey, Carlton and Boots, Byron},
  year = {2018},
  month = feb,
  abstract = {Abstract: Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing. Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model. PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule. Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well. Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train. Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation. Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting. In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs. In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed. TL;DR: Improving Predictive State Recurrent Neural Networks via Orthogonal Random Features Keywords: recurrent neural networks, orthogonal random features, predictive state representations},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VEANLWQK\\Choromanski, Downey, Boots - 2018 - Initialization matters Orthogonal Predictive State Recurrent Neural Networks.pdf}
}

@misc{chrabaszczDownsampledVariantImageNet2017,
  title = {A {{Downsampled Variant}} of {{ImageNet}} as an {{Alternative}} to the {{CIFAR}} Datasets},
  author = {Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
  year = {2017},
  month = aug,
  number = {arXiv:1707.08819},
  eprint = {1707.08819},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1707.08819},
  abstract = {The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32\$\textbackslash times\$32 (and its variants ImageNet64\$\textbackslash times\$64 and ImageNet16\$\textbackslash times\$16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32\$\textbackslash times\$32 pixels per image (64\$\textbackslash times\$64 and 16\$\textbackslash times\$16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at http://image-net.org/download-images and https://github.com/PatrykChrabaszcz/Imagenet32\_Scripts},
  archiveprefix = {arXiv},
  keywords = {dataset},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\U9DJ8PP3\\1707.html}
}

@article{chrysakisOnlineContinualLearning,
  title = {Online {{Continual Learning}} from {{Imbalanced Data}}},
  author = {Chrysakis, Aristotelis and Moens, Marie-Francine},
  pages = {10},
  abstract = {A well-documented weakness of neural networks is the fact that they suffer from catastrophic for\- getting when trained on data provided by a nonstationary distribution. Recent work in the field of continual learning attempts to understand and overcome this issue. Unfortunately, the majority of relevant work embraces the implicit assump\- tion that the distribution of observed data is per\- fectly balanced, despite the fact that, in the real world, humans and animals learn from observa\- tions that are temporally correlated and severely imbalanced. Motivated by this remark, we aim to evaluate memory population methods that are used in online continual learning, when dealing with highly imbalanced and temporally correlated streams of data. More importantly, we introduce a new memory population approach, which we call class-balancing reservoir sampling (CBRS). We demonstrate that CBRS outperforms the state-of\- the-art memory population algorithms in a consid\- erably challenging learning setting, over a range of different datasets, and for multiple architec\- tures.},
  langid = {english},
  keywords = {continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UD3EXYK2\\Chrysakis and Moens - Online Continual Learning from Imbalanced Data.pdf}
}

@article{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  journal = {arXiv e-prints},
  volume = {abs/1412.3555},
  pages = {1--9},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  keywords = {LSTM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\T4ID8E9B\\Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf}
}

@inproceedings{chungHIERARCHICALMULTISCALERECURRENT2017,
  title = {{{HIERARCHICAL MULTISCALE RECURRENT NEURAL NETWORKS}}},
  booktitle = {{{ICLR}}},
  author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  year = {2017},
  abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.},
  keywords = {Hierarchical RNN,ICLR,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GNC9KI4D\\Chung, Ahn, Bengio - 2017 - HIERARCHICAL MULTISCALE RECURRENT NEURAL NETWORKS.pdf}
}

@article{chungRecurrentLatentVariable2015,
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
  year = {2015},
  month = jun,
  abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
  keywords = {RNN,vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\I7DUHPNV\\Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential Data.pdf}
}

@article{cohenDNNNNThat2018,
  title = {{{DNN}} or \$k\$-{{NN}}: {{That}} Is the {{Generalize}} vs. {{Memorize Question}}},
  author = {Cohen, Gilad and Sapiro, Guillermo and Giryes, Raja},
  year = {2018},
  month = may,
  abstract = {This paper studies the relationship between the classification performed by deep neural networks and the \$k\$-NN decision at the embedding space of these networks. This simple important connection shown here provides a better understanding of the relationship between the ability of neural networks to generalize and their tendency to memorize the training data, which are traditionally considered to be contradicting to each other and here shown to be compatible and complementary. Our results support the conjecture that deep neural networks approach Bayes optimal error rates.},
  keywords = {generalization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZC94L9PK\\Cohen, Sapiro, Giryes - 2018 - DNN or $k$-NN That is the Generalize vs. Memorize Question(2).pdf}
}

@inproceedings{cohenSphericalCNNs2018,
  title = {Spherical {{CNNs}}},
  booktitle = {{{ICLR}}},
  author = {Cohen, Taco S. and Geiger, Mario and K{\"o}hler, Jonas and Welling, Max},
  year = {2018},
  month = feb,
  abstract = {Abstract: Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression. TL;DR: We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression. Keywords: deep learning, equivariance, convolution, group convolution, 3D, vision, omnidirectional, shape recognition, molecular energy regression},
  keywords = {CNN,ICLR},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7K9ZBV47\\Cohen et al. - 2018 - Spherical CNNs(2).pdf}
}

@article{collaborationDescriptionPerformanceTrack2014,
  title = {Description and Performance of Track and Primary-Vertex Reconstruction with the {{CMS}} Tracker},
  author = {{collaboration}, C M S and others},
  year = {2014},
  journal = {Journal of Instrumentation},
  volume = {9},
  number = {10},
  pages = {P10009-P10009},
  keywords = {high-energy-physics}
}

@inproceedings{collierImplementingNeuralTuring2018,
  title = {Implementing {{Neural Turing Machines}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2018},
  author = {Collier, Mark and Beel, Joeran},
  editor = {K{\r{u}}rkov{\'a}, V{\v e}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {94--104},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01424-7_10},
  abstract = {Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural Networks, a new class of recurrent neural networks which decouple computation from memory by introducing an external memory unit. NTMs have demonstrated superior performance over Long Short-Term Memory Cells in several sequence learning tasks. A number of open source implementations of NTMs exist but are unstable during training and/or fail to replicate the reported performance of NTMs. This paper presents the details of our successful implementation of a NTM. Our implementation learns to solve three sequential learning tasks from the original NTM paper. We find that the choice of memory contents initialization scheme is crucial in successfully implementing a NTM. Networks with memory contents initialized to small constant values converge on average 2 times faster than the next best memory contents initialization scheme.},
  isbn = {978-3-030-01424-7},
  langid = {english},
  keywords = {Memory Augmented Neural Networks,Neural Turing Machines},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\G3VR9X4X\\Collier_Beel_2018_Implementing Neural Turing Machines.pdf}
}

@inproceedings{collinsCapacityTrainabilityRecurrent2017,
  title = {Capacity and {{Trainability}} in {{Recurrent Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Collins, Jasmine and {Sohl-Dickstein}, Jascha and Sussillo, David},
  year = {2017},
  pages = {1--17},
  keywords = {ICLR,learning,RNN}
}

@inproceedings{collobertUnifiedArchitectureNatural2008,
  title = {A Unified Architecture for Natural Language Processing: {{Deep}} Neural Networks with Multitask Learning},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Collobert, Ronan and Weston, Jason},
  year = {2008},
  pages = {160--167},
  keywords = {nlp}
}

@misc{ComprehensiveIntroductionDifferential,
  title = {A {{Comprehensive Introduction}} to {{Differential Geometry}}},
  author = {, Spivak},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HBR9NT97\\spivak-intro-diff-geom-v1-3ed.pdf}
}

@article{conneauSentEvalEvaluationToolkit2018,
  title = {{{SentEval}}: {{An Evaluation Toolkit}} for {{Universal Sentence Representations}}},
  author = {Conneau, Alexis and Kiela, Douwe},
  year = {2018},
  month = mar,
  abstract = {We introduce SentEval, a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to provide a fairer, less cumbersome and more centralized way for evaluating sentence representations.},
  keywords = {nlp,software},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JP6IBI3R\\Conneau, Kiela - 2018 - SentEval An Evaluation Toolkit for Universal Sentence Representations(2).pdf}
}

@article{conneauSupervisedLearningUniversal2017,
  title = {Supervised {{Learning}} of {{Universal Sentence Representations}} from {{Natural Language Inference Data}}},
  author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
  year = {2017},
  month = may,
  journal = {EMNLP},
  issn = {978-1-109-24088-7},
  doi = {10.18653/v1/D17-1070},
  abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
  keywords = {nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\58S4Z7LT\\Conneau et al. - 2017 - Supervised Learning of Universal Sentence Representations from Natural Language Inference Data(3).pdf}
}

@article{conneauWhatYouCan2018,
  title = {What You Can Cram into a Single Vector: {{Probing}} Sentence Embeddings for Linguistic Properties},
  author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"i}c and Baroni, Marco},
  year = {2018},
  month = may,
  abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. "Downstream" tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
  keywords = {nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5TW3BQNG\\Conneau et al. - 2018 - What you can cram into a single vector Probing sentence embeddings for linguistic properties(2).pdf}
}

@inproceedings{conneauWordTranslationParallel2018,
  title = {Word {{Translation Without Parallel Data}}},
  booktitle = {{{ICLR}}},
  author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  year = {2018},
  month = oct,
  doi = {10.1111/j.1540-4560.2007.00543.x},
  abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
  keywords = {GAN,ICLR,NMT,RNN,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6SVYDH4T\\Conneau et al. - 2018 - Word Translation Without Parallel Data.pdf}
}

@article{cooijmansRecurrentBatchNormalization2016,
  title = {Recurrent {{Batch Normalization}}},
  author = {Cooijmans, Tim and Ballas, Nicolas and Laurent, C{\'e}sar and G{\"u}l{\c c}ehre, {\c C}a{\u g}lar and Courville, Aaron},
  year = {2016},
  month = mar,
  abstract = {We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.},
  keywords = {regularization,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UD56EU58\\Cooijmans et al. - 2016 - Recurrent Batch Normalization(2).pdf}
}

@inproceedings{coopMitigationCatastrophicForgetting2013,
  title = {Mitigation of Catastrophic Forgetting in Recurrent Neural Networks Using a {{Fixed Expansion Layer}}},
  booktitle = {2013 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}} 2013 - {{Dallas}})},
  author = {Coop, Robert and Arel, Itamar},
  year = {2013},
  pages = {1--7},
  publisher = {{IEEE}},
  doi = {10.1109/IJCNN.2013.6707047},
  abstract = {Catastrophic forgetting (or catastrophic interference) in supervised learning systems is the drastic loss of previously stored information caused by the learning of new information. While substantial work has been published on addressing catastrophic forgetting in memoryless supervised learning systems (e.g. feedforward neural networks), the problem has received limited attention in the context of dynamic systems, particularly recurrent neural networks. In this paper, we introduce a solution for mitigating catastrophic forgetting in RNNs based on enhancing the Fixed Expansion Layer (FEL) neural network which exploits sparse coding of hidden neuron activations. Simulation results on several non-stationary data sets clearly demonstrate the effectiveness of the proposed architecture.},
  isbn = {978-1-4673-6129-3 978-1-4673-6128-6},
  langid = {english},
  keywords = {architectural,continual,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JXEB4PRX\\Coop, Arel - 2013 - Mitigation of catastrophic forgetting in recurrent neural networks using a Fixed Expansion Layer.pdf}
}

@article{cossu2021continual,
  title = {Continual Learning for Recurrent Neural Networks: {{An}} Empirical Evaluation},
  author = {Cossu, Andrea and Carta, Antonio and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2021},
  journal = {Neural Networks},
  volume = {143},
  pages = {607--627},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.07.021},
  abstract = {Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.},
  keywords = {Benchmarks,continual,Evaluation,RNN},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Cossu et al_2021_Continual Learning for Recurrent Neural Networks.pdf}
}

@article{cossuClassIncrementalEnoughContinual2021,
  title = {Is {{Class-Incremental Enough}} for {{Continual Learning}}?},
  author = {Cossu, Andrea and Graffieti, Gabriele and Pellegrini, Lorenzo and Maltoni, Davide and Bacciu, Davide and Carta, Antonio and Lomonaco, Vincenzo},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.02925 [cs]},
  eprint = {2112.02925},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The ability of a model to learn continually can be empirically assessed in different continual learning scenarios. Each scenario defines the constraints and the opportunities of the learning environment. Here, we challenge the current trend in the continual learning literature to experiment mainly on class-incremental scenarios, where classes present in one experience are never revisited. We posit that an excessive focus on this setting may be limiting for future research on continual learning, since class-incremental scenarios artificially exacerbate catastrophic forgetting, at the expense of other important objectives like forward transfer and computational efficiency. In many real-world environments, in fact, repetition of previously encountered concepts occurs naturally and contributes to softening the disruption of previous knowledge. We advocate for a more in-depth study of alternative continual learning scenarios, in which repetition is integrated by design in the stream of incoming information. Starting from already existing proposals, we describe the advantages such class-incremental with repetition scenarios could offer for a more comprehensive assessment of continual learning models.},
  archiveprefix = {arXiv},
  keywords = {continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Cossu et al_2021_Is Class-Incremental Enough for Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\T3WVAYD9\\2112.html}
}

@inproceedings{cossuContinualLearningEcho2021,
  title = {Continual {{Learning}} with {{Echo State Networks}}},
  booktitle = {{{ESANN}} 2021},
  author = {Cossu, Andrea and Bacciu, Davide and Carta, Antonio and Gallicchio, Claudio and Lomonaco, Vincenzo},
  year = {2021},
  month = jun,
  eprint = {2105.07674},
  eprinttype = {arxiv},
  abstract = {Continual Learning (CL) refers to a learning setup where data is non stationary and the model has to learn without forgetting existing knowledge. The study of CL for sequential patterns revolves around trained recurrent networks. In this work, instead, we introduce CL in the context of Echo State Networks (ESNs), where the recurrent component is kept fixed. We provide the first evaluation of catastrophic forgetting in ESNs and we highlight the benefits in using CL strategies which are not applicable to trained recurrent models. Our results confirm the ESN as a promising model for CL and open to its use in streaming scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Cossu et al_2021_Continual Learning with Echo State Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\2YR9BRDZ\\2105.html}
}

@article{cossuContinualLearningRecurrent2021,
  title = {Continual {{Learning}} for {{Recurrent Neural Networks}}: An {{Empirical Evaluation}}},
  shorttitle = {Continual {{Learning}} for {{Recurrent Neural Networks}}},
  author = {Cossu, Andrea and Carta, Antonio and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2021},
  month = may,
  journal = {arXiv:2103.07492 [cs]},
  eprint = {2103.07492},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CWV6TPPM\\2103.html}
}

@misc{cossuContinualPreTrainingMitigates2022,
  title = {Continual {{Pre-Training Mitigates Forgetting}} in {{Language}} and {{Vision}}},
  author = {Cossu, Andrea and Tuytelaars, Tinne and Carta, Antonio and Passaro, Lucia and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2022},
  month = may,
  number = {arXiv:2205.09357},
  eprint = {2205.09357},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.09357},
  abstract = {Pre-trained models are nowadays a fundamental component of machine learning research. In continual learning, they are commonly used to initialize the model before training on the stream of non-stationary data. However, pre-training is rarely applied during continual learning. We formalize and investigate the characteristics of the continual pre-training scenario in both language and vision environments, where a model is continually pre-trained on a stream of incoming data and only later fine-tuned to different downstream tasks. We show that continually pre-trained models are robust against catastrophic forgetting and we provide strong empirical evidence supporting the fact that self-supervised pre-training is more effective in retaining previous knowledge than supervised protocols. Code is provided at https://github.com/AndreaCossu/continual-pretraining-nlp-vision .},
  archiveprefix = {arXiv},
  keywords = {continual,nlp,pretraining,vision},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GQ7L4BEL\\2205.html}
}

@article{courbariauxBinarizedNeuralNetworks2016,
  title = {Binarized {{Neural Networks}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  shorttitle = {Binarized {{Neural Networks}}},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and {El-Yaniv}, Ran and Bengio, Yoshua},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.02830 [cs]},
  eprint = {1602.02830},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NAFJ5MB8\\Courbariaux et al. - 2016 - Binarized Neural Networks Training Deep Neural Ne.pdf;C\:\\Users\\w-32\\Zotero\\storage\\FJRTI88F\\1602.html}
}

@article{courbariauxBinaryNetTrainingDeep2016,
  ids = {courbariauxBinaryNetTrainingDeep2016a},
  title = {{{BinaryNet}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  author = {Courbariaux, Matthieu and Bengio, Yoshua},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1602.02830},
  keywords = {bnn}
}

@article{crammerAlgorithmicImplementationMulticlass,
  title = {On the {{Algorithmic Implementation}} of {{Multiclass Kernel-based Vector Machines}}},
  author = {Crammer, Koby and Singer, Yoram},
  pages = {28},
  abstract = {In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy.},
  langid = {english},
  keywords = {Kernel,multiclass SVM,SVM},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H53RS8KT\\Crammer and Singer - On the Algorithmic Implementation of Multiclass Ke.pdf}
}

@article{cravensAnnotatingProteinSecondary2015,
  title = {Annotating Protein Secondary Structure from Sequence},
  author = {Cravens, Aaron and Probert, Christopher},
  year = {2015},
  pages = {1--11},
  abstract = {Proteins are linear chain biomolecules composed of 20 different amino acids which form secondary structures such as helixes and loops [8]. Here, we use deep recurrent memory networks and deep convectional networks for predicting sec-ondary structure annotations on sequences from the Uniprot database. We com-pare different network architectures and memory models, and discuss possible improvements. Our results on several tasks are significantly better than current methods that use SVMs and feed-forward neural networks.},
  keywords = {BIOINF},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LLNRUK22\\Cravens, Probert - 2015 - Annotating protein secondary structure from sequence(2).pdf}
}

@article{criadoNonIIDDataContinual2021,
  title = {Non-{{IID}} Data and {{Continual Learning}} Processes in {{Federated Learning}}: {{A}} Long Road Ahead},
  shorttitle = {Non-{{IID}} Data and {{Continual Learning}} Processes in {{Federated Learning}}},
  author = {Criado, Marcos F. and Casado, Fernando E. and Iglesias, Roberto and Regueiro, Carlos V. and Barro, Sen{\'e}n},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.13394 [cs]},
  eprint = {2111.13394},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Federated Learning is a novel framework that allows multiple devices or institutions to train a machine learning model collaboratively while preserving their data private. This decentralized approach is prone to suffer the consequences of data statistical heterogeneity, both across the different entities and over time, which may lead to a lack of convergence. To avoid such issues, different methods have been proposed in the past few years. However, data may be heterogeneous in lots of different ways, and current proposals do not always determine the kind of heterogeneity they are considering. In this work, we formally classify data statistical heterogeneity and review the most remarkable learning strategies that are able to face it. At the same time, we introduce approaches from other machine learning frameworks, such as Continual Learning, that also deal with data heterogeneity and could be easily adapted to the Federated Learning settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,continual,exmodel,federated},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Criado et al_2021_Non-IID data and Continual Learning processes in Federated Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\63H6BW4X\\2111.html}
}

@article{cumminsLearningForgetContinual2000,
  title = {Learning to {{Forget}}: {{Continual Prediction}} with {{LSTM}}},
  author = {Cummins, Fred and Gers, Felix A. and Schmidhuber, Jurgen},
  year = {2000},
  journal = {Neural Computation},
  volume = {2},
  number = {June 2016},
  pages = {850--855},
  issn = {0899766003000},
  doi = {10.1197/jamia.M2577},
  abstract = {BackgroundA significant portion of patients already known to be colonized or infected with Methicillin-Resistant Staphylococcus aureus (MRSA) may not be identified at admission by neighboring hospitals. MethodsWe utilized data from a Regional Health Information Exchange to assess the frequency that patients known to have MRSA at one healthcare system are admitted to a neighboring healthcare system unaware of their MRSA status. We conducted a retrospective, registry trial from January 1999 through January 2006 involving three healthcare systems in central Indianapolis, representing six hospitals. ResultsOver one year, 286 unique patients generated 587 admissions accounting for 4,335 inpatient days where the receiving hospital was not aware of the prior history of MRSA. The patients accounted for an additional 10\% of MRSA admissions received by study hospitals over one year and over 3,600 inpatient days without contact isolation. ConclusionsInformation exchange could improve timely identification of known MRSA patients within an urban setting.},
  keywords = {LSTM,RNN,rnn-gates},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8VWNJ6X9\\Cummins, Gers, Schmidhuber - 2000 - Learning to Forget Continual Prediction with LSTM(2).pdf}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, George},
  year = {1989},
  journal = {Mathematics of control, signals and systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  keywords = {learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H8D2LHSL\\Cybenko - 1989 - Approximation by superpositions of a sigmoidal function.pdf}
}

@article{czarneckiUnderstandingSyntheticGradients2017,
  title = {Understanding {{Synthetic Gradients}} and {{Decoupled Neural Interfaces}}},
  author = {Czarnecki, Wojciech Marian and {\'S}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2017},
  month = mar,
  abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KDLYBS6B\\Czarnecki et al. - 2017 - Understanding Synthetic Gradients and Decoupled Neural Interfaces(2).pdf}
}

@article{daiLearningCombinatorialOptimization2017,
  title = {Learning {{Combinatorial Optimization Algorithms}} over {{Graphs}}},
  author = {Dai, Hanjun and Khalil, Elias B. and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  year = {2017},
  month = apr,
  abstract = {The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.},
  keywords = {DGN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LMDGYBHS\\Dai et al. - 2017 - Learning Combinatorial Optimization Algorithms over Graphs.pdf}
}

@article{daiSemisupervisedSequenceLearning2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  year = {2015},
  month = nov,
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\X2KAXTQK\\Dai, Le - 2015 - Semi-supervised Sequence Learning(2).pdf}
}

@article{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  journal = {arXiv:1901.02860 [cs, stat]},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KXUAAG55\\Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;C\:\\Users\\w-32\\Zotero\\storage\\CYIAYNQX\\1901.html}
}

@incollection{dandekarDifferentialPrivacyRegularised2018,
  title = {Differential {{Privacy}} for {{Regularised Linear Regression}}},
  booktitle = {Database and {{Expert Systems Applications}}},
  author = {Dandekar, Ashish and Basu, Debabrota and Bressan, St{\'e}phane},
  editor = {Hartmann, Sven and Ma, Hui and Hameurlain, Abdelkader and Pernul, G{\"u}nther and Wagner, Roland R.},
  year = {2018},
  volume = {11030},
  pages = {483--491},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-98812-2_44},
  abstract = {We present -differentially private functional mechanisms for variants of regularised linear regression, LASSO, Ridge, and elastic net. We empirically and comparatively analyse their effectiveness. We quantify the error incurred by these -differentially private functional mechanisms with respect to the non-private linear regression. We show that the functional mechanism is more effective than the state-of-art differentially private mechanism using input perturbation for the three main regularised linear regression models. We also discuss caveats in the functional mechanism, such as non-convexity of the noisy loss function, which causes instability in the results.},
  isbn = {978-3-319-98811-5 978-3-319-98812-2},
  langid = {english},
  keywords = {diff-privacy,project-dp-rnn-galli,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IJEL4IB9\\Dandekar et al. - 2018 - Differential Privacy for Regularised Linear Regres.pdf}
}

@article{dangovskiRotationalUnitMemory2017,
  title = {Rotational {{Unit}} of {{Memory}}},
  author = {Dangovski, Rumen and Jing, Li and Soljacic, Marin},
  year = {2017},
  month = oct,
  abstract = {The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks. However, RNN still have a limited capacity to manipulate long-term memory. To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is, naturally, a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem. Moreover, the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks. RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task. RUM's performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QXK995IV\\Dangovski, Jing, Soljacic - 2017 - Rotational Unit of Memory(2).pdf}
}

@inproceedings{davariProbingRepresentationForgetting2022,
  title = {Probing {{Representation Forgetting}} in {{Supervised}} and {{Unsupervised Continual Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Davari, MohammadReza and Asadi, Nader and Mudur, Sudhir and Aljundi, Rahaf and Belilovsky, Eugene},
  year = {2022},
  pages = {16712--16721},
  langid = {english},
  keywords = {Continual,continual-representation-learning,linear-probe},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\49BDRJX7\\Davari_Probing_Representation_Forgetting_in_Supervised_and_Unsupervised_Continual_Learning_CVPR.html}
}

@article{DBLP:journals/corr/abs-1901-05498,
  title = {Deep Learning-Based Electroencephalography Analysis: A Systematic Review},
  author = {Roy, Yannick and Banville, Hubert J. and Albuquerque, Isabela and Gramfort, Alexandre and Falk, Tiago H. and Faubert, Jocelyn},
  year = {2019},
  journal = {CoRR},
  volume = {abs/1901.05498},
  eprint = {1901.05498},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/abs-1901-05498.bib},
  keywords = {human_state_monitoring},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100}
}

@article{deanLargeScaleDistributed2012,
  title = {Large Scale Distributed Deep Networks},
  author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V},
  year = {2012},
  journal = {Advances in Neural Information Processing Systems},
  pages = {1223--1231},
  issn = {9781627480031},
  doi = {10.1109/ICDAR.2011.95},
  abstract = {Recentwork in unsupervised feature learning and deep learning has shown that be- ing able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network train- ing. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k cate- gories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition ser- vice. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
  keywords = {software},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RTYM2WPC\\Dean et al. - 2012 - Large scale distributed deep networks.pdf}
}

@inproceedings{decaroAIasaServiceToolkitHumanCentered2022a,
  title = {{{AI-as-a-Service Toolkit}} for {{Human-Centered Intelligence}} in {{Autonomous Driving}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications Workshops}} and Other {{Affiliated Events}} ({{PerCom Workshops}})},
  author = {De Caro, Valerio and Bano, Saira and Machumilane, Achilles and Gotta, Alberto and Cassar{\`a}, Pietro and Carta, Antonio and Semola, Rudy and Sardianos, Christos and Chronis, Christos and Varlamis, Iraklis and Tserpes, Konstantinos and Lomonaco, Vincenzo and Gallicchio, Claudio and Bacciu, Davide},
  year = {2022},
  month = mar,
  pages = {91--93},
  doi = {10.1109/PerComWorkshops53856.2022.9767501},
  abstract = {This paper presents a proof-of-concept implementation of the AI-as-a-Service toolkit developed within the H2020 TEACHING project and designed to implement an autonomous driving personalization system according to the output of an automatic driver's stress recognition algorithm, both of them realizing a Cyber-Physical System of Systems. In addition, we implemented a data-gathering subsystem to collect data from different sensors, i.e., wearables and cameras, to automatize stress recognition. The system was attached for testing to a driving emulation software, CARLA, which allows testing the approach's feasibility with minimum cost and without putting at risk drivers and passengers. At the core of the relative subsystems, different learning algorithms were implemented using Deep Neural Networks, Recurrent Neural Networks, and Reinforcement Learning.},
  keywords = {teaching},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2YD3VQT6\\9767501.html}
}

@misc{decaroThesisProposalValerio2022,
  title = {Thesis {{Proposal Valerio De Caro}} (Internal Committee)},
  author = {De Caro, Valerio},
  year = {2022},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QANLCDBL\\phd_thesis_proposal_DeCaro.pdf}
}

@article{dehghaniUniversalTransformers2018,
  title = {Universal {{Transformers}}},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  year = {2018},
  month = jul,
  keywords = {attention,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9P5CCR8W\\Dehghani et al. - 2018 - Universal Transformers(2).pdf}
}

@article{delangeContinualEvaluationLifelong2022,
  title = {Continual Evaluation for Lifelong Learning: {{Identifying}} the Stability Gap},
  shorttitle = {Continual Evaluation for Lifelong Learning},
  author = {De Lange, Matthias and {van de Ven}, Gido and Tuytelaars, Tinne},
  year = {2022},
  month = may,
  doi = {10.48550/arXiv.2205.13452},
  abstract = {Introducing a time dependency on the data generating distribution has proven to be difficult for gradient-based training of neural networks, as the greedy updates result in catastrophic forgetting of previous timesteps. Continual learning aims to overcome the greedy optimization to enable continuous accumulation of knowledge over time. The data stream is typically divided into locally stationary distributions, called tasks, allowing task-based evaluation on held-out data from the training tasks. Contemporary evaluation protocols and metrics in continual learning are task-based and quantify the trade-off between stability and plasticity only at task transitions. However, our empirical evidence suggests that between task transitions significant, temporary forgetting can occur, remaining unidentified in task-based evaluation. Therefore, we propose a framework for continual evaluation that establishes per-iteration evaluation and define a new set of metrics that enables identifying the worst-case performance of the learner over its lifetime. Performing continual evaluation, we empirically identify that replay suffers from a stability gap: upon learning a new task, there is a substantial but transient decrease in performance on past tasks. Further conceptual and empirical analysis suggests not only replay-based, but also regularization-based continual learning methods are prone to the stability gap.},
  langid = {english},
  keywords = {anytime-inference,continual,evaluation,ocl,stability-plasticity},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FAHKE33L\\De Lange et al_2022_Continual evaluation for lifelong learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\UFHJ2PFA\\2205.html}
}

@article{delangeContinualLearningSurvey2022,
  title = {A {{Continual Learning Survey}}: {{Defying Forgetting}} in {{Classification Tasks}}},
  shorttitle = {A {{Continual Learning Survey}}},
  author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v s} and Slabaugh, Gregory and Tuytelaars, Tinne},
  year = {2022},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {7},
  pages = {3366--3385},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3057446},
  abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern: (1) a taxonomy and extensive overview of the state-of-the-art; (2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner; (3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods; and (4) baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
  keywords = {class-incremental,continual,review},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DZ488GMU\\De Lange et al_2022_A Continual Learning Survey.pdf;C\:\\Users\\w-32\\Zotero\\storage\\RZGCAZSX\\9349197.html}
}

@article{delangeContinualPrototypeEvolution2021,
  title = {Continual {{Prototype Evolution}}: {{Learning Online}} from {{Non-Stationary Data Streams}}},
  shorttitle = {Continual {{Prototype Evolution}}},
  author = {De Lange, Matthias and Tuytelaars, Tinne},
  year = {2021},
  month = feb,
  journal = {arXiv:2009.00919 [cs]},
  eprint = {2009.00919},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PKL2VCUY\\De Lange_Tuytelaars_2021_Continual Prototype Evolution.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MQUCECV3\\2009.html}
}

@article{demosthenousContinualLearningEdge2021,
  title = {Continual {{Learning}} on the {{Edge}} with {{TensorFlow Lite}}},
  author = {Demosthenous, Giorgos and Vassiliades, Vassilis},
  year = {2021},
  month = may,
  journal = {arXiv:2105.01946 [cs]},
  eprint = {2105.01946},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deploying sophisticated deep learning models on embedded devices with the purpose of solving real-world problems is a struggle using today's technology. Privacy and data limitations, network connection issues, and the need for fast model adaptation are some of the challenges that constitute today's approaches unfit for many applications on the edge and make real-time on-device training a necessity. Google is currently working on tackling these challenges by embedding an experimental transfer learning API to their TensorFlow Lite, machine learning library. In this paper, we show that although transfer learning is a good first step for on-device model training, it suffers from catastrophic forgetting when faced with more realistic scenarios. We present this issue by testing a simple transfer learning model on the CORe50 benchmark as well as by demonstrating its limitations directly on an Android application we developed. In addition, we expand the TensorFlow Lite library to include continual learning capabilities, by integrating a simple replay approach into the head of the current transfer learning model. We test our continual learning model on the CORe50 benchmark to show that it tackles catastrophic forgetting, and we demonstrate its ability to continually learn, even under non-ideal conditions, using the application we developed. Finally, we open-source the code of our Android application to enable developers to integrate continual learning to their own smartphone applications, as well as to facilitate further development of continual learning functionality into the TensorFlow Lite environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LAEBNYNV\\Demosthenous_Vassiliades_2021_Continual Learning on the Edge with TensorFlow Lite.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VWQSUQDD\\2105.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,notag,Ontologies,Robustness,Spine},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZGFFNQIA\\Deng et al_2009_ImageNet.pdf;C\:\\Users\\w-32\\Zotero\\storage\\RU9UR5KT\\5206848.html}
}

@article{dengLatentAlignmentVariational2018,
  title = {Latent {{Alignment}} and {{Variational Attention}}},
  author = {Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander M.},
  year = {2018},
  month = jul,
  abstract = {Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.},
  keywords = {attention,RNN,vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HC6FPD2F\\Deng et al. - 2018 - Latent Alignment and Variational Attention(3).pdf}
}

@inproceedings{dengNewTypesDeep2013,
  title = {New Types of Deep Neural Network Learning for Speech Recognition and Related Applications: {{An}} Overview},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
  year = {2013},
  pages = {8599--8603},
  keywords = {review,speech}
}

@article{denoyerSaLinASequentialLearning2021,
  title = {{{SaLinA}}: {{Sequential Learning}} of {{Agents}}},
  shorttitle = {{{SaLinA}}},
  author = {Denoyer, Ludovic and {de la Fuente}, Alfredo and Duong, Song and Gaya, Jean-Baptiste and Kamienny, Pierre-Alexandre and Thompson, Daniel H.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07910 [cs]},
  eprint = {2110.07910},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {SaLinA is a simple library that makes implementing complex sequential learning models easy, including reinforcement learning algorithms. It is built as an extension of PyTorch: algorithms coded with \textbackslash SALINA\{\} can be understood in few minutes by PyTorch users and modified easily. Moreover, SaLinA naturally works with multiple CPUs and GPUs at train and test time, thus being a good fit for the large-scale training use cases. In comparison to existing RL libraries, SaLinA has a very low adoption cost and capture a large variety of settings (model-based RL, batch RL, hierarchical RL, multi-agent RL, etc.). But SaLinA does not only target RL practitioners, it aims at providing sequential learning capabilities to any deep learning programmer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Python,pytorch,RL,software},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5UXE2RC8\\Denoyer et al. - 2021 - SaLinA Sequential Learning of Agents.pdf;C\:\\Users\\w-32\\Zotero\\storage\\V3HMYXVY\\2110.html}
}

@article{desaiContinualLearningDifferential2021,
  title = {Continual {{Learning}} with {{Differential Privacy}}},
  author = {Desai, Pradnya and Lai, Phung and Phan, NhatHai and Thai, My T.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.05223 [cs]},
  eprint = {2110.05223},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we focus on preserving differential privacy (DP) in continual learning (CL), in which we train ML models to learn a sequence of new tasks while memorizing previous tasks. We first introduce a notion of continual adjacent databases to bound the sensitivity of any data record participating in the training process of CL. Based upon that, we develop a new DP-preserving algorithm for CL with a data sampling strategy to quantify the privacy risk of training data in the well-known Averaged Gradient Episodic Memory (A-GEM) approach by applying a moments accountant. Our algorithm provides formal guarantees of privacy for data records across tasks in CL. Preliminary theoretical analysis and evaluations show that our mechanism tightens the privacy loss while maintaining a promising model utility.},
  archiveprefix = {arXiv},
  keywords = {continual,diff-privacy},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Desai et al_2021_Continual Learning with Differential Privacy.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KH9W7MUQ\\Commenti di FIlippo Galli.pdf;C\:\\Users\\w-32\\Zotero\\storage\\TJ47CIPV\\2110.html}
}

@article{desjardinsNaturalNeuralNetworks2015,
  title = {Natural {{Neural Networks}}},
  author = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray},
  year = {2015},
  month = jul,
  abstract = {We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DHQZ3QYZ\\Desjardins et al. - 2015 - Natural Neural Networks.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {{{arXiv}}:1810.04805 [Cs]},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {BERT,nlp,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5WU8K2S7\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;C\:\\Users\\w-32\\Zotero\\storage\\CA777WC4\\1810.html}
}

@article{diaz-rodriguezDonForgetThere2018,
  title = {Don't Forget, There Is More than Forgetting: New Metrics for {{Continual Learning}}},
  author = {{D{\'i}az-Rodr{\'i}guez}, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},
  year = {2018},
  number = {Nips},
  abstract = {Continual learning consists of algorithms that learn from a stream of data/tasks continuously and adaptively thought time, enabling the incremental development of ever more complex knowledge and skills. The lack of consensus in evaluating continual learning algorithms and the almost exclusive focus on forgetting motivate us to propose a more comprehensive set of implementation independent metrics accounting for several factors we believe have practical implications worth considering in the deployment of real AI systems that learn continually: accuracy or performance over time, backward and forward knowledge transfer, memory overhead as well as computational efficiency. Drawing inspiration from the standard Multi-Attribute Value Theory (MAVT) we further propose to fuse these metrics into a single score for ranking purposes and we evaluate our proposal with five continual learning strategies on the iCIFAR-100 continual learning benchmark.},
  keywords = {continual,evaluation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5JHESYET\\Daz-Rodrguez et al. - 2018 - Don't forget, there is more than forgetting new metrics for Continual Learning(2).pdf}
}

@misc{diaz-ruizIthaca365DatasetDriving2022,
  title = {Ithaca365: {{Dataset}} and {{Driving Perception}} under {{Repeated}} and {{Challenging Weather Conditions}}},
  shorttitle = {Ithaca365},
  author = {{Diaz-Ruiz}, Carlos A. and Xia, Youya and You, Yurong and Nino, Jose and Chen, Junan and Monica, Josephine and Chen, Xiangyu and Luo, Katie and Wang, Yan and Emond, Marc and Chao, Wei-Lun and Hariharan, Bharath and Weinberger, Kilian Q. and Campbell, Mark},
  year = {2022},
  month = aug,
  number = {arXiv:2208.01166},
  eprint = {2208.01166},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.01166},
  abstract = {Advances in perception for self-driving cars have accelerated in recent years due to the availability of large-scale datasets, typically collected at specific locations and under nice weather conditions. Yet, to achieve the high safety requirement, these perceptual systems must operate robustly under a wide variety of weather conditions including snow and rain. In this paper, we present a new dataset to enable robust autonomous driving via a novel data collection process - data is repeatedly recorded along a 15 km route under diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time (day/night), and traffic conditions (pedestrians, cyclists and cars). The dataset includes images and point clouds from cameras and LiDAR sensors, along with high-precision GPS/INS to establish correspondence across routes. The dataset includes road and object annotations using amodal masks to capture partial occlusions and 3D bounding boxes. We demonstrate the uniqueness of this dataset by analyzing the performance of baselines in amodal segmentation of road and objects, depth estimation, and 3D object detection. The repeated routes opens new research directions in object discovery, continual learning, and anomaly detection. Link to Ithaca365: https://ithaca365.mae.cornell.edu/},
  archiveprefix = {arXiv},
  keywords = {autonomous-driving,continual,dataset,domain-incremental,object-detection,weather},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\R89T3FZ7\\2208.html}
}

@article{dinhSharpMinimaCan2017,
  title = {Sharp {{Minima Can Generalize For Deep Nets}}},
  author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  year = {2017},
  abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
  keywords = {generalization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\V3FWQ75H\\Dinh et al. - 2017 - Sharp Minima Can Generalize For Deep Nets.pdf}
}

@article{diniCombiningQlearningArtificial2012,
  title = {Combining Q-Learning with Artificial Neural Networks in an Adaptive Light Seeking Robot},
  author = {Dini, Steve and Serrano, Mark},
  year = {2012},
  journal = {Swarthmore College, CS81 Adaptive Robotics Final Projects Swarthmore College},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ET6CTC54\\Q_learning_ANN.pdf}
}

@article{dipietroAnalyzingExploitingNARX2017,
  title = {Analyzing and {{Exploiting NARX Recurrent Neural Networks}} for {{Long-Term Dependencies}}},
  author = {DiPietro, Robert and Rupprecht, Christian and Navab, Nassir and Hager, Gregory D.},
  year = {2017},
  month = feb,
  abstract = {Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FX6CZAZQ\\DiPietro et al. - 2017 - Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies(2).pdf}
}

@article{ditzlerLearningNonstationaryEnvironments2015,
  title = {Learning in {{Nonstationary Environments}}: {{A Survey}}},
  author = {Ditzler, Gregory and Roveri, Manuel and Alippi, Cesare and Polikar, Robi},
  year = {2015},
  journal = {IEEE Computational Intelligence Magazine},
  volume = {10},
  number = {4},
  pages = {12--25},
  doi = {10.1109/MCI.2015.2471196},
  abstract = {The prevalence of mobile phones, the internet-of-things technology, and networks of sensors has led to an enormous and ever increasing amount of data that are now more commonly available in a streaming fashion [1]-[5]. Often, it is assumed - either implicitly or explicitly - that the process generating such a stream of data is stationary, that is, the data are drawn from a fixed, albeit unknown probability distribution. In many real-world scenarios, however, such an assumption is simply not true, and the underlying process generating the data stream is characterized by an intrinsic nonstationary (or evolving or drifting) phenomenon. The nonstationarity can be due, for example, to seasonality or periodicity effects, changes in the users' habits or preferences, hardware or software faults affecting a cyber-physical system, thermal drifts or aging effects in sensors. In such nonstationary environments, where the probabilistic properties of the data change over time, a non-adaptive model trained under the false stationarity assumption is bound to become obsolete in time, and perform sub-optimally at best, or fail catastrophically at worst.},
  keywords = {online learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JFTER4CJ\\Ditzler et al. - 2015 - Learning in Nonstationary Environments A Survey.pdf}
}

@article{DLIAUserGuide,
  title = {\{\vphantom\}{{DLIA}}\vphantom\{\} {{User Guide}}},
  keywords = {software}
}

@article{doanTheoreticalAnalysisCatastrophic2020,
  title = {A {{Theoretical Analysis}} of {{Catastrophic Forgetting}} through the {{NTK Overlap Matrix}}},
  author = {Doan, Thang and Bennani, Mehdi and Mazoure, Bogdan and Rabusseau, Guillaume and Alquier, Pierre},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.04003 [cs, stat]},
  eprint = {2010.04003},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Continual learning (CL) is a setting in which an agent has to learn from an incoming stream of data during its entire lifetime. Although major advances have been made in the field, one recurring problem which remains unsolved is that of Catastrophic Forgetting (CF). While the issue has been extensively studied empirically, little attention has been paid from a theoretical angle. In this paper, we show that the impact of CF increases as two tasks increasingly align. We introduce a measure of task similarity called the NTK overlap matrix which is at the core of CF. We analyze common projected gradient algorithms and demonstrate how they mitigate forgetting. Then, we propose a variant of Orthogonal Gradient Descent (OGD) which leverages structure of the data through Principal Component Analysis (PCA). Experiments support our theoretical findings and show how our method reduces CF on classical CL datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,task-similarity},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9HTYEWPP\\Doan et al_2020_A Theoretical Analysis of Catastrophic Forgetting through the NTK Overlap Matrix.pdf;C\:\\Users\\w-32\\Zotero\\storage\\FMZTCFFZ\\2010.html}
}

@article{doerschTutorialVariationalAutoencoders2016,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  month = jun,
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  keywords = {vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TI39GQKH\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf}
}

@article{dohareContinualBackpropStochastic2021,
  title = {Continual {{Backprop}}: {{Stochastic Gradient Descent}} with {{Persistent Randomness}}},
  shorttitle = {Continual {{Backprop}}},
  author = {Dohare, Shibhansh and Mahmood, A. Rupam and Sutton, Richard S.},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.06325 [cs]},
  eprint = {2108.06325},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The Backprop algorithm for learning in neural networks utilizes two mechanisms: first, stochastic gradient descent and second, initialization with small random weights, where the latter is essential to the effectiveness of the former. We show that in continual learning setups, Backprop performs well initially, but over time its performance degrades. Stochastic gradient descent alone is insufficient to learn continually; the initial randomness enables only initial learning but not continual learning. To the best of our knowledge, ours is the first result showing this degradation in Backprop's ability to learn. To address this issue, we propose an algorithm that continually injects random features alongside gradient descent using a new generate-and-test process. We call this the Continual Backprop algorithm. We show that, unlike Backprop, Continual Backprop is able to continually adapt in both supervised and reinforcement learning problems. We expect that as continual learning becomes more common in future applications, a method like Continual Backprop will be essential where the advantages of random initialization are present throughout learning.},
  archiveprefix = {arXiv},
  keywords = {cl-sgd,Computer Science - Machine Learning,continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Dohare et al_2021_Continual Backprop.pdf;C\:\\Users\\w-32\\Zotero\\storage\\SIAJNM8C\\2108.html}
}

@article{doi:10.1162/089976602760407955,
  title = {Real-Time Computing without Stable States: {{A}} New Framework for Neural Computation Based on Perturbations},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  year = {2002},
  journal = {Neural Computation},
  volume = {14},
  number = {11},
  eprint = {https://doi.org/10.1162/089976602760407955},
  pages = {2531--2560},
  doi = {10.1162/089976602760407955},
  abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.}
}

@article{domingosFewUsefulThings2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  year = {2012},
  journal = {Communications of the ACM},
  volume = {55},
  number = {10},
  pages = {78--87},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SZVQL6KS\\Domingos - A Few Useful Things to Know about Machine Learning.pdf}
}

@inproceedings{dongBridgingNonCooccurrence2021a,
  ids = {dongBridgingNonCooccurrence2021},
  title = {Bridging {{Non Co-occurrence}} with {{Unlabeled In-the-wild Data}} for {{Incremental Object Detection}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {DONG, NA and Zhang, Yongqiang and Ding, Mingli and Lee, Gim Hee},
  year = {2021},
  volume = {34},
  eprint = {2110.15017},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {30492--30503},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Deep networks have shown remarkable results in the task of object detection. However, their performance suffers critical drops when they are subsequently trained on novel classes without any sample from the base classes originally used to train the model. This phenomenon is known as catastrophic forgetting. Recently, several incremental learning methods are proposed to mitigate catastrophic forgetting for object detection. Despite the effectiveness, these methods require co-occurrence of the unlabeled base classes in the training data of the novel classes. This requirement is impractical in many real-world settings since the base classes do not necessarily co-occur with the novel classes. In view of this limitation, we consider a more practical setting of complete absence of co-occurrence of the base and novel classes for the object detection task. We propose the use of unlabeled in-the-wild data to bridge the non co-occurrence caused by the missing base classes during the training of additional novel classes. To this end, we introduce a blind sampling strategy based on the responses of the base-class model and pre-trained novel-class model to select a smaller relevant dataset from the large in-the-wild dataset for incremental learning. We then design a dual-teacher distillation framework to transfer the knowledge distilled from the base- and novel-class teacher models to the student model using the sampled in-the-wild data. Experimental results on the PASCAL VOC and MS COCO datasets show that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods when there is no co-occurrence between the base and novel classes during training.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CKXK6UX4\\Dong et al_2021_Bridging Non Co-occurrence with Unlabeled In-the-wild Data for Incremental.pdf;C\:\\Users\\w-32\\Zotero\\storage\\WJSS62GA\\2110.html}
}

@inproceedings{dongNeuralMeanDiscrepancy2022,
  title = {Neural {{Mean Discrepancy}} for {{Efficient Out-of-Distribution Detection}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Dong, Xin and Guo, Junfeng and Li, Ang and Ting, Wei-Te and Liu, Cong and Kung, H. T.},
  year = {2022},
  pages = {19217--19227},
  langid = {english},
  keywords = {ood-detection},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XT2JW98H\\Dong_Neural_Mean_Discrepancy_for_Efficient_Out-of-Distribution_Detection_CVPR_2022_paper.html}
}

@article{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  journal = {arXiv:2010.11929 [cs]},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {cv,transformer,vision},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Dosovitskiy et al_2021_An Image is Worth 16x16 Words.pdf;C\:\\Users\\w-32\\Zotero\\storage\\I4HC5IV6\\2010.html}
}

@article{dosovitskiyInvertingVisualRepresentations2016,
  title = {Inverting {{Visual Representations}} with {{Convolutional Networks}}},
  author = {Dosovitskiy, Alexey and Brox, Thomas},
  year = {2016},
  month = apr,
  journal = {arXiv:1506.02753 [cs]},
  eprint = {1506.02753},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,data-free,vision},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Dosovitskiy_Brox_2016_Inverting Visual Representations with Convolutional Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\CT486LIL\\1506.html}
}

@article{douillardContinuumSimpleManagement2021,
  title = {Continuum: {{Simple Management}} of {{Complex Continual Learning Scenarios}}},
  shorttitle = {Continuum},
  author = {Douillard, Arthur and Lesort, Timoth{\'e}e},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.06253 [cs]},
  eprint = {2102.06253},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Continual learning is a machine learning sub-field specialized in settings with non-iid data. Hence, the training data distribution is not static and drifts through time. Those drifts might cause interferences in the trained model and knowledge learned on previous states of the data distribution might be forgotten. Continual learning's challenge is to create algorithms able to learn an ever-growing amount of knowledge while dealing with data distribution drifts. One implementation difficulty in these field is to create data loaders that simulate non-iid scenarios. Indeed, data loaders are a key component for continual algorithms. They should be carefully designed and reproducible. Small errors in data loaders have a critical impact on algorithm results, e.g. with bad preprocessing, wrong order of data or bad test set. Continuum is a simple and efficient framework with numerous data loaders that avoid researcher to spend time on designing data loader and eliminate time-consuming errors. Using our proposed framework, it is possible to directly focus on the model design by using the multiple scenarios and evaluation metrics implemented. Furthermore the framework is easily extendable to add novel settings for specific needs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QU9N594R\\Douillard_Lesort_2021_Continuum.pdf;C\:\\Users\\w-32\\Zotero\\storage\\QE5FH5JI\\2102.html}
}

@inproceedings{douillardDyToxTransformersContinual2022,
  title = {{{DyTox}}: {{Transformers}} for {{Continual Learning With DYnamic TOken eXpansion}}},
  shorttitle = {{{DyTox}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Douillard, Arthur and Ram{\'e}, Alexandre and Couairon, Guillaume and Cord, Matthieu},
  year = {2022},
  pages = {9285--9295},
  langid = {english},
  keywords = {Continual,knowledge-distillation,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\P2CSYPF7\\Douillard_DyTox_Transformers_for_Continual_Learning_With_DYnamic_TOken_eXpansion_CVPR_2022_pape.html}
}

@article{douillardPODNetPooledOutputs2020,
  title = {{{PODNet}}: {{Pooled Outputs Distillation}} for {{Small-Tasks Incremental Learning}}},
  shorttitle = {{{PODNet}}},
  author = {Douillard, Arthur and Cord, Matthieu and Ollion, Charles and Robert, Thomas and Valle, Eduardo},
  year = {2020},
  month = oct,
  journal = {arXiv:2004.13513 [cs]},
  eprint = {2004.13513},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks --a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatial-based distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively. Code is available at https://github.com/arthurdouillard/incremental\_learning.pytorch},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VTCIYAEZ\\Douillard et al_2020_PODNet.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EVAPUHW8\\2004.html}
}

@article{douillardSmallTaskIncrementalLearning2020,
  title = {Small-{{Task Incremental Learning}}},
  author = {Douillard, Arthur and Cord, Matthieu and Ollion, Charles and Robert, Thomas and Valle, Eduardo},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.13513 [cs]},
  eprint = {2004.13513},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks \textendash{} a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatialbased distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 4.83, and 2.85 percentage points, respectively. Code will be released at this http address.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {CIFAR,class-incremental,continual,ImageNet,small-task},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QYAE8FYX\\2004.13513.pdf}
}

@inproceedings{doyaBifurcationsLearningRecurrent1992,
  title = {Bifurcations in the Learning of Recurrent Neural Networks},
  booktitle = {[{{Proceedings}}] 1992 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {Doya, K.},
  year = {1992},
  month = may,
  volume = {6},
  pages = {2777-2780 vol.6},
  issn = {null},
  doi = {10.1109/ISCAS.1992.230622},
  abstract = {Gradient descent algorithms in recurrent neural networks can have problems when the network dynamics experience bifurcations in the course of learning. The possible hazards caused by the bifurcations of the network dynamics and the learning equations are investigated. The roles of teacher forcing, preprogramming of network structures, and the approximate learning algorithms are discussed.{$<>$}},
  keywords = {learning,network dynamics,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\L9RTJ2IN\\Doya - 1992 - Bifurcations in the learning of recurrent neural n.pdf;C\:\\Users\\w-32\\Zotero\\storage\\9N58HMEC\\230622.html}
}

@article{draxlerEssentiallyNoBarriers,
  title = {Essentially {{No Barriers}} in {{Neural Network Energy Landscape}}},
  author = {Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred A},
  pages = {10},
  langid = {english},
  keywords = {mode-connectivity},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DVVIDUTJ\\Draxler et al. - Essentially No Barriers in Neural Network Energy L.pdf}
}

@article{dridiDeepHistDeepLearningbased2019,
  title = {{{DeepHist}} : {{Towards}} a {{Deep Learning-based Computational History}} of {{Trends}} in the {{NIPS}}},
  author = {Dridi, Amna and Azad, R Muhammad Atif},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2EQIJV83\\Dridi, Azad - 2019 - DeepHist Towards a Deep Learning-based Computational History of Trends in the NIPS(2).pdf}
}

@article{duCertRNNCertifyingRobustness2021,
  title = {Cert-{{RNN}}: {{Towards Certifying}} the {{Robustness}} of {{Recurrent Neural Networks}}},
  author = {Du, Tianyu and Ji, Shouling and Shen, Lujia and Zhang, Yao and Li, Jinfeng and Shi, Jie and Fang, Chengfang and Yin, Jianwei and Beyah, Raheem and Wang, Ting},
  year = {2021},
  journal = {South Korea},
  pages = {19},
  abstract = {Certifiable robustness, the functionality of verifying whether the given region surrounding a data point admits any adversarial example, provides guaranteed security for neural networks deployed in adversarial environments. A plethora of work has been proposed to certify the robustness of feed-forward networks, e.g., FCNs and CNNs. Yet, most existing methods cannot be directly applied to recurrent neural networks (RNNs), due to their sequential inputs and unique operations.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CBUTU24Z\\Du et al. - 2021 - Cert-RNN Towards Certifying the Robustness of Rec.pdf}
}

@article{duMETANORMLEARNINGNORMALIZE2021,
  title = {{{METANORM}}: {{LEARNING TO NORMALIZE FEW-SHOT BATCHES ACROSS DOMAINS}}},
  author = {Du, Yingjun and Zhen, Xiantong and Shao, Ling and Snoek, Cees G M},
  year = {2021},
  pages = {23},
  abstract = {Batch normalization plays a crucial role when training deep neural networks. However, batch statistics become unstable with small batch sizes and are unreliable in the presence of distribution shifts. We propose MetaNorm, a simple yet effective meta-learning normalization. It tackles the aforementioned issues in a unified way by leveraging the meta-learning setting and learns to infer adaptive statistics for batch normalization. MetaNorm is generic, flexible and model-agnostic, making it a simple plug-and-play module that is seamlessly embedded into existing meta-learning approaches. It can be efficiently implemented by lightweight hypernetworks with low computational cost. We verify its effectiveness by extensive evaluation on representative tasks suffering from the small batch and domain shift problems: few-shot learning and domain generalization. We further introduce an even more challenging setting: few-shot domain generalization. Results demonstrate that MetaNorm consistently achieves better, or at least competitive, accuracy compared to existing batch normalization methods.},
  langid = {english},
  keywords = {notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UGEAMVEJ\\Du et al. - 2021 - METANORM LEARNING TO NORMALIZE FEW-SHOT BATCHES A.pdf}
}

@article{dumoulinGuideConvolutionArithmetic2016,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2016},
  journal = {arXiv preprint arXiv:1603.07285},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UFJRJXHJ\\convolution_arithmetic.pdf}
}

@article{dunckerOrganizingRecurrentNetwork,
  title = {Organizing Recurrent Network Dynamics by Task-Computation to Enable Continual Learning},
  author = {Duncker, Lea and Driscoll, Laura N and Shenoy, Krishna V and Sahani, Maneesh and Sussillo, David},
  pages = {11},
  abstract = {Biological systems face dynamic environments that require continual learning. It is not well understood how these systems balance the tension between flexibility for learning and robustness for memory of previous behaviors. Continual learning without catastrophic interference also remains a challenging problem in machine learning. Here, we develop a novel learning rule designed to minimize interference between sequentially learned tasks in recurrent networks. Our learning rule preserves network dynamics within activity-defined subspaces used for previously learned tasks. It encourages dynamics associated with new tasks that might otherwise interfere to instead explore orthogonal subspaces, and it allows for reuse of previously established dynamical motifs where possible. Employing a set of tasks used in neuroscience, we demonstrate that our approach successfully eliminates catastrophic interference and offers a substantial improvement over previous continual learning algorithms. Using dynamical systems analysis, we show that networks trained using our approach can reuse similar dynamical structures across similar tasks. This possibility for shared computation allows for faster learning during sequential training. Finally, we identify organizational differences that emerge when training tasks sequentially versus simultaneously.},
  langid = {english},
  keywords = {continual,orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NNVQFMJ7\\Duncker et al. - Organizing recurrent network dynamics by task-comp.pdf}
}

@inproceedings{duranIntelManyIntegrated2012,
  title = {The {{Intel}}\textregistered many Integrated Core Architecture},
  booktitle = {High {{Performance Computing}} and {{Simulation}} ({{HPCS}}), 2012 {{International Conference}} On},
  author = {Duran, Alejandro and Klemm, Michael},
  year = {2012},
  pages = {365--366},
  keywords = {hardware}
}

@inproceedings{dworkAnalyzeGaussOptimal2014,
  title = {Analyze Gauss: Optimal Bounds for Privacy-Preserving Principal Component Analysis},
  shorttitle = {Analyze Gauss},
  booktitle = {Proceedings of the Forty-Sixth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Dwork, Cynthia and Talwar, Kunal and Thakurta, Abhradeep and Zhang, Li},
  year = {2014},
  month = may,
  pages = {11--20},
  publisher = {{ACM}},
  address = {{New York New York}},
  doi = {10.1145/2591796.2591883},
  abstract = {We consider the problem of privately releasing a low dimensional approximation to a set of data records, represented as a matrix A in which each row corresponds to an individual and each column to an attribute. Our goal is to compute a subspace that captures the covariance of A as much as possible, classically known as principal component analysis (PCA). We assume that each row of A has 2 norm bounded by one, and the privacy guarantee is defined with respect to addition or removal of any single row. We show that the well-known, but misnamed, randomized response algorithm, with properly tuned parameters, provides nearly optimal additive quality gap compared to the best possible singular subspace of A. We further show that when AT A has a large eigenvalue gap \textendash{} a reason often cited for PCA \textendash{} the quality improves significantly. Optimality (up to logarithmic factors) is proved using techniques inspired by the recent work of Bun, Ullman, and Vadhan on applying Tardos's fingerprinting codes to the construction of hard instances for private mechanisms for 1-way marginal queries. Along the way we define a list culling game which may be of independent interest.},
  isbn = {978-1-4503-2710-7},
  langid = {english},
  keywords = {diff-privacy},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EGFHVMMG\\Dwork et al. - 2014 - Analyze gauss optimal bounds for privacy-preservi.pdf}
}

@inproceedings{dworkCalibratingNoiseSensitivity2006,
  title = {Calibrating {{Noise}} to {{Sensitivity}} in {{Private Data Analysis}}},
  booktitle = {Theory of {{Cryptography}}},
  author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  editor = {Halevi, Shai and Rabin, Tal},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {265--284},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11681878_14},
  abstract = {We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.Previous work focused on the case of noisy sums, in which f = {$\sum$} i g(x i ), where x i denotes the ith row of the database and g maps database rows to [0,1]. We extend the study to general functions f, proving that privacy can be preserved by calibrating the standard deviation of the noise according to the sensitivity of the function f. Roughly speaking, this is the amount that any single argument to f can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case.The first step is a very clean characterization of privacy in terms of indistinguishability of transcripts. Additionally, we obtain separation results showing the increased value of interactive sanitization mechanisms over non-interactive.},
  isbn = {978-3-540-32732-5},
  langid = {english},
  keywords = {diff-privacy,Laplace Distribution,Privacy Breach,Query Function,Semantic Security,True Answer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WCSVRUJR\\Dwork et al_2006_Calibrating Noise to Sensitivity in Private Data Analysis.pdf}
}

@article{ebrahimiUncertaintyguidedContinualLearning2020,
  title = {Uncertainty-Guided {{Continual Learning}} with {{Bayesian Neural Networks}}},
  author = {Ebrahimi, Sayna and Elhoseiny, Mohamed and Darrell, Trevor and Rohrbach, Marcus},
  year = {2020},
  month = feb,
  journal = {arXiv:1906.02425 [cs, stat]},
  eprint = {1906.02425},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters' \textbackslash textit\{importance\}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in networks. Uncertainty is a natural way to identify \textbackslash textit\{what to remember\} and \textbackslash textit\{what to change\} as we continually learn, and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning and retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.},
  archiveprefix = {arXiv},
  keywords = {bayesian,continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HESPKKEU\\1906.html}
}

@article{ehretContinualLearningRecurrent2020,
  title = {Continual {{Learning}} in {{Recurrent Neural Networks}} with {{Hypernetworks}}},
  author = {Ehret, Benjamin and Henning, Christian and Cervera, Maria R. and Meulemans, Alexander and {von Oswald}, Johannes and Grewe, Benjamin F.},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.12109 [cs, stat]},
  eprint = {2006.12109},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The last decade has seen a surge of interest in continual learning (CL), and a variety of methods have been developed to alleviate catastrophic forgetting. However, most prior work has focused on tasks with static data, while CL on sequential data has remained largely unexplored. Here we address this gap in two ways. First, we evaluate the performance of established CL methods when applied to recurrent neural networks (RNNs). We primarily focus on elastic weight consolidation, which is limited by a stability-plasticity trade-off, and explore the particularities of this trade-off when using sequential data. We show that high working memory requirements, but not necessarily sequence length, lead to an increased need for stability at the cost of decreased performance on subsequent tasks. Second, to overcome this limitation we employ a recent method based on hypernetworks and apply it to RNNs to address catastrophic forgetting on sequential data. By generating the weights of a main RNN in a task-dependent manner, our approach disentangles stability and plasticity, and outperforms alternative methods in a range of experiments. Overall, our work provides several key insights on the differences between CL in feedforward networks and in RNNs, while offering a novel solution to effectively tackle CL on sequential data.},
  archiveprefix = {arXiv},
  keywords = {architectural,continual,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7JXLR3DJ\\Ehret et al. - 2020 - Continual Learning in Recurrent Neural Networks wi.pdf;C\:\\Users\\w-32\\Zotero\\storage\\WDI5L4CP\\2006.html}
}

@misc{el-noubyAreLargescaleDatasets2021,
  title = {Are {{Large-scale Datasets Necessary}} for {{Self-Supervised Pre-training}}?},
  author = {{El-Nouby}, Alaaeldin and Izacard, Gautier and Touvron, Hugo and Laptev, Ivan and Jegou, Herv{\'e} and Grave, Edouard},
  year = {2021},
  month = dec,
  number = {arXiv:2112.10740},
  eprint = {2112.10740},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10740},
  abstract = {Pre-training models on large scale datasets, like ImageNet, is a standard practice in computer vision. This paradigm is especially effective for tasks with small training sets, for which high-capacity models tend to overfit. In this work, we consider a self-supervised pre-training scenario that only leverages the target task data. We consider datasets, like Stanford Cars, Sketch or COCO, which are order(s) of magnitude smaller than Imagenet. Our study shows that denoising autoencoders, such as BEiT or a variant that we introduce in this paper, are more robust to the type and size of the pre-training data than popular self-supervised methods trained by comparing image embeddings.We obtain competitive performance compared to ImageNet pre-training on a variety of classification datasets, from different domains. On COCO, when pre-training solely using COCO images, the detection and instance segmentation performance surpasses the supervised ImageNet pre-training in a comparable setting.},
  archiveprefix = {arXiv},
  keywords = {large-models,self-supervised,vision},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ICHZ3QDL\\2112.html}
}

@article{elmanFindingStructureTime1990,
  title = {Finding Structure in Time},
  author = {Elman, Jeffrey L},
  year = {1990},
  journal = {Cognitive science},
  volume = {14},
  number = {2},
  pages = {179--211},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EXHSDBPT\\Elman - 1990 - Finding structure in time.pdf}
}

@misc{EmotionMeterMultimodalFramework,
  title = {{{EmotionMeter}}: {{A Multimodal Framework}} for {{Recognizing Human Emotions}}},
  shorttitle = {{{EmotionMeter}}},
  abstract = {In this paper, we present a multimodal emotion recognition framework called EmotionMeter that combines brain waves and eye movements. To increase the feasibility and wearability of EmotionMeter in real-world applications, we design a six-electrode placement above the ears to collect electroencephalography (EEG) signals. We combine EEG and eye movements for integrating the internal cognitive states and external subconscious behaviors of users to improve the recognition accuracy of EmotionMeter. The experimental results demonstrate that modality fusion with multimodal deep neural networks can significantly enhance the performance compared with a single modality, and the best mean accuracy of 85.11\% is achieved for four emotions (happy, sad, fear, and neutral). We explore the complementary characteristics of EEG and eye movements for their representational capacities and identify that EEG has the advantage of classifying happy emotion, whereas eye movements outperform EEG in recognizing fear emotion. To investigate the stability of EmotionMeter over time, each subject performs the experiments three times on different days. EmotionMeter obtains a mean recognition accuracy of 72.39\% across sessions with the six-electrode EEG and eye movement features. These experimental results demonstrate the effectiveness of EmotionMeter within and between sessions.},
  howpublished = {https://ieeexplore.ieee.org/document/8283814},
  langid = {american},
  keywords = {human_state_monitoring},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5CWZSR25\\8283814.html}
}

@article{ergenEfficientOnlineLearning2018,
  title = {Efficient Online Learning Algorithms Based on {{LSTM}} Neural Networks},
  author = {Ergen, Tolga and Kozat, Suleyman Serdar},
  year = {2018},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {8},
  pages = {3772--3783},
  doi = {10.1109/TNNLS.2017.2741598},
  abstract = {We investigate online nonlinear regression and introduce novel regression structures based on the long short term memory (LSTM) networks. For the introduced structures, we also provide highly efficient and effective online training methods. To train these novel LSTM-based structures, we put the underlying architecture in a state space form and introduce highly efficient and effective particle filtering (PF)-based updates. We also provide stochastic gradient descent and extended Kalman filter-based updates. Our PF-based training method guarantees convergence to the optimal parameter estimation in the mean square error sense provided that we have a sufficient number of particles and satisfy certain technical conditions. More importantly, we achieve this performance with a computational complexity in the order of the first-order gradient-based methods by controlling the number of particles. Since our approach is generic, we also introduce a gated recurrent unit (GRU)-based approach by directly replacing the LSTM architecture with the GRU architecture, where we demonstrate the superiority of our LSTM-based approach in the sequential prediction task via different real life data sets. In addition, the experimental results illustrate significant performance improvements achieved by the introduced algorithms with respect to the conventional methods over several different benchmark real life data sets.},
  keywords = {Kalman,LSTM,online learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SMM8FKDX\\Ergen, Kozat - 2018 - Efficient online learning algorithms based on LSTM neural networks(2).pdf}
}

@article{ethayarajhUnderstandingLinearWord2018,
  title = {Towards {{Understanding Linear Word Analogies}}},
  author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
  year = {2018},
  abstract = {A surprising property of word vectors is that vector algebra can often be used to solve word analogies. However, it is unclear why -- and when -- linear operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a rigorous explanation of this phenomenon without making the strong assumptions that past work has made about the vector space and word distribution. Our theory has several implications. Past work has often conjectured that linear structures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel theoretical justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, providing rigorous justification for its use in capturing word dissimilarity.},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IGU5SLG5\\Ethayarajh, Duvenaud, Hirst - 2018 - Towards Understanding Linear Word Analogies(3).pdf}
}

@article{ethayarajhUnsupervisedRandomWalk2018,
  title = {Unsupervised {{Random Walk Sentence Embeddings}}: {{A Strong}} but {{Simple Baseline}}},
  author = {Ethayarajh, Kawin},
  year = {2018},
  journal = {ACL},
  pages = {1--10},
  abstract = {Using a random walk model of text gen-eration, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings: take a weighted average of word embeddings and modify with SVD. This simple method even outperforms far more complex approaches such as LSTMs on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.'s model. We propose a random walk model that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our ap-proach beats Arora et al.'s by up to 44.4\% on textual similarity tasks and is competi-tive with state-of-the-art methods. Unlike Arora et al.'s method, ours requires no hy-perparameter tuning, which means it can be used when there is no labelled data.},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IGRAT3Y2\\Ethayarajh - 2018 - Unsupervised Random Walk Sentence Embeddings A Strong but Simple Baseline(2).pdf}
}

@article{EuropeanStrategyParticle2013,
  title = {The {{European Strategy}} for {{Particle Physics Update}} 2013. {{La}} Strat\dbend gie Europ\dbend enne Pour La Physique Des Particules {{Mise}} \dbend{} Jour 2013. 16th {{Session}} of {{European Strategy Council}}},
  year = {2013},
  keywords = {high-energy-physics}
}

@article{fabiusVariationalRecurrentAutoEncoders2014,
  title = {Variational {{Recurrent Auto-Encoders}}},
  author = {Fabius, Otto and {van Amersfoort}, Joost R.},
  year = {2014},
  month = dec,
  abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
  keywords = {RNN,vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Y5FPL7TE\\Fabius, van Amersfoort - 2014 - Variational Recurrent Auto-Encoders(3).pdf}
}

@article{fagotEvidenceLargeLongterm2006,
  title = {Evidence for Large Long-Term Memory Capacities in Baboons and Pigeons and Its Implications for Learning and the Evolution of Cognition},
  author = {Fagot, Jo{\"e}l and Cook, Robert G.},
  year = {2006},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {103},
  number = {46},
  pages = {17564--17567},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0605184103},
  abstract = {Previous research has shown that birds and primates have a rich repertoire of behavioral and cognitive skills, but the mechanisms underlying these abilities are not well understood. A common hypothesis is that these adaptations are mediated by an efficient long-term memory, allowing animals to remember specific external events and associate appropriate behaviors to these events. Because earlier studies have not sufficiently challenged memory capacity in animals, our comparative research examined with equivalent procedures the size and mechanisms of long-term memory in baboons and pigeons. Findings revealed very large, but different, capacities in both species to learn and remember picture\textendash response associations. Pigeons could maximally memorize between 800 and 1,200 picture\textendash response associations before reaching the limit of their performance. In contrast, baboons minimally memorized 3,500\textendash 5,000 items and had not reached their limit after more than 3 years of testing. No differences were detected in how these associations were retained or otherwise processed by these species. These results demonstrate that pigeons and monkeys have sufficient memory resources to develop memory-based exemplar or feature learning strategies in many test situations. They further suggest that the evolution of cognition and behavior importantly may have involved the gradual enlargement of the long-term memory capacities of the brain.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2006 by The National Academy of Sciences of the USA.                    Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {17088563},
  keywords = {bird,categorization,cl-neuroscience,continual,intelligence,picture processing,primate},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Fagot_Cook_2006_Evidence for large long-term memory capacities in baboons and pigeons and its.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EC93P8TB\\17564.html}
}

@article{fahlmannCascadeCorrelationLearningArchitecture1990,
  title = {The {{Cascade-Correlation Learning Architecture}}},
  author = {Fahlmann, Scott E. and Lebiere, Christian},
  year = {1990},
  journal = {Advances in Neural Information Processing Systems (NIPS)},
  pages = {524--532},
  issn = {1558601007},
  doi = {10.1190/1.1821929},
  abstract = {Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network . determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.},
  keywords = {cascade correlation}
}

@article{fahlmanRecurrentCascadecorrelationLearning1991,
  title = {The Recurrent Cascade-Correlation Learning Algorithm},
  author = {Fahlman, S E},
  year = {1991},
  journal = {Advances in Neural Information Processing Systems (NIPS)},
  pages = {190--196},
  keywords = {cascade correlation,RNN}
}

@article{fangContrastiveModelInversion2021,
  title = {Contrastive {{Model Inversion}} for {{Data-Free Knowledge Distillation}}},
  author = {Fang, Gongfan and Song, Jie and Wang, Xinchao and Shen, Chengchao and Wang, Xingen and Song, Mingli},
  year = {2021},
  month = may,
  journal = {arXiv:2105.08584 [cs]},
  eprint = {2105.08584},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Model inversion, whose goal is to recover training data from a pre-trained model, has been recently proved feasible. However, existing inversion methods usually suffer from the mode collapse problem, where the synthesized instances are highly similar to each other and thus show limited effectiveness for downstream tasks, such as knowledge distillation. In this paper, we propose Contrastive Model Inversion\textasciitilde (CMI), where the data diversity is explicitly modeled as an optimizable objective, to alleviate the mode collapse issue. Our main observation is that, under the constraint of the same amount of data, higher data diversity usually indicates stronger instance discrimination. To this end, we introduce in CMI a contrastive learning objective that encourages the synthesizing instances to be distinguishable from the already synthesized ones in previous batches. Experiments of pre-trained models on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CMI not only generates more visually plausible instances than the state of the arts, but also achieves significantly superior performance when the generated data are used for knowledge distillation. Code is available at \textbackslash url\{https://github.com/zju-vipa/DataFree\}.},
  archiveprefix = {arXiv},
  keywords = {exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PIZI7LPQ\\Fang et al_2021_Contrastive Model Inversion for Data-Free Knowledge Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MXLP2IGT\\2105.html}
}

@inproceedings{fangMosaickingDistillKnowledge2021,
  title = {Mosaicking to {{Distill}}: {{Knowledge Distillation}} from {{Out-of-Domain Data}}},
  shorttitle = {Mosaicking to {{Distill}}},
  booktitle = {Thirty-{{Fifth Conference}} on {{Neural Information Processing Systems}}},
  author = {Fang, Gongfan and Bao, Yifan and Song, Jie and Wang, Xinchao and Xie, Donglin and Shen, Chengchao and Song, Mingli},
  year = {2021},
  month = may,
  abstract = {Knowledge distillation\textasciitilde (KD) aims to craft a compact student model that imitates the behavior of a pre-trained teacher in a target domain. Prior KD approaches, despite their gratifying results, have...},
  langid = {english},
  keywords = {data-free,exmodel,knowledge distillation,notag,ood-detection},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Fang et al_2021_Mosaicking to Distill.pdf;C\:\\Users\\w-32\\Zotero\\storage\\X9NL7E53\\forum.html}
}

@article{farajtabarOrthogonalGradientDescent2019,
  title = {Orthogonal {{Gradient Descent}} for {{Continual Learning}}},
  author = {Farajtabar, Mehrdad and Azizan, Navid and Mott, Alex and Li, Ang},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.07104 [cs, stat]},
  eprint = {1910.07104},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural networks are achieving state of the art and sometimes super-human performance on learning tasks across a variety of domains. Whenever these problems require learning in a continual or sequential manner, however, neural networks suffer from the problem of catastrophic forgetting; they forget how to solve previous tasks after being trained on a new task, despite having the essential capacity to solve both tasks if they were trained on both simultaneously. In this paper, we propose to address this issue from a parameter space perspective and study an approach to restrict the direction of the gradient updates to avoid forgetting previously-learned data. We present the Orthogonal Gradient Descent (OGD) method, which accomplishes this goal by projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task. Our approach utilizes the high capacity of a neural network more efficiently and does not require storing the previously learned data that might raise privacy concerns. Experiments on common benchmarks reveal the effectiveness of the proposed OGD method.},
  archiveprefix = {arXiv},
  keywords = {continual,regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\86V4W6H6\\Farajtabar et al. - 2019 - Orthogonal Gradient Descent for Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\E8SH535J\\1910.html}
}

@article{FARKAS2016109,
  title = {Computational Analysis of Memory Capacity in Echo State Networks},
  author = {Farka{\v s}, Igor and Bos{\'a}k, Radom{\'i}r and Gerge{\v l}, Peter},
  year = {2016},
  journal = {Neural Networks},
  volume = {83},
  pages = {109--120},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2016.07.012},
  abstract = {Reservoir computing became very popular due to its potential for efficient design of recurrent neural networks, exploiting the computational properties of the reservoir structure. Various approaches, ranging from appropriate reservoir initialization to its optimization by training have been proposed. In this paper, we extend our previous work and focus on short-term memory capacity, introduced by Jaeger in case of echo state networks. Memory capacity has been previously shown to peak at criticality, when the network switches from a stable regime to an unstable dynamic regime. Using computational experiments with nonlinear ESNs, we systematically analyze the memory capacity from the perspective of several parameters and their relationship, namely the input and reservoir weights scaling, reservoir size and its sparsity. We also derive and test two gradient descent based orthogonalization procedures for recurrent weights matrix, which considerably increase the memory capacity, approaching the upper bound, which is equal to the reservoir size, as proved for linear reservoirs. Orthogonalization procedures are discussed in the context of existing methods and their benefit is assessed.},
  keywords = {Echo-state network,Memory capacity,Reservoir orthogonalization,Spectral properties}
}

@article{farquharRobustEvaluationsContinual2018,
  title = {Towards {{Robust Evaluations}} of {{Continual Learning}}},
  author = {Farquhar, Sebastian and Gal, Yarin},
  year = {2018},
  month = may,
  abstract = {The experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. We examine standard evaluations and show why these evaluations make some types of continual learning approaches look better than they are. In particular, current evaluations are biased towards continual learning approaches that treat previous models as a prior (e.g., EWC, VCL). We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Our analysis calls for a reprioritization of research effort by the community.},
  keywords = {continual,evaluation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H34AJVJZ\\Farquhar, Gal - 2018 - Towards Robust Evaluations of Continual Learning(2).pdf}
}

@misc{farquharUnderstandingApproximationBayesian2022,
  title = {Understanding {{Approximation}} for {{Bayesian Inference}} in {{Neural Networks}}},
  author = {Farquhar, Sebastian},
  year = {2022},
  month = nov,
  number = {arXiv:2211.06139},
  eprint = {2211.06139},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.06139},
  abstract = {Bayesian inference has theoretical attractions as a principled framework for reasoning about beliefs. However, the motivations of Bayesian inference which claim it to be the only 'rational' kind of reasoning do not apply in practice. They create a binary split in which all approximate inference is equally 'irrational'. Instead, we should ask ourselves how to define a spectrum of more- and less-rational reasoning that explains why we might prefer one Bayesian approximation to another. I explore approximate inference in Bayesian neural networks and consider the unintended interactions between the probabilistic model, approximating distribution, optimization algorithm, and dataset. The complexity of these interactions highlights the difficulty of any strategy for evaluating Bayesian approximations which focuses entirely on the method, outside the context of specific datasets and decision-problems. For given applications, the expected utility of the approximate posterior can measure inference quality. To assess a model's ability to incorporate different parts of the Bayesian framework we can identify desirable characteristic behaviours of Bayesian reasoning and pick decision-problems that make heavy use of those behaviours. Here, we use continual learning (testing the ability to update sequentially) and active learning (testing the ability to represent credence). But existing continual and active learning set-ups pose challenges that have nothing to do with posterior quality which can distort their ability to evaluate Bayesian approximations. These unrelated challenges can be removed or reduced, allowing better evaluation of approximate inference methods.},
  archiveprefix = {arXiv},
  keywords = {bayesian,continual,thesis},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DD6Y88RS\\Farquhar_2022_Understanding Approximation for Bayesian Inference in Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\A2SK6ZQG\\2211.html}
}

@article{farquharUnifyingBayesianView2019,
  title = {A {{Unifying Bayesian View}} of {{Continual Learning}}},
  author = {Farquhar, Sebastian and Gal, Yarin},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.06494 [cs, stat]},
  eprint = {1902.06494},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Some machine learning applications require continual learning - where data comes in a sequence of datasets, each is used for training and then permanently discarded. From a Bayesian perspective, continual learning seems straightforward: Given the model posterior one would simply use this as the prior for the next task. However, exact posterior evaluation is intractable with many models, especially with Bayesian neural networks (BNNs). Instead, posterior approximations are often sought. Unfortunately, when posterior approximations are used, prior-focused approaches do not succeed in evaluations designed to capture properties of realistic continual learning use cases. As an alternative to prior-focused methods, we introduce a new approximate Bayesian derivation of the continual learning loss. Our loss does not rely on the posterior from earlier tasks, and instead adapts the model itself by changing the likelihood term. We call these approaches likelihood-focused. We then combine prior- and likelihood-focused methods into one objective, tying the two views together under a single unifying framework of approximate Bayesian continual learning.},
  archiveprefix = {arXiv},
  keywords = {bayesian,continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HYYKEHBB\\1902.html}
}

@misc{febrinantoGraphLifelongLearning2022,
  title = {Graph {{Lifelong Learning}}: {{A Survey}}},
  shorttitle = {Graph {{Lifelong Learning}}},
  author = {Febrinanto, Falih Gozi and Xia, Feng and Moore, Kristen and Thapa, Chandra and Aggarwal, Charu},
  year = {2022},
  month = nov,
  number = {arXiv:2202.10688},
  eprint = {2202.10688},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.10688},
  abstract = {Graph learning is a popular approach for performing machine learning on graph-structured data. It has revolutionized the machine learning ability to model graph data to address downstream tasks. Its application is wide due to the availability of graph data ranging from all types of networks to information systems. Most graph learning methods assume that the graph is static and its complete structure is known during training. This limits their applicability since they cannot be applied to problems where the underlying graph grows over time and/or new tasks emerge incrementally. Such applications require a lifelong learning approach that can learn the graph continuously and accommodate new information whilst retaining previously learned knowledge. Lifelong learning methods that enable continuous learning in regular domains like images and text cannot be directly applied to continuously evolving graph data, due to its irregular structure. As a result, graph lifelong learning is gaining attention from the research community. This survey paper provides a comprehensive overview of recent advancements in graph lifelong learning, including the categorization of existing methods, and the discussions of potential applications and open research problems.},
  archiveprefix = {arXiv},
  keywords = {68T07; 68T05,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual,dgn,I.2.6},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EKQTK677\\2202.html}
}

@article{fernandoPathnetEvolutionChannels2017,
  title = {Pathnet: {{Evolution}} Channels Gradient Descent in Super Neural Networks},
  shorttitle = {Pathnet},
  author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.08734},
  eprint = {1701.08734},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\V9JGEKV8\\PathNet.pdf}
}

@inproceedings{fernandoPatternRecognitionBucket2003,
  title = {Pattern {{Recognition}} in a {{Bucket}}},
  booktitle = {Advances in {{Artificial Life}}},
  author = {Fernando, Chrisantha and Sojakka, Sampsa},
  editor = {Banzhaf, Wolfgang and Ziegler, Jens and Christaller, Thomas and Dittrich, Peter and Kim, Jan T.},
  year = {2003},
  pages = {588--597},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {This paper demonstrates that the waves produced on the surface of water can be used as the medium for a ``Liquid State Machine'' that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass' Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this ``for free'', and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.},
  isbn = {978-3-540-39432-7}
}

@article{firthSynopsisLinguisticTheory1957,
  title = {A Synopsis of Linguistic Theory, 1930-1955},
  author = {Firth, John R},
  year = {1957},
  journal = {Studies in linguistic analysis},
  keywords = {nlp}
}

@article{flennerhagMetaLearningWarpedGradient2020,
  title = {Meta-{{Learning}} with {{Warped Gradient Descent}}},
  author = {Flennerhag, Sebastian and Rusu, Andrei A. and Pascanu, Razvan and Visin, Francesco and Yin, Hujun and Hadsell, Raia},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.00025 [cs, stat]},
  eprint = {1909.00025},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In this work, we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution. Preconditioning arises by interleaving non-linear layers, referred to as warp-layers, between the layers of a task-learner. Warp-layers are meta-learned without backpropagating through the task training process in a manner similar to methods that learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can scale to arbitrarily large meta-learning problems. We provide a geometrical interpretation of the approach and evaluate its effectiveness in a variety of settings, including few-shot, standard supervised, continual and reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,meta-learn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6A5NJZ5C\\Flennerhag et al_2020_Meta-Learning with Warped Gradient Descent.pdf;C\:\\Users\\w-32\\Zotero\\storage\\P6WM6SKB\\1909.html}
}

@article{florioConvolutionalNeuralNetwork2018,
  title = {Convolutional {{Neural Network}} for {{Track Seed Filtering}} at the \{\vphantom\}{{CMS}}\vphantom\{\} {{High-Level Trigger}}},
  author = {Florio, Adriano Di and Pantaleo, Felice and Carta, Antonio},
  year = {2018},
  journal = {Journal of Physics: Conference Series},
  volume = {1085},
  pages = {42040--42040},
  doi = {10.1088/1742-6596/1085/4/042040}
}

@article{forcione-lambertInvestigationMemoryRecurrent2019,
  title = {An {{Investigation}} of {{Memory}} in {{Recurrent Neural Networks}}},
  author = {{Forcione-Lambert}, Aude and Wolf, Guy and Lajoie, Guillaume},
  year = {2019},
  month = may,
  abstract = {We investigate the learned dynamical landscape of a recurrent neural network solving a simple task requiring the interaction of two memory mechanisms: long- and short-term. Our results show that...},
  keywords = {Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IRT3QNQP\\Forcione-Lambert et al. - 2019 - An Investigation of Memory in Recurrent Neural Net.pdf;C\:\\Users\\w-32\\Zotero\\storage\\DTTJPDR7\\forum.html}
}

@article{fouratiEEGFeatureLearning,
  title = {{{EEG}} Feature Learning with {{Intrinsic Plasticity}} Based {{Deep Echo State Network}}},
  author = {Fourati, Rahma and Ammar, Boudour and Jin, Yaochu and Alimi, Adel M},
  pages = {8},
  abstract = {In this paper, deep EEG feature learning method is proposed for emotion recognition. It is well known that EEG signals dramatically vary from person to person, thereby making subject-independent emotion recognition very challenging. To address the above challenge, this work presents a deep echo state network (DeepESN) to learn temporal representation from raw EEG data. DeepESN as an input-driven discrete time non-linear dynamical system allows to process the temporal information at each time step in a deep temporal fashion by means of a hierarchical composition of multiple levels of recurrent neurons. To make the DeepESN robust, we pre-train the reservoir connections with an unsupervised intrinsic plasticity rule to generate activities following a desired Gaussian distribution. Then, we propose a hybrid learning algorithm for training the output weights which benefits from both the ridge regression and the online delta rule. Our leaky DeepESN achieved encouraging results when tested on the well-known affective benchmarks DEAP and DREAMER.},
  langid = {english},
  keywords = {notag,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5ZGYW9SZ\\Fourati et al. - EEG feature learning with Intrinsic Plasticity bas.pdf}
}

@article{francois-lavetHowDiscountDeep2015,
  title = {How to Discount Deep Reinforcement Learning: {{Towards}} New Dynamic Strategies},
  shorttitle = {How to Discount Deep Reinforcement Learning},
  author = {{Fran{\c c}ois-Lavet}, Vincent and Fonteneau, Raphael and Ernst, Damien},
  year = {2015},
  journal = {arXiv preprint arXiv:1512.02011},
  eprint = {1512.02011},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ADQJEK39\\Discount Factor in Deep Reinforcement Learning - Dynamic Strategies.pdf}
}

@inproceedings{frankleLinearModeConnectivity2022,
  title = {Linear Mode Connectivity and the Lottery Ticket Hypothesis},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  year = {2022},
  month = apr,
  series = {{{ICML}}'20},
  pages = {3259--3269},
  publisher = {{JMLR.org}},
  abstract = {We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).},
  keywords = {mode-connectivity},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7KP6BCIL\\Frankle et al_2022_Linear mode connectivity and the lottery ticket hypothesis.pdf}
}

@inproceedings{frankleLotteryTicketHypothesis2022,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2022},
  month = feb,
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  langid = {english},
  keywords = {lottery-ticket-hypothesis,sparsity},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9AUPX9D5\\Frankle_Carbin_2022_The Lottery Ticket Hypothesis.pdf;C\:\\Users\\w-32\\Zotero\\storage\\HCFTXP2Y\\forum.html}
}

@article{frasconiGeneralFrameworkAdaptive1998,
  title = {A General Framework for Adaptive Processing of Data Structures},
  author = {Frasconi, P. and Gori, M. and Sperduti, A.},
  year = {1998},
  journal = {IEEE Transactions on Neural Networks},
  volume = {9},
  number = {5},
  pages = {768--786},
  issn = {1045-9227},
  doi = {10.1109/72.712151},
  abstract = {A structured organization of information is typically required by symbolic processing. On the other hand, most connectionist models assume that data are organized according to relatively poor structures, like arrays or sequences. The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information. In particular, relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist. The general framework proposed in this paper can be regarded as an extension of both recurrent neural networks and hidden Markov models to the case of acyclic graphs. In particular we study the supervised learning problem as the problem of learning transductions from an input structured space to an output structured space, where transductions are assumed to admit a recursive hidden state-space representation. We introduce a graphical formalism for representing this class of adaptive transductions by means of recursive networks, i,e,, cyclic graphs where nodes are labeled by variables and edges are labeled by generalized delay elements, This representation makes it possible to incorporate the symbolic and subsymbolic nature of data. Structures are processed by unfolding the recursive network into an acyclic graph called encoding network. In so doing, inference and learning algorithms can be easily inherited from the corresponding algorithms for artificial neural networks or probabilistic graphical model.},
  keywords = {DGN,HMM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JD9R5WUB\\Frasconi, Gori, Sperduti - 1998 - A general framework for adaptive processing of data structures(2).pdf}
}

@article{frenchCatastrophicForgettingConnectionist1999,
  title = {Catastrophic Forgetting in Connectionist Networks},
  author = {French, Robert M.},
  year = {1999},
  month = apr,
  journal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {4},
  pages = {128--135},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(99)01294-2},
  abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget `catastrophically'. Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting. The challenge in this field is to discover how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks are examined. The review will consider how the brain might have overcome this problem and will also explore the consequences of this solution for distributed connectionist networks.},
  langid = {english},
  keywords = {Catastrophic forgetting,catastrophic-forgetting,Connectionism,Connectionist networks,continual,Interference,Learning,Memory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7NT8KE7K\\S1364661399012942.html}
}

@article{furlanelloBornAgainNeural,
  title = {Born {{Again Neural Networks}}},
  author = {Furlanello, Tommaso and Lipton, Zachary C and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  abstract = {Knowledge distillation (KD) consists of trans-ferring " knowledge " from one machine learn-ing model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's com-pactness. We study KD from a new perspec-tive: rather than compressing models, we train students parameterized identically to their teach-ers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language model-ing tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art perfor-mance on the CIFAR-10 (3.5\%) and CIFAR-100 (15.5\%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predic-tions (DKPP). Both methods elucidate the essen-tial components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experi-ments show significant advantages from transfer-ring knowledge between DenseNets and ResNets in either direction.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3EA5PEWJ\\Furlanello et al. - Unknown - Born Again Neural Networks(2).pdf}
}

@article{GALLICCHIO2011440,
  title = {Architectural and {{Markovian}} Factors of Echo State Networks},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  year = {2011},
  journal = {Neural Networks},
  volume = {24},
  number = {5},
  pages = {440--456},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2011.02.002},
  abstract = {Echo State Networks (ESNs) constitute an emerging approach for efficiently modeling Recurrent Neural Networks (RNNs). In this paper we investigate some of the main aspects that can be accounted for the success and limitations of this class of models. In particular, we propose complementary classes of factors related to contractivity and architecture of reservoirs and we study their relative relevance. First, we show the existence of a class of tasks for which ESN performance is independent of the architectural design. The effect of the Markovian factor, characterizing a significant class within these cases, is shown by introducing instances of easy/hard tasks for ESNs featured by contractivity of reservoir dynamics. In the complementary cases, for which architectural design is effective, we investigate and decompose the aspects of network design that allow a larger reservoir to progressively improve the predictive performance. In particular, we introduce four key architectural factors: input variability, multiple time-scales dynamics, non-linear interactions among units and regression in an augmented feature space. To investigate the quantitative effects of the different architectural factors within this class of tasks successfully approached by ESNs, variants of the basic ESN model are proposed and tested on instances of datasets of different nature and difficulty. Experimental evidences confirm the role of the Markovian factor and show that all the identified key architectural factors have a major role in determining ESN performances.},
  keywords = {Architectural design analysis,Echo state networks,Markovianity,RNN,Sequence processing}
}

@article{Gallicchio2019ComparisonBD,
  title = {Comparison between {{DeepESNs}} and Gated {{RNNs}} on Multivariate Time-Series Prediction},
  author = {Gallicchio, C. and Micheli, A. and Pedrelli, L.},
  year = {2019},
  journal = {ESANN},
  volume = {abs/1812.11527}
}

@article{gallicchioDeepReservoirComputing2017,
  title = {Deep Reservoir Computing: {{A}} Critical Experimental Analysis},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  year = {2017},
  month = dec,
  journal = {Neurocomputing},
  volume = {268},
  pages = {87--99},
  doi = {10.1016/J.NEUCOM.2016.12.089},
  abstract = {In this paper, we propose an empirical analysis of deep recurrent neural network (RNN) architectures with stacked layers. The main aim is to address some fundamental open research issues on the significance of creating deep layered architectures in RNN and to characterize the inherent hierarchical representation of time in such models, especially for efficient implementations. In particular, the analysis aims at the study and proposal of approaches to develop and enhance hierarchical dynamics in deep architectures within the efficient Reservoir Computing (RC) framework for RNN modeling. The effect of a deep layered organization of RC models is investigated in terms of both occurrence of multiple time-scale and increasing of richness of the dynamics. It turns out that a deep layering of recurrent models allows an effective diversification of temporal representations in the layers of the hierarchy, by amplifying the effects of the factors influencing the time-scales and the richness of the dynamics, measured as the entropy of recurrent units activations. The advantages of the proposed approach are also highlighted by measuring the increment of the short-term memory capacity of the RC models.},
  keywords = {Deep ESN,ESN,RNN,unipi},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\M7MQPIX2\\Gallicchio, Micheli, Pedrelli - 2017 - Deep reservoir computing A critical experimental analysis(2).pdf}
}

@article{gallicchioDesignDeepEcho2018,
  title = {Design of Deep Echo State Networks},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  year = {2018},
  month = dec,
  journal = {Neural Networks},
  volume = {108},
  pages = {33--47},
  doi = {10.1016/j.neunet.2018.08.002},
  abstract = {In this paper, we provide a novel approach to the architectural design of deep Recurrent Neural Networks using signal frequency analysis. In particular, focusing on the Reservoir Computing framework and inspired by the principles related to the inherent effect of layering, we address a fundamental open issue in deep learning, namely the question of how to establish the number of layers in recurrent architectures in the form of deep echo state networks (DeepESNs). The proposed method is first analyzed and refined on a controlled scenario and then it is experimentally assessed on challenging real-world tasks. The achieved results also show the ability of properly designed DeepESNs to outperform RC approaches on a speech recognition task, and to compete with the state-of-the-art in time-series prediction on polyphonic music tasks.},
  keywords = {Deep ESN,ESN,RNN,unipi},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5FSK74FQ\\Gallicchio, Micheli, Pedrelli - 2018 - Design of deep echo state networks.pdf}
}

@article{gallicchioDesignDeepEcho2018a,
  title = {Design of Deep Echo State Networks},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  year = {2018},
  month = dec,
  journal = {Neural Networks},
  volume = {108},
  pages = {33--47},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.08.002},
  abstract = {In this paper, we provide a novel approach to the architectural design of deep Recurrent Neural Networks using signal frequency analysis. In particular, focusing on the Reservoir Computing framework and inspired by the principles related to the inherent effect of layering, we address a fundamental open issue in deep learning, namely the question of how to establish the number of layers in recurrent architectures in the form of deep echo state networks (DeepESNs). The proposed method is first analyzed and refined on a controlled scenario and then it is experimentally assessed on challenging real-world tasks. The achieved results also show the ability of properly designed DeepESNs to outperform RC approaches on a speech recognition task, and to compete with the state-of-the-art in time-series prediction on polyphonic music tasks.},
  langid = {english},
  keywords = {Architectural design of recurrent neural networks,Deep echo state networks,Deep recurrent neural networks,Echo state networks,ESN,memory,Reservoir computing,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HHAFF6M4\\S0893608018302223.html}
}

@article{gallicchioFastDeepGraph2019,
  title = {Fast and {{Deep Graph Neural Networks}}},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.08941 [cs, math, stat]},
  eprint = {1911.08941},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We address the efficiency issue for the construction of a deep graph neural network (GNN). The approach exploits the idea of representing each input graph as a fixed point of a dynamical system (implemented through a recurrent neural network), and leverages a deep architectural organization of the recurrent units. Efficiency is gained by many aspects, including the use of small and very sparse networks, where the weights of the recurrent units are left untrained under the stability condition introduced in this work. This can be viewed as a way to study the intrinsic power of the architecture of a deep GNN, and also to provide insights for the set-up of more complex fully-trained models. Through experimental results, we show that even without training of the recurrent connections, the architecture of small deep GNN is surprisingly able to achieve or improve the state-of-the-art performance on a significant set of tasks in the field of graphs classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AS8YDE7V\\Gallicchio_Micheli_2019_Fast and Deep Graph Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\Q2XPYH7L\\1911.html}
}

@article{gallicchioHierarchicalTemporalRepresentation2017,
  title = {Hierarchical {{Temporal Representation}} in {{Linear Reservoir Computing}}},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  year = {2017},
  month = may,
  abstract = {Recently, studies on deep Reservoir Computing (RC) highlighted the role of layering in deep recurrent neural networks (RNNs). In this paper, the use of linear recurrent units allows us to bring more evidence on the intrinsic hierarchical temporal representation in deep RNNs through frequency analysis applied to the state signals. The potentiality of our approach is assessed on the class of Multiple Superimposed Oscillator tasks. Furthermore, our investigation provides useful insights to open a discussion on the main aspects that characterize the deep learning framework in the temporal domain.},
  keywords = {Deep ESN,ESN,RNN,unipi},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BAM4X3EI\\Gallicchio, Micheli, Pedrelli - 2017 - Hierarchical Temporal Representation in Linear Reservoir Computing(2).pdf}
}

@article{gallicchioLocalLyapunovExponents2018,
  title = {Local {{Lyapunov}} Exponents of Deep Echo State Networks},
  author = {Gallicchio, Claudio and Micheli, Alessio and Silvestri, Luca},
  year = {2018},
  month = jul,
  journal = {Neurocomputing},
  volume = {298},
  pages = {34--45},
  issn = {09252312},
  doi = {10.1016/j.neucom.2017.11.073},
  abstract = {The analysis of deep Recurrent Neural Network (RNN) models represents a research area of increasing interest. In this context, the recent introduction of Deep Echo State Networks (DeepESNs) within the Reservoir Computing paradigm, enabled to study the intrinsic properties of hierarchically organized RNN architectures.In this paper we investigate the DeepESN model under a dynamical system perspective, aiming at characterizing the important aspect of stability of layered recurrent dynamics excited by external input signals.To this purpose, we develop a framework based on the study of the local Lyapunov exponents of stacked recurrent models, enabling the analysis and control of the resulting dynamical regimes. The introduced framework is demonstrated on artificial as well as real-world datasets. The results of our analysis on DeepESNs provide interesting insights on the real effect of layering in RNNs. In particular, they show that when recurrent units are organized in layers, then the resulting network intrinsically develops a richer dynamical behavior that is naturally driven closer to the edge of criticality. As confirmed by experiments on the short-term Memory Capacity task, this characterization makes the layered design effective, with respect to the shallow counterpart with the same number of units, especially in tasks that require much in terms of memory.},
  langid = {english},
  keywords = {ESN,RNN,unipi},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GQP2BNNY\\Gallicchio et al. - 2018 - Local Lyapunov exponents of deep echo state networ.pdf}
}

@article{gallicchioRingReservoirNeural2020,
  title = {Ring {{Reservoir Neural Networks}} for {{Graphs}}},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  year = {2020},
  month = may,
  journal = {arXiv:2005.05294 [cs, stat]},
  eprint = {2005.05294},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Machine Learning for graphs is nowadays a research topic of consolidated relevance. Common approaches in the field typically resort to complex deep neural network architectures and demanding training algorithms, highlighting the need for more efficient solutions. The class of Reservoir Computing (RC) models can play an important role in this context, enabling to develop fruitful graph embeddings through untrained recursive architectures. In this paper, we study progressive simplifications to the design strategy of RC neural networks for graphs. Our core proposal is based on shaping the organization of the hidden neurons to follow a ring topology. Experimental results on graph classification tasks indicate that ring-reservoirs architectures enable particularly effective network configurations, showing consistent advantages in terms of predictive performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {DGN,ESN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WBG54DFY\\Gallicchio and Micheli - 2020 - Ring Reservoir Neural Networks for Graphs.pdf}
}

@article{gallicchioShorttermMemoryDeep2018,
  title = {Short-Term Memory of Deep {{RNN}}},
  author = {Gallicchio, Claudio},
  year = {2018},
  journal = {ESANN 2018 - Proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  pages = {633--638},
  issn = {9782875870476},
  abstract = {The extension of deep learning towards temporal data processing is gaining an increasing research interest. In this paper we investigate the properties of state dynamics developed in successive levels of deep recurrent neural networks (RNNs) in terms of short-term memory abilities. Our results reveal interesting insights that shed light on the nature of layering as a factor of RNN design. Noticeably, higher layers in a hierarchically organized RNN architecture results to be inherently biased towards longer memory spans even prior to training of the recurrent connections. Moreover, in the context of Reservoir Computing framework, our analysis also points out the benefit of a layered recurrent organization as an efficient approach to improve the memory skills of reservoir models.},
  keywords = {ESN,Memory,RNN,unipi},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\758WLJU6\\Gallicchio - 2018 - Short-term memory of deep RNN.pdf}
}

@misc{galliGroupPrivacyPersonalized2022,
  title = {Group Privacy for Personalized Federated Learning},
  author = {Galli, Filippo and Biswas, Sayan and Jung, Kangsoo and Palamidessi, Catuscia and Cucinotta, Tommaso},
  year = {2022},
  month = jun,
  number = {arXiv:2206.03396},
  eprint = {2206.03396},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Federated learning (FL) is a particular type of collaborative machine learning, where participating peers/clients process their data locally, sharing only updates to the collaborative model. This enables to build privacy-aware distributed machine learning models, among others. The goal is the optimization of a statistical model's parameters by minimizing a cost function of a collection of datasets which are stored locally by a set of clients. This process exposes the clients to two issues: leakage of private information and lack of personalization of the model. On the other hand, with the recent advancements in various techniques to analyze and handle data, there is a surge of concern for the privacy violation of the participating clients. To mitigate this, differential privacy and its variants serve as a standard for providing formal privacy guarantees. Often the clients represent very heterogeneous communities and hold data which are very diverse. Therefore, aligned with the recent focus of the FL community to build a framework of personalized models for the users representing their diversity, it is also of utmost importance to protect against potential threats against the sensitive and personal information of the clients. -privacy, which is a generalization of geo-indistinguishability, the lately popularized paradigm of location privacy, uses a metric-based obfuscation technique that preserves the spatial distribution of the original data. To address the issue of protecting the privacy of the clients and allowing for personalized model training to enhance the fairness and utility of the system, we propose a method to provide group privacy guarantees exploiting some key properties of -privacy which enables personalized models under the framework of FL. We provide with theoretical justifications to the applicability and experimental validation on real-world datasets to illustrate the working of the proposed method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {diff-privacy,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IASBNJM9\\Galli et al. - 2022 - Group privacy for personalized federated learning.pdf}
}

@phdthesis{galUncertaintyDeepLearning2016,
  title = {Uncertainty in Deep Learning},
  author = {Gal, Yarin},
  year = {2016},
  school = {PhD thesis, University of Cambridge},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H4TFVD5R\\uncertainty in deep learning.pdf}
}

@inproceedings{gamaLearningDriftDetection2004,
  title = {Learning with {{Drift Detection}}},
  booktitle = {Advances in {{Artificial Intelligence}} \textendash{} {{SBIA}} 2004},
  author = {Gama, Jo{\~a}o and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
  editor = {Bazzan, Ana L. C. and Labidi, Sofiane},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {286--295},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28645-5_29},
  abstract = {Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classified using the actual model. Statistical theory guarantees that while the distribution is stationary, the error will decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we define a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example kw, and the drift level at example kd. This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since kw. The method was tested with a set of eight artificial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and with learning the new concept. We also observe that the method is independent of the learning algorithm.},
  isbn = {978-3-540-28645-5},
  langid = {english},
  keywords = {Concept Drift,drift-detection,Incremental Supervised Learning,Machine Learning}
}

@article{ganguliMemoryTracesDynamical2008,
  title = {Memory {{Traces}} in {{Dynamical Systems}} - {{Supplementary Material Contents}}},
  author = {Ganguli, S and Huh, D and Sompolinsky, Haim},
  year = {2008},
  journal = {Proceedings of the National Academy of Sciences},
  number = {3},
  pages = {1--13},
  doi = {10.1073/pnas.0804451105},
  abstract = {To perform nontrivial, real-time computations on a sensory input stream, biological systems must retain a short-term memory trace of their recent inputs. It has been proposed that generic high-dimensional dynamical systems could retain a memory trace for past inputs in their current state. This raises important questions about the fundamental limits of such memory traces and the properties required of dynamical systems to achieve these limits. We address these issues by applying Fisher information theory to dynamical systems driven by time-dependent signals corrupted by noise. We introduce the Fisher Memory Curve (FMC) as a measure of the signal-to-noise ratio (SNR) embedded in the dynamical state relative to the input SNR. The integrated FMC indicates the total memory capacity. We apply this theory to linear neuronal networks and show that the capacity of networks with normal connectivity matrices is exactly 1 and that of any network of N neurons is, at most, N. A nonnormal network achieving this bound is subject to stringent design constraints: It must have a hidden feedforward architecture that superlinearly amplifies its input for a time of order N, and the input connectivity must optimally match this architecture. The memory capacity of networks subject to saturating nonlinearities is further limited, and cannot exceed . This limit can be realized by feedforward structures with divergent fan out that distributes the signal across neurons, thereby avoiding saturation. We illustrate the generality of the theory by showing that memory in fluid systems can be sustained by transient nonnormal amplification due to convective instability or the onset of turbulence.},
  keywords = {LDS,Memory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NDAUREPQ\\Ganguli, Huh, Sompolinsky - 2008 - Memory Traces in Dynamical Systems - Supplementary Material Contents.pdf}
}

@article{ganguliMemoryTracesDynamical2008a,
  title = {Memory Traces in Dynamical Systems},
  author = {Ganguli, S and Huh, D and Sompolinsky, Haim},
  year = {2008},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {105},
  number = {48},
  pages = {18970--18975},
  issn = {1091-6490},
  doi = {10.1073/pnas.0804451105},
  abstract = {E-mail: \{at\}phy.ucsf.edu;  References. {$\carriagereturn$}: Lowenstein Y,;  H. (2003) Temporal integration by calcium dynamics in a model neuron.},
  keywords = {LDS,Memory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SS52NYB3\\Ganguli, Huh, Sompolinsky - 2008 - Memory traces in dynamical systems.pdf}
}

@article{gaoDemystifyingDropout2019,
  title = {Demystifying {{Dropout}}},
  author = {Gao, Hongchang and Pei, Jian and Huang, Heng},
  year = {2019},
  journal = {Icml},
  pages = {2112--2121},
  abstract = {Dropout is a popular technique to train large-scale deep neural networks to alleviate the overfitting problem. To disclose the underlying reasons for its gain, numerous works have tried to explain it from different perspectives. In this paper, unlike existing works, we explore it from a new perspective to provide new insight into this line of research. In detail, we disentangle the forward and backward pass of dropout. Then, we find that these two passes need different levels of noise to improve the generalization performance of deep neural networks. Based on this observation, we propose the augmented dropout which employs different dropping strategies in the forward and backward pass. Experimental results have verified the effectiveness of our proposed method.},
  keywords = {ICML,regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QE7MFU5Y\\Gao, Pei, Huang - 2019 - Demystifying Dropout.pdf}
}

@article{garipovLossSurfacesMode2018,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2018},
  month = oct,
  journal = {arXiv:1802.10026 [cs, stat]},
  eprint = {1802.10026},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
  archiveprefix = {arXiv},
  keywords = {exmodel,mode-connectivity,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\G5QYLGPB\\Garipov et al. - 2018 - Loss Surfaces, Mode Connectivity, and Fast Ensembl.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NQQF74CU\\1802.html}
}

@inproceedings{garofoloDARPATIMITAcousticphonetic1993,
  title = {{{DARPA TIMIT}}: : Acoustic-Phonetic Continuous Speech Corpus {{CD-ROM}}, {{NIST}} Speech Disc 1-1.1},
  author = {Garofolo, John S and Fisher, William M and Fiscus, Jonathan G and Pallett, David S and Dahlgren, Nancy L},
  year = {1993},
  keywords = {data}
}

@article{GATOGatesAre1999,
  title = {{{GATO}}: {{Gates Are Not The Only Option}}},
  year = {1999},
  volume = {42},
  number = {12},
  pages = {41--46},
  issn = {1891562401},
  keywords = {RNN,rnn-gates},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RH7WLIB2\\Unknown - 1999 - GATO Gates Are Not The Only Option.pdf}
}

@inproceedings{gatysImageStyleTransfer2016,
  title = {Image Style Transfer Using Convolutional Neural Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
  year = {2016},
  pages = {2414--2423},
  keywords = {CNN,style-transfer}
}

@article{gehringConvolutionalSequenceSequence2017,
  title = {Convolutional {{Sequence}} to {{Sequence Learning}}},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
  year = {2017},
  month = may,
  abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
  keywords = {CNN,seq2seq},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZGB9UPTF\\Gehring et al. - 2017 - Convolutional Sequence to Sequence Learning(3).pdf}
}

@article{gehrSafetyRobustnessCertification,
  title = {Safety and {{Robustness Certification}} of {{Neural Networks}} with {{Abstract Interpretation}}},
  author = {Gehr, Timon and Mirman, Matthew and {Drachsler-Cohen}, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  abstract = {\textemdash We present AI 2 , the first sound and scalable an-alyzer for deep neural networks. Based on overapproximation, AI 2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI 2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI 2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI 2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI 2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI 2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI 2 can handle deep convolutional networks, which are beyond the reach of existing methods.},
  keywords = {adversarial-examples},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KMYASNZH\\Gehr et al. - Unknown - Safety and Robustness Certification of Neural Networks with Abstract Interpretation(2).pdf}
}

@article{gelbard-sagivInternallyGeneratedReactivation2008,
  title = {Internally Generated Reactivation of Single Neurons in Human Hippocampus during Free Recall},
  author = {{Gelbard-Sagiv}, Hagar and Mukamel, Roy and Harel, Michal and Malach, Rafael and Fried, Itzhak},
  year = {2008},
  month = oct,
  journal = {Science (New York, N.Y.)},
  volume = {322},
  number = {5898},
  pages = {96--101},
  issn = {1095-9203},
  doi = {10.1126/science.1164685},
  abstract = {The emergence of memory, a trace of things past, into human consciousness is one of the greatest mysteries of the human mind. Whereas the neuronal basis of recognition memory can be probed experimentally in human and nonhuman primates, the study of free recall requires that the mind declare the occurrence of a recalled memory (an event intrinsic to the organism and invisible to an observer). Here, we report the activity of single neurons in the human hippocampus and surrounding areas when subjects first view cinematic episodes consisting of audiovisual sequences and again later when they freely recall these episodes. A subset of these neurons exhibited selective firing, which often persisted throughout and following specific episodes for as long as 12 seconds. Verbal reports of memories of these specific episodes at the time of free recall were preceded by selective reactivation of the same hippocampal and entorhinal cortex neurons. We suggest that this reactivation is an internally generated neuronal correlate for the subjective experience of spontaneous emergence of human recollection.},
  langid = {english},
  pmcid = {PMC2650423},
  pmid = {18772395},
  keywords = {Brain Mapping,cl-neuroscience,continual,Cues,Electrodes; Implanted,Entorhinal Cortex,Epilepsy,Hippocampus,Humans,Mental Recall,Neurons},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Gelbard-Sagiv et al_2008_Internally generated reactivation of single neurons in human hippocampus during.pdf}
}

@article{germainMADEMaskedAutoencoder2015,
  title = {{{MADE}}: {{Masked Autoencoder}} for {{Distribution Estimation}}},
  author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  year = {2015},
  month = feb,
  abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
  keywords = {autoencoders},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GAWBCIX6\\Germain et al. - 2015 - MADE Masked Autoencoder for Distribution Estimation(2).pdf}
}

@article{ghahramaniProbabilisticMachineLearning2015,
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  author = {Ghahramani, Zoubin},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {452--459},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14541},
  keywords = {bayesian},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZUM57FQX\\probabilistc machine learning nature review.pdf}
}

@article{ghazizadehSlowManifoldsRecurrent2021,
  title = {Slow Manifolds in Recurrent Networks Encode Working Memory Efficiently and Robustly},
  author = {Ghazizadeh, Elham and Ching, ShiNung},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.03163 [cs, q-bio]},
  eprint = {2101.03163},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Working memory is a cognitive function involving the storage and manipulation of latent information over brief intervals of time, thus making it crucial for context-dependent computation. Here, we use a top-down modeling approach to examine network-level mechanisms of working memory, an enigmatic issue and central topic of study in neuroscience and machine intelligence. We train thousands of recurrent neural networks on a working memory task and then perform dynamical systems analysis on the ensuing optimized networks, wherein we find that four distinct dynamical mechanisms can emerge. In particular, we show the prevalence of a mechanism in which memories are encoded along slow stable manifolds in the network state space, leading to a phasic neuronal activation profile during memory periods. In contrast to mechanisms in which memories are directly encoded at stable attractors, these networks naturally forget stimuli over time. Despite this seeming functional disadvantage, they are more efficient in terms of how they leverage their attractor landscape and paradoxically, are considerably more robust to noise. Our results provide new dynamical hypotheses regarding how working memory function is encoded in both natural and artificial neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Memory,Quantitative Biology - Neurons and Cognition,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\C3KTUX8P\\Ghazizadeh_Ching_2021_Slow manifolds in recurrent networks encode working memory efficiently and.pdf;C\:\\Users\\w-32\\Zotero\\storage\\QNBCBACF\\2101.html}
}

@article{giurgiuPixelHitReconstruction2008,
  ids = {giurgiuPixelHitReconstruction2008a},
  title = {Pixel {{Hit Reconstruction}} with the {{CMS Detector}}},
  author = {Giurgiu, Gavril A and others},
  year = {2008},
  journal = {arXiv preprint arXiv:0808.3804},
  eprint = {0808.3804},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {high-energy-physics}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {{{AISTATS}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  keywords = {learning}
}

@inproceedings{gomez-villaContinuallyLearningSelfSupervised2022,
  title = {Continually {{Learning Self-Supervised Representations With Projected Functional Regularization}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Gomez-Villa}, Alex and Twardowski, Bartlomiej and Yu, Lu and Bagdanov, Andrew D. and {van de Weijer}, Joost},
  year = {2022},
  eprint = {2112.15022},
  eprinttype = {arxiv},
  pages = {3867--3877},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {cl-replay,class-incremental,continual,replay-free,visiting-joost},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YR72A7ES\\Gomez-Villa et al_2022_Continually Learning Self-Supervised Representations with Projected Functional.pdf;C\:\\Users\\w-32\\Zotero\\storage\\2LNCC5IP\\2112.html;C\:\\Users\\w-32\\Zotero\\storage\\PDXFKH22\\Gomez-Villa_Continually_Learning_Self-Supervised_Representations_With_Projected_Functional_Regu.html}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  volume = {1},
  publisher = {{MIT press Cambridge}}
}

@misc{goodfellowEmpiricalInvestigationCatastrophic2015,
  title = {An {{Empirical Investigation}} of {{Catastrophic Forgetting}} in {{Gradient-Based Neural Networks}}},
  author = {Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  year = {2015},
  month = mar,
  number = {arXiv:1312.6211},
  eprint = {1312.6211},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.6211},
  abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
  archiveprefix = {arXiv},
  keywords = {cl-regularization,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,continual,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H833DT4G\\1312.html}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative Adversarial Nets},
  booktitle = {{{NIPS}}},
  author = {Goodfellow, Ian J and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua B T - International Conference on Neural Information Processing Systems},
  year = {2014},
  pages = {2672--2680},
  abstract = {We propose a new framework for estimating generative models via an adversar- ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G . The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D , a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  keywords = {GAN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\N57LKELK\\Goodfellow et al. - 2014 - Generative adversarial nets(3).pdf}
}

@article{gouKnowledgeDistillationSurvey2021,
  title = {Knowledge {{Distillation}}: {{A Survey}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen John and Tao, Dacheng},
  year = {2021},
  month = mar,
  journal = {International Journal of Computer Vision},
  eprint = {2006.05525},
  eprinttype = {arxiv},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-021-01453-z},
  abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,notag,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PSWIMN4K\\Gou et al_2021_Knowledge Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\PH5SF3VB\\2006.html}
}

@article{goukRegularisationNeuralNetworks2018,
  title = {Regularisation of {{Neural Networks}} by {{Enforcing Lipschitz Continuity}}},
  author = {Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael},
  year = {2018},
  month = apr,
  abstract = {We investigate the effect of explicitly enforcing the Lipschitz continuity of neural networks. Our main hypothesis is that constraining the Lipschitz constant of a networks will have a regularising effect. To this end, we provide a simple technique for computing the Lipschitz constant of a feed forward neural network composed of commonly used layer types. This technique is then utilised to formulate training a Lipschitz continuous neural network as a constrained optimisation problem, which can be easily solved using projected stochastic gradient methods. Our evaluation study shows that, in isolation, our method performs comparatively to state-of-the-art regularisation techniques. Moreover, when combined with existing approaches to regularising neural networks the performance gains are cumulative.},
  keywords = {regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\R2IJEMXL\\Gouk et al. - 2018 - Regularisation of Neural Networks by Enforcing Lipschitz Continuity(3).pdf}
}

@article{graffietiGenerativeNegativeReplay2022,
  title = {Generative {{Negative Replay}} for {{Continual Learning}}},
  author = {Graffieti, Gabriele and Maltoni, Davide and Pellegrini, Lorenzo and Lomonaco, Vincenzo},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.05842 [cs, stat]},
  eprint = {2204.05842},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning continually is a key aspect of intelligence and a necessary ability to solve many real-life problems. One of the most effective strategies to control catastrophic forgetting, the Achilles' heel of continual learning, is storing part of the old data and replaying them interleaved with new experiences (also known as the replay approach). Generative replay, which is using generative models to provide replay patterns on demand, is particularly intriguing, however, it was shown to be effective mainly under simplified assumptions, such as simple scenarios and low-dimensional data. In this paper, we show that, while the generated data are usually not able to improve the classification accuracy for the old classes, they can be effective as negative examples (or antagonists) to better learn the new classes, especially when the learning experiences are small and contain examples of just one or few classes. The proposed approach is validated on complex class-incremental and data-incremental continual learning scenarios (CORe50 and ImageNet-1000) composed of high-dimensional data and a large number of training experiences: a setup where existing generative replay approaches usually fail.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,continual,generative,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PUZD5XW4\\2204.html}
}

@inproceedings{grathwohlYourClassifierSecretly2019,
  title = {Your Classifier Is Secretly an Energy Based Model and You Should Treat It like One},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Joern-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
  year = {2019},
  month = sep,
  abstract = {We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.},
  langid = {english},
  keywords = {bayesian,exmodel},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\N3EI3M9G\\Grathwohl et al_2019_Your classifier is secretly an energy based model and you should treat it like.pdf;C\:\\Users\\w-32\\Zotero\\storage\\J3XQ2MCQ\\forum.html}
}

@article{graveImprovingNeuralLanguage2016,
  title = {Improving {{Neural Language Models}} with a {{Continuous Cache}}},
  author = {Grave, Edouard and Joulin, Armand and Usunier, Nicolas},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.04426 [cs]},
  eprint = {1612.04426},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,ICLR,nlp,pointer-net,rnn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HYWL5FTX\\Grave et al. - 2016 - Improving Neural Language Models with a Continuous.pdf;C\:\\Users\\w-32\\Zotero\\storage\\C36LUYNY\\1612.html}
}

@inproceedings{gravesConnectionistTemporalClassification2006,
  ids = {gravesConnectionistTemporalClassification},
  title = {Connectionist Temporal Classification: {{Labelling}} Unsegmented Sequence Data with Recurrent Neural Networks},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  series = {{{ICML}} '06},
  pages = {369--376},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143891},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  isbn = {1-59593-383-2},
  keywords = {ICML,LSTM,RNN,speech},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CF8ZLCA4\\Graves et al. - Unknown - Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks.pdf}
}

@article{gravesFramewisePhonemeClassification2005,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  author = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2005},
  journal = {Neural Networks},
  volume = {18},
  number = {5-6},
  pages = {602--610},
  keywords = {LSTM,RNN,speech},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7L8CDGW7\\Graves, Schmidhuber - 2005 - Framewise phoneme classification with bidirectional LSTM and other neural network architectures(2).pdf}
}

@article{gravesGeneratingSequencesRecurrent2013,
  ids = {gravesGeneratingSequencesRecurrent2013a},
  title = {Generating Sequences with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2013},
  journal = {arXiv preprint arXiv:1308.0850},
  eprint = {1308.0850},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\F8FTBGQP\\Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf}
}

@article{gravesHybridComputingUsing2016,
  ids = {gravesHybridComputingUsing2016a},
  title = {Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and {Grabska-Barwi{\'n}ska}, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`a} Puigdom{\`e}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2016},
  month = oct,
  journal = {Nature},
  volume = {538},
  number = {7626},
  pages = {471--476},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature20101},
  keywords = {MANN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ADZFCN7R\\nature - DNC hybrid computing using NN with external memory.pdf;C\:\\Users\\w-32\\Zotero\\storage\\HSZS5D7N\\Graves et al. - 2016 - Hybrid computing using a neural network with dynamic external memory.pdf}
}

@article{gravesNeuralTuringMachines2014,
  ids = {gravesNeuralTuringMachines2014a},
  title = {Neural Turing Machines},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  journal = {arXiv preprint arXiv:1410.5401},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {MANN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BJ64XRAR\\Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EIVLWS9E\\Neural Turing Machines.pdf}
}

@article{gravesSequenceTransductionRecurrent2012,
  title = {Sequence {{Transduction}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2012},
  issn = {9781937284725},
  doi = {10.3115/v1/P14-1062},
  abstract = {Many machine learning tasks can be expressed as the transformation---or \textbackslash emph\{transduction\}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \textbackslash emph\{finding\} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.},
  keywords = {LSTM,RNN,seq2seq},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\68N6ZT7P\\Graves - 2012 - Sequence Transduction with Recurrent Neural Networks(2).pdf}
}

@article{gravesSPEECHRECOGNITIONDEEP2013,
  title = {{{SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS Alex Graves}}, {{Abdel-rahman Mohamed}} and {{Geoffrey Hinton Department}} of {{Computer Science}}, {{University}} of {{Toronto}}},
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  year = {2013},
  journal = {IEEE International Conference},
  number = {3},
  pages = {6645--6649},
  issn = {9781479903566},
  doi = {10.1093/ndt/gfr624},
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such asConnec- tionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output align- ment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cur- sive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper in- vestigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suit- able regularisation, we find that deep Long Short-term Mem- ory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score},
  keywords = {LSTM,RNN,speech},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PW2QCFJV\\Graves, Mohamed, Hinton - 2013 - SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS Alex Graves, Abdel-rahman Mohamed and Geoffre(2).pdf}
}

@inproceedings{gravesUnconstrainedOnlineHandwriting2008,
  title = {Unconstrained {{On-line Handwriting Recognition}} with {{Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Graves, Alex and Liwicki, Marcus and Bunke, Horst and Schmidhuber, J{\"u}rgen and Fern{\'a}ndez, Santiago},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  year = {2008},
  pages = {577--584},
  publisher = {{Curran Associates, Inc.}},
  keywords = {CTC,iamondb,LSTM,RNN,seq2seq},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FXYGBSHD\\Graves et al. - 2008 - Unconstrained On-line Handwriting Recognition with.pdf;C\:\\Users\\w-32\\Zotero\\storage\\LL5RYJUJ\\3213-unconstrained-on-line-handwriting-recognition-with-recurrent-neural-networks.html}
}

@article{greaves-tunnellStatisticalInvestigationLong2019,
  title = {A {{Statistical Investigation}} of {{Long Memory}} in {{Language}} and {{Music}}},
  author = {{Greaves-Tunnell}, Alexander and Harchaoui, Zaid},
  year = {2019},
  month = jun,
  journal = {arXiv:1904.03834 [cs, eess, stat]},
  eprint = {1904.03834},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {Representation and learning of long-range dependencies is a central challenge confronted in modern applications of machine learning to sequence data. Yet despite the prominence of this issue, the basic problem of measuring long-range dependence, either in a given data source or as represented in a trained deep model, remains largely limited to heuristic tools. We contribute a statistical framework for investigating long-range dependence in current applications of deep sequence modeling, drawing on the well-developed theory of long memory stochastic processes. This framework yields testable implications concerning the relationship between long memory in real-world data and its learned representation in a deep learning architecture, which are explored through a semiparametric framework adapted to the high-dimensional setting.},
  archiveprefix = {arXiv},
  keywords = {Memory,music,nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JQN8L6D5\\Greaves-Tunnell and Harchaoui - 2019 - A Statistical Investigation of Long Memory in Lang.pdf;C\:\\Users\\w-32\\Zotero\\storage\\Y7W859XB\\1904.html}
}

@incollection{grefenstetteLearningTransduceUnbounded2015,
  title = {Learning to {{Transduce}} with {{Unbounded Memory}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
  editor = {Cortes, C and Lawrence, N D and Lee, D D and Sugiyama, M and Garnett, R},
  year = {2015},
  pages = {1828--1836},
  publisher = {{Curran Associates, Inc.}},
  doi = {10.1103/PhysRevLett.115.218702},
  abstract = {Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.},
  isbn = {1045-9227 VO - 5},
  keywords = {attention,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DJ6EWL9F\\Grefenstette et al. - 2015 - Learning to Transduce with Unbounded Memory(2).pdf}
}

@inproceedings{greffKubricScalableDataset2022,
  title = {Kubric: {{A Scalable Dataset Generator}}},
  shorttitle = {Kubric},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Greff, Klaus and Belletti, Francois and Beyer, Lucas and Doersch, Carl and Du, Yilun and Duckworth, Daniel and Fleet, David J. and Gnanapragasam, Dan and Golemo, Florian and Herrmann, Charles and Kipf, Thomas and Kundu, Abhijit and Lagun, Dmitry and Laradji, Issam and Liu, Hsueh-Ti (Derek) and Meyer, Henning and Miao, Yishu and Nowrouzezahrai, Derek and Oztireli, Cengiz and Pot, Etienne and Radwan, Noha and Rebain, Daniel and Sabour, Sara and Sajjadi, Mehdi S. M. and Sela, Matan and Sitzmann, Vincent and Stone, Austin and Sun, Deqing and Vora, Suhani and Wang, Ziyu and Wu, Tianhao and Yi, Kwang Moo and Zhong, Fangcheng and Tagliasacchi, Andrea},
  year = {2022},
  pages = {3749--3761},
  langid = {english},
  keywords = {simulator},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\52RCCLVU\\Greff_Kubric_A_Scalable_Dataset_Generator_CVPR_2022_paper.html}
}

@article{greffLSTMSearchSpace2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'i}k, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
  year = {2017},
  month = mar,
  journal = {IEEE transactions on neural networks and learning systems},
  volume = {28},
  number = {10},
  pages = {2222--2232},
  doi = {10.1109/TNNLS.2016.2582924},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\$\textbackslash approx 15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  keywords = {LSTM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\V8ZH9CP2\\Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf}
}

@article{grutzendlerLongtermDendriticSpine2002,
  title = {Long-Term Dendritic Spine Stability in the Adult Cortex},
  author = {Grutzendler, Jaime and Kasthuri, Narayanan and Gan, Wen-Biao},
  year = {2002},
  month = dec,
  journal = {Nature},
  volume = {420},
  number = {6917},
  pages = {812--816},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature01276},
  abstract = {The structural dynamics of synapses probably has a crucial role in the development and plasticity of the nervous system. In the mammalian brain, the vast majority of excitatory axo-dendritic synapses occur on dendritic specializations called `spines'. However, little is known about their long-term changes in the intact developing or adult animal. To address this question we developed a transcranial two-photon imaging technique to follow identified spines of layer-5 pyramidal neurons in the primary visual cortex of living transgenic mice expressing yellow fluorescent protein. Here we show that filopodia-like dendritic protrusions, extending and retracting over hours, are abundant in young animals but virtually absent from the adult. In young mice, within the `critical period' for visual cortex development, {$\sim$}73\% of spines remain stable over a one-month interval; most changes are associated with spine elimination. In contrast, in adult mice, the overwhelming majority of spines ({$\sim$}96\%) remain stable over the same interval with a half-life greater than 13 months. These results indicate that spines, initially plastic during development, become remarkably stable in the adult, providing a potential structural basis for long-term information storage.},
  copyright = {2003 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {cl-neuroscience,continual,Humanities and Social Sciences,multidisciplinary},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Grutzendler et al_2002_Long-term dendritic spine stability in the adult cortex.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VW5NXQBI\\nature01276.html}
}

@article{guHiPPORecurrentMemory2020,
  title = {{{HiPPO}}: {{Recurrent Memory}} with {{Optimal Polynomial Projections}}},
  shorttitle = {{{HiPPO}}},
  author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and Re, Christopher},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.07669 [cs, stat]},
  eprint = {2008.07669},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3\%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40\% accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,imdb,Memory,RNN,rnn-pmnist,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NBJTUQ8L\\Gu et al_2020_HiPPO.pdf;C\:\\Users\\w-32\\Zotero\\storage\\JSKTEIME\\2008.html}
}

@article{guidottiStabilityInterpretableModels2018,
  title = {On {{The Stability}} of {{Interpretable Models}}},
  author = {Guidotti, Riccardo and Ruggieri, Salvatore},
  year = {2018},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  abstract = {Interpretable classification models are built with the purpose of providing a comprehensible description of the decision logic to an external oversight agent. When considered in isolation, a decision tree, a set of classification rules, or a linear model, are widely recognized as human-interpretable. However, such models are generated as part of a larger analytical process. Bias in data collection and preparation, or in model's construction may severely affect the accountability of the design process. We conduct an experimental study of the stability of interpretable models with respect to feature selection, instance selection, and model selection. Our conclusions should raise awareness and attention of the scientific community on the need of a stability impact assessment of interpretable models.},
  keywords = {interpretability},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SFHILLFF\\Guidotti, Ruggieri - 2018 - On The Stability of Interpretable Models(2).pdf}
}

@article{guImprovingGatingMechanism2020,
  title = {Improving the {{Gating Mechanism}} of {{Recurrent Neural Networks}}},
  author = {Gu, Albert and Gulcehre, Caglar and Paine, Tom Le and Hoffman, Matt and Pascanu, Razvan},
  year = {2020},
  month = jun,
  journal = {arXiv:1910.09890 [cs]},
  eprint = {1910.09890},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their saturation property introduces problems of its own. For example, in recurrent models these gates need to have outputs near 1 to propagate information over long time-delays, which requires them to operate in their saturation regime and hinders gradient-based learning of the gate mechanism. We address this problem by deriving two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation. We show how these changes are related to and improve on alternative recently proposed gating mechanisms such as chrono initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve the performance of recurrent models on a range of applications, including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning, particularly when long-term dependencies are involved.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,long-term,RNN,rnn-gates},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Y4XMNULR\\Gu et al_2020_Improving the Gating Mechanism of Recurrent Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NMA729R4\\1910.html}
}

@article{gulcehreDynamicNeuralTuring2016,
  title = {Dynamic {{Neural Turing Machine}} with {{Soft}} and {{Hard Addressing Schemes}}},
  author = {Gulcehre, Caglar and Chandar, Sarath and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = jun,
  abstract = {We extend neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We have done extensive analysis of our model and different variations of NTM on bAbI task. We also provide further experimental results on sequential pMNIST, Stanford Natural Language Inference, associative recall and copy tasks.},
  keywords = {MANN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2KMBZAU6\\Gulcehre et al. - 2016 - Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes.pdf}
}

@article{gulcehreHyperbolicAttentionNetworks2018,
  title = {Hyperbolic {{Attention Networks}}},
  author = {Gulcehre, Caglar and Denil, Misha and Malinowski, Mateusz and Razavi, Ali and Pascanu, Razvan and Hermann, Karl Moritz and Battaglia, Peter and Bapst, Victor and Raposo, David and Santoro, Adam and {de Freitas}, Nando},
  year = {2018},
  month = may,
  abstract = {We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact.},
  keywords = {attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\E655VEVT\\Gulcehre et al. - 2018 - Hyperbolic Attention Networks(2).pdf}
}

@article{gulcehreMemoryAugmentedNeural2017,
  title = {Memory {{Augmented Neural Networks}} with {{Wormhole Connections}}},
  author = {Gulcehre, Caglar and Chandar, Sarath and Bengio, Yoshua},
  year = {2017},
  month = jan,
  abstract = {Recent empirical results on long-term dependency tasks have shown that neural networks augmented with an external memory can learn the long-term dependency tasks more easily and achieve better generalization than vanilla recurrent neural networks (RNN). We suggest that memory augmented neural networks can reduce the effects of vanishing gradients by creating shortcut (or wormhole) connections. Based on this observation, we propose a novel memory augmented neural network model called TARDIS (Temporal Automatic Relation Discovery in Sequences). The controller of TARDIS can store a selective set of embeddings of its own previous hidden states into an external memory and revisit them as and when needed. For TARDIS, memory acts as a storage for wormhole connections to the past to propagate the gradients more effectively and it helps to learn the temporal dependencies. The memory structure of TARDIS has similarities to both Neural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but both read and write operations of TARDIS are simpler and more efficient. We use discrete addressing for read/write operations which helps to substantially to reduce the vanishing gradient problem with very long sequences. Read and write operations in TARDIS are tied with a heuristic once the memory becomes full, and this makes the learning problem simpler when compared to NTM or D-NTM type of architectures. We provide a detailed analysis on the gradient propagation in general for MANNs. We evaluate our models on different long-term dependency tasks and report competitive results in all of them.},
  keywords = {MANN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FGMBE9ZT\\Gulcehre, Chandar, Bengio - 2017 - Memory Augmented Neural Networks with Wormhole Connections.pdf}
}

@inproceedings{gulrajaniImprovedTrainingWasserstein2017,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  keywords = {gan,gexml,notag,toread},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Gulrajani et al_2017_Improved Training of Wasserstein GANs.pdf}
}

@article{guoDeepCoreComprehensiveLibrary2022,
  title = {{{DeepCore}}: {{A Comprehensive Library}} for {{Coreset Selection}} in {{Deep Learning}}},
  shorttitle = {{{DeepCore}}},
  author = {Guo, Chengcheng and Zhao, Bo and Bai, Yanbing},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.08499 [cs]},
  eprint = {2204.08499},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Coreset selection, which aims to select a subset of the most informative training samples, is a long-standing learning problem that can benefit many downstream tasks such as data-efficient learning, continual learning, neural architecture search, active learning, etc. However, many existing coreset selection methods are not designed for deep learning, which may have high complexity and poor generalization ability to unseen representations. In addition, the recently proposed methods are evaluated on models, datasets, and settings of different complexities. To advance the research of coreset selection in deep learning, we contribute a comprehensive code library, namely DeepCore, and provide an empirical study on popular coreset selection methods on CIFAR10 and ImageNet datasets. Extensive experiment results show that, although some methods perform better in certain experiment settings, random selection is still a strong baseline.},
  archiveprefix = {arXiv},
  keywords = {cl-code,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\S9Z7ZZLN\\2204.html}
}

@article{guptaNeuralTopicModeling2020,
  title = {Neural {{Topic Modeling}} with {{Continual Lifelong Learning}}},
  author = {Gupta, Pankaj and Chaudhary, Yatin and Runkler, Thomas and Sch{\"u}tze, Hinrich},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.10909 [cs]},
  eprint = {2006.10909},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\URSDVQ58\\Gupta et al_2020_Neural Topic Modeling with Continual Lifelong Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\7GSHQXD7\\2006.html}
}

@article{gururanganDonStopPretraining2020,
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  shorttitle = {Don't {{Stop Pretraining}}},
  author = {Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  year = {2020},
  month = may,
  journal = {arXiv:2004.10964 [cs]},
  eprint = {2004.10964},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
  archiveprefix = {arXiv},
  keywords = {cl-nlp,Computer Science - Computation and Language,Computer Science - Machine Learning,continual,transformer},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Gururangan et al_2020_Don't Stop Pretraining.pdf;C\:\\Users\\w-32\\Zotero\\storage\\4VP29LWU\\2004.html}
}

@article{hacohenPowerCurriculumLearning2019,
  title = {On {{The Power}} of {{Curriculum Learning}} in {{Training Deep Networks}}},
  author = {Hacohen, Guy and Weinshall, Daphna},
  year = {2019},
  abstract = {Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\C28XPKMH\\Hacohen, Weinshall - 2019 - On The Power of Curriculum Learning in Training Deep Networks.pdf}
}

@article{hadjeresDeepBachSteerableModel2016,
  title = {{{DeepBach}}: A {{Steerable Model}} for {{Bach Chorales Generation}}},
  author = {Hadjeres, Ga{\"e}tan and Pachet, Fran{\c c}ois and Nielsen, Frank},
  year = {2016},
  issn = {4573196480018},
  abstract = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
  keywords = {music},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YL98WIRE\\Hadjeres, Pachet, Nielsen - 2016 - DeepBach a Steerable Model for Bach Chorales Generation.pdf}
}

@inproceedings{hadjeresDeepbachSteerableModel2017,
  title = {Deepbach: A Steerable Model for Bach Chorales Generation},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning-Volume}} 70},
  author = {Hadjeres, Ga{\"e}tan and Pachet, Fran{\c c}ois and Nielsen, Frank},
  year = {2017},
  pages = {1362--1371},
  keywords = {music}
}

@article{haHyperNetworks2016,
  title = {{{HyperNetworks}}},
  author = {Ha, David and Dai, Andrew and Le, Quoc V.},
  year = {2016},
  month = dec,
  journal = {arXiv:1609.09106 [cs]},
  eprint = {1609.09106},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3TLND8ZP\\Ha et al_2016_HyperNetworks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\I2ILA59D\\1609.html}
}

@article{hanBorderlineSMOTENewOversampling2005,
  title = {Borderline-{{SMOTE}}: A New over-Sampling Method in Imbalanced Data Sets Learning},
  shorttitle = {Borderline-{{SMOTE}}},
  author = {Han, Hui and Wang, Wen-Yuan and Mao, Bing-Huan},
  year = {2005},
  journal = {Advances in intelligent computing},
  pages = {878--887},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EA5XND4Z\\han_borderline_smote.pdf}
}

@article{hardtGradientDescentLearns2016,
  title = {Gradient {{Descent Learns Linear Dynamical Systems}}},
  author = {Hardt, Moritz and Ma, Tengyu and Recht, Benjamin},
  year = {2016},
  month = sep,
  abstract = {We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider.},
  keywords = {sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RTNIL7VP\\Hardt, Ma, Recht - 2016 - Gradient Descent Learns Linear Dynamical Systems.pdf}
}

@article{hardtTrainFasterGeneralize2015,
  title = {Train Faster, Generalize Better: {{Stability}} of Stochastic Gradient Descent},
  author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  year = {2015},
  pages = {1--32},
  issn = {9781510829008},
  abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.},
  keywords = {sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GDM3ZPKI\\Hardt, Recht, Singer - 2015 - Train faster, generalize better Stability of stochastic gradient descent.pdf}
}

@article{harrisDistributionalStructure1954,
  title = {Distributional Structure},
  author = {Harris, Zellig S},
  year = {1954},
  journal = {Word},
  volume = {10},
  number = {2-3},
  pages = {146--162},
  keywords = {nlp,nlp-embeddings}
}

@article{hashashEdgeContinualLearning2022,
  title = {Edge {{Continual Learning}} for {{Dynamic Digital Twins}} over {{Wireless Networks}}},
  author = {Hashash, Omar and Chaccour, Christina and Saad, Walid},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.04795 [cs, math]},
  eprint = {2204.04795},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Digital twins (DTs) constitute a critical link between the real-world and the metaverse. To guarantee a robust connection between these two worlds, DTs should maintain accurate representations of the physical applications, while preserving synchronization between real and digital entities. In this paper, a novel edge continual learning framework is proposed to accurately model the evolving affinity between a physical twin (PT) and its corresponding cyber twin (CT) while maintaining their utmost synchronization. In particular, a CT is simulated as a deep neural network (DNN) at the wireless network edge to model an autonomous vehicle traversing an episodically dynamic environment. As the vehicular PT updates its driving policy in each episode, the CT is required to concurrently adapt its DNN model to the PT, which gives rise to a de-synchronization gap. Considering the history-aware nature of DTs, the model update process is posed a dual objective optimization problem whose goal is to jointly minimize the loss function over all encountered episodes and the corresponding de-synchronization time. As the de-synchronization time continues to increase over sequential episodes, an elastic weight consolidation (EWC) technique that regularizes the DT history is proposed to limit de-synchronization time. Furthermore, to address the plasticity-stability tradeoff accompanying the progressive growth of the EWC regularization terms, a modified EWC method that considers fair execution between the historical episodes of the DTs is adopted. Ultimately, the proposed framework achieves a simultaneously accurate and synchronous CT model that is robust to catastrophic forgetting. Simulation results show that the proposed solution can achieve an accuracy of 90 \% while guaranteeing a minimal desynchronization time.},
  archiveprefix = {arXiv},
  keywords = {continual,edge,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\J22UXTMP\\2204.html}
}

@article{hashimotoWordEmbeddingsMetric2016,
  title = {Word Embeddings as Metric Recovery in Semantic Spaces},
  author = {Hashimoto, Tatsunori B and {Alvarez-Melis}, David and Jaakkola, Tommi S},
  year = {2016},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {4},
  pages = {273--286},
  keywords = {nlp,nlp-embeddings}
}

@article{havivUnderstandingControllingMemory2019,
  title = {Understanding and {{Controlling Memory}} in {{Recurrent Neural Networks}}},
  author = {Haviv, Doron and Rivkind, Alexander and Barak, Omri},
  year = {2019},
  abstract = {To be effective in sequential data processing, Recurrent Neural Networks (RNNs) are required to keep track of past events by creating memories. While the relation between memories and the network's hidden state dynamics was established over the last decade, previous works in this direction were of a predominantly descriptive nature focusing mainly on locating the dynamical objects of interest. In particular, it remained unclear how dynamical observables affect the performance, how they form and whether they can be manipulated. Here, we utilize different training protocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar performance, alongside substantial differences in their ability to extrapolate for longer delays. We analyze the dynamics of the network's hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a 'slow point'. Slow point speeds predict extrapolation performance across all datasets, protocols and architectures tested. Furthermore, by tracking the formation of the slow points we are able to understand the origin of differences between training protocols. Finally, we propose a novel regularization technique that is based on the relation between hidden state speeds and memory longevity. Our technique manipulates these speeds, thereby leading to a dramatic improvement in memory robustness over time, and could pave the way for a new class of regularization methods.},
  keywords = {Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\T6TDQC46\\Haviv, Rivkind, Barak - 2019 - Understanding and Controlling Memory in Recurrent Neural Networks(2).pdf}
}

@inproceedings{hayesLifelongMachineLearning2020,
  title = {Lifelong {{Machine Learning}} with {{Deep Streaming Linear Discriminant Analysis}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Hayes, Tyler L. and Kanan, Christopher},
  year = {2020},
  month = jun,
  eprint = {1909.01520},
  eprinttype = {arxiv},
  pages = {887--896},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPRW50498.2020.00118},
  abstract = {When an agent acquires new information, ideally it would immediately be capable of using that information to understand its environment. This is not possible using conventional deep neural networks, which suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. A variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, where a model learns from a series of large collections of labeled samples. However, in this setting, inference is only possible after a batch has been accumulated, which prohibits many applications. An alternative paradigm is online learning in a single pass through the training dataset on a resource constrained budget, which is known as streaming learning. Streaming learning has been much less studied in the deep learning community. In streaming learning, an agent learns instances one-by-one and can be tested at any time, rather than only after learning a large batch. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet ILSVRC-2012 and CORe50, a dataset that involves learning to classify from temporally ordered samples1.},
  archiveprefix = {arXiv},
  isbn = {978-1-72819-360-1},
  langid = {english},
  keywords = {cl-online,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,linear,online learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\95HVWDBL\\Hayes_Kanan_2020_Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis.pdf;C\:\\Users\\w-32\\Zotero\\storage\\CCBGVIX7\\Hayes e Kanan - 2020 - Lifelong Machine Learning with Deep Streaming Line.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZHASFXEC\\1909.html}
}

@article{hayesMemoryEfficientExperience2019,
  title = {Memory {{Efficient Experience Replay}} for {{Streaming Learning}}},
  author = {Hayes, Tyler L. and Cahill, Nathan D. and Kanan, Christopher},
  year = {2019},
  month = feb,
  journal = {arXiv:1809.05922 [cs, stat]},
  eprint = {1809.05922},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In supervised machine learning, an agent is typically trained once and then deployed. While this works well for static settings, robots often operate in changing environments and must quickly learn new things from data streams. In this paradigm, known as streaming learning, a learner is trained online, in a single pass, from a data stream that cannot be assumed to be independent and identically distributed (iid). Streaming learning will cause conventional deep neural networks (DNNs) to fail for two reasons: 1) they need multiple passes through the entire dataset; and 2) non-iid data will cause catastrophic forgetting. An old fix to both of these issues is rehearsal. To learn a new example, rehearsal mixes it with previous examples, and then this mixture is used to update the DNN. Full rehearsal is slow and memory intensive because it stores all previously observed examples, and its effectiveness for preventing catastrophic forgetting has not been studied in modern DNNs. Here, we describe the ExStream algorithm for memory efficient rehearsal and compare it to alternatives. We find that full rehearsal can eliminate catastrophic forgetting in a variety of streaming learning settings, with ExStream performing well using far less memory and computation.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,continual,online learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CCWJF2YS\\Hayes et al_2019_Memory Efficient Experience Replay for Streaming Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KYJY54VJ\\1809.html}
}

@misc{hayesMemoryEfficientExperience2019b,
  title = {Memory {{Efficient Experience Replay}} for {{Streaming Learning}}},
  author = {Hayes, Tyler L. and Cahill, Nathan D. and Kanan, Christopher},
  year = {2019},
  month = feb,
  number = {arXiv:1809.05922},
  eprint = {1809.05922},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1809.05922},
  abstract = {In supervised machine learning, an agent is typically trained once and then deployed. While this works well for static settings, robots often operate in changing environments and must quickly learn new things from data streams. In this paradigm, known as streaming learning, a learner is trained online, in a single pass, from a data stream that cannot be assumed to be independent and identically distributed (iid). Streaming learning will cause conventional deep neural networks (DNNs) to fail for two reasons: 1) they need multiple passes through the entire dataset; and 2) non-iid data will cause catastrophic forgetting. An old fix to both of these issues is rehearsal. To learn a new example, rehearsal mixes it with previous examples, and then this mixture is used to update the DNN. Full rehearsal is slow and memory intensive because it stores all previously observed examples, and its effectiveness for preventing catastrophic forgetting has not been studied in modern DNNs. Here, we describe the ExStream algorithm for memory efficient rehearsal and compare it to alternatives. We find that full rehearsal can eliminate catastrophic forgetting in a variety of streaming learning settings, with ExStream performing well using far less memory and computation.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual,Statistics - Machine Learning}
}

@misc{hayesOnlineContinualLearning2022,
  title = {Online {{Continual Learning}} for {{Embedded Devices}}},
  author = {Hayes, Tyler L. and Kanan, Christopher},
  year = {2022},
  month = jul,
  number = {arXiv:2203.10681},
  eprint = {2203.10681},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.10681},
  abstract = {Real-time on-device continual learning is needed for new applications such as home robots, user personalization on smartphones, and augmented/virtual reality headsets. However, this setting poses unique challenges: embedded devices have limited memory and compute capacity and conventional machine learning models suffer from catastrophic forgetting when updated on non-stationary data streams. While several online continual learning models have been developed, their effectiveness for embedded applications has not been rigorously studied. In this paper, we first identify criteria that online continual learners must meet to effectively perform real-time, on-device learning. We then study the efficacy of several online continual learning methods when used with mobile neural networks. We measure their performance, memory usage, compute requirements, and ability to generalize to out-of-domain inputs.},
  archiveprefix = {arXiv},
  keywords = {continual,embedded},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9IDZ6242\\2203.html}
}

@article{hayesReplayDeepLearning2021,
  title = {Replay in {{Deep Learning}}: {{Current Approaches}} and {{Missing Biological Elements}}},
  shorttitle = {Replay in {{Deep Learning}}},
  author = {Hayes, Tyler L. and Krishnan, Giri P. and Bazhenov, Maxim and Siegelmann, Hava T. and Sejnowski, Terrence J. and Kanan, Christopher},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.04132 [cs, q-bio]},
  eprint = {2104.04132},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Replay is the reactivation of one or more neural patterns, which are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated into deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this paper, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be utilized to improve artificial neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notag,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4PMJHHTW\\Hayes et al_2021_Replay in Deep Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\4U7RCZCR\\2104.html}
}

@book{haykinNeuralNetworksLearning2009,
  title = {Neural Networks and Learning Machines},
  author = {Haykin, Simon S and Haykin, Simon S and Haykin, Simon S and Haykin, Simon S},
  year = {2009},
  volume = {3},
  publisher = {{Pearson Upper Saddle River, NJ, USA:}}
}

@article{HeavyFlavorIdentification2017,
  title = {Heavy Flavor Identification at {{CMS}} with Deep Neural Networks},
  year = {2017},
  month = mar,
  keywords = {high-energy-physics}
}

@article{heContinualLearningPerspective2020,
  title = {Continual {{Learning}} from the {{Perspective}} of {{Compression}}},
  author = {He, Xu and Lin, Min},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.15078 [cs, stat]},
  eprint = {2006.15078},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Connectionist models such as neural networks suffer from catastrophic forgetting. In this work, we study this problem from the perspective of information theory and define forgetting as the increase of description lengths of previous data when they are compressed with a sequentially learned model. In addition, we show that continual learning approaches based on variational posterior approximation and generative replay can be considered as approximations to two prequential coding methods in compression, namely, the Bayesian mixture code and maximum likelihood (ML) plug-in code. We compare these approaches in terms of both compression and forgetting and empirically study the reasons that limit the performance of continual learning methods based on variational posterior approximation. To address these limitations, we propose a new continual learning method that combines ML plug-in and Bayesian mixture codes.},
  archiveprefix = {arXiv},
  keywords = {compression,continual,info-theory,theory-framework},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UWWKPG24\\He and Lin - 2020 - Continual Learning from the Perspective of Compres.pdf;C\:\\Users\\w-32\\Zotero\\storage\\SAAG7DEU\\2006.html}
}

@inproceedings{heDeepResidualLearning2016,
  ids = {heDeepResidualLearning2016a},
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778}
}

@article{heDelvingDeepRectifiers2015,
  title = {Delving Deep into Rectifiers: {{Surpassing}} Human-Level Performance on Imagenet Classification},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  volume = {2015 Inter},
  pages = {1026--1034},
  issn = {9781467383912},
  doi = {10.1109/ICCV.2015.123},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  keywords = {CNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H9ITHLGI\\He et al. - 2015 - Delving deep into rectifiers Surpassing human-level performance on imagenet classification.pdf}
}

@inproceedings{heKnowledgeDistillationEfficient2022,
  title = {Knowledge {{Distillation As Efficient Pre-Training}}: {{Faster Convergence}}, {{Higher Data-Efficiency}}, and {{Better Transferability}}},
  shorttitle = {Knowledge {{Distillation As Efficient Pre-Training}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Ruifei and Sun, Shuyang and Yang, Jihan and Bai, Song and Qi, Xiaojuan},
  year = {2022},
  pages = {9161--9171},
  langid = {english},
  keywords = {knowledge-distillation,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Y7V32ZRI\\He_Knowledge_Distillation_As_Efficient_Pre-Training_Faster_Convergence_Higher_Data-Efficiency_a.html}
}

@article{helfrichOrthogonalRecurrentNeural2018,
  title = {Orthogonal {{Recurrent Neural Networks}} with {{Scaled Cayley Transform}}},
  author = {Helfrich, Kyle and Willmott, Devin and Ye, Qiang},
  year = {2018},
  month = jul,
  pages = {1974--1983},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\U4WLVY58\\Helfrich, Willmott, Ye - 2018 - Orthogonal Recurrent Neural Networks with Scaled Cayley Transform.pdf}
}

@inproceedings{heMaskedAutoencodersAre2022,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2022},
  pages = {16000--16009},
  langid = {english},
  keywords = {masked-image-modeling,transformer,vision},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PL5QK4YI\\He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html}
}

@inproceedings{henaffRecurrentOrthogonalNetworks2016,
  title = {Recurrent Orthogonal Networks and Long-Memory Tasks},
  booktitle = {33rd {{International Conference}} on {{Machine Learning}}, {{ICML}} 2016},
  author = {Henaff, Mikael and Szlam, Arthur and Lecun, Yann},
  year = {2016},
  month = feb,
  volume = {5},
  pages = {2978--2986},
  abstract = {Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.},
  isbn = {978-1-5108-2900-8},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PQAJ35SE\\Henaff, Szlam, Lecun - 2016 - Recurrent orthogonal networks and long-memory tasks(3).pdf}
}

@article{hendriksLinearlyConstrainedNeural2020,
  title = {Linearly {{Constrained Neural Networks}}},
  author = {Hendriks, Johannes and Jidling, Carl and Wills, Adrian and Sch{\"o}n, Thomas},
  year = {2020},
  month = jul,
  journal = {arXiv:2002.01600 [physics, stat]},
  eprint = {2002.01600},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {We present an approach to designing neural network based models that will explicitly satisfy known linear operator constraints. To achieve this, the target function is modelled as a linear transformation of an underlying function. This transformation is chosen such that any prediction of the target function is guaranteed to satisfy the constraints. The approach is demonstrated on both simulated and real-data examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\W9V2XRSI\\Hendriks et al. - 2020 - Linearly Constrained Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\PD95ZLTV\\2002.html}
}

@article{hendryxFederatedReconnaissanceEfficient2021,
  title = {Federated {{Reconnaissance}}: {{Efficient}}, {{Distributed}}, {{Class-Incremental Learning}}},
  shorttitle = {Federated {{Reconnaissance}}},
  author = {Hendryx, Sean M. and KC, Dharma Raj and Walls, Bradley and Morrison, Clayton T.},
  year = {2021},
  month = aug,
  journal = {arXiv:2109.00150 [cs]},
  eprint = {2109.00150},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We describe federated reconnaissance, a class of learning problems in which distributed clients learn new concepts independently and communicate that knowledge efficiently. In particular, we propose an evaluation framework and methodological baseline for a system in which each client is expected to learn a growing set of classes and communicate knowledge of those classes efficiently with other clients, such that, after knowledge merging, the clients should be able to accurately discriminate between classes in the superset of classes observed by the set of clients. We compare a range of learning algorithms for this problem and find that prototypical networks are a strong approach in that they are robust to catastrophic forgetting while incorporating new information efficiently. Furthermore, we show that the online averaging of prototype vectors is effective for client model merging and requires only a small amount of communication overhead, memory, and update time per class with no gradient-based learning or hyperparameter tuning. Additionally, to put our results in context, we find that a simple, prototypical network with four convolutional layers significantly outperforms complex, state of the art continual learning algorithms, increasing the accuracy by over 22\% after learning 600 Omniglot classes and over 33\% after learning 20 mini-ImageNet classes incrementally. These results have important implications for federated reconnaissance and continual learning more generally by demonstrating that communicating feature vectors is an efficient, robust, and effective means for distributed, continual learning.},
  archiveprefix = {arXiv},
  keywords = {class-incremental,continual,exmodel,federated,few-shot,mini-imagenet,omniglot,prototype},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Hendryx et al_2021_Federated Reconnaissance.pdf;C\:\\Users\\w-32\\Zotero\\storage\\4DZZMZ2T\\2109.html}
}

@article{henningAreBayesianNeural2021,
  title = {Are {{Bayesian}} Neural Networks Intrinsically Good at Out-of-Distribution Detection?},
  author = {Henning, Christian and D'Angelo, Francesco and Grewe, Benjamin F.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.12248 [cs, stat]},
  eprint = {2107.12248},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The need to avoid confident predictions on unfamiliar data has sparked interest in out-of-distribution (OOD) detection. It is widely assumed that Bayesian neural networks (BNN) are well suited for this task, as the endowed epistemic uncertainty should lead to disagreement in predictions on outliers. In this paper, we question this assumption and provide empirical evidence that proper Bayesian inference with common neural network architectures does not necessarily lead to good OOD detection. To circumvent the use of approximate inference, we start by studying the infinite-width case, where Bayesian inference can be exact considering the corresponding Gaussian process. Strikingly, the kernels induced under common architectural choices lead to uncertainties that do not reflect the underlying data generating process and are therefore unsuited for OOD detection. Finally, we study finite-width networks using HMC, and observe OOD behavior that is consistent with the infinite-width case. Overall, our study discloses fundamental problems when naively using BNNs for OOD detection and opens interesting avenues for future research.},
  archiveprefix = {arXiv},
  keywords = {bayesian,notag,ood-detection,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IUVPDY2C\\2107.html}
}

@article{henningPosteriorMetaReplayContinual2021,
  title = {Posterior {{Meta-Replay}} for {{Continual Learning}}},
  author = {Henning, Christian and Cervera, Maria R. and D'Angelo, Francesco and {von Oswald}, Johannes and Traber, Regina and Ehret, Benjamin and Kobayashi, Seijin and Grewe, Benjamin F. and Sacramento, Jo{\~a}o},
  year = {2021},
  month = oct,
  journal = {arXiv:2103.01133 [cs]},
  eprint = {2103.01133},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning a sequence of tasks without access to i.i.d. observations is a widely studied form of continual learning (CL) that remains challenging. In principle, Bayesian learning directly applies to this setting, since recursive and one-off Bayesian updates yield the same result. In practice, however, recursive updating often leads to poor trade-off solutions across tasks because approximate inference is necessary for most models of interest. Here, we describe an alternative Bayesian approach where task-conditioned parameter distributions are continually inferred from data. We offer a practical deep learning implementation of our framework based on probabilistic task-conditioned hypernetworks, an approach we term posterior meta-replay. Experiments on standard benchmarks show that our probabilistic hypernetworks compress sequences of posterior parameter distributions with virtually no forgetting. We obtain considerable performance gains compared to existing Bayesian CL methods, and identify task inference as our major limiting factor. This limitation has several causes that are independent of the considered sequential setting, opening up new avenues for progress in CL.},
  archiveprefix = {arXiv},
  keywords = {bayesian,cifar10,continual,hypernetwork,mnist},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TG98JHSJ\\Henning et al_2021_Posterior Meta-Replay for Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\TMHYIUAD\\2103.html}
}

@article{heOutOfDistributionDetectionUnsupervised2022,
  title = {Out-{{Of-Distribution Detection In Unsupervised Continual Learning}}},
  author = {He, Jiangpeng and Zhu, Fengqing},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.05462 [cs]},
  eprint = {2204.05462},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Unsupervised continual learning aims to learn new tasks incrementally without requiring human annotations. However, most existing methods, especially those targeted on image classification, only work in a simplified scenario by assuming all new data belong to new tasks, which is not realistic if the class labels are not provided. Therefore, to perform unsupervised continual learning in real life applications, an out-of-distribution detector is required at beginning to identify whether each new data corresponds to a new task or already learned tasks, which still remains under-explored yet. In this work, we formulate the problem for Out-of-distribution Detection in Unsupervised Continual Learning (OOD-UCL) with the corresponding evaluation protocol. In addition, we propose a novel OOD detection method by correcting the output bias at first and then enhancing the output confidence for in-distribution data based on task discriminativeness, which can be applied directly without modifying the learning procedures and objectives of continual learning. Our method is evaluated on CIFAR-100 dataset by following the proposed evaluation protocol and we show improved performance compared with existing OOD detection methods under the unsupervised continual learning scenario.},
  archiveprefix = {arXiv},
  keywords = {continual,ood-detection,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VVULLRR5\\2204.html}
}

@article{heOvercomingCatastrophicInterference2018,
  title = {Overcoming {{Catastrophic Interference}} Using {{Conceptor-Aided Backpropagation}}},
  author = {He, Xu and Jaeger, Herbert},
  year = {2018},
  journal = {ICLR},
  number = {2014},
  pages = {1--11},
  abstract = {Catastrophic interference has been a major roadblock in the research of con-tinual learning. Here we propose a variant of the back-propagation algorithm, " conceptor-aided backprop " (CAB), in which gradients are shielded by concep-tors against degradation of previously learned tasks. Conceptors have their ori-gin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.},
  keywords = {catastrophic forgetting,Conceptor,continual,regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SAFWFKPB\\He, Jaeger - 2018 - Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation.pdf}
}

@article{hermannMultilingualDistributedRepresentations2013,
  title = {Multilingual Distributed Representations without Word Alignment},
  author = {Hermann, Karl Moritz and Blunsom, Phil},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.6173},
  eprint = {1312.6173},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {nlp}
}

@article{hermansMemoryLinearRecurrent2010,
  title = {Memory in Linear Recurrent Neural Networks in Continuous Time},
  author = {Hermans, Michiel and Schrauwen, Benjamin},
  year = {2010},
  month = apr,
  journal = {Neural Networks},
  volume = {23},
  number = {3},
  pages = {341--355},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2009.08.008},
  abstract = {Reservoir Computing is a novel technique which employs recurrent neural networks while circumventing difficult training algorithms. A very recent trend in Reservoir Computing is the use of real physical dynamical systems as implementation platforms, rather than the customary digital emulations. Physical systems operate in continuous time, creating a fundamental difference with the classic discrete time definitions of Reservoir Computing. The specific goal of this paper is to study the memory properties of such systems, where we will limit ourselves to linear dynamics. We develop an analytical model which allows the calculation of the memory function for continuous time linear dynamical systems, which can be considered as networks of linear leaky integrator neurons. We then use this model to research memory properties for different types of reservoir. We start with random connection matrices with a shifted eigenvalue spectrum, which perform very poorly. Next, we transform two specific reservoir types, which are known to give good performance in discrete time, to the continuous time domain. Reservoirs based on uniform spreading of connection matrix eigenvalues on the unit disk in discrete time give much better memory properties than reservoirs with random connection matrices, where reservoirs based on orthogonal connection matrices in discrete time are very robust against noise and their memory properties can be tuned. The overall results found in this work yield important insights into how to design networks for continuous time.},
  langid = {english},
  keywords = {Continuous time,ESN,Linear dynamics,Memory,Memory function,Reservoir computing,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8VIUDWCM\\Hermans_Schrauwen_2010_Memory in linear recurrent neural networks in continuous time.pdf;C\:\\Users\\w-32\\Zotero\\storage\\4K5SEVG7\\S0893608009002305.html}
}

@inproceedings{hermansTrainingAnalysingDeep2013,
  title = {Training and {{Analysing Deep Recurrent Neural Networks}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hermans, Michiel and Schrauwen, Benjamin},
  year = {2013},
  pages = {190--198},
  keywords = {Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2ADTBV73\\Hermans, Schrauwen - 2013 - Training and Analysing Deep Recurrent Neural Networks(2).pdf}
}

@inproceedings{herscheConstrainedFewShotClassIncremental2022,
  title = {Constrained {{Few-Shot Class-Incremental Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hersche, Michael and Karunaratne, Geethan and Cherubini, Giovanni and Benini, Luca and Sebastian, Abu and Rahimi, Abbas},
  year = {2022},
  pages = {9057--9067},
  langid = {english},
  keywords = {Continual,few-shot},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WIT24GVA\\Hersche_Constrained_Few-Shot_Class-Incremental_Learning_CVPR_2022_paper.html}
}

@misc{hessProceduralWorldGeneration2021,
  title = {A {{Procedural World Generation Framework}} for {{Systematic Evaluation}} of {{Continual Learning}}},
  author = {Hess, Timm and Mundt, Martin and Pliushch, Iuliia and Ramesh, Visvanathan},
  year = {2021},
  month = dec,
  number = {arXiv:2106.02585},
  eprint = {2106.02585},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2106.02585},
  abstract = {Several families of continual learning techniques have been proposed to alleviate catastrophic interference in deep neural network training on non-stationary data. However, a comprehensive comparison and analysis of limitations remains largely open due to the inaccessibility to suitable datasets. Empirical examination not only varies immensely between individual works, it further currently relies on contrived composition of benchmarks through subdivision and concatenation of various prevalent static vision datasets. In this work, our goal is to bridge this gap by introducing a computer graphics simulation framework that repeatedly renders only upcoming urban scene fragments in an endless real-time procedural world generation process. At its core lies a modular parametric generative model with adaptable generative factors. The latter can be used to flexibly compose data streams, which significantly facilitates a detailed analysis and allows for effortless investigation of various continual learning schemes.},
  archiveprefix = {arXiv},
  keywords = {continual,dataset,evaluation,simulator},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KCWS3IDE\\2106.html}
}

@article{heTaskAgnosticContinual2019,
  title = {Task {{Agnostic Continual Learning}} via {{Meta Learning}}},
  author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.05201 [cs, stat]},
  eprint = {1906.05201},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering -- i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,continual,meta-learn,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\S7FPMIBL\\He et al. - 2019 - Task Agnostic Continual Learning via Meta Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VC6CBUBX\\1906.html}
}

@article{heuselGANsTrainedTwo2018,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  year = {2018},
  month = jan,
  journal = {arXiv:1706.08500 [cs, stat]},
  eprint = {1706.08500},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  archiveprefix = {arXiv},
  keywords = {exmodel,GAN},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Heusel et al_2018_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZRDLJBBJ\\1706.html}
}

@inproceedings{heWiderDeeperCheaper2017,
  title = {Wider and {{Deeper}}, {{Cheaper}} and {{Faster}}: {{Tensorized LSTMs}} for {{Sequence Learning}}},
  booktitle = {{{NIPS}}},
  author = {He, Zhen and Gao, Shaobing and Xiao, Liang and Liu, Daxue and He, Hangen and Barber, David},
  year = {2017},
  abstract = {Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.},
  keywords = {LSTM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\S9PCGU6M\\He et al. - 2017 - Wider and Deeper, Cheaper and Faster Tensorized LSTMs for Sequence Learning.pdf}
}

@article{higginsVae2017,
  title = {{$B$} -{{Vae}}: {{L}}},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander and Deepmind, Google},
  year = {2017},
  pages = {1--22},
  issn = {1078-0874},
  doi = {10.1177/1078087408328050},
  keywords = {vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\F6FUK7TB\\Higgins et al. - 2017 -  -Vae L(2).pdf}
}

@article{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  journal = {Arxiv preprint},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1503.02531},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\U3J6F4ZE\\Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VS9Y3NMR\\1503.html}
}

@inproceedings{hintonStochasticNeighborEmbedding2002,
  title = {Stochastic {{Neighbor Embedding}}},
  booktitle = {{{NIPS}}},
  author = {Hinton, Geoffrey and Roweis, Sam},
  year = {2002},
  abstract = {We describe a probabilistic approach to the task of placing objects, de- scribed by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional ``images'' of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture ofwidely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word ``bank'', to have versions close to the images of both ``river'' and ``finance'' without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
  keywords = {unsupervised,visualization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\C8Z6XYAU\\Hinton, Roweis - 2002 - Stochastic Neighbor Embedding(2).pdf}
}

@article{hochreiterFastModelbasedProtein2007,
  title = {Fast Model-Based Protein Homology Detection without Alignment},
  author = {Hochreiter, Sepp and Heusel, Martin and Obermayer, Klaus},
  year = {2007},
  journal = {Bioinformatics},
  volume = {23 14},
  pages = {1728--1736},
  keywords = {BIOINF}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp; and Schmidhuber, Jurgen;},
  year = {1997},
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1--32},
  issn = {9781457711022},
  doi = {10.1144/GSL.MEM.1999.018.01.02},
  keywords = {LSTM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6AVV7U5D\\Hochreiter, Schmidhuber - 1997 - Long Short-Term Memory.pdf}
}

@article{hochreiterVanishingGradientProblem1998,
  title = {The Vanishing Gradient Problem during Learning Recurrent Neural Nets and Problem Solutions},
  author = {Hochreiter, Sepp},
  year = {1998},
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume = {6},
  number = {02},
  pages = {107--116},
  keywords = {learning,RNN}
}

@article{hofferBlindspotsConvolutionalNetworks2018,
  title = {On the {{Blindspots}} of {{Convolutional Networks}}},
  author = {Hoffer, Elad and Fine, Shai and Soudry, Daniel},
  year = {2018},
  month = feb,
  abstract = {Deep convolutional network has been the state-of-the-art approach for a wide variety of tasks over the last few years. Its successes have, in many cases, turned it into the default model in quite a few domains. In this work we will demonstrate that convolutional networks have limitations that may, in some cases, hinder it from learning properties of the data, which are easily recognizable by traditional, less demanding, models. To this end, we present a series of competitive analysis studies on image recognition and text analysis tasks, for which convolutional networks are known to provide state-of-the-art results. In our studies, we inject a truth-reveling signal, indiscernible for the network, thus hitting time and again the network's blind spots. The signal does not impair the network's existing performances, but it does provide an opportunity for a significant performance boost by models that can capture it. The various forms of the carefully designed signals shed a light on the strengths and weaknesses of convolutional network, which may provide insights for both theoreticians that study the power of deep architectures, and for practitioners that consider to apply convolutional networks to the task at hand.},
  keywords = {CNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\REUQLVM8\\Hoffer, Fine, Soudry - 2018 - On the Blindspots of Convolutional Networks.pdf}
}

@article{hopfield1982neural,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
  author = {Hopfield, John J},
  year = {1982},
  journal = {Proceedings of the national academy of sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  publisher = {{National Acad Sciences}}
}

@article{hospedalesMetaLearningNeuralNetworks2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.05439 [cs, stat]},
  eprint = {2004.05439},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,meta-learn,review,Statistics - Machine Learning,survey},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\82S55BPL\\Hospedales et al_2020_Meta-Learning in Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\D8H4YENJ\\2004.html}
}

@inproceedings{houLearningUnifiedClassifier2019,
  title = {Learning a {{Unified Classifier Incrementally}} via {{Rebalancing}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  year = {2019},
  month = jun,
  pages = {831--839},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00092},
  abstract = {Conventionally, deep neural networks are trained offline, relying on a large dataset prepared in advance. This paradigm is often challenged in real-world applications, e.g. online services that involve continuous streams of incoming data. Recently, incremental learning receives increasing attention, and is considered as a promising solution to the practical challenges mentioned above. However, it has been observed that incremental learning is subject to a fundamental difficulty \textendash{} catastrophic forgetting, namely adapting a model to new data often results in severe performance degradation on previous tasks or classes. Our study reveals that the imbalance between previous and new data is a crucial cause to this problem. In this work, we develop a new framework for incrementally learning a unified classifier, i.e. a classifier that treats both old and new classes uniformly. Specifically, we incorporate three components, cosine normalization, less-forget constraint, and inter-class separation, to mitigate the adverse effects of the imbalance. Experiments show that the proposed method can effectively rebalance the training process, thus obtaining superior performance compared to the existing methods. On CIFAR100 and ImageNet, our method can reduce the classification errors by more than 6\% and 13\% respectively, under the incremental setting of 10 phases.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  keywords = {continual,Incremental class learning,readout},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TQPYKZLJ\\Hou et al. - 2019 - Learning a Unified Classifier Incrementally via Re.pdf}
}

@incollection{houLifelongLearningProgressive2018,
  title = {Lifelong {{Learning}} via {{Progressive Distillation}} and {{Retrospection}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11207},
  pages = {452--467},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01219-9_27},
  abstract = {Lifelong learning aims at adapting a learned model to new tasks while retaining the knowledge gained earlier. A key challenge for lifelong learning is how to strike a balance between the preservation on old tasks and the adaptation to a new one within a given model. Approaches that combine both objectives in training have been explored in previous works. Yet the performance still suffers from considerable degradation in a long sequence of tasks. In this work, we propose a novel approach to lifelong learning, which tries to seek a better balance between preservation and adaptation via two techniques: Distillation and Retrospection. Specifically, the target model adapts to the new task by knowledge distillation from an intermediate expert, while the previous knowledge is more effectively preserved by caching a small subset of data for old tasks. The combination of Distillation and Retrospection leads to a more gentle learning curve for the target model, and extensive experiments demonstrate that our approach can bring consistent improvements on both old and new tasks4 .},
  isbn = {978-3-030-01218-2 978-3-030-01219-9},
  langid = {english},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZUA68K7M\\Hou et al. - 2018 - Lifelong Learning via Progressive Distillation and.pdf}
}

@article{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  journal = {arXiv:1704.04861 [cs]},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CGGSK9E3\\Howard et al_2017_MobileNets.pdf;C\:\\Users\\w-32\\Zotero\\storage\\K8XQQH74\\1704.html}
}

@article{howardUniversalLanguageModel2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  year = {2018},
  month = jan,
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  keywords = {nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2MRZZEN4\\Howard, Ruder - 2018 - Universal Language Model Fine-tuning for Text Classification.pdf}
}

@article{hsuCloserLookKnowledge2022,
  title = {A {{Closer Look}} at {{Knowledge Distillation}} with {{Features}}, {{Logits}}, and {{Gradients}}},
  author = {Hsu, Yen-Chang and Smith, James and Shen, Yilin and Kira, Zsolt and Jin, Hongxia},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.10163 [cs]},
  eprint = {2203.10163},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Knowledge distillation (KD) is a substantial strategy for transferring learned knowledge from one neural network model to another. A vast number of methods have been developed for this strategy. While most method designs a more efficient way to facilitate knowledge transfer, less attention has been put on comparing the effect of knowledge sources such as features, logits, and gradients. This work provides a new perspective to motivate a set of knowledge distillation strategies by approximating the classical KL-divergence criteria with different knowledge sources, making a systematic comparison possible in model compression and incremental learning. Our analysis indicates that logits are generally a more efficient knowledge source and suggests that having sufficient feature dimensions is crucial for the model design, providing a practical guideline for effective KD-based transfer learning.},
  archiveprefix = {arXiv},
  keywords = {continual,distillation,exmodel,knowledge distillation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\W9CJ3FDG\\Hsu et al_2022_A Closer Look at Knowledge Distillation with Features, Logits, and Gradients.pdf;C\:\\Users\\w-32\\Zotero\\storage\\B8VACWLA\\2203.html}
}

@inproceedings{huang2004extreme,
  title = {Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks},
  booktitle = {2004 {{IEEE}} International Joint Conference on Neural Networks ({{IEEE Cat}}. {{No}}. {{04CH37541}})},
  author = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
  year = {2004},
  volume = {2},
  pages = {985--990},
  organization = {{IEEE}}
}

@article{huangContinualLearningPeertoPeer2022,
  title = {Continual {{Learning}} for {{Peer-to-Peer Federated Learning}}: {{A Study}} on {{Automated Brain Metastasis Identification}}},
  shorttitle = {Continual {{Learning}} for {{Peer-to-Peer Federated Learning}}},
  author = {Huang, Yixing and Bert, Christoph and Fischer, Stefan and Schmidt, Manuel and D{\"o}rfler, Arnd and Maier, Andreas and Fietkau, Rainer and Putz, Florian},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.13591 [cs]},
  eprint = {2204.13591},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Due to data privacy constraints, data sharing among multiple centers is restricted. Continual learning, as one approach to peer-to-peer federated learning, can promote multicenter collaboration on deep learning algorithm development by sharing intermediate models instead of training data. This work aims to investigate the feasibility of continual learning for multicenter collaboration on an exemplary application of brain metastasis identification using DeepMedic. 920 T1 MRI contrast enhanced volumes are split to simulate multicenter collaboration scenarios. A continual learning algorithm, synaptic intelligence (SI), is applied to preserve important model weights for training one center after another. In a bilateral collaboration scenario, continual learning with SI achieves a sensitivity of 0.917, and naive continual learning without SI achieves a sensitivity of 0.906, while two models trained on internal data solely without continual learning achieve sensitivity of 0.853 and 0.831 only. In a seven-center multilateral collaboration scenario, the models trained on internal datasets (100 volumes each center) without continual learning obtain a mean sensitivity value of 0.725. With single-visit continual learning (i.e., the shared model visits each center only once during training), the sensitivity is improved to 0.788 and 0.849 without SI and with SI, respectively. With iterative continual learning (i.e., the shared model revisits each center multiple times during training), the sensitivity is further improved to 0.914, which is identical to the sensitivity using mixed data for training. Our experiments demonstrate that continual learning can improve brain metastasis identification performance for centers with limited data. This study demonstrates the feasibility of applying continual learning for peer-to-peer federated learning in multicenter collaboration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual,distributed,federated},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NZ77A2CR\\2204.html}
}

@article{huangMultipleTimescaleGated,
  title = {Multiple {{Timescale}} and {{Gated Mechanisms}} for {{Action}} and {{Language Learning}} in {{Robotics}}},
  author = {Huang, Wenjie and Zhong, Junpei and Cangelosi, Angelo},
  pages = {7},
  abstract = {Recurrent Neural Network (RNN) have been used for sequence-related learning tasks, such as language and action, in the field of cognitive robotics. Gated mechanisms used in LSTM and GRU perform well in remembering long-term dependency. But to better mimic the neural dynamics in cognitive processes, the Multiple Time-scales (MT) RNN uses a hierarchical organization of memory updates which is similar to human cognition. Since the MT feature is typically used with a vanilla RNN or different gated mechanisms, its effect on the updates and training is still not fully uncovered. Therefore, we conduct a comparative experiment on two MT recurrent neural network models, i.e. the Multiple Time-Scale Recurrent Neural Network (MTRNN) and the Multiple Time-Scale Gated Recurrent Unit (MTGRU), for action sequence learning in robotics. The experiment shows that the MTRNN model can be used in learning tasks with low requirements for learning of long-term dependency due to its low computation. On the other hand, the MTGRU model is appropriate for learning the longterm dependency. Furthermore, because of the duplicated feature of the MT and the GRU feature, we also propose a simplified MTGRU model, named Multiple Time-scale SingleGate Recurrent Unit (MTSRU) which could reduce computational cost while it achieves the similar performance as the original version.},
  langid = {english},
  keywords = {RNN,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IKIWQESC\\Huang et al. - Multiple Timescale and Gated Mechanisms for Action.pdf}
}

@article{huangMusicTransformer2018,
  title = {Music {{Transformer}}},
  author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
  year = {2018},
  month = sep,
  abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity is quadratic in the sequence length. We propose an algorithm that reduces the intermediate memory requirements to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long (thousands of steps) compositions with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-competition, and obtain state-of-the-art results on the latter.},
  keywords = {music,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BA6U7RCH\\Huang et al. - 2018 - Music Transformer(3).pdf}
}

@inproceedings{huControlledGenerationText2017,
  title = {Toward Controlled Generation of Text},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning-Volume}} 70},
  author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
  year = {2017},
  pages = {1587--1596},
  keywords = {generative,nlp}
}

@article{huDrinkingFirehoseContinual2020,
  title = {Drinking from a {{Firehose}}: {{Continual Learning}} with {{Web-scale Natural Language}}},
  shorttitle = {Drinking from a {{Firehose}}},
  author = {Hu, Hexiang and Sener, Ozan and Sha, Fei and Koltun, Vladlen},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.09335 [cs, stat]},
  eprint = {2007.09335},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Continual learning systems will interact with humans, with each other, and with the physical world through time -- and continue to learn and adapt as they do. Such systems have typically been evaluated in artificial settings: for example, classifying randomly permuted images. A key limitation of these settings is the unnatural construct of discrete, sharply demarcated tasks that are solved in sequence. In this paper, we study a natural setting for continual learning on a massive scale. We introduce the problem of personalized online language learning (POLL), which involves fitting personalized language models to a population of users that evolves over time. To facilitate research on POLL, we collect massive datasets of Twitter posts. These datasets, Firehose10M and Firehose100M, comprise 100 million tweets, posted by one million users over six years. Enabled by the Firehose datasets, we present a rigorous evaluation of continual learning algorithms on an unprecedented scale. Based on this analysis, we develop a simple algorithm for continual gradient descent (ConGraD) that outperforms prior continual learning methods on the Firehose datasets as well as earlier benchmarks. Collectively, the POLL problem setting, the Firehose datasets, and the ConGraD algorithm enable reproducible research on web-scale continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,continual,data,nlp,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\F53AWRML\\Hu et al_2020_Drinking from a Firehose.pdf;C\:\\Users\\w-32\\Zotero\\storage\\8MQTCVR5\\2007.html}
}

@article{hudsonMusicalBeautyInformation2011,
  title = {Musical Beauty and Information Compression: {{Complex}} to the Ear but Simple to the Mind?},
  author = {Hudson, Nicholas J.},
  year = {2011},
  journal = {BMC Research Notes},
  volume = {4},
  issn = {1756-0500 (Electronic)\textbackslash n1756-0500 (Linking)},
  doi = {10.1186/1756-0500-4-9},
  abstract = {BMC Research Notes 2011, 4:9. doi:10.1186/1756-0500-4-9},
  keywords = {music},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GH5J8SR6\\Hudson - 2011 - Musical beauty and information compression Complex to the ear but simple to the mind.pdf}
}

@inproceedings{hundmanDetectingSpacecraftAnomalies2018,
  title = {Detecting {{Spacecraft Anomalies Using LSTMs}} and {{Nonparametric Dynamic Thresholding}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Hundman, Kyle and Constantinou, Valentino and Laporte, Christopher and Colwell, Ian and Soderstrom, Tom},
  year = {2018},
  month = jul,
  series = {{{KDD}} '18},
  pages = {387--395},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3219819.3219845},
  abstract = {As spacecraft send back increasing amounts of telemetry data, improved anomaly detection systems are needed to lessen the monitoring burden placed on operations engineers and reduce operational risk. Current spacecraft monitoring systems only target a subset of anomaly types and often require costly expert knowledge to develop and maintain due to challenges involving scale and complexity. We demonstrate the effectiveness of Long Short-Term Memory (LSTMs) networks, a type of Recurrent Neural Network (RNN), in overcoming these issues using expert-labeled telemetry anomaly data from the Soil Moisture Active Passive (SMAP) satellite and the Mars Science Laboratory (MSL) rover, Curiosity. We also propose a complementary unsupervised and nonparametric anomaly thresholding approach developed during a pilot implementation of an anomaly detection system for SMAP, and offer false positive mitigation strategies along with other key improvements and lessons learned during development.},
  isbn = {978-1-4503-5552-0},
  keywords = {anomaly-detection,esn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LCDUKITU\\Hundman et al. - 2018 - Detecting Spacecraft Anomalies Using LSTMs and Non.pdf}
}

@misc{hunterTwoSparsitiesAre2021,
  title = {Two {{Sparsities Are Better Than One}}: {{Unlocking}} the {{Performance Benefits}} of {{Sparse-Sparse Networks}}},
  shorttitle = {Two {{Sparsities Are Better Than One}}},
  author = {Hunter, Kevin Lee and Spracklen, Lawrence and Ahmad, Subutai},
  year = {2021},
  month = dec,
  number = {arXiv:2112.13896},
  eprint = {2112.13896},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.13896},
  abstract = {In principle, sparse neural networks should be significantly more efficient than traditional dense networks. Neurons in the brain exhibit two types of sparsity; they are sparsely interconnected and sparsely active. These two types of sparsity, called weight sparsity and activation sparsity, when combined, offer the potential to reduce the computational cost of neural networks by two orders of magnitude. Despite this potential, today's neural networks deliver only modest performance benefits using just weight sparsity, because traditional computing hardware cannot efficiently process sparse networks. In this article we introduce Complementary Sparsity, a novel technique that significantly improves the performance of dual sparse networks on existing hardware. We demonstrate that we can achieve high performance running weight-sparse networks, and we can multiply those speedups by incorporating activation sparsity. Using Complementary Sparsity, we show up to 100X improvement in throughput and energy efficiency performing inference on FPGAs. We analyze scalability and resource tradeoffs for a variety of kernels typical of commercial convolutional networks such as ResNet-50 and MobileNetV2. Our results with Complementary Sparsity suggest that weight plus activation sparsity can be a potent combination for efficiently scaling future AI models.},
  archiveprefix = {arXiv},
  keywords = {sparsity},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4SLAFGYL\\2112.html}
}

@article{huRCapsNetRecurrentCapsule,
  title = {{{RCapsNet}}: {{A Recurrent Capsule Network}} for {{Text Classification}}},
  author = {Hu, Junfeng and Liao, Jun and Liu, Li and Ma, Wenchao},
  pages = {8},
  abstract = {In this paper, we propose RCapsNet, a recurrent capsule network for text classification. Although a variety of neural networks have been proposed recently, existing models are mainly based either on RNN or on CNN, which are rather limited in encoding temporal features in these network structures. In addition, most of these models require to integrate prior linguistic knowledge into them, which is not practical for a non-linguistician to handcraft such knowledge. To address these issues on temporal relational variabilities in text classification, the RCapsNet is presented by employing a hierarchy of recurrent structure-based capsules. It consists of two components: the recurrent module considered as the backbone of the RCapsNet and the reconstruction module designed to enhance the generalization capability of the model. Empirical evaluations on four benchmark datasets demonstrate the competitiveness of the RCapsNet. In particular, it is shown that prior linguistic knowledge is dispensable for the training of our model.},
  langid = {english},
  keywords = {CapsuleNet,RNN,text-classification,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8YRR977Z\\Hu et al. - RCapsNet A Recurrent Capsule Network for Text Cla.pdf}
}

@article{hylandLearningUnitaryOperators2016,
  title = {Learning {{Unitary Operators}} with {{Help From}} u(n)},
  author = {Hyland, Stephanie L. and R{\"a}tsch, Gunnar},
  year = {2016},
  month = jul,
  abstract = {A major challenge in the training of recurrent neural networks is the so-called vanishing or exploding gradient problem. The use of a norm-preserving transition operator can address this issue, but parametrization is challenging. In this work we focus on unitary operators and describe a parametrization using the Lie algebra \$\textbackslash mathfrak\{u\}(n)\$ associated with the Lie group \$U(n)\$ of \$n \textbackslash times n\$ unitary matrices. The exponential map provides a correspondence between these spaces, and allows us to define a unitary matrix using \$n\^2\$ real coefficients relative to a basis of the Lie algebra. The parametrization is closed under additive updates of these coefficients, and thus provides a simple space in which to do gradient descent. We demonstrate the effectiveness of this parametrization on the problem of learning arbitrary unitary operators, comparing to several baselines and outperforming a recently-proposed lower-dimensional parametrization. We additionally use our parametrization to generalize a recently-proposed unitary recurrent neural network to arbitrary unitary matrices, using it to solve standard long-memory tasks.},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LHIAKEND\\Hyland, Rtsch - 2016 - Learning Unitary Operators with Help From u(n)(2).pdf}
}

@misc{ICML2020Paper,
  title = {{{ICML}} 2020 {{Paper Detail}}},
  howpublished = {https://icml.cc/virtual/2020/poster/6338\#details}
}

@misc{IllustratingConvolutionalNeural2020,
  title = {Illustrating ({{Convolutional}}) {{Neural Networks}} in {{LaTeX}} with {{TikZ}} \textbullet{} {{David Stutz}}},
  year = {2020},
  month = jun,
  journal = {David Stutz},
  abstract = {Many papers and theses provide high-level overviews of the proposed methods. Nowadays, in computer vision, natural language processing or similar research areas strongly driven by deep learning, these illustrations commonly include architectures of the used (convolutional) neural network. In this article, I want to provide a collection of examples using LaTeX and TikZ to produce nice figures of (convolutional) neural networks. All the discussed examples can also be found on GitHub.},
  langid = {american},
  keywords = {tikz},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BERX5IAW\\illustrating-convolutional-neural-networks-in-latex-with-tikz.html}
}

@misc{InstallGuideCentOS7WarewulfSLURM19x86,
  title = {Install\_guide-{{CentOS7-Warewulf-SLURM-1}}.3.9-X86\_64.Pdf},
  keywords = {cluster,docs,hardware,software},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SBRY2LTS\\Install_guide-CentOS7-Warewulf-SLURM-1.3.9-x86_64.pdf}
}

@article{inubushiReservoirComputingMemoryNonlinearity2017,
  title = {Reservoir {{Computing Beyond Memory-Nonlinearity Trade-off}}},
  author = {Inubushi, Masanobu and Yoshimura, Kazuyuki},
  year = {2017},
  month = aug,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {1--10},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-10257-6},
  abstract = {Reservoir computing is a brain-inspired machine learning framework that employs a signal-driven dynamical system, in particular harnessing common-signal-induced synchronization which is a widely observed nonlinear phenomenon. Basic understanding of a working principle in reservoir computing can be expected to shed light on how information is stored and processed in nonlinear dynamical systems, potentially leading to progress in a broad range of nonlinear sciences. As a first step toward this goal, from the viewpoint of nonlinear physics and information theory, we study the memory-nonlinearity trade-off uncovered by Dambre et al. (2012). Focusing on a variational equation, we clarify a dynamical mechanism behind the trade-off, which illustrates why nonlinear dynamics degrades memory stored in dynamical system in general. Moreover, based on the trade-off, we propose a mixture reservoir endowed with both linear and nonlinear dynamics and show that it improves the performance of information processing. Interestingly, for some tasks, significant improvements are observed by adding a few linear dynamics to the nonlinear dynamical system. By employing the echo state network model, the effect of the mixture reservoir is numerically verified for a simple function approximation task and for more complex tasks.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {ESN,memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CJN9ADSR\\Inubushi and Yoshimura - 2017 - Reservoir Computing Beyond Memory-Nonlinearity Tra.pdf;C\:\\Users\\w-32\\Zotero\\storage\\QZJAL89E\\s41598-017-10257-6.html}
}

@article{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  keywords = {regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\N34ZX2A2\\Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift(3).pdf}
}

@misc{irieDualFormNeural2022,
  title = {The {{Dual Form}} of {{Neural Networks Revisited}}: {{Connecting Test Time Predictions}} to {{Training Patterns}} via {{Spotlights}} of {{Attention}}},
  shorttitle = {The {{Dual Form}} of {{Neural Networks Revisited}}},
  author = {Irie, Kazuki and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  year = {2022},
  month = jun,
  number = {arXiv:2202.05798},
  eprint = {2202.05798},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.05798},
  abstract = {Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.},
  archiveprefix = {arXiv},
  keywords = {continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3263TKCX\\Irie et al_2022_The Dual Form of Neural Networks Revisited.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KFGCV3HT\\2202.html}
}

@inproceedings{iyyerDeepUnorderedComposition2015,
  title = {Deep Unordered Composition Rivals Syntactic Methods for Text Classification},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Iyyer, Mohit and Manjunatha, Varun and {Boyd-Graber}, Jordan and Daum{\'e} III, Hal},
  year = {2015},
  volume = {1},
  pages = {1681--1691},
  keywords = {nlp}
}

@article{jaderbergDecoupledNeuralInterfaces,
  title = {Decoupled {{Neural Interfaces}} Using {{Synthetic Gradients}}},
  author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  abstract = {Training directed neural networks typically re-quires forward-propagating data through a com-putation graph, followed by backpropagating er-ror signal, to produce weight updates. All lay-ers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introduc-ing a model of the future computation of the net-work graph. These models predict what the re-sult of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropa-gated error gradients we decouple subgraphs, and can update them independently and asyn-chronously i.e. we realise decoupled neural in-terfaces. We show results for feed-forward mod-els, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predict-ing one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at differ-ent timescales. Finally, we demonstrate that in addition to predicting gradients, the same frame-work can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass \textendash{} amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\58BCWT3K\\Jaderberg et al. - Unknown - Decoupled Neural Interfaces using Synthetic Gradients(2).pdf}
}

@article{jaegerConceptorsEasyIntroduction2014,
  title = {Conceptors: An Easy Introduction},
  author = {Jaeger, Herbert},
  year = {2014},
  pages = {1--11},
  abstract = {Conceptors provide an elementary neuro-computational mechanism which sheds a fresh and unifying light on a diversity of cognitive phenomena. A number of demanding learning and processing tasks can be solved with unprecedented ease, robustness and accuracy. Some of these tasks were impossible to solve before. This entirely informal paper introduces the basic principles of conceptors and highlights some of their usages.},
  keywords = {Conceptor},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LXYGCBG3\\Jaeger - 2014 - Conceptors an easy introduction.pdf}
}

@article{jaegerControllingRecurrentNeural2014,
  title = {Controlling {{Recurrent Neural Networks}} by {{Conceptors}}},
  author = {Jaeger, Herbert},
  year = {2014},
  number = {31},
  abstract = {The human brain is a dynamical system whose extremely complex sensor-driven neural processes give rise to conceptual, logical cognition. Understanding the interplay between nonlinear neural dynamics and concept-level cognition remains a major scientific challenge. Here I propose a mechanism of neurodynamical organization, called conceptors, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic. It becomes possible to learn, store, abstract, focus, morph, generalize, de-noise and recognize a large number of dynamical patterns within a single neural system; novel patterns can be added without interfering with previously acquired ones; neural noise is automatically filtered. Conceptors help explaining how conceptual-level information processing emerges naturally and robustly in neural systems, and remove a number of roadblocks in the theory and applications of recurrent neural networks.},
  keywords = {Conceptor,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DNDRI8H6\\Jaeger - 2014 - Controlling Recurrent Neural Networks by Conceptors.pdf}
}

@article{jaegerHarnessingNonlinearityPredicting2004,
  title = {Harnessing Nonlinearity: {{Predicting}} Chaotic Systems and Saving Energy in Wireless Communication},
  author = {Jaeger, Herbert and Haas, Harald},
  year = {2004},
  journal = {science},
  volume = {304},
  number = {5667},
  pages = {78--80},
  keywords = {ESN,RNN}
}

@book{jaegerShortTermMemory2001,
  title = {Short {{Term Memory}} in {{Echo State Networks}}},
  author = {Jaeger, Herbert},
  year = {2001},
  volume = {5},
  publisher = {{GMD-Forschungszentrum Informationstechnik}},
  keywords = {ESN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\U7NG57QB\\Jaeger - 2001 - Short Term Memory in Echo State Networks.pdf}
}

@article{jaegerUsingConceptorsManage2017,
  title = {Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns},
  author = {Jaeger, Herbert},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--43},
  abstract = {Biological brains can learn, recognize, organize, and re-generate large repertoires of tempo-ral patterns. Here I propose a mechanism of neurodynamical pattern learning and represen-tation, called conceptors, which offers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and " focussed " . Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content-addressed in ways that are analog to recalling static patterns in Hopfield networks.},
  keywords = {Conceptor,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\N7I9TEWR\\Jaeger - 2017 - Using conceptors to manage neural long-term memories for temporal patterns.pdf}
}

@article{jangCategoricalReparameterizationGumbelSoftmax2016,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2016},
  month = nov,
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9UDG4IUR\\Jang, Gu, Poole - 2016 - Categorical Reparameterization with Gumbel-Softmax(2).pdf}
}

@article{jastrzebskiCatastrophicFisherExplosion2020,
  title = {Catastrophic {{Fisher Explosion}}: {{Early Phase Fisher Matrix Impacts Generalization}}},
  shorttitle = {Catastrophic {{Fisher Explosion}}},
  author = {Jastrzebski, Stanislaw and Arpit, Devansh and Astrand, Oliver and Kerg, Giancarlo and Wang, Huan and Xiong, Caiming and Socher, Richard and Cho, Kyunghyun and Geras, Krzysztof},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.14193 [cs, stat]},
  eprint = {2012.14193},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The early phase of training has been shown to be important in two ways for deep neural networks. First, the degree of regularization in this phase significantly impacts the final generalization. Second, it is accompanied by a rapid change in the local loss curvature influenced by regularization choices. Connecting these two findings, we show that stochastic gradient descent (SGD) implicitly penalizes the trace of the Fisher Information Matrix (FIM) from the beginning of training. We argue it is an implicit regularizer in SGD by showing that explicitly penalizing the trace of the FIM can significantly improve generalization. We further show that the early value of the trace of the FIM correlates strongly with the final generalization. We highlight that in the absence of implicit or explicit regularization, the trace of the FIM can increase to a large value early in training, to which we refer as catastrophic Fisher explosion. Finally, to gain insight into the regularization effect of penalizing the trace of the FIM, we show that 1) it limits memorization by reducing the learning speed of examples with noisy labels more than that of the clean examples, and 2) trajectories with a low initial trace of the FIM end in flat minima, which are commonly associated with good generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RJ3R6NQ3\\Jastrzebski et al_2020_Catastrophic Fisher Explosion.pdf;C\:\\Users\\w-32\\Zotero\\storage\\RU28H26K\\2012.html}
}

@article{javedMetaLearningRepresentationsContinual2019,
  title = {Meta-{{Learning Representations}} for {{Continual Learning}}},
  author = {Javed, Khurram and White, Martha},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.12588 [cs, stat]},
  eprint = {1905.12588},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite---they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. We release an implementation of our method at https://github.com/khurramjaved96/mrcl .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual,meta-learn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4RKMRL4Y\\Javed_White_2019_Meta-Learning Representations for Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NM3WK594\\1905.html}
}

@article{javedScalableOnlineRecurrent2021,
  title = {Scalable {{Online Recurrent Learning Using Columnar Neural Networks}}},
  author = {Javed, Khurram and White, Martha and Sutton, Rich},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.05787 [cs]},
  eprint = {2103.05787},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Structural credit assignment for recurrent learning is challenging. An algorithm called RTRL can compute gradients for recurrent networks online but is computationally intractable for large networks. Alternatives, such as BPTT, are not online. In this work, we propose a credit-assignment algorithm -- \textbackslash algoname\{\} -- that approximates the gradients for recurrent learning in real-time using \$O(n)\$ operations and memory per-step. Our method builds on the idea that for modular recurrent networks, composed of columns with scalar states, it is sufficient for a parameter to only track its influence on the state of its column. We empirically show that as long as connections between columns are sparse, our method approximates the true gradient well. In the special case when there are no connections between columns, the \$O(n)\$ gradient estimate is exact. We demonstrate the utility of the approach for both recurrent state learning and meta-learning by comparing the estimated gradient to the true gradient on a synthetic test-bed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FAPI7U8U\\Javed et al_2021_Scalable Online Recurrent Learning Using Columnar Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\A66DNJEH\\2103.html}
}

@book{jeffersIntelXeonPhi2016,
  title = {Intel {{Xeon Phi Processor High Performance Programming}}: {{Knights Landing Edition}}},
  author = {Jeffers, James and Reinders, James and Sodani, Avinash},
  year = {2016},
  publisher = {{Morgan Kaufmann}},
  keywords = {hardware}
}

@article{jiaCaffeConvolutionalArchitecture2014,
  title = {Caffe: {{Convolutional Architecture}} for {{Fast Feature Embedding}}},
  author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  year = {2014},
  journal = {arXiv preprint arXiv:1408.5093},
  eprint = {1408.5093},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {software}
}

@article{jiangEffectiveTrainingMethod2017,
  title = {An {{Effective Training Method For Deep Convolutional Neural Network}}},
  author = {Jiang, Yang and Dou, Zeyang and Cao, Jie and Gao, Kun and Chen, Xi},
  year = {2017},
  month = jul,
  journal = {arXiv:1708.01666 [cs, stat]},
  eprint = {1708.01666},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a training method to speed up the training and improve the performance of deep convolutional neural networks (CNN). We propose a nonlinearity generator, which makes the deep CNN as a linear model in the initial state, and then introduces nonlinearity during the training procedure to improve the model capacity. We theoretically show that the mean shift problem in the neural network makes the training unstable, and the proposed method can partly solve this problem. The nonlinearity generator (NG) can be considered as a regularizer to make the feature map more discriminative than traditional methods. Experiments show that our method speeds up the convergence of the back propagation algorithm, enables larger learning rate and allows less careful initialization; it also improves the performance of CNN at negligible extra computational cost and can be jointly used with batch normalization to further improve the model performance. We train an extremely deep CNN with the proposed method easily without the extra training tricks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FG25XTH8\\Jiang et al. - 2017 - An Effective Training Method For Deep Convolutiona.pdf;C\:\\Users\\w-32\\Zotero\\storage\\4D9CBV27\\1708.html}
}

@article{jiAutomaticRecallMachines2020,
  title = {Automatic {{Recall Machines}}: {{Internal Replay}}, {{Continual Learning}} and the {{Brain}}},
  shorttitle = {Automatic {{Recall Machines}}},
  author = {Ji, Xu and Henriques, Joao and Tuytelaars, Tinne and Vedaldi, Andrea},
  year = {2020},
  month = dec,
  journal = {arXiv:2006.12323 [cs, q-bio, stat]},
  eprint = {2006.12323},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {Replay in neural networks involves training on sequential data with memorized samples, which counteracts forgetting of previous behavior caused by non-stationarity. We present a method where these auxiliary samples are generated on the fly, given only the model that is being trained for the assessed objective, without extraneous buffers or generator networks. Instead the implicit memory of learned samples within the assessed model itself is exploited. Furthermore, whereas existing work focuses on reinforcing the full seen data distribution, we show that optimizing for not forgetting calls for the generation of samples that are specialized to each real training batch, which is more efficient and scalable. We consider high-level parallels with the brain, notably the use of a single model for inference and recall, the dependency of recalled samples on the current environment batch, top-down modulation of activations and learning, abstract recall, and the dependency between the degree to which a task is learned and the degree to which it is recalled. These characteristics emerge naturally from the method without being controlled for.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Ji et al_2020_Automatic Recall Machines.pdf;C\:\\Users\\w-32\\Zotero\\storage\\8G7SSFHH\\2006.html}
}

@article{jiDifferentialPrivacyMachine2014,
  title = {Differential {{Privacy}} and {{Machine Learning}}: A {{Survey}} and {{Review}}},
  shorttitle = {Differential {{Privacy}} and {{Machine Learning}}},
  author = {Ji, Zhanglong and Lipton, Zachary C. and Elkan, Charles},
  year = {2014},
  month = dec,
  journal = {arXiv:1412.7584 [cs]},
  eprint = {1412.7584},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals. In this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially private algorithms. Finally, we present some open questions, including how to incorporate public data, how to deal with missing data in private datasets, and whether, as the number of observed samples grows arbitrarily large, differentially private machine learning algorithms can be achieved at no cost to utility as compared to corresponding non-differentially private algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Databases,Computer Science - Machine Learning,diff-privacy},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RRB2Y85X\\Ji et al_2014_Differential Privacy and Machine Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MTJ5RP7J\\1412.html}
}

@misc{jinDatalessKnowledgeFusion2022,
  title = {Dataless {{Knowledge Fusion}} by {{Merging Weights}} of {{Language Models}}},
  author = {Jin, Xisen and Ren, Xiang and {Preotiuc-Pietro}, Daniel and Cheng, Pengxiang},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09849},
  eprint = {2212.09849},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.09849},
  abstract = {Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,llm,model-patching,nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\C46AL8SH\\2212.html}
}

@article{jingNeuralStyleTransfer2019,
  title = {Neural Style Transfer: {{A}} Review},
  author = {Jing, Yongcheng and Yang, Yezhou and Feng, Zunlei and Ye, Jingwen and Yu, Yizhou and Song, Mingli},
  year = {2019},
  journal = {IEEE transactions on visualization and computer graphics},
  keywords = {style-transfer}
}

@inproceedings{jingTunableEfficientUnitary2016,
  title = {Tunable {{Efficient Unitary Neural Networks}} ({{EUNN}}) and Their Application to {{RNNs}}},
  booktitle = {{{ICML}}},
  author = {Jing, Li and Shen, Yichen and Dub{\v c}ek, Tena and Peurifoy, John and Skirlo, Scott and LeCun, Yann and Tegmark, Max and Solja{\v c}i{\'c}, Marin},
  year = {2016},
  month = dec,
  pages = {1733--1741},
  abstract = {Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely \$\textbackslash mathcal\{O\}(1)\$ per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.},
  keywords = {ICML,orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CUU5KC88\\Jing et al. - 2016 - Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs(2).pdf}
}

@article{johnsonComposingMusicRecurrent2015,
  title = {Composing {{Music With Recurrent Neural Networks}}},
  author = {Johnson, Daniel},
  year = {2015},
  journal = {Hexahedria},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  abstract = {It's hard not to be blown away by the surprising power of neural networks these days. With enough training, so called ``deep neural networks'', with many nodes and hidden layers, can do impressively well on modeling and predicting all kinds of data. (If you don't know what I'm talking about, I recommend reading about recurrent character-level language models, Google Deep Dream, and neural Turing machines. Very cool stuff!) Now seems like as good a time as ever to experiment with what a neural network can do. For a while now, I've been floating around vague ideas about writing a program to compose music. My original idea was based on a fractal decomposition of time and some sort of repetition mechanism, but after reading more about neural networks, I decided that they would be a better fit. So a few weeks ago, I got to work designing my network. And after training for a while, I am happy to report remarkable success!},
  keywords = {music,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DXZCIYPA\\Johnson - 2015 - Composing Music With Recurrent Neural Networks.pdf}
}

@article{johnsonGeneratingPolyphonicMusic2017,
  title = {Generating Polyphonic Music Using Tied Parallel Networks},
  author = {Johnson, Daniel D.},
  year = {2017},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {10198 LNCS},
  pages = {128--143},
  issn = {9783319557496},
  doi = {10.1007/978-3-319-55750-2_9},
  abstract = {We describe a neural network architecture which enables prediction and composition of polyphonic music in a manner that preserves translation-invariance of the dataset. Specifically, we demonstrate training a probabilistic model of polyphonic music using a set of parallel, tied-weight recurrent networks, inspired by the structure of convolutional neural networks. This model is designed to be invariant to transpositions, but otherwise is intentionally given minimal in-formation about the musical domain, and tasked with discovering patterns present in the source dataset. We present two versions of the model, denoted TP-LSTM-NADE and BALSTM, and also give methods for training the network and for generating novel music. This approach attains high performance at a musical pre-diction task and successfully creates note sequences which possess measure-level musical structure.},
  keywords = {music},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DW3QTJA6\\Johnson - 2017 - Generating polyphonic music using tied parallel networks.pdf}
}

@inproceedings{johnsonGoogleMultilingualNeural2017,
  title = {Google's {{Multilingual Neural Machine Translation System}}: {{Enabling Zero-Shot Translation}}},
  booktitle = {{{TACL}}},
  author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2017},
  month = nov,
  abstract = {We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English\$\textbackslash rightarrow\$French and surpasses state-of-the-art results for English\$\textbackslash rightarrow\$German. Similarly, a single multilingual model surpasses state-of-the-art results for French\$\textbackslash rightarrow\$English and German\$\textbackslash rightarrow\$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
  keywords = {attention,LSTM,nlp,NMT}
}

@article{johnsonPerceptualLossesRealTime2016,
  title = {Perceptual {{Losses}} for {{Real-Time Style Transfer}} and {{Super-Resolution}}},
  author = {Johnson, Justin and Alahi, Alexandre and {Fei-Fei}, Li},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.08155 [cs]},
  eprint = {1603.08155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \textbackslash emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \textbackslash emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,cv,notag,toread,vision},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf;C\:\\Users\\w-32\\Zotero\\storage\\H6876YNQ\\1603.html}
}

@article{johnsonPerceptualLossesRealTime2016a,
  title = {Perceptual {{Losses}} for {{Real-Time Style Transfer}} and {{Super-Resolution}}},
  author = {Johnson, Justin and Alahi, Alexandre and {Fei-Fei}, Li},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.08155 [cs]},
  eprint = {1603.08155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \textbackslash emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \textbackslash emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  archiveprefix = {arXiv},
  keywords = {exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RL3CVP6T\\Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KUNXAPTW\\1603.html}
}

@inproceedings{joseKroneckerRecurrentUnits2018,
  title = {Kronecker {{Recurrent Units}}},
  booktitle = {35th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2018},
  author = {Jose, Cijo and Ciss{\'e}, Moustapha and Fleuret, Fran{\c c}ois},
  year = {2018},
  month = may,
  volume = {6},
  pages = {3725--3742},
  abstract = {Our work addresses two important issues with recurrent neural networks: (1) they are overparametrized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on seven standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.},
  isbn = {978-1-5108-6796-3},
  keywords = {ICML,orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6PGZNFJA\\Jose, Ciss, Fleuret - 2018 - Kronecker Recurrent Units.pdf}
}

@inproceedings{josephEnergyBasedLatentAligner2022,
  title = {Energy-{{Based Latent Aligner}} for {{Incremental Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Joseph, K. J. and Khan, Salman and Khan, Fahad Shahbaz and Anwer, Rao Muhammad and Balasubramanian, Vineeth N.},
  year = {2022},
  pages = {7452--7461},
  langid = {english},
  keywords = {Continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QS7W8L89\\Joseph_Energy-Based_Latent_Aligner_for_Incremental_Learning_CVPR_2022_paper.html}
}

@article{jotyConvKNSemEval2016Task2016,
  title = {{{ConvKN}} at {{SemEval-2016 Task}} 3: {{Answer}} and Question Selection for Question Answering on {{Arabic}} and {{English}} Fora},
  shorttitle = {{{ConvKN}} at {{SemEval-2016 Task}} 3},
  author = {Joty, Shafiq and Moschitti, Alessandro and Al Obaidli, Fahad A. and Romeo, Salvatore and Tymoshenko, Kateryna and Uva, Antonio},
  year = {2016},
  journal = {Proceedings of SemEval},
  pages = {896--903},
  keywords = {nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\U783LVVP\\convKN semeval2016 task3 community question answering.pdf}
}

@article{joulinBagTricksEfficient2016,
  title = {Bag of Tricks for Efficient Text Classification},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  year = {2016},
  journal = {arXiv preprint arXiv:1607.01759},
  eprint = {1607.01759},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7HNEMYCT\\fastText.pdf}
}

@article{joulinInferringAlgorithmicPatterns2015,
  title = {Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
  author = {Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  journal = {Advances in Neural Information Processing Systems},
  volume = {2015-Janua},
  pages = {190--198},
  abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
  keywords = {MANN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8WHDEJJD\\Joulin, Mikolov - 2015 - Inferring algorithmic patterns with stack-augmented recurrent nets.pdf}
}

@article{jungContinualLearningNodeImportance2020,
  title = {Continual {{Learning}} with {{Node-Importance}} Based {{Adaptive Group Sparse Regularization}}},
  author = {Jung, Sangwon and Ahn, Hongjoon and Cha, Sungmin and Moon, Taesup},
  year = {2020},
  month = jun,
  journal = {arXiv:2003.13726 [cs, stat]},
  eprint = {2003.13726},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each node based its the importance, which is adaptively updated after learning each new task. By utilizing the proximal gradient descent method for learning, the exact sparsity and freezing of the model is guaranteed, and thus, the learner can explicitly control the model capacity as the learning continues. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to prevent the negative transfer that causes the catastrophic forgetting and facilitate efficient learning of new tasks. Throughout the extensive experimental results, we show that our AGS-CL uses much less additional memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative continual learning benchmarks for both supervised and reinforcement learning tasks.},
  archiveprefix = {arXiv},
  keywords = {cl-regularization,Computer Science - Machine Learning,continual,sparsity,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UQ5WK8IU\\Jung et al_2020_Continual Learning with Node-Importance based Adaptive Group Sparse.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NPIQZC98\\2003.html}
}

@article{kairouzAdvancesOpenProblems2021,
  title = {Advances and {{Open Problems}} in {{Federated Learning}}},
  author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gasc{\'o}n, Adri{\`a} and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konecn{\'y}, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancr{\`e}de and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and {\"O}zg{\"u}r, Ayfer and Pagh, Rasmus and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Raykova, Mariana and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tram{\`e}r, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
  year = {2021},
  month = jun,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {14},
  number = {1\textendash 2},
  eprint = {1912.04977},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1--210},
  publisher = {{Now Publishers, Inc.}},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000083},
  abstract = {Advances and Open Problems in Federated Learning},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {federated},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\22AISDJA\\Kairouz et al_2021_Advances and Open Problems in Federated Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\36B2VQC2\\MAL-083.html;C\:\\Users\\w-32\\Zotero\\storage\\GIJGE8SU\\1912.html}
}

@article{kaiserCanActiveMemory2016,
  title = {Can {{Active Memory Replace Attention}}?},
  author = {Kaiser, {\L}ukasz and Bengio, Samy},
  year = {2016},
  number = {NIPS},
  pages = {1--9},
  abstract = {Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.},
  keywords = {attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3ZWK6RPN\\Kaiser, Bengio - 2016 - Can Active Memory Replace Attention(2).pdf}
}

@article{kaiserNeuralGPUsLearn2015,
  title = {Neural {{GPUs Learn Algorithms}}},
  author = {Kaiser, Lukasz and Sutskever, Ilya},
  year = {2015},
  journal = {CoRR},
  volume = {abs/1511.0},
  keywords = {MANN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JNSZJWV9\\Kaiser, Sutskever - 2015 - Neural GPUs Learn Algorithms(2).pdf}
}

@article{kalchbrennerConvolutionalNeuralNetwork2014,
  title = {A Convolutional Neural Network for Modelling Sentences},
  author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  year = {2014},
  journal = {arXiv preprint arXiv:1404.2188},
  eprint = {1404.2188},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {CNN,nlp}
}

@article{kalchbrennerNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} in {{Linear Time}}},
  author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and van den Oord, Aaron and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = oct,
  abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
  keywords = {nlp,NMT},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Q8BBNKV2\\Kalchbrenner et al. - 2016 - Neural Machine Translation in Linear Time(2).pdf}
}

@article{kalmanNewApproachLinear1960,
  ids = {kalmanNewApproachLinear1960a},
  title = {A New Approach to Linear Filtering and Prediction Problems},
  author = {Kalman, Rudolph Emil and others},
  year = {1960},
  journal = {Journal of basic Engineering},
  volume = {82},
  number = {1},
  pages = {35--45},
  keywords = {Kalman}
}

@inproceedings{kanaiPreventingGradientExplosions2017,
  title = {Preventing Gradient Explosions in Gated Recurrent Units},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kanai, Sekitoshi and Fujiwara, Yasuhiro and Iwamura, Sotetsu},
  year = {2017},
  volume = {2017-Decem},
  pages = {436--445},
  abstract = {A gated recurrent unit (GRU) is a successful recurrent neural network architecture for time-series data. The GRU is typically trained using a gradient-based method, which is subject to the exploding gradient problem in which the gradient increases significantly. This problem is caused by an abrupt change in the dynamics of the GRU due to a small variation in the parameters. In this paper, we find a condition under which the dynamics of the GRU changes drastically and propose a learning method to address the exploding gradient problem. Our method constrains the dynamics of the GRU so that it does not drastically change. We evaluated our method in experiments on language modeling and polyphonic music modeling. Our experiments showed that our method can prevent the exploding gradient problem and improve modeling accuracy.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\55UHTXZU\\Kanai, Fujiwara, Iwamura - 2017 - Preventing gradient explosions in gated recurrent units(3).pdf}
}

@inproceedings{kangClassIncrementalLearningKnowledge2022,
  title = {Class-{{Incremental Learning}} by {{Knowledge Distillation With Adaptive Feature Consolidation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kang, Minsoo and Park, Jaeyoo and Han, Bohyung},
  year = {2022},
  pages = {16071--16080},
  langid = {english},
  keywords = {Continual,cvpr22,knowledge-distillation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7QZMNH9C\\Kang_Class-Incremental_Learning_by_Knowledge_Distillation_With_Adaptive_Feature_Consolidation_C.html}
}

@inproceedings{kanuparthiHDETACHMODIFYINGLSTM2018,
  title = {H-{{DETACH}}: {{MODIFYING THE LSTM GRADIENT TOWARDS BETTER OPTIMIZATION}}},
  booktitle = {{{ICLR}}},
  author = {Kanuparthi, Bhargav and Arpit, Devansh and Kerg, Giancarlo and Ke, Nan Rosemary and Mitliagkas, Ioannis and Bengio, Yoshua},
  year = {2018},
  pages = {1--21},
  keywords = {LSTM,orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\U7Q9FPDR\\Kanuparthi et al. - 2018 - h-DETACH MODIFYING THE LSTM GRADIENT TOWARDS BETTER OPTIMIZATION(2).pdf}
}

@article{kaoNaturalContinualLearning2021,
  title = {Natural Continual Learning: Success Is a Journey, Not (Just) a Destination},
  shorttitle = {Natural Continual Learning},
  author = {Kao, Ta-Chu and Jensen, Kristopher T. and {van de Ven}, Gido M. and Bernacchia, Alberto and Hennequin, Guillaume},
  year = {2021},
  month = dec,
  journal = {arXiv:2106.08085 [cs, q-bio]},
  eprint = {2106.08085},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Biological agents are known to learn many different tasks over the course of their lives, and to be able to revisit previous tasks and behaviors with little to no loss in performance. In contrast, artificial agents are prone to 'catastrophic forgetting' whereby performance on previous tasks deteriorates rapidly as new ones are acquired. This shortcoming has recently been addressed using methods that encourage parameters to stay close to those used for previous tasks. This can be done by (i) using specific parameter regularizers that map out suitable destinations in parameter space, or (ii) guiding the optimization journey by projecting gradients into subspaces that do not interfere with previous tasks. However, these methods often exhibit subpar performance in both feedforward and recurrent neural networks, with recurrent networks being of interest to the study of neural dynamics supporting biological continual learning. In this work, we propose Natural Continual Learning (NCL), a new method that unifies weight regularization and projected gradient descent. NCL uses Bayesian weight regularization to encourage good performance on all tasks at convergence and combines this with gradient projection using the prior precision, which prevents catastrophic forgetting during optimization. Our method outperforms both standard weight regularization techniques and projection based approaches when applied to continual learning problems in feedforward and recurrent networks. Finally, the trained networks evolve task-specific dynamics that are strongly preserved as new tasks are learned, similar to experimental findings in biological circuits.},
  archiveprefix = {arXiv},
  keywords = {bayesian,cl-replay,continual,regularization,replay},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AIL6VK8Z\\Kao et al_2021_Natural continual learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\AHU2FXXX\\2106.html}
}

@article{karpathyVisualizingUnderstandingRecurrent2015,
  title = {Visualizing and {{Understanding Recurrent Networks}}},
  author = {Karpathy, Andrej and Johnson, Justin and {Fei-Fei}, Li},
  year = {2015},
  month = jun,
  abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
  keywords = {RNN,visualization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\G23DVK2N\\Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks(3).pdf}
}

@misc{kasarlaMaximumClassSeparation2022,
  title = {Maximum {{Class Separation}} as {{Inductive Bias}} in {{One Matrix}}},
  author = {Kasarla, Tejaswi and Burghouts, Gertjan J. and {van Spengler}, Max and {van der Pol}, Elise and Cucchiara, Rita and Mettes, Pascal},
  year = {2022},
  month = oct,
  number = {arXiv:2206.08704},
  eprint = {2206.08704},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.08704},
  abstract = {Maximizing the separation between classes constitutes a well-known inductive bias in machine learning and a pillar of many traditional algorithms. By default, deep networks are not equipped with this inductive bias and therefore many alternative solutions have been proposed through differential optimization. Current approaches tend to optimize classification and separation jointly: aligning inputs with class vectors and separating class vectors angularly. This paper proposes a simple alternative: encoding maximum separation as an inductive bias in the network by adding one fixed matrix multiplication before computing the softmax activations. The main observation behind our approach is that separation does not require optimization but can be solved in closed-form prior to training and plugged into a network. We outline a recursive approach to obtain the matrix consisting of maximally separable vectors for any number of classes, which can be added with negligible engineering effort and computational overhead. Despite its simple nature, this one matrix multiplication provides real impact. We show that our proposal directly boosts classification, long-tailed recognition, out-of-distribution detection, and open-set recognition, from CIFAR to ImageNet. We find empirically that maximum separation works best as a fixed bias; making the matrix learnable adds nothing to the performance. The closed-form implementation and code to reproduce the experiments are available on github.},
  archiveprefix = {arXiv},
  keywords = {class-separation,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QQQ2ETEI\\2206.html}
}

@article{katharopoulosTransformersAreRNNs2020,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.16236 [cs, stat]},
  eprint = {2006.16236},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \$\textbackslash mathcal\{O\}\textbackslash left(N\^2\textbackslash right)\$ to \$\textbackslash mathcal\{O\}\textbackslash left(N\textbackslash right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,efficient,sparse-attention,Statistics - Machine Learning,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MTMWJXMH\\Katharopoulos et al_2020_Transformers are RNNs.pdf;C\:\\Users\\w-32\\Zotero\\storage\\TS9X8CMR\\2006.html}
}

@article{kawaguchiGeneralizationDeepLearning2017,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  year = {2017},
  month = oct,
  abstract = {With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.},
  keywords = {generalization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CM8E8A6B\\Kawaguchi, Kaelbling, Bengio - 2017 - Generalization in Deep Learning(2).pdf}
}

@article{keFocusedHierarchicalRNNs2018,
  title = {Focused {{Hierarchical RNNs}} for {{Conditional Sequence Processing}}},
  author = {Ke, Nan Rosemary and Zolna, Konrad and Sordoni, Alessandro and Lin, Zhouhan and Trischler, Adam and Bengio, Yoshua and Pineau, Joelle and Charlin, Laurent and Pal, Chris},
  year = {2018},
  month = jun,
  abstract = {Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multi-layer conditional sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.},
  keywords = {Hierarchical RNN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PIVAP8DR\\Ke et al. - 2018 - Focused Hierarchical RNNs for Conditional Sequence Processing(2).pdf}
}

@inproceedings{kemkerFearNetBrainInspiredModel2018,
  title = {{{FearNet}}: {{Brain-Inspired Model}} for {{Incremental Learning}}},
  shorttitle = {{{FearNet}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kemker, Ronald and Kanan, Christopher},
  year = {2018},
  month = feb,
  abstract = {FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.},
  langid = {english},
  keywords = {notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IKI4UAMG\\Kemker_Kanan_2018_FearNet.pdf;C\:\\Users\\w-32\\Zotero\\storage\\CHLBPQI7\\forum.html}
}

@article{kergUntanglingTradeoffsRecurrence2020,
  title = {Untangling Tradeoffs between Recurrence and Self-Attention in Neural Networks},
  author = {Kerg, Giancarlo and Kanuparthi, Bhargav and Goyal, Anirudh and Goyette, Kyle and Bengio, Yoshua and Lajoie, Guillaume},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.09471 [cs, stat]},
  eprint = {2006.09471},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Attention and self-attention mechanisms, inspired by cognitive processes, are now central to state-of-the-art deep learning on sequential tasks. However, most recent progress hinges on heuristic approaches with limited understanding of attention's role in model optimization and computation, and rely on considerable memory and computational resources that scale poorly. In this work, we present a formal analysis of how self-attention affects gradient propagation in recurrent networks, and prove that it mitigates the problem of vanishing gradients when trying to capture long-term dependencies. Building on these results, we propose a relevancy screening mechanism, inspired by the cognitive process of memory consolidation, that allows for a scalable use of sparse self-attention with recurrence. While providing guarantees to avoid vanishing gradients, we use simple numerical experiments to demonstrate the tradeoffs in performance and computational resources by efficiently balancing attention and recurrence. Based on our results, we propose a concrete direction of research to improve scalability of attentive networks.},
  archiveprefix = {arXiv},
  keywords = {attention,RNN,sparse-attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3GCGXD99\\Kerg et al. - 2020 - Untangling tradeoffs between recurrence and self-a.pdf;C\:\\Users\\w-32\\Zotero\\storage\\9SXZRRTR\\2006.html}
}

@article{kharitonovWhatTheyWhen2020,
  title = {What They Do When in Doubt: A Study of Inductive Biases in Seq2seq Learners},
  shorttitle = {What They Do When in Doubt},
  author = {Kharitonov, Eugene and Chaabouni, Rahma},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.14953 [cs, math]},
  eprint = {2006.14953},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Sequence-to-sequence (seq2seq) learners are widely used, but we still have only limited knowledge about what inductive biases shape the way they generalize. We address that by investigating how popular seq2seq learners generalize in tasks that have high ambiguity in the training data. We use SCAN and three new tasks to study learners' preferences for memorization, arithmetic, hierarchical, and compositional reasoning. Further, we connect to Solomonoff's theory of induction and propose to use description length as a principled and sensitive measure of inductive biases. In our experimental study, we find that LSTM-based learners can learn to perform counting, addition, and multiplication by a constant from a single training example. Furthermore, Transformer and LSTM-based learners show a bias toward the hierarchical induction over the linear one, while CNN-based learners prefer the opposite. On the SCAN dataset, we find that CNN-based, and, to a lesser degree, Transformer- and LSTM-based learners have a preference for compositional generalization over memorization. Finally, across all our experiments, description length proved to be a sensitive measure of inductive biases.},
  archiveprefix = {arXiv},
  keywords = {architectural-bias,RNN,seq2seq},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\E4ZMEI69\\Kharitonov and Chaabouni - 2020 - What they do when in doubt a study of inductive b.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KKRQKD54\\2006.html}
}

@article{khasahmadiMemoryBasedGraphNetworks2020,
  title = {Memory-{{Based Graph Networks}}},
  author = {Khasahmadi, Amir Hosein and Hassani, Kaveh and Moradi, Parsa and Lee, Leo and Morris, Quaid},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.09518 [cs, stat]},
  eprint = {2002.09518},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Graph neural networks (GNNs) are a class of deep models that operate on data with arbitrary topology represented as graphs. We introduce an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. We also introduce two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results shows that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data. Code and reference implementations are released at: https://github.com/amirkhas/GraphMemoryNet},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UACT6EAJ\\Khasahmadi et al_2020_Memory-Based Graph Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\R5EWKQ45\\2002.html}
}

@article{khatibPreemptingCatastrophicForgetting2019,
  title = {Preempting {{Catastrophic Forgetting}} in {{Continual Learning Models}} by {{Anticipatory Regularization}}},
  author = {Khatib, Alaa El},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BAUJV62I\\Khatib - 2019 - Preempting Catastrophic Forgetting in Continual Learning Models by Anticipatory Regularization(2).pdf}
}

@article{khrulkovExpressivePowerRecurrent2017,
  title = {Expressive Power of Recurrent Neural Networks},
  author = {Khrulkov, Valentin and Novikov, Alexander and Oseledets, Ivan},
  year = {2017},
  month = nov,
  abstract = {Deep neural networks are surprisingly efficient at solving practical tasks, but the theory behind this phenomenon is only starting to catch up with the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks -- namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition -- has been proven to have exponentially higher expressive power than shallow networks. I.e. a shallow network of exponential width is required to realize the same score function as computed by the deep architecture. In this paper, we prove the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of recurrent neural networks -- ones that correspond to the Tensor Train (TT) decomposition. This means that even processing an image patch by patch with an RNN can be exponentially more efficient than a (shallow) convolutional network with one hidden layer. Using theoretical results on the relation between the tensor decompositions we compare expressive powers of the HT- and TT-Networks. We also implement the recurrent TT-Networks and provide numerical evidence of their expressivity.},
  keywords = {generalization,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JTGEEW2R\\Khrulkov, Novikov, Oseledets - 2017 - Expressive power of recurrent neural networks(2).pdf}
}

@article{khrulkovGeneralizedTensorModels2019,
  title = {Generalized {{Tensor Models}} for {{Recurrent Neural Networks}}},
  author = {Khrulkov, Valentin and Hrinchuk, Oleksii and Oseledets, Ivan},
  year = {2019},
  month = jan,
  abstract = {Recurrent Neural Networks (RNNs) are very successful at solving challenging problems with sequential data. However, this observed efficiency is not yet entirely explained by theory. It is known that a certain class of multiplicative RNNs enjoys the property of depth efficiency --- a shallow network of exponentially large width is necessary to realize the same score function as computed by such an RNN. Such networks, however, are not very often applied to real life tasks. In this work, we attempt to reduce the gap between theory and practice by extending the theoretical analysis to RNNs which employ various nonlinearities, such as Rectified Linear Unit (ReLU), and show that they also benefit from properties of universality and depth efficiency. Our theoretical results are verified by a series of extensive computational experiments.},
  keywords = {RNN,tensor},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KKLH7T8X\\Khrulkov, Hrinchuk, Oseledets - 2019 - Generalized Tensor Models for Recurrent Neural Networks(2).pdf}
}

@misc{kimMultiHeadModelContinual2022,
  title = {A {{Multi-Head Model}} for {{Continual Learning}} via {{Out-of-Distribution Replay}}},
  author = {Kim, Gyuhak and Ke, Zixuan and Liu, Bing},
  year = {2022},
  month = aug,
  number = {arXiv:2208.09734},
  eprint = {2208.09734},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.09734},
  abstract = {This paper studies class incremental learning (CIL) of continual learning (CL). Many approaches have been proposed to deal with catastrophic forgetting (CF) in CIL. Most methods incrementally construct a single classifier for all classes of all tasks in a single head network. To prevent CF, a popular approach is to memorize a small number of samples from previous tasks and replay them during training of the new task. However, this approach still suffers from serious CF as the parameters learned for previous tasks are updated or adjusted with only the limited number of saved samples in the memory. This paper proposes an entirely different approach that builds a separate classifier (head) for each task (called a multi-head model) using a transformer network, called MORE. Instead of using the saved samples in memory to update the network for previous tasks/classes in the existing approach, MORE leverages the saved samples to build a task specific classifier (adding a new classification head) without updating the network learned for previous tasks/classes. The model for the new task in MORE is trained to learn the classes of the task and also to detect samples that are not from the same data distribution (i.e., out-of-distribution (OOD)) of the task. This enables the classifier for the task to which the test instance belongs to produce a high score for the correct class and the classifiers of other tasks to produce low scores because the test instance is not from the data distributions of these classifiers. Experimental results show that MORE outperforms state-of-the-art baselines and is also naturally capable of performing OOD detection in the continual learning setting.},
  archiveprefix = {arXiv},
  keywords = {continual,ood-detection},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8VEGL3YK\\2208.html}
}

@article{kimResidualLSTMDesign2017,
  title = {Residual {{LSTM}}: {{Design}} of a Deep Recurrent Architecture for Distant Speech Recognition},
  author = {Kim, Jaeyoung and {El-Khamy}, Mostafa and Lee, Jungwon},
  year = {2017},
  journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  volume = {2017-Augus},
  pages = {1591--1595},
  doi = {10.21437/Interspeech.2017-477},
  abstract = {In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The residual LSTM provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM separates a spatial shortcut path with temporal one by using output layers, which can help to avoid a conflict between spatial and temporal-domain gradient flows. Furthermore, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10\% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus shows that 10-layer plain and highway LSTM networks presented 13.7\% and 6.2\% increase in WER over 3-layer aselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0\%, which corresponds to 3.3\% and 2.8\% WER reduction over plain and highway LSTM networks, respectively.},
  keywords = {CNN,GMM,LSTM,RNN,speech},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TBBKX97Y\\Kim, El-Khamy, Lee - 2017 - Residual LSTM Design of a deep recurrent architecture for distant speech recognition(2).pdf}
}

@article{kimSequenceLevelKnowledgeDistillation2016,
  title = {Sequence-{{Level Knowledge Distillation}}},
  author = {Kim, Yoon and Rush, Alexander M.},
  year = {2016},
  month = sep,
  journal = {arXiv:1606.07947 [cs]},
  eprint = {1606.07947},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13\texttimes{} fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BATS3EPK\\Kim and Rush - 2016 - Sequence-Level Knowledge Distillation.pdf}
}

@article{kimStructuredAttentionNetworks2017,
  title = {Structured {{Attention Networks}}},
  author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
  year = {2017},
  month = feb,
  journal = {CoRR},
  volume = {abs/1702.0},
  abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
  keywords = {attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\G5YM4KA8\\Kim et al. - 2017 - Structured Attention Networks.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  pages = {1--15},
  issn = {9781450300728},
  doi = {10.1145/1830483.1830503},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VPAYFNCL\\Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf}
}

@article{kingmaAutoEncodingVariationalBayes2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P and Welling, Max},
  year = {2013},
  number = {Ml},
  pages = {1--14},
  issn = {1312.6114v10},
  doi = {10.1051/0004-6361/201527329},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  keywords = {vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6UJF3Z74\\Kingma, Welling - 2013 - Auto-Encoding Variational Bayes(3).pdf}
}

@misc{kirichenkoLastLayerReTraining2022,
  title = {Last {{Layer Re-Training}} Is {{Sufficient}} for {{Robustness}} to {{Spurious Correlations}}},
  author = {Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  year = {2022},
  month = apr,
  number = {arXiv:2204.02937},
  eprint = {2204.02937},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.02937},
  abstract = {Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.},
  archiveprefix = {arXiv},
  keywords = {nn-robustness,noread,vision},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GA8WZH3X\\2204.html}
}

@article{kirichenkoTaskagnosticContinualLearning2021,
  title = {Task-Agnostic {{Continual Learning}} with {{Hybrid Probabilistic Models}}},
  author = {Kirichenko, Polina and Farajtabar, Mehrdad and Rao, Dushyant and Lakshminarayanan, Balaji and Levine, Nir and Li, Ang and Hu, Huiyi and Wilson, Andrew Gordon and Pascanu, Razvan},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.12772 [cs, stat]},
  eprint = {2106.12772},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning new tasks continuously without forgetting on a constantly changing data distribution is essential for real-world problems but extremely challenging for modern deep learning. In this work we propose HCL, a Hybrid generative-discriminative approach to Continual Learning for classification. We model the distribution of each task and each class with a normalizing flow. The flow is used to learn the data distribution, perform classification, identify task changes, and avoid forgetting, all leveraging the invertibility and exact likelihood which are uniquely enabled by the normalizing flow model. We use the generative capabilities of the flow to avoid catastrophic forgetting through generative replay and a novel functional regularization technique. For task identification, we use state-of-the-art anomaly detection techniques based on measuring the typicality of the model's statistics. We demonstrate the strong performance of HCL on a range of continual learning benchmarks such as split-MNIST, split-CIFAR, and SVHN-MNIST.},
  archiveprefix = {arXiv},
  keywords = {bayesian,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\R9J36TSB\\2106.html}
}

@article{kirkpatrickOvercomingCatastrophicForgetting2017,
  ids = {kirkpatrickOvercomingCatastrophicForgetting2017a},
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and others},
  year = {2017},
  journal = {Proceedings of the National Academy of Sciences},
  pages = {201611835},
  keywords = {continual,ewc,regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HQ3DMSWV\\Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MDD7WC2R\\pnas.201611835SI.pdf}
}

@article{kirosSkipThoughtVectors2015,
  title = {Skip-{{Thought Vectors}}},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  year = {2015},
  month = jun,
  journal = {Advances in Neural Information Processing Systems},
  abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
  keywords = {nlp,nlp-embeddings}
}

@inproceedings{klambauerSelfNormalizingNeuralNetworks2017,
  title = {Self-{{Normalizing Neural Networks}}},
  booktitle = {{{NIPS}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QVSIEGWE\\Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf}
}

@inproceedings{klapper-rybickaUnsupervisedLearningLSTM2001,
  title = {Unsupervised {{Learning}} in {{LSTM Recurrent Neural Networks}}},
  booktitle = {{{ICANN}}},
  author = {{Klapper-Rybicka}, Magdalena and Schraudolph, Nicol N. and Schmidhuber, J{\"u}rgen},
  year = {2001},
  keywords = {info-theory,LSTM,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TG2ZTCCS\\Klapper-Rybicka, Schraudolph, Schmidhuber - 2001 - Unsupervised Learning in LSTM Recurrent Neural Networks(2).pdf}
}

@article{knoblauchOptimalContinualLearning2020,
  title = {Optimal {{Continual Learning}} Has {{Perfect Memory}} and Is {{NP-hard}}},
  author = {Knoblauch, Jeremias and Husain, Hisham and Diethe, Tom},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.05188 [cs, stat]},
  eprint = {2006.05188},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Continual Learning (CL) algorithms incrementally learn a predictor or representation across multiple sequentially observed tasks. Designing CL algorithms that perform reliably and avoid so-called catastrophic forgetting has proven a persistent challenge. The current paper develops a theoretical approach that explains why. In particular, we derive the computational properties which CL algorithms would have to possess in order to avoid catastrophic forgetting. Our main finding is that such optimal CL algorithms generally solve an NP-hard problem and will require perfect memory to do so. The findings are of theoretical interest, but also explain the excellent performance of CL algorithms using experience replay, episodic memory and core sets relative to regularization-based approaches.},
  archiveprefix = {arXiv},
  keywords = {continual,theory-framwork},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8ZGA7CUL\\Knoblauch et al. - 2020 - Optimal Continual Learning has Perfect Memory and .pdf;C\:\\Users\\w-32\\Zotero\\storage\\FYS8UYSE\\2006.html}
}

@inproceedings{koehnSixChallengesNeural2017,
  title = {Six {{Challenges}} for {{Neural Machine Translation}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Neural Machine Translation}}},
  author = {Koehn, Philipp and Knowles, Rebecca},
  year = {2017},
  month = jun,
  abstract = {We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.},
  keywords = {NMT},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\A3HDJLJR\\Koehn, Knowles - 2017 - Six Challenges for Neural Machine Translation(2).pdf}
}

@inproceedings{koehnStatisticalPhraseBasedTranslation2003,
  title = {Statistical {{Phrase-Based Translation}}},
  booktitle = {{{HLT-NAACL}}},
  author = {Koehn, Philipp and Och, Franz Josef and Marcu, Daniel},
  year = {2003},
  pages = {48--54},
  doi = {10.1007/BF00293403},
  abstract = {We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, pre vi- ously proposed phrase-based translation mod- els. Within our frame work, we carry out a lar ge number of experiments to understand bet- ter and explain why phrase-based models out- perform word-based models. Our empirical re- sults, which hold for all examined language pairs, suggest that the highest levels of perfor - mance can be obtained through relati vely sim- ple means: heuristic learning of phrase trans- lations from word-based alignments and lexi- cal weighting of phrase translations. Surpris- ingly , learning phrases longer than three words and learning phrases from high-accurac y word- level alignment models does not have a strong impact on performance. Learning only syntac- tically moti vated phrases degrades the perfor - mance of our systems.},
  isbn = {0001-6322},
  keywords = {NMT,probabilistic},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5JPDPHVR\\Koehn, Och, Marcu - 2003 - Statistical Phrase-Based Translation(3).pdf}
}

@article{kohlerTheoreticalUnderstandingBatch2018,
  title = {Towards a {{Theoretical Understanding}} of {{Batch Normalization}}},
  author = {Kohler, Jonas and Daneshmand, Hadi and Lucchi, Aurelien and Zhou, Ming and Neymeyr, Klaus and Hofmann, Thomas},
  year = {2018},
  month = may,
  abstract = {Normalization techniques such as Batch Normalization have been applied very successfully for training deep neural networks. Yet, despite its apparent empirical benefits, the reasons behind the success of Batch Normalization are mostly hypothetical. We thus aim to provide a more thorough theoretical understanding from an optimization perspective. Our main contribution towards this goal is the identification of various problem instances in the realm of machine learning where, under certain assumptions, Batch Normalization can provably accelerate optimization with gradient-based methods. We thereby turn Batch Normalization from an effective practical heuristic into a provably converging algorithm for these settings. Furthermore, we substantiate our analysis with empirical evidence that suggests the validity of our theoretical results in a broader context.},
  keywords = {regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5JPC6VDB\\Kohler et al. - 2018 - Towards a Theoretical Understanding of Batch Normalization(2).pdf}
}

@article{koolAttentionSolvesYour2018,
  title = {Attention {{Solves Your TSP}}},
  author = {Kool, W. W. M. and Welling, M.},
  year = {2018},
  month = mar,
  journal = {CoRR},
  volume = {abs/1803.0},
  abstract = {We propose a framework for solving combinatorial optimization problems of which the output can be represented as a sequence of input elements. As an alternative to the Pointer Network, we parameterize a policy by a model based entirely on (graph) attention layers, and train it efficiently using REINFORCE with a simple and robust baseline based on a deterministic (greedy) rollout of the best policy found during training. We significantly improve over state-of-the-art results for learning algorithms for the 2D Euclidean TSP, reducing the optimality gap for a single tour construction by more than 75\% (to 0.33\%) and 50\% (to 2.28\%) for instances with 20 and 50 nodes respectively.},
  keywords = {attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5H3DRFWE\\Kool, Welling - 2018 - Attention Solves Your TSP(2).pdf}
}

@article{kossenSelfAttentionDatapointsGoing2021,
  title = {Self-{{Attention Between Datapoints}}: {{Going Beyond Individual Input-Output Pairs}} in {{Deep Learning}}},
  shorttitle = {Self-{{Attention Between Datapoints}}},
  author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N. and Rainforth, Tom and Gal, Yarin},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.02584 [cs, stat]},
  eprint = {2106.02584},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input. To this end, we introduce a general-purpose deep learning architecture that takes as input the entire dataset instead of processing one datapoint at a time. Our approach uses self-attention to reason about relationships between datapoints explicitly, which can be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive results on tabular data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points.},
  archiveprefix = {arXiv},
  keywords = {bayesian,Computer Science - Machine Learning,nonparametric,Statistics - Machine Learning,transformer},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Kossen et al_2021_Self-Attention Between Datapoints.pdf;C\:\\Users\\w-32\\Zotero\\storage\\5I6NCF3F\\2106.html}
}

@inproceedings{koutnikClockworkRNN2014,
  title = {A {{Clockwork RNN}}},
  booktitle = {Proceedings of the 31 St {{International Conference}} on {{Machine Learning}}, {{Beijing}}, {{China}}},
  author = {Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2014},
  volume = {32},
  pages = {1--9},
  abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learn-ing that can require identifying complex depen-dencies between temporally distant inputs. Recur-rent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. How-ever, in practice they are difficult to train success-fully when long-term memory is required. This paper introduces a simple, yet powerful modifica-tion to the simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN mod-els more complex, CW-RNN reduces the number of SRN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word clas-sification, where it outperforms both SRN and LSTM networks, and online handwriting recogni-tion, where it outperforms SRNs.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3BSKLGA8\\Koutnik et al. - 2014 - A Clockwork RNN.pdf}
}

@article{kraskaCaseLearnedIndex2017,
  title = {The {{Case}} for {{Learned Index Structures}}},
  author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
  year = {2017},
  number = {1},
  pages = {1--27},
  issn = {0897915240},
  doi = {10.1109/TKDE.2015.2507132},
  abstract = {Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70\% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8HDTKI8X\\Kraska et al. - 2017 - The Case for Learned Index Structures.pdf}
}

@article{krauseDynamicEvaluationNeural2017,
  title = {Dynamic {{Evaluation}} of {{Neural Sequence Models}}},
  author = {Krause, Ben and Kahembwe, Emmanuel and Murray, Iain and Renals, Steve},
  year = {2017},
  month = sep,
  abstract = {We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WU7ZABJV\\Krause et al. - 2017 - Dynamic Evaluation of Neural Sequence Models(2).pdf}
}

@inproceedings{krizhevskyImagenetClassificationDeep2012,
  ids = {krizhevskyImagenetClassificationDeep2012a},
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  pages = {1097--1105},
  keywords = {CNN}
}

@techreport{krizhevskyLearningMultipleLayers2009,
  ids = {krizhevskyLearningMultipleLayersa},
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  year = {2009},
  month = apr,
  pages = {60},
  institution = {{University of Toronto}},
  langid = {english},
  keywords = {notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HBKWGTXG\\Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@inproceedings{kruegerRegularizingRNNsStabilizing2015,
  title = {Regularizing {{RNNs}} by {{Stabilizing Activations}}},
  booktitle = {{{ICLR}} 2016},
  author = {Krueger, David and Memisevic, Roland},
  year = {2015},
  month = nov,
  abstract = {We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\textbackslash\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.},
  keywords = {regularization,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\R8TCG8GV\\Krueger, Memisevic - 2015 - Regularizing RNNs by Stabilizing Activations.pdf}
}

@article{kruegerZoneoutRegularizingRNNs2016,
  title = {Zoneout: {{Regularizing RNNs}} by {{Randomly Preserving Hidden Activations}}},
  author = {Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
  year = {2016},
  month = jun,
  abstract = {We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.},
  keywords = {regularization,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MJZL5EEW\\Krueger et al. - 2016 - Zoneout Regularizing RNNs by Randomly Preserving Hidden Activations(2).pdf}
}

@article{kruszewskiEvaluatingOnlineContinual2021,
  title = {Evaluating {{Online Continual Learning}} with {{CALM}}},
  author = {Kruszewski, Germ{\'a}n and Sorodoc, Ionut-Teodor and Mikolov, Tomas},
  year = {2021},
  month = feb,
  journal = {arXiv:2004.03340 [cs]},
  eprint = {2004.03340},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn "on-the-wild". Yet, commonly available benchmarks are far from these real-world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.},
  archiveprefix = {arXiv},
  keywords = {continual,fast-adaptation,nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\A595TGXW\\Kruszewski et al_2021_Evaluating Online Continual Learning with CALM.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZMSRU94D\\2004.html}
}

@article{kudithipudiBiologicalUnderpinningsLifelong2022,
  title = {Biological Underpinnings for Lifelong Learning Machines},
  author = {Kudithipudi, Dhireesha and {Aguilar-Simon}, Mario and Babb, Jonathan and Bazhenov, Maxim and Blackiston, Douglas and Bongard, Josh and Brna, Andrew P. and Chakravarthi Raja, Suraj and Cheney, Nick and Clune, Jeff and Daram, Anurag and Fusi, Stefano and Helfer, Peter and Kay, Leslie and Ketz, Nicholas and Kira, Zsolt and Kolouri, Soheil and Krichmar, Jeffrey L. and Kriegman, Sam and Levin, Michael and Madireddy, Sandeep and Manicka, Santosh and Marjaninejad, Ali and McNaughton, Bruce and Miikkulainen, Risto and Navratilova, Zaneta and Pandit, Tej and Parker, Alice and Pilly, Praveen K. and Risi, Sebastian and Sejnowski, Terrence J. and Soltoggio, Andrea and Soures, Nicholas and Tolias, Andreas S. and {Urbina-Mel{\'e}ndez}, Dar{\'i}o and {Valero-Cuevas}, Francisco J. and {van de Ven}, Gido M. and Vogelstein, Joshua T. and Wang, Felix and Weiss, Ron and {Yanguas-Gil}, Angel and Zou, Xinyun and Siegelmann, Hava},
  year = {2022},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {196--210},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00452-0},
  abstract = {Biological organisms learn from interactions with their environment throughout their lifetime. For artificial systems to successfully act and adapt in the real world, it is desirable to similarly be able to learn on a continual basis. This challenge is known as lifelong learning, and remains to a large extent unsolved. In this Perspective article, we identify a set of key capabilities that artificial systems will need to achieve lifelong learning. We describe a number of biological mechanisms, both neuronal and non-neuronal, that help explain how organisms solve these challenges, and present examples of biologically inspired models and biologically plausible mechanisms that have been applied to artificial systems in the quest towards development of lifelong learning machines. We discuss opportunities to further our understanding and advance the state of the art in lifelong learning, aiming to bridge the gap between natural and artificial intelligence. It is an outstanding challenge to develop intelligent machines that can learn continually from interactions with their environment, throughout their lifetime. Kudithipudi et al. review neuronal and non-neuronal processes in organisms that address this challenge and discuss pathways to developing biologically inspired approaches for lifelong learning machines.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Continual,neuroscience,noread,replay},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EG2CF6IP\\s42256-022-00452-0.html}
}

@misc{kumarFineTuningCanDistort2022,
  title = {Fine-{{Tuning}} Can {{Distort Pretrained Features}} and {{Underperform Out-of-Distribution}}},
  author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
  year = {2022},
  month = feb,
  number = {arXiv:2202.10054},
  eprint = {2202.10054},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.10054},
  abstract = {When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \$\textbackslash to\$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full fine-tuning).},
  archiveprefix = {arXiv},
  keywords = {finetuning,pretraining},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EVJJ7WXK\\2202.html}
}

@inproceedings{kunduAnalyzingConfidentialityUndistillable2021,
  title = {Analyzing the {{Confidentiality}} of {{Undistillable Teachers}} in {{Knowledge Distillation}}},
  booktitle = {Thirty-{{Fifth Conference}} on {{Neural Information Processing Systems}}},
  author = {Kundu, Souvik and Sun, Qirui and Fu, Yao and Pedram, Massoud and Beerel, Peter Anthony},
  year = {2021},
  month = may,
  abstract = {We analyze the limitations of undistillable teachers in the context of model IP protection and propose an effective solution to distill knowledge from even a nasty teacher raising a fundamental...},
  langid = {english},
  keywords = {exmodel,knowledge distillation,notag},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Kundu et al_2021_Analyzing the Confidentiality of Undistillable Teachers in Knowledge.pdf;C\:\\Users\\w-32\\Zotero\\storage\\WLLHMBJ7\\forum.html}
}

@misc{kunstnerLimitationsEmpiricalFisher2020,
  title = {Limitations of the {{Empirical Fisher Approximation}} for {{Natural Gradient Descent}}},
  author = {Kunstner, Frederik and Balles, Lukas and Hennig, Philipp},
  year = {2020},
  month = jun,
  number = {arXiv:1905.12558},
  eprint = {1905.12558},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.12558},
  abstract = {Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher---unlike the Fisher---does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.},
  archiveprefix = {arXiv},
  keywords = {fisher,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8V5ZRXB4\\Kunstner et al_2020_Limitations of the Empirical Fisher Approximation for Natural Gradient Descent.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NAQIL2BA\\1905.html}
}

@article{kuoInputResidualConnection,
  title = {An {{Input Residual Connection}} for {{Simplifying Gated Recurrent Neural Networks}}},
  author = {Kuo, Nicholas I H and Harandi, Mehrtash and Fourrier, Nicolas and Walder, Christian and Ferraro, Gabriela and Suominen, Hanna},
  pages = {8},
  abstract = {Gated Recurrent Neural Networks (GRNNs) are important models that continue to push the state-of-the-art solutions across different machine learning problems. However, they are composed of intricate components that are generally not well understood. We increase GRNN interpretability by linking the canonical Gated Recurrent Unit (GRU) design to the well-studied Hopfield network. This connection allowed us to identify network redundancies, which we simplified with an Input Residual Connection (IRC). We tested GRNNs against their IRC counterparts on language modelling. In addition, we proposed an Input Highway Connection (IHC) as an advance application of the IRC and then evaluated the most widely applied GRNN of the Long Short-Term Memory (LSTM) and IHC-LSTM on tasks of i) image generation and ii) learning to learn to update another learner-network. Despite parameter reductions, all IRC-GRNNs showed either comparative or superior generalisation than their baseline models. Furthermore, compared to LSTM, the IHC-LSTM removed 85.4\% parameters on image generation. In conclusion, the IRC is applicable, but not limited, to the GRNN designs of GRUs and LSTMs but also to FastGRNNs, Simple Recurrent Units (SRUs), and Strongly-Typed Recurrent Neural Networks (T-RNNs).},
  langid = {english},
  keywords = {gradient-flow,RNN,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SQLCJ974\\Kuo et al. - An Input Residual Connection for Simplifying Gated.pdf}
}

@article{kurachGANLandscapeLosses2018,
  title = {The {{GAN Landscape}}: {{Losses}}, {{Architectures}}, {{Regularization}}, and {{Normalization}}},
  author = {Kurach, Karol and Lucic, Mario and Zhai, Xiaohua and Michalski, Marcin and Gelly, Sylvain},
  year = {2018},
  month = jul,
  abstract = {Generative Adversarial Networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant amount of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of "tricks". The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, and neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We reproduce the current state of the art and go beyond fairly exploring the GAN landscape. We discuss common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.},
  keywords = {GAN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9TXDKHNC\\Kurach et al. - 2018 - The GAN Landscape Losses, Architectures, Regularization, and Normalization.pdf}
}

@inproceedings{kurleContinualLearningBayesian2019,
  title = {Continual {{Learning}} with {{Bayesian Neural Networks}} for {{Non-Stationary Data}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kurle, Richard and Cseke, Botond and Klushyn, Alexej and van der Smagt, Patrick and G{\"u}nnemann, Stephan},
  year = {2019},
  month = sep,
  abstract = {This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes.},
  langid = {english},
  keywords = {bayesian,continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\E9KDTJIB\\forum.html}
}

@article{kusupatiFastGRNNFastAccurate2019,
  title = {{{FastGRNN}}: {{A Fast}}, {{Accurate}}, {{Stable}} and {{Tiny Kilobyte Sized Gated Recurrent Neural Network}}},
  shorttitle = {{{FastGRNN}}},
  author = {Kusupati, Aditya and Singh, Manish and Bhatia, Kush and Kumar, Ashish and Jain, Prateek and Varma, Manik},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.02358 [cs, stat]},
  eprint = {1901.02358},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix's singular values but have also increased the model size as they require a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by adding a residual connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the residual connection to a gate by reusing the RNN matrices to match state-of-the-art gated RNN accuracies but with a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank, sparse and quantized resulted in accurate models that could be up to 35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to accurately recognize the "Hey Cortana" wakeword with a 1 KB model and to be deployed on severely resource-constrained IoT microcontrollers too tiny to store other RNN models. FastGRNN's code is available at https://github.com/Microsoft/EdgeML/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\F39CHT89\\Kusupati et al_2019_FastGRNN.pdf;C\:\\Users\\w-32\\Zotero\\storage\\SIC2HT7I\\1901.html}
}

@article{kwonExploringSystemPerformance2021,
  title = {Exploring {{System Performance}} of {{Continual Learning}} for {{Mobile}} and {{Embedded Sensing Applications}}},
  author = {Kwon, Young D. and Chauhan, Jagmohan and Kumar, Abhishek and Hui, Pan and Mascolo, Cecilia},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.13290 [cs]},
  eprint = {2110.13290},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Continual learning approaches help deep neural network models adapt and learn incrementally by trying to solve catastrophic forgetting. However, whether these existing approaches, applied traditionally to image-based tasks, work with the same efficacy to the sequential time series data generated by mobile or embedded sensing systems remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the performance of three predominant continual learning schemes (i.e., regularization, replay, and replay with examples) on six datasets from three mobile and embedded sensing applications in a range of scenarios having different learning complexities. More specifically, we implement an end-to-end continual learning framework on edge devices. Then we investigate the generalizability, trade-offs between performance, storage, computational costs, and memory footprint of different continual learning methods. Our findings suggest that replay with exemplars-based schemes such as iCaRL has the best performance trade-offs, even in complex scenarios, at the expense of some storage space (few MBs) for training examples (1\% to 5\%). We also demonstrate for the first time that it is feasible and practical to run continual learning on-device with a limited memory budget. In particular, the latency on two types of mobile and embedded devices suggests that both incremental learning time (few seconds - 4 minutes) and training time (1 - 75 minutes) across datasets are acceptable, as training could happen on the device when the embedded device is charging thereby ensuring complete data privacy. Finally, we present some guidelines for practitioners who want to apply a continual learning paradigm for mobile sensing tasks.},
  archiveprefix = {arXiv},
  keywords = {continual,embedded,pailab},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Kwon et al_2021_Exploring System Performance of Continual Learning for Mobile and Embedded.pdf;C\:\\Users\\w-32\\Zotero\\storage\\A5VC7SAV\\2110.html}
}

@article{lakeGeneralizationSystematicityCompositional2017,
  title = {Generalization without Systematicity: {{On}} the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author = {Lake, Brenden M. and Baroni, Marco},
  year = {2017},
  month = oct,
  abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
  keywords = {nlp,seq2seq},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\J9U9S3LB\\Lake, Baroni - 2017 - Generalization without systematicity On the compositional skills of sequence-to-sequence recurrent networks(3).pdf}
}

@inproceedings{lamplePhraseBasedNeuralUnsupervised2018,
  title = {Phrase-{{Based Neural Unsupervised Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  year = {2018},
  month = apr,
  abstract = {Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.},
  keywords = {GAN,nlp,NMT,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KYWTMZBG\\Lample et al. - 2018 - Phrase-Based Neural Unsupervised Machine Translation.pdf}
}

@inproceedings{lampleUnsupervisedMachineTranslation2018,
  title = {Unsupervised {{Machine Translation Using Monolingual Corpora Only}}},
  booktitle = {{{ICLR}}},
  author = {Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  year = {2018},
  month = oct,
  abstract = {Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.},
  keywords = {GAN,ICLR,nlp,NMT,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RU44YEJH\\Lample et al. - 2018 - Unsupervised Machine Translation Using Monolingual Corpora Only(2).pdf}
}

@article{langkvistReviewUnsupervisedFeature2014,
  title = {A Review of Unsupervised Feature Learning and Deep Learning for Time-Series Modeling},
  author = {L{\"a}ngkvist, Martin and Karlsson, Lars and Loutfi, Amy},
  year = {2014},
  journal = {Pattern Recognition Letters},
  volume = {42},
  pages = {11--24},
  keywords = {time-series,unsupervised}
}

@article{lattnerPredictiveModelMusic2018,
  title = {A {{Predictive Model}} for {{Music Based}} on {{Learned Interval Representations}}},
  author = {Lattner, Stefan and Grachten, Maarten and Widmer, Gerhard},
  year = {2018},
  abstract = {Connectionist sequence models (e.g., RNNs) applied to musical sequences suffer from two known problems: First, they have strictly "absolute pitch perception". Therefore, they fail to generalize over musical concepts which are commonly perceived in terms of relative distances between pitches (e.g., melodies, scale types, modes, cadences, or chord types). Second, they fall short of capturing the concepts of repetition and musical form. In this paper we introduce the recurrent gated autoencoder (RGAE), a recurrent neural network which learns and operates on interval representations of musical sequences. The relative pitch modeling increases generalization and reduces sparsity in the input data. Furthermore, it can learn sequences of copy-and-shift operations (i.e. chromatically transposed copies of musical fragments)---a promising capability for learning musical repetition structure. We show that the RGAE improves the state of the art for general connectionist sequence models in learning to predict monophonic melodies, and that ensembles of relative and absolute music processing models improve the results appreciably. Furthermore, we show that the relative pitch processing of the RGAE naturally facilitates the learning and the generation of sequences of copy-and-shift operations, wherefore the RGAE greatly outperforms a common absolute pitch recurrent neural network on this task.},
  keywords = {music},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WS4EYLXA\\Lattner, Grachten, Widmer - 2018 - A Predictive Model for Music Based on Learned Interval Representations(2).pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--436}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  year = {1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324}
}

@article{lecunMNISTDatabaseHandwritten1998,
  title = {The {{MNIST}} Database of Handwritten Digits},
  author = {LeCun, Yann},
  year = {1998},
  journal = {http://yann. lecun. com/exdb/mnist/},
  keywords = {data}
}

@inproceedings{leDistributedRepresentationsSentences2014,
  ids = {leDistributedRepresentationsSentences2014a},
  title = {Distributed Representations of Sentences and Documents},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}} ({{ICML-14}})},
  author = {Le, Quoc and Mikolov, Tomas},
  year = {2014},
  pages = {1188--1196},
  keywords = {ICML,nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\M8T53QDJ\\distributed representation of sentences and documents.pdf;C\:\\Users\\w-32\\Zotero\\storage\\UXFK6B6V\\Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents(2).pdf}
}

@article{leeContinualLearningDeep2017,
  title = {Continual {{Learning}} with {{Deep Generative Replay}}},
  author = {Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon and {T-brain}, S K},
  year = {2017},
  number = {Nips},
  keywords = {continual,gen-exml,generative},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\D8WFZ2TN\\Lee et al. - 2017 - Continual Learning with Deep Generative Replay(2).pdf}
}

@article{leeContinualLearningTeacherStudent2021,
  title = {Continual {{Learning}} in the {{Teacher-Student Setup}}: {{Impact}} of {{Task Similarity}}},
  shorttitle = {Continual {{Learning}} in the {{Teacher-Student Setup}}},
  author = {Lee, Sebastian and Goldt, Sebastian and Saxe, Andrew},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.04384 [cond-mat, stat]},
  eprint = {2107.04384},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, stat},
  abstract = {Continual learning-the ability to learn many tasks in sequence-is critical for artificial learning systems. Yet standard training methods for deep networks often suffer from catastrophic forgetting, where learning new tasks erases knowledge of earlier tasks. While catastrophic forgetting labels the problem, the theoretical reasons for interference between tasks remain unclear. Here, we attempt to narrow this gap between theory and practice by studying continual learning in the teacher-student setup. We extend previous analytical work on two-layer networks in the teacher-student setup to multiple teachers. Using each teacher to represent a different task, we investigate how the relationship between teachers affects the amount of forgetting and transfer exhibited by the student when the task switches. In line with recent work, we find that when tasks depend on similar features, intermediate task similarity leads to greatest forgetting. However, feature similarity is only one way in which tasks may be related. The teacher-student approach allows us to disentangle task similarity at the level of readouts (hidden-to-output weights) and features (input-to-hidden weights). We find a complex interplay between both types of similarity, initial transfer/forgetting rates, maximum transfer/forgetting, and long-term transfer/forgetting. Together, these results help illuminate the diverse factors contributing to catastrophic forgetting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,continual,exmodel,notag,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8U9FN9V4\\Lee et al. - 2021 - Continual Learning in the Teacher-Student Setup I.pdf;C\:\\Users\\w-32\\Zotero\\storage\\GF64T3U5\\2107.html}
}

@article{leeNeuralDirichletProcess2020,
  title = {A {{Neural Dirichlet Process Mixture Model}} for {{Task-Free Continual Learning}}},
  author = {Lee, Soochan and Ha, Junsoo and Zhang, Dongsu and Kim, Gunhee},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.00689 [cs, stat]},
  eprint = {2001.00689},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.},
  archiveprefix = {arXiv},
  keywords = {bayesian,continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Lee et al_2020_A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MHUDM7JZ\\2001.html}
}

@article{leeOvercomingCatastrophicForgetting2019a,
  ids = {leeOvercomingCatastrophicForgetting2019},
  title = {Overcoming {{Catastrophic Forgetting With Unlabeled Data}} in the {{Wild}}},
  author = {Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
  year = {2019},
  eprint = {1903.12648},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {312--321},
  doi = {10.48550/arXiv.1903.12648},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YZNEGWUI\\Lee et al_2019_Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZGJA3NK2\\1903.html}
}

@misc{leeSimpleUnifiedFramework2018,
  title = {A {{Simple Unified Framework}} for {{Detecting Out-of-Distribution Samples}} and {{Adversarial Attacks}}},
  author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  year = {2018},
  month = oct,
  number = {arXiv:1807.03888},
  eprint = {1807.03888},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.03888},
  abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
  archiveprefix = {arXiv},
  keywords = {ood-detection},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TPV689BI\\1807.html}
}

@misc{leeTrainingConfidencecalibratedClassifiers2018,
  title = {Training {{Confidence-calibrated Classifiers}} for {{Detecting Out-of-Distribution Samples}}},
  author = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  year = {2018},
  month = feb,
  number = {arXiv:1711.09325},
  eprint = {1711.09325},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.09325},
  abstract = {The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.},
  archiveprefix = {arXiv},
  keywords = {ood-detection},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VBUIMTSD\\1711.html}
}

@article{leSimpleWayInitialize2015,
  ids = {leSimpleWayInitialize2015a},
  title = {A Simple Way to Initialize Recurrent Networks of Rectified Linear Units},
  author = {Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  year = {2015},
  journal = {arXiv preprint arXiv:1504.00941},
  eprint = {1504.00941},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8RGXYDJ5\\Le, Jaitly, Hinton - 2015 - A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.pdf}
}

@article{lesortContinualLearningRobotics2020,
  ids = {lesortContinualLearningRobotics2019},
  title = {Continual Learning for Robotics: {{Definition}}, Framework, Learning Strategies, Opportunities and Challenges},
  shorttitle = {Continual Learning for Robotics},
  author = {Lesort, Timoth{\'e}e and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and {D{\'i}az-Rodr{\'i}guez}, Natalia},
  year = {2020},
  month = jun,
  journal = {Information Fusion},
  volume = {58},
  eprint = {1907.00182},
  eprinttype = {arxiv},
  pages = {52--68},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.12.004},
  abstract = {Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Catastrophic Forgetting,Computer Science - Machine Learning,Computer Science - Robotics,continual,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YYUMQ794\\Lesort et al_2019_Continual Learning for Robotics.pdf;C\:\\Users\\w-32\\Zotero\\storage\\663B2LMD\\1907.html;C\:\\Users\\w-32\\Zotero\\storage\\AGWK88XT\\S1566253519307377.html}
}

@article{lesortGenerativeModelsPerspective2018,
  title = {Generative {{Models}} from the Perspective of {{Continual Learning}}},
  author = {Lesort, Timoth{\'e}e and {Caselles-Dupr{\'e}}, Hugo and {Garcia-Ortiz}, Michael and Stoian, Andrei and Filliat, David},
  year = {2018},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  abstract = {Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge. Our code is available online \textbackslash footnote\{\textbackslash url\{https://github.com/TLESORT/Generative\textbackslash\_Continual\textbackslash\_Learning\}\}.},
  keywords = {continual,gen-exml,generative},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YXRX7N9W\\Lesort et al. - 2018 - Generative Models from the perspective of Continual Learning(2).pdf}
}

@article{lesortRegularizationShortcomingsContinual2020,
  title = {Regularization {{Shortcomings}} for {{Continual Learning}}},
  author = {Lesort, Timoth{\'e}e and Stoian, Andrei and Filliat, David},
  year = {2020},
  month = feb,
  journal = {arXiv:1912.03049 [cs, stat]},
  eprint = {1912.03049},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In most machine learning algorithms, training data are assumed independent and identically distributed (iid). Otherwise, the algorithms' performances are challenged. A famous phenomenon with non-iid data distribution is known as ``catastrophic forgetting''. Algorithms dealing with it are gathered in the Continual Learning research field. In this article, we study the regularization based approaches to continual learning. We show that those approaches can not learn to discriminate classes from different tasks in an elemental continual benchmark: class-incremental setting. We make theoretical reasoning to prove this shortcoming and illustrate it with examples and experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {continual,regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\M6YK3EMN\\Lesort et al. - 2020 - Regularization Shortcomings for Continual Learning.pdf}
}

@misc{lesortScalingNumberTasks2022,
  title = {Scaling the {{Number}} of {{Tasks}} in {{Continual Learning}}},
  author = {Lesort, Timoth{\'e}e and Ostapenko, Oleksiy and Misra, Diganta and Arefin, Md Rifat and Rodr{\'i}guez, Pau and Charlin, Laurent and Rish, Irina},
  year = {2022},
  month = jul,
  number = {arXiv:2207.04543},
  eprint = {2207.04543},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.04543},
  abstract = {Standard gradient descent algorithms applied to sequences of tasks are known to produce catastrophic forgetting in deep neural networks. When trained on a new task in a sequence, the model updates its parameters on the current task, forgetting past knowledge. This article explores scenarios where we scale the number of tasks in a finite environment. Those scenarios are composed of a long sequence of tasks with reoccurring data. We show that in such setting, stochastic gradient descent can learn, progress, and converge to a solution that according to existing literature needs a continual learning algorithm. In other words, we show that the model performs knowledge retention and accumulation without specific memorization mechanisms. We propose a new experimentation framework, SCoLe (Scaling Continual Learning), to study the knowledge retention and accumulation of algorithms in potentially infinite sequences of tasks. To explore this setting, we performed a large number of experiments on sequences of 1,000 tasks to better understand this new family of settings. We also propose a slight modifications to the vanilla stochastic gradient descent to facilitate continual learning in this setting. The SCoLe framework represents a good simulation of practical training environments with reoccurring situations and allows the study of convergence behavior in long sequences. Our experiments show that previous results on short scenarios cannot always be extrapolated to longer scenarios.},
  archiveprefix = {arXiv},
  keywords = {cir,continual,large-scale},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CSLLUSJR\\2207.html}
}

@article{leSupervisedAutoencodersImproving2018,
  title = {Supervised Autoencoders : {{Improving}} Generalization Performance with Unsupervised Regularizers},
  author = {Le, Lei and Patterson, Andrew and White, Martha},
  year = {2018},
  journal = {Neural Information Processing Systems (NIPS)},
  number = {Nips},
  pages = {25--27},
  abstract = {Generalization performance is a central goal in machine learning, with explicit generalization strategies needed when training over-parametrized models, like large neural networks. There is growing interest in using multiple, potentially auxiliary tasks, as one strategy towards this goal. In this work, we theoretically and empirically analyze one such model, called a supervised auto-encoder: a neural network that jointly predicts targets and inputs (reconstruction). We provide a novel generalization result for linear auto-encoders, proving uniform stability based on the inclusion of the reconstruction error-particularly as an improvement on simplistic regularization such as norms. We then demonstrate empirically that, across an array of architectures with a different number of hidden units and activation functions, the supervised auto-encoder compared to the corresponding standard neural network never harms performance and can improve generalization.},
  keywords = {autoencoders},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\F5KC9NZD\\Le, Patterson, White - 2018 - Supervised autoencoders Improving generalization performance with unsupervised regularizers(2).pdf}
}

@article{levyImprovingDistributionalSimilarity2015,
  ids = {levyImprovingDistributionalSimilarity2015a},
  title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
  author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  year = {2015},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {211--225},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5WL9JZG9\\improving-distributional-similarity-tacl-2015.pdf}
}

@inproceedings{levyLongShortTermMemory2018,
  title = {Long {{Short-Term Memory}} as a {{Dynamically Computed Element-wise Weighted Sum}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Levy, Omer and Lee, Kenton and FitzGerald, Nicholas and Zettlemoyer, Luke},
  year = {2018},
  pages = {732--739},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2116},
  abstract = {LSTMs were introduced to combat vanishing gradients in simple RNNs by augmenting them with gated additive recurrent connections. We present an alternative view to explain the success of LSTMs: the gates themselves are versatile recurrent models that provide more representational power than previously appreciated. We do this by decoupling the LSTM's gates from the embedded simple RNN, producing a new class of RNNs where the recurrence computes an element-wise weighted sum of context-independent functions of the input. Ablations on a range of problems demonstrate that the gating mechanism alone performs as well as an LSTM in most settings, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.},
  langid = {english},
  keywords = {attention,LSTM,nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HUBNYE5Y\\Levy et al. - 2018 - Long Short-Term Memory as a Dynamically Computed E.pdf}
}

@inproceedings{levyNeuralWordEmbedding2014,
  title = {Neural {{Word Embedding}} as {{Implicit Matrix Factorization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Levy, Omer and Goldberg, Yoav},
  year = {2014},
  pages = {2177--2185},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4M868864\\Levy, Goldberg - 2014 - Neural Word Embedding as Implicit Matrix Factorization(3).pdf}
}

@inproceedings{lezcano-casadoCheapOrthogonalConstraints2019,
  title = {Cheap {{Orthogonal Constraints}} in {{Neural Networks}}: {{A Simple Parametrization}} of the {{Orthogonal}} and {{Unitary Group}}},
  booktitle = {{{ICML}}},
  author = {{Lezcano-Casado}, Mario and {Mart{\'i}nez-Rubio}, David},
  year = {2019},
  abstract = {We introduce a novel approach to perform first-order optimization with orthogonal and unitary constraints. This approach is based on a parametrization stemming from Lie group theory through the exponential map. The parametrization transforms the constrained optimization problem into an unconstrained one over a Euclidean space, for which common first-order optimization methods can be used. The theoretical results presented are general enough to cover the special orthogonal group, the unitary group and, in general, any connected compact Lie group. We discuss how this and other parametrizations can be computed efficiently through an implementation trick, making numerically complex parametrizations usable at a negligible runtime cost in neural networks. In particular, we apply our results to RNNs with orthogonal recurrent weights, yielding a new architecture called expRNN. We demonstrate how our method constitutes a more robust approach to optimization with orthogonal constraints, showing faster, accurate, and more stable convergence in several tasks designed to test RNNs.},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4UIDE3FC\\Lezcano-Casado, Martnez-Rubio - 2019 - Cheap Orthogonal Constraints in Neural Networks A Simple Parametrization of the Orthogonal an(2).pdf}
}

@article{liAdversarialCameraStickers2019,
  title = {Adversarial Camera Stickers: {{A}} Physical Camera-Based Attack on Deep Learning Systems},
  shorttitle = {Adversarial Camera Stickers},
  author = {Li, Juncheng and Schmidt, Frank R. and Kolter, J. Zico},
  year = {2019},
  month = jun,
  journal = {arXiv:1904.00759 [cs, stat]},
  eprint = {1904.00759},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent work has documented the susceptibility of deep learning systems to adversarial examples, but most such attacks directly manipulate the digital input to a classifier. Although a smaller line of work considers physical adversarial attacks, in all cases these involve manipulating the object of interest, e.g., putting a physical sticker on an object to misclassify it, or manufacturing an object specifically intended to be misclassified. In this work, we consider an alternative question: is it possible to fool deep classifiers, over all perceived objects of a certain type, by physically manipulating the camera itself? We show that by placing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal perturbations of the observed images that are inconspicuous, yet misclassify target objects as a different (targeted) class. To accomplish this, we propose an iterative procedure for both updating the attack perturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is physically realizable). For example, we show that we can achieve physically-realizable attacks that fool ImageNet classifiers in a targeted fashion 49.6\% of the time. This presents a new class of physically-realizable threat models to consider in the context of adversarially robust machine learning. Our demo video can be viewed at: https://youtu.be/wUVmL33Fx54},
  archiveprefix = {arXiv},
  keywords = {adversarial-examples,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,dependability,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Li et al_2019_Adversarial camera stickers.pdf;C\:\\Users\\w-32\\Zotero\\storage\\YA7B97RN\\1904.html}
}

@misc{liangEnhancingReliabilityOutofdistribution2020,
  title = {Enhancing {{The Reliability}} of {{Out-of-distribution Image Detection}} in {{Neural Networks}}},
  author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
  year = {2020},
  month = aug,
  number = {arXiv:1706.02690},
  eprint = {1706.02690},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1706.02690},
  abstract = {We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7\% to 4.3\% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95\%.},
  archiveprefix = {arXiv},
  keywords = {exmodel,ood-detection},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DVKBLAAD\\Liang et al_2020_Enhancing The Reliability of Out-of-distribution Image Detection in Neural.pdf;C\:\\Users\\w-32\\Zotero\\storage\\LB2LDYK7\\1706.html}
}

@misc{liBranchTrainMergeEmbarrassinglyParallel2022,
  title = {Branch-{{Train-Merge}}: {{Embarrassingly Parallel Training}} of {{Expert Language Models}}},
  shorttitle = {Branch-{{Train-Merge}}},
  author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03306},
  eprint = {2208.03306},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.03306},
  abstract = {We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.},
  archiveprefix = {arXiv},
  keywords = {exmodel,language-model,large-scale,model-patching,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AUBGJQPR\\Li et al_2022_Branch-Train-Merge.pdf;C\:\\Users\\w-32\\Zotero\\storage\\IDZSUI8B\\2208.html}
}

@article{liContinualLearningUsing2020,
  title = {Continual {{Learning Using Bayesian Neural Networks}}},
  author = {Li, HongLin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
  year = {2020},
  month = aug,
  journal = {arXiv:1910.04112 [cs, stat]},
  eprint = {1910.04112},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Continual learning models allow them to learn and adapt to new changes and tasks over time. However, in continual and sequential learning scenarios in which the models are trained using different data with various distributions, neural networks tend to forget the previously learned knowledge. This phenomenon is often referred to as catastrophic forgetting. The catastrophic forgetting is an inevitable problem in continual learning models for dynamic environments. To address this issue, we propose a method, called Continual Bayesian Learning Networks (CBLN), which enables the networks to allocate additional resources to adapt to new tasks without forgetting the previously learned tasks. Using a Bayesian Neural Network, CBLN maintains a mixture of Gaussian posterior distributions that are associated with different tasks. The proposed method tries to optimise the number of resources that are needed to learn each task and avoids an exponential increase in the number of resources that are involved in learning multiple tasks. The proposed method does not need to access the past training data and can choose suitable weights to classify the data points during the test time automatically based on an uncertainty criterion. We have evaluated our method on the MNIST and UCR timeseries datasets. The evaluation results show that our method can address the catastrophic forgetting problem at a promising rate compared to the state-of-the-art models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3HBBBUV4\\Li et al. - 2020 - Continual Learning Using Bayesian Neural Networks.pdf}
}

@misc{liContinualLearningUsing2020a,
  title = {Continual {{Learning Using Bayesian Neural Networks}}},
  author = {Li, HongLin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
  year = {2020},
  month = aug,
  number = {arXiv:1910.04112},
  eprint = {1910.04112},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1910.04112},
  abstract = {Continual learning models allow to learn and adapt to new changes and tasks over time. However, in continual and sequential learning scenarios in which the models are trained using different data with various distributions, neural networks tend to forget the previously learned knowledge. This phenomenon is often referred to as catastrophic forgetting. The catastrophic forgetting is an inevitable problem in continual learning models for dynamic environments. To address this issue, we propose a method, called Continual Bayesian Learning Networks (CBLN), which enables the networks to allocate additional resources to adapt to new tasks without forgetting the previously learned tasks. Using a Bayesian Neural Network, CBLN maintains a mixture of Gaussian posterior distributions that are associated with different tasks. The proposed method tries to optimise the number of resources that are needed to learn each task and avoids an exponential increase in the number of resources that are involved in learning multiple tasks. The proposed method does not need to access the past training data and can choose suitable weights to classify the data points during the test time automatically based on an uncertainty criterion. We have evaluated our method on the MNIST and UCR time-series datasets. The evaluation results show that our method can address the catastrophic forgetting problem at a promising rate compared to the state-of-the-art models.},
  archiveprefix = {arXiv},
  keywords = {bayesian,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,continual,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QVWTLK8S\\1910.html}
}

@article{liContinualLearningUsing2021,
  title = {Continual {{Learning Using Bayesian Neural Networks}}},
  author = {Li, Honglin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
  year = {2021},
  month = sep,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {9},
  pages = {4243--4252},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3017292},
  abstract = {Continual learning models allow them to learn and adapt to new changes and tasks over time. However, in continual and sequential learning scenarios, in which the models are trained using different data with various distributions, neural networks (NNs) tend to forget the previously learned knowledge. This phenomenon is often referred to as catastrophic forgetting. The catastrophic forgetting is an inevitable problem in continual learning models for dynamic environments. To address this issue, we propose a method, called continual Bayesian learning networks (CBLNs), which enables the networks to allocate additional resources to adapt to new tasks without forgetting the previously learned tasks. Using a Bayesian NN, CBLN maintains a mixture of Gaussian posterior distributions that are associated with different tasks. The proposed method tries to optimize the number of resources that are needed to learn each task and avoids an exponential increase in the number of resources that are involved in learning multiple tasks. The proposed method does not need to access the past training data and can choose suitable weights to classify the data points during the test time automatically based on an uncertainty criterion. We have evaluated the method on the MNIST and UCR time-series data sets. The evaluation results show that the method can address the catastrophic forgetting problem at a promising rate compared to the state-of-the-art models.},
  keywords = {bayesian,continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\R9I69YXS\\9181489.html}
}

@article{liConvergenceAnalysisTwolayer2017,
  title = {Convergence {{Analysis}} of {{Two-layer Neural Networks}} with {{ReLU Activation}}},
  author = {Li, Yuanzhi and Yuan, Yang},
  year = {2017},
  journal = {CoRR},
  volume = {abs/1705.0},
  keywords = {learning}
}

@article{liDemystifyingNeuralStyle2017,
  title = {Demystifying Neural Style Transfer},
  author = {Li, Yanghao and Wang, Naiyan and Liu, Jiaying and Hou, Xiaodi},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.01036},
  eprint = {1701.01036},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {style-transfer}
}

@article{liDisentangledSequentialAutoencoder2018,
  ids = {yingzhenDisentangledSequentialAutoencoder2018},
  title = {Disentangled {{Sequential Autoencoder}}},
  author = {Li, Yingzhen and Mandt, Stephan},
  year = {2018},
  issn = {9781510867963},
  abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
  keywords = {disentangle,music,vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\T39LHANW\\Yingzhen, Mandt - 2018 - Disentangled Sequential Autoencoder(2).pdf;C\:\\Users\\w-32\\Zotero\\storage\\WNIEEYGM\\Li, Mandt - 2018 - Disentangled Sequential Autoencoder(2).pdf}
}

@article{liEnergyBasedModelsContinual2021,
  title = {Energy-{{Based Models}} for {{Continual Learning}}},
  author = {Li, Shuang and Du, Yilun and {van de Ven}, Gido M. and Mordatch, Igor},
  year = {2021},
  month = feb,
  journal = {arXiv:2011.12216 [cs, stat]},
  eprint = {2011.12216},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We motivate Energy-Based Models (EBMs) as a promising model class for continual learning problems. Instead of tackling continual learning via the use of external memory, growing models, or regularization, EBMs have a natural way to support a dynamically-growing number of tasks or classes that causes less interference with previously learned information. Our proposed version of EBMs for continual learning is simple, efficient and outperforms baseline methods by a large margin on several benchmarks. Moreover, our proposed contrastive divergence based training objective can be applied to other continual learning methods, resulting in substantial boosts in their performance. We also show that EBMs are adaptable to a more general continual learning setting where the data distribution changes without the notion of explicitly delineated tasks. These observations point towards EBMs as a class of models naturally inclined towards the continual learning regime.},
  archiveprefix = {arXiv},
  keywords = {bayesian,continual,exmodel,notag},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Li et al_2021_Energy-Based Models for Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\AQBZMG9Y\\2011.html}
}

@article{liFederatedLearningChallenges2020,
  title = {Federated {{Learning}}: {{Challenges}}, {{Methods}}, and {{Future Directions}}},
  shorttitle = {Federated {{Learning}}},
  author = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  year = {2020},
  month = may,
  journal = {IEEE Signal Processing Magazine},
  volume = {37},
  number = {3},
  pages = {50--60},
  issn = {1558-0792},
  doi = {10.1109/MSP.2020.2975749},
  abstract = {Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.},
  keywords = {Data models,Data privacy,Distributed databases,federated,Machine learning,Predictive models,Privacy,Training data},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QXYTTMX7\\9084352.html}
}

@article{liIndependentlyRecurrentNeural2018,
  title = {Independently {{Recurrent Neural Network}} ({{IndRNN}}): {{Building A Longer}} and {{Deeper RNN}}},
  author = {Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},
  year = {2018},
  month = mar,
  abstract = {Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM. The code is available at https://github.com/Sunnydreamrain/IndRNN\_Theano\_Lasagne.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3TSXA6LA\\Li et al. - 2018 - Independently Recurrent Neural Network (IndRNN) Building A Longer and Deeper RNN(3).pdf}
}

@article{liLearnGrowContinual2019,
  title = {Learn to {{Grow}}: {{A Continual Structure Learning Framework}} for {{Overcoming Catastrophic Forgetting}}},
  author = {Li, Xilai and Zhou, Yingbo and Wu, Tianfu and Socher, Richard and Xiong, Caiming},
  year = {2019},
  abstract = {Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting.},
  keywords = {architectural,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YC86FJ98\\Li et al. - 2019 - Learn to Grow A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting(2).pdf}
}

@article{liLearningForgetting2018,
  ids = {liLearningForgetting2017},
  title = {Learning without {{Forgetting}}},
  author = {Li, Zhizhong and Hoiem, Derek},
  year = {2018},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {12},
  eprint = {1606.09282},
  eprinttype = {arxiv},
  pages = {2935--2947},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2017.2773081},
  abstract = {When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
  archiveprefix = {arXiv},
  keywords = {continual,Convolutional neural networks,deep learning,Deep learning,distillation,Feature extraction,Knowledge engineering,Learning systems,lwf,multi-task learning,Neural networks,regularization,Training data,transfer learning,Visual perception,visual recognition},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MHMX44EW\\Li and Hoiem - 2017 - Learning without Forgetting.pdf;C\:\\Users\\w-32\\Zotero\\storage\\JNRUJX4U\\1606.html;C\:\\Users\\w-32\\Zotero\\storage\\PEE374H8\\8107520.html}
}

@article{lillicrapRandomSynapticFeedback2016,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  year = {2016},
  month = dec,
  journal = {Nature Communications},
  volume = {7},
  number = {1},
  pages = {13276},
  issn = {2041-1723},
  doi = {10.1038/ncomms13276},
  langid = {english},
  keywords = {Backpropagation,biologically-plausible,credit-assignment,local-error},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\84IRFKHG\\Lillicrap et al. - 2016 - Random synaptic feedback weights support error bac.pdf}
}

@article{liMixMixAllYou2022,
  title = {{{MixMix}}: {{All You Need}} for {{Data-Free Compression Are Feature}} and {{Data Mixing}}},
  shorttitle = {{{MixMix}}},
  author = {Li, Yuhang and Zhu, Feng and Gong, Ruihao and Shen, Mingzhu and Dong, Xin and Yu, Fengwei and Lu, Shaoqing and Gu, Shi},
  year = {2022},
  month = jan,
  journal = {arXiv:2011.09899 [cs]},
  eprint = {2011.09899},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {User data confidentiality protection is becoming a rising challenge in the present deep learning research. Without access to data, conventional data-driven model compression faces a higher risk of performance degradation. Recently, some works propose to generate images from a specific pretrained model to serve as training data. However, the inversion process only utilizes biased feature statistics stored in one model and is from low-dimension to high-dimension. As a consequence, it inevitably encounters the difficulties of generalizability and inexact inversion, which leads to unsatisfactory performance. To address these problems, we propose MixMix based on two simple yet effective techniques: (1) Feature Mixing: utilizes various models to construct a universal feature space for generalized inversion; (2) Data Mixing: mixes the synthesized images and labels to generate exact label information. We prove the effectiveness of MixMix from both theoretical and empirical perspectives. Extensive experiments show that MixMix outperforms existing methods on the mainstream compression tasks, including quantization, knowledge distillation, and pruning. Specifically, MixMix achieves up to 4\% and 20\% accuracy uplift on quantization and pruning, respectively, compared to existing data-free compression work.},
  archiveprefix = {arXiv},
  keywords = {data-free,exmodel,knowledge distillation},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Li et al_2022_MixMix.pdf;C\:\\Users\\w-32\\Zotero\\storage\\886ZITGX\\2011.html}
}

@article{LIN20097313,
  title = {Short-Term Stock Price Prediction Based on Echo State Networks},
  author = {Lin, Xiaowei and Yang, Zehong and Song, Yixu},
  year = {2009},
  journal = {Expert Systems with Applications},
  volume = {36},
  number = {3, Part 2},
  pages = {7313--7317},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2008.09.049},
  abstract = {Neural network has been popular in time series prediction in financial areas because of their advantages in handling nonlinear systems. This paper presents a study of using a novel recurrent neural network\textendash echo state network (ESN) to predict the next closing price in stock markets. The Hurst exponent is applied to adaptively determine initial transient and choose sub-series with greatest predictability during training. The experiment results on nearly all stocks of S\&P 500 demonstrate that ESN outperforms other conventional neural networks in most cases. Experiments also indicate that if we include principle component analysis (PCA) to filter noise in data pretreatment and choose appropriate parameters, we can effectively prevent coarse prediction performance. But in most cases PCA improves the prediction accuracy only a little.},
  keywords = {Echo state network,Neural networks,Principle component analysis,Short-term price prediction}
}

@misc{linEnsembleDistillationRobust2021,
  title = {Ensemble {{Distillation}} for {{Robust Model Fusion}} in {{Federated Learning}}},
  author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
  year = {2021},
  month = mar,
  number = {arXiv:2006.07242},
  eprint = {2006.07242},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.07242},
  abstract = {Federated Learning (FL) is a machine learning setting where many devices collaboratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios. In this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,federated,knowledge-distillation,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XY7QCIQY\\2006.html}
}

@article{lingCharacterbasedNeuralMachine2015,
  title = {Character-Based Neural Machine Translation},
  author = {Ling, Wang and Trancoso, Isabel and Dyer, Chris and Black, Alan W},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.04586},
  eprint = {1511.04586},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {nlp,NMT}
}

@article{linStructuredSelfattentiveSentence2017,
  title = {A {{Structured Self-attentive Sentence Embedding}}},
  author = {Lin, Zhouhan and Feng, Minwei and dos Santos, Cicero Nogueira and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  year = {2017},
  month = mar,
  abstract = {This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.},
  keywords = {attention,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BJ9GTBK9\\Lin et al. - 2017 - A Structured Self-attentive Sentence Embedding(3).pdf}
}

@article{liRankingNeuralCheckpoints2021,
  title = {Ranking {{Neural Checkpoints}}},
  author = {Li, Yandong and Jia, Xuhui and Sang, Ruoxin and Zhu, Yukun and Green, Bradley and Wang, Liqiang and Gong, Boqing},
  year = {2021},
  month = mar,
  journal = {arXiv:2011.11200 [cs]},
  eprint = {2011.11200},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper is concerned with ranking many pre-trained deep neural networks (DNNs), called checkpoints, for the transfer learning to a downstream task. Thanks to the broad use of DNNs, we may easily collect hundreds of checkpoints from various sources. Which of them transfers the best to our downstream task of interest? Striving to answer this question thoroughly, we establish a neural checkpoint ranking benchmark (NeuCRaB) and study some intuitive ranking measures. These measures are generic, applying to the checkpoints of different output types without knowing how the checkpoints are pre-trained on which dataset. They also incur low computation cost, making them practically meaningful. Our results suggest that the linear separability of the features extracted by the checkpoints is a strong indicator of transferability. We also arrive at a new ranking measure, NLEEP, which gives rise to the best performance in the experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,exmodel,notag},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Li et al_2021_Ranking Neural Checkpoints.pdf;C\:\\Users\\w-32\\Zotero\\storage\\S8333JEB\\2011.html}
}

@article{liSlidingHierarchicalRecurrent,
  title = {Sliding {{Hierarchical Recurrent Neural Networks}} for {{Sequence Classification}}},
  author = {Li, Bo and Sheng, Zhonghao and Ye, Wei and Zhang, Jinglei and Liu, Kai and Zhang, Shikun},
  pages = {8},
  abstract = {Hierarchical Recurrent Neural Networks (HRNN) is an important advance in improving efficiency and performance of sequence classification in recent years. The intuition behind this approach is to slice long sequences into many short sub-sequences and process them in parallel, then capturing the long-term dependencies between those sub-sequences by deeper layers of the networks. In this paper, we propose a novel architecture called Sliding Hierarchical Recurrent Neural Network (SHRNN). We introduce a new sliding mechanism on the input sequence of each layer, named recursive block, so that SHRNN can process the input sequence effectively. We also introduce layer-wise attention and multi-layer regularization for further improvements. We perform large-scale experiments in sequence classification task of both text and image on 8 datasets. As result, we not only achieve new start-of-the-art performance on all datasets by SHRNN, but also investigate effects of different components of SHRNN systematically and thoroughly, which provides best practice for the usage of SHRNN.},
  langid = {english},
  keywords = {RNN,sequence-classification,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BMH6IDBJ\\Li et al. - Sliding Hierarchical Recurrent Neural Networks for.pdf}
}

@article{liSupportNetSolvingCatastrophic2018,
  title = {{{SupportNet}}: Solving Catastrophic Forgetting in Class Incremental Learning with Support Data},
  shorttitle = {{{SupportNet}}},
  author = {Li, Yu and Li, Zhongxiao and Ding, Lizhong and Pan, Yijie and Huang, Chao and Hu, Yuhui and Chen, Wei and Gao, Xin},
  year = {2018},
  month = sep,
  abstract = {A plain well-trained deep learning model often does not have the ability to learn new knowledge without forgetting the previously learned knowledge, which is known as catastrophic forgetting. Here...},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H5B7ABYE\\document.pdf;C\:\\Users\\w-32\\Zotero\\storage\\J4ZK3KP8\\forum.html}
}

@article{liTechnicalReportICCV2022,
  title = {Technical {{Report}} for {{ICCV}} 2021 {{Challenge SSLAD-Track3B}}: {{Transformers Are Better Continual Learners}}},
  shorttitle = {Technical {{Report}} for {{ICCV}} 2021 {{Challenge SSLAD-Track3B}}},
  author = {Li, Duo and Cao, Guimei and Xu, Yunlu and Cheng, Zhanzhan and Niu, Yi},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.04924 [cs]},
  eprint = {2201.04924},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In the SSLAD-Track 3B challenge on continual learning, we propose the method of COntinual Learning with Transformer (COLT). We find that transformers suffer less from catastrophic forgetting compared to convolutional neural network. The major principle of our method is to equip the transformer based feature extractor with old knowledge distillation and head expanding strategies to compete catastrophic forgetting. In this report, we first introduce the overall framework of continual learning for object detection. Then, we analyse the key elements' effect on withstanding catastrophic forgetting in our solution. Our method achieves 70.78 mAP on the SSLAD-Track 3B challenge test set.},
  archiveprefix = {arXiv},
  keywords = {continual,transformer},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Li et al_2022_Technical Report for ICCV 2021 Challenge SSLAD-Track3B.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EDG3A2HC\\2201.html}
}

@article{liTellMeWhere2018,
  title = {Tell {{Me Where}} to {{Look}}: {{Guided Attention Inference Network}}},
  author = {Li, Kunpeng and Wu, Ziyan and Peng, Kuan-Chuan and Ernst, Jan and Fu, Yun},
  year = {2018},
  month = feb,
  abstract = {Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) first time make attention maps an explicit and natural component of the end-to-end training, (2) provide self-guidance directly on these maps by exploring supervision form the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on Pascal VOC 2012 val. and test set. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance.},
  keywords = {attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TQ4UVQZ5\\Li et al. - 2018 - Tell Me Where to Look Guided Attention Inference Network.pdf}
}

@article{liuDataFreeKnowledgeTransfer2021,
  title = {Data-{{Free Knowledge Transfer}}: {{A Survey}}},
  shorttitle = {Data-{{Free Knowledge Transfer}}},
  author = {Liu, Yuang and Zhang, Wei and Wang, Jun and Wang, Jianyong},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.15278 [cs]},
  eprint = {2112.15278},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In the last decade, many deep learning models have been well trained and made a great success in various fields of machine intelligence, especially for computer vision and natural language processing. To better leverage the potential of these well-trained models in intra-domain or cross-domain transfer learning situations, knowledge distillation (KD) and domain adaptation (DA) are proposed and become research highlights. They both aim to transfer useful information from a well-trained model with original training data. However, the original data is not always available in many cases due to privacy, copyright or confidentiality. Recently, the data-free knowledge transfer paradigm has attracted appealing attention as it deals with distilling valuable knowledge from well-trained models without requiring to access to the training data. In particular, it mainly consists of the data-free knowledge distillation (DFKD) and source data-free domain adaptation (SFDA). On the one hand, DFKD aims to transfer the intra-domain knowledge of original data from a cumbersome teacher network to a compact student network for model compression and efficient inference. On the other hand, the goal of SFDA is to reuse the cross-domain knowledge stored in a well-trained source model and adapt it to a target domain. In this paper, we provide a comprehensive survey on data-free knowledge transfer from the perspectives of knowledge distillation and unsupervised domain adaptation, to help readers have a better understanding of the current research status and ideas. Applications and challenges of the two areas are briefly reviewed, respectively. Furthermore, we provide some insights to the subject of future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,data-free,exmodel,knowledge distillation},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Liu et al_2021_Data-Free Knowledge Transfer.pdf;C\:\\Users\\w-32\\Zotero\\storage\\M6HLL2R6\\2112.html}
}

@article{liuDeepRecurrentNeural2017,
  title = {Deep {{Recurrent Neural Network}} for {{Protein Function Prediction}} from {{Sequence}}},
  author = {Liu, Xueliang},
  year = {2017},
  doi = {10.1101/103994},
  abstract = {As high-throughput biological sequencing becomes faster and cheaper, the need to extract useful information from sequencing becomes ever more paramount, often limited by low-throughput experimental characterizations. For proteins, accurate prediction of their functions directly from their primary amino-acid sequences has been a long standing challenge. Here, machine learning using artificial recurrent neural networks (RNN) was applied towards classification of protein function directly from primary sequence without sequence alignment, heuristic scoring or feature engineering. The RNN models containing long-short-term-memory (LSTM) units trained on public, annotated datasets from UniProt achieved high performance for in-class prediction of four important protein functions tested, particularly compared to other machine learning algorithms using sequence-derived protein features. RNN models were used also for out-of-class predictions of phylogenetically distinct protein families with similar functions, including proteins of the CRISPR-associated nuclease, ferritin-like iron storage and cytochrome P450 families. Applying the trained RNN models on the partially unannotated UniRef100 database predicted not only candidates validated by existing annotations but also currently unannotated sequences. Some RNN predictions for the ferritin-like iron sequestering function were experimentally validated, even though their sequences differ significantly from known, characterized proteins and from each other and cannot be easily predicted using popular bioinformatics methods. As sequencing and experimental characterization data increases rapidly, the machine-learning approach based on RNN could be useful for discovery and prediction of homologues for a wide range of protein functions.},
  keywords = {BIOINF},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RSULYE8G\\Liu - 2017 - Deep Recurrent Neural Network for Protein Function Prediction from Sequence.pdf}
}

@misc{liuEnergybasedOutofdistributionDetection2021,
  title = {Energy-Based {{Out-of-distribution Detection}}},
  author = {Liu, Weitang and Wang, Xiaoyun and Owens, John D. and Li, Yixuan},
  year = {2021},
  month = apr,
  number = {arXiv:2010.03759},
  eprint = {2010.03759},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2010.03759},
  abstract = {Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax confidence score suffer from overconfident posterior distributions for OOD data. We propose a unified framework for OOD detection that uses an energy score. We show that energy scores better distinguish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax confidence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconfidence issue. Within this framework, energy can be flexibly used as a scoring function for any pre-trained neural classifier as well as a trainable cost function to shape the energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained WideResNet, using the energy score reduces the average FPR (at TPR 95\%) by 18.03\% compared to the softmax confidence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks.},
  archiveprefix = {arXiv},
  keywords = {exmodel,ood-detection},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\N5RXDPIA\\Liu et al_2021_Energy-based Out-of-distribution Detection.pdf;C\:\\Users\\w-32\\Zotero\\storage\\9JRY2HP2\\2010.html}
}

@article{liuFeatureSelectionOrthogonal2019,
  title = {Feature Selection for Orthogonal Broad Learning System Based on Mutual Information},
  author = {Liu, Zhicheng and Chen, Bao and Xie, Bingxue and Qiang, Huangping and Zhu, Ziqi},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {info-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YTLD9MUM\\Liu et al. - 2019 - Feature selection for orthogonal broad learning system based on mutual information(2).pdf}
}

@inproceedings{liuGenerativeFeatureReplay2020,
  title = {Generative {{Feature Replay For Class-Incremental Learning}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Liu, Xialei and Wu, Chenshen and Menta, Mikel and Herranz, Luis and Raducanu, Bogdan and Bagdanov, Andrew D. and Jui, Shangling and {van de Weijer}, Joost},
  year = {2020},
  month = jun,
  pages = {915--924},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPRW50498.2020.00121},
  abstract = {Humans are capable of learning new tasks without forgetting previous ones, while neural networks fail due to catastrophic forgetting between new and previously-learned tasks. We consider a class-incremental setting which means that the task-ID is unknown at inference time. The imbalance between old and new classes typically results in a bias of the network towards the newest ones. This imbalance problem can either be addressed by storing exemplars from previous tasks, or by using image replay methods. However, the latter can only be applied to toy datasets since image generation for complex datasets is a hard problem.},
  isbn = {978-1-72819-360-1},
  langid = {english},
  keywords = {continual,exmodel,GAN,replay,visiting-joost},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CPZZWF8U\\Liu et al. - 2020 - Generative Feature Replay For Class-Incremental Le.pdf}
}

@article{liuOnlineDeepMetric2022,
  title = {Online {{Deep Metric Learning}} via {{Mutual Distillation}}},
  author = {Liu, Gao-Dong and Zhao, Wan-Lei and Zhao, Jie},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.05201 [cs]},
  eprint = {2203.05201},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep metric learning aims to transform input data into an embedding space, where similar samples are close while dissimilar samples are far apart from each other. In practice, samples of new categories arrive incrementally, which requires the periodical augmentation of the learned model. The fine-tuning on the new categories usually leads to poor performance on the old, which is known as "catastrophic forgetting". Existing solutions either retrain the model from scratch or require the replay of old samples during the training. In this paper, a complete online deep metric learning framework is proposed based on mutual distillation for both one-task and multi-task scenarios. Different from the teacher-student framework, the proposed approach treats the old and new learning tasks with equal importance. No preference over the old or new knowledge is caused. In addition, a novel virtual feature estimation approach is proposed to recover the features assumed to be extracted by the old models. It allows the distillation between the new and the old models without the replay of old training samples or the holding of old models during the training. A comprehensive study shows the superior performance of our approach with the support of different backbones.},
  archiveprefix = {arXiv},
  keywords = {continual,distillation,exmodel,knowledge distillation,metric-learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DSACAKTK\\Liu et al_2022_Online Deep Metric Learning via Mutual Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\P2QN45C6\\2203.html}
}

@misc{liuOnlineHyperparameterOptimization2023,
  title = {Online {{Hyperparameter Optimization}} for {{Class-Incremental Learning}}},
  author = {Liu, Yaoyao and Li, Yingying and Schiele, Bernt and Sun, Qianru},
  year = {2023},
  month = jan,
  number = {arXiv:2301.05032},
  eprint = {2301.05032},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.05032},
  abstract = {Class-incremental learning (CIL) aims to train a classification model while the number of classes increases phase-by-phase. An inherent challenge of CIL is the stability-plasticity tradeoff, i.e., CIL models should keep stable to retain old knowledge and keep plastic to absorb new knowledge. However, none of the existing CIL models can achieve the optimal tradeoff in different data-receiving settings--where typically the training-from-half (TFH) setting needs more stability, but the training-from-scratch (TFS) needs more plasticity. To this end, we design an online learning method that can adaptively optimize the tradeoff without knowing the setting as a priori. Specifically, we first introduce the key hyperparameters that influence the trade-off, e.g., knowledge distillation (KD) loss weights, learning rates, and classifier types. Then, we formulate the hyperparameter optimization process as an online Markov Decision Process (MDP) problem and propose a specific algorithm to solve it. We apply local estimated rewards and a classic bandit algorithm Exp3 [4] to address the issues when applying online MDP methods to the CIL protocol. Our method consistently improves top-performing CIL methods in both TFH and TFS settings, e.g., boosting the average accuracy of TFH and TFS by 2.2 percentage points on ImageNet-Full, compared to the state-of-the-art [23].},
  archiveprefix = {arXiv},
  keywords = {continual,hyperparameter-optimization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NGTKBTX3\\2301.html}
}

@article{liuRotateYourNetworks2018,
  title = {Rotate Your {{Networks}}: {{Better Weight Consolidation}} and {{Less Catastrophic Forgetting}}},
  shorttitle = {Rotate Your {{Networks}}},
  author = {Liu, Xialei and Masana, Marc and Herranz, Luis and {Van de Weijer}, Joost and Lopez, Antonio M. and Bagdanov, Andrew D.},
  year = {2018},
  month = dec,
  journal = {arXiv:1802.02950 [cs]},
  eprint = {1802.02950},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to other state-of-the-art in lifelong learning without forgetting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,continual,ewc,fisher-info},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DXVLBUMZ\\Liu et al. - 2018 - Rotate your Networks Better Weight Consolidation .pdf;C\:\\Users\\w-32\\Zotero\\storage\\KXANB9PR\\1802.html}
}

@article{liuSelfSupervisedGeneralisationMeta2019,
  title = {Self-{{Supervised Generalisation}} with {{Meta Auxiliary Learning}}},
  author = {Liu, Shikun and Davison, Andrew J. and Johns, Edward},
  year = {2019},
  month = jan,
  abstract = {Learning with auxiliary tasks has been shown to improve the generalisation of a primary task. However, this comes at the cost of manually-labelling additional tasks which may, or may not, be useful for the primary task. We propose a new method which automatically learns labels for an auxiliary task, such that any supervised learning task can be improved without requiring access to additional data. The approach is to train two neural networks: a label-generation network to predict the auxiliary labels, and a multi-task network to train the primary task alongside the auxiliary task. The loss for the label-generation network incorporates the multi-task network's performance, and so this interaction between the two networks can be seen as a form of meta learning. We show that our proposed method, Meta AuXiliary Learning (MAXL), outperforms single-task learning on 7 image datasets by a significant margin, without requiring additional auxiliary labels. We also show that MAXL outperforms several other baselines for generating auxiliary labels, and is even competitive when compared with human-defined auxiliary labels. The self-supervised nature of our method leads to a promising new direction towards automated generalisation. The source code is available at \textbackslash url\{https://github.com/lorenmt/maxl\}.},
  keywords = {unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\S92LCY7Z\\Liu, Davison, Johns - 2019 - Self-Supervised Generalisation with Meta Auxiliary Learning(2).pdf}
}

@article{liuWakeningConceptsData2021,
  title = {Wakening {{Past Concepts}} without {{Past Data}}: {{Class-incremental Learning}} from {{Placebos}}},
  shorttitle = {Wakening {{Past Concepts}} without {{Past Data}}},
  author = {Liu, Yaoyao and Schiele, Bernt and Sun, Qianru},
  year = {2021},
  month = nov,
  abstract = {Not forgetting knowledge about previous classes is one of the key challenges in class-incremental learning (CIL). A common technique to address this challenge is knowledge distillation (KD) that...},
  langid = {english},
  keywords = {continual,exmodel,ood-data,RL},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GUZJHP8Q\\Liu et al_2021_Wakening Past Concepts without Past Data.pdf;C\:\\Users\\w-32\\Zotero\\storage\\4TJXNWG6\\forum.html}
}

@article{liviDeterminationEdgeCriticality2018,
  title = {Determination of the {{Edge}} of {{Criticality}} in {{Echo State Networks Through Fisher Information Maximization}}},
  author = {Livi, Lorenzo and Bianchi, Filippo Maria and Alippi, Cesare},
  year = {2018},
  month = mar,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {3},
  pages = {706--717},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2016.2644268},
  abstract = {It is a widely accepted fact that the computational capability of recurrent neural networks (RNNs) is maximized on the so-called ``edge of criticality.'' Once the network operates in this configuration, it performs efficiently on a specific application both in terms of: 1) low prediction error and 2) high short-term memory capacity. Since the behavior of recurrent networks is strongly influenced by the particular input signal driving the dynamics, a universal, application-independent method for determining the edge of criticality is still missing. In this paper, we aim at addressing this issue by proposing a theoretically motivated, unsupervised method based on Fisher information for determining the edge of criticality in RNNs. It is proved that Fisher information is maximized for (finite-size) systems operating in such critical regions. However, Fisher information is notoriously difficult to compute and requires the analytic form of the probability density function ruling the system behavior. This paper takes advantage of a recently developed nonparametric estimator of the Fisher information matrix and provides a method to determine the critical region of echo state networks (ESNs), a particular class of recurrent networks. The considered control parameters, which indirectly affect the ESN performance, are explored to identify those configurations lying on the edge of criticality and, as such, maximizing Fisher information and computational performance. Experimental results on benchmarks and real-world data demonstrate the effectiveness of the proposed method.},
  keywords = {chaos,Echo state network (ESN),echo state networks,edge of criticality,ESN,Fisher information,Fisher information matrix,Fisher information maximization,fisher-info,Jacobian matrices,Learning systems,Neurons,nonparametric estimation,nonparametric estimator,nonparametric statistics,prediction error,Probability density function,recurrent neural nets,recurrent neural networks,Reservoirs,RNN,short-term memory capacity,Training},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WHHHPG8X\\Livi et al. - 2018 - Determination of the Edge of Criticality in Echo S.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VG9N8MQN\\7817870.html}
}

@article{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year = {2018},
  month = nov,
  journal = {arXiv:1712.09913 [cs, stat]},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix = {arXiv},
  keywords = {loss-landscape,optimization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MJC4ZDSY\\Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf;C\:\\Users\\w-32\\Zotero\\storage\\M2EAEEVK\\1712.html}
}

@misc{LocalLyapunovExponents,
  title = {Local {{Lyapunov}} Exponents of Deep Echo State Networks | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neucom.2017.11.073},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0925231218302157?token=DC93A5BAEA6EC60D97968AFD526DA4AD594830F75314192614563E2845871CC3ED89E9B353A831AF79A28B8A2EE2FF44},
  langid = {english},
  keywords = {rnn,unipi},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JLLHQBD3\\S0925231218302157.html}
}

@inproceedings{lomonacoAvalancheEndtoEndLibrary2021,
  title = {Avalanche: {{An End-to-End Library}} for {{Continual Learning}}},
  shorttitle = {Avalanche},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lomonaco, Vincenzo and Pellegrini, Lorenzo and Cossu, Andrea and Carta, Antonio and Graffieti, Gabriele and Hayes, Tyler L. and De Lange, Matthias and Masana, Marc and Pomponi, Jary and {van de Ven}, Gido M. and Mundt, Martin and She, Qi and Cooper, Keiland and Forest, Jeremy and Belouadah, Eden and Calderara, Simone and Parisi, German I. and Cuzzolin, Fabio and Tolias, Andreas S. and Scardapane, Simone and Antiga, Luca and Ahmad, Subutai and Popescu, Adrian and Kanan, Christopher and {van de Weijer}, Joost and Tuytelaars, Tinne and Bacciu, Davide and Maltoni, Davide},
  year = {2021},
  pages = {3600--3610},
  langid = {english},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Lomonaco et al_2021_Avalanche.pdf;C\:\\Users\\w-32\\Zotero\\storage\\LM97FTBP\\Lomonaco_Avalanche_An_End-to-End_Library_for_Continual_Learning_CVPRW_2021_paper.html}
}

@article{lomonacoContinualLearningDeep,
  title = {Continual {{Learning}} with {{Deep Architectures}}},
  author = {Lomonaco, Vincenzo},
  pages = {150},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\69S8C8PV\\Lomonaco - Continual Learning with Deep Architectures.pdf}
}

@article{lomonacoRehearsalFreeContinualLearning,
  title = {Rehearsal-{{Free Continual Learning Over Small Non-I}}.{{I}}.{{D}}. {{Batches}}},
  author = {Lomonaco, Vincenzo and Maltoni, Davide and Pellegrini, Lorenzo},
  pages = {10},
  abstract = {Robotic vision is a field where continual learning can play a significant role. An embodied agent operating in a complex environment subject to frequent and unpredictable changes is required to learn and adapt continuously. In the context of object recognition, for example, a robot should be able to learn (without forgetting) objects of never before seen classes as well as improving its recognition capabilities as new instances of already known classes are discovered. Ideally, continual learning should be triggered by the availability of short videos of single objects and performed on-line on on-board hardware with fine-grained updates. In this paper, we introduce a novel continual learning protocol based on the CORe50 benchmark and propose two rehearsal-free continual learning techniques, CWR* and AR1*, that can learn effectively even in the challenging case of nearly 400 small non-i.i.d. incremental batches. In particular, our experiments show that AR1* can outperform other state-of-the-art rehearsal-free techniques by more than 15\% accuracy in some cases, with a very light and constant computational and memory overhead across training batches.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZHJCYTF7\\Lomonaco et al. - Rehearsal-Free Continual Learning Over Small Non-I.pdf}
}

@article{lopesDataFreeKnowledgeDistillation2017,
  title = {Data-{{Free Knowledge Distillation}} for {{Deep Neural Networks}}},
  author = {Lopes, Raphael Gontijo and Fenu, Stefano and Starner, Thad},
  year = {2017},
  month = nov,
  journal = {arXiv:1710.07535 [cs]},
  eprint = {1710.07535},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent advances in model compression have provided procedures for compressing large neural networks to a fraction of their original size while retaining most if not all of their accuracy. However, all of these approaches rely on access to the original training set, which might not always be possible if the network to be compressed was trained on a very large dataset, or on a dataset whose release poses privacy or safety concerns as may be the case for biometrics tasks. We present a method for data-free knowledge distillation, which is able to compress deep neural networks trained on large-scale datasets to a fraction of their size leveraging only some extra metadata to be provided with a pretrained model release. We also explore different kinds of metadata that can be used with our method, and discuss tradeoffs involved in using each of them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,continual,data-free,exmodel},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4QV3DQLC\\Lopes et al. - 2017 - Data-Free Knowledge Distillation for Deep Neural N.pdf}
}

@article{lopez-pazGradientEpisodicMemory2017,
  title = {Gradient Episodic Memory for Continual Learning},
  author = {{Lopez-Paz}, David and Ranzato, Marc'Aurelio},
  year = {2017},
  journal = {Advances in Neural Information Processing Systems},
  volume = {2017-Decem},
  number = {Nips},
  pages = {6468--6477},
  abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
  keywords = {continual,regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NBPAJT5R\\Lopez-Paz, Ranzato - 2017 - Gradient episodic memory for continual learning.pdf}
}

@article{losingRandomizingSelfAdjustingMemory,
  title = {Randomizing the {{Self-Adjusting Memory}} for {{Enhanced Handling}} of {{Concept Drift}}},
  author = {Losing, Viktor and Hammer, Barbara and Wersing, Heiko and Bifet, Albert},
  pages = {8},
  abstract = {Real-time learning from data streams in nonstationary environments gains ever more relevance due to the exponentially increasing amounts of generated data. Recently, the Self-Adjusting Memory (SAM) was proposed, an algorithm able to robustly handle heterogeneous types on the basis of two dedicated memories for the current and former concepts that continuously preserve consistency with explicit filtering. Yet, since the algorithm is restricted to one memory architecture, the variety of possible alternatives is limited by design in favor of an overall model consistency. Moreover, it does not actively detect drift, thus adapting with a relatively high delay in case of abrupt changes. We propose a dynamic ensemble on the basis of the SAM algorithm, which is triggered by both, the inherent passive adaptation of SAM and active drift detection. Further, since SAM is based on the stable k-Nearest-Neighbor algorithm, we investigate multiple approaches to obtain a high diversity in the ensemble, resulting in an effective overall strategy. The increased computational demand is countered on the basis of a parallel implementation. We extensively evaluate the method on numerous benchmarks, where it consistently achieves superior results in comparison to state-of-the-art methods.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2QEDPQAD\\Losing et al. - Randomizing the Self-Adjusting Memory for Enhanced.pdf}
}

@inproceedings{lucchesiAvalancheRLContinual2022,
  title = {Avalanche {{RL}}: {{A Continual Reinforcement Learning Library}}},
  shorttitle = {Avalanche {{RL}}},
  booktitle = {Image {{Analysis}} and {{Processing}} \textendash{} {{ICIAP}} 2022},
  author = {Lucchesi, Nicol{\'o} and Carta, Antonio and Lomonaco, Vincenzo and Bacciu, Davide},
  editor = {Sclaroff, Stan and Distante, Cosimo and Leo, Marco and Farinella, Giovanni M. and Tombari, Federico},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {524--535},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-06427-2_44},
  abstract = {Continual Reinforcement Learning (CRL) is a challenging setting where an agent learns to interact with an environment that is constantly changing over time (the stream of experiences). In this paper, we describe Avalanche RL, a library for Continual Reinforcement Learning which allows users to easily train agents on a continuous stream of tasks. Avalanche RL is based on PyTorch [23] and supports any OpenAI Gym [4] environment. Its design is based on Avalanche [16], one of the most popular continual learning libraries, which allow us to reuse a large number of continual learning strategies and improve the interaction between reinforcement learning and continual learning researchers. Additionally, we propose Continual Habitat-Lab, a novel benchmark and a high-level library which enables the usage of the photorealistic simulator Habitat-Sim [28] for CRL research. Overall, Avalanche RL attempts to unify under a common framework continual reinforcement learning applications, which we hope will foster the growth of the field.},
  isbn = {978-3-031-06427-2},
  langid = {english},
  keywords = {continual,RL}
}

@incollection{Lukoeviius2012,
  title = {A Practical Guide to Applying Echo State Networks},
  booktitle = {Neural Networks: {{Tricks}} of the Trade: {{Second}} Edition},
  author = {Luko{\v s}evi{\v c}ius, Mantas},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  pages = {659--686},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-86},
  abstract = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing ``flavors''. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-specific modifications.},
  isbn = {978-3-642-35289-8},
  keywords = {ESN,RNN}
}

@article{luoCosineNormalizationUsing2018,
  title = {Cosine Normalization: {{Using}} Cosine Similarity Instead of Dot Product in Neural Networks},
  author = {Luo, Chunjie and Zhan, Jianfeng and Xue, Xiaohe and Wang, Lei and Ren, Rui and Yang, Qiang},
  year = {2018},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {11139 LNCS},
  pages = {382--391},
  issn = {9783030014179},
  doi = {10.1007/978-3-030-01418-6_38},
  abstract = {Traditionally, multi-layer neural networks use dot product between the output vector of previous layer and the incoming weight vector as the input to activation function. The result of dot product is unbounded, thus increases the risk of large variance. Large variance of neuron makes the model sensitive to the change of input distribution, thus results in poor generalization, and aggravates the internal covariate shift which slows down the training. To bound dot product and decrease the variance, we propose to use cosine similarity or centered cosine similarity (Pearson Correlation Coefficient) instead of dot product in neural networks, which we call cosine normalization. We compare cosine normalization with batch, weight and layer normalization in fully-connected neural networks as well as convolutional networks on the data sets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that cosine normalization achieves better performance than other normalization techniques.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MRR3ZT37\\Luo et al. - 2018 - Cosine normalization Using cosine similarity instead of dot product in neural networks(2).pdf}
}

@techreport{luPhaseUpgradeCMS2012,
  ids = {luPhaseUpgradeCMS2012a},
  title = {The {{Phase}} 1 {{Upgrade}} of the {{CMS Pixel Detector}}},
  author = {Lu, Rong-Shyang},
  year = {2012},
  month = dec,
  number = {CMS-CR-2012-371},
  address = {{Geneva}},
  institution = {{CERN}},
  keywords = {high-energy-physics}
}

@misc{macedoRobustDeepLearning2022,
  title = {Towards {{Robust Deep Learning}} Using {{Entropic Losses}}},
  author = {Mac{\^e}do, David},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03566},
  eprint = {2208.03566},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.03566},
  abstract = {Current deep learning solutions are well known for not informing whether they can reliably classify an example during inference. One of the most effective ways to build more reliable deep learning solutions is to improve their performance in the so-called out-of-distribution detection task, which essentially consists of "know that you do not know" or "know the unknown". In other words, out-of-distribution detection capable systems may reject performing a nonsense classification when submitted to instances of classes on which the neural network was not trained. This thesis tackles the defiant out-of-distribution detection task by proposing novel loss functions and detection scores. Uncertainty estimation is also a crucial auxiliary task in building more robust deep learning systems. Therefore, we also deal with this robustness-related task, which evaluates how realistic the probabilities presented by the deep neural network are. To demonstrate the effectiveness of our approach, in addition to a substantial set of experiments, which includes state-of-the-art results, we use arguments based on the principle of maximum entropy to establish the theoretical foundation of the proposed approaches. Unlike most current methods, our losses and scores are seamless and principled solutions that produce accurate predictions in addition to fast and efficient inference. Moreover, our approaches can be incorporated into current and future projects simply by replacing the loss used to train the deep neural network and computing a rapid score for detection.},
  archiveprefix = {arXiv},
  keywords = {notag,ood-detection,thesis,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BWDJU6WD\\Macdo_2022_Towards Robust Deep Learning using Entropic Losses.pdf;C\:\\Users\\w-32\\Zotero\\storage\\GWR6Y82I\\2208.html}
}

@article{mackayReversibleRecurrentNeural2018,
  title = {Reversible {{Recurrent Neural Networks}}},
  author = {MacKay, Matthew and Vicol, Paul and Ba, Jimmy and Grosse, Roger B.},
  year = {2018},
  pages = {9043--9054},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MJKGVQQC\\MacKay et al. - 2018 - Reversible Recurrent Neural Networks(3).pdf}
}

@article{madaanRepresentationalContinuityUnsupervised2022,
  title = {Representational {{Continuity}} for {{Unsupervised Continual Learning}}},
  author = {Madaan, Divyam and Yoon, Jaehong and Li, Yuanchun and Liu, Yunxin and Hwang, Sung Ju},
  year = {2022},
  month = apr,
  journal = {arXiv:2110.06976 [cs]},
  eprint = {2110.06976},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\A8RE2UL8\\2110.html}
}

@article{mahendranUnderstandingDeepImage2014,
  title = {Understanding {{Deep Image Representations}} by {{Inverting Them}}},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  year = {2014},
  month = nov,
  journal = {arXiv:1412.0035 [cs]},
  eprint = {1412.0035},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,data-free,vision},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Mahendran_Vedaldi_2014_Understanding Deep Image Representations by Inverting Them.pdf;C\:\\Users\\w-32\\Zotero\\storage\\Y3XXM2DL\\1412.html}
}

@article{maheswaranathanHowRecurrentNetworks,
  title = {How Recurrent Networks Implement Contextual Processing in Sentiment Analysis},
  author = {Maheswaranathan, Niru and Sussillo, David},
  pages = {12},
  abstract = {Neural networks have a remarkable capacity for contextual processing\textemdash using recent or nearby inputs to modify processing of current input. For example, in natural language, contextual processing is necessary to correctly interpret negation (e.g. phrases such as ``not bad''). However, our ability to understand how networks process context is limited. Here, we propose general methods for reverse engineering recurrent neural networks (RNNs) to identify and elucidate contextual processing. We apply these methods to understand RNNs trained on sentiment classification. This analysis reveals inputs that induce contextual effects, quantifies the strength and timescale of these effects, and identifies sets of these inputs with similar properties. Additionally, we analyze contextual effects related to differential processing of the beginning and end of documents. Using the insights learned from the RNNs we improve baseline Bag-of-Words models with simple extensions that incorporate contextual modification, recovering greater than 90\% of the RNN's performance increase over the baseline. This work yields a new understanding of how RNNs process contextual information, and provides tools that should provide similar insight more broadly.},
  langid = {english},
  keywords = {notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7665T9YQ\\Maheswaranathan and Sussillo - How recurrent networks implement contextual proces.pdf}
}

@article{maheswaranathanReverseEngineeringRecurrent2019,
  title = {Reverse Engineering Recurrent Networks for Sentiment Classification Reveals Line Attractor Dynamics},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
  year = {2019},
  month = jun,
  abstract = {Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.},
  keywords = {Dynamical systems,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LAUHK3BU\\Maheswaranathan et al. - 2019 - Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics(2).pdf}
}

@incollection{maheswaranathanUniversalityIndividualityNeural2019,
  title = {Universality and Individuality in Neural Dynamics across Large Populations of Recurrent Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {15629--15641},
  publisher = {{Curran Associates, Inc.}},
  keywords = {Dynamical systems,learn-dynamics,NIPS,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BIRLT9HM\\Maheswaranathan et al. - 2019 - Universality and individuality in neural dynamics .pdf;C\:\\Users\\w-32\\Zotero\\storage\\GKHE2PGH\\9694-universality-and-individuality-in-neural-dynamics-across-large-populations-of-recurrent-ne.html}
}

@article{maillardJointlyLearningSentence2017,
  title = {Jointly {{Learning Sentence Embeddings}} and {{Syntax}} with {{Unsupervised Tree-LSTMs}}},
  author = {Maillard, Jean and Clark, Stephen and Yogatama, Dani},
  year = {2017},
  abstract = {We introduce a neural network that represents sentences by composing their words according to induced binary parse trees. We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser. Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees. As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation. We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task.},
  keywords = {LSTM,nlp,RNN,tree},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LALUP7RY\\Maillard, Clark, Yogatama - 2017 - Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs(2).pdf}
}

@article{maiOnlineContinualLearning2021,
  title = {Online {{Continual Learning}} in {{Image Classification}}: {{An Empirical Survey}}},
  shorttitle = {Online {{Continual Learning}} in {{Image Classification}}},
  author = {Mai, Zheda and Li, Ruiwen and Jeong, Jihwan and Quispe, David and Kim, Hyunwoo and Sanner, Scott},
  year = {2021},
  month = oct,
  journal = {arXiv:2101.10423 [cs]},
  eprint = {2101.10423},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Online continual learning for image classification studies the problem of learning to classify images from an online stream of data and tasks, where tasks may include new classes (class incremental) or data nonstationarity (domain incremental). One of the key challenges of continual learning is to avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence of more recent tasks. Over the past few years, many methods and tricks have been introduced to address this problem, but many have not been fairly and systematically compared under a variety of realistic and practical settings. To better understand the relative advantages of various approaches and the settings where they work best, this survey aims to (1) compare state-of-the-art methods such as MIR, iCARL, and GDumb and determine which works best at different experimental settings; (2) determine if the best class incremental methods are also competitive in domain incremental setting; (3) evaluate the performance of 7 simple but effective trick such as "review" trick and nearest class mean (NCM) classifier to assess their relative impact. Regarding (1), we observe iCaRL remains competitive when the memory buffer is small; GDumb outperforms many recently proposed methods in medium-size datasets and MIR performs the best in larger-scale datasets. For (2), we note that GDumb performs quite poorly while MIR -- already competitive for (1) -- is also strongly competitive in this very different but important setting. Overall, this allows us to conclude that MIR is overall a strong and versatile method across a wide variety of settings. For (3), we find that all 7 tricks are beneficial, and when augmented with the "review" trick and NCM classifier, MIR produces performance levels that bring online continual learning much closer to its ultimate goal of matching offline training.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Mai et al_2021_Online Continual Learning in Image Classification.pdf;C\:\\Users\\w-32\\Zotero\\storage\\UFWKBPL8\\2101.html}
}

@article{makhzaniAdversarialAutoencoders2015,
  title = {Adversarial {{Autoencoders}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  year = {2015},
  month = nov,
  abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  keywords = {GAN,vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZVNPZCR5\\Makhzani et al. - 2015 - Adversarial Autoencoders(2).pdf}
}

@misc{MakingBrainMachine,
  title = {Making Brain\textendash Machine Interfaces Robust to Future Neural Variability | {{Nature Communications}}},
  howpublished = {https://www.nature.com/articles/ncomms13749},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9A74F75X\\ncomms13749.html}
}

@inproceedings{maLayerWisedModelAggregation2022,
  title = {Layer-{{Wised Model Aggregation}} for {{Personalized Federated Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ma, Xiaosong and Zhang, Jie and Guo, Song and Xu, Wenchao},
  year = {2022},
  pages = {10092--10101},
  langid = {english},
  keywords = {Continual,federated,hypernetwork},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H4VI6XFV\\Ma_Layer-Wised_Model_Aggregation_for_Personalized_Federated_Learning_CVPR_2022_paper.html}
}

@inproceedings{malhotraLongShortTerm2015,
  title = {Long {{Short Term Memory Networks}} for {{Anomaly Detection}} in {{Time Series}}},
  booktitle = {{{ESANN}}},
  author = {Malhotra, Pankaj and Vig, L. and Shroff, Gautam M. and Agarwal, P.},
  year = {2015},
  abstract = {The efficacy of stacked LSTM networks for anomaly/fault detection in time series on ECG, space shuttle, power demand, and multi-sensor engine dataset is demonstrated. Long Short Term Memory (LSTM) networks have been demonstrated to be particularly useful for learning sequences containing longer term patterns of unknown length, due to their ability to maintain long term memory. Stacking recurrent hidden layers in such networks also enables the learning of higher level temporal features, for faster learning with sparser representations. In this paper, we use stacked LSTM networks for anomaly/fault detection in time series. A network is trained on non-anomalous data and used as a predictor over a number of time steps. The resulting prediction errors are modeled as a multivariate Gaussian distribution, which is used to assess the likelihood of anomalous behavior. The efficacy of this approach is demonstrated on four datasets: ECG, space shuttle, power demand, and multi-sensor engine dataset.},
  keywords = {anomaly-detection,RNN}
}

@article{malhotraLSTMbasedEncoderDecoderMultisensor2016,
  title = {{{LSTM-based Encoder-Decoder}} for {{Multi-sensor Anomaly Detection}}},
  author = {Malhotra, Pankaj and Ramakrishnan, Anusha and Anand, Gaurangi and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.00148 [cs, stat]},
  eprint = {1607.00148},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
  archiveprefix = {arXiv},
  keywords = {anomaly-detection,RNN},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Malhotra et al_2016_LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection.pdf;C\:\\Users\\w-32\\Zotero\\storage\\Y63ZUWR3\\1607.html}
}

@article{Mali2019TheNS,
  title = {The Neural State Pushdown Automata},
  author = {Mali, Ankur and Ororbia, Alexander and Giles, C. Lee},
  year = {2019},
  journal = {ArXiv},
  volume = {abs/1909.05233}
}

@inproceedings{mallyaPackNetAddingMultiple2018,
  title = {{{PackNet}}: {{Adding Multiple Tasks}} to a {{Single Network}} by {{Iterative Pruning}}},
  shorttitle = {{{PackNet}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mallya, Arun and Lazebnik, Svetlana},
  year = {2018},
  pages = {7765--7773},
  keywords = {continual,masking},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6AD7467R\\Mallya_Lazebnik_2018_PackNet.pdf}
}

@article{mallyaPiggybackAdaptingSingle2018,
  title = {Piggyback: {{Adapting}} a {{Single Network}} to {{Multiple Tasks}} by {{Learning}} to {{Mask Weights}}},
  shorttitle = {Piggyback},
  author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  year = {2018},
  month = mar,
  journal = {arXiv:1801.06519 [cs]},
  eprint = {1801.06519},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that piggyback on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Unlike prior work, we do not suffer from catastrophic forgetting or competition between tasks, and our performance is agnostic to task ordering. Code available at https://github.com/arunmallya/piggyback.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,exmodel,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UUYJ2E8N\\Mallya et al_2018_Piggyback.pdf;C\:\\Users\\w-32\\Zotero\\storage\\GPZIJTGW\\1801.html}
}

@article{maltoniContinuousLearningSingleincrementaltask2019,
  title = {Continuous Learning in Single-Incremental-Task Scenarios},
  author = {Maltoni, Davide and Lomonaco, Vincenzo},
  year = {2019},
  month = aug,
  journal = {Neural Networks},
  volume = {116},
  pages = {56--73},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.03.010},
  abstract = {It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.},
  langid = {english},
  keywords = {continual,Continuous learning,Deep learning,Incremental class learning,Lifelong learning,Object recognition,Single-incremental-task},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Z4M8RIEP\\Maltoni and Lomonaco - 2019 - Continuous learning in single-incremental-task sce.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KF7JC8P8\\S0893608019300838.html}
}

@inproceedings{mandtVariationalAnalysisStochastic2016,
  title = {A Variational Analysis of Stochastic Gradient Algorithms},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 48},
  author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  year = {2016},
  month = jun,
  series = {{{ICML}}'16},
  pages = {354--363},
  publisher = {{JMLR.org}},
  address = {{New York, NY, USA}},
  abstract = {Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuoustime stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.},
  keywords = {notag}
}

@misc{maPrinciplesParsimonySelfConsistency2022,
  title = {On the {{Principles}} of {{Parsimony}} and {{Self-Consistency}} for the {{Emergence}} of {{Intelligence}}},
  author = {Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
  year = {2022},
  month = jul,
  number = {arXiv:2207.04630},
  eprint = {2207.04630},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.04630},
  abstract = {Ten years into the revival of deep networks and artificial intelligence, we propose a theoretical framework that sheds light on understanding deep networks within a bigger picture of Intelligence in general. We introduce two fundamental principles, Parsimony and Self-consistency, that address two fundamental questions regarding Intelligence: what to learn and how to learn, respectively. We believe the two principles are the cornerstones for the emergence of Intelligence, artificial or natural. While these two principles have rich classical roots, we argue that they can be stated anew in entirely measurable and computable ways. More specifically, the two principles lead to an effective and efficient computational framework, compressive closed-loop transcription, that unifies and explains the evolution of modern deep networks and many artificial intelligence practices. While we mainly use modeling of visual data as an example, we believe the two principles will unify understanding of broad families of autonomous intelligent systems and provide a framework for understanding the brain.},
  archiveprefix = {arXiv},
  keywords = {framework,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QIZV9EPG\\2207.html}
}

@inproceedings{marelliSICKCureEvaluation2014,
  title = {A {{SICK}} Cure for the Evaluation of Compositional Distributional Semantic Models},
  booktitle = {Proceedings of the 10th {{Edition}} of the {{Language}}, {{Resources}} and {{Evaluation Conference}} ({{LREC}} 2016)},
  author = {Marelli, Marco and Menini, Stefano and Baroni, Marco and Bentivogli, Luisa and Bernardi, Raffaella and Zamparelli, Roberto},
  year = {2014},
  pages = {216--223},
  abstract = {Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs. By means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral). The SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes.},
  isbn = {978-2-9517408-8-4},
  keywords = {data,nlp-embeddings,textual entailment},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H4SVXW45\\Marelli et al. - 2014 - A SICK cure for the evaluation of compositional distributional semantic models(2).pdf}
}

@article{marquezDeepCascadeLearning2018,
  title = {Deep {{Cascade Learning}}},
  author = {Marquez, Enrique S. and Hare, Jonathon S. and Niranjan, Mahesan},
  year = {2018},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {11},
  pages = {5475--5485},
  issn = {1614688370},
  doi = {10.1109/TNNLS.2018.2805098},
  abstract = {\textemdash In this paper, we propose a novel approach for efficient training of deep neural networks in a bottom-up fash-ion using a layered structure. Our algorithm, which we refer to as Deep Cascade Learning, is motivated by the Cascade Correlation approach of Fahlman [1] who introduced it in the context of perceptrons. We demonstrate our algorithm on networks of convolutional layers, though its applicability is more general. Such training of deep networks in a cascade, directly circumvents the well-known vanishing gradient problem by ensuring that the output is always adjacent to the layer being trained. We present empirical evaluations comparing our deep cascade training with standard End-End training using back propagation of two convolutional neural network architectures on benchmark image classification tasks (CIFAR-10 and CIFAR-100). We then investigate the features learned by the approach and find that better, domain-specific, representations are learned in early layers when compared to what is learned in End-End training. This is partially attributable to the vanishing gradient problem which inhibits early layer filters to change significantly from their initial settings. While both networks perform similarly overall, recognition accuracy increases progressively with each added layer, with discriminative features learnt in every stage of the network, whereas in End-End training, no such systematic feature representation was observed. We also show that such cascade training has significant computational and memory advantages over End-End training, and can be used as a pre-training algorithm to obtain a better performance.},
  keywords = {cascade correlation,CNN,vision}
}

@article{martensDeepLearningHessianfree2010,
  title = {Deep Learning via {{Hessian-free}} Optimization},
  author = {Martens, James},
  year = {2010},
  journal = {27th International Conference on Machine Learning},
  volume = {951},
  pages = {735--742},
  issn = {9781605589077},
  doi = {10.1155/2011/176802},
  abstract = {We develop a 2nd-order optimization method based on the Hessian - free approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton Salakhutdinov (2006) on the same tasks they considered. Our ...},
  keywords = {sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GJLWWWN9\\Martens - 2010 - Deep learning via Hessian-free optimization.pdf}
}

@inproceedings{martensLearningRecurrentNeural2011,
  title = {Learning {{Recurrent Neural Networks}} with {{Hessian-Free Optimization James}}},
  booktitle = {{{ICML}}},
  author = {Martens, James and Sutskever, Ilya},
  year = {2011},
  keywords = {ICML,optimization,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\763EF45G\\Martens, Sutskever - 2011 - Learning Recurrent Neural Networks with Hessian-Free Optimization James.pdf}
}

@article{martinabadiTensorFlowLargeScaleMachine2015,
  title = {\{\vphantom\}{{TensorFlow}}\vphantom\{\}: {{Large-Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {{Mart\textbackslash '\textbackslash in\textasciitilde Abadi} and {Ashish\textasciitilde Agarwal} and {Paul\textasciitilde Barham} and {Eugene\textasciitilde Brevdo} and {Zhifeng\textasciitilde Chen} and {Craig\textasciitilde Citro} and {Greg\textasciitilde S.\textasciitilde Corrado} and {Andy\textasciitilde Davis} and {Jeffrey\textasciitilde Dean} and {Matthieu\textasciitilde Devin} and {Sanjay\textasciitilde Ghemawat} and {Ian\textasciitilde Goodfellow} and {Andrew\textasciitilde Harp} and {Geoffrey\textasciitilde Irving} and {Michael\textasciitilde Isard} and Jia, Yangqing and {Rafal\textasciitilde Jozefowicz} and {Lukasz\textasciitilde Kaiser} and {Manjunath\textasciitilde Kudlur} and {Josh\textasciitilde Levenberg} and {Dan\textasciitilde Man\'e} and {Rajat\textasciitilde Monga} and {Sherry\textasciitilde Moore} and {Derek\textasciitilde Murray} and {Chris\textasciitilde Olah} and {Mike\textasciitilde Schuster} and {Jonathon\textasciitilde Shlens} and {Benoit\textasciitilde Steiner} and {Ilya\textasciitilde Sutskever} and {Kunal\textasciitilde Talwar} and {Paul\textasciitilde Tucker} and {Vincent\textasciitilde Vanhoucke} and {Vijay\textasciitilde Vasudevan} and {Fernanda\textasciitilde Vi\'egas} and {Oriol\textasciitilde Vinyals} and {Pete\textasciitilde Warden} and {Martin\textasciitilde Wattenberg} and {Martin\textasciitilde Wicke} and {Yuan\textasciitilde Yu} and {Xiaoqiang\textasciitilde Zheng}},
  year = {2015},
  keywords = {software}
}

@article{martinParallelizingLinearRecurrent2017,
  title = {Parallelizing {{Linear Recurrent Neural Nets Over Sequence Length}}},
  author = {Martin, Eric and Cundy, Chris},
  year = {2017},
  month = sep,
  abstract = {Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences even with small minibatch size. We develop a parallel linear recurrence CUDA kernel and show that it can be applied to immediately speed up training and inference of several state of the art RNN architectures by up to 9x. We abstract recent work on linear RNNs into a new framework of linear surrogate RNNs and develop a linear surrogate model for the long short-term memory unit, the GILR-LSTM, that utilizes parallel linear recurrence. We extend sequence learning to new extremely long sequence regimes that were previously out of reach by successfully training a GILR-LSTM on a synthetic sequence classification task with a one million timestep dependency.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VPSU9A56\\Martin, Cundy - 2017 - Parallelizing Linear Recurrent Neural Nets Over Sequence Length(2).pdf}
}

@article{marzenDifferenceMemoryPrediction2017,
  title = {Difference between Memory and Prediction in Linear Recurrent Networks},
  author = {Marzen, Sarah},
  year = {2017},
  journal = {Physical Review E},
  volume = {96},
  number = {3},
  pages = {1--9},
  doi = {10.1103/PhysRevE.96.032308},
  abstract = {Recurrent networks are trained to memorize their input better, often in the hopes that such training will increase the ability of the network to predict. We show that networks designed to memorize input can be arbitrarily bad at prediction. We also find, for several types of inputs, that one-node networks optimized for prediction are nearly at upper bounds on predictive capacity given by Wiener filters, and are roughly equivalent in performance to randomly generated five-node networks. Our results suggest that maximizing memory capacity leads to very different networks than maximizing predictive capacity, and that optimizing recurrent weights can decrease reservoir size by half an order of magnitude.},
  keywords = {linear,Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\J9Q4LYTC\\Marzen - 2017 - Difference between memory and prediction in linear recurrent networks.pdf}
}

@article{masanaClassIncrementalLearningSurvey2022,
  ids = {masanaClassincrementalLearningSurvey2020},
  title = {Class-{{Incremental Learning}}: {{Survey}} and {{Performance Evaluation}} on {{Image Classification}}},
  shorttitle = {Class-{{Incremental Learning}}},
  author = {Masana, Marc and Liu, Xialei and Twardowski, Bart{\l}omiej and Menta, Mikel and Bagdanov, Andrew D. and {van de Weijer}, Joost},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2010.15277},
  eprinttype = {arxiv},
  pages = {1--20},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3213473},
  abstract = {For future learning systems, incremental learning is desirable because it allows for: efficient resource usage by eliminating the need to retrain from scratch at the arrival of new data; reduced memory usage by preventing or limiting the amount of data required to be stored \textendash{} also important when privacy limitations are imposed; and learning that more closely resembles human learning. The main challenge for incremental learning is catastrophic forgetting, which refers to the precipitous drop in performance on previously learned tasks after learning a new one. Incremental learning of deep neural networks has seen explosive growth in recent years. Initial work focused on task-incremental learning, where a task-ID is provided at inference time. Recently, we have seen a shift towards class-incremental learning where the learner must discriminate at inference time between all classes seen in previous tasks without recourse to a task-ID. In this paper, we provide a complete survey of existing class-incremental learning methods for image classification, and in particular, we perform an extensive experimental evaluation on thirteen class-incremental methods. We consider several new experimental scenarios, including a comparison of class-incremental methods on multiple large-scale image classification datasets, an investigation into small and large domain shifts, and a comparison of various network architectures.},
  archiveprefix = {arXiv},
  keywords = {class-incremental,continual,review},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\G9IRWLK4\\Masana et al_2020_Class-incremental learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\VCC8VVUV\\2010.15277.pdf;C\:\\Users\\w-32\\Zotero\\storage\\IHIQMLIP\\2010.html;C\:\\Users\\w-32\\Zotero\\storage\\QFCWCAS7\\9915459.html}
}

@article{masarczykRobustnessGenerativeRepresentations2021,
  title = {On Robustness of Generative Representations against Catastrophic Forgetting},
  author = {Masarczyk, Wojciech and Deja, Kamil and Trzci{\'n}ski, Tomasz},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.01844 [cs]},
  eprint = {2109.01844},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Catastrophic forgetting of previously learned knowledge while learning new tasks is a widely observed limitation of contemporary neural networks. Although many continual learning methods are proposed to mitigate this drawback, the main question remains unanswered: what is the root cause of catastrophic forgetting? In this work, we aim at answering this question by posing and validating a set of research hypotheses related to the specificity of representations built internally by neural models. More specifically, we design a set of empirical evaluations that compare the robustness of representations in discriminative and generative models against catastrophic forgetting. We observe that representations learned by discriminative models are more prone to catastrophic forgetting than their generative counterparts, which sheds new light on the advantages of developing generative models for continual learning. Finally, our work opens new research pathways and possibilities to adopt generative models in continual learning beyond mere replay mechanisms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,exmodel,toread},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Masarczyk et al_2021_On robustness of generative representations against catastrophic forgetting.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NZF3D3H7\\2109.html}
}

@article{masseAlleviatingCatastrophicForgetting2018,
  title = {Alleviating Catastrophic Forgetting Using Context-Dependent Gating and Synaptic Stabilization},
  author = {Masse, Nicolas Y. and Grant, Gregory D. and Freedman, David J.},
  year = {2018},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {44},
  pages = {E10467-E10475},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1803839115},
  abstract = {Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks (ANNs) on new tasks typically causes them to forget previously learned tasks. This phenomenon is the result of ``catastrophic forgetting,'' in which training an ANN disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of ANNs that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly nonoverlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows ANNs to maintain high performance across large numbers of sequentially presented tasks, particularly when combined with weight stabilization. We show that this method works for both feedforward and recurrent network architectures, trained using either supervised or reinforcement-based learning. This suggests that using multiple, complementary methods, akin to what is believed to occur in the brain, can be a highly effective strategy to support continual learning.},
  chapter = {PNAS Plus},
  copyright = {\textcopyright{} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {30315147},
  keywords = {continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Masse et al_2018_Alleviating catastrophic forgetting using context-dependent gating and synaptic.pdf;C\:\\Users\\w-32\\Zotero\\storage\\YQMEG67D\\E10467.html}
}

@article{matenaMergingModelsFisherWeighted2021,
  title = {Merging {{Models}} with {{Fisher-Weighted Averaging}}},
  author = {Matena, Michael and Raffel, Colin},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.09832 [cs]},
  eprint = {2111.09832},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transfer learning provides a way of leveraging knowledge from one task when learning another task. Performing transfer learning typically involves iteratively updating a model's parameters through gradient descent on a training dataset. In this paper, we introduce a fundamentally different method for transferring knowledge across models that amounts to "merging" multiple models into one. Our approach effectively involves computing a weighted average of the models' parameters. We show that this averaging is equivalent to approximately sampling from the posteriors of the model weights. While using an isotropic Gaussian approximation works well in some cases, we also demonstrate benefits by approximating the precision matrix via the Fisher information. In sum, our approach makes it possible to combine the "knowledge" in multiple models at an extremely low computational cost compared to standard gradient-based training. We demonstrate that model merging achieves comparable performance to gradient descent-based transfer learning on intermediate-task training and domain adaptation problems. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. To measure the robustness of our approach, we perform an extensive ablation on the design of our algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,exmodel,fisher,model-patching},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Matena_Raffel_2021_Merging Models with Fisher-Weighted Averaging.pdf;C\:\\Users\\w-32\\Zotero\\storage\\UTQIW4ID\\2111.html}
}

@inproceedings{mccoyRNNsImplicitlyImplement2018,
  title = {{{RNNs}} Implicitly Implement Tensor-Product Representations},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {McCoy, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
  year = {2018},
  month = sep,
  abstract = {Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities...},
  keywords = {nlp,RNN,tensor},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KPVM45HU\\McCoy et al. - 2018 - RNNs implicitly implement tensor-product represent.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZJISX3YC\\forum.html}
}

@article{mcmahanCommunicationEfficientLearningDeep2017,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2017},
  month = feb,
  journal = {arXiv:1602.05629 [cs]},
  eprint = {1602.05629},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,exmodel,federated},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\McMahan et al_2017_Communication-Efficient Learning of Deep Networks from Decentralized Data.pdf;C\:\\Users\\w-32\\Zotero\\storage\\YWXS9EL7\\1602.html}
}

@article{mcmahanLearningDifferentiallyPrivate2018,
  title = {Learning {{Differentially Private Recurrent Language Models}}},
  author = {McMahan, H. Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  year = {2018},
  month = feb,
  journal = {arXiv:1710.06963 [cs]},
  eprint = {1710.06963},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes "large step" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,diff-privacy},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DSCR9W72\\McMahan et al_2018_Learning Differentially Private Recurrent Language Models.pdf;C\:\\Users\\w-32\\Zotero\\storage\\A55LD6UK\\1710.html}
}

@article{menschDifferentiableDynamicProgramming2018,
  title = {Differentiable {{Dynamic Programming}} for {{Structured Prediction}} and {{Attention}}},
  author = {Mensch, Arthur and Blondel, Mathieu},
  year = {2018},
  month = jul,
  pages = {3459--3468},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UJYRSB9C\\Mensch, Blondel - 2018 - Differentiable Dynamic Programming for Structured Prediction and Attention.pdf}
}

@article{merityAnalysisNeuralLanguage2018,
  title = {An {{Analysis}} of {{Neural Language Modeling}} at {{Multiple Scales}}},
  author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  year = {2018},
  month = mar,
  abstract = {Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.},
  keywords = {nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\X6WBS5NM\\Merity, Keskar, Socher - 2018 - An Analysis of Neural Language Modeling at Multiple Scales(2).pdf}
}

@article{merityRegularizingOptimizingLSTM2017,
  title = {Regularizing and {{Optimizing LSTM Language Models}}},
  author = {Merity, Stephen and Shirish Keskar, Nitish and Socher, Richard},
  year = {2017},
  abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regulariza-tion. Further, we introduce NT-ASGD, a vari-ant of the averaged stochastic gradient method, wherein the averaging trigger is determined us-ing a non-monotonic condition as opposed to be-ing tuned by the user. Using these and other reg-ularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In ex-ploring the effectiveness of a neural cache in con-junction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
  keywords = {LSTM,nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4TXJXILD\\Merity, Shirish Keskar, Socher - 2017 - Regularizing and Optimizing LSTM Language Models(2).pdf}
}

@article{meritySingleHeadedAttention2019,
  title = {Single {{Headed Attention RNN}}: {{Stop Thinking With Your Head}}},
  shorttitle = {Single {{Headed Attention RNN}}},
  author = {Merity, Stephen},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.11423 [cs]},
  eprint = {1911.11423},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,lstm,rnn,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\S7R3EHVV\\Merity - 2019 - Single Headed Attention RNN Stop Thinking With Yo.pdf;C\:\\Users\\w-32\\Zotero\\storage\\DM4WBTRW\\1911.html}
}

@inproceedings{merlinPracticalRecommendationsReplayBased2022,
  ids = {merlinPracticalRecommendationsReplaybased2022},
  title = {Practical {{Recommendations}} for~{{Replay-Based Continual Learning Methods}}},
  booktitle = {Image {{Analysis}} and {{Processing}}. {{ICIAP}} 2022 {{Workshops}}},
  author = {Merlin, Gabriele and Lomonaco, Vincenzo and Cossu, Andrea and Carta, Antonio and Bacciu, Davide},
  editor = {Mazzeo, Pier Luigi and Frontoni, Emanuele and Sclaroff, Stan and Distante, Cosimo},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  number = {arXiv:2203.10317},
  eprint = {2203.10317},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {548--559},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-13324-4_47},
  abstract = {Continual Learning requires the model to learn from a stream of dynamic, non-stationary data without forgetting previous knowledge. Several approaches have been developed in the literature to tackle the Continual Learning challenge. Among them, Replay approaches have empirically proved to be the most effective ones [16]. Replay operates by saving some samples in memory which are then used to rehearse knowledge during training in subsequent tasks. However, an extensive comparison and deeper understanding of different replay implementation subtleties is still missing in the literature. The aim of this work is to compare and analyze existing replay-based strategies and provide practical recommendations on developing efficient, effective and generally applicable replay-based strategies. In particular, we investigate the role of the memory size value, different weighting policies and discuss about the impact of data augmentation, which allows reaching better performance with lower memory sizes.},
  archiveprefix = {arXiv},
  isbn = {978-3-031-13324-4},
  langid = {english},
  keywords = {cl-replay,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EII4MJYZ\\2203.html}
}

@book{metzPracticalObjectorientedDesign2019,
  title = {Practical Object-Oriented Design: An Agile Primer Using {{Ruby}}},
  shorttitle = {Practical Object-Oriented Design},
  author = {Metz, Sandi},
  year = {2019},
  edition = {Second edition},
  publisher = {{Addison-Wesley}},
  address = {{Boston}},
  abstract = {Object-oriented programming languages exist to help you create beautiful, straightforward applications that are easy to change and simple to extend. Unfortunately, the world is awash with object-oriented (OO) applications that are difficult to understand and expensive to change. Practical Object-Oriented Design, Second Edition, immerses you in an OO mindset and teaches you powerful, real-world, object-oriented design techniques with simple and practical examples. Sandi Metz demonstrates how to build new applications that can "survive success" and repair existing applications that have become impossible to change},
  isbn = {978-0-13-445647-8},
  langid = {english},
  lccn = {QA76.64 .M484 2019},
  keywords = {Object-oriented programming (Computer science),Ruby (Computer program language)},
  annotation = {OCLC: ocn951926800},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZHHRSFES\\Metz - 2019 - Practical object-oriented design an agile primer .pdf}
}

@inproceedings{mhammediEfficientOrthogonalParametrisation2017,
  title = {Efficient {{Orthogonal Parametrisation}} of {{Recurrent Neural Networks Using Householder Reflections}}},
  booktitle = {{{ICML}}},
  author = {Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
  year = {2017},
  month = dec,
  pages = {2401--2409},
  abstract = {The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations.},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TZB4NS7Y\\Mhammedi et al. - 2017 - Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections(2).pdf}
}

@article{micheliNeuralNetworkGraphs2009,
  title = {Neural Network for Graphs: {{A}} Contextual Constructive Approach},
  author = {Micheli, Alessio},
  year = {2009},
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {3},
  pages = {498--511},
  doi = {10.1109/TNN.2008.2010350},
  abstract = {This paper presents a new approach for learning in structured domains (SDs) using a constructive neural network for graphs (NN4G). The new model allows the extension of the input domain for supervised neural networks to a general class of graphs including both acyclic/cyclic, directed/undirected labeled graphs. In particular, the model can realize adaptive contextual transductions, learning the mapping from graphs for both classification and regression tasks. In contrast to previous neural networks for structures that had a recursive dynamics, NN4G is based on a constructive feedforward architecture with state variables that uses neurons with no feedback connections. The neurons are applied to the input graphs by a general traversal process that relaxes the constraints of previous approaches derived by the causality assumption over hierarchical input data. Moreover, the incremental approach eliminates the need to introduce cyclic dependencies in the definition of the system state variables. In the traversal process, the NN4G units exploit (local) contextual information of the graphs vertices. In spite of the simplicity of the approach, we show that, through the compositionality of the contextual information developed by the learning, the model can deal with contextual information that is incrementally extended according to the graphs topology. The effectiveness and the generality of the new approach are investigated by analyzing its theoretical properties and providing experimental results.},
  keywords = {cascade correlation,DGN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9MRXK2C8\\Micheli - 2009 - Neural network for graphs A contextual constructive approach.pdf}
}

@article{miconiDifferentiablePlasticityTraining2018,
  title = {Differentiable Plasticity: Training Plastic Neural Networks with Backpropagation},
  author = {Miconi, Thomas and Clune, Jeff and Stanley, Kenneth O.},
  year = {2018},
  month = apr,
  issn = {1804.02464v3},
  abstract = {How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains: synaptic plasticity, carefully tuned by evolution to produce efficient lifelong learning. We show that plasticity, just like connection weights, can be optimized by gradient descent in large (millions of parameters) recurrent networks with Hebbian plastic connections. First, recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel, high-dimensional 1000+ pixels natural images not seen during training. Crucially, traditional non-plastic recurrent networks fail to solve this task. Furthermore, trained plastic networks can also solve generic meta-learning tasks such as the Omniglot task, with competitive results and little parameter overhead. Finally, in reinforcement learning settings, plastic networks outperform a non-plastic equivalent in a maze exploration task. We conclude that differentiable plasticity may provide a powerful novel approach to the learning-to-learn problem.},
  keywords = {continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8KN9QVP3\\Miconi, Clune, Stanley - 2018 - Differentiable plasticity training plastic neural networks with backpropagation(3).pdf}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year = {2013},
  pages = {3111--3119},
  keywords = {nlp,nlp-embeddings}
}

@article{mikolovEfficientEstimationWord2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  journal = {arXiv preprint arXiv:1301.3781},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2F3D66DH\\word2vec.pdf}
}

@article{mikolovStatisticalLanguageModels2012,
  title = {Statistical Language Models Based on Neural Networks},
  author = {Mikolov, Tom{\'a}{\v s}},
  year = {2012},
  journal = {Presentation at Google, Mountain View, 2nd April},
  keywords = {nlp}
}

@book{millingtonArtificialIntelligenceGames2006,
  title = {Artificial Intelligence for Games},
  author = {Millington, Ian},
  year = {2006},
  series = {The {{Morgan Kaufmann}} Series in Interactive {{3D}} Technology},
  publisher = {{Elsevier}},
  address = {{Amsterdam ; Boston : Morgan Kaufmann}},
  isbn = {978-0-12-497782-2 978-0-12-373661-1},
  langid = {english},
  lccn = {QA76.76.C672 M549 2006},
  annotation = {OCLC: ocm71264730},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KRCZ6G8V\\Millington - 2006 - Artificial intelligence for games.pdf}
}

@article{mirmanDifferentiableAbstractInterpretation2018,
  title = {Differentiable {{Abstract Interpretation}} for {{Provably Robust Neural Networks}}},
  author = {Mirman, Matthew and Gehr, Timon and Vechev, Martin},
  year = {2018},
  number = {Section 6},
  keywords = {adversarial-examples},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FWIMHPAW\\Mirman, Gehr, Vechev - 2018 - Differentiable Abstract Interpretation for Provably Robust Neural Networks(2).pdf}
}

@article{mirzadehArchitectureMattersContinual2022,
  title = {Architecture {{Matters}} in {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Chaudhry, Arslan and Yin, Dong and Nguyen, Timothy and Pascanu, Razvan and Gorur, Dilan and Farajtabar, Mehrdad},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.00275 [cs]},
  eprint = {2202.00275},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A large body of research in continual learning is devoted to overcoming the catastrophic forgetting of neural networks by designing new algorithms that are robust to the distribution shifts. However, the majority of these works are strictly focused on the "algorithmic" part of continual learning for a "fixed neural network architecture", and the implications of using different architectures are mostly neglected. Even the few existing continual learning methods that modify the model assume a fixed architecture and aim to develop an algorithm that efficiently uses the model throughout the learning experience. However, in this work, we show that the choice of architecture can significantly impact the continual learning performance, and different architectures lead to different trade-offs between the ability to remember previous tasks and learning new ones. Moreover, we study the impact of various architectural decisions, and our findings entail best practices and recommendations that can improve the continual learning performance.},
  archiveprefix = {arXiv},
  keywords = {cl-architectural,continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Mirzadeh et al_2022_Architecture Matters in Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\JJ6Q3GFZ\\2202.html}
}

@article{mirzadehLinearModeConnectivity2020,
  title = {Linear {{Mode Connectivity}} in {{Multitask}} and {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Gorur, Dilan and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.04495 [cs]},
  eprint = {2010.04495},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,mode-connectivity},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3EZECTSH\\Mirzadeh et al_2020_Linear Mode Connectivity in Multitask and Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\YXSSFGWX\\2010.html}
}

@misc{mirzadehUnderstandingRoleTraining2020,
  title = {Understanding the {{Role}} of {{Training Regimes}} in {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = {2020},
  month = jun,
  number = {arXiv:2006.06958},
  eprint = {2006.06958},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.06958},
  abstract = {Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks. This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with different degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes -- learning rate, batch size, regularization method-- can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks' local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.},
  archiveprefix = {arXiv},
  keywords = {cl-regularization,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,continual,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\68DFFQFR\\2006.html}
}

@article{mirzadehWideNeuralNetworks2021,
  title = {Wide {{Neural Networks Forget Less Catastrophically}}},
  author = {Mirzadeh, Seyed Iman and Chaudhry, Arslan and Hu, Huiyi and Pascanu, Razvan and Gorur, Dilan and Farajtabar, Mehrdad},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.11526 [cs]},
  eprint = {2110.11526},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A growing body of research in continual learning is devoted to overcoming the "Catastrophic Forgetting" of neural networks by designing new algorithms that are more robust to the distribution shifts. While the recent progress in continual learning literature is encouraging, our understanding of what properties of neural networks contribute to catastrophic forgetting is still limited. To address this, instead of focusing on continual learning algorithms, in this work, we focus on the model itself and study the impact of "width" of the neural network architecture on catastrophic forgetting, and show that width has a surprisingly significant effect on forgetting. To explain this effect, we study the learning dynamics of the network from various perspectives such as gradient norm and sparsity, orthogonalization, and lazy training regime. We provide potential explanations that are consistent with the empirical results across different architectures and continual learning benchmarks.},
  archiveprefix = {arXiv},
  keywords = {cl-architectural,continual,sparsity},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Mirzadeh et al_2021_Wide Neural Networks Forget Less Catastrophically.pdf;C\:\\Users\\w-32\\Zotero\\storage\\TVJI28YA\\2110.html}
}

@book{mitchellMachineLearning1997,
  ids = {mitchellMachineLearning1997a},
  title = {Machine {{Learning}}},
  author = {Mitchell, Thomas M.},
  year = {1997},
  edition = {First},
  publisher = {{McGraw-Hill, Inc.}},
  address = {{New York, NY, USA}},
  isbn = {0-07-042807-7 978-0-07-042807-2}
}

@misc{mitchellMemoryBasedModelEditing2022,
  title = {Memory-{{Based Model Editing}} at {{Scale}}},
  author = {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D. and Finn, Chelsea},
  year = {2022},
  month = jun,
  number = {arXiv:2206.06520},
  eprint = {2206.06520},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.06520},
  abstract = {Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model's predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.},
  archiveprefix = {arXiv},
  keywords = {fairness},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VIRCTWBZ\\2206.html}
}

@inproceedings{mitchellNeverEndingLearning2015,
  title = {Never {{Ending Learning}}.},
  booktitle = {{{AAAI}}},
  author = {Mitchell, Tom M. and Cohen, William W. and Hruschka Jr, Estevam R. and Talukdar, Partha Pratim and Betteridge, Justin and Carlson, Andrew and Mishra, Bhavana Dalvi and Gardner, Matthew and Kisiel, Bryan and Krishnamurthy, Jayant and others},
  year = {2015},
  pages = {2302--2310},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KVDG9R74\\NELL_aaai15.pdf}
}

@article{mittalEssentialsClassIncremental,
  title = {Essentials for {{Class Incremental Learning}}},
  author = {Mittal, Sudhanshu and Galesso, Silvio and Brox, Thomas},
  pages = {10},
  abstract = {Contemporary neural networks are limited in their ability to learn from evolving streams of training data. When trained sequentially on new or evolving tasks, their accuracy drops sharply, making them unsuitable for many real-world applications. In this work, we shed light on the causes of this well known yet unsolved phenomenon \textendash often referred to as catastrophic forgetting \textendash{} in a classincremental setup. We show that a combination of simple components and a loss that balances intra-task and intertask learning can already resolve forgetting to the same extent as more complex measures proposed in literature. Moreover, we identify poor quality of the learned representation as another reason for catastrophic forgetting in classIL. We show that performance is correlated with secondary class information (dark knowledge) learned by the model and it can be improved by an appropriate regularizer. With these lessons learned, class-incremental learning results on CIFAR-100 and ImageNet improve over the state-of-the-art by a large margin, while keeping the approach simple.},
  langid = {english},
  keywords = {class-incremental,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6Z3EFPXH\\Mittal et al. - Essentials for Class Incremental Learning.pdf}
}

@article{mittalLearningCombineTopDown2020,
  title = {Learning to {{Combine Top-Down}} and {{Bottom-Up Signals}} in {{Recurrent Neural Networks}} with {{Attention}} over {{Modules}}},
  author = {Mittal, Sarthak and Lamb, Alex and Goyal, Anirudh and Voleti, Vikram and Shanahan, Murray and Lajoie, Guillaume and Mozer, Michael and Bengio, Yoshua},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.16981 [cs, stat]},
  eprint = {2006.16981},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Robust perception relies on both bottom-up and top-down signals. Bottom-up signals consist of what's directly observed through sensation. Top-down signals consist of beliefs and expectations based on past experience and short-term memory, such as how the phrase `peanut butter and\textasciitilde...' will be completed. The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow. We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention. Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data. We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \textbackslash emph\{bidirectional\} information flow can improve results over strong baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,RNN,sparse-attention,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FEHN2UFD\\Mittal et al_2020_Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\4YUK3KKU\\2006.html}
}

@article{mnihPlayingAtariDeep2013,
  title = {Playing Atari with Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.5602},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {RL},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\R5JN2UUC\\Playing Atari with Deep Reinforcement Learning.pdf}
}

@misc{ModelingLongShortTerm,
  title = {Modeling {{Long-}} and {{Short-Term Temporal Patterns}} with {{Deep Neural Networks}} | {{The}} 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3209978.3210006},
  langid = {english},
  keywords = {CNN,RNN,time-series},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LQYK3IWV\\Modeling Long- and Short-Term Temporal Patterns wi.pdf;C\:\\Users\\w-32\\Zotero\\storage\\2AVTDPGU\\3209978.html}
}

@techreport{MogrifierLSTM,
  title = {Mogrifier {{LSTM}}},
  author = {{{$\spot$} {$\spot$} {$\spot$} {$\spot$}}},
  abstract = {Figure 1: Mogrifier with 5 rounds of updates. The previous state h 0 = hprev is transformed linearly (dashed arrows), fed through a sigmoid and gates x -1 = x in an elementwise manner producing x 1. Conversely, the linearly transformed x 1 gates h 0 and produces h 2. After a number of repetitions of this mutual gating cycle, the last values of h * and x * sequences are fed to an LSTM cell. The prev subscript of h is omitted to reduce clutter. this line of research are the Multiplicative Integration LSTM (Wu et al. 2016) and-closest to our model in the literature-the Multiplicative LSTM (Krause et al. 2016). The results in Section 3.4 demonstrate the utility of our approach which consistently improves on the LSTM and establishes a new state of the art on all but the largest dataset, Enwik8, where we match similarly sized transformer models. 2 MODEL To allow for ease of subsequent extension, we present the standard LSTM update (Sak et al. 2014) with input and state of size m and n respectively as the following function: LSTM : R m \texttimes{} R n \texttimes{} R n \textrightarrow{} R n \texttimes{} R n LSTM(x, c prev , h prev) = (c, h) The updated state c and the output h are computed as follows: f = {$\sigma$}(W f x x + W f h h prev + b f) i = {$\sigma$}(W ix x + W ih h prev + b i) j = tanh(W jx x + W jh h prev + b j) o = {$\sigma$}(W ox x + W oh h prev + b o) c = f c prev + i j h = o tanh(c) where {$\sigma$} is the logistic sigmoid function, is the elementwise product, W * * and b * are weight matrices and biases. While the LSTM is typically presented as a solution to the vanishing gradients problem, its gate i can also be interpreted as scaling the rows of weight matrices W j * (ignoring the non-linearity in j). In this sense, the LSTM nudges Elman Networks towards context-dependent transitions and the extreme case of Input Switched Affine Networks. If we took another, larger step towards that extreme we could end up with Hypernetworks (Ha et al. 2016). Here, instead, we take a more cautious step and equip the LSTM with gates that scale the columns of all its weight matrices W * * in a context-dependent manner. The scaling of the matrices W * x (those that transform the cell input) makes the input embeddings dependent on the cell state, while the scaling of W * h does the reverse. The Mogrifier 1 LSTM is an LSTM where two inputs x and h prev modulate one another in an alternating fashion before the usual LSTM computation takes place (see Fig. 1). That is, Mogrify(x, c prev , h prev) = LSTM(x {$\uparrow$} , c prev , h {$\uparrow$} prev) where the modulated inputs x {$\uparrow$} and h {$\uparrow$} prev are defined as the highest indexed x i and h i prev , respectively, from the interleaved sequences: x i = 2{$\sigma$}(Q i h i-1 prev) x i-2 , for odd i {$\in$} [1. .. r] (1) 1 It's like a transmogrifier 2 without the magic: it can only shrink or expand objects. 2 Transmogrify (verb, 1650s): to completely alter the form of something in a surprising or magical manner. 2},
  keywords = {ICLR,LSTM,RNN,under-review},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DLW3LYCL\\    - Unknown - Under review as a conference paper at ICLR 2020 x 1 h 0 x-1 x 3 x 5  h 2 h 4 LSTM(2).pdf}
}

@inproceedings{mohammadiCollaborativeLearningShared2019,
  title = {Collaborative {{Learning Through Shared Collective Knowledge}} and {{Local Expertise}}},
  booktitle = {2019 {{IEEE}} 29th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Mohammadi, Javad and Kolouri, Soheil},
  year = {2019},
  month = oct,
  pages = {1--6},
  issn = {1551-2541},
  doi = {10.1109/MLSP.2019.8918888},
  abstract = {Collaborative lifelong learning for collective knowledge acquisition has recently attracted plethora of attention from various societies. Life long learning is rooted in continuous learning over a series of consecutive tasks. The learning tasks can be carried out by a single agent with access to a complete set of information or collaborative agents with restricted access to partial data sets. Existing lifelong learning methods are mostly based on centralized learning structure, which may limit applicability in most real settings. In this paper, we introduce a novel collaborative learning approach that relies on shared collective knowledge while preserving local expertise. In this collaborative lifelong knowledge acquisition each agent obtains agent-specific know-how, i.e., through learning from agent-specific tasks and has access to global knowledge accumulated by other agents dealing with similar tasks. The structure of our proposed solution ensures agreement among neighboring agents on a shared collective knowledge while maintaining a local expertise knowledge base. We tested our algorithm on several benchmark data sets.},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\I2UFDY6D\\8918888.html}
}

@misc{moraKnowledgeDistillationFederated2022,
  title = {Knowledge {{Distillation}} for {{Federated Learning}}: A {{Practical Guide}}},
  shorttitle = {Knowledge {{Distillation}} for {{Federated Learning}}},
  author = {Mora, Alessio and Tenison, Irene and Bellavista, Paolo and Rish, Irina},
  year = {2022},
  month = nov,
  number = {arXiv:2211.04742},
  eprint = {2211.04742},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.04742},
  abstract = {Federated Learning (FL) enables the training of Deep Learning models without centrally collecting possibly sensitive raw data. This paves the way for stronger privacy guarantees when building predictive models. The most used algorithms for FL are parameter-averaging based schemes (e.g., Federated Averaging) that, however, have well known limits: (i) Clients must implement the same model architecture; (ii) Transmitting model weights and model updates implies high communication cost, which scales up with the number of model parameters; (iii) In presence of non-IID data distributions, parameter-averaging aggregation schemes perform poorly due to client model drifts. Federated adaptations of regular Knowledge Distillation (KD) can solve and/or mitigate the weaknesses of parameter-averaging FL algorithms while possibly introducing other trade-offs. In this article, we provide a review of KD-based algorithms tailored for specific FL issues.},
  archiveprefix = {arXiv},
  keywords = {federated,knowledge-distillation,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Z68GH2Y4\\2211.html}
}

@article{moravcikDeepStackExpertLevelArtificial2017,
  title = {{{DeepStack}}: {{Expert-Level Artificial Intelligence}} in {{No-Limit Poker}}},
  author = {Morav{\v c}{\'i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\'y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  year = {2017},
  month = jan,
  doi = {10.1126/science.aam6960},
  abstract = {Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches.},
  keywords = {RL},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CA8NJSG9\\Moravk et al. - 2017 - DeepStack Expert-Level Artificial Intelligence in No-Limit Poker(2).pdf}
}

@article{morcosInsightsRepresentationalSimilarity2018,
  title = {Insights on Representational Similarity in Neural Networks with Canonical Correlation},
  author = {Morcos, Ari S. and Raghu, Maithra and Bengio, Samy},
  year = {2018},
  month = oct,
  journal = {arXiv:1806.05759 [cs, stat]},
  eprint = {1806.05759},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method (Raghu et al., 2017). We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
  archiveprefix = {arXiv},
  keywords = {learn-dynamics,visualization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZXHX24A7\\Morcos et al_2018_Insights on representational similarity in neural networks with canonical.pdf;C\:\\Users\\w-32\\Zotero\\storage\\XU4C6NN2\\1806.html}
}

@article{moreno-torresUnifyingViewDataset2012,
  title = {A Unifying View on Dataset Shift in Classification},
  author = {{Moreno-Torres}, Jose G. and Raeder, Troy and {Alaiz-Rodr{\'i}guez}, Roc{\'i}o and Chawla, Nitesh V. and Herrera, Francisco},
  year = {2012},
  month = jan,
  journal = {Pattern Recognition},
  volume = {45},
  number = {1},
  pages = {521--530},
  issn = {00313203},
  doi = {10.1016/j.patcog.2011.06.019},
  abstract = {The field of dataset shift has received a growing amount of interest in the last few years. The fact that most real-world applications have to cope with some form of shift makes its study highly relevant. The literature on the topic is mostly scattered, and different authors use different names to refer to the same concepts, or use the same name for different concepts. With this work, we attempt to present a unifying framework through the review and comparison of some of the most important works in the literature.},
  langid = {english},
  keywords = {dataset-shift},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\V8EIESBE\\Moreno-Torres et al. - 2012 - A unifying view on dataset shift in classification.pdf}
}

@article{moritzRayDistributedFramework2017,
  title = {Ray: {{A Distributed Framework}} for {{Emerging AI Applications}}},
  author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Paul, William and Jordan, Michael I. and Stoica, Ion},
  year = {2017},
  abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a dynamic task graph computation model that supports both the task-parallel and the actor programming models. To meet the performance requirements of AI applications, we propose an architecture that logically centralizes the system's control state using a sharded storage system and a novel bottom-up distributed scheduler. In our experiments, we demonstrate sub-millisecond remote task latencies and linear throughput scaling beyond 1.8 million tasks per second. We empirically validate that Ray speeds up challenging benchmarks and serves as both a natural and performant fit for an emerging class of reinforcement learning applications and algorithms.},
  keywords = {software},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SBWBW7YG\\Moritz et al. - 2017 - Ray A Distributed Framework for Emerging AI Applications.pdf}
}

@article{muellerSiameseRecurrentArchitectures,
  title = {Siamese {{Recurrent Architectures}} for {{Learning Sentence Similarity}}},
  author = {Mueller, Jonas and Thyagarajan, Aditya},
  pages = {7},
  abstract = {We present a siamese adaptation of the Long Short-Term Memory (LSTM) network for labeled data comprised of pairs of variable-length sequences. Our model is applied to assess semantic similarity between sentences, where we exceed state of the art, outperforming carefully handcrafted features and recently proposed neural network systems of greater complexity. For these applications, we provide wordembedding vectors supplemented with synonymic information to the LSTMs, which use a fixed size vector to encode the underlying meaning expressed in a sentence (irrespective of the particular wording/syntax). By restricting subsequent operations to rely on a simple Manhattan metric, we compel the sentence representations learned by our model to form a highly structured space whose geometry reflects complex semantic relationships. Our results are the latest in a line of findings that showcase LSTMs as powerful language models capable of tasks requiring intricate understanding.},
  langid = {english},
  keywords = {entailment,lstm,sent-emb,sick},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZAIDF7DF\\Mueller and Thyagarajan - Siamese Recurrent Architectures for Learning Sente.pdf}
}

@article{mundtCLEVACompassContinualLearning2021,
  title = {{{CLEVA-Compass}}: {{A Continual Learning EValuation Assessment Compass}} to {{Promote Research Transparency}} and {{Comparability}}},
  shorttitle = {{{CLEVA-Compass}}},
  author = {Mundt, Martin and Lang, Steven and Delfosse, Quentin and Kersting, Kristian},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.03331 [cs]},
  eprint = {2110.03331},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation. The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and evaluation protocols being too narrow, resulting in several attempts at constructing guidelines in favor of specific desiderata or arguing against the validity of prevalent assumptions. In this work, we depart from this mindset and argue that the goal of a precise formulation of desiderata is an ill-posed one, as diverse applications may always warrant distinct scenarios. Instead, we introduce the Continual Learning EValuation Assessment Compass, CLEVA-Compass for short. The compass provides the visual means to both identify how approaches are practically reported and how works can simultaneously be contextualized in the broader literature landscape. In addition to promoting compact specification in the spirit of recent replication trends, the CLEVA-Compass thus provides an intuitive chart to understand the priorities of individual systems, where they resemble each other, and what elements are missing towards a fair comparison.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,continual,metrics},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Mundt et al_2021_CLEVA-Compass.pdf;C\:\\Users\\w-32\\Zotero\\storage\\5XGIWFNP\\2110.html}
}

@inproceedings{mundtNeuralArchitectureSearch2021,
  title = {Neural {{Architecture Search}} of {{Deep Priors}}: {{Towards Continual Learning Without Catastrophic Interference}}},
  shorttitle = {Neural {{Architecture Search}} of {{Deep Priors}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mundt, Martin and Pliushch, Iuliia and Ramesh, Visvanathan},
  year = {2021},
  pages = {3523--3532},
  langid = {english},
  keywords = {continual,neural-arch-search,random-net},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Mundt et al_2021_Neural Architecture Search of Deep Priors.pdf;C\:\\Users\\w-32\\Zotero\\storage\\6ZGUR5XQ\\Mundt_Neural_Architecture_Search_of_Deep_Priors_Towards_Continual_Learning_Without_CVPRW_2021_p.html}
}

@article{mundtUnifiedProbabilisticDeep2022,
  title = {Unified {{Probabilistic Deep Continual Learning}} through {{Generative Replay}} and {{Open Set Recognition}}},
  author = {Mundt, Martin and Pliushch, Iuliia and Majumder, Sagnik and Hong, Yongwon and Ramesh, Visvanathan},
  year = {2022},
  month = mar,
  journal = {Journal of Imaging},
  volume = {8},
  number = {4},
  eprint = {1905.12019},
  eprinttype = {arxiv},
  pages = {93},
  issn = {2313-433X},
  doi = {10.3390/jimaging8040093},
  abstract = {Modern deep neural networks are well known to be brittle in the face of unknown data instances and recognition of the latter remains a challenge. Although it is inevitable for continual-learning systems to encounter such unseen concepts, the corresponding literature appears to nonetheless focus primarily on alleviating catastrophic interference with learned representations. In this work, we introduce a probabilistic approach that connects these perspectives based on variational inference in a single deep autoencoder model. Specifically, we propose to bound the approximate posterior by fitting regions of high density on the basis of correctly classified data points. These bounds are shown to serve a dual purpose: unseen unknown out-of-distribution data can be distinguished from already trained known tasks towards robust application. Simultaneously, to retain already acquired knowledge, a generative replay process can be narrowed to strictly in-distribution samples, in order to significantly alleviate catastrophic interference.},
  archiveprefix = {arXiv},
  keywords = {continual,generative-replay,notag,ood-detection,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VIS599Y6\\1905.html}
}

@misc{mundtWholisticViewContinual2020,
  title = {A {{Wholistic View}} of {{Continual Learning}} with {{Deep Neural Networks}}: {{Forgotten Lessons}} and the {{Bridge}} to {{Active}} and {{Open World Learning}}},
  shorttitle = {A {{Wholistic View}} of {{Continual Learning}} with {{Deep Neural Networks}}},
  author = {Mundt, Martin and Hong, Yong Won and Pliushch, Iuliia and Ramesh, Visvanathan},
  year = {2020},
  month = sep,
  number = {arXiv:2009.01797},
  eprint = {2009.01797},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2009.01797},
  abstract = {Current deep learning research is dominated by benchmark evaluation. A method is regarded as favorable if it empirically performs well on the dedicated test set. This mentality is seamlessly reflected in the resurfacing area of continual learning, where consecutively arriving sets of benchmark data are investigated. The core challenge is framed as protecting previously acquired representations from being catastrophically forgotten due to the iterative parameter updates. However, comparison of individual methods is nevertheless treated in isolation from real world application and typically judged by monitoring accumulated test set performance. The closed world assumption remains predominant. It is assumed that during deployment a model is guaranteed to encounter data that stems from the same distribution as used for training. This poses a massive challenge as neural networks are well known to provide overconfident false predictions on unknown instances and break down in the face of corrupted data. In this work we argue that notable lessons from open set recognition, the identification of statistically deviating data outside of the observed dataset, and the adjacent field of active learning, where data is incrementally queried such that the expected performance gain is maximized, are frequently overlooked in the deep learning era. Based on these forgotten lessons, we propose a consolidated view to bridge continual learning, active learning and open set recognition in deep neural networks. Our results show that this not only benefits each individual paradigm, but highlights the natural synergies in a common framework. We empirically demonstrate improvements when alleviating catastrophic forgetting, querying data in active learning, selecting task orders, while exhibiting robust open world application where previously proposed methods fail.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,ood-detection,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CN38HSIQ\\Mundt et al_2020_A Wholistic View of Continual Learning with Deep Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\6LK9LLWR\\2009.html}
}

@article{munkhdalaiMetaNetworks2017,
  title = {Meta {{Networks}}},
  author = {Munkhdalai, Tsendsuren and Yu, Hong},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.00837 [cs, stat]},
  eprint = {1703.00837},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6\% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,meta-learn,notag,Statistics - Machine Learning,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\29V3LKD7\\Munkhdalai_Yu_2017_Meta Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZYA9F25J\\1703.html}
}

@article{nalisnickDeepGenerativeModels2019,
  title = {Do {{Deep Generative Models Know What They Don}}'t {{Know}}?},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  year = {2019},
  month = feb,
  journal = {arXiv:1810.09136 [cs, stat]},
  eprint = {1810.09136},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,generative,ood-detection,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\83MD623D\\1810.html}
}

@article{nashRiverFlowForecasting1970,
  title = {River Flow Forecasting through Conceptual Models Part {{I}} \textemdash{} {{A}} Discussion of Principles},
  author = {Nash, J. E. and Sutcliffe, J. V.},
  year = {1970},
  month = apr,
  journal = {Journal of Hydrology},
  volume = {10},
  number = {3},
  pages = {282--290},
  issn = {0022-1694},
  doi = {10.1016/0022-1694(70)90255-6},
  abstract = {The principles governing the application of the conceptual model technique to river flow forecasting are discussed. The necessity for a systematic approach to the development and testing of the model is explained and some preliminary ideas suggested.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HYE2CF26\\0022169470902556.html}
}

@article{nassarTREESTRUCTUREDRECURRENTSWITCHING2019,
  title = {{{TREE-STRUCTURED RECURRENT SWITCHING LINEAR DYNAMICAL SYSTEMS FOR MULTI-SCALE MODELING}}},
  author = {Nassar, Josue and Linderman, Scott W and Bugallo, M{\'o}nica F},
  year = {2019},
  journal = {ICLR},
  pages = {17},
  abstract = {Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models1. Our probabilistic model achieves this multi-scale property through a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using P\'olya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling. Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability.},
  langid = {english},
  keywords = {Hierarchical RNN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KA6PEWIQ\\Nassar et al. - 2019 - TREE-STRUCTURED RECURRENT SWITCHING LINEAR DYNAMIC.pdf}
}

@article{nayakMiningDataImpressions2021,
  title = {Mining {{Data Impressions}} from {{Deep Models}} as {{Substitute}} for the {{Unavailable Training Data}}},
  author = {Nayak, Gaurav Kumar and Mopuri, Konda Reddy and Jain, Saksham and Chakraborty, Anirban},
  year = {2021},
  month = aug,
  journal = {arXiv:2101.06069 [cs, stat]},
  eprint = {2101.06069},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Pretrained deep models hold their learnt knowledge in the form of model parameters. These parameters act as "memory" for the trained models and help them generalize well on unseen data. However, in absence of training data, the utility of a trained model is merely limited to either inference or better initialization towards a target task. In this paper, we go further and extract synthetic data by leveraging the learnt model parameters. We dub them "Data Impressions", which act as proxy to the training data and can be used to realize a variety of tasks. These are useful in scenarios where only the pretrained models are available and the training data is not shared (e.g., due to privacy or sensitivity concerns). We show the applicability of data impressions in solving several computer vision tasks such as unsupervised domain adaptation, continual learning as well as knowledge distillation. We also study the adversarial robustness of lightweight models trained via knowledge distillation using these data impressions. Further, we demonstrate the efficacy of data impressions in generating data-free Universal Adversarial Perturbations (UAPs) with better fooling rates. Extensive experiments performed on benchmark datasets demonstrate competitive performance achieved using data impressions in absence of original training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,data-distill,data-free,exmodel,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Nayak et al_2021_Mining Data Impressions from Deep Models as Substitute for the Unavailable.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EF9UL5LZ\\2101.html}
}

@article{nayakZeroShotKnowledgeDistillation2019,
  title = {Zero-{{Shot Knowledge Distillation}} in {{Deep Networks}}},
  author = {Nayak, Gaurav Kumar and Mopuri, Konda Reddy and Shaj, Vaisakh and Babu, R. Venkatesh and Chakraborty, Anirban},
  year = {2019},
  month = may,
  journal = {arXiv:1905.08114 [cs, stat]},
  eprint = {1905.08114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Knowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method ``ZeroShot Knowledge Distillation'' and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,data-distill,data-free,exmodel,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HFCAP26M\\Nayak et al. - 2019 - Zero-Shot Knowledge Distillation in Deep Networks.pdf}
}

@inproceedings{neilPhasedLSTMAccelerating2016,
  title = {Phased {{LSTM}}: {{Accelerating Recurrent Network Training}} for {{Long}} or {{Event-based Sequences}}},
  booktitle = {{{NIPS}}},
  author = {Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
  year = {2016},
  month = oct,
  abstract = {Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.},
  keywords = {Hierarchical RNN,LSTM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XJESXXZD\\Neil, Pfeiffer, Liu - 2016 - Phased LSTM Accelerating Recurrent Network Training for Long or Event-based Sequences(2).pdf}
}

@misc{NeuralScalingLaws,
  title = {{Neural Scaling Laws course MIla/UdeM Winter 2022 - Topics\&Papers}},
  abstract = {Here is a suggested list of topics and papers - still UNDER CONSTRUCTION. If you would like to suggest a relevant paper not in the list, please contact the instructor and/or the TAs (contact info on the course descriptions page).   Here is  paper presentation schedule \& sign up sheet},
  howpublished = {https://sites.google.com/view/nsl-course/topicspapers},
  langid = {italian},
  keywords = {continual,course,teaching},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JCK8DDL3\\topicspapers.html}
}

@inproceedings{NEURIPS2019_9d7099d8,
  ids = {kergNonnormalRecurrentNeural2019,kergNonnormalRecurrentNeural2019a},
  title = {Non-Normal {{Recurrent Neural Network}} ({{nnRNN}}): Learning Long Time Dependencies While Improving Expressivity with Transient Dynamics},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kerg, Giancarlo and Goyette, Kyle and Puelma Touzel, Maximilian and Gidel, Gauthier and Vorontsov, Eugene and Bengio, Yoshua and Lajoie, Guillaume},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  volume = {32},
  pages = {13613--13623},
  publisher = {{Curran Associates, Inc.}},
  keywords = {NIPS,orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5JM57NS4\\9513-non-normal-recurrent-neural-network-nnrnn-learning-long-time-dependencies-while-improving-expressivity-with-transient-dynamics.pdf;C\:\\Users\\w-32\\Zotero\\storage\\UENZRJL3\\Kerg et al. - 2019 - Non-normal Recurrent Neural Network (nnRNN) learning long time dependencies while improving expressivity with trans.pdf}
}

@misc{NewMethodAsynchronous,
  title = {{A new method for asynchronous federated learning}},
  abstract = {Federated learning (FL) is an important privacy-preserving method for training AI models. We believe this is the first asynchronous FL system running at scale, training a model on 100 million Android devices. Our results show that asynchronous FL is five times faster and nearly eight times more communication-efficient than existing synchronous FL. This will enable FL-trained models to adapt quickly, improving the product experience while preserving privacy.},
  howpublished = {https://ai.facebook.com/blog/asynchronous-federated-learning/},
  langid = {italian},
  keywords = {asynchronous,federated},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8HTQ3AYN\\asynchronous-federated-learning.html}
}

@misc{NewModelDataset,
  title = {{A new model and dataset for long-range memory}},
  journal = {Deepmind},
  abstract = {This blog introduces a new long-range memory model, the Compressive Transformer, alongside a new benchmark for book-level language modelling, PG19. We provide the conceptual tools needed to understand this new research in the context of recent developments in memory models and language modelling.},
  howpublished = {/blog/article/A\_new\_model\_and\_dataset\_for\_long-range\_memory},
  langid = {ALL},
  keywords = {data,long-term,nlp,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IYK3M9VC\\A_new_model_and_dataset_for_long-range_memory.html}
}

@article{nguyenDeepNeuralNetworks2015,
  title = {Deep {{Neural Networks}} Are {{Easily Fooled}}: {{High Confidence Predictions}} for {{Unrecognizable Images}}},
  shorttitle = {Deep {{Neural Networks}} Are {{Easily Fooled}}},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  year = {2015},
  month = apr,
  journal = {arXiv:1412.1897 [cs]},
  eprint = {1412.1897},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99\% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
  archiveprefix = {arXiv},
  keywords = {adversarial-examples,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,vision},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Nguyen et al_2015_Deep Neural Networks are Easily Fooled.pdf;C\:\\Users\\w-32\\Zotero\\storage\\V7SB3Y6D\\1412.html}
}

@misc{nguyenFederatedLearningBuffered2022,
  title = {Federated {{Learning}} with {{Buffered Asynchronous Aggregation}}},
  author = {Nguyen, John and Malik, Kshitiz and Zhan, Hongyuan and Yousefpour, Ashkan and Rabbat, Michael and Malek, Mani and Huba, Dzmitry},
  year = {2022},
  month = mar,
  number = {arXiv:2106.06639},
  eprint = {2106.06639},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.06639},
  abstract = {Scalability and privacy are two critical concerns for cross-device federated learning (FL) systems. In this work, we identify that synchronous FL - synchronized aggregation of client updates in FL - cannot scale efficiently beyond a few hundred clients training in parallel. It leads to diminishing returns in model performance and training speed, analogous to large-batch training. On the other hand, asynchronous aggregation of client updates in FL (i.e., asynchronous FL) alleviates the scalability issue. However, aggregating individual client updates is incompatible with Secure Aggregation, which could result in an undesirable level of privacy for the system. To address these concerns, we propose a novel buffered asynchronous aggregation method, FedBuff, that is agnostic to the choice of optimizer, and combines the best properties of synchronous and asynchronous FL. We empirically demonstrate that FedBuff is 3.3x more efficient than synchronous FL and up to 2.5x more efficient than asynchronous FL, while being compatible with privacy-preserving technologies such as Secure Aggregation and differential privacy. We provide theoretical convergence guarantees in a smooth non-convex setting. Finally, we show that under differentially private training, FedBuff can outperform FedAvgM at low privacy settings and achieve the same utility for higher privacy settings.},
  archiveprefix = {arXiv},
  keywords = {async-sgd,federated,unread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\45ID9JZ8\\2106.html}
}

@article{nguyenMomentumRNNIntegratingMomentum2020,
  title = {{{MomentumRNN}}: {{Integrating Momentum}} into {{Recurrent Neural Networks}}},
  shorttitle = {{{MomentumRNN}}},
  author = {Nguyen, Tan M. and Baraniuk, Richard G. and Bertozzi, Andrea L. and Osher, Stanley J. and Wang, Bao},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.06919 [cs, math, stat]},
  eprint = {2006.06919},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Designing deep neural networks is an art that often involves an expensive search over candidate architectures. To overcome this for recurrent neural nets (RNNs), we establish a connection between the hidden state dynamics in an RNN and gradient descent (GD). We then integrate momentum into this framework and propose a new family of RNNs, called \{\textbackslash em MomentumRNNs\}. We theoretically prove and numerically demonstrate that MomentumRNNs alleviate the vanishing gradient issue in training RNNs. We study the momentum long-short term memory (MomentumLSTM) and verify its advantages in convergence speed and accuracy over its LSTM counterpart across a variety of benchmarks, with little compromise in computational or memory efficiency. We also demonstrate that MomentumRNN is applicable to many types of recurrent cells, including those in the state-of-the-art orthogonal RNNs. Finally, we show that other advanced momentum-based optimization methods, such as Adam and Nesterov accelerated gradients with a restart, can be easily incorporated into the MomentumRNN framework for designing new recurrent cells with even better performance. The code is available at \textbackslash url\{https://github.com/minhtannguyen/MomentumRNN\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Memory,ptb,RNN,rnn-pmnist,Statistics - Machine Learning,timit},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\E9CES9NC\\Nguyen et al_2020_MomentumRNN.pdf;C\:\\Users\\w-32\\Zotero\\storage\\94JF8H38\\2006.html}
}

@article{nguyenVariationalContinualLearning2018,
  title = {Variational {{Continual Learning}}},
  author = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner, Richard E.},
  year = {2018},
  month = may,
  journal = {arXiv:1710.10628 [cs, stat]},
  eprint = {1710.10628},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.},
  archiveprefix = {arXiv},
  keywords = {continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZQS2U424\\1710.html}
}

@inproceedings{nguyenWideDeepNetworks2022,
  title = {Do {{Wide}} and {{Deep Networks Learn}} the {{Same Things}}? {{Uncovering How Neural Network Representations Vary}} with {{Width}} and {{Depth}}},
  shorttitle = {Do {{Wide}} and {{Deep Networks Learn}} the {{Same Things}}?},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
  year = {2022},
  month = feb,
  abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design...},
  langid = {english},
  keywords = {cka,net-similarity},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XBZJMBUY\\forum.html}
}

@article{nickollsScalableParallelProgramming2008,
  title = {Scalable Parallel Programming with {{CUDA}}},
  author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
  year = {2008},
  journal = {Queue},
  volume = {6},
  number = {2},
  pages = {40--53},
  keywords = {software}
}

@incollection{NIPS2018_7894,
  title = {Approximating Real-Time Recurrent Learning with Random Kronecker Factors},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Mujika, Asier and Meier, Florian and Steger, Angelika},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {6594--6603},
  publisher = {{Curran Associates, Inc.}},
  keywords = {optimization,RNN,rnn-optimization,RTRL}
}

@incollection{NIPS2018_7991,
  title = {Sparse Attentive Backtracking: {{Temporal}} Credit Assignment through Reminding},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Ke, Nan Rosemary and ALIAS PARTH GOYAL, Anirudh Goyal and Bilaniuk, Olexa and Binas, Jonathan and Mozer, Michael C and Pal, Chris and Bengio, Yoshua},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {7640--7651},
  publisher = {{Curran Associates, Inc.}},
  keywords = {attention,biologically-plausible,RNN,sparsity}
}

@techreport{NVIDIATeslaP1002016,
  title = {{{NVIDIA Tesla P100}} Whitepaper},
  year = {2016},
  keywords = {hardware}
}

@inproceedings{odenaConditionalImageSynthesis2017,
  title = {Conditional Image Synthesis with Auxiliary Classifier {{GANs}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  year = {2017},
  month = aug,
  series = {{{ICML}}'17},
  pages = {2642--2651},
  publisher = {{JMLR.org}},
  address = {{Sydney, NSW, Australia}},
  abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 x 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice as discriminable as artificially resized 32 x 32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
  keywords = {gan,gen-exml},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Odena et al_2017_Conditional image synthesis with auxiliary classifier GANs.pdf}
}

@article{ohAlphastarPreprint,
  title = {Alphastar - Preprint},
  author = {Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P and Jaderberg, Max and Vezhnevets, Alexander S and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Paine, Tom L and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Yogatama, Dani and W{\"u}nsch, Dario and Mckinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  keywords = {RL},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FRSJ4Y9X\\Oh et al. - Unknown - Alphastar - preprint.pdf}
}

@article{olandBeCarefulWhat2017,
  title = {Be {{Careful What You Backpropagate}}: {{A Case For Linear Output Activations}} \& {{Gradient Boosting}}},
  author = {Oland, Anders and Bansal, Aayush and Dannenberg, Roger B. and Raj, Bhiksha},
  year = {2017},
  month = jul,
  abstract = {In this work, we show that saturating output activation functions, such as the softmax, impede learning on a number of standard classification tasks. Moreover, we present results showing that the utility of softmax does not stem from the normalization, as some have speculated. In fact, the normalization makes things worse. Rather, the advantage is in the exponentiation of error gradients. This exponential gradient boosting is shown to speed up convergence and improve generalization. To this end, we demonstrate faster convergence and better performance on diverse classification tasks: image classification using CIFAR-10 and ImageNet, and semantic segmentation using PASCAL VOC 2012. In the latter case, using the state-of-the-art neural network architecture, the model converged 33\% faster with our method (roughly two days of training less) than with the standard softmax activation, and with a slightly better performance to boot.}
}

@inproceedings{olivaStatisticalRecurrentUnit2017,
  title = {The Statistical Recurrent Unit},
  booktitle = {34th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2017},
  author = {Oliva, Junier B. and Poczos, Barnabas and Schneider, Jeff},
  year = {2017},
  month = mar,
  volume = {6},
  pages = {4098--4107},
  abstract = {Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters in a Bayesian optimization scheme for both synthetic and real-world tasks.},
  isbn = {978-1-5108-5514-4},
  keywords = {ICML,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IQ95T7M3\\Oliva, Poczos, Schneider - 2017 - The statistical recurrent unit(3).pdf}
}

@article{oneillPlayItAgain2010,
  title = {Play It Again: Reactivation of Waking Experience and Memory},
  shorttitle = {Play It Again},
  author = {O'Neill, Joseph and {Pleydell-Bouverie}, Barty and Dupret, David and Csicsvari, Jozsef},
  year = {2010},
  month = may,
  journal = {Trends in Neurosciences},
  volume = {33},
  number = {5},
  pages = {220--229},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2010.01.006},
  abstract = {Episodic and spatial memories each involve the encoding of complex associations in hippocampal neuronal circuits. Such memory traces could be stabilised from short- to long-term forms by consolidation processes involving the `reactivation' of the original network firing patterns during sleep and rest. Waking experience can be replayed in many different brain areas, but an important role for the hippocampus lies in the organisation of the `reactivation' process. Emerging evidence suggests that sharp wave/ripple (SWR) events in the hippocampus could coordinate the reactivation of memory traces and direct their reinstatement in cortical circuits. Although the mechanisms remain uncertain, there is a growing consensus that such SWR-directed reactivation of brain-wide memory traces could underlie memory consolidation.},
  langid = {english},
  keywords = {cl-neuroscience,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\K5XN3FZ4\\S0166223610000172.html}
}

@article{oordWaveNetGenerativeModel2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.03499 [cs]},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,music,time-series},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BYWWSEH9\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf;C\:\\Users\\w-32\\Zotero\\storage\\5K5CT9UF\\1609.html}
}

@article{oreillyHippocampalNeocorticalContributions2002,
  title = {Hippocampal and Neocortical Contributions to Memory: Advances in the Complementary Learning Systems Framework},
  shorttitle = {Hippocampal and Neocortical Contributions to Memory},
  author = {O'Reilly, Randall C. and Norman, Kenneth A.},
  year = {2002},
  month = dec,
  journal = {Trends in Cognitive Sciences},
  volume = {6},
  number = {12},
  pages = {505--510},
  issn = {1879-307X},
  doi = {10.1016/s1364-6613(02)02005-3},
  abstract = {The complementary learning systems framework provides a simple set of principles, derived from converging biological, psychological and computational constraints, for understanding the differential contributions of the neocortex and hippocampus to learning and memory. The central principles are that the neocortex has a low learning rate and uses overlapping distributed representations to extract the general statistical structure of the environment, whereas the hippocampus learns rapidly using separated representations to encode the details of specific events while minimizing interference. In recent years, we have instantiated these principles in working computational models, and have used these models to address human and animal learning and memory findings, across a wide range of domains and paradigms. Here, we review a few representative applications of our models, focusing on two domains: recognition memory and animal learning in the fear-conditioning paradigm. In both domains, the models have generated novel predictions that have been tested and confirmed.},
  langid = {english},
  pmid = {12475710},
  keywords = {cl-neuroscience,continual}
}

@inproceedings{orhanImprovedMemoryRecurrent2019,
  title = {Improved Memory in Recurrent Neural Networks with Sequential Non-Normal Dynamics},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Orhan, Emin and Pitkow, Xaq},
  year = {2019},
  month = sep,
  abstract = {Training recurrent neural networks (RNNs) is a hard problem due to degeneracies in the optimization landscape, a problem also known as vanishing/exploding gradients. Short of designing new RNN...},
  keywords = {ICLR,Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HD38YP4Y\\Orhan and Pitkow - 2019 - Improved memory in recurrent neural networks with .pdf;C\:\\Users\\w-32\\Zotero\\storage\\SMSH6UU2\\forum.html}
}

@article{ororbiaConductingCreditAssignment2018,
  title = {Conducting {{Credit Assignment}} by {{Aligning Local Representations}}},
  author = {Ororbia, Alexander G. and Mali, Ankur and Kifer, Daniel and Giles, C. Lee},
  year = {2018},
  month = jul,
  journal = {arXiv:1803.01834 [cs, stat]},
  eprint = {1803.01834},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Using back-propagation and its variants to train deep networks is often problematic for new users. Issues such as exploding gradients, vanishing gradients, and high sensitivity to weight initialization strategies often make networks difficult to train, especially when users are experimenting with new architectures. Here, we present Local Representation Alignment (LRA), a training procedure that is much less sensitive to bad initializations, does not require modifications to the network architecture, and can be adapted to networks with highly nonlinear and discrete-valued activation functions. Furthermore, we show that one variation of LRA can start with a null initialization of network weights and still successfully train networks with a wide variety of nonlinearities, including tanh, ReLU-6, softplus, signum and others that may draw their inspiration from biology. A comprehensive set of experiments on MNIST and the much harder Fashion MNIST data sets show that LRA can be used to train networks robustly and effectively, succeeding even when back-propagation fails and outperforming other alternative learning algorithms, such as target propagation and feedback alignment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7R97A3P5\\Ororbia et al. - 2018 - Conducting Credit Assignment by Aligning Local Rep.pdf;C\:\\Users\\w-32\\Zotero\\storage\\XW7Y4WDB\\1803.html}
}

@article{ororbiaContinualLearningRecurrent2020,
  title = {Continual {{Learning}} of {{Recurrent Neural Networks}} by {{Locally Aligning Distributed Representations}}},
  author = {Ororbia, Alexander and Mali, Ankur and Giles, C. Lee and Kifer, Daniel},
  year = {2020},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--12},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2953622},
  abstract = {Temporal models based on recurrent neural networks have proven to be quite powerful in a wide variety of applications, including language modeling and speech processing. However, training these models often relies on backpropagation through time (BPTT), which entails unfolding the network over many time steps, making the process of conducting credit assignment considerably more challenging. Furthermore, the nature of backpropagation itself does not permit the use of nondifferentiable activation functions and is inherently sequential, making parallelization of the underlying training process difficult. Here, we propose the parallel temporal neural coding network (P-TNCN), a biologically inspired model trained by the learning algorithm we call local representation alignment. It aims to resolve the difficulties and problems that plague recurrent networks trained by BPTT. The architecture requires neither unrolling in time nor the derivatives of its internal activation functions. We compare our model and learning procedure with other BPTT alternatives (which also tend to be computationally expensive), including real-time recurrent learning, echo state networks, and unbiased online recurrent optimization. We show that it outperforms these on-sequence modeling benchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing NotMNIST, and Penn Treebank. Notably, our approach can, in some instances, outperform full BPTT as well as variants such as sparse attentive backtracking. Significantly, the hidden unit correction phase of P-TNCN allows it to adapt to new data sets even if its synaptic weights are held fixed (zero-shot adaptation) and facilitates retention of prior generative knowledge when faced with a task sequence. We present results that show the P-TNCN's ability to conduct zero-shot adaptation and online continual sequence modeling.},
  keywords = {architectural,biologically-plausible,Brain modeling,Computational modeling,Computer architecture,continual,generative models,learning algorithms,online learning,predictive coding,Predictive coding,Predictive models,recurrent neural networks (RNNs).,RNN,Training},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\P4EVNHEP\\Ororbia et al. - 2020 - Continual Learning of Recurrent Neural Networks by.pdf;C\:\\Users\\w-32\\Zotero\\storage\\JB3RX69Z\\8963851.html}
}

@article{ororbiaSpikingNeuralPredictive2020,
  title = {Spiking {{Neural Predictive Coding}} for {{Continual Learning}} from {{Data Streams}}},
  author = {Ororbia, Alexander},
  year = {2020},
  month = jan,
  journal = {arXiv:1908.08655 [cs, q-bio]},
  eprint = {1908.08655},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {For energy-efficient computation in specialized neuromorphic hardware, we present the Spiking Neural Coding Network, an instantiation of a family of artificial neural models strongly motivated by the theory of predictive coding. The model, in essence, works by operating in a never-ending process of "guess-and-check", where neurons predict the activity values of one another and then immediately adjust their own activities to make better future predictions. The interactive, iterative nature of our neural system fits well into the continuous time formulation of data sensory stream prediction and, as we show, the model's structure yields a simple, local synaptic update rule, which could be used to complement or replace online spike-timing dependent plasticity. In this article, we experiment with an instantiation of our model that consists of leaky integrate-and-fire units. However, the general framework within which our model is situated can naturally incorporate more complex, formal neurons such as the Hodgkin-Huxley model. Our experimental results in pattern recognition demonstrate the potential of the proposed model when binary spike trains are the primary paradigm for inter-neuron communication. Notably, our model is competitive in terms of classification performance, can conduct online semi-supervised learning, naturally experiences less forgetting when learning from a sequence of tasks, and is more computationally economical and biologically-plausible than popular artificial neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,continual,Quantitative Biology - Neurons and Cognition,spiking},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2NQNDNQP\\Ororbia_2020_Spiking Neural Predictive Coding for Continual Learning from Data Streams.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ULHW33KI\\1908.html}
}

@article{ostapenkoLearningRememberSynaptic2019,
  title = {Learning to {{Remember}}: {{A Synaptic Plasticity Driven Framework}} for {{Continual Learning}}},
  shorttitle = {Learning to {{Remember}}},
  author = {Ostapenko, Oleksiy and Puscas, Mihai and Klein, Tassilo and J{\"a}hnichen, Patrick and Nabi, Moin},
  year = {2019},
  month = dec,
  journal = {arXiv:1904.03137 [cs]},
  eprint = {1904.03137},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Models trained in the context of continual learning (CL) should be able to learn from a stream of data over an undefined period of time. The main challenges herein are: 1) maintaining old knowledge while simultaneously benefiting from it when learning new tasks, and 2) guaranteeing model scalability with a growing amount of data to learn from. In order to tackle these challenges, we introduce Dynamic Generative Memory (DGM) - a synaptic plasticity driven framework for continual learning. DGM relies on conditional generative adversarial networks with learnable connection plasticity realized with neural masking. Specifically, we evaluate two variants of neural masking: applied to (i) layer activations and (ii) to connection weights directly. Furthermore, we propose a dynamic network expansion mechanism that ensures sufficient model capacity to accommodate for continually incoming tasks. The amount of added capacity is determined dynamically from the learned binary mask. We evaluate DGM in the continual class-incremental setup on visual classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2W8ZBA47\\Ostapenko et al_2019_Learning to Remember.pdf;C\:\\Users\\w-32\\Zotero\\storage\\SNKGF75Q\\1904.html}
}

@inproceedings{oswaldContinualLearningHypernetworks2019,
  title = {Continual Learning with Hypernetworks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {von Oswald, Johannes and Henning, Christian and Jo\&\#xE3 and Sacramento, O and Grewe, Benjamin F.},
  year = {2019},
  month = sep,
  abstract = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned...},
  keywords = {continual,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YEAWRL63\\Oswald et al. - 2019 - Continual learning with hypernetworks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\3J3HS2DC\\forum.html}
}

@inproceedings{ozenSqueezingCorrelatedNeurons,
  title = {Squeezing {{Correlated Neurons}} for {{Resource-Efficient Deep Neural Networks}}},
  booktitle = {{{ECML}} 2020},
  author = {Ozen, Elbruz and Orailoglu, Alex},
  pages = {16},
  abstract = {DNNs are abundantly represented in real-life applications because of their accuracy in challenging problems, yet their demanding memory and computational costs challenge their applicability to resource-constrained environments. Taming computational costs has hitherto focused on first-order techniques, such as eliminating numerically insignificant neurons/filters through numerical contribution metric prioritizations, yielding passable improvements. Yet redundancy in DNNs extends well beyond the limits of numerical insignificance. Modern DNN layers exhibit a significant correlation among output activations; hence, the number of extracted orthogonal features at each layer rarely exceeds a small fraction of the layer size. The exploitation of this observation necessitates the quantification of information content at layer outputs. To this end, we employ practical data analysis techniques coupled with a novel feature elimination algorithm to identify a minimal set of computation units that capture the information content of the layer and squash the rest. Linear transformations on the subsequent layer ensure accuracy retention despite the removal of a significant portion of the computation units. The one-shot application of the outlined technique can shrink the VGG-16 model size 4.9\texttimes{} and speed up its execution by 3.4\texttimes{} with negligible accuracy loss while requiring no additional fine-tuning. The proposed approach, in addition to delivering results overwhelmingly superior to hitherto promulgated heuristics, furthermore promises to spearhead the design of more compact deep learning models through an improved understanding of DNN redundancy.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\56FBZJS2\\Ozen and Orailoglu - Squeezing Correlated Neurons for Resource-Ecient .pdf}
}

@article{paassenTreeEchoState,
  title = {Tree {{Echo State Autoencoders}} with {{Grammars}}},
  author = {Paassen, Benjamin and Koprinska, Irena and Yacef, Kalina},
  pages = {8},
  abstract = {Tree data occurs in many forms, such as computer programs, chemical molecules, or natural language. Unfortunately, the non-vectorial and discrete nature of trees makes it challenging to construct functions with tree-formed output, complicating tasks such as optimization or time series prediction. Autoencoders address this challenge by mapping trees to a vectorial latent space, where tasks are easier to solve, and then mapping the solution back to a tree structure. However, existing autoencoding approaches for tree data fail to take the specific grammatical structure of tree domains into account and rely on deep learning, thus requiring large training datasets and long training times. In this paper, we propose tree echo state autoencoders (TES-AE), which are guided by a tree grammar and can be trained within seconds by virtue of reservoir computing. In our evaluation on three datasets, we demonstrate that our proposed approach is not only much faster than a state-of-theart deep learning autoencoding approach (D-VAE) but also has less autoencoding error if little data and time is given.},
  langid = {english},
  keywords = {notag,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4ZFIBSIX\\Paassen et al. - Tree Echo State Autoencoders with Grammars.pdf}
}

@article{pachetSamplingVariationsSequences2017,
  title = {Sampling {{Variations}} of {{Sequences}} for {{Structured Music Generation}}},
  author = {Pachet, Fran{\c c}ois and Papadopoulos, Alexandre and Roy, Pierre},
  year = {2017},
  journal = {Proceedings of the 18th International Society for Music Information Retrieval Conference (ISMIR)},
  abstract = {Recently, machine-learning techniques have been success-fully used for the generation of complex artifacts such as music or text. However, these techniques are still unable to capture and generate artifacts that are convincingly struc-tured. In particular, musical sequences do not exhibit pat-tern structure, as typically found in human composed mu-sic. We present an approach to generate structured se-quences, based on a mechanism for sampling efficiently variations of musical sequences. Given an input sequence and a statistical model, this mechanism uses belief propa-gation to sample a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism uses local fields to bias the generation. We show experimentally that sampled sequences are in-deed closely correlated to the standard musical similarity function defined by Mongeau and Sankoff. We then show how this mechanism can be used to implement composi-tion strategies that enforce arbitrary structure on a musical lead sheet generation problem. We illustrate our approach with a convincingly structured generated lead sheet in the style of the Beatles.},
  keywords = {music},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\W7P8ILRW\\Pachet, Papadopoulos, Roy - 2017 - Sampling Variations of Sequences for Structured Music Generation.pdf}
}

@inproceedings{pagliardini-etal-2018-unsupervised,
  ids = {pagliardiniUnsupervisedLearningSentence2017},
  title = {Unsupervised Learning of Sentence Embeddings Using Compositional N-{{Gram}} Features},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Pagliardini, Matteo and Gupta, Prakhar and Jaggi, Martin},
  year = {2018},
  month = jun,
  pages = {528--540},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1049},
  abstract = {The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.},
  keywords = {nlp,nlp-embeddings,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Y3UAF9FC\\Pagliardini, Gupta, Jaggi - 2017 - Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features(2).pdf}
}

@article{panayotovLibrispeechASRCorpus2015,
  title = {Librispeech: {{An ASR}} Corpus Based on Public Domain Audio Books},
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2015},
  journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  volume = {2015-Augus},
  pages = {5206--5210},
  issn = {9781467369978},
  doi = {10.1109/ICASSP.2015.7178964},
  abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The Lib-riSpeech corpus is derived from audiobooks that are part of the Lib-riVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built lan-guage models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
  keywords = {data,speech},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\X8XXAFMQ\\Panayotov et al. - 2015 - Librispeech An ASR corpus based on public domain audio books.pdf}
}

@phdthesis{pantaleoNewTrackSeeding2017,
  title = {New {{Track Seeding Techniques}} for the {{CMS Experiment}}},
  author = {Pantaleo, Felice},
  year = {2017},
  keywords = {high-energy-physics}
}

@article{papaefstathiouHiggsBosonPair2013,
  title = {Higgs Boson Pair Production at the {{LHC}} in the b b {{W}}+ {{W-}} Channel},
  author = {Papaefstathiou, Andreas and Yang, Li Lin and Zurita, Jos{\'e}},
  year = {2013},
  journal = {Physical Review D},
  volume = {87},
  number = {1},
  pages = {11301--11301},
  keywords = {high-energy-physics}
}

@article{parisiContinualLifelongLearning2019,
  title = {Continual Lifelong Learning with Neural Networks: {{A}} Review},
  author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  year = {2019},
  month = feb,
  journal = {Neural Networks},
  volume = {113},
  pages = {54--71},
  issn = {9781450362078},
  doi = {10.1016/j.neunet.2019.01.012},
  abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
  keywords = {continual,review},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\S93CVBF9\\Parisi et al. - 2019 - Continual lifelong learning with neural networks A review.pdf}
}

@article{pasaLearningSequentialData2016,
  title = {Learning {{Sequential Data}} with the {{Help}} of {{Linear Systems}}},
  author = {Pasa, Luca and B, Alessandro Sperduti},
  year = {2016},
  volume = {9896},
  pages = {3--17},
  issn = {978-3-319-46181-6},
  doi = {10.1007/978-3-319-46182-3},
  keywords = {LAES,LDS},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JJI64CZL\\Pasa, B - 2016 - Learning Sequential Data with the Help of Linear Systems.pdf}
}

@article{pasaLinearDynamicalBased2017,
  title = {Linear Dynamical Based Models for Sequential Domains},
  author = {Pasa, Luca and Sperduti, Alessandro and Tino, Peter},
  year = {2017},
  journal = {Proceedings of the International Joint Conference on Neural Networks},
  volume = {2017-May},
  number = {1},
  pages = {2201--2208},
  issn = {9781509061815},
  doi = {10.1109/IJCNN.2017.7966122},
  keywords = {LAES},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\99S66M3R\\Pasa, Sperduti, Tino - 2017 - Linear dynamical based models for sequential domains.pdf}
}

@article{pasaPretrainingRecurrentNeural2014,
  title = {Pre-Training of {{Recurrent Neural Networks}} via {{Linear Autoencoders}}},
  author = {Pasa, Luca and Sperduti, Alessandro},
  year = {2014},
  journal = {Advances in Neural Information Processing Systems 27},
  pages = {3572--3580},
  abstract = {We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the input- to-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.},
  keywords = {LAES,NIPS,RNN}
}

@inproceedings{pascanuDifficultyTrainingRecurrent2013,
  title = {On the Difficulty of Training {{Recurrent Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  year = {2013},
  month = nov,
  abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  keywords = {learning,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\M35FAXRD\\Pascanu, Mikolov, Bengio - 2013 - On the difficulty of training Recurrent Neural Networks.pdf}
}

@article{pascanuHowConstructDeep2013,
  title = {How to {{Construct Deep Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2013},
  month = dec,
  abstract = {In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \textbackslash textit\{deep\} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XD5D2QVA\\Pascanu et al. - 2013 - How to Construct Deep Recurrent Neural Networks.pdf}
}

@article{pascanuRevisitingNaturalGradient2014,
  title = {Revisiting {{Natural Gradient}} for {{Deep Networks}}},
  author = {Pascanu, Razvan and Bengio, Yoshua},
  year = {2014},
  month = feb,
  journal = {arXiv:1301.3584 [cs]},
  eprint = {1301.3584},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods for training deep models: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.},
  archiveprefix = {arXiv},
  keywords = {fisher-info},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BKUEWT4X\\Pascanu and Bengio - 2014 - Revisiting Natural Gradient for Deep Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\W5KPG2CE\\1301.html}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H and Larochelle, H and Beygelzimer, A and {d{\textbackslash}textquotesingle Alch{\'e}-Buc}, F and Fox, E and Garnett, R},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
  keywords = {software}
}

@misc{PatchingModelsPdf,
  title = {Patching\_models (6).Pdf},
  keywords = {continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\I5G24EL9\\Patching_models (6).pdf}
}

@article{patrauceanSpatiotemporalVideoAutoencoder2016,
  title = {Spatio-Temporal Video Autoencoder with Differentiable Memory},
  author = {Patraucean, Viorica and Handa, Ankur and Cipolla, Roberto},
  year = {2016},
  month = sep,
  journal = {arXiv:1511.06309 [cs]},
  eprint = {1511.06309},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We present one direct application of the proposed framework in weakly-supervised semantic segmentation of videos through label propagation using optical flow.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,cv,video},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\64566YEE\\Patraucean et al_2016_Spatio-temporal video autoencoder with differentiable memory.pdf;C\:\\Users\\w-32\\Zotero\\storage\\TYLD2LVA\\1511.html}
}

@article{patrignaniWhyShouldAnyone2020,
  title = {Why {{Should Anyone}} Use {{Colours}}? Or, {{Syntax Highlighting Beyond Code Snippets}}},
  shorttitle = {Why {{Should Anyone}} Use {{Colours}}?},
  author = {Patrignani, Marco},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.11334 [cs]},
  eprint = {2001.11334},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Syntax highlighting in the form of colours and font diversification, is an excellent tool to provide clarity, concision and correctness to writings. Unfortunately, this practice is not widely adopted, which results in often hard-to-parse papers. The reasons for this lack of adoption is that researchers often struggle to embrace new technologies, piling up unconvincing motivations. This paper argues against such motivations and justifies the usage of syntax highlighting so that it can become a new standard for dissemination of clearer and more understandable research. Moreover, this paper reports on the criticism grounded on the shortcomings of using syntax highlighting in LATEX and suggests remedies to that. We believe this paper can be used as a guide to using syntax highlighting as well as a reference to counter unconvincing motivations against it.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KW2R5S3E\\Patrignani_2020_Why Should Anyone use Colours.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZJQDZFAT\\2001.html}
}

@article{pattersonNOvAExperimentStatus2013,
  title = {The {{NOvA}} Experiment: Status and Outlook},
  author = {Patterson, R B},
  year = {2013},
  journal = {Nuclear Physics B-Proceedings Supplements},
  volume = {235},
  pages = {151--157},
  keywords = {high-energy-physics}
}

@misc{paulUnmaskingLotteryTicket2022,
  title = {Unmasking the {{Lottery Ticket Hypothesis}}: {{What}}'s {{Encoded}} in a {{Winning Ticket}}'s {{Mask}}?},
  shorttitle = {Unmasking the {{Lottery Ticket Hypothesis}}},
  author = {Paul, Mansheej and Chen, Feng and Larsen, Brett W. and Frankle, Jonathan and Ganguli, Surya and Dziugaite, Gintare Karolina},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03044},
  eprint = {2210.03044},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03044},
  abstract = {Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that can still be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP operates by iterative cycles of training, masking smallest magnitude weights, rewinding back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed? We develop answers in terms of the geometry of the error landscape. First, we find that\$\textbackslash unicode\{x2014\}\$at higher sparsities\$\textbackslash unicode\{x2014\}\$pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training determines a limit on the fraction of weights that can be pruned at each iteration of IMP. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,lottery-ticket-hypothesis,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FPR43GKC\\2210.html}
}

@article{pedrelliHierarchicalTaskReservoirAnytime,
  title = {Hierarchical-{{Task Reservoir}} for {{Anytime POS Tagging}} from {{Continuous Speech}}},
  author = {Pedrelli, Luca and Hinaut, Xavier},
  pages = {8},
  abstract = {We propose a novel architecture called HierarchicalTask Reservoir (HTR) suitable for real-time sentence parsing from continuous speech. Accordingly, we introduce a novel task that consists in performing anytime Part-of-Speech (POS) tagging from continuous speech. This HTR architecture is designed to address three sub-tasks (phone, word and POS tag estimation) with increasing levels of abstraction. These tasks are performed by the consecutive layers of the HTR architecture. Interestingly, the qualitative results show that the learning of sub-tasks enforces low frequency dynamics (i.e. with longer timescales) in the more abstract layers. We compared HTR with a baseline hierarchical reservoir architecture (in which each layer is an ESN that addresses the same POS tag estimation). Moreover, we also performed a thorough experimental comparison with several architectural variants. Finally, the HTR obtained the best performance in all experimental comparisons. Overall, the proposed approach will be a useful tool for further studies regarding both the modeling of language comprehension in a neuroscience context and for real-time implementations in Human-Robot Interaction (HRI) context.},
  langid = {english},
  keywords = {ESN,speech,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2D6WAG7Y\\Pedrelli and Hinaut - Hierarchical-Task Reservoir for Anytime POS Taggin.pdf}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  title = {Glove: {{Global}} Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  keywords = {nlp,nlp-embeddings}
}

@inproceedings{pereiraSupportVectorDecomposition2006,
  title = {The Support Vector Decomposition Machine},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Pereira, Francisco and Gordon, Geoffrey},
  year = {2006},
  pages = {689--696},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania}},
  doi = {10.1145/1143844.1143931},
  abstract = {In machine learning problems with tens of thousands of features and only dozens or hundreds of independent training examples, dimensionality reduction is essential for good learning performance. In previous work, many researchers have treated the learning problem in two separate phases: first use an algorithm such as singular value decomposition to reduce the dimensionality of the data set, and then use a classification algorithm such as na\textasciidieresis\i ve Bayes or support vector machines to learn a classifier. We demonstrate that it is possible to combine the two goals of dimensionality reduction and classification into a single learning objective, and present a novel and efficient algorithm which optimizes this objective directly. We present experimental results in fMRI analysis which show that we can achieve better learning performance and lower-dimensional representations than two-phase approaches can.},
  isbn = {978-1-59593-383-6},
  langid = {english},
  keywords = {feature selection,PCA,SVD},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2VX4KT5J\\Pereira and Gordon - 2006 - The support vector decomposition machine.pdf}
}

@article{pereyraRegularizingNeuralNetworks2017,
  title = {Regularizing {{Neural Networks}} by {{Penalizing Confident Output Distributions}}},
  author = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  year = {2017},
  pages = {1--12},
  abstract = {We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.},
  keywords = {regularization,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\F8PI8KW5\\Pereyra et al. - 2017 - Regularizing Neural Networks by Penalizing Confident Output Distributions.pdf}
}

@article{perezML2014HiggsBoson2014,
  ids = {perezML2014HiggsBoson2014a},
  title = {{{ML2014}}: {{Higgs Boson Machine Learning Challenge}}},
  author = {Perez, Jocelyn and Ponmalai, Ravi and Silver, Alex and Strack, Dacoda},
  year = {2014},
  keywords = {high-energy-physics}
}

@misc{pfulbContinualLearningFully2021,
  title = {Continual {{Learning}} with {{Fully Probabilistic Models}}},
  author = {Pf{\"u}lb, Benedikt and Gepperth, Alexander and Bagus, Benedikt},
  year = {2021},
  month = apr,
  number = {arXiv:2104.09240},
  eprint = {2104.09240},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2104.09240},
  abstract = {We present an approach for continual learning (CL) that is based on fully probabilistic (or generative) models of machine learning. In contrast to, e.g., GANs that are "generative" in the sense that they can generate samples, fully probabilistic models aim at modeling the data distribution directly. Consequently, they provide functionalities that are highly relevant for continual learning, such as density estimation (outlier detection) and sample generation. As a concrete realization of generative continual learning, we propose Gaussian Mixture Replay (GMR). GMR is a pseudo-rehearsal approach using a Gaussian Mixture Model (GMM) instance for both generator and classifier functionalities. Relying on the MNIST, FashionMNIST and Devanagari benchmarks, we first demonstrate unsupervised task boundary detection by GMM density estimation, which we also use to reject untypical generated samples. In addition, we show that GMR is capable of class-conditional sampling in the way of a cGAN. Lastly, we verify that GMR, despite its simple structure, achieves state-of-the-art performance on common class-incremental learning problems at very competitive time and memory complexity.},
  archiveprefix = {arXiv},
  keywords = {bayesian,Computer Science - Machine Learning,continual,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JSCWMF5H\\2104.html}
}

@misc{pfulbOvercomingCatastrophicForgetting2021,
  title = {Overcoming {{Catastrophic Forgetting}} with {{Gaussian Mixture Replay}}},
  author = {Pf{\"u}lb, Benedikt and Gepperth, Alexander},
  year = {2021},
  month = apr,
  number = {arXiv:2104.09220},
  eprint = {2104.09220},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2104.09220},
  abstract = {We present Gaussian Mixture Replay (GMR), a rehearsal-based approach for continual learning (CL) based on Gaussian Mixture Models (GMM). CL approaches are intended to tackle the problem of catastrophic forgetting (CF), which occurs for Deep Neural Networks (DNNs) when sequentially training them on successive sub-tasks. GMR mitigates CF by generating samples from previous tasks and merging them with current training data. GMMs serve several purposes here: sample generation, density estimation (e.g., for detecting outliers or recognizing task boundaries) and providing a high-level feature representation for classification. GMR has several conceptual advantages over existing replay-based CL approaches. First of all, GMR achieves sample generation, classification and density estimation in a single network structure with strongly reduced memory requirements. Secondly, it can be trained at constant time complexity w.r.t. the number of sub-tasks, making it particularly suitable for life-long learning. Furthermore, GMR minimizes a differentiable loss function and seems to avoid mode collapse. In addition, task boundaries can be detected by applying GMM density estimation. Lastly, GMR does not require access to sub-tasks lying in the future for hyper-parameter tuning, allowing CL under real-world constraints. We evaluate GMR on multiple image datasets, which are divided into class-disjoint sub-tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,continual,generative-replay,gmm},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XQLIZUK2\\2104.html}
}

@inproceedings{phamContinualNormalizationRethinking2021,
  title = {Continual {{Normalization}}: {{Rethinking Batch Normalization}} for {{Online Continual Learning}}},
  shorttitle = {Continual {{Normalization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
  year = {2021},
  month = sep,
  abstract = {Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and non-stationary nature of continual...},
  langid = {english},
  keywords = {batch-normalization,continual,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PETR3WCU\\forum.html}
}

@misc{phamDualNetContinualLearning2021,
  title = {{{DualNet}}: {{Continual Learning}}, {{Fast}} and {{Slow}}},
  shorttitle = {{{DualNet}}},
  author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
  year = {2021},
  month = sep,
  number = {arXiv:2110.00175},
  eprint = {2110.00175},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {According to Complementary Learning Systems (CLS) theory [McClelland et al., 1995] in neuroscience, humans do effective continual learning through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics and individual experiences, and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose a novel continual learning framework named ``DualNet", which comprises a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for unsupervised representation learning of task-agnostic general representation via a Self-Supervised Learning (SSL) technique. The two fast and slow learning systems are complementary and work seamlessly in a holistic continual learning framework. Our extensive experiments on two challenging continual learning benchmarks of CORE50 and miniImageNet show that DualNet outperforms state-of-the-art continual learning methods by a large margin. We further conduct ablation studies of different SSL objectives to validate DualNet's efficacy, robustness, and scalability. Code will be made available upon acceptance.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DFN99QVZ\\2110.00175.pdf;C\:\\Users\\w-32\\Zotero\\storage\\GL8HQMYI\\Pham et al. - 2021 - DualNet Continual Learning, Fast and Slow.pdf}
}

@misc{phamDualNetContinualLearning2021a,
  title = {{{DualNet}}: {{Continual Learning}}, {{Fast}} and {{Slow}}},
  shorttitle = {{{DualNet}}},
  author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
  year = {2021},
  month = sep,
  number = {arXiv:2110.00175},
  eprint = {2110.00175},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2110.00175},
  abstract = {According to Complementary Learning Systems (CLS) theory\textasciitilde\textbackslash citep\{mcclelland1995there\} in neuroscience, humans do effective \textbackslash emph\{continual learning\} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics and individual experiences, and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose a novel continual learning framework named "DualNet", which comprises a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for unsupervised representation learning of task-agnostic general representation via a Self-Supervised Learning (SSL) technique. The two fast and slow learning systems are complementary and work seamlessly in a holistic continual learning framework. Our extensive experiments on two challenging continual learning benchmarks of CORE50 and miniImageNet show that DualNet outperforms state-of-the-art continual learning methods by a large margin. We further conduct ablation studies of different SSL objectives to validate DualNet's efficacy, robustness, and scalability. Code will be made available upon acceptance.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DMNG3IG2\\Pham et al_2021_DualNet.pdf;C\:\\Users\\w-32\\Zotero\\storage\\W7F6X7FJ\\2110.html}
}

@inproceedings{phuongUnderstandingKnowledgeDistillation2019,
  title = {Towards {{Understanding Knowledge Distillation}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Phuong, Mary and Lampert, Christoph},
  year = {2019},
  month = may,
  pages = {5142--5151},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Knowledge distillation, i.e., one classifier being trained on the outputs of another classifier, is an empirically very successful technique for knowledge transfer between classifiers. It has even ...},
  langid = {english},
  keywords = {notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\T8BXJKJU\\Phuong_Lampert_2019_Towards Understanding Knowledge Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\K2DAT6Y2\\phuong19a.html}
}

@inproceedings{pierrejeanPredictingWordEmbeddings2018,
  title = {Predicting {{Word Embeddings Variability}}},
  booktitle = {*{{SEM}}@{{NAACL-HLT}}},
  author = {Pierrejean, Benedicte and Tanguy, Ludovic},
  year = {2018},
  keywords = {nlp,nlp-embeddings}
}

@incollection{plumbleyLieGroupMethods2004,
  title = {Lie {{Group Methods}} for {{Optimization}} with {{Orthogonality Constraints}}},
  booktitle = {Independent {{Component Analysis}} and {{Blind Signal Separation}}},
  author = {Plumbley, Mark},
  year = {2004},
  volume = {3195},
  pages = {1245--1252},
  abstract = {Optimization of a cost function J ( W ) under an orthogonality constraint WW T = I is a common requirement for ICA methods. In this paper, we will review the use of Lie group methods to perform this constrained optimization. Instead of searching in the space of n \texttimes{} n matrices W , we will introduce the concept of the Lie group SO( n ) of orthogonal matrices, and the corresponding Lie algebraso ( n ). Using so ( n ) for our coordinates, we can multiplicatively update W by a rotation matrix R so that W {${'}$}= RW always remains orthogonal. Steepest descent and conjugate gradient algorithms can be used in this framework.},
  isbn = {978-3-540-23056-4},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3DLF5F6M\\Plumbley - 2004 - Lie Group Methods for Optimization with Orthogonality Constraints(2).pdf}
}

@inproceedings{pmlr-v78-lomonaco17a,
  title = {{{CORe50}}: A New Dataset and Benchmark for Continuous Object Recognition},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  author = {Lomonaco, Vincenzo and Maltoni, Davide},
  editor = {Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
  year = {2017},
  month = nov,
  series = {Proceedings of Machine Learning Research},
  volume = {78},
  pages = {17--26},
  publisher = {{PMLR}},
  abstract = {Continuous/Lifelong learning of high-dimensional data streams is a challenging research problem. In fact, fully retraining models each time new data become available is infeasible, due to computational and storage issues, while na\"ive incremental strategies have been shown to suffer from catastrophic forgetting. In the context of real-world object recognition applications (e.g., robotic vision), where continuous learning is crucial, very few datasets and benchmarks are available to evaluate and compare emerging techniques. In this work we propose a new dataset and benchmark CORe50, specifically designed for continuous object recognition, and introduce baseline approaches for different continuous learning scenarios.},
  pdf = {http://proceedings.mlr.press/v78/lomonaco17a/lomonaco17a.pdf}
}

@article{pollackRecursiveDistributedRepresentations1990,
  title = {Recursive {{Distributed Representations}}},
  author = {Pollack, Jordan B.},
  year = {1990},
  journal = {Artificial Intelligence},
  volume = {46},
  issn = {0893-6080 (Print)},
  doi = {10.1016/j.neunet.2005.01.005},
  abstract = {Connectionist networks have been criticized for their inability to represent complex structures with systematicity. That is, while they can be trained to represent and manipulate complex objects made of several constituents, they generally fail to generalize to novel combinations of the same constituents. This paper presents a modification of Pollack's Recursive Auto-Associative Memory (RAAM), that addresses this criticism. The network uses linear units and is trained with Oja's rule, in which it generalizes PCA to tree-structured data. Learned representations may be linearly combined, in order to represent new complex structures. This results in unprecedented generalization capabilities. Capacity is orders of magnitude higher than that of a RAAM trained with back-propagation. Moreover, regularities of the training set are preserved in the new formed objects. The formation of new structures displays developmental effects similar to those observed in children when learning to generalize about the argument structure of verbs. \textcopyright{} 2005 Elsevier Ltd. All rights reserved.},
  keywords = {tree},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TTDVZK4B\\Pollack - 1990 - Recursive Distributed Representations(2).pdf}
}

@article{pomponiEfficientContinualLearning2020,
  title = {Efficient {{Continual Learning}} in {{Neural Networks}} with {{Embedding Regularization}}},
  author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
  year = {2020},
  month = jul,
  journal = {Neurocomputing},
  volume = {397},
  eprint = {1909.03742},
  eprinttype = {arxiv},
  pages = {139--148},
  issn = {09252312},
  doi = {10.1016/j.neucom.2020.01.093},
  abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YTN7EHDM\\Pomponi et al_2020_Efficient Continual Learning in Neural Networks with Embedding Regularization.pdf;C\:\\Users\\w-32\\Zotero\\storage\\XUZWENS6\\1909.html}
}

@article{pomponiPseudoRehearsalContinualLearning2021,
  title = {Pseudo-{{Rehearsal}} for {{Continual Learning}} with {{Normalizing Flows}}},
  author = {Pomponi, Jary and Scardapane, Simone and Uncini, Aurelio},
  year = {2021},
  month = aug,
  journal = {arXiv:2007.02443 [cs, stat]},
  eprint = {2007.02443},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Catastrophic forgetting (CF) happens whenever a neural network overwrites past knowledge while being trained on new tasks. Common techniques to handle CF include regularization of the weights (using, e.g., their importance on past tasks), and rehearsal strategies, where the network is constantly re-trained on past data. Generative models have also been applied for the latter, in order to have endless sources of data. In this paper, we propose a novel method that combines the strengths of regularization and generative-based rehearsal approaches. Our generative model consists of a normalizing flow (NF), a probabilistic and invertible neural network, trained on the internal embeddings of the network. By keeping a single NF conditioned on the task, we show that our memory overhead remains constant. In addition, exploiting the invertibility of the NF, we propose a simple approach to regularize the network's embeddings with respect to past tasks. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, with bounded computational power and memory overheads.},
  archiveprefix = {arXiv},
  keywords = {notag,toread},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Pomponi et al_2021_Pseudo-Rehearsal for Continual Learning with Normalizing Flows.pdf;C\:\\Users\\w-32\\Zotero\\storage\\2U383DLB\\2007.html}
}

@incollection{prabhuGDumbSimpleApproach2020,
  title = {{{GDumb}}: {{A Simple Approach}} That {{Questions Our Progress}} in {{Continual Learning}}},
  shorttitle = {{{GDumb}}},
  booktitle = {{{ECCV}} 2020},
  author = {Prabhu, Ameya and Torr, Philip H. S. and Dokania, Puneet K.},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12347},
  pages = {524--540},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58536-5_31},
  abstract = {We discuss a general formulation for the Continual Learning (CL) problem for classification\textemdash a learning task where a stream provides samples to a learner and the goal of the learner, depending on the samples it receives, is to continually upgrade its knowledge about the old classes and learn new ones. Our formulation takes inspiration from the open-set recognition problem where test scenarios do not necessarily belong to the training distribution. We also discuss various quirks and assumptions encoded in recently proposed approaches for CL. We argue that some oversimplify the problem to an extent that leaves it with very little practical importance, and makes it extremely easy to perform well on. To validate this, we propose GDumb that (1) greedily stores samples in memory as they come and; (2) at test time, trains a model from scratch using samples only in the memory. We show that even though GDumb is not specifically designed for CL problems, it obtains state-of-the-art accuracies (often with large margins) in almost all the experiments when compared to a multitude of recently proposed algorithms. Surprisingly, it outperforms approaches in CL formulations for which they were specifically designed. This, we believe, raises concerns regarding our progress in CL for classification. Overall, we hope our formulation, characterizations and discussions will help in designing realistically useful CL algorithms, and GDumb will serve as a strong contender for the same.},
  isbn = {978-3-030-58535-8 978-3-030-58536-5},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JQXV9ZNE\\Prabhu et al. - 2020 - GDumb A Simple Approach that Questions Our Progre.pdf}
}

@article{pratamaAutonomousDeepLearning2018,
  title = {Autonomous {{Deep Learning}}: {{Incremental Learning}} of {{Denoising Autoencoder}} for {{Evolving Data Streams}}},
  author = {Pratama, Mahardhika and Ashfahani, Andri and Ong, Yew Soon and Ramasamy, Savitha and Lughofer, Edwin},
  year = {2018},
  month = sep,
  abstract = {The generative learning phase of Autoencoder (AE) and its successor Denosing Autoencoder (DAE) enhances the flexibility of data stream method in exploiting unlabelled samples. Nonetheless, the feasibility of DAE for data stream analytic deserves in-depth study because it characterizes a fixed network capacity which cannot adapt to rapidly changing environments. An automated construction of a denoising autoeconder, namely deep evolving denoising autoencoder (DEVDAN), is proposed in this paper. DEVDAN features an open structure both in the generative phase and in the discriminative phase where input features can be automatically added and discarded on the fly. A network significance (NS) method is formulated in this paper and is derived from the bias-variance concept. This method is capable of estimating the statistical contribution of the network structure and its hidden units which precursors an ideal state to add or prune input features. Furthermore, DEVDAN is free of the problem- specific threshold and works fully in the single-pass learning fashion. The efficacy of DEVDAN is numerically validated using nine non-stationary data stream problems simulated under the prequential test-then-train protocol where DEVDAN is capable of delivering an improvement of classification accuracy to recently published online learning works while having flexibility in the automatic extraction of robust input features and in adapting to rapidly changing environments.},
  keywords = {autoencoders,continual,incremental learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9KG4QH9T\\Pratama et al. - 2018 - Autonomous Deep Learning Incremental Learning of Denoising Autoencoder for Evolving Data Streams(2).pdf}
}

@misc{PrefaceDiveDeep,
  title = {Preface \textemdash{} {{Dive}} into {{Deep Learning}} 0.7.1 Documentation},
  howpublished = {https://d2l.ai/chapter\_preface/index.html},
  keywords = {book},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NPP86CB8\\index.html}
}

@article{ProbabilitaProcessiStocastici,
  title = {{Probabilit\`a e Processi Stocastici (455AA)}},
  pages = {350},
  langid = {italian},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VXF2LC2R\\Probabilit e Processi Stocastici (455AA).pdf}
}

@article{ProtobufDocumentation,
  title = {Protobuf {{Documentation}}},
  keywords = {software}
}

@article{qinDualStageAttentionBasedRecurrent2017,
  title = {A {{Dual-Stage Attention-Based Recurrent Neural Network}} for {{Time Series Prediction}}},
  author = {Qin, Yao and Song, Dongjin and Chen, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison},
  year = {2017},
  month = apr,
  abstract = {The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction.},
  keywords = {attention,RNN,time-series},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IT6F9JFJ\\Qin et al. - 2017 - A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction(2).pdf}
}

@article{qingDeepWideFeature2020,
  title = {Deep and Wide Feature Based Extreme Learning Machine for Image Classification},
  author = {Qing, Yuanyuan and Zeng, Yijie and Li, Yue and Huang, Guang-Bin},
  year = {2020},
  month = oct,
  journal = {Neurocomputing},
  volume = {412},
  pages = {426--436},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.06.110},
  abstract = {Extreme Learning Machine (ELM) is a powerful and favorable classifier used in various applications due to its fast speed and good generalization capability. However, when dealing with complex visual tasks, the shallow architecture of ELM makes it infeasible to have good performance when raw image data are directly fed in as input. Therefore, several works tried to make use of deep neural networks (DNNs) to extract features before ELM classification. On the other hand, when the depth of DNN is too deep, the ELM classifier may suffer from overfitting problem. To solve this issue, a novel deep and wide feature based Extreme Learning Machine (DW-ELM) has been proposed in this research work. We show that the overfitting problem can be largely remedied by employing a ``widened'' convolutional neural network (CNN) for feature extraction, in the sense that the number of feature maps for each convolutional layer is increased by factor of k compared to a reference model, i.e. deep residual networks (ResNets). While the wide design of residual networks has been shown to benefit image classification in terms of accuracy and efficiency, its application for feature extraction is not fully investigated. We provide an extensive experimental study in this work, showing that when combined with ELM that serves as a classifier, using wide ResNets (WRNs) for feature extraction can produce a performance leap on all benchmark datasets compared to a plain end-to-end trained network over a wide range of selections regardless of architecture choices and ELM designs, while normal ResNets as feature extractors do not provide a performance gain. The gap is even larger when fewer training iterations are employed. This indicates that a good feature extractor for ELM must be wide and deep. Experiments conducted on five benchmark datasets (CIFAR-100, CIFAR-10, STL-10, Flower-102 and Fashion-MNIST) have shown significant accuracy enhancement as well as training stability of the proposed DW-ELM. Ablation studies also demonstrate that the ELM classifier is an important component for DW-ELM which enables superior performance compared with SVM based image classification approaches.},
  langid = {english},
  keywords = {Extreme Learning Machine,Image Classification,random-net,vision,Wide Residual Network},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YPLFW4NS\\Qing et al_2020_Deep and wide feature based extreme learning machine for image classification.pdf;C\:\\Users\\w-32\\Zotero\\storage\\SJ4M2E8F\\S0925231220310997.html}
}

@inproceedings{qiuGoingDeeperEmbedded2016,
  title = {Going Deeper with Embedded Fpga Platform for Convolutional Neural Network},
  booktitle = {Proceedings of the 2016 {{ACM}}/{{SIGDA International Symposium}} on {{Field-Programmable Gate Arrays}}},
  author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and others},
  year = {2016},
  pages = {26--35},
  keywords = {hardware}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00020},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {clip,foundation-models,open-vocabulary,vision,vision-language},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5SN6QHJQ\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KAI7PMJZ\\2103.html}
}

@article{radfordUnsupervisedRepresentationLearning2016,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  year = {2016},
  month = jan,
  journal = {arXiv:1511.06434 [cs]},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,exmodel,gan,notag,toread},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative.pdf;C\:\\Users\\w-32\\Zotero\\storage\\FZNDF6FJ\\1511.html}
}

@article{raeCompressiveTransformersLongRange2019,
  title = {Compressive {{Transformers}} for {{Long-Range Sequence Modelling}}},
  author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Lillicrap, Timothy P.},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.05507 [cs, stat]},
  eprint = {1911.05507},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Memory,nlp,Statistics - Machine Learning,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PIMAF9CN\\Rae et al. - 2019 - Compressive Transformers for Long-Range Sequence M.pdf;C\:\\Users\\w-32\\Zotero\\storage\\YYN5P58R\\1911.html}
}

@article{raeScalingMemoryAugmentedNeural2016,
  title = {Scaling {{Memory-Augmented Neural Networks}} with {{Sparse Reads}} and {{Writes}}},
  author = {Rae, Jack W and Hunt, Jonathan J and Harley, Tim and Danihelka, Ivo and Senior, Andrew and Wayne, Greg and Graves, Alex and Lillicrap, Timothy P},
  year = {2016},
  month = oct,
  abstract = {Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs \$1,\textbackslash!000\textbackslash times\$ faster and with \$3,\textbackslash!000\textbackslash times\$ less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring \$100,\textbackslash!000\$s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.},
  keywords = {MANN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7E9VQIGK\\Rae et al. - 2016 - Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes(3).pdf;C\:\\Users\\w-32\\Zotero\\storage\\Q9EF6TQG\\Rae et al. - 2016 - Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes(4).pdf}
}

@article{raghuExpressivePowerDeep2016,
  title = {On the {{Expressive Power}} of {{Deep Neural Networks}}},
  author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and {Sohl-Dickstein}, Jascha},
  year = {2016},
  abstract = {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings can be summarized as follows: (1) The complexity of the computed function grows exponentially with depth. (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights. (3) Regularizing on trajectory length (trajectory regularization) is a simpler alternative to batch normalization, with the same performance.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9C4GFS9D\\Raghu et al. - 2016 - On the Expressive Power of Deep Neural Networks.pdf}
}

@article{raghuSVCCASingularVector2017,
  title = {{{SVCCA}}: {{Singular Vector Canonical Correlation Analysis}} for {{Deep Learning Dynamics}} and {{Interpretability}}},
  shorttitle = {{{SVCCA}}},
  author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and {Sohl-Dickstein}, Jascha},
  year = {2017},
  month = nov,
  journal = {arXiv:1706.05806 [cs, stat]},
  eprint = {1706.05806},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/},
  archiveprefix = {arXiv},
  keywords = {visualization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6F9F7KA6\\Raghu et al_2017_SVCCA.pdf;C\:\\Users\\w-32\\Zotero\\storage\\BW4XYCV3\\1706.html}
}

@article{rahamanS2RMsSpatiallyStructured2020,
  title = {{{S2RMs}}: {{Spatially Structured Recurrent Modules}}},
  shorttitle = {{{S2RMs}}},
  author = {Rahaman, Nasim and Goyal, Anirudh and Gondal, Muhammad Waleed and Wuthrich, Manuel and Bauer, Stefan and Sharma, Yash and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.06533 [cs, stat]},
  eprint = {2007.06533},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Capturing the structure of a data-generating process by means of appropriate inductive biases can help in learning models that generalize well and are robust to changes in the input distribution. While methods that harness spatial and temporal structures find broad application, recent work has demonstrated the potential of models that leverage sparse and modular structure using an ensemble of sparingly interacting modules. In this work, we take a step towards dynamic models that are capable of simultaneously exploiting both modular and spatiotemporal structures. We accomplish this by abstracting the modeled dynamical system as a collection of autonomous but sparsely interacting sub-systems. The sub-systems interact according to a topology that is learned, but also informed by the spatial structure of the underlying real-world system. This results in a class of models that are well suited for modeling the dynamics of systems that only offer local views into their state, along with corresponding spatial locations of those views. On the tasks of video prediction from cropped frames and multi-agent world modeling from partial observations in the challenging Starcraft2 domain, we find our models to be more robust to the number of available views and better capable of generalization to novel tasks without additional training, even when compared against strong baselines that perform equally well or better on the training distribution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,modular-arch,RNN,sparse-attention,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\C423ZHI8\\Rahaman et al_2020_S2RMs.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MGNAIM8P\\2007.html}
}

@misc{rahbarUnreasonableEffectivenessKnowledge2020,
  title = {On the {{Unreasonable Effectiveness}} of {{Knowledge Distillation}}: {{Analysis}} in the {{Kernel Regime}}},
  shorttitle = {On the {{Unreasonable Effectiveness}} of {{Knowledge Distillation}}},
  author = {Rahbar, Arman and Panahi, Ashkan and Bhattacharyya, Chiranjib and Dubhashi, Devdatt and Chehreghani, Morteza Haghir},
  year = {2020},
  month = sep,
  number = {arXiv:2003.13438},
  eprint = {2003.13438},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.13438},
  abstract = {Knowledge distillation (KD), i.e. one classifier being trained on the outputs of another classifier, is an empirically very successful technique for knowledge transfer between classifiers. It has even been observed that classifiers learn much faster and more reliably if trained with the outputs of another classifier as soft labels, instead of from ground truth data. However, there has been little or no theoretical analysis of this phenomenon. We provide the first theoretical analysis of KD in the setting of extremely wide two layer non-linear networks in model and regime in (Arora et al., 2019; Du \& Hu, 2019; Cao \& Gu, 2019). We prove results on what the student network learns and on the rate of convergence for the student network. Intriguingly, we also confirm the lottery ticket hypothesis (Frankle \& Carbin, 2019) in this model. To prove our results, we extend the repertoire of techniques from linear systems dynamics. We give corresponding experimental analysis that validates the theoretical results and yields additional insights.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,knowledge-distillation,neural-tangent-kernel,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FEQVBAFG\\2003.html}
}

@article{ramaseshAnatomyCatastrophicForgetting2020,
  title = {Anatomy of {{Catastrophic Forgetting}}: {{Hidden Representations}} and {{Task Semantics}}},
  shorttitle = {Anatomy of {{Catastrophic Forgetting}}},
  author = {Ramasesh, Vinay V. and Dyer, Ethan and Raghu, Maithra},
  year = {2020},
  month = jul,
  journal = {ICLR 2021},
  eprint = {2007.07400},
  eprinttype = {arxiv},
  abstract = {A central challenge in developing versatile machine learning systems is catastrophic forgetting: a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split CIFAR-10 setup and also introduce a novel CIFAR-100 based task approximating realistic input distribution shift.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notag,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Ramasesh et al_2020_Anatomy of Catastrophic Forgetting.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KZLRB8XJ\\2007.html}
}

@inproceedings{ramaseshEffectScaleCatastrophic2021,
  title = {Effect of Scale on Catastrophic Forgetting in Neural Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ramasesh, Vinay Venkatesh and Lewkowycz, Aitor and Dyer, Ethan},
  year = {2021},
  month = sep,
  abstract = {Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning, i.e. learning tasks sequentially. Recently, both computer vision and natural-language...},
  langid = {english},
  keywords = {continual,large-models},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KN4BHQVK\\forum.html}
}

@inproceedings{rameshModelZooGrowing2021,
  title = {Model {{Zoo}}: {{A Growing Brain That Learns Continually}}},
  shorttitle = {Model {{Zoo}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ramesh, Rahul and Chaudhari, Pratik},
  year = {2021},
  month = sep,
  abstract = {This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models. We use statistical learning theory and experimental analysis to show...},
  langid = {english},
  keywords = {continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EQFPXTCG\\forum.html}
}

@article{ramirezCreatingFalseMemory2013,
  title = {Creating a False Memory in the Hippocampus},
  author = {Ramirez, Steve and Liu, Xu and Lin, Pei-Ann and Suh, Junghyup and Pignatelli, Michele and Redondo, Roger L. and Ryan, Tom{\'a}s J. and Tonegawa, Susumu},
  year = {2013},
  month = jul,
  journal = {Science (New York, N.Y.)},
  volume = {341},
  number = {6144},
  pages = {387--391},
  issn = {1095-9203},
  doi = {10.1126/science.1239073},
  abstract = {Memories can be unreliable. We created a false memory in mice by optogenetically manipulating memory engram-bearing cells in the hippocampus. Dentate gyrus (DG) or CA1 neurons activated by exposure to a particular context were labeled with channelrhodopsin-2. These neurons were later optically reactivated during fear conditioning in a different context. The DG experimental group showed increased freezing in the original context, in which a foot shock was never delivered. The recall of this false memory was context-specific, activated similar downstream regions engaged during natural fear memory recall, and was also capable of driving an active fear response. Our data demonstrate that it is possible to generate an internally represented and behaviorally expressed fear memory via artificial means.},
  langid = {english},
  pmid = {23888038},
  keywords = {cl-neuroscience,continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Ramirez et al_2013_Creating a false memory in the hippocampus.pdf}
}

@article{ramsauerHopfieldNetworksAll2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2020},
  month = jul,
  journal = {arXiv:2008.02217 [cs, stat]},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arXiv},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,hopfield,Statistics - Machine Learning,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Q9DJG9NK\\Ramsauer et al_2020_Hopfield Networks is All You Need.pdf;C\:\\Users\\w-32\\Zotero\\storage\\PF6CVAP7\\2008.html}
}

@inproceedings{ranasingheSelfSupervisedVideoTransformer2022,
  title = {Self-{{Supervised Video Transformer}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ranasinghe, Kanchana and Naseer, Muzammal and Khan, Salman and Khan, Fahad Shahbaz and Ryoo, Michael S.},
  year = {2022},
  pages = {2874--2884},
  langid = {english},
  keywords = {self-supervised,transformer,video},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XD3L3WU6\\Ranasinghe_Self-Supervised_Video_Transformer_CVPR_2022_paper.html}
}

@inproceedings{rannenEncoderBasedLifelong2017,
  title = {Encoder {{Based Lifelong Learning}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Rannen, Amal and Aljundi, Rahaf and Blaschko, Matthew B. and Tuytelaars, Tinne},
  year = {2017},
  month = oct,
  volume = {2017-Octob},
  pages = {1329--1337},
  publisher = {{IEEE}},
  doi = {10.1109/ICCV.2017.148},
  abstract = {This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art},
  isbn = {978-1-5386-1032-9},
  keywords = {autoencoders,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EWETZ6LU\\Rannen et al. - 2017 - Encoder Based Lifelong Learning.pdf}
}

@inproceedings{raoContinualUnsupervisedRepresentation2019,
  title = {Continual {{Unsupervised Representation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei and Pascanu, Razvan and Teh, Yee Whye and Hadsell, Raia},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  keywords = {continual,notag,toread,unsupervised},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Rao et al_2019_Continual Unsupervised Representation Learning.pdf}
}

@article{raoRegularizationIterativeInitialization2019,
  title = {Regularization and {{Iterative Initialization}} of {{Softmax}} for {{Fast Training}} of {{Convolutional Neural Networks}}},
  author = {Rao, Qiang and Yu, Bing and He, Kun and Feng, Bailan},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {CNN,softmax},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JBEZWPT7\\Rao et al. - 2019 - Regularization and Iterative Initialization of Softmax for Fast Training of Convolutional Neural Networks.pdf}
}

@inproceedings{rashtchianCollectingImageAnnotations2010,
  title = {Collecting {{Image Annotations Using Amazon}}'s {{Mechanical Turk}}},
  booktitle = {Mturk@{{HLT-NAACL}}},
  author = {Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
  year = {2010}
}

@misc{ravfogelLinearAdversarialConcept2022,
  title = {Linear {{Adversarial Concept Erasure}}},
  author = {Ravfogel, Shauli and Twiton, Michael and Goldberg, Yoav and Cotterell, Ryan},
  year = {2022},
  month = jul,
  number = {arXiv:2201.12091},
  eprint = {2201.12091},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.12091},
  abstract = {Modern neural models trained on textual data rely on pre-trained representations that emerge without direct supervision. As these representations are increasingly being used in real-world applications, the inability to \textbackslash emph\{control\} their content becomes an increasingly important problem. We formulate the problem of identifying and erasing a linear subspace that corresponds to a given concept, in order to prevent linear predictors from recovering the concept. We model this problem as a constrained, linear minimax game, and show that existing solutions are generally not optimal for this task. We derive a closed-form solution for certain objectives, and propose a convex relaxation, R-LACE, that works well for others. When evaluated in the context of binary gender removal, the method recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation. We show that the method -- despite being linear -- is highly expressive, effectively mitigating bias in deep nonlinear classifiers while maintaining tractability and interpretability.},
  archiveprefix = {arXiv},
  keywords = {continual,positive-forgetting},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZCTEGKAX\\2201.html}
}

@article{rebuffiICaRLIncrementalClassifier2016,
  title = {{{iCaRL}}: {{Incremental Classifier}} and {{Representation Learning}}},
  author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
  year = {2016},
  month = nov,
  abstract = {A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.},
  keywords = {continual,episodic-mem},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\L93N98IZ\\Rebuffi et al. - 2016 - iCaRL Incremental Classifier and Representation Learning(3).pdf}
}

@article{reddyUnboundedHumanLearning2016,
  title = {Unbounded {{Human Learning}}: {{Optimal Scheduling}} for {{Spaced Repetition}}},
  shorttitle = {Unbounded {{Human Learning}}},
  author = {Reddy, Siddharth and Labutov, Igor and Banerjee, Siddhartha and Joachims, Thorsten},
  year = {2016},
  month = aug,
  journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  eprint = {1602.07032},
  eprinttype = {arxiv},
  pages = {1815--1824},
  doi = {10.1145/2939672.2939850},
  abstract = {In the study of human learning, there is broad evidence that our ability to retain information improves with repeated exposure and decays with delay since last exposure. This plays a crucial role in the design of educational software, leading to a trade-off between teaching new material and reviewing what has already been taught. A common way to balance this trade-off is spaced repetition, which uses periodic review of content to improve long-term retention. Though spaced repetition is widely used in practice, e.g., in electronic flashcard software, there is little formal understanding of the design of these systems. Our paper addresses this gap in three ways. First, we mine log data from spaced repetition software to establish the functional dependence of retention on reinforcement and delay. Second, we use this memory model to develop a stochastic model for spaced repetition systems. We propose a queueing network model of the Leitner system for reviewing flashcards, along with a heuristic approximation that admits a tractable optimization problem for review scheduling. Finally, we empirically evaluate our queueing model through a Mechanical Turk experiment, verifying a key qualitative prediction of our model: the existence of a sharp phase transition in learning outcomes upon increasing the rate of new item introductions.},
  archiveprefix = {arXiv},
  keywords = {continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Reddy et al_2016_Unbounded Human Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZALNXMWX\\1602.html}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You Only Look Once: {{Unified}}, Real-Time Object Detection},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  pages = {779--788},
  keywords = {CNN,vision}
}

@article{reevesParticleSystemsTechnique1983,
  title = {Particle {{Systems}}\textemdash a {{Technique}} for {{Modeling}} a {{Class}} of {{Fuzzy Objects}}},
  author = {Reeves, W. T.},
  year = {1983},
  month = apr,
  journal = {ACM Transactions on Graphics},
  volume = {2},
  number = {2},
  pages = {91--108},
  issn = {0730-0301},
  doi = {10.1145/357318.357320},
  keywords = {genart,notag,toread}
}

@article{renSemisupervisedClassificationUsing2019,
  title = {A {{Semi-supervised Classification Using Gated Linear Model}}},
  author = {Ren, Yanni and Li, Weite and Hu, Jinglu},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {gated linear},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IBQX8PR7\\Ren, Li, Hu - 2019 - A Semi-supervised Classification Using Gated Linear Model(2).pdf}
}

@inproceedings{rethmeierMoRTyUnsupervisedLearning2019,
  title = {{{MoRTy}}: {{Unsupervised Learning}} of {{Task-specialized Word Embeddings}} by {{Autoencoding}}},
  shorttitle = {{{MoRTy}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Representation Learning}} for {{NLP}} ({{RepL4NLP-2019}})},
  author = {Rethmeier, Nils and Plank, Barbara},
  year = {2019},
  pages = {49--54},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4307},
  abstract = {Word embeddings have undoubtedly revolutionized NLP. However, pre-trained embeddings do not always work for a specific task (or set of tasks), particularly in limited resource setups. We introduce a simple yet effective, self-supervised post-processing method that constructs task-specialized word representations by picking from a menu of reconstructing transformations to yield improved end-task performance (MORTY). The method is complementary to recent state-ofthe-art approaches to inductive transfer via fine-tuning, and forgoes costly model architectures and annotation. We evaluate MORTY on a broad range of setups, including different word embedding methods, corpus sizes and end-task semantics. Finally, we provide a surprisingly simple recipe to obtain specialized embeddings that better fit end-tasks.},
  langid = {english},
  keywords = {finetuning,nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WQNAZ2SE\\Rethmeier and Plank - 2019 - MoRTy Unsupervised Learning of Task-specialized W.pdf}
}

@article{ribeiroExplodingVanishingGradients,
  title = {Beyond Exploding and Vanishing Gradients: Analysing {{RNN}} Training Using Attractors and Smoothness},
  author = {Ribeiro, Ant{\^o}nio H and Tiels, Koen and Aguirre, Luis A and Sch{\"o}n, Thomas B},
  pages = {10},
  abstract = {The exploding and vanishing gradient problem has been the major conceptual principle behind most architecture and training improvements in recurrent neural networks (RNNs) during the last decade. In this paper, we argue that this principle, while powerful, might need some refinement to explain recent developments. We refine the concept of exploding gradients by reformulating the problem in terms of the cost function smoothness, which gives insight into higher-order derivatives and the existence of regions with many close local minima. We also clarify the distinction between vanishing gradients and the need for the RNN to learn attractors to fully use its expressive power. Through the lens of these refinements, we shed new light on recent developments in the RNN field, namely stable RNN and unitary (or orthogonal) RNNs.},
  langid = {english},
  keywords = {attractor,LDS,memory,RNN,vanishing-gradient},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3Q4CQPXQ\\ribeiro20a-supp.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZZ733JQQ\\Ribeiro et al. - Beyond exploding and vanishing gradients analysin.pdf}
}

@article{richardsSoftwareArchitecturePatterns,
  title = {Software {{Architecture Patterns}}},
  author = {Richards, Mark and Media, O'Reilly},
  pages = {55},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\83GKN6LI\\Richards and Media - Software Architecture Patterns.pdf}
}

@article{riemerLearningLearnForgetting2018,
  ids = {riemerLearningLearnForgetting2019},
  title = {Learning to {{Learn}} without {{Forgetting By Maximizing Transfer}} and {{Minimizing Interference}}},
  author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  year = {2018},
  number = {NeurIPS},
  eprint = {1810.11910},
  eprinttype = {arxiv},
  pages = {1--24},
  abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual,interference,meta-learn,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2YBMYPUD\\Riemer et al_2019_Learning to Learn without Forgetting by Maximizing Transfer and Minimizing.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KWEEJ2NT\\Riemer et al. - 2018 - Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference.pdf;C\:\\Users\\w-32\\Zotero\\storage\\87NKWPEZ\\1810.html}
}

@article{riesenhuberHierarchicalModelsObject1999,
  ids = {riesenhuberHierarchicalModelsObject1999a},
  title = {Hierarchical Models of Object Recognition in Cortex},
  author = {Riesenhuber, Maximilian and Poggio, Tomaso},
  year = {1999},
  journal = {Nature neuroscience},
  volume = {2},
  number = {11},
  pages = {1019--1025}
}

@article{ringCHILDFirstStep1997,
  title = {{{CHILD}}: {{A First Step Towards Continual Learning}}},
  shorttitle = {{{CHILD}}},
  author = {Ring, Mark B.},
  year = {1997},
  month = jul,
  journal = {Machine Learning},
  volume = {28},
  number = {1},
  pages = {77--104},
  issn = {1573-0565},
  doi = {10.1023/A:1007331723572},
  abstract = {Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.},
  langid = {english},
  keywords = {notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4TIAR5I3\\Ring_1997_CHILD.pdf}
}

@inproceedings{rishClosedformSupervisedDimensionality2008,
  title = {Closed-Form Supervised Dimensionality Reduction with Generalized Linear Models},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Rish, Irina and Grabarnik, Genady and Cecchi, Guillermo and Pereira, Francisco and Gordon, Geoffrey J.},
  year = {2008},
  pages = {832--839},
  publisher = {{ACM Press}},
  address = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390261},
  abstract = {We propose a family of supervised dimensionality reduction (SDR) algorithms that combine feature extraction (dimensionality reduction) with learning a predictive model in a unified optimization framework, using data- and class-appropriate generalized linear models (GLMs), and handling both classification and regression problems. Our approach uses simple closed-form update rules and is provably convergent. Promising empirical results are demonstrated on a variety of high-dimensional datasets.},
  isbn = {978-1-60558-205-4},
  langid = {english},
  keywords = {feature selection,linear,SVD},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\B6FR336F\\Rish et al. - 2008 - Closed-form supervised dimensionality reduction wi.pdf}
}

@inproceedings{robertsHierarchicalLatentVector2018,
  title = {A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
  booktitle = {35th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2018},
  author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
  year = {2018},
  month = mar,
  volume = {10},
  pages = {6939--6954},
  abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.},
  isbn = {978-1-5108-6796-3},
  keywords = {Hierarchical RNN,ICML,music,vae},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QCBMZJY7\\Roberts et al. - 2018 - A hierarchical latent vector model for learning long-term structure in music(3).pdf}
}

@book{robertsPrinciplesDeepLearning2022,
  title = {The {{Principles}} of {{Deep Learning Theory}}},
  author = {Roberts, Daniel A. and Yaida, Sho and Hanin, Boris},
  year = {2022},
  month = may,
  eprint = {2106.10165},
  eprinttype = {arxiv},
  primaryclass = {hep-th, stat},
  doi = {10.1017/9781009023405},
  abstract = {This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.},
  archiveprefix = {arXiv},
  keywords = {deep-learning,noread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\63CZESHK\\2106.html}
}

@article{rockiSurprisalDrivenFeedbackRecurrent2016,
  title = {Surprisal-{{Driven Feedback}} in {{Recurrent Networks}}},
  author = {Rocki, Kamil M},
  year = {2016},
  month = aug,
  abstract = {Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC on the test portion of the text.},
  keywords = {RNN,Surprisal},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\L9STUCRA\\Rocki - 2016 - Surprisal-Driven Feedback in Recurrent Networks(3).pdf}
}

@article{rockiSurprisalDrivenZoneout2016,
  title = {Surprisal-{{Driven Zoneout}}},
  author = {Rocki, Kamil and Kornuta, Tomasz and Maharaj, Tegan},
  year = {2016},
  month = oct,
  abstract = {We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods.},
  keywords = {regularization,RNN,Surprisal},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HDC2CXBB\\Rocki, Kornuta, Maharaj - 2016 - Surprisal-Driven Zoneout(2).pdf}
}

@article{rodanMinimumComplexityEcho2011,
  title = {Minimum {{Complexity Echo State Network}}},
  author = {Rodan, A. and Tino, P.},
  year = {2011},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {22},
  number = {1},
  pages = {131--144},
  issn = {1941-0093},
  doi = {10.1109/TNN.2010.2089641},
  abstract = {Reservoir computing (RC) refers to a new class of state-space models with a fixed state transition structure (the reservoir) and an adaptable readout form the state space. The reservoir is supposed to be sufficiently complex so as to capture a large number of features of the input stream that can be exploited by the reservoir-to-output readout mapping. The field of RC has been growing rapidly with many successful applications. However, RC has been criticized for not being principled enough. Reservoir construction is largely driven by a series of randomized model-building stages, with both researchers and practitioners having to rely on a series of trials and errors. To initialize a systematic study of the field, we concentrate on one of the most popular classes of RC methods, namely echo state network, and ask: What is the minimal complexity of reservoir construction for obtaining competitive models and what is the memory capacity (MC) of such simplified reservoirs? On a number of widely used time series benchmarks of different origin and characteristics, as well as by conducting a theoretical analysis we show that a simple deterministically constructed cycle reservoir is comparable to the standard echo state network methodology. The (short-term) of linear cyclic reservoirs can be made arbitrarily close to the proved optimal value.},
  keywords = {Algorithms,Artificial Intelligence,Communication channels,Computational modeling,Computer Simulation,Echo state networks,Linear Models,memory capability,memory capacity,minimum complexity echo state network,neural nets,neural networks,Neural Networks (Computer),Nonlinear Dynamics,notag,reservoir computing,Reservoirs,simple recurrent time-series prediction,Software Design,state transition structure,state-space models,storage management,Thyristors,Time Factors,time series,Time series analysis,Topology,Training},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AK2DZUBY\\Rodan_Tino_2011_Minimum Complexity Echo State Network.pdf;C\:\\Users\\w-32\\Zotero\\storage\\7PAPPIQT\\citations.html}
}

@misc{rodriguezShortTermPlasticityNeurons2022,
  title = {Short-{{Term Plasticity Neurons Learning}} to {{Learn}} and {{Forget}}},
  author = {Rodriguez, Hector Garcia and Guo, Qinghai and Moraitis, Timoleon},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14048},
  eprint = {2206.14048},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.14048},
  abstract = {Short-term plasticity (STP) is a mechanism that stores decaying memories in synapses of the cerebral cortex. In computing practice, STP has been used, but mostly in the niche of spiking neurons, even though theory predicts that it is the optimal solution to certain dynamic tasks. Here we present a new type of recurrent neural unit, the STP Neuron (STPN), which indeed turns out strikingly powerful. Its key mechanism is that synapses have a state, propagated through time by a self-recurrent connection-within-the-synapse. This formulation enables training the plasticity with backpropagation through time, resulting in a form of learning to learn and forget in the short term. The STPN outperforms all tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, and differentiable plasticity. We confirm this in both supervised and reinforcement learning (RL), and in tasks such as Associative Retrieval, Maze Exploration, Atari video games, and MuJoCo robotics. Moreover, we calculate that, in neuromorphic or biological circuits, the STPN minimizes energy consumption across models, as it depresses individual synapses dynamically. Based on these, biological STP may have been a strong evolutionary attractor that maximizes both efficiency and computational power. The STPN now brings these neuromorphic advantages also to a broad spectrum of machine learning practice. Code is available at https://github.com/NeuromorphicComputing/stpn},
  archiveprefix = {arXiv},
  keywords = {continual,positive-forgetting},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JIQPJLXC\\2206.html}
}

@article{rolfeDiscriminativeRecurrentSparse2013,
  title = {Discriminative {{Recurrent Sparse Auto-Encoders}}},
  author = {Rolfe, Jason Tyler and LeCun, Yann},
  year = {2013},
  month = jan,
  abstract = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
  keywords = {autoencoders},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WYKBM9ZQ\\Rolfe, LeCun - 2013 - Discriminative Recurrent Sparse Auto-Encoders(2).pdf}
}

@article{ronenDeepDPMDeepClustering2022,
  title = {{{DeepDPM}}: {{Deep Clustering With}} an {{Unknown Number}} of {{Clusters}}},
  shorttitle = {{{DeepDPM}}},
  author = {Ronen, Meitar and Finder, Shahaf E. and Freifeld, Oren},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.14309 [cs, stat]},
  eprint = {2203.14309},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep Learning (DL) has shown great promise in the unsupervised task of clustering. That said, while in classical (i.e., non-deep) clustering the benefits of the nonparametric approach are well known, most deep-clustering methods are parametric: namely, they require a predefined and fixed number of clusters, denoted by K. When K is unknown, however, using model-selection criteria to choose its optimal value might become computationally expensive, especially in DL as the training process would have to be repeated numerous times. In this work, we bridge this gap by introducing an effective deep-clustering method that does not require knowing the value of K as it infers it during the learning. Using a split/merge framework, a dynamic architecture that adapts to the changing K, and a novel loss, our proposed method outperforms existing nonparametric methods (both classical and deep ones). While the very few existing deep nonparametric methods lack scalability, we demonstrate ours by being the first to report the performance of such a method on ImageNet. We also demonstrate the importance of inferring K by showing how methods that fix it deteriorate in performance when their assumed K value gets further from the ground-truth one, especially on imbalanced datasets. Our code is available at https://github.com/BGU-CS-VIL/DeepDPM.},
  archiveprefix = {arXiv},
  keywords = {bayesian,continual,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ISZ9HFDX\\2203.html}
}

@article{rosascoDistilledReplayOvercoming2021,
  title = {Distilled {{Replay}}: {{Overcoming Forgetting}} through {{Synthetic Samples}}},
  shorttitle = {Distilled {{Replay}}},
  author = {Rosasco, Andrea and Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2021},
  month = jun,
  journal = {arXiv:2103.15851 [cs]},
  eprint = {2103.15851},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Replay strategies are Continual Learning techniques which mitigate catastrophic forgetting by keeping a buffer of patterns from previous experiences, which are interleaved with new data during training. The amount of patterns stored in the buffer is a critical parameter which largely influences the final performance and the memory footprint of the approach. This work introduces Distilled Replay, a novel replay strategy for Continual Learning which is able to mitigate forgetting by keeping a very small buffer (1 pattern per class) of highly informative samples. Distilled Replay builds the buffer through a distillation process which compresses a large dataset into a tiny set of informative examples. We show the effectiveness of our Distilled Replay against popular replay-based strategies on four Continual Learning benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Rosasco et al_2021_Distilled Replay.pdf;C\:\\Users\\w-32\\Zotero\\storage\\9Y27W75D\\2103.html}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  ids = {rosenblattPerceptronProbabilisticModel1958a},
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  author = {Rosenblatt, Frank},
  year = {1958},
  journal = {Psychological review},
  volume = {65},
  number = {6},
  pages = {386}
}

@article{rossIncrementalLearningRobust,
  title = {Incremental {{Learning}} for {{Robust Visual Tracking}}},
  author = {Ross, David A and Lim, Jongwoo and Lin, Ruei-Sung and Yang, Ming-Hsuan},
  abstract = {Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object's appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modelled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental algorithms for principal component analysis, includes two important features: a method for correctly updating the sample mean, and a forgetting factor to ensure less modelling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large changes in pose, scale, and illumination.},
  keywords = {incremental learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\G2VLW4M8\\Ross et al. - Unknown - Incremental Learning for Robust Visual Tracking.pdf}
}

@article{rostamiLifelongLearningNetworks2018,
  title = {Lifelong {{Learning Networks}}: {{Beyond Single Agent Lifelong Learning}}},
  shorttitle = {Lifelong {{Learning Networks}}},
  author = {Rostami, Mohammad and Eaton, Eric},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  abstract = {Lifelong machine learning (LML) is a paradigm to design adaptive agents that can learn in dynamic environments. Current LML algorithms consider a single agent that has centralized access to all data. However, given privacy and security constraints, data might be distributed among multiple agents that can collaborate and learn from collective experience. Our goal is to extend LML from a single agent to a network of multiple agents that collectively learn a series of tasks.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QJX9EDKS\\Rostami_Eaton_2018_Lifelong Learning Networks.pdf}
}

@article{rostamiMultiAgentDistributedLifelong2018,
  title = {Multi-{{Agent Distributed Lifelong Learning}} for {{Collective Knowledge Acquisition}}},
  author = {Rostami, Mohammad and Kolouri, Soheil and Kim, Kyungnam and Eaton, Eric},
  year = {2018},
  month = feb,
  journal = {arXiv:1709.05412 [cs]},
  eprint = {1709.05412},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Lifelong machine learning methods acquire knowledge over a series of consecutive tasks, continually building upon their experience. Current lifelong learning algorithms rely upon a single learning agent that has centralized access to all data. In this paper, we extend the idea of lifelong learning from a single agent to a network of multiple agents that collectively learn a series of tasks. Each agent faces some (potentially unique) set of tasks; the key idea is that knowledge learned from these tasks may benefit other agents trying to learn different (but related) tasks. Our Collective Lifelong Learning Algorithm (CoLLA) provides an efficient way for a network of agents to share their learned knowledge in a distributed and decentralized manner, while preserving the privacy of the locally observed data. Note that a decentralized scheme is a subclass of distributed algorithms where a central server does not exist and in addition to data, computations are also distributed among the agents. We provide theoretical guarantees for robust performance of the algorithm and empirically demonstrate that CoLLA outperforms existing approaches for distributed multi-task learning on a variety of data sets.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RMGDHK6Z\\Rostami et al_2018_Multi-Agent Distributed Lifelong Learning for Collective Knowledge Acquisition.pdf;C\:\\Users\\w-32\\Zotero\\storage\\P6VMA7S4\\1709.html}
}

@book{rougierRougierFromPythonToNumpyVersion2016,
  title = {Rougier/{{From-Python-To-Numpy}}: {{Version}} 1.1},
  shorttitle = {Rougier/{{From-Python-To-Numpy}}},
  author = {Rougier, Nicolas P.},
  year = {2016},
  month = dec,
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.225783},
  abstract = {First complete version. Still need reviews.},
  copyright = {Creative Commons Attribution-NonCommercial 4.0, Open Access},
  keywords = {book,Numerical computation,Numpy,Python,software}
}

@misc{ruizDreamBoothFineTuning2022,
  title = {{{DreamBooth}}: {{Fine Tuning Text-to-Image Diffusion Models}} for {{Subject-Driven Generation}}},
  shorttitle = {{{DreamBooth}}},
  author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  year = {2022},
  month = aug,
  number = {arXiv:2208.12242},
  eprint = {2208.12242},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.12242},
  abstract = {Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models (specializing them to users' needs). Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model (Imagen, although our method is not limited to a specific model) such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views, and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, appearance modification, and artistic rendering (all while preserving the subject's key features). Project page: https://dreambooth.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,image-generation,stable-diffusion},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5WFD4EJW\\2208.html}
}

@book{rumelhartParallelDistributedProcessing1988,
  ids = {rumelhartParallelDistributedProcessing1988a},
  title = {Parallel Distributed Processing},
  author = {Rumelhart, David E and McClelland, James L and Group, PDP Research and others},
  year = {1988},
  volume = {1},
  publisher = {{IEEE}}
}

@inproceedings{ruschLongExpressiveMemory2021,
  title = {Long {{Expressive Memory}} for {{Sequence Modeling}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Rusch, T. Konstantin and Mishra, Siddhartha and Erichson, N. Benjamin and Mahoney, Michael W.},
  year = {2021},
  month = sep,
  abstract = {We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term...},
  langid = {english},
  keywords = {ode,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\738XG2RC\\forum.html}
}

@article{rusuProgressiveNeuralNetworks2016,
  title = {Progressive {{Neural Networks}}},
  author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  year = {2016},
  month = jun,
  abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  keywords = {architectural,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YLRU7CW3\\Rusu et al. - 2016 - Progressive Neural Networks(3).pdf}
}

@inproceedings{ryabininScalingEnsembleDistribution2021,
  title = {Scaling {{Ensemble Distribution Distillation}} to {{Many Classes}} with {{Proxy Targets}}},
  booktitle = {Thirty-{{Fifth Conference}} on {{Neural Information Processing Systems}}},
  author = {Ryabinin, Max and Malinin, Andrey and Gales, Mark},
  year = {2021},
  month = may,
  abstract = {We overcome unstable training and show how to scale Ensemble Distribution Distillation to classification tasks with arbitrary numbers of classes.},
  langid = {english},
  keywords = {dirichlet,ensemble,exmodel,knowledge distillation,notag},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Ryabinin et al_2021_Scaling Ensemble Distribution Distillation to Many Classes with Proxy Targets.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EXK87FJ6\\forum.html}
}

@article{sabourDynamicRoutingCapsules2017,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  year = {2017},
  number = {Nips},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3MEHCKUT\\Sabour, Frosst, Hinton - 2017 - Dynamic Routing Between Capsules.pdf}
}

@article{sainathConvolutionalNeuralNetworks,
  title = {Convolutional {{Neural Networks}} for {{Small-Footprint Keyword Spotting}}},
  author = {Sainath, Tara N and Parada, Carolina},
  pages = {5},
  abstract = {We explore using Convolutional Neural Networks (CNNs) for a small-footprint keyword spotting (KWS) task. CNNs are attractive for KWS since they have been shown to outperform DNNs with far fewer parameters. We consider two different applications in our work, one where we limit the number of multiplications of the KWS system, and another where we limit the number of parameters. We present new CNN architectures to address the constraints of each applications. We find that the CNN architectures offer between a 27-44\% relative improvement in false reject rate compared to a DNN, while fitting into the constraints of each application.},
  langid = {english},
  keywords = {CNN,embedded,speech,SpeechCommands},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NZB984RL\\Sainath and Parada - Convolutional Neural Networks for Small-Footprint .pdf}
}

@inproceedings{salakhutdinovEfficientLearningDeep2010,
  title = {Efficient Learning of Deep {{Boltzmann}} Machines},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Salakhutdinov, Ruslan and Larochelle, Hugo},
  year = {2010},
  pages = {693--700},
  keywords = {RBM}
}

@inproceedings{salakhutdinovQuantitativeAnalysisDeep2008,
  title = {On the Quantitative Analysis of Deep Belief Networks},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Salakhutdinov, Ruslan and Murray, Iain},
  year = {2008},
  pages = {872--879},
  publisher = {{ACM}},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\I4S4P44M\\dbn_ais.pdf}
}

@article{salimansImprovedTechniquesTraining2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.03498 [cs]},
  eprint = {1606.03498},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  archiveprefix = {arXiv},
  keywords = {exmodel,GAN},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Salimans et al_2016_Improved Techniques for Training GANs.pdf;C\:\\Users\\w-32\\Zotero\\storage\\J77A9I9S\\1606.html}
}

@article{salmanSparsityImplicitGating2019,
  title = {Sparsity as the {{Implicit Gating Mechanism}} for {{Residual Blocks}}},
  author = {Salman, Shaeke and Liu, Xiuwen},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\K96T68X8\\Salman, Liu - 2019 - Sparsity as the Implicit Gating Mechanism for Residual Blocks(2).pdf}
}

@article{santoroMeasuringAbstractReasoning2018,
  title = {Measuring Abstract Reasoning in Neural Networks},
  author = {Santoro, Adam and Hill, Felix and Barrett, David and Morcos, Ari and Lillicrap, Timothy},
  year = {2018},
  month = jul,
  pages = {4477--4486},
  keywords = {relational},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SGCZTEA3\\Santoro et al. - 2018 - Measuring abstract reasoning in neural networks.pdf}
}

@article{santoroRelationalRecurrentNeural2018,
  title = {Relational Recurrent Neural Networks},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Th{\'e}ophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  year = {2018},
  journal = {Advances in Neural Information Processing Systems},
  volume = {2018-Decem},
  pages = {7299--7310},
  abstract = {Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a \textbackslash textit\{Relational Memory Core\} (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.},
  keywords = {relational,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TS3JG95J\\Santoro et al. - 2018 - Relational recurrent neural networks.pdf}
}

@article{santoroSimpleNeuralNetwork2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  year = {2017},
  month = jun,
  abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
  keywords = {relational},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FGGHYBDX\\Santoro et al. - 2017 - A simple neural network module for relational reasoning(3).pdf}
}

@misc{sattlerFedAUXLeveragingUnlabeled2021,
  title = {{{FedAUX}}: {{Leveraging Unlabeled Auxiliary Data}} in {{Federated Learning}}},
  shorttitle = {{{FedAUX}}},
  author = {Sattler, Felix and Korjakow, Tim and Rischke, Roman and Samek, Wojciech},
  year = {2021},
  month = feb,
  number = {arXiv:2102.02514},
  eprint = {2102.02514},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.02514},
  abstract = {Federated Distillation (FD) is a popular novel algorithmic paradigm for Federated Learning, which achieves training performance competitive to prior parameter averaging based methods, while additionally allowing the clients to train different model architectures, by distilling the client predictions on an unlabeled auxiliary set of data into a student model. In this work we propose FedAUX, an extension to FD, which, under the same set of assumptions, drastically improves performance by deriving maximum utility from the unlabeled auxiliary data. FedAUX modifies the FD training procedure in two ways: First, unsupervised pre-training on the auxiliary data is performed to find a model initialization for the distributed training. Second, \$(\textbackslash varepsilon, \textbackslash delta)\$-differentially private certainty scoring is used to weight the ensemble predictions on the auxiliary data according to the certainty of each client model. Experiments on large-scale convolutional neural networks and transformer models demonstrate, that the training performance of FedAUX exceeds SOTA FL baseline methods by a substantial margin in both the iid and non-iid regime, further closing the gap to centralized training performance. Code is available at github.com/fedl-repo/fedaux.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,federated,privacy,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DIVVZKV2\\2102.html}
}

@article{saxeExactSolutionsNonlinear2014,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = {2014},
  month = feb,
  journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
  eprint = {1312.6120},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, q-bio, stat},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LTSDJC6Y\\Saxe et al_2014_Exact solutions to the nonlinear dynamics of learning in deep linear neural.pdf;C\:\\Users\\w-32\\Zotero\\storage\\8MPA534W\\1312.html}
}

@inproceedings{saxeInformationBottleneckTheory2018,
  title = {On the {{Information Bottleneck Theory}} of {{Deep Learning}}},
  author = {Saxe, Andrew Michael and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan Daniel and Cox, David Daniel},
  year = {2018},
  month = feb,
  abstract = {Abstract: The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period. TL;DR: We show that several claims of the information bottleneck theory of deep learning are not true in the general case. Keywords: information bottleneck, deep learning, deep linear networks},
  keywords = {info-bottleneck}
}

@article{saxeInformationBottleneckTheory2019,
  title = {On the Information Bottleneck Theory of Deep Learning},
  author = {Saxe, Andrew M. and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D. and Cox, David D.},
  year = {2019},
  month = dec,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2019},
  number = {12},
  pages = {124020},
  publisher = {{IOP Publishing}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ab3985},
  abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case, and instead reflect assumptions made to compute a finite mutual information metric in deterministic networks. When computed using simple binning, we demonstrate through a combination of analytical results and simulation that the information plane trajectory observed in prior work is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.},
  langid = {english},
  keywords = {information-bottleneck,learning-dynamics}
}

@article{schaalReceptiveFieldWeighted1997,
  title = {Receptive {{Field Weighted Regression}}},
  author = {Schaal, Stefan and Atkeson, Christopher G},
  year = {1997},
  journal = {Learning},
  abstract = {Neurons selective to oriented visual stimuli participate in the early steps of visual signal processes in higher mammals. A highly ordered neural connectivity responds to such a selectivity. The information necessary to establish this connectivity is too large to be contained in a genome. Moreover, neurophysiological experiments show that the alteration of small parts of the developing visual neural system affects the normal development of the whole system. Self-organizing processes able to produce local order from general properties have already been proposed as the driving force of these development stages. In order to model these self-organizing processes we study a two layer neural network based on simple organizational rules such as (1) diffusion of neural signal in the same layer, (2) Hebbian and anti-Hebbian learning, and (3) individual restrictions to the growth of each neuronal connection. We simulate the development of neurons selective to orientation and size organized in a map, underscoring the importance of anti-Hebbian learning for normal neural visual system development. Copyright 1997 Elsevier Science Ltd.},
  keywords = {gated linear,incremental learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\48LU84ZR\\Schaal, Atkeson - 1997 - Receptive Field Weighted Regression(2).pdf}
}

@article{schaettiBehaviorsReservoirComputing2019,
  title = {Behaviors of {{Reservoir Computing Models}} for {{Textual Documents Classification}}},
  author = {Schaetti, Nils},
  year = {2019},
  volume = {50},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {ESN,nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\I8MWPBHS\\Schaetti - 2019 - Behaviors of Reservoir Computing Models for Textual Documents Classification.pdf}
}

@article{schallerMooreLawPresent1997,
  title = {Moore's Law: Past, Present and Future},
  author = {Schaller, Robert R},
  year = {1997},
  journal = {IEEE spectrum},
  volume = {34},
  number = {6},
  pages = {52--59}
}

@article{schmidtInferringDynamicalSystems2019,
  title = {Inferring {{Dynamical Systems}} with {{Long-Range Dependencies}} through {{Line Attractor Regularization}}},
  author = {Schmidt, Dominik and Koppe, Georgia and Beutelspacher, Max and Durstewitz, Daniel},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.03471 [cs, q-bio, stat]},
  eprint = {1910.03471},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {Vanilla RNN with ReLU activation have a simple structure that is amenable to systematic dynamical systems analysis and interpretation, but they suffer from the exploding vs. vanishing gradients problem. Recent attempts to retain this simplicity while alleviating the gradient problem are based on proper initialization schemes or orthogonality/unitary constraints on the RNN's recurrence matrix, which, however, comes with limitations to its expressive power with regards to dynamical systems phenomena like chaos or multi-stability. Here, we instead suggest a regularization scheme that pushes part of the RNN's latent subspace toward a line attractor configuration that enables long short-term memory and arbitrarily slow time scales. We show that our approach excels on a number of benchmarks like the sequential MNIST or multiplication problems, and enables reconstruction of dynamical systems which harbor widely different time scales.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,linear,memory,Quantitative Biology - Quantitative Methods,rnn,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\H7P4BY5R\\Schmidt et al. - 2019 - Inferring Dynamical Systems with Long-Range Depend.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NP4DRPSC\\1910.html}
}

@inproceedings{schmidtIntroducingWESADMultimodal2018,
  title = {Introducing {{WESAD}}, a {{Multimodal Dataset}} for {{Wearable Stress}} and {{Affect Detection}}},
  booktitle = {Proceedings of the 20th {{ACM International Conference}} on {{Multimodal Interaction}}},
  author = {Schmidt, Philip and Reiss, Attila and Duerichen, Robert and Marberger, Claus and Van Laerhoven, Kristof},
  year = {2018},
  month = oct,
  pages = {400--408},
  publisher = {{ACM}},
  address = {{Boulder CO USA}},
  doi = {10.1145/3242969.3242985},
  abstract = {Affect recognition aims to detect a person's affective state based on observables, with the goal to e.g. improve human-computer interaction. Long-term stress is known to have severe implications on wellbeing, which call for continuous and automated stress monitoring systems. However, the affective computing community lacks commonly used standard datasets for wearable stress detection which a) provide multimodal high-quality data, and b) include multiple affective states. Therefore, we introduce WESAD, a new publicly available dataset for wearable stress and affect detection. This multimodal dataset features physiological and motion data, recorded from both a wrist- and a chest-worn device, of 15 subjects during a lab study. The following sensor modalities are included: blood volume pulse, electrocardiogram, electrodermal activity, electromyogram, respiration, body temperature, and threeaxis acceleration. Moreover, the dataset bridges the gap between previous lab studies on stress and emotions, by containing three different affective states (neutral, stress, amusement). In addition, self-reports of the subjects, which were obtained using several established questionnaires, are contained in the dataset. Furthermore, a benchmark is created on the dataset, using well-known features and standard machine learning methods. Considering the threeclass classification problem (baseline vs. stress vs. amusement), we achieved classification accuracies of up to 80 \%. In the binary case (stress vs. non-stress), accuracies of up to 93 \% were reached. Finally, we provide a detailed analysis and comparison of the two device locations (chest vs. wrist) as well as the different sensor modalities.},
  isbn = {978-1-4503-5692-3},
  langid = {english},
  keywords = {dataset},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EZF4P7XP\\Schmidt et al. - 2018 - Introducing WESAD, a Multimodal Dataset for Wearab.pdf}
}

@article{schonemannGeneralizedSolutionOrthogonal1966,
  title = {A Generalized Solution of the Orthogonal Procrustes Problem},
  author = {Sch{\"o}nemann, Peter H.},
  year = {1966},
  month = mar,
  journal = {Psychometrika},
  volume = {31},
  number = {1},
  pages = {1--10},
  doi = {10.1007/BF02289451},
  keywords = {math}
}

@article{schuesslerInterplayRandomnessStructure,
  title = {The Interplay between Randomness and Structure during Learning in {{RNNs}}},
  author = {Schuessler, Friedrich and Mastrogiuseppe, Francesca and Dubreuil, Alexis and Ostojic, Srdjan and Barak, Omri},
  pages = {11},
  abstract = {Recurrent neural networks (RNNs) trained on low-dimensional tasks have been widely used to model functional biological networks. However, the solutions found by learning and the effect of initial connectivity are not well understood. Here, we examine RNNs trained using gradient descent on different tasks inspired by the neuroscience literature. We find that the changes in recurrent connectivity can be described by low-rank matrices, despite the unconstrained nature of the learning algorithm. To identify the origin of the low-rank structure, we turn to an analytically tractable setting: training a linear RNN on a simplified task. We show how the low-dimensional task structure leads to low-rank changes to connectivity. This low-rank structure allows us to explain and quantify the phenomenon of accelerated learning in the presence of random initial connectivity. Altogether, our study opens a new perspective to understanding trained RNNs in terms of both the learning process and the resulting network structure.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SY74HIHJ\\Schuessler et al. - The interplay between randomness and structure dur.pdf}
}

@article{schuesslerInterplayRandomnessStructure2020,
  title = {The Interplay between Randomness and Structure during Learning in {{RNNs}}},
  author = {Schuessler, Friedrich and Mastrogiuseppe, Francesca and Dubreuil, Alexis and Ostojic, Srdjan and Barak, Omri},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.11036 [q-bio]},
  eprint = {2006.11036},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {Recurrent neural networks (RNNs) trained on low-dimensional tasks have been widely used to model functional biological networks. However, the solutions found by learning and the effect of initial connectivity are not well understood. Here, we examine RNNs trained using gradient descent on different tasks inspired by the neuroscience literature. We find that the changes in recurrent connectivity can be described by low-rank matrices, despite the unconstrained nature of the learning algorithm. To identify the origin of the low-rank structure, we turn to an analytically tractable setting: training a linear RNN on a simplified task. We show how the low-dimensional task structure leads to low-rank changes to connectivity. This low-rank structure allows us to explain and quantify the phenomenon of accelerated learning in the presence of random initial connectivity. Altogether, our study opens a new perspective to understanding trained RNNs in terms of both the learning process and the resulting network structure.},
  archiveprefix = {arXiv},
  keywords = {interference,linear,low-rank,neuroscience,Quantitative Biology - Neurons and Cognition,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LJ4LHPFR\\Schuessler et al_2020_The interplay between randomness and structure during learning in RNNs.pdf;C\:\\Users\\w-32\\Zotero\\storage\\3ID7YGXR\\2006.html}
}

@article{schwarzNaturalBenchmarkContinual2018,
  title = {Towards a Natural Benchmark for Continual Learning},
  author = {Schwarz, Jonathan and Altman, Daniel and Dudzik, Andrew and Vinyals, Oriol and Whye Teh, Yee and Pascanu, Razvan},
  year = {2018},
  journal = {Continual learning Workshop NeurIPS},
  number = {Cl},
  abstract = {Continual Learning (CL) has recently seen a surge of interest, highlighting the importance of the topic for machine learning and artificial intelligence. While most recent work has been dedicated towards algorithmic improvements, robust evaluation remains unsolved - a shortcoming we explore in this work. The CL problem is usually described as a list of desiderata when facing a non-stationary stream of data, in most cases structured as a known sequence of different tasks. The goal of a CL scheme is then to leverage previously acquired skills when facing new problems, but also to retain skills and perform well on previously encountered tasks. Ideally, a continual learning algorithm does so with constant memory and computational footprint in order to scale, and can learn without being aware of task boundaries. These different desiderata are in tension with each other, making both problem specification and evaluation difficult. In this work, we explore the relationship between data and evaluation and argue that one needs to test the system in natural scenarios. Thus, we propose a new benchmark based on the popular video game StarCraft II in the hope to better understand existing approaches to continual learning. A video of human play on the proposed benchmark can be found here: https://goo.gl/vdzkut.},
  keywords = {continual,data,evaluation,starcraft},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\T37C35IF\\Schwarz et al. - 2018 - Towards a natural benchmark for continual learning(2).pdf}
}

@article{schwarzProgressCompressScalable2018,
  title = {Progress \& Compress: {{A}} Scalable Framework for Continual Learning},
  author = {Schwarz, Jonathan and Luketina, Jelena and Czarnecki, Wojciech M. and {Grabska-Barwinska}, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  year = {2018},
  journal = {35th International Conference on Machine Learning, ICML 2018},
  volume = {10},
  pages = {7199--7208},
  issn = {9781510867963},
  abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress \& compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.},
  keywords = {architectural,continual,ewc,ICML,regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ITM3IDZK\\Schwarz et al. - 2018 - Progress & compress A scalable framework for continual learning.pdf}
}

@inproceedings{sennrichImprovingNeuralMachine2016,
  title = {Improving {{Neural Machine Translation Models}} with {{Monolingual Data}}},
  booktitle = {{{ACL}}},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  month = nov,
  doi = {10.5606/ArchRheumatol.2018.6327},
  abstract = {Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English{$<$}-{$>$}German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish-{$>$}English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English-{$>$}German.},
  isbn = {978-1-5108-2758-5},
  keywords = {GAN,nlp,NMT,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\48Z9KJV8\\Sennrich, Haddow, Birch - 2016 - Improving Neural Machine Translation Models with Monolingual Data(2).pdf}
}

@inproceedings{sennrichNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  booktitle = {{{ACL}}},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  month = aug,
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  keywords = {nlp,NMT},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AP2IFPDW\\Sennrich, Haddow, Birch - 2016 - Neural Machine Translation of Rare Words with Subword Units.pdf}
}

@misc{seoFederatedKnowledgeDistillation2020,
  title = {Federated {{Knowledge Distillation}}},
  author = {Seo, Hyowoon and Park, Jihong and Oh, Seungeun and Bennis, Mehdi and Kim, Seong-Lyun},
  year = {2020},
  month = nov,
  number = {arXiv:2011.02367},
  eprint = {2011.02367},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.02367},
  abstract = {Distributed learning frameworks often rely on exchanging model parameters across workers, instead of revealing their raw data. A prime example is federated learning that exchanges the gradients or weights of each neural network model. Under limited communication resources, however, such a method becomes extremely costly particularly for modern deep neural networks having a huge number of model parameters. In this regard, federated distillation (FD) is a compelling distributed learning solution that only exchanges the model outputs whose dimensions are commonly much smaller than the model sizes (e.g., 10 labels in the MNIST dataset). The goal of this chapter is to provide a deep understanding of FD while demonstrating its communication efficiency and applicability to a variety of tasks. To this end, towards demystifying the operational principle of FD, the first part of this chapter provides a novel asymptotic analysis for two foundational algorithms of FD, namely knowledge distillation (KD) and co-distillation (CD), by exploiting the theory of neural tangent kernel (NTK). Next, the second part elaborates on a baseline implementation of FD for a classification task, and illustrates its performance in terms of accuracy and communication efficiency compared to FL. Lastly, to demonstrate the applicability of FD to various distributed learning tasks and environments, the third part presents two selected applications, namely FD over asymmetric uplink-and-downlink wireless channels and FD for reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,federated,knowledge-distillation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NQ729NGQ\\2011.html}
}

@inproceedings{serraOvercomingCatastrophicForgetting2018,
  title = {Overcoming {{Catastrophic Forgetting}} with {{Hard Attention}} to the {{Task}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Serra, Joan and Suris, Didac and Miron, Marius and Karatzoglou, Alexandros},
  year = {2018},
  month = jul,
  pages = {4548--4557},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
  langid = {english},
  keywords = {continual,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RXVDXGRI\\Serra et al_2018_Overcoming Catastrophic Forgetting with Hard Attention to the Task.pdf}
}

@article{shahLearningRecognizeCodeswitched2020,
  title = {Learning to {{Recognize Code-switched Speech Without Forgetting Monolingual Speech Recognition}}},
  author = {Shah, Sanket and Abraham, Basil and M, Gurunath Reddy and Sitaram, Sunayana and Joshi, Vikas},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.00782 [cs, eess]},
  eprint = {2006.00782},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Recently, there has been significant progress made in Automatic Speech Recognition (ASR) of code-switched speech, leading to gains in accuracy on code-switched datasets in many language pairs. Code-switched speech co-occurs with monolingual speech in one or both languages being mixed. In this work, we show that fine-tuning ASR models on code-switched speech harms performance on monolingual speech. We point out the need to optimize models for code-switching while also ensuring that monolingual performance is not sacrificed. Monolingual models may be trained on thousands of hours of speech which may not be available for re-training a new model. We propose using the Learning Without Forgetting (LWF) framework for code-switched ASR when we only have access to a monolingual model and do not have the data it was trained on. We show that it is possible to train models using this framework that perform well on both code-switched and monolingual test sets. In cases where we have access to monolingual training data as well, we propose regularization strategies for fine-tuning models for code-switching without sacrificing monolingual accuracy. We report improvements in Word Error Rate (WER) in monolingual and code-switched test sets compared to baselines that use pooled data and simple fine-tuning.},
  archiveprefix = {arXiv},
  keywords = {code-switching,continual,finetuning,lwf,speech},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HDDXMJUP\\Shah et al. - 2020 - Learning to Recognize Code-switched Speech Without.pdf;C\:\\Users\\w-32\\Zotero\\storage\\2K38WJ8G\\2006.html}
}

@article{shakerLearningTransferNeumann2022,
  title = {Learning to {{Transfer}} with von {{Neumann Conditional Divergence}}},
  author = {Shaker, Ammar and Yu, Shujian and {O{\~n}oro-Rubio}, Daniel},
  year = {2022},
  month = jan,
  journal = {arXiv:2108.03531 [cs, stat]},
  eprint = {2108.03531},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The similarity of feature representations plays a pivotal role in the success of problems related to domain adaptation. Feature similarity includes both the invariance of marginal distributions and the closeness of conditional distributions given the desired response \$y\$ (e.g., class labels). Unfortunately, traditional methods always learn such features without fully taking into consideration the information in \$y\$, which in turn may lead to a mismatch of the conditional distributions or the mix-up of discriminative structures underlying data distributions. In this work, we introduce the recently proposed von Neumann conditional divergence to improve the transferability across multiple domains. We show that this new divergence is differentiable and eligible to easily quantify the functional dependence between features and \$y\$. Given multiple source tasks, we integrate this divergence to capture discriminative information in \$y\$ and design novel learning objectives assuming those source tasks are observed either simultaneously or sequentially. In both scenarios, we obtain favorable performance against state-of-the-art methods in terms of smaller generalization error on new tasks and less catastrophic forgetting on source tasks (in the sequential setup).},
  archiveprefix = {arXiv},
  keywords = {exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BAA6J4RM\\Shaker et al_2022_Learning to Transfer with von Neumann Conditional Divergence.pdf;C\:\\Users\\w-32\\Zotero\\storage\\3CSLBN5K\\2108.html}
}

@article{shamirAreResNetsProvably2018,
  title = {Are {{ResNets Provably Better}} than {{Linear Predictors}}?},
  author = {Shamir, Ohad},
  year = {2018},
  month = apr,
  abstract = {A residual network (or ResNet) is a standard deep neural net architecture, with state-of-the-art performance across numerous applications. The main premise of ResNets is that they allow the training of each layer to focus on fitting just the residual of the previous layer's output and the target output. Thus, we should expect that the trained network is no worse than what we can obtain if we remove the residual layers and train a shallower network instead. However, due to the non-convexity of the optimization problem, it is not at all clear that ResNets indeed achieve this behavior, rather than getting stuck at some arbitrarily poor local minimum. In this paper, we rigorously prove that arbitrarily deep, nonlinear residual units indeed exhibit this behavior, in the sense that the optimization landscape contains no local minima with value above what can be obtained with a linear predictor (namely a 1-layer network). Notably, we show this under minimal or no assumptions on the precise network architecture, data distribution, or loss function used. We also provide a quantitative analysis of approximate stationary points for this problem. Finally, we show that with a certain tweak to the architecture, training the network with standard stochastic gradient descent achieves an objective value close or better than any linear predictor.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\D97UTT4J\\Shamir - 2018 - Are ResNets Provably Better than Linear Predictors.pdf}
}

@article{shaoNeighbourhoodDistillationBenefits2020,
  title = {Neighbourhood {{Distillation}}: {{On}} the Benefits of Non End-to-End Distillation},
  shorttitle = {Neighbourhood {{Distillation}}},
  author = {Shao, La{\"e}titia and Moroz, Max and Eban, Elad and {Movshovitz-Attias}, Yair},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.01189 [cs, stat]},
  eprint = {2010.01189},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {End-to-end training with back propagation is the standard method for training deep neural networks. However, as networks become deeper and bigger, end-to-end training becomes more challenging: highly non-convex models gets stuck easily in local optima, gradients signals are prone to vanish or explode during back-propagation, training requires computational resources and time. In this work, we propose to break away from the end-to-end paradigm in the context of Knowledge Distillation. Instead of distilling a model end-to-end, we propose to split it into smaller sub-networks - also called neighbourhoods - that are then trained independently. We empirically show that distilling networks in a non end-to-end fashion can be beneficial in a diverse range of use cases. First, we show that it speeds up Knowledge Distillation by exploiting parallelism and training on smaller networks. Second, we show that independently distilled neighbourhoods may be efficiently re-used for Neural Architecture Search. Finally, because smaller networks model simpler functions, we show that they are easier to train with synthetic data than their deeper counterparts.},
  archiveprefix = {arXiv},
  keywords = {data-free,exmodel,knowledge distillation},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Shao et al_2020_Neighbourhood Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\SFVSKBXE\\2010.html}
}

@article{shazeerOutrageouslyLargeNeural2017,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = {2017},
  month = jan,
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  keywords = {rnn-gates},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NESXMQXI\\Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-Gated Mixture-of-Experts Layer(2).pdf}
}

@article{sheffetDifferentiallyPrivateOrdinary2017,
  title = {Differentially {{Private Ordinary Least Squares}}},
  author = {Sheffet, Or},
  year = {2017},
  month = aug,
  journal = {arXiv:1507.02482 [cs]},
  eprint = {1507.02482},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Linear regression is one of the most prevalent techniques in machine learning, however, it is also common to use linear regression for its \textbackslash emph\{explanatory\} capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS assumes a particular model that randomly generates the data, and derives \textbackslash emph\{\$t\$-values\} --- representing the likelihood of each real value to be the true correlation. Using \$t\$-values, OLS can release a \textbackslash emph\{confidence interval\}, which is an interval on the reals that is likely to contain the true correlation, and when this interval does not intersect the origin, we can \textbackslash emph\{reject the null hypothesis\} as it is likely that the true correlation is non-zero. Our work aims at achieving similar guarantees on data under differentially private estimators. First, we show that for well-spread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of \$t\$-values, secondly, when JLT approximates Ridge regression (linear regression with \$l\_2\$-regularization) we derive, under certain conditions, confidence intervals using the projected data, lastly, we derive, under different conditions, confidence intervals for the "Analyze Gauss" algorithm (Dwork et al, STOC 2014).},
  archiveprefix = {arXiv},
  keywords = {diff-privacy,project-dp-rnn-galli},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3893GG75\\1507.html}
}

@misc{shenConnectNotCollapse2022a,
  title = {Connect, {{Not Collapse}}: {{Explaining Contrastive Learning}} for {{Unsupervised Domain Adaptation}}},
  shorttitle = {Connect, {{Not Collapse}}},
  author = {Shen, Kendrick and Jones, Robbie and Kumar, Ananya and Xie, Sang Michael and HaoChen, Jeff Z. and Ma, Tengyu and Liang, Percy},
  year = {2022},
  month = jul,
  number = {arXiv:2204.00570},
  eprint = {2204.00570},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.00570},
  abstract = {We consider unsupervised domain adaptation (UDA), where labeled data from a source domain (e.g., photographs) and unlabeled data from a target domain (e.g., sketches) are used to learn a classifier for the target domain. Conventional UDA methods (e.g., domain adversarial training) learn domain-invariant features to improve generalization to the target domain. In this paper, we show that contrastive pre-training, which learns features on unlabeled source and target data and then fine-tunes on labeled source data, is competitive with strong UDA methods. However, we find that contrastive pre-training does not learn domain-invariant features, diverging from conventional UDA intuitions. We show theoretically that contrastive pre-training can learn features that vary subtantially across domains but still generalize to the target domain, by disentangling domain and class information. Our results suggest that domain invariance is not necessary for UDA. We empirically validate our theory on benchmark vision datasets.},
  archiveprefix = {arXiv},
  keywords = {notag,self-supervised,toread,uda},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\M3QNN66R\\2204.html}
}

@inproceedings{shenStyleTransferNonparallel2017,
  title = {Style Transfer from Non-Parallel Text by Cross-Alignment},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  year = {2017},
  pages = {6830--6841},
  keywords = {nlp,style-transfer}
}

@inproceedings{shiMimickingOracleInitial2022,
  title = {Mimicking the {{Oracle}}: {{An Initial Phase Decorrelation Approach}} for {{Class Incremental Learning}}},
  shorttitle = {Mimicking the {{Oracle}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shi, Yujun and Zhou, Kuangqi and Liang, Jian and Jiang, Zihang and Feng, Jiashi and Torr, Philip H. S. and Bai, Song and Tan, Vincent Y. F.},
  year = {2022},
  pages = {16722--16731},
  langid = {english},
  keywords = {Continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XF37XZDL\\Shi_Mimicking_the_Oracle_An_Initial_Phase_Decorrelation_Approach_for_Class_CVPR_2022_paper.html}
}

@inproceedings{shinInterpretingWordEmbeddings2018,
  title = {Interpreting {{Word Embeddings}} with {{Eigenvector Analysis}}},
  author = {Shin, Jamin and Madotto, Andrea and Fung, Pascale},
  year = {2018},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CGL7U8GD\\Shin, Madotto, Fung - 2018 - Interpreting Word Embeddings with Eigenvector Analysis(2).pdf}
}

@article{siegelmannComputationalPowerNeural1995,
  title = {On the Computational Power of Neural Nets},
  author = {Siegelmann, Hava T and Sontag, Eduardo D},
  year = {1995},
  journal = {Journal of computer and system sciences},
  volume = {50},
  number = {1},
  pages = {132--150},
  keywords = {generalization,learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AH7NJKEL\\Siegelmann, Sontag - 1995 - On the computational power of neural nets(2).pdf}
}

@article{silvaExploringTimeseriesMotifs,
  title = {Exploring Time-Series Motifs through {{DTW-SOM}}},
  author = {Silva, Maria Ines and Henriques, Roberto},
  pages = {8},
  abstract = {Motif discovery is a fundamental step in data mining tasks for time-series data such as clustering, classification and anomaly detection. Even though many papers have addressed the problem of how to find motifs in time-series by proposing new motif discovery algorithms, not much work has been done on the exploration of the motifs extracted by these algorithms. In this paper, we argue that visually exploring time-series motifs computed by motif discovery algorithms can be useful to understand and debug results.},
  langid = {english},
  keywords = {motif,time-series,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZPPN65PJ\\Silva and Henriques - Exploring time-series motifs through DTW-SOM.pdf}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\X6PJTBY5\\AlphaGo nature article.pdf}
}

@inproceedings{simonGeneralizingDomainsCrossDomain2022,
  title = {On {{Generalizing Beyond Domains}} in {{Cross-Domain Continual Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Simon, Christian and Faraki, Masoud and Tsai, Yi-Hsuan and Yu, Xiang and Schulter, Samuel and Suh, Yumin and Harandi, Mehrtash and Chandraker, Manmohan},
  year = {2022},
  pages = {9265--9274},
  langid = {english},
  keywords = {Continual,knowledge-distillation,moving-average-model},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TPXIJZI2\\Simon_On_Generalizing_Beyond_Domains_in_Cross-Domain_Continual_Learning_CVPR_2022_paper.html}
}

@article{simonyanDeepConvolutionalNetworks2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014},
  month = apr,
  journal = {arXiv:1312.6034 [cs]},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,vision},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Simonyan et al_2014_Deep Inside Convolutional Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\5XQAMQTE\\1312.html}
}

@article{simonyanVeryDeepConvolutional2014,
  ids = {simonyanVeryDeepConvolutional2014a},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.1556},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {CNN}
}

@inproceedings{simsEvolvingVirtualCreatures1994,
  title = {Evolving Virtual Creatures},
  booktitle = {Proceedings of the 21st Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '94},
  author = {Sims, Karl},
  year = {1994},
  pages = {15--22},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  doi = {10.1145/192161.192167},
  abstract = {This paper describes a novel system for creating virtual creatures that move and behave in simulated three-dimensional physical worlds. The morphologies of creatures and the neural systems for controlling their muscle forces are both generated automatically using genetic algorithms. Different fitness evaluation functions are used to direct simulated evolutions towards specific behaviors such as swimming, walking, jumping, and following.},
  isbn = {978-0-89791-667-7},
  langid = {english},
  keywords = {genart,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PCHQK4NT\\Sims - 1994 - Evolving virtual creatures.pdf}
}

@misc{singhModelFusionOptimal2021,
  title = {Model {{Fusion}} via {{Optimal Transport}}},
  author = {Singh, Sidak Pal and Jaggi, Martin},
  year = {2021},
  month = feb,
  number = {arXiv:1910.05653},
  eprint = {1910.05653},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.05653},
  abstract = {Combining different models is a widely used paradigm in machine learning applications. While the most common approach is to form an ensemble of models and average their individual predictions, this approach is often rendered infeasible by given resource constraints in terms of memory and computation, which grow linearly with the number of models. We present a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters. We show that this can successfully yield "one-shot" knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings , we illustrate that our approach significantly outperforms vanilla averaging, as well as how it can serve as an efficient replacement for the ensemble with moderate fine-tuning, for standard convolutional networks (like VGG11), residual networks (like ResNet18), and multi-layer perceptrons on CIFAR10, CIFAR100, and MNIST. Finally, our approach also provides a principled way to combine the parameters of neural networks with different widths, and we explore its application for model compression. The code is available at the following link, https://github.com/sidak/otfusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,model-patching,optimal-transport,Statistics - Machine Learning,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XNQYZ2YA\\Singh_Jaggi_2021_Model Fusion via Optimal Transport.pdf;C\:\\Users\\w-32\\Zotero\\storage\\LNLRWAG4\\1910.html}
}

@inproceedings{slimDatasetKnowledgeTransfer2022,
  title = {Dataset {{Knowledge Transfer}} for {{Class-Incremental Learning Without Memory}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Slim, Habib and Belouadah, Eden and Popescu, Adrian and Onchis, Darian},
  year = {2022},
  pages = {483--492},
  langid = {english},
  keywords = {cil,continual,notag,replay-free},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\J7I4664U\\Slim_Dataset_Knowledge_Transfer_for_Class-Incremental_Learning_Without_Memory_WACV_2022_paper.html}
}

@inproceedings{smithAlwaysBeDreaming2021,
  ids = {smithAlwaysBeDreaming2021a},
  title = {Always {{Be Dreaming}}: {{A New Approach}} for {{Data-Free Class-Incremental Learning}}},
  shorttitle = {Always {{Be Dreaming}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Smith, James and Hsu, Yen-Chang and Balloch, Jonathan and Shen, Yilin and Jin, Hongxia and Kira, Zsolt},
  year = {2021},
  eprint = {2106.09701},
  eprinttype = {arxiv},
  pages = {9374--9384},
  issn = {2380-7504},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Benchmark testing,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer vision,continual,data-distill,data-free,exmodel,generative,Law,Learning systems,Memory management,Recognition and classification,Training,Training data,Transfer/Low-shot/Semi/Unsupervised Learning},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Smith et al_2021_Always Be Dreaming.pdf;C\:\\Users\\w-32\\Zotero\\storage\\NT75EXQJ\\2106.09701.pdf;C\:\\Users\\w-32\\Zotero\\storage\\GFK7YSAX\\2106.html;C\:\\Users\\w-32\\Zotero\\storage\\NHBWP57X\\Smith_Always_Be_Dreaming_A_New_Approach_for_Data-Free_Class-Incremental_Learning_ICCV_2021_pape.html;C\:\\Users\\w-32\\Zotero\\storage\\TR44AQYY\\9711051.html}
}

@techreport{smolenskyInformationProcessingDynamical1986,
  title = {Information Processing in Dynamical Systems: {{Foundations}} of Harmony Theory},
  author = {Smolensky, Paul},
  year = {1986}
}

@misc{snellPrototypicalNetworksFewshot2017,
  title = {Prototypical {{Networks}} for {{Few-shot Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  year = {2017},
  month = jun,
  number = {arXiv:1703.05175},
  eprint = {1703.05175},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1703.05175},
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  archiveprefix = {arXiv},
  keywords = {meta-learning,notag,prototype,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PHTXG2YM\\Snell et al_2017_Prototypical Networks for Few-shot Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\J8HIJAG4\\1703.html}
}

@inproceedings{socherRecursiveDeepModels2013,
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality}}},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  year = {2013},
  pages = {1631--1642},
  doi = {10.1371/journal.pone.0073791},
  isbn = {978-1-937284-97-8},
  keywords = {nlp,tree},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZQSLRN7W\\Socher et al. - 2013 - Recursive Deep Models for Semantic Compositionality(3).pdf}
}

@inproceedings{socherSemanticCompositionalityRecursive2012,
  title = {Semantic Compositionality through Recursive Matrix-Vector Spaces},
  booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  author = {Socher, Richard and Huval, Brody and Manning, Christopher D. and Ng, Andrew Y.},
  year = {2012},
  pages = {1201--1211},
  publisher = {{Association for Computational Linguistics}},
  keywords = {nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2FJEBXLV\\SemanticCompositionalitythroughRecursiveMatrix-VectorSpaces.pdf}
}

@article{socherSemisupervisedRecursiveAutoencoders2011,
  title = {Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions},
  author = {Socher, Richard and Pennington, Jeffrey and Huang, Eric H. and Ng, Andrew Y. and Manning, Christopher D.},
  year = {2011},
  journal = {Conference on Empirical Methods in Natural Language Processing, EMNLP},
  number = {i},
  pages = {151--161},
  issn = {978-1-937284-11-4},
  abstract = {We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. \^A\textcopyright{} 2011 Association for Computational Linguistics.},
  keywords = {nlp,tree}
}

@misc{sodhaniIntroductionLifelongSupervised2022,
  title = {An {{Introduction}} to {{Lifelong Supervised Learning}}},
  author = {Sodhani, Shagun and Farmazi, Mojtaba and Mehta, Sanket Vaibhav and Malviya, Pranshu and Abdelsalam, Mohamed and Janarthanan, Janarthanan and Chandar, Sarath},
  year = {2022},
  month = jul,
  number = {arXiv:2207.04354},
  eprint = {2207.04354},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.04354},
  abstract = {This primer is an attempt to provide a detailed summary of the different facets of lifelong learning. We start with Chapter 2 which provides a high-level overview of lifelong learning systems. In this chapter, we discuss prominent scenarios in lifelong learning (Section 2.4), provide 8 Introduction a high-level organization of different lifelong learning approaches (Section 2.5), enumerate the desiderata for an ideal lifelong learning system (Section 2.6), discuss how lifelong learning is related to other learning paradigms (Section 2.7), describe common metrics used to evaluate lifelong learning systems (Section 2.8). This chapter is more useful for readers who are new to lifelong learning and want to get introduced to the field without focusing on specific approaches or benchmarks.},
  archiveprefix = {arXiv},
  keywords = {book,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8RZXCCML\\2207.html}
}

@article{sodhaniTrainingRecurrentNeural2018,
  title = {On {{Training Recurrent Neural Networks}} for {{Lifelong Learning}}},
  author = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
  year = {2018},
  month = nov,
  abstract = {Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step towards developing true lifelong learning systems, we unify Gradient Episodic Memory (a catastrophic forgetting alleviation approach) and Net2Net(a capacity expansion approach). Both these models are proposed in the context of feedforward networks and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.},
  keywords = {architectural,continual,regularization,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\8S6XX5F5\\Sodhani, Chandar, Bengio - 2018 - On Training Recurrent Neural Networks for Lifelong Learning(3).pdf}
}

@article{sokarSpaceNetMakeFree2021,
  title = {{{SpaceNet}}: {{Make Free Space For Continual Learning}}},
  shorttitle = {{{SpaceNet}}},
  author = {Sokar, Ghada and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  year = {2021},
  month = jun,
  journal = {Neurocomputing},
  volume = {439},
  eprint = {2007.07617},
  eprinttype = {arxiv},
  pages = {1--11},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.01.078},
  abstract = {The continual learning (CL) paradigm aims to enable neural networks to learn tasks continually in a sequential fashion. The fundamental challenge in this learning paradigm is catastrophic forgetting previously learned tasks when the model is optimized for a new task, especially when their data is not accessible. Current architectural-based methods aim at alleviating the catastrophic forgetting problem but at the expense of expanding the capacity of the model. Regularization-based methods maintain a fixed model capacity; however, previous studies showed the huge performance degradation of these methods when the task identity is not available during inference (e.g. class incremental learning scenario). In this work, we propose a novel architectural-based method referred as SpaceNet for class incremental learning scenario where we utilize the available fixed capacity of the model intelligently. SpaceNet trains sparse deep neural networks from scratch in an adaptive way that compresses the sparse connections of each task in a compact number of neurons. The adaptive training of the sparse connections results in sparse representations that reduce the interference between the tasks. Experimental results show the robustness of our proposed method against catastrophic forgetting old tasks and the efficiency of SpaceNet in utilizing the available capacity of the model, leaving space for more tasks to be learned. In particular, when SpaceNet is tested on the well-known benchmarks for CL: split MNIST, split Fashion-MNIST, and CIFAR-10/100, it outperforms regularization-based methods by a big performance gap. Moreover, it achieves better performance than architectural-based methods without model expansion and achieved comparable results with rehearsal-based methods, while offering a huge memory reduction.},
  archiveprefix = {arXiv},
  keywords = {continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Sokar et al_2021_SpaceNet.pdf;C\:\\Users\\w-32\\Zotero\\storage\\3QN3L4Q5\\2007.html}
}

@article{sonderbyProteinSecondaryStructure2014,
  title = {Protein {{Secondary Structure Prediction}} with {{Long Short Term Memory Networks}}},
  author = {S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  year = {2014},
  month = dec,
  abstract = {Prediction of protein secondary structure from the amino acid sequence is a classical bioinformatics problem. Common methods use feed forward neural networks or SVMs combined with a sliding window, as these models does not naturally handle sequential data. Recurrent neural networks are an generalization of the feed forward neural network that naturally handle sequential data. We use a bidirectional recurrent neural network with long short term memory cells for prediction of secondary structure and evaluate using the CB513 dataset. On the secondary structure 8-class problem we report better performance (0.674) than state of the art (0.664). Our model includes feed forward networks between the long short term memory cells, a path that can be further explored.},
  keywords = {BIOINF,LSTM},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LK8W7DAL\\Snderby, Winther - 2014 - Protein Secondary Structure Prediction with Long Short Term Memory Networks(2).pdf}
}

@misc{sorscherNeuralScalingLaws2022,
  title = {Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning},
  shorttitle = {Beyond Neural Scaling Laws},
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  year = {2022},
  month = aug,
  number = {arXiv:2206.14486},
  eprint = {2206.14486},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.14486},
  abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how both in theory and practice we can break beyond power law scaling and reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this new exponential scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling performance on ResNets trained on CIFAR-10, SVHN, and ImageNet. Given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
  archiveprefix = {arXiv},
  keywords = {large-scale,notag,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LEFJWQM2\\2206.html}
}

@article{soutif-cormeraisImportanceCrosstaskFeatures,
  title = {On the Importance of Cross-Task Features for Class-Incremental Learning},
  author = {{Soutif{\textendash}Cormerais}, Albin and Masana, Marc},
  pages = {10},
  abstract = {In class-incremental learning, an agent with limited resources needs to learn a sequence of classification tasks, forming an ever growing classification problem, with the constraint of not being able to access data from previous tasks. The main difference with task-incremental learning, where a task-ID is available at inference time, is that the learner also needs to perform cross-task discrimination, i.e. distinguish between classes that have not been seen together. Approaches to tackle this problem are numerous and mostly make use of an external memory (buffer) of nonnegligible size. In this paper, we ablate the learning of cross-task features and study its influence on the performance of basic replay strategies used for class-IL. We also define a new forgetting measure for class-incremental learning, and see that forgetting is not the principal cause of low performance. Our experimental results show that future algorithms for class-incremental learning should not only prevent forgetting, but also aim to improve the quality of the cross-task features, and the knowledge transfer between tasks. This is especially important when tasks contain limited amount of data.},
  langid = {english},
  keywords = {continual,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7RGLGNNR\\SoutifCormerais and Masana - On the importance of cross-task features for class.pdf}
}

@inproceedings{sperdutiEfficientComputationRecursive2007,
  ids = {sperdutiEfficientComputationRecursive2007a},
  title = {Efficient {{Computation}} of {{Recursive Principal Component Analysis}} for {{Structured Input}}},
  booktitle = {Machine {{Learning}}: \{\vphantom\}{{ECML}}\vphantom\{\} 2007, 18th {{European Conference}} on {{Machine Learning}}, {{Warsaw}}, {{Poland}}, {{September}} 17-21, 2007, {{Proceedings}}},
  author = {Sperduti, Alessandro},
  year = {2007},
  pages = {335--346},
  doi = {10.1007/978-3-540-74958-5_32},
  keywords = {LAES},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LMMTJ6HQ\\Sperduti - 2007 - Efficient computation of recursive principal component analysis for structured input.pdf}
}

@article{sperdutiEquivalenceResultsFeedforward2015,
  title = {Equivalence Results between Feedforward and Recurrent Neural Networks for Sequences},
  author = {Sperduti, Alessandro},
  year = {2015},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2015-Janua},
  number = {Ijcai},
  pages = {3827--3833},
  issn = {9781577357384},
  keywords = {LAES},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\P7AIYRA8\\Sperduti - 2015 - Equivalence results between feedforward and recurrent neural networks for sequences.pdf}
}

@inproceedings{sperdutiExactSolutionsRecursive2006,
  title = {Exact {{Solutions}} for {{Recursive Principal Components Analysis}} of {{Sequences}} and {{Trees}}},
  booktitle = {Artificial {{Neural Networks}} - \{\vphantom\}{{ICANN}}\vphantom\{\} 2006, 16th {{International Conference}}, {{Athens}}, {{Greece}}, {{September}} 10-14, 2006. {{Proceedings}}, {{Part}} \{\vphantom\}{{I}}\vphantom\{\}},
  author = {Sperduti, Alessandro},
  year = {2006},
  pages = {349--356},
  doi = {10.1007/11840817_37},
  keywords = {LAES}
}

@inproceedings{sperdutiLinearAutoencoderNetworks2013,
  title = {Linear Autoencoder Networks for Structured Data},
  booktitle = {International {{Workshop}} on {{Neural-Symbolic Learning}} and {{Reasoning}}},
  author = {Sperduti, Alessandro},
  year = {2013},
  keywords = {LAES}
}

@article{sprechmannMemorybasedParameterAdaptation2018,
  title = {Memory-Based {{Parameter Adaptation}}},
  author = {Sprechmann, Pablo and Jayakumar, Siddhant M. and Rae, Jack W. and Pritzel, Alexander and Badia, Adri{\`a} Puigdom{\`e}nech and Uria, Benigno and Vinyals, Oriol and Hassabis, Demis and Pascanu, Razvan and Blundell, Charles},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.10542 [cs, stat]},
  eprint = {1802.10542},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep neural networks have excelled on a wide range of problems, from vision to language and game playing. Neural networks very gradually incorporate information into weights as they process data, requiring very low learning rates. If the training distribution shifts, the network is slow to adapt, and when it does adapt, it typically performs badly on the training distribution before the shift. Our method, Memory-based Parameter Adaptation, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. Much higher learning rates can be used for this local adaptation, reneging the need for many iterations over similar data before good predictions can be made. As our method is memory-based, it alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning with an imbalanced class labels, and fast learning during evaluation. We demonstrate this on a range of supervised tasks: large-scale image classification and language modelling.},
  archiveprefix = {arXiv},
  keywords = {continual,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KKYDKF7H\\Sprechmann et al_2018_Memory-based Parameter Adaptation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MTS2INTW\\1802.html}
}

@article{srivastavaCompeteCompute,
  title = {Compete to {{Compute}}},
  author = {Srivastava, Rupesh K and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  pages = {9},
  abstract = {Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.},
  langid = {english},
  keywords = {architectural,cl-pmnist,competition,continual,lwta},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9T5MTX3U\\Srivastava et al. - Compete to Compute.pdf}
}

@article{srivastavaDropoutSimpleWay2014,
  ids = {srivastavaDropoutSimpleWay2014b},
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting.},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of machine learning research},
  volume = {15},
  number = {1},
  pages = {1929--1958},
  keywords = {regularization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\IRBSZ8MS\\Dropout - simple way to prevent NN overfitting.pdf}
}

@article{srivastavaTrainingVeryDeep,
  title = {Training {{Very Deep Networks}}},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Urgen},
  abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9MYIPIMR\\Srivastava, Greff, Schmidhuber - Unknown - Training Very Deep Networks(2).pdf}
}

@article{srivastavaUnsupervisedLearningVideo2016,
  title = {Unsupervised {{Learning}} of {{Video Representations}} Using {{LSTMs}}},
  author = {Srivastava, Nitish and Mansimov, Elman and Salakhutdinov, Ruslan},
  year = {2016},
  month = jan,
  journal = {arXiv:1502.04681 [cs]},
  eprint = {1502.04681},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences \textendash{} patches of image pixels and high-level representations (``percepts'') of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem \textendash human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {autoencoders,LSTM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\B4Q2XCF7\\Srivastava et al. - 2016 - Unsupervised Learning of Video Representations usi.pdf}
}

@article{steilBackpropagationDecorrelationOnlineRecurrent2004,
  title = {Backpropagation-{{Decorrelation}}: {{Online}} Recurrent Learning with {{O}}({{N}}) Complexity},
  author = {Steil, Jochen J.},
  year = {2004},
  journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
  volume = {2},
  pages = {843--848},
  issn = {0780383591},
  doi = {10.1109/IJCNN.2004.1380039},
  abstract = {We introduce a new learning rule for fully recurrent neural networks which we call backpropagation-decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.},
  keywords = {sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\43VCKNYH\\Steil - 2004 - Backpropagation-Decorrelation Online recurrent learning with O(N) complexity(3).pdf}
}

@article{stepletonLowpassRecurrentNeural2018,
  title = {Low-Pass {{Recurrent Neural Networks}} - {{A}} Memory Architecture for Longer-Term Correlation Discovery},
  author = {Stepleton, Thomas and Pascanu, Razvan and Dabney, Will and Jayakumar, Siddhant M. and Soyer, Hubert and Munos, Remi},
  year = {2018},
  abstract = {Reinforcement learning (RL) agents performing complex tasks must be able to remember observations and actions across sizable time intervals. This is especially true during the initial learning stages, when exploratory behaviour can increase the delay between specific actions and their effects. Many new or popular approaches for learning these distant correlations employ backpropagation through time (BPTT), but this technique requires storing observation traces long enough to span the interval between cause and effect. Besides memory demands, learning dynamics like vanishing gradients and slow convergence due to infrequent weight updates can reduce BPTT's practicality; meanwhile, although online recurrent network learning is a developing topic, most approaches are not efficient enough to use as replacements. We propose a simple, effective memory strategy that can extend the window over which BPTT can learn without requiring longer traces. We explore this approach empirically on a few tasks and discuss its implications.},
  keywords = {long-term,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TMPSAHPQ\\Stepleton et al. - 2018 - Low-pass Recurrent Neural Networks - A memory architecture for longer-term correlation discovery.pdf}
}

@article{stickgoldSleepDependentMemoryConsolidation2007,
  title = {Sleep-{{Dependent Memory Consolidation}} and {{Reconsolidation}}},
  author = {Stickgold, Robert and Walker, Matthew P.},
  year = {2007},
  month = jun,
  journal = {Sleep medicine},
  volume = {8},
  number = {4},
  pages = {331--343},
  issn = {1389-9457},
  doi = {10.1016/j.sleep.2007.03.011},
  abstract = {Molecular, cellular, and systems-level processes convert initial, labile memory representations into more permanent ones, available for continued reactivation and recall over extended periods of time. These processes of memory consolidation and reconsolidation are not all-or-none phenomena, but rather a continuing series of biological adjustments that enhance both the efficiency and utility of stored memories over time. In this chapter, we review the role of sleep in supporting these disparate but related processes.},
  pmcid = {PMC2680680},
  pmid = {17470412},
  keywords = {cl-neuroscience,continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Stickgold_Walker_2007_Sleep-Dependent Memory Consolidation and Reconsolidation.pdf}
}

@inproceedings{stojanovIncrementalObjectLearning2019,
  title = {Incremental {{Object Learning From Contiguous Views}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Stojanov, Stefan and Mishra, Samarth and Thai, Ngoc Anh and Dhanda, Nikhil and Humayun, Ahmad and Yu, Chen and Smith, Linda B. and Rehg, James M.},
  year = {2019},
  month = jun,
  pages = {8769--8778},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00898},
  abstract = {In this work, we present CRIB (Continual Recognition Inspired by Babies), a synthetic incremental object learning environment that can produce data that models visual imagery produced by object exploration in early infancy. CRIB is coupled with a new 3D object dataset, Toys-200, that contains 200 unique toy-like object instances, and is also compatible with existing 3D datasets. Through extensive empirical evaluation of state-of-the-art incremental learning algorithms, we find the novel empirical result that repetition can significantly ameliorate the effects of catastrophic forgetting. Furthermore, we find that in certain cases repetition allows for performance approaching that of batch learning algorithms. Finally, we propose an unsupervised incremental learning task with intriguing baseline results.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HIPK9YUB\\Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf}
}

@article{stolzenburgPowerLinearRecurrent2020,
  title = {The {{Power}} of {{Linear Recurrent Neural Networks}}},
  author = {Stolzenburg, Frieder and Litz, Sandra and Michael, Olivia and Obst, Oliver},
  year = {2020},
  month = mar,
  journal = {arXiv:1802.03308 [cs]},
  eprint = {1802.03308},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recurrent neural networks are a powerful means to cope with time series. We show how a type of linearly activated recurrent neural networks, which we call predictive neural networks, can approximate any time-dependent function f(t) given by a number of function values. The approximation can effectively be learned by simply solving a linear equation system; no backpropagation or similar methods are needed. Furthermore, the network size can be reduced by taking only most relevant components. Thus, in contrast to others, our approach not only learns network weights but also the network architecture. The networks have interesting properties: They end up in ellipse trajectories in the long run and allow the prediction of further values and compact representations of functions. We demonstrate this by several experiments, among them multiple superimposed oscillators (MSO), robotic soccer, and predicting stock prices. Predictive neural networks outperform the previous state-of-the-art for the MSO task with a minimal number of units.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,linear,memory,rnn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SSJRIL9L\\Stolzenburg et al. - 2020 - The Power of Linear Recurrent Neural Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\95GGJPEZ\\1802.html}
}

@article{stoneOpenCLParallelProgramming2010,
  title = {{{OpenCL}}: {{A}} Parallel Programming Standard for Heterogeneous Computing Systems},
  author = {Stone, John E and Gohara, David and Shi, Guochun},
  year = {2010},
  journal = {Computing in science \& engineering},
  volume = {12},
  number = {3},
  pages = {66--73},
  keywords = {software}
}

@article{suchGenerativeTeachingNetworks,
  title = {Generative {{Teaching Networks}}: {{Accelerating Neural Architecture Search}} by {{Learning}} to {{Generate Synthetic Training Data}}},
  author = {Such, Felipe Petroski and Rawal, Aditya and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  pages = {11},
  abstract = {This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS). GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.},
  langid = {english},
  keywords = {continual,data-distill,meta-learn,neural-arch-search},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\V8ZKZ56I\\Such et al. - Generative Teaching Networks Accelerating Neural .pdf}
}

@article{sunContextualMemoryTrees2018,
  title = {Contextual {{Memory Trees}}},
  author = {Sun, Wen and Beygelzimer, Alina and Daum{\'e}, Hal and Langford, John and Mineiro, Paul},
  year = {2018},
  abstract = {We design and study a Contextual Memory Tree (CMT), a learning memory controller that inserts new memories into an experience store of unbounded size. It is designed to efficiently query for memories from that store, supporting logarithmic time insertion and retrieval operations. Hence CMT can be integrated into existing statistical learning algorithms as an augmented memory unit without substantially increasing training and inference computation. Furthermore CMT operates as a reduction to classification, allowing it to benefit from advances in representation or architecture. We demonstrate the efficacy of CMT by augmenting existing multi-class and multi-label classification algorithms with CMT and observe statistical improvement. We also test CMT learning on several image-captioning tasks to demonstrate that it performs computationally better than a simple nearest neighbors memory system while benefitting from reward learning.},
  keywords = {MANN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Q2QT3CUJ\\Sun et al. - 2018 - Contextual Memory Trees.pdf}
}

@article{sunERNIEContinualPretraining2019,
  title = {{{ERNIE}} 2.0: {{A Continual Pre-training Framework}} for {{Language Understanding}}},
  shorttitle = {{{ERNIE}} 2.0},
  author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
  year = {2019},
  month = nov,
  journal = {arXiv:1907.12412 [cs]},
  eprint = {1907.12412},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
  archiveprefix = {arXiv},
  keywords = {continual,nlp,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DV5QU329\\Sun et al. - 2019 - ERNIE 2.0 A Continual Pre-training Framework for .pdf;C\:\\Users\\w-32\\Zotero\\storage\\A9QQCYPK\\1907.html}
}

@misc{sunExploringExampleInfluence2022,
  ids = {sunExploringExampleInfluence2022a},
  title = {Exploring {{Example Influence}} in {{Continual Learning}}},
  author = {Sun, Qing and Lyu, Fan and Shang, Fanhua and Feng, Wei and Wan, Liang},
  year = {2022},
  month = sep,
  number = {arXiv:2209.12241},
  eprint = {2209.12241},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.12241},
  abstract = {Continual Learning (CL) sequentially learns new tasks like human beings, with the goal to achieve better Stability (S, remembering past tasks) and Plasticity (P, adapting to new tasks). Due to the fact that past training data is not available, it is valuable to explore the influence difference on S and P among training examples, which may improve the learning pattern towards better SP. Inspired by Influence Function (IF), we first study example influence via adding perturbation to example weight and computing the influence derivation. To avoid the storage and calculation burden of Hessian inverse in neural networks, we propose a simple yet effective MetaSP algorithm to simulate the two key steps in the computation of IF and obtain the S- and P-aware example influence. Moreover, we propose to fuse two kinds of example influence by solving a dual-objective optimization problem, and obtain a fused influence towards SP Pareto optimality. The fused influence can be used to control the update of model and optimize the storage of rehearsal. Empirical results show that our algorithm significantly outperforms state-of-the-art methods on both task- and class-incremental benchmark CL datasets.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,Computer Science - Machine Learning,continual,online-cl,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\235CSISQ\\2209.html}
}

@article{sungTrainingNeuralNetworks2021,
  title = {Training {{Neural Networks}} with {{Fixed Sparse Masks}}},
  author = {Sung, Yi-Lin and Nair, Varun and Raffel, Colin},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.09839 [cs]},
  eprint = {2111.09839},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {During typical gradient-based training of deep neural networks, all of the model's parameters are updated at each iteration. Recent work has shown that it is possible to update only a small subset of the model's parameters during training, which can alleviate storage and communication requirements. In this paper, we show that it is possible to induce a fixed sparse mask on the model's parameters that selects a subset to update over many iterations. Our method constructs the mask out of the \$k\$ parameters with the largest Fisher information as a simple approximation as to which parameters are most important for the task at hand. In experiments on parameter-efficient transfer learning and distributed training, we show that our approach matches or exceeds the performance of other methods for training with sparse updates while being more efficient in terms of memory usage and communication costs. We release our code publicly to promote further applications of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,exmodel,model-patching,notag,toread},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Sung et al_2021_Training Neural Networks with Fixed Sparse Masks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\TR257X3R\\2111.html}
}

@misc{suOneshotFederatedLearning2022,
  title = {One-Shot {{Federated Learning}} without {{Server-side Training}}},
  author = {Su, Shangchao and Li, Bin and Xue, Xiangyang},
  year = {2022},
  month = apr,
  number = {arXiv:2204.12493},
  eprint = {2204.12493},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.12493},
  abstract = {Federated Learning (FL) has recently made significant progress as a new machine learning paradigm for privacy protection. Due to the high communication cost of traditional FL, one-shot federated learning is gaining popularity as a way to reduce communication cost between clients and the server. Most of the existing one-shot FL methods are based on Knowledge Distillation; however, distillation based approach requires an extra training phase and depends on publicly available data sets. In this work, we consider a novel and challenging setting: performing a single round of parameter aggregation on the local models without server-side training on a public data set. In this new setting, we propose an effective algorithm for Model Aggregation via Exploring Common Harmonized Optima (MA-Echo), which iteratively updates the parameters of all local models to bring them close to a common low-loss area on the loss surface, without harming performance on their own data sets at the same time. Compared to the existing methods, MA-Echo can work well even in extremely non-identical data distribution settings where the support categories of each local model have no overlapped labels with those of the others. We conduct extensive experiments on two popular image classification data sets to compare the proposed method with existing methods and demonstrate the effectiveness of MA-Echo, which clearly outperforms the state-of-the-arts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,federated},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PDGNTDR9\\2204.html}
}

@article{sussilloMakingBrainMachine2016,
  title = {Making Brain\textendash Machine Interfaces Robust to Future Neural Variability},
  author = {Sussillo, David and Stavisky, Sergey D. and Kao, Jonathan C. and Ryu, Stephen I. and Shenoy, Krishna V.},
  year = {2016},
  month = dec,
  journal = {Nature Communications},
  volume = {7},
  number = {1},
  pages = {13749},
  issn = {2041-1723},
  doi = {10.1038/ncomms13749},
  abstract = {A major hurdle to clinical translation of brain\textendash machine interfaces (BMIs) is that current decoders, which are trained from a small quantity of recent data, become ineffective when neural recording conditions subsequently change. We tested whether a decoder could be made more robust to future neural variability by training it to handle a variety of recording conditions sampled from months of previously collected data as well as synthetic training data perturbations. We developed a new multiplicative recurrent neural network BMI decoder that successfully learned a large variety of neural-to-kinematic mappings and became more robust with larger training data sets. Here we demonstrate that when tested with a non-human primate preclinical BMI model, this decoder is robust under conditions that disabled a state-of-the-art Kalman filter-based decoder. These results validate a new BMI strategy in which accumulated data history are effectively harnessed, and may facilitate reliable BMI use by reducing decoder retraining downtime.},
  keywords = {human_state_monitoring}
}

@article{sussilloOpeningBlackBox,
  title = {Opening the {{Black Box}}:{{Low-dimensional}} Dynamics in High-Dimensionalrecurrent Neural Networks},
  author = {Sussillo, David},
  keywords = {Dynamical systems,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\C8UTRCKW\\Sussillo - Unknown - Opening the Black BoxLow-dimensional dynamics in high-dimensionalrecurrent neural networks.pdf}
}

@book{sussmanStructureInterpretationClassical2014,
  title = {Structure and Interpretation of Classical Mechanics},
  author = {Sussman, Gerald Jay and Wisdom, Jack},
  year = {2014},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-02896-7},
  langid = {english},
  lccn = {QC125.2 .S895 2014},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7K8UJMRL\\Sussman and Wisdom - 2014 - Structure and interpretation of classical mechanic.pdf}
}

@inproceedings{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  year = {2014},
  pages = {3104--3112},
  keywords = {seq2seq},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LC2EB5MC\\Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EPSHI7L9\\Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@article{suzgunLSTMNetworksCan2019,
  title = {{{LSTM Networks Can Perform Dynamic Counting}}},
  author = {Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart and Gehrmann, Sebastian},
  year = {2019},
  number = {2018},
  pages = {44--54},
  doi = {10.18653/v1/w19-3905},
  abstract = {In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-\$1\$) and the shuffles of multiple Dyck-\$1\$ languages, each defined over different parenthesis-pairs, by emulating simple real-time \$k\$-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-\$1\$ language. However, none of our recurrent networks was able to yield a good performance on the Dyck-\$2\$ language learning task, which requires a model to have a stack-like mechanism for recognition.},
  keywords = {LSTM},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BGEQJW72\\Suzgun et al. - 2019 - LSTM Networks Can Perform Dynamic Counting.pdf}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  ids = {szegedyGoingDeeperConvolutions2015a},
  title = {Going Deeper with Convolutions},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  pages = {1--9},
  keywords = {CNN}
}

@article{szeliskiComputerVisionAlgorithms,
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}, 2nd {{Edition}}},
  author = {Szeliski, Richard},
  pages = {1232},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2SX43NSK\\Szeliski - Computer Vision Algorithms and Applications, 2nd .pdf}
}

@article{tagareNotesOptimizationStiefel2011,
  title = {Notes on {{Optimization}} on {{Stiefel Manifolds}}},
  author = {Tagare, Hd},
  year = {2011},
  journal = {Ipag.Med.Yale.Edu},
  pages = {1--10},
  keywords = {math},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FXU9CXN6\\Tagare - 2011 - Notes on Optimization on Stiefel Manifolds.pdf}
}

@inproceedings{taiImprovedSemanticRepresentations2015,
  title = {Improved {{Semantic Representations From Tree-Structured Long Short-Term Memory Networks}}},
  booktitle = {{{arXiv}} Preprint {{arXiv}}:1503.00075},
  author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
  year = {2015},
  month = feb,
  eprint = {1503.00075},
  eprinttype = {arxiv},
  abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
  archiveprefix = {arXiv},
  keywords = {LSTM,tree},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\K9NR85PI\\Tai, Socher, Manning - 2015 - Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.pdf}
}

@inproceedings{tallec2018can,
  ids = {tallecCanRecurrentNeural2018},
  title = {Can Recurrent Neural Networks Warp Time?},
  booktitle = {International Conference on Learning Representations},
  author = {Tallec, Corentin and Ollivier, Yann},
  year = {2018},
  keywords = {LSTM,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FSYBS9NC\\Tallec, Ollivier - 2018 - Can recurrent neural networks warp time(2).pdf}
}

@inproceedings{tallecUnbiasedOnlineRecurrent2018,
  title = {Unbiased {{Online Recurrent Optimization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Tallec, Corentin and Ollivier, Yann},
  year = {2018},
  month = feb,
  abstract = {The novel \textbackslash emph\{Unbiased Online Recurrent Optimization\} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a...},
  keywords = {online learning,optimization,RNN,rnn-optimization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XKZJ2X2N\\Tallec and Ollivier - 2018 - Unbiased Online Recurrent Optimization.pdf;C\:\\Users\\w-32\\Zotero\\storage\\X2D3ZQ2M\\forum.html}
}

@article{tangADMMiRNNTrainingRNN2020,
  title = {{{ADMMiRNN}}: {{Training RNN}} with {{Stable Convergence}} via {{An Efficient ADMM Approach}}},
  shorttitle = {{{ADMMiRNN}}},
  author = {Tang, Yu and Kan, Zhigang and Sun, Dequan and Qiao, Linbo and Xiao, Jingjing and Lai, Zhiquan and Li, Dongsheng},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.05622 [cs, stat]},
  eprint = {2006.05622},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It is hard to train Recurrent Neural Network (RNN) with stable convergence and avoid gradient vanishing and exploding, as the weights in the recurrent unit are repeated from iteration to iteration. Moreover, RNN is sensitive to the initialization of weights and bias, which brings difficulty in the training phase. With the gradient-free feature and immunity to poor conditions, the Alternating Direction Method of Multipliers (ADMM) has become a promising algorithm to train neural networks beyond traditional stochastic gradient algorithms. However, ADMM could not be applied to train RNN directly since the state in the recurrent unit is repetitively updated over timesteps. Therefore, this work builds a new framework named ADMMiRNN upon the unfolded form of RNN to address the above challenges simultaneously and provides novel update rules and theoretical convergence analysis. We explicitly specify key update rules in the iterations of ADMMiRNN with deliberately constructed approximation techniques and solutions to each subproblem instead of vanilla ADMM. Numerical experiments are conducted on MNIST and text classification tasks, where ADMMiRNN achieves convergent results and outperforms compared baselines. Furthermore, ADMMiRNN trains RNN in a more stable way without gradient vanishing or exploding compared to the stochastic gradient algorithms. Source code has been available at https://github.com/TonyTangYu/ADMMiRNN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,ECML,rnn,rnn-optimization,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9UBZ8KCZ\\Tang et al_2020_ADMMiRNN.pdf;C\:\\Users\\w-32\\Zotero\\storage\\C7ZHNNIV\\2006.html}
}

@misc{tangDecentralizedTrainingDecentralized2018,
  title = {D\$\^2\$: {{Decentralized Training}} over {{Decentralized Data}}},
  shorttitle = {D\$\^2\$},
  author = {Tang, Hanlin and Lian, Xiangru and Yan, Ming and Zhang, Ce and Liu, Ji},
  year = {2018},
  month = apr,
  number = {arXiv:1803.07068},
  eprint = {1803.07068},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {While training a machine learning model using multiple workers, each of which collects data from their own data sources, it would be most useful when the data collected from different workers can be \{\textbackslash em unique\} and \{\textbackslash em different\}. Ironically, recent analysis of decentralized parallel stochastic gradient descent (D-PSGD) relies on the assumption that the data hosted on different workers are \{\textbackslash em not too different\}. In this paper, we ask the question: \{\textbackslash em Can we design a decentralized parallel stochastic gradient descent algorithm that is less sensitive to the data variance across workers?\} In this paper, we present D\$\^2\$, a novel decentralized parallel stochastic gradient descent algorithm designed for large data variance \textbackslash xr\{among workers\} (imprecisely, "decentralized" data). The core of D\$\^2\$ is a variance blackuction extension of the standard D-PSGD algorithm, which improves the convergence rate from \$O\textbackslash left(\{\textbackslash sigma \textbackslash over \textbackslash sqrt\{nT\}\} + \{(n\textbackslash zeta\^2)\^\{\textbackslash frac\{1\}\{3\}\} \textbackslash over T\^\{2/3\}\}\textbackslash right)\$ to \$O\textbackslash left(\{\textbackslash sigma \textbackslash over \textbackslash sqrt\{nT\}\}\textbackslash right)\$ where \$\textbackslash zeta\^\{2\}\$ denotes the variance among data on different workers. As a result, D\$\^2\$ is robust to data variance among workers. We empirically evaluated D\$\^2\$ on image classification tasks where each worker has access to only the data of a limited set of labels, and find that D\$\^2\$ significantly outperforms D-PSGD.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {decentralized,federated},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VN3T8B8V\\1803.07068.pdf}
}

@article{tangLearningGenerativeModels2014,
  title = {Learning {{Generative Models}} with {{Visual Attention}}},
  author = {Tang, Yichuan and Srivastava, Nitish and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {NIPS},
  pages = {1--12},
  abstract = {Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by the visual attention models in computational neuroscience and the need of object-centric data for generative models, we describe for generative learning framework using attentional mechanisms. Attentional mechanisms can propagate signals from region of interest in a scene to an aligned canonical representation, where generative modeling takes place. By ignoring background clutter, generative models can concentrate their resources on the object of interest. Our model is a proper graphical model where the 2D Similarity transformation is a part of the top-down process. A ConvNet is employed to provide good initializations during posterior inference which is based on Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to face regions of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.},
  keywords = {attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SCI6BW2A\\Tang, Srivastava, Salakhutdinov - 2014 - Learning Generative Models with Visual Attention.pdf}
}

@inproceedings{tangLearningImagineDiversify2022,
  title = {Learning {{To Imagine}}: {{Diversify Memory}} for {{Incremental Learning Using Unlabeled Data}}},
  shorttitle = {Learning {{To Imagine}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tang, Yu-Ming and Peng, Yi-Xing and Zheng, Wei-Shi},
  year = {2022},
  pages = {9549--9558},
  langid = {english},
  keywords = {cl-replay,Continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WAJ2K7AQ\\Tang_Learning_To_Imagine_Diversify_Memory_for_Incremental_Learning_Using_Unlabeled_CVPR_2022_pa.html}
}

@inproceedings{taran.sainathConvolutionalLongShortTerm2015,
  title = {Convolutional, {{Long Short-Term Memory}}, Fully Connected {{Deep Neural Networks}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {{Tara N. Sainath} and Vinyals, Oriol and Senior, Andrew and Sak, Has{\c}im},
  year = {2015},
  pages = {4580--4584},
  doi = {10.1109/ICASSP.2015.7178838},
  keywords = {CNN,LSTM,speech},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\245ES7GF\\Tara N. Sainath et al. - Unknown - CONVOLUTIONAL, LONG SHORT-TERM MEMORY, FULLY CONNECTED DEEP NEURAL NETWORKS(2).pdf}
}

@misc{tarvainenMeanTeachersAre2018,
  title = {Mean Teachers Are Better Role Models: {{Weight-averaged}} Consistency Targets Improve Semi-Supervised Deep Learning Results},
  shorttitle = {Mean Teachers Are Better Role Models},
  author = {Tarvainen, Antti and Valpola, Harri},
  year = {2018},
  month = apr,
  number = {arXiv:1703.01780},
  eprint = {1703.01780},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1703.01780},
  abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.},
  archiveprefix = {arXiv},
  keywords = {knowledge-distillation,self-consistency},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9SASIN4D\\Tarvainen_Valpola_2018_Mean teachers are better role models.pdf;C\:\\Users\\w-32\\Zotero\\storage\\7WUBRGID\\1703.html}
}

@article{tatwawadiDeepZipLosslessCompression2017,
  title = {{{DeepZip}}: {{Lossless Compression}} Using {{Recurrent Networks}}},
  author = {Tatwawadi, Kedar},
  year = {2017},
  pages = {1--9},
  abstract = {There has been a tremendous surge in the amount of data generated. New types of data, such as Genomic data [1], 3D-360 degree VR Data, Autonomous Driving Point Cloud data are being generated. A lot of human effort is spent in analyzing the statistics of these new data formats for designing good compressors. We know from Information theory that good predictors form good Compressors [2]. We know that Recurrent Neural Networks (LSTM/GRU) based models are good at capturing long term dependencies [3], and can predict the next character/word very well. Thus can RNNs be efficiently used for compression? We analyze the usage of Recurrent Neural Networks for the problem of Data Compression. DeepZip Compressor consists of two major blocks: RNN based probability esti-mator and Arithmetic Coding block [4]. In the first section, we discuss existing literature and the basic model framework. We then take a look at experiment re-sults on synthetic as well as real Text and Genomic datasets. Finally, we conclude by discussing the observations and further work.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QIF8ZVV3\\Tatwawadi - 2017 - DeepZip Lossless Compression using Recurrent Networks.pdf}
}

@article{taufiqueConDAContinualUnsupervised2021,
  title = {{{ConDA}}: {{Continual Unsupervised Domain Adaptation}}},
  shorttitle = {{{ConDA}}},
  author = {Taufique, Abu Md Niamul and Jahan, Chowdhury Sadman and Savakis, Andreas},
  year = {2021},
  month = apr,
  journal = {arXiv:2103.11056 [cs]},
  eprint = {2103.11056},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Domain Adaptation (DA) techniques are important for overcoming the domain shift between the source domain used for training and the target domain where testing takes place. However, current DA methods assume that the entire target domain is available during adaptation, which may not hold in practice. This paper considers a more realistic scenario, where target data become available in smaller batches and adaptation on the entire target domain is not feasible. In our work, we introduce a new, data-constrained DA paradigm where unlabeled target samples are received in batches and adaptation is performed continually. We propose a novel source-free method for continual unsupervised domain adaptation that utilizes a buffer for selective replay of previously seen samples. In our continual DA framework, we selectively mix samples from incoming batches with data stored in a buffer using buffer management strategies and use the combination to incrementally update our model. We evaluate the classification performance of the continual DA approach with state-of-the-art DA methods based on the entire target domain. Our results on three popular DA datasets demonstrate that our method outperforms many existing state-of-the-art DA methods with access to the entire target domain during adaptation.},
  archiveprefix = {arXiv},
  keywords = {continual,notag,unread,unsupervised},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\N6QIQD8T\\2103.html}
}

@misc{TechReportParameter,
  title = {Tech {{Report}} - {{Parameter Estimation}} for {{Linear Dynamical Systems}}},
  howpublished = {http://mlg.eng.cam.ac.uk/zoubin/course04/tr-96-2.pdf},
  keywords = {LDS},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SWI8WTQY\\tr-96-2.pdf}
}

@article{thaiSurprisingPositiveKnowledge2022,
  title = {The {{Surprising Positive Knowledge Transfer}} in {{Continual 3D Object Shape Reconstruction}}},
  author = {Thai, Anh and Stojanov, Stefan and Huang, Zixuan and Rehg, Isaac and Rehg, James M.},
  year = {2022},
  month = mar,
  journal = {arXiv:2101.07295 [cs]},
  eprint = {2101.07295},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Continual learning has been extensively studied for classification tasks with methods developed to primarily avoid catastrophic forgetting, a phenomenon where earlier learned concepts are forgotten at the expense of more recent samples. In this work, we present a set of continual 3D object shape reconstruction tasks including complete 3D shape reconstruction from different input modalities and visible surface (2.5D) reconstruction which surprisingly demonstrates positive knowledge (backward and forward) transfer when training with solely vanilla SGD and without additional heuristics. We provide evidence that continuously updated representation learning of single-view 3D shape reconstruction improves the performance on learned and novel categories over time. We provide a novel analysis of knowledge transfer ability by looking at the output distribution shift across sequential learning tasks. Finally, we show that the robustness of these tasks leads to the potential of having a proxy representation learning task for continual classification. The codebase, dataset, and pre-trained models released with this article can be found at https://github.com/rehg-lab/CLRec.},
  archiveprefix = {arXiv},
  keywords = {continual,forward-transfer,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9WRNR6E2\\2101.html}
}

@article{thanh-tungCatastrophicForgettingMode,
  title = {Catastrophic Forgetting and Mode Collapse in {{GANs}}},
  author = {{Thanh-Tung}, Hoang and Tran, Truyen},
  pages = {10},
  abstract = {In this paper, we show that Generative Adversarial Networks (GANs) suffer from catastrophic forgetting even when they are trained to approximate a single target distribution. We show that GAN training is a continual learning problem in which the sequence of changing model distributions is the sequence of tasks to the discriminator. The level of mismatch between tasks in the sequence determines the level of forgetting. Catastrophic forgetting is interrelated to mode collapse and can make the training of GANs non-convergent. We investigate the landscape of the discriminator's output in different variants of GANs and find that when a GAN converges to a good equilibrium, real training datapoints are wide local maxima of the discriminator. We empirically show the relationship between the sharpness of local maxima and mode collapse and generalization in GANs. We show how catastrophic forgetting prevents the discriminator from making real datapoints local maxima, and thus causes non-convergence. Finally, we study methods for preventing catastrophic forgetting in GANs.},
  langid = {english},
  keywords = {catastrophic forgetting,GAN,mode-collapse,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Y8CG5F6K\\Thanh-Tung and Tran - Catastrophic forgetting and mode collapse in GANs.pdf}
}

@article{theisNoteEvaluationGenerative2016,
  title = {A Note on the Evaluation of Generative Models},
  author = {Theis, Lucas and van den Oord, A{\"a}ron and Bethge, Matthias},
  year = {2016},
  month = apr,
  journal = {arXiv:1511.01844 [cs, stat]},
  eprint = {1511.01844},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
  archiveprefix = {arXiv},
  keywords = {notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JYEYKKL7\\Theis et al_2016_A note on the evaluation of generative models.pdf;C\:\\Users\\w-32\\Zotero\\storage\\EVX7Q7TJ\\1511.html}
}

@misc{thenganeCLIPModelEfficient2022,
  title = {{{CLIP}} Model Is an {{Efficient Continual Learner}}},
  author = {Thengane, Vishal and Khan, Salman and Hayat, Munawar and Khan, Fahad},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03114},
  eprint = {2210.03114},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03114},
  abstract = {The continual learning setting aims to learn new tasks over time without forgetting the previous ones. The literature reports several significant efforts to tackle this problem with limited or no access to previous task data. Among such efforts, typical solutions offer sophisticated techniques involving memory replay, knowledge distillation, model regularization, and dynamic network expansion. The resulting methods have a retraining cost at each learning task, dedicated memory requirements, and setting-specific design choices. In this work, we show that a frozen CLIP (Contrastive Language-Image Pretraining) model offers astounding continual learning performance without any fine-tuning (zero-shot evaluation). We evaluate CLIP under a variety of settings including class-incremental, domain-incremental and task-agnostic incremental learning on five popular benchmarks (ImageNet-100 \& 1K, CORe50, CIFAR-100, and TinyImageNet). Without any bells and whistles, the CLIP model outperforms the state-of-the-art continual learning approaches in the majority of the settings. We show the effect on the CLIP model's performance by varying text inputs with simple prompt templates. To the best of our knowledge, this is the first work to report the CLIP zero-shot performance in a continual setting. We advocate the use of this strong yet embarrassingly simple baseline for future comparisons in the continual learning tasks.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5IUX4PDY\\2210.html}
}

@article{tielemanLecture5rmspropDivide2012,
  title = {Lecture 6.5-Rmsprop: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  year = {2012},
  journal = {COURSERA: Neural networks for machine learning},
  volume = {4},
  number = {2},
  pages = {26--31},
  keywords = {learning}
}

@inproceedings{tielemanTrainingRestrictedBoltzmann2008,
  title = {Training Restricted {{Boltzmann}} Machines Using Approximations to the Likelihood Gradient},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Tieleman, Tijmen},
  year = {2008},
  pages = {1064--1071},
  publisher = {{ACM}},
  keywords = {RBM},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QLM76WVS\\pcd.pdf}
}

@book{tikhonovSolutionsIllposedProblems1977,
  ids = {tikhonovSolutionsIllposedProblems1977a},
  title = {Solutions of Ill-Posed Problems},
  author = {Tikhonov, Andrej Nikolaevich and Arsenin, Vasilij IAkovlevich and John, Fritz},
  year = {1977},
  volume = {14},
  publisher = {{Winston Washington, DC}},
  keywords = {regularization}
}

@article{tinoAsymptoticFisherMemory2018,
  title = {Asymptotic {{Fisher}} Memory of Randomized Linear Symmetric {{Echo State Networks}}},
  author = {Ti{\v n}o, Peter},
  year = {2018},
  month = jul,
  journal = {Neurocomputing},
  volume = {298},
  pages = {4--8},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.11.076},
  abstract = {We study asymptotic properties of Fisher memory of linear Echo State Networks with randomized symmetric state space coupling. In particular, two reservoir constructions are considered: (1) More direct dynamic coupling construction using a class of Wigner matrices and (2) positive semi-definite dynamic coupling obtained as a product of unconstrained stochastic matrices. We show that the maximal Fisher memory is achieved when the input-to-state coupling is collinear with the dominant eigenvector of the reservoir coupling matrix. In the case of Wigner reservoirs we show that as the system size grows, the contribution to the Fisher memory of self-coupling of reservoir units is negligible. We also prove that when the input-to-state coupling is collinear with the sum of eigenvectors of the state space coupling, the expected normalized memory is four and eight time smaller than the maximal memory value for the Wigner and product constructions, respectively.},
  langid = {english},
  keywords = {Echo State Network,Fisher memory of dynamical systems,Recurrent neural network,Reservoir Computing},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TET6H27X\\Tio_2018_Asymptotic Fisher memory of randomized linear symmetric Echo State Networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KDDHEJYN\\S0925231218302200.html;C\:\\Users\\w-32\\Zotero\\storage\\Q9LXK62E\\S0925231218302200.html}
}

@article{tinoDynamicalSystemsTemporal2020,
  title = {Dynamical {{Systems}} as {{Temporal Feature Spaces}}},
  author = {Tino, Peter},
  year = {2020},
  month = feb,
  journal = {arXiv:1907.06382 [cs, stat]},
  eprint = {1907.06382},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Parameterized state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the input driving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks (ESN). In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: (1) The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; (3) Simple cycle high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability.},
  archiveprefix = {arXiv},
  keywords = {ESN,Kernel,LDS,RNN,SVM},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\C7FRFULZ\\Tino - 2020 - Dynamical Systems as Temporal Feature Spaces.pdf;C\:\\Users\\w-32\\Zotero\\storage\\D5TK3FDU\\1907.html}
}

@article{tinoMarkovianArchitecturalBias2004,
  title = {Markovian {{Architectural Bias}} of {{Recurrent Neural Networks}}},
  author = {Tino, P. and Cernansky, M. and Benuskova, L.},
  year = {2004},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {15},
  number = {1},
  pages = {6--15},
  doi = {10.1109/TNN.2003.820839},
  keywords = {ESN,Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NVICJ5IF\\Tino, Cernansky, Benuskova - 2004 - Markovian Architectural Bias of Recurrent Neural Networks.pdf}
}

@article{tinoShortTermMemory2013,
  title = {Short Term Memory in Input-Driven Linear Dynamical Systems},
  author = {Ti{\v n}o, P and Rodan, A},
  year = {2013},
  journal = {Neurocomputing},
  volume = {112},
  pages = {58--63},
  doi = {10.1016/j.neucom.2012.12.041},
  abstract = {We investigate the relation between two quantitative measures characterizing short term memory in input driven dynamical systems, namely the short term memory capacity (MC) [3] and the Fisher memory curve (FMC) [2]. We show that even though MC and FMC map the memory structure of the system under investigation from two quite different perspectives, for linear input driven dynamical systems they are in fact closely related. In particular, under some assumptions, the two quantities can be interpreted as squared 'Mahalanobis' norms of images of the input vector under the system's dynamics. We also offer a detailed rigorous analysis of the relation between MC and FMC in cases of symmetric and cyclic dynamic couplings. \textcopyright{} 2013 Elsevier B.V.},
  keywords = {Dynamical systems,ESN,linear,Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5JVCYXRK\\Tio, Rodan - 2013 - Short term memory in input-driven linear dynamical systems(2).pdf}
}

@article{tishbyDeepLearningInformation2015,
  title = {Deep {{Learning}} and the {{Information Bottleneck Principle}}},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  year = {2015},
  month = mar,
  journal = {arXiv:1503.02406 [cs]},
  eprint = {1503.02406},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZV9FXGNF\\Tishby_Zaslavsky_2015_Deep Learning and the Information Bottleneck Principle.pdf;C\:\\Users\\w-32\\Zotero\\storage\\6GSNSTE7\\1503.html}
}

@inproceedings{tiwariGCRGradientCoreset2022,
  title = {{{GCR}}: {{Gradient Coreset Based Replay Buffer Selection}} for {{Continual Learning}}},
  shorttitle = {{{GCR}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tiwari, Rishabh and Killamsetty, Krishnateja and Iyer, Rishabh and Shenoy, Pradeep},
  year = {2022},
  pages = {99--108},
  langid = {english},
  keywords = {cl-replay,Continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\C2UP3WRK\\Tiwari_GCR_Gradient_Coreset_Based_Replay_Buffer_Selection_for_Continual_Learning_CVPR_2022_pape.html}
}

@inproceedings{toldoBringEvanescentRepresentations2022,
  title = {Bring {{Evanescent Representations}} to {{Life}} in {{Lifelong Class Incremental Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Toldo, Marco and Ozay, Mete},
  year = {2022},
  pages = {16732--16741},
  langid = {english},
  keywords = {Continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DYU3UFEI\\Toldo_Ozay_2022_Bring Evanescent Representations to Life in Lifelong Class Incremental Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KNFV9HNS\\Toldo_Bring_Evanescent_Representations_to_Life_in_Lifelong_Class_Incremental_Learning_CVPR_2022.html}
}

@misc{tongVideoMAEMaskedAutoencoders2022,
  title = {{{VideoMAE}}: {{Masked Autoencoders}} Are {{Data-Efficient Learners}} for {{Self-Supervised Video Pre-Training}}},
  shorttitle = {{{VideoMAE}}},
  author = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
  year = {2022},
  month = mar,
  number = {arXiv:2203.12602},
  eprint = {2203.12602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.12602},
  abstract = {Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking and reconstruction. These simple designs turn out to be effective for overcoming information leakage caused by the temporal correlation during video reconstruction. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90\% to 95\%) still yields favorable performance of VideoMAE. The temporally redundant video content enables higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. This is partially ascribed to the challenging task of video reconstruction to enforce high-level structure learning. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets are important issues in SSVP. Notably, our VideoMAE with the vanilla ViT backbone can achieve 83.9\% on Kinects-400, 75.3\% on Something-Something V2, 90.8\% on UCF101, and 61.1\% on HMDB51 without using any extra data. Code will be released at https://github.com/MCG-NJU/VideoMAE.},
  archiveprefix = {arXiv},
  keywords = {large-models,pretraining,self-supervised,video},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3GN5GYPX\\2203.html}
}

@techreport{TRAININGRECURRENTNEURAL,
  title = {{{TRAINING RECURRENT NEURAL NETWORKS ONLINE BY LEARNING EXPLICIT STATE VARIABLES}}},
  abstract = {Recurrent neural networks (RNNs) provide a powerful tool for online prediction in online partially observable problems. However, there are two primary issues one must overcome when training an RNN: the sensitivity of the learning algorithm's performance to truncation length and and long training times. There are variety of strategies to improve training in RNNs, particularly with Backprop Through Time (BPTT) and by Real-Time Recurrent Learning. These strategies, however, are typically computationally expensive and focus computation on computing gradients back in time. In this work, we reformulate the RNN training objective to explicitly learn state vectors; this breaks the dependence across time and so avoids the need to estimate gradients far back in time. We show that for a fixed buffer of data, our algorithm-called Fixed Point Propagation (FPP)-is sound: it converges to a stationary point of the new objective. We investigate the empirical performance of our online FPP algorithm, particularly in terms of computation compared to truncated BPTT with varying truncation levels.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4VDQWM35\\Unknown - Unknown - TRAINING RECURRENT NEURAL NETWORKS ONLINE BY LEARNING EXPLICIT STATE VARIABLES.pdf}
}

@misc{TransferLearningSmart,
  title = {Transfer {{Learning}} in {{Smart Home Scenario}} | {{IEEE WCCI}} 2020},
  howpublished = {https://2020.wcci-virtual.org/presentation/poster/transfer-learning-smart-home-scenario\#media/}
}

@misc{TreeStructuredRecurrentSwitching,
  title = {Tree-{{Structured Recurrent Switching Linear Dynamical Systems}} for {{Multi-Scale Modeling}} | {{OpenReview}}},
  howpublished = {https://openreview.net/forum?id=HkzRQhR9YX},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PKRJEUQT\\forum.html}
}

@article{triggerCMSHighLevel2005a,
  title = {The {{CMS High Level Trigger}}},
  author = {Trigger, The CMS and Group, Data Acquisition},
  year = {2005},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.HEP-EX/0512077},
  abstract = {At the Large Hadron Collider at CERN the proton bunches cross at a rate of 40MHz. At the Compact Muon Solenoid experiment the original collision rate is reduced by a factor of O (1000) using a Level-1 hardware trigger. A subsequent factor of O(1000) data reduction is obtained by a software-implemented High Level Trigger (HLT) selection that is executed on a multi-processor farm. In this review we present in detail prototype CMS HLT physics selection algorithms, expected trigger rates and trigger performance in terms of both physics efficiency and timing.},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004},
  keywords = {FOS: Physical sciences,High Energy Physics - Experiment (hep-ex)}
}

@inproceedings{trinhLearningLongertermDependencies2018,
  title = {Learning {{Longer-term Dependencies}} in {{RNNs}} with {{Auxiliary Losses}}},
  booktitle = {{{ICML}}},
  author = {Trinh, Trieu H. and Dai, Andrew M. and Luong, Minh-Thang and Le, Quoc V.},
  year = {2018},
  month = feb,
  abstract = {Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16\textbackslash,000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.},
  keywords = {autoencoders,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6NPBMYQF\\Trinh et al. - 2018 - Learning Longer-term Dependencies in RNNs with Auxiliary Losses(2).pdf}
}

@article{tuorAsynchronousCollaborativeLearning2022,
  title = {Asynchronous {{Collaborative Learning Across Data Silos}}},
  author = {Tuor, Tiffany and Lockhart, Joshua and Magazzeni, Daniele},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.12637 [cs]},
  eprint = {2203.12637},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Machine learning algorithms can perform well when trained on large datasets. While large organisations often have considerable data assets, it can be difficult for these assets to be unified in a manner that makes training possible. Data is very often 'siloed' in different parts of the organisation, with little to no access between silos. This fragmentation of data assets is especially prevalent in heavily regulated industries like financial services or healthcare. In this paper we propose a framework to enable asynchronous collaborative training of machine learning models across data silos. This allows data science teams to collaboratively train a machine learning model, without sharing data with one another. Our proposed approach enhances conventional federated learning techniques to make them suitable for this asynchronous training in this intra-organisation, cross-silo setting. We validate our proposed approach via extensive experiments.},
  archiveprefix = {arXiv},
  keywords = {exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UPTKCQC5\\2203.html}
}

@article{turekApproximatingStackedBidirectional,
  title = {Approximating {{Stacked}} and {{Bidirectional Recurrent Architectures}} with the {{Delayed Recurrent Neural Network}}},
  author = {Turek, Javier S and Jain, Shailee and Vo, Vy A and Capota, Mihai and Huth, Alexander G and Willke, Theodore L},
  pages = {11},
  abstract = {Recent work has shown that topological enhance\- ments to recurrent neural networks (RNNs) can increase their expressiveness and representational capacity. Two popular enhancements are stacked RNNs, which increases the capacity for learn\- ing non-linear functions, and bidirectional pro\- cessing, which exploits acausal information in a sequence. In this work, we explore the delayedRNN, which is a single-layer RNN that has a delay between the input and output. We prove that a weight-constrained version of the delayedRNN is equivalent to a stacked-RNN. We also show that the delay gives rise to partial acausality, much like bidirectional networks. Synthetic exper\- iments confirm that the delayed-RNN can mimic bidirectional networks, solving some acausal tasks similarly, and outperforming them in oth\- ers. Moreover, we show similar performance to bidirectional networks in a real-world natural language processing task. These results suggest that delayed-RNNs can approximate topologies including stacked RNNs, bidirectional RNNs, and stacked bidirectional RNNs \textendash{} but with equivalent or faster runtimes for the delayed-RNNs.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4FNCPERA\\Turek et al. - Approximating Stacked and Bidirectional Recurrent .pdf}
}

@article{tzengImproveLSTMGRU,
  title = {Improve the {{LSTM}} and {{GRU}} Model for Small Training Data by Wavelet Transformation},
  author = {Tzeng, Jengnan and Lin, Yu-Han and Lai, Yen-Ru and Lin, Ming-Lai and Shih, Yu-Cheng},
  pages = {6},
  abstract = {Regarding collision prediction technology, the most common are car reversing radar and infrared rays, which provide warnings by sensing the distance between objects and cars. Although radar detection is accurate, radar systems that can provide instant 360-degree feedback are very expensive. The cost of infrared is much lower, but they cannot be applied to high-temperature environments. As the result, the technology of preventing collisions using only images from cameras is an important artificial intelligence topic in recent years. If a lowresolution CCD and artificial intelligence technology can be used to achieve a certain degree of accuracy in collision prediction, then low-cost anti-collision technology is worth looking forward to. Furthermore, we hope to provide anti-collision warnings on motorcycles and bicycles using this technology. In order to achieve this goal, computational simplification is a technical threshold. It's only when simple calculations achieve highprecision prediction can we meet the low power consumption requirements for image AI to be applied small vehicles. Therefore, we hope to find out a better image representation basis and combine it with AI technology to fulfill the requirements of less calculation and high accuracy. In addition, we also hope to create models with sufficient accuracy with small training data. This experiment will reduce development costs and get better efficiency in the early stage of developing ADAS.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QVIJBP7B\\Tzeng et al. - Improve the LSTM and GRU model for small training .pdf}
}

@misc{ulmerKnowYourLimits2021,
  title = {Know {{Your Limits}}: {{Uncertainty Estimation}} with {{ReLU Classifiers Fails}} at {{Reliable OOD Detection}}},
  shorttitle = {Know {{Your Limits}}},
  author = {Ulmer, Dennis and Cin{\`a}, Giovanni},
  year = {2021},
  month = jun,
  number = {arXiv:2012.05329},
  eprint = {2012.05329},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.05329},
  abstract = {A crucial requirement for reliable deployment of deep learning models for safety-critical applications is the ability to identify out-of-distribution (OOD) data points, samples which differ from the training data and on which a model might underperform. Previous work has attempted to tackle this problem using uncertainty estimation techniques. However, there is empirical evidence that a large family of these techniques do not detect OOD reliably in classification tasks. This paper gives a theoretical explanation for said experimental findings and illustrates it on synthetic data. We prove that such techniques are not able to reliably identify OOD samples in a classification setting, since their level of confidence is generalized to unseen areas of the feature space. This result stems from the interplay between the representation of ReLU networks as piece-wise affine transformations, the saturating nature of activation functions like softmax, and the most widely-used uncertainty metrics.},
  archiveprefix = {arXiv},
  keywords = {noread,ood-detection},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\L4HTC4YC\\2012.html}
}

@article{ulyanovDeepImagePrior2017,
  title = {Deep {{Image Prior}}},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2017},
  issn = {978-1-5386-0457-1},
  doi = {10.1109/CVPR.2017.623},
  abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep\_image\_prior .},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VBS5GH32\\Ulyanov, Vedaldi, Lempitsky - 2017 - Deep Image Prior.pdf}
}

@article{umerTargetedForgettingFalse,
  title = {Targeted {{Forgetting}} and {{False Memory Formation}} in {{Continual Learners}} through {{Adversarial Backdoor Attacks}}},
  author = {Umer, Muhammad and Dawson, Glenn and Polikar, Robi},
  pages = {8},
  abstract = {Artificial neural networks are well-known to be susceptible to catastrophic forgetting when continually learning from sequences of tasks. Various continual (or ``incremental'') learning approaches have been proposed to avoid catastrophic forgetting, but they are typically adversary agnostic, i.e., they do not consider the possibility of a malicious attack. In this effort, we explore the vulnerability of Elastic Weight Consolidation (EWC), a popular continual learning algorithm for avoiding catastrophic forgetting. We show that an intelligent adversary can take advantage of EWC's continual learning capabilities to cause gradual and deliberate forgetting by introducing small amounts of misinformation to the model during training. We demonstrate such an adversary's ability to assume control of the model via injection of backdoor attack samples on both permuted and split benchmark variants of the MNIST dataset. Importantly, once the model has learned the adversarial misinformation, the adversary can then control the amount of forgetting of any task. Equivalently, the malicious actor can create a ``false memory'' about any task by inserting carefully-designed backdoor samples to any fraction of the test instances of that task. Perhaps most damaging, we show this vulnerability to be very acute; the model memory can be easily compromised with the addition of backdoor samples into as little as 1\% of the training data of even a single task.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\I8U7APJY\\Umer et al. - Targeted Forgetting and False Memory Formation in .pdf}
}

@inproceedings{umurogluFinnFrameworkFast2017,
  ids = {umurogluFinnFrameworkFast2017b},
  title = {Finn: {{A}} Framework for Fast, Scalable Binarized Neural Network Inference},
  shorttitle = {Finn},
  booktitle = {Proceedings of the 2017 {{ACM}}/{{SIGDA International Symposium}} on {{Field-Programmable Gate Arrays}}},
  author = {Umuroglu, Yaman and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Leong, Philip and Jahre, Magnus and Vissers, Kees},
  year = {2017},
  pages = {65--74},
  publisher = {{ACM}},
  keywords = {bnn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KBS93B9R\\1612.07119.pdf}
}

@misc{UnconstrainedOnlineHandwriting,
  title = {Unconstrained {{On-line Handwriting Recognition}} with {{Recurrent Neural Networks}}},
  howpublished = {https://papers.nips.cc/paper/3213-unconstrained-on-line-handwriting-recognition-with-recurrent-neural-networks}
}

@article{usmanovaDistillationbasedApproachIntegrating2021,
  title = {A Distillation-Based Approach Integrating Continual Learning and Federated Learning for Pervasive Services},
  author = {Usmanova, Anastasiia and Portet, Fran{\c c}ois and Lalanda, Philippe and Vega, German},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.04197 [cs]},
  eprint = {2109.04197},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Federated Learning, a new machine learning paradigm enhancing the use of edge devices, is receiving a lot of attention in the pervasive community to support the development of smart services. Nevertheless, this approach still needs to be adapted to the specificity of the pervasive domain. In particular, issues related to continual learning need to be addressed. In this paper, we present a distillation-based approach dealing with catastrophic forgetting in federated learning scenario. Specifically, Human Activity Recognition tasks are used as a demonstration domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,continual,exmodel,federated,lwf},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Usmanova et al_2021_A distillation-based approach integrating continual learning and federated.pdf;C\:\\Users\\w-32\\Zotero\\storage\\L9VXMCV4\\2109.html}
}

@inproceedings{valentiAdversarialAutoencodersSymbolic2020,
  title = {Adversarial {{Autoencoders}} for {{Symbolic Music Generation}}},
  booktitle = {{{ECAI}}},
  author = {Valenti, Andrea and Carta, Antonio and Bacciu, Davide},
  year = {2020}
}

@article{valScalableImplementationMeasuring2019,
  title = {Scalable Implementation of Measuring Distances in a {{Riemannian}} Manifold Based on the {{Fisher Information}} Metric},
  author = {Val, Universitat De},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BZ7KRRDY\\Val - 2019 - Scalable implementation of measuring distances in a Riemannian manifold based on the Fisher Information metric(2).pdf}
}

@article{vanderwesthuizenUnreasonableEffectivenessForget2018,
  title = {The Unreasonable Effectiveness of the Forget Gate},
  author = {{van der Westhuizen}, Jos and Lasenby, Joan},
  year = {2018},
  eprint = {1804.04849},
  eprinttype = {arxiv},
  primaryclass = {cs.NE},
  archiveprefix = {arXiv},
  keywords = {LSTM,rnn-gates},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UHGIGB6P\\van der Westhuizen, Lasenby - 2018 - The unreasonable effectiveness of the forget gate(2).pdf}
}

@article{vandevenBraininspiredReplayContinual2020,
  title = {Brain-Inspired Replay for Continual Learning with Artificial Neural Networks},
  author = {{van de Ven}, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},
  year = {2020},
  month = aug,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {4069},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17866-2},
  abstract = {Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the reactivation of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay can be implemented as `generative replay', which can successfully \textendash{} and surprisingly efficiently \textendash{} prevent catastrophic forgetting on toy examples even in a class-incremental learning scenario. However, scaling up generative replay to complicated problems with many tasks or complex inputs is challenging. We propose a new, brain-inspired variant of replay in which internal or hidden representations are replayed that are generated by the network's own, context-modulated feedback connections. Our method achieves state-of-the-art performance on challenging continual learning benchmarks (e.g., class-incremental learning on CIFAR-100) without storing data, and it provides a novel model for replay in the brain.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {cl-neuroscience,cl-replay,Computational neuroscience,Computer science,continual,gen-exml,generative,Learning and memory},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Computer science;Learning and memory Subject\_term\_id: computational-neuroscience;computer-science;learning-and-memory},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\van de Ven et al_2020_Brain-inspired replay for continual learning with artificial neural networks.pdf;C\:\\Users\\w-32\\Zotero\\storage\\C6CZ7ZME\\s41467-020-17866-2.html}
}

@inproceedings{vandevenClassIncrementalLearningGenerative2021,
  title = {Class-{{Incremental Learning With Generative Classifiers}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{van de Ven}, Gido M. and Li, Zhe and Tolias, Andreas S.},
  year = {2021},
  pages = {3611--3620},
  langid = {english},
  keywords = {bayesian,class-incremental,continual},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\van de Ven et al_2021_Class-Incremental Learning With Generative Classifiers.pdf;C\:\\Users\\w-32\\Zotero\\storage\\2WC8X72K\\van_de_Ven_Class-Incremental_Learning_With_Generative_Classifiers_CVPRW_2021_paper.html}
}

@article{vandevenThreeScenariosContinual2019,
  title = {Three Scenarios for Continual Learning},
  author = {{van de Ven}, Gido M. and Tolias, Andreas S.},
  year = {2019},
  month = apr,
  journal = {Arxiv preprint},
  eprint = {1904.07734},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1904.07734},
  abstract = {Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and--in case it is not--whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,scenarios,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\VX4DT87T\\1904.html}
}

@book{varoquauxScipyLectureNotes2015,
  title = {Scipy {{Lecture Notes}}},
  author = {Varoquaux, Ga{\"e}l and Gouillart, Emmanuelle and Vahtras, Olav and Haenel, Valentin and Rougier, Nicolas P. and Gommers, Ralf and Pedregosa, Fabian and {J{\k{e}}drzejewski-Szmek}, Zbigniew and Virtanen, Pauli and Combelles, Christophe and others},
  year = {2015},
  publisher = {{Zenodo}},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\K44QR93A\\ScipyLectures-simple.pdf}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish},
  year = {2017},
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BUN2588Y\\Vaswani - 2017 - Attention Is All You Need.pdf}
}

@article{velickovicGraphAttentionNetworks2017,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2017},
  month = oct,
  journal = {CoRR},
  volume = {abs/1710.1},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  keywords = {attention,DGN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\R99HKKX4\\Velikovi et al. - 2017 - Graph Attention Networks(2).pdf}
}

@article{venessOnlineLearningGated2017,
  title = {Online {{Learning}} with {{Gated Linear Networks}}},
  author = {Veness, Joel and Lattimore, Tor and Bhoopchand, Avishkar and {Grabska-Barwinska}, Agnieszka and Mattern, Christopher and Toth, Peter},
  year = {2017},
  month = dec,
  abstract = {This paper describes a family of probabilistic architectures designed for online learning under the logarithmic loss. Rather than relying on non-linear transfer functions, our method gains representational power by the use of data conditioning. We state under general conditions a learnable capacity theorem that shows this approach can in principle learn any bounded Borel-measurable function on a compact subset of euclidean space; the result is stronger than many universality results for connectionist architectures because we provide both the model and the learning procedure for which convergence is guaranteed.},
  keywords = {gated linear,linear},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3H9L25A4\\Veness et al. - 2017 - Online Learning with Gated Linear Networks(2).pdf}
}

@inproceedings{venugopalanSequenceSequencevideoText2015,
  title = {Sequence to Sequence-Video to Text},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  year = {2015},
  pages = {4534--4542},
  keywords = {seq2seq}
}

@article{verwimpRehearsalRevealedLimits2021,
  title = {Rehearsal Revealed: {{The}} Limits and Merits of Revisiting Samples in Continual Learning},
  shorttitle = {Rehearsal Revealed},
  author = {Verwimp, Eli and De Lange, Matthias and Tuytelaars, Tinne},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.07446 [cs]},
  eprint = {2104.07446},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning from non-stationary data streams and overcoming catastrophic forgetting still poses a serious challenge for machine learning research. Rather than aiming to improve state-of-the-art, in this work we provide insight into the limits and merits of rehearsal, one of continual learning's most established methods. We hypothesize that models trained sequentially with rehearsal tend to stay in the same low-loss region after a task has finished, but are at risk of overfitting on its sample memory, hence harming generalization. We provide both conceptual and strong empirical evidence on three benchmarks for both behaviors, bringing novel insights into the dynamics of rehearsal and continual learning in general. Finally, we interpret important continual learning works in the light of our findings, allowing for a deeper understanding of their successes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NT7R2LEG\\Verwimp et al_2021_Rehearsal revealed.pdf;C\:\\Users\\w-32\\Zotero\\storage\\ECJWNRFT\\2104.html}
}

@article{verzelliEchoStateNetworks2019,
  title = {Echo {{State Networks}} with {{Self-Normalizing Activations}} on the {{Hyper-Sphere}}},
  author = {Verzelli, Pietro and Alippi, Cesare and Livi, Lorenzo},
  year = {2019},
  journal = {Nature Scientific Reports},
  pages = {1--14},
  doi = {10.1038/s41598-019-50158-4},
  abstract = {Among the various architectures of Recurrent Neural Networks, Echo State Networks (ESNs) emerged due to their simplified and inexpensive training procedure. These networks are known to be sensitive to the setting of hyper-parameters, which critically affect their behaviour. Results show that their performance is usually maximized in a narrow region of hyper-parameter space called edge of chaos. Finding such a region requires searching in hyper-parameter space in a sensible way: hyper-parameter configurations marginally outside such a region might yield networks exhibiting fully developed chaos, hence producing unreliable computations. The performance gain due to optimizing hyper-parameters can be studied by considering the memory--nonlinearity trade-off, i.e., the fact that increasing the nonlinear behavior of the network degrades its ability to remember past inputs, and vice-versa. In this paper, we propose a model of ESNs that eliminates critical dependence on hyper-parameters, resulting in networks that provably cannot enter a chaotic regime and, at the same time, denotes nonlinear behaviour in phase space characterised by a large memory of past inputs, comparable to the one of linear networks. Our contribution is supported by experiments corroborating our theoretical findings, showing that the proposed model displays dynamics that are rich-enough to approximate many common nonlinear systems used for benchmarking.},
  keywords = {ESN,RNN}
}

@inproceedings{verzelliHypersphericalReservoirsEcho2019,
  title = {Hyper-Spherical {{Reservoirs}} for {{Echo State Networks}}},
  booktitle = {{{ICANN}}},
  author = {Verzelli, Pietro and Alippi, Cesare and Livi, Lorenzo},
  year = {2019},
  publisher = {{Springer Nature}},
  doi = {10.1007/978-3-030-30493-5},
  isbn = {978-3-030-30493-5},
  keywords = {ESN,Memory,RNN}
}

@article{vijayakumarLocallyWeightedProjection2000,
  title = {Locally Weighted Projection Regression: {{An O}} (n) Algorithm for Incremental Real Time Learning in High Dimensional Space},
  author = {Vijayakumar, Sethu and Schaal, Stefan},
  year = {2000},
  journal = {Proceedings of the Seventeenth International Conference on Machine Learning},
  volume = {1086},
  pages = {288--293},
  abstract = {Locally weighted projection regression is a new algorithm that achieves nonlinear function approximation\textbackslash nin high dimensional spaces with redundant and irrelevant input dimensions. At its core, it uses locally linear models, spanned by a small number of univariate regressions in selected directions in input space. This paper evaluates different methods of projection regression and derives a nonlinear function approximator\textbackslash nbased on them. This nonparametric local learning system i) learns rapidly with second order learning methods based on incremental training, ii) uses statistically sound stochastic cross validation to learn iii) adjusts its weighting kernels based on local information only, iv) has a computational complexity that is linear in the number of inputs, and v) can deal with a large number of - possibly redundant - inputs, as shown in evaluations with up to 50 dimensional data sets. To our knowledge, this is the first truly incremental spatially localized learning method to combine all these properties.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\7AMJ2QXS\\Vijayakumar, Schaal - 2000 - Locally weighted projection regression An O (n) algorithm for incremental real time learning in high dimens.pdf}
}

@article{vinyalsPointerNetworks2015,
  title = {Pointer {{Networks}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2015},
  pages = {2692--2700},
  keywords = {MANN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FPRK85QP\\Vinyals, Fortunato, Jaitly - 2015 - Pointer Networks(2).pdf}
}

@inproceedings{vinyalsShowTellNeural2015,
  title = {Show and Tell: {{A}} Neural Image Caption Generator},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  year = {2015},
  pages = {3156--3164},
  keywords = {vision}
}

@article{vinyalsStarCraftIINew2017,
  title = {{{StarCraft II}} : {{A New Challenge}} for {{Reinforcement Learning}}},
  author = {Vinyals, Oriol and Vezhnevets, Alexander Sasha and Silver, David},
  year = {2017},
  keywords = {RL},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CUJ8QLNP\\Vinyals, Vezhnevets, Silver - 2017 - StarCraft II A New Challenge for Reinforcement Learning.pdf}
}

@article{vlachasForecastingSpatiotemporalChaotic2019,
  title = {Forecasting of {{Spatio-temporal Chaotic Dynamics}} with {{Recurrent Neural Networks}}: A Comparative Study of {{Reservoir Computing}} and {{Backpropagation Algorithms}}},
  shorttitle = {Forecasting of {{Spatio-temporal Chaotic Dynamics}} with {{Recurrent Neural Networks}}},
  author = {Vlachas, Pantelis R. and Pathak, Jaideep and Hunt, Brian R. and Sapsis, Themistoklis P. and Girvan, Michelle and Ott, Edward and Koumoutsakos, Petros},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.05266 [physics]},
  eprint = {1910.05266},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {How effective are Recurrent Neural Networks (RNNs) in forecasting the spatiotemporal dynamics of chaotic systems ? We address this question through a comparative study of Reservoir Computing (RC) and backpropagation through time (BPTT) algorithms for gated network architectures on a number of benchmark problems. We quantify their relative prediction accuracy on the long-term forecasting of Lorenz-96 and the Kuramoto-Sivashinsky equation and calculation of its Lyapunov spectrum. We discuss their implementation on parallel computers and highlight advantages and limitations of each method. We find that, when the full state dynamics are available for training, RC outperforms BPTT approaches in terms of predictive performance and capturing of the long-term statistics, while at the same time requiring much less time for training. However, in the case of reduced order data, large RC models can be unstable and more likely, than the BPTT algorithms, to diverge in the long term. In contrast, RNNs trained via BPTT capture well the dynamics of these reduced order models. This study confirms that RNNs present a potent computational framework for the forecasting of complex spatio-temporal dynamics.},
  archiveprefix = {arXiv},
  keywords = {chaos,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,ESN,Physics - Fluid Dynamics,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UJWTZ9IZ\\Vlachas et al. - 2019 - Forecasting of Spatio-temporal Chaotic Dynamics wi.pdf;C\:\\Users\\w-32\\Zotero\\storage\\MQEYESUL\\1910.html}
}

@article{voelkerImprovingSpikingDynamical2018,
  title = {Improving {{Spiking Dynamical Networks}}: {{Accurate Delays}}, {{Higher-Order Synapses}}, and {{Time Cells}}},
  shorttitle = {Improving {{Spiking Dynamical Networks}}},
  author = {Voelker, Aaron R. and Eliasmith, Chris},
  year = {2018},
  month = mar,
  journal = {Neural Computation},
  volume = {30},
  number = {3},
  pages = {569--609},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01046},
  langid = {english},
  keywords = {Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HG8DF2XK\\Voelker and Eliasmith - 2018 - Improving Spiking Dynamical Networks Accurate Del.pdf}
}

@incollection{voelkerLegendreMemoryUnits2019,
  title = {Legendre {{Memory Units}}: {{Continuous-Time Representation}} in {{Recurrent Neural Networks}}},
  shorttitle = {Legendre {{Memory Units}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Voelker, Aaron and Kaji{\'c}, Ivana and Eliasmith, Chris},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {15544--15553},
  publisher = {{Curran Associates, Inc.}},
  keywords = {Memory,NIPS,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\B7G3Q6LN\\Voelker et al. - 2019 - Legendre Memory Units Continuous-Time Representat.pdf;C\:\\Users\\w-32\\Zotero\\storage\\XWUKCBM2\\9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.html}
}

@article{vongkulbhisalUnifyingHeterogeneousClassifiers2019,
  title = {Unifying {{Heterogeneous Classifiers}} with {{Distillation}}},
  author = {Vongkulbhisal, Jayakorn and Vinayavekhin, Phongtharin and {Visentini-Scarzanella}, Marco},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.06062 [cs]},
  eprint = {1904.06062},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we study the problem of unifying knowledge from a set of classifiers with different architectures and target classes into a single classifier, given only a generic set of unlabelled data. We call this problem Unifying Heterogeneous Classifiers (UHC). This problem is motivated by scenarios where data is collected from multiple sources, but the sources cannot share their data, e.g., due to privacy concerns, and only privately trained models can be shared. In addition, each source may not be able to gather data to train all classes due to data availability at each source, and may not be able to train the same classification model due to different computational resources. To tackle this problem, we propose a generalisation of knowledge distillation to merge HCs. We derive a probabilistic relation between the outputs of HCs and the probability over all classes. Based on this relation, we propose two classes of methods based on cross-entropy minimisation and matrix factorisation, which allow us to estimate soft labels over all classes from unlabelled samples and use them in lieu of ground truth labels to train a unified classifier. Our extensive experiments on ImageNet, LSUN, and Places365 datasets show that our approaches significantly outperform a naive extension of distillation and can achieve almost the same accuracy as classifiers that are trained in a centralised, supervised manner.},
  archiveprefix = {arXiv},
  keywords = {double-distillation,exmodel},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\5NZYCQM7\\Vongkulbhisal et al_2019_Unifying Heterogeneous Classifiers with Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\6BSWIC38\\1904.html}
}

@inproceedings{vorontsovOrthogonalityLearningRecurrent2017,
  title = {On Orthogonality and Learning Recurrent Networks with Long Term Dependencies},
  booktitle = {{{ICML}}},
  author = {Vorontsov, Eugene and Trabelsi, Chiheb and Kadoury, Samuel and Pal, Chris},
  year = {2017},
  month = jan,
  pages = {3570--3578},
  abstract = {It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.},
  keywords = {orthogonal-nn,RNN}
}

@article{wangACAEREMINDOnlineContinual2021,
  title = {{{ACAE-REMIND}} for Online Continual Learning with Compressed Feature Replay},
  author = {Wang, Kai and {van de Weijer}, Joost and Herranz, Luis},
  year = {2021},
  month = oct,
  journal = {Pattern Recognition Letters},
  volume = {150},
  pages = {122--129},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2021.06.025},
  abstract = {Online continual learning aims to learn from a non-IID stream of data from a number of different tasks, where the learner is only allowed to consider data once. Methods are typically allowed to use a limited buffer to store some of the images in the stream. Recently, it was found that feature replay, where an intermediate layer representation of the image is stored (or generated) leads to superior results than image replay, while requiring less memory. Quantized exemplars can further reduce the memory usage. However, a drawback of these methods is that they use a fixed (or very intransigent) backbone network. This significantly limits the learning of representations that can discriminate between all tasks. To address this problem, we propose an auxiliary classifier auto-encoder (ACAE) module for feature replay at intermediate layers with high compression rates. The reduced memory footprint per image allows us to save more exemplars for replay. In our experiments, we conduct task-agnostic evaluation under online continual learning setting and get state-of-the-art performance on ImageNet-Subset, CIFAR100 and CIFAR10 dataset.},
  langid = {english},
  keywords = {feature-replay,notag,ocl,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4L67QFBK\\S0167865521002312.html}
}

@inproceedings{wangCAFELearningCondense2022,
  title = {{{CAFE}}: {{Learning To Condense Dataset}} by {{Aligning Features}}},
  shorttitle = {{{CAFE}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Kai and Zhao, Bo and Peng, Xiangyu and Zhu, Zheng and Yang, Shuo and Wang, Shuo and Huang, Guan and Bilen, Hakan and Wang, Xinchao and You, Yang},
  year = {2022},
  pages = {12196--12205},
  langid = {english},
  keywords = {dataset-distillation,meta-learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WHNP95VY\\Wang_CAFE_Learning_To_Condense_Dataset_by_Aligning_Features_CVPR_2022_paper.html}
}

@inproceedings{wangCseConceptualSentence2016,
  title = {Cse: {{Conceptual}} Sentence Embeddings Based on Attention Model},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Yashen and Huang, Heyan and Feng, Chong and Zhou, Qiang and Gu, Jiahui and Gao, Xiong},
  year = {2016},
  volume = {1},
  pages = {505--515},
  keywords = {attention,nlp,nlp-embeddings}
}

@article{wangDatasetDistillation2019,
  title = {Dataset {{Distillation}}},
  author = {Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.},
  year = {2019},
  month = sep,
  abstract = {We propose to distill a large dataset into a small set of synthetic data that can train networks close to original performance.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DXYM4G57\\Wang et al_2019_Dataset Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\AE4K9QCE\\forum.html;C\:\\Users\\w-32\\Zotero\\storage\\WKF7QCCP\\forum.html}
}

@article{wangDatasetDistillation2020,
  title = {Dataset {{Distillation}}},
  author = {Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.},
  year = {2020},
  month = feb,
  journal = {arXiv:1811.10959 [cs, stat]},
  eprint = {1811.10959},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60,000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to original performance with only a few gradient descent steps, given a fixed network initialization. We evaluate our method in various initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,data-distill,dataset condensation,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6DXGVMP9\\Wang et al_2020_Dataset Distillation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\DP7CEKYG\\1811.html}
}

@article{wangDistillingKnowledgeMimicking2021,
  title = {Distilling {{Knowledge}} by {{Mimicking Features}}},
  author = {Wang, Guo-Hua and Ge, Yifan and Wu, Jianxin},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2011.01424},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3103973},
  abstract = {Knowledge distillation (KD) is a popular method to train efficient networks (``student'') with the help of high-capacity networks (``teacher''). Traditional methods use the teacher's soft logits as extra supervision to train the student network. In this paper, we argue that it is more advantageous to make the student mimic the teacher's features in the penultimate layer. Not only the student can directly learn more effective information from the teacher feature, feature mimicking can also be applied for teachers trained without a softmax layer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {latent-knowledge-distillation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\U768STD4\\Wang et al. - 2021 - Distilling Knowledge by Mimicking Features.pdf}
}

@misc{wangDualPromptComplementaryPrompting2022,
  title = {{{DualPrompt}}: {{Complementary Prompting}} for {{Rehearsal-free Continual Learning}}},
  shorttitle = {{{DualPrompt}}},
  author = {Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  year = {2022},
  month = aug,
  number = {arXiv:2204.04799},
  eprint = {2204.04799},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.04799},
  abstract = {Continual learning aims to enable a single model to learn a sequence of tasks without catastrophic forgetting. Top-performing methods usually require a rehearsal buffer to store past pristine examples for experience replay, which, however, limits their practical value due to privacy and memory constraints. In this work, we present a simple yet effective framework, DualPrompt, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially without buffering past examples. DualPrompt presents a novel approach to attach complementary prompts to the pre-trained backbone, and then formulates the objective as learning task-invariant and task-specific "instructions". With extensive experimental validation, DualPrompt consistently sets state-of-the-art performance under the challenging class-incremental setting. In particular, DualPrompt outperforms recent advanced continual learning methods with relatively large buffer sizes. We also introduce a more challenging benchmark, Split ImageNet-R, to help generalize rehearsal-free continual learning research. Source code is available at https://github.com/google-research/l2p.},
  archiveprefix = {arXiv},
  keywords = {continual,prompt-tuning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EBXDNNFU\\Wang et al_2022_DualPrompt.pdf;C\:\\Users\\w-32\\Zotero\\storage\\S2LUYKWL\\2204.html}
}

@inproceedings{wangFederatedLearningMatched2020,
  title = {Federated {{Learning}} with {{Matched Averaging}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman},
  year = {2020},
  month = mar,
  abstract = {Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.},
  langid = {english},
  keywords = {federated,model-patching},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\EXS4MMWW\\Wang et al_2020_Federated Learning with Matched Averaging.pdf;C\:\\Users\\w-32\\Zotero\\storage\\83F22ITZ\\forum.html}
}

@misc{wangFieldGuideFederated2021,
  title = {A {{Field Guide}} to {{Federated Optimization}}},
  author = {Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H. Brendan and y Arcas, Blaise Aguera and {Al-Shedivat}, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and Diggavi, Suhas and Eichner, Hubert and Gadhikar, Advait and Garrett, Zachary and Girgis, Antonious M. and Hanzely, Filip and Hard, Andrew and He, Chaoyang and Horvath, Samuel and Huo, Zhouyuan and Ingerman, Alex and Jaggi, Martin and Javidi, Tara and Kairouz, Peter and Kale, Satyen and Karimireddy, Sai Praneeth and Konecny, Jakub and Koyejo, Sanmi and Li, Tian and Liu, Luyang and Mohri, Mehryar and Qi, Hang and Reddi, Sashank J. and Richtarik, Peter and Singhal, Karan and Smith, Virginia and Soltanolkotabi, Mahdi and Song, Weikang and Suresh, Ananda Theertha and Stich, Sebastian U. and Talwalkar, Ameet and Wang, Hongyi and Woodworth, Blake and Wu, Shanshan and Yu, Felix X. and Yuan, Honglin and Zaheer, Manzil and Zhang, Mi and Zhang, Tong and Zheng, Chunxiang and Zhu, Chen and Zhu, Wennan},
  year = {2021},
  month = jul,
  number = {arXiv:2107.06917},
  eprint = {2107.06917},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.06917},
  abstract = {Federated learning and analytics are a distributed approach for collaboratively learning models (or statistics) from decentralized data, motivated by and designed for privacy protection. The distributed learning process can be formulated as solving federated optimization problems, which emphasize communication efficiency, data heterogeneity, compatibility with privacy and system requirements, and other constraints that are not primary considerations in other problem settings. This paper provides recommendations and guidelines on formulating, designing, evaluating and analyzing federated optimization algorithms through concrete examples and practical implementation, with a focus on conducting effective simulations to infer real-world performance. The goal of this work is not to survey the current literature, but to inspire researchers and practitioners to design federated learning algorithms that can be used in various practical applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,federated},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YQ653L2B\\2107.html}
}

@article{wangGatedRecurrentUnit,
  title = {A {{Gated Recurrent Unit}} Based {{Echo State Network}}},
  author = {Wang, Xinjie and Jin, Yaochu and Hao, Kuangrong},
  pages = {7},
  abstract = {Echo State Network (ESN) is a fast and efficient recurrent neural network with a sparsely connected reservoir and a simple linear output layer, which has been widely used for real-world prediction problems. However, the capability of the ESN of handling complex nonlinear problems is limited by the relatively simple neuronal dynamics in the reservoir. Although the gated recurrent unit (GRU) model with multiple nonlinear operators has achieved an excellent performance, gradient-based training algorithms usually require intensive computational resources. In this paper, we present a novel ESN model based on GRUs to tackle complex real-world tasks while reducing the computational costs, taking advantage of the characteristics of both the ESN and the GRU models. In the proposed model, the reservoir unit is replaced by the sparsely connected GRU neurons. Experimental results on three regression problems demonstrate that the proposed method performs better than the original ESN and GRU models.},
  langid = {english},
  keywords = {ESN,LSTM,rnn-gates,WCCI20},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\RIBT5NN3\\Wang et al. - A Gated Recurrent Unit based Echo State Network.pdf}
}

@article{wangKnowledgeGraphEmbedding2017,
  title = {Knowledge Graph Embedding: {{A}} Survey of Approaches and Applications},
  author = {Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},
  year = {2017},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {29},
  number = {12},
  pages = {2724--2743},
  doi = {10.1109/TKDE.2017.2754499},
  abstract = {\textemdash Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.},
  keywords = {DGN,tensor},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Y57M2ZAT\\Wang et al. - 2017 - Knowledge graph embedding A survey of approaches and applications.pdf}
}

@misc{wangLearningPromptContinual2022,
  title = {Learning to {{Prompt}} for {{Continual Learning}}},
  author = {Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  year = {2022},
  month = mar,
  number = {arXiv:2112.08654},
  eprint = {2112.08654},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.08654},
  abstract = {The mainstream paradigm behind continual learning has been to adapt the model parameters to non-stationary data distributions, where catastrophic forgetting is the central challenge. Typical methods rely on a rehearsal buffer or known task identity at test time to retrieve learned knowledge and address forgetting, while this work presents a new paradigm for continual learning that aims to train a more succinct memory system without accessing task identity at test time. Our method learns to dynamically prompt (L2P) a pre-trained model to learn tasks sequentially under different task transitions. In our proposed framework, prompts are small learnable parameters, which are maintained in a memory space. The objective is to optimize prompts to instruct the model prediction and explicitly manage task-invariant and task-specific knowledge while maintaining model plasticity. We conduct comprehensive experiments under popular image classification benchmarks with different challenging continual learning settings, where L2P consistently outperforms prior state-of-the-art methods. Surprisingly, L2P achieves competitive results against rehearsal-based methods even without a rehearsal buffer and is directly applicable to challenging task-agnostic continual learning. Source code is available at https://github.com/google-research/l2p.},
  archiveprefix = {arXiv},
  keywords = {clip,continual,notag,prompt-engineering},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PB6JL57Z\\Wang et al_2022_Learning to Prompt for Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\W4NF279T\\2112.html}
}

@article{wangLinformerSelfAttentionLinear2020,
  title = {Linformer: {{Self-Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.04768 [cs, stat]},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\^2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\^2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslash textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archiveprefix = {arXiv},
  keywords = {linear,model compression,nlp,transformer},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\P8YRTEIR\\Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf;C\:\\Users\\w-32\\Zotero\\storage\\LBUYBUNB\\2006.html}
}

@inproceedings{wangLongShorttermMemory2015,
  title = {A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Wang, Di and Nyberg, Eric},
  year = {2015},
  volume = {2},
  pages = {707--712},
  keywords = {LSTM,nlp,RNN}
}

@misc{wangOnlineContinualLearning2022,
  title = {Online {{Continual Learning}} with {{Contrastive Vision Transformer}}},
  author = {Wang, Zhen and Liu, Liu and Kong, Yajing and Guo, Jiaxian and Tao, Dacheng},
  year = {2022},
  month = jul,
  number = {arXiv:2207.13516},
  eprint = {2207.13516},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.13516},
  abstract = {Online continual learning (online CL) studies the problem of learning sequential tasks from an online data stream without task boundaries, aiming to adapt to new data while alleviating catastrophic forgetting on the past tasks. This paper proposes a framework Contrastive Vision Transformer (CVT), which designs a focal contrastive learning strategy based on a transformer architecture, to achieve a better stability-plasticity trade-off for online CL. Specifically, we design a new external attention mechanism for online CL that implicitly captures previous tasks' information. Besides, CVT contains learnable focuses for each class, which could accumulate the knowledge of previous classes to alleviate forgetting. Based on the learnable focuses, we design a focal contrastive loss to rebalance contrastive learning between new and past classes and consolidate previously learned representations. Moreover, CVT contains a dual-classifier structure for decoupling learning current classes and balancing all observed classes. The extensive experimental results show that our approach achieves state-of-the-art performance with even fewer parameters on online CL benchmarks and effectively alleviates the catastrophic forgetting.},
  archiveprefix = {arXiv},
  keywords = {continual,contrastive,notag,transformer,unread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\NZZPDSV4\\2207.html}
}

@article{wangPickingWinningTickets2020,
  title = {Picking {{Winning Tickets Before Training}} by {{Preserving Gradient Flow}}},
  author = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  year = {2020},
  month = feb,
  abstract = {Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80\% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6\% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels.},
  langid = {english},
  keywords = {lottery-ticket,pruning,sparsity,training},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LRLTC5FQ\\Wang et al_2020_Picking Winning Tickets Before Training by Preserving Gradient Flow.pdf;C\:\\Users\\w-32\\Zotero\\storage\\9VWP92C2\\2002.html}
}

@article{wangRecurrentNeuralNetworks2018,
  title = {Recurrent Neural Networks with Auxiliary Memory Units},
  author = {Wang, Jianyong and Zhang, Lei and Guo, Quan and Yi, Zhang},
  year = {2018},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {5},
  pages = {1652--1661},
  doi = {10.1109/TNNLS.2017.2677968},
  abstract = {\textcopyright{} 2012 IEEE. Memory is one of the most important mechanisms in recurrent neural networks (RNNs) learning. It plays a crucial role in practical applications, such as sequence learning. With a good memory mechanism, long term history can be fused with current information, and can thus improve RNNs learning. Developing a suitable memory mechanism is always desirable in the field of RNNs. This paper proposes a novel memory mechanism for RNNs. The main contributions of this paper are: 1) an auxiliary memory unit (AMU) is proposed, which results in a new special RNN model (AMU-RNN), separating the memory and output explicitly and 2) an efficient learning algorithm is developed by employing the technique of error flow truncation. The proposed AMU-RNN model, together with the developed learning algorithm, can learn and maintain stable memory over a long time range. This method overcomes both the learning conflict problem and gradient vanishing problem. Unlike the traditional method, which mixes the memory and output with a single neuron in a recurrent unit, the AMU provides an auxiliary memory neuron to maintain memory in particular. By separating the memory and output in a recurrent unit, the problem of learning conflicts can be eliminated easily. Moreover, by using the technique of error flow truncation, each auxiliary memory neuron ensures constant error flow during the learning process. The experiments demonstrate good performance of the proposed AMU-RNNs and the developed learning algorithm. The method exhibits quite efficient learning performance with stable convergence in the AMU-RNN learning and outperforms the state-of-the-art RNN models in sequence generation and sequence classification tasks.},
  keywords = {MANN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\Z9GPLXVV\\Wang et al. - 2018 - Recurrent neural networks with auxiliary memory units.pdf}
}

@article{wangRecurrentNeuralNetworks2018a,
  title = {Recurrent {{Neural Networks With Auxiliary Memory Units}}},
  author = {Wang, J. and Zhang, L. and Guo, Q. and Yi, Z.},
  year = {2018},
  month = may,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {5},
  pages = {1652--1661},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2017.2677968},
  abstract = {Memory is one of the most important mechanisms in recurrent neural networks (RNNs) learning. It plays a crucial role in practical applications, such as sequence learning. With a good memory mechanism, long term history can be fused with current information, and can thus improve RNNs learning. Developing a suitable memory mechanism is always desirable in the field of RNNs. This paper proposes a novel memory mechanism for RNNs. The main contributions of this paper are: 1) an auxiliary memory unit (AMU) is proposed, which results in a new special RNN model (AMU-RNN), separating the memory and output explicitly and 2) an efficient learning algorithm is developed by employing the technique of error flow truncation. The proposed AMU-RNN model, together with the developed learning algorithm, can learn and maintain stable memory over a long time range. This method overcomes both the learning conflict problem and gradient vanishing problem. Unlike the traditional method, which mixes the memory and output with a single neuron in a recurrent unit, the AMU provides an auxiliary memory neuron to maintain memory in particular. By separating the memory and output in a recurrent unit, the problem of learning conflicts can be eliminated easily. Moreover, by using the technique of error flow truncation, each auxiliary memory neuron ensures constant error flow during the learning process. The experiments demonstrate good performance of the proposed AMU-RNNs and the developed learning algorithm. The method exhibits quite efficient learning performance with stable convergence in the AMU-RNN learning and outperforms the state-of-the-art RNN models in sequence generation and sequence classification tasks.},
  keywords = {auxiliary memory neuron,auxiliary memory unit,Biological neural networks,Computer architecture,Convergence,developed learning algorithm,efficient learning algorithm,error flow truncation,good memory mechanism,important mechanisms,learning (artificial intelligence),Learning algorithms,learning conflict problem,learning conflicts,learning process,Logic gates,long term history,memory,Microprocessors,neural nets,Neurons,novel memory mechanism,recurrent neural nets,recurrent neural networks,recurrent neural networks (RNNs),recurrent unit,RNN,RNNs,sequence learning,special RNN model,stable memory,state-of-the-art RNN models,suitable memory mechanism},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\92NZBAFB\\Wang et al_2018_Recurrent Neural Networks With Auxiliary Memory Units.pdf;C\:\\Users\\w-32\\Zotero\\storage\\9Z2QQUIG\\7883962.html}
}

@article{wangRecurrentResidualLearning2016,
  title = {Recurrent {{Residual Learning}} for {{Sequence Classification}}},
  author = {Wang, Yiren and Tian, Fei},
  year = {2016},
  journal = {In. EMNLP},
  pages = {938--943},
  doi = {10.1016/j.bmcl.2015.06.073},
  abstract = {In this paper, we explore the possibility of leveraging Residual Networks (ResNet), a powerful structure in constructing extremely deep neural network for image understanding, to improve recurrent neural networks (RNN) for modeling sequential data. We show that for sequence classification tasks, incorporating residual connections into recurrent structures yields similar accuracy to Long Short Term Memory (LSTM) RNN with much fewer model parameters. In addition, we propose two novel models which combine the best of both residual learning and LSTM. Experiments show that the new models significantly outperform LSTM.},
  keywords = {nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WQW7HAH5\\Wang, Tian - 2016 - Recurrent Residual Learning for Sequence Classification(2).pdf}
}

@misc{wangSparCLSparseContinual2022,
  title = {{{SparCL}}: {{Sparse Continual Learning}} on the {{Edge}}},
  shorttitle = {{{SparCL}}},
  author = {Wang, Zifeng and Zhan, Zheng and Gong, Yifan and Yuan, Geng and Niu, Wei and Jian, Tong and Ren, Bin and Ioannidis, Stratis and Wang, Yanzhi and Dy, Jennifer},
  year = {2022},
  month = sep,
  number = {arXiv:2209.09476},
  eprint = {2209.09476},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.09476},
  abstract = {Existing work in continual learning (CL) focuses on mitigating catastrophic forgetting, i.e., model performance deterioration on past tasks when learning a new task. However, the training efficiency of a CL system is under-investigated, which limits the real-world application of CL systems under resource-limited scenarios. In this work, we propose a novel framework called Sparse Continual Learning(SparCL), which is the first study that leverages sparsity to enable cost-effective continual learning on edge devices. SparCL achieves both training acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efficiency, and gradient sparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a sparse network throughout the entire CL process, dynamic data removal (DDR) to remove less informative training data, and dynamic gradient masking (DGM) to sparsify the gradient updates. Each of them not only improves efficiency, but also further mitigates catastrophic forgetting. SparCL consistently improves the training efficiency of existing state-of-the-art (SOTA) CL methods by at most 23X less training FLOPs, and, surprisingly, further improves the SOTA accuracy by at most 1.7\%. SparCL also outperforms competitive baselines obtained from adapting SOTA sparse training methods to the CL setting in both efficiency and accuracy. We also evaluate the effectiveness of SparCL on a real mobile phone, further indicating the practical potential of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,edge-computing,sparsity,unread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3E646Z3V\\2209.html}
}

@inproceedings{wangStateRegularizedRecurrentNeural2019,
  title = {State-{{Regularized Recurrent Neural Networks}}},
  author = {Wang, Cheng and Niepert, Mathias},
  year = {2019},
  volume = {36},
  pages = {101--118},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3QU5J3NK\\Wang, Niepert - 2019 - State-Regularized Recurrent Neural Networks.pdf}
}

@article{wardenSpeechCommandsDataset2018,
  title = {Speech {{Commands}}: {{A Dataset}} for {{Limited-Vocabulary Speech Recognition}}},
  shorttitle = {Speech {{Commands}}},
  author = {Warden, Pete},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.03209 [cs]},
  eprint = {1804.03209},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset.},
  archiveprefix = {arXiv},
  keywords = {data,speech,SpeechCommands},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZK5UQQIT\\Warden - 2018 - Speech Commands A Dataset for Limited-Vocabulary .pdf;C\:\\Users\\w-32\\Zotero\\storage\\ZVWDMX7Q\\1804.html}
}

@article{wenStructuredPruningRecurrent2020,
  title = {Structured Pruning of Recurrent Neural Networks through Neuron Selection},
  author = {Wen, Liangjian and Zhang, Xuanyang and Bai, Haoli and Xu, Zenglin},
  year = {2020},
  month = mar,
  journal = {Neural Networks},
  volume = {123},
  pages = {134--141},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.11.018},
  abstract = {Recurrent neural networks (RNNs) have recently achieved remarkable successes in a number of applications. However, the huge sizes and computational burden of these models make it difficult for their deployment on edge devices. A practically effective approach is to reduce the overall storage and computation costs of RNNs by network pruning techniques. Despite their successful applications, those pruning methods based on Lasso either produce irregular sparse patterns in weight matrices, which is not helpful in practical speedup. To address these issues, we propose a structured pruning method through neuron selection which can remove the independent neuron of RNNs. More specifically, we introduce two sets of binary random variables, which can be interpreted as gates or switches to the input neurons and the hidden neurons, respectively. We demonstrate that the corresponding optimization problem can be addressed by minimizing the L0 norm of the weight matrix. Finally, experimental results on language modeling and machine reading comprehension tasks have indicated the advantages of the proposed method in comparison with state-of-the-art pruning competitors. In particular, nearly 20\texttimes{} practical speedup during inference was achieved without losing performance for the language model on the Penn TreeBank dataset, indicating the promising performance of the proposed method.},
  langid = {english},
  keywords = {Feature selection,Learning sparse models,Model compression,pruning,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4RWPLNYA\\Wen et al_2020_Structured pruning of recurrent neural networks through neuron selection.pdf;C\:\\Users\\w-32\\Zotero\\storage\\KM3DY4TZ\\S0893608019303776.html}
}

@article{werbosGeneralizationBackpropagationApplication1988,
  title = {Generalization of Backpropagation with Application to a Recurrent Gas Market Model},
  author = {Werbos, Paul J.},
  year = {1988},
  month = jan,
  journal = {Neural Networks},
  volume = {1},
  pages = {339--356},
  doi = {10.1016/0893-6080(88)90007-x},
  abstract = {n/a},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TTAA86IZ\\Werbos - 1988 - Generalization of backpropagation with application.pdf}
}

@article{westonAICompleteQuestionAnswering2015,
  ids = {westonAICompleteQuestionAnswering2015a},
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and {van Merri{\"e}nboer}, Bart and Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  issn = {1502.05698},
  doi = {10.1016/j.jpowsour.2014.09.131},
  abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
  keywords = {data,nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BR6XXV7Z\\Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of Prerequisite Toy Tasks(2).pdf;C\:\\Users\\w-32\\Zotero\\storage\\CDJNHZ9A\\Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of Prerequisite Toy Tasks(3).pdf}
}

@article{westonMemoryNetworks2014,
  title = {Memory {{Networks}}},
  author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  year = {2014},
  month = oct,
  journal = {CoRR},
  volume = {abs/1410.3},
  issn = {9781424469178},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  keywords = {MANN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CY7R7FT4\\Weston, Chopra, Bordes - 2014 - Memory Networks(3).pdf}
}

@article{whiteShorttermMemoryOrthogonal2004,
  title = {Short-Term Memory in Orthogonal Neural Networks},
  author = {White, Olivia L. and Lee, Daniel D. and Sompolinsky, Haim},
  year = {2004},
  journal = {Physical Review Letters},
  volume = {92},
  number = {14},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.92.148102},
  abstract = {We study the ability of linear recurrent networks obeying discrete time dynamics to store long temporal sequences that are retrievable from the instantaneous state of the network. We calculate this temporal memory capacity for both distributed shift register and random orthogonal connectivity matrices. We show that the memory capacity of these networks scales with system size.},
  keywords = {Memory,orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QRWGJVD6\\White, Lee, Sompolinsky - 2004 - Short-term memory in orthogonal neural networks.pdf}
}

@techreport{WHYGRADIENTCLIPPING,
  title = {{{WHY GRADIENT CLIPPING ACCELERATES TRAINING}}: {{A THEORETICAL JUSTIFICATION FOR ADAPTIVITY}}},
  abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
  keywords = {learning,optimization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2IQP99IT\\Unknown - Unknown - WHY GRADIENT CLIPPING ACCELERATES TRAINING A THEORETICAL JUSTIFICATION FOR ADAPTIVITY.pdf}
}

@article{wietingNoTrainingRequired2019,
  title = {No {{Training Required}}: {{Exploring Random Encoders}} for {{Sentence Classification}}},
  author = {Wieting, John and Kiela, Douwe},
  year = {2019},
  month = jan,
  abstract = {We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZMQSR2HT\\Wieting, Kiela - 2019 - No Training Required Exploring Random Encoders for Sentence Classification.pdf}
}

@article{wietingParaphraseDatabaseCompositional2015,
  title = {From Paraphrase Database to Compositional Paraphrase Model and Back},
  author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen and Roth, Dan},
  year = {2015},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {345--358},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\582M9WC6\\Wieting et al. - 2015 - From paraphrase database to compositional paraphrase model and back(2).pdf}
}

@inproceedings{wietingRevisitingRecurrentNetworks2017,
  title = {Revisiting {{Recurrent Networks}} for {{Paraphrastic Sentence Embeddings}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wieting, John and Gimpel, Kevin},
  year = {2017},
  month = apr,
  pages = {2078--2088},
  abstract = {We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.},
  keywords = {nlp,nlp-embeddings,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QPDY9WFW\\Wieting, Gimpel - 2017 - Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings(3).pdf}
}

@article{wietingUniversalParaphrasticSentence2015,
  title = {Towards {{Universal Paraphrastic Sentence Embeddings}}},
  author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  year = {2015},
  month = nov,
  journal = {arXiv preprint arXiv:1511.08198},
  eprint = {1511.08198},
  eprinttype = {arxiv},
  abstract = {We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use. In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classification. We again find that the word averaging models perform well for sentence similarity and entailment, outperforming LSTMs. However, on sentiment classification, we find that the LSTM performs very strongly-even recording new state-of-the-art performance on the Stanford Sentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings with these supervised tasks, using them both as a prior and as a black box feature extractor. This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks. We release all of our resources to the research community with the hope that they can serve as the new baseline for further work on universal sentence embeddings.},
  archiveprefix = {arXiv},
  keywords = {nlp,nlp-embeddings},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\UXHDPF93\\Wieting et al. - 2015 - Towards Universal Paraphrastic Sentence Embeddings(3).pdf}
}

@article{wiewelCondensedCompositeMemory2021,
  title = {Condensed {{Composite Memory Continual Learning}}},
  author = {Wiewel, Felix and Yang, Bin},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.09890 [cs]},
  eprint = {2102.09890},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep Neural Networks (DNNs) suffer from a rapid decrease in performance when trained on a sequence of tasks where only data of the most recent task is available. This phenomenon, known as catastrophic forgetting, prevents DNNs from accumulating knowledge over time. Overcoming catastrophic forgetting and enabling continual learning is of great interest since it would enable the application of DNNs in settings where unrestricted access to all the training data at any time is not always possible, e.g. due to storage limitations or legal issues. While many recently proposed methods for continual learning use some training examples for rehearsal, their performance strongly depends on the number of stored examples. In order to improve performance of rehearsal for continual learning, especially for a small number of stored examples, we propose a novel way of learning a small set of synthetic examples which capture the essence of a complete dataset. Instead of directly learning these synthetic examples, we learn a weighted combination of shared components for each example that enables a significant increase in memory efficiency. We demonstrate the performance of our method on commonly used datasets and compare it to recently proposed related methods and baselines.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\KLF89CBR\\Wiewel_Yang_2021_Condensed Composite Memory Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\N99KTSLD\\2102.html}
}

@article{wilamowskiSolvingParityNProblems2004,
  title = {Solving Parity-{{N}} Problems with Feedforward Neural Networks},
  author = {Wilamowski, B.M. and Hunter, D. and Mabnowski, A.},
  year = {2004},
  pages = {2546--2551},
  issn = {0780378989},
  doi = {10.1109/ijcnn.2003.1223966},
  keywords = {learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ZXIB5H45\\Wilamowski, Hunter, Mabnowski - 2004 - Solving parity-N problems with feedforward neural networks.pdf}
}

@article{wilsonMarginalValueAdaptive2017,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  year = {2017},
  pages = {1--14},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
  keywords = {sgd-theory},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4LI4T698\\Wilson et al. - 2017 - The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf}
}

@article{wintersSoftwareEngineeringGoogle,
  title = {Software {{Engineering}} at {{Google}}},
  author = {Winters, Titus and Manschreck, Tom and Wright, Hyrum},
  pages = {602},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\W8T8WH4M\\Winters et al. - Software Engineering at Google.pdf}
}

@inproceedings{wisdomFullCapacityUnitaryRecurrent2016,
  title = {Full-{{Capacity Unitary Recurrent Neural Networks}}},
  booktitle = {{{NIPS}}},
  author = {Wisdom, Scott and Powers, Thomas and Hershey, John R. and Roux, Jonathan Le and Atlas, Les},
  year = {2016},
  month = oct,
  pages = {4880--4888},
  abstract = {Recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems. Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues. However, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned? To address this question, we propose full-capacity uRNNs that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. Our contribution consists of two main components. First, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity. Using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. Second, we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices. The resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation. We confirm the utility of our claims by empirically evaluating our new full-capacity uRNNs on both synthetic and natural data, achieving superior performance compared to both LSTMs and the original restricted-capacity uRNNs.},
  keywords = {orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9TYSNRVN\\Wisdom et al. - 2016 - Full-Capacity Unitary Recurrent Neural Networks(2).pdf}
}

@inproceedings{wolczykContinualWorldRobotic2021,
  title = {Continual {{World}}: {{A Robotic Benchmark For Continual Reinforcement Learning}}},
  shorttitle = {Continual {{World}}},
  booktitle = {Thirty-{{Fifth Conference}} on {{Neural Information Processing Systems}}},
  author = {Wolczyk, Maciej and Zaj{\k{a}}c, Micha{\l} and Pascanu, Razvan and Kuci{\'n}ski, {\L}ukasz and Mi{\l}o{\'s}, Piotr},
  year = {2021},
  month = may,
  abstract = {A Robotic Benchmark for Continual Reinforcement Learning},
  langid = {english},
  keywords = {Benchmarks,continual,notag,RL},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Wolczyk et al_2021_Continual World.pdf;C\:\\Users\\w-32\\Zotero\\storage\\J8U7HQ64\\forum.html}
}

@article{wolterGatedComplexRecurrent2018,
  title = {Gated {{Complex Recurrent Neural Networks}}},
  author = {Wolter, Moritz and Yao, Angela},
  year = {2018},
  month = jun,
  abstract = {Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gate recurrent cell. When used together with norm-preserving state transition matrices, our complex gated RNN exhibits excellent stability and convergence properties. We demonstrate competitive performance of our complex gated RNN on the synthetic memory and adding task, as well as on the real-world task of human motion prediction.},
  keywords = {RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TM4G3AU5\\Wolter, Yao - 2018 - Gated Complex Recurrent Neural Networks(2).pdf}
}

@misc{wortsmanLearningNeuralNetwork2021,
  title = {Learning {{Neural Network Subspaces}}},
  author = {Wortsman, Mitchell and Horton, Maxwell and Guestrin, Carlos and Farhadi, Ali and Rastegari, Mohammad},
  year = {2021},
  month = sep,
  number = {arXiv:2102.10472},
  eprint = {2102.10472},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2102.10472},
  abstract = {Recent observations have advanced our understanding of the neural network optimization landscape, revealing the existence of (1) paths of high accuracy containing diverse solutions and (2) wider minima offering improved performance. Previous methods observing diverse paths require multiple training runs. In contrast we aim to leverage both property (1) and (2) with a single method and in a single training run. With a similar computational cost as training one model, we learn lines, curves, and simplexes of high-accuracy neural networks. These neural network subspaces contain diverse solutions that can be ensembled, approaching the ensemble performance of independently trained networks without the training cost. Moreover, using the subspace midpoint boosts accuracy, calibration, and robustness to label noise, outperforming Stochastic Weight Averaging.},
  archiveprefix = {arXiv},
  keywords = {lottery-ticket,mode-connectivity,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\YMGAT99Q\\Wortsman et al_2021_Learning Neural Network Subspaces.pdf;C\:\\Users\\w-32\\Zotero\\storage\\QMHCDRCM\\2102.html}
}

@misc{wortsmanRobustFinetuningZeroshot2022,
  title = {Robust Fine-Tuning of Zero-Shot Models},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and {Gontijo-Lopes}, Raphael and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
  year = {2022},
  month = jun,
  number = {arXiv:2109.01903},
  eprint = {2109.01903},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.01903},
  abstract = {Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on seven commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.},
  archiveprefix = {arXiv},
  keywords = {clip,model-patching,vision-language},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\MVP89QKS\\2109.html}
}

@article{wortsmanSupermasksSuperposition2020,
  title = {Supermasks in {{Superposition}}},
  author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.14769 [cs, stat]},
  eprint = {2006.14769},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.},
  archiveprefix = {arXiv},
  keywords = {architectural,continual,lottery-ticket,masking},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6D3GTRY3\\Wortsman et al. - 2020 - Supermasks in Superposition.pdf;C\:\\Users\\w-32\\Zotero\\storage\\JTRTSE4K\\2006.html}
}

@inproceedings{wuClassIncrementalLearningStrong2022,
  title = {Class-{{Incremental Learning With Strong Pre-Trained Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wu, Tz-Ying and Swaminathan, Gurumurthy and Li, Zhizhong and Ravichandran, Avinash and Vasconcelos, Nuno and Bhotika, Rahul and Soatto, Stefano},
  year = {2022},
  pages = {9601--9610},
  langid = {english},
  keywords = {Continual,foundation-models,pretraining},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ULZGUPZP\\Wu_Class-Incremental_Learning_With_Strong_Pre-Trained_Models_CVPR_2022_paper.html}
}

@inproceedings{wuKanervaMachineGenerative2018,
  title = {The {{Kanerva Machine}}: {{A Generative Distributed Memory}}},
  booktitle = {{{ICLR}}},
  author = {Wu, Yan and Wayne, Greg and Graves, Alex and Lillicrap, Timothy},
  year = {2018},
  month = feb,
  abstract = {Abstract: We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train. TL;DR: A generative memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory. Keywords: memory, generative model, inference, neural network, hierarchical model},
  keywords = {ICLR,MANN,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FRQC247S\\Wu et al. - 2018 - The Kanerva Machine A Generative Distributed Memory(4).pdf}
}

@inproceedings{wuPretrainedLanguageModel2021,
  title = {Pretrained {{Language Model}} in {{Continual Learning}}: {{A Comparative Study}}},
  shorttitle = {Pretrained {{Language Model}} in {{Continual Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wu, Tongtong and Caccia, Massimo and Li, Zhuang and Li, Yuan-Fang and Qi, Guilin and Haffari, Gholamreza},
  year = {2021},
  month = sep,
  abstract = {Continual learning (CL) is a  setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been...},
  langid = {english},
  keywords = {continual,large-models,pretraining,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4G8YCQWV\\forum.html}
}

@inproceedings{wuPretrainedLanguageModel2022,
  title = {Pretrained {{Language Model}} in {{Continual Learning}}: {{A Comparative Study}}},
  shorttitle = {Pretrained {{Language Model}} in {{Continual Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wu, Tongtong and Caccia, Massimo and Li, Zhuang and Li, Yuan-Fang and Qi, Guilin and Haffari, Gholamreza},
  year = {2022},
  month = mar,
  abstract = {Continual learning (CL) is a  setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been...},
  langid = {english},
  keywords = {Continual,large-language-models,pretraining},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\42UPI83Q\\forum.html}
}

@inproceedings{wuWhySkipIf2020,
  title = {Why {{Skip If You Can Combine}}: {{A Simple Knowledge Distillation Technique}} for {{Intermediate Layers}}},
  shorttitle = {Why {{Skip If You Can Combine}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Wu, Yimeng and Passban, Peyman and Rezagholizadeh, Mehdi and Liu, Qun},
  year = {2020},
  pages = {1016--1021},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.74},
  abstract = {With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T ) into a compact student network (S). Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative. In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese\textrightarrow English, Turkish\textrightarrow English, and English\textrightarrow German directions. Students trained using our technique have 50\% fewer parameters and can still deliver comparable results to those of 12-layer teachers.},
  langid = {english},
  keywords = {latent-knowledge-distillation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\JKTKXJBL\\Wu et al. - 2020 - Why Skip If You Can Combine A Simple Knowledge Di.pdf}
}

@article{xiaoFashionMNISTNovelImage2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  shorttitle = {Fashion-{{MNIST}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017},
  month = sep,
  journal = {arXiv:1708.07747 [cs, stat]},
  eprint = {1708.07747},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QNA7IVT6\\Xiao et al_2017_Fashion-MNIST.pdf;C\:\\Users\\w-32\\Zotero\\storage\\JSGDALVQ\\1708.html}
}

@article{xieExploringRandomlyWired2019,
  title = {Exploring {{Randomly Wired Neural Networks}} for {{Image Recognition}}},
  author = {Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.01569 [cs]},
  eprint = {1904.01569},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,random-net},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Xie et al_2019_Exploring Randomly Wired Neural Networks for Image Recognition.pdf;C\:\\Users\\w-32\\Zotero\\storage\\C8GV64MK\\1904.html}
}

@inproceedings{xuClosingGeneralizationGap2022,
  title = {Closing the {{Generalization Gap}} of {{Cross-Silo Federated Medical Image Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xu, An and Li, Wenqi and Guo, Pengfei and Yang, Dong and Roth, Holger R. and Hatamizadeh, Ali and Zhao, Can and Xu, Daguang and Huang, Heng and Xu, Ziyue},
  year = {2022},
  pages = {20866--20875},
  langid = {english},
  keywords = {Continual,federated},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\94ETRU8C\\2203.10144.pdf;C\:\\Users\\w-32\\Zotero\\storage\\SB7QRK9Y\\Xu_Closing_the_Generalization_Gap_of_Cross-Silo_Federated_Medical_Image_Segmentation_CVPR_2022_.html}
}

@article{xuForgetMeNot,
  title = {Forget {{Me Not}}: {{Reducing Catastrophic Forgetting}} for {{Domain Adaptation}} in {{Reading Comprehension}}},
  author = {Xu, Ying and Zhong, Xu and Yepes, Antonio Jose Jimeno and Lau, Jey Han},
  pages = {8},
  abstract = {The creation of large-scale open domain reading comprehension data sets in recent years has enabled the development of end-to-end neural comprehension models with promising results. To use these models for domains with limited training data, one of the most effective approach is to first pre-train them on large out-of-domain source data and then fine-tune them with the limited target data. The caveat of this is that after fine-tuning the comprehension models tend to perform poorly in the source domain, a phenomenon known as catastrophic forgetting. In this paper, we explore methods that reduce catastrophic forgetting during fine-tuning without assuming access to data from the source domain. We introduce new auxiliary penalty terms and observe the best performance when a combination of auxiliary penalty terms is used to regularise the fine-tuning process for adapting comprehension models. To test our methods, we develop and release 6 narrow domain data sets that can potentially be used as reading comprehension benchmarks.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\32BJRB6S\\Xu et al. - Forget Me Not Reducing Catastrophic Forgetting fo.pdf}
}

@inproceedings{xuForgetMeNot2020,
  title = {Forget {{Me Not}}: {{Reducing Catastrophic Forgetting}} for {{Domain Adaptation}} in {{Reading Comprehension}}},
  shorttitle = {Forget {{Me Not}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Xu, Ying and Zhong, Xu and Yepes, Antonio Jose Jimeno and Lau, Jey Han},
  year = {2020},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9206891},
  abstract = {The creation of large-scale open domain reading comprehension data sets in recent years has enabled the development of end-to-end neural comprehension models with promising results. To use these models for domains with limited training data, one of the most effective approach is to first pre-train them on large out-of-domain source data and then fine-tune them with the limited target data. The caveat of this is that after fine-tuning the comprehension models tend to perform poorly in the source domain, a phenomenon known as catastrophic forgetting. In this paper, we explore methods that reduce catastrophic forgetting during fine-tuning without assuming access to data from the source domain. We introduce new auxiliary penalty terms and observe the best performance when a combination of auxiliary penalty terms is used to regularise the fine-tuning process for adapting comprehension models. To test our methods, we develop and release 6 narrow domain data sets that can potentially be used as reading comprehension benchmarks.},
  keywords = {continual,nlp},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BJHFS929\\9206891.html}
}

@inproceedings{xuShowAttendTell2015,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  booktitle = {International Conference on Machine Learning},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  year = {2015},
  pages = {2048--2057},
  doi = {10.1109/72.279181},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  isbn = {1045-9227 VO - 5},
  keywords = {attention},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XIBBWX8R\\Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf}
}

@misc{Yadan2019Hydra,
  title = {Hydra - {{A}} Framework for Elegantly Configuring Complex Applications},
  author = {Yadan, Omry},
  year = {2019},
  howpublished = {Github}
}

@article{yalnizyan-carsonForgettingEnhancesEpisodic2022,
  title = {Forgetting {{Enhances Episodic Control With Structured Memories}}},
  author = {{Yalnizyan-Carson}, Annik and Richards, Blake A.},
  year = {2022},
  journal = {Frontiers in Computational Neuroscience},
  volume = {16},
  issn = {1662-5188},
  abstract = {Forgetting is a normal process in healthy brains, and evidence suggests that the mammalian brain forgets more than is required based on limitations of mnemonic capacity. Episodic memories, in particular, are liable to be forgotten over time. Researchers have hypothesized that it may be beneficial for decision making to forget episodic memories over time. Reinforcement learning offers a normative framework in which to test such hypotheses. Here, we show that a reinforcement learning agent that uses an episodic memory cache to find rewards in maze environments can forget a large percentage of older memories without any performance impairments, if they utilize mnemonic representations that contain structural information about space. Moreover, we show that some forgetting can actually provide a benefit in performance compared to agents with unbounded memories. Our analyses of the agents show that forgetting reduces the influence of outdated information and states which are not frequently visited on the policies produced by the episodic control system. These results support the hypothesis that some degree of forgetting can be beneficial for decision making, which can help to explain why the brain forgets more than is required by capacity limitations.},
  keywords = {Continual,forgetting,neuroscience,no-read,positive-forgetting}
}

@article{yamazakiAnalysisTimeSeries2019,
  title = {Analysis of {{Time Series Generated By Long Short-Term Memory Trained}} with {{Adversarial Imitation Learning}}},
  author = {Yamazaki, Seiya and Iizuka, Hiroyuki and Yamamoto, Masahito},
  year = {2019},
  pages = {224--228},
  issn = {9781728124841},
  keywords = {adversarial imitation,LSTM,time-series}
}

@inproceedings{yangAnarchicFederatedLearning2022,
  ids = {yangAnarchicFederatedLearning},
  title = {Anarchic {{Federated Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Yang, Haibo and Zhang, Xin and Khanduri, Prashant and Liu, Jia},
  year = {2022},
  month = jun,
  pages = {25331--25363},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Present-day federated learning (FL) systems deployed over edge networks consists of a large number of workers with high degrees of heterogeneity in data and/or computing capabilities, which call for flexible worker participation in terms of timing, effort, data heterogeneity, etc. To satisfy the need for flexible worker participation, we consider a new FL paradigm called ``Anarchic Federated Learning'' (AFL) in this paper. In stark contrast to conventional FL models, each worker in AFL has the freedom to choose i) when to participate in FL, and ii) the number of local steps to perform in each round based on its current situation (e.g., battery level, communication channels, privacy concerns). However, such chaotic worker behaviors in AFL impose many new open questions in algorithm design. In particular, it remains unclear whether one could develop convergent AFL training algorithms, and if yes, under what conditions and how fast the achievable convergence speed is. Toward this end, we propose two Anarchic Federated Averaging (AFA) algorithms with two-sided learning rates for both cross-device and cross-silo settings, which are named AFA-CD and AFA-CS, respectively. Somewhat surprisingly, we show that, under mild anarchic assumptions, both AFL algorithms achieve the best known convergence rate as the state-of-the-art algorithms for conventional FL. Moreover, they retain the highly desirable linear speedup effect with respect of both the number of workers and local steps in the new AFL paradigm. We validate the proposed algorithms with extensive experiments on real-world datasets.},
  langid = {english},
  keywords = {asynchronous,federated,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\P6R4RYJ5\\Yang et al. - Anarchic Federated Learning.pdf}
}

@incollection{yangContinualLearningBayesian2021,
  title = {Continual {{Learning}} with {{Bayesian Model Based}} on a {{Fixed Pre-trained Feature Extractor}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2021},
  author = {Yang, Yang and Cui, Zhiying and Xu, Junjie and Zhong, Changhong and Wang, Ruixuan and Zheng, Wei-Shi},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  volume = {12905},
  pages = {397--406},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-87240-3_38},
  abstract = {Current deep learning models are characterised by catastrophic forgetting of old knowledge when learning new classes. This poses a challenge in intelligent diagnosis systems where initially only training data of a limited number of diseases are available. In this case, updating the intelligent system with data of new diseases would inevitably downgrade its performance on previously learned diseases. Inspired by the process of learning new knowledge in human brains, we propose a Bayesian generative model for continual learning built on a fixed pretrained feature extractor. In this model, knowledge of each old class can be compactly represented by a collection of statistical distributions, e.g. with Gaussian mixture models, and naturally kept from forgetting in continual learning. Experiments on two skin image sets showed that the proposed approach outperforms state-of-the-art approaches which even keep some images of old classes during continual learning of new classes.},
  isbn = {978-3-030-87239-7 978-3-030-87240-3},
  langid = {english},
  keywords = {bayesian,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4KHG264L\\Yang et al. - 2021 - Continual Learning with Bayesian Model Based on a .pdf}
}

@article{yangContinualLearningBayesian2022,
  title = {Continual {{Learning}} with {{Bayesian Model}} Based on a {{Fixed Pre-trained Feature Extractor}}},
  author = {Yang, Yang and Cui, Zhiying and Xu, Junjie and Zhong, Changhong and Zheng, Wei-Shi and Wang, Ruixuan},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.13349 [cs]},
  eprint = {2204.13349},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep learning has shown its human-level performance in various applications. However, current deep learning models are characterised by catastrophic forgetting of old knowledge when learning new classes. This poses a challenge particularly in intelligent diagnosis systems where initially only training data of a limited number of diseases are available. In this case, updating the intelligent system with data of new diseases would inevitably downgrade its performance on previously learned diseases. Inspired by the process of learning new knowledge in human brains, we propose a Bayesian generative model for continual learning built on a fixed pre-trained feature extractor. In this model, knowledge of each old class can be compactly represented by a collection of statistical distributions, e.g. with Gaussian mixture models, and naturally kept from forgetting in continual learning over time. Unlike existing class-incremental learning methods, the proposed approach is not sensitive to the continual learning process and can be additionally well applied to the data-incremental learning scenario. Experiments on multiple medical and natural image classification tasks showed that the proposed approach outperforms state-of-the-art approaches which even keep some images of old classes during continual learning of new classes.},
  archiveprefix = {arXiv},
  keywords = {bayesian,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,GMM,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WRC7WEC9\\2204.html}
}

@inproceedings{yangDivideConquerCompositional2022,
  title = {Divide and {{Conquer}}: {{Compositional Experts}} for {{Generalized Novel Class Discovery}}},
  shorttitle = {Divide and {{Conquer}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Yang, Muli and Zhu, Yuehua and Yu, Jiaping and Wu, Aming and Deng, Cheng},
  year = {2022},
  pages = {14268--14277},
  langid = {english},
  keywords = {Continual,exmodel,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DHJGGC3Z\\Yang_Divide_and_Conquer_Compositional_Experts_for_Generalized_Novel_Class_Discovery_CVPR_2022_p.html}
}

@article{yangFixedtimeSynchronizationCoupled2019,
  title = {Fixed-Time Synchronization of Coupled Memristor-Based Neural Networks with Time-Varying Delays},
  author = {Yang, Chao and Huang, Lihong and Cai, Zuowei},
  year = {2019},
  month = aug,
  journal = {Neural Networks},
  volume = {116},
  pages = {101--109},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.04.008},
  abstract = {This paper investigates the fixed-time synchronization of Memristor-based neural networks with time-delayed and coupled. In view of the retarded differential inclusions theory, drive\textendash response concept, the authors give some sufficient conditions to ensure the fixed-time synchronization issue of Memristor-based neural networks. Two novel state-feedback controllers and adaptive controller are designed such that the system can realize fixed-time complete synchronization by means of inequality technique and non-smooth analysis theory. It is worth to point out that, without desiring values of the initial conditions or under the linear growth condition of the controller, the settling time of fixed-time synchronization is estimated. Finally, an example is given to further illustrate the benefits of the proposed switched control approach.},
  langid = {english},
  keywords = {Coupled,Differential inclusion,fixed-time synchronization,Memristor-based neural networks},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\D5M5T4WC\\Yang et al. - 2019 - Fixed-time synchronization of coupled memristor-ba.pdf;C\:\\Users\\w-32\\Zotero\\storage\\RHTYSYN8\\S0893608019301066.html}
}

@article{yanPartInvariantModelMusic2018,
  title = {Part-{{Invariant Model}} for {{Music Generation}} and {{Harmonization}}},
  author = {Yan, Yujia and Lustig, Ethan and Vanderstel, Joseph and Duan, Zhiyao},
  year = {2018},
  journal = {19th International Society for Music Information Retrieval Conference},
  pages = {204--210},
  abstract = {Automatic music generation has been gaining more attention in recent years. Existing approaches, however, are mostly ad hoc to specific rhythmic structures or instrumen-tation layouts, and lack music-theoretic rigor in their evaluations. In this paper, we present a neural language (mu-sic) model that tries to model symbolic multi-part music. Our model is part-invariant, i.e., it can process/generate any part (voice) of a music score consisting of an arbitrary number of parts, using a single trained model. For better incorporating structural information of pitch spaces, we use a structured embedding matrix to encode multiple aspects of a pitch into a vector representation. The generation is performed by Gibbs Sampling. Meanwhile, our model directly generates note spellings to make outputs human-readable. We performed objective (grading) and subjective (listening) evaluations by recruiting music the-orists to compare the outputs of our algorithm with those of music students on the task of bassline harmonization (a traditional pedagogical task). Our experiment shows that errors of our algorithm and students are differently distributed, and the range of ratings for generated pieces overlaps with students' to varying extents for our three provided basslines. This experiment suggests some future research directions.},
  keywords = {music}
}

@inproceedings{yinDreamingDistillDataFree2020,
  ids = {yinDreamingDistillDatafree2020},
  title = {Dreaming to {{Distill}}: {{Data-Free Knowledge Transfer}} via {{DeepInversion}}},
  shorttitle = {Dreaming to {{Distill}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M. and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K. and Kautz, Jan},
  year = {2020},
  eprint = {1912.08795},
  eprinttype = {arxiv},
  pages = {8715--8724},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,data-distill,data-free,exmodel,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\N5GRBLKD\\Yin et al. - 2020 - Dreaming to Distill Data-free Knowledge Transfer .pdf}
}

@article{yinUnderstandingStraightThroughEstimator2018,
  title = {Understanding {{Straight-Through Estimator}} in {{Training Activation Quantized Neural Nets}}},
  author = {Yin, Penghang and Lyu, Jiancheng and Zhang, Shuai and Osher, Stanley and Qi, Yingyong and Xin, Jack},
  year = {2018},
  journal = {Iclr},
  pages = {1--30},
  abstract = {Training activation quantized neural networks involves minimizing a piecewise constant training loss whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in the backward pass, so that the "gradient" through the modified chain rule becomes non-trivial. Since this unusual "gradient" is certainly not the gradient of loss function, the following question arises: why searching in its negative direction minimizes the training loss? In this paper, we provide the theoretical justification of the concept of STE by answering this question. We consider the problem of learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. We shall refer to the unusual "gradient" given by the STE-modifed chain rule as coarse gradient. The choice of STE is not unique. We prove that if the STE is properly chosen, the expected coarse gradient correlates positively with the population gradient (not available for the training), and its negation is a descent direction for minimizing the population loss. We further show the associated coarse gradient descent algorithm converges to a critical point of the population loss minimization problem. Moreover, we show that a poor choice of STE may lead to instability of the training algorithm near certain local minima, which is also observed in our CIFAR-10 experiments.},
  keywords = {discrete-backprop,ICLR},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\HUAQ3UFA\\Yin et al. - 2018 - Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets.pdf}
}

@article{yogatamaMemoryArchitecturesRecurrent2018,
  title = {Memory {{Architectures}} in {{Recurrent Neural Network Language Models}}},
  author = {Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil and And, DeepMind},
  year = {2018},
  journal = {International Conference on Learning Representations},
  pages = {10--10},
  abstract = {We compare and analyze sequential, random access, and stack memory architec-tures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin \& Mikolov, 2015; Grefenstette et al., 2015) to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.},
  keywords = {MANN,nlp,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\27BY4SEF\\Yogatama et al. - 2018 - Memory Architectures in Recurrent Neural Network Language Models.pdf}
}

@article{yooKnowledgeExtractionNo,
  title = {Knowledge {{Extraction}} with {{No Observable Data}}},
  author = {Yoo, Jaemin and Cho, Minyong and Kim, Taebum and Kang, U},
  pages = {10},
  abstract = {Knowledge distillation is to transfer the knowledge of a large neural network into a smaller one and has been shown to be effective especially when the amount of training data is limited or the size of the student model is very small. To transfer the knowledge, it is essential to observe the data that have been used to train the network since its knowledge is concentrated on a narrow manifold rather than the whole input space. However, the data are not accessible in many cases due to the privacy or confidentiality issues in medical, industrial, and military domains. To the best of our knowledge, there has been no approach that distills the knowledge of a neural network when no data are observable. In this work, we propose KEGNET (Knowledge Extraction with Generative Networks), a novel approach to extract the knowledge of a trained deep neural network and to generate artificial data points that replace the missing training data in knowledge distillation. Experiments show that KEGNET outperforms all baselines for data-free knowledge distillation. We provide the source code of our paper in https://github.com/snudatalab/KegNet.},
  langid = {english},
  keywords = {data-free,exmodel},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4IEUXS3F\\Yoo et al. - Knowledge Extraction with No Observable Data.pdf}
}

@article{yoonFederatedContinualLearning2021,
  title = {Federated {{Continual Learning}} with {{Weighted Inter-client Transfer}}},
  author = {Yoon, Jaehong and Jeong, Wonyong and Lee, Giwoong and Yang, Eunho and Hwang, Sung Ju},
  year = {2021},
  month = jun,
  journal = {arXiv:2003.03196 [cs, stat]},
  eprint = {2003.03196},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {There has been a surge of interest in continual learning and federated learning, both of which are important in deep neural networks in real-world scenarios. Yet little research has been done regarding the scenario where each client learns on a sequence of tasks from a private local data stream. This problem of federated continual learning poses new challenges to continual learning, such as utilizing knowledge from other clients, while preventing interference from irrelevant knowledge. To resolve these issues, we propose a novel federated continual learning framework, Federated Weighted Inter-client Transfer (FedWeIT), which decomposes the network weights into global federated parameters and sparse task-specific parameters, and each client receives selective knowledge from other clients by taking a weighted combination of their task-specific parameters. FedWeIT minimizes interference between incompatible tasks, and also allows positive knowledge transfer across clients during learning. We validate our FedWeIT against existing federated learning and continual learning methods under varying degrees of task similarity across clients, and our model significantly outperforms them with a large reduction in the communication cost. Code is available at https://github.com/wyjeong/FedWeIT},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,continual,exmodel,federated,Statistics - Machine Learning},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\BZQBHEG9\\2003.03196.pdf;C\:\\Users\\w-32\\Zotero\\storage\\5EWNQUF5\\2003.html}
}

@inproceedings{yoonLifelongLearningWih2018,
  title = {Lifelong {{Learning}} Wih {{Dynamically Expandable Networks}}},
  booktitle = {{{ICLR}}},
  author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
  year = {2018},
  abstract = {We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network fine-tuned on all tasks obtained siginficantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the first place.},
  keywords = {architectural,continual,ICLR},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GHFFWYWI\\Yoon et al. - 2018 - Lifelong Learning wih Dynamically Expandable Networks(2).pdf}
}

@inproceedings{yoonOnlineCoresetSelection2021,
  title = {Online {{Coreset Selection}} for {{Rehearsal-based Continual Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yoon, Jaehong and Madaan, Divyam and Yang, Eunho and Hwang, Sung Ju},
  year = {2021},
  month = sep,
  abstract = {A dataset is a shred of crucial evidence to describe a task. However, each data point in the dataset does not have the same potential, as some of the data points can be more representative or...},
  langid = {english},
  keywords = {continual,coreset-selection,ocl,online,replay},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SM4PCZFQ\\forum.html}
}

@article{yosinskiUnderstandingNeuralNetworks,
  title = {Understanding {{Neural Networks Through Deep Visualization}}},
  author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  pages = {12},
  abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pretrained convnet with minimal setup.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CJANYGC8\\Yosinski et al. - Understanding Neural Networks Through Deep Visuali.pdf}
}

@article{yosinskiUnderstandingNeuralNetworksa,
  title = {Understanding {{Neural Networks Through Deep Visualization}}},
  author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  pages = {12},
  abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pretrained convnet with minimal setup.},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DUAGLUV7\\Yosinski et al. - Understanding Neural Networks Through Deep Visuali.pdf}
}

@article{youngRecentTrendsDeep2018,
  title = {Recent {{Trends}} in {{Deep Learning Based Natural Language Processing}}},
  author = {Young, T and Hazarika, Devamanyu and Poria, S and Cambria, E},
  year = {2018},
  journal = {IEEE Computational Intelligence Magazine},
  volume = {13},
  pages = {55--75},
  keywords = {nlp,review},
  annotation = {@article\{Young2018RecentTI,   title=\{Recent Trends in Deep Learning Based Natural Language Processing [Review Article]\},   author=\{Tom Young and Devamanyu Hazarika and Soujanya Poria and E. Cambria\},   journal=\{IEEE Computational Intelligence Magazine\},   year=\{2018\},   volume=\{13\},   pages=\{55-75\} \}}
}

@misc{yuanFlorenceNewFoundation2021,
  title = {Florence: {{A New Foundation Model}} for {{Computer Vision}}},
  shorttitle = {Florence},
  author = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Liu, Mengchen and Liu, Zicheng and Lu, Yumao and Shi, Yu and Wang, Lijuan and Wang, Jianfeng and Xiao, Bin and Xiao, Zhen and Yang, Jianwei and Zeng, Michael and Zhou, Luowei and Zhang, Pengchuan},
  year = {2021},
  month = nov,
  number = {arXiv:2111.11432},
  eprint = {2111.11432},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.11432},
  abstract = {Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.},
  archiveprefix = {arXiv},
  keywords = {notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2YQI4B9D\\2111.html}
}

@inproceedings{yunCutMixRegularizationStrategy2019a,
  ids = {CutMixRegularizationStrategy,yunCutMixRegularizationStrategy2019},
  title = {{{CutMix}}: {{Regularization Strategy}} to {{Train Strong Classifiers With Localizable Features}}},
  shorttitle = {{{CutMix}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  year = {2019},
  eprint = {1905.04899},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {6023--6032},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,exmodel,notag,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\CF3VKTKG\\[1905.04899] CutMix Regularization Strategy to Tr.pdf;C\:\\Users\\w-32\\Zotero\\storage\\QSPU3IIB\\1905.html;C\:\\Users\\w-32\\Zotero\\storage\\UEPG8WEU\\1905.html}
}

@inproceedings{yuReBalancingStrategyClassImbalanced2022,
  title = {A {{Re-Balancing Strategy}} for {{Class-Imbalanced Classification Based}} on {{Instance Difficulty}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Yu, Sihao and Guo, Jiafeng and Zhang, Ruqing and Fan, Yixing and Wang, Zizhen and Cheng, Xueqi},
  year = {2022},
  pages = {70--79},
  langid = {english},
  keywords = {class-imbalance},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\4NEP53R4\\Yu_A_Re-Balancing_Strategy_for_Class-Imbalanced_Classification_Based_on_Instance_Difficulty_CVP.html}
}

@inproceedings{yuSemanticDriftCompensation2020,
  title = {Semantic {{Drift Compensation}} for {{Class-Incremental Learning}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yu, Lu and Twardowski, Bartlomiej and Liu, Xialei and Herranz, Luis and Wang, Kai and Cheng, Yongmei and Jui, Shangling and {van de Weijer}, Joost},
  year = {2020},
  month = jun,
  pages = {6980--6989},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00701},
  abstract = {Class-incremental learning of deep networks sequentially increases the number of classes to be classified. During training, the network has only access to data of one task at a time, where each task contains several classes. In this setting, networks suffer from catastrophic forgetting which refers to the drastic drop in performance on previous tasks. The vast majority of methods have studied this scenario for classification networks, where for each new task the classification layer of the network must be augmented with additional weights to make room for the newly added classes.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  keywords = {continual,drift-compensation,metric-learning,notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WJ7W2JFX\\Yu et al. - 2020 - Semantic Drift Compensation for Class-Incremental .pdf}
}

@article{yuUnderstandingAutoencodersInformation2019,
  title = {Understanding Autoencoders with Information Theoretic Concepts},
  author = {Yu, Shujian and Pr{\'i}ncipe, Jos{\'e} C.},
  year = {2019},
  journal = {Neural Networks},
  volume = {117},
  pages = {104--123},
  doi = {10.1016/j.neunet.2019.05.003},
  abstract = {Despite their great success in practical applications, there is still a lack of theoretical and systematic methods to analyze deep neural networks. In this paper, we illustrate an advanced information theoretic methodology to understand the dynamics of learning and the design of autoencoders, a special type of deep learning architectures that resembles a communication channel. By generalizing the information plane to any cost function, and inspecting the roles and dynamics of different layers using layer-wise information quantities, we emphasize the role that mutual information plays in quantifying learning from data. We further suggest and also experimentally validate, for mean square error training, three fundamental properties regarding the layer-wise flow of information and intrinsic dimensionality of the bottleneck layer, using respectively the data processing inequality and the identification of a bifurcation point in the information plane that is controlled by the given data. Our observations have direct impact on the optimal design of autoencoders, the design of alternative feedforward training methods, and even in the problem of generalization.},
  keywords = {autoencoders,info-theory,Intrinsic dimensionality},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GNB565SC\\Yu, Prncipe - 2019 - Understanding autoencoders with information theoretic concepts.pdf}
}

@article{zaadnoordijkLessonsInfantLearning2022,
  title = {Lessons from Infant Learning for Unsupervised Machine Learning},
  author = {Zaadnoordijk, Lorijn and Besold, Tarek R. and Cusack, Rhodri},
  year = {2022},
  month = jun,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {6},
  pages = {510--520},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00488-2},
  abstract = {The desire to reduce the dependence on curated, labeled datasets and to leverage the vast quantities of unlabeled data has triggered renewed interest in unsupervised (or self-supervised) learning algorithms. Despite improved performance due to approaches such as the identification of disentangled latent representations, contrastive learning and clustering optimizations, unsupervised machine learning still falls short of its hypothesized potential as a breakthrough paradigm enabling generally intelligent systems. Inspiration from cognitive (neuro)science has been based mostly on adult learners with access to labels and a vast amount of prior knowledge. To push unsupervised machine learning forward, we argue that developmental science of infant cognition might hold the key to unlocking the next generation of unsupervised learning approaches. We identify three crucial factors enabling infants' quality and speed of learning: (1) babies' information processing is guided and constrained; (2) babies are learning from diverse, multimodal inputs; and (3) babies' input is shaped by development and active learning. We assess the extent to which these insights from infant learning have already been exploited in machine learning, examine how closely these implementations resemble the core insights, and propose how further adoption of these factors can give rise to previously unseen performance levels in unsupervised learning.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {continual,neuroscience},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FMXWJCT5\\s42256-022-00488-2.html}
}

@article{zancaVisualAttentionDriven2018,
  title = {Visual {{Attention}} Driven by {{Convolutional Features}}},
  author = {Zanca, Dario and Gori, Marco},
  year = {2018},
  month = jul,
  abstract = {The understanding of where humans look in a scene is a problem of great interest in visual perception and computer vision. When eye-tracking devices are not a viable option, models of human attention can be used to predict fixations. In this paper we give two contribution. First, we show a model of visual attention that is simply based on deep convolutional neural networks trained for object classification tasks. A method for visualizing saliency maps is defined which is evaluated in a saliency prediction task. Second, we integrate the information of these maps with a bottom-up differential model of eye-movements to simulate visual attention scanpaths. Results on saliency prediction and scores of similarity with human scanpaths demonstrate the effectiveness of this model.},
  keywords = {attention,CNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6JHCQ6SH\\Zanca, Gori - 2018 - Visual Attention driven by Convolutional Features(3).pdf}
}

@article{zarconeSalienceAttentionSurprisalbased2016,
  title = {Salience and Attention in Surprisal-Based Accounts of Language Processing},
  author = {Zarcone, Alessandra and {van Schijndel}, Marten and Vogels, Jorrig and Demberg, Vera},
  year = {2016},
  journal = {Frontiers in Psychology},
  volume = {7},
  number = {JUN},
  pages = {1--17},
  doi = {10.3389/fpsyg.2016.00844},
  abstract = {The notion of salience has been singled out as the explanatory factor for a diverse range of linguistic phenomena. In particular, perceptual salience (e.g., visual salience of objects in the world, acoustic prominence of linguistic sounds) and semantic-pragmatic salience (e.g., prominence of recently mentioned or topical referents) have been shown to influence language comprehension and production. A different line of research has sought to account for behavioral correlates of cognitive load during comprehension as well as for certain patterns in language usage using information-theoretic notions, such as surprisal. Surprisal and salience both affect language processing at different levels, but the relationship between the two has not been adequately elucidated, and the question of whether salience can be reduced to surprisal / predictability is still open. Our review identifies two main challenges in addressing this question: terminological inconsistency and lack of integration between high and low levels of representations in salience-based accounts and surprisal-based accounts. We capitalize upon work in visual cognition in order to orient ourselves in surveying the different facets of the notion of salience in linguistics and their relation with models of surprisal. We find that work on salience highlights aspects of linguistic communication that models of surprisal tend to overlook, namely the role of attention and relevance to current goals, and we argue that the Predictive Coding framework provides a unified view which can account for the role played by attention and predictability at different levels of processing and which can clarify the interplay between low and high levels of processes and between predictability-driven expectation and attention-driven focus.},
  keywords = {attention,nlp,Surprisal}
}

@article{zeilerADADELTAAdaptiveLearning2012,
  title = {{{ADADELTA}}: An Adaptive Learning Rate Method},
  author = {Zeiler, Matthew D},
  year = {2012},
  journal = {arXiv preprint arXiv:1212.5701},
  eprint = {1212.5701},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {learning}
}

@article{zenkeContinualLearningSynaptic2017,
  title = {Continual {{Learning Through Synaptic Intelligence}}},
  author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.04200 [cs, q-bio, stat]},
  eprint = {1703.04200},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.},
  archiveprefix = {arXiv},
  keywords = {continual,ICML,param-importance},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3AMZ465K\\Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf;C\:\\Users\\w-32\\Zotero\\storage\\QM757FP6\\1703.html}
}

@article{zentnerSimpleApproachContinual2021,
  title = {A {{Simple Approach}} to {{Continual Learning}} by {{Transferring Skill Parameters}}},
  author = {Zentner, K. R. and Julian, Ryan and Puri, Ujjwal and Zhang, Yulun and Sukhatme, Gaurav S.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.10255 [cs]},
  eprint = {2110.10255},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In order to be effective general purpose machines in real world environments, robots not only will need to adapt their existing manipulation skills to new circumstances, they will need to acquire entirely new skills on-the-fly. A great promise of continual learning is to endow robots with this ability, by using their accumulated knowledge and experience from prior skills. We take a fresh look at this problem, by considering a setting in which the robot is limited to storing that knowledge and experience only in the form of learned skill policies. We show that storing skill policies, careful pre-training, and appropriately choosing when to transfer those skill policies is sufficient to build a continual learner in the context of robotic manipulation. We analyze which conditions are needed to transfer skills in the challenging Meta-World simulation benchmark. Using this analysis, we introduce a pair-wise metric relating skills that allows us to predict the effectiveness of skill transfer between tasks, and use it to reduce the problem of continual learning to curriculum selection. Given an appropriate curriculum, we show how to continually acquire robotic manipulation skills without forgetting, and using far fewer samples than needed to train them from scratch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,continual,exmodel,neural-skill},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Zentner et al_2021_A Simple Approach to Continual Learning by Transferring Skill Parameters.pdf;C\:\\Users\\w-32\\Zotero\\storage\\L3KXJS7Z\\2110.html}
}

@inproceedings{zhang2018cascade,
  title = {Cascade and Parallel Convolutional Recurrent Neural Networks on {{EEG-based}} Intention Recognition for Brain Computer Interface},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Zhang, Dalin and Yao, Lina and Zhang, Xiang and Wang, Sen and Chen, Weitong and Boots, Robert and Benatallah, Boualem},
  year = {2018},
  volume = {32},
  keywords = {human_state_monitoring}
}

@article{zhangAreAllLayers2019,
  title = {Are {{All Layers Created Equal}}?},
  author = {Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  year = {2019},
  month = may,
  journal = {arXiv:1902.01996 [cs, stat]},
  eprint = {1902.01996},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Understanding deep neural networks has been a major research objective in recent years with notable theoretical progress. A focal point of those studies stems from the success of excessively large networks which defy the classical wisdom of uniform convergence and learnability. We study empirically the layer-wise functional structure of overparameterized deep models. We provide evidence for the heterogeneous characteristic of layers. To do so, we introduce the notion of robustness to post-training re-initialization and re-randomization. We show that the layers can be categorized as either ``ambient'' or ``critical''. Resetting the ambient layers to their initial values has no negative consequence, and in many cases they barely change throughout training. On the contrary, resetting the critical layers completely destroys the predictor and the performance drops to chanceh. Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization of deep models, and flatness or robustness analysis of the models needs to respect the network architectures.},
  archiveprefix = {arXiv},
  keywords = {notag},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\SDHJA9YL\\1902.html}
}

@inproceedings{zhangClassincrementalLearningDeep2020a,
  ids = {zhangClassincrementalLearningDeep2020},
  title = {Class-Incremental {{Learning}} via {{Deep Model Consolidation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Zhang, Junting and Zhang, Jie and Ghosh, Shalini and Li, Dawei and Tasci, Serafettin and Heck, Larry and Zhang, Heming and Kuo, C.-C. Jay},
  year = {2020},
  eprint = {1903.07864},
  eprinttype = {arxiv},
  pages = {1131--1140},
  archiveprefix = {arXiv},
  keywords = {cl-regularization,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual,data-free,exmodel},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Zhang et al_2020_Class-incremental Learning via Deep Model Consolidation.pdf;C\:\\Users\\w-32\\Zotero\\storage\\RPQ4VPYF\\1903.html}
}

@article{zhangDeepLearningElastic2014,
  title = {Deep Learning with {{Elastic Averaging SGD}}},
  author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
  year = {2014},
  pages = {1--24},
  abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6CU4PZFM\\Zhang, Choromanska, LeCun - 2014 - Deep learning with Elastic Averaging SGD.pdf}
}

@article{zhangDiveDeepLearning,
  title = {Dive into {{Deep Learning}}},
  author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  pages = {995},
  langid = {english},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\LLR9YSPF\\Zhang et al. - Dive into Deep Learning.pdf}
}

@misc{zhangFeatureForgettingContinual2022,
  title = {Feature {{Forgetting}} in {{Continual Representation Learning}}},
  author = {Zhang, Xiao and Dou, Dejing and Wu, Ji},
  year = {2022},
  month = may,
  number = {arXiv:2205.13359},
  eprint = {2205.13359},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.13359},
  abstract = {In continual and lifelong learning, good representation learning can help increase performance and reduce sample complexity when learning new tasks. There is evidence that representations do not suffer from "catastrophic forgetting" even in plain continual learning, but little further fact is known about its characteristics. In this paper, we aim to gain more understanding about representation learning in continual learning, especially on the feature forgetting problem. We devise a protocol for evaluating representation in continual learning, and then use it to present an overview of the basic trends of continual representation learning, showing its consistent deficiency and potential issues. To study the feature forgetting problem, we create a synthetic dataset to identify and visualize the prevalence of feature forgetting in neural networks. Finally, we propose a simple technique using gating adapters to mitigate feature forgetting. We conclude by discussing that improving representation learning benefits both old and new tasks in continual learning.},
  archiveprefix = {arXiv},
  keywords = {cl-representation,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TS9UB4SF\\Zhang et al_2022_Feature Forgetting in Continual Representation Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\W4RG7PUX\\2205.html}
}

@inproceedings{zhangFineTuningGlobalModel2022,
  title = {Fine-{{Tuning Global Model}} via {{Data-Free Knowledge Distillation}} for {{Non-IID Federated Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Lin and Shen, Li and Ding, Liang and Tao, Dacheng and Duan, Ling-Yu},
  year = {2022},
  pages = {10174--10183},
  langid = {english},
  keywords = {Continual,data-free,federated,knowledge-distillation},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6FGWBMEG\\Zhang_Fine-Tuning_Global_Model_via_Data-Free_Knowledge_Distillation_for_Non-IID_Federated_CVPR_.html}
}

@article{zhangFixupInitialization2019,
  title = {Fixup {{Initialization}}},
  author = {Zhang, Hongyi and Dauphin, Yann N and Brain, Google},
  year = {2019},
  pages = {1--16},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\WSE4E7LP\\Zhang, Dauphin, Brain - 2019 - Fixup Initialization.pdf}
}

@article{zhangFunctionalMechanismRegression2012,
  title = {Functional Mechanism: Regression Analysis under Differential Privacy},
  shorttitle = {Functional Mechanism},
  author = {Zhang, Jun and Zhang, Zhenjie and Xiao, Xiaokui and Yang, Yin and Winslett, Marianne},
  year = {2012},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {5},
  number = {11},
  pages = {1364--1375},
  issn = {2150-8097},
  doi = {10.14778/2350229.2350253},
  langid = {english},
  keywords = {diff-privacy},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\QPN2HVT8\\Zhang et al. - 2012 - Functional mechanism regression analysis under di.pdf}
}

@article{zhangFunctionalMechanismRegression2012a,
  title = {Functional {{Mechanism}}: {{Regression Analysis}} under {{Differential Privacy}}},
  shorttitle = {Functional {{Mechanism}}},
  author = {Zhang, Jun and Zhang, Zhenjie and Xiao, Xiaokui and Yang, Yin and Winslett, Marianne},
  year = {2012},
  month = aug,
  journal = {arXiv:1208.0219 [cs]},
  eprint = {1208.0219},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {\textbackslash epsilon-differential privacy is the state-of-the-art model for releasing sensitive information while protecting privacy. Numerous methods have been proposed to enforce epsilon-differential privacy in various analytical tasks, e.g., regression analysis. Existing solutions for regression analysis, however, are either limited to non-standard types of regression or unable to produce accurate regression results. Motivated by this, we propose the Functional Mechanism, a differentially private method designed for a large class of optimization-based analyses. The main idea is to enforce epsilon-differential privacy by perturbing the objective function of the optimization problem, rather than its results. As case studies, we apply the functional mechanism to address two most widely used regression models, namely, linear regression and logistic regression. Both theoretical analysis and thorough experimental evaluations show that the functional mechanism is highly effective and efficient, and it significantly outperforms existing solutions.},
  archiveprefix = {arXiv},
  keywords = {diff-privacy,project-dp-rnn-galli,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PHRFRWQP\\1208.html}
}

@article{zhangONESHOTPRUNINGRECURRENT2020,
  title = {{{ONE-SHOT PRUNING OF RECURRENT NEURAL NET- WORKS BY JACOBIAN SPECTRUM EVALUATION}}},
  author = {Zhang, Matthew Shunshi and Stadie, Bradly C},
  year = {2020},
  pages = {12},
  abstract = {Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 \% sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext.},
  langid = {english},
  keywords = {pruning,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\AQS3VZUB\\Zhang and Stadie - 2020 - ONE-SHOT PRUNING OF RECURRENT NEURAL NET- WORKS BY.pdf}
}

@article{zhangPrediction8stateProtein2018,
  title = {Prediction of 8-State Protein Secondary Structures by a Novel Deep Learning Architecture},
  author = {Zhang, Buzhong and Li, Jinyan and L{\"u}, Qiang},
  year = {2018},
  journal = {BMC Bioinformatics},
  volume = {19},
  number = {1},
  pages = {1--13},
  doi = {10.1186/s12859-018-2280-5},
  abstract = {Protein secondary structure can be regarded as an information bridge that links the primary sequence and tertiary structure. Accurate 8-state secondary structure prediction can significantly give more precise and high resolution on structure-based properties analysis. We present a novel deep learning architecture which exploits an integrative synergy of prediction by a convolutional neural network, residual network, and bidirectional recurrent neural network to improve the performance of protein secondary structure prediction. A local block comprised of convolutional filters and original input is designed for capturing local sequence features. The subsequent bidirectional recurrent neural network consisting of gated recurrent units can capture global context features. Furthermore, the residual network can improve the information flow between the hidden layers and the cascaded recurrent neural network. Our proposed deep network achieved 71.4\% accuracy on the benchmark CB513 dataset for the 8-state prediction; and the ensemble learning by our model achieved 74\% accuracy. Our model generalization capability is also evaluated on other three independent datasets CASP10, CASP11 and CASP12 for both 8- and 3-state prediction. These prediction performances are superior to the state-of-the-art methods. Our experiment demonstrates that it is a valuable method for predicting protein secondary structure, and capturing local and global features concurrently is very useful in deep learning.},
  keywords = {BIOINF},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\GYFY4WFD\\Zhang, Li, L - 2018 - Prediction of 8-state protein secondary structures by a novel deep learning architecture(2).pdf}
}

@inproceedings{zhangStabilizingGradientsDeep2018,
  title = {Stabilizing {{Gradients}} for {{Deep Neural Networks}} via {{Efficient SVD Parameterization}}},
  booktitle = {{{ICML}}},
  author = {Zhang, Jiong and Lei, Qi and Dhillon, Inderjit},
  year = {2018},
  month = jul,
  pages = {5801--5809},
  keywords = {ICML,orthogonal-nn,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\M2LE7RX2\\Zhang, Lei, Dhillon - 2018 - Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization(2).pdf}
}

@article{zhangUnderstandingDeepLearning2016,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2016},
  month = nov,
  journal = {arXiv preprint arXiv:1611.03530},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  archiveprefix = {arXiv},
  keywords = {generalization},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\9RUP97Z8\\Zhang et al. - 2016 - Understanding deep learning requires rethinking generalization(3).pdf}
}

@article{zhanHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Zhan, Yusen and Ammar, Haitham Bou and Taylor, Matthew E.},
  year = {2015},
  journal = {Nature},
  volume = {2016-Janua},
  number = {7540},
  pages = {2315--2321},
  issn = {1476-4687 (Electronic) 0028-0836 (Linking)},
  doi = {10.1038/nature14236},
  abstract = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment.Touse reinforcement learning successfully insituations approaching real-world complexity, however, agents are confronted with a difficult task: theymust derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to newsituations.Remarkably, humans and other animals seemto solve this problemthrough a harmonious combination of reinforcement learningandhierarchical sensoryprocessing systems4,5, the former evidenced by a wealth of neural data revealingnotable parallels betweenthe phasic signals emitted bydopaminergic neurons and temporal difference reinforcement learning algorithms3.While reinforcement learning agents have achieved some successes in a variety of domains6\textendash 8, their applicability has previously been limited to domains inwhich useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9\textendash 11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly fromhigh-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  keywords = {RL},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\DZKTC63M\\Zhan, Ammar, Taylor - 2015 - Human-level control through deep reinforcement learning.pdf}
}

@misc{zhaoDatasetCondensationDistribution2022,
  title = {Dataset {{Condensation}} with {{Distribution Matching}}},
  author = {Zhao, Bo and Bilen, Hakan},
  year = {2022},
  month = apr,
  number = {arXiv:2110.04181},
  eprint = {2110.04181},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.04181},
  abstract = {Computational cost of training state-of-the-art deep models in many learning problems is rapidly increasing due to more sophisticated models and larger datasets. A recent promising direction for reducing training cost is dataset condensation that aims to replace the original large training set with a significantly smaller learned synthetic set while preserving the original information. While training deep models on the small set of condensed images can be extremely fast, their synthesis remains computationally expensive due to the complex bi-level optimization and second-order derivative computation. In this work, we propose a simple yet effective method that synthesizes condensed images by matching feature distributions of the synthetic and original training images in many sampled embedding spaces. Our method significantly reduces the synthesis cost while achieving comparable or better performance. Thanks to its efficiency, we apply our method to more realistic and larger datasets with sophisticated neural architectures and obtain a significant performance boost. We also show promising practical benefits of our method in continual learning and neural architecture search.},
  archiveprefix = {arXiv},
  keywords = {Continual,meta-learning,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ICMDA94Y\\2110.html}
}

@inproceedings{zhaoDatasetCondensationGradient2020,
  title = {Dataset {{Condensation}} with {{Gradient Matching}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  year = {2020},
  month = sep,
  abstract = {As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing them and training models on them becomes more expensive. This paper proposes a training set...},
  langid = {english},
  keywords = {continual,data-distill,dataset condensation,meta-learn},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\FZRUPW5K\\Zhao et al_2020_Dataset Condensation with Gradient Matching.pdf;C\:\\Users\\w-32\\Zotero\\storage\\L3F8JZYP\\forum.html}
}

@inproceedings{zhaoDecoupledKnowledgeDistillation2022,
  title = {Decoupled {{Knowledge Distillation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
  year = {2022},
  pages = {11953--11962},
  langid = {english},
  keywords = {knowledge-distillation,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PSATPT95\\Zhao_Decoupled_Knowledge_Distillation_CVPR_2022_paper.html}
}

@article{zhaoJointPrincipalComponent2020,
  title = {Joint {{Principal Component}} and {{Discriminant Analysis}} for {{Dimensionality Reduction}}},
  author = {Zhao, Xiaowei and Guo, Jun and Nie, Feiping and Chen, Ling and Li, Zhihui and Zhang, Huaxiang},
  year = {2020},
  month = feb,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {2},
  pages = {433--444},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2904701},
  abstract = {Linear discriminant analysis (LDA) is the most widely used supervised dimensionality reduction approach. After removing the null space of the total scatter matrix \textsubscript{t} via principal component analysis (PCA), the LDA algorithm can avoid the small sample size problem. Most existing supervised dimensionality reduction methods extract the principal component of data first, and then conduct LDA on it. However, ``most variance'' is very often the most important, but not always in PCA. Thus, this two-step strategy may not be able to obtain the most discriminant information for classification tasks. Different from traditional approaches which conduct PCA and LDA in sequence, we propose a novel method referred to as joint principal component and discriminant analysis (JPCDA) for dimensionality reduction. Using this method, we are able to not only avoid the small sample size problem but also extract discriminant information for classification tasks. An iterative optimization algorithm is proposed to solve the method. To validate the efficacy of the proposed method, we perform extensive experiments on several benchmark data sets in comparison with some state-of-the-art dimensionality reduction methods. A large number of experimental results illustrate that the proposed method has quite promising classification performance.},
  keywords = {lda,linear,PCA},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\3PZFM7EZ\\Zhao et al. - 2020 - Joint Principal Component and Discriminant Analysi.pdf;C\:\\Users\\w-32\\Zotero\\storage\\A99XTPH7\\8718522.html}
}

@article{zhaoRNNLSTMHave2020,
  title = {Do {{RNN}} and {{LSTM}} Have {{Long Memory}}?},
  author = {Zhao, Jingyu and Huang, Feiqing and Lv, Jia and Duan, Yanjie and Qin, Zhen and Li, Guodong and Tian, Guangjian},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.03860 [cs, stat]},
  eprint = {2006.03860},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The LSTM network was proposed to overcome the difficulty in learning long-term dependence, and has made significant advancements in applications. With its success and drawbacks in mind, this paper raises the question - do RNN and LSTM have long memory? We answer it partially by proving that RNN and LSTM do not have long memory from a statistical perspective. A new definition for long memory networks is further introduced, and it requires the model weights to decay at a polynomial rate. To verify our theory, we convert RNN and LSTM into long memory networks by making a minimal modification, and their superiority is illustrated in modeling long-term dependence of various datasets.},
  archiveprefix = {arXiv},
  keywords = {LSTM,Memory,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\ETU8SA7J\\2006.html}
}

@article{zheImproveL2normalizedSoftmax2019,
  title = {Improve {{L2-normalized Softmax}} with {{Exponential Moving Average}}},
  author = {Zhe, Xuefei},
  year = {2019},
  number = {July},
  pages = {14--19},
  issn = {9781728120096},
  keywords = {softmax},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\XHZGNM8Z\\Zhe - 2019 - Improve L2-normalized Softmax with Exponential Moving Average(2).pdf}
}

@article{zhengRFORCERobustLearning2020,
  title = {R-{{FORCE}}: {{Robust Learning}} for {{Random Recurrent Neural Networks}}},
  shorttitle = {R-{{FORCE}}},
  author = {Zheng, Yang and Shlizerman, Eli},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.11660 [cs, q-bio]},
  eprint = {2003.11660},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Random Recurrent Neural Networks (RRNN) are the simplest recurrent networks to model and extract features from sequential data. The simplicity however comes with a price; RRNN are known to be susceptible to diminishing/exploding gradient problem when trained with gradient-descent based optimization. To enhance robustness of RRNN, alternative training approaches have been proposed. Specifically, FORCE learning approach proposed a recursive least squares alternative to train RRNN and was shown to be applicable even for the challenging task of target-learning, where the network is tasked with generating dynamic patterns with no guiding input. While FORCE training indicates that solving target-learning is possible, it appears to be effective only in a specific regime of network dynamics (edge-of-chaos). We thereby investigate whether initialization of RRNN connectivity according to a tailored distribution can guarantee robust FORCE learning. We are able to generate such distribution by inference of four generating principles constraining the spectrum of the network Jacobian to remain in stability region. This initialization along with FORCE learning provides a robust training method, i.e., Robust-FORCE (R-FORCE). We validate R-FORCE performance on various target functions for a wide range of network configurations and compare with alternative methods. Our experiments indicate that R-FORCE facilitates significantly more stable and accurate target-learning for a wide class of RRNN. Such stability becomes critical in modeling multi-dimensional sequences as we demonstrate on modeling time-series of human body joints during physical movements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,optimization,Quantitative Biology - Neurons and Cognition,RNN,training},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PZ2RR8T8\\Zheng and Shlizerman - 2020 - R-FORCE Robust Learning for Random Recurrent Neur.pdf;C\:\\Users\\w-32\\Zotero\\storage\\DB3QI4C5\\2003.html}
}

@article{zhongDiscriminativeDistillationReduce2021,
  title = {Discriminative {{Distillation}} to {{Reduce Class Confusion}} in {{Continual Learning}}},
  author = {Zhong, Changhong and Cui, Zhiying and Wang, Ruixuan and Zheng, Wei-Shi},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.05187 [cs]},
  eprint = {2108.05187},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Successful continual learning of new knowledge would enable intelligent systems to recognize more and more classes of objects. However, current intelligent systems often fail to correctly recognize previously learned classes of objects when updated to learn new classes. It is widely believed that such downgraded performance is solely due to the catastrophic forgetting of previously learned knowledge. In this study, we argue that the class confusion phenomena may also play a role in downgrading the classification performance during continual learning, i.e., the high similarity between new classes and any previously learned classes would also cause the classifier to make mistakes in recognizing these old classes, even if the knowledge of these old classes is not forgotten. To alleviate the class confusion issue, we propose a discriminative distillation strategy to help the classify well learn the discriminative features between confusing classes during continual learning. Experiments on multiple natural image classification tasks support that the proposed distillation strategy, when combined with existing methods, is effective in further improving continual learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,continual,exmodel,knowledge distillation,notag,toread},
  file = {C\:\\Users\\w-32\\OneDrive - University of Pisa\\zotero\\zotfiles\\Zhong et al_2021_Discriminative Distillation to Reduce Class Confusion in Continual Learning.pdf;C\:\\Users\\w-32\\Zotero\\storage\\94FUYCH4\\2108.html}
}

@misc{zhouDatasetDistillationUsing2022a,
  title = {Dataset {{Distillation}} Using {{Neural Feature Regression}}},
  author = {Zhou, Yongchao and Nezhadarya, Ehsan and Ba, Jimmy},
  year = {2022},
  month = jun,
  number = {arXiv:2206.00719},
  eprint = {2206.00719},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2206.00719},
  abstract = {Dataset distillation aims to learn a small synthetic dataset that preserves most of the information from the original dataset. Dataset distillation can be formulated as a bi-level meta-learning problem where the outer loop optimizes the meta-dataset and the inner loop trains a model on the distilled data. Meta-gradient computation is one of the key challenges in this formulation, as differentiating through the inner loop learning procedure introduces significant computation and memory costs. In this paper, we address these challenges using neural Feature Regression with Pooling (FRePo), achieving the state-of-the-art performance with an order of magnitude less memory requirement and two orders of magnitude faster training than previous methods. The proposed algorithm is analogous to truncated backpropagation through time with a pool of models to alleviate various types of overfitting in dataset distillation. FRePo significantly outperforms the previous methods on CIFAR100, Tiny ImageNet, and ImageNet-1K. Furthermore, we show that high-quality distilled data can greatly improve various downstream applications, such as continual learning and membership inference defense.},
  archiveprefix = {arXiv},
  keywords = {cl-replay,continual,meta-learning,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\D4NITY4W\\2206.html}
}

@misc{zhouLearnwareSmallModels2022,
  title = {Learnware: {{Small Models Do Big}}},
  shorttitle = {Learnware},
  author = {Zhou, Zhi-Hua and Tan, Zhi-Hao},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03647},
  eprint = {2210.03647},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03647},
  abstract = {There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identified to reuse according to the requirement of future users who know nothing about the model in advance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual,model-patching,skill-marketplace},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TIVFYSJY\\2210.html}
}

@article{zhouOfflineDistillationRobot2022,
  title = {Offline {{Distillation}} for {{Robot Lifelong Learning}} with {{Imbalanced Experience}}},
  author = {Zhou, Wenxuan and Bohez, Steven and Humplik, Jan and Abdolmaleki, Abbas and Rao, Dushyant and Wulfmeier, Markus and Haarnoja, Tuomas and Heess, Nicolas},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.05893 [cs]},
  eprint = {2204.05893},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Robots will experience non-stationary environment dynamics throughout their lifetime: the robot dynamics can change due to wear and tear, or its surroundings may change over time. Eventually, the robots should perform well in all of the environment variations it has encountered. At the same time, it should still be able to learn fast in a new environment. We investigate two challenges in such a lifelong learning setting: first, existing off-policy algorithms struggle with the trade-off between being conservative to maintain good performance in the old environment and learning efficiently in the new environment. We propose the Offline Distillation Pipeline to break this trade-off by separating the training procedure into interleaved phases of online interaction and offline distillation. Second, training with the combined datasets from multiple environments across the lifetime might create a significant performance drop compared to training on the datasets individually. Our hypothesis is that both the imbalanced quality and size of the datasets exacerbate the extrapolation error of the Q-function during offline training over the "weaker" dataset. We propose a simple fix to the issue by keeping the policy closer to the dataset during the distillation phase. In the experiments, we demonstrate these challenges and the proposed solutions with a simulated bipedal robot walking task across various environment changes. We show that the Offline Distillation Pipeline achieves better performance across all the encountered environments without affecting data collection. We also provide a comprehensive empirical study to support our hypothesis on the data imbalance issue.},
  archiveprefix = {arXiv},
  keywords = {continual,exmodel,knowledge distillation,replay,RL},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\TEFQ9479\\Zhou et al. - 2022 - Offline Distillation for Robot Lifelong Learning w.pdf;C\:\\Users\\w-32\\Zotero\\storage\\4F8C7WGI\\2204.html}
}

@article{zhouOnlineIncrementalFeature2012,
  title = {Online {{Incremental Feature Learning}} with {{Denoising Autoencoders}}},
  author = {Zhou, Guanyu and Sohn, Kihyuk and Lee, Honglak},
  year = {2012},
  journal = {AISTATS},
  volume = {60},
  number = {4},
  pages = {533--535},
  issn = {0035-9203 (Print)\textbackslash r0035-9203 (Linking)},
  doi = {10.1016/0035-9203(66)90279-3},
  abstract = {While determining model complexity is an important problem in machine learning, many feature learning algorithms rely on cross-validation to choose an optimal num- ber of features, which is usually challenging for online learning from a massive stream of data. In this paper, we propose an incremen- tal feature learning algorithm to determine the optimal model complexity for large-scale, online datasets based on the denoising au- toencoder. This algorithm is composed of two processes: adding features and merging features. Specifically, it adds new features to minimize the objective function's resid- ual and merges similar features to obtain a compact feature representation and prevent over-fitting. Our experiments show that the proposed model quickly converges to the op- timal number of features in a large-scale on- line setting. In classification tasks, our model outperforms the (non-incremental) denoising autoencoder, and deep networks constructed from our algorithm perform favorably com- pared to deep belief networks and stacked de- noising autoencoders. Further, the algorithm is effective in recognizing new patterns when the data distribution changes over time in the massive online data stream.},
  keywords = {autoencoders,continual},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\B458KGJP\\Zhou, Sohn, Lee - 2012 - Online Incremental Feature Learning with Denoising Autoencoders(2).pdf}
}

@inproceedings{zhuDataFreeKnowledgeDistillation2021,
  ids = {zhuDataFreeKnowledgeDistillation},
  title = {Data-{{Free Knowledge Distillation}} for {{Heterogeneous Federated Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
  year = {2021},
  month = jul,
  pages = {12878--12889},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Federated Learning (FL) is a decentralized machine-learning paradigm, in which a global server iteratively averages the model parameters of local users without accessing their data. User heterogeneity has imposed significant challenges to FL, which can incur drifted global models that are slow to converge. Knowledge Distillation has recently emerged to tackle this issue, by refining the server model using aggregated knowledge from heterogeneous users, other than directly averaging their model parameters. This approach, however, depends on a proxy dataset, making it impractical unless such a prerequisite is satisfied. Moreover, the ensemble knowledge is not fully utilized to guide local model learning, which may in turn affect the quality of the aggregated model. Inspired by the prior art, we propose a data-free knowledge distillation approach to address heterogeneous FL, where the server learns a lightweight generator to ensemble user information in a data-free manner, which is then broadcasted to users, regulating local training using the learned knowledge as an inductive bias. Empirical studies powered by theoretical implications show that our approach facilitates FL with better generalization performance using fewer communication rounds, compared with the state-of-the-art.},
  langid = {english},
  keywords = {continual,federated},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\PGULF3XX\\zhu21b.pdf}
}

@article{zhuDeepLeakageGradients2019,
  title = {Deep {{Leakage}} from {{Gradients}}},
  author = {Zhu, Ligeng and Liu, Zhijian and Han, Song},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  langid = {english},
  keywords = {Privacy Breach},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6BLHC4G6\\Zhu et al_2019_Deep Leakage from Gradients.pdf;C\:\\Users\\w-32\\Zotero\\storage\\M3JBFJBA\\60a6c4002cc7b29142def8871531281a-Abstract.html}
}

@inproceedings{zhuSelfSustainingRepresentationExpansion2022,
  title = {Self-{{Sustaining Representation Expansion}} for {{Non-Exemplar Class-Incremental Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhu, Kai and Zhai, Wei and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun},
  year = {2022},
  pages = {9296--9305},
  langid = {english},
  keywords = {Continual,knowledge-distillation,prototype,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\6NGFGBKS\\Zhu_Self-Sustaining_Representation_Expansion_for_Non-Exemplar_Class-Incremental_Learning_CVPR_2.html}
}

@inproceedings{zhuYouGetWhat2021,
  title = {You {{Get What You Sow}}: {{High Fidelity Image Synthesis}} with a {{Single Pretrained Network}}},
  shorttitle = {You {{Get What You Sow}}},
  booktitle = {Twenty-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhu, Kefeng and Tong, Peilin and Kan, Hongwei and Li, Rengang},
  year = {2021},
  month = aug,
  volume = {3},
  pages = {3477--3483},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2021/479},
  abstract = {Electronic proceedings of IJCAI 2021},
  langid = {english},
  keywords = {data-distill,data-free,model-inversion,toread},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\B8TL7FKN\\479.html}
}

@inproceedings{zillyRecurrentHighwayNetworks2017,
  title = {Recurrent {{Highway Networks}}},
  booktitle = {{{ICML}}},
  author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn{\'i}k, Jan and Schmidhuber, J{\"u}rgen},
  year = {2017},
  month = jul,
  pages = {4189--4198},
  abstract = {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.},
  keywords = {ICML,RNN},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\2SLAHF2S\\Zilly et al. - 2016 - Recurrent Highway Networks(3).pdf}
}

@misc{ZoteroDownloads,
  ids = {ZoteroDownloadsa},
  title = {Zotero | {{Downloads}}},
  howpublished = {https://www.zotero.org/download/},
  file = {C\:\\Users\\w-32\\Zotero\\storage\\I9FLBDSK\\download.html;C\:\\Users\\w-32\\Zotero\\storage\\ZL7LYTVW\\download.html}
}
