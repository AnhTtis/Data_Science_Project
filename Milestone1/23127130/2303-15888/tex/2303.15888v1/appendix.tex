\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reproducibility}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We release our source code (anonymized in the supplementary material for the review, public on github afterwards). All of our experiments use Avalanche~\citep{lomonacoAvalancheEndtoEndLibrary2021}, a continual learning library based on PyTorch~\citep{paszkePyTorchImperativeStyle2019}. We release the experiments' configurations using Hydra~\citep{Yadan2019Hydra}(hierarchical yaml configuration files), which means that each experiment in the paper can be reproduced by running a main python script with the desired configuration, as detailed in the README of the source code. Experimental details relevant for independent implementations are available in the following sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Comparison Between Related Learning Scenarios}\label{apx:scenario_compare}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The distributed continual learning scenario present some similarities with other scenarios in the literature. We provide a more detailed discussion of their differences here hoping to highlight their difference:
\begin{description}
    \item[continual learning]: a single model learning from a nonstationary stream of data. Knowledge sharing between models is not possible and current data is always available. Plasticity is suboptimal due to the stability/plasticity tradeoff.
    \item[rehearsal-free CL]: includes scenarios such as data-free class-incremental learning (DFCIL), where a single model learns from a nonstationary stream of data. Data from previous experiences is unavailable due to privacy constraints or severe storage limitations. Knowledge sharing is not possible and current data is always available. Plasticity is suboptimal due to the stability/plasticity tradeoff.
    \item[federated]: client-server organization with a single centralized controller. All the clients are learning the same task, possibly on different data. The server has full control over the training process and the client synchronize every few training iterations, resulting in multiple communication rounds. SCD data is private but plasticity is suboptimal because SCD are optimized on the global instead of the local task.
    \item[DCL]: Each SCD learn its own task, keeping all the data private. Plasticity is optimal because SCD are optimizing their own task. Communication is minimized by using a single round of communication.
\end{description}

Figure \ref{fig:scenario_full} shows the training process of the four different scenarios, mapping the adaptation and consolidation phases defined in Section \ref{sec:scenario} to the other scenarios. Table \ref{tbl:scenario_properties} summarizes the properties of the different scenario.

\begin{table}[h]%{.99\textwidth}
    \centering
    \caption{Summary of the main properties of different scenarios related to distributed continual learning.}\label{tbl:scenario_properties}
    \begin{tabular}{lrrrrr}
        \toprule
         & \thead{Past Data \\ Privacy} & \thead{Current Data \\ Privacy} & \thead{Optimal \\ plasticity} & \thead{Multiple \\ Devices} & \thead{single round \\ of communication} \\
        \midrule
        Continual Learning &  & & & & \\
        Rehearsal-free Continual Learning & \checkmark & & & \\
        Federated Learning & \checkmark & & & \checkmark & \\
        Ours & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark\\
        \bottomrule
    \end{tabular}
\end{table}


\begin{center}
    % \centering
    \includegraphics[width=0.9\textwidth]{draw/scenario_dcl_rfcl.pdf}
    \captionof{figure}{Schematic comparison of Sequential DCL vs Rehearsal-free CL, showing a possible implementation of DAC in rehearsal-free. Notice that in rehearsal-free CL we have the possibility to use the original data during consolidation and all the learning steps can happen on the same device.}\label{fig:scenario_full}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Sources}\label{apx:data_sources}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we show samples from the images used for knowledge distillation.
\emph{city}: high-resolution (around 2560x1920, 1.85MB) image of a japanese market;

\begin{minipage}{0.45\textwidth}
    animals: medium-resolution (600x225, 338KB) poster with several animals;
    
    \includegraphics[width=0.4\textwidth]{imgs/sources/ameyoko.jpg}
    \includegraphics[width=0.1\textwidth]{imgs/aug/ameyoko.png}
    
    hubble: high-resolution image (2300x2100, 6.90MB) from the Hubble telescope;
    
    \includegraphics[width=0.4\textwidth]{imgs/sources/hubble.jpg}
    \includegraphics[width=0.1\textwidth]{imgs/aug/hubble.png}
    
    animals: medium-resolution (600x225, 338KB) poster with several animals;
    \includegraphics[width=0.4\textwidth]{imgs/sources/animals.png}
    \includegraphics[width=0.1\textwidth]{imgs/aug/ameyoko.png}

\end{minipage}
\begin{minipage}{0.45\textwidth}
    bridge: Image of the San Francisco Golden Gate Bridge (1165x585, 1.17MB);
    
    \includegraphics[width=0.4\textwidth]{imgs/sources/sf.png}
    \includegraphics[width=0.1\textwidth]{imgs/aug/bridge_32px.png}
    
    ImageNet: samples from ImageNet (without augmentations);
    
    \includegraphics[width=0.1\textwidth]{imgs/aug/imagenet.png}
    
    noise: static noise.
    
    \includegraphics[width=0.3\textwidth]{imgs/sources/11_octaves_small.png}
    \includegraphics[width=0.1\textwidth]{imgs/aug/noise.png}
\end{minipage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hyperparameters}\label{apx:hparams}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{SplitCIFAR100:} We use a slimmed ResNet18 as a backbone for both the teacher and consolidated model.
During the consolidation, we use Adam with learning rate set to $0.0001$, with a batch size of $512$ and $500'000$ iterations. We use a temperature of $0.5$ for distillation. For the PLD loss, we set $\lambda=0.01$ and apply the loss at \verb|layer4.0| and \verb|linear| (logits). 

\paragraph{CORe50:} We use a MobileNet v2 pretrained on ImageNet as a backbone for both the teacher and consolidated model.
During the consolidation, we use Adam with learning rate set to $0.0001$, with a batch size of $128$ and $100'000$ iterations. We use a temperature of $1.0$ for distillation. For the PLD loss, we set $\lambda=100.0$ and apply the loss at \verb|classifier| (logits). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CKA}\label{apx:cka}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we show the CKA, as described in Section \ref{sec:cka_forward}, for the entire stream. We compute the CKA between the first expert and the expert after experience $i$.

% \begin{center}
%     \includegraphics[width=\textwidth]{imgs/ckas_same.pdf}
%     \captionof{figure}{CKA for \indsame.}
% \end{center}

% \begin{center}
%     \includegraphics[width=\textwidth]{imgs/ckas_rand.pdf}
%     \captionof{figure}{CKA for \indrand.}
% \end{center}

\begin{center}
    \includegraphics[width=\textwidth]{imgs/ckas_seq_oodkd.pdf}
    \captionof{figure}{CKA for \nopld.}
\end{center}


\begin{center}
    \includegraphics[width=\textwidth]{imgs/ckas_seq_latproj.pdf}
    \captionof{figure}{CKA for \expdac.}
\end{center}