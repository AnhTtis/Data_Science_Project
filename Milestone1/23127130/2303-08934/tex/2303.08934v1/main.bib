
@article{Robillard2010RecommendationSystems4SE,
	title = {Recommendation {Systems} for {Software} {Engineering}},
	abstract = {Software development can be challenging because of the large information spaces that developers must navigate. Without assistance, developers can become bogged down and spend a disproportionate amount of their time seeking information at the expense of other value-producing tasks. Recommendation systems for software engineering (RSSEs) are software tools that can assist developers with a wide range of activities, from reusing code to writing effective bug reports. The authors provide an overview of recommendation systems for software engineering: what they are, what they can do for developers, and what they might do in the future.},
	journal = {IEEE Software},
	author = {Robillard, Martin and Walker, Robert and Zimmermann, Thomas},
	year = {2010},
	keywords = {Navigation, Programming, Software engineering, Software tools, Writing, coding tools and techniques, design tools and techniques, development tools, programming environments, software construction tools, software engineering},
}

@inproceedings{Ray2014StudyofProgrammingLanguagesandCodeQualityinGitHub,
	title = {A {Large} {Scale} {Study} of {Programming} {Languages} and {Code} {Quality} in {Github}},
	abstract = {What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating ﬁndings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a signiﬁcant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also ﬁnd that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages.},
	booktitle = {{IEEE} {International} {Conf.} on {Software} {Maintenance}},
	author = {Ray, Baishakhi and Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
	year = {2009},
}

@article{Malinen2015UserParticipationinOnlineCommunities,
	title = {Understanding user participation in online communities: {A} systematic literature review of empirical studies},
	shorttitle = {Understanding user participation in online communities},
	journal = {Computers in Human Behavior},
	author = {Malinen, Sanna},
	year = {2015},
}

@article{fan_automatic_2012,
	title = {Automatic knowledge extraction from documents},
	volume = {56},
	issn = {0018-8646},
	doi = {10.1147/JRD.2012.2186519},
	abstract = {Access to a large amount of knowledge is critical for success at answering open-domain questions for DeepQA systems such as IBM Watson™. Formal representation of knowledge has the advantage of being easy to reason with, but acquisition of structured knowledge in open domains from unstructured data is often difficult and expensive. Our central hypothesis is that shallow syntactic knowledge and its implied semantics can be easily acquired and can be used in many areas of a question-answering system. We take a two-stage approach to extract the syntactic knowledge and implied semantics. First, shallow knowledge from large collections of documents is automatically extracted. Second, additional semantics are inferred from aggregate statistics of the automatically extracted shallow knowledge. In this paper, we describe in detail what kind of shallow knowledge is extracted, how it is automatically done from a large corpus, and how additional semantics are inferred from aggregate statistics. We also briefly discuss the various ways extracted knowledge is used throughout the IBM DeepQA system.},
	number = {3.4},
	journal = {IBM Journal of Research and Development},
	author = {Fan, J. and Kalyanpur, A. and Gondek, D. C. and Ferrucci, D. A.},
	month = may,
	year = {2012},
	note = {Conference Name: IBM Journal of Research and Development},
	keywords = {Computational linguistics, Context awareness, Information analysis, Knowledge based systems, Knowledge management, Semantics, Syntactics},
}

@inproceedings{Li2008RegexLearning4IE,
	title = {Regular {Expression} {Learning} for {Information} {Extraction}},
	booktitle = {{Conf.} on {Empirical} {Methods} in NLP},
	publisher = {Association for Computational Linguistics},
	author = {Li, Yunyao and Krishnamurthy, Rajasekar and {others}},
	year = {2008},
}

@inproceedings{Hong2022BROSKeyInformationExtractionfromDocs,
	title = {{BROS}: {A} {Pre}-trained {Language} {Model} {Focusing} on {Text} and {Layout} for {Better} {Key} {Information} {Extraction} from {Documents}},
	journal = {AAAI Conference on Artificial Intelligence},
	author = {Hong, Teakgyu and Kim, DongHyun and {others}},
	year = {2022},
}

@article{Shafiq2021LitReviewofMLinSWDevLifeCycle,
	title = {A {Literature} {Review} of {Using} {Machine} {Learning} in {Software} {Development} {Life} {Cycle} {Stages}},
	doi = {10.1109/ACCESS.2021.3119746},
	abstract = {The software engineering community is rapidly adopting machine learning for transitioning modern-day software towards highly intelligent and self-learning systems. However, the software engineering community is still discovering new ways how machine learning can offer help for various software development life cycle stages. In this article, we present a study on the use of machine learning across various software development life cycle stages. The overall aim of this article is to investigate the relationship between software development life cycle stages, and machine learning tools, techniques, and types. We attempt a holistic investigation in part to answer the question of whether machine learning favors certain stages and/or certain techniques.},
	journal = {IEEE Access},
	author = {Shafiq, Saad and Mashkoor, Atif and Mayr-Dorn, Christoph and Egyed, Alexander},
	year = {2021},
	keywords = {Data mining, Machine learning, Software engineering, Software systems, Software testing, Support vector machines, Tools, literature review, machine learning},
}

@misc{watson_systematic_2021,
	title = {A {Systematic} {Literature} {Review} on the {Use} of {Deep} {Learning} in {Software} {Engineering} {Research}},
	url = {http://arxiv.org/abs/2009.06520},
	abstract = {An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this crosscutting area of work, from its modern inception to the present, this paper presents a systematic literature review of research at the intersection of SE \& DL. The review canvases work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23 unique SE tasks. We center our analysis around the components of learning, a set of principles that govern the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research, and highlights likely areas of fertile exploration for the future.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Watson, Cody and Cooper, Nathan and Palacio, David Nader and Moran, Kevin and Poshyvanyk, Denys},
	month = sep,
	year = {2021},
	note = {arXiv:2009.06520 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Software Engineering},
}

@inproceedings{tawosi_versatile_2022,
	address = {Pittsburgh Pennsylvania},
	title = {A versatile dataset of agile open source software projects},
	isbn = {978-1-4503-9303-4},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528029},
	doi = {10.1145/3524842.3528029},
	abstract = {Agile software development is nowadays a widely adopted practise in both open-source and industrial software projects. Agile teams typically heavily rely on issue management tools to document new issues and keep track of outstanding ones, in addition to storing their technical details, effort estimates, assignment to developers, and more. Previous work utilised the historical information stored in issue management systems for various purposes; however, when researchers make their empirical data public, it is usually relevant solely to the study’s objective. In this paper, we present a more holistic and versatile dataset containing a wealth of information on more than half a million issues from 44 open-source Agile software, making it well-suited to several research avenues, and cross-analyses therein, including effort estimation, issue prioritization, issue assignment and many more. We make this data publicly available on GitHub to facilitate ease of use, maintenance, and extensibility.},
	language = {en},
	urldate = {2023-01-20},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Tawosi, Vali and Al-Subaihin, Afnan and Moussa, Rebecca and Sarro, Federica},
	month = may,
	year = {2022},
	pages = {707--711},
}

@misc{cheng_masked-attention_2022,
	title = {Masked-attention {Mask} {Transformer} for {Universal} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2112.01527},
	abstract = {Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics deﬁnes a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Maskedattention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a signiﬁcant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).},
	language = {en},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
	month = jun,
	year = {2022},
	note = {arXiv:2112.01527 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cheng_per-pixel_2021,
	title = {Per-{Pixel} {Classification} is {Not} {All} {You} {Need} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2107.06278},
	abstract = {Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Cheng, Bowen and Schwing, Alexander G. and Kirillov, Alexander},
	month = oct,
	year = {2021},
	note = {arXiv:2107.06278 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{Slankas2013AutomatedExtractionofNonfunctionalRequirements,
	title = {Automated extraction of non-functional requirements in available documentation},
	abstract = {While all systems have non-functional requirements (NFRs), they may not be explicitly stated in a formal requirements specification. Furthermore, NFRs may also be externally imposed via government regulations or industry standards. As some NFRs represent emergent system proprieties, those NFRs require appropriate analysis and design efforts to ensure they are met. When the specified NFRs are not met, projects incur costly re-work to correct the issues. The goal of our research is to aid analysts in more effectively extracting relevant non-functional requirements in available unconstrained natural language documents through automated natural language processing. Specifically, we examine which document types (data use agreements, install manuals, regulations, request for proposals, requirements specifications, and user manuals) contain NFRs categorized to 14 NFR categories (e.g. capacity, reliability, and security). We measure how effectively we can identify and classify NFR statements within these documents. In each of the documents evaluated, we found NFRs present. Using a word vector representation of the NFRs, a support vector machine algorithm performed twice as effectively compared to the same input to a multinomial naïve Bayes classifier. Our k-nearest neighbor classifier with a unique distance metric had an F1 measure of 0.54, outperforming in our experiments the optimal naïve Bayes classifier which had a F1 measure of 0.32. We also found that stop word lists beyond common determiners had no minimal performance effect.},
	booktitle = {{International} {Workshop} on {Natural} {Language} {Analysis} in {SW} {Engineering}},
	author = {Slankas, John and Williams, Laurie},
	year = {2013},
	keywords = {Classification algorithms, Documentation, Machine learning algorithms, Measurement, Natural languages, Security, Standards, classification, documentation, machine learning, natural language processing, non-functional requirements},
}

@inproceedings{Allamanis2016AttentionNetwork4ExtremeSummarizationofSourceCode,
	title={A convolutional attention network for extreme summarization of source code},
	author={Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
	booktitle={{ICML}},
	pages={2091--2100},
	year={2016},
}


@inproceedings{jesse_manytypes4typescript_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{ManyTypes4TypeScript}: a comprehensive {TypeScript} dataset for sequence-based type inference},
	isbn = {978-1-4503-9303-4},
	shorttitle = {{ManyTypes4TypeScript}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528507},
	doi = {10.1145/3524842.3528507},
	abstract = {In this paper, we present ManyTypes4TypeScript, a very large corpus for training and evaluating machine-learning models for sequence-based type inference in TypeScript. The dataset includes over 9 million type annotations, across 13,953 projects and 539,571 files. The dataset is approximately 10x larger than analogous type inference datasets for Python, and is the largest available for TypeScript. We also provide API access to the dataset, which can be integrated into any tokenizer and used with any state-of-the-art sequence-based model. Finally, we provide analysis and performance results for state-of-the-art code-specific models, for baselining. ManyTypes4TypeScript is available on Huggingface, Zenodo, and CodeXGLUE.},
	language = {en},
	urldate = {2023-01-17},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Jesse, Kevin and Devanbu, Premkumar T.},
	month = may,
	year = {2022},
	pages = {294--298},
}

@article{Ma2021OpenSourceVCSData,
	title = {World of code: enabling a research workflow for mining and analyzing the universe of open source {VCS} data},
	journal = {Empirical Software Engineering},
	author = {Ma, Yuxing and Dey, Tapajit and Bogart, Chris and {others}},
	year = {2021},
}

@inproceedings{Alfadel2021SecurityAnalysisinPythonPackages,
	title = {Empirical {Analysis} of {Security} {Vulnerabilities} in {Python} {Packages}},
	booktitle = {{International} {Conf.} on {SW} {Analysis}, {Evolution} and {Reengineering}},
	author = {Alfadel, Mahmoud and Costa, Diego Elias and Shihab, Emad},
	year = {2021},
}

@inproceedings{Elsner2021RegressionTestOptimizationinCI,
	title = {Empirically {Evaluating} {Readily} {Available} {Information} for {Regression} {Test} {Optimization} in {Continuous} {Integration}},
	author={Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
	booktitle={{ISSTA}},
	pages={491--504},
	year={2021}
}

@inproceedings{Gousios2014PullBasedSWDevelopment,
	title = {An exploratory study of the pull-based software development model},
	abstract = {The advent of distributed version control systems has led to the development of a new paradigm for distributed software development; instead of pushing changes to a central repository, developers pull them from other repositories and merge them locally. Various code hosting sites, notably Github, have tapped on the opportunity to facilitate pull-based development by offering workﬂow support tools, such as code reviewing systems and integrated issue trackers. In this work, we explore how pull-based software development works, ﬁrst on the GHTorrent corpus and then on a carefully selected sample of 291 projects. We ﬁnd that the pull request model offers fast turnaround, increased opportunities for community engagement and decreased time to incorporate contributions. We show that a relatively small number of factors affect both the decision to merge a pull request and the time to process it. We also examine the reasons for pull request rejection and ﬁnd that technical ones are only a small minority.},
	booktitle = {ICSE},
	author = {Gousios, Georgios and Pinzger, Martin and Deursen, Arie van},
	year = {2014},
}

@article{allamanis_convolutional_nodate,
	title = {A {Convolutional} {Attention} {Network}  for {Extreme} {Summarization} of {Source} {Code}},
	abstract = {Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have ﬁxed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features speciﬁcally. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.},
	language = {en},
	author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
}

@inproceedings{Mattis2020RTPTorrent,
	title = {{RTPTorrent}: {An} {Open}-source {Dataset} for {Evaluating} {Regression} {Test} {Prioritization}},
	abstract = {The software engineering practice of automated testing helps programmers find defects earlier during development. With growing software projects and longer-running test suites, frequency and immediacy of feedback decline, thereby making defects harder to repair. Regression test prioritization (RTP) is concerned with running relevant tests earlier to lower the costs of defect localization and to improve feedback. Finding representative data to evaluate RTP techniques is nontrivial, as most software is published without failing tests. In this work, we systematically survey a wide range of RTP literature regarding whether their dataset uses real or synthetic defects or tests, whether they are publicly available, and whether datasets are reused. We observed that some datasets are reused, however, many projects study only few projects and these rarely resemble real-world development activity.},
	booktitle = {MSR},
	author = {Mattis, Toni and Rein, Patrick and Dürsch, Falco and Hirschfeld, Robert},
	year = {2020},
}

@inproceedings{Beller2017TravisTorrent,
	title = {{TravisTorrent}: {Synthesizing} {Travis} {CI} and {GitHub} for {Full}-{Stack} {Research} on {Continuous} {Integration}},
	shorttitle = {{TravisTorrent}},
	abstract = {Continuous Integration (CI) has become a best practice of modern software development. Thanks in part to its tight integration with GitHub, Travis CI has emerged as arguably the most widely used CI platform for Open-Source Software (OSS) development. However, despite its prominent role in Software Engineering in practice, the benefits, costs, and implications of doing CI are all but clear from an academic standpoint. Little research has been done, and even less was of quantitative nature. In order to lay the groundwork for data-driven research on CI, we built TravisTorrent, travistorrent.testroots.org, a freely available data set based on Travis CI and GitHub that provides easy access to hundreds of thousands of analyzed builds from more than 1,000 projects. Unique to TravisTorrent is that each of its 2,640,825 Travis builds is synthesized with meta data from Travis CI's API, the results of analyzing its textual build log, a link to the GitHub commit which triggered the build, and dynamically aggregated project data from the time of commit extracted through GHTorrent.},
	booktitle = {MSR},
	author = {Beller, Moritz and Gousios, Georgios and Zaidman, Andy},
	year = {2017},
}

@inproceedings{Beller2017ExplorativeStudyofTravisCI,
	title = {Oops, {My} {Tests} {Broke} the {Build}: {An} {Explorative} {Analysis} of {Travis} {CI} with {GitHub}},
	booktitle = {MSR},
	author = {Beller, Moritz and Gousios, Georgios and Zaidman, Andy},
	year = {2017},
	keywords = {Best practices, Java, Programming, Software, Testing, Tools},
}

@inproceedings{Zampetti2017HowOSProjectsUseStaticCodeAnalysisinCIPipelines,
	title = {How {Open} {Source} {Projects} {Use} {Static} {Code} {Analysis} {Tools} in {Continuous} {Integration} {Pipelines}},
	abstract = {Static analysis tools are often used by software developers to entail early detection of potential faults, vulnerabilities, code smells, or to assess the source code adherence to coding standards and guidelines. Also, their adoption within Continuous Integration (CI) pipelines has been advocated by researchers and practitioners. This paper studies the usage of static analysis tools in 20 Java open source projects hosted on GitHub and using Travis CI as continuous integration infrastructure. Specifically, we investigate (i) which tools are being used and how they are configured for the CI, (ii) what types of issues make the build fail or raise warnings, and (iii) whether, how, and after how long are broken builds and warnings resolved. Results indicate that in the analyzed projects build breakages due to static analysis tools are mainly related to adherence to coding standards, and there is also some attention to missing licenses. Build failures related to tools identifying potential bugs or vulnerabilities occur less frequently, and in some cases such tools are activated in a "softer" mode, without making the build fail. Also, the study reveals that build breakages due to static analysis tools are quickly fixed by actually solving the problem, rather than by disabling the warning, and are often properly documented.},
	booktitle = {{MSR}},
	author = {Zampetti, Fiorella and Scalabrino, Simone and Oliveto, Rocco and {others}},
	year = {2017},
	keywords = {Continuous Integration, Data mining, Empirical Study, Encoding, History, Java, Open Source Projects, Pipelines, Software, Static Analysis Tools, Tools},
}

@misc{sinha_extractive_2018,
	title = {Extractive {Text} {Summarization} using {Neural} {Networks}},
	url = {http://arxiv.org/abs/1802.10137},
	abstract = {Text Summarization has been an extensively studied problem. Traditional approaches to text summarization rely heavily on feature engineering. In contrast to this, we propose a fully data-driven approach using feedforward neural networks for single document summarization. We train and evaluate the model on standard DUC 2002 dataset which shows results comparable to the state of the art models. The proposed model is scalable and is able to produce the summary of arbitrarily sized documents by breaking the original document into fixed sized parts and then feeding it recursively to the network.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Sinha, Aakash and Yadav, Abhishek and Gahlot, Akshay},
	month = feb,
	year = {2018},
	note = {arXiv:1802.10137 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
}

@misc{wu_learning_2018,
	title = {Learning to {Extract} {Coherent} {Summary} via {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1804.07036},
	abstract = {Coherence plays a critical role in producing a high-quality summary from a document. In recent years, neural extractive summarization is becoming increasingly attractive. However, most of them ignore the coherence of summaries when extracting sentences. As an effort towards extracting coherent summaries, we propose a neural coherence model to capture the cross-sentence semantic and syntactic coherence patterns. The proposed neural coherence model obviates the need for feature engineering and can be trained in an end-to-end fashion using unlabeled data. Empirical results show that the proposed neural coherence model can efficiently capture the cross-sentence coherence patterns. Using the combined output of the neural coherence model and ROUGE package as the reward, we design a reinforcement learning method to train a proposed neural extractive summarizer which is named Reinforced Neural Extractive Summarization (RNES) model. The RNES model learns to optimize coherence and informative importance of the summary simultaneously. Experimental results show that the proposed RNES outperforms existing baselines and achieves state-of-the-art performance in term of ROUGE on CNN/Daily Mail dataset. The qualitative evaluation indicates that summaries produced by RNES are more coherent and readable.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Wu, Yuxiang and Hu, Baotian},
	month = apr,
	year = {2018},
	note = {arXiv:1804.07036 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhu_automatic_2019,
	title = {Automatic {Code} {Summarization}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Automatic {Code} {Summarization}},
	url = {http://arxiv.org/abs/1909.04352},
	abstract = {Background: During software maintenance and development, the comprehension of program code is key to success. High-quality comments can help us better understand programs, but they're often missing or outmoded in today's programs. Automatic code summarization is proposed to solve these problems. During the last decade, huge progress has been made in this field, but there is a lack of an up-to-date survey. Aims: We studied publications concerning code summarization in the field of program comprehension to investigate state-of-the-art approaches. By reading and analyzing relevant articles, we aim at obtaining a comprehensive understanding of the current status of automatic code summarization. Method: In this paper, we performed a systematic literature review over the automatic source code summarization field. Furthermore, we synthesized the obtained data and investigated different approaches. Results: We successfully collected and analyzed 41 selected studies from the different research communities. We exhaustively investigated and described the data extraction techniques, description generation methods, evaluation methods and relevant artifacts of those works. Conclusions: Our systematic review provides an overview of the state of the art, and we also discuss further research directions. By fully elaborating current approaches in the field, our work sheds light on future research directions of program comprehension and comment generation.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Zhu, Yuxiang and Pan, Minxue},
	month = oct,
	year = {2019},
	note = {arXiv:1909.04352 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{allahyari_text_2017,
	title = {Text {Summarization} {Techniques}: {A} {Brief} {Survey}},
	shorttitle = {Text {Summarization} {Techniques}},
	url = {http://arxiv.org/abs/1707.02268},
	abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saeid and Trippe, Elizabeth D. and Gutierrez, Juan B. and Kochut, Krys},
	month = jul,
	year = {2017},
	note = {arXiv:1707.02268 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{hendrycks_using_2019,
	title = {Using {Pre}-{Training} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	url = {http://arxiv.org/abs/1901.09960},
	abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10\% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
	month = oct,
	year = {2019},
	note = {arXiv:1901.09960 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cox_surviving_2019,
	title = {Surviving software dependencies},
	volume = {62},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3347446},
	doi = {10.1145/3347446},
	abstract = {Software reuse is finally here but comes with risks.},
	language = {en},
	number = {9},
	urldate = {2023-01-05},
	journal = {Communications of the ACM},
	author = {Cox, Russ},
	month = aug,
	year = {2019},
	pages = {36--43},
}

@misc{noauthor_towards_nodate,
	title = {Towards {Reliability} in {Deep} {Learning} {Systems}},
	url = {https://ai.googleblog.com/2022/07/towards-reliability-in-deep-learning.html},
	language = {en},
	urldate = {2023-01-05},
}

@article{neelamegam_survey_nodate,
	title = {A {Survey} - {Object} {Oriented} {Quality} {Metrics}},
	abstract = {Object oriented design is becoming more popular in software development environment and object oriented design metrics is an essential part of software environment. This study focus on a set of object oriented metrics that can be used to measure the quality of an object oriented design. The metrics for object oriented design focus on measurements that are applied to the class and design characteristics. These measurements permit designers to access the software early in process, making changes that will reduce complexity and improve the continuing capability of the design. This report summarizes the existing metrics, which will guide the designers to support their design. We have categorized metrics and discussed in such a way that novice designers can apply metrics in their design as needed.},
	language = {en},
	author = {Neelamegam, C and Punithavalli, Dr M},
}

@inproceedings{goyal_qmood_2014,
	title = {{QMOOD} metric sets to assess quality of {Java} program},
	doi = {10.1109/ICICICT.2014.6781337},
	abstract = {This paper describes the model to evaluate and grade the Java programs, based on QMOOD (Quality Model for Object Oriented Design). QMOOD is the hierarchical model that defines relation between qualities attributes(like reusability, functionality, effectiveness, understand ability, extendibility, flexibility) and design properties with the help of equations. In this research we have developed the system based on QMOOD which is use to evaluate the quality of JAVA programs. In this we would try evaluate all quality attributes by finding the design metrics that is based on design property of object oriented design. Though QMOOD metrics are subjective in nature but still with the help of relationship between quality attributes and design property defined, we have calculated and aggregated these qualities attributes and evaluate the quality of program input We have tested the system with number of different programs that vary in their complexities and functionalities.},
	booktitle = {2014 {International} {Conference} on {Issues} and {Challenges} in {Intelligent} {Computing} {Techniques} ({ICICT})},
	author = {Goyal, Puneet Kumar and Joshi, Gamini},
	month = feb,
	year = {2014},
	keywords = {Computational modeling, Couplings, Encapsulation, ISO, Java, Java Programming language, Measurement, QMOOD metrics, QMOOD model, software engineering, software quality},
	pages = {520--533},
}

@inproceedings{Garcia2020AVBugs,
	title = {A comprehensive study of autonomous vehicle bugs},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380397},
	abstract = {Self-driving cars, or Autonomous Vehicles (AVs), are increasingly becoming an integral part of our daily life. About 50 corporations are actively working on AVs, including large companies such as Google, Ford, and Intel. Some AVs are already operating on public roads, with at least one unfortunate fatality recently on record. As a result, understanding bugs in AVs is critical for ensuring their security, safety, robustness, and correctness. While previous studies have focused on a variety of domains (e.g., numerical software; machine learning; and error-handling, concurrency, and performance bugs) to investigate bug characteristics, AVs have not been studied in a similar manner. Recently, two software systems for AVs, Baidu Apollo and Autoware, have emerged as frontrunners in the opensource community and have been used by large companies and governments (e.g., Lincoln, Volvo, Ford, Intel, Hitachi, LG, and the US Department of Transportation). From these two leading AV software systems, this paper describes our investigation of 16,851 commits and 499 AV bugs and introduces our classiﬁcation of those bugs into 13 root causes, 20 bug symptoms, and 18 categories of software components those bugs often affect. We identify 16 major ﬁndings from our study and draw broader lessons from them to guide the research community towards future directions in software bug detection, localization, and repair.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Garcia, Joshua and Feng, Yang and Shen, Junjie and Almanee, Sumaya and Xia, Yuan and Chen, {and} Qi Alfred},
	year = {2020},
}

@article{Guest2006InterviewDataSaturationandVariability,
	title = {How {Many} {Interviews} {Are} {Enough}?: {An} {Experiment} with {Data} {Saturation} and {Variability}},
	issn = {1525-822X, 1552-3969},
	shorttitle = {How {Many} {Interviews} {Are} {Enough}?},
	url = {http://journals.sagepub.com/doi/10.1177/1525822X05279903},
	doi = {10.1177/1525822X05279903},
	abstract = {Guidelines for determining nonprobabilistic sample sizes are virtually nonexistent. Purposive samples are the most commonly used form of nonprobabilistic sampling, and their size typically relies on the concept of “saturation,” or the point at which no new information or themes are observed in the data. Although the idea of saturation is helpful at the conceptual level, it provides little practical guidance for estimating sample sizes, prior to data collection, necessary for conducting quality research. Using data from a study involving sixty in-depth interviews with women in two West African countries, the authors systematically document the degree of data saturation and variability over the course of thematic analysis. They operationalize saturation and make evidence-based recommendations regarding nonprobabilistic sample sizes for interviews. Based on the data set, they found that saturation occurred within the first twelve interviews, although basic elements for metathemes were present as early as six interviews. Variability within the data followed similar patterns.},
	language = {en},
	urldate = {2022-12-27},
	journal = {Field Methods},
	author = {Guest, Greg and Bunce, Arwen and Johnson, Laura},
	year = {2006},
}

@inproceedings{Abdalkareem2017TrivialPackages,
	title = {Why do developers use trivial packages? an empirical case study on npm},
	abstract = {Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call ‘trivial packages’. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages.},
	booktitle = {European {Software} {Engineering} {Conference}/{Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Abdalkareem, Rabe and Nourry, Olivier and Wehaibi, Sultan and Mujahid, Suhaib and Shihab, Emad},
	year = {2017},
}

@article{Makitalo2020OpportunisticSWReuse,
	title = {On opportunistic software reuse},
	volume = {102},
	issn = {0010-485X, 1436-5057},
	url = {https://link.springer.com/10.1007/s00607-020-00833-6},
	doi = {10.1007/s00607-020-00833-6},
	abstract = {The availability of open source assets for almost all imaginable domains has led the software industry to opportunistic design—an approach in which people develop new software systems in an ad hoc fashion by reusing and combining components that were not designed to be used together. In this paper we investigate this emerging approach. We demonstrate the approach with an industrial example in which Node.js modules and various subsystems are used in an opportunistic way. Furthermore, to study opportunistic reuse as a phenomenon, we present the results of three contextual interviews and a survey with reuse practitioners to understand to what extent opportunistic reuse offers improvements over traditional systematic reuse approaches.},
	language = {en},
	number = {11},
	urldate = {2022-12-30},
	journal = {Computing},
	author = {Mäkitalo, Niko and Taivalsaari, Antero and Kiviluoto, Arto and Mikkonen, Tommi and Capilla, Rafael},
	month = nov,
	year = {2020},
	pages = {2385--2408},
}

@misc{HuggingFaceWeb,
	title = {Hugging {Face} – {The} {AI} community building the future.},
	url = {https://huggingface.co/},
	urldate = {2021-11-17},
	author = {Hugging Face},
	year = {2021},
}

@misc{ModelhubWeb,
	title = {Modelhub},
	url = {http://modelhub.ai/},
	author = {{Computational Imaging {and} Bioinformatics Lab}},
	year = {2022},
}

@inproceedings{Vu2021LastPyMile,
	title = {{LastPyMile}: identifying the discrepancy between sources and packages},
	abstract = {Open source packages have source code available on repositories for inspection (e.g. on GitHub) but developers use pre-built packages directly from the package repositories (such as npm for JavaScript, PyPI for Python, or RubyGems for Ruby).},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Vu, Duc-Ly and Massacci, Fabio and Pashchenko, Ivan and Plate, Henrik and Sabetta, Antonino},
	year = {2021},
}

@article{Wang2019SurveyofDLMLFrameworkandLibrary,
	title = {Various {Frameworks} and {Libraries} of {Machine} {Learning} and {Deep} {Learning}: {A} {Survey}},
	issn = {1134-3060, 1886-1784},
	shorttitle = {Various {Frameworks} and {Libraries} of {Machine} {Learning} and {Deep} {Learning}},
	url = {http://link.springer.com/10.1007/s11831-018-09312-w},
	doi = {10.1007/s11831-018-09312-w},
	abstract = {With the rapid development of deep learning in various fields, the big companies and research teams have developed independent and unique tools. This paper collects 18 common deep learning frameworks and libraries (Caffe, Caffe2, Tensorflow, Theano include Keras Lasagnes and Blocks, MXNet, CNTK, Torch, PyTorch, Pylearn2, Scikit-learn, Matlab include MatconvNet Matlab deep learning and Deep learning tool box, Chainer, Deeplearning4j) and introduces a large number of benchmarking data. In addition, we give the overall score of the current eight mainstream deep learning frameworks from six aspects (model design ability, interface property, deployment ability, performance, framework design and prospects for development). Based on our overview, the deep learning researchers can choose the appropriate development tools according to the evaluation criteria. By summarizing the 18 deep learning frameworks and libraries, we have found that most of the deep learning tools are moving closer to the mobile terminal, and the role of ASICs is gradually emerging. It is believed that the future deep learning applications will be inseparable from the ASIC support.},
	language = {en},
	urldate = {2022-01-12},
	journal = {Archives of Computational Methods in Engineering},
	author = {Wang, Zhaobin and Liu, Ke and Li, Jian and Zhu, Ying and Zhang, Yaonan},
	month = feb,
	year = {2019},
}

@misc{Chen2022DLFrameworkBug,
	title = {Toward {Understanding} {Deep} {Learning} {Framework} {Bugs}},
	url = {http://arxiv.org/abs/2203.04026},
	abstract = {DL frameworks are the basis of constructing all DL programs and models, and thus their bugs could lead to the unexpected behaviors of any DL program or model relying on them. Such wide effect demonstrates the necessity and importance of guaranteeing DL frameworks' quality. Understanding the characteristics of DL framework bugs is a fundamental step for this quality assurance task, facilitating to design effective bug detection and debugging approaches. Hence, in this work we conduct the most large-scale study on 800 bugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch, MXNet, and DL4J). By analyzing the root causes and symptoms of DL framework bugs associated with 5 components decomposed from DL frameworks, as well as measuring test coverage achieved by three state-of-the-art testing techniques and developers' efforts on fixing those bugs, we obtain 14 major findings for the comprehensive understanding of DL framework bugs and the current status of existing DL framework testing and debugging practice, and then provide a series of actionable guidelines for better DL framework bug detection and debugging.},
	urldate = {2022-10-04},
	publisher = {arXiv},
	author = {Chen, Junjie and Liang, Yihua and Shen, Qingchao and Jiang, Jiajun},
	month = mar,
	year = {2022},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{Wang2022WechatBugStudy,
	title = {Characterizing and {Detecting} {Bugs} in {WeChat} {Mini}-{Programs}},
	doi = {10.1145/3510003.3510114},
	abstract = {Built on the WeChat social platform, WeChat Mini-Programs are widely used by more than 400 million users every day. Consequently, the reliability of Mini-Programs is particularly crucial. However, WeChat Mini-Programs suffer from various bugs related to execution environment, lifecycle management, asynchronous mechanism, etc. These bugs have seriously affected users' experience and caused serious impacts. In this paper, we conduct the first empirical study on 83 WeChat Mini-Program bugs, and perform an in-depth analysis of their root causes, impacts and fixes. From this study, we obtain many interesting findings that can open up new research directions for combating WeChat Mini-Program bugs. Based on the bug patterns found in our study, we further develop WeDetector to detect WeChat Mini-Program bugs. Our evaluation on 25 real-world Mini-Programs has found 11 previously unknown bugs, and 7 of them have been confirmed by developers.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Wang, Tao and Xu, Qingxin and Chang, Xiaoning and Dou, Wensheng and Zhu, Jiaxin and Xie, Jinhui and Deng, Yuetang and Yang, Jianbo and Yang, Jiaheng and Wei, Jun and Huang, Tao},
	year = {2022},
	keywords = {Computer bugs, Message service, Reliability, Social networking (online), Software engineering, User experience, WeChat Mini-Programs, bug detection, empirical study},
	pages = {363--375},
}

@article{baltes_sampling_2022,
	title = {Sampling in software engineering research: a critical review and guidelines},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Sampling in software engineering research},
	url = {https://link.springer.com/10.1007/s10664-021-10072-8},
	doi = {10.1007/s10664-021-10072-8},
	abstract = {Representative sampling appears rare in empirical software engineering research. Not all studies need representative samples, but a general lack of representative sampling undermines a scientific field. This article therefore reports a critical review of the state of sampling in recent, high-quality software engineering research. The key findings are: (1) random sampling is rare; (2) sophisticated sampling strategies are very rare; (3) sampling, representativeness and randomness often appear misunderstood. These findings suggest that software engineering research has a generalizability crisis. To address these problems, this paper synthesizes existing knowledge of sampling into a succinct primer and proposes extensive guidelines for improving the conduct, presentation and evaluation of sampling in software engineering research. It is further recommended that while researchers should strive for more representative samples, disparaging non-probability sampling is generally capricious and particularly misguided for predominately qualitative research.},
	language = {en},
	number = {4},
	urldate = {2022-12-27},
	journal = {Empirical Software Engineering},
	author = {Baltes, Sebastian and Ralph, Paul},
	month = jul,
	year = {2022},
	pages = {94},
}

@article{Anasuodei2021SWReusabilityApproachesandChallenges,
	title = {Software {Reusability}: {Approaches} and {Challenges}},
	volume = {06},
	issn = {24546194},
	shorttitle = {Software {Reusability}},
	url = {https://www.rsisinternational.org/journals/ijrias/DigitalLibrary/volume-6-issue-5/142-146.pdf},
	doi = {10.51584/IJRIAS.2021.6510},
	abstract = {Software reuse is used to aid the software development process which in recent times can improve the resulting quality and productivity of software development, by assisting software engineers throughout various software engineering phases to enhance the quality of software, provide quick turnaround time for software development using few people, tools, and methods, which creates a good software quality by enhancing integration of the software system to provide a competitive advantage. This paper examines the concept of software reuse, the approaches to be considered for software reuse, which is broadly shared into three categories: component-based software reuse, domain engineering and software product lines, architecture-based software reuse and challenges that affect the software reuse development process.},
	language = {en},
	number = {05},
	urldate = {2022-12-22},
	journal = {International Journal of Research and Innovation in Applied Science},
	author = {Anasuodei, Moko and {Ojekudo} and Akpofure, Nathaniel},
	year = {2021},
	pages = {142--146},
}

@article{hristov_structuring_2012,
	title = {Structuring {Software} {Reusability} {Metrics} for {Component}-{Based} {Software} {Development}},
	abstract = {The idea of reusing software components has been present in software engineering for several decades. Although the software industry developed massively in recent decades, component reuse is still facing numerous challenges and lacking adoption by practitioners. One of the impediments preventing efficient and effective reuse is the difficulty to determine which artifacts are best suited to solve a particular problem in a given context and how easy it will be to reuse them there. So far, no clear framework is describing the reusability of software and structuring appropriate metrics that can be found in literature. Nevertheless, a good understanding of reusability as well as adequate and easy to use metrics for quantification of reusability are necessary to simplify and accelerate the adoption of component reuse in software development. Thus, we propose an initial version of such a framework intended to structure existing reusability metrics for component-based software development that we have collected for this paper.},
	language = {en},
	author = {Hristov, Danail and Hummel, Oliver and Huq, Mahmudul and Janjic, Werner},
	year = {2012},
}

@inproceedings{Shiva2007SWReuseResearchandPractice,
	address = {Las Vegas, NV, USA},
	title = {Software {Reuse}: {Research} and {Practice}},
	isbn = {978-0-7695-2776-5},
	shorttitle = {Software {Reuse}},
	url = {http://ieeexplore.ieee.org/document/4151749/},
	doi = {10.1109/ITNG.2007.182},
	abstract = {It has been more than three decades since the idea of software reuse was proposed. Many success stories have been told, yet it is believed that software reuse is still in the development phase and has not reached its full potential. How far are we with software reuse research and practice? This paper is an attempt to answer this question. Keywords: Software Reuse, Architecture, Domain Engineering, Metrics, Component.},
	language = {en},
	urldate = {2022-12-22},
	booktitle = {Fourth {International} {Conference} on {Information} {Technology} ({ITNG}'07)},
	publisher = {IEEE},
	author = {Shiva, Sajjan G. and Shala, Lubna Abou},
	month = apr,
	year = {2007},
	pages = {603--609},
}

@misc{MicrosoftSTRIDE,
	title = {The {STRIDE} {Threat} {Model}},
	url = {https://learn.microsoft.com/en-us/previous-versions/commerce-server/ee823878(v=cs.20)},
	language = {en-us},
	urldate = {2022-12-20},
	author = {Microsoft},
	year = {2021},
}

@misc{AWSSTRIDE,
	title = {How to approach threat modeling},
	url = {https://aws.amazon.com/blogs/security/how-to-approach-threat-modeling/},
	urldate = {2022-12-20},
	author = {Boyd, Darran},
	year = {2021},
}

@article{monteil_nine_2020,
	title = {Nine {Best} {Practices} for {Research} {Software} {Registries} and {Repositories}: {A} {Concise} {Guide}},
	shorttitle = {Nine {Best} {Practices} for {Research} {Software} {Registries} and {Repositories}},
	url = {http://arxiv.org/abs/2012.13117},
	abstract = {Scientific software registries and repositories serve various roles in their respective disciplines. These resources improve software discoverability and research transparency, provide information for software citations, and foster preservation of computational methods that might otherwise be lost over time, thereby supporting research reproducibility and replicability. However, developing these resources takes effort, and few guidelines are available to help prospective creators of registries and repositories. To address this need, we present a set of nine best practices that can help managers define the scope, practices, and rules that govern individual registries and repositories. These best practices were distilled from the experiences of the creators of existing resources, convened by a Task Force of the FORCE11 Software Citation Implementation Working Group during the years 2019-2020. We believe that putting in place specific policies such as those presented here will help scientific software registries and repositories better serve their users and their disciplines.},
	urldate = {2022-05-05},
	journal = {arXiv:2012.13117 [cs]},
	author = {Monteil, Alain and Gonzalez-Beltran, Alejandra and Ioannidis, Alexandros and Allen, Alice and Lee, Allen and Bandrowski, Anita and Wilson, Bruce E. and Mecum, Bryce and Du, Cai Fan and Robinson, Carly and Garijo, Daniel and Katz, Daniel S. and Long, David and Milliken, Genevieve and Ménager, Hervé and Hausman, Jessica and Spaaks, Jurriaan H. and Fenlon, Katrina and Vanderbilt, Kristin and Hwang, Lorraine and Davis, Lynn and Fenner, Martin and Crusoe, Michael R. and Hucka, Michael and Wu, Mingfang and Hong, Neil Chue and Teuben, Peter and Stall, Shelley and Druskat, Stephan and Carnevale, Ted and Morrell, Thomas},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.13117},
	keywords = {Computer Science - Computers and Society, Computer Science - Digital Libraries},
}

@inproceedings{Pham2020AnalysisofVarianceinDLSWSystems,
	title = {Problems and {Opportunities} in {Training} {Deep} {Learning} {Software} {Systems}: {An} {Analysis} of {Variance}},
	doi = {10.1145/3324884.3416545},
	abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models’ accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.},
	booktitle = {ASE},
	author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
	year = {2020},
}

@inproceedings{mmdnn,
	title = {Enhancing the interoperability between deep learning frameworks by model conversion},
	abstract = {Deep learning (DL) has become one of the most successful machine learning techniques. To achieve the optimal development result, there are emerging requirements on the interoperability between DL frameworks that the trained model files and training/serving programs can be re-utilized. Faithful model conversion is a promising technology to enhance the framework interoperability in which a source model is transformed into the semantic equivalent in another target framework format. However, several major challenges need to be addressed. First, there are apparent discrepancies between DL frameworks. Second, understanding the semantics of a source model could be difficult due to the framework scheme and optimization. Lastly, there exist a large number of DL frameworks, bringing potential significant engineering efforts.},
	booktitle = {European {Software} {Engineering} {Conference}/{Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Liu, Yu and Chen, Cheng and Zhang, Ru and Qin, Tingting and Ji, Xiang and Lin, Haoxiang and Yang, Mao},
	year = {2020},
}

@inproceedings{kaplunovich_refactoring_2020,
	address = {Seoul Republic of Korea},
	title = {Refactoring of {Neural} {Network} {Models} for {Hyperparameter} {Optimization} in {Serverless} {Cloud}},
	isbn = {978-1-4503-7963-2},
	url = {https://dl.acm.org/doi/10.1145/3387940.3392268},
	doi = {10.1145/3387940.3392268},
	abstract = {Machine Learning and Neural Networks in particular have become hot topics in Computer Science. The recent 2019 Turing award to the forefathers of Deep Learning and AI - Yoshua Bengio, Geoffrey Hinton, and Yann LeCun proves the importance of the technology and its effect on science and industry. However, we have realized that even nowadays, the state of the art methods require several manual steps for neural network hyperparameter optimization. Our approach automates the model tuning by refactoring the original Python code using open-source libraries for processing. We were able to identify hyperparameters by parsing the original source and analyzing it. Given these parameters, we refactor the model, add the state of the art optimization library calls, and run the updated code in the Serverless Cloud. Our approach has proven to eliminate manual steps for an arbitrary TensorFlow and Keras tuning. We have created a tool called OptPar which automatically refactors an arbitrary Deep Neural Network optimizing its hyperparameters. Such a transformation can save hours of time for Data Scientists, giving them an opportunity to concentrate on designing their Machine Learning algorithms.},
	language = {en},
	urldate = {2022-12-13},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {ACM},
	author = {Kaplunovich, Alex and Yesha, Yelena},
	month = jun,
	year = {2020},
	pages = {311--314},
}

@inproceedings{kaur_improving_2017,
	title = {Improving the quality of software by refactoring},
	doi = {10.1109/ICCONS.2017.8250707},
	abstract = {Software code management has become another key skill required by software architects and software developers. Size of software increases with increase in count of features in software. Code refactoring is process of reducing code maintenance cost. It is achieved by many different techniques like extract, move methods, fields or classes in code. In this research we focused on improving the maintainability of the code by looking into the different refactoring techniques and improving upon them. We proposed an algorithm to improve the refactoring process which results in higher maintainability. To look into the validity of our proposed algorithm, we have used Junit and reffinder to analyse the code and generate the result metrics. We have observed the effectiveness of our work by comparing the different code maintainability indexes generated by the tool. In our research we have examined four releases of the software project for code refactoring and maintainability. Adding some extra features and using enhanced refactoring techniques measuring the code metrics and comparing the results of current releases with the previous releases.},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Computing} and {Control} {Systems} ({ICICCS})},
	author = {Kaur, Gurpreet and Singh, Balraj},
	month = jun,
	year = {2017},
	keywords = {Cloning, Code refactoring, Couplings, Data mining, Maintenance engineering, Size measurement, Software, bad smells, refactoring process, software metrics, software quality attributes},
	pages = {185--191},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1374314},
	urldate = {2022-12-13},
}

@inproceedings{kaur_improving_2017-1,
	title = {Improving the quality of software by refactoring},
	doi = {10.1109/ICCONS.2017.8250707},
	abstract = {Software code management has become another key skill required by software architects and software developers. Size of software increases with increase in count of features in software. Code refactoring is process of reducing code maintenance cost. It is achieved by many different techniques like extract, move methods, fields or classes in code. In this research we focused on improving the maintainability of the code by looking into the different refactoring techniques and improving upon them. We proposed an algorithm to improve the refactoring process which results in higher maintainability. To look into the validity of our proposed algorithm, we have used Junit and reffinder to analyse the code and generate the result metrics. We have observed the effectiveness of our work by comparing the different code maintainability indexes generated by the tool. In our research we have examined four releases of the software project for code refactoring and maintainability. Adding some extra features and using enhanced refactoring techniques measuring the code metrics and comparing the results of current releases with the previous releases.},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Computing} and {Control} {Systems} ({ICICCS})},
	author = {Kaur, Gurpreet and Singh, Balraj},
	month = jun,
	year = {2017},
	keywords = {Cloning, Code refactoring, Couplings, Data mining, Maintenance engineering, Size measurement, Software, bad smells, refactoring process, software metrics, software quality attributes},
	pages = {185--191},
}

@inproceedings{murgia_parameter-based_2011,
	title = {Parameter-{Based} {Refactoring} and the {Relationship} with {Fan}-in/{Fan}-out {Coupling}},
	doi = {10.1109/ICSTW.2011.26},
	abstract = {Refactoring is an activity which, in theory, should have minimal impact on the overall structure of a system. That said, certain refactorings change the coupling profile of a system and over time those cumulative changes in coupling can have serious implications for system maintenance effort. In this paper, we analyse effect of the fan-in and fan-out metrics from the perspective of two refactorings - namely 'Add parameter' to, and 'Remove Parameter' from, a method. We developed a bespoke pattern-matching tool to collect these two refactorings from multiple releases of the Tomcat open-source system using the Evolizer tool to extract method signature data and the JHawk metrics tool to collect the two coupling metrics. Results point to significant differences in the profiles of fan-in and fan-out between refactored and non-refactored classes. We describe how software company can take advantage from this knowledge by defining a priority list of classes which could require a refactoring. A strong over-arching theme emerged: developers seemed to focus on the refactoring of classes with relatively high fan-in and fan-out rather than classes with high values in any one. The study is the first that we know of to analyse the direct effect of a subset of Fowler's refactorings on fan-in and fan-out - relevant metrics of the overall structure of a system.},
	booktitle = {2011 {IEEE} {Fourth} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops}},
	author = {Murgia, A. and Marchesi, M. and Concas, G. and Tonelli, R. and Counsell, S.},
	month = mar,
	year = {2011},
	keywords = {Conferences, Helium, Refactoring, Software testing, fan-in, fan-out, parameters},
	pages = {430--436},
}

@inproceedings{alizadeh_refbot_2019,
	title = {{RefBot}: {Intelligent} {Software} {Refactoring} {Bot}},
	shorttitle = {{RefBot}},
	doi = {10.1109/ASE.2019.00081},
	abstract = {The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost. In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any "open" or "merge" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Alizadeh, Vahid and Ouali, Mohamed Amine and Kessentini, Marouane and Chater, Meriem},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Bot (Internet), Manuals, Measurement, Object oriented modeling, Pipelines, Software, Software bot, Software quality, Tools, refactoring},
	pages = {823--834},
}

@misc{noauthor_ieee_nodate-1,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9835158},
	urldate = {2022-12-05},
}

@misc{noauthor_ieee_nodate-2,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8945075},
	urldate = {2022-12-05},
}

@inproceedings{hohman_understanding_2020,
	address = {Honolulu HI USA},
	title = {Understanding and {Visualizing} {Data} {Iteration} in {Machine} {Learning}},
	isbn = {978-1-4503-6708-0},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376177},
	doi = {10.1145/3313831.3376177},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hohman, Fred and Wongsuphasawat, Kanit and Kery, Mary Beth and Patel, Kayur},
	month = apr,
	year = {2020},
	pages = {1--13},
}

@book{wang_atmseer_2019,
	title = {{ATMSeer}: {Increasing} {Transparency} and {Controllability} in {Automated} {Machine} {Learning}},
	shorttitle = {{ATMSeer}},
	abstract = {To relieve the pain of manually selecting machine learning algorithms and tuning hyperparameters, automated machine learning (AutoML) methods have been developed to automatically search for good models. Due to the huge model search space, it is impossible to try all models. Users tend to distrust automatic results and increase the search budget as much as they can, thereby undermining the efficiency of AutoML. To address these issues, we design and implement ATMSeer, an interactive visualization tool that supports users in refining the search space of AutoML and analyzing the results. To guide the design of ATMSeer, we derive a workflow of using AutoML based on interviews with machine learning experts. A multi-granularity visualization is proposed to enable users to monitor the AutoML process, analyze the searched models, and refine the search space in real time. We demonstrate the utility and usability of ATMSeer through two case studies, expert interviews, and a user study with 13 end users.},
	author = {Wang, Qianwen and Ming, Yao and Jin, Zhihua and Shen, Qiaomu and Liu, Dongyu and Smith, Micah and Veeramachaneni, Kalyan and Qu, Huamin},
	month = feb,
	year = {2019},
	doi = {10.1145/3290605.3300911},
}

@inproceedings{bogart_increasing_2020,
	address = {Seoul Republic of Korea},
	title = {Increasing the {Trust} {In} {Refactoring} {Through} {Visualization}},
	isbn = {978-1-4503-7963-2},
	url = {https://dl.acm.org/doi/10.1145/3387940.3392190},
	doi = {10.1145/3387940.3392190},
	abstract = {In software development, maintaining good design is essential. The process of refactoring enables developers to improve this design during development without altering the program’s existing behavior. However, this process can be time-consuming, introduce semantic errors, and be diﬃcult for developers inexperienced with refactoring or unfamiliar with a given code base. Automated refactoring tools can help not only by applying these changes, but by identifying opportunities for refactoring. Yet, developers have not been quick to adopt these tools due to a lack of trust between the developer and the tool. We propose an approach in the form of a visualization to aid developers in understanding these suggested operations and increasing familiarity with automated refactoring tools. We also provide a manual validation of this approach and identify options to continue experimentation.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {ACM},
	author = {Bogart, Alex and AlOmar, Eman Abdullah and Mkaouer, Mohamed Wiem and Ouni, Ali},
	month = jun,
	year = {2020},
	pages = {334--341},
}

@article{murphy-hill_how_2012,
	title = {How {We} {Refactor}, and {How} {We} {Know} {It}},
	volume = {38},
	issn = {1939-3520},
	doi = {10.1109/TSE.2011.41},
	abstract = {Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Murphy-Hill, Emerson and Parnin, Chris and Black, Andrew P.},
	month = jan,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Java, Refactoring, Software tools, floss refactoring, refactoring tools, root-canal refactoring.},
	pages = {5--18},
}

@inproceedings{sellitto_toward_2022,
	address = {Honolulu, HI, USA},
	title = {Toward {Understanding} the {Impact} of {Refactoring} on {Program} {Comprehension}},
	isbn = {978-1-66543-786-8},
	url = {https://ieeexplore.ieee.org/document/9825885/},
	doi = {10.1109/SANER53432.2022.00090},
	abstract = {Software refactoring is the activity associated with developers changing the internal structure of source code without modifying its external behavior. The literature argues that refactoring might have beneﬁcial and harmful implications for software maintainability, primarily when performed without the support of automated tools. This paper continues the narrative on the effects of refactoring by exploring the dimension of program comprehension, namely the property that describes how easy it is for developers to understand source code. We start our investigation by assessing the basic unit of program comprehension, namely program readability. Next, we set up a large-scale empirical investigation – conducted on 156 opensource projects – to quantify the impact of refactoring on program readability. First, we mine refactoring data and, for each commit involving a refactoring, we compute (i) the amount and type(s) of refactoring actions performed and (ii) eight stateof-the-art program comprehension metrics. Afterwards, we build statistical models relating the various refactoring operations to each of the readability metrics considered to quantify the extent to which each refactoring impacts the metrics in either a positive or negative manner. The key results are that refactoring has a notable impact on most of the readability metrics considered.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	publisher = {IEEE},
	author = {Sellitto, Giulia and Iannone, Emanuele and Codabux, Zadia and Lenarduzzi, Valentina and De Lucia, Andrea and Palomba, Fabio and Ferrucci, Filomena},
	month = mar,
	year = {2022},
	pages = {731--742},
}

@inproceedings{silva_recommending_2014,
	address = {Hyderabad, India},
	title = {Recommending automated extract method refactorings},
	isbn = {978-1-4503-2879-1},
	url = {http://dl.acm.org/citation.cfm?doid=2597008.2597141},
	doi = {10.1145/2597008.2597141},
	abstract = {Extract Method is a key refactoring for improving program comprehension. However, recent empirical research shows that refactoring tools designed to automate Extract Methods are often underused. To tackle this issue, we propose a novel approach to identify and rank Extract Method refactoring opportunities that are directly automated by IDE-based refactoring tools. Our approach aims to recommend new methods that hide structural dependencies that are rarely used by the remaining statements in the original method. We conducted an exploratory study to experiment and deﬁne the best strategies to compute the dependencies and the similarity measures used by the proposed approach. We also evaluated our approach in a sample of 81 extract method opportunities generated for JUnit and JHotDraw, achieving a precision of 48\% (JUnit) and 38\% (JHotDraw).},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Program} {Comprehension} - {ICPC} 2014},
	publisher = {ACM Press},
	author = {Silva, Danilo and Terra, Ricardo and Valente, Marco Tulio},
	year = {2014},
	pages = {146--156},
}

@article{murphy-hill_breaking_nodate,
	title = {Breaking the {Barriers} to {Successful} {Refactoring}: {Observations} and {Tools} for {Extract} {Method}},
	abstract = {Refactoring is the process of changing the structure of code without changing its behavior. Refactoring can be semi-automated with tools, which should make it easier for programmers to refactor quickly and correctly. However, we have observed that many tools do a poor job of communicating errors triggered by the refactoring process and that programmers using them sometimes refactor slowly, conservatively, and incorrectly. In this paper we characterize problems with current refactoring tools, demonstrate three new tools to assist in refactoring, and report on a user study that compares these new tools against existing tools. The results of the study show that speed, accuracy, and user satisfaction can be significantly increased. From the new tools we induce a set of usability recommendations that we hope will help inspire a new generation of programmer-friendly refactoring tools.},
	language = {en},
	author = {Murphy-Hill, Emerson and Black, Andrew P},
	pages = {10},
}

@phdthesis{nalepa_oizumi_identification_2022,
	address = {Rio de Janeiro, Brazil},
	type = {{DOUTOR} {EM} {CIÊNCIAS} - {INFORMÁTICA}},
	title = {{IDENTIFICATION} {AND} {REFACTORING} {OF} {DESIGN} {PROBLEMS} {IN} {SOFTWARE} {SYSTEMS}},
	url = {http://www.maxwell.vrac.puc-rio.br/Busca_etds.php?strSecao=resultado&nrSeq=60990@2},
	abstract = {Oizumi, Willian Nalepa; Garcia, Alessandro Fabricio (Advisor). Identification and Refactoring of Design Problems in Software Systems. Rio de Janeiro, 2022. 235p. Tese de doutorado –Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro.},
	language = {en},
	urldate = {2022-12-05},
	school = {PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO},
	author = {Nalepa Oizumi, Willian},
	month = apr,
	year = {2022},
	doi = {10.17771/PUCRio.acad.60990},
}

@article{ratzinger_improving_nodate,
	title = {Improving {Evolvability} through {Refactoring}},
	abstract = {Refactoring is one means of improving the structure of existing software. Locations for the application of refactoring are often based on subjective perceptions such as ”bad smells”, which are vague suspicions of design shortcomings. We exploit historical data extracted from repositories such as CVS and focus on change couplings: if some software parts change at the same time very often over several releases, this data can be used to point to candidates for refactoring. We adopt the concept of bad smells and provide additional change smells. Such a smell is hardly visible in the code, but easy to spot when viewing the change history. Our approach enables the detection of such smells allowing an engineer to apply refactoring on these parts of the source code to improve the evolvability of the software. For that, we analyzed the history of a large industrial system for a period of 15 months, proposed spots for refactorings based on change couplings, and performed them with the developers. After observing the system for another 15 months we ﬁnally analyzed the eﬀectiveness of our approach. Our results support our hypothesis that the combination of change dependency analysis and refactoring is applicable and eﬀective.},
	language = {en},
	author = {Ratzinger, Jacek and Fischer, Michael and Gall, Harald},
	pages = {5},
}

@inproceedings{advani_extracting_2006,
	address = {Dijon, France},
	title = {Extracting refactoring trends from open-source software and a possible solution to the 'related refactoring' conundrum},
	isbn = {978-1-59593-108-5},
	url = {http://portal.acm.org/citation.cfm?doid=1141277.1141685},
	doi = {10.1145/1141277.1141685},
	abstract = {Refactoring, as a software engineering discipline has emerged over recent years to become an important aspect of maintaining software. Refactoring refers to the restructuring of software according to specific mechanics and principles. In this paper, we describe a tool that allows refactoring data across multiple versions of seven opensource software systems to be collected. The tool automates the identification of refactorings as program transformations between consecutive software releases. The same tool thus allowed an empirical analysis of software development across versions from the perspective of those transformations. We describe results for the systems analysed and point to key conclusions from our analysis. In particular, we investigate a problematic empirical question as to whether certain refactorings are related, i.e., they cannot be undertaken in isolation without other refactorings being undertaken in parallel. In this context, we focus specifically on the four most common refactorings identified by the tool from three of the opensource systems and use a dependency graph to inform conclusions about the empirical data extracted by the tool. An interesting result relating to some common refactorings is described.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 2006 {ACM} symposium on {Applied} computing  - {SAC} '06},
	publisher = {ACM Press},
	author = {Advani, Deepak and Hassoun, Youssef and Counsell, Steve},
	year = {2006},
	pages = {1713},
}

@article{sculley_machine_nodate,
	title = {Machine {Learning}: {The} {High}-{Interest} {Credit} {Card} of {Technical} {Debt}},
	abstract = {Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning speciﬁc risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
	language = {en},
	author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
	pages = {9},
}

@inproceedings{velez_challenges_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Challenges in migrating imperative deep learning programs to graph execution: an empirical study},
	isbn = {978-1-4503-9303-4},
	shorttitle = {Challenges in migrating imperative deep learning programs to graph execution},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528455},
	doi = {10.1145/3524842.3528455},
	abstract = {Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. While hybrid approaches aim for the “best of both worlds,” the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges—and resultant bugs—involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation—the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Vélez, Tatiana Castro and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Raja, Anita},
	month = may,
	year = {2022},
	pages = {469--481},
}

@inproceedings{dilhara_discovering_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Discovering repetitive code changes in python {ML} systems},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510225},
	doi = {10.1145/3510003.3510225},
	abstract = {Over the years, researchers capitalized on the repetitiveness of software changes to automate many software evolution tasks. Despite the extraordinary rise in popularity of Python-based ML systems, they do not benefit from these advances. Without knowing what are the repetitive changes that ML developers make, researchers, tool, and library designers miss opportunities for automation, and ML developers fail to learn and use best coding practices.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Dilhara, Malinda and Ketkar, Ameya and Sannidhi, Nikhith and Dig, Danny},
	month = may,
	year = {2022},
	pages = {736--748},
}

@inproceedings{li_scaling_2014,
	address = {Beijing, China},
	title = {Scaling {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	isbn = {978-1-4503-2891-3},
	url = {http://dl.acm.org/citation.cfm?doid=2640087.2644155},
	doi = {10.1145/2640087.2644155},
	abstract = {We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports ﬂexible consistency models, elastic scalability, and continuous fault tolerance.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Big} {Data} {Science} and {Computing} - {BigDataScience} '14},
	publisher = {ACM Press},
	author = {Li, Mu},
	year = {2014},
	pages = {1--1},
}

@inproceedings{tang_empirical_2021,
	title = {An {Empirical} {Study} of {Refactorings} and {Technical} {Debt} in {Machine} {Learning} {Systems}},
	doi = {10.1109/ICSE43902.2021.00033},
	abstract = {Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major cross-cutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Best practices, Complex systems, Data mining, Deep learning, Open source software, Software engineering, Tools, empirical studies, machine learning systems, refactoring, software evolution, software repository mining, technical debt},
	pages = {238--250},
}

@inproceedings{Ross2018RefactoringML,
	title = {Refactoring {Machine} {Learning}},
	abstract = {Results in machine learning scholarship are sometimes based on untested, difﬁcultto-read code that has only been seen by a single researcher. We argue that this is bad, and that machine learning scholarship could be improved by adhering to software development best practices. We identify several practices which are not widely followed within the research community but we believe would help improve the reliability of results. We describe how to apply these best practices in a machine learning research setting. Finally, we suggest several modiﬁcations to the publication review process to encourage adherence.},
	language = {en},
	booktitle = {Workshop on {Critiquing} and {Correcting} {Trends} in {Machine} {Learning} at {NeuRIPS}},
	author = {Ross, Andrew Slavin and Forde, Jessica Zosa},
	year = {2018},
	pages = {6},
}

@article{olah_research_2017,
	title = {Research {Debt}},
	volume = {2},
	issn = {2476-0757},
	url = {https://distill.pub/2017/research-debt},
	doi = {10.23915/distill.00005},
	abstract = {Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...},
	language = {en},
	number = {3},
	urldate = {2022-12-05},
	journal = {Distill},
	author = {Olah, Chris and Carter, Shan},
	month = mar,
	year = {2017},
	pages = {e5},
}

@article{smith_collaborative_nodate,
	title = {Collaborative, open, and automated data science},
	abstract = {Data science and machine learning have already revolutionized many industries and organizations and are increasingly being used in an open-source setting to address important societal problems. However, there remain many challenges to developing predictive machine learning models in practice, such as the complexity of the steps in the modern data science development process, the involvement of many different people with varying skills and roles, and the necessity of, yet difficulty in, collaborating across steps and people. In this thesis, I describe progress in two directions in supporting the development of predictive models.},
	language = {en},
	author = {Smith, Micah J},
	pages = {187},
}

@article{smith_enabling_2021,
	title = {Enabling {Collaborative} {Data} {Science} {Development} with the {Ballet} {Framework}},
	volume = {5},
	issn = {2573-0142},
	url = {https://dl.acm.org/doi/10.1145/3479575},
	doi = {10.1145/3479575},
	abstract = {While the open-source software development model has led to successful large-scale collaborations in building software systems, data science projects are frequently developed by individuals or small teams. We describe challenges to scaling data science collaborations and present a conceptual framework and ML programming model to address them. We instantiate these ideas in Ballet, the first lightweight framework for collaborative, open-source data science through a focus on feature engineering, and an accompanying cloud-based development environment. Using our framework, collaborators incrementally propose feature definitions to a repository which are each subjected to software and ML performance validation and can be automatically merged into an executable feature engineering pipeline. We leverage Ballet to conduct a case study analysis of an income prediction problem with 27 collaborators, and discuss implications for future designers of collaborative projects.},
	language = {en},
	number = {CSCW2},
	urldate = {2022-12-02},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Smith, Micah J. and Cito, Jürgen and Lu, Kelvin and Veeramachaneni, Kalyan},
	month = oct,
	year = {2021},
	pages = {1--39},
}

@inproceedings{van_der_weide_versioning_2017,
	address = {Chicago IL USA},
	title = {Versioning for {End}-to-{End} {Machine} {Learning} {Pipelines}},
	isbn = {978-1-4503-5026-6},
	url = {https://dl.acm.org/doi/10.1145/3076246.3076248},
	doi = {10.1145/3076246.3076248},
	abstract = {End-to-end machine learning pipelines that run in shared environments are challenging to implement. Production pipelines typically consist of multiple interdependent processing stages. Between stages, the intermediate results are persisted to reduce redundant computation and to improve robustness. Those results might come in the form of datasets for data processing pipelines or in the form of model coefficients in case of model training pipelines. Reusing persisted results improves efficiency but at the same time creates complicated dependencies. Every time one of the processing stages is changed, either due to code change or due to parameters change, it becomes difficult to find which datasets can be reused and which should be recomputed.},
	language = {en},
	urldate = {2022-11-30},
	booktitle = {Proceedings of the 1st {Workshop} on {Data} {Management} for {End}-to-{End} {Machine} {Learning}},
	publisher = {ACM},
	author = {van der Weide, Tom and Papadopoulos, Dimitris and Smirnov, Oleg and Zielinski, Michal and van Kasteren, Tim},
	month = may,
	year = {2017},
	pages = {1--9},
}

@book{Fowler2018SWRefactoring,
	title = {Refactoring},
	isbn = {978-0-13-475770-4},
	abstract = {Fully Revised and Updated–Includes New Refactorings and Code Examples  “Any fool can write code that a computer can understand. Good programmers write code that humans can understand.”  —M. Fowler (1999)For more than twenty years, experienced programmers worldwide have relied on Martin Fowler’s Refactoring to improve the design of existing code and to enhance software maintainability, as well as to make existing code easier to understand.    This eagerly awaited new edition has been fully updated to reflect crucial changes in the programming landscape.  Refactoring, Second Edition,  features an updated catalog of refactorings and includes JavaScript code examples, as well as new functional examples that demonstrate refactoring without classes.    Like the original, this edition explains what refactoring is; why you should refactor; how to recognize code that needs refactoring; and how to actually do it successfully, no matter what language you use.  Understand the process and general principles of refactoring Quickly apply useful refactorings to make a program easier to comprehend and change Recognize “bad smells” in code that signal opportunities to refactor Explore the refactorings, each with explanations, motivation, mechanics, and simple examples Build solid tests for your refactorings Recognize tradeoffs and obstacles to refactoring   Includes free access to the canonical web edition, with even more refactoring resources. (See inside the book for details about how to access the web edition.)},
	language = {en},
	publisher = {Addison-Wesley Professional},
	author = {Fowler, Martin},
	month = nov,
	year = {2018},
	keywords = {Computers / Programming / Object Oriented, Computers / Software Development \& Engineering / General},
}

@article{Opdyke1992Refactoring,
	title = {Refactoring {Object}-{Oriented} {Frameworks}},
	language = {en},
	author = {Opdyke, William},
	year = {1992},
	pages = {206},
}

@article{Mesfin2014SWRefactoringLitReview,
	title = {Trends, {Opportunities} and {Challenges} of {Software} {Refactoring}: {A} {Systematic} {Literature} {Review}},
	abstract = {Software refactoring is a technique that transforms the various types of software artifacts to improve the software internal structure without affecting the external behavior. Refactoring is commonly applied to improve the software quality after a significant amount of features are added. Researchers in the area have studied the different angles of refactoring and developed the right evidence, knowledge and skill. And they published their research findings through journals and conference papers to provide an easy access to everyone. Eventually, the knowledge accumulated in these literatures is huge, so that it needs structuring and organizing. The main purpose of this study is to extend a previously conducted study by covering more literatures and applying a systematic literature review method to increase the accuracy and validity of the study. We study a collection of literature from different electronic databases, published since 1999 to understand and extract the software refactoring knowledge through classification and summarization. The classification and summarization can reveal the research pattern, common concerns and statistics of the published papers in the last fifteen years. The extracted information should help the researchers to formulate better research topics that can solve the crucial problems in software refactoring and save the researchers effort and time.},
	language = {en},
	journal = {International Journal of Software Engineering and Its Applications},
	author = {Abebe, Mesfin and Yoo, Cheol-Jung},
	year = {2014},
	pages = {20},
}

@article{Ward1992TechDebt,
	title = {The {WyCash} portfolio management system},
	volume = {ACM SIGPLAN OOPS Messenger},
	language = {en},
	author = {Cunningham, Ward},
	year = {1992},
	pages = {29--30},
}

@incollection{viale_pereira_technical_2020,
	address = {Cham},
	title = {Technical {Debt} {Management}: {A} {Systematic} {Literature} {Review} and {Research} {Agenda} for {Digital} {Government}},
	volume = {12219},
	isbn = {978-3-030-57598-4 978-3-030-57599-1},
	shorttitle = {Technical {Debt} {Management}},
	url = {https://link.springer.com/10.1007/978-3-030-57599-1_10},
	abstract = {Technical debt is created when software engineers knowingly or unknowingly introduce shortcuts or unsuitable choices in the development or maintenance of the software system, that will have a negative impact on the future evolution of the system until corrected. Therefore, it is crucial to manage established debt particular in the public sector. The aim of this study is to introduce Technical debt to the field of Digital Government. We create an overview of the state of the art of the knowledge on technical debt management, the methods applied to gain this knowledge, and propose a research agenda to Digital Government scholars. We conduct a systematic literature review, which focuses on the concept of technical debt management. Forty-nine papers published within 2017-2020 are selected and analyzed. We identify several gaps in the existing literature: 1) an absence of theory explaining the relation of events, 2) a shortage of studies conducted in the public sector, 3) and an absence of specific techniques such as observation to study actual technical debt management behavior.},
	language = {en},
	urldate = {2022-11-29},
	booktitle = {Electronic {Government}},
	publisher = {Springer International Publishing},
	author = {Nielsen, Mille Edith and Østergaard Madsen, Christian and Lungu, Mircea Filip},
	editor = {Viale Pereira, Gabriela and Janssen, Marijn and Lee, Habin and Lindgren, Ida and Rodríguez Bolívar, Manuel Pedro and Scholl, Hans Jochen and Zuiderwijk, Anneke},
	year = {2020},
	doi = {10.1007/978-3-030-57599-1_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {121--137},
}

@incollection{Viale2022TDManagement,
	address = {Cham},
	title = {Technical {Debt} {Management}: {A} {Systematic} {Literature} {Review} and {Research} {Agenda} for {Digital} {Government}},
	volume = {12219},
	isbn = {978-3-030-57598-4 978-3-030-57599-1},
	shorttitle = {Technical {Debt} {Management}},
	url = {https://link.springer.com/10.1007/978-3-030-57599-1_10},
	abstract = {Technical debt is created when software engineers knowingly or unknowingly introduce shortcuts or unsuitable choices in the development or maintenance of the software system, that will have a negative impact on the future evolution of the system until corrected. Therefore, it is crucial to manage established debt particular in the public sector. The aim of this study is to introduce Technical debt to the field of Digital Government. We create an overview of the state of the art of the knowledge on technical debt management, the methods applied to gain this knowledge, and propose a research agenda to Digital Government scholars. We conduct a systematic literature review, which focuses on the concept of technical debt management. Forty-nine papers published within 2017-2020 are selected and analyzed. We identify several gaps in the existing literature: 1) an absence of theory explaining the relation of events, 2) a shortage of studies conducted in the public sector, 3) and an absence of specific techniques such as observation to study actual technical debt management behavior.},
	language = {en},
	urldate = {2022-11-29},
	booktitle = {Electronic {Government}},
	publisher = {Springer International Publishing},
	author = {Nielsen, Mille Edith and Østergaard Madsen, Christian and Lungu, Mircea Filip},
	editor = {Viale Pereira, Gabriela and Janssen, Marijn and Lee, Habin and Lindgren, Ida and Rodríguez Bolívar, Manuel Pedro and Scholl, Hans Jochen and Zuiderwijk, Anneke},
	year = {2020},
	pages = {121--137},
}

@inproceedings{Ernst2015SWPractitionersandTechnicalDebt,
	address = {Bergamo Italy},
	title = {Measure it? {Manage} it? {Ignore} it? software practitioners and technical debt},
	isbn = {978-1-4503-3675-8},
	shorttitle = {Measure it?},
	url = {https://dl.acm.org/doi/10.1145/2786805.2786848},
	doi = {10.1145/2786805.2786848},
	abstract = {The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches.},
	language = {en},
	urldate = {2022-03-02},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ernst, Neil A. and Bellomo, Stephany and Ozkaya, Ipek and Nord, Robert L. and Gorton, Ian},
	month = aug,
	year = {2015},
	pages = {50--60},
}

@inproceedings{hempel_d_2018,
	address = {Gothenburg Sweden},
	title = {D {\textless}span style="font-variant:small-caps;"{\textgreater}euce{\textless}/span{\textgreater}: a lightweight user interface for structured editing},
	isbn = {978-1-4503-5638-1},
	shorttitle = {D {\textless}span style="font-variant},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180165},
	doi = {10.1145/3180155.3180165},
	abstract = {We present a structure-aware code editor, called Deuce, that is equipped with direct manipulation capabilities for invoking automated program transformations. Compared to traditional refactoring environments, Deuce employs a direct manipulation interface that is tightly integrated within a text-based editing workflow. In particular, Deuce draws (i) clickable widgets atop the source code that allow the user to structurally select the unstructured text for subexpressions and other relevant features, and (ii) a lightweight, interactive menu of potential transformations based on the current selections. We implement and evaluate our design with mostly standard transformations in the context of a small functional programming language. A controlled user study with 21 participants demonstrates that structural selection is preferred to a more traditional text-selection interface and may be faster overall once users gain experience with the tool. These results accord with Deuce’s aim to provide human-friendly structural interactions on top of familiar text-based editing.},
	language = {en},
	urldate = {2022-11-29},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Hempel, Brian and Lubin, Justin and Lu, Grace and Chugh, Ravi},
	month = may,
	year = {2018},
	pages = {654--664},
}

@misc{aniche_effectiveness_2020,
	title = {The {Effectiveness} of {Supervised} {Machine} {Learning} {Algorithms} in {Predicting} {Software} {Refactoring}},
	url = {http://arxiv.org/abs/2001.03338},
	abstract = {Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers' expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90\%. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts.},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Aniche, Maurício and Maziero, Erick and Durelli, Rafael and Durelli, Vinicius},
	month = sep,
	year = {2020},
	note = {arXiv:2001.03338 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{bokhari_deep_2017,
	address = {Berlin Germany},
	title = {Deep parameter optimisation on {Android} smartphones for energy minimisation: a tale of woe and a proof-of-concept},
	isbn = {978-1-4503-4939-0},
	shorttitle = {Deep parameter optimisation on {Android} smartphones for energy minimisation},
	url = {https://dl.acm.org/doi/10.1145/3067695.3082519},
	doi = {10.1145/3067695.3082519},
	abstract = {With power demands of mobile devices rising, it is becoming increasingly important to make mobile so ware applications more energy e cient. Unfortunately, mobile platforms are diverse and very complex which makes energy behaviours di cult to model. is complexity presents challenges to the e ectiveness of o -line optimisation of mobile applications. In this paper, we demonstrate that it is possible to automatically optimise an application for energy on a mobile device by evaluating energy consumption in-vivo. In contrast to previous work, we use only the device’s own internal meter. Our approach involves many technical challenges but represents a realistic path toward learning hardware speci c energy models for program code features.},
	language = {en},
	urldate = {2022-11-04},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {ACM},
	author = {Bokhari, Mahmoud A. and Bruce, Bobby R. and Alexander, Brad and Wagner, Markus},
	month = jul,
	year = {2017},
	pages = {1501--1508},
}

@inproceedings{xu_empirical_2022,
	address = {Honolulu, HI, USA},
	title = {An {Empirical} {Study} on the {Impact} of {Deep} {Parameters} on {Mobile} {App} {Energy} {Usage}},
	isbn = {978-1-66543-786-8},
	url = {https://ieeexplore.ieee.org/document/9825870/},
	doi = {10.1109/SANER53432.2022.00103},
	abstract = {Improving software performance through conﬁguration parameter tuning is a common activity during software maintenance. Beyond traditional performance metrics like latency, mobile app developers are interested in reducing app energy usage. Some mobile apps have centralized locations for parameter tuning, similar to databases and operating systems, but it is common for mobile apps to have hundreds of parameters scattered around the source code. The correlation between these “deep” parameters and app energy usage is unclear. Researchers have studied the energy effects of deep parameters in speciﬁc modules, but we lack a systematic understanding of the energy impact of mobile deep parameters.},
	language = {en},
	urldate = {2022-10-31},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	publisher = {IEEE},
	author = {Xu, Qiang and Davis, James C. and Hu, Y. Charlie and Jindal, Abhilash},
	month = mar,
	year = {2022},
	pages = {844--855},
}

@article{Zaharia2018MLFlow,
	title = {Accelerating the {Machine} {Learning} {Lifecycle} with {MLﬂow}},
	abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLﬂow, an open source platform we recently launched to streamline the machine learning lifecycle. MLﬂow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
	language = {en},
	author = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
	pages = {7},
}

@inproceedings{Vartak2022ModelDB,
	address = {San Francisco, California},
	title = {{ModelDB}: a system for machine learning model management},
	isbn = {978-1-4503-4207-0},
	shorttitle = {M {\textless}span style="font-variant},
	url = {http://dl.acm.org/citation.cfm?doid=2939502.2939516},
	doi = {10.1145/2939502.2939516},
	abstract = {Building a machine learning model is an iterative process. A data scientist will build many tens to hundreds of models before arriving at one that meets some acceptance criteria (e.g. AUC cutoff, accuracy threshold). However, the current style of model building is ad-hoc and there is no practical way for a data scientist to manage models that are built over time. As a result, the data scientist must attempt to “remember” previously constructed models and insights obtained from them. This task is challenging for more than a handful of models and can hamper the process of sensemaking. Without a means to manage models, there is no easy way for a data scientist to answer questions such as “Which models were built using an incorrect feature?”, “Which model performed best on American customers?” or “How did the two top models compare?” In this paper, we describe our ongoing work on ModelDB, a novel end-to-end system for the management of machine learning models. ModelDB clients automatically track machine learning models in their native environments (e.g. scikit-learn, spark.ml), the ModelDB backend introduces a common layer of abstractions to represent models and pipelines, and the ModelDB frontend allows visual exploration and analyses of models via a web-based interface.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics} - {HILDA} '16},
	publisher = {ACM Press},
	author = {Vartak, Manasi and Subramanyam, Harihar and Lee, Wei-En and Viswanathan, Srinidhi and Husnoo, Saadiyah and Madden, Samuel and Zaharia, Matei},
	year = {2016},
	pages = {1--3},
}

@inproceedings{Sculley2015HiddenTechnicalDebtinMLSystems,
	title = {Hidden {Technical} {Debt} in {Machine} {Learning} {Systems}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html},
	abstract = {Machine learning offers a fantastically powerful toolkit for building useful complexprediction systems quickly. This paper argues it is dangerous to think ofthese quick wins as coming for free. Using the software engineering frameworkof technical debt, we find it is common to incur massive ongoing maintenancecosts in real-world ML systems. We explore several ML-specific risk factors toaccount for in system design. These include boundary erosion, entanglement,hidden feedback loops, undeclared consumers, data dependencies, configurationissues, changes in the external world, and a variety of system-level anti-patterns.},
	urldate = {2022-10-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan},
	year = {2015},
}

@inproceedings{Breck2017MLtestscore,
	title = {The {ML} test score: {A} rubric for {ML} production readiness and technical debt reduction},
	shorttitle = {The {ML} test score},
	doi = {10.1109/BigData.2017.8258038},
	abstract = {Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large offline research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems. But it can be difficult to formulate specific tests, given that the actual prediction behavior of any given model is difficult to specify a priori. In this paper, we present 28 specific tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D.},
	month = dec,
	year = {2017},
	keywords = {Best Practices, Data models, Machine Learning, Measurement, Monitoring, Production, Reliability, Technical Debt, Testing, Training},
	pages = {1123--1132},
}

@techreport{vanOort2022ProjectsmellsExperiencesinAnalysingtheSWQualityofMLProjectswithMllint,
	title = {"{Project} smells" -- {Experiences} in {Analysing} the {Software} {Quality} of {ML} {Projects} with mllint},
	url = {http://arxiv.org/abs/2201.08246},
	abstract = {Machine Learning (ML) projects incur novel challenges in their development and productionisation over traditional software applications, though established principles and best practices in ensuring the project's software quality still apply. While using static analysis to catch code smells has been shown to improve software quality attributes, it is only a small piece of the software quality puzzle, especially in the case of ML projects given their additional challenges and lower degree of Software Engineering (SE) experience in the data scientists that develop them. We introduce the novel concept of project smells which consider deficits in project management as a more holistic perspective on software quality in ML projects. An open-source static analysis tool mllint was also implemented to help detect and mitigate these. Our research evaluates this novel concept of project smells in the industrial context of ING, a global bank and large software- and data-intensive organisation. We also investigate the perceived importance of these project smells for proof-of-concept versus production-ready ML projects, as well as the perceived obstructions and benefits to using static analysis tools such as mllint. Our findings indicate a need for context-aware static analysis tools, that fit the needs of the project at its current stage of development, while requiring minimal configuration effort from the user.},
	number = {arXiv:2201.08246},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {van Oort, Bart and Cruz, Luís and Loni, Babak and van Deursen, Arie},
	month = jan,
	year = {2022},
	keywords = {68-06, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{Ma2021AntiPatternsinPythonCodebases,
	address = {Chicago IL USA},
	title = {“{You} have said too much”: {Java}-like verbosity anti-patterns in {Python} codebases},
	isbn = {978-1-4503-9089-7},
	shorttitle = {“{You} have said too much”},
	url = {https://dl.acm.org/doi/10.1145/3484272.3484960},
	doi = {10.1145/3484272.3484960},
	abstract = {As a popular language for teaching introductory programming, Java can profoundly influence beginner programmers with its coding style and idioms. Despite its many advantages, the paradigmatic coding style in Java is often described as verbose. As a result, when writing code in more concise languages, such programmers tend to emulate the familiar Java coding idioms, thus neglecting to take advantage of the more succinct counterparts in those languages. As a result of such verbosity, not only the overall code quality suffers, but the verbose non-idiomatic patterns also render code hard to understand and maintain. In this paper, we study the incidences of Java-like verbosity as they occur in Python codebases. We present a collection of Java-Like Verbosity Anti-patterns and our pilot study of their presence in representative open-source Python codebases. We discuss our findings as a call for action to computing educators, particularly those who work with introductory students. We need novel pedagogical interventions that encourage budding programmers to write concise idiomatic code in any language.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the 2021 {ACM} {SIGPLAN} {International} {Symposium} on {SPLASH}-{E}},
	publisher = {ACM},
	author = {Ma, Yuzhi and Tilevich, Eli},
	month = oct,
	year = {2021},
	pages = {13--18},
}

@inproceedings{ma_you_2021,
	address = {Chicago IL USA},
	title = {“{You} have said too much”: {Java}-like verbosity anti-patterns in {Python} codebases},
	isbn = {978-1-4503-9089-7},
	shorttitle = {“{You} have said too much”},
	url = {https://dl.acm.org/doi/10.1145/3484272.3484960},
	doi = {10.1145/3484272.3484960},
	abstract = {As a popular language for teaching introductory programming, Java can profoundly influence beginner programmers with its coding style and idioms. Despite its many advantages, the paradigmatic coding style in Java is often described as verbose. As a result, when writing code in more concise languages, such programmers tend to emulate the familiar Java coding idioms, thus neglecting to take advantage of the more succinct counterparts in those languages. As a result of such verbosity, not only the overall code quality suffers, but the verbose non-idiomatic patterns also render code hard to understand and maintain. In this paper, we study the incidences of Java-like verbosity as they occur in Python codebases. We present a collection of Java-Like Verbosity Anti-patterns and our pilot study of their presence in representative open-source Python codebases. We discuss our findings as a call for action to computing educators, particularly those who work with introductory students. We need novel pedagogical interventions that encourage budding programmers to write concise idiomatic code in any language.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the 2021 {ACM} {SIGPLAN} {International} {Symposium} on {SPLASH}-{E}},
	publisher = {ACM},
	author = {Ma, Yuzhi and Tilevich, Eli},
	month = oct,
	year = {2021},
	pages = {13--18},
}

@inproceedings{li_testing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Testing machine learning systems in industry: an empirical study},
	isbn = {978-1-4503-9226-6},
	shorttitle = {Testing machine learning systems in industry},
	url = {https://dl.acm.org/doi/10.1145/3510457.3513036},
	doi = {10.1145/3510457.3513036},
	language = {en},
	urldate = {2022-10-24},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Li, Shuyue and Guo, Jiaqi and Lou, Jian-Guang and Fan, Ming and Liu, Ting and Zhang, Dongmei},
	month = may,
	year = {2022},
	pages = {263--272},
}

@article{wang_yolov7_nodate,
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutionalbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https:// github.com/ WongKinYiu/ yolov7.},
	language = {en},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	pages = {17},
}

@article{noauthor_extraction_nodate,
	title = {Extraction and {Management} of {Rationale}},
	language = {en},
	pages = {3},
}

@article{noauthor_extraction_nodate-1,
	title = {Extraction and {Management} of {Rationale}},
	language = {en},
	pages = {3},
}

@inproceedings{gharibi_modelkb_2022,
	title = {{ModelKB}: {Towards} {Automated} {Management} of the {Modeling} {Lifecycle} in {Deep} {Learning}},
	urldate = {2022-10-06},
	booktitle = {International {Workshop} on {Realizing} {Artificial} {Intelligence} {Synergies} in {Software} {Engineering} ({RAISE})},
	publisher = {arXiv},
	author = {Gharibi, Gharib and Walunj, Vijay and Rella, Sirisha and Lee, Yugyung},
	month = sep,
	year = {2022},
	note = {CitationKey: Gharibi2022ModelKB},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{Gharibi2019ModelKB,
	title = {{ModelKB}: {Towards} {Automated} {Management} of the {Modeling} {Lifecycle} in {Deep} {Learning}},
	shorttitle = {{ModelKB}},
	doi = {10.1109/RAISE.2019.00013},
	abstract = {Deep Learning has improved the state-of-the-art results in an ever-growing number of domains. This success heavily relies on the development and training of deep learning models, also known as deep neural networks (DNN). Often, developing a DNN is an ad-hoc, iterative process that results in producing tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of tools and frameworks that aim at facilitating deep learning, the issues of model management have been largely ignored. In particular, deep learning practitioners have to manually track their experiments using text files, spreadsheets or folder hierarchies, which is expensive, time-consuming, and error-prone. In this paper, we present our ongoing work and vision towards automating end-to-end model management in deep learning. Specifically, we introduce a tool prototype, named ModelKB, that can automatically (1) extract and store the model's metadata-including its architecture, weights, and configuration; (2) visualize, query, and compare experiments; and (3) reproduce experiments. Our overarching goal is to automate the model management process with minimal user intervention using the user's favorite framework. We report the current status of ModelKB, a pilot user study, and the challenges of automating model management in deep learning.},
	booktitle = {2019 {IEEE}/{ACM} 7th {International} {Workshop} on {Realizing} {Artificial} {Intelligence} {Synergies} in {Software} {Engineering} ({RAISE})},
	author = {Gharibi, Gharib and Walunj, Vijay and Rella, Sirisha and Lee, Yugyung},
	month = may,
	year = {2019},
	keywords = {Computational modeling, Deep learning, Measurement, Metadata, Software, Training, data management, deep learning, software engineering},
	pages = {28--34},
}

@article{Amazon2017AutomaticallyTrackingMetadataandProvenanceofMLExperiments,
	title = {Automatically {Tracking} {Metadata} and {Provenance} of {Machine} {Learning} {Experiments}},
	volume = {Long Beach},
	abstract = {We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workﬂow, and provides a basis for comparability and repeatability of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.},
	language = {en},
	journal = {NIPS Machine Learning Systems Workshop},
	author = {Schelter, Sebastian and Böse, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
	year = {2017},
	pages = {8},
}

@article{Benton2020MLSystemsandIntelligentApplications,
	title = {Machine {Learning} {Systems} and {Intelligent} {Applications}},
	volume = {37},
	issn = {1937-4194},
	doi = {10.1109/MS.2020.2985224},
	abstract = {Machine learning techniques are useful in a wide range of contexts, but techniques alone are insufficient to solve real business problems. We introduce the intelligent applications concept, which characterizes the structure and responsibilities of contemporary machine learning systems.},
	number = {4},
	journal = {IEEE Software},
	author = {Benton, William C.},
	month = jul,
	year = {2020},
	keywords = {Artificial Intelligence, Data models, Feature extraction, Machine learning, Pipelines, Task analysis, Training data, distributed applications, machine learning},
	pages = {43--49},
}

@misc{Giray2021SEonEngineeringMLSystems,
	title = {A {Software} {Engineering} {Perspective} on {Engineering} {Machine} {Learning} {Systems}: {State} of the {Art} and {Challenges}},
	shorttitle = {A {Software} {Engineering} {Perspective} on {Engineering} {Machine} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2012.07919},
	abstract = {Context: Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems. Method: I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions. Conclusion: The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Giray, Görkem},
	month = jun,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, D.2.0},
}

@article{souza_workflow_2022,
	title = {Workflow provenance in the lifecycle of scientific machine learning},
	volume = {34},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6544},
	doi = {10.1002/cpe.6544},
	abstract = {Machine learning (ML) has already fundamentally changed several businesses. More recently, it has also been profoundly impacting the computational science and engineering domains, like geoscience, climate science, and health science. In these domains, users need to perform comprehensive data analyses combining scientific data and ML models to provide for critical requirements, such as reproducibility, model explainability, and experiment data understanding. However, scientific ML is multidisciplinary, heterogeneous, and affected by the physical constraints of the domain, making such analyses even more challenging. In this work, we leverage workflow provenance techniques to build a holistic view to support the lifecycle of scientific ML. We contribute with (i) characterization of the lifecycle and taxonomy for data analyses; (ii) design decisions to build this view, with a W3C PROV compliant data representation and a reference system architecture; and (iii) lessons learned after an evaluation in an Oil \& Gas case using an HPC cluster with 393 nodes and 946 GPUs. The experiments show that the decisions enable queries that integrate domain semantics with ML models while keeping low overhead ({\textless}1\%), high scalability, and an order of magnitude of query acceleration under certain workloads against without our representation.},
	language = {en},
	number = {14},
	urldate = {2022-10-16},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Souza, Renan and Azevedo, Leonardo G. and Lourenço, Vítor and Soares, Elton and Thiago, Raphael and Brandão, Rafael and Civitarese, Daniel and Vital Brazil, Emilio and Moreno, Marcio and Valduriez, Patrick and Mattoso, Marta and Cerqueira, Renato and Netto, Marco A. S.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.6544},
	keywords = {artificial intelligence, e-Science, explainability, lineage, machine learning lifecycle, provenance, reproducibility, scientific machine learning, scientific workflow, taxonomy},
	pages = {e6544},
}

@article{biessmann_automated_nodate,
	title = {Automated {Data} {Validation} in {Machine} {Learning} {Systems}},
	abstract = {Machine Learning (ML) algorithms are a standard component of modern software systems. The validation of data ingested and produced by ML components has become a central challenge in the deployment and maintenance of ML systems. Subtle changes in the input data can result in unpredictable behavior of an ML algorithm that can lead to unreliable or unfair ML predictions. Responsible usage of ML components thus requires well calibrated and scalable data validation systems. Here, we highlight some challenges associated with data validation in ML systems. We review some of the solutions developed to validate data at the various stages of a data pipeline in modern ML systems, discuss their strengths and weaknesses and assess to what extent these solutions are being used in practice. The research reviewed indicates that the increasing need for data validation in ML systems has driven enormous progress in an emerging community of ML and Data Base Management Systems (DBMS) researchers. While this research has led to a number of technical solutions we ﬁnd that many ML systems deployed in industrial applications are not leveraging the full potential of data validation in practice. The reasons for this are not only technical challenges, but there are also cultural, ethical and legal aspects that need to be taken into account when building data validation solutions for ML systems. We identify the lack of automation in data validation as one of the key factors slowing down adoption of validation solutions and translation of research into useful and robust ML applications. We conclude with an outlook on research directions at the intersection of ML and DBMS research to improve the development, deployment and maintenance of ML systems.},
	language = {en},
	author = {Biessmann, Felix and Golebiowski, Jacek and Rukat, Tammo and Lange, Dustin and Schmidt, Philipp},
	pages = {14},
}

@article{gharibi_automated_2021,
	title = {Automated end-to-end management of the modeling lifecycle in deep learning},
	volume = {26},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-020-09894-9},
	doi = {10.1007/s10664-020-09894-9},
	abstract = {Deep learning has improved the state-of-the-art results in an ever-growing number of domains. This success heavily relies on the development and training of deep learning models–an experimental, iterative process that produces tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of tools and frameworks that aim at facilitating deep learning, the process of managing the models and their artifacts is still surprisingly challenging and time-consuming. Existing modelmanagement solutions are either tailored for commercial platforms or require significant code changes. Moreover, most of the existing solutions address a single phase of the modeling lifecycle, such as experiment monitoring, while ignoring other essential tasks, such as model deployment. In this paper, we present a software system to facilitate and accelerate the deep learning lifecycle, named ModelKB. ModelKB can automatically manage the modeling lifecycle end-to-end, including (1) monitoring and tracking experiments; (2) visualizing, searching for, and comparing models and experiments; (3) deploying models locally and on the cloud; and (4) sharing and publishing trained models. Moreover, our system provides a stepping-stone for enhanced reproducibility. ModelKB currently supports TensorFlow 2.0, Keras, and PyTorch, and it can be extended to other deep learning frameworks easily.},
	language = {en},
	number = {2},
	urldate = {2022-10-16},
	journal = {Empirical Software Engineering},
	author = {Gharibi, Gharib and Walunj, Vijay and Nekadi, Raju and Marri, Raj and Lee, Yugyung},
	month = mar,
	year = {2021},
	pages = {17},
}

@article{Amazon2018MLModelManagementChallenge,
	title = {On {Challenges} in {Machine} {Learning} {Model} {Management}},
	abstract = {The training, maintenance, deployment, monitoring, organization and documentation of machine learning (ML) models – in short model management – is a critical task in virtually all production ML use cases. Wrong model management decisions can lead to poor performance of a ML system and result in high maintenance cost. As both research on infrastructure as well as on algorithms is quickly evolving, there is a lack of understanding of challenges and best practices for ML model management. Therefore, this ﬁeld is receiving increased attention in recent years, both from the data management as well as from the ML community. In this paper, we discuss a selection of ML use cases, develop an overview over conceptual, engineering, and data-related challenges arising in the management of the corresponding ML models, and point out future research directions.},
	journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
	author = {Schelter, Sebastian and Biessmann, Felix and Januschowski, Tim and Salinas, David and Seufert, Stephan and Szarvas, Gyuri},
	year = {2018},
}

@article{schelter_automatically_nodate,
	title = {Automatically {Tracking} {Metadata} and {Provenance} of {Machine} {Learning} {Experiments}},
	abstract = {We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workﬂow, and provides a basis for comparability and repeatability of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.},
	language = {en},
	author = {Schelter, Sebastian and Böse, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
	pages = {8},
}

@article{martinez-fernandez_software_2022,
	title = {Software {Engineering} for {AI}-{Based} {Systems}: {A} {Survey}},
	volume = {31},
	issn = {1049-331X, 1557-7392},
	shorttitle = {Software {Engineering} for {AI}-{Based} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3487043},
	doi = {10.1145/3487043},
	abstract = {AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on
              Software Engineering (SE)
              approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.},
	language = {en},
	number = {2},
	urldate = {2022-10-16},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Martínez-Fernández, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan},
	month = apr,
	year = {2022},
	pages = {1--59},
}

@article{yang_complex_2022,
	title = {Complex {Python} {Features} in the {Wild}},
	abstract = {While Python is increasingly popular, program analysis tooling for Python is lagging. This is due, in part, to complex features of the Python language—features with difficult to understand and model semantics. Besides the “usual suspects”, reflection and dynamic execution, complex Python features include context managers, decorators, and generators, among others. This paper explores how often and in what ways developers use certain complex features. We analyze over 3 million Python files mined from GitHub to address three research questions: (i) How often do developers use certain complex Python features? (ii) In what ways do developers use these features? (iii) Does use of complex features increase or decrease over time? Our findings show that usage of dynamic features that pose a threat to static analysis is infrequent. On the other hand, usage of context managers and decorators is surprisingly widespread. Our actionable result is a list of Python features that any “minimal syntax” ought to handle in order to capture developers’ use of the Python language. We hope that understanding the usage of Python features will help tool-builders improve Python tools, which can in turn lead to more correct, secure, and performant Python code.},
	language = {en},
	author = {Yang, Yi and Milanova, Ana and Hirzel, Martin},
	year = {2022},
	pages = {12},
}

@article{Xie2011TestingandValidatingMLClassifiersbyMetamorphicTesting,
	title = {Testing and validating machine learning classifiers by metamorphic testing},
	volume = {84},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121210003213},
	doi = {10.1016/j.jss.2010.11.920},
	abstract = {Machine learning algorithms have provided core functionality to many application domains – such as bioinformatics, computational linguistics, etc. However, it is difﬁcult to detect faults in such applications because often there is no “test oracle” to verify the correctness of the computed outputs. To help address the software quality, in this paper we present a technique for testing the implementations of machine learning classiﬁcation algorithms which support such applications. Our approach is based on the technique “metamorphic testing”, which has been shown to be effective to alleviate the oracle problem. Also presented include a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufﬁciently effective to detect faults in a supervised classiﬁcation program. The effectiveness of metamorphic testing is further conﬁrmed by the detection of real faults in a popular open-source classiﬁcation program.},
	language = {en},
	number = {4},
	urldate = {2022-09-30},
	journal = {Journal of Systems and Software},
	author = {Xie, Xiaoyuan and Ho, Joshua W.K. and Murphy, Christian and Kaiser, Gail and Xu, Baowen and Chen, Tsong Yueh},
	month = apr,
	year = {2011},
	pages = {544--558},
}

@article{Chen2022DeepPerform,
	title = {{DeepPerform}: {An} {Efficient} {Approach} for {Performance} {Testing} of {Resource}-{Constrained} {Neural} {Networks}},
	abstract = {Today, an increasing number of Adaptive Deep Neural Networks (AdNNs) are being used on resource-constrained embedded devices. We observe that, similar to traditional software, redundant computation exists in AdNNs, resulting in considerable performance degradation. The performance degradation is dependent on the input and is referred to as input-dependent performance bottlenecks (IDPBs). To ensure an AdNN satisfies the performance requirements of resource-constrained applications, it is essential to conduct performance testing to detect IDPBs in the AdNN. Existing neural network testing methods are primarily concerned with correctness testing, which does not involve performance testing. To fill this gap, we propose DeepPerform, a scalable approach to generate test samples to detect the IDPBs in AdNNs. We first demonstrate how the problem of generating performance test samples detecting IDPBs can be formulated as an optimization problem. Following that, we demonstrate how DeepPerform efficiently handles the optimization problem by learning and estimating the distribution of AdNNs’ computational consumption. We evaluate DeepPerform on three widely used datasets against five popular AdNN models. The results show that DeepPerform generates test samples that cause more severe performance degradation (FLOPs: increase up to 552\%). Furthermore, DeepPerform is substantially more efficient than the baseline methods in generating test inputs (runtime overhead: only 6–10 milliseconds).},
	language = {en},
	author = {Chen, Simin and Haque, Mirazul and Liu, Cong and Yang, Wei},
	year = {2022},
	pages = {13},
}

@misc{chatterjee_testing_2022,
	title = {Testing of {Machine} {Learning} {Models} with {Limited} {Samples}: {An} {Industrial} {Vacuum} {Pumping} {Application}},
	shorttitle = {Testing of {Machine} {Learning} {Models} with {Limited} {Samples}},
	url = {http://arxiv.org/abs/2208.04062},
	abstract = {There is often a scarcity of training data for machine learning (ML) classification and regression models in industrial production, especially for time-consuming or sparsely run manufacturing processes. A majority of the limited ground-truth data is used for training, while a handful of samples are left for testing. Here, the number of test samples is inadequate to properly evaluate the robustness of the ML models under test for classification and regression. Furthermore, the output of these ML models may be inaccurate or even fail if the input data differ from the expected. This is the case for ML models used in the Electroslag Remelting (ESR) process in the refined steel industry to predict the pressure in a vacuum chamber. A vacuum pumping event that occurs once a workday generates a few hundred samples in a year of pumping for training and testing. In the absence of adequate training and test samples, this paper first presents a method to generate a fresh set of augmented samples based on vacuum pumping principles. Based on the generated augmented samples, three test scenarios and one test oracle are presented to assess the robustness of an ML model used for production on an industrial scale. Experiments are conducted with real industrial production data obtained from Uddeholms AB steel company. The evaluations indicate that Ensemble and Neural Network are the most robust when trained on augmented data using the proposed testing strategy. The evaluation also demonstrates the proposed method's effectiveness in checking and improving ML algorithms' robustness in such situations. The work improves software testing's state-of-the-art robustness testing in similar settings. Finally, the paper presents an MLOps implementation of the proposed approach for real-time ML model prediction and action on the edge node and automated continuous delivery of ML software from the cloud.},
	urldate = {2022-10-06},
	publisher = {arXiv},
	author = {Chatterjee, Ayan and Ahmed, Bestoun S. and Hallin, Erik and Engman, Anton},
	month = aug,
	year = {2022},
	note = {arXiv:2208.04062 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{zhang_toward_2022,
	title = {Toward {Improving} the {Robustness} of {Deep} {Learning} {Models} via {Model} {Transformation}},
	abstract = {Deep learning (DL) techniques have attracted much attention in recent years, and have been applied to many application scenarios, including those that are safety-critical. Improving the universal robustness of DL models is vital and many approaches have been proposed in the last decades aiming at such a purpose. Among existing approaches, adversarial training is the most representative. It advocates a post model tuning process via incorporating adversarial samples. Although successful, they still suffer from the challenge of generalizability issues in the face of various attacks with unsatisfactory effectiveness. Targeting this problem, in this paper we propose a novel model training framework, which aims at improving the universal robustness of DL models via model transformation incorporated with a data augmentation strategy in a delta debugging fashion. We have implemented our approach in a tool, called Dare, and conducted an extensive evaluation on 9 DL models. The results show that our approach significantly outperforms existing adversarial training techniques. Specifically, Dare has achieved the highest Empirical Robustness in 29 of 45 testing scenarios under various attacks, while the number drops to 5 of 45 for the best baseline approach.},
	language = {en},
	author = {Zhang, Yingyi and Wang, Zan and Jiang, Jiajun and You, Hanmo and Chen, Junjie},
	year = {2022},
	pages = {13},
}

@article{chen_maat_2022,
	title = {{MAAT}: {A} {Novel} {Ensemble} {Approach} to {Addressing} {Fairness} and {Performance} {Bugs} for {Machine} {Learning} {Software}},
	abstract = {Machine Learning (ML) software can lead to unfair and unethical decisions, making software fairness bugs an increasingly significant concern for software engineers. However, addressing fairness bugs often comes at the cost of introducing more ML performance (e.g., accuracy) bugs. In this paper, we propose MAAT, a novel ensemble approach to improving fairness-performance trade-off for ML software. Conventional ensemble methods combine different models with identical learning objectives. MAAT, instead, combines models optimized for different objectives: fairness and ML performance. We conduct an extensive evaluation of MAAT with 5 state-of-the-art methods, 9 software decision tasks, and 15 fairness-performance measurements. The results show that MAAT significantly outperforms the state-of-the-art. In particular, MAAT beats the trade-off baseline constructed by a recent benchmarking tool in 92.2\% of the overall cases evaluated, 12.2 percentage points more than the best technique currently available. Moreover, the superiority of MAAT over the state-of-the-art holds on all the tasks and measurements that we study. We have made publicly available the code and data of this work to allow for future replication and extension.},
	language = {en},
	author = {Chen, Zhenpeng and Sarro, Federica and Zhang, Jie M and Harman, Mark},
	year = {2022},
	pages = {13},
}

@misc{wang_no_2022,
	title = {No {More} {Fine}-{Tuning}? {An} {Experimental} {Evaluation} of {Prompt} {Tuning} in {Code} {Intelligence}},
	shorttitle = {No {More} {Fine}-{Tuning}?},
	url = {http://arxiv.org/abs/2207.11680},
	doi = {10.1145/3540250.3549113},
	abstract = {Pre-trained models have been shown effective in many code intelligence tasks. These models are pre-trained on large-scale unlabeled corpus and then fine-tuned in downstream tasks. However, as the inputs to pre-training and downstream tasks are in different forms, it is hard to fully explore the knowledge of pre-trained models. Besides, the performance of fine-tuning strongly relies on the amount of downstream data, while in practice, the scenarios with scarce data are common. Recent studies in the natural language processing (NLP) field show that prompt tuning, a new paradigm for tuning, alleviates the above issues and achieves promising results in various NLP tasks. In prompt tuning, the prompts inserted during tuning provide task-specific knowledge, which is especially beneficial for tasks with relatively scarce data. In this paper, we empirically evaluate the usage and effect of prompt tuning in code intelligence tasks. We conduct prompt tuning on popular pre-trained models CodeBERT and CodeT5 and experiment with three code intelligence tasks including defect prediction, code summarization, and code translation. Our experimental results show that prompt tuning consistently outperforms fine-tuning in all three tasks. In addition, prompt tuning shows great potential in low-resource scenarios, e.g., improving the BLEU scores of fine-tuning by more than 26{\textbackslash}\% on average for code summarization. Our results suggest that instead of fine-tuning, we could adapt prompt tuning for code intelligence tasks to achieve better performance, especially when lacking task-specific data.},
	urldate = {2022-10-06},
	author = {Wang, Chaozheng and Yang, Yuanhang and Gao, Cuiyun and Peng, Yun and Zhang, Hongyu and Lyu, Michael R.},
	month = jul,
	year = {2022},
	note = {arXiv:2207.11680 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{dutta_seed_2022,
	address = {Valencia, Spain},
	title = {To {Seed} or {Not} to {Seed}? {An} {Empirical} {Analysis} of {Usage} of {Seeds} for {Testing} in {Machine} {Learning} {Projects}},
	isbn = {978-1-66546-679-0},
	shorttitle = {To {Seed} or {Not} to {Seed}?},
	url = {https://ieeexplore.ieee.org/document/9787895/},
	doi = {10.1109/ICST53961.2022.00026},
	abstract = {Many Machine Learning (ML) algorithms are inherently random in nature – executing them using the same inputs may lead to slightly different results across different runs. Such randomness makes it challenging for developers to write tests for their implementations of ML algorithms. A natural consequence of randomness is test flakiness – tests both pass and fail non-deterministically for same version of code.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {2022 {IEEE} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	publisher = {IEEE},
	author = {Dutta, Saikat and Arunachalam, Anshul and Misailovic, Sasa},
	month = apr,
	year = {2022},
	pages = {151--161},
}

@article{kelley_framework_2021,
	title = {A framework for creating knowledge graphs of scientific software metadata},
	volume = {2},
	issn = {2641-3337},
	url = {https://direct.mit.edu/qss/article/2/4/1423/108045/A-framework-for-creating-knowledge-graphs-of},
	doi = {10.1162/qss_a_00167},
	abstract = {An increasing number of researchers rely on computational methods to generate or manipulate the results described in their scientific publications. Software created to this end—scientific software—is key to understanding, reproducing, and reusing existing work in many disciplines, ranging from Geosciences to Astronomy or Artificial Intelligence. However, scientific software is usually challenging to find, set up, and compare to similar software due to its disconnected documentation (dispersed in manuals, readme files, websites, and code comments) and the lack of structured metadata to describe it. As a result, researchers have to manually inspect existing tools to understand their differences and incorporate them into their work. This approach scales poorly with the number of publications and tools made available every year. In this paper we address these issues by introducing a framework for automatically extracting scientific software metadata from its documentation (in particular, their readme files); a methodology for structuring the extracted metadata in a Knowledge Graph (KG) of scientific software; and an exploitation framework for browsing and comparing the contents of the generated KG. We demonstrate our approach by creating a KG with metadata from over 10,000 scientific software entries from public code repositories.},
	language = {en},
	number = {4},
	urldate = {2022-09-30},
	journal = {Quantitative Science Studies},
	author = {Kelley, Aidan and Garijo, Daniel},
	month = dec,
	year = {2021},
	pages = {1423--1446},
}

@inproceedings{ponta_beyond_2018,
	title = {Beyond {Metadata}: {Code}-{Centric} and {Usage}-{Based} {Analysis} of {Known} {Vulnerabilities} in {Open}-{Source} {Software}},
	shorttitle = {Beyond {Metadata}},
	doi = {10.1109/ICSME.2018.00054},
	abstract = {The use of open-source software (OSS) is ever-increasing, and so is the number of open-source vulnerabilities being discovered and publicly disclosed. The gains obtained from the reuse of community-developed libraries may be offset by the cost of detecting, assessing, and mitigating their vulnerabilities in a timely manner. In this paper we present a novel method to detect, assess and mitigate OSS vulnerabilities that improves on state-of-the-art approaches, which commonly depend on metadata to identify vulnerable OSS dependencies. Our solution instead is code-centric and combines static and dynamic analysis to determine the reachability of the vulnerable portion of libraries used (directly or transitively) by an application. Taking this usage into account, our approach then supports developers in choosing among the existing non-vulnerable library versions. Vulas, the tool implementing our code-centric and usage-based approach, is officially recommended by SAP to scan its Java software, and has been successfully used to perform more than 250000 scans of about 500 applications since December 2016. We report on our experience and on the lessons we learned when maturing the tool from a research prototype to an industrial-grade solution.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Ponta, Serena Elisa and Plate, Henrik and Sabetta, Antonino},
	month = sep,
	year = {2018},
	note = {ISSN: 2576-3148},
	keywords = {Libraries, Metadata, Open source software, Silicon, Static analysis, Tools, code-centric vulnerability analysis, known vulnerabilities, metric-based update support, open source software},
	pages = {449--460},
}

@inproceedings{gil_ontosoft_2015,
	address = {Palisades NY USA},
	title = {{OntoSoft}: {Capturing} {Scientific} {Software} {Metadata}},
	isbn = {978-1-4503-3849-3},
	shorttitle = {{OntoSoft}},
	url = {https://dl.acm.org/doi/10.1145/2815833.2816955},
	doi = {10.1145/2815833.2816955},
	abstract = {This paper presents OntoSoft, an ontology to describe metadata for scientific software. The ontology is designed considering how scientists would approach the reuse and sharing of software. This includes supporting a scientist to: 1) identify software, 2) understand and assess software, 3) execute software, 4) get support for the software, 5) do research with the software, and 6) update the software. The ontology is available in OWL and contains more than fifty terms. We are using OntoSoft to structure a software registry for geosciences, and to develop user interfaces to capture its metadata.},
	language = {en},
	urldate = {2022-09-30},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {ACM},
	author = {Gil, Yolanda and Ratnakar, Varun and Garijo, Daniel},
	month = oct,
	year = {2015},
	pages = {1--4},
}

@misc{peebles_learning_2022,
	title = {Learning to {Learn} with {Generative} {Models} of {Neural} {Network} {Checkpoints}},
	url = {http://arxiv.org/abs/2209.12892},
	abstract = {We explore a data-driven approach for learning to optimize neural networks. We construct a dataset of neural network checkpoints and train a generative model on the parameters. In particular, our model is a conditional diffusion transformer that, given an initial input parameter vector and a prompted loss, error, or return, predicts the distribution over parameter updates that achieve the desired metric. At test time, it can optimize neural networks with unseen parameters for downstream tasks in just one update. We find that our approach successfully generates parameters for a wide range of loss prompts. Moreover, it can sample multimodal parameter solutions and has favorable scaling properties. We apply our method to different neural network architectures and tasks in supervised and reinforcement learning.},
	urldate = {2022-09-28},
	publisher = {arXiv},
	author = {Peebles, William and Radosavovic, Ilija and Brooks, Tim and Efros, Alexei A. and Malik, Jitendra},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12892 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wang_-target_2021,
	title = {On-target {Adaptation}},
	url = {http://arxiv.org/abs/2109.01087},
	abstract = {Domain adaptation seeks to mitigate the shift between training on the {\textbackslash}emph\{source\} domain and testing on the {\textbackslash}emph\{target\} domain. Most adaptation methods rely on the source data by joint optimization over source data and target data. Source-free methods replace the source data with a source model by fine-tuning it on target. Either way, the majority of the parameter updates for the model representation and the classifier are derived from the source, and not the target. However, target accuracy is the goal, and so we argue for optimizing as much as possible on the target data. We show significant improvement by on-target adaptation, which learns the representation purely from target data while taking only the source predictions for supervision. In the long-tailed classification setting, we show further improvement by on-target class distribution learning, which learns the (im)balance of classes from target data.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Wang, Dequan and Liu, Shaoteng and Ebrahimi, Sayna and Shelhamer, Evan and Darrell, Trevor},
	month = sep,
	year = {2021},
	note = {arXiv:2109.01087 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{schreiber_model_2022,
	title = {Model {Selection}, {Adaptation}, and {Combination} for {Transfer} {Learning} in {Wind} and {Photovoltaic} {Power} {Forecasts}},
	url = {http://arxiv.org/abs/2204.13293},
	abstract = {There is recent interest in using model hubs, a collection of pre-trained models, in computer vision tasks. To utilize the model hub, we first select a source model and then adapt the model for the target to compensate for differences. While there is yet limited research on model selection and adaption for computer vision tasks, this holds even more for the field of renewable power. At the same time, it is a crucial challenge to provide forecasts for the increasing demand for power forecasts based on weather features from a numerical weather prediction. We close these gaps by conducting the first thorough experiment for model selection and adaptation for transfer learning in renewable power forecast, adopting recent results from the field of computer vision on 667 wind and photovoltaic parks. To the best of our knowledge, this makes it the most extensive study for transfer learning in renewable power forecasts reducing the computational effort and improving the forecast error. Therefore, we adopt source models based on target data from different seasons and limit the amount of training data. As an extension of the current state of the art, we utilize a Bayesian linear regression for forecasting the response based on features extracted from a neural network. This approach outperforms the baseline with only seven days of training data. We further show how combining multiple models through ensembles can significantly improve the model selection and adaptation approach.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Schreiber, Jens and Sick, Bernhard},
	month = jul,
	year = {2022},
	note = {arXiv:2204.13293 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{shao_not_2022,
	title = {Not {All} {Models} {Are} {Equal}: {Predicting} {Model} {Transferability} in a {Self}-challenging {Fisher} {Space}},
	shorttitle = {Not {All} {Models} {Are} {Equal}},
	url = {http://arxiv.org/abs/2207.03036},
	abstract = {This paper addresses an important problem of ranking the pre-trained deep neural networks and screening the most transferable ones for downstream tasks. It is challenging because the ground-truth model ranking for each task can only be generated by fine-tuning the pre-trained models on the target dataset, which is brute-force and computationally expensive. Recent advanced methods proposed several lightweight transferability metrics to predict the fine-tuning results. However, these approaches only capture static representations but neglect the fine-tuning dynamics. To this end, this paper proposes a new transferability metric, called {\textbackslash}textbf\{S\}elf-challenging {\textbackslash}textbf\{F\}isher {\textbackslash}textbf\{D\}iscriminant {\textbackslash}textbf\{A\}nalysis ({\textbackslash}textbf\{SFDA\}), which has many appealing benefits that existing works do not have. First, SFDA can embed the static features into a Fisher space and refine them for better separability between classes. Second, SFDA uses a self-challenging mechanism to encourage different pre-trained models to differentiate on hard examples. Third, SFDA can easily select multiple pre-trained models for the model ensemble. Extensive experiments on \$33\$ pre-trained models of \$11\$ downstream tasks show that SFDA is efficient, effective, and robust when measuring the transferability of pre-trained models. For instance, compared with the state-of-the-art method NLEEP, SFDA demonstrates an average of \$59.1\${\textbackslash}\% gain while bringing \$22.5\$x speedup in wall-clock time. The code will be available at {\textbackslash}url\{https://github.com/TencentARC/SFDA\}.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Shao, Wenqi and Zhao, Xun and Ge, Yixiao and Zhang, Zhaoyang and Yang, Lei and Wang, Xiaogang and Shan, Ying and Luo, Ping},
	month = jul,
	year = {2022},
	note = {arXiv:2207.03036 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{you_ranking_2022,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain {\textbackslash}emph\{under-exploited\} -- practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top \$K\$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as {\textbackslash}emph\{B-Tuning\}, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	urldate = {2022-09-26},
	booktitle = {Journal of {Machine} {Learning} {Research}},
	author = {You, Kaichao and Liu, Yong and Zhang, Ziyang and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = jul,
	year = {2022},
	note = {arXiv:2110.10545 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{tsay_extracting_2022,
	title = {Extracting enhanced artificial intelligence model metadata from software repositories},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-022-10206-6},
	doi = {10.1007/s10664-022-10206-6},
	abstract = {While artificial intelligence (AI) models have improved at understanding large-scale data, understanding AI models themselves at any scale is difficult. For example, even two models that implement the same network architecture may differ in frameworks, datasets, or even domains. Furthermore, attempting to use either model often requires much manual effort to understand it. As software engineering and AI development share many of the same languages and tools, techniques in mining software repositories should enable more scalable insights into AI models and AI development. However, much of the relevant metadata around models are not easily extractable. This paper (an extension of our MSR 2020 paper) presents a library called AIMMX for AI Model Metadata eXtraction from software repositories into enhanced metadata that conforms to a flexible metadata schema. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. We also explored how AIMMX can enable studies and tools to advance engineering support for AI development. As preliminary examples, we present an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. We also demonstrate the flexibility of extracted metadata by using the evaluation dataset in an existing natural language processing (NLP) analysis platform to identify trends in the dataset. Overall, we hope AIMMX fosters research towards better AI development.},
	language = {en},
	number = {7},
	urldate = {2022-09-26},
	journal = {Empirical Software Engineering},
	author = {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd},
	month = dec,
	year = {2022},
	pages = {176},
}

@misc{you_ranking_2022-1,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain {\textbackslash}emph\{under-exploited\} -- practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top \$K\$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as {\textbackslash}emph\{B-Tuning\}, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {You, Kaichao and Liu, Yong and Zhang, Ziyang and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = jul,
	year = {2022},
	note = {arXiv:2110.10545 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{you_ranking_2022-2,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain {\textbackslash}emph\{under-exploited\} -- practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top \$K\$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as {\textbackslash}emph\{B-Tuning\}, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {You, Kaichao and Liu, Yong and Zhang, Ziyang and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = jul,
	year = {2022},
	note = {arXiv:2110.10545 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{you_ranking_2022-3,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain {\textbackslash}emph\{under-exploited\} -- practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top \$K\$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as {\textbackslash}emph\{B-Tuning\}, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {You, Kaichao and Liu, Yong and Zhang, Ziyang and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = jul,
	year = {2022},
	note = {arXiv:2110.10545 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{pintor_imagenet-patch_nodate,
	title = {{ImageNet}-{Patch}: {A} {Dataset} for {Benchmarking} {Machine} {Learning} {Robustness} against {Adversarial} {Patches}},
	abstract = {Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machinelearning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and applied to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations.},
	language = {en},
	author = {Pintor, Maura and Angioni, Daniele and Sotgiu, Angelo and Demetrio, Luca and Demontis, Ambra and Biggio, Battista and Roli, Fabio},
	pages = {9},
}

@article{liang_metashift_nodate,
	title = {{MetaShift}: {A} {Dataset} of {Datasets} for {Evaluating} {Contextual} {Distribution} {Shifts}},
	abstract = {We start from the pre-processed and cleaned version of Visual Genome to construct MetaShift, which contains 12,868 sets of natural images from 410 classes. The subsets are characterized by a diverse vocabulary of 1,853 distinct contexts. Beyond 1,702 contexts defined by object presence, the dataset also leverages the 37 distinct general contexts and 114 object attributes from Visual Genome. Appendix A present examples and more information of the contexts.},
	language = {en},
	author = {Liang, Weixin and Yang, Xinyu and Zou, James},
	pages = {8},
}

@misc{rudin_interpretable_2021,
	title = {Interpretable {Machine} {Learning}: {Fundamental} {Principles} and 10 {Grand} {Challenges}},
	shorttitle = {Interpretable {Machine} {Learning}},
	url = {http://arxiv.org/abs/2103.11251},
	abstract = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the “Rashomon set” of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.},
	language = {en},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
	month = jul,
	year = {2021},
	note = {arXiv:2103.11251 [cs, stat]},
	keywords = {68T01, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
}

@misc{damour_underspecification_2020,
	title = {Underspecification {Presents} {Challenges} for {Credibility} in {Modern} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2011.03395},
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	month = nov,
	year = {2020},
	note = {arXiv:2011.03395 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yamada_does_nodate,
	title = {Does {Robustness} on {ImageNet} {Transfer} to {Downstream} {Tasks}?},
	abstract = {As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classiﬁcation. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classiﬁcation from different domains. This raises a question: Can these robust image classiﬁers transfer robustness to downstream tasks? For object detection and semantic segmentation, we ﬁnd that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classiﬁcation, we ﬁnd that models that are robustiﬁed for ImageNet do not retain robustness when fully ﬁne-tuned. These ﬁndings suggest that current robustiﬁcation techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning.},
	language = {en},
	author = {Yamada, Yutaro and Otani, Mayu},
	pages = {10},
}

@inproceedings{nguyen_leep_2020,
	title = {{LEEP}: {A} {New} {Measure} to {Evaluate} {Transferability} of {Learned} {Representations}},
	shorttitle = {{LEEP}},
	url = {https://proceedings.mlr.press/v119/nguyen20b.html},
	abstract = {We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30\% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nguyen, Cuong and Hassner, Tal and Seeger, Matthias and Archambeau, Cedric},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7294--7305},
}

@misc{poth_what_2021,
	title = {What to {Pre}-{Train} on? {Efficient} {Intermediate} {Task} {Selection}},
	shorttitle = {What to {Pre}-{Train} on?},
	url = {http://arxiv.org/abs/2104.08247},
	abstract = {Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to run the cross-product of all combinations to find the best transfer setting. In this work we first establish that similar sequential fine-tuning gains can be achieved in adapter settings, and subsequently consolidate previously proposed methods that efficiently identify beneficial tasks for intermediate transfer learning. We experiment with a diverse set of 42 intermediate and 11 target English classification, multiple choice, question answering, and sequence tagging tasks. Our results show that efficient embedding based methods that rely solely on the respective datasets outperform computational expensive few-shot fine-tuning approaches. Our best methods achieve an average Regret@3 of less than 1\% across all target tasks, demonstrating that we are able to efficiently identify the best datasets for intermediate training.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Poth, Clifton and Pfeiffer, Jonas and Rücklé, Andreas and Gurevych, Iryna},
	month = sep,
	year = {2021},
	note = {arXiv:2104.08247 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{jomaa_dataset2vec_2021,
	title = {{Dataset2Vec}: learning dataset meta-features},
	volume = {35},
	issn = {1384-5810, 1573-756X},
	shorttitle = {{Dataset2Vec}},
	url = {https://link.springer.com/10.1007/s10618-021-00737-9},
	doi = {10.1007/s10618-021-00737-9},
	abstract = {Meta-learning, or learning to learn, is a machine learning approach that utilizes prior learning experiences to expedite the learning process on unseen tasks. As a data-driven approach, meta-learning requires meta-features that represent the primary learning tasks or datasets, and are estimated traditonally as engineered dataset statistics that require expert domain knowledge tailored for every meta-task. In this paper, ﬁrst, we propose a meta-feature extractor called Dataset2Vec that combines the versatility of engineered dataset meta-features with the expressivity of meta-features learned by deep neural networks. Primary learning tasks or datasets are represented as hierarchical sets, i.e., as a set of sets, esp. as a set of predictor/target pairs, and then a DeepSet architecture is employed to regress meta-features on them. Second, we propose a novel auxiliary meta-learning task with abundant data called dataset similarity learning that aims to predict if two batches stem from the same dataset or different ones. In an experiment on a large-scale hyperparameter optimization task for 120 UCI datasets with varying schemas as a meta-learning task, we show that the meta-features of Dataset2Vec outperform the expert engineered meta-features and thus demonstrate the usefulness of learned meta-features for datasets with varying schemas for the ﬁrst time.},
	language = {en},
	number = {3},
	urldate = {2022-09-22},
	journal = {Data Mining and Knowledge Discovery},
	author = {Jomaa, Hadi S. and Schmidt-Thieme, Lars and Grabocka, Josif},
	month = may,
	year = {2021},
	pages = {964--985},
}

@inproceedings{subbaswamy_evaluating_2021,
	title = {Evaluating {Model} {Robustness} and {Stability} to {Dataset} {Shift}},
	url = {https://proceedings.mlr.press/v130/subbaswamy21a.html},
	abstract = {As the use of machine learning in high impact domains becomes widespread, the importance of evaluating safety has increased. An important aspect of this is evaluating how robust a model is to changes in setting or population, which typically requires applying the model to multiple, independent datasets. Since the cost of collecting such datasets is often prohibitive, in this paper, we propose a framework for evaluating this type of stability using the available data. We use the original evaluation data to determine distributions under which the algorithm performs poorly, and estimate the algorithm’s performance on the "worst-case" distribution. We consider shifts in user defined conditional distributions, allowing some distributions to shift while keeping other portions of the data distribution fixed. For example, in a healthcare context, this allows us to consider shifts in clinical practice while keeping the patient population fixed. To address the challenges associated with estimation in complex, high-dimensional distributions, we derive a "debiased" estimator which maintains root-N consistency even when machine learning methods with slower convergence rates are used to estimate the nuisance parameters. In experiments on a real medical risk prediction task, we show this estimator can be used to analyze stability and accounts for realistic shifts that could not previously be expressed. The proposed framework allows practitioners to proactively evaluate the safety of their models without requiring additional data collection.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Subbaswamy, Adarsh and Adams, Roy and Saria, Suchi},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2611--2619},
}

@misc{damour_underspecification_2020-1,
	title = {Underspecification {Presents} {Challenges} for {Credibility} in {Modern} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2011.03395},
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	month = nov,
	year = {2020},
	note = {arXiv:2011.03395 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yamada_does_nodate-1,
	title = {Does {Robustness} on {ImageNet} {Transfer} to {Downstream} {Tasks}?},
	abstract = {As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classiﬁcation. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classiﬁcation from different domains. This raises a question: Can these robust image classiﬁers transfer robustness to downstream tasks? For object detection and semantic segmentation, we ﬁnd that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classiﬁcation, we ﬁnd that models that are robustiﬁed for ImageNet do not retain robustness when fully ﬁne-tuned. These ﬁndings suggest that current robustiﬁcation techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning.},
	language = {en},
	author = {Yamada, Yutaro and Otani, Mayu},
	pages = {10},
}

@inproceedings{ghodsi_generating_2021,
	title = {Generating and {Characterizing} {Scenarios} for {Safety} {Testing} of {Autonomous} {Vehicles}},
	doi = {10.1109/IV48863.2021.9576023},
	abstract = {Extracting interesting scenarios from real-world data as well as generating failure cases is important for the development and testing of autonomous systems. We propose efficient mechanisms to both characterize and generate testing scenarios using a state-of-the-art driving simulator. For any scenario, our method generates a set of possible driving paths and identifies all the possible safe driving trajectories that can be taken starting at different times, to compute metrics that quantify the complexity of the scenario. We use our method to characterize real driving data from the Next Generation Simulation (NGSIM) project, as well as adversarial scenarios generated in simulation. We rank the scenarios by defining metrics based on the complexity of avoiding accidents and provide insights into how the AV could have minimized the probability of incurring an accident. We demonstrate a strong correlation between the proposed metrics and human intuition.},
	booktitle = {2021 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Ghodsi, Zahra and Hari, Siva Kumar Sastry and Frosio, Iuri and Tsai, Timothy and Troccoli, Alejandro and Keckler, Stephen W. and Garg, Siddharth and Anandkumar, Anima},
	month = jul,
	year = {2021},
	keywords = {Complexity theory, Computational modeling, Data mining, Data models, Measurement, Safety, Trajectory},
	pages = {157--164},
}

@inproceedings{ghodsi_generating_2021-1,
	title = {Generating and {Characterizing} {Scenarios} for {Safety} {Testing} of {Autonomous} {Vehicles}},
	doi = {10.1109/IV48863.2021.9576023},
	abstract = {Extracting interesting scenarios from real-world data as well as generating failure cases is important for the development and testing of autonomous systems. We propose efficient mechanisms to both characterize and generate testing scenarios using a state-of-the-art driving simulator. For any scenario, our method generates a set of possible driving paths and identifies all the possible safe driving trajectories that can be taken starting at different times, to compute metrics that quantify the complexity of the scenario. We use our method to characterize real driving data from the Next Generation Simulation (NGSIM) project, as well as adversarial scenarios generated in simulation. We rank the scenarios by defining metrics based on the complexity of avoiding accidents and provide insights into how the AV could have minimized the probability of incurring an accident. We demonstrate a strong correlation between the proposed metrics and human intuition.},
	booktitle = {2021 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Ghodsi, Zahra and Hari, Siva Kumar Sastry and Frosio, Iuri and Tsai, Timothy and Troccoli, Alejandro and Keckler, Stephen W. and Garg, Siddharth and Anandkumar, Anima},
	month = jul,
	year = {2021},
	keywords = {Complexity theory, Computational modeling, Data mining, Data models, Measurement, Safety, Trajectory},
	pages = {157--164},
}

@inproceedings{Dube2019DataLabeling4TransferLearning,
	title = {Automatic {Labeling} of {Data} for {Transfer} {Learning}},
	abstract = {Transfer learning uses trained weights from a source model as the initial weights for the training of a target dataset. A well chosen source with a large number of labeled data leads to signiﬁcant improvement in accuracy. We demonstrate a technique that automatically labels large unlabeled datasets so that they can train source models for transfer learning. We experimentally evaluate this method, using a baseline dataset of human-annotated ImageNet1K labels, against ﬁve variations of this technique. We show that the performance of these automatically trained models come within 6\% of baseline.},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}) {Workshops}},
	author = {Dube, Parijat and Bhattacharjee, Bishwaranjan and Huo, Siyu and Watson, Patrick and Belgodere, Brian},
	year = {2019},
	pages = {122--129},
}

@inproceedings{Du2017DeepLog,
	address = {Dallas, Texas, USA},
	title = {{DeepLog}: {Anomaly} {Detection} and {Diagnosis} from {System} {Logs} through {Deep} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134015},
	doi = {10.1145/3133956.3134015},
	abstract = {Anomaly detection is a critical step towards building a secure and trustworthy system. e primary purpose of a system log is to record system states and signi cant events at various critical points to help debug system failures and perform root cause analysis. Such log data is universally available in nearly all computer systems. Log data is an important and valuable resource for understanding system status and performance issues; therefore, the various system logs are naturally excellent source of information for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing Long Short-Term Memory (LSTM), to model a system log as a natural language sequence. is allows DeepLog to automatically learn log pa erns from normal execution, and detect anomalies when log pa erns deviate from the model trained from log data under normal execution. In addition, we demonstrate how to incrementally update the DeepLog model in an online fashion so that it can adapt to new log pa erns over time. Furthermore, DeepLog constructs work ows from the underlying system log so that once an anomaly is detected, users can diagnose the detected anomaly and perform root cause analysis e ectively. Extensive experimental evaluations over large log data have shown that DeepLog has outperformed other existing log-based anomaly detection methods based on traditional data mining methodologies.},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security} ({CCS})},
	publisher = {Association for Computing Machinery},
	author = {Du, Min and Li, Feifei and Zheng, Guineng and Srikumar, Vivek},
	year = {2017},
	pages = {1285--1298},
}

@inproceedings{He2016ResNet,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://doi.org/10.1109/CVPR.2016.90},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	booktitle = {Conference on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@article{DelaCruz2019PT4DeepRL,
	title = {Pre-training with {Non}-expert {Human} {Demonstration} for {Deep} {Reinforcement} {Learning}},
	volume = {34},
	url = {https://doi.org/10.1017/S0269888919000055},
	doi = {10.1017/S0269888919000055},
	abstract = {Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using deep neural networks as function approximators to learn directly from raw input images. However, learning directly from raw images is data inefficient. The agent must learn feature representation of complex states in addition to learning a policy. As a result, deep RL typically suffers from slow learning speeds and often requires a prohibitively large amount of training time and data to reach reasonable performance, making it inapplicable to real-world settings where data is expensive. In this work, we improve data efficiency in deep RL by addressing one of the two learning goals, feature learning. We leverage supervised learning to pre-train on a small set of non-expert human demonstrations and empirically evaluate our approach using the asynchronous advantage actor-critic algorithms (A3C) in the Atari domain. Our results show significant improvements in learning speed, even when the provided demonstration is noisy and of low quality.},
	journal = {The Knowledge Engineering Review},
	author = {de la Cruz, Gabriel V. and Du, Yunshu and Taylor, Matthew E.},
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{patterson2021carbon,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	url = {https://arxiv.org/abs/2104.10350},
	doi = {10.48550/arXiv.2104.10350},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	publisher = {arXiv},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	year = {2021},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{Rezende2017MaliciousSWClassificationUsingTLofResNet,
	title = {Malicious {Software} {Classification} {Using} {Transfer} {Learning} of {ResNet}-50 {Deep} {Neural} {Network}},
	abstract = {Malicious software (malware) has been extensively used for illegal activity and new malware variants are discovered at an alarmingly high rate. The ability to group malware variants into families with similar characteristics makes possible to create mitigation strategies that work for a whole class of programs. In this paper, we present a malware family classification approach using a deep neural network based on the ResNet-50 architecture. Malware samples are represented as byteplot grayscale images and a deep neural network is trained freezing the convolutional layers of ResNet-50 pre-trained on the ImageNet dataset and adapting the last layer to malware family classification. The experimental results on a dataset comprising 9,339 samples from 25 different families showed that our approach can effectively be used to classify malware families with an accuracy of 98.62\%.},
	booktitle = {International {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Rezende, Edmar and Ruppert, Guilherme and Carvalho, Tiago and Ramos, Fabio and de Geus, Paulo},
	year = {2017},
	keywords = {Convolutional Neural Networks, Convolutional codes, Deep Learning, Feature extraction, Gray-scale, Malicious Software Classification, Malware, Neural networks, Transfer Learning, Visualization},
	pages = {1011--1014},
}

@inproceedings{Reddy2019TL4MalariaCellImageClassification,
	title = {Transfer {Learning} with {ResNet}-50 for {Malaria} {Cell}-{Image} {Classification}},
	abstract = {Malaria is an infectious disease caused by single-celled parasite of plasmodium group. The disease is more often spread by an Infected Female Anopheles mosquito. In 2017 alone 219 million cases and nearly 435,000 deaths were reported, with more than 40\% of global population at risk. In spite of many advanced evaluation techniques for identifying the infection, microscopists at resource constrained regions face challenge in improving the diagnostic accuracy. Deep learning based classification of cell images prevent the wrong diagnostic decisions. This paper focuses on the implementation of Transfer learning based classification of malarial infected cells to improve the diagnostic accuracy. The experimental results show that transfer learning performs well on microscopic cell-images.},
	booktitle = {International {Conference} on {Communication} and {Signal} {Processing} ({ICCSP})},
	author = {Reddy, A. Sai Bharadwaj and Juliet, D. Sujitha},
	year = {2019},
	keywords = {Biomedical imaging, Deep Learning, Deep learning, Diseases, Inception model, Malaria, Neural networks, Neurons, ResNet50 model, Task analysis, Training, Transfer Learning, VGG-16 model},
	pages = {0945--0949},
}

@article{Qiu2020PTM4NLP,
	title = {Pre-trained models for natural language processing: {A} survey},
	volume = {63},
	journal = {Science China Technological Sciences},
	author = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
	year = {2020},
	pages = {1872--1897},
}

@article{Pan2010TransferLearning,
	title = {A {Survey} on {Transfer} {Learning}},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	year = {2010},
	keywords = {Data mining, Knowledge engineering, Knowledge transfer, Labeling, Learning systems, Machine learning, Machine learning algorithms, Space technology, Testing, Training data, Transfer learning, data mining., machine learning, survey},
}

@article{Manikas2013SWEcosystem,
	title = {Software ecosystems – {A} systematic literature review},
	volume = {86},
	journal = {Journal of Systems and Software (JSS)},
	author = {Manikas, Konstantinos and Hansen, Klaus Marius},
	year = {2013},
	pages = {1294--1306},
}

@inproceedings{ImageNet,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {6},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	pages = {84--90},
}

@article{Han2021PTM,
	title = {Pre-trained models: {Past}, present and future},
	volume = {2},
	abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
	journal = {AI Open},
	author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
	year = {2021},
	pages = {225--250},
}

@article{Hamon2020RobustnessandExplainabilityofAI,
	title = {Robustness and explainability of {Artificial} {Intelligence}: from technical to policy solutions.},
	shorttitle = {Robustness and explainability of {Artificial} {Intelligence}},
	journal = {Publications Office of the European Union},
	author = {Hamon, Ronan and Junklewitz, Henrik and Sanchez, Ignacio},
	year = {2020},
}

@inproceedings{Guo2019DLDevelopmentandDeploymentAcrossDifferentFrameworksandPlatforms,
	title = {An empirical study towards characterizing deep learning development and deployment across different frameworks and platforms},
	doi = {10.1109/ASE.2019.00080},
	abstract = {Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Guo, Qianyu and Chen, Sen and Xie, Xiaofei and Ma, Lei and Hu, Qiang and Liu, Hongtao and Liu, Yang and Zhao, Jianjun and Li, Xiaohong},
	year = {2019},
	keywords = {Deep learning deployment, Deep learning frameworks, Deep learning platforms, Empirical study},
	pages = {810--822},
}

@misc{Goyal2018ImageNetin1Hour,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	url = {https://arxiv.org/abs/1706.02677},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{Fujiyoshi2019ImageRecognition4AV,
	title = {Deep learning-based image recognition for autonomous driving},
	volume = {43},
	abstract = {Various image recognition tasks were handled in the image recognition ﬁeld prior to 2010 by combining image local features manually designed by researchers (called handcrafted features) and machine learning method. After entering the 2010, However, many image recognition methods that use deep learning have been proposed. The image recognition methods using deep learning are far superior to the methods used prior to the appearance of deep learning in general object recognition competitions. Hence, this paper will explain how deep learning is applied to the ﬁeld of image recognition, and will also explain the latest trends of deep learning-based autonomous driving.},
	journal = {IATSS Research},
	author = {Fujiyoshi, Hironobu and Hirakawa, Tsubasa and Yamashita, Takayoshi},
	year = {2019},
	pages = {244--252},
}

@misc{DoshiVelez2017RigoriousInterpretableML,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {https://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	author = {Doshi-Velez, Finale and Kim, Been},
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Creswell2018GAN,
	title = {Generative {Adversarial} {Networks}: {An} {Overview}},
	volume = {35},
	doi = {10.1109/MSP.2017.2765202},
	abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
	journal = {IEEE Signal Processing Magazine},
	author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
	year = {2018},
	keywords = {Convolutional codes, Data models, Generators, Image resolution, Machine learning, Semantics, Signal resolution, Training data},
	pages = {53--65},
}

@article{Braiek2020onTestingMLPrograms,
	title = {On testing machine learning programs},
	volume = {164},
	abstract = {Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.},
	journal = {Journal of Systems and Software (JSS)},
	author = {Braiek, Houssem Ben and Khomh, Foutse},
	year = {2020},
	pages = {110542},
}

@inproceedings{Bibal2016InterpretabilityofMLModelsandRepresentations,
	title = {Interpretability of {Machine} {Learning} {Models} and {Representations}: an {Introduction}},
	abstract = {Interpretability is often a major concern in machine learning. Although many authors agree with this statement, interpretability is often tackled with intuitive arguments, distinct (yet related) terms and heuristic quantifications. This short survey aims to clarify the concepts related to interpretability and emphasises the distinction between interpreting models
and representations, as well as heuristic-based and user-based approaches.},
	booktitle = {European {Symposium} on {Artificial} {Neural} {Networks}},
	author = {Bibal, Adrien and Frénay, Benoît},
	year = {2016},
}

@article{deng_fuzzing_2022,
	title = {Fuzzing {Deep}-{Learning} {Libraries} via {Automated} {Relational} {API} {Inference}},
	abstract = {Deep Learning (DL) has gained wide attention in recent years. Meanwhile, bugs in DL systems can lead to serious consequences, and may even threaten human lives. As a result, a growing body of research has been dedicated to DL model testing. However, there is still limited work on testing DL libraries, e.g., PyTorch and TensorFlow, which serve as the foundations for building, training, and running DL models. Prior work on fuzzing DL libraries can only generate tests for APIs which have been invoked by documentation examples, developer tests, or DL models, leaving a large number of APIs untested. In this paper, we propose DeepREL, the first approach to automatically inferring relational APIs for more effective DL library fuzzing. Our basic hypothesis is that for a DL library under test, there may exist a number of APIs sharing similar input parameters and outputs; in this way, we can easily “borrow” test inputs from invoked APIs to test other relational APIs. Furthermore, we formalize the notion of value equivalence and status equivalence for relational APIs to serve as the oracle for effective bug finding. We have implemented DeepREL as a fully automated end-to-end relational API inference and fuzzing technique for DL libraries, which 1) automatically infers potential API relations based on API syntactic/semantic information, 2) synthesizes concrete test programs for invoking relational APIs, 3) validates the inferred relational APIs via representative test inputs, and finally 4) performs fuzzing on the verified relational APIs to find potential inconsistencies. Our evaluation on two of the most popular DL libraries, PyTorch and TensorFlow, demonstrates that DeepREL can cover 157\% more APIs than state-of-the-art FreeFuzz. To date, DeepREL has detected 162 bugs in total, with 106 already confirmed by the developers as previously unknown bugs. Surprisingly, DeepREL has detected 13.5\% of the high-priority bugs for the entire PyTorch issue-tracking system in a three-month period. Also, besides the 162 code bugs, we have also detected 14 documentation bugs (all confirmed).},
	language = {en},
	author = {Deng, Yinlin and Yang, Chenyuan and Wei, Anjiang and Zhang, Lingming},
	year = {2022},
	pages = {13},
}

@article{chiang_transferability_2022,
	title = {On the {Transferability} of {Pre}-trained {Language} {Models}: {A} {Study} from {Artificial} {Datasets}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	shorttitle = {On the {Transferability} of {Pre}-trained {Language} {Models}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21295},
	doi = {10.1609/aaai.v36i10.21295},
	abstract = {Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks. In this work, we study what speciﬁc traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks. We propose to use artiﬁcially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have. By ﬁne-tuning the pre-trained models on GLUE benchmark, we can learn how beneﬁcial it is to transfer the knowledge from the model trained on the dataset possessing that speciﬁc trait. We deﬁne and discuss three different characteristics in the artiﬁcial dataset: 1) matching the token’s uni-gram or bi-gram distribution between pre-training and downstream ﬁne-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence. Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance. Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies. Based on our analysis, we ﬁnd that models pretrained with artiﬁcial datasets are prone to learn spurious correlation in downstream tasks. Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences. This result helps us understand the exceptional transferability of pre-trained LMs.},
	language = {en},
	number = {10},
	urldate = {2022-09-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chiang, Cheng-Han and Lee, Hung-yi},
	month = jun,
	year = {2022},
	pages = {10518--10525},
}

@inproceedings{you_logme_2021,
	title = {{LogME}: {Practical} {Assessment} of {Pre}-trained {Models} for {Transfer} {Learning}},
	shorttitle = {{LogME}},
	url = {https://proceedings.mlr.press/v139/you21b.html},
	abstract = {This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo {\textbackslash}emph\{without fine-tuning\}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is {\textbackslash}emph\{immune to over-fitting\}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is {\textbackslash}emph\{fast, accurate, and general\}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most \$3000{\textbackslash}times\$ speedup in wall-clock time and requires only \$1\%\$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: {\textbackslash}href\{https://github.com/thuml/LogME\}\{https://github.com/thuml/LogME\}.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Long, Mingsheng},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12133--12143},
}

@inproceedings{haque_ereba_2022,
	title = {{EREBA}: {Black}-box {Energy} {Testing} of {Adaptive} {Neural} {Networks}},
	shorttitle = {{EREBA}},
	url = {http://arxiv.org/abs/2202.06084},
	doi = {10.1145/3510003.3510088},
	abstract = {Recently, various Deep Neural Network (DNN) models have been proposed for environments like embedded systems with stringent energy constraints. The fundamental problem of determining the robustness of a DNN with respect to its energy consumption (energy robustness) is relatively unexplored compared to accuracy-based robustness. This work investigates the energy robustness of Adaptive Neural Networks (AdNNs), a type of energy-saving DNNs proposed for many energy-sensitive domains and have recently gained traction. We propose EREBA, the first black-box testing method for determining the energy robustness of an AdNN. EREBA explores and infers the relationship between inputs and the energy consumption of AdNNs to generate energy surging samples. Extensive implementation and evaluation using three state-of-the-art AdNNs demonstrate that test inputs generated by EREBA could degrade the performance of the system substantially. The test inputs generated by EREBA can increase the energy consumption of AdNNs by 2,000\% compared to the original inputs. Our results also show that test inputs generated via EREBA are valuable in detecting energy surging inputs.},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	author = {Haque, Mirazul and Yadlapalli, Yaswanth and Yang, Wei and Liu, Cong},
	month = may,
	year = {2022},
	note = {arXiv:2202.06084 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {835--846},
}

@inproceedings{yu_automated_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Automated assertion generation via information retrieval and its integration with deep learning},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510149},
	doi = {10.1145/3510003.3510149},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Yu, Hao and Lou, Yiling and Sun, Ke and Ran, Dezhi and Xie, Tao and Hao, Dan and Li, Ying and Li, Ge and Wang, Qianxiang},
	month = may,
	year = {2022},
	pages = {163--174},
}

@inproceedings{li_testing_2022-1,
	title = {Testing {Machine} {Learning} {Systems} in {Industry}: {An} {Empirical} {Study}},
	shorttitle = {Testing {Machine} {Learning} {Systems} in {Industry}},
	doi = {10.1145/3510457.3513036},
	abstract = {Machine learning becomes increasingly prevalent and integrated into a wide range of software systems. These systems, named ML systems, must be adequately tested to gain confidence that they behave correctly. Although many research efforts have been devoted to testing technologies for ML systems, the industrial teams are faced with new challenges on testing the ML systems in real-world settings. To absorb inspirations from the industry on the problems in ML testing, we conducted an empirical study including a survey with 87 responses and interviews with 7 senior ML practitioners from well-known IT companies. Our study uncovers significant industrial concerns on major testing activities, i.e., test data collection, test execution, and test result analysis, and also the good practices and open challenges from the perspective of the industry. (1) Test data collection is conducted in different ways on ML model, data, and code and faced with different challenges. (2) Test execution in ML systems suffers from two major problems: entanglement among the components and the regression on model performance. (3) Test result analysis centers on quantitative methods, e.g., metric-based evaluation, and is combined with some qualitative methods based on practitioners’ experience. Based on our findings, we highlight the research opportunities and also provide some implications for practitioners.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Li†, Shuyue and Guo†, Jiaqi and Lou, Jian-Guang and Fan, Ming and Liu‡, Ting and Zhang, Dongmei},
	year = {2022},
	keywords = {Analytical models, Data collection, Industries, Machine learning, Software systems, Software testing, Technological innovation, machine learning, software testing, survey},
	pages = {263--272},
}

@inproceedings{liu_deepstate_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{DeepState}: selecting test suites to enhance the robustness of recurrent neural networks},
	isbn = {978-1-4503-9221-1},
	shorttitle = {{DeepState}},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510231},
	doi = {10.1145/3510003.3510231},
	abstract = {Deep Neural Networks (DNN) have achieved tremendous success in various software applications. However, accompanied by outstanding effectiveness, DNN-driven software systems could also exhibit incorrect behaviors and result in some critical accidents and losses. The testing and optimization of DNN-driven software systems rely on a large number of labeled data that often require many human efforts, resulting in high test costs and low efficiency. Although plenty of coverage-based criteria have been proposed to assist in the data selection of convolutional neural networks, it is difficult to apply them on Recurrent Neural Network (RNN) models due to the difference between the working nature.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Liu, Zixi and Feng, Yang and Yin, Yining and Chen, Zhenyu},
	month = may,
	year = {2022},
	pages = {598--609},
}

@book{fingscheidt_deep_2022,
	address = {Cham},
	title = {Deep {Neural} {Networks} and {Data} for {Automated} {Driving}: {Robustness}, {Uncertainty} {Quantification}, and {Insights} {Towards} {Safety}},
	isbn = {978-3-031-01232-7 978-3-031-01233-4},
	shorttitle = {Deep {Neural} {Networks} and {Data} for {Automated} {Driving}},
	url = {https://link.springer.com/10.1007/978-3-031-01233-4},
	language = {en},
	urldate = {2022-09-18},
	publisher = {Springer International Publishing},
	editor = {Fingscheidt, Tim and Gottschalk, Hanno and Houben, Sebastian},
	year = {2022},
	doi = {10.1007/978-3-031-01233-4},
}

@misc{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	url = {http://arxiv.org/abs/1903.12261},
	abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	urldate = {2022-09-18},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	month = mar,
	year = {2019},
	note = {arXiv:1903.12261 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hu_characterizing_2022,
	title = {Characterizing and {Understanding} the {Behavior} of {Quantized} {Models} for {Reliable} {Deployment}},
	url = {http://arxiv.org/abs/2204.04220},
	abstract = {Deep Neural Networks (DNNs) have gained considerable attention in the past decades due to their astounding performance in different applications, such as natural language modeling, self-driving assistance, and source code understanding. With rapid exploration, more and more complex DNN architectures have been proposed along with huge pre-trained model parameters. The common way to use such DNN models in user-friendly devices (e.g., mobile phones) is to perform model compression before deployment. However, recent research has demonstrated that model compression, e.g., model quantization, yields accuracy degradation as well as outputs disagreements when tested on unseen data. Since the unseen data always include distribution shifts and often appear in the wild, the quality and reliability of quantized models are not ensured. In this paper, we conduct a comprehensive study to characterize and help users understand the behaviors of quantized models. Our study considers 4 datasets spanning from image to text, 8 DNN architectures including feed-forward neural networks and recurrent neural networks, and 42 shifted sets with both synthetic and natural distribution shifts. The results reveal that 1) data with distribution shifts happen more disagreements than without. 2) Quantization-aware training can produce more stable models than standard, adversarial, and Mixup training. 3) Disagreements often have closer top-1 and top-2 output probabilities, and \$Margin\$ is a better indicator than the other uncertainty metrics to distinguish disagreements. 4) Retraining with disagreements has limited efficiency in removing disagreements. We opensource our code and models as a new benchmark for further studying the quantized models.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Hu, Qiang and Guo, Yuejun and Cordy, Maxime and Xie, Xiaofei and Ma, Wei and Papadakis, Mike and Traon, Yves Le},
	month = apr,
	year = {2022},
	note = {arXiv:2204.04220 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{zhu_fuzzing_2022,
	title = {Fuzzing: {A} {Survey} for {Roadmap}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Fuzzing},
	url = {https://dl.acm.org/doi/10.1145/3512345},
	doi = {10.1145/3512345},
	abstract = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this article, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
	language = {en},
	number = {11s},
	urldate = {2022-09-16},
	journal = {ACM Computing Surveys},
	author = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
	month = jan,
	year = {2022},
	pages = {1--36},
}

@misc{yu_deeprepair_2020,
	title = {{DeepRepair}: {Style}-{Guided} {Repairing} for {DNNs} in the {Real}-world {Operational} {Environment}},
	shorttitle = {{DeepRepair}},
	url = {http://arxiv.org/abs/2011.09884},
	abstract = {Deep neural networks (DNNs) are being widely applied for various real-world applications across domains due to their high performance (e.g., high accuracy on image classification). Nevertheless, a well-trained DNN after deployment could oftentimes raise errors during practical use in the operational environment due to the mismatching between distributions of the training dataset and the potential unknown noise factors in the operational environment, e.g., weather, blur, noise etc. Hence, it poses a rather important problem for the DNNs' real-world applications: how to repair the deployed DNNs for correcting the failure samples (i.e., incorrect prediction) under the deployed operational environment while not harming their capability of handling normal or clean data. The number of failure samples we can collect in practice, caused by the noise factors in the operational environment, is often limited. Therefore, It is rather challenging how to repair more similar failures based on the limited failure samples we can collect. In this paper, we propose a style-guided data augmentation for repairing DNN in the operational environment. We propose a style transfer method to learn and introduce the unknown failure patterns within the failure data into the training data via data augmentation. Moreover, we further propose the clustering-based failure data generation for much more effective style-guided data augmentation. We conduct a large-scale evaluation with fifteen degradation factors that may happen in the real world and compare with four state-of-the-art data augmentation methods and two DNN repairing methods, demonstrating that our method can significantly enhance the deployed DNNs on the corrupted data in the operational environment, and with even better accuracy on clean datasets.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Yu, Bing and Qi, Hua and Guo, Qing and Juefei-Xu, Felix and Xie, Xiaofei and Ma, Lei and Zhao, Jianjun},
	month = nov,
	year = {2020},
	note = {arXiv:2011.09884 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{yokoyama_towards_2020,
	title = {Towards {Building} {Robust} {DNN} {Applications}: {An} {Industrial} {Case} {Study} of {Evolutionary} {Data} {Augmentation}},
	shorttitle = {Towards {Building} {Robust} {DNN} {Applications}},
	abstract = {Data augmentation techniques that increase the amount of training data by adding realistic transformations are used in machine learning to improve the level of accuracy. Recent studies have demonstrated that data augmentation techniques improve the robustness of image classification models with open datasets; however, it has yet to be investigated whether these techniques are effective for industrial datasets. In this study, we investigate the feasibility of data augmentation techniques for industrial use. We evaluate data augmentation techniques in image classification and object detection tasks using an industrial in-house graphical user interface dataset. As the results indicate, the genetic algorithm-based data augmentation technique outperforms two random-based methods in terms of the robustness of the image classification model. In addition, through this evaluation and interviews with the developers, we learned following two lessons: data augmentation techniques should (1) maintain the training speed to avoid slowing the development and (2) include extensibility for a variety of tasks.},
	booktitle = {2020 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Yokoyama, Haruki and Onoue, Satoshi and Kikuchi, Shinji},
	month = sep,
	year = {2020},
	note = {ISSN: 2643-1572},
	keywords = {Data models, Image classification, Object detection, Robustness, Software algorithms, Task analysis, Training data, data augmentation, datasets, machine learning, neural networks, object detection},
	pages = {1184--1188},
}

@misc{shen_improving_2021,
	title = {Improving {Robustness} of {Learning}-based {Autonomous} {Steering} {Using} {Adversarial} {Images}},
	url = {http://arxiv.org/abs/2102.13262},
	abstract = {For safety of autonomous driving, vehicles need to be able to drive under various lighting, weather, and visibility conditions in different environments. These external and environmental factors, along with internal factors associated with sensors, can pose significant challenges to perceptual data processing, hence affecting the decision-making and control of the vehicle. In this work, we address this critical issue by introducing a framework for analyzing robustness of the learning algorithm w.r.t varying quality in the image input for autonomous driving. Using the results of sensitivity analysis, we further propose an algorithm to improve the overall performance of the task of "learning to steer". The results show that our approach is able to enhance the learning outcomes up to 48\%. A comparative study drawn between our approach and other related techniques, such as data augmentation and adversarial training, confirms the effectiveness of our algorithm as a way to improve the robustness and generalization of neural network training for autonomous driving.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Shen, Yu and Zheng, Laura and Shu, Manli and Li, Weizi and Goldstein, Tom and Lin, Ming C.},
	month = feb,
	year = {2021},
	note = {arXiv:2102.13262 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{bonomi_fog_nodate,
	title = {Fog computing and its role in the internet of things},
	abstract = {Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. Deﬁning characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution; c) Mobility; d) Very large number of nodes, e) Predominant role of wireless access, f) Strong presence of streaming and real time applications, g) Heterogeneity. In this paper we argue that the above characteristics make the Fog the appropriate platform for a number of critical Internet of Things (IoT) services and applications, namely, Connected Vehicle, Smart Grid , Smart Cities, and, in general, Wireless Sensors and Actuators Networks (WSANs).},
	language = {en},
	author = {Bonomi, Flavio and Milito, Rodolfo and Zhu, Jiang and Addepalli, Sateesh},
	pages = {3},
}

@misc{geirhos_imagenet-trained_2019,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {http://arxiv.org/abs/1811.12231},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = jan,
	year = {2019},
	note = {arXiv:1811.12231 [cs, q-bio, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@inproceedings{salman_adversarially_2020,
	title = {Do {Adversarially} {Robust} {ImageNet} {Models} {Transfer} {Better}?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/24357dd085d2c4b1a88a7e0692e60294-Abstract.html},
	abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Code and models is available in the supplementary material.},
	urldate = {2022-09-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
	year = {2020},
	pages = {3533--3545},
}

@misc{du_survey_2022,
	title = {A {Survey} of {Vision}-{Language} {Pre}-{Trained} {Models}},
	url = {http://arxiv.org/abs/2202.10936},
	abstract = {As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Du, Yifan and Liu, Zikang and Li, Junyi and Zhao, Wayne Xin},
	month = jul,
	year = {2022},
	note = {arXiv:2202.10936 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_benchmarking_2021,
	title = {Benchmarking {Detection} {Transfer} {Learning} with {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2111.11429},
	abstract = {Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4\% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Li, Yanghao and Xie, Saining and Chen, Xinlei and Dollar, Piotr and He, Kaiming and Girshick, Ross},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11429 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_benchmarking_2021-1,
	title = {Benchmarking {Detection} {Transfer} {Learning} with {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2111.11429},
	abstract = {Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4\% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Li, Yanghao and Xie, Saining and Chen, Xinlei and Dollar, Piotr and He, Kaiming and Girshick, Ross},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11429 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chiang_transferability_2022-1,
	title = {On the {Transferability} of {Pre}-trained {Language} {Models}: {A} {Study} from {Artificial} {Datasets}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	shorttitle = {On the {Transferability} of {Pre}-trained {Language} {Models}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21295},
	doi = {10.1609/aaai.v36i10.21295},
	abstract = {Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks. In this work, we study what speciﬁc traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks. We propose to use artiﬁcially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have. By ﬁne-tuning the pre-trained models on GLUE benchmark, we can learn how beneﬁcial it is to transfer the knowledge from the model trained on the dataset possessing that speciﬁc trait. We deﬁne and discuss three different characteristics in the artiﬁcial dataset: 1) matching the token’s uni-gram or bi-gram distribution between pre-training and downstream ﬁne-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence. Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance. Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies. Based on our analysis, we ﬁnd that models pretrained with artiﬁcial datasets are prone to learn spurious correlation in downstream tasks. Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences. This result helps us understand the exceptional transferability of pre-trained LMs.},
	language = {en},
	number = {10},
	urldate = {2022-09-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chiang, Cheng-Han and Lee, Hung-yi},
	month = jun,
	year = {2022},
	pages = {10518--10525},
}

@inproceedings{you_logme_2021-1,
	title = {{LogME}: {Practical} {Assessment} of {Pre}-trained {Models} for {Transfer} {Learning}},
	shorttitle = {{LogME}},
	url = {https://proceedings.mlr.press/v139/you21b.html},
	abstract = {This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo {\textbackslash}emph\{without fine-tuning\}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is {\textbackslash}emph\{immune to over-fitting\}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is {\textbackslash}emph\{fast, accurate, and general\}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most \$3000{\textbackslash}times\$ speedup in wall-clock time and requires only \$1\%\$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: {\textbackslash}href\{https://github.com/thuml/LogME\}\{https://github.com/thuml/LogME\}.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Long, Mingsheng},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12133--12143},
}

@inproceedings{davis_testing_2019,
	address = {San Diego, CA, USA},
	title = {Testing {Regex} {Generalizability} {And} {Its} {Implications}: {A} {Large}-{Scale} {Many}-{Language} {Measurement} {Study}},
	isbn = {978-1-72812-508-4},
	shorttitle = {Testing {Regex} {Generalizability} {And} {Its} {Implications}},
	url = {https://ieeexplore.ieee.org/document/8952443/},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and ﬁndings to date are typically generalized from regexes written in only 1–2 programming languages. This is an incomplete foundation.},
	language = {en},
	urldate = {2022-09-13},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Davis, James C and Moyer, Daniel and Kazerouni, Ayaan M and Lee, Dongyoon},
	month = nov,
	year = {2019},
	pages = {427--439},
}

@inproceedings{sillito_failures_2020,
	title = {Failures and {Fixes}: {A} {Study} of {Software} {System} {Incident} {Response}},
	shorttitle = {Failures and {Fixes}},
	doi = {10.1109/ICSME46990.2020.00027},
	abstract = {This paper presents the results of a research study related to software system failures, with the goal of understanding how we might better evolve, maintain and support software systems in production. We have qualitatively analyzed thirty incidents: fifteen collected through in depth interviews with engineers, and fifteen sampled from publicly published incident reports (generally produced as part of postmortem reviews). Our analysis focused on understanding and categorizing how failures occurred, and how they were detected, investigated and mitigated. We also captured analytic insights related to the current state of the practice and associated challenges in the form of 11 key observations. For example, we observed that failures can cascade through a system leading to major outages; and that often engineers do not understand the scaling limits of systems they are supporting until those limits are exceeded. We argue that the challenges we have identified can lead to improvements to how systems are engineered and supported.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Sillito, Jonathan and Kutomi, Esdras},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Conferences, Interviews, Monitoring, Production, Software maintenance, Software systems, empirical studies, incident response, software failures, software monitoring},
	pages = {185--195},
}

@inproceedings{sillito_failures_2020,
	title = {Failures and {Fixes}: {A} {Study} of {Software} {System} {Incident} {Response}},
	shorttitle = {Failures and {Fixes}},
	doi = {10.1109/ICSME46990.2020.00027},
	abstract = {This paper presents the results of a research study related to software system failures, with the goal of understanding how we might better evolve, maintain and support software systems in production. We have qualitatively analyzed thirty incidents: fifteen collected through in depth interviews with engineers, and fifteen sampled from publicly published incident reports (generally produced as part of postmortem reviews). Our analysis focused on understanding and categorizing how failures occurred, and how they were detected, investigated and mitigated. We also captured analytic insights related to the current state of the practice and associated challenges in the form of 11 key observations. For example, we observed that failures can cascade through a system leading to major outages; and that often engineers do not understand the scaling limits of systems they are supporting until those limits are exceeded. We argue that the challenges we have identified can lead to improvements to how systems are engineered and supported.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Sillito, Jonathan and Kutomi, Esdras},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Conferences, Interviews, Monitoring, Production, Software maintenance, Software systems, empirical studies, incident response, software failures, software monitoring},
	pages = {185--195},
}

@inproceedings{Wang2021UAVBug,
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	booktitle = {{ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FES})},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	year = {2021},
}

@inproceedings{roth_odds_2019,
	title = {The {Odds} are {Odd}: {A} {Statistical} {Test} for {Detecting} {Adversarial} {Examples}},
	shorttitle = {The {Odds} are {Odd}},
	url = {https://proceedings.mlr.press/v97/roth19a.html},
	abstract = {We investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.},
	language = {en},
	urldate = {2022-09-08},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Roth, Kevin and Kilcher, Yannic and Hofmann, Thomas},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5498--5507},
}

@techreport{Boyens2015SupplyChainRiskManagementPractices,
	title = {Supply {Chain} {Risk} {Management} {Practices} for {Federal} {Information} {Systems} and {Organizations}},
	abstract = {Federal agencies are concerned about the risks associated with information and communications technology (ICT) products and services that may contain potentially malicious functionality, are counterfeit, or are vulnerable due to poor manufacturing and development practices within the ICT supply chain. These risks are associated with the federal agencies’ decreased visibility into, understanding of, and control over how the technology that they acquire is developed, integrated and deployed, as well as the processes, procedures, and practices used to assure the integrity, security, resilience, and quality of the products and services.},
	number = {NIST SP 800-161},
	institution = {National Institute of Standards and Technology},
	author = {Boyens, Jon M. and Paulsen, Celia and Moorthy, Rama and Bartol, Nadya},
	year = {2015},
	doi = {10.6028/NIST.SP.800-161},
}

@techreport{boyens_supply_2015,
	title = {Supply {Chain} {Risk} {Management} {Practices} for {Federal} {Information} {Systems} and {Organizations}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-161.pdf},
	abstract = {Federal agencies are concerned about the risks associated with information and communications technology (ICT) products and services that may contain potentially malicious functionality, are counterfeit, or are vulnerable due to poor manufacturing and development practices within the ICT supply chain. These risks are associated with the federal agencies’ decreased visibility into, understanding of, and control over how the technology that they acquire is developed, integrated and deployed, as well as the processes, procedures, and practices used to assure the integrity, security, resilience, and quality of the products and services.},
	language = {en},
	number = {NIST SP 800-161},
	urldate = {2022-09-08},
	institution = {National Institute of Standards and Technology},
	author = {Boyens, Jon M. and Paulsen, Celia and Moorthy, Rama and Bartol, Nadya},
	month = apr,
	year = {2015},
	doi = {10.6028/NIST.SP.800-161},
	pages = {NIST SP 800--161},
}

@inproceedings{Zhang2020RetrainRecommenderSystem,
	title = {How to {Retrain} {Recommender} {System}?: {A} {Sequential} {Meta}-{Learning} {Method}},
	doi = {10.1145/3397271.3401167},
	abstract = {Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference. However, a full model retraining could be very time-consuming and memory-costly, especially when the scale of historical data is large. In this work, we study the model retraining mechanism for recommender systems, a topic of high practical values but has been relatively little explored in the research community.},
	booktitle = {International {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Zhang, Yang and Feng, Fuli and Wang, Chenxu and He, Xiangnan and Wang, Meng and Li, Yan and Zhang, Yongdong},
	year = {2020},
}

@article{Derakhshan2019CDofMLPipelines,
	title = {Continuous {Deployment} of {Machine} {Learning} {Pipelines}},
	abstract = {Today machine learning is entering many business and scienti c applications. The life cycle of machine learning applications consists of data preprocessing for transforming the raw data into features, training a model using the features, and deploying the model for answering prediction queries. In order to guarantee accurate predictions, one has to continuously monitor and update the deployed model and pipeline. Current deployment platforms update the model using online learning methods. When online learning alone is not adequate to guarantee the prediction accuracy, some deployment platforms provide a mechanism for automatic or manual retraining of the model. While the online training is fast, the retraining of the model is time-consuming and adds extra overhead and complexity to the process of deployment. We propose a novel continuous deployment approach for updating the deployed model using a combination of the incoming realtime data and the historical data. We utilize sampling techniques to include the historical data in the training process, thus eliminating the need for retraining the deployed model. We also o er online statistics computation and dynamic materialization of the preprocessed features, which further reduces the total training and data preprocessing time. In our experiments, we design and deploy two pipelines and models to process two real-world datasets. The experiments show that continuous deployment reduces the total training cost up to 15 times while providing the same level of quality when compared to the state-of-the-art deployment approaches.},
	journal = {EDBT},
	author = {Derakhshan, Behrouz and Mahdiraji, Alireza Rezaei and Rabl, Tilmann and Markl, Volker},
	year = {2019},
}

@inproceedings{WhatsinaGithubStar,
	title = {What's in a {GitHub} {Star}? {Understanding} {Repository} {Starring} {Practices} in a {Social} {Coding} {Platform}},
	doi = {10.1016/j.jss.2018.09.016},
	abstract = {Besides a git-based version control system, GitHub integrates several social coding features. Particularly, GitHub users can star a repository, presumably to manifest interest or satisfaction with an open source project. However, the real and practical meaning of starring a project was never the subject of an in-depth and well-founded empirical investigation. Therefore, we provide in this paper a throughout study on the meaning, characteristics, and dynamic growth of GitHub stars. First, by surveying 791 developers, we report that three out of four developers consider the number of stars before using or contributing to a GitHub project. Then, we report a quantitative analysis on the characteristics of the top-5,000 most starred GitHub repositories. We propose four patterns to describe stars growth, which are derived after clustering the time series representing the number of stars of the studied repositories; we also reveal the perception of 115 developers about these growth patterns. To conclude, we provide a list of recommendations to open source project managers (e.g., on the importance of social media promotion) and to GitHub users and Software Engineering researchers (e.g., on the risks faced when selecting projects by GitHub stars).},
	booktitle = {Journal of {Systems} and {Software} ({JSS})},
	author = {Borges, Hudson and Valente, Marco Tulio},
	year = {2018},
	keywords = {Computer Science - Software Engineering},
}

@article{Huston2018AIfacesReproducibilityCrisis,
	title = {Artificial intelligence faces reproducibility crisis},
	doi = {10.1126/science.359.6377.725},
	abstract = {Unpublished code and sensitivity to training conditions make many claims hard to verify.
          , 
            The booming field of artificial intelligence (AI) is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade. Just because algorithms are based on code doesn't mean experiments are easily replicated. Far from it. Unpublished codes and a sensitivity to training conditions have made it difficult for AI researchers to reproduce many key results. That is leading to a new conscientiousness about research methods and publication protocols. Last week, at a meeting of the Association for the Advancement of Artificial Intelligence in New Orleans, Louisiana, reproducibility was on the agenda, with some teams diagnosing the problem—and one laying out tools to mitigate it.},
	journal = {Science},
	author = {Hutson, Matthew},
	year = {2018},
}

@inproceedings{Rahman2019MLSEinPractice,
	title = {Machine learning software engineering in practice: {An} industrial case study},
	doi = {10.1109/ICSE-SEIP.2019.00042},
	abstract = {SAP is the market leader in enterprise software offering an end-to-end suite of applications and services to enable their customers worldwide to operate their business. Especially, retail customers of SAP deal with millions of sales transactions for their day-to-day business. Transactions are created during retail sales at the point of sale (POS) terminals and then sent to some central servers for validations and other business operations. A considerable proportion of the retail transactions may have inconsistencies due to many technical and human errors. SAP provides an automated process for error detection but still requires a manual process by dedicated employees using workbench software for correction. However, manual corrections of these errors are time-consuming, labor-intensive, and may lead to further errors due to incorrect modifications. This is not only a performance overhead on the customers' business workflow but it also incurs high operational costs. Thus, automated detection and correction of transaction errors are very important regarding their potential business values and the improvement in the business workflow. In this paper, we present an industrial case study where we apply machine learning (ML) to automatically detect transaction errors and propose corrections.We identify and discuss the challenges that we faced during this collaborative research and devel- opment project, from three distinct perspectives: Software Engineering, Machine Learning, and industry-academia collaboration. We report on our experience and insights from the project with guidelines for the identified challenges.We believe that our findings and recommendations can help researchers and practitioners embarking into similar endeavors.},
	booktitle = {International {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Rahman, Saidur and River, Emilio and Khomh, Foutse and Guhneuc, Yann Gal and Lehnert, Bernd},
	year = {2019},
}

@article{Sakurada2021Industry4NewJob,
	title = {Analysis of {New} {Job} {Profiles} for the {Factory} of the {Future}},
	doi = {10.1007/978-3-030-69373-2_18},
	abstract = {Industry 4.0 is being promoting the digitisation of manufacturing sector towards smart products, machines, processes and factories. The adoption of disruptive technologies associated to this industrial revolution will lead to re-shaping the manufacturing environment, decreasing the low-skilled activities and increasing the high-skill activities, being expected to grow the complexity and number of new job profiles. In this context, this paper aims to analyse the literature and recruitment repositories to identify the new job profiles in the factory of the future (FoF) across six industrial technological sectors, namely Collaborative Robotics (Cobots), Additive Manufacturing (AM), Mechatronics and Machine Automation (MMA), Data Analytics (DA), Cybersecurity (CS) and Human-Machine Interface (HMI). The performed analysis allowed to compile a catalogue of 100 new job profiles that were characterised and analysed in terms of technical and soft skills, type and level of profile, as well as the frequency demand.},
	journal = {International Workshop on Service Orientation in Holonic and Multi-Agent Manufacturing},
	author = {Sakurada, Lucas and Geraldes, Carla A. S. and Fernandes, Florbela P. and Pontes, Joseane and Leitao, Paulo},
	year = {2020},
}

@inproceedings{IBM2020AIMMX,
	title = {{AIMMX}: {Artificial} {Intelligence} {Model} {Metadata} {Extractor}},
	doi = {10.1145/3379597.3387448},
	abstract = {Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-theart AI papers. Our platform extracted metadata with 87\% precision and 83\% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42\% of models in our sample citing their datasets, method reproducibility is more common at 72\% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd},
	year = {2020},
}

@article{Zaidi2022SurveyofObjectDetectionModels,
	title = {A {Survey} of {Modern} {Deep} {Learning} based {Object} {Detection} {Models}},
	abstract = {Object Detection is the task of classification and localization of objects in an image or video. It has gained prominence in recent years due to its widespread applications. This article surveys recent developments in deep learning based object detectors. Concise overview of benchmark datasets and evaluation metrics used in detection is also provided along with some of the prominent backbone architectures used in recognition tasks. It also covers contemporary lightweight classification models used on edge devices. Lastly, we compare the performances of these architectures on multiple metrics.},
	journal = {Digital Signal Processing},
	author = {Zaidi, Syed Sahil Abbas and Ansari, Mohammad Samar and Aslam, Asra and Kanwal, Nadia and Asghar, Mamoona and Lee, Brian},
	year = {2022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{Bogart2015HowEcosystemDevelopersReasonabouttheStabilityofDependencies,
	title = {When {It} {Breaks}, {It} {Breaks}: {How} {Ecosystem} {Developers} {Reason} about the {Stability} of {Dependencies}},
	abstract = {Dependencies among software projects and libraries are an indicator of the often implicit collaboration among many developers in software ecosystems. Negotiating change can be tricky: changes to one module may cause ripple effects to many other modules that depend on it, yet insisting on only backward-compatible changes may incur significant opportunity cost and stifle change. We argue that awareness mechanisms based on various notions of stability can enable developers to make decisions that are independent yet wise and provide stewardship rather than disruption to the ecosystem. In ongoing interviews with developers in two software ecosystems (CRAN and Node.js), we are finding that developers in fact struggle with change, that they often use adhoc mechanisms to negotiate change, and that existing awareness mechanisms like Github notification feeds are rarely used due to information overload. We study the state of the art and current information needs and outline a vision toward a change-based awareness system.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} {Workshop} ({ASEW})},
	author = {Bogart, Christopher and Kästner, Christian and Herbsleb, James},
	year = {2015},
	keywords = {Computer science, Ecosystems, History, Interviews, Planning, Software, Stability analysis},
}

@inproceedings{pashchenko_qualitative_2020,
	address = {Virtual Event USA},
	title = {A {Qualitative} {Study} of {Dependency} {Management} and {Its} {Security} {Implications}},
	isbn = {978-1-4503-7089-9},
	url = {https://dl.acm.org/doi/10.1145/3372297.3417232},
	doi = {10.1145/3372297.3417232},
	abstract = {Several large scale studies on the Maven, NPM, and Android ecosystems point out that many developers do not often update their vulnerable software libraries thus exposing the user of their code to security risks. The purpose of this study is to qualitatively investigate the choices and the interplay of functional and security concerns on the developers’ overall decision-making strategies for selecting, managing, and updating software dependencies.},
	language = {en},
	urldate = {2022-09-02},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Pashchenko, Ivan and Vu, Duc-Ly and Massacci, Fabio},
	month = oct,
	year = {2020},
	pages = {1513--1531},
}

@inproceedings{de_souza_empirical_2008,
	address = {Leipzig, Germany},
	title = {An empirical study of software developers' management of dependencies and changes},
	isbn = {978-1-60558-079-1},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368122},
	doi = {10.1145/1368088.1368122},
	abstract = {Different approaches and tools have been proposed to support change impact analysis, i.e., the identification of the potential consequences of a change, or the estimation of what needs to be modified to accomplish a change. However, just a few empirical studies of software developers’ actual change impact analysis approaches have been reported in the literature. To minimize this gap, this paper describes an empirical study of two software development teams. It describes, through the presentation of ethnographic data, the strategies used by software developers to handle the effect of software dependencies and changes in their work. The concept of impact management is proposed as an analytical framework to present these practices and is used to suggest avenues for future research in change impact analysis techniques.},
	language = {en},
	urldate = {2022-09-02},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering  - {ICSE} '08},
	publisher = {ACM Press},
	author = {de Souza, Cleidson R. B. and Redmiles, David F.},
	year = {2008},
	pages = {241},
}

@inproceedings{Thung2016APIRecommendationSystem,
	title = {{API} recommendation system for software development},
	abstract = {Nowadays, software developers often utilize existing third party libraries and make use of Application Programming Interface (API) to develop a software. However, it is not always obvious which library to use or whether the chosen library will play well with other libraries in the system. Furthermore, developers need to spend some time to understand the API to the point that they can freely use the API methods and putting the right parameters inside them. In this work, I plan to automatically recommend relevant APIs to developers. This API recommendation can be divided into multiple stages. First, we can recommend relevant libraries provided a given task to complete. Second, we can recommend relevant API methods that developer can use to program the required task. Third, we can recommend correct parameters for a given method according to its context. Last but not least, we can recommend how different API methods can be combined to achieve a given task. In effort to realize this API recommendation system, I have published two related papers. The first one deals with recommending additional relevant API libraries given known useful API libraries for the target program. This system can achieve recall rate@5 of 0.852 and recall rate@10 of 0.894 in recommending additional relevant libraries. The second one deals with recommending relevant API methods a given target API and a textual description of the task. This system can achieve recall-rate@5 of 0.690 and recall-rate@10 of 0.779. The results for both system indicate that the systems are useful and capable in recommending the right API/library reasonably well. Currently, I am working on another system which can recommend web APIs (i.e., libraries) given a description of the task. I am also working on a system that recommends correct parameters given an API method. In the future, I also plan to realize API composition recommendation for the given task.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Thung, Ferdian},
	year = {2016},
	keywords = {API, Context, Databases, Feature extraction, History, Libraries, Library, Recommendation System, Software, Training},
}

@inproceedings{Wang2020EmpiricalStudyofThirdPartyLibsinJavaProjects,
	title = {An {Empirical} {Study} of {Usages}, {Updates} and {Risks} of {Third}-{Party} {Libraries} in {Java} {Projects}},
	abstract = {Third-party libraries play a key role in software development as they can relieve developers of the heavy burden of re-implementing common functionalities. However, third-party libraries and client projects evolve asynchronously. As a result, out-dated third-party libraries might be used in client projects while developers are not aware of the potential risk (e.g., security bug). Outdated third-party libraries may be updated in client projects in a delayed way, and developers may be less aware of the potential risk (e.g., API incompatibility) in updates. Developers of third-party libraries may be unaware of how their third-party libraries are used or updated in client projects. Therefore, a quantitative and holistic study on usages, updates and risks of third-party libraries in open-source projects can provide concrete evidences on these problems, and practical insights to improve the ecosystem. In this paper, we contribute such a study in Java ecosystem. In particular, we conduct a library usage analysis (e.g., usage intensity and outdatedness) and library update analysis (e.g., update intensity and delay) on 806 open-source projects and 13,565 third- party libraries. Then, we carry out a library risk analysis (e.g., usage risk and update risk) on 806 open-source projects and 544 security bugs. These analyses aim to quantify the usage and update practices and the potential risk of using and updating outdated third-party libraries with respect to security bugs from two holistic perspectives (i.e., open-source projects and third-party libraries). Our findings suggest practical implications to developers and researchers on problems and potential solutions in maintaining third-party libraries (e.g., smart alerting and automated updating of outdated third-party libraries). To indicate the usefulness of our findings, we design a smart alerting system for assisting developers to make confident decisions when updating third-party libraries. 33 and 24 open-source projects have confirmed and updated third-party libraries after receiving our alerts.},
	booktitle = {International {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Wang, Ying and Chen, Bihuan and Huang, Kaifeng and Shi, Bowen and Xu, Congying and Peng, Xin and Wu, Yijian and Liu, Yang},
	year = {2020},
	keywords = {Computer bugs, Java, Libraries, Open source software, Security, Software maintenance, Tools, outdated libraries, security bugs},
}

@inproceedings{LariosVargas2020ThirdPartyLibrariesSelection,
	title = {Selecting third-party libraries: the practitioners’ perspective},
	abstract = {The selection of third-party libraries is an essential element of virtually any software development project. However, deciding which libraries to choose is a challenging practical problem. Selecting the wrong library can severely impact a software project in terms of cost, time, and development effort, with the severity of the impact depending on the role of the library in the software architecture, among others. Despite the importance of following a careful library selection process, in practice, the selection of third-party libraries is still conducted in an ad-hoc manner, where dozens of factors play an influential role in the decision.},
	booktitle = {{ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Larios Vargas, Enrique and Aniche, Maurício and Treude, Christoph and Bruntink, Magiel and Gousios, Georgios},
	year = {2020},
}

@article{Wing2021TrustworthyAI,
	title = {Trustworthy {AI}},
	abstract = {The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.},
	journal = {Communications of the ACM},
	author = {Wing, Jeannette M.},
	year = {2021},
}

@article{Marijan2020SWTesting4ML,
	title = {Software {Testing} for {Machine} {Learning}},
	doi = {10.1609/aaai.v34i09.7084},
	abstract = {Machine learning has become prevalent across a wide variety of applications. Unfortunately, machine learning has also shown to be susceptible to deception, leading to errors, and even fatal failures. This circumstance calls into question the widespread use of machine learning, especially in safety-critical applications, unless we are able to assure its correctness and trustworthiness properties. Software verification and testing are established technique for assuring such properties, for example by detecting errors. However, software testing challenges for machine learning are vast and profuse - yet critical to address. This summary talk discusses the current state-of-the-art of software testing for machine learning. More specifically, it discusses six key challenge areas for software testing of machine learning systems, examines current approaches to these challenges and highlights their limitations. The paper provides a research agenda with elaborated directions for making progress toward advancing the state-of-the-art on testing of machine learning.},
	journal = {AAAI Conference on Artificial Intelligence},
	author = {Marijan, Dusica and Gotlieb, Arnaud},
	year = {2020},
	keywords = {Summary Talks},
}

@article{He2021AutoMLSurvey,
	title = {{AutoML}: {A} survey of the state-of-the-art},
	abstract = {Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods –covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) – with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and further discuss the following subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.},
	journal = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	year = {2021},
}

@article{mohagheghi_quality_2007,
	title = {Quality, productivity and economic benefits of software reuse: a review of industrial studies},
	volume = {12},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Quality, productivity and economic benefits of software reuse},
	url = {http://link.springer.com/10.1007/s10664-007-9040-x},
	doi = {10.1007/s10664-007-9040-x},
	abstract = {Systematic software reuse is proposed to increase productivity and software quality and lead to economic benefits. Reports of successful software reuse programs in industry have been published. However, there has been little effort to organize the evidence systematically and appraise it. This review aims to assess the effects of software reuse in industrial contexts. Journals and major conferences between 1994 and 2005 were searched to find observational studies and experiments conducted in industry, returning eleven papers of observational type. Systematic software reuse is significantly related to lower problem (defect, fault or error) density in five studies and to decreased effort spent on correcting problems in three studies. The review found evidence for significant gains in apparent productivity in three studies. Other significant benefits of software reuse were reported in single studies or the results were inconsistent. Evidence from industry is sparse and combining results was done by vote-counting. Researchers should pay more attention to using comparable metrics, performing longitudinal studies, and explaining the results and impact on industry. For industry, evaluating reuse of COTS or OSS components, integrating reuse activities in software processes, better data collection and evaluating return on investment are major challenges.},
	language = {en},
	number = {5},
	urldate = {2022-08-31},
	journal = {Empirical Software Engineering},
	author = {Mohagheghi, Parastoo and Conradi, Reidar},
	month = sep,
	year = {2007},
	pages = {471--516},
}

@misc{VanOort2022SWQualityofMLProjects,
	title = {"{Project} smells" -- {Experiences} in {Analysing} the {Software} {Quality} of {ML} {Projects} with mllint},
	url = {http://arxiv.org/abs/2201.08246},
	abstract = {Machine Learning (ML) projects incur novel challenges in their development and productionisation over traditional software applications, though established principles and best practices in ensuring the project's software quality still apply. While using static analysis to catch code smells has been shown to improve software quality attributes, it is only a small piece of the software quality puzzle, especially in the case of ML projects given their additional challenges and lower degree of Software Engineering (SE) experience in the data scientists that develop them. We introduce the novel concept of project smells which consider deficits in project management as a more holistic perspective on software quality in ML projects. An open-source static analysis tool mllint was also implemented to help detect and mitigate these. Our research evaluates this novel concept of project smells in the industrial context of ING, a global bank and large software- and data-intensive organisation. We also investigate the perceived importance of these project smells for proof-of-concept versus production-ready ML projects, as well as the perceived obstructions and benefits to using static analysis tools such as mllint. Our findings indicate a need for context-aware static analysis tools, that fit the needs of the project at its current stage of development, while requiring minimal configuration effort from the user.},
	author = {van Oort, Bart and Cruz, Luís and Loni, Babak and van Deursen, Arie},
	year = {2022},
	keywords = {68-06, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2022-02-23},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{michael2019RegexesareHard,
	title = {Regexes are {Hard}: {Decision}-{Making}, {Difficulties}, and {Risks} in {Programming} {Regular} {Expressions}},
	abstract = {Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Michael, Louis G. and Donohue, James and Davis, James C. and Lee, Dongyoon and Servant, Francisco},
	year = {2019},
	keywords = {Decision making, Face, Interviews, Programming profession, Software, Tools, regular expressions, developer process, qualitative research},
}

@inproceedings{Bacchelli2013CodeReview,
	title = {Expectations, outcomes, and challenges of modern code review},
	abstract = {Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more “lightweight” than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classified hundreds of review comments across diverse teams at Microsoft. Our study reveals that while finding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional benefits such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we find that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Bacchelli, Alberto and Bird, Christian},
	year = {2013},
	keywords = {Context, Guidelines, Inspection, Interviews, Knowledge transfer, Software, Sorting},
}

@misc{STRIDEAnalysis,
	title = {The {STRIDE} {Threat} {Model}},
	url = {https://docs.microsoft.com/en-us/azure/security/develop/threat-modeling-tool-threats},
	author = {Microsoft},
	year = {2022},
}

@inproceedings{Smith2020HarnessingMLEcosystem,
	title = {The {Machine} {Learning} {Bazaar}: {Harnessing} the {ML} {Ecosystem} for {Effective} {System} {Development}},
	abstract = {As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of “pipeline jungles” — brittle, ad hoc ML systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce ML primitives, a unified API and specification for data processing and ML components from different software libraries. Next, we compose primitives into usable ML pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of AutoML strategies — Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end AutoML system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world ML tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Smith, Micah J. and Sala, Carles and Kanter, James Max and Veeramachaneni, Kalyan},
	month = jun,
	year = {2020},
}

@misc{Brundage2020TowardTrustworthyAIDevelopment,
	address = {arXiv},
	title = {Toward {Trustworthy} {AI} {Development}: {Mechanisms} for {Supporting} {Verifiable} {Claims}},
	url = {https://arxiv.org/abs/2004.07213},
	abstract = {With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.},
	author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Théo and Rubinovitz, J. B. and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and de Haas, Sarah and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and hÉigeartaigh, Seán Ó and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and Herbert-Voss, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
	year = {2020},
	keywords = {Computer Science - Computers and Society},
}

@inproceedings{Baltes2018SOTorrent,
	title = {{SOTorrent}: {Reconstructing} and {Analyzing} the {Evolution} of {Stack} {Overflow} {Posts}},
	abstract = {Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Baltes, Sebastian},
	year = {2018},
}

@inproceedings{Gousios2012GHTorrent,
	title = {{GHTorrent}: {Github}'s data from a firehose},
	abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.},
	booktitle = {{MSR}},
	author = {Gousios, Georgios and Spinellis, Diomidis},
	year = {2012},
	keywords = {Communities, Distributed databases, Electronic mail, GitHub, Organizations, Peer to peer computing, Protocols, commits, dataset, events, repository},
}

@article{Injadat2021MLtowardsIntelligentSystems,
	title = {Machine learning towards intelligent systems: applications, challenges, and opportunities},
	abstract = {The emergence and continued reliance on the Internet and related technologies has resulted in the generation of large amounts of data that can be made available for analyses. However, humans do not possess the cognitive capabilities to understand such large amounts of data. Machine learning (ML) provides a mechanism for humans to process large amounts of data, gain insights about the behavior of the data, and make more informed decision based on the resulting analysis. ML has applications in various fields. This review focuses on some of the fields and applications such as education, healthcare, network security, banking and finance, and social media. Within these fields, there are multiple unique challenges that exist. However, ML can provide solutions to these challenges, as well as create further research opportunities. Accordingly, this work surveys some of the challenges facing the aforementioned fields and presents some of the previous literature works that tackled them. Moreover, it suggests several research opportunities that benefit from the use of ML to address these challenges.},
	journal = {Artificial Intelligence Review},
	author = {Injadat, MohammadNoor and Moubayed, Abdallah and Nassif, Ali Bou and Shami, Abdallah},
	year = {2021},
}

@misc{Neptune2021MLModelRegistry,
	title = {{ML} {Model} {Registry}: {What} {It} {Is}, {Why} {It} {Matters}, {How} to {Implement} {It}},
	shorttitle = {{ML} {Model} {Registry}},
	url = {https://neptune.ai/blog/ml-model-registry},
	abstract = {Why do you have to know more about model registry? If you were once the only data scientist on your team you can probably relate to this: you start working on a machine learning project and perform a series of experiments that produce various models (and artifacts) that you “track” through non-standard naming conventions. Since […]},
	journal = {neptune.ai},
	year = {2021},
}

@misc{kaplan_survey_2021,
	title = {A {Survey} on {Common} {Threats} in npm and {PyPi} {Registries}},
	url = {http://arxiv.org/abs/2108.09576},
	abstract = {Software engineers regularly use JavaScript and Python for both front-end and back-end automation tasks. On top of JavaScript and Python, there are several frameworks to facilitate automation tasks further. Some of these frameworks are Node Manager Package (npm) and Python Package Index (PyPi), which are open source (OS) package libraries. The public registries npm and PyPi use to host packages allow any user with a verified email to publish code. The lack of a comprehensive scanning tool when publishing to the registry creates security concerns. Users can report malicious code on the registry; however, attackers can still cause damage until they remove their tool from the platform. Furthermore, several packages depend on each other, making them more vulnerable to a bad package in the dependency tree. The heavy code reuse creates security artifacts developers have to consider, such as the package reach. This project will illustrate a high-level overview of common risks associated with OS registries and the package dependency structure. There are several attack types, such as typosquatting and combosquatting, in the OS package registries. Outdated packages pose a security risk, and we will examine the extent of technical lag present in the npm environment. In this paper, our main contribution consists of a survey of common threats in OS registries. Afterward, we will offer countermeasures to mitigate the risks presented. These remedies will heavily focus on the applications of Machine Learning (ML) to detect suspicious activities. To the best of our knowledge, the ML-focused countermeasures are the first proposed possible solutions to the security problems listed. In addition, this project is the first survey of threats in npm and PyPi, although several studies focus on a subset of threats.},
	urldate = {2022-08-26},
	publisher = {arXiv},
	author = {Kaplan, Berkay and Qian, Jingyu},
	month = aug,
	year = {2021},
	note = {arXiv:2108.09576 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@misc{arani_comprehensive_2022,
	title = {A {Comprehensive} {Study} of {Real}-{Time} {Object} {Detection} {Networks} {Across} {Multiple} {Domains}: {A} {Survey}},
	shorttitle = {A {Comprehensive} {Study} of {Real}-{Time} {Object} {Detection} {Networks} {Across} {Multiple} {Domains}},
	url = {http://arxiv.org/abs/2208.10895},
	abstract = {Deep neural network based object detectors are continuously evolving and are used in a multitude of applications, each having its own set of requirements. While safety-critical applications need high accuracy and reliability, low-latency tasks need resource and energy-efficient networks. Real-time detectors, which are a necessity in high-impact real-world applications, are continuously proposed, but they overemphasize the improvements in accuracy and speed while other capabilities such as versatility, robustness, resource and energy efficiency are omitted. A reference benchmark for existing networks does not exist, nor does a standard evaluation guideline for designing new networks, which results in ambiguous and inconsistent comparisons. We, thus, conduct a comprehensive study on multiple real-time detectors (anchor-, keypoint-, and transformer-based) on a wide range of datasets and report results on an extensive set of metrics. We also study the impact of variables such as image size, anchor dimensions, confidence thresholds, and architecture layers on the overall performance. We analyze the robustness of detection networks against distribution shifts, natural corruptions, and adversarial attacks. Also, we provide a calibration analysis to gauge the reliability of the predictions. Finally, to highlight the real-world impact, we conduct two unique case studies, on autonomous driving and healthcare applications. To further gauge the capability of networks in critical real-time applications, we report the performance after deploying the detection networks on edge devices. Our extensive empirical study can act as a guideline for the industrial community to make an informed choice on the existing networks. We also hope to inspire the research community towards a new direction in the design and evaluation of networks that focuses on a bigger and holistic overview for a far-reaching impact.},
	urldate = {2022-08-25},
	publisher = {arXiv},
	author = {Arani, Elahe and Gowda, Shruthi and Mukherjee, Ratnajit and Magdy, Omar and Kathiresan, Senthilkumar and Zonooz, Bahram},
	month = aug,
	year = {2022},
	note = {arXiv:2208.10895 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{Abdellatif2020SimplifyingSearchofNPMPackages,
	title = {Simplifying the {Search} of npm {Packages}},
	abstract = {Objective: The goal of this paper is to empirically improve the eﬃciency of npms by simplifying the used components without impacting the current npms package ranks.
Method: We use feature selection methods with the aim of simplifying npms’ equations. We remove the features that do not have a signiﬁcant eﬀect on the package’s rank. Then, we study the impact of the simpliﬁed npms on the packages’ rank, the amount of resources saved compared to the original npms, and the performance of the simpliﬁed npms as npm evolves.
Results: Our ﬁndings indicate that (1) 31\% of the unique variables of npms’ equation can be removed without breaking the original packages’ ranks; (2) The simpliﬁed npms, on average, preserves the overlapping of the packages by 98\% and the ranking of those packages by 97\%; (3) Using the simpliﬁed npms saves 10\% of packages scoring time and more than 1.47 million network requests on each scoring run; (4) As the npm evolve through a period of 12 months, the simpliﬁed-npms was able to achieve results similar to the original npms.
Conclusion: Our results show that the simpliﬁed npms preserves the original ranks of packages and is more eﬃcient than the original npms. We believe that using our approach, helps the npms community speed up the scoring process by saving computational resources and time.},
	journal = {Information and Software Technology},
	author = {Abdellatif, Ahmad and Zeng, Yi and Elshafei, Mohamed and Shihab, Emad and Shang, Weiyi},
	year = {2020},
}

@misc{noauthor_trustworthy_nodate,
	title = {Trustworthy {PTNNs}},
	url = {https://www.overleaf.com/project/629661e7c60a7b25a4c76547},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2022-08-19},
}

@book{Guest2011ThematicAnalysi,
	title = {Applied {Thematic} {Analysis}},
	publisher = {SAGE Publications},
	author = {Guest, Greg and MacQueen, Kathleen and Namey, Emily},
	year = {2011},
}

@article{Decan2019DependencyNetworkEvolution,
	title = {An empirical comparison of dependency network evolution in seven software packaging ecosystems},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Decan, Alexandre and Mens, Tom and Grosjean, Philippe},
	year = {2019},
}

@article{Mujahid2022CharacteristicsofHighlySlectedPackages,
	title = {What are the {Characteristics} of {Highly}-{Selected} {Packages}? {A} {Case} {Study} on the {NPM} {Ecosystem}},
	abstract = {With the popularity of software ecosystems, the number of open source components (known as packages) has grown rapidly. Identifying high-quality and well-maintained packages from a large pool of packages to depend on is a basic and important problem, as it is beneﬁcial for various applications, such as package recommendation and package search. However, no systematic and comprehensive work focuses on addressing this problem except in online discussions or informal literature and interviews. To ﬁll this gap, in this paper, we conducted a mixed qualitative and quantitative analysis to understand how developers identify and select relevant open source packages. In particular, we started by surveying 118 JavaScript developers from the npm ecosystem to qualitatively understand the factors that make a package to be highly-selected within the npm ecosystem. The survey results showed that JavaScript developers believe that highly-selected packages are well-documented, receive a high number of stars on GitHub, have a large number of downloads, and do not suﬀer from vulnerabilities. Then, we conducted an experiment to quantitatively validate the developers’ perception of the factors that make a highly-selected package. In this analysis, we collected and mined historical data from 2,527 packages divided into highly-selected and not highly-selected packages. For each package in the dataset, we collected quantitative data to present the factors studied in the developers’ survey. Next, we used regression analysis to quantitatively investigate which of the studied factors are the most important. Our regression analysis complements our survey results about highly-selected packages. In particular, the results showed that highly-selected packages tend to be correlated by the number of downloads, stars, and how large the package’s readme ﬁle is.},
	journal = {SSRN Electronic Journal},
	author = {Mujahid, Suhaib and Abdalkareem, Rabe and Shihab, Emad},
	year = {2022},
}

@inproceedings{Rafiuzzaman2021EfficientConfigurationofPipelinedApplicationsontheEdge,
	title = {π-{Configurator}: {Enabling} {Efficient} {Configuration} of {Pipelined} {Applications} on the {Edge}},
	abstract = {Modern edge computing applications involve a computational pipeline of multiple stages. Each stage typically involves many configuration options that affect application-level quality of service. Identifying an optimal configuration is challenging but important when the applications run under resource constraints. The main challenge is that when pipelines have many stages and each stage has many settings, the overall configuration state space is exceedingly large. We propose π-Configurator, a system for sampling application-level quality of service (QoS) metrics, constructing an approximation of the configuration state space, and finally identifying an optimal configuration for the application. We demonstrate the accuracy and effectiveness of π - Configurator with four multi-stage data processing applications on resource-limited edge computing platforms. π-Configurator incurs low approximation error, and is one to two orders of magnitude faster than complete sampling approaches. The configurations identified by π-Configurator outperform those identified by existing local adaptation approaches by 99\%.},
	booktitle = {International {Conference} on {Internet}-of-{Things} {Design} and {Implementation} ({IoTDI})},
	author = {Rafiuzzaman, Mohammad and Gopalakrishnan, Sathish and Pattabiraman, Karthik},
	year = {2022},
	keywords = {Approximation error, Data processing, Edge computing, IoT, Measurement, Pipelines, Quality of service, complex state-space, general framework, global configuration, pipelined applications, state space approximation, symbolic state space, systematic sampling},
}

@inproceedings{xin_production_2021,
	title = {Production {Machine} {Learning} {Pipelines}: {Empirical} {Analysis} and {Optimization} {Opportunities}},
	shorttitle = {Production {Machine} {Learning} {Pipelines}},
	abstract = {Machine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike the traditional perception of ML in research, ML production pipelines are complex, with many interlocking analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, and complexity of these pipelines to understand how data management research can be used to make them more efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four months, in an effort to understand the complexity and challenges underlying production ML. Our analysis reveals the characteristics, components, and topologies of typical industrystrength ML pipelines at various granularities. Along the way, we introduce a specialized data model for representing and reasoning about repeatedly run components in these ML pipelines, which we call model graphlets. We identify several rich opportunities for optimization, leveraging traditional data management ideas. We show how targeting even one of these opportunities, i.e., identifying and pruning wasted computation that does not translate to model deployment, can reduce wasted computation cost by 50\% without compromising the model deployment cadence.},
	booktitle = {International {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Xin, Doris and Miao, Hui and Parameswaran, Aditya and Polyzotis, Neoklis},
	year = {2021},
}

@inproceedings{WU2017,
	title = {Domain-specific modelware: to make the machine learning model reusable and reproducible},
	booktitle = {International {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Wu, Xizhu and Zhou, Zhihua},
	year = {2017},
	keywords = {machine learning, model, model specification, modelware},
}

@inproceedings{Nahar2022CollaborationChallengesinBuildingMLSystems,
	title = {Collaboration {Challenges} in {Building} {ML}-{Enabled} {Systems}: {Communication}, {Documentation}, {Engineering}, and {Process}},
	abstract = {The introduction of machine learning (ML) components in software projects has created the need for software engineers to collaborate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through interviews with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common collaboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. We find that most of these challenges center around communication, documentation, engineering, and process and collect recommendations to address these challenges.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Nahar, Nadia and Zhou, Shurui and Lewis, Grace and Kästner, Christian},
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{Nikitin2022AutomatedEvolutionaryApproach4theDesignofCompositeMLPipelines,
	title = {Automated evolutionary approach for the design of composite machine learning pipelines},
	abstract = {The effectiveness of the machine learning methods for real-world tasks depends on the proper structure of the modeling pipeline. The proposed approach is aimed to automate the design of composite machine learning pipelines, which is equivalent to computation workflows that consist of models and data operations. The approach combines key ideas of both automated machine learning and workflow management systems. It designs the pipelines with a customizable graph-based structure, analyzes the obtained results, and reproduces them. The evolutionary approach is used for the flexible identification of pipeline structure. The additional algorithms for sensitivity analysis, atomization, and hyperparameter tuning are implemented to improve the effectiveness of the approach. Also, the software implementation on this approach is presented as an open-source framework. The set of experiments is conducted for the different datasets and tasks (classification, regression, time series forecasting). The obtained results confirm the correctness and effectiveness of the proposed approach in the comparison with the state-of-the-art competitors and baseline solutions.},
	journal = {Future Generation Computer Systems},
	author = {Nikitin, Nikolay O. and Vychuzhanin, Pavel and Sarafanov, Mikhail and Polonskaia, Iana S. and Revin, Ilia and Barabanova, Irina V. and Maximov, Gleb and Kalyuzhnaya, Anna V. and Boukhanovsky, Alexander},
	year = {2022},
}

@inproceedings{Chan2021ResilienceofNNEnsemblesAgainstFaultyTrainingData,
	title = {Understanding the {Resilience} of {Neural} {Network} {Ensembles} against {Faulty} {Training} {Data}},
	abstract = {Machine learning is becoming more prevalent in safety-critical systems like autonomous vehicles and medical imaging. Faulty training data, where data is either misla-belled, missing, or duplicated, can increase the chance of misclassification, resulting in serious consequences. In this paper, we evaluate the resilience of ML ensembles against faulty training data, in order to understand how to build better ensembles. To support our evaluation, we develop a fault injection framework to systematically mutate training data, and introduce two diversity metrics that capture the distribution and entropy of predicted labels. Our experiments find that ensemble learning is more resilient than any individual model and that high accuracy neural networks are not necessarily more resilient to faulty training data. Further, we find that simple majority voting suffices in most cases for resilience in ML ensembles. Finally, we observe diminishing returns for resilience as we increase the number of models in an ensemble. These findings can help machine learning developers build ensembles that are both more resilient and more efficient.},
	booktitle = {International {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Chan, Abraham and Narayanan, Niranjhana and Gujarati, Arpan and Pattabiraman, Karthik and Gopalakrishnan, Sathish},
	year = {2021},
	keywords = {Data models, Entropy, Error resilience, Machine learning, Measurement, Neural networks, Software quality, Training, Training data},
}

@inproceedings{chan_understanding_2021,
	title = {Understanding the {Resilience} of {Neural} {Network} {Ensembles} against {Faulty} {Training} {Data}},
	doi = {10.1109/QRS54544.2021.00118},
	abstract = {Machine learning is becoming more prevalent in safety-critical systems like autonomous vehicles and medical imaging. Faulty training data, where data is either misla-belled, missing, or duplicated, can increase the chance of misclassification, resulting in serious consequences. In this paper, we evaluate the resilience of ML ensembles against faulty training data, in order to understand how to build better ensembles. To support our evaluation, we develop a fault injection framework to systematically mutate training data, and introduce two diversity metrics that capture the distribution and entropy of predicted labels. Our experiments find that ensemble learning is more resilient than any individual model and that high accuracy neural networks are not necessarily more resilient to faulty training data. Further, we find that simple majority voting suffices in most cases for resilience in ML ensembles. Finally, we observe diminishing returns for resilience as we increase the number of models in an ensemble. These findings can help machine learning developers build ensembles that are both more resilient and more efficient.},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Chan, Abraham and Narayanan, Niranjhana and Gujarati, Arpan and Pattabiraman, Karthik and Gopalakrishnan, Sathish},
	year = {2021},
	note = {ISSN: 2693-9177},
	keywords = {Data models, Entropy, Error resilience, Machine learning, Measurement, Neural networks, Software quality, Training, Training data},
	pages = {1100--1111},
}

@inproceedings{Pan2022DecomposingCNN,
	address = {Pittsburgh Pennsylvania},
	title = {Decomposing convolutional neural networks into reusable and replaceable modules},
	abstract = {Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously build CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replaceability in various scenarios. However, this work is limited to the dense layers and based on the one-toone relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77\% and 0.85\% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3\% and 0.5\% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by ∼37 times compared to training the model from scratch.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {ACM},
	author = {Pan, Rangeet and Rajan, Hridesh},
	year = {2022},
}

@inproceedings{chakraborty_bias_2021,
	title = {Bias in {Machine} {Learning} {Software}: {Why}? {How}? {What} to do?},
	shorttitle = {Bias in {Machine} {Learning} {Software}},
	url = {http://arxiv.org/abs/2105.12195},
	doi = {10.1145/3468264.3468537},
	abstract = {Increasingly, software is making autonomous decisions in case of criminal sentencing, approving credit cards, hiring employees, and so on. Some of these decisions show bias and adversely affect certain social groups (e.g. those defined by sex, race, age, marital status). Many prior works on bias mitigation take the following form: change the data or learners in multiple ways, then see if any of that improves fairness. Perhaps a better approach is to postulate root causes of bias and then applying some resolution strategy.},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Chakraborty, Joymallya and Majumder, Suvodeep and Menzies, Tim},
	month = aug,
	year = {2021},
	note = {arXiv:2105.12195 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	pages = {429--440},
}

@misc{lin_truthfulqa_2022,
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {http://arxiv.org/abs/2109.07958},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2022-08-11},
	publisher = {arXiv},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = may,
	year = {2022},
	note = {arXiv:2109.07958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{lin_truthfulqa_2022-1,
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {http://arxiv.org/abs/2109.07958},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2022-08-11},
	publisher = {arXiv},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = may,
	year = {2022},
	note = {arXiv:2109.07958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{Vishnu2021TechReport,
	title = {An {Experience} {Report} on {Machine} {Learning} {Reproducibility}: {Guidance} for {Practitioners} and {TensorFlow} {Model} {Garden} {Contributors}},
	url = {http://arxiv.org/abs/2107.00821},
	abstract = {Machine learning techniques are becoming a fundamental tool for scientific and engineering progress. These techniques are applied in contexts as diverse as astronomy and spam filtering. However, correctly applying these techniques requires careful engineering. Much attention has been paid to the technical potential; relatively little attention has been paid to the software engineering process required to bring research-based machine learning techniques into practical utility. Technology companies have supported the engineering community through machine learning frameworks such as TensorFLow and PyTorch, but the details of how to engineer complex machine learning models in these frameworks have remained hidden. To promote best practices within the engineering community, academic institutions and Google have partnered to launch a Special Interest Group on Machine Learning Models (SIGMODELS) whose goal is to develop exemplary implementations of prominent machine learning models in community locations such as the TensorFlow Model Garden (TFMG). The purpose of this report is to define a process for reproducing a state-of-the-art machine learning model at a level of quality suitable for inclusion in the TFMG. We define the engineering process and elaborate on each step, from paper analysis to model release. We report on our experiences implementing the YOLO model family with a team of 26 student researchers, share the tools we developed, and describe the lessons we learned along the way.},
	author = {Banna, Vishnu and Chinnakotla, Akhil and Yan, Zhengxin and Vegesana, Anirudh and Vivek, Naveen and Krishnappa, Kruthi and Jiang, Wenxin and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{Hosny2019ModelHubPaper,
	title = {{ModelHub}.{AI}: {Dissemination} {Platform} for {Deep} {Learning} {Models}},
	url = {http://arxiv.org/abs/1911.13218},
	abstract = {Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present ModelHub.AI (www.modelhub.ai), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on ModelHub.AI and allows both researchers and developers to utilize models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.},
	author = {Hosny, Ahmed and Schwier, Michael and Berger, Christoph and Örnek, Evin P. and Turan, Mehmet and Tran, Phi V. and Weninger, Leon and Isensee, Fabian and Maier-Hein, Klaus H. and McKinley, Richard and Lu, Michael T. and Hoffmann, Udo and Menze, Bjoern and Bakas, Spyridon and Fedorov, Andriy and Aerts, Hugo JWL},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{ModelZooWeb,
	title = {Model {Zoo} - {Deep} learning code and pretrained models},
	url = {https://modelzoo.co/},
	urldate = {2022-01-18},
	author = {Jing, Yu Koh},
	year = {2021},
}

@article{idowu_asset_2022,
	title = {Asset {Management} in {Machine} {Learning}: {State}-of-research and {State}-of-practice},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Asset {Management} in {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3543847},
	doi = {10.1145/3543847},
	abstract = {Machine learning components are essential for today’s software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets.
            This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning asset management. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces. We identify asynchronous collaboration and asset reusability as directions for new tools and techniques.},
	language = {en},
	urldate = {2022-08-04},
	journal = {ACM Computing Surveys},
	author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
	month = jun,
	year = {2022},
	pages = {3543847},
}

@inproceedings{idowu_asset_2021,
	title = {Asset {Management} in {Machine} {Learning}: {A} {Survey}},
	shorttitle = {Asset {Management} in {Machine} {Learning}},
	doi = {10.1109/ICSE-SEIP52600.2021.00014},
	abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
	year = {2021},
	keywords = {Asset management, Control systems, Machine learning, Monitoring, Software engineering, Software systems, Tools, asset management, experiment management, machine learning, se4ml},
	pages = {51--60},
}

@misc{kapoor_leakage_2022,
	title = {Leakage and the {Reproducibility} {Crisis} in {ML}-based {Science}},
	url = {http://arxiv.org/abs/2207.07048},
	abstract = {The use of machine learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. In this paper, we systematically investigate reproducibility issues in ML-based science. We show that data leakage is indeed a widespread problem and has led to severe reproducibility failures. Specifically, through a survey of literature in research communities that adopted ML methods, we find 17 fields where errors have been found, collectively affecting 329 papers and in some cases leading to wildly overoptimistic conclusions. Based on our survey, we present a fine-grained taxonomy of 8 types of leakage that range from textbook errors to open research problems. We argue for fundamental methodological changes to ML-based science so that cases of leakage can be caught before publication. To that end, we propose model info sheets for reporting scientific claims based on ML models that would address all types of leakage identified in our survey. To investigate the impact of reproducibility errors and the efficacy of model info sheets, we undertake a reproducibility study in a field where complex ML models are believed to vastly outperform older statistical models such as Logistic Regression (LR): civil war prediction. We find that all papers claiming the superior performance of complex ML models compared to LR models fail to reproduce due to data leakage, and complex ML models don't perform substantively better than decades-old LR models. While none of these errors could have been caught by reading the papers, model info sheets would enable the detection of leakage in each case.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Kapoor, Sayash and Narayanan, Arvind},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07048 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Methodology},
}

@misc{kapoor_leakage_2022-1,
	title = {Leakage and the {Reproducibility} {Crisis} in {ML}-based {Science}},
	url = {http://arxiv.org/abs/2207.07048},
	abstract = {The use of machine learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. In this paper, we systematically investigate reproducibility issues in ML-based science. We show that data leakage is indeed a widespread problem and has led to severe reproducibility failures. Specifically, through a survey of literature in research communities that adopted ML methods, we find 17 fields where errors have been found, collectively affecting 329 papers and in some cases leading to wildly overoptimistic conclusions. Based on our survey, we present a fine-grained taxonomy of 8 types of leakage that range from textbook errors to open research problems. We argue for fundamental methodological changes to ML-based science so that cases of leakage can be caught before publication. To that end, we propose model info sheets for reporting scientific claims based on ML models that would address all types of leakage identified in our survey. To investigate the impact of reproducibility errors and the efficacy of model info sheets, we undertake a reproducibility study in a field where complex ML models are believed to vastly outperform older statistical models such as Logistic Regression (LR): civil war prediction. We find that all papers claiming the superior performance of complex ML models compared to LR models fail to reproduce due to data leakage, and complex ML models don't perform substantively better than decades-old LR models. While none of these errors could have been caught by reading the papers, model info sheets would enable the detection of leakage in each case.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Kapoor, Sayash and Narayanan, Arvind},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07048 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Methodology},
}

@misc{Dang2021AccuracyandLoss,
	title = {Accuracy and {Loss}: {Things} to {Know} about {The} {Top} 1 and {Top} 5 {Accuracy}},
	url = {https://towardsdatascience.com/accuracy-and-loss-things-to-know-about-the-top-1-and-top-5-accuracy-1d6beb8f6df3},
	abstract = {Measure the performance of our model},
	journal = {Medium},
	author = {Dang, Anh T.},
	year = {2021},
}

@article{sanyal_towards_nodate,
	title = {Towards {Data}-{Free} {Model} {Stealing} in a {Hard} {Label} {Setting}},
	abstract = {Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework1 that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim’s gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-ofthe-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.},
	language = {en},
	author = {Sanyal, Sunandini and Addepalli, Sravanti and Babu, R Venkatesh},
	pages = {10},
}

@misc{wojcik_hard_2022,
	title = {Hard hat wearing detection based on head keypoint localization},
	url = {http://arxiv.org/abs/2106.10944},
	abstract = {In recent years, a lot of attention is paid to deep learning methods in the context of vision-based construction site safety systems, especially regarding personal protective equipment. However, despite all this attention, there is still no reliable way to establish the relationship between workers and their hard hats. To answer this problem a combination of deep learning, object detection and head keypoint localization, with simple rule-based reasoning is proposed in this article. In tests, this solution surpassed the previous methods based on the relative bounding box position of different instances, as well as direct detection of hard hat wearers and non-wearers. The results show that the conjunction of novel deep learning methods with humanly-interpretable rule-based systems can result in a solution that is both reliable and can successfully mimic manual, on-site supervision. This work is the next step in the development of fully autonomous construction site safety systems and shows that there is still room for improvement in this area.},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Wójcik, Bartosz and Żarski, Mateusz and Książek, Kamil and Miszczak, Jarosław Adam and Skibniewski, Mirosław Jan},
	month = jun,
	year = {2022},
	note = {arXiv:2106.10944 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{hou_large-scale_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Large-scale security measurements on the android firmware ecosystem},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510072},
	doi = {10.1145/3510003.3510072},
	abstract = {Android is the most popular smartphone platform with over 85\% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous work focuses on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem security and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale comprehensive measurement of the Android firmware ecosystem security. Our study is based on 6,261 firmware images from 153 vendors and 602 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabilities, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, AndScanner, to complete ROM crawling, ROM parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, several interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android images, say 24.2\% and 6.1\% of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In addition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities via 38 newfound vulnerabilities by our framework, 32 of which have been assigned CVE/CNVD numbers. This study provides much new knowledge of the Android firmware ecosystem with deep understanding of software engineering security practices.},
	language = {en},
	urldate = {2022-07-25},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Hou, Qinsheng and Diao, Wenrui and Wang, Yanhao and Liu, Xiaofeng and Liu, Song and Ying, Lingyun and Guo, Shanqing and Li, Yuanzhi and Nie, Meining and Duan, Haixin},
	month = may,
	year = {2022},
	pages = {1257--1268},
}

@inproceedings{Zeager2017AdversarialLearninginCreditCardFraudDetection,
	title = {Adversarial learning in credit card fraud detection},
	abstract = {Credit card fraud is an expensive problem for many financial institutions, costing billions of dollars to companies annually. Many adversaries still evade fraud detection systems because these systems often do not include information about the adversary's knowledge of the fraud detection mechanism. This project aims to include information about the “fraudster's” motivations and knowledge base into an adaptive fraud detection system. In this project, we use a game theoretical adversarial learning approach in order to model the fraudster's best strategy and pre-emptively adapt the fraud detection system to better classify these future fraudulent transactions. Using a logistic regression classifier as the fraud detection mechanism, we initially identify the best strategy for the adversary based on the number of fraudulent transactions that go undetected, and assume that the adversary uses this strategy for future transactions in order to improve our classifier. Prior research has used game theoretic models for adversarial learning in the domains of credit card fraud and email spam, but this project adds to the literature by extending these frameworks to a practical, real-world data set. Test results show that our adversarial framework produces an increasing AUC score on validation sets over several iterations in comparison to the static model usually employed by credit card companies.},
	booktitle = {Systems and {Information} {Engineering} {Design} {Symposium} ({SIEDS})},
	author = {Zeager, Mary Frances and Sridhar, Aksheetha and Fogal, Nathan and Adams, Stephen and Brown, Donald E. and Beling, Peter A.},
	year = {2017},
	keywords = {Adaptation models, Classification algorithms, Credit cards, Game theory, Games, Gaussian mixture model, Hidden Markov models, Logistics, Oversampling, Prediction algorithms, Synthetic data},
}

@inproceedings{zeager_adversarial_2017,
	title = {Adversarial learning in credit card fraud detection},
	doi = {10.1109/SIEDS.2017.7937699},
	abstract = {Credit card fraud is an expensive problem for many financial institutions, costing billions of dollars to companies annually. Many adversaries still evade fraud detection systems because these systems often do not include information about the adversary's knowledge of the fraud detection mechanism. This project aims to include information about the “fraudster's” motivations and knowledge base into an adaptive fraud detection system. In this project, we use a game theoretical adversarial learning approach in order to model the fraudster's best strategy and pre-emptively adapt the fraud detection system to better classify these future fraudulent transactions. Using a logistic regression classifier as the fraud detection mechanism, we initially identify the best strategy for the adversary based on the number of fraudulent transactions that go undetected, and assume that the adversary uses this strategy for future transactions in order to improve our classifier. Prior research has used game theoretic models for adversarial learning in the domains of credit card fraud and email spam, but this project adds to the literature by extending these frameworks to a practical, real-world data set. Test results show that our adversarial framework produces an increasing AUC score on validation sets over several iterations in comparison to the static model usually employed by credit card companies.},
	booktitle = {2017 {Systems} and {Information} {Engineering} {Design} {Symposium} ({SIEDS})},
	author = {Zeager, Mary Frances and Sridhar, Aksheetha and Fogal, Nathan and Adams, Stephen and Brown, Donald E. and Beling, Peter A.},
	month = apr,
	year = {2017},
	keywords = {Adaptation models, Classification algorithms, Credit cards, Game theory, Games, Gaussian mixture model, Hidden Markov models, Logistics, Oversampling, Prediction algorithms, Synthetic data},
	pages = {112--116},
}

@inproceedings{tan_exploratory_2022,
	address = {Pittsburgh Pennsylvania},
	title = {An exploratory study of deep learning supply chain},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510199},
	doi = {10.1145/3510003.3510199},
	abstract = {Deep learning becomes the driving force behind many contemporary technologies and has been successfully applied in many fields. Through software dependencies, a multi-layer supply chain (SC) with a deep learning framework as the core and substantial downstream projects as the periphery has gradually formed and is constantly developing. However, basic knowledge about the structure and characteristics of the SC is lacking, which hinders effective support for its sustainable development. Previous studies on software SC usually focus on the packages in different registries without paying attention to the SCs derived from a single project. We present an empirical study on two deep learning SCs: TensorFlow and PyTorch SCs. By constructing and analyzing their SCs, we aim to understand their structure, application domains, and evolutionary factors. We find that both SCs exhibit a short and sparse hierarchy structure. Overall, the relative growth of new projects increases month by month. Projects have a tendency to attract downstream projects shortly after the release of their packages, later the growth becomes faster and tends to stabilize. We propose three criteria to identify vulnerabilities and identify 51 types of packages and 26 types of projects involved in the two SCs. A comparison reveals their similarities and differences, e.g., TensorFlow SC provides a wealth of packages in experiment result analysis, while PyTorch SC contains more specific framework packages. By fitting the GAM model, we find that the number of dependent packages is significantly negatively associated with the number of downstream projects, but the relationship with the number of authors is nonlinear. Our findings ∗Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.},
	language = {en},
	urldate = {2022-07-22},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Tan, Xin and Gao, Kai and Zhou, Minghui and Zhang, Li},
	month = may,
	year = {2022},
	pages = {86--98},
}

@misc{Khalel2018PixelwiseObjectLabelingusingStackedUNets,
	title = {Automatic {Pixelwise} {Object} {Labeling} for {Aerial} {Imagery} {Using} {Stacked} {U}-{Nets}},
	url = {http://arxiv.org/abs/1803.04953},
	abstract = {Automation of objects labeling in aerial imagery is a computer vision task with numerous practical applications. Fields like energy exploration require an automated method to process a continuous stream of imagery on a daily basis. In this paper we propose a pipeline to tackle this problem using a stack of convolutional neural networks (U-Net architecture) arranged end-to-end. Each network works as post-processor to the previous one. Our model outperforms current state-of-the-art on two different datasets: Inria Aerial Image Labeling dataset and Massachusetts Buildings dataset each with different characteristics such as spatial resolution, object shapes and scales. Moreover, we experimentally validate computation time savings by processing sub-sampled images and later upsampling pixelwise labeling. These savings come at a negligible degradation in segmentation quality. Though the conducted experiments in this paper cover only aerial imagery, the technique presented is general and can handle other types of images.},
	publisher = {arXiv},
	author = {Khalel, Andrew and El-Saban, Motaz},
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{HuggingFacePaper2020,
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and {others}},
	year = {2020},
}

@misc{NeptuneAI2022MLModelRegistry,
	title = {{ML} {Model} {Registry}: {What} {It} {Is}, {Why} {It} {Matters}, {How} to {Implement} {It} - neptune.ai},
	url = {https://neptune.ai/blog/ml-model-registry},
	urldate = {2022-06-10},
	author = {Oladele, Stephen},
	year = {2022},
}

@article{Shakhatreh2019UAVCivilApplicationsandChallenges,
	title = {Unmanned {Aerial} {Vehicles} ({UAVs}): {A} {Survey} on {Civil} {Applications} and {Key} {Research} {Challenges}},
	abstract = {The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than \$45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.},
	journal = {IEEE Access},
	author = {Shakhatreh, Hazim and Sawalmeh, Ahmad H. and Al-Fuqaha, Ala and Dou, Zuochao and Almaita, Eyad and Khalil, Issa and Othman, Noor Shamsiah and Khreishah, Abdallah and Guizani, Mohsen},
	year = {2019},
	keywords = {Civil infrastructure inspection, Communication system security, Market research, Security, Surveillance, UAVs, Unmanned aerial vehicles, Wireless communication, Wireless sensor networks, delivery of goods, precision agriculture, real-time monitoring, remote sensing, search and rescue, security and surveillance, wireless coverage},
}

@misc{ClocTool,
	title = {cloc},
	copyright = {GPL-2.0},
	url = {https://github.com/AlDanial/cloc},
	abstract = {cloc counts blank lines, comment lines, and physical lines of source code in many programming languages.},
	author = {AlDanial},
	year = {2022},
	keywords = {cloc, count-lines, programming-language},
}

@article{Runeson2009Guidelines4CaseStudyinSE,
	title = {Guidelines for conducting and reporting case study research in software engineering},
	abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Runeson, Per and Höst, Martin},
	year = {2009},
}

@inproceedings{Perry2004CaseStudy4SE,
	title = {Case studies for software engineers},
	abstract = {The topic of this paper was the correct use and interpretation of case studies as an empirical research method. Using an equal blend of lecture and discussion, it gave a foundation for conducting, reviewing, and reading case studies. There were lessons for software engineers as researchers who conduct and report case studies, reviewers who evaluate papers, and practitioners who are attempting to apply results from papers. The main resource for the course was the book Case Study Research: Design and Methods by Robert K. Yin. This text was supplemented with positive and negative examples from the literature.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Perry, D.E. and Sim, S.E. and Easterbrook, S.M.},
	year = {2004},
	keywords = {Books, Computer aided software engineering, Data analysis, Design methodology, Grounding, History, Paper technology, Software engineering, Solids},
}

@misc{CaffeModelZoo,
	title = {Caffe {\textbar} {Model} {Zoo}},
	url = {https://caffe.berkeleyvision.org/model_zoo.html},
	urldate = {2022-07-13},
	author = {BAIR},
}

@misc{Neptune2022LimitsofDLLanguageModels,
	title = {Can {GPT}-3 or {BERT} {Ever} {Understand} {Language}?⁠—{The} {Limits} of {Deep} {Learning} {Language} {Models}},
	shorttitle = {Can {GPT}-3 or {BERT} {Ever} {Understand} {Language}?},
	url = {https://neptune.ai/blog/gpt-3-bert-limits-of-deep-learning-language-models},
	abstract = {It’s safe to assume a topic can be considered mainstream when it is the basis for an opinion piece in the Guardian. What is unusual is when that topic is a fairly niche area that involves applying Deep Learning techniques to develop natural language models. What is even more unusual is when one of those […]},
	urldate = {2022-07-13},
	journal = {neptune.ai},
	author = {Horan, Cathal},
	year = {2020},
}

@misc{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv:2104.14294 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_yolov7_2022,
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	shorttitle = {{YOLOv7}},
	url = {http://arxiv.org/abs/2207.02696},
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02696 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{pan_decomposing_2022,
	title = {Decomposing {Convolutional} {Neural} {Networks} into {Reusable} and {Replaceable} {Modules}},
	abstract = {Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously built CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replace ability in various scenarios. However, this work is limited to the dense layers and is based on the one-to-one relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77\% and 0.85\% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3\% and 0.5\% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by {\textasciitilde}37 times compared to training the model from scratch.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Pan, Rangeet and Rajan, Hridesh},
	year = {2022},
	note = {ISSN: 1558-1225},
	keywords = {Adaptation models, Buildings, Computer architecture, Convolutional neural networks, Costs, Software engineering, Training, cnn, decomposition, deep learning, deep neural network, modularity},
	pages = {524--535},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-06-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{PackageFormatWiki,
	title = {Package format},
	url = {https://en.wikipedia.org/w/index.php?title=Package_format&oldid=1081782986},
	abstract = {A package format is a type of archive containing computer programs and additional metadata needed by package managers. While the archive file format itself may be unchanged, package formats bear additional metadata, such as a manifest file or certain directory layouts. Packages may contain either source code or executable files.
Packages may be converted from one type to another with software such as Alien.},
	language = {en},
	urldate = {2022-06-14},
	journal = {Wikipedia},
	year = {2022},
}

@incollection{kruchten_taxonomy_2019,
	address = {Cham},
	title = {A {Taxonomy} of {Software} {Engineering} {Challenges} for {Machine} {Learning} {Systems}: {An} {Empirical} {Investigation}},
	volume = {355},
	isbn = {978-3-030-19033-0 978-3-030-19034-7},
	shorttitle = {A {Taxonomy} of {Software} {Engineering} {Challenges} for {Machine} {Learning} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-030-19034-7_14},
	abstract = {Artiﬁcial intelligence enabled systems have been an inevitable part of everyday life. However, eﬃcient software engineering principles and processes need to be considered and extended when developing AI- enabled systems. The objective of this study is to identify and classify software engineering challenges that are faced by diﬀerent companies when developing software-intensive systems that incorporate machine learning components. Using case study approach, we explored the development of machine learning systems from six diﬀerent companies across various domains and identiﬁed main software engineering challenges. The challenges are mapped into a proposed taxonomy that depicts the evolution of use of ML components in software-intensive system in industrial settings. Our study provides insights to software engineering community and research to guide discussions and future research into applied machine learning.},
	language = {en},
	urldate = {2022-06-13},
	booktitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	publisher = {Springer International Publishing},
	author = {Lwakatare, Lucy Ellen and Raj, Aiswarya and Bosch, Jan and Olsson, Helena Holmström and Crnkovic, Ivica},
	editor = {Kruchten, Philippe and Fraser, Steven and Coallier, François},
	year = {2019},
	doi = {10.1007/978-3-030-19034-7_14},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {227--243},
}

@article{muiruri_practices_2022,
	title = {Practices and {Infrastructures} for {Machine} {Learning} {Systems}: {An} {Interview} {Study} in {Finnish} {Organizations}},
	volume = {55},
	issn = {1558-0814},
	shorttitle = {Practices and {Infrastructures} for {Machine} {Learning} {Systems}},
	doi = {10.1109/MC.2022.3161161},
	abstract = {Using interviews, we investigated the practices and toolchains for machine learning (ML)-enabled systems from 16 organizations across various domains in Finland. We observed some well-established artificial intelligence engineering approaches, but practices and tools are still needed for the testing and monitoring of ML-enabled systems.},
	number = {6},
	journal = {Computer},
	author = {Muiruri, Dennis and Lwakatare, Lucy Ellen and Nurminen, Jukka K. and Mikkonen, Tommi},
	month = jun,
	year = {2022},
	note = {Conference Name: Computer},
	keywords = {Artificial intelligence, Business practices, Computational modeling, Data models, Interviews, Learning systems, Machine learning, Monitoring, Organizations, Software engineering},
	pages = {18--29},
}

@article{pfefferkorn_content-oblivious_2022,
	title = {Content-{Oblivious} {Trust} and {Safety} {Techniques}: {Results} from a {Survey} of {Online} {Service} {Providers}},
	volume = {1},
	copyright = {Copyright (c) 2022 Journal of Online Trust and Safety},
	issn = {2770-3142},
	shorttitle = {Content-{Oblivious} {Trust} and {Safety} {Techniques}},
	url = {https://tsjournal.org/index.php/jots/article/view/14},
	doi = {10.54501/jots.v1i2.14},
	abstract = {We present the results of a survey about the trust and safety techniques of a group of online service providers that collectively serve billions of users. We classify techniques that require the provider to be able to access the contents of users’ files and communications at will as content dependent, and content oblivious otherwise. We find that more providers use abuse-reporting features (which are content oblivious) than other abuse-detection techniques, but that participants’ abuse-reporting tools do not consistently cover the types of abuse that users may encounter. We also find that, despite strong consensus among participating providers that automated content scanning (which is content dependent) is the most useful means of detecting child sex abuse imagery, they do not consider it to be nearly as useful for other kinds of abuse. These results indicate that content-dependent techniques do not constitute a silver bullet to protect users against abuse. They also demonstrate that the impact of end-to-end encryption (which, controversially, impedes outside access to user content) on abuse detection may vary by abuse type. These findings have implications for policy debates over the regulation of online service providers’ anti-abuse obligations and their use of end-to-end encryption.},
	language = {en},
	number = {2},
	urldate = {2022-06-12},
	journal = {Journal of Online Trust and Safety},
	author = {Pfefferkorn, Riana},
	month = feb,
	year = {2022},
	note = {Number: 2},
	keywords = {trust and safety},
}

@article{Jeyanthi2020ResNetTL4HumanActionRecognitionusingLSTM,
	title = {Inception {ResNet} deep transfer learning model for human action recognition using {LSTM}},
	abstract = {In recent days, variety of approaches using deep learning features have been proposed for human action recognition due to the worth of deep neural networks. In this work, we put forward a new deep neural network architecture based on transfer learning [TL] for human action recognition. We illustrate how to progress the recognition of human activities with a video data set of small size using transfer learning. The model constructed is based on Inception ResNet convolutional neural networks (CNN) and long short term model (LSTM).We train the model by extracting feature vectors from Inception\_ResNet\_v2 and then the output feature vectors from CNN are applied onto the RNN for learning the action sequence. Then we attempt to classify the input videos according to the model trained. He accuracy score of the model was compared against VGG16, ResNet152 and Inception\_v3 models studied on. The results prove the LSTM architecture using Inception\_ ResNet\_v2 provide the best accuracy score of 92\%, 91\% on UCI 101 and HMDB 51 data sets.},
	journal = {Materials Today: Proceedings},
	author = {Jeyanthi Suresh, A. and Visumathi, J.},
	year = {2020},
}

@inproceedings{Ren2015FasterRCNN,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
}

@article{shakhatreh_unmanned_2019,
	title = {Unmanned {Aerial} {Vehicles} ({UAVs}): {A} {Survey} on {Civil} {Applications} and {Key} {Research} {Challenges}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Unmanned {Aerial} {Vehicles} ({UAVs})},
	doi = {10.1109/ACCESS.2019.2909530},
	abstract = {The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than \$45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.},
	journal = {IEEE Access},
	author = {Shakhatreh, Hazim and Sawalmeh, Ahmad H. and Al-Fuqaha, Ala and Dou, Zuochao and Almaita, Eyad and Khalil, Issa and Othman, Noor Shamsiah and Khreishah, Abdallah and Guizani, Mohsen},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Civil infrastructure inspection, Communication system security, Market research, Security, Surveillance, UAVs, Unmanned aerial vehicles, Wireless communication, Wireless sensor networks, delivery of goods, precision agriculture, real-time monitoring, remote sensing, search and rescue, security and surveillance, wireless coverage},
	pages = {48572--48634},
}

@inproceedings{ji_model-reuse_2018,
	address = {Toronto Canada},
	title = {Model-{Reuse} {Attacks} on {Deep} {Learning} {Systems}},
	isbn = {978-1-4503-5693-0},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243757},
	doi = {10.1145/3243734.3243757},
	abstract = {Many of today’s machine learning (ML) systems are built by reusing an array of, often pre-trained, primitive models, each fulfilling distinct functionality (e.g., feature extraction). The increasing use of primitive models significantly simplifies and expedites the development cycles of ML systems. Yet, because most of such models are contributed and maintained by untrusted sources, their lack of standardization or regulation entails profound security implications, about which little is known thus far.},
	language = {en},
	urldate = {2022-06-10},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Ji, Yujie and Zhang, Xinyang and Ji, Shouling and Luo, Xiapu and Wang, Ting},
	month = oct,
	year = {2018},
	pages = {349--363},
}

@misc{Cui2020PTM4ChineseNLP,
	title = {Revisiting {Pre}-{Trained} {Models} for {Chinese} {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2004.13922},
	abstract = {Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. Resources available: https://github.com/ymcui/MacBERT},
	author = {Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Wang, Shijin and Hu, Guoping},
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@techreport{Chen2021bert2BERT,
	title = {{bert2BERT}: {Towards} {Reusable} {Pretrained} {Language} {Models}},
	url = {http://arxiv.org/abs/2110.07143},
	abstract = {In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model (e.g., BERT\_BASE) to a large model (e.g., BERT\_LARGE) through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving on Transformer-based language model, and further improve it by proposing advanced knowledge for large model's initialization. In addition, a two-stage pre-training method is proposed to further accelerate the training process. We did extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45\% and 47\% computational cost of pre-training BERT\_BASE and GPT\_BASE by reusing the models of almost their half sizes. The source code will be publicly available upon publication.},
	number = {arXiv:2110.07143},
	institution = {arXiv},
	author = {Chen, Cheng and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Qin, Yujia and Wang, Fengyu and Wang, Zhi and Chen, Xiao and Liu, Zhiyuan and Liu, Qun},
	year = {2021},
	keywords = {Computer Science - Computation and Language},
}

@book{thrun_learning_2012,
	title = {Learning to {Learn}},
	isbn = {978-1-4615-5529-2},
	abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications.  Learning to Learn is an exciting new research direction within machine learning. Similar to traditional machine-learning algorithms, the methods described in Learning to Learn induce general functions from experience. However, the book investigates algorithms that can change the way they generalize, i.e., practice the task of learning itself, and improve on it.  To illustrate the utility of learning to learn, it is worthwhile comparing machine learning with human learning. Humans encounter a continual stream of learning tasks. They do not just learn concepts or motor skills, they also learn bias, i.e., they learn how to generalize. As a result, humans are often able to generalize correctly from extremely few examples - often just a single example suffices to teach us a new thing.  A deeper understanding of computer programs that improve their ability to learn can have a large practical impact on the field of machine learning and beyond. In recent years, the field has made significant progress towards a theory of learning to learn along with practical new algorithms, some of which led to impressive results in real-world applications.  Learning to Learn provides a survey of some of the most exciting new research approaches, written by leading researchers in the field. Its objective is to investigate the utility and feasibility of computer programs that can learn how to learn, both from a practical and a theoretical point of view.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Thrun, Sebastian and Pratt, Lorien},
	month = dec,
	year = {2012},
	note = {Google-Books-ID: X\_jpBwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Information Technology},
}

@techreport{Brown2020gpt,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	institution = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@techreport{Devlin2019BERT,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	institution = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{decan_impact_2018,
	address = {Gothenburg Sweden},
	title = {On the impact of security vulnerabilities in the npm package dependency network},
	booktitle = {{International} {Conf.} on {Mining} {Software} {Repositories}},
	author = {Decan, Alexandre and Mens, Tom and Constantinou, Eleni},
	year = {2018},
}

@inproceedings{Mens2014ECOS,
	title = {{ECOS}: {Ecological} studies of open source software ecosystems},
	abstract = {Software ecosystems, collections of projects developed by the same community, are among the most complex artefacts constructed by humans. Collaborative development of open source software (OSS) has witnessed an exponential increase in two decades. Our hypothesis is that software ecosystems bear many similarities with natural ecosystems. While natural ecosystems have been the subject of study for many decades, research on software ecosystems is more recent. For this reason, the ECOS research project aims to determine whether and how selected ecological models and theories from natural ecosystems can be adapted and adopted to understand and better explain how OSS projects (akin to biological species) evolve, and to determine what are the main factors that drive the success or popularity of these projects. Expressed in biological terms, we wish to use knowledge on the evolution of natural ecosystems to provide support aiming to optimize the fitness of OSS projects, and to increase the resistance and resilience of OSS ecosystems.},
	booktitle = {{IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering}},
	author = {Mens, Tom and Claes, Maëlick and Grosjean, Philippe},
	year = {2014},
	keywords = {Communities, Ecosystems, Evolution (biology), Open source software},
}

@article{Najafabadi2015DLinBigDataAnalytics,
	title = {Deep learning applications and challenges in big data analytics},
	abstract = {Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.},
	journal = {Journal of Big Data},
	author = {Najafabadi, Maryam M and Villanustre, Flavio and Khoshgoftaar, Taghi M and Seliya, Naeem and Wald, Randall and Muharemagic, Edin},
	year = {2015},
}

@article{Daniele2018LPDLonDrones,
	title = {Ultra {Low} {Power} {Deep}-{Learning}-powered {Autonomous} {Nano} {Drones}},
	author = {Daniele, Palossi and Antonio, Loquercio and Francesco, Conti and Eric, Flamand and Davide, Scaramuzza and Luca, Benini},
	year = {2018},
}

@article{Lee2021Industry4throughML,
	title = {From technological development to social advance: {A} review of {Industry} 4.0 through machine learning},
	abstract = {Industry 4.0 has attracted considerable interest from firms, governments, and individuals as the new concept of future computer, industrial, and social systems. However, the concept has yet to be fully explored in the scientific literature. Given the topic’s broad scope, this work attempts to understand and clarify Industry 4.0 by analyzing 660 journal papers and 3,901 news articles through text mining with unsupervised machine learning algorithms. Based on the results, this work identifies 31 research and application issues related to Industry 4.0. These issues are categorized and described within a five-level hierarchy: 1) infrastructure development for connection, 2) artificial intelligence development for data-driven decision making, 3) system and process optimization, 4) in­ dustrial innovation, and 5) social advance. Further, a framework for convergence in Industry 4.0 is proposed, featuring six dimensions: connection, collection, communication, computation, control, and creation. The research outcomes are consistent with and complementary to existing relevant discussion and debate on Industry 4.0, which validates the utility and efficiency of the data-driven approach of this work to support experts’ in­ sights on Industry 4.0. This work helps establish a common ground for understanding Industry 4.0 across multiple disciplinary perspectives, enabling further research and development for industrial innovation and social advance.},
	journal = {Technological Forecasting and Social Change},
	author = {Lee, Changhun and Lim, Chiehyeon},
	year = {2021},
}

@article{Ghimire2022SurveyonEfficientCNNandHWAcceleration,
	title = {A {Survey} on {Efficient} {Convolutional} {Neural} {Networks} and {Hardware} {Acceleration}},
	abstract = {Over the past decade, deep-learning-based representations have demonstrated remarkable performance in academia and industry. The learning capability of convolutional neural networks (CNNs) originates from a combination of various feature extraction layers that fully utilize a large amount of data. However, they often require substantial computation and memory resources while replacing traditional hand-engineered features in existing systems. In this review, to improve the efﬁciency of deep learning research, we focus on three aspects: quantized/binarized models, optimized architectures, and resource-constrained systems. Recent advances in light-weight deep learning models and network architecture search (NAS) algorithms are reviewed, starting with simpliﬁed layers and efﬁcient convolution and including new architectural design and optimization. In addition, several practical applications of efﬁcient CNNs have been investigated using various types of hardware architectures and platforms.},
	journal = {Electronics},
	author = {Ghimire, Deepak and Kil, Dayoung and Kim, Seong-heum},
	year = {2022},
}

@techreport{kurita_weight_2020,
	title = {Weight {Poisoning} {Attacks} on {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2004.06660},
	abstract = {Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/neulab/RIPPLe.},
	number = {arXiv:2004.06660},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Kurita, Keita and Michel, Paul and Neubig, Graham},
	month = apr,
	year = {2020},
	note = {arXiv:2004.06660 [cs, stat]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{kurita_weight_2020-1,
	title = {Weight {Poisoning} {Attacks} on {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2004.06660},
	abstract = {Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/neulab/RIPPLe.},
	number = {arXiv:2004.06660},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Kurita, Keita and Michel, Paul and Neubig, Graham},
	month = apr,
	year = {2020},
	note = {arXiv:2004.06660 [cs, stat]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Tan2018DeepTransferLearningSurvey,
	title = {A {Survey} on {Deep} {Transfer} {Learning}},
	abstract = {As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
	journal = {IEEE Transactions on knowledge and data engineering},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Zhang2022OPT,
	title = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
	url = {http://arxiv.org/abs/2205.01068},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	journal = {arXiv},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	year = {2022},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{cohen_emnist_2017,
	title={EMNIST: Extending MNIST to handwritten letters},
	author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
	booktitle={International Joint Conference on Neural Networks (IJCNN)},
	pages={2921--2926},
	year={2017},
}


@inproceedings{cohen_emnist_2017-1,
	title = {{EMNIST}: {Extending} {MNIST} to handwritten letters},
	shorttitle = {{EMNIST}},
	doi = {10.1109/IJCNN.2017.7966217},
	abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, the relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a dataset that constitutes a more challenging classification task involving letters and digits, and one that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results using an online ELM algorithm are presented along with a validation of the conversion process through the comparison of the classification results on NIST digits and the MNIST digits.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {Benchmark testing, Databases, NIST, Training},
	pages = {2921--2926},
}

@inproceedings{Mitropoulos2014BugCataLogofMaven,
	title = {The bug catalog of the maven ecosystem},
	abstract = {Examining software ecosystems can provide the research community with data regarding artifacts, processes, and communities. We present a dataset obtained from the Maven central repository ecosystem (approximately 265gb of data) by statically analyzing the repository to detect potential software bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics results that FindBugs reports for every project version (a jar) included in the ecosystem. For every version we also stored speciﬁc metadata such as the jar’s size, its dependencies and others. Our dataset can be used to produce interesting research results, as we show in speciﬁc examples.},
	booktitle = {Working {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Mitropoulos, Dimitris and Karakoidas, Vassilios and Louridas, Panos and Gousios, Georgios and Spinellis, Diomidis},
	year = {2014},
}

@inproceedings{Zerouali2019DiversityofSWPackagePopularityMetrics,
	title = {On the {Diversity} of {Software} {Package} {Popularity} {Metrics}: {An} {Empirical} {Study} of npm},
	abstract = {Software systems often leverage on open source software libraries to reuse functionalities. Such libraries are readily available through software package managers like npm for JavaScript. Due to the huge amount of packages available in such package distributions, developers often decide to rely on or contribute to a software package based on its popularity. Moreover, it is a common practice for researchers to depend on popularity metrics for data sampling and choosing the right candidates for their studies. However, the meaning of popularity is relative and can be defined and measured in a diversity of ways, that might produce different outcomes even when considered for the same studies. In this paper, we show evidence of how different is the meaning of popularity in software engineering research. Moreover, we empirically analyse the relationship between different software popularity measures. As a case study, for a large dataset of 175k npm packages, we computed and extracted 9 different popularity metrics from three open source tracking systems: libraries.io, npmjs.com and GitHub. We found that indeed popularity can be measured with different unrelated metrics, each metric can be defined within a specific context. This indicates a need for a generic framework that would use a portfolio of popularity metrics drawing from different concepts.},
	booktitle = {International {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Zerouali, Ahmed and Mens, Tom and Robles, Gregorio and Gonzalez-Barahona, Jesus M.},
	year = {2019},
	keywords = {Correlation, Data mining, Libraries, Measurement, Runtime, Software packages, empirical analysis, npm, popularity, software package},
}

@inproceedings{SotoValero2019EmergenceofSWDiversityinMavenCentral,
	title = {The {Emergence} of {Software} {Diversity} in {Maven} {Central}},
	abstract = {Maven artifacts are immutable: an artifact that is uploaded on Maven Central cannot be removed nor modified. The only way for developers to upgrade their library is to release a new version. Consequently, Maven Central accumulates all the versions of all the libraries that are published there, and applications that declare a dependency towards a library can pick any version. In this work, we hypothesize that the immutability of Maven artifacts and the ability to choose any version naturally support the emergence of software diversity within Maven Central. We analyze 1,487,956 artifacts that represent all the versions of 73,653 libraries. We observe that more than 30\% of libraries have multiple versions that are actively used by latest artifacts. In the case of popular libraries, more than 50\% of their versions are used. We also observe that more than 17\% of libraries have several versions that are significantly more used than the other versions. Our results indicate that the immutability of artifacts in Maven Central does support a sustained level of diversity among versions of libraries in the repository.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Soto-Valero, César and Benelallam, Amine and Harrand, Nicolas and Barais, Olivier and Baudry, Benoit},
	year = {2019},
	keywords = {Data mining, History, Indexes, Java, Labeling, Libraries, Software, dataset, development history graph, digital preservation, free software, mining software repositories, open source software, source code},
}

@inproceedings{Vu2021py2src,
	title = {py2src: {Towards} the {Automatic} (and {Reliable}) {Identification} of {Sources} for {PyPI} {Package}},
	abstract = {Selecting which libraries (‘dependencies’ or ‘packages’ in the industry’s jargon) to adopt in a project is an essential task in software development. The quality of the corresponding source code is a key factor behind this selection (from security to timeliness). Yet, how easy is it to find the ‘actual’ source? How reliable is this information? To address this problem, we developed an approach called py2src to automatically identify GitHub source code repositories corresponding to packages in PyPI and automatically provide an indicator of the reliability of such information. We also report a preliminary empirical evaluation of the approach on the top PyPI packages.},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Vu, Duc-Ly},
	year = {2021},
	keywords = {Codes, Libraries, Mining software repository, PyPI, Python packages, Reliability engineering, Security, Software, Software factors, Software reliability, Software supply chain, Supply chains, quantitavie study},
}

@inproceedings{Goswami2020ReproducibilityofNPMPackages,
	title = {Investigating {The} {Reproducibility} of {NPM} {Packages}},
	abstract = {Node.js has been popularly used for web application development, partially because of its large software ecosystem known as NPM (Node Package Manager) packages. When using open-source NPM packages, most developers download prebuilt packages on npmjs.com instead of building those packages from available source, and implicitly trust the downloaded packages. However, it is unknown whether the blindly trusted prebuilt NPM packages are reproducible (i.e., whether there is always a verifiable path from source code to any published NPM package). Therefore, for this paper, we conducted an empirical study to examine the reproducibility of NPM packages, and to understand why some packages are not reproducible.Specifically, we downloaded versions/releases of 226 most popularly used NPM packages and then built each version with the available source on GitHub. Next, we applied a differencing tool to compare the versions we built against versions downloaded from NPM, and further inspected any reported difference. Among the 3,390 versions of the 226 packages, only 2,087 versions are reproducible. Based on our manual analysis, multiple factors contribute to the non-reproducibility issues, such as flexible versioning information in package.json file and the divergent behaviors between distinct versions of tools used in the build process. Our investigation reveals challenges of verifying NPM reproducibility with existing tools, and provides insights for future verifiable build procedures.},
	booktitle = {International {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Goswami, Pronnoy and Gupta, Saksham and Li, Zhiyuan and Meng, Na and Yao, Daphne},
	year = {2020},
	keywords = {JavaScript, Manuals, NPM packages, Open source software, Packaging, Software development management, Software maintenance, Standards, Tools, reproducibility},
}

@techreport{mujahid2022highlySelectedPackages,
	title = {What are the characteristics of highly-selected packages? {A} case study on the npm ecosystem},
	shorttitle = {What are the characteristics of highly-selected packages?},
	url = {http://arxiv.org/abs/2204.04562},
	abstract = {With the popularity of software ecosystems, the number of open source components (known as packages) has grown rapidly. Identifying high-quality and well-maintained packages from a large pool of packages to depend on is a basic and important problem, as it is beneficial for various applications, such as package recommendation and package search. However, no systematic and comprehensive work focuses on addressing this problem except in online discussions or informal literature and interviews. To fill this gap, in this paper, we conducted a mixed qualitative and quantitative analysis to understand how developers identify and select relevant open source packages. In particular, we started by surveying 118 JavaScript developers from the npm ecosystem to qualitatively understand the factors that make a package to be highly-selected within the npm ecosystem. The survey results showed that JavaScript developers believe that highly-selected packages are well-documented, receive a high number of stars on GitHub, have a large number of downloads, and do not suffer from vulnerabilities. Then, we conducted an experiment to quantitatively validate the developers' perception of the factors that make a highly-selected package. In this analysis, we collected and mined historical data from 2,527 packages divided into highly-selected and not highly-selected packages. For each package in the dataset, we collected quantitative data to present the factors studied in the developers' survey. Next, we used regression analysis to quantitatively investigate which of the studied factors are the most important. Our regression analysis complements our survey results about highly-selected packages. In particular, the results showed that highly-selected packages tend to be correlated by the number of downloads, stars, and how large the package's readme file is.},
	number = {arXiv:2204.04562},
	urldate = {2022-06-03},
	institution = {arXiv},
	author = {Mujahid, Suhaib and Abdalkareem, Rabe and Shihab, Emad},
	month = apr,
	year = {2022},
	keywords = {Computer Science - Software Engineering},
}

@article{Bianco2011SurveyonOpenSourceSWTrustworthiness,
	title = {A {Survey} on {Open} {Source} {Software} {Trustworthiness}},
	abstract = {Trustworthiness is a crucial characteristic when it comes to evaluating any product, even more so for open source software, which is now becoming widely used. The authors conducted a survey to identify the reasons and motivations that lead software companies to adopt or reject open source software; they then ranked, according to importance, the specific trust factors used when selecting an open source software component or product. The motivations and importance ranking of factors might be useful for both developers of open source software (to make their products and components more useful for other stakeholders) and to future prospective open source software users.},
	journal = {IEEE Software},
	author = {del Bianco, Vieri and Lavazza, Luigi and Morasca, Sandro and Taibi, Davide},
	year = {2011},
	keywords = {Open source software, Reliability, Software engineering, Software reliability, external software qualities, internal software qualities, open source software, pragmatic software engineering, trustworthiness},
}

@inproceedings{Gao2020FuzzTestingbasedDataAugtoImproveRobustnessofDNN,
	title = {Fuzz {Testing} based {Data} {Augmentation} to {Improve} {Robustness} of {Deep} {Neural} {Networks}},
	abstract = {Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9\% and 5.5\% on average. Further, Sensei-SA can reduce the average DNN training time by 25\%, while still improving robust accuracy.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Gao, Xiang and Saha, Ripon K. and Prasad, Mukul R. and Roychoudhury, Abhik},
	year = {2020},
	keywords = {DNN, Data Augmentation, Genetic Algorithm, Maintenance engineering, Neural networks, Robustness, Software systems, Test pattern generators, Tools, Training},
}

@inproceedings{Hutchison2018RobustnessTestingofAutonomySW,
	title = {Robustness {Testing} of {Autonomy} {Software}},
	abstract = {As robotic and autonomy systems become progressively more present in industrial and human-interactive applications, it is increasingly critical for them to behave safely in the presence of unexpected inputs. While robustness testing for traditional software systems is long-studied, robustness testing for autonomy systems is relatively uncharted territory. In our role as engineers, testers, and researchers we have observed that autonomy systems are importantly different from traditional systems, requiring novel approaches to effectively test them. We present Automated Stress Testing for Autonomy Architectures (ASTAA), a system that effectively, automatically robustness tests autonomy systems by building on classic principles, with important innovations to support this new domain. Over five years, we have used ASTAA to test 17 real-world autonomy systems, robots, and robotics-oriented libraries, across commercial and academic applications, discovering hundreds of bugs. We outline the ASTAA approach and analyze more than 150 bugs we found in real systems. We discuss what we discovered about testing autonomy systems, specifically focusing on how doing so differs from and is similar to traditional software robustness testing and other high-level lessons.},
	booktitle = {International {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} {Track} ({ICSE}-{SEIP})},
	author = {Hutchison, Casidhe and Zizyte, Milda and Lanigan, Patrick E. and Guttendorf, David and Wagner, Michael and Le Goues, Claire and Koopman, Philip},
	year = {2018},
	keywords = {Computer bugs, Dictionaries, Robots, Robustness, Safety, Software, Testing, autonomy, dependability, robustness testing, safety-critical systems},
}

@article{european_commission_joint_research_centre_robustness_2020,
	title = {Robustness and explainability of {Artificial} {Intelligence}: from technical to policy solutions.},
	volume = {Publications Office of the European Union},
	author = {{European Commission. Joint Research Centre.}},
	year = {2020},
}

@inproceedings{Shi2020RobustnessVerification4Transformers,
	title = {Robustness {Verification} for {Transformers}},
	volume = {arXiv},
	url = {https://arxiv.org/abs/2002.06622},
	abstract = {Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis.},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Shi, Zhouxing and Zhang, Huan and Chang, Kai-Wei and Huang, Minlie and Hsieh, Cho-Jui},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{Tong2019ImprovingRobustnessofMLClassifiers,
	title = {Improving {Robustness} of {ML} {Classiﬁers} against {Realizable} {Evasion} {Attacks} {Using} {Conserved} {Features}},
	abstract = {Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, ML models are often susceptible to evasion attacks, in which an adversary makes changes to the input (such as malware) in order to avoid being detected. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simpliﬁed feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. We investigate the effectiveness of this approach to designing robust ML in the face of attacks that can be realized in actual malware (realizable attacks). We demonstrate that in the context of structure-based PDF malware detection, such techniques appear to have limited effectiveness, but they are effective with content-based detectors. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modiﬁed without compromising malicious functionality) signiﬁcantly improves performance. Finally, we show that feature space models enable generalized robustness when faced with a variety of realizable attacks, as compared to classiﬁers which are tuned to be robust to a speciﬁc realizable attack.},
	booktitle = {{USENIX} {Security} {Symposium}},
	author = {Tong, Liang and Li, Bo and Zhang, Ning and Hajaj, Chen and Xiao, Chaowei and Vorobeychik, Yevgeniy},
	year = {2019},
}

@inproceedings{Gehr2018AI2,
	title = {{AI2}: {Safety} and {Robustness} {Certification} of {Neural} {Networks} with {Abstract} {Interpretation}},
	abstract = {We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.},
	booktitle = {{IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
	year = {2018},
	keywords = {Abstract Interpretation, Biological neural networks, Cats, Neural Networks, Neurons, Perturbation methods, Reliable Machine Learning, Robustness, Safety},
}

@article{Rasheed2021ML4healthcare,
	title = {Explainable, {Trustworthy}, and {Ethical} {Machine} {Learning} for {Healthcare}: {A} {Survey}},
	abstract = {With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a speciﬁc decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the ﬁeld of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.},
	author = {Rasheed, Khansa and Qayyum, Adnan and Ghaly, Mohammed and Al-Fuqaha, Ala and Razi, Adeel and Qadir, Junaid},
	year = {2021},
}

@article{Floridi2019Rules4TrustworthyAI,
	title = {Establishing the rules for building trustworthy {AI}},
	journal = {Nature Machine Intelligence},
	author = {Floridi, Luciano},
	year = {2019},
}

@article{Marcal2021Traceability4TrustworthyAI,
	title = {Traceability for {Trustworthy} {AI}: {A} {Review} of {Models} and {Tools}},
	abstract = {Traceability is considered a key requirement for trustworthy artiﬁcial intelligence (AI), related to the need to maintain a complete account of the provenance of data, processes, and artifacts involved in the production of an AI model. Traceability in AI shares part of its scope with general purpose recommendations for provenance as W3C PROV, and it is also supported to different extents by speciﬁc tools used by practitioners as part of their efforts in making data analytic processes reproducible or repeatable. Here, we review relevant tools, practices, and data models for traceability in their connection to building AI models and systems. We also propose some minimal requirements to consider a model traceable according to the assessment list of the High-Level Expert Group on AI. Our review shows how, although a good number of reproducibility tools are available, a common approach is currently lacking, together with the need for shared semantics. Besides, we have detected that some tools have either not achieved full maturity, or are already falling into obsolescence or in a state of near abandonment by its developers, which might compromise the reproducibility of the research trusted to them.},
	journal = {Big Data and Cognitive Computing},
	author = {Mora-Cantallops, Marçal and Sánchez-Alonso, Salvador and García-Barriocanal, Elena and Sicilia, Miguel-Angel},
	year = {2021},
}

@article{Jadhav2011Framework4EvaluationaandSelectionofSWPackages,
	title = {Framework for evaluation and selection of the software packages: {A} hybrid knowledge based system approach},
	abstract = {Evaluation and selection of the software packages is complicated and time consuming decision making process. Selection of inappropriate software package can turn out to be costly and adversely affects business processes and functioning of the organization. In this paper we describe (i) generic methodology for software selection, (ii) software evaluation criteria, and (iii) hybrid knowledge based system (HKBS) approach to assist decision makers in evaluation and selection of the software packages. The proposed HKBS approach employs an integrated rule based and case based reasoning techniques. Rule based reasoning is used to capture user needs of the software package and formulate a problem case. Case based reasoning is used to retrieve and compare candidate software packages with the user needs of the package. This paper also evaluates and compares HKBS approach with the widely used existing software evaluation techniques such as analytic hierarchy process (AHP) and weighted scoring method (WSM).},
	journal = {Journal of Systems and Software},
	author = {Jadhav, Anil S. and Sonar, Rajendra M.},
	year = {2011},
}

@inproceedings{Franch2002QualityModelBasedApproach4DescribingEvaluatingSWPackages,
	title = {A quality-model-based approach for describing and evaluating software packages},
	abstract = {Selection of software packages from user requirements is a central task in software engineering. Selection of inappropriate packages may compromise business processes and may interfere negatively in the functioning of the involved organization. Success of package selection is endangered because of many factors, one of the most important being the absence of structured descriptions of both package features and user quality requirements. In this paper, we propose a methodology for describing quality factors of software packages using the ISO/IEC quality standard as a framework. Following this standard, relevant attributes for a specific software domain are identified and structured as a hierarchy, and metrics for them are chosen. Software packages in this domain can then be described in a uniform and comprehensive way. Therefore, selection of packages can be ameliorated by transforming user quality requirements into requirements expressed in terms of quality model attributes. We illustrate the approach by presenting, in some depth, a quality model for the mail server domain.},
	booktitle = {{IEEE} {Joint} {International} {Conference} on {Requirements} {Engineering}},
	author = {Franch, X. and Carvallo, J.P.},
	year = {2002},
	keywords = {Buildings, Business, IEC standards, ISO standards, Packaging, Software engineering, Software packages, Software quality, Software standards, Taxonomy},
}

@article{Jadhav2009EvaluatingSelectingSWPackages,
	title = {Evaluating and selecting software packages: {A} review},
	abstract = {Evaluating and selecting software packages that meet an organization’s requirements is a difﬁcult software engineering process. Selection of a wrong software package can turn out to be costly and adversely affect business processes. The aim of this paper is to provide a basis to improve the process of evaluation and selection of the software packages. This paper reports a systematic review of papers published in journals and conference proceedings. The review investigates methodologies for selecting software packages, software evaluation techniques, software evaluation criteria, and systems that support decision makers in evaluating software packages. The key ﬁndings of the review are: (1) analytic hierarchy process has been widely used for evaluation of the software packages, (2) there is lack of a common list of generic software evaluation criteria and its meaning, and (3) there is need to develop a framework comprising of software selection methodology, evaluation technique, evaluation criteria, and system to assist decision makers in software selection.},
	journal = {Information and Software Technology},
	author = {Jadhav, Anil S. and Sonar, Rajendra M.},
	year = {2009},
}

@techreport{haque_ereba_2022,
	title = {{EREBA}: {Black}-box {Energy} {Testing} of {Adaptive} {Neural} {Networks}},
	shorttitle = {{EREBA}},
	url = {http://arxiv.org/abs/2202.06084},
	abstract = {Recently, various Deep Neural Network (DNN) models have been proposed for environments like embedded systems with stringent energy constraints. The fundamental problem of determining the robustness of a DNN with respect to its energy consumption (energy robustness) is relatively unexplored compared to accuracy-based robustness. This work investigates the energy robustness of Adaptive Neural Networks (AdNNs), a type of energy-saving DNNs proposed for many energy-sensitive domains and have recently gained traction. We propose EREBA, the first black-box testing method for determining the energy robustness of an AdNN. EREBA explores and infers the relationship between inputs and the energy consumption of AdNNs to generate energy surging samples. Extensive implementation and evaluation using three state-of-the-art AdNNs demonstrate that test inputs generated by EREBA could degrade the performance of the system substantially. The test inputs generated by EREBA can increase the energy consumption of AdNNs by 2,000\% compared to the original inputs. Our results also show that test inputs generated via EREBA are valuable in detecting energy surging inputs.},
	urldate = {2022-05-31},
	author = {Haque, Mirazul and Yadlapalli, Yaswanth and Yang, Wei and Liu, Cong},
	month = feb,
	year = {2022},
	doi = {10.1145/3510003.3510088},
	note = {arXiv:2202.06084 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@techreport{wan_what_2022,
	title = {What {Do} {They} {Capture}? -- {A} {Structural} {Analysis} of {Pre}-{Trained} {Language} {Models} for {Source} {Code}},
	shorttitle = {What {Do} {They} {Capture}?},
	url = {http://arxiv.org/abs/2202.06840},
	abstract = {Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., CodeBERT, and GraphCodeBERT) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.},
	number = {arXiv:2202.06840},
	urldate = {2022-05-31},
	institution = {arXiv},
	author = {Wan, Yao and Zhao, Wei and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Jin, Hai},
	month = feb,
	year = {2022},
	note = {arXiv:2202.06840 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{daoudi_lessons_2021,
	title = {Lessons {Learnt} on {Reproducibility} in {Machine} {Learning} {Based} {Android} {Malware} {Detection}},
	volume = {26},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-021-09955-7},
	doi = {10.1007/s10664-021-09955-7},
	abstract = {A well-known curse of computer security research is that it often produces systems that, while technically sound, fail operationally. To overcome this curse, the community generally seeks to assess proposed systems under a variety of settings in order to make explicit every potential bias. In this respect, recently, research achievements on machine learning based malware detection are being considered for thorough evaluation by the community. Such an effort of comprehensive evaluation supposes first and foremost the possibility to perform an independent reproduction study in order to sharpen evaluations presented by approaches’ authors. The question Can published approaches actually be reproduced? thus becomes paramount despite the little interest such mundane and practical aspects seem to attract in the malware detection field. In this paper, we attempt a complete reproduction of five Android Malware Detectors from the literature and discuss to what extent they are “reproducible”. Notably, we provide insights on the implications around the guesswork that may be required to finalise a working implementation. Finally, we discuss how barriers to reproduction could be lifted, and how the malware detection field would benefit from stronger reproducibility standards—like many various fields already have.},
	language = {en},
	number = {4},
	urldate = {2022-05-27},
	journal = {Empirical Software Engineering},
	author = {Daoudi, Nadia and Allix, Kevin and Bissyandé, Tegawendé F. and Klein, Jacques},
	month = jul,
	year = {2021},
	pages = {74},
}

@article{Ashqar2019InvasiveHydrangea_PTNN,
	title = {Identifying {Images} of {Invasive} {Hydrangea} {Using} {Pre}-{Trained} {Deep} {Convolutional} {Neural} {Networks}},
	abstract = {Invasive species are threatening habitats of native species in many countries around the world. The current methods of monitoring them depend on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient and insufficient since humans cannot cover large areas when sampling. In this paper, machine learning based approach is presented for identifying images of invasive hydrangea (a beautiful invasive species original of Asia) with a dataset that contains approximately 3,800 images taken in a Brazilian national forest and in some of the pictures there is Hydrangea. A deep learning technique that extensively applied to image recognition was used. Our trained model achieved an accuracy of 99.71\% on a held-out test set, demonstrating the feasibility of this approach.},
	journal = {International Journal of Control and Automation (IJCA)},
	author = {Ashqar, Belal A. M. and Abu-Naser, Samy S.},
	year = {2019},
}

@article{Sokipriala2021AV_PTNN,
	title = {Prediction of {Steering} {Angle} for {Autonomous} {Vehicles} {Using} {Pre}-{Trained} {Neural} {Network}},
	abstract = {Autonomous driving is one promising research area that would not only revolutionize the transportation industry but would as well save thousands of lives. accurate correct Steering angle prediction plays a crucial role in the development of the autonomous vehicle .This research attempts to design a model that would be able to clone a drivers behavior using transfer learning from pretrained VGG16, the results showed that the model was able to use less training parameters and achieved a low mean squared error(MSE) of less than 2\% without overfitting to the training set hence was able to drive on new road it was not trained on.},
	journal = {European Journal of Engineering and Technology Research},
	author = {Sokipriala, Jonah},
	year = {2021},
}

@article{subcommittee_ai_nodate,
	title = {{AI} and {Cybersecurity}: {Opportunities} and {Challenges}},
	language = {en},
	author = {Subcommittee, NSTC MLAI and Subcommittee, NSTC NITRD and Science, National and Council, Technology},
	pages = {15},
}

@article{li_empirical_2022,
	title = {An {Empirical} {Study} of {Yanked} {Releases} in the {Rust} {Package} {Registry}},
	issn = {1939-3520},
	doi = {10.1109/TSE.2022.3152148},
	abstract = {Cargo, the software packaging manager of Rust, provides a yank mechanism to support release-level deprecation, which can prevent packages from depending on yanked releases. Most prior studies focused on code-level (i.e., deprecated APIs) and package-level deprecation (i.e., deprecated packages). However, few studies have focused on release-level deprecation. In this study, we investigate how often and how the yank mechanism is used, the rationales behind its usage, and the adoption of yanked releases in the Cargo ecosystem. Our study shows that 9.6\% of the packages in Cargo have at least one yanked release, and the proportion of yanked releases kept increasing from 2014 to 2020. Package owners yank releases for other reasons than withdrawing a defective release, such as fixing a release that does not follow semantic versioning or indicating a package is removed or replaced. In addition, we found that 46\% of the packages directly adopted at least one yanked release and the yanked releases propagated through the dependency network, which leads to 1.4\% of the releases in the ecosystem having unresolved dependencies.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Li, Hao and Cogo, Filipe R. and Bezemer, Cor-Paul},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Cargo, Computer bugs, Ecosystems, Indexes, Libraries, Packaging, Release deprecation, Rust, Safety, Software, Software ecosystems, Yanking},
	pages = {1--1},
}

@inproceedings{Wittern2016JSPackageEcosystem,
	title = {A look at the dynamics of the {JavaScript} package ecosystem},
	abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node.js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem’s growth and activity, into conﬂicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
	year = {2016},
}

@article{devanbu_deep_2020,
	title = {Deep {Learning} \& {Software} {Engineering}: {State} of {Research} and {Future} {Directions}},
	shorttitle = {Deep {Learning} \& {Software} {Engineering}},
	url = {http://arxiv.org/abs/2009.08525},
	abstract = {Given the current transformative potential of research that sits at the intersection of Deep Learning (DL) and Software Engineering (SE), an NSF-sponsored community workshop was conducted in co-location with the 34th IEEE/ACM International Conference on Automated Software Engineering (ASE'19) in San Diego, California. The goal of this workshop was to outline high priority areas for cross-cutting research. While a multitude of exciting directions for future work were identified, this report provides a general summary of the research areas representing the areas of highest priority which were discussed at the workshop. The intent of this report is to serve as a potential roadmap to guide future work that sits at the intersection of SE \& DL.},
	urldate = {2022-05-17},
	journal = {arXiv:2009.08525 [cs]},
	author = {Devanbu, Prem and Dwyer, Matthew and Elbaum, Sebastian and Lowry, Michael and Moran, Kevin and Poshyvanyk, Denys and Ray, Baishakhi and Singh, Rishabh and Zhang, Xiangyu},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.08525},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{danks_value_2019,
	address = {Honolulu HI USA},
	title = {The {Value} of {Trustworthy} {AI}},
	isbn = {978-1-4503-6324-2},
	url = {https://dl.acm.org/doi/10.1145/3306618.3314228},
	doi = {10.1145/3306618.3314228},
	abstract = {Trust is one of the most critical relations in our human lives, whether trust in one another, trust in the artifacts that we use everyday, or trust of an AI system. Even a cursory examination of the literatures in human-computer interaction, human-robot interaction, and numerous other disciplines reveals a deep, persistent concern with the nature of trust in AI, and the conditions under which it can be generated, reduced, repaired, or influenced.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Danks, David},
	month = jan,
	year = {2019},
	pages = {521--522},
}

@article{liu_trustworthy_2021,
	title = {Trustworthy {AI}: {A} {Computational} {Perspective}},
	shorttitle = {Trustworthy {AI}},
	url = {http://arxiv.org/abs/2107.06641},
	abstract = {In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone's daily life and profoundly altering the course of human society. The intention of developing AI is to benefit humans, by reducing human labor, bringing everyday convenience to human lives, and promoting social good. However, recent research and AI applications show that AI can cause unintentional harm to humans, such as making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against one group. Thus, trustworthy AI has attracted immense attention recently, which requires careful consideration to avoid the adverse effects that AI may bring to humans, so that humans can fully trust and live in harmony with AI technologies. Recent years have witnessed a tremendous amount of research on trustworthy AI. In this survey, we present a comprehensive survey of trustworthy AI from a computational perspective, to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex area, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety \& Robustness, (ii) Non-discrimination \& Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability \& Auditability, and (vi) Environmental Well-Being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.},
	urldate = {2022-05-16},
	journal = {arXiv:2107.06641 [cs]},
	author = {Liu, Haochen and Wang, Yiqi and Fan, Wenqi and Liu, Xiaorui and Li, Yaxin and Jain, Shaili and Liu, Yunhao and Jain, Anil K. and Tang, Jiliang},
	month = aug,
	year = {2021},
	note = {arXiv: 2107.06641},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{franco_toward_2021,
	title = {Toward {Learning} {Trustworthily} from {Data} {Combining} {Privacy}, {Fairness}, and {Explainability}: {An} {Application} to {Face} {Recognition}},
	volume = {23},
	issn = {1099-4300},
	shorttitle = {Toward {Learning} {Trustworthily} from {Data} {Combining} {Privacy}, {Fairness}, and {Explainability}},
	url = {https://www.mdpi.com/1099-4300/23/8/1047},
	doi = {10.3390/e23081047},
	abstract = {In many decision-making scenarios, ranging from recreational activities to healthcare and policing, the use of artiﬁcial intelligence coupled with the ability to learn from historical data is becoming ubiquitous. This widespread adoption of automated systems is accompanied by the increasing concerns regarding their ethical implications. Fundamental rights, such as the ones that require the preservation of privacy, do not discriminate based on sensible attributes (e.g., gender, ethnicity, political/sexual orientation), or require one to provide an explanation for a decision, are daily undermined by the use of increasingly complex and less understandable yet more accurate learning algorithms. For this purpose, in this work, we work toward the development of systems able to ensure trustworthiness by delivering privacy, fairness, and explainability by design. In particular, we show that it is possible to simultaneously learn from data while preserving the privacy of the individuals thanks to the use of Homomorphic Encryption, ensuring fairness by learning a fair representation from the data, and ensuring explainable decisions with local and global explanations without compromising the accuracy of the ﬁnal models. We test our approach on a widespread but still controversial application, namely face recognition, using the recent FairFace dataset to prove the validity of our approach.},
	language = {en},
	number = {8},
	urldate = {2022-05-16},
	journal = {Entropy},
	author = {Franco, Danilo and Oneto, Luca and Navarin, Nicolò and Anguita, Davide},
	month = aug,
	year = {2021},
	pages = {1047},
}

@inproceedings{rawat_secure_2021,
	address = {Online Only, United States},
	title = {Secure and trustworthy machine learning/artificial intelligence for multi-domain operations},
	isbn = {978-1-5106-4329-1 978-1-5106-4330-7},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11746/2592860/Secure-and-trustworthy-machine-learning-artificial-intelligence-for-multi-domain/10.1117/12.2592860.full},
	doi = {10.1117/12.2592860},
	abstract = {Machine Learning (ML) algorithms and Artiﬁcial Intelligence (AI) systems have already had an immense impact on our society as they have shown to be able to create machine cognition comparable to or even better than human cognition for some applications. ML algorithms are now regarded as very useful for data-driven applications including resilient multi-domain operations. However, ML algorithms and AI systems can be controlled, dodged, biased, and misled through ﬂawed learning models and input data, they need robust security features and trust. Furthermore, ML algorithms and AI systems add challenges when we have (unlabeled/labeled) sparse/small data or big data for training and evaluation. It is very important to design, evaluate and test ML algorithms and AI systems that produce reliable, robust, trustworthy, explainable, and fair/unbiased outcomes to make them acceptable and reliable in mission critical multi-domain operations. ML algorithms rely on data and work on the principle of “Garbage In, Garbage Out,” which means that if the input data to learning model is corrupted or compromised, the outcomes of the ML/AI would not be optimal, reliable and trustworthy. This paper focuses on achieving secure and trustworthy machine learning and artiﬁcial intelligence operations using context aware selection of learning models and blockchain for multi-domain battleﬁeld operations.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Multi}-{Domain} {Operations} {Applications} {III}},
	publisher = {SPIE},
	author = {Rawat, Danda},
	editor = {Pham, Tien and Solomon, Latasha and Hohil, Myron E.},
	month = apr,
	year = {2021},
	pages = {99},
}

@article{hamon_bridging_2022,
	title = {Bridging the {Gap} {Between} {AI} and {Explainability} in the {GDPR}: {Towards} {Trustworthiness}-by-{Design} in {Automated} {Decision}-{Making}},
	volume = {17},
	issn = {1556-6048},
	shorttitle = {Bridging the {Gap} {Between} {AI} and {Explainability} in the {GDPR}},
	doi = {10.1109/MCI.2021.3129960},
	abstract = {Can satisfactory explanations for complex machine learning models be achieved in high-risk automated decision-making? How can such explanations be integrated into a data protection framework safeguarding a right to explanation? This article explores from an interdisciplinary point of view the connection between existing legal requirements for the explainability of AI systems set out in the General Data Protection Regulation (GDPR) and the current state of the art in the field of explainable AI. It studies the challenges of providing human legible explanations for current and future AI-based decision-making systems in practice, based on two scenarios of automated decision-making in credit scoring risks and medical diagnosis of COVID-19. These scenarios exemplify the trend towards increasingly complex machine learning algorithms in automated decision-making, both in terms of data and models. Current machine learning techniques, in particular those based on deep learning, are unable to make clear causal links between input data and final decisions. This represents a limitation for providing exact, human-legible reasons behind specific decisions, and presents a serious challenge to the provision of satisfactory, fair and transparent explanations. Therefore, the conclusion is that the quality of explanations might not be considered as an adequate safeguard for automated decision-making processes under the GDPR. Accordingly, additional tools should be considered to complement explanations. These could include algorithmic impact assessments, other forms of algorithmic justifications based on broader AI principles, and new technical developments in trustworthy AI. This suggests that eventually all of these approaches would need to be considered as a whole.},
	number = {1},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Hamon, Ronan and Junklewitz, Henrik and Sanchez, Ignacio and Malgieri, Gianclaudio and De Hert, Paul},
	year = {2022},
	note = {Conference Name: IEEE Computational Intelligence Magazine},
	keywords = {COVID-19, Data models, Decision making, Deep learning, General Data Protection Regulation, Law, Machine learning algorithms, Security},
	pages = {72--85},
}

@inproceedings{dilmaghani_privacy_2019,
	title = {Privacy and {Security} of {Big} {Data} in {AI} {Systems}: {A} {Research} and {Standards} {Perspective}},
	shorttitle = {Privacy and {Security} of {Big} {Data} in {AI} {Systems}},
	doi = {10.1109/BigData47090.2019.9006283},
	abstract = {The huge volume, variety, and velocity of big data have empowered Machine Learning (ML) techniques and Artificial Intelligence (AI) systems. However, the vast portion of data used to train AI systems is sensitive information. Hence, any vulnerability has a potentially disastrous impact on privacy aspects and security issues. Nevertheless, the increased demands for high-quality AI from governments and companies require the utilization of big data in the systems. Several studies have highlighted the threats of big data on different platforms and the countermeasures to reduce the risks caused by attacks. In this paper, we provide an overview of the existing threats which violate privacy aspects and security issues inflicted by big data as a primary driving force within the AI/ML workflow. We define an adversarial model to investigate the attacks. Additionally, we analyze and summarize the defense strategies and countermeasures of these attacks. Furthermore, due to the impact of AI systems in the market and the vast majority of business sectors, we also investigate Standards Developing Organizations (SDOs) that are actively involved in providing guidelines to protect the privacy and ensure the security of big data and AI systems. Our far-reaching goal is to bridge the research and standardization frame to increase the consistency and efficiency of AI systems developments guaranteeing customer satisfaction while transferring a high degree of trustworthiness.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Dilmaghani, Saharnaz and Brust, Matthias R. and Danoy, Grégoire and Cassagnes, Natalia and Pecero, Johnatan and Bouvry, Pascal},
	year = {2019},
	keywords = {Artificial intelligence, Big Data, Data models, Data privacy, IEC Standards, Security},
	pages = {5737--5743},
}

@inproceedings{spring_managing_2020,
	address = {Online USA},
	title = {On managing vulnerabilities in {AI}/{ML} systems},
	isbn = {978-1-4503-8995-2},
	url = {https://dl.acm.org/doi/10.1145/3442167.3442177},
	doi = {10.1145/3442167.3442177},
	abstract = {This paper explores how the current paradigm of vulnerability management might adapt to include machine learning systems through a thought experiment: what if flaws in machine learning (ML) were assigned Common Vulnerabilities and Exposures (CVE) identifiers (CVE-IDs)? We consider both ML algorithms and model objects. The hypothetical scenario is structured around exploring the changes to the six areas of vulnerability management: discovery, report intake, analysis, coordination, disclosure, and response. While algorithm flaws are well-known in academic research community, there is no apparent clear line of communication between this research community and the operational communities that deploy and manage systems that use ML. The thought experiments identify some ways in which CVE-IDs may establish some useful lines of communication between these two communities. In particular, it would start to introduce the research community to operational security concepts, which appears to be a gap left by existing efforts.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {New {Security} {Paradigms} {Workshop} 2020},
	publisher = {ACM},
	author = {Spring, Jonathan M. and Galyardt, April and Householder, Allen D. and VanHoudnos, Nathan},
	month = oct,
	year = {2020},
	pages = {111--126},
}

@inproceedings{rawat_secure_2021-1,
	address = {Online Only, United States},
	title = {Secure and trustworthy machine learning/artificial intelligence for multi-domain operations},
	isbn = {978-1-5106-4329-1 978-1-5106-4330-7},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11746/2592860/Secure-and-trustworthy-machine-learning-artificial-intelligence-for-multi-domain/10.1117/12.2592860.full},
	doi = {10.1117/12.2592860},
	abstract = {Machine Learning (ML) algorithms and Artiﬁcial Intelligence (AI) systems have already had an immense impact on our society as they have shown to be able to create machine cognition comparable to or even better than human cognition for some applications. ML algorithms are now regarded as very useful for data-driven applications including resilient multi-domain operations. However, ML algorithms and AI systems can be controlled, dodged, biased, and misled through ﬂawed learning models and input data, they need robust security features and trust. Furthermore, ML algorithms and AI systems add challenges when we have (unlabeled/labeled) sparse/small data or big data for training and evaluation. It is very important to design, evaluate and test ML algorithms and AI systems that produce reliable, robust, trustworthy, explainable, and fair/unbiased outcomes to make them acceptable and reliable in mission critical multi-domain operations. ML algorithms rely on data and work on the principle of “Garbage In, Garbage Out,” which means that if the input data to learning model is corrupted or compromised, the outcomes of the ML/AI would not be optimal, reliable and trustworthy. This paper focuses on achieving secure and trustworthy machine learning and artiﬁcial intelligence operations using context aware selection of learning models and blockchain for multi-domain battleﬁeld operations.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Multi}-{Domain} {Operations} {Applications} {III}},
	publisher = {SPIE},
	author = {Rawat, Danda},
	editor = {Pham, Tien and Solomon, Latasha and Hohil, Myron E.},
	month = apr,
	year = {2021},
	pages = {99},
}

@misc{noauthor_randomized_nodate,
	title = {A {Randomized} {Trial} of {Intraarterial} {Treatment} for {Acute} {Ischemic} {Stroke} {\textbar} {NEJM}},
	url = {https://www.nejm.org/doi/full/10.1056/NEJMoa1411587},
	urldate = {2022-05-16},
}

@book{ng_animal_2022,
	title = {Animal {Kingdom}: {A} {Large} and {Diverse} {Dataset} for {Animal} {Behavior} {Understanding}},
	shorttitle = {Animal {Kingdom}},
	abstract = {Understanding animals' behaviors is significant for a wide range of applications. However, existing animal behavior datasets have limitations in multiple aspects, including limited numbers of animal classes, data samples and provided tasks, and also limited variations in environmental conditions and viewpoints. To address these limitations , we create a large and diverse dataset, Animal Kingdom, that provides multiple annotated tasks to enable a more thorough understanding of natural animal behaviors. The wild animal footages used in our dataset record different times of the day in extensive range of environments containing variations in backgrounds, viewpoints, illumination and weather conditions. More specifically, our dataset contains 50 hours of annotated videos to localize relevant animal behavior segments in long videos for the video grounding task, 30K video sequences for the fine-grained multi-label action recognition task, and 33K frames for the pose estimation task, which correspond to a diverse range of animals with 850 species across 6 major animal classes. Such a challenging and comprehensive dataset shall be able to facilitate the community to develop, adapt, and evaluate various types of advanced methods for animal behavior analysis. Moreover, we propose a Collabo-rative Action Recognition (CARe) model that learns general and specific features for action recognition with unseen new animals. This method achieves promising performance in our experiments. Our dataset can be found at https://sutdcv.github.io/Animal-Kingdom.},
	author = {Ng, Xun and Ong, Kian Eng and Zheng, Qichen and Ni, Yun and Yeo, Si and Liu, Jun},
	month = apr,
	year = {2022},
}

@inproceedings{chatzidimitriou_npm-miner_2018,
	title = {npm-{Miner}: {An} {Infrastructure} for {Measuring} the {Quality} of the npm {Registry}},
	abstract = {As the popularity of the JavaScript language is constantly increasing, one of the most important challenges today is to assess the quality of JavaScript packages. Developers often employ tools for code linting and for the extraction of static analysis metrics in order to assess and/or improve their code. In this context, we have developed npn-miner, a platform that crawls the npm registry and analyzes the packages using static analysis tools in order to extract detailed quality metrics as well as high-level quality attributes, such as maintainability and security. Our infrastructure includes an index that is accessible through a web interface, while we have also constructed a dataset with the results of a detailed analysis for 2000 popular npm packages.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Chatzidimitriou, Kyriakos and Papamichail, Michail and Diamantopoulos, Themistoklis and Tsapanos, Michail and Symeonidis, Andreas},
	year = {2018},
	keywords = {Computer architecture, Measurement, Security, Software, Static analysis, Tools, Uniform resource locators, javascript, npm, software quality, static analysis},
}

@article{tay_are_2022,
	title = {Are {Pre}-trained {Convolutions} {Better} than {Pre}-trained {Transformers}?},
	url = {http://arxiv.org/abs/2105.03322},
	abstract = {In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.},
	urldate = {2022-05-05},
	journal = {arXiv:2105.03322 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Gupta, Jai and Bahri, Dara and Aribandi, Vamsi and Qin, Zhen and Metzler, Donald},
	month = jan,
	year = {2022},
	note = {arXiv: 2105.03322},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{marcelino_transfer_2022,
	title = {Transfer learning from pre-trained models},
	url = {https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751},
	abstract = {How to solve any image classification problem quickly and easily},
	language = {en},
	urldate = {2022-05-05},
	journal = {Medium},
	author = {Marcelino, Pedro},
	month = apr,
	year = {2022},
}

@inproceedings{chen_pre-trained_2021,
	address = {Nashville, TN, USA},
	title = {Pre-{Trained} {Image} {Processing} {Transformer}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577359/},
	doi = {10.1109/CVPR46437.2021.01212},
	language = {en},
	urldate = {2022-05-05},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
	month = jun,
	year = {2021},
	pages = {12294--12305},
}

@inproceedings{zerouali_diversity_2019,
	title = {On the {Diversity} of {Software} {Package} {Popularity} {Metrics}: {An} {Empirical} {Study} of npm},
	shorttitle = {On the {Diversity} of {Software} {Package} {Popularity} {Metrics}},
	doi = {10.1109/SANER.2019.8667997},
	abstract = {Software systems often leverage on open source software libraries to reuse functionalities. Such libraries are readily available through software package managers like npm for JavaScript. Due to the huge amount of packages available in such package distributions, developers often decide to rely on or contribute to a software package based on its popularity. Moreover, it is a common practice for researchers to depend on popularity metrics for data sampling and choosing the right candidates for their studies. However, the meaning of popularity is relative and can be defined and measured in a diversity of ways, that might produce different outcomes even when considered for the same studies. In this paper, we show evidence of how different is the meaning of popularity in software engineering research. Moreover, we empirically analyse the relationship between different software popularity measures. As a case study, for a large dataset of 175k npm packages, we computed and extracted 9 different popularity metrics from three open source tracking systems: libraries.io, npmjs.com and GitHub. We found that indeed popularity can be measured with different unrelated metrics, each metric can be defined within a specific context. This indicates a need for a generic framework that would use a portfolio of popularity metrics drawing from different concepts.},
	booktitle = {2019 {IEEE} 26th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Zerouali, Ahmed and Mens, Tom and Robles, Gregorio and Gonzalez-Barahona, Jesus M.},
	year = {2019},
	note = {ISSN: 1534-5351},
	keywords = {Correlation, Data mining, Libraries, Measurement, Runtime, Software packages, empirical analysis, npm, popularity, software package},
	pages = {589--593},
}

@article{rooney_root_nodate,
	title = {Root {Cause} {Analysis} {For} {Beginners}},
	language = {en},
	author = {Rooney, James J and Heuvel, Lee N Vanden},
	pages = {9},
}

@article{abdalkareem_beyond_nodate,
	title = {Beyond {Traditional} {Software} {Development}: {Studying} and {Supporting} the {Role} of {Reusing} {Crowdsourced} {Knowledge} in {Software} {Development}},
	language = {en},
	author = {Abdalkareem, Rabe Muftah B},
	pages = {164},
}

@misc{noauthor_code_nodate,
	title = {Code {Reuse} in {Open} {Source} {Software}},
	url = {https://pubsonline.informs.org/doi/epdf/10.1287/mnsc.1070.0748},
	language = {en},
	urldate = {2022-04-26},
	doi = {10.1287/mnsc.1070.0748},
}

@article{buda_systematic_2018,
	title = {A systematic study of the class imbalance problem in convolutional neural networks},
	volume = {106},
	issn = {08936080},
	url = {http://arxiv.org/abs/1710.05381},
	doi = {10.1016/j.neunet.2018.07.011},
	abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
	urldate = {2022-04-21},
	journal = {Neural Networks},
	author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A.},
	month = oct,
	year = {2018},
	note = {arXiv: 1710.05381},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {249--259},
}

@article{von_der_mosel_validity_2021,
	title = {On the validity of pre-trained transformers for natural language processing in the software engineering domain},
	url = {http://arxiv.org/abs/2109.04738},
	abstract = {Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.},
	urldate = {2022-04-17},
	journal = {arXiv:2109.04738 [cs]},
	author = {von der Mosel, Julian and Trautsch, Alexander and Herbold, Steffen},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.04738},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{chakraborty_does_2021,
	address = {Athens Greece},
	title = {Does reusing pre-trained {NLP} model propagate bugs?},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3473494},
	doi = {10.1145/3468264.3473494},
	abstract = {In this digital era, the textual content has become a seemingly ubiquitous part of our life. Natural Language Processing (NLP) empowers machines to comprehend the intricacies of textual data and eases human-computer interaction. Advancement in language modeling, continual learning, availability of a large amount of linguistic data, and large-scale computational power have made it feasible to train models for downstream tasks related to text analysis, including safety-critical ones, e.g., medical, airlines, etc. Compared to other deep learning (DL) models, NLP-based models are widely reused for various tasks. However, the reuse of pre-trained models in a new setting is still a complex task due to the limitations of the training dataset, model structure, specification, usage, etc. With this motivation, we study BERT, a vastly used language model (LM), from the direction of reusing in the code. We mined 80 posts from Stack Overflow related to BERT and found 4 types of bugs observed in clients’ code. Our results show that 13.75\% are fairness, 28.75\% are parameter, 15\% are token, and 16.25\% are version-related bugs.},
	language = {en},
	urldate = {2022-04-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Chakraborty, Mohna},
	month = aug,
	year = {2021},
	pages = {1686--1688},
}

@article{spinellis_package_2012,
	title = {Package {Management} {Systems}},
	volume = {29},
	issn = {1937-4194},
	doi = {10.1109/MS.2012.38},
	abstract = {A package management system organizes and simplifies the installation and maintenance of software by standardizing and organizing the production and consumption of software collections. As a software developer, you can benefit from package managers in two ways: through a rich and stable development environment and through friction-free reuse. Promisingly, the structure that package managers bring both to the tools we use in our development process and the libraries we reuse in our products ties nicely with the recent move emphasizing DevOps (development operations) as an integration between software development and IT operations.},
	number = {2},
	journal = {IEEE Software},
	author = {Spinellis, Diomidis},
	month = mar,
	year = {2012},
	note = {Conference Name: IEEE Software},
	keywords = {DevOps, Maintenance engineering, Product management, Software libraries, Software reusability, module dependencies, package management system, shared library, software reuse},
	pages = {84--86},
}

@inproceedings{dietrich_dependency_2019,
	title = {Dependency {Versioning} in the {Wild}},
	doi = {10.1109/MSR.2019.00061},
	abstract = {Many modern software systems are built on top of existing packages (modules, components, libraries). The increasing number and complexity of dependencies has given rise to automated dependency management where package managers resolve symbolic dependencies against a central repository. When declaring dependencies, developers face various choices, such as whether or not to declare a fixed version or a range of versions. The former results in runtime behaviour that is easier to predict, whilst the latter enables flexibility in resolution that can, for example, prevent different versions of the same package being included and facilitates the automated deployment of bug fixes. We study the choices developers make across 17 different package managers, investigating over 70 million dependencies. This is complemented by a survey of 170 developers. We find that many package managers support - and the respective community adapts - flexible versioning practices. This does not always work: developers struggle to find the sweet spot between the predictability of fixed version dependencies, and the agility of flexible ones, and depending on their experience, adjust practices. We see some uptake of semantic versioning in some package managers, supported by tools. However, there is no evidence that projects switch to semantic versioning on a large scale. The results of this study can guide further research into better practices for automated dependency management, and aid the adaptation of semantic versioning.},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Dietrich, Jens and Pearce, David and Stringer, Jacob and Tahir, Amjed and Blincoe, Kelly},
	year = {2019},
	note = {ISSN: 2574-3864},
	keywords = {Computer bugs, Contracts, Runtime, Semantics, Software, Syntactics, Tools, dependency management, maven, npm, package managers, repository studies, semantic versioning},
	pages = {349--359},
}

@inproceedings{ignatiev_towards_2014,
	address = {Hyderabad India},
	title = {Towards efficient optimization in package management systems},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568306},
	doi = {10.1145/2568225.2568306},
	abstract = {Package management as a means of reuse of software artifacts has become extremely popular, most notably in Linux distributions. At the same time, successful package management brings about a number of computational challenges. Whenever a user requires a new package to be installed, a package manager not only installs the new package but it might also install other packages or uninstall some old ones in order to respect dependencies and conﬂicts of the packages. Coming up with a new conﬁguration of packages is computationally challenging. It is in particular complex when we also wish to optimize for user preferences, such as that the resulting package conﬁguration should not diﬀer too much from the original one. A number of exact approaches for solving this problem have been proposed in recent years. These approaches, however, do not have guaranteed runtime due to the high computational complexity of the problem. This paper addresses this issue by devising a hybrid approach that integrates exact solving with approximate solving by invoking the approximate part whenever the solver is running out of time. Experimental evaluation shows that this approach enables returning high-quality package conﬁgurations with rapid response time.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Ignatiev, Alexey and Janota, Mikoláš and Marques-Silva, Joao},
	month = may,
	year = {2014},
	pages = {745--755},
}

@article{pham_problems_2020,
	title = {Problems and {Opportunities} in {Training} {Deep} {Learning} {Software} {Systems}: {An} {Analysis} of {Variance}},
	abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models’ accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.},
	language = {en},
	author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
	year = {2020},
	pages = {13},
}

@article{qian_are_nodate,
	title = {Are {My} {Deep} {Learning} {Systems} {Fair}? {An} {Empirical} {Study} of {Fixed}-{Seed} {Training}},
	abstract = {Deep learning (DL) systems have been gaining popularity in critical tasks such as credit evaluation and crime prediction. Such systems demand fairness. Recent work shows that DL software implementations introduce variance: identical DL training runs (i.e., identical network, data, conﬁguration, software, and hardware) with a ﬁxed seed produce diﬀerent models. Such variance could make DL models and networks violate fairness compliance laws, resulting in negative social impact. In this paper, we conduct the ﬁrst empirical study to quantify the impact of software implementation on the fairness and its variance of DL systems. Our study of 22 mitigation techniques and ﬁve baselines reveals up to 12.6\% fairness variance across identical training runs with identical seeds. In addition, most debiasing algorithms have a negative impact on the model such as reducing model accuracy, increasing fairness variance, or increasing accuracy variance. Our literature survey shows that while fairness is gaining popularity in artiﬁcial intelligence (AI) related conferences, only 34.4\% of the papers use multiple identical training runs to evaluate their approach, raising concerns about their results’ validity. We call for better fairness evaluation and testing protocols to improve fairness and fairness variance of DL systems as well as DL research validity and reproducibility at large.},
	language = {en},
	author = {Qian, Shangshu and Pham, Hung Viet and Lutellier, Thibaud and Hu, Zeou and Kim, Jungwon and Tan, Lin and Yu, Yaoliang and Chen, Jiahao and Shah, Sameena},
	pages = {24},
}

@article{pham_problems_2020-1,
	title = {Problems and {Opportunities} in {Training} {Deep} {Learning} {Software} {Systems}: {An} {Analysis} of {Variance}},
	abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models’ accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.},
	language = {en},
	author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
	year = {2020},
	pages = {13},
}

@article{golendukhina_what_2022,
	title = {What is {Software} {Quality} for {AI} {Engineers}? {Towards} a {Thinning} of the {Fog}},
	shorttitle = {What is {Software} {Quality} for {AI} {Engineers}?},
	url = {http://arxiv.org/abs/2203.12697},
	abstract = {It is often overseen that AI-enabled systems are also software systems and therefore rely on software quality assurance (SQA). Thus, the goal of this study is to investigate the software quality assurance strategies adopted during the development, integration, and maintenance of AI/ML components and code. We conducted semi-structured interviews with representatives of ten Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the interview data identified 12 issues in the development of AI/ML components. Furthermore, we identified when quality issues arise in AI/ML components and how they are detected. The results of this study should guide future work on software quality assurance processes and techniques for AI/ML components.},
	urldate = {2022-03-29},
	journal = {arXiv:2203.12697 [cs]},
	author = {Golendukhina, Valentina and Lenarduzzi, Valentina and Felderer, Michael},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.12697},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{bhatia_towards_2022,
	title = {Towards a {Change} {Taxonomy} for {Machine} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2203.11365},
	abstract = {Machine Learning (ML) research publications commonly provide open-source implementations on GitHub, allowing their audience to replicate, validate, or even extend machine learning algorithms, data sets, and metadata. However, thus far little is known about the degree of collaboration activity happening on such ML research repositories, in particular regarding (1) the degree to which such repositories receive contributions from forks, (2) the nature of such contributions (i.e., the types of changes), and (3) the nature of changes that are not contributed back to forks, which might represent missed opportunities. In this paper, we empirically study contributions to 1,346 ML research repositories and their 67,369 forks, both quantitatively and qualitatively (by building on Hindle et al.'s seminal taxonomy of code changes). We found that while ML research repositories are heavily forked, only 9\% of the forks made modifications to the forked repository. 42\% of the latter sent changes to the parent repositories, half of which (52\%) were accepted by the parent repositories. Our qualitative analysis on 539 contributed and 378 local (fork-only) changes, extends Hindle et al.'s taxonomy with one new top-level change category related to ML (Data), and 15 new sub-categories, including nine ML-specific ones (input data, output data, program data, sharing, change evaluation, parameter tuning, performance, pre-processing, model training). While the changes that are not contributed back by the forks mostly concern domain-specific customizations and local experimentation (e.g., parameter tuning), the origin ML repositories do miss out on a non-negligible 15.4\% of Documentation changes, 13.6\% of Feature changes and 11.4\% of Bug fix changes. The findings in this paper will be useful for practitioners, researchers, toolsmiths, and educators.},
	urldate = {2022-03-28},
	journal = {arXiv:2203.11365 [cs]},
	author = {Bhatia, Aaditya and Eghan, Ellis E. and Grichi, Manel and Cavanagh, William G. and Ming, Zhen and Jiang and Adams, Bram},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.11365},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{jabbarvand_search-based_2019,
	address = {Montreal, QC, Canada},
	title = {Search-{Based} {Energy} {Testing} of {Android}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8812097/},
	doi = {10.1109/ICSE.2019.00115},
	abstract = {The utility of a smartphone is limited by its battery capacity and the ability of its hardware and software to efﬁciently use the device’s battery. To properly characterize the energy consumption of an app and identify energy defects, it is critical that apps are properly tested, i.e., analyzed dynamically to assess the app’s energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. We present COBWEB, a search-based energy testing technique for Android. By leveraging a set of novel models, representing both the functional behavior of an app as well as the contextual conditions affecting the app’s energy behavior, COBWEB generates a test suite that can effectively ﬁnd energy defects. Our experimental results using real-world apps demonstrate not only its ability to effectively and efﬁciently test energy behavior of apps, but also its superiority over prior techniques by ﬁnding a wider and more diverse set of energy defects.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Jabbarvand, Reyhaneh and Lin, Jun-Wei and Malek, Sam},
	month = may,
	year = {2019},
	pages = {1119--1130},
}

@article{xu_empirical_2022,
	title = {An {Empirical} {Study} on the {Impact} of {Deep} {Parameters} on {Mobile} {App} {Energy} {Usage}},
	url = {http://arxiv.org/abs/2009.12156},
	abstract = {Improving software performance through conﬁguration parameter tuning is a common activity during software maintenance. Beyond traditional performance metrics like latency, mobile app developers are interested in reducing app energy usage. Some mobile apps have centralized locations for parameter tuning, similar to databases and operating systems, but it is common for mobile apps to have hundreds of parameters scattered around the source code. The correlation between these “deep” parameters and app energy usage is unclear. Researchers have studied the energy effects of deep parameters in speciﬁc modules, but we lack a systematic understanding of the energy impact of mobile deep parameters.},
	language = {en},
	urldate = {2022-03-23},
	journal = {arXiv:2009.12156 [cs]},
	author = {Xu, Qiang and Davis, James C. and Hu, Y. Charlie and Jindal, Abhilash},
	month = jan,
	year = {2022},
	note = {arXiv: 2009.12156},
	keywords = {Computer Science - Software Engineering},
}

@article{ousterhout_always_2018,
	title = {Always measure one level deeper},
	volume = {61},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3213770},
	doi = {10.1145/3213770},
	abstract = {Performance measurements often go wrong, reporting surface-level results that are more marketing than science.},
	language = {en},
	number = {7},
	urldate = {2022-03-23},
	journal = {Communications of the ACM},
	author = {Ousterhout, John},
	month = jun,
	year = {2018},
	pages = {74--83},
}

@article{stamoulis_single-path_2019,
	title = {Single-{Path} {NAS}: {Designing} {Hardware}-{Efficient} {ConvNets} in less than 4 {Hours}},
	shorttitle = {Single-{Path} {NAS}},
	url = {http://arxiv.org/abs/1904.02877},
	abstract = {Can we automatically design a Convolutional Network (ConvNet) with the highest image classification accuracy under the runtime constraint of a mobile device? Neural architecture search (NAS) has revolutionized the design of hardware-efficient ConvNets by automating this process. However, the NAS problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours. Our contributions are as follows: 1. Single-path search space: Compared to previous differentiable NAS methods, Single-Path NAS uses one single-path over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2. Hardware-efficient ImageNet classification: Single-Path NAS achieves 74.96\% top-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar constraints ({\textless}80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30 TPU-hours), which is up to 5,000x faster compared to prior work. 4. Reproducibility: Unlike all recent mobile-efficient NAS methods which only release pretrained models, we open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.},
	urldate = {2022-03-23},
	journal = {arXiv:1904.02877 [cs, stat]},
	author = {Stamoulis, Dimitrios and Ding, Ruizhou and Wang, Di and Lymberopoulos, Dimitrios and Priyantha, Bodhi and Liu, Jie and Marculescu, Diana},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.02877},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{goel_survey_2020,
	title = {A {Survey} of {Methods} for {Low}-{Power} {Deep} {Learning} and {Computer} {Vision}},
	doi = {10.1109/WF-IoT48130.2020.9221198},
	abstract = {Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research.},
	booktitle = {2020 {IEEE} 6th {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	author = {Goel, Abhinav and Tung, Caleb and Lu, Yung-Hsiang and Thiruvathukal, George K.},
	month = jun,
	year = {2020},
	keywords = {Computational modeling, Computer vision, Deep learning, Knowledge engineering, Measurement, Memory management, Quantization (signal), computer vision, low-power, neural networks},
	pages = {1--6},
}

@inproceedings{phaithoon_fixme_2021,
	title = {{FixMe}: {A} {GitHub} {Bot} for {Detecting} and {Monitoring} {On}-{Hold} {Self}-{Admitted} {Technical} {Debt}},
	shorttitle = {{FixMe}},
	doi = {10.1109/ASE51524.2021.9678680},
	abstract = {Self-Admitted Technical Debt (SATD) is a special form of technical debt in which developers intentionally record their hacks in the code by adding comments for attention. Here, we focus on issue-related "On-hold SATD", where developers suspend proper implementation due to issues reported inside or outside the project. When the referenced issues are resolved, the On-hold SATD also need to be addressed, but since monitoring these issue reports takes a lot of time and effort, developers may not be aware of the resolved issues and leave the On-hold SATD in the code. In this paper, we propose FixMe, a GitHub bot that helps developers detecting and monitoring On-hold SATD in their repositories and notify them whenever the On-hold SATDs are ready to be fixed (i.e. the referenced issues are resolved). The bot can automatically detect On-hold SATD comments from source code using machine learning techniques and discover referenced issues. When the referenced issues are resolved, developers will be notified by FixMe bot. The evaluation conducted with 11 participants shows that our FixMe bot can support them in dealing with On-hold SATD. FixMe is available at https://www.fixmebot.app/ and FixMe's VDO is at https://youtu.be/YSz9kFxN\_YQ.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Phaithoon, Saranphon and Wongnil, Supakarn and Pussawong, Patiphol and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee and Maipradit, Rungroj and Hata, Hideaki and Matsumoto, Kenichi},
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {Codes, Computer science, Filtering, Machine learning, Monitoring, Software, Usability},
	pages = {1257--1261},
}

@inproceedings{shen_somanyconflicts_2021,
	title = {{SoManyConflicts}: {Resolve} {Many} {Merge} {Conflicts} {Interactively} and {Systematically}},
	shorttitle = {{SoManyConflicts}},
	doi = {10.1109/ASE51524.2021.9678937},
	abstract = {Code merging plays an important role in collaborative software development. However, it is often tedious and error-prone for developers to manually resolve merge conflicts, especially when there are many conflicts after merging long-lived branches or parallel versions. In this paper, we present SoManyConflicts, a language-agnostic approach to help developers resolve merge conflicts systematically, by utilizing their interrelations (e.g., dependency, similarity, etc.). SoManyConflicts employs a graph representation to model these interrelations and provides 3 major features: 1) cluster and order related conflict based on the graph connectivity; 2) suggest related conflicts of one focused conflict based on the topological sorting, 3) suggest resolution strategies for unresolved conflicts based already resolved ones. We have implemented SoManyConflicts as a Visual Studio Code extension that supports multiple languages (Java, JavaScript, and TypeScript, etc.), which is briefly introduced in the video: https://youtu.be/asWhj1KTU. The source code is publicly available at: https://github.com/Symbolk/somanyconflicts.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Shen, Bo and Zhang, Wei and Yu, Ailun and Shi, Yifan and Zhao, Haiyan and Jin, Zhi},
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {Codes, Collaborative software, Merging, Productivity, Software algorithms, Systematics, Version control, Visualization, code merging, conflict resolution, graph partitioning},
	pages = {1291--1295},
}

@inproceedings{ying_nas-bench-101_2019,
	title = {{NAS}-{Bench}-101: {Towards} {Reproducible} {Neural} {Architecture} {Search}},
	shorttitle = {{NAS}-{Bench}-101},
	url = {https://proceedings.mlr.press/v97/ying19a.html},
	abstract = {Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ying, Chris and Klein, Aaron and Christiansen, Eric and Real, Esteban and Murphy, Kevin and Hutter, Frank},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7105--7114},
}

@article{elsken_neural_nodate,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	language = {en},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	pages = {21},
}

@article{Wu2016,
	title = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
	url = {https://arxiv.org/abs/1609.08144},
	abstract = {others},
	journal = {arXiv},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus},
	year = {2016},
}

@article{Willemink2020MedicalImagingData4ML,
	title = {Preparing {Medical} {Imaging} {Data} for {Machine} {Learning}},
	abstract = {Supervised artificial intelligence (AI) methods for evaluation of medical images require a curation process for data to optimally train, validate, and test algorithms. The chief obstacles to development and clinical implementation of AI algorithms include availability of sufficiently large, curated, and representative training data that includes expert labeling (eg, annotations).},
	journal = {Radiological Society of North America},
	author = {Willemink, Martin J. and Koszek, Wojciech A. and Hardell, Cailin and Wu, Jie and Fleischmann, Dominik and Harvey, Hugh and Folio, Les R. and Summers, Ronald M. and Rubin, Daniel L. and Lungren, Matthew P.},
	year = {2020},
}

@inproceedings{Tucker2010CaseStudyinSWREengineering,
	title = {A {Case} {Study} in {Software} {Reengineering}},
	abstract = {Software reengineering results from several needs including fixing defects (corrective reengineering), modifying the software to address weaknesses or to mitigate potential malfunctions (preventive reengineering), and extending the software to accommodate changes in its external environment (adaptive reengineering). This paper describes a case study in perfective and adaptive reengineering. The rationale for the reengineering decisions, the results of the project, lessons learned and the current state of the system are described.},
	booktitle = {International {Conference} on {Informatio} ({ITNG})n {Technology}: {New} {Generations}},
	author = {Tucker, D. Casey and Devon, M. Simmonds},
	year = {2010},
	keywords = {Advertising, Audio recording, Business, Companies, Databases, Documentation, Hardware, Software design, Software maintenance, Software performance, UML, design, software reengineering},
}

@article{Tan2014OSSBugCharacteristics,
	title = {Bug characteristics in open source software},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Tan, Lin and Liu, Chen and Li, Zhenmin and Wang, Xuanhui and Zhou, Yuanyuan and Zhai, Chengxiang},
	year = {2014},
}

@inproceedings{Sun2017RealBugsforML,
	title = {An {Empirical} {Study} on {Real} {Bugs} for {Machine} {Learning} {Programs}},
	abstract = {Due to the availability of various open source Machine Learning (ML) tools and libraries, developers nowadays can easily implement their purposes by just invoking machine learning APIs without knowing the details of the algorithm. However, the owners of ML tools and libraries usually pay more attention to the correctness and functionality of their algorithm, while spending much less effort on maintaining their code and keeping their code at a high quality level. Considering the popularity of machine learning in today's world, low quality ML tools and libraries can have a huge impact on the software products that use ML algorithms. So in this paper, we conduct an empirical study on real machine learning bugs to examine their patterns and how they evolve over time. We collect three popular machine learning projects on Github, and manually analyzed 329 closed bugs from the perspectives of their bug category, fix pattern, fix scale, fix duration, and type of software maintenance. The results show that (1) there are seven categories of bugs in machine learning programs; (2) twelve different fix patterns are commonly used to fix the bugs; (3) 63.83\% of the patches belong to micro-scale-fix and small-scale-fix, and 68.39\% of the bugs are fixed within one month; (4) 47.77\% of the bug fixes belong to corrective activity from the view of software maintenance.},
	booktitle = {Asia-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Sun, Xiaobing and Zhou, Tianchi and Li, Gengjie and Hu, Jiajun and Yang, Hui and Li, Bin},
	year = {2017},
	keywords = {bug, bug fix, empirical study, machine learning programs},
}

@inproceedings{Serban2020SEBPinMLAdoptionEffects,
	title = {Adoption and effects of software engineering best practices in machine learning},
	abstract = {Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner. Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components. Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models. Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied. Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.},
	booktitle = {Empirical {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Serban, Alex and Blom, Koen Van Der and Hoos, Holger and Visser, Joost},
	year = {2020},
	keywords = {Best practices, Machine learning engineering, Survey},
}

@inproceedings{Seaman2008DefectCategorization,
	title = {Defect categorization: making use of a decade of widely varying historical data},
	abstract = {This paper describes our experience in aggregating a number of historical datasets containing inspection defect data using different categorization schemes. Our goal was to make use of the historical data by creating models to guide future development projects. We describe our approach to reconciling the different choices used in the historical datasets to categorize defects, and the challenges we faced. We also present a set of recommendations for others involved in classifying defects.},
	urldate = {2021-10-25},
	booktitle = {Empirical {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Seaman, Carolyn B. and Shull, Forrest and Regardie, Myrna and Elbert, Denis and Feldmann, Raimund L and Guo, Yuepu and Godfrey, Sally},
	year = {2008},
}

@article{Ren2017FasterRCNN,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2017},
	keywords = {Object detection, convolutional neural network, region proposal},
}

@article{Pineau2020,
	title = {Improving {Reproducibility} in {Machine} {Learning} {Research}},
	abstract = {One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.},
	journal = {Journal of Machine Learning Research},
	author = {Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha, Koustuv and Lariviere, Vincent and Beygelzimer, Alina},
	year = {2020},
}

@article{Mckeeman1998DifferentialTesting,
	title = {Differential {Testing} for {Software}.},
	abstract = {Differential testing, a form of random testing, is a component of a mature testing technology for large software systems. It complements regression testing based on commercial test suites and locally developed tests During prod- uct development and deployment. Differential que testing requires two or more comparable systems be available to the tester. These sys- tems are presented with an exhaustive series of mechanically generated test cases. If (w might say when) the results differ or one of the systems loops indefinitely or crashes, the tester has a candidate for a bug-exposing test. Implementing differential testing is an interest- ing technical problem. Getting it into use is an even more interesting social challenge. This paper is derived from experience in differential testing of compilers and run-time systems at DIGITAL over the last few years and recently at Compaq. A working prototype for testing C compilers is available on the web.},
	journal = {Digital Technical Journal},
	author = {Mckeeman, William M. and Problem, The Testing},
	year = {1998},
	keywords = {Computer architecture, Computer science, Dynamic testing, Integration testing, Manual testing, Non-regression testing, Regression testing, Software performance testing, Software reliability testing, Software system},
}

@article{Lorenzoni2021MLModelDevelopmentfromSEPerspective,
	title = {Machine {Learning} {Model} {Development} from a {Software} {Engineering} {Perspective}: {A} {Systematic} {Literature} {Review}},
	url = {https://arxiv.org/abs/2102.07574},
	abstract = {Data scientists often develop machine learning models to solve a variety of problems in the industry and academy but not without facing several challenges in terms of Model Development. The problems regarding Machine Learning Development involves the fact that such professionals do not realize that they usually perform ad-hoc practices that could be improved by the adoption of activities presented in the Software Engineering Development Lifecycle. Of course, since machine learning systems are different from traditional Software systems, some differences in their respective development processes are to be expected. In this context, this paper is an effort to investigate the challenges and practices that emerge during the development of ML models from the software engineering perspective by focusing on understanding how software developers could benefit from applying or adapting the traditional software engineering process to the Machine Learning workflow.},
	journal = {arXiv},
	author = {Lorenzoni, Giuliano and Alencar, Paulo and Nascimento, Nathalia and Cowan, Donald},
	year = {2021},
}

@inproceedings{COCO,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and others},
	year = {2014},
}

@article{Gundersen2018ReproducibilityinAI,
	title = {State of the art: {Reproducibility} in artificial intelligence},
	abstract = {Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.},
	journal = {AAAI Conference on Artificial Intelligence (AAAI)},
	author = {Gundersen, Odd Erik and Kjensmo, Sigbjørn},
	year = {2018},
}

@inproceedings{Goel2020LPDL,
	title = {A {Survey} of {Methods} for {Low}-{Power} {Deep} {Learning} and {Computer} {Vision}},
	abstract = {Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research.},
	booktitle = {{IEEE} {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	author = {Goel, Abhinav and Tung, Caleb and Lu, Yung Hsiang and Thiruvathukal, George K.},
	year = {2020},
	keywords = {computer vision, low-power, neural networks},
}

@article{Devanbu2020SE4DLVision,
	title = {Deep {Learning} \& {Software} {Engineering}: {State} of {Research} and {Future} {Directions}},
	shorttitle = {Deep {Learning} \& {Software} {Engineering}},
	url = {https://arxiv.org/abs/2009.08525},
	abstract = {Given the current transformative potential of research that sits at the intersection of Deep Learning (DL) and Software Engineering (SE), an NSF-sponsored community workshop was conducted in co-location with the 34th IEEE/ACM International Conference on Automated Software Engineering (ASE'19) in San Diego, California. The goal of this workshop was to outline high priority areas for cross-cutting research. While a multitude of exciting directions for future work were identified, this report provides a general summary of the research areas representing the areas of highest priority which were discussed at the workshop. The intent of this report is to serve as a potential roadmap to guide future work that sits at the intersection of SE \& DL.},
	journal = {arXiv},
	author = {Devanbu, Prem and Dwyer, Matthew and Elbaum, Sebastian and Lowry, Michael and Moran, Kevin and Poshyvanyk, Denys and Ray, Baishakhi and Singh, Rishabh and Zhang, Xiangyu},
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{Behera2017SurveyonMLConceptAlgorithmsApplications,
	title = {A {Survey} on {Machine} {Learning}: {Concept}, {Algorithms} and {Applications}},
	journal = {International Journal of Innovative Research in Computer and Communication Engineering (IJIRCCE)},
	author = {Behera, Rabi and Das, Kajaree},
	year = {2017},
}

@inproceedings{Byrne1992ConceptualFoundation4SWReengineering,
	title = {A conceptual foundation for software re-engineering},
	abstract = {The author presents a conceptual foundation for software re-engineering. The foundation is composed of properties and principles that underlie re-engineering methods, and assumptions about re-engineering. The value of this conceptual foundation is its ability to model understanding of re-engineering, how it is practiced, and how it can be practiced. A general model of software re-engineering is established, based on this foundation. This model, along with its underlying foundation, proves useful for examining issues such as the re-engineering process and strategies.{\textless}{\textgreater}},
	booktitle = {Conference on {Software} {Maintenance}},
	author = {Byrne, E.J.},
	year = {1992},
	keywords = {Control systems, Databases, Documentation, Programming, Software maintenance, Software quality, Software reusability, Software systems, Software testing, Usability},
}

@article{Dota2019,
	title = {Dota 2 with {Large} {Scale} {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1912.06680},
	abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
	journal = {arXiv preprint arXiv:1912.06680},
	author = {{OpenAI} and {:} and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
	year = {2019},
}

@article{Zhang2020NumericalBugsinNN,
	title = {Detecting numerical bugs in neural network architectures},
	abstract = {Detecting bugs in deep learning software at the architecture level provides additional benefits that detecting bugs at the model level does not provide. This paper makes the first attempt to conduct static analysis for detecting numerical bugs at the architecture level. We propose a static analysis approach for detecting numerical bugs in neural architectures based on abstract interpretation. Our approach mainly comprises two kinds of abstraction techniques, i.e., one for tensors and one for numerical values. Moreover, to scale up while maintaining adequate detection precision, we propose two abstraction techniques: tensor partitioning and (elementwise) affine relation analysis to abstract tensors and numerical values, respectively. We realize the combination scheme of tensor partitioning and affine relation analysis (together with interval analysis) as DEBAR, and evaluate it on two datasets: neural architectures with known bugs (collected from existing studies) and real-world neural architectures. The evaluation results show that DEBAR outperforms other tensor and numerical abstraction techniques on accuracy without losing scalability. DEBAR successfully detects all known numerical bugs with no false positives within 1.7-2.3 seconds per architecture. On the real-world architectures, DEBAR reports 529 warnings within 2.6-135.4 seconds per architecture, where 299 warnings are true positives.},
	journal = {European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)},
	author = {Zhang, Yuhao and Ren, Luyao and Chen, Liqian and Xiong, Yingfei and Cheung, Shing-Chi and Xie, Tao},
	year = {2020},
	keywords = {Neural Network, Numerical Bugs, Static Analysis},
}

@article{Zhang2018TFBugs,
	title = {An empirical study on {TensorFlow} program bugs},
	abstract = {Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research e orts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To ll this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These ndings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.},
	urldate = {2021-10-04},
	journal = {International Symposium on Software Testing and Analysis (ISSTA)},
	author = {Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu},
	year = {2018},
}

@article{Zhang2020DifferentialFuzzing4DLOps,
	title = {Duo: {Differential} {Fuzzing} for {Deep} {Learning} {Operators}},
	shorttitle = {Duo},
	abstract = {Deep learning (DL) libraries reduce the barriers to the DL model construction. In DL libraries, various building blocks are DL operators with different functionality, responsible for processing high-dimensional tensors during training and inference. Thus, the quality of operators could directly impact the quality of models. However, existing DL testing techniques mainly focus on robustness testing of trained neural network models and cannot locate DL operators’ defects. The insufficient test input and undetermined test output in operator testing have become challenging for DL library developers. In this article, we propose an approach, namely Duo, which combines fuzzing techniques and differential testing techniques to generate input and evaluate corresponding output. It implements mutation-based fuzzing to produce tensor inputs by employing nine mutation operators derived from genetic algorithms and differential testing to evaluate outputs’ correctness from multiple operator instances. Duo is implemented in a tool and used to evaluate seven operators from TensorFlow, PyTorch, MNN, and MXNet in an experiment. The result shows that Duo can expose defects of DL operators and realize multidimension evaluation for DL operators from different DL libraries.},
	journal = {IEEE Transactions on Reliability},
	author = {Zhang, Xufan and Liu, Jiawei and Sun, Ning and Fang, Chunrong and Liu, Jia and Wang, Jiang and Chai, Dong and Chen, Zhenyu},
	year = {2021},
	keywords = {Computer bugs, Deep learning, Deep learning operators, Fuzzing, Learning systems, Tensors, Testing, Training data, deep learning libraries, defect detection, differential testing, fuzzing},
}

@inproceedings{Zhang2019CommonChallengesinDevelopingDLApplications,
	title = {An {Empirical} {Study} of {Common} {Challenges} in {Developing} {Deep} {Learning} {Applications}},
	abstract = {Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated - what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q\&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining accepted answers of these questions, we summarize five main root causes that may deserve attention from the research community, including API misuse, incorrect hyperparameter selection, GPU computation, static graph computation, and limited debugging and profiling support. Our results highlight the need for new techniques such as cross-framework differential testing to improve software development productivity and software reliability in deep learning.},
	booktitle = {International {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Zhang, Tianyi and Gao, Cuiyun and Ma, Lei and Lyu, Michael and Kim, Miryung},
	year = {2019},
	keywords = {deep learning, Stack Overflow, programming issues, software reliability},
}

@inproceedings{Zhang2020ProgramFailuresofDLjobs,
	title = {An empirical study on program failures of deep learning jobs},
	abstract = {Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Zhang, Ru and Xiao, Wencong and Zhang, Hongyu and Liu, Yu and Lin, Haoxiang and Yang, Mao},
	year = {2020},
}

@article{Xu2021CVinConstructionACriticalReview,
	title = {Computer {Vision} {Techniques} in {Construction}: {A} {Critical} {Review}},
	abstract = {Computer vision has been gaining interest in a wide range of research areas in recent years, from medical to industrial robotics. The architecture, engineering and construction and facility management sector ranks as one of the most intensive fields where vision-based systems/methods are used to facilitate decision making processes during the construction phase. Construction sites make efficient monitoring extremely tedious and difficult due to clutter and disorder. Extensive research has been carried out to investigate the potential to utilise computer vision for assisting on-site managerial tasks. This paper reviews studies on computer vision in the past decade, with a focus on state-of-the-art methods in a typical vision-based scheme, and discusses challenges associated with their application. This research aims to guide practitioners to successfully find suitable approaches for a particular project.},
	journal = {Archives of Computational Methods in Engineering},
	author = {Xu, Shuyuan and Wang, Jun and Shou, Wenchi and Ngo, Tuan and Sadick, Abdul-Manan and Wang, Xiangyu},
	year = {2021},
}

@article{DL4CV,
	title = {Deep {Learning} for {Computer} {Vision}: {A} {Brief} {Review}},
	shorttitle = {Deep {Learning} for {Computer} {Vision}},
	abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
	journal = {Computational Intelligence and Neuroscience},
	author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
	year = {2018},
}

@article{StarCraft2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Michael and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
	year = {2019},
}

@inproceedings{Sculley2014MLTechDebt,
	title = {Machine {Learning} : {The} {High}-{Interest} {Credit} {Card} of {Technical} {Debt}},
	abstract = {Machine learning offers a fantastically powerful toolkit for building complex sys-tems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is re-markably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several ma-chine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
	booktitle = {{NIPS} {Workshop} on {Software} {Engineering} for {Machine} {Learning} ({SE4ML})},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
	year = {2014},
}

@inproceedings{Islam2020RepairingDNN:FixpatternsChallenges,
	title = {Repairing deep neural networks: fix patterns and challenges},
	shorttitle = {Repairing deep neural networks},
	abstract = {Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
	month = jun,
	year = {2020},
}

@inproceedings{Islam2019DLBugCharacteristics,
	title = {A comprehensive study on deep learning bug characteristics},
	abstract = {Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48\% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43\% of the times. We have also found that the bugs in the usage of deep learning libraries have some common antipatterns that lead to a strong correlation of bug types among the libraries.},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh},
	year = {2019},
	keywords = {Bugs, Deep learning bugs, Deep learning software, Empirical study of bugs, Q\&A forums},
}

@inproceedings{Humbatova2020TaxonomyofRealFaultsinDLSystems,
	title = {Taxonomy of real faults in deep learning systems},
	abstract = {The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems.We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50\% of the survey participants.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Humbatova, Nargiz and Jahangirova, Gunel and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo},
	year = {2020},
	keywords = {Deep learning, Real faults, Software testing, Taxonomy},
}

@inproceedings{Guo2018DLFuzz,
	title = {{DLFuzz}: {Differential} {Fuzzing} {Testing} of {Deep} {Learning} {Systems}},
	abstract = {Deep learning (DL) systems are increasingly applied to safety-critical domains such as autonomous driving cars. It is of significant importance to ensure the reliability and robustness of DL systems. Existing testing methodologies always fail to include rare inputs in the testing dataset and exhibit low neuron coverage. In this paper, we propose DLFuzz, the frst differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality. We present empirical evaluations on two well-known datasets to demonstrate its efficiency. Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check, but could generate 338.59\% more adversarial inputs with 89.82\% smaller perturbations, averagely obtain 2.86\% higher neuron coverage, and save 20.11\% time consumption.},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Guo, Jianmin and Jiang, Yu and Zhao, Yue and Chen, Quan and Sun, Jiaguang},
	year = {2018},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{Breck2019DataValidation4ML,
	title = {Data {Validation} for {Machine} {Learning}},
	abstract = {Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efﬁciency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any beneﬁts on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning.},
	booktitle = {the {Conference} on {Machine} {Learning} and {Systems} ({MLSys})},
	author = {Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven and Zinkevich, Martin},
	year = {2019},
}

@article{openai_dota_2019,
	title = {Dota 2 with {Large} {Scale} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1912.06680},
	abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
	urldate = {2022-03-17},
	journal = {arXiv:1912.06680 [cs, stat]},
	author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.06680},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{Amershi2019SE4MLCaseStudy,
	title = {Software {Engineering} for {Machine} {Learning}: {A} {Case} {Study}},
	shorttitle = {Software {Engineering} for {Machine} {Learning}},
	abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workﬂow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workﬂow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identiﬁed three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difﬁcult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difﬁcult to handle as distinct modules than traditional software components — models may be “entangled” in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
	booktitle = {International {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald},
	year = {2019},
}

@inproceedings{Gopalakrishna2022IoTPractices,
	title = {``{If} security is required'': {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	abstract = {The latest generation of IoT systems incorporate machine learning (ML) technologies on edge devices. This introduces new engineering challenges to bring ML onto resource-constrained hardware, and complications for ensuring system security and privacy. Existing research prescribes iterative processes for machine learning enabled IoT products to ease development and increase product success. However, these processes mostly focus on existing practices used in other generic software development areas and are not specialized for the purpose of machine learning or IoT devices.},
	booktitle = {International {Workshop} on {Software} {Engineering} {Research} \& {Practices} for the {Internet} of {Things} ({SERP4IoT})},
	author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C},
	year = {2022},
}

@inproceedings{Shen2021DLCompilerBugs,
	title = {A comprehensive study of deep learning compiler bugs},
	abstract = {There are increasing uses of deep learning (DL) compilers to generate optimized code, boosting the runtime performance of DL models on speci�c hardware. Like their traditional counterparts, DL compilers can generate incorrect code, resulting in unexpected model behaviors that may cause catastrophic consequences in missioncritical systems. On the other hand, the DL models processed by DL compilers di�er fundamentally from imperative programs in that the program logic in DL models is implicit. As such, various characteristics of the bugs arising from traditional compilers need to be revisited in the context of DL compilers.},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Shen, Qingchao and Ma, Haoyang and Chen, Junjie and Tian, Yongqiang and Cheung, Shing-Chi and Chen, Xiang},
	year = {2021},
}

@article{manes_art_2021,
	title = {The {Art}, {Science}, and {Engineering} of {Fuzzing}: {A} {Survey}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {The {Art}, {Science}, and {Engineering} of {Fuzzing}},
	doi = {10.1109/TSE.2019.2946563},
	abstract = {Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.},
	number = {11},
	journal = {IEEE Transactions on Software Engineering},
	author = {Manès, Valentin J.M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Computer bugs, Fuzzing, Security, Software security, Terminology, automated software testing, fuzz testing, fuzzing},
	pages = {2312--2331},
}

@article{zhu_fuzzing_2022,
	title = {Fuzzing: {A} {Survey} for {Roadmap}},
	issn = {0360-0300},
	shorttitle = {Fuzzing},
	url = {https://doi.org/10.1145/3512345},
	doi = {10.1145/3512345},
	abstract = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this paper, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
	urldate = {2022-03-14},
	journal = {ACM Computing Surveys},
	author = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
	year = {2022},
	note = {Just Accepted},
	keywords = {Automation, Fuzz Testing, Fuzzing Theory, Input Space, Security},
}

@article{Shorten2019SurveyonImageDataAug,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year = {2019},
}

@article{Lettner2021DaQL2,
	title = {{DaQL} 2.0: {Measure} {Data} {Quality} based on {Entity} {Models}},
	abstract = {IAnbosrtdrearcto make good decisions, the data used for decision-making needs to be of high quality. As the volume of data continually increases, ensuring high data quality is a big challenge nowadays and needs to be automated with tools. The goal of the Data QInuoarlditeyrLtoibmraarkye(DgoaoQdLd)eicsistoiopnrso, vthideedaattaooulsetod cfoorntdiencuiosuiosnly-menaksuinreg annededms etoasbuereodf ahtiaghquqaulaitlyitya.sApsrotphoesveodluinm[e5]o.fIdnatthaiscopnatpineur,awllye pinrcerseeanstetsh,eecnusrurreinntgsthaitguhs odfattaheqdueavlietylopismaenbtigofcthhaellneenwgeDnaoQwLadvaeyrssioannd2.0n.eeTdhse tmo abine caountotrmibautetidonwoifthDtaoQolLs.2T.0hies gthoealpoosfstihbeiliDtyattoa dQeuﬁanlietydaLtiabqraurayli(tDy aruQleLs) fiosrtocopmropvleidxedaattaooolbtjoecctson(ctianluleoduselnytietinessu),rewahnicdhmreeparseusreendtabtuasqinueaslistyobajsecptrso.pIonsceodnitnra[s5t]t.oInextihsitsinpgaptoeor,lsw, ae upsreesrednotetshenocturrreeqnutirsetadtuestaoilfetdhekndoewvelelodpgme eanbtoouft tthhee ndeawtabDaaseQsLchveemrsiaotnha2t.0is. Tohbesemrvaeidn.contribution of DaQL 2.0 is the possibility to deﬁne data quality rules for complex data objects (called entities), which represent business objects. In contrast to existing tools, a ucse2r0d2o1esThneotAreuqthuoirres.dPeutabilliesdhekdnobwy lEedlsgeeviaebroBu.tVt.he database schema that is observed.},
	journal = {Procedia Computer Science},
	author = {Lettner, Christian and Stumptner, Reinhard and Fragner, Werner and Rauchenzauner, Franz and Ehrlinger, Lisa},
	year = {2021},
}

@inproceedings{Aranda2009SecretLifeofBugs,
	title = {The secret life of bugs: {Going} past the errors and omissions in software repositories},
	abstract = {Every bug has a story behind it. The people that discover and resolve it need to coordinate, to get information from documents, tools, or other people, and to navigate through issues of accountability, ownership, and organizational structure. This paper reports on a field study of coordination activities around bug fixing that used a combination of case study research and a survey of software professionals. Results show that the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through automation of electronic repositories, and that such automation provides incomplete and often erroneous accounts of coordination. The paper uses rich bug histories and survey results to identify common bug fixing coordination patterns and to provide implications for tool designers and researchers of coordination in software development.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Aranda, Jorge and Venolia, Gina},
	year = {2009},
	keywords = {Automation, Computer bugs, Data mining, History, Navigation, Productivity, Programming, Software debugging, Software development management, Spatial databases},
}

@article{Ocariza2017JavaScriptBugs,
	title = {A {Study} of {Causes} and {Consequences} of {Client}-{Side} {JavaScript} {Bugs}},
	abstract = {Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ocariza, Frolin S. and Bajaj, Kartik and Pattabiraman, Karthik and others},
	year = {2017},
	keywords = {Cascading style sheets, Computer bugs, Data mining, Document Object Model (DOM), Faults, HTML, JavaScript, Market research, Reliability, Servers, bug reports, empirical study},
}

@article{fang_you_2021,
	title = {You {Only} {Look} at {One} {Sequence}: {Rethinking} {Transformer} in {Vision} through {Object} {Detection}},
	shorttitle = {You {Only} {Look} at {One} {Sequence}},
	url = {http://arxiv.org/abs/2106.00666},
	abstract = {Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.},
	urldate = {2022-03-07},
	journal = {arXiv:2106.00666 [cs]},
	author = {Fang, Yuxin and Liao, Bencheng and Wang, Xinggang and Fang, Jiemin and Qi, Jiyang and Wu, Rui and Niu, Jianwei and Liu, Wenyu},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.00666},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{ahmed_impact_2020,
	title = {The {Impact} of {Filter} {Size} and {Number} of {Filters} on {Classification} {Accuracy} in {CNN}},
	doi = {10.1109/CSASE48920.2020.9142089},
	abstract = {Convolution Neural Networks (CNNs) have received considerable attention due to their ability to learn directly from data classification features. CNNs used for human motion classification, where predefined and fixed convolutional filter size used. In this paper, different sizes and numbers of filters were used with CNN to determine their effect on accuracy of human motion classification. This work has been done through series of experiments; in each experiment, different filter size and number of filters have been applied. The best performance has been obtained when using 4 convolution layers and 2 pooling layers, whereas has been used the large filter size with upper convolution layer and with each layer the size of filter decreased and number of filters increased, so that, the maximum value of the accuracy classification was 98.98\%.},
	booktitle = {2020 {International} {Conference} on {Computer} {Science} and {Software} {Engineering} ({CSASE})},
	author = {Ahmed, Wafaa Shihab and Karim, Abdul amir A.},
	month = apr,
	year = {2020},
	keywords = {Computer architecture, Computer science, Convolution, Convolution Neural Networks (CNNs), Convolutional filters size, Neural networks, Software engineering, Testing, Training, human Activity Recognition (HAR) and Convolutional layers},
	pages = {88--93},
}

@article{benz_adversarial_2021,
	title = {Adversarial {Robustness} {Comparison} of {Vision} {Transformer} and {MLP}-{Mixer} to {CNNs}},
	url = {http://arxiv.org/abs/2110.02797},
	abstract = {Convolutional Neural Networks (CNNs) have become the de facto gold standard in computer vision applications in the past years. Recently, however, new model architectures have been proposed challenging the status quo. The Vision Transformer (ViT) relies solely on attention modules, while the MLP-Mixer architecture substitutes the self-attention modules with Multi-Layer Perceptrons (MLPs). Despite their great success, CNNs have been widely known to be vulnerable to adversarial attacks, causing serious concerns for security-sensitive applications. Thus, it is critical for the community to know whether the newly proposed ViT and MLP-Mixer are also vulnerable to adversarial attacks. To this end, we empirically evaluate their adversarial robustness under several adversarial attack setups and benchmark them against the widely used CNNs. Overall, we find that the two architectures, especially ViT, are more robust than their CNN models. Using a toy example, we also provide empirical evidence that the lower adversarial robustness of CNNs can be partially attributed to their shift-invariant property. Our frequency analysis suggests that the most robust ViT architectures tend to rely more on low-frequency features compared with CNNs. Additionally, we have an intriguing finding that MLP-Mixer is extremely vulnerable to universal adversarial perturbations.},
	urldate = {2022-03-07},
	journal = {arXiv:2110.02797 [cs]},
	author = {Benz, Philipp and Ham, Soomin and Zhang, Chaoning and Karjauv, Adil and Kweon, In So},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.02797},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{mahmood_robustness_2021,
	address = {Montreal, QC, Canada},
	title = {On the {Robustness} of {Vision} {Transformers} to {Adversarial} {Examples}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710333/},
	doi = {10.1109/ICCV48922.2021.00774},
	abstract = {Recent advances in attention-based networks have shown that Vision Transformers can achieve state-of-the-art or near state-of-the-art results on many image classiﬁcation tasks. This puts transformers in the unique position of being a promising alternative to traditional convolutional neural networks (CNNs). While CNNs have been carefully studied with respect to adversarial attacks, the same cannot be said of Vision Transformers. In this paper, we study the robustness of Vision Transformers to adversarial examples. Our analyses of transformer security is divided into three parts. First, we test the transformer under standard whitebox and black-box attacks. Second, we study the transferability of adversarial examples between CNNs and transformers. We show that adversarial examples do not readily transfer between CNNs and transformers. Based on this ﬁnding, we analyze the security of a simple ensemble defense of CNNs and transformers. By creating a new attack, the self-attention blended gradient attack, we show that such an ensemble is not secure under a white-box adversary. However, under a black-box adversary, we show that an ensemble can achieve unprecedented robustness without sacriﬁcing clean accuracy. Our analysis for this work is done using six types of white-box attacks and two types of black-box attacks. Our study encompasses multiple Vision Transformers, Big Transfer Models and CNN architectures trained on CIFAR-10, CIFAR-100 and ImageNet.},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Mahmood, Kaleel and Mahmood, Rigel and van Dijk, Marten},
	month = oct,
	year = {2021},
	pages = {7818--7827},
}

@article{mao_towards_2021,
	title = {Towards {Robust} {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2105.07926},
	abstract = {Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By using and combining robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. We further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results on ImageNet and six robustness benchmarks show the advanced robustness and generalization ability of RVT compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* also achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C and ImageNet-Sketch. The code will be available at {\textbackslash}url\{https://git.io/Jswdk\}.},
	urldate = {2022-03-07},
	journal = {arXiv:2105.07926 [cs]},
	author = {Mao, Xiaofeng and Qi, Gege and Chen, Yuefeng and Li, Xiaodan and Duan, Ranjie and Ye, Shaokai and He, Yuan and Xue, Hui},
	month = may,
	year = {2021},
	note = {arXiv: 2105.07926},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_hierarchical_nodate,
	title = {Hierarchical multimodal transformer to summarize videos {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0925231221015253?token=A480B732FEA04B74A20DDBD67A0CC28A2B91FD9056B487CAF437F4B139AAB37975F505BFA65F58F054634DC87178D63B&originRegion=us-east-1&originCreation=20220307232017},
	language = {en},
	urldate = {2022-03-07},
	doi = {10.1016/j.neucom.2021.10.039},
}

@article{ahmad_transformer-based_2020,
	title = {A {Transformer}-based {Approach} for {Source} {Code} {Summarization}},
	url = {http://arxiv.org/abs/2005.00653},
	abstract = {Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.},
	urldate = {2022-03-07},
	journal = {arXiv:2005.00653 [cs, stat]},
	author = {Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
	month = may,
	year = {2020},
	note = {arXiv: 2005.00653},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Machine Learning},
}

@inproceedings{zhang_sentiment_2020,
	title = {Sentiment {Analysis} for {Software} {Engineering}: {How} {Far} {Can} {Pre}-trained {Transformer} {Models} {Go}?},
	shorttitle = {Sentiment {Analysis} for {Software} {Engineering}},
	doi = {10.1109/ICSME46990.2020.00017},
	abstract = {Extensive research has been conducted on sentiment analysis for software engineering (SA4SE). Researchers have invested much effort in developing customized tools (e.g., SentiStrength-SE, SentiCR) to classify the sentiment polarity for Software Engineering (SE) specific contents (e.g., discussions in Stack Overflow and code review comments). Even so, there is still much room for improvement. Recently, pre-trained Transformer-based models (e.g., BERT, XLNet) have brought considerable breakthroughs in the field of natural language processing (NLP). In this work, we conducted a systematic evaluation of five existing SA4SE tools and variants of four state-of-the-art pre-trained Transformer-based models on six SE datasets. Our work is the first to fine-tune pre-trained Transformer-based models for the SA4SE task. Empirically, across all six datasets, our fine-tuned pre-trained Transformer-based models outperform the existing SA4SE tools by 6.5-35.6\% in terms of macro/micro-averaged F1 scores.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Zhang, Ting and Xu, Bowen and Thung, Ferdian and Haryono, Stefanus Agus and Lo, David and Jiang, Lingxiao},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Analytical models, Natural Language Processing, Pre-trained Models, Sentiment Analysis, Sentiment analysis, Software Mining, Software engineering, Software maintenance, Systematics, Task analysis, Tools},
	pages = {70--80},
}

@inproceedings{dong_image_2021,
	title = {Image transformer for explainable autonomous driving system},
	doi = {10.1109/ITSC48978.2021.9565103},
	abstract = {In the last decade, deep learning (DL) approaches have been used successfully in computer vision (CV) applications. However, DL-based CV models are generally considered to be black boxes due to their lack of interpretability. This black box behavior has exacerbated user distrust and therefore has prevented widespread deployment DLCV models in autonomous driving tasks even though some of these models exhibit superiority over human performance. For this reason, it is essential to develop explainable DL models for autonomous driving task. Explainable DL models are able to not only boost user trust in autonomy but also serve as a diagnostic approach to identify the defects and weaknesses of the model during the system development phase. In this paper, we propose such an explainable end-to-end autonomous driving system using “Transformer,” a state-of-the-art (SOTA) self-attention based model, to map visual features from images collected by onboard cameras to guide potential driving actions with corresponding explanations. The results demonstrate the efficacy of our proposed model as it outperforms the benchmark model by a significant margin in terms of actions and explanations prediction with lower computational cost.},
	booktitle = {2021 {IEEE} {International} {Intelligent} {Transportation} {Systems} {Conference} ({ITSC})},
	author = {Dong, Jiqian and Chen, Sikai and Zong, Shuya and Chen, Tiantian and Labi, Samuel},
	month = sep,
	year = {2021},
	keywords = {Computational modeling, Computer architecture, Predictive models, Training, Transformers, Vehicular ad hoc networks, Visualization},
	pages = {2732--2737},
}

@inproceedings{prakash_multi-modal_2021,
	address = {Nashville, TN, USA},
	title = {Multi-{Modal} {Fusion} {Transformer} for {End}-to-{End} {Autonomous} {Driving}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578103/},
	doi = {10.1109/CVPR46437.2021.00700},
	abstract = {How should representations from complementary sensors be integrated for autonomous driving? Geometrybased sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in trafﬁc light state can affect the behavior of a vehicle geometrically distant from that trafﬁc light. Geometry alone may therefore be insufﬁcient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling trafﬁc oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efﬁcacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76\% compared to geometry-based fusion.},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
	month = jun,
	year = {2021},
	pages = {7073--7083},
}

@article{yuan_temporal-channel_2021,
	title = {Temporal-{Channel} {Transformer} for {3D} {Lidar}-{Based} {Video} {Object} {Detection} for {Autonomous} {Driving}},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2021.3082763},
	abstract = {The strong demand of autonomous driving in the industry has led to vigorous interest in 3D object detection and resulted in many excellent 3D object detection algorithms. However, the vast majority of algorithms only model single-frame data, ignoring the temporal clue in video sequence. In this work, we propose a new transformer, called Temporal-Channel Transformer (TCTR), to model the temporal-channel domain and spatial-wise relationships for video object detecting from Lidar data. As the special design of this transformer, the information encoded in the encoder is different from that in the decoder. The encoder encodes temporal-channel information of multiple frames while the decoder decodes the spatial-wise information for the current frame in a voxel-wise manner. Specifically, the temporal-channel encoder of the transformer is designed to encode the information of different channels and frames by utilizing the correlation among features from different channels and frames. On the other hand, the spatial decoder of the transformer decodes the information for each location of the current frame. Before conducting the object detection with detection head, a gate mechanism is further deployed for re-calibrating the features of current frame, which filters out the object-irrelevant information by repetitively refining the representation of target frame along with the up-sampling process. Experimental results reveal that TCTR achieves the state-of-the-art performance in grid voxel-based 3D object detection on the nuScenes benchmark.},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Yuan, Zhenxun and Song, Xiao and Bai, Lei and Wang, Zhe and Ouyang, Wanli},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {3D object detection, Correlation, Decoding, Feature extraction, Head, Laser radar, Lidar-based video, Object detection, Temporal-channel attention, Three-dimensional displays, Transformer},
	pages = {1--1},
}

@article{soleymani_construction_2021,
	title = {Construction material classification on imbalanced datasets for construction monitoring automation using {Vision} {Transformer} ({ViT}) architecture},
	url = {http://arxiv.org/abs/2108.09527},
	abstract = {Nowadays, automation is a critical topic due to its significant impacts on the productivity of construction projects. Utilizing automation in this industry brings about great results, such as remarkable improvements in the efficiency, quality, and safety of construction activities. The scope of automation in construction includes a wide range of stages, and monitoring construction projects is no exception. Additionally, it is of great importance in project management since an accurate and timely assessment of project progress enables managers to quickly identify deviations from the schedule and take the required actions at the right time. In this stage, one of the most important tasks is to daily keep track of the project progress, which is very time-consuming and labor-intensive, but automation has facilitated and accelerated this task. It also eliminated or at least decreased the risk of many dangerous tasks. In this way, the first step of construction automation is to detect used materials in a project site automatically. In this paper, a novel deep learning architecture is utilized, called Vision Transformer (ViT), for detecting and classifying construction materials. To evaluate the applicability and performance of the proposed method, it is trained and tested on three large imbalanced datasets, namely Construction Material Library (CML) and Building Material Dataset (BMD), used in the previous papers, as well as a new dataset created by combining them. The achieved results revealed an accuracy of 100 percent in all parameters and also in each material category. It is believed that the proposed method provides a novel and robust tool for detecting and classifying different material types.},
	urldate = {2022-03-07},
	journal = {arXiv:2108.09527 [cs]},
	author = {Soleymani, Maryam and Bonyani, Mahdi and Mahami, Hadi and Nasirzadeh, Farnad},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.09527},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{postnikov_transformer_2021,
	title = {Transformer based trajectory prediction},
	url = {http://arxiv.org/abs/2112.04350},
	abstract = {To plan a safe and efficient route, an autonomous vehicle should anticipate future motions of other agents around it. Motion prediction is an extremely challenging task which recently gained significant attention of the research community. In this work, we present a simple and yet strong baseline for uncertainty aware motion prediction based purely on transformer neural networks, which has shown its effectiveness in conditions of domain change. While being easy-to-implement, the proposed approach achieves competitive performance and ranks 1\${\textasciicircum}\{st\}\$ on the 2021 Shifts Vehicle Motion Prediction Competition.},
	urldate = {2022-03-07},
	journal = {arXiv:2112.04350 [cs]},
	author = {Postnikov, Aleksey and Gamayunov, Aleksander and Ferrer, Gonzalo},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.04350},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{postnikov_transformer_2021-1,
	title = {Transformer based trajectory prediction},
	url = {http://arxiv.org/abs/2112.04350},
	abstract = {To plan a safe and efficient route, an autonomous vehicle should anticipate future motions of other agents around it. Motion prediction is an extremely challenging task which recently gained significant attention of the research community. In this work, we present a simple and yet strong baseline for uncertainty aware motion prediction based purely on transformer neural networks, which has shown its effectiveness in conditions of domain change. While being easy-to-implement, the proposed approach achieves competitive performance and ranks 1\${\textasciicircum}\{st\}\$ on the 2021 Shifts Vehicle Motion Prediction Competition.},
	urldate = {2022-03-07},
	journal = {arXiv:2112.04350 [cs]},
	author = {Postnikov, Aleksey and Gamayunov, Aleksander and Ferrer, Gonzalo},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.04350},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{ivanov_comparative_2021,
	title = {Comparative {Analysis} of {Deep} {Neural} {Networks} {Architectures} for {Visual} {Recognition} in the {Autonomous} {Transport} {Systems}},
	volume = {2096},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/2096/1/012101},
	doi = {10.1088/1742-6596/2096/1/012101},
	abstract = {Abstract
            This paper analyses and presents an experimental investigation of the efficiency of modern models for object recognition in computer vision systems of robotic complexes. In this article, the applicability of transformers for experimental classification problems has been investigated. The comparison results are presented taking into account various limitations specific to robotics. Based on the results of the undertaken studies, recommendations on the use of models in the marine vessels classification problem are proposed},
	language = {en},
	number = {1},
	urldate = {2022-03-07},
	journal = {Journal of Physics: Conference Series},
	author = {Ivanov, Y S and Zhiganov, S V and Liubushkina, N N},
	month = nov,
	year = {2021},
	pages = {012101},
}

@article{kargar_vision_2021,
	title = {Vision {Transformer} for {Learning} {Driving} {Policies} in {Complex} {Multi}-{Agent} {Environments}},
	url = {http://arxiv.org/abs/2109.06514},
	abstract = {Driving in a complex urban environment is a difficult task that requires a complex decision policy. In order to make informed decisions, one needs to gain an understanding of the long-range context and the importance of other vehicles. In this work, we propose to use Vision Transformer (ViT) to learn a driving policy in urban settings with birds-eye-view (BEV) input images. The ViT network learns the global context of the scene more effectively than with earlier proposed Convolutional Neural Networks (ConvNets). Furthermore, ViT's attention mechanism helps to learn an attention map for the scene which allows the ego car to determine which surrounding cars are important to its next decision. We demonstrate that a DQN agent with a ViT backbone outperforms baseline algorithms with ConvNet backbones pre-trained in various ways. In particular, the proposed method helps reinforcement learning algorithms to learn faster, with increased performance and less data than baselines.},
	urldate = {2022-03-07},
	journal = {arXiv:2109.06514 [cs]},
	author = {Kargar, Eshagh and Kyrki, Ville},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.06514},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2022-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@article{kolesnikov_big_2020,
	title = {Big {Transfer} ({BiT}): {General} {Visual} {Representation} {Learning}},
	shorttitle = {Big {Transfer} ({BiT})},
	url = {http://arxiv.org/abs/1912.11370},
	abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5\% top-1 accuracy on ILSVRC-2012, 99.4\% on CIFAR-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8\% on ILSVRC-2012 with 10 examples per class, and 97.0\% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
	urldate = {2022-03-07},
	journal = {arXiv:1912.11370 [cs]},
	author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
	month = may,
	year = {2020},
	note = {arXiv: 1912.11370},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-03-07},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{noauthor_overview_nodate,
	title = {An overview of {Transformer} {Architectures} in {Computer} {Vision}},
	url = {https://broutonlab.com/blog/broutonlab.com/blog/an-overview-of-transformer-architectures-in-computer-vision},
	abstract = {In the article, we explore novel vision transformers architectures and their application to сomputer vision problems.},
	language = {en},
	urldate = {2022-03-07},
	journal = {BroutonLab},
}

@article{khan_transformers_2022,
	title = {Transformers in {Vision}: {A} {Survey}},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Transformers in {Vision}},
	url = {http://arxiv.org/abs/2101.01169},
	doi = {10.1145/3505244},
	abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
	urldate = {2022-03-07},
	journal = {ACM Computing Surveys},
	author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
	month = jan,
	year = {2022},
	note = {arXiv: 2101.01169},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {3505244},
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0743731518308773?token=79A9BBCFF621EC1AEB097349D1692BED717F04D949C9517DBF01B885A909258CC6C2CC0A9E8293E1C3409648722BAC5F&originRegion=us-east-1&originCreation=20220304021905},
	language = {en},
	urldate = {2022-03-04},
	doi = {10.1016/j.jpdc.2019.07.007},
}

@article{han_ghostnet_2020,
	title = {{GhostNet}: {More} {Features} from {Cheap} {Operations}},
	shorttitle = {{GhostNet}},
	url = {http://arxiv.org/abs/1911.11907},
	abstract = {Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. \$75.7{\textbackslash}\%\$ top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at https://github.com/huawei-noah/ghostnet},
	urldate = {2022-02-28},
	journal = {arXiv:1911.11907 [cs]},
	author = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.11907},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{redmon_yolo9000_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	urldate = {2022-02-28},
	journal = {arXiv:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.08242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{miller_empirical_1990,
	title = {An empirical study of the reliability of {UNIX} utilities},
	volume = {33},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/96267.96279},
	doi = {10.1145/96267.96279},
	abstract = {Operating system facilities, such as the kernel and utility programs, are typically assumed to be reliable. In our recent experiments, we have been able to crash 25-33\% of the utility programs on any version of UNIX that was tested. This report describes these tests and an analysis of the program bugs that caused the crashes.},
	language = {en},
	number = {12},
	urldate = {2022-02-24},
	journal = {Communications of the ACM},
	author = {Miller, Barton P. and Fredriksen, Louis and So, Bryan},
	month = dec,
	year = {1990},
	pages = {32--44},
}

@inproceedings{chen_performance_2018,
	address = {Taipei City, Taiwan},
	title = {Performance {Evaluation} of {Edge} {Computing}-{Based} {Deep} {Learning} {Object} {Detection}},
	isbn = {978-1-4503-6553-6},
	url = {http://dl.acm.org/citation.cfm?doid=3301326.3301369},
	doi = {10.1145/3301326.3301369},
	abstract = {This article presents a method for implementing the deep learning object detection based on a low-cost edge computing IoT device. The limit of the hardware is a challenge for working the pretrained neural network model on a low-cost IoT device. Hence, we utilize the Neural Compute Stick (NCS) to accelerate the neural network model on a low-cost IoT device by its high efficiency floating-point operation. With the NCS, the low-cost IoT device can successfully work the pre-trained neural network model and become an edge computing device. The experimental results show the proposed method can effectively detect the objects based on deep learning on an edge computing IoT device. Furthermore, the objective experiment demonstrates the proposed method can immediately infer the neural network model for images in average 1.7 seconds with only one of the NCS and the neural network model can reach average 9.2 fps for the video sequences with four NCSs acceleration. In addition, the discrepancy of the neural network model between the edge device and the edge server is less than 2\% mean average precision (mAP).},
	language = {en},
	urldate = {2022-02-24},
	booktitle = {Proceedings of the 2018 {VII} {International} {Conference} on {Network}, {Communication} and {Computing} - {ICNCC} 2018},
	publisher = {ACM Press},
	author = {Chen, Chuan-Wen and Ruan, Shanq-Jang and Lin, Chang-Hong and Hung, Chun-Chi},
	year = {2018},
	pages = {40--43},
}

@inproceedings{tuli_edgelens_2019,
	title = {{EdgeLens}: {Deep} {Learning} based {Object} {Detection} in {Integrated} {IoT}, {Fog} and {Cloud} {Computing} {Environments}},
	shorttitle = {{EdgeLens}},
	doi = {10.1109/ISCON47742.2019.9036216},
	abstract = {Data-intensive applications are growing at an increasing rate and there is a growing need to solve scalability and high-performance issues in them. By the advent of Cloud computing paradigm, it became possible to harness remote resources to build and deploy these applications. In recent years, new set of applications and services based on Internet of Things (IoT) paradigm, require to process large amount of data in very less time. Among them surveillance and object detection have gained prime importance, but cloud is unable to bring down the network latencies to meet the response time requirements. This problem is solved by Fog computing which harnesses resources in the edge of the network along with remote cloud resources as required. However, there is still a lack of frameworks that are successfully able to integrate sophisticated software and applications, especially deep learning, with fog and cloud computing environments. In this work, we propose a framework to deploy deep learning-based applications in fog-cloud environments to harness edge and cloud resources to provide better service quality for such applications. Our proposed framework, called EdgeLens, adapts to the application or user requirements to provide high accuracy or low latency modes of services. We also tested the performance of the software in terms of accuracy, response time, jitter, network bandwidth and power consumption and show how EdgeLens adapts to different service requirements.},
	booktitle = {2019 4th {International} {Conference} on {Information} {Systems} and {Computer} {Networks} ({ISCON})},
	author = {Tuli, Shreshth and Basumatary, Nipam and Buyya, Rajkumar},
	year = {2019},
	keywords = {Cloud computing, Computational modeling, Deep Learning, Deep learning, Fog computing, Image edge detection, Internet of Things, Logic gates, Object Detection, Object detection, Task analysis},
	pages = {496--502},
}

@article{muhawenayo_compressed_2021,
	title = {Compressed {Object} {Detection}},
	url = {http://arxiv.org/abs/2102.02896},
	abstract = {Deep learning approaches have achieved unprecedented performance in visual recognition tasks such as object detection and pose estimation. However, state-of-the-art models have millions of parameters represented as floats which make them computationally expensive and constrain their deployment on hardware such as mobile phones and IoT nodes. Most commonly, activations of deep neural networks tend to be sparse thus proving that models are over parametrized with redundant neurons. Model compression techniques, such as pruning and quantization, have recently shown promising results by improving model complexity with little loss in performance. In this work, we extended pruning, a compression technique that discards unnecessary model connections, and weight sharing techniques for the task of object detection. With our approach, we are able to compress a state-of-the-art object detection model by 30.0\% without a loss in performance. We also show that our compressed model can be easily initialized with existing pre-trained weights, and thus is able to fully utilize published state-of-the-art model zoos.},
	urldate = {2022-02-23},
	journal = {arXiv:2102.02896 [cs, eess]},
	author = {Muhawenayo, Gedeon and Gkioxari, Georgia},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.02896},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{yang_method_2017,
	title = {A method to estimate the energy consumption of deep neural networks},
	doi = {10.1109/ACSSC.2017.8335698},
	abstract = {Deep Neural Networks (DNNs) have enabled state-of-the-art accuracy on many challenging artificial intelligence tasks. While most of the computation currently resides in the cloud, it is desirable to embed DNN processing locally near the sensor due to privacy, security, and latency concerns or limitations in communication bandwidth. Accordingly, there has been increasing interest in the research community to design energy-efficient DNNs. However, estimating energy consumption from the DNN model is much more difficult than other metrics such as storage cost (model size) and throughput (number of operations). This is due to the fact that a significant portion of the energy is consumed by data movement, which is difficult to extract directly from the DNN model. This work proposes an energy estimation methodology that can estimate the energy consumption of a DNN based on its architecture, sparsity, and bitwidth. This methodology can be used to evaluate the various DNN architectures and energy-efficient techniques that are currently being proposed in the field and guide the design of energy-efficient DNNs. We have released an online version of the energy estimation tool at energyestimation.mit.edu. We believe that this method will play a critical role in bridging the gap between algorithm and hardware design and provide useful insights for the development of energy-efficient DNNs.},
	booktitle = {2017 51st {Asilomar} {Conference} on {Signals}, {Systems}, and {Computers}},
	author = {Yang, Tien-Ju and Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
	year = {2017},
	note = {ISSN: 2576-2303},
	keywords = {Deep learning, Energy consumption, Estimation, Hardware, Measurement, Memory management, Neural networks, Optimization, deep neural network, energy estimation, energy metric, machine learning},
	pages = {1916--1920},
}

@article{zou_object_2019,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {http://arxiv.org/abs/1905.05055},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	urldate = {2022-02-23},
	journal = {arXiv:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{sinha_deep_2018,
	title = {Deep {Learning} {For} {Computer} {Vision} {Tasks}: {A} review},
	shorttitle = {Deep {Learning} {For} {Computer} {Vision} {Tasks}},
	url = {http://arxiv.org/abs/1804.03928},
	abstract = {Deep learning has recently become one of the most popular sub-fields of machine learning owing to its distributed data representation with multiple levels of abstraction. A diverse range of deep learning algorithms are being employed to solve conventional artificial intelligence problems. This paper gives an overview of some of the most widely used deep learning algorithms applied in the field of computer vision. It first inspects the various approaches of deep learning algorithms, followed by a description of their applications in image classification, object identification, image extraction and semantic segmentation in the presence of noise. The paper concludes with the discussion of the future scope and challenges for construction and training of deep neural networks.},
	urldate = {2022-02-23},
	journal = {arXiv:1804.03928 [cs]},
	author = {Sinha, Rajat Kumar and Pandey, Ruchi and Pattnaik, Rohan},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.03928},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{Schmidhuber2015DLinNN,
	title = {Deep learning in neural networks: {An} overview},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	year = {2015},
}

@inproceedings{jose_real-time_2019,
	address = {Seoul, Korea (South)},
	title = {Real-{Time} {Object} {Detection} {On} {Low} {Power} {Embedded} {Platforms}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9021990/},
	doi = {10.1109/ICCVW.2019.00304},
	abstract = {Low power real-time object detection is an interesting application in deep learning with applications in smart wearables, Advanced Driver Assistance Systems (ADAS), drone surveillance systems, etc. In this paper, we discuss the limitations with existing networks and enumerate the various factors to keep in mind while designing neural networks for a target hardware. Based on our experience of working with TI embedded platform, we provide a systematic approach for designing real time object detection networks on low power embedded platforms. First stage involves identifying the optimal layers for the hardware, by understanding it’s computational and memory limitations. The next step is to use these layers to come up with a basic building block that has low computational complexity. The ﬁnal stage involves using model compression techniques like sparsiﬁcation/quantization to accelerate the inference process. Based on this design approach, we were able to come up with a low latency object detection model HX-LPNet that operates at 22 FPS on low power TDA2PX System on Chip(SoC) provided by Texas Instruments (TI).},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Jose, George and Kumar, Aashish and Kruthiventi, Srinivas and Saha, Sambuddha and Muralidhara, Harikrishna},
	month = oct,
	year = {2019},
	pages = {2485--2492},
}

@article{liu_deep_2020,
	title = {Deep {Learning} for {Generic} {Object} {Detection}: {A} {Survey}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Deep {Learning} for {Generic} {Object} {Detection}},
	url = {https://link.springer.com/10.1007/s11263-019-01247-4},
	doi = {10.1007/s11263-019-01247-4},
	abstract = {Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predeﬁned categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the ﬁeld of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this ﬁeld brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We ﬁnish the survey by identifying promising directions for future research.},
	language = {en},
	number = {2},
	urldate = {2022-02-22},
	journal = {International Journal of Computer Vision},
	author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietikäinen, Matti},
	month = feb,
	year = {2020},
	pages = {261--318},
}

@article{deng_model_2020,
	title = {Model {Compression} and {Hardware} {Acceleration} for {Neural} {Networks}: {A} {Comprehensive} {Survey}},
	volume = {108},
	issn = {1558-2256},
	shorttitle = {Model {Compression} and {Hardware} {Acceleration} for {Neural} {Networks}},
	doi = {10.1109/JPROC.2020.2976475},
	abstract = {Domain-specific hardware is becoming a promising topic in the backdrop of improvement slow down for general-purpose processors due to the foreseeable end of Moore's Law. Machine learning, especially deep neural networks (DNNs), has become the most dazzling domain witnessing successful applications in a wide spectrum of artificial intelligence (AI) tasks. The incomparable accuracy of DNNs is achieved by paying the cost of hungry memory consumption and high computational complexity, which greatly impedes their deployment in embedded systems. Therefore, the DNN compression concept was naturally proposed and widely used for memory saving and compute acceleration. In the past few years, a tremendous number of compression techniques have sprung up to pursue a satisfactory tradeoff between processing efficiency and application accuracy. Recently, this wave has spread to the design of neural network accelerators for gaining extremely high performance. However, the amount of related works is incredibly huge and the reported approaches are quite divergent. This research chaos motivates us to provide a comprehensive survey on the recent advances toward the goal of efficient compression and execution of DNNs without significantly compromising accuracy, involving both the high-level algorithms and their applications in hardware design. In this article, we review the mainstream compression approaches such as compact model, tensor decomposition, data quantization, and network sparsification. We explain their compression principles, evaluation metrics, sensitivity analysis, and joint-way use. Then, we answer the question of how to leverage these methods in the design of neural network accelerators and present the state-of-the-art hardware architectures. In the end, we discuss several existing issues such as fair comparison, testing workloads, automatic compression, influence on security, and framework/hardware-level support, and give promising topics in this field and the possible challenges as well. This article attempts to enable readers to quickly build up a big picture of neural network compression and acceleration, clearly evaluate various methods, and confidently get started in the right way.},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
	month = apr,
	year = {2020},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Acceleration, Compact neural network, Data quantization, Machine learning, Neural networks, Program processors, Task analysis, Tensor decomposition, data quantization, neural network acceleration, neural network compression, sparse neural network, tensor decomposition},
	pages = {485--532},
}

@inproceedings{singh_leveraging_2020,
	address = {Snowmass Village, CO, USA},
	title = {Leveraging {Filter} {Correlations} for {Deep} {Model} {Compression}},
	isbn = {978-1-72816-553-0},
	url = {https://ieeexplore.ieee.org/document/9093331/},
	doi = {10.1109/WACV45572.2020.9093331},
	abstract = {We present a ﬁlter correlation based model compression approach for deep convolutional neural networks. Our approach iteratively identiﬁes pairs of ﬁlters with the largest pairwise correlations and drops one of the ﬁlters from each such pair. However, instead of discarding one of the ﬁlters from each such pair na¨ıvely, the model is re-optimized to make the ﬁlters in these pairs maximally correlated, so that discarding one of the ﬁlters from the pair results in minimal information loss. Moreover, after discarding the ﬁlters in each round, we further ﬁnetune the model to recover from the potential small loss incurred by the compression. We evaluate our proposed approach using a comprehensive set of experiments and ablation studies. Our compression method yields state-of-the-art FLOPs compression rates on various benchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving excellent predictive performance for tasks such as object detection on benchmark datasets.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Singh, Pravendra and Verma, Vinay Kumar and Rai, Piyush and Namboodiri, Vinay P.},
	month = mar,
	year = {2020},
	pages = {824--833},
}

@inproceedings{hu_2020_2021,
	title = {The 2020 {Low}-{Power} {Computer} {Vision} {Challenge}},
	doi = {10.1109/AICAS51828.2021.9458522},
	abstract = {AI computer vision has advanced significantly in recent years. IoT and edge computing devices such as mobile phones have become the primary computing platform for many end users. Mobile devices such as robots and drones that rely on batteries demand for energy efficient computation. Since 2015, the IEEE Annual International Low-Power Computer Vision Challenge (LPCVC) was held to identify energy-efficient AI and computer vision solutions. The 2020 LPCVC includes three challenge tracks: (1) PyTorch UAV Video Track, (2) FPGA Image Track, and (3) On-device Visual Intelligence Competition (OVIC) Tenforflow Track. This paper summarizes the 2020 winning solutions from the three tracks of LPCVC competitions. Methods and future directions for energy-efficient AI and computer vision research are discussed.},
	booktitle = {2021 {IEEE} 3rd {International} {Conference} on {Artificial} {Intelligence} {Circuits} and {Systems} ({AICAS})},
	author = {Hu, Xiao and Chang, Ming-Ching and Chen, Yuwei and Sridhar, Rahul and Hu, Zhenyu and Xue, Yunhe and Wu, Zhenyu and Pi, Pengcheng and Shen, Jiayi and Tan, Jianchao and Lian, Xiangru and Liu, Ji and Wang, Zhangyang and Liu, Chia-Hsiang and Han, Yu-Shin and Sung, Yuan-Yao and Lee, Yi and Wu, Kai-Chiang and Guo, Wei-Xiang and Lee, Rick and Liang, Shengwen and Wang, Zerun and Ding, Guiguang and Zhang, Gang and Xi, Teng and Chen, Yubei and Cai, Han and Zhu, Ligeng and Zhang, Zhekai and Han, Song and Jeong, Seonghwan and Kwon, YoungMin and Wang, Tianzhe and Pan, Jeffery},
	month = jun,
	year = {2021},
	keywords = {Artificial intelligence, Computational efficiency, Computer vision, Conferences, Energy efficiency, FPGA, Low-power, Mobile handsets, NAS, Visualization, challenge, computer vision, drone, knowledge distilling, model compression, scene text},
	pages = {1--4},
}

@inproceedings{davis_testing_2019,
	address = {San Diego, CA, USA},
	title = {Testing {Regex} {Generalizability} {And} {Its} {Implications}: {A} {Large}-{Scale} {Many}-{Language} {Measurement} {Study}},
	isbn = {978-1-72812-508-4},
	shorttitle = {Testing {Regex} {Generalizability} {And} {Its} {Implications}},
	url = {https://ieeexplore.ieee.org/document/8952443/},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and ﬁndings to date are typically generalized from regexes written in only 1–2 programming languages. This is an incomplete foundation.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Davis, James C and Moyer, Daniel and Kazerouni, Ayaan M and Lee, Dongyoon},
	month = nov,
	year = {2019},
	pages = {427--439},
}

@article{cheng_survey_2020,
	title = {A {Survey} of {Model} {Compression} and {Acceleration} for {Deep} {Neural} {Networks}},
	abstract = {Deep neural networks (DNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past five years, tremendous progress has been made in this area. In this paper, we review the recent techniques for compacting and accelerating DNN models. In general, these techniques are divided into four categories: parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization are described first, after that the other techniques are introduced. For each category, we also provide insightful analysis about the performance, related applications, advantages, and drawbacks. Then we go through some very recent successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrices, the main datasets used for evaluating the model performance, and recent benchmark efforts. Finally, we conclude this paper, discuss remaining the challenges and possible directions for future work.},
	journal = {arXiv},
	author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wu_iou-aware_2020,
	title = {{IoU}-aware single-stage object detector for accurate localization},
	volume = {97},
	issn = {02628856},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885620300433},
	doi = {10.1016/j.imavis.2020.103911},
	abstract = {Single-stage object detectors have been widely applied in many computer vision applications due to their simpleness and high efﬁciency. However, the low correlation between the classiﬁcation score and localization accuracy in detection results severely hurts the average precision of the detection model. To solve this problem, an IoUaware single-stage object detector is proposed in this paper. Speciﬁcally, IoU-aware single-stage object detector predicts the IoU for each detected box. Then the predicted IoU is multiplied by the classiﬁcation score to compute the ﬁnal detection conﬁdence, which is more correlated with the localization accuracy. The detection conﬁdence is then used as the input of the subsequent NMS and COCO AP computation, which substantially improves the localization accuracy of model. Sufﬁcient experiments on COCO and PASCOL VOC datasets demonstrate the effectiveness of IoU-aware single-stage object detector on improving model's localization accuracy. Without whistles and bells, the proposed method can substantially improve AP by 1.7\%–1.9\% and AP75 by 2.2\%–2.5\% on COCO testdev. And it can also substantially improve AP by 2.9\%–4.4\% and AP80, AP90 by 4.6\%–10.2\% on PASCAL VOC. The source code will be made publicly available.},
	language = {en},
	urldate = {2022-02-18},
	journal = {Image and Vision Computing},
	author = {Wu, Shengkai and Li, Xiaoping and Wang, Xinggang},
	month = may,
	year = {2020},
	pages = {103911},
}

@article{long_pp-yolo_2020,
	title = {{PP}-{YOLO}: {An} {Effective} and {Efficient} {Implementation} of {Object} {Detector}},
	shorttitle = {{PP}-{YOLO}},
	url = {http://arxiv.org/abs/2007.12099},
	abstract = {Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacrifice accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efficiency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, we develop a new object detector based on YOLOv3. We mainly try to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, we call it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2\% mAP) and efficiency (72.9 FPS), surpassing the existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source code is at https://github.com/PaddlePaddle/PaddleDetection.},
	urldate = {2022-02-18},
	journal = {arXiv:2007.12099 [cs]},
	author = {Long, Xiang and Deng, Kaipeng and Wang, Guanzhong and Zhang, Yang and Dang, Qingqing and Gao, Yuan and Shen, Hui and Ren, Jianguo and Han, Shumin and Ding, Errui and Wen, Shilei},
	month = aug,
	year = {2020},
	note = {arXiv: 2007.12099
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{adelson_problem_1981,
	title = {Problem solving and the development of abstract categories in programming languages},
	volume = {9},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03197568},
	doi = {10.3758/BF03197568},
	language = {en},
	number = {4},
	urldate = {2022-02-16},
	journal = {Memory \& Cognition},
	author = {Adelson, Beth},
	month = jul,
	year = {1981},
	pages = {422--433},
}

@article{adelson_when_nodate,
	title = {When novices surpass experts: {The} difficulty of a task may increase with expertise.},
	volume = {10},
	issn = {1939-1285},
	shorttitle = {When novices surpass experts},
	url = {https://psycnet.apa.org/fulltext/1985-11314-001.pdf},
	doi = {10.1037/0278-7393.10.3.483},
	number = {3},
	urldate = {2022-02-16},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Adelson, Beth},
	note = {Publisher: US: American Psychological Association},
	pages = {483},
}

@inproceedings{louis_where_2020,
	address = {Seoul South Korea},
	title = {Where should {I} comment my code?: a dataset and model for predicting locations that need comments},
	isbn = {978-1-4503-7126-1},
	shorttitle = {Where should {I} comment my code?},
	url = {https://dl.acm.org/doi/10.1145/3377816.3381736},
	doi = {10.1145/3377816.3381736},
	abstract = {Programmers should write code comments, but not on every line of code. We have created a machine learning model that suggests locations where a programmer should write a code comment. We trained it on existing commented code to learn locations that are chosen by developers. Once trained, the model can predict locations in new code. Our models achieved precision of 74\% and recall of 13\% in identifying commentworthy locations. This first success opens the door to future work, both in the new where-to-comment problem and in guiding comment generation. Our code and data is available at http://groups.inf.ed.ac.uk/cup/comment-locator/.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results}},
	publisher = {ACM},
	author = {Louis, Annie and Dash, Santanu Kumar and Barr, Earl T. and Ernst, Michael D. and Sutton, Charles},
	month = jun,
	year = {2020},
	pages = {21--24},
}

@inproceedings{lin_feature_2017,
	address = {Honolulu, HI},
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099589/},
	doi = {10.1109/CVPR.2017.106},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows signiﬁcant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	language = {en},
	urldate = {2022-02-11},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = jul,
	year = {2017},
	pages = {936--944},
}

@article{noauthor_notitle_nodate,
}

@misc{MLOpsWorkflow,
	title = {Machine {Learning} {Operations}},
	url = {https://ml-ops.org/},
	abstract = {Machine Learning Operations},
	language = {en-us},
	urldate = {2021-12-08},
	year = {2021},
}

@inproceedings{Redmon2018YOLO,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
}

@inproceedings{Thung2012BugsinMLSystems,
	title = {An empirical study of bugs in machine learning systems},
	abstract = {Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6\% of the bugs belong to the algorithm/method category, 15.6\% of the bugs belong to the non-functional category, and 13\% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research. © 2012 IEEE.},
	booktitle = {International {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Thung, Ferdian and Wang, Shaowei and Lo, David and Jiang, Lingxiao},
	year = {2012},
}

@inproceedings{Tatman2018TaxonomyofReproducibility4MLResearch,
	title = {A {Practical} {Taxonomy} of {Reproducibility} for {Machine} {Learning} {Research}},
	booktitle = {Reproducibility in {Machine} {Learning} {Workshop} at {ICML}},
	author = {Tatman, Rachael and Vanderplas, Jake and Dane, Sohier},
	year = {2018},
}

@inproceedings{Nikanjam2021DesignSmellinDL,
	title = {Design {Smells} in {Deep} {Learning} {Programs}: {An} {Empirical} {Study}},
	shorttitle = {Design {Smells} in {Deep} {Learning} {Programs}},
	abstract = {Nowadays, we are witnessing an increasing adoption of Deep Learning (DL) based software systems in many industries. Designing a DL program requires constructing a deep neural network (DNN) and then training it on a dataset. This process requires that developers make multiple architectural (e.g., type, size, number, and order of layers) and configuration (e.g., optimizer, regularization methods, and activation functions) choices that affect the quality of the DL models, and consequently software quality. An under-specified or poorly-designed DL model may train successfully but is likely to perform poorly when deployed in production. Design smells in DL programs are poor design and-or configuration decisions taken during the development of DL components, that are likely to have a negative impact on the performance (i.e., prediction accuracy) and then quality of DL based software systems. In this paper, we present a catalogue of 8 design smells for a popular DL architecture, namely deep Feedforward Neural Networks which is widely employed in industrial applications. The design smells were identified through a review of the existing literature on DL design and a manual inspection of 659 DL programs with performance issues and design inefficiencies. The smells are specified by describing their context, consequences, and recommended refactorings. To provide empirical evidence on the relevance and perceived impact of the proposed design smells, we conducted a survey with 81 DL developers. In general, the developers perceived the proposed design smells as reflective of design or implementation problems, with agreement levels varying between 47{\textbackslash}\% and 68{\textbackslash}\%.},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Nikanjam, Amin and Khomh, Foutse},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{pineau_reproducible_2018,
	title = {Reproducible, {Reusable}, and {Robust} {Reinforcement} {Learning}},
	url = {https://media.neurips.cc/Conferences/NIPS2018/Slides/jpineau-NeurIPS-dec18-fb.pdf},
	journal = {Presentation in 31st Neural Information Processing Systems},
	author = {Pineau, Joelle},
	year = {2018},
}

@inproceedings{Kumar2017DataManagementinML,
	title = {Data {Management} in {Machine} {Learning}: {Challenges}, {Techniques}, and {Systems}},
	shorttitle = {Data {Management} in {Machine} {Learning}},
	abstract = {Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.},
	booktitle = {International {Conference} on {Management} of {Data}},
	author = {Kumar, Arun and Boehm, Matthias and Yang, Jun},
	year = {2017},
}

@inproceedings{VOCDataset,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	booktitle = {International {Journal} of {Computer} {Vision} ({IJCV})},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	year = {2010},
}

@inproceedings{Robert2007DifferentialTesting,
	title = {Differential {Testing} {A} new approach to change detection},
	abstract = {Regression testing, as it’s commonly practiced, is unsound due to inconsistent test repair and test addition. This paper presents a new technique, differential testing, that alleviates the test repair problem and detects more changes than regression testing alone. Differential testing works by creating test suites for both the original system and the modified system and contrasting both versions of the system with these two suites.},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Evans, Robert B and Savoia, Alberto},
	year = {2007},
}

@article{Braiek2020c,
	title = {On testing machine learning programs},
	abstract = {Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.},
	journal = {Journal of Systems and Software (JSS)},
	author = {Braiek, Houssem Ben and Khomh, Foutse},
	year = {2020},
	keywords = {Data cleaning, Feature engineering testing, Implementation testing, Machine learning, Model testing},
}

@misc{CMUMLReproducibility,
	title = {Reproducibility},
	url = {https://blog.ml.cmu.edu/2020/08/31/5-reproducibility/},
	urldate = {2021-12-08},
	author = {Ding, Zihao and Reddy, Aniketh and Joshi, Aparna},
	year = {2021},
}

@inproceedings{Boehm2010ChangingNatureofSWEvolution,
	title = {The changing nature of software evolution; {The} inevitability of evolution},
	abstract = {In "The Changing Nature of Software Evolution," Barry Boehm discusses how a one-size-fits-all approach no longer works and how different types of software now require different types of approaches. In "The Inevitability of Evolution," Kent Beck describes how developers must factor in cost, time, and risk when considering changes to their software.},
	booktitle = {{IEEE} {Software}},
	author = {Boehm, Barry and Beck, Kent},
	year = {2010},
	keywords = {Costs, Lead, Marine vehicles, Programming, development, software change, software engineering, software evolution},
}

@article{han_pre-trained_2021,
	title = {Pre-{Trained} {Models}: {Past}, {Present} and {Future}},
	issn = {26666510},
	shorttitle = {Pre-{Trained} {Models}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651021000231},
	doi = {10.1016/j.aiopen.2021.08.002},
	language = {en},
	urldate = {2022-02-10},
	journal = {AI Open},
	author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
	month = aug,
	year = {2021},
	pages = {S2666651021000231},
}

@inproceedings{you_logme_2021,
	title = {{LogME}: {Practical} {Assessment} of {Pre}-trained {Models} for {Transfer} {Learning}},
	shorttitle = {{LogME}},
	url = {https://proceedings.mlr.press/v139/you21b.html},
	abstract = {This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo {\textbackslash}emph\{without fine-tuning\}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is {\textbackslash}emph\{immune to over-fitting\}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is {\textbackslash}emph\{fast, accurate, and general\}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most \$3000{\textbackslash}times\$ speedup in wall-clock time and requires only \$1\%\$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: {\textbackslash}href\{https://github.com/thuml/LogME\}\{https://github.com/thuml/LogME\}.},
	language = {en},
	urldate = {2022-02-10},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Long, Mingsheng},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12133--12143},
}

@article{you_ranking_2021,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} of {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Pre-trained model hubs with many pre-trained models (PTMs) have been a cornerstone in deep learning. Although built at a high cost, they are in fact {\textbackslash}emph\{under-exploited\}: practitioners usually pick one PTM from the provided model hub by popularity, and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ve but common practice poses two obstacles to sufficiently exploiting pre-trained model hubs: (1) the PTM selection procedure has no optimality guarantee; (2) only one PTM is used while the rest PTMs are overlooked. Ideally, to maximally exploit pre-trained model hubs, trying all combinations of PTMs and extensively fine-tuning each combination of PTMs are required, which incurs exponential combinations and unaffordable computational budget. In this paper, we propose a new paradigm of exploiting model hubs by ranking and tuning pre-trained models: (1) Our conference work{\textasciitilde}{\textbackslash}citep\{you\_logme:\_2021\} proposed LogME to estimate the maximum value of label evidence given features extracted by pre-trained models, which can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) the best ranked PTM can be fine-tuned and deployed if we have no preference for the model's architecture, or the target PTM can be tuned by top-K ranked PTMs via the proposed B-Tuning algorithm. The ranking part is based on the conference paper, and we complete its theoretical analysis (convergence proof of the heuristic evidence maximization procedure, and the influence of feature dimension) in this paper. The tuning part introduces a novel Bayesian Tuning (B-Tuning) method for multiple PTMs tuning, which surpasses dedicated methods designed for homogeneous PTMs tuning and sets up new state of the art for heterogeneous PTMs tuning. We believe the new paradigm of exploiting PTM hubs can interest a large audience of the community.},
	urldate = {2022-02-10},
	journal = {arXiv:2110.10545 [cs]},
	author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.10545},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{Bahdanau2015,
	title = {Neural machine translation by jointly learning to align and translate},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
	year = {2015},
}

@inproceedings{Bosu2013DataQualityinESE,
	title = {Data quality in empirical software engineering: a targeted review},
	abstract = {Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering.},
	booktitle = {International {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	author = {Bosu, Michael Franklin and MacDonell, Stephen G},
	year = {2013},
}

@inproceedings{Destefanis2018AffectsofGithubIssuesCommenters,
	title = {On measuring affects of github issues' commenters},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Destefanis, Giuseppe and Ortu, Marco and Bowes, David and Marchesi, Michele and Tonelli, Roberto},
	year = {2018},
}

@inproceedings{Eghbali2020String-relatedBugs,
	title = {No strings attached: an empirical study of string-related software bugs},
	abstract = {Strings play many roles in programming because they often contain complex and semantically rich information. For example, programmers use strings to filter inputs via regular expression matching, to express the names of program elements accessed through some form of reflection, to embed code written in another formal language, and to assemble textual output produced by a program. The omnipresence of strings leads to a wide range of mistakes that developers may make, yet little is currently known about these mistakes. The lack of knowledge about string-related bugs leads to developers repeating the same mistakes again and again, and to poor support for finding and fixing such bugs. This paper presents the first empirical study of the root causes, consequences, and other properties of string-related bugs. We systematically study 204 string-related bugs in a diverse set of projects written in JavaScript, a language where strings play a particularly important role. Our findings include (i) that many string-related mistakes are caused by a recurring set of root cause patterns, such as incorrect string literals and regular expressions, (ii) that string-related bugs have a diverse set of consequences, including incorrect output or silent omission of expected behavior, (iii) that fixing string-related bugs often requires changing just a single line, with many of the required repair ingredients available in the surrounding code, (iv) that stringrelated bugs occur across all parts of applications, including the core components, and (v) that almost none of these bugs are detected by existing static analyzers. Our findings not only show the importance and prevalence of string-related bugs, but they help developers to avoid common mistakes and tool builders to tackle the challenge of finding and fixing string-related bugs.},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Eghbali, Aryaz and Pradel, Michael},
	year = {2020},
}

@inproceedings{Ehrlinger2020DaQLDataQualityinMLApp,
	title = {A {DaQL} to {Monitor} {Data} {Quality} in {Machine} {Learning} {Applications}},
	abstract = {Machine learning models can only be as good as the data used to train them. Despite this obvious correlation, there is little research about data quality measurement to ensure the reliability and trustworthiness of machine learning models. Especially in industrial settings, where sensors produce large amounts of highly volatile data, a one-time measurement of the data quality is not sufficient since errors in new data should be detected as early as possible. Thus, in this paper, we present DaQL (Data Quality Library), a generally-applicable tool to continuously monitor the quality of data to increase the prediction accuracy of machine learning models. We demonstrate and evaluate DaQL within an industrial real-world machine learning application at Siemens.},
	booktitle = {International {Conference} on {Database} and {Expert} {Systems} {Applications}},
	author = {Ehrlinger, Lisa and Haunschmid, Verena and Palazzini, Davide and Lettner, Christian},
	year = {2019},
	keywords = {Data quality, Machine learning, Trust},
}

@article{Gundersen2018onReproducibleAI,
	title = {On reproducible {AI}: {Towards} reproducible research, open science, and digital scholarship in {AI} publications},
	abstract = {Artificial intelligence, like any science, must rely on reproducible experiments to validate results. Our objective is to give practical and pragmatic recommendations for how to document AI research so that results are reproducible. Our analysis of the literature shows that AI publications currently fall short of providing enough documentation to facilitate reproducibility. Our suggested best practices are based on a framework for reproducibility and recommendations for best practices given by scientific organizations, scholars, and publishers. We have made a reproducibility checklist based on our investigation and described how every item in the checklist can be documented by authors and examined by reviewers. We encourage authors and reviewers to use the suggested best practices and author checklist when considering submissions for AAAI publications and conferences.},
	journal = {AI Magazine},
	author = {Gundersen, Odd Erik and Gil, Yolanda and Aha, David W.},
	year = {2018},
	keywords = {Gundersun2018},
}

@inproceedings{Jarzabek1993SWReengineering4Reusability,
	title = {Software reengineering for reusability},
	abstract = {Programs are often reengineered for better maintainability or in order to migrate programs into newer computer/software platforms. However, many of the aging business systems must be also upgraded in order to meet strategic goals of an organization. To meet such ambitious objectives, we must fundamentally redesign programs, rather than merely restructure them for improved maintainability. When much program re-design is involved, the reengineering option becomes challenging at the technical level, expensive and risky. To increase the value of the reengineering solution, we address reusability issues in the context of reengineering. In this paper, we discuss lifecycle phases and outline a possible technical scenario for reengineering for reusability.{\textless}{\textgreater}},
	booktitle = {International {Computer} {Software} and {Applications} {Conference} ({COMPSAC})},
	author = {Jarzabek, S.},
	year = {1993},
	keywords = {Aging, Companies, Computer interfaces, Computer science, Costs, Databases, Electronic mail, Information systems, Maintenance, Software reusability},
}

@article{MixedMethodsResearch,
	title = {Mixed {Methods} {Research}: {A} {Research} {Paradigm} {Whose} {Time} {Has} {Come}},
	abstract = {The purposes of this article are to position mixed methods research (mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm ?wars? and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research (mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it.},
	journal = {Educational Researcher},
	author = {Johnson, R. Burke and Onwuegbuzie, Anthony J.},
	year = {2004},
}

@article{jordan2015ML,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	shorttitle = {Machine learning},
	abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of ...},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	year = {2015},
}

@article{SEReengineering,
	title = {Software {Re}-engineering},
	abstract = {The essence of software re-engineering is to improve or transform existing software so that it can be understand, controlled, and used anew. The need for software re-engineering has increased greatly, as heritage software systems have become obsolescent in terms of their architecture, the platforms on which they run, and their suitability and stability to support evolution to support changing needs. Software re-engineering is important for recovering and reusing existing software assets, putting high software maintenance costs under control, and establishing a base for future software evolution. The growth in cost and importance of software to NASA, and the aging of many of the Agency's important software systems, has necessitated software reengineering efforts. This technical report is designed to give the reader an overview of the concepts, approaches and risks of re-engineering. It is intended to serve as a basis for understanding software reengineering technology. The information is primarily compiled from experts referenced at the end of the report, modified by approaches taken by NASA Projects. Section 8, however is new. It discusses Hybrid Re-engineering, an approach used by some NASA projects to combine the use of Commercial-Off-The-Shelf (COTS) software with new software development. The technical reports concludes with some industry lessons learned. 1.},
	journal = {Software Assurance Technology Center},
	author = {Linda, Dr and Rosenberg, H. and Hyatt, Lawrence E.},
	year = {1996},
}

@article{Nguyen2019MLDLFrameworksLibraries4LargeScaleDataMining,
	title = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining: a survey},
	abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
	journal = {Artificial Intelligence Review},
	author = {Nguyen, Giang and Dlugolinsky, Stefan and Bobák, Martin and Tran, Viet and García, Álvaro López and Heredia, Ignacio and Malík, Peter and Hluchý, Ladislav},
	year = {2019},
	keywords = {Artificial Intelligence software, Deep Learning, Graphics processing unit (GPU), Intensive computing, Large-scale data mining, Machine Learning, Parallel processing},
}

@inproceedings{Pei2017DeepXplore,
	title = {{DeepXplore}: {Automated} {Whitebox} {Testing} of {Deep} {Learning} {Systems}},
	shorttitle = {{DeepXplore}},
	abstract = {Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system’s behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.},
	booktitle = {Symposium on {Operating} {Systems} {Principles} ({SOSP})},
	author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
	month = oct,
	year = {2017},
}

@article{SIGSOFT2020EmpiricalStandards4SEResearch,
	title = {Empirical {Standards} for {Software} {Engineering} {Research}},
	abstract = {Empirical Standards are natural-language models of a scientific community's expectations for a specific kind of study (e.g. a questionnaire survey). The ACM SIGSOFT Paper and Peer Review Quality Initiative generated empirical standards for research methods commonly used in software engineering. These living documents, which should be continuously revised to reflect evolving consensus around research best practices, will improve research quality and make peer review more effective, reliable, transparent and fair.},
	journal = {arXiv},
	author = {Ralph, Paul and Ali, Nauman bin and Baltes, Sebastian and Bianculli, Domenico and Diaz, Jessica and Dittrich, Yvonne and Ernst, Neil and Felderer, Michael and Feldt, Robert and Filieri, Antonio and de França, Breno Bernard Nicolau and Furia, Carlo Alberto and Gay, Greg and Gold, Nicolas and Graziotin, Daniel and He, Pinjia and Hoda, Rashina and Juristo, Natalia and Kitchenham, Barbara and Lenarduzzi, Valentina and Martínez, Jorge and Melegati, Jorge and Mendez, Daniel and Menzies, Tim and Molleri, Jefferson and Pfahl, Dietmar and Robbes, Romain and Russo, Daniel and Saarimäki, Nyyti and Sarro, Federica and Taibi, Davide and Siegmund, Janet and Spinellis, Diomidis and Staron, Miroslaw and Stol, Klaas and Storey, Margaret-Anne and Taibi, Davide and Tamburri, Damian and Torchiano, Marco and Treude, Christoph and Turhan, Burak and Wang, Xiaofeng and Vegas, Sira},
	year = {2021},
	keywords = {Computer Science - General Literature, Computer Science - Software Engineering},
}

@article{Zou2019ObjectDetectionSurvey,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	urldate = {2022-01-18},
	journal = {arXiv},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{Alahmari2020RepeatabilityofDLModels,
	title = {Challenges for the {Repeatability} of {Deep} {Learning} {Models}},
	abstract = {Deep learning training typically starts with a random sampling initialization approach to set the weights of trainable layers. Therefore, different and/or uncontrolled weight initialization prevents learning the same model multiple times. Consequently, such models yield different results during testing. However, even with the exact same initialization for the weights, a lack of repeatability, replicability, and reproducibility may still be observed during deep learning for many reasons such as software versions, implementation variations, and hardware differences. In this article, we study repeatability when training deep learning models for segmentation and classification tasks using U-Net and LeNet-5 architectures in two development environments Pytorch and Keras (with TensorFlow backend). We show that even with the available control of randomization in Keras and TensorFlow, there are uncontrolled randomizations. We also show repeatable results for the same deep learning architectures using the Pytorch deep learning library. Finally, we discuss variations in the implementation of the weight initialization algorithm across deep learning libraries as a source of uncontrolled error in deep learning results.},
	journal = {IEEE Access},
	author = {Alahmari, Saeed S. and Goldgof, Dmitry B. and Mouton, Peter R. and Hall, Lawrence O.},
	year = {2020},
	keywords = {Computational modeling, Computer architecture, Deep learning, Keras, Libraries, Microprocessors, Pytorch, Software, TensorFlow, Training, determinism, deterministic models, repeatability, replicability, replicable deep learning models, reproducibility, reproducible, torch},
}

@inproceedings{Tian2018DeepTest,
	title = {{DeepTest}: automated testing of deep-neural-network-driven autonomous cars},
	shorttitle = {{DeepTest}},
	abstract = {Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.},
	urldate = {2022-01-01},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
	year = {2018},
	keywords = {DeepTest},
}

@article{Unceta2020EnvironmentalAdaptationDifferentialReplicationinML,
	title = {Environmental adaptation and differential replication in machine learning},
	abstract = {When deployed in the wild, machine learning models are usually confronted with an environment that imposes severe constraints. As this environment evolves, so do these constraints. As a result, the feasible set of solutions for the considered need is prone to change in time. We refer to this problem as that of environmental adaptation. In this paper, we formalize environmental adaptation and discuss how it differs from other problems in the literature. We propose solutions based on differential replication, a technique where the knowledge acquired by the deployed models is reused in specific ways to train more suitable future generations. We discuss different mechanisms to implement differential replications in practice, depending on the considered level of knowledge. Finally, we present seven examples where the problem of environmental adaptation can be solved through differential replication in real-life applications.},
	journal = {Entropy},
	author = {Unceta, Irene and Nin, Jordi and Pujol, Oriol},
	year = {2020},
	keywords = {Copying, Differential replication, Editing, Knowledge distillation, Machine learning, Natural selection},
}

@misc{DeterminedAIReproducibility,
	title = {Reproducibility in {ML}: why it matters and how to achieve it},
	shorttitle = {Reproducibility in {ML}},
	url = {https://determined.ai/blog/reproducibility-in-ml},
	abstract = {Reproducible machine learning is hard, particularly when training deep learning models. We review common sources of DL non-determinism and how to address them.},
	language = {en},
	urldate = {2021-12-08},
	journal = {Determined AI},
	author = {Villa, Jennifer and Zimmerman, Yoav},
	year = {2018},
}

@article{Wang2020RegexBugs,
	title = {An {Empirical} {Study} on {Regular} {Expression} {Bugs}},
	abstract = {Understanding the nature of regular expression (regex) issues is important to tackle practical issues developers face in regular expression usage. Knowledge about the nature and frequency of various types of regular expression issues, such as those related to performance, API misuse, and code smells, can guide testing, inform documentation writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression Denial of Service), little is known about to what extent regular expression issues affect software development and how these issues are addressed in practice. This paper presents a comprehensive empirical study of 350 merged regex-related pull requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying the root causes and manifestations of those bugs, we show that incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3\%). The remaining root causes are incorrect API usage (9.3\%) and other code issues that require regular expression changes in the fix (29.5\%). By studying the code changes of regex-related pull requests, we observe that fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests. The results of this study contribute to a broader understanding of the practical problems faced by developers when using regular expressions.},
	journal = {International Conference on Mining Software Repositories (MSR)},
	author = {Wang, Peipei and Brown, Chris and Jennings, Jamie A. and Stolee, Kathryn T.},
	year = {2020},
	keywords = {Regular expression bug characteristics, bug fixes, pull requests},
}

@article{zhang_software_2019,
	title = {Software engineering practice in the development of deep learning applications},
	url = {https://arxiv.org/abs/1910.03156},
	abstract = {Deep-Learning(DL) applications have been widely employed to assist in various tasks. They are constructed based on a data-driven programming paradigm that is different from conventional software applications. Given the increasing popularity and importance of DL applications, software engineering practitioners have some techniques specifically for them. However, little research is conducted to identify the challenges and lacks in practice. To fill this gap, in this paper, we surveyed 195 practitioners to understand their insight and experience in the software engineering practice of DL applications. Specifically, we asked the respondents to identify lacks and challenges in the practice of the development life cycle of DL applications. The results present 13 findings that provide us with a better understanding of software engineering practice of DL applications. Further, we distil these findings into 7 actionable recommendations for software engineering researchers and practitioners to improve the development of DL applications.},
	journal = {arXiv},
	author = {Zhang, Xufan and Yang, Yilin and Feng, Yang and Chen, Zhenyu},
	year = {2019},
	keywords = {Deep learning applications, Practitioner perception, Software engineering in practice},
}

@misc{FacebookAIReproducibility,
	title = {How the {AI} community can get serious about reproducibility},
	url = {https://ai.facebook.com/blog/how-the-ai-community-can-get-serious-about-reproducibility/},
	abstract = {Facebook AI Managing Director Joelle Pineau discusses the importance of reproducibility in AI research, and shares early results from the first reproducibility challenge at NeurIPS 2019.},
	language = {zh-Hans},
	urldate = {2021-12-08},
	author = {Pineau, Joelle},
	year = {2022},
}

@misc{Rathod2020TF2meetsODAPI,
	title = {{TensorFlow} 2 meets the {Object} {Detection} {API}},
	url = {https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html},
	abstract = {Object detection in TensorFlow 2, with SSD, MobileNet, RetinaNet, Faster R-CNN, Mask R-CNN, CenterNet, EfficientNet, and more.},
	author = {Rathod, Vivek and Huang, Jonathan},
	year = {2021},
}

@misc{Jaeyoun2020Intro2ModelGarden,
	title = {Introducing the {Model} {Garden} for {TensorFlow} 2},
	url = {https://blog.tensorflow.org/2020/03/introducing-model-garden-for-tensorflow-2.html},
	abstract = {The TensorFlow blog contains regular news from the TensorFlow team and the community, with articles on Python, TensorFlow.js, TF Lite, TFX, and more.},
	language = {en},
	author = {Kim, Jaeyoun and Li, Jing},
	year = {2021},
}

@misc{noauthor_introducing_nodate,
	title = {Introducing the {Model} {Garden} for {TensorFlow} 2},
	url = {https://blog.tensorflow.org/2020/03/introducing-model-garden-for-tensorflow-2.html},
	abstract = {The TensorFlow blog contains regular news from the TensorFlow team and the community, with articles on Python, TensorFlow.js, TF Lite, TFX, and more.},
	language = {en},
	urldate = {2022-02-04},
}

@article{choromanski_rethinking_2021,
	title = {Rethinking {Attention} with {Performers}},
	url = {http://arxiv.org/abs/2009.14794},
	abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
	urldate = {2022-02-02},
	journal = {arXiv:2009.14794 [cs, stat]},
	author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.14794},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{PytorchWeb,
	title = {{PyTorch}},
	url = {https://pytorch.org/},
	urldate = {2021-11-17},
	year = {2021},
}

@misc{TensorFlowWeb,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	urldate = {2021-11-17},
	year = {2021},
}

@misc{MLReproducibilityChallengeSpring2021,
	title = {Papers with {Code} - {ML} {Reproducibility} {Challenge} 2021 {Edition}},
	url = {https://paperswithcode.com/rc2021},
	abstract = {The ML Reproducibility Challenge 2021 covering paper published in seven major ML conferences: NeurIPS, ACL, EMNLP, ICLR, ICML, CVPR and ECCV.},
	language = {en},
	urldate = {2021-12-08},
	year = {2020},
}

@misc{ONNX,
	title = {{ONNX} {\textbar} {Home}},
	url = {https://onnx.ai/},
	urldate = {2021-12-10},
	year = {2019},
}

@misc{GithubLabels,
	title = {Managing labels},
	url = {https://docs.github.com/en/issues/using-labels-and-milestones-to-track-work/managing-labels},
	abstract = {You can classify issues, pull requests, and discussions by creating, editing, applying, and deleting labels.},
	language = {en},
	urldate = {2021-12-05},
	journal = {GitHub Docs},
	year = {2020},
}

@article{valett1989SWMeasurementExperiencesinSELab,
	title = {A {Summary} of {Software} {Measurement} {Experiences} in the {Software} {Engineering} {Laboratory}},
	volume = {9},
	language = {en},
	journal = {Journal of Systems and Software},
	author = {Valett, Jon D and McGarry, Frank E},
	year = {1989},
	pages = {137 -- 148},
}

@article{rupprecht_improving_2020,
	title = {Improving reproducibility of data science pipelines through transparent provenance capture},
	volume = {13},
	issn = {2150-8097},
	doi = {10.14778/3415478.3415556},
	abstract = {Data science has become prevalent in a large variety of domains. Inherent in its practice is an exploratory, probing, and fact finding journey, which consists of the assembly, adaptation, and execution of complex data science pipelines. The trustworthiness of the results of such pipelines rests entirely on their ability to be reproduced with fidelity, which is difficult if pipelines are not documented or recorded minutely and consistently. This difficulty has led to a reproducibility crisis and presents a major obstacle to the safe adoption of the pipeline results in production environments. The crisis can be resolved if the provenance for each data science pipeline is captured transparently as pipelines are executed. However, due to the complexity of modern data science pipelines, transparently capturing sufficient provenance to allow for reproducibility is challenging. As a result, most existing systems require users to augment their code or use specific tools to capture provenance, which hinders productivity and results in a lack of adoption.In this paper, we present Ursprung,1 a transparent provenance collection system designed for data science environments.2 The Ursprung philosophy is to capture provenance and build lineage by integrating with the execution environment to automatically track static and runtime configuration parameters of data science pipelines. Rather than requiring data scientists to make changes to their code, Ursprung records basic provenance information from system-level sources and combines it with provenance from application-level sources (e.g., log files, stdout), which can be accessed and recorded through a domain-specific language. In our evaluation, we show that Ursprung is able to capture sufficient provenance for a variety of use cases and only adds an overhead of up to 4\%.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Rupprecht, Lukas and Davis, James C. and Arnold, Constantine and Gur, Yaniv and Bhagwat, Deepavali},
	year = {2020},
}

@article{arnab_vivit_2021,
	title = {{ViViT}: {A} {Video} {Vision} {Transformer}},
	shorttitle = {{ViViT}},
	url = {http://arxiv.org/abs/2103.15691},
	abstract = {We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit},
	urldate = {2022-02-01},
	journal = {arXiv:2103.15691 [cs]},
	author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
	month = nov,
	year = {2021},
	note = {arXiv: 2103.15691},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{yan_contnet_2021,
	title = {{ConTNet}: {Why} not use convolution and transformer at the same time?},
	shorttitle = {{ConTNet}},
	url = {http://arxiv.org/abs/2104.13497},
	abstract = {Although convolutional networks (ConvNets) have enjoyed great success in computer vision (CV), it suffers from capturing global information crucial to dense prediction tasks such as object detection and segmentation. In this work, we innovatively propose ConTNet (ConvolutionTransformer Network), combining transformer with ConvNet architectures to provide large receptive fields. Unlike the recently-proposed transformer-based models (e.g., ViT, DeiT) that are sensitive to hyper-parameters and extremely dependent on a pile of data augmentations when trained from scratch on a midsize dataset (e.g., ImageNet1k), ConTNet can be optimized like normal ConvNets (e.g., ResNet) and preserve an outstanding robustness. It is also worth pointing that, given identical strong data augmentations, the performance improvement of ConTNet is more remarkable than that of ResNet. We present its superiority and effectiveness on image classification and downstream tasks. For example, our ConTNet achieves 81.8\% top-1 accuracy on ImageNet which is the same as DeiT-B with less than 40\% computational complexity. ConTNet-M also outperforms ResNet50 as the backbone of both Faster-RCNN (by 2.6\%) and Mask-RCNN (by 3.2\%) on COCO2017 dataset. We hope that ConTNet could serve as a useful backbone for CV tasks and bring new ideas for model design},
	urldate = {2022-02-01},
	journal = {arXiv:2104.13497 [cs]},
	author = {Yan, Haotian and Li, Zhe and Li, Weijian and Wang, Changhu and Wu, Ming and Zhang, Chuang},
	month = may,
	year = {2021},
	note = {arXiv: 2104.13497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wang_benchmarking_2019,
	title = {Benchmarking {TPU}, {GPU}, and {CPU} {Platforms} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1907.10701},
	abstract = {Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware specialization to improve performance. To systematically benchmark deep learning platforms, we introduce ParaDnn, a parameterized benchmark suite for deep learning that generates end-to-end models for fully connected (FC), convolutional (CNN), and recurrent (RNN) neural networks. Along with six real-world models, we benchmark Google's Cloud TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep dive into TPU architecture, reveal its bottlenecks, and highlight valuable lessons learned for future specialized system design. We also provide a thorough comparison of the platforms and find that each has unique strengths for some types of models. Finally, we quantify the rapid performance improvements that specialized software stacks provide for the TPU and GPU platforms.},
	urldate = {2022-01-22},
	journal = {arXiv:1907.10701 [cs, stat]},
	author = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
	month = oct,
	year = {2019},
	note = {arXiv: 1907.10701},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance, Statistics - Machine Learning},
}

@article{ribeiro_beyond_2020,
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} models with {CheckList}},
	shorttitle = {Beyond {Accuracy}},
	url = {http://arxiv.org/abs/2005.04118},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	urldate = {2022-01-19},
	journal = {arXiv:2005.04118 [cs]},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	month = may,
	year = {2020},
	note = {arXiv: 2005.04118},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{baker_detect_nodate,
	title = {Detect, {Fix}, and {Verify} {TensorFlow} {API} {Misuses}},
	abstract = {The growing application of DL makes detecting and ﬁxing defective DL programs of paramount importance. Recent studies on DL defects report that TensorFlow API misuses represent a common class of DL defects. However to effectively detect, ﬁx, and verify them remains an understudied problem. This paper presents the TensorFlow API misuses Detector And Fixer (TADAF) technique, which relies on 11 common API misuses patterns and corresponding ﬁxes that we extracted from StackOverﬂow. TADAF statically analyses a TensorFlow program for identifying matches of any of the 11 patterns. If it ﬁnds a match, it automatically generates a ﬁxed version of the program. To verify that the misuse brings a tangible negative effect, TADAF reports functional, accuracy, or efﬁciency differences when training and testing (with the same data) the original and ﬁxed versions of the program. Our preliminary evaluation on ﬁve GitHub projects shows that TADAF detected and ﬁxed all the API misuses.},
	language = {en},
	author = {Baker, Wilson and O’Connor, Michael and Shahamiri, Seyed Reza and Terragni, Valerio},
	pages = {5},
}

@article{dilhara_discovering_2022,
	title = {Discovering {Repetitive} {Code} {Changes} in {Python} {ML} {Systems}},
	abstract = {Over the years, researchers capitalized on the repetitiveness of software changes to automate many software evolution tasks. Despite the extraordinary rise in popularity of Python-based ML systems, they do not benefit from these advances. Without knowing what are the repetitive changes that ML developers make, researchers, tool, and library designers miss opportunities for automation, and ML developers fail to learn and use common practices.},
	language = {en},
	author = {Dilhara, Malinda and Sannidhi, Nikhith and Ketkar, Ameya},
	year = {2022},
	pages = {13},
}

@article{del_barrio_review_2020,
	title = {Review of {Mathematical} frameworks for {Fairness} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/2005.13755},
	abstract = {A review of the main fairness definitions and fair learning methodologies proposed in the literature over the last years is presented from a mathematical point of view. Following our independence-based approach, we consider how to build fair algorithms and the consequences on the degradation of their performance compared to the possibly unfair case. This corresponds to the price for fairness given by the criteria \${\textbackslash}textit\{statistical parity\}\$ or \${\textbackslash}textit\{equality of odds\}\$. Novel results giving the expressions of the optimal fair classifier and the optimal fair predictor (under a linear regression gaussian model) in the sense of \${\textbackslash}textit\{equality of odds\}\$ are presented.},
	urldate = {2022-01-12},
	journal = {arXiv:2005.13755 [cs, stat]},
	author = {del Barrio, Eustasio and Gordaliza, Paula and Loubes, Jean-Michel},
	month = may,
	year = {2020},
	note = {arXiv: 2005.13755},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Atkinson2008AccidentalComplexity,
	title = {Reducing accidental complexity in domain models},
	volume = {7},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-007-0061-0},
	doi = {10.1007/s10270-007-0061-0},
	abstract = {A fundamental principle in engineering, including software engineering, is to minimize the amount of accidental complexity which is introduced into engineering solutions due to mismatches between a problem and the technology used to represent the problem. As model-driven development moves to the center stage of software engineering, it is particularly important that this principle be applied to the technologies used to create and manipulate models, especially models that are intended to be free of solution decisions. At present, however, there is a signiﬁcant mismatch between the “two level” modeling paradigm used to construct mainstream domain models and the conceptual information such models are required to represent—a mismatch that makes such models more complex than they need be. In this paper, we identify the precise nature of the mismatch, discuss a number of more or less satisfactory workarounds, and show how it can be avoided.},
	language = {en},
	number = {3},
	urldate = {2022-01-07},
	journal = {Software \& Systems Modeling},
	author = {Atkinson, Colin and Kühne, Thomas},
	month = jul,
	year = {2008},
	pages = {345--359},
}

@article{biran_explanation_nodate,
	title = {Explanation and {Justification} in {Machine} {Learning}: {A} {Survey}},
	abstract = {We present a survey of the research concerning explanation and justiﬁcation in the Machine Learning literature and several adjacent ﬁelds. Within Machine Learning, we differentiate between two main branches of current research: interpretable models, and prediction interpretation and justiﬁcation.},
	language = {en},
	author = {Biran, Or and Cotton, Courtenay},
	pages = {6},
}

@inproceedings{Bengtsson1998ScenarioBasedSWArchitectureReengineering,
	title = {Scenario-based software architecture reengineering},
	doi = {10.1109/ICSR.1998.685756},
	abstract = {The paper presents a method for reengineering software architectures. The method explicitly addresses the quality attributes of the software architecture. Assessment of quality attributes is performed primarily using scenarios. Design transformations are done to improve quality attributes that do not satisfy the requirements. Assessment and design transformation can be performed for several iterations until all requirements are met. To illustrate the method we use the reengineering of a prototypical measurement system into a domain-specific software architecture as an example.},
	booktitle = {Proceedings. {Fifth} {International} {Conference} on {Software} {Reuse} ({Cat}. {No}.{98TB100203})},
	author = {Bengtsson, P. and Bosch, J.},
	month = jun,
	year = {1998},
	note = {ISSN: 1085-9098},
	keywords = {Application software, Computer architecture, Design methodology, Electrical capacitance tomography, Prototypes, Software architecture, Software maintenance, Software prototyping, Software quality, Software systems},
	pages = {308--317},
}

@article{gudivada_data_2017,
	title = {Data {Quality} {Considerations} for {Big} {Data} and {Machine} {Learning}: {Going} {Beyond} {Data} {Cleaning} and {Transformations}},
	volume = {10},
	shorttitle = {Data {Quality} {Considerations} for {Big} {Data} and {Machine} {Learning}},
	abstract = {Data quality issues trace back their origin to the early days of computing. A wide range of domain-specific techniques to assess and improve the quality of data exist in the literature. These solutions primarily target data which resides in relational databases and data warehouses. The recent emergence of big data analytics and renaissance in machine learning necessitates evaluating the suitability relational database-centric approaches to data quality. In this paper, we describe the nature of the data quality issues in the context of big data and machine learning. We discuss facets of data quality, present a data governance-driven framework for data quality lifecycle for this new scenario, and describe an approach to its implementation. A sampling of the tools available for data quality management is indicated and future trends are discussed.},
	journal = {International Journal on Advances in Software},
	author = {Gudivada, Venkat and Apon, Amy and Ding, Junhua},
	year = {2017},
	pages = {1--20},
}

@article{roscher_explainable_2020,
	title = {Explainable {Machine} {Learning} for {Scientific} {Insights} and {Discoveries}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2976199},
	abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.},
	journal = {IEEE Access},
	author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Approximation algorithms, Biological system modeling, Data mining, Data models, Explainable machine learning, Kernel, Machine learning, Mathematical model, informed machine learning, interpretability, scientific consistency, transparency},
	pages = {42200--42216},
}

@inproceedings{odena_tensorfuzz_2019,
	title = {{TensorFuzz}: {Debugging} {Neural} {Networks} with {Coverage}-{Guided} {Fuzzing}},
	shorttitle = {{TensorFuzz}},
	url = {https://proceedings.mlr.press/v97/odena19a.html},
	abstract = {Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms can provide this coverage metric for neural networks. We then combine these methods with techniques for property-based testing (PBT). In PBT, one asserts properties that a function should satisfy and the system automatically generates tests exercising those properties. We then apply this system to practical goals including (but not limited to) surfacing broken loss functions in popular GitHub repositories and making performance improvements to TensorFlow. Finally, we release an open source library called TensorFuzz that implements the described techniques.},
	language = {en},
	urldate = {2022-01-01},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Odena, Augustus and Olsson, Catherine and Andersen, David and Goodfellow, Ian},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4901--4911},
}

@misc{ONNXBlog,
	title = {Portability between deep learning frameworks – with {ONNX}},
	url = {https://blog.codecentric.de/en/2019/08/portability-deep-learning-frameworks-onnx/},
	abstract = {The need for model portability of deep learning frameworks is greater than ever. This posts explains how ONNX can help here.},
	language = {en-US},
	urldate = {2021-12-31},
	journal = {codecentric AG Blog},
	month = aug,
	year = {2019},
}

@inproceedings{patel_gestalt_2010,
	address = {New York New York USA},
	title = {Gestalt: integrated support for implementation and analysis in machine learning},
	isbn = {978-1-4503-0271-5},
	shorttitle = {Gestalt},
	url = {https://dl.acm.org/doi/10.1145/1866029.1866038},
	doi = {10.1145/1866029.1866038},
	abstract = {We present Gestalt, a development environment designed to support the process of applying machine learning. While traditional programming environments focus on source code, we explicitly support both code and data. Gestalt allows developers to implement a classification pipeline, analyze data as it moves through that pipeline, and easily transition between implementation and analysis. An experiment shows this significantly improves the ability of developers to find and fix bugs in machine learning systems. Our discussion of Gestalt and our experimental observations provide new insight into general-purpose support for the machine learning process.},
	language = {en},
	urldate = {2021-12-31},
	booktitle = {Proceedings of the 23nd annual {ACM} symposium on {User} interface software and technology},
	publisher = {ACM},
	author = {Patel, Kayur and Bancroft, Naomi and Drucker, Steven M. and Fogarty, James and Ko, Amy J. and Landay, James},
	month = oct,
	year = {2010},
	pages = {37--46},
}

@inproceedings{Jebnoun2020DLCodeScent,
	address = {Seoul Republic of Korea},
	title = {The {Scent} of {Deep} {Learning} {Code}: {An} {Empirical} {Study}},
	isbn = {978-1-4503-7517-7},
	shorttitle = {The {Scent} of {Deep} {Learning} {Code}},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387479},
	doi = {10.1145/3379597.3387479},
	abstract = {Deep learning practitioners are often interested in improving their model accuracy rather than the interpretability of their models. As a result, deep learning applications are inherently complex in their structures. They also need to continuously evolve in terms of code changes and model updates. Given these confounding factors, there is a great chance of violating the recommended programming practices by the developers in their deep learning applications. In particular, the code quality might be negatively affected due to their drive for the higher model performance. Unfortunately, the code quality of deep learning applications has rarely been studied to date. In this paper, we conduct an empirical study to investigate the distribution of code smells in deep learning applications. To this end, we perform a comparative analysis between deep learning and traditional open-source applications collected from GitHub. We have several major findings. First, long lambda expression, long ternary conditional expression, and complex container comprehension smells are frequently found in deep learning projects. That is, deep learning code involves more complex or longer expressions than the traditional code does. Second, the number of code smells increases across the releases of deep learning applications. Third, we found that there is a co-existence between code smells and software bugs in the studied deep learning code, which confirms our conjecture on the degraded code quality of deep learning applications.},
	language = {en},
	urldate = {2021-12-30},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Jebnoun, Hadhemi and Ben Braiek, Houssem and Rahman, Mohammad Masudur and Khomh, Foutse},
	month = jun,
	year = {2020},
	pages = {420--430},
}

@inproceedings{singla_analysis_2018,
	title = {Analysis of {Software} {Engineering} for {Agile} {Machine} {Learning} {Projects}},
	doi = {10.1109/INDICON45594.2018.8987154},
	abstract = {The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects.},
	booktitle = {2018 15th {IEEE} {India} {Council} {International} {Conference} ({INDICON})},
	author = {Singla, Kushal and Bose, Joy and Naik, Chetan},
	month = dec,
	year = {2018},
	note = {ISSN: 2325-9418},
	keywords = {Data science, Machine learning, Maximum likelihood estimation, Software, Software engineering, Tag clouds, Task analysis, agile methodology, machine learning project, scrum, software engineering},
	pages = {1--5},
}

@inproceedings{ComputationalCostofDL,
	title = {Predicting the {Computational} {Cost} of {Deep} {Learning} {Models}},
	doi = {10.1109/BigData.2018.8622396},
	abstract = {Deep learning is rapidly becoming a go-to tool for many artificial intelligence problems due to its ability to outperform other approaches and even humans at many problems. Despite its popularity we are still unable to accurately predict the time it will take to train a deep learning network to solve a given problem. This training time can be seen as the product of the training time per epoch and the number of epochs which need to be performed to reach the desired level of accuracy. Some work has been carried out to predict the training time for an epoch - most have been based around the assumption that the training time is linearly related to the number of floating point operations required. However, this relationship is not true and becomes exacerbated in cases where other activities start to dominate the execution time. Such as the time to load data from memory or loss of performance due to non-optimal parallel execution. In this work we propose an alternative approach in which we train a deep learning network to predict the execution time for parts of a deep learning network. Timings for these individual parts can then be combined to provide a prediction for the whole execution time. This has advantages over linear approaches as it can model more complex scenarios. But, also, it has the ability to predict execution times for scenarios unseen in the training data. Therefore, our approach can be used not only to infer the execution time for a batch, or entire epoch, but it can also support making a well-informed choice for the appropriate hardware and model.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Justus, Daniel and Brennan, John and Bonner, Stephen and McGough, Andrew Stephen},
	month = dec,
	year = {2018},
	keywords = {Benchmark, Computational modeling, Deep learning, Hardware, Machine Learning, Neural networks, Performance, Prediction, Predictive models, Timing, Training},
	pages = {3873--3882},
}

@article{shridhar_interoperating_2019,
	title = {Interoperating {Deep} {Learning} models with {ONNX}.jl},
	abstract = {Flux [17] is a machine learning framework, written using the numerical computing language Julia[4]. The framework makes writing layers as simple as writing mathematical formulae, and it’s advanced AD, Zygote [11] , applies automatic differentiation (AD) to calculate derivatives and train the model. It makes heavy use of Julia’s language and compiler features to carry out code analysis and make optimisations. For example, Julia’s GPU compilation support [3] can be used to JIT-compile custom GPU kernels for model layers [19]. Flux also supports a number of a hardware options, from CPUs, GPUs and even TPUs via XLA.jl, that compiles Julia code to XLA: an advanced compiler for linear algebra that is capable of greatly optimizing speed and memory usage in large deep learning models.},
	language = {en},
	author = {Shridhar, Ayush and Tomson, Phil and Innes, Mike},
	year = {2019},
	pages = {6},
}

@article{Washizaki2019SEPatterns4LSystems,
	title = {Studying {Software} {Engineering} {Patterns} for {Designing} {Machine} {Learning} {Systems}},
	doi = {10.1109/IWESEP49350.2019.00017},
	abstract = {Machine-learning (ML) techniques are becoming more prevalent. ML techniques rely on mathematics and software engineering. Researchers and practitioners studying best practices strive to design ML systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. However, a systematic study to collect, classify, and discuss these software-engineering (SE) design patterns for ML techniques have yet to be reported. Our research collects good/bad SE design patterns for ML techniques to provide developers with a comprehensive classification of such patterns. Herein we report the preliminary results of a systematic-literature review (SLR) of good/bad design patterns for ML.},
	journal = {Proceedings - 2019 10th International Workshop on Empirical Software Engineering in Practice, IWESEP 2019},
	author = {Washizaki, Hironori and Uchida, Hiromu and Khomh, Foutse and Guéhéneuc, Yann Gaël},
	year = {2019},
	note = {ISBN: 9781728155906
Publisher: IEEE},
	keywords = {Anti-patterns, Architecture Patterns, Design Patterns, ML Patterns, Machine Learning},
	pages = {49--54},
}

@article{Wang2020DLSE,
	title = {Synergy between {Machine}/{Deep} {Learning} and {Software} {Engineering}: {How} {Far} {Are} {We}?},
	shorttitle = {Synergy between {Machine}/{Deep} {Learning} and {Software} {Engineering}},
	url = {http://arxiv.org/abs/2008.05515},
	abstract = {Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Machine Learning (ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the quality (especially the applicability and generalizability) of ML/DLrelated SE studies, and to stimulate and enhance future collaborations between SE/AI researchers and industry practitioners, we conducted a 10-year Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that ML/DL and SE have had on each other. At the same time, however, we also observed a paucity of replicable and reproducible ML/DL-related SE studies and identiﬁed ﬁve factors that inﬂuence their replicability and reproducibility. To improve the applicability and generalizability of research results, we analyzed what ingredients in a study would facilitate an understanding of why a ML/DL technique was selected for a speciﬁc SE problem. In addition, we identiﬁed the unique trends of impacts of DL models on SE tasks, as well as ﬁve unique challenges that needed to be met in order to better leverage DL to improve the productivity of SE tasks. Finally, we outlined a road-map that we believe can facilitate the transfer of ML/DL-based SE research results into real-world industry practices.},
	language = {en},
	urldate = {2021-12-05},
	journal = {arXiv:2008.05515 [cs]},
	author = {Wang, Simin and Huang, Liguo and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Li, Ming and Zhang, He and Ng, Vincent},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.05515},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{Thomas2019GlobalDistributionofCVEngineeringWork,
	title = {Migration {Versus} {Management}: the {Global} {Distribution} of {Computer} {Vision} {Engineering} {Work}},
	shorttitle = {Migration {Versus} {Management}},
	doi = {10.1109/ICGSE.2019.00017},
	abstract = {Computer vision professionals develop systems that monitor endangered fish species, alert car dealerships of potential theft, and track inventory on retailers' shelves, to name a few examples. While their products vary, their day-to-day work practices rarely do. Most work in small, co-located, multi-disciplinary teams and rapidly iterate systems built with algorithms, products, and services provided by similar teams but at different companies. Their examples challenge global software engineering research to look beyond the social and technical coordination work of large, internal software development teams. Their stories, culled from ethnographic interviews with eighty computer vision engineers and research scientists, echo the heady days of 1980s Silicon Valley when Bay Area social networks and the global migration of professional talent fueled the rapid growth of the high technology industry. These engineers, then and now, migrate from task to task, versus the task from engineer to engineer.},
	booktitle = {2019 {ACM}/{IEEE} 14th {International} {Conference} on {Global} {Software} {Engineering} ({ICGSE})},
	author = {Thomas, Suzanne L.},
	month = may,
	year = {2019},
	keywords = {Computer vision, Deep learning, Software, Software algorithms, Software engineering, Task analysis, computer vision, ethnography, software engineering},
	pages = {12--17},
}

@misc{CVEngineer2021,
	title = {Being a {Computer} {Vision} {Engineer} in 2021},
	url = {https://viso.ai/computer-vision/computer-vision-engineer/},
	abstract = {Learn what computer vision engineers do, what skills are required to be a successful computer vision engineer, and the job outlook.},
	language = {en-US},
	urldate = {2021-12-08},
	journal = {viso.ai},
	month = jun,
	year = {2021},
}

@article{wang_synergy_2020,
	title = {Synergy between {Machine}/{Deep} {Learning} and {Software} {Engineering}: {How} {Far} {Are} {We}?},
	shorttitle = {Synergy between {Machine}/{Deep} {Learning} and {Software} {Engineering}},
	url = {http://arxiv.org/abs/2008.05515},
	abstract = {Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Machine Learning (ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the quality (especially the applicability and generalizability) of ML/DL-related SE studies, and to stimulate and enhance future collaborations between SE/AI researchers and industry practitioners, we conducted a 10-year Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that ML/DL and SE have had on each other. At the same time, however, we also observed a paucity of replicable and reproducible ML/DL-related SE studies and identified five factors that influence their replicability and reproducibility. To improve the applicability and generalizability of research results, we analyzed what ingredients in a study would facilitate an understanding of why a ML/DL technique was selected for a specific SE problem. In addition, we identified the unique trends of impacts of DL models on SE tasks, as well as five unique challenges that needed to be met in order to better leverage DL to improve the productivity of SE tasks. Finally, we outlined a road-map that we believe can facilitate the transfer of ML/DL-based SE research results into real-world industry practices.},
	urldate = {2021-12-05},
	journal = {arXiv:2008.05515 [cs]},
	author = {Wang, Simin and Huang, Liguo and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Li, Ming and Zhang, He and Ng, Vincent},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.05515},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{bajammal_survey_2020,
	title = {A {Survey} on the {Use} of {Computer} {Vision} to {Improve} {Software} {Engineering} {Tasks}},
	issn = {1939-3520},
	doi = {10.1109/TSE.2020.3032986},
	abstract = {Software engineering (SE) research has traditionally revolved around engineering the source code. However, novel approaches that analyze software through computer vision have been increasingly adopted in SE. These approaches allow analyzing the software from a different complementary perspective other than the source code, and they are used to either complement existing source code-based methods, or to overcome their limitations. The goal of this manuscript is to survey the use of computer vision techniques in SE with the aim of assessing their potential in advancing the field of SE research. We examined an extensive body of literature from top-tier SE venues, as well as venues from closely related fields (machine learning, computer vision, and human-computer interaction). Our inclusion criteria targeted papers applying computer vision techniques that address problems related to any area of SE. We collected an initial pool of 2,716 papers, from which we obtained 66 final relevant papers covering a variety of SE areas. We analyzed what computer vision techniques have been adopted or designed, for what reasons, how they are used, what benefits they provide, and how they are evaluated. Our findings highlight that visual approaches have been adopted in a wide variety of SE tasks, predominantly for effectively tackling software analysis and testing challenges in the web and mobile domains. The results also show a rapid growth trend of the use of computer vision techniques in SE research.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Bajammal, Mohammad and Stocco, Andrea and Mazinanian, Davood and Mesbah, Ali},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Computer Vision, Computer vision, Graphical user interfaces, Software, Software Engineering, Software engineering, Survey, Task analysis, Testing, Visualization},
	pages = {1--1},
}

@misc{noauthor_state_2019,
	title = {The {State} of {Machine} {Learning} {Frameworks} in 2019},
	url = {https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/},
	abstract = {Since deep learning regained prominence in 2012, many machine learning frameworks have clamored to become the new favorite among researchers and industry practitioners. From the early academic outputs Caffe and Theano to the massive industry-backed PyTorch and TensorFlow, this deluge of options makes it difficult to keep track of what},
	language = {en},
	urldate = {2021-11-19},
	journal = {The Gradient},
	month = oct,
	year = {2019},
}

@article{han_what_2020,
	title = {What do {Programmers} {Discuss} about {Deep} {Learning} {Frameworks}},
	volume = {25},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-020-09819-6},
	doi = {10.1007/s10664-020-09819-6},
	abstract = {Deep learning has gained tremendous traction from the developer and researcher communities. It plays an increasingly significant role in a number of application domains. Deep learning frameworks are proposed to help developers and researchers easily leverage deep learning technologies, and they attract a great number of discussions on popular platforms, i.e., Stack Overflow and GitHub. To understand and compare the insights from these two platforms, we mine the topics of interests from these two platforms. Specifically, we apply Latent Dirichlet Allocation (LDA) topic modeling techniques to derive the discussion topics related to three popular deep learning frameworks, namely, Tensorflow, PyTorch and Theano. Within each platform, we compare the topics across the three deep learning frameworks. Moreover, we make a comparison of topics between the two platforms. Our observations include 1) a wide range of topics that are discussed about the three deep learning frameworks on both platforms, and the most popular workflow stages are Model Training and Preliminary Preparation. 2) the topic distributions at the workflow level and topic category level on Tensorflow and PyTorch are always similar while the topic distribution pattern on Theano is quite different. In addition, the topic trends at the workflow level and topic category level of the three deep learning frameworks are quite different. 3) the topics at the workflow level show different trends across the two platforms. e.g., the trend of the Preliminary Preparation stage topic on Stack Overflow comes to be relatively stable after 2016, while the trend of it on GitHub shows a stronger upward trend after 2016. Besides, the Model Training stage topic still achieves the highest impact scores across two platforms. Based on the findings, we also discuss implications for practitioners and researchers.},
	language = {en},
	number = {4},
	urldate = {2021-11-19},
	journal = {Empirical Software Engineering},
	author = {Han, Junxiao and Shihab, Emad and Wan, Zhiyuan and Deng, Shuiguang and Xia, Xin},
	month = jul,
	year = {2020},
	pages = {2694--2747},
}

@inproceedings{parvat_survey_2017,
	title = {A survey of deep-learning frameworks},
	doi = {10.1109/ICISC.2017.8068684},
	abstract = {Deep learning is a model of machine learning loosely based on our brain. Artificial neural network has been around since the 1950s, but recent advances in hardware like graphical processing units (GPU), software like cuDNN, TensorFlow, Torch, Caffe, Theano, Deeplearning4j, etc. and new training methods have made training artificial neural networks fast and easy. In this paper, we are comparing some of the deep learning frameworks on the basis of parameters like modeling capability, interfaces available, platforms supported, parallelizing techniques supported, availability of pre-trained models, community support and documentation quality.},
	booktitle = {2017 {International} {Conference} on {Inventive} {Systems} and {Control} ({ICISC})},
	author = {Parvat, Aniruddha and Chavan, Jai and Kadam, Siddhesh and Dev, Souradeep and Pathak, Vidhi},
	month = jan,
	year = {2017},
	keywords = {Artificial neural networks, Computational modeling, Deep learning, Graphics processing units, Libraries, Machine learning, Mathematical model, Neural networks, Software libraries, Training},
	pages = {1--7},
}

@article{alexopoulos_how_nodate,
	title = {How {Long} {Do} {Vulnerabilities} {Live} in the {Code}? {A} {Large}-{Scale} {Empirical} {Measurement} {Study} on {FOSS} {Vulnerability} {Lifetimes}},
	abstract = {How long do vulnerabilities live in the repositories of large, evolving projects? Although the question has been identified as an interesting problem by the software community in online forums, it has not been investigated yet in adequate depth and scale, since the process of identifying the exact point in time when a vulnerability was introduced is particularly cumbersome. In this paper, we provide an automatic approach for accurately estimating how long vulnerabilities remain in the code (their lifetimes). Our method relies on the observation that while it is difficult to pinpoint the exact point of introduction for one vulnerability, it is possible to accurately estimate the average lifetime of a large enough sample of vulnerabilities, via a heuristic approach.},
	language = {en},
	author = {Alexopoulos, Nikolaos and Brack, Manuel and Wagner, Jan Philipp and Grube, Tim and Mühlhäuser, Max},
	pages = {18},
}

@inproceedings{xin_production_2021,
	address = {Virtual Event China},
	title = {Production {Machine} {Learning} {Pipelines}: {Empirical} {Analysis} and {Optimization} {Opportunities}},
	isbn = {978-1-4503-8343-1},
	shorttitle = {Production {Machine} {Learning} {Pipelines}},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457566},
	doi = {10.1145/3448016.3457566},
	abstract = {Machine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike the traditional perception of ML in research, ML production pipelines are complex, with many interlocking analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, and complexity of these pipelines to understand how data management research can be used to make them more efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four months, in an effort to understand the complexity and challenges underlying production ML. Our analysis reveals the characteristics, components, and topologies of typical industrystrength ML pipelines at various granularities. Along the way, we introduce a specialized data model for representing and reasoning about repeatedly run components in these ML pipelines, which we call model graphlets. We identify several rich opportunities for optimization, leveraging traditional data management ideas. We show how targeting even one of these opportunities, i.e., identifying and pruning wasted computation that does not translate to model deployment, can reduce wasted computation cost by 50\% without compromising the model deployment cadence.},
	language = {en},
	urldate = {2021-11-17},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Xin, Doris and Miao, Hui and Parameswaran, Aditya and Polyzotis, Neoklis},
	month = jun,
	year = {2021},
	pages = {2639--2652},
}

@misc{KerasWeb,
	title = {Keras: the {Python} deep learning {API}},
	url = {https://keras.io/},
	urldate = {2021-11-17},
}

@inproceedings{sharma_are_2018,
	title = {Are {Existing} {Knowledge} {Transfer} {Techniques} {Effective} for {Deep} {Learning} with {Edge} {Devices}?},
	doi = {10.1109/EDGE.2018.00013},
	abstract = {With the emergence of edge computing paradigm, many applications such as image recognition and augmented reality require to perform machine learning (ML) and artificial intelligence (AI) tasks on edge devices. Most AI and ML models are large and computational-heavy, whereas edge devices are usually equipped with limited computational and storage resources. Such models can be compressed and reduced for deployment on edge devices, but they may lose their capability and not perform well. Recent works used knowledge transfer techniques to transfer information from a large network (termed teacher) to a small one (termed student) in order to improve the performance of the latter. This approach seems to be promising for learning on edge devices, but a thorough investigation on its effectiveness is lacking. This paper provides an extensive study on the performance (in both accuracy and convergence speed) of knowledge transfer, considering different student architectures and different techniques for transferring knowledge from teacher to student. The results show that the performance of KT does vary by architectures and transfer techniques. A good performance improvement is obtained by transferring knowledge from both the intermediate layers and last layer of the teacher to a shallower student. But other architectures and transfer techniques do not fare so well and some of them even lead to negative performance impact.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Edge} {Computing} ({EDGE})},
	author = {Sharma, Ragini and Biookaghazadeh, Saman and Li, Baoxin and Zhao, Ming},
	month = jul,
	year = {2018},
	keywords = {Biological system modeling, Computational modeling, Data models, Deep neural networks, Image edge detection, Knowledge transfer, Machine learning, Performance evaluation, cloud computing, edge computing, knowledge transfer},
	pages = {42--49},
}

@article{muiruri_practices_2021,
	title = {Practices and {Infrastructures} for {ML} {Systems} – {An} {Interview} {Study}},
	url = {https://www.techrxiv.org/articles/preprint/Practices_and_Infrastructures_for_ML_Systems_An_Interview_Study/16939192/1},
	doi = {10.36227/techrxiv.16939192.v1},
	abstract = {The best practices and infrastructures for developing and maintaining machine learning (ML) enabled software systems are often reported by large and experienced data-driven organizations. However, little is known about the state of practice across other organizations. Using interviews, we investigated practices and tool-chains for ML-enabled systems from 16 organizations in various domains. Our study makes three broad observations related to data management practices, monitoring practices and automation practices in ML model training, and serving workflows. These have limited number of generic practices and tools applicable across organizations in different domains.},
	language = {en},
	urldate = {2021-11-16},
	author = {Muiruri, Dennis and Lwakatare, Lucy Ellen and K. Nurminen, Jukka and Mikkonen, Tommi},
	month = nov,
	year = {2021},
	note = {Publisher: TechRxiv},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	urldate = {2021-11-12},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{bochkovskiy_yolov4_2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	shorttitle = {{YOLOv4}},
	url = {http://arxiv.org/abs/2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
	urldate = {2021-11-12},
	journal = {arXiv:2004.10934 [cs, eess]},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.10934},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{gafford_synthesis-based_2020,
	address = {Virtual Event Australia},
	title = {Synthesis-based resolution of feature interactions in cyber-physical systems},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416630},
	doi = {10.1145/3324884.3416630},
	abstract = {The feature interaction problem arises when two or more independent features interact with each other in an undesirable manner. Feature interactions remain a challenging and important problem in emerging domains of cyber-physical systems (CPS), such as intelligent vehicles, unmanned aerial vehicles (UAVs) and the Internet of Things (IoT), where the outcome of an unexpected interaction may result in a safety failure. Existing approaches to resolving feature interactions rely on priority lists or fixed strategies, but may not be effective in scenarios where none of the competing feature actions are satisfactory with respect to system requirements. This paper proposes a novel synthesis-based approach to resolution, where a conflict among features is resolved by synthesizing an action that best satisfies the specification of desirable system behaviors in the given environmental context. Unlike existing resolution methods, our approach is capable of producing a desirable system outcome even when none of the conflicting actions are satisfactory. The effectiveness of the proposed approach is demonstrated using a case study involving interactions among safety-critical features in an autonomous drone.},
	language = {en},
	urldate = {2021-11-07},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Gafford, Benjamin and Dürschmid, Tobias and Moreno, Gabriel A. and Kang, Eunsuk},
	month = dec,
	year = {2020},
	pages = {1090--1102},
}

@article{liu_path_2018,
	title = {Path {Aggregation} {Network} for {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/1803.01534},
	abstract = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github.com/ShuLiu1993/PANet},
	urldate = {2021-11-05},
	journal = {arXiv:1803.01534 [cs]},
	author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
	month = sep,
	year = {2018},
	note = {arXiv: 1803.01534},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{potvin_why_2016,
	title = {Why {Google} stores billions of lines of code in a single repository},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2854146},
	doi = {10.1145/2854146},
	abstract = {Google's monolithic repository provides a common source of truth for tens of thousands of developers around the world.},
	language = {en},
	number = {7},
	urldate = {2021-11-05},
	journal = {Communications of the ACM},
	author = {Potvin, Rachel and Levenberg, Josh},
	month = jun,
	year = {2016},
	pages = {78--87},
}

@article{malhotra_empirical_2020,
	title = {An empirical study on predictability of software maintainability using imbalanced data},
	volume = {28},
	issn = {0963-9314, 1573-1367},
	url = {https://link.springer.com/10.1007/s11219-020-09525-y},
	doi = {10.1007/s11219-020-09525-y},
	abstract = {In software engineering predictive modeling, early prediction of software modules or classes that possess high maintainability effort is a challenging task. Many prediction models are constructed to predict the maintainability of software classes or modules by applying various machine learning (ML) techniques. If the software modules or classes need high maintainability, effort would be reduced in a dataset, and there would be imbalanced data to train the model. The imbalanced datasets make ML techniques bias their predictions towards low maintainability effort or majority classes, and minority class instances get discarded as noise by the machine learning (ML) techniques. In this direction, this paper presents empirical work to improve the performance of software maintainability prediction (SMP) models developed with ML techniques using imbalanced data. For developing the models, the imbalanced data is preprocessed by applying data resampling methods. Fourteen data resampling methods, including oversampling, undersampling, and hybrid resampling, are used in the study. The study results recommend that the safe-level synthetic minority oversampling technique (Safe-LevelSMOTE) is a useful method to deal with the imbalanced datasets and to develop competent prediction models to forecast software maintainability.},
	language = {en},
	number = {4},
	urldate = {2021-11-05},
	journal = {Software Quality Journal},
	author = {Malhotra, Ruchika and Lata, Kusum},
	month = dec,
	year = {2020},
	pages = {1581--1614},
}

@article{elmidaoui_machine_2020,
	title = {Machine {Learning} {Techniques} for {Software} {Maintainability} {Prediction}: {Accuracy} {Analysis}},
	volume = {35},
	issn = {1000-9000, 1860-4749},
	shorttitle = {Machine {Learning} {Techniques} for {Software} {Maintainability} {Prediction}},
	url = {https://link.springer.com/10.1007/s11390-020-9668-1},
	doi = {10.1007/s11390-020-9668-1},
	abstract = {Maintaining software once implemented on the end-user side is laborious and, over its lifetime, is most often considerably more expensive than the initial software development. The prediction of software maintainability has emerged as an important research topic to address industry expectations for reducing costs, in particular, maintenance costs. Researchers and practitioners have been working on proposing and identifying a variety of techniques ranging from statistical to machine learning (ML) for better prediction of software maintainability. This review has been carried out to analyze the empirical evidence on the accuracy of software product maintainability prediction (SPMP) using ML techniques. This paper analyzes and discusses the ﬁndings of 77 selected studies published from 2000 to 2018 according to the following criteria: maintainability prediction techniques, validation methods, accuracy criteria, overall accuracy of ML techniques, and the techniques oﬀering the best performance. The review process followed the well-known systematic review process. The results show that ML techniques are frequently used in predicting maintainability. In particular, artiﬁcial neural network (ANN), support vector machine/regression (SVM/R), regression \& decision trees (DT), and fuzzy \& neuro fuzzy (FNF) techniques are more accurate in terms of PRED and MMRE. The N -fold and leave-one-out cross-validation methods, and the MMRE and PRED accuracy criteria are frequently used in empirical studies. In general, ML techniques outperformed non-machine learning techniques, e.g., regression analysis (RA) techniques, while FNF outperformed SVM/R, DT, and ANN in most experiments. However, while many techniques were reported superior, no speciﬁc one can be identiﬁed as the best.},
	language = {en},
	number = {5},
	urldate = {2021-11-05},
	journal = {Journal of Computer Science and Technology},
	author = {Elmidaoui, Sara and Cheikhi, Laila and Idri, Ali and Abran, Alain},
	month = oct,
	year = {2020},
	pages = {1147--1174},
}

@article{alsolai_systematic_2020,
	title = {A systematic literature review of machine learning techniques for software maintainability prediction},
	volume = {119},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584919302228},
	doi = {10.1016/j.infsof.2019.106214},
	abstract = {Context: Software maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process. Objective: The major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models. Method: The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018. Results: We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely. Conclusion: Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other, models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.},
	language = {en},
	urldate = {2021-11-05},
	journal = {Information and Software Technology},
	author = {Alsolai, Hadeel and Roper, Marc},
	month = mar,
	year = {2020},
	pages = {106214},
}

@article{malhotra_software_2012,
	title = {Software {Maintainability} {Prediction} using {Machine} {Learning} {Algorithms}},
	volume = {2},
	abstract = {Software maintainability is one of the most important aspects while evaluating quality of the software product. It is deined as the ease with which a software system or component can be modiied to correct faults, improve performance or other attributes or adapt to a changed environment. Tracking the maintenance behaviour of the software product is very complex. This is precisely the reason that predicting the cost and risk associated with maintenance after delivery is extremely dificult which is widely acknowledged by the researchers and practitioners. In an attempt to address this issue quantitatively, the main purpose of this paper is to propose use of few machine learning algorithms with an objective to predict software maintainability and evaluate them. The proposed models are Group Method of Data Handling (GMDH), Genetic Algorithms (GA) and Probabilistic Neural Network (PNN) with Gaussian activation function. The prediction model is constructed using the above said machine learning techniques. In order to study and evaluate its performance, two commercial datasets UIMS (User Interface Management System) and QUES (Quality Evaluation System) are used. The code for these two systems was written in Classical Ada. The UIMS contains 39 classes and QUES datasets contains 71 classes. To measure the maintainability, number of “CHANGE” is observed over a period of three years. We can deine CHANGE as the number of lines of code which were added, deleted or modiied during a three year maintenance period. After conducting empirical study, performance of these three proposed machine learning algorithms was compared with prevailing models such as GRNN (General Regression Neural Network) Model, ANN (Artiicial Neural Network) Model, Bayesian Model, RT (Regression Tree) Model, Backward Elimination Model, Stepwise Selection Model, MARS (Multiple Adaptive Regression Splines) Model, TreeNets Model, GN (Generalized Regression) Model, ANFIS (Adaptive Neuro Fuzzy inference System) Model, SVM (Support Vector Machine) Model and MLR (Multiple Linear Regressions) Model which were taken from the literature. Based on experiments conducted, it was found that GMDH can be applied as a sound alternative to the existing techniques used for software maintainability prediction since it assists in predicting the maintainability more accurately and precisely than prevailing models.},
	language = {en},
	number = {2},
	journal = {Software engineering},
	author = {Malhotra, Ruchika and Chug, Anuradha},
	year = {2012},
	pages = {19},
}

@article{jha_deep_2019,
	title = {Deep {Learning} {Approach} for {Software} {Maintainability} {Metrics} {Prediction}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2913349},
	abstract = {Software maintainability predicts changes or failures that may occur in software after it has been deployed. Since it deals with the degree to which an application may be understood, repaired, or enhanced, it also takes into account the overall cost of the project. In the past, several measures have been taken into account for predicting metrics that influence software maintainability. However, deep learning is yet to be explored for the same. In this paper, we perform deep learning for software maintainability metrics' prediction on a large number of datasets. Unlike the previous research works, we have relied on large datasets from 299 software and subsequently applied various metrics and functions to the same; 29 object-oriented metrics have been considered along with their impact on software maintainability of open source software. Several metrics have been analyzed and descriptive statistics of these metrics have been pointed out. The proposed long short term memory has been evaluated using measures, such as mean absolute error, root mean square error and accuracy. Five machine learning algorithms, namely, ridge regression with variable selection, decision tree, quantile regression forest, support vector machine, and principal component analysis have been applied to the original datasets, as well as, to the refined datasets. It was found that this paper provides results in the form of metrics that may be used in the prediction of software maintenance and the proposed deep learning model outperforms all of the other methods that were considered. Furthermore, the results of experiment affirm the efficiency of the proposed deep learning model for software maintainability prediction.},
	journal = {IEEE Access},
	author = {Jha, Sudan and Kumar, Raghvendra and Hoang Son, Le and Abdel-Basset, Mohamed and Priyadarshini, Ishaani and Sharma, Rohit and Viet Long, Hoang},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Cloning, Deep learning, Machine learning algorithms, Software, Software measurement, machine learning, prediction, software maintainability, software metrics},
	pages = {61840--61855},
}

@article{gold_ethics_2022,
	title = {Ethics in the mining of software repositories},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-021-10057-7},
	doi = {10.1007/s10664-021-10057-7},
	abstract = {Research in Mining Software Repositories (MSR) is research involving human subjects, as the repositories usually contain data about developers’ and users’ interactions with the repositories and with each other. The ethics issues raised by such research therefore need to be considered before beginning. This paper presents a discussion of ethics issues that can arise in MSR research, using the mining challenges from the years 2006 to 2021 as a case study to identify the kinds of data used. On the basis of contemporary research ethics frameworks we discuss ethics challenges that may be encountered in creating and using repositories and associated datasets. We also report some results from a small community survey of approaches to ethics in MSR research. In addition, we present four case studies illustrating typical ethics issues one encounters in projects and how ethics considerations can shape projects before they commence. Based on our experience, we present some guidelines and practices that can help in considering potential ethics issues and reducing risks.},
	language = {en},
	number = {1},
	urldate = {2021-11-03},
	journal = {Empirical Software Engineering},
	author = {Gold, Nicolas E. and Krinke, Jens},
	month = jan,
	year = {2022},
	pages = {17},
}

@inproceedings{luzgin_overview_2020,
	title = {Overview of {Mining} {Software} {Repositories}},
	doi = {10.1109/EIConRus49466.2020.9039225},
	abstract = {Mining of software repositories (MSR) provides opportunities to enhance software engineering activities and to analyze violations to prevent it. Software engineering quality and security challenges can be solved by analyzing unstructured information from various artifacts of software development. This paper describes current challenges and applications in the field. This work also proposes an overview of the directions of mining software repositories and describes the methods and techniques that are used in this area.},
	booktitle = {2020 {IEEE} {Conference} of {Russian} {Young} {Researchers} in {Electrical} and {Electronic} {Engineering} ({EIConRus})},
	author = {Luzgin, Victor A. and Kholod, Ivan I.},
	month = jan,
	year = {2020},
	note = {ISSN: 2376-6565},
	keywords = {Data mining, Deep learning, Natural language processing, Software, Software engineering, Software metrics, mining software repositories, software engineering, unstructured data},
	pages = {400--404},
}

@article{kaur_systematic_nodate,
	title = {Systematic {Literature} {Review} on {Mining} {Software} {Repositories}},
	abstract = {Mining Software Repositories (MSR) explores the complex software systems to unfold valuable and interesting knowledge. The systematic literature review is performed on MSR studies published over a decade. The review is conducted on 300 selected papers. The study aims to identify – i) popular application areas, ii) popular tools, iii) datasets and software projects, along with their type and SDLC phase in which they are used, iv) popular techniques, v) top conferences and journals along with the types of studies and vi) current trends, of MSR research. The important results implied from the reviewed studies are: 1) Bug Prediction (25\%) and Change Prediction (17\%) are the most popular application areas of MSR, 2) Data Modelling and Statistical Tools, (19\%) were employed in most of the reviewed studies, 3) About 71 \% of reviewed studies use datasets from open source repositories, 4) There is a large number of mining techniques with Classification techniques (29\%) dominating the field, 5) The most popular conference for MSR is the International Conference on Mining Software Repositories (MSR), and 6) Research gaps in evolving application areas of clone detection, code reuse and software evolution are identified.},
	language = {en},
	author = {Kaur, Arvinder and Kaur, Kamaldeep and Chopra, Deepti and Kaur, Harguneet},
	pages = {37},
}

@article{barros_mining_2021,
	title = {A {Mining} {Software} {Repository} {Extended} {Cookbook}: {Lessons} learned from a literature review},
	shorttitle = {A {Mining} {Software} {Repository} {Extended} {Cookbook}},
	url = {http://arxiv.org/abs/2110.04095},
	abstract = {The main purpose of Mining Software Repositories (MSR) is to discover the latest enhancements and provide an insight into how to make improvements in a software project. In light of it, this paper updates the MSR findings of the original MSR Cookbook, by first conducting a systematic mapping study to elicit and analyze the state-of-the-art, and then proposing an extended version of the Cookbook. This extended Cookbook was built on four high-level themes, which were derived from the analysis of a list of 112 selected studies. Hence, it was used to consolidate the extended Cookbook as a contribution to practice and research in the following areas by: 1) including studies published in all available and relevant publication venues; 2) including and updating recommendations in all four high-level themes, with an increase of 84\% in comments in this study when compared with the original MSR Cookbook; 3) summarizing the tools employed for each high-level theme; and 4) providing lessons learned for future studies. Thus, the extended Cookbook examined in this work can support new research projects, as upgraded recommendations and the lessons learned are available with the aid of samples and tools.},
	urldate = {2021-11-01},
	journal = {arXiv:2110.04095 [cs]},
	author = {Barros, Daniel and Horita, Flavio and Wiese, Igor and Silva, Kanan},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.04095},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{romano_g-repo_2021,
	title = {G-{Repo}: a {Tool} to {Support} {MSR} {Studies} on {GitHub}},
	shorttitle = {G-{Repo}},
	doi = {10.1109/SANER50967.2021.00064},
	abstract = {GitHub currently hosts more than 100 million public repositories. This has made it very popular to conduct Mining Software Repositories (MSR) studies. Researchers have been exploiting the information stored in GitHub (e.g., commits, pull requests, or issues) to investigate both developer- and project-related aspects. GitHub provides the REST API to make queries without cloning repositories. In this tool-demo paper, we highlight some issues we noticed when conducting an MSR study on GitHub by using the REST API and present G-Repo: a tool developed to support researchers when tackling these issues able to ease the creation of datasets for MSR studies. Also, we provide a manually-annotated dataset with information about the kind and the (spoken) languages of 1,500 repositories hosted on GitHub. A video showing the functioning of G-Repo is available at: https://youtu.be/mb9CIALBFZk.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Romano, Simone and Caulo, Maria and Buompastore, Matteo and Guerra, Leonardo and Mounsif, Anas and Telesca, Michele and Baldassarre, Maria Teresa and Scanniello, Giuseppe},
	month = mar,
	year = {2021},
	note = {ISSN: 1534-5351},
	keywords = {Cloning, Conferences, Data mining, G-Repo, GitHub, MSR, Manuals, Software, Software development management, Tools},
	pages = {551--555},
}

@misc{noauthor_mining_nodate,
	title = {Mining {Industry} {Project} {Management} {Software}},
	url = {https://www.aresprism.com/mining-industry-project-management-software/},
	abstract = {ARES PRISM Software is an enterprise solution vastly used by mining industry giants to digitize their companies \& ensure their projects are on-time \& on budget},
	language = {en-US},
	urldate = {2021-11-01},
	journal = {ARES PRISM},
}

@article{nosco_industrial_nodate,
	title = {The {Industrial} {Age} of {Hacking}},
	abstract = {There is a cognitive bias in the hacker community to select a piece of software and invest signiﬁcant human resources into ﬁnding bugs in that software without any prior indication of success. We label this strategy depth-ﬁrst search and propose an alternative: breadth-ﬁrst search. In breadthﬁrst search, humans perform minimal work to enable automated analysis on a range of targets before committing additional time and effort to research any particular one. We present a repeatable human study that leverages teams of varying skill while using automation to the greatest extent possible. Our goal is a process that is effective at ﬁnding bugs; has a clear plan for the growth, coaching, and efﬁcient use of team members; and supports measurable, incremental progress. We derive an assembly-line process that improves on what was once intricate, manual work. Our work provides evidence that the breadth-ﬁrst approach increases the effectiveness of teams.},
	language = {en},
	author = {Nosco, Tim and Ziegler, Jared and Clark, Zechariah},
	pages = {19},
}

@inproceedings{proksch_enriched_2018,
	address = {Gothenburg Sweden},
	title = {Enriched event streams: a general dataset for empirical studies on in-{IDE} activities of software developers},
	isbn = {978-1-4503-5716-6},
	shorttitle = {Enriched event streams},
	url = {https://dl.acm.org/doi/10.1145/3196398.3196400},
	doi = {10.1145/3196398.3196400},
	abstract = {Developers have been the subject of many empirical studies over the years. To assist developers in their everyday work, an understanding of their activities is necessary, especially how they develop source code. Unfortunately, conducting such studies is very expensive and researchers often resort to studying artifacts after the fact. To pave the road for future empirical studies on developer activities, we built FeedBaG, a general-purpose interaction tracker for Visual Studio that monitors development activities. The observations are stored in enriched event streams that encode a holistic picture of the in-IDE development process. Enriched event streams capture all commands invoked in the IDE with additional context information, such as the test being run or the accompanying fine-grained code edits. We used FeedBaG to collect enriched event streams from 81 developers. Over 1,527 days, we collected more than 11M events that correspond to 15K hours of working time.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Proksch, Sebastian and Amann, Sven and Nadi, Sarah},
	month = may,
	year = {2018},
	pages = {62--65},
}

@article{odom_research_2016,
	title = {From research prototype to research product},
	doi = {10.1145/2858036.2858447},
	abstract = {Prototypes and prototyping have had a long and important history in the HCI community and have played a highly significant role in creating technology that is easier and more fulfilling to use. Yet, as focus in HCI is expanding to investigate complex matters of human relationships with technology over time in the intimate and contested contexts of everyday life, the notion of a 'prototype' may not be fully sufficient to support these kinds of inquiries. We propose the research product as an extension and evolution of the research prototype to support generative inquiries in this emerging research area. We articulate four interrelated qualities of research products-inquiry-driven, finish, fit, and independent-and draw on these qualities to describe and analyze five different yet related design research cases we have collectively conducted over the past six years. We conclude with a discussion of challenges and opportunities for crafting research products and the implications they suggest for future design-oriented HCI research.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Odom, William and Wakkary, Ron and Lim, Youn Kyung and Desjardins, Audrey and Hengeveld, Bart and Banks, Richard},
	year = {2016},
	note = {ISBN: 9781450333627},
	keywords = {Design, Interaction design research, Research product},
	pages = {2549--2561},
}

@article{eisty_use_2019,
	title = {Use of software process in research software development: {A} survey},
	doi = {10.1145/3319008.3319351},
	abstract = {Background: Developers face challenges in building high-quality research software due to its inherent complexity. These challenges can reduce the confidence users have in the quality of the result produced by the software. Use of a defined software development process, which divides the development into distinct phases, results in improved design, more trustworthy results, and better project management. Aims: This paper focuses on gaining a better understanding of the use of software development process for research software. Method: We surveyed research software developers to collect information about their use of software development processes. We analyze whether and demographic factors influence the respondents’ use of and perceived value in defined process. Results: Based on 98 responses, research software developers appear to follow a defined software development process at least some of the time. The respondents also have a strong positive perception about the value of following processes. Conclusions: To produce high-quality and reliable research software, which is critical for many research domains, research software developers must follow a proper software development process. The results indicate a positive perception of value about using defined development processes that should lead to both short-term benefits through improved results and long-term benefits through more maintainable software.},
	journal = {ACM International Conference Proceeding Series},
	author = {Eisty, Nasir U. and Thiruvathukal, George K. and Carver, Jeffrey C.},
	year = {2019},
	note = {ISBN: 9781450371452},
	keywords = {Research software, Software process, Survey},
	pages = {276--282},
}

@article{redmon_yolo_2018,
	title = {{YOLO} v.3},
	url = {https://pjreddie.com/media/files/papers/YOLOv3.pdf},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP 50 in 51 ms on a Titan X, compared to 57.5 AP 50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
	journal = {Tech report},
	author = {Redmon, Joseph and Farhadi, Ali},
	year = {2018},
	pages = {1--6},
}

@article{li_model_2020,
	title = {Model {Adaptation}: {Unsupervised} {Domain} {Adaptation} without {Source} {Data}},
	issn = {10636919},
	doi = {10.1109/CVPR42600.2020.00966},
	abstract = {In this paper, we investigate a challenging unsupervised domain adaptation setting - - unsupervised model adaptation. We aim to explore how to rely only on unlabeled target data to improve performance of an existing source prediction model on the target domain, since labeled source data may not be available in some real-world scenarios due to data privacy issues. For this purpose, we propose a new framework, which is referred to as collaborative class conditional generative adversarial net to bypass the dependence on the source data. Specifically, the prediction model is to be improved through generated target-style data, which provides more accurate guidance for the generator. As a result, the generator and the prediction model can collaborate with each other without source data. Furthermore, due to the lack of supervision from source data, we propose a weight constraint that encourages similarity to the source model. A clustering-based regularization is also introduced to produce more discriminative features in the target domain. Compared to conventional domain adaptation methods, our model achieves superior performance on multiple adaptation tasks with only unlabeled target data, which verifies its effectiveness in this challenging setting.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Li, Rui and Jiao, Qianfen and Cao, Wenming and Wong, Hau San and Wu, Si},
	year = {2020},
	pages = {9638--9647},
}

@article{das_model_2020,
	title = {Model adaptation and unsupervised learning with non-stationary batch data under smooth concept drift},
	url = {http://arxiv.org/abs/2002.04094},
	abstract = {Most predictive models assume that training and test data are generated from a stationary process. However, this assumption does not hold true in practice. In this paper, we consider the scenario of a gradual concept drift due to the underlying non-stationarity of the data source. While previous work has investigated this scenario under a supervised-learning and adaption conditions, few have addressed the common, real-world scenario when labels are only available during training. We propose a novel, iterative algorithm for unsupervised adaptation of predictive models. We show that the performance of our batch adapted prediction algorithm is better than that of its corresponding unadapted version. The proposed algorithm provides similar (or better, in most cases) performance within significantly less run time compared to other state of the art methods. We validate our claims though extensive numerical evaluations on both synthetic and real data.},
	author = {Das, Subhro and Lade, Prasanth and Srinivasan, Soundar},
	year = {2020},
}

@article{rushby_evaluation_1990,
	title = {Evaluation of {Safety}- {Critical} {Software}},
	volume = {33},
	number = {6},
	author = {Rushby, John and Parnas, David L. and Schouwen, A. John Van and Kwan, Shu P. O.},
	year = {1990},
}

@article{rekdal_academic_2014,
	title = {Academic urban legends},
	volume = {44},
	issn = {14603659},
	doi = {10.1177/0306312714535679},
	abstract = {Many of the messages presented in respectable scientific publications are, in fact, based on various forms of rumors. Some of these rumors appear so frequently, and in such complex, colorful, and entertaining ways that we can think of them as academic urban legends. The explanation for this phenomenon is usually that authors have lazily, sloppily, or fraudulently employed sources, and peer reviewers and editors have not discovered these weaknesses in the manuscripts during evaluation. To illustrate this phenomenon, I draw upon a remarkable case in which a decimal point error appears to have misled millions into believing that spinach is a good nutritional source of iron. Through this example, I demonstrate how an academic urban legend can be conceived and born, and can continue to grow and reproduce within academia and beyond. © 2014, The Author(s). All rights reserved.},
	number = {4},
	journal = {Social Studies of Science},
	author = {Rekdal, Ole Bjørn},
	year = {2014},
	pmid = {25272616},
	keywords = {academic shortcuts, academic urban legends, citation practices, iron, spinach},
	pages = {638--654},
}

@article{joanne_wendelberger_ross_whitaker_workshop_nodate,
	title = {Workshop on {Quantification}, {Communication}, and {Interpretation} of {Uncertainty} in {Simulatio} and {Data} {Science}},
	abstract = {Eigene Verhaltensweisen zu unterbrechen, zu unterdr\{ü\}cken oder in anderer Weise zu ver\{ä\}ndern, Pl\{ä\}ne zu schmieden und langfristig auch gegen innere und \{ä\}u\{ß\}ere Widerst\{ä\}nde eigene Ziele zu verfolgen sowie Versuchungen oder Ablenkungen zu widerstehen geh\{ö\}rt zu den beeindruckendsten psychischen Funktionen des Menschen (Carver \{\&\} Scheier, 1981; Baumeister, Heatherton \{\&\} Tice, 1994).},
	author = {Joanne Wendelberger Ross Whitaker, William Thompson, James Berger, Baruch Fischhof, Michael Goodchild, Mary Hegarty, Christopher Jermaine, Kathryn S. McKinley, Alex Pang},
}

@article{sampson_expressing_2014,
	title = {Expressing and {Verifying} {Probabilistic} {Assertions}},
	author = {Sampson, Adrian and Panchekha, Pavel and Mytkowicz, Todd and McKinley, Kathryn S. and Grossman, Dan and Ceze, Luis},
	year = {2014},
	note = {ISBN: 9781450327848},
	keywords = {a search, a variable, abilistic data, approximate computing, consume prob-, data obfuscation, differential privacy, however, many applications produce or, of a document to, probabilistic programming, sensors, such as the relevance, symbolic execution, the},
}

@article{bond_probabilistic_2007,
	title = {Probabilistic calling context},
	doi = {10.1145/1297027.1297035},
	abstract = {Calling context enhances program understanding and dynamic analyses by providing a rich representation of program location. Compared to imperative programs, objectoriented programs use more interprocedural and less intraprocedural control flow, increasing the importance of context sensitivity for analysis. However, prior online methods for computing calling context, such as stack-walking or maintaining the current location in a calling context tree, are expensive in time and space. This paper introduces a new online approach called probabilistic calling context (PCC) that continuously maintains a probabilistically unique value representing the current calling context. For millions of unique contexts, a 32-bit PCC value has few conflicts. Computing the PCC value adds 3\% average overhead to a Java virtual machine. PCC is well-suited to clients that detect new or anomalous behavior since PCC values from training and production runs can be compared easily to detect new contextsensitive behavior; clients that query the PCC value at every system call, Java utility call, and Java API call add 0-9\% overhead on average. PCC adds space overhead proportional to the distinct contexts stored by the client (one word per context). Our results indicate PCC is efficient and accurate enough to use in deployed software for residual testing, bug detection, and intrusion detection. Copyright © 2007 ACM.},
	journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
	author = {Bond, Michael D. and McKinley, Kathryn S.},
	year = {2007},
	note = {ISBN: 9781595937865},
	keywords = {Anomaly-based bug detection, Calling context, Dynamic context sensitivity, Intrusion detection, Managed languages, Probabilistic, Residual testing},
	pages = {97--111},
}

@article{bomholt_m_2009,
	title = {M anaging {An} {U} ncertain {F} uture},
	volume = {4},
	number = {October},
	author = {Bomholt, James},
	year = {2009},
	pages = {1--34},
}

@article{bornholt_uncertainlttgt_2014,
	title = {Uncertain\&lt;t\&gt;},
	issn = {0362-1340},
	url = {http://dl.acm.org/citation.cfm?doid=2541940.2541958},
	abstract = {Emerging applications increasingly use estimates such as sensor data (GPS), probabilistic models, machine learning, big data, and human data. Unfortunately, representing this uncertain data with discrete types (floats, integers, and booleans) encourages developers to pretend it is not probabilistic, which causes three types of uncertainty bugs. (1) Using estimates as facts ignores random error in estimates. (2) Computation compounds that error. (3) Boolean questions on probabilistic data induce false positives and negatives. This paper introduces Uncertain〈T〉, a new programming language abstraction for uncertain data. We implement a Bayesian network semantics for computation and conditionals that improves program correctness. The runtime uses sampling and hypothesis tests to evaluate computation and conditionals lazily and efficiently. We illustrate with sensor and machine learning applications that Uncertain〈T〉 improves expressiveness and accuracy. Whereas previous probabilistic programming languages focus on experts, Uncertain〈T〉 serves a wide range of developers. Experts still identify error distributions. However, both experts and application writers compute with distributions, improve estimates with domain knowledge, and ask questions with conditionals. The Uncertain〈T〉 type system and operators encourage developers to expose and reason about uncertainty explicitly, controlling false positives and false negatives. These benefits make Uncertain〈T〉 a compelling programming model for modern applications facing the challenge of uncertainty. Copyright © 2014 ACM.},
	journal = {Proceedings of the 19th international conference on Architectural support for programming languages and operating systems - ASPLOS '14},
	author = {Bornholt, James and Mytkowicz, Todd and McKinley, Kathryn S.},
	year = {2014},
	note = {ISBN: 9781450323055},
	keywords = {estimates, probabilistic programming, uncertain data},
	pages = {51--66},
}

@article{wang_achieving_2009,
	title = {Achieving high and consistent rendering performance of java {AWT}/{Swing} on multiple platforms},
	volume = {39},
	issn = {00380644},
	doi = {10.1002/spe},
	abstract = {Wang et al. (Softw. Pract. Exper. 2007; 37(7):727-745) observed a phenomenon of performance inconsistency in the graphics of Java Abstract Window Toolkit (AWT)/Swing among different Java runtime environments (JREs) on Windows XP. This phenomenon makes it difficult to predict the performance of Java game applications. Therefore, they proposed a portable AWT/Swing architecture, called CYC Window Toolkit (CWT), to provide programmers with high and consistent rendering performance for Java game development among different JREs. They implemented a DirectX version to demonstrate the feasibility of the architecture. This paper extends the above research to other environments in two aspects. First, we evaluate the rendering performance of the original Java AWT with different combinations of JREs, image application programming interfaces, system properties and operating systems (OSs), including Windows XP, Windows Vista, Fedora and Mac OS X. The evaluation results indicate that the performance inconsistency of Java AWT also exists among the four OSs, even if the same hardware configuration is used. Second, we design an OpenGL version of CWT, named CWT-GL, to take advantage of modern 3D graphics cards, and compare the rendering performance of CWT with Java AWT/Swing. The results show that CWT-GL achieves more consistent and higher rendering performance in JREs 1.4to 1.6 on the four OSs. The results also hint at two approaches: (a) decouple the rendering pipelines of Java AWT/Swing from the JREs for faster upgrading and supporting old JREs and (b) use other graphics libraries, such as CWT, instead of Java AWT/Swing to develop cross-platform Java games with higher and more consistent rendering performance © 2009 John Wiley \& Sons, Ltd.},
	number = {7},
	journal = {Software - Practice and Experience},
	author = {Wang, Yi Hsien and Wu, I. Chen},
	year = {2009},
	keywords = {CYC Window Toolkit, Directx, Linux, Mac OS x, OpenGL, Windows},
	pages = {701--736},
}

@article{nandi_debugging_2017,
	title = {Debugging probabilistic programs},
	doi = {10.1145/3088525.3088564},
	abstract = {Many applications compute with estimated and uncertain data. While advances in probabilistic programming help developers build such applications, debugging them remains extremely challenging. New types of errors in probabilistic programs include 1) ignoring dependencies and correlation between random variables and in training data, 2) poorly chosen inference hyper-parameters, and 3) incorrect statistical models. A partial solution to prevent these errors in some languages forbids developers from explicitly invoking inference. While this prevents some dependence errors, it limits composition and control over inference, and does not guarantee absence of other types of errors. This paper presents the FLEXI programming model which supports constructs for invoking inference in the language and reusing the results in other statistical computations. We define a novel formalism for inference with a Decorated Bayesian Network and present a tool, DePP, that analyzes this representation to identify the above errors. We evaluate DePP on a range of prototypical examples to show how it helps developers to detect errors.},
	journal = {MAPL 2017 - Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, co-located with PLDI 2017},
	author = {Nandi, Chandrakana and Grossman, Dan and Sampson, Adrian and Mytkowicz, Todd and McKinley, Kathryn S.},
	year = {2017},
	note = {ISBN: 9781450350716},
	keywords = {Debugging, Probabilistic programming, Program analysis, Statistical inference},
	pages = {18--26},
}

@article{cito_empirical_2020,
	title = {An {Empirical} {Study} on the {Impact} of {Parameters} on {Mobile} {App} {Energy} {Usage}},
	author = {Cito, Jürgen and Rinard, Martin},
	year = {2020},
	note = {ISBN: 9781450375177},
	pages = {598--601},
}

@article{akhlaghi_towards_2021,
	title = {Towards {Long}-term and {Archivable} {Reproducibility}},
	issn = {1558366X},
	doi = {10.1109/MCSE.2021.3072860},
	abstract = {Analysis pipelines commonly use high-level technologies that are popular when created, but are unlikely to be readable, executable, or sustainable in the long term. A set of criteria is introduced to address this problem: Completeness (no execution requirement beyond a minimal Unix-like operating system, no administrator privileges, no network connection, and storage primarily in plain text); modular design; minimal complexity; scalability; verifiable inputs and outputs; version control; linking analysis with narrative; and free software. As a proof of concept, we introduce "Maneage" (Managing data lineage), enabling cheap archiving, provenance extraction, and peer verification that been tested in several research publications. We show that longevity is a realistic requirement that does not sacrifice immediate or short-term reproducibility. The caveats (with proposed solutions) are then discussed and we conclude with the benefits for the various stakeholders. This paper is itself written with Maneage (project commit eeff5de).},
	number = {April},
	journal = {Computing in Science and Engineering},
	author = {Akhlaghi, Mohammad and Infante-Sainz, Raul and Roukema, Boudewijn and Khellat, Mohammadreza and Valls-Gabaud, David and Galle, Roberto Baena},
	year = {2021},
	keywords = {Buildings, Containers, Kernel, Libraries, Software, Tools, Virtual machining},
}

@article{oberhauser_software_2015,
	title = {From {Software} {Engineering} {Process} {Models} to {Operationally} {Relevant} {Context}-aware {Workflows} : {A} {Model}-{Driven} {Method}},
	volume = {8},
	number = {1},
	author = {Oberhauser, Roy},
	year = {2015},
	keywords = {- process-centered software engineering, engineering process modeling, environments, model transformation, software, software engineering environments, software engineering process, spem, uma, unified method},
	pages = {167--181},
}

@article{meyer_software_2001,
	title = {Software engineering in the academy},
	volume = {34},
	issn = {00189162},
	doi = {10.1109/2.920608},
	number = {5},
	journal = {Computer},
	author = {Meyer, Bertrand},
	year = {2001},
	pages = {28},
}

@article{grambow_towards_2011,
	title = {Towards a workflow language for software engineering},
	doi = {10.2316/P.2011.720-020},
	abstract = {Software development processes are broadly used by software providers to ensure the quality and reproducibility of their development endeavors. These processes are typically abstractly defined, individually interpreted by individuals, and manually executed, making governance and compliance difficult. Additionally, process tailoring, reuse, exchange, and any IT-based automation or guidance at the more practical lower level workflows is hindered or more burdensome without a common language for expression. Automated guidance and highly integrated process support holds potential for retaining process-centered advantages while reducing hindrances. In this paper, work on a language for the description of software engineering processes is presented. It unifies the abstract specification and documentation of processes with automated process enactment support, while, in turn, fostering reusability and tailoring of these processes. For enactment, various workflow management systems can be chosen whose models are automatically generated. The approach shows promise for enabling IT process support in the software engineering domain while supporting the exchange and objective comparison of enactable processes and practices.},
	journal = {Proceedings of the 10th IASTED International Conference on Software Engineering, SE 2011},
	author = {Grambow, Gregor and Oberhauser, Roy and Reichert, Manfred},
	year = {2011},
	note = {ISBN: 9780889868649},
	keywords = {Process enactment, Process language, Process modeling, Process reuse, Process-centered software engineering, Software engineering environments, Workflow management},
	pages = {130--137},
}

@article{salayandia_model-based_2006,
	title = {A {Model}-{Based} {Workflow} {Approach} for {Scientific} {Applications}},
	journal = {Proceedings of the 6th OOPSLA Workshop on Domain-Specific Modeling},
	author = {Salayandia, Leonardo and Silva, Paulo Pinheiro Da and Gates, Ann Q. and Rebellon, Alvaro},
	year = {2006},
}

@book{shull_guide_2008,
	title = {Guide to advanced empirical software engineering},
	isbn = {978-1-84800-043-8},
	abstract = {Empirical studies have become an integral element of software engineering research and practice. This unique text/reference includes chapters from some of the top international empirical software engineering researchers and focuses on the practical knowledge necessary for conducting, reporting and using empirical methods in software engineering. Part 1, 'Research Methods and Techniques', examines the proper use of various strategies for collecting and analysing data, and the uses for which those strategies are most appropriate. Part 2, 'Practical Foundations', provides a discussion of several important global issues that need to be considered from the very beginning of research planning. Finally, 'Knowledge Creation' offers insight on using a set of disparate studies to provide useful decision support. Topics and features: Offers information across a range of techniques, methods, and qualitative and quantitative issues, providing a toolkit for the reader that is applicable across the diversity of software development contexts Presents reference material with concrete software engineering examples Provides guidance on how to design, conduct, analyse, interpret and report empirical studies, taking into account the common difficulties and challenges encountered in the field Arms researchers with the information necessary to avoid fundamental risks Tackles appropriate techniques for addressing disparate studies - ensuring the relevance of empirical software engineering, and showing its practical impact Describes methods that are less often used in the field, providing less conventional but still rigorous and useful ways of collecting data Supplies detailed information on topics (such as surveys) that often contain methodological errors This broad-ranging, practical guide will prove an invaluable and useful reference for practising software engineers and researchers. In addition, it will be suitable for graduate students studying empirical methods in software development. © Springer-Verlag London Limited 2008.},
	author = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	year = {2008},
	doi = {10.1007/978-1-84800-044-5},
	note = {Publication Title: Guide to Advanced Empirical Software Engineering},
}

@article{rochaeli_using_2007,
	title = {Using patterns paradigm to refine workflow policies},
	issn = {15294188},
	doi = {10.1109/DEXA.2007.63},
	abstract = {We propose an approach to formalize the patterns and to automatically apply the formalized patterns. In our case, we use the pattern paradigm to refine security policies of a workflow. A policy refinement process derives low-level workflow policies from high-level and abstract policies specified by stakeholders. Such refinement process requires domain-specific expertise knowledge, which will be captured by using the pattern paradigm. These refinement patterns are formalized by using both description logic and temporal logic formalisms. © 2007 IEEE.},
	journal = {Proceedings - International Workshop on Database and Expert Systems Applications, DEXA},
	author = {Rochaeli, Taufiq and Eckert, Claudia},
	year = {2007},
	note = {ISBN: 0769529321},
	pages = {760--764},
}

@article{godoi_final_2021,
	title = {Final report: {National} {Security} {Commission} on {Artificial} {Intelligence}},
	abstract = {National Security Commission on Artificial Intelligence\_Full-Report-Digital-1},
	journal = {Public},
	author = {Godoi, Fernanda C. and Prakash, Sangeeta and Bhandari, Bhesh R.},
	year = {2021},
	note = {ISBN: 9789221336228},
}

@article{thiruvathukal_software_2013,
	title = {Software {Engineering} {Need} not be {Difficult}},
	url = {http://dx.doi.org/10.6084/m9.figshare.830442},
	abstract = {Position paper on how to apply the principles of software engineering and agile methods to support sustainable scientific software. Accepted at the WSSSPE workshop at SC 13.},
	journal = {Workshop on Sustainable Software for Science: Practice and Experiences at SC13},
	author = {Thiruvathukal, G. K. and J, Carver},
	year = {2013},
	pages = {1--6},
}

@article{herrmannsfeldt_1_1990,
	title = {1. {Introducti} o n},
	author = {Herrmannsfeldt, W. B.},
	year = {1990},
	pages = {1--27},
}

@article{iot__nodate,
	title = {“ {If} security is required ”: {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	author = {Iot, Ml-enabled and Iot, Ml-based and Iot, Ml-based and Ml, The},
}

@article{michael_regexes_2019,
	title = {Regexes are hard: {Decision}-making, difficulties, and risks in programming regular expressions},
	doi = {10.1109/ASE.2019.00047},
	abstract = {Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.},
	journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
	author = {Michael, Louis G. and Donohue, James and Davis, James C. and Lee, Dongyoon and Servant, Francisco},
	year = {2019},
	note = {ISBN: 9781728125084},
	keywords = {Developer process, Qualitative research, Regular expressions},
	pages = {415--426},
}

@article{approach_fault-tolerant_1985,
	title = {to {Fault}-{Tolerant} {Software}},
	number = {12},
	author = {Approach, N.-version},
	year = {1985},
	pages = {1491--1501},
}

@article{blackford_updated_2002,
	title = {An {Updated} {Set} of {Basic} {Linear} {Algebra} {Subprograms} ({BLAS})},
	volume = {28},
	issn = {00983500},
	doi = {10.1145/567806.567807},
	number = {2},
	journal = {ACM Transactions on Mathematical Software},
	author = {Blackford, L. Susan and Demmel, James and Dongarra, Jack and Duff, Iain and Hammarling, Sven and Henry, Greg and Heroux, Michael and Kaufman, Linda and Lumsdaine, Andrew and Petitet, Antoine and Pozo, Roldan and Remington, Karin and Whaley, R. Clint},
	year = {2002},
	keywords = {Algorithms, BLAS, G.1.3 [Numerical Analysis]: Numerical Linear Algeb, G.4 [Mathematical Software], Linear algebra, Standardization, Standards},
	pages = {135--151},
}

@article{goswami_investigating_2020,
	title = {Investigating the {Reproducibility} of {NPM} {Packages}},
	doi = {10.1109/ICSME46990.2020.00071},
	abstract = {Node.js has been popularly used for web application development, partially because of its large software ecosystem known as NPM (Node Package Manager) packages. When using open-source NPM packages, most developers download prebuilt packages on npmjs.com instead of building those packages from available source, and implicitly trust the downloaded packages. However, it is unknown whether the blindly trusted prebuilt NPM packages are reproducible (i.e., whether there is always a verifiable path from source code to any published NPM package). Therefore, for this paper, we conducted an empirical study to examine the reproducibility of NPM packages, and to understand why some packages are not reproducible.Specifically, we downloaded versions/releases of 226 most popularly used NPM packages and then built each version with the available source on GitHub. Next, we applied a differencing tool to compare the versions we built against versions downloaded from NPM, and further inspected any reported difference. Among the 3,390 versions of the 226 packages, only 2,087 versions are reproducible. Based on our manual analysis, multiple factors contribute to the non-reproducibility issues, such as flexible versioning information in package.json file and the divergent behaviors between distinct versions of tools used in the build process. Our investigation reveals challenges of verifying NPM reproducibility with existing tools, and provides insights for future verifiable build procedures.},
	journal = {Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020},
	author = {Goswami, Pronnoy and Gupta, Saksham and Li, Zhiyuan and Meng, Na and Yao, Daphne},
	year = {2020},
	note = {ISBN: 9781728156194},
	keywords = {JavaScript, NPM packages, reproducibility},
	pages = {677--681},
}

@article{nascimento_software_2020,
	title = {Software engineering for artificial intelligence and machine learning software: {A} systematic literature review},
	url = {http://arxiv.org/abs/2011.03751},
	abstract = {Artificial Intelligence (AI) or Machine Learning (ML) systems have been widely adopted as value propositions by companies in all industries in order to create or extend the services and products they offer. However, developing AI/ML systems has presented several engineering problems that are different from those that arise in, non-AI/ML software development. This study aims to investigate how software engineering (SE) has been applied in the development of AI/ML systems and identify challenges and practices that are applicable and determine whether they meet the needs of professionals. Also, we assessed whether these SE practices apply to different contexts, and in which areas they may be applicable. We conducted a systematic review of literature from 1990 to 2019 to (i) understand and summarize the current state of the art in this field and (ii) analyze its limitations and open challenges that will drive future research. Our results show these systems are developed on a lab context or a large company and followed a research-driven development process. The main challenges faced by professionals are in areas of testing, AI software quality, and data management. The contribution types of most of the proposed SE practices are guidelines, lessons learned, and tools.},
	author = {Nascimento, Elizamary and Nguyen-Duc, Anh and Sundbø, Ingrid and Conte, Tayana},
	year = {2020},
	keywords = {artificial intelligence, machine learning, practices, software engineering},
}

@article{shetty_neural_2021,
	title = {Neural {Knowledge} {Extraction} {From} {Cloud} {Service} {Incidents}},
	doi = {10.1109/icse-seip52600.2021.00031},
	abstract = {In the last decade, two paradigm shifts have reshaped the software industry - the move from boxed products to services and the widespread adoption of cloud computing. This has had a huge impact on the software development life cycle and the DevOps processes. Particularly, incident management has become critical for developing and operating large-scale services. Incidents are created to ensure timely communication of service issues and, also, their resolution. Prior work on incident management has been heavily focused on the challenges with incident triaging and de-duplication. In this work, we address the fundamental problem of structured knowledge extraction from service incidents. We have built SoftNER, a framework for unsupervised knowledge extraction from service incidents. We frame the knowledge extraction problem as a Named-entity Recognition task for extracting factual information. SoftNER leverages structural patterns like key,value pairs and tables for bootstrapping the training data. Further, we build a novel multi-task learning based BiLSTM-CRF model which leverages not just the semantic context but also the data-types for named-entity extraction. We have deployed SoftNER at Microsoft, a major cloud service provider and have evaluated it on more than 2 months of cloud incidents. We show that the unsupervised machine learning based approach has a high precision of 0.96. Our multi-task learning based deep learning model also outperforms the state of the art NER models. Lastly, using the knowledge extracted by SoftNER we are able to build significantly more accurate models for important downstream tasks like incident triaging.},
	author = {Shetty, Manish and Bansal, Chetan and Kumar, Sumit and Rao, Nikitha and Nagappan, Nachiappan and Zimmermann, Thomas},
	year = {2021},
	keywords = {all or part of, cloud services, deep, knowledge extraction, learning, machine learning, or, or hard copies of, permission to make digital, service incidents, this work for personal},
	pages = {218--227},
}

@article{nguyen_consistent_1994,
	title = {The {Consistent} {Comparison} {Problem} in {N}-{Version} {Software}},
	volume = {6},
	issn = {10414347},
	doi = {10.1109/69.334887},
	abstract = {Knowledge base is the most important component in a knowledge-based system. Because a knowledge base is often built in an incremental, piecemeal fashion, potential errors may be inadvertently brought into it. One of the critical issues in developing reliable knowledge-based systems is how to verify the correctness of a knowledge base. This concise paper describes an automated tool called PREPARE for detecting potential errors in a knowledge base. PREPARE is based on modeling a knowledge base by using a Predicate/Transition net representation. Inconsistent, redundant, subsumed, circular, and incomplete rules in a knowledge base are then defined as patterns of the Predicate/Transition net model, and are detected through a syntactic pattern recognition method. The research results to date have indicated that 1) the methodology can be adopted in knowledge-based systems where logic is used as knowledge representation formalism; 2) the tool can be invoked at any stage of the system’s development, even without a fully functioning inference engine; 3) the Predicate/Transition net model of knowledge bases is easy to implement and provides a clear and understandable display of the knowledge to be used by the system. © 1994 IEEE},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Nguyen, Doan and Zhang, D.},
	year = {1994},
	keywords = {Knowledge base, Predicate/Transition nets, circularity and incompleteness, inconsistency, redundancy, syntactic pattern recognition, verification},
	pages = {983--989},
}

@article{knight_experimental_1986,
	title = {An {Experimental} {Evaluation} of the {Assumption} of {Independence} in {Multiversion} {Programming}},
	volume = {SE-12},
	issn = {00985589},
	doi = {10.1109/TSE.1986.6312924},
	abstract = {N-version programming has been proposed as a method of incorporating fault tolerance into software. Multiple versions of a program (i.e., “N”) are prepared and executed in parallel. Their outputs are collected and examined by a voter, and, if they are not identical, it is assumed that the majority is correct. This method depends for its reliability improvement on the assumption that programs that have been developed independently will fail independently. In this paper an experiment is described in which the fundamental axiom is tested. A total of 27 versions of a program were prepared independently from the same specification at two universities and then subjected to one million tests. The results of the tests revealed that the programs were individually extremely reliable but that the number of tests in which more than one program failed was substantially more than expected. The results of these tests are presented along with an analysis of some of the faults that were found in the programs. Background information on the programmers used is also summarized. The conclusion from this experiment is that N-version programming must be used with care and that analysis of its reliability must include the effect of dependent errors. © 1986 IEEE},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Knight, John C. and Leveson, Nancy G.},
	year = {1986},
	note = {Publisher: IEEE},
	keywords = {Design diversity, N-version programming, fault-tolerant software, multiversion, programming, software reliability},
	pages = {96--109},
}

@article{thiruvathukal_how_2020,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	number = {1},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
	year = {2020},
}

@article{sangeetha_deep_2006,
	title = {Deep {Residual} {Learning} for {Image} {Recognition} {Kaiming}},
	volume = {45},
	issn = {03764699},
	doi = {10.1002/chin.200650130},
	abstract = {An elegant one pot syntheses of the titled compounds 3a-d, 4a-d and 6a-d have been presented starting from 1-hydroxycarbazole-2-carbaldehydes 2a-d in good yields. Treatment of 1-hydroxycarbazole-2-carbaldehydes 2a-d with chloroacetone and with o-aminothiophenol have afforded the novel 2-acetylfuro[2,3-a]carbazoles 3a-d and benzo-[1,2-b]-1,4-thiazepino[2,3-a] carbazoles 4a-d respectively. Further carbazoles 2a-d are treated with phenyl acetic acid in an attempt to synthesize 3-phenyl-2-oxopyrano[2,3-a]-carbazoles 5a-d, but the reaction did not proceed in the anticipated direction and only acetyl derivatives 6a-d are obtained. All the products thus obtained from these reactions are well characterized by spectroscopic and analytical data.},
	number = {8},
	journal = {Indian Journal of Chemistry - Section B Organic and Medicinal Chemistry},
	author = {Sangeetha, V. and Prasad, K. J. Rajendra},
	year = {2006},
	keywords = {1-hydroxycarbazole-2-carbaldehydes, 2-acetylfuro carbazoles, Benzo carbazoles, Phenyl oxopyranocarbazoles, o-aminothiophenol},
	pages = {1951--1954},
}

@article{plastiras_you_2018,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	doi = {10.1145/3243394.3243692},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to per- form detection. Instead, we frame object detection as a re- gression problem to spatially separated bounding boxes and associated class probabilities. A single neural network pre- dicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detec- tors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations ofobjects. It outperforms other de- tection methods, including DPM and R-CNN, when gener- alizing from natural images to other domains like artwork.},
	journal = {ACM International Conference Proceeding Series},
	author = {Plastiras, George and Kyrkou, Christos and Theocharides, Theocharis},
	year = {2018},
	note = {ISBN: 9781450365116},
}

@article{theisen_writing_2017,
	title = {Writing good software engineering research papers: {Revisited}},
	doi = {10.1109/ICSE-C.2017.51},
	abstract = {With the goal of helping software engineering researchers understand how to improve their papers, Mary Shaw presented 'Writing Good Software Engineering Research Papers' in 2003. Shaw analyzed the abstracts of the papers submitted to the 2002 International Conference of Software Engineering (ICSE) to determine trends in research question type, contribution type, and validation approach. We revisit Shaw's work to see how the software engineering research community has evolved since 2002. The goal of this paper is to aid software engineering researchers in understanding trends in research question design, research question type, and validation approach by analyzing the abstracts of the papers submitted to ICSE 2016. We implemented Shaw's recommendation for replicating her study through the use of multiple coders and the calculation of inter-rater reliability and demonstrate that her approach can be repeated. Our results indicate that reviewers have increased expectations that papers have solid evaluations of the research contribution. Additionally, the 2016 results include at least 17\% mining software repository (MSR) papers, a category of papers not seen in 2002. The advent of MSR papers has increased the use of generalization/characterization research questions, the production of empirical report contribution, and validation by evaluation.},
	journal = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
	author = {Theisen, Christopher and Dunaiski, Marcel and Williams, Laurie and Visser, Willem},
	year = {2017},
	note = {ISBN: 9781538615898
Publisher: IEEE},
	keywords = {Abstracts, Guidelines, Research, Writing},
	pages = {402},
}

@article{barba_reproducible_2017,
	title = {Reproducible {Research} for {Computing} in {Science} \& {Engineering}},
	volume = {19},
	issn = {15219615},
	doi = {10.1109/MCSE.2017.3971172},
	abstract = {The editors of the new track for reproducible research outline the parameters for future peer review, submission, and access, highlighting the magazine's previous work in this field and some of the challenges still to come.},
	number = {6},
	journal = {Computing in Science and Engineering},
	author = {Barba, Lorena A. and Thiruvathukal, George K.},
	year = {2017},
	keywords = {open access, peer review, reproducible research, scientific computing},
	pages = {85--87},
}

@article{thiruvathukal_how_nodate,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	number = {2},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{thiruvathukal_how_nodate-1,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	number = {1},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
	pages = {24--26},
}

@article{theisen_writing_2017,
	title = {Writing good software engineering research papers: {Revisited}},
	doi = {10.1109/ICSE-C.2017.51},
	abstract = {With the goal of helping software engineering researchers understand how to improve their papers, Mary Shaw presented 'Writing Good Software Engineering Research Papers' in 2003. Shaw analyzed the abstracts of the papers submitted to the 2002 International Conference of Software Engineering (ICSE) to determine trends in research question type, contribution type, and validation approach. We revisit Shaw's work to see how the software engineering research community has evolved since 2002. The goal of this paper is to aid software engineering researchers in understanding trends in research question design, research question type, and validation approach by analyzing the abstracts of the papers submitted to ICSE 2016. We implemented Shaw's recommendation for replicating her study through the use of multiple coders and the calculation of inter-rater reliability and demonstrate that her approach can be repeated. Our results indicate that reviewers have increased expectations that papers have solid evaluations of the research contribution. Additionally, the 2016 results include at least 17\% mining software repository (MSR) papers, a category of papers not seen in 2002. The advent of MSR papers has increased the use of generalization/characterization research questions, the production of empirical report contribution, and validation by evaluation.},
	journal = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
	author = {Theisen, Christopher and Dunaiski, Marcel and Williams, Laurie and Visser, Willem},
	year = {2017},
	note = {ISBN: 9781538615898},
	keywords = {Abstracts, Guidelines, Research, Writing},
	pages = {402},
}

@article{noauthor_orthogonal_1992,
	title = {Orthogonal defect classification-a concept for in-process measurements},
	abstract = {By the early 1990s the need for reengineering legacy systems was already acute, but recently the demand has increased significantly with the shift toward web-based user interfaces. The demand by all business sectors to adapt their information systems to the Web has created a tremendous need for methods, tools, and infrastructures to evolve and exploit existing applications efficiently and cost-effectively. Reverse engineering has been heralded as one of the most promising technologies to combat this.legacy systems problem. This paper presents a roadmap for reverse engineering research for the first decade of the new millennium, building on the program comprehension theories of the 1980s and the reverse engineering technology of the 1990s.},
	year = {1992},
}

@article{thiruvathukal_how_nodate-2,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	number = {2},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{noauthor_understanding_nodate,
	title = {Understanding and {Detecting} {Performance} {Bugs} in {Markdown} {Compilers}},
}

@article{noauthor_empirical_nodate,
	title = {An {Empirical} {Study} on {Information} {Discrepancies} across {Heterogeneous} {Vulnerability} {Databases}},
}

@article{lagouvardos_static_2020,
	title = {Static {Analysis} of {Shape} in {TensorFlow} {Programs}},
	volume = {6},
	abstract = {Machine learning has been widely adopted in diverse science and engineering domains, aided by reusable libraries and quick development patterns. The TensorFlow library is probably the best- known representative of this trend and most users employ the Python API to its powerful back-end. TensorFlow programs are susceptible to several systematic errors, especially in the dynamic typing setting of Python. We present Pythia, a static analysis that tracks the shapes of tensors across Python library calls and warns of several possible mismatches. The key technical aspects are a close modeling of library semantics with respect to tensor shape, and an identification of violations and error-prone patterns. Pythia is powerful enough to statically detect (with 84.62\% precision) 11 of the 14 shape-related TensorFlow bugs in the recent Zhang et al. empirical study—an independent slice of real-world bugs.},
	number = {15},
	journal = {34th European Conference on Object-Oriented Programming (ECOOP 2020).},
	author = {Lagouvardos, Sifis and Dolby, Julian and Grech, Neville},
	year = {2020},
	keywords = {Doop, Python, TensorFlow, Wala, static analysis},
	pages = {1--15},
}

@article{verma_shapeflow_2020,
	title = {{ShapeFlow}: {Dynamic} shape interpreter for {TensorFlow}},
	issn = {23318422},
	abstract = {We present ShapeFlow, a dynamic abstract interpreter for TensorFlow which quickly catches tensor shape incompatibility errors, one of the most common bugs in deep learning code. ShapeFlow shares the same APIs as TensorFlow but only captures and emits tensor shapes, its abstract domain. ShapeFlow constructs a custom shape computational graph, similar to the computational graph used by TensorFlow. ShapeFlow requires no code annotation or code modification by the programmer, and therefore is convenient to use. We evaluate ShapeFlow on 52 programs collected by prior empirical studies to show how fast and accurately it can catch shape incompatibility errors compared to TensorFlow. We use two baselines: a worst-case training dataset size and a more realistic dataset size. ShapeFlow detects shape incompatibility errors highly accurately — with no false positives and a single false negative — and highly efficiently —with an average speed-up of 499X and 24X for the first and second baseline, respectively. We believe ShapeFlow is a practical tool that benefits machine learning developers. We will open-source ShapeFlow on GitHub to make it publicly available to both the developer and research communities.},
	journal = {arXiv},
	author = {Verma, Sahil and Su, Zhendong},
	year = {2020},
}

@article{monat_static_2020,
	title = {Static {Type} {Analysis} by {Abstract} {Interpretation} of {Python} {Programs}},
	number = {17},
	author = {Monat, Raphaël and Miné, Antoine},
	year = {2020},
	keywords = {17, 2020, 4230, abstract interpretation, acknowledgements we thank the, and phrases formal methods, anonymous reviewers for their, digital object identifier 10, dynamic programming language, ecoop, lipics, python semantics, static analysis, type analysis, valuable comments and feedback},
	pages = {1--17},
}

@article{miao_towards_2017,
	title = {Towards unified data and lifecycle management for deep learning},
	issn = {10844627},
	doi = {10.1109/ICDE.2017.112},
	abstract = {Deep learning has improved state-of-The-Art results in many important fields, and has been the subject of much research in recent years, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of data management, model sharing, and lifecycle management are largely ignored. Deep learning modeling lifecycle generates a rich set of data artifacts, e.g., learned parameters and training logs, and it comprises of several frequently conducted tasks, e.g., to understand the model behaviors and to try out new models. Dealing with such artifacts and tasks is cumbersome and largely left to the users. This paper describes our vision and implementation of a data and lifecycle management system for deep learning. First, we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers, and propose a high-level domain specific language (DSL), inspired by SQL, to raise the abstraction level and thereby accelerate the modeling process. To manage the variety of data artifacts, especially the large amount of checkpointed float parameters, we design a novel model versioning system (dlv), and a read-optimized parameter archival storage system (PAS) that minimizes storage footprint and accelerates query workloads with minimal loss of accuracy. PAS archives versioned models using deltas in a multi-resolution fashion by separately storing the less significant bits, and features a novel progressive query (inference) evaluation algorithm. Third, we develop efficient algorithms for archiving versioned models using deltas under co-retrieval constraints. We conduct extensive experiments over several real datasets from computer vision domain to show the efficiency of the proposed techniques.},
	journal = {Proceedings - International Conference on Data Engineering},
	author = {Miao, Hui and Li, Ang and Davis, Larry S. and Deshpande, Amol},
	year = {2017},
	note = {ISBN: 9781509065431
Publisher: IEEE},
	pages = {571--582},
}

@article{yang_deep_2017,
	title = {Deep learning for fixed model reuse},
	abstract = {Model reuse attempts to construct a model by utilizing existing available models, mostly trained for other tasks, rather than building a model from scratch. It is helpful to reduce the time cost, data amount, and expertise required. Deep learning has achieved great success in various tasks involving images, voices and videos. There are several studies have the sense of model reuse, by trying to reuse pre-trained deep networks architectures or deep model features to train a new deep model. They, however, neglect the fact that there are many other fixed models or features available. In this paper, we propose a more thorough model reuse scheme, FMR (Fixed Model Reuse). FMR utilizes the learning power of deep models to implicitly grab the useful discriminative information from fixed model/features that have been widely used in general tasks. We firstly arrange the convolution layers of a deep network and the provided fixed model/features in parallel, fully connecting to the output layer nodes. Then, the dependencies between the output layer nodes and the fixed model/features are knockdown such that only the raw feature inputs are needed when the model is being used for testing, though the helpful information in the fixed model/features have already been incorporated into the model. On one hand, by the FMR scheme, the required amount of training data can be significantly reduced because of the reuse of fixed model/features. On the other hand, the fixed model/features are not explicitly used in testing, and thus, the scheme can be quite useful in applications where the fixed model/features are protected by patents or commercial secrets. Experiments on five real-world datasets validate the effectiveness of FMR compared with state-of-the-art deep methods.},
	journal = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
	author = {Yang, Yang and Zhan, De Chuan and Fan, Ying and Jiang, Yuan and Zhou, Zhi Hua},
	year = {2017},
	keywords = {Machine Learning Methods},
	pages = {2831--2837},
}

@article{zhou_learnware_2016,
	title = {Learnware: on the future of machine learning},
	volume = {10},
	issn = {20952236},
	doi = {10.1007/s11704-016-6906-3},
	number = {4},
	journal = {Frontiers of Computer Science},
	author = {Zhou, Zhi Hua},
	year = {2016},
	pages = {589--590},
}

@article{monat_static_2020-1,
	title = {Static {Type} {Analysis} by {Abstract} {Interpretation} of {Python} {Programs}},
	volume = {6},
	number = {17},
	author = {Monat, Raphaël and Miné, Antoine},
	year = {2020},
	keywords = {17, 2020, 4230, abstract interpretation, acknowledgements we thank the, and phrases formal methods, anonymous reviewers for their, digital object identifier 10, dynamic programming language, ecoop, lipics, python semantics, static analysis, type analysis, valuable comments and feedback},
	pages = {1--17},
}

@article{wu_model_2017,
	title = {Model reuse with domain knowledge},
	volume = {47},
	issn = {1674-7267},
	doi = {10.1360/n112017-00106},
	number = {11},
	journal = {SCIENTIA SINICA Informationis},
	author = {WU, Xizhu and ZHOU, Zhihua},
	year = {2017},
	pages = {1483--1492},
}

@article{allamanis_mining_2014,
	title = {Mining idioms from source code},
	volume = {16-21-Nove},
	doi = {10.1145/2635868.2635901},
	abstract = {We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic purpose. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q\&A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management.},
	journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
	author = {Allamanis, Miltiadis and Sutton, Charles},
	year = {2014},
	note = {ISBN: 9781450330565},
	keywords = {Code idioms, Naturalness of source code, Syntactic code patterns},
	pages = {472--483},
}

@article{zhang_end--end_2019,
	title = {An end-to-end automatic cloud database tuning system using deep reinforcement learning},
	issn = {07308078},
	doi = {10.1145/3299869.3300085},
	abstract = {Configuration tuning is vital to optimize the performance of database management system (DBMS). It becomes more tedious and urgent for cloud databases (CDB) due to the diverse database instances and query workloads, which make the database administrator (DBA) incompetent. Although there are some studies on automatic DBMS configuration tuning, they have several limitations. Firstly, they adopt a pipelined learning model but cannot optimize the overall performance in an end-to-end manner. Secondly, they rely on large-scale high-quality training samples which are hard to obtain. Thirdly, there are a large number of knobs that are in continuous space and have unseen dependencies, and they cannot recommend reasonable configurations in such high-dimensional continuous space. Lastly, in cloud environment, they can hardly cope with the changes of hardware configurations and workloads, and have poor adaptability. To address these challenges, we design an end-to-end automatic CDB tuning system, CDBTune, using deep reinforcement learning (RL). CDBTune utilizes the deep deterministic policy gradient method to find the optimal configurations in high-dimensional continuous space. CDBTune adopts a try-and-error strategy to learn knob settings with a limited number of samples to accomplish the initial training, which alleviates the difficulty of collecting massive high-quality samples. CDBTune adopts the reward-feedback mechanism in RL instead of traditional regression, which enables end-to-end learning and accelerates the convergence speed of our model and improves efficiency of online tuning. We conducted extensive experiments under 6 different workloads on real cloud databases to demonstrate the superiority of CDBTune. Experimental results showed that CDBTune had a good adaptability and significantly outperformed the state-of-the-art tuning tools and DBA experts.},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	author = {Zhang, Ji and Liu, Yu and Zhou, Ke and Li, Guoliang and Xiao, Zhili and Cheng, Bin and Xing, Jiashu and Wang, Yangtao and Cheng, Tianheng and Liu, Li and Ran, Minwei and Li, Zekang},
	year = {2019},
	note = {ISBN: 9781450356435},
	pages = {415--432},
}

@article{grossner_new_2021,
	title = {New {Compute} {Ecosystem} from {Cloud} to {Edge} {Report} - 2021},
	author = {Grossner, Cliff},
	year = {2021},
}

@article{granger_python_2011,
	title = {Python : {An} {Ecosystem}},
	author = {Granger, Brian E. and Hunter, John D.},
	year = {2011},
	pages = {13--21},
}

@article{margolis_institutionalized_2001,
	title = {Institutionalized older adults in a health district in the {United} {Arab} {Emirates}: {Health} status and utilization rate},
	volume = {47},
	issn = {0304324X},
	doi = {10.1159/000052791},
	abstract = {Background: Little is known about the rate of institutionalization and health status of nursing home (NH) type patients living in the Middle East. This study was set in the Al-Ain Medical District, a geographically discrete region of the United Arab Emirates, a country with a developing economy located on the shores of the Arabian Gulf. NH-type patients were defined as people aged 60 years and older who were admitted to a hospital or a long-term institutionalized setting for at least 6 weeks and with no evidence of an expectation of discharge at the time of the evaluation. Objective: To determine the clinical, functional, cognitive, and nutritional status of NH-type patients living in a defined community within a developing country. Method: Cross-sectional survey. Results: All NH-type patients were identified, and all were included in this study (n = 47, 100\% participation rate). All were located within three public institutions, none of which was a dedicated NH facility. The rate of institutionalization was 7.0-14.0 per 1,000 people aged 65 or older. The age distribution was 30\% (60-74 years), 49\% (75-84 years), and 21\% (85+ years). The length of stay was 3.8 years. The female:male ratio was 1.6. All except 1 had a neurological disorder, and 89\% had dementia. The cognitive deficits were severe with only 61\% alert, 41\% able to speak, 17\% orientated in place, and 15\% orientated in time. The functional status was also poor: 98\% received assistance with all instrumental activities of daily living, 85\% received assistance with five activities of daily living, and 94\% were bed bound. The nutritional status was also impaired with a mean body weight of 45 ± 14 kg and a mean albumin level of 3.1 ± 0.6 g/dl. When compared with the USA data from the National Center for Health Statistics, the study population was younger, had a longer length of stay, a lower female:male ratio, a higher rate of neurological diseases and dementia, and were far more dependent and disorientated. The rate of institutionalization was one sixth to one third of that in the USA. Conclusion: From these data we concluded that this region has a distinctly different population of institutionalized older people who demonstrate greater impairments in all domains of health status. Copyright © 2001 S. Karger AG, Basel.},
	number = {3},
	journal = {Gerontology},
	author = {Margolis, Stephen A. and Reed, Richard L.},
	year = {2001},
	pmid = {11340323},
	keywords = {Developing countries, Nursing homes, Older people, institutionalization},
	pages = {161--167},
}

@article{this_rules_2021,
	title = {Rules of {Machine} {Learning} : {Best} {Practices} for {ML} {Engineering}},
	abstract = {This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine?learned model, then you have the necessary background to read this document. Terminology},
	author = {This, Martin Zinkevich and Guide, Style},
	year = {2021},
	pages = {1--28},
}

@article{molina_training_2019,
	title = {Training {Binary} {Classifiers} as {Data} {Structure} {Invariants}},
	volume = {2019-May},
	issn = {02705257},
	doi = {10.1109/ICSE.2019.00084},
	abstract = {We present a technique to distinguish valid from invalid data structure objects. The technique is based on building an artificial neural network, more precisely a binary classifier, and training it to identify valid and invalid instances of a data structure. The obtained classifier can then be used in place of the data structure's invariant, in order to attempt to identify (in)correct behaviors in programs manipulating the structure. In order to produce the valid objects to train the network, an assumed-correct set of object building routines is randomly executed. Invalid instances are produced by generating values for object fields that 'break' the collected valid values, i.e., that assign values to object fields that have not been observed as feasible in the assumed-correct executions that led to the collected valid instances. We experimentally assess this approach, over a benchmark of data structures. We show that this learning technique produces classifiers that achieve significantly better accuracy in classifying valid/invalid objects compared to a technique for dynamic invariant detection, and leads to improved bug finding.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Molina, Facundo and Degiovanni, Renzo and Ponzio, Pablo and Regis, German and Aguirre, Nazareno and Frias, Marcelo},
	year = {2019},
	note = {ISBN: 9781728108698},
	keywords = {Bug finding, Machine learning, Specification inference},
	pages = {759--770},
}

@article{freese_defining_2014,
	title = {Defining a cloud ecosystem},
	url = {https://www.ibm.com/downloads/cas/AP9YY90D},
	journal = {Ibm},
	author = {Freese, Bob},
	year = {2014},
}

@article{systems_cloud_nodate,
	title = {{THE} {CLOUD} {COMPUTING} {ECOSYSTEM} {Evolu0on} ( of ( {IT} (},
	author = {Systems, Backend},
}

@article{michael_regexes_2019,
	title = {Regexes are hard: {Decision}-making, difficulties, and risks in programming regular expressions},
	doi = {10.1109/ASE.2019.00047},
	abstract = {Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.},
	journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
	author = {Michael, Louis G. and Donohue, James and Davis, James C. and Lee, Dongyoon and Servant, Francisco},
	year = {2019},
	note = {ISBN: 9781728125084},
	keywords = {Developer process, Qualitative research, Regular expressions},
	pages = {415--426},
}

@article{park_systematic_2015,
	title = {Systematic {Testing} of {Reactive} {Software} with {Non}-{Deterministic} {Events}: {A} {Case} {Study} on {LG} {Electric} {Oven}},
	volume = {2},
	issn = {02705257},
	doi = {10.1109/ICSE.2015.132},
	abstract = {Most home appliance devices such as electric ovens are reactive systems which repeat receiving a user input/event through an event handler, updating their internal state based on the input, and generating outputs. A challenge to test a reactive program is to check if the program correctly reacts to various non-deterministic sequence of events because an unexpected sequence of events may make the system fail due to the race conditions between the main loop and asynchronous event handlers. Thus, it is important to systematically generate/test various sequences of events by controlling the order of events and relative timing of event occurrences with respect to the main loop execution. In this paper, we report our industrial experience to solve the aforementioned problem by developing a systematic event generation framework based on concolic testing technique. We have applied the framework to a LG electric oven and detected several critical bugs including one that makes the oven ignore user inputs due to the illegal state transition.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Park, Yongbae and Hong, Shin and Kim, Moonzoo and Lee, Dongju and Cho, Junhee},
	year = {2015},
	note = {ISBN: 9781479919345
Publisher: IEEE},
	pages = {29--38},
}

@article{koziolek_openpnp_2019,
	title = {{OpenPnP}: {A} {Plug}-{And}-{Produce} {Architecture} for the {Industrial} {Internet} of {Things}},
	doi = {10.1109/ICSE-SEIP.2019.00022},
	abstract = {Industrial control systems are complex, software-intensive systems that manage mission-critical production processes.commissioning such systems requires installing, configuring, and integrating thousands of sensors, actuators, and controllers and is still a largely manual and costly process. Therefore, practitioners and researchers have been working on 'plug and produce' approaches that automate commissioning for more than 15 years, but have often focused on network discovery and proprietary technologies. We introduce the vendor-neutral OpenPnP reference architecture, which can largely automate the configuration and integration tasks for commissioning. Using an example implementation, we demonstrate that OpenPnP can reduce the configuration and integration effort up to 90 percent and scales up to tens of thousands of communicated signals per second for large Industrial Internet-of-Things (IIoT) systems. OpenPnP can serve as a template for practitioners implementing IIoT applications throughout the automation industry and streamline commissioning processes in many thousands of control system installations.},
	journal = {Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP 2019},
	author = {Koziolek, Heiko and Burger, Andreas and Platenius-Mohr, Marie and Ruckert, Julius and Stomberg, Gosta},
	year = {2019},
	note = {ISBN: 9781728117607},
	keywords = {Client-server systems, Control engineering, Internet of Things, Real-Time systems, Software architecture},
	pages = {131--140},
}

@article{davis_impact_2020,
	title = {On the {Impact} and {Defeat} of {Regular} {Expression} {Denial} of {Service} {On} the {Impact} and {Defeat} of {Regular} {Expression} {Denial} of {Service}},
	author = {Davis, James C. and Yao, Danfeng Daphne and Butt, Ali R. and Davis, James C.},
	year = {2020},
	keywords = {denial of service, empirical software engineering, redos, regular expressions},
}

@article{botchkarev_evaluating_2018,
	title = {Evaluating {Performance} of {Regression} {Machine} {Learning} {Models} {Using} {Multiple} {Error} {Metrics} in {Azure} {Machine} {Learning} {Studio}},
	issn = {1556-5068},
	doi = {10.2139/ssrn.3177507},
	abstract = {Data driven companies effectively use regression machine learning methods for making predictions in many sectors. Cloud-based Azure Machine Learning Studio (MLS) has a potential of expediting machine learning experiments by offering a convenient and powerful integrated development environment. The process of evaluating machine learning models in Azure MLS has certain limitations, e.g. small number of performance metrics and lack of functionality to evaluate custom built regression models with R language. This paper reports the results of an effort to build an Enhanced Evaluate Model (EEM) module which facilitates and accelerates Azure experiments development and evaluation. The EEM combines multiple performance metrics allowing for multisided evaluation of the regression models. EEM offers 4 times more metrics than built-in Azure Evaluate Model module. EEM metrics include: CoD, GMRAE, MAE, MAPE, MASE, MdAE, MdAPE, MdRAE, ME, MPE, MRAE, MSE, NRMSE\_mm, NRMSE\_sd, RAE, RMdSPE, RMSE, RMSPE, RSE, sMAPE, SMdAPE, SSE. Also, EEM supports evaluation of the R language based regression models. The operational Enhanced Evaluate Model module has been published to the web and openly available for experiments and extensions.},
	journal = {SSRN Electronic Journal},
	author = {Botchkarev, Alexei},
	year = {2018},
	keywords = {accuracy, azure machine learning studio, error, error measure, evaluation, forecast, machine learning, models, multiple types, performance metrics, prediction, r, regression},
	pages = {1--16},
}

@inproceedings{egziabher_performance_2013,
	title = {Performance {Metrics} ({Error} {Measures}) in {Machine} {Learning} {Regression}, {Forecasting} and {Prognostics}: {Properties} and {Typology}},
	volume = {53},
	isbn = {978-85-7811-079-6},
	abstract = {Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. The intention of this study was to overview of a variety of performance metrics and approaches to their classification. The main goal of the study was to develop a typology that will help to improve our knowledge and understanding of metrics and facilitate their selection in machine learning regression, forecasting and prognostics. Based on the analysis of the structure of numerous performance metrics, we propose a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set. The paper proposed a new primary metrics typology designed around the key metrics components. The suggested typology has been shown to cover most of the commonly used primary metrics – total of over 40. The main contribution of this paper is in ordering knowledge of performance metrics and enhancing understanding of their structure and properties by proposing a new typology, generic primary metrics mathematic formula and a visualization chart. Keywords:},
	booktitle = {Africa’s potential for the ecological intensification of agriculture},
	author = {Egziabher, Tewolde Berhan Gebre and Edwards, Sue},
	year = {2013},
	pmid = {25246403},
	note = {ISSN: 1098-6596
Issue: 9},
	keywords = {icle},
	pages = {1689--1699},
}

@article{rauber_foolbox_2017,
	title = {Foolbox: {A} {Python} toolbox to benchmark the robustness of machine learning models},
	issn = {23318422},
	abstract = {Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox. The most up-to-date documentation can be found at http://foolbox.readthedocs.io.},
	journal = {arXiv},
	author = {Rauber, Jonas and Brendel, Wieland and Bethge, Matthias},
	year = {2017},
}

@article{shami_evaluation_2007,
	title = {An evaluation of the robustness of existing supervised machine learning approaches to the classification of emotions in speech},
	volume = {49},
	issn = {01676393},
	doi = {10.1016/j.specom.2007.01.006},
	abstract = {In this study, the robustness of approaches to the automatic classification of emotions in speech is addressed. Among the many types of emotions that exist, two groups of emotions are considered, adult-to-adult acted vocal expressions of common types of emotions like happiness, sadness, and anger and adult-to-infant vocal expressions of affective intents also known as "motherese". Specifically, we estimate the generalization capability of two feature extraction approaches, the approach developed for Sony's robotic dog AIBO (AIBO) and the segment-based approach (SBA) of [Shami, M., Kamel, M., 2005. Segment-based approach to the recognition of emotions in speech. In: IEEE Conf. on Multimedia and Expo (ICME05), Amsterdam, The Netherlands]. Three machine learning approaches are considered, K-nearest neighbors (KNN), Support vector machines (SVM) and Ada-boosted decision trees and four emotional speech databases are employed, Kismet, BabyEars, Danish, and Berlin databases. Single corpus experiments show that the considered feature extraction approaches AIBO and SBA are competitive on the four databases considered and that their performance is comparable with previously published results on the same databases. The best choice of machine learning algorithm seems to depend on the feature extraction approach considered. Multi-corpus experiments are performed with the Kismet-BabyEars and the Danish-Berlin database pairs that contain parallel emotional classes. Automatic clustering of the emotional classes in the database pairs shows that the patterns behind the emotions in the Kismet-BabyEars pair are less database dependent than the patterns in the Danish-Berlin pair. In off-corpus testing the classifier is trained on one database of a pair and tested on the other. This provides little improvement over baseline classification. In integrated corpus testing, however, the classifier is machine learned on the merged databases and this gives promisingly robust classification results, which suggest that emotional corpora with parallel emotion classes recorded under different conditions can be used to construct a single classifier capable of distinguishing the emotions in the merged corpora. Such a classifier is more robust than a classifier learned on a single corpus as it can recognize more varied expressions of the same emotional classes. These findings suggest that the existing approaches for the classification of emotions in speech are efficient enough to handle larger amounts of training data without any reduction in classification accuracy. © 2007 Elsevier B.V. All rights reserved.},
	number = {3},
	journal = {Speech Communication},
	author = {Shami, Mohammad and Verhelst, Werner},
	year = {2007},
	keywords = {Analysis of intent, Emotion recognition, Speech processing, Vocal expressiveness},
	pages = {201--212},
}

@article{bhagoji_enhancing_2017,
	title = {Enhancing robustness of machine learning systems via data transformations},
	issn = {23318422},
	abstract = {We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis and data 'anti-whitening' to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.},
	journal = {arXiv},
	author = {Bhagoji, Arjun Nitin and Cullina, Daniel and Sitawarin, Chawin and Mittal, Prateek},
	year = {2017},
	note = {ISBN: 9781538605790
Publisher: IEEE},
	pages = {1--5},
}

@article{botchkarev_evaluating_2018-1,
	title = {Evaluating {Performance} of {Regression} {Machine} {Learning} {Models} {Using} {Multiple} {Error} {Metrics} in {Azure} {Machine} {Learning} {Studio}},
	issn = {1556-5068},
	doi = {10.2139/ssrn.3177507},
	abstract = {Data driven companies effectively use regression machine learning methods for making predictions in many sectors. Cloud-based Azure Machine Learning Studio (MLS) has a potential of expediting machine learning experiments by offering a convenient and powerful integrated development environment. The process of evaluating machine learning models in Azure MLS has certain limitations, e.g. small number of performance metrics and lack of functionality to evaluate custom built regression models with R language. This paper reports the results of an effort to build an Enhanced Evaluate Model (EEM) module which facilitates and accelerates Azure experiments development and evaluation. The EEM combines multiple performance metrics allowing for multisided evaluation of the regression models. EEM offers 4 times more metrics than built-in Azure Evaluate Model module. EEM metrics include: CoD, GMRAE, MAE, MAPE, MASE, MdAE, MdAPE, MdRAE, ME, MPE, MRAE, MSE, NRMSE\_mm, NRMSE\_sd, RAE, RMdSPE, RMSE, RMSPE, RSE, sMAPE, SMdAPE, SSE. Also, EEM supports evaluation of the R language based regression models. The operational Enhanced Evaluate Model module has been published to the web and openly available for experiments and extensions.},
	journal = {SSRN Electronic Journal},
	author = {Botchkarev, Alexei},
	year = {2018},
	keywords = {accuracy, azure machine learning studio, error, error measure, evaluation, forecast, machine learning, models, multiple types, performance metrics, prediction, r, regression},
	pages = {1--16},
}

@article{schelter_jenga-framework_2021,
	title = {{JENGA}-{A} {Framework} to {Study} the {Impact} of {Data} {Errors} on the {Predictions} of {Machine} {Learning} {Models}},
	url = {https://github.com/awslabs/deequ},
	abstract = {Machine learning (ML) is increasingly used to automate decision making in various domains. Almost all common ML models are susceptible to data errors in the serving data (for which the model makes predictions). Such errors frequently occur in practice , caused for example by program bugs in data preprocessing code or non-anticipated schema changes in external data sources. These errors can have devastating effects on the prediction quality of ML models and are, at the same time, hard to anticipate and capture. In order to empower data scientists to study the impact as well as mitigation techniques for data errors in ML models, we propose Jenga, a lightweight , open source experimentation library. Jenga allows its users to test their models for robustness against common data errors. Jenga contains an abstraction for prediction tasks based on a dataset and a model, an easily extendable set of synthethic data corruptions (e.g., for missing values, outliers, ty-pos and noisy measurements) as well as evaluation functionality to experiment with different data corruptions. Jenga supports researchers and practitioners in the difficult task of data validation for ML applications. As a showcase for this, we discuss two use cases of Jenga: studying the robustness of a model against incomplete data, as well as automatically stress testing integrity constraints for ML data expressed with tensorflow data validation.},
	author = {Schelter, Sebastian and Rukat, Tammo and Biessmann, Felix},
	year = {2021},
	note = {ISBN: 9783893180844},
}

@article{sehwag_analyzing_2019,
	title = {Analyzing the robustness of open-world machine learning},
	issn = {15437221},
	doi = {10.1145/3338501.335737},
	abstract = {When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing OOD adversarial examples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that OOD adversarial examples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.},
	journal = {Proceedings of the ACM Conference on Computer and Communications Security},
	author = {Sehwag, Vikash and Sitawarin, Chawin and Bhagoji, Arjun Nitin and Cullina, Daniel and Mittal, Prateek and Song, Liwei and Chiang, Mung},
	year = {2019},
	note = {ISBN: 9781450368339},
	keywords = {Adversarial example, Deep learning, Open world recognition},
	pages = {105--116},
}

@article{elmes_accounting_2020,
	title = {Accounting for training data error in machine learning applied to earth observations},
	volume = {12},
	issn = {20724292},
	doi = {10.3390/rs12061034},
	abstract = {Remote sensing, or Earth Observation (EO), is increasingly used to understand Earth system dynamics and create continuous and categorical maps of biophysical properties and land cover, especially based on recent advances in machine learning (ML). ML models typically require large, spatially explicit training datasets to make accurate predictions. Training data (TD) are typically generated by digitizing polygons on high spatial-resolution imagery, by collecting in situ data, or by using pre-existing datasets. TD are often assumed to accurately represent the truth, but in practice almost always have error, stemming from (1) sample design, and (2) sample collection errors. The latter is particularly relevant for image-interpreted TD, an increasingly commonly used method due to its practicality and the increasing training sample size requirements of modern ML algorithms. TD errors can cause substantial errors in the maps created using ML algorithms, which may impact map use and interpretation. Despite these potential errors and their real-world consequences for map-based decisions, TD error is often not accounted for or reported in EO research. Here we review the current practices for collecting and handling TD. We identify the sources of TD error, and illustrate their impacts using several case studies representing different EO applications (infrastructure mapping, global surface flux estimates, and agricultural monitoring), and provide guidelines for minimizing and accounting for TD errors. To harmonize terminology, we distinguish TD from three other classes of data that should be used to create and assess ML models: training reference data, used to assess the quality of TD during data generation; validation data, used to iteratively improve models; and map reference data, used only for final accuracy assessment. We focus primarily on TD, but our advice is generally applicable to all four classes, and we ground our review in established best practices for map accuracy assessment literature. EO researchers should start by determining the tolerable levels of map error and appropriate error metrics. Next, TD error should be minimized during sample design by choosing a representative spatio-temporal collection strategy, by using spatially and temporally relevant imagery and ancillary data sources during TD creation, and by selecting a set of legend definitions supported by the data. Furthermore, TD error can be minimized during the collection of individual samples by using consensus-based collection strategies, by directly comparing interpreted training observations against expert-generated training reference data to derive TD error metrics, and by providing image interpreters with thorough application-specific training. We strongly advise that TD error is incorporated in model outputs, either directly in bias and variance estimates or, at a minimum, by documenting the sources and implications of error. TD should be fully documented and made available via an open TD repository, allowing others to replicate and assess its use. To guide researchers in this process, we propose three tiers of TD error accounting standards. Finally, we advise researchers to clearly communicate the magnitude and impacts of TD error on map outputs, with specific consideration given to the likely map audience.},
	number = {6},
	journal = {Remote Sensing},
	author = {Elmes, Arthur and Alemohammad, Hamed and Avery, Ryan and Caylor, Kelly and Eastman, J. Ronald and Fishgold, Lewis and Friedl, Mark A. and Jain, Meha and Kohli, Divyani and Bayas, Juan Carlos Laso and Lunga, Dalton and McCarty, Jessica L. and Pontius, Robert Gilmore and Reinmann, Andrew B. and Rogan, John and Song, Lei and Stoynova, Hristiana and Ye, Su and Yi, Zhuang Fang and Estes, Lyndon},
	year = {2020},
	keywords = {Error propagation, Machine learning, Map accuracy, Training data},
	pages = {1--39},
}

@article{didona_enhancing_2015,
	title = {Enhancing performance prediction robustness by combining analytical modeling and machine learning},
	doi = {10.1145/2668930.2688047},
	abstract = {Classical approaches to performance prediction rely on two, typically antithetic, techniques: Machine Learning (ML) and Analytical Modeling (AM). ML takes a black box ap- proach, whose accuracy strongly depends on the represen- tativeness of the dataset used during the initial training phase. Specifically, it can achieve very good accuracy in areas of the features' space that have been sufficiently ex- plored during the training process. Conversely, AM tech- niques require no or minimal training, hence exhibiting the potential for supporting prompt instantiation of the perfor- mance model of the target system. However, in order to ensure their tractability, they typically rely on a set of sim- plifying assumptions. Consequently, AM's accuracy can be seriously challenged in scenarios (e.g., workload conditions) in which such assumptions are not matched. In this paper we explore several hybrid/gray box techniques that exploit AM and ML in synergy in order to get the best of the two worlds. We evaluate the proposed techniques in case stud- ies targeting two complex and widely adopted middleware systems: a NoSQL distributed key-value store and a Total Order Broadcast (TOB) service.},
	journal = {ICPE 2015 - Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
	author = {Didona, Diego and Quaglia, Francesco and Romano, Paolo and Torre, Ennio},
	year = {2015},
	note = {ISBN: 9781450332484},
	pages = {145--156},
}

@article{munaiah_curating_2017,
	title = {Curating {GitHub} for engineered software projects},
	volume = {22},
	issn = {15737616},
	doi = {10.1007/s10664-017-9512-6},
	abstract = {Software forges like GitHub host millions of repositories. Software engineering researchers have been able to take advantage of such a large corpora of potential study subjects with the help of tools like GHTorrent and Boa. However, the simplicity in querying comes with a caveat: there are limited means of separating the signal (e.g. repositories containing engineered software projects) from the noise (e.g. repositories containing home work assignments). The proportion of noise in a random sample of repositories could skew the study and may lead to researchers reaching unrealistic, potentially inaccurate, conclusions. We argue that it is imperative to have the ability to sieve out the noise in such large repository forges. We propose a framework, and present a reference implementation of the framework as a tool called reaper, to enable researchers to select GitHub repositories that contain evidence of an engineered software project. We identify software engineering practices (called dimensions) and propose means for validating their existence in a GitHub repository. We used reaper to measure the dimensions of 1,857,423 GitHub repositories. We then used manually classified data sets of repositories to train classifiers capable of predicting if a given GitHub repository contains an engineered software project. The performance of the classifiers was evaluated using a set of 200 repositories with known ground truth classification. We also compared the performance of the classifiers to other approaches to classification (e.g. number of GitHub Stargazers) and found our classifiers to outperform existing approaches. We found stargazers-based classifier (with 10 as the threshold for number of stargazers) to exhibit high precision (97\%) but an inversely proportional recall (32\%). On the other hand, our best classifier exhibited a high precision (82\%) and a high recall (86\%). The stargazer-based criteria offers precision but fails to recall a significant portion of the population.},
	number = {6},
	journal = {Empirical Software Engineering},
	author = {Munaiah, Nuthan and Kroh, Steven and Cabrey, Craig and Nagappan, Meiyappan},
	year = {2017},
	keywords = {Curation tools, Data curation, GitHub, Mining software repositories},
	pages = {3219--3253},
}

@article{thiruvathukal_how_nodate-3,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{rozsa_are_2017,
	title = {Are accuracy and robustness correlated?},
	doi = {10.1109/ICMLA.2016.42},
	abstract = {Machine learning models are vulnerable to adversarial examples formed by applying small carefully chosen perturbations to inputs that cause unexpected classification errors. In this paper, we perform experiments on various adversarial example generation approaches with multiple deep convolutional neural networks including Residual Networks, the best performing models on ImageNet Large-Scale Visual Recognition Challenge 2015. We compare the adversarial example generation techniques with respect to the quality of the produced images, and measure the robustness of the tested machine learning models to adversarial examples. Finally, we conduct large-scale experiments on cross-model adversarial portability. We find that adversarial examples are mostly transferable across similar network topologies, and we demonstrate that better machine learning models are less vulnerable to adversarial examples.},
	journal = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
	author = {Rozsa, Andras and Günther, Manuel and Boult, Terrance E.},
	year = {2017},
	note = {ISBN: 9781509061662},
	pages = {227--232},
}

@article{fagan_foundational_nodate,
	title = {Foundational {Cybersecurity} {Activities} for {IoT} {Device} {Manufacturers} {Foundational} {Cybersecurity} {Activities} for {IoT} {Device} {Manufacturers}},
	abstract = {Internet of Things (IoT) devices often lack device cybersecurity capabilities their customers—organizations and individuals—can use to help mitigate their cybersecurity risks. Manufacturers can help their customers by improving how securable the IoT devices they make are by providing necessary cybersecurity functionality and by providing customers with the cybersecurity-related information they need. This publication describes recommended activities related to cybersecurity that manufacturers should consider performing before their IoT devices are sold to customers. These foundational cybersecurity activities can help manufacturers lessen the cybersecurity-related efforts needed by customers, which in turn can reduce the prevalence and severity of IoT device compromises and the attacks performed using compromised devices.},
	author = {Fagan, Michael and Megas, Katerina N. and Fagan, Michael and Megas, Katerina N.},
	keywords = {Internet of Things (IoT), cybersecurity risk, manufacturing, risk management, risk mitigation, securable computing devices, software development},
}

@article{cha_principled_2020,
	title = {A principled approach to {GraphQL} {Query} {Cost} {Analysis}},
	issn = {23318422},
	abstract = {The landscape of web APIs is evolving to meet new client requirements and to facilitate how providers fulfill them. A recent web API model is GraphQL, which is both a query language and a runtime. Using GraphQL, client queries express the data they want to retrieve or mutate, and servers respond with exactly those data or changes. GraphQL’s expressiveness is risky for service providers because clients can succinctly request stupendous amounts of data, and responding to overly complex queries can be costly or disrupt service availability. Recent empirical work has shown that many service providers are at risk. Using traditional API management methods is not sufficient, and practitioners lack principled means of estimating and measuring the cost of the GraphQL queries they receive. In this work, we present a linear-time GraphQL query analysis that can measure the cost of a query without executing it. Our approach can be applied in a separate API management layer and used with arbitrary GraphQL backends. In contrast to existing static approaches, our analysis supports common GraphQL conventions that affect query cost, and our analysis is provably correct based on our formal specification of GraphQL semantics. We demonstrate the potential of our approach using a novel GraphQL query-response corpus for two commercial GraphQL APIs. Our query analysis consistently obtains upper cost bounds, tight enough relative to the true response sizes to be actionable for service providers. In contrast, existing static GraphQL query analyses exhibit over-estimates and under-estimates because they fail to support GraphQL conventions.},
	journal = {arXiv},
	author = {Cha, Alan and Wittern, Erik and Baudart, Guillaume and Davis, James C. and Mandel, Louis and Laredo, Jim A.},
	year = {2020},
	note = {ISBN: 9781450370431},
	keywords = {Algorithmic complexity attacks, GraphQL, Static analysis},
	pages = {257--268},
}

@article{pirelli_requirements_2019,
	title = {Requirements elicitation with a service canvas for packaged enterprise systems},
	volume = {2019-Septe},
	issn = {23326441},
	doi = {10.1109/RE.2019.00043},
	abstract = {We present a technique for eliciting requirements based on the use of a service canvas and the results of its application in the early phase of a customer relationship management integration project. The project was a collaboration between a research group and two industry partners. We describe (1) our service canvas, (2) how we designed a set of workshops to elicit the requirements, (3) the support tools used for running the workshops, and (4) the resulting canvas, listing the customer relationship management requirements, that was the basis for the project proposal. We explain how, as participant observers, we conducted the project and how we collected and analyzed the data. We describe what worked well and the lessons we learned. We outline some practical problems that remain unsolved.},
	journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
	author = {Pirelli, Blagovesta and Etzlinger, Lucien and Derrier, David and Regev, Gil and Wegmann, Alain},
	year = {2019},
	note = {ISBN: 9781728139128},
	keywords = {CRM, Canvas, Packaged enterprise system, Requirements elicitation, Service, Workshop},
	pages = {340--350},
}

@article{chitchyan_sustainability_2016,
	title = {Sustainability design in requirements engineering: {State} of practice},
	issn = {02705257},
	doi = {10.1145/2889160.2889217},
	abstract = {Sustainability is now a major concern in society, but there is little understanding of how it is perceived by software engineering professionals and how sustainability design can become an embedded part of software engineering process. This paper presents the results of a qualitative study exploring requirements engineering practitioners' perceptions and attitudes towards sustainability. It identifies obstacles and mitigation strategies regarding the application of sustainability design principles in daily work life. The results of this study reveal several factors that can prevent sustainability design from becoming a first class citizen in software engineering: software practitioners tend to have a narrow understanding of the concept of sustainability; organizations show limited awareness of its potential opportunities and benefits; and the norms in the discipline are not conducive to sustainable outcomes. These findings suggest the need for focused efforts in sustainability education, but also a need to rethink professional norms and practices.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Chitchyan, Ruzanna and Becker, Christoph and Betz, Stefanie and Duboc, Leticia and Penzenstadler, Birgit and Seyff, Norbert and Venters, Colin C.},
	year = {2016},
	note = {ISBN: 9781450341615},
	keywords = {Obstacles, Perceptions, Requirements engineering, Sustainability, Sustainability design},
	pages = {533--542},
}

@article{thiruvathukal_how_nodate-4,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{iv_exploring_2019,
	title = {Exploring the {Process} and {Challenges} of {Programming} with {Regular} {Expressions}},
	author = {IV, Louis G. Michael},
	year = {2019},
	keywords = {challenges of programming with, copyright 2019, developer process, exploring the process and, louis g, michael iv, qualitative research, redos, regex},
}

@article{garlan_update_2009,
	title = {update},
	author = {Garlan, David and Allen, Robert},
	year = {2009},
}

@article{lago_framing_2015,
	title = {Framing sustainability as a property of software quality},
	volume = {58},
	issn = {15577317},
	doi = {10.1145/2714560},
	number = {10},
	journal = {Communications of the ACM},
	author = {Lago, Patricia and Koçak, Sedef Akinli and Crnkovic, Ivica and Penzenstadler, Birgit},
	year = {2015},
	pages = {70--78},
}

@article{penzenstadler_safety_2014,
	title = {Safety, security, now sustainability: {The} nonfunctional requirement for the 21st century},
	volume = {31},
	issn = {07407459},
	doi = {10.1109/MS.2014.22},
	abstract = {Many software systems today control large-scale sociotechnical systems. These systems aren't just entangled with the environment but also with our dwindling resources and mostly unsustainable way of living, while the planet's population continues to grow. Dealing with sustainability requirements and systematically supporting their elicitation, analysis, and realization is a problem that has yet to be solved. Decades ago, the discipline of software engineering dealt with similar shortcomings in its processes by including safety and security as new system qualities. In light of the increasing consequences of inadequately addressing sustainability in developing software systems, software engineers must apply the lessons learned from these prior research efforts and identify the necessary research agenda. Considering sustainability in software engineering means more than energy efficiency and green IT, which are concerned with the first-order impacts of software systems. Software engineers must also take into account the second-and third-order impacts in the system context, even if they're hard to assess. By doing so, engineers have the potential to considerably improve civilization's sustainability. The Web extra at http://youtu.be/VC07j6a1XUw is a video in which author Birgit Penzenstadler talks about how software engineers can considerably improve civilization's sustainability by taking into account not just the first-order impacts of software systems but also their second-and third-order impacts. © 2014 IEEE.},
	number = {3},
	journal = {IEEE Software},
	author = {Penzenstadler, Birgit and Raturi, Ankita and Richardson, Debra and Tomlinson, Bill},
	year = {2014},
	note = {Publisher: IEEE},
	keywords = {green software, nonfunctional requirements, requirements engineering, security, software, sustainability},
	pages = {40--47},
}

@article{thiruvathukal_how_nodate-5,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{garlan_architectural_1995,
	title = {Architectural {Mismatch}: {Why} {Reuse} {Is} {So} {Hard}},
	volume = {12},
	issn = {07407459},
	doi = {10.1109/52.469757},
	number = {6},
	journal = {IEEE Software},
	author = {Garlan, David and Allen, Robert and Ockerbloom, John},
	year = {1995},
	pages = {17--26},
}

@article{kalleberg_finding_2011,
	title = {Finding {Software} {License} {Violations} {Through} {Binary} {Code} {Clone} {Detection}},
	author = {Kalleberg, Karl Trygve and Vermaas, Rob and Dolstra, Eelco},
	year = {2011},
	note = {ISBN: 9781450305747},
	keywords = {binary analysis, code clone detection, repository mining},
	pages = {63--72},
}

@article{chapman_short_2010,
	title = {A {Short} {Guide} {To} {Open} {Source} {Licenses}},
	url = {https://www.smashingmagazine.com/2010/03/a-short-guide-to-open-source-and-similar-licenses/},
	journal = {Smashing Magazine},
	author = {Chapman, Cameron},
	year = {2010},
	pages = {1--9},
}

@article{thiruvathukal_how_nodate-6,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{schroepfer_accelerating_2016,
	title = {Accelerating {Innovation} and {Powering} {New} {Experiences} with {AI}},
	url = {http://newsroom.fb.com/news/2016/11/accelerating-innovation-and-powering-new-experiences-with-ai/},
	author = {Schroepfer, Mike},
	year = {2016},
}

@article{neelamegam_survey_2012,
	title = {A {Survey} - {Object} {Oriented} {Quality} {Metrics}},
	volume = {12},
	url = {http://paper.ijcsns.org/07_book/201204/20120420.pdf},
	number = {4},
	journal = {Ijcsns},
	author = {Neelamegam, C. and Punithavalli, M.},
	year = {2012},
	keywords = {Tesi, class complexity, fault prediction, object oriented quality metrics, software, support vector machine},
	pages = {183--186},
}

@article{bassi_measuring_2018,
	title = {Measuring {Developers} ’ {Contribution} in {Source} {Code} using {Quality} {Metrics}},
	author = {Bassi, Patricia Rücker De},
	year = {2018},
	note = {ISBN: 9781538614822},
	keywords = {collaborative software development, developer contribution, metrics, open source projects, quality},
	pages = {39--44},
}

@article{lee_software_2014,
	title = {Software {Quality} {Factors} and {Software} {Quality} {Metrics} to {Enhance} {Software} {Quality} {Assurance}},
	volume = {4},
	number = {21},
	author = {Lee, Ming-chang},
	year = {2014},
	pages = {3069--3095},
}

@article{sadowski_how_2015,
	title = {How {Developers} {Search} for {Code} : {A} {Case} {Study}},
	author = {Sadowski, Caitlin and Stolee, Kathryn T. and Elbaum, Sebastian},
	year = {2015},
	note = {ISBN: 9781450336758},
	keywords = {17, 28, 3, 31, code search, code search appears to, developer tools, have ce-, mented its role in, software development, throughout this evolution, user evaluation},
	pages = {191--201},
}

@article{andersson_survey_1990,
	title = {A survey on software quality metrics},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.7502&rep=rep1&type=pdf},
	abstract = {Software Engineering is defined as “The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is, the application of engineering to software.”This is a survey of Software Quality Metrics. Definitions of the terms Software Crisis and Software Metrics found in the literature are presented. A need for the software metrics is given. A classification of software quality metrics is presented. The classification is: classical, product and process metrics. A number of different metrics are described. This will be followed by brief discussions about Measurement Scales and Current state of Software Metrics.},
	author = {Andersson, Thorbjorn},
	year = {1990},
	keywords = {data collection of software, metrics, quality, software quality metrics},
}

@article{pantiuchina_improving_2018,
	title = {Improving {Code} : {The} ( {Mis} ) perception of {Quality} {Metrics}},
	doi = {10.1109/ICSME.2018.00017},
	number = {Section V},
	author = {Pantiuchina, Jevgenija and Lanza, Michele and Bavota, Gabriele},
	year = {2018},
	note = {Publisher: IEEE},
}

@article{noauthor_quality_2020,
	title = {Quality assurance in research software},
	year = {2020},
}

@article{thiruvathukal_use_2019,
	title = {Use of {Software} {Process} in {Research} {Software} {Development} : {A} {Survey}},
	author = {Thiruvathukal, George K. and Carver, Jeffrey C.},
	year = {2019},
	note = {ISBN: 9781450371452},
	keywords = {2019, acm reference format, and jeffrey c, carver, eisty, george k, nasir u, research software, software process, survey, thiruvathukal, use},
}

@article{noauthor_no_nodate,
	title = {No {Title}},
}

@article{xu_quantitative_2020,
	title = {A {Quantitative} {Comparison} of {Different} {Machine} {Learning} {Approaches} for {Human} {Spermatozoa} {Quality} {Prediction} {Using} {Multimodal} {Datasets}},
	author = {Xu, Kele and Wang, Yin},
	year = {2020},
	note = {ISBN: 9781450379885},
	keywords = {2020, Machine learning, a quantitative comparison of, acm reference format, and yin wang, kele xu, machine learning, ming feng, multimo, multimodal, quantitative comparison},
	pages = {4659--4663},
}

@article{rawat_survey_2012,
	title = {Survey on {Impact} of {Software} {Metrics} on {Software} {Quality}},
	volume = {3},
	number = {1},
	author = {Rawat, Mrinal Singh},
	year = {2012},
	keywords = {- software metrics, and, function points, intended to measure, lines of code, object oriented metrics, software quality, software reliability, the metric should measure, valid, what it is},
	pages = {137--141},
}

@article{bamizadeh_analytical_1848,
	title = {An {Analytical} {Study} of {Code} {Smells}},
	volume = {6168},
	author = {Bamizadeh, Lida and Kumar, Binod and Kumar, Ajay and Shirwaikar, Shailaja},
	year = {1848},
	note = {ISBN: 2021020509},
	keywords = {code smells, data mining, knowledge repository, refactoring methods, software metrics},
	pages = {121--126},
}

@article{thiruvathukal_metrics_nodate,
	title = {Metrics {Dashboard} : {A} {Hosted} {Platform} for {Software} {Quality} {Metrics}},
	author = {Thiruvathukal, George K. and Hayward, Nicholas J. and Konstantin, L.},
}

@article{leotta_how_2019,
	title = {How {Do} {Implementation} {Bugs} {Affect} the {Results} of {Machine} {Learning} {Algorithms} ?},
	author = {Leotta, Maurizio and Informatica, Dip and Ingegneria, Robotica and Dibris, Sistemi and Genova, Università and Olianas, Dario and Informatica, Dip and Ingegneria, Robotica and Dibris, Sistemi and Genova, Università and Ricca, Filippo and Informatica, Dip and Ingegneria, Robotica and Dibris, Sistemi and Genova, Università and Dibris, Sistemi and Genova, Università},
	year = {2019},
	note = {ISBN: 9781450359337},
	keywords = {Accuracy, Bug, Machine Learning, Oracle P, Testing, accuracy, all or part of, bug, machine learning, or, or hard copies of, oracle problem, permission to make digital, soft-, testing, this work for personal, ware quality assurance},
	pages = {1304--1313},
}

@article{pecorelli_role_nodate,
	title = {On the {Role} of {Data} {Balancing} for {Machine} {Learning}-{Based} {Code} {Smell} {Detection}},
	author = {Pecorelli, Fabiano and Nucci, Dario Di and Lucia, Andrea De},
	note = {ISBN: 9781450368551},
	keywords = {acm reference format, and andrea de lucia, code smells, coen de roover, dario di nucci, data balancing, fabiano pecorelli, machine learning},
}

@article{jain_overview_2020,
	title = {Overview and {Importance} of {Data} {Quality} for {Machine} {Learning} {Tasks}},
	author = {Jain, Abhinav and Patel, Hima and Nagalapatti, Lokesh and Gupta, Nitin and Mehta, Sameep and Guttula, Shanmukha and Mujumdar, Shashank and Afzal, Shazia and Mittal, Ruhi Sharma and Munigala, Vitobha},
	year = {2020},
	note = {ISBN: 9781450379984},
	keywords = {abhinav jain, acm reference format, data quality, hima patel, lokesh nagalapatti, machine learning, nitin gupta, quality metrics, sameep mehta},
	pages = {3561--3562},
}

@article{shin_cross-domain_nodate,
	title = {Cross-domain meta-learning for bug finding in the source codes with a small dataset},
	author = {Shin, Jongho},
	note = {ISBN: 9781450375993},
	keywords = {all or part of, cross domain, cyber security, deep neural network, few-shot learning, learning, meta-, or, or hard copies of, permission to make digital, this work for personal, vulnerability detection},
}

@article{cosentino_assessing_2015,
	title = {Assessing the bus factor of {Git} repositories},
	doi = {10.1109/SANER.2015.7081864},
	abstract = {Software development projects face a lot of risks (requirements inflation, poor scheduling, technical problems, etc.). Underestimating those risks may put in danger the project success. One of the most critical risks is the employee turnover, that is the risk of key personnel leaving the project. A good indicator to evaluate this risk is to measure the concentration of information in individual developers. This is also popularly known as the bus factor ('number of key developers who would need to be incapacitated, i.e. hit by a bus, to make a project unable to proceed'). Despite the simplicity of the concept, calculating the actual bus factor for specific projects can quickly turn into an error-prone and time-consuming activity as soon as the size of the project and development team increase. In order to help project managers to assess the bus factor of their projects, in this paper we present a tool that, given a Git-based repository, automatically measures the bus factor for any file, directory and branch in the repository and for the project itself. You can also simulate with the tool what would happen to the project (e.g., which files would become orphans) if one or more developers disappeared.},
	journal = {2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
	author = {Cosentino, Valerio and Izquierdo, Javier Luis Canovas and Cabot, Jordi},
	year = {2015},
	note = {ISBN: 9781479984695
Publisher: IEEE},
	pages = {499--503},
}

@article{pineau_iclr_2018,
	title = {{ICLR} {Reproducibility} {Challenge}},
	volume = {2},
	url = {https://reproducibility-challenge.github.io/iclr_2019/},
	doi = {10.5281/zenodo.3158244},
	abstract = {Second Edition, 2019},
	journal = {ICLR Reproducibility Challenge},
	author = {Pineau, Joelle and Sinha, Koustuv and Fried, Genevieve and Ke, Rosemary Nan and Larochelle, Hugo},
	year = {2018},
	pages = {4--6},
}

@article{lin_perceptual_2011,
	title = {Perceptual visual quality metrics: {A} survey},
	volume = {22},
	issn = {10473203},
	url = {http://dx.doi.org/10.1016/j.jvcir.2011.01.005},
	doi = {10.1016/j.jvcir.2011.01.005},
	abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. © 2011 Elsevier Inc. All rights reserved.},
	number = {4},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Lin, Weisi and Kuo, C. C. Jay},
	year = {2011},
	note = {Publisher: Elsevier Inc.},
	keywords = {Common feature and artifact detection, Full reference, Human visual system (HVS), Just-noticeable distortion, No reference, Reduced reference, Signal decomposition, Signal-driven model, Vision-based model, Visual attention},
	pages = {297--312},
}

@article{potvin_why_2016,
	title = {Why google stores billions of lines of code in a single repository},
	volume = {59},
	issn = {15577317},
	doi = {10.1145/2854146},
	number = {7},
	journal = {Communications of the ACM},
	author = {Potvin, Rachel and Levenberg, Josh},
	year = {2016},
	pages = {78--87},
}

@article{bacchelli_expectations_2013,
	title = {Expectations, outcomes, and challenges of modern code review},
	issn = {02705257},
	doi = {10.1109/ICSE.2013.6606617},
	abstract = {Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more 'lightweight' than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classified hundreds of review comments across diverse teams at Microsoft. Our study reveals that while finding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional benefits such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we find that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers. © 2013 IEEE.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Bacchelli, Alberto and Bird, Christian},
	year = {2013},
	note = {ISBN: 9781467330763},
	pages = {712--721},
}

@article{ananthanarayanan_keeping_2019,
	title = {Keeping master green at scale},
	doi = {10.1145/3302424.3303970},
	abstract = {Giant monolithic source-code repositories are one of the fundamental pillars of the back end infrastructure in large and fast-paced software companies. The sheer volume of everyday code changes demands a reliable and efficient change management system with three uncompromisable key requirements — always green master, high throughput, and low commit turnaround time. Green refers to a master branch that always successfully compiles and passes all build steps, the opposite being red. A broken master (red) leads to delayed feature rollouts because a faulty code commit needs to be detected and rolled backed. Additionally, a red master has a cascading effect that hampers developer productivity—developers might face local test/build failures, or might end up working on a codebase that will eventually be rolled back. This paper presents the design and implementation of SubmitQueue. It guarantees an always green master branch at scale: all build steps (e.g., compilation, unit tests, UI tests) successfully execute for every commit point. SubmitQueue has been in production for over a year, and can scale to thousands of daily commits to giant monolithic repositories.},
	journal = {Proceedings of the 14th EuroSys Conference 2019},
	author = {Ananthanarayanan, Sundaram and Ardekani, Masoud Saeida and Haenikel, Denis and Varadarajan, Balaji and Soriano, Simon and Patel, Dhaval and Adl-Tabatabai, Ali Reza},
	year = {2019},
	note = {ISBN: 9781450362818},
}

@article{mcdermott_reproducibility_2019,
	title = {Reproducibility in machine learning for health},
	issn = {23318422},
	abstract = {Machine learning algorithms designed to characterize, monitor, and intervene on human health (ML4H) are expected to perform safely and reliably when operating at scale, potentially outside strict human supervision. This requirement warrants a stricter attention to issues of reproducibility than other fields of machine learning. In this work, we conduct a systematic evaluation of over 100 recently published ML4H research papers along several dimensions related to reproducibility. We find that the field of ML4H compares poorly to more established machine learning fields, particularly concerning data and code accessibility. Finally, drawing from success in other fields of science, we propose recommendations to data providers, academic publishers, and the ML4H research community in order to promote reproducible research moving forward.},
	journal = {arXiv},
	author = {McDermott, Matthew B. A. and Wang, Shirly and Marinsek, Nikki and Ranganath, Rajesh and Ghassemi, Marzyeh and Foschini, Luca},
	year = {2019},
}

@article{schroder_reproducible_2019,
	title = {Reproducible {Research} is more than {Publishing} {Research} {Artefacts}: {A} systematic analysis of jupyter notebooks from research articles},
	journal = {arXiv},
	author = {Schröder, Max and Krüger, Frank and Spors, Sascha},
	year = {2019},
	pages = {2--5},
}

@article{rule_ten_2018,
	title = {Ten simple rules for reproducible research in jupyter notebooks},
	issn = {23318422},
	abstract = {Reproducibility of computational studies is a hallmark of scientific methodology. It enables researchers to build with confidence on the methods and findings of others, reuse and extend computational pipelines, and thereby drive scientific progress. Since many experimental studies rely on computational analyses, biologists need guidance on how to set up and document reproducible data analyses or simulations. In this paper, we address several questions about reproducibility. For example, what are the technical and non-technical barriers to reproducible computational studies? What opportunities and challenges do computational notebooks offer to overcome some of these barriers? What tools are available and how can they be used effectively? We have developed a set of rules to serve as a guide to scientists with a specific focus on computational notebook systems, such as Jupyter Notebooks, which have become a tool of choice for many applications. Notebooks combine detailed workflows with narrative text and visualization of results. Combined with software repositories and open source licensing, notebooks are powerful tools for transparent, collaborative, reproducible, and reusable data analyses.},
	journal = {arXiv},
	author = {Rule, Adam and Birmingham, Amanda and Zuniga, Cristal and Altintas, Ilkay and Huang, Shih Cheng and Knight, Rob and Moshiri, Niema and Nguyen, Mai H. and Rosenthal, Sara Brin and Pérez, Fernando and Rose, Peter W.},
	year = {2018},
}

@article{graphs_neurips_2020,
	title = {{NeurIPS} 2020},
	author = {Graphs, Dynamic and Pooling, Graph and Power, Expressive},
	year = {2020},
	pages = {3--7},
}

@article{science_machine_2020,
	title = {The {Machine} {Learning} {Paper} {Paper} {Reproducibility} {Checklist}},
	author = {Science, McGill School of Computer},
	year = {2020},
	pages = {0--1},
}

@article{wang_assessing_2020,
	title = {Assessing and {Restoring} {Reproducibility} of {Jupyter} {Notebooks}},
	doi = {10.1145/3324884.3416585},
	abstract = {Jupyter notebooks-documents that contain live code, equations, visualizations, and narrative text-now are among the most popular means to compute, present, discuss and disseminate scientific findings. In principle, Jupyter notebooks should easily allow to reproduce and extend scientific computations and their findings; but in practice, this is not the case. The individual code cells in Jupyter notebooks can be executed in any order, with identifier usages preceding their definitions and results preceding their computations. In a sample of 936 published notebooks that would be executable in principle, we found that 73\% of them would not be reproducible with straightforward approaches, requiring humans to infer (and often guess) the order in which the authors created the cells. In this paper, we present an approach to (1) automatically satisfy dependencies between code cells to reconstruct possible execution orders of the cells; and (2) instrument code cells to mitigate the impact of non-reproducible statements (i.e., random functions) in Jupyter notebooks. Our Osiris prototype takes a notebook as input and outputs the possible execution schemes that reproduce the exact notebook results. In our sample, Osiris was able to reconstruct such schemes for 82.23\% of all executable notebooks, which has more than three times better than the state-of-the-art; the resulting reordered code is valid program code and thus available for further testing and analysis.},
	journal = {Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020},
	author = {Wang, Jiawei and Kuo, Tzu Yang and Li, Li and Zeller, Andreas},
	year = {2020},
	note = {ISBN: 9781450367684},
	pages = {138--149},
}

@article{zhou_metamorphic_2016,
	title = {Metamorphic {Testing} for {Software} {Quality} {Assessment}: {A} {Study} of {Search} {Engines}},
	volume = {42},
	issn = {19393520},
	doi = {10.1109/TSE.2015.2478001},
	abstract = {Metamorphic testing is a testing technique that can be used to verify the functional correctness of software in the absence of an ideal oracle. This paper extends metamorphic testing into a user-oriented approach to software verification, validation, and quality assessment, and conducts large scale empirical studies with four major web search engines: Google, Bing, Chinese Bing, and Baidu. These search engines are very difficult to test and assess using conventional approaches owing to the lack of an objective and generally recognized oracle. The results are useful for both search engine developers and users, and demonstrate that our approach can effectively alleviate the oracle problem and challenges surrounding a lack of specifications when verifying, validating, and evaluating large and complex software systems.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhou, Zhi Quan and Xiang, Shaowen and Chen, Tsong Yueh},
	year = {2016},
	keywords = {Software quality, lack of system specification, metamorphic testing, oracle problem, quality assessment, search engine, user-oriented testing, validation, verification},
	pages = {260--280},
}

@article{id_based_2006,
	title = {Based {Systems}},
	volume = {66},
	number = {March},
	author = {Id, Publication},
	year = {2006},
	pages = {3714},
}

@article{noauthor_mlreproducibility_2019,
	title = {{MLReproducibility} {Checklist} from {NeurIPS}},
	year = {2019},
	pages = {2019},
}

@article{information_neurips_2021,
	title = {{NeurIPS} 2019},
	author = {Information, Neural and Systems, Processing},
	year = {2021},
	pages = {2019--2021},
}

@article{hoang_reproducibility_2020,
	title = {Reproducibility {Companion} {Paper}: {Selective} {Deep} {Convolutional} {Features} for {Image} {Retrieval}},
	doi = {10.1145/3394171.3414814},
	author = {Hoang, Tuan and Do, Thanh-Toan and Cheung, Ngai-Man and Riegler, Michael and Zahálka, Jan},
	year = {2020},
	note = {ISBN: 9781450379885},
	keywords = {Aggregat, Content-Based Image Retrieval, Embedding, aggregating, content-based image retrieval, deep, embedding},
	pages = {4448--4452},
}

@article{chen_replication_2019,
	title = {Replication can improve prior results: {A} github study of pull request acceptance},
	volume = {2019-May},
	doi = {10.1109/ICPC.2019.00037},
	abstract = {Crowdsourcing and data mining can be used to effectively reduce the effort associated with the partial replication and enhancement of qualitative studies. For example, in a primary study, other researchers explored factors influencing the fate of GitHub pull requests using an extensive qualitative analysis of 20 pull requests. Guided by their findings, we mapped some of their qualitative insights onto quantitative questions. To determine how well their findings generalize, we collected much more data (170 additional pull requests from 142 GitHub projects). Using crowdsourcing, that data was augmented with subjective qualitative human opinions about how pull requests extended the original issue. The crowd's answers were then combined with quantitative features and, using data mining, used to build a predictor for whether code would be merged. That predictor was far more accurate than the one built from the primary study's qualitative factors (F1=90 vs 68\%), illustrating the value of a mixed-methods approach and replication to improve prior results. To test the generality of this approach, the next step in future work is to conduct other studies that extend qualitative studies with crowdsourcing and data mining.},
	journal = {IEEE International Conference on Program Comprehension},
	author = {Chen, Di and Stolee, Kathyrn and Menzies, Tim},
	year = {2019},
	note = {ISBN: 9781728115191},
	keywords = {Crowdsourcing, Github, Replication, Software Engineering},
	pages = {179--190},
}

@article{supeshala_yolo_2020,
	title = {{YOLO} v4 or {YOLO} v5 or {PP}-{YOLO}?},
	author = {Supeshala, Chamidu},
	year = {2020},
	pages = {1--11},
}

@article{bell_deflaker_2018,
	title = {{DeFlaker}: {Automatically} {Detecting} {Flaky} {Tests}},
	url = {http://dl.acm.org/citation.cfm?doid=3180155.3180164},
	abstract = {Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle. We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in the 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1, 874 flaky tests from 4, 846 failures, with a low false alarm rate (1.5\%). DeFlaker had a higher recall (95.5\% vs. 23\%) of confirmed flaky tests than Maven's default flaky test detector. CCS CONCEPTS • Software and its engineering → Software testing and de-bugging;},
	journal = {Proceedings of the 40th International Conference on Software Engineering - ICSE '18},
	author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
	year = {2018},
	note = {ISBN: 9781450356381},
	keywords = {acm reference format, code coverage, flaky tests, jonathan bell, lamyaa eloussi, michael hilton, owolabi legunsen, software testing, tifany},
	pages = {433--444},
}

@article{chen_metamorphic_1998,
	title = {Metamorphic testing: {A} new approach for generating next test cases},
	issn = {23318422},
	abstract = {In software testing, a set of test cases is constructed according to some predefined selection criteria. The software is then examined against these test cases. Three interesting observations have been made on the current artifacts of software testing. Firstly, an errorrevealing test case is considered useful while a successful test case which does not reveal software errors is usually not further investigated. Whether these successful test cases still contain useful information for revealing software errors has not been properly studied. Secondly, no matter how extensive the testing has been conducted in the development phase, errors may still exist in the software [5]. These errors, if left undetected, may eventually cause damage to the production system. The study of techniques for uncovering software errors in the production phase is seldom addressed in the literature. Thirdly, as indicated by Weyuker in [6], the availability of test oracles is pragmatically unattainable in most situations. However, the availability of test oracles is generally assumed in conventional software testing techniques. In this paper, we propose a novel test case selection technique that derives new test cases from the successful ones. The selection aims at revealing software errors that are possibly left undetected in successful test cases which may be generated using some existing strategies. As such, the proposed technique augments the effectiveness of existing test selection strategies. The technique also helps uncover software errors in the production phase and can be used in the absence of test oracles.},
	journal = {arXiv},
	author = {Chen, T. Y. and Cheung, S. C. and Yiu, S. M.},
	year = {1998},
	keywords = {Selection of Test Cases, Software Quality, Software Testing},
	pages = {1--11},
}

@article{islam_reproducibility_2017,
	title = {Reproducibility of benchmarked deep reinforcement learning tasks for continuous control},
	issn = {23318422},
	abstract = {Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.},
	journal = {arXiv},
	author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
	year = {2017},
}

@article{raff_step_2019,
	title = {A step toward quantifying independently reproducible machine learning research},
	issn = {23318422},
	abstract = {What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.},
	number = {NeurIPS},
	journal = {arXiv},
	author = {Raff, Edward},
	year = {2019},
}

@article{bouthillier_unreproducible_2019,
	title = {Unreproducible research is reproducible},
	volume = {2019-June},
	abstract = {The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.},
	journal = {36th International Conference on Machine Learning, ICML 2019},
	author = {Bouthillier, Xavier and Laurent, César and Vincent, Pascal},
	year = {2019},
	note = {ISBN: 9781510886988},
	pages = {1150--1159},
}

@article{thulasidasan_mixup_2019,
	title = {On mixup training: {Improved} calibration and predictive uncertainty for deep neural networks},
	issn = {23318422},
	abstract = {Mixup [28] is a recently proposed method for training deep neural networks where additional samples are generated during training by convexly combining random pairs of images and their associated labels. While simple to implement, it has shown to be a surprisingly effective method of data augmentation for image classification; DNNs trained with mixup show noticeable gains in classification performance on a number of image classification benchmarks. In this work, we discuss a hitherto untouched aspect of mixup training-the calibration and predictive uncertainty of models trained with mixup. We find that DNNs trained with mixup are significantly better calibrated-i.e the predicted softmax scores are much better indicators of the actual likelihood of a correct prediction-than DNNs trained in the regular fashion. We conduct experiments on a number of image classification architectures and datasets-including large-scale datasets like ImageNet-and find this to be the case. Additionally, we find that merely mixing features does not result in the same calibration benefit and that the label smoothing in mixup training plays a significant role in improving calibration. Finally, we also observe that mixuptrained DNNs are less prone to over-confident predictions on out-of-distribution and random-noise data. We conclude that the typical overconfidence seen in neural networks, even on in-distribution data is likely a consequence of training with hard labels, suggesting that mixup training be employed for classification tasks where predictive uncertainty is a significant concern.},
	number = {1},
	journal = {arXiv},
	author = {Thulasidasan, Sunil and Chennupati1, Gopinath and Bilmes, Jeff and Bhattacharya1, Tanmoy and Michalak, Sarah},
	year = {2019},
	pages = {1--7},
}

@article{olorisade_reproducibility_2017,
	title = {Reproducibility in {Machine} {Learning}-{Based} {Studies}: {An} {Example} of {Text} {Mining}},
	abstract = {Reproducibility is an essential requirement for computational studies including those based on machine learning techniques. However, many machine learning studies are either not reproducible or are difficult to reproduce. In this paper, we consider what information about text mining studies is crucial to successful reproduction of such studies. We identify a set of factors that affect reproducibility based on our experience of attempting to reproduce six studies proposing text mining techniques for the automation of the citation screening stage in the systematic review process. Subsequently, the reproducibility of 30 studies was evaluated based on the presence or otherwise of information relating to the factors. While the studies provide useful reports of their results, they lack information on access to the dataset in the form and order as used in the original study (as against raw data), the software environment used, randomization control and the implementation of proposed techniques. In order to increase the chances of being reproduced, researchers should ensure that details about and/or access to information about these factors are provided in their reports.},
	number = {Icml 2017},
	journal = {Reproducibility in ML Workshop at the 34th International Conference on Machine Learnin},
	author = {Olorisade, Babatunde K. and Brereton, Pearl and Andras, Peter},
	year = {2017},
	pages = {471--486},
}

@article{freire_computational_2012,
	title = {Computational {Reproducibility}: {State}-of-the-{Art}, {Challenges}, and {Database} {Research} {Opportunities}},
	doi = {10.1145/2213836.2213908},
	abstract = {Computational experiments have become an integral part of the scientific method, but reproducing, archiving, and querying them is still a challenge. The first barrier to a wider adoption is the fact that it is hard both for authors to derive a compendium that encapsulates all the components needed to reproduce a result and for reviewers to verify the results. In this tutorial, we will present a series of guidelines and, through hands-on examples, review existing tools to help authors create of reproducible results. We will also outline open problems and new directions for database-related research having to do with querying computational experiments.},
	author = {Freire, Juliana and Bonnet, Philippe and Shasha, Dennis},
	year = {2012},
	note = {ISBN: 9781450312479},
	keywords = {but also natural science, computational experiments support not, computational reproducibily, only computer science, provenance, re-, research, social science and humanities},
	pages = {593},
}

@article{song_generative_2019,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	issn = {23318422},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients might be ill-defined when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.91 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	number = {Section 4},
	journal = {arXiv},
	author = {Song, Yang and Ermon, Stefano},
	year = {2019},
	pages = {1--15},
}

@article{samuel_machine_2020,
	title = {Machine learning pipelines: {Provenance}, reproducibility and {FAIR} data principles},
	issn = {23318422},
	abstract = {Machine learning (ML) is an increasingly important scientific tool supporting decision making and knowledge generation in numerous fields. With this, it also becomes more and more important that the results of ML experiments are reproducible. Unfortunately, that often is not the case. Rather, ML, similar to many other disciplines, faces a reproducibility crisis. In this paper, we describe our goals and initial steps in supporting the end-to-end reproducibility of ML pipelines. We investigate which factors beyond the availability of source code and datasets influence reproducibility of ML experiments. We propose ways to apply FAIR data practices to ML workflows. We present our preliminary results on the role of our tool, ProvBook, in capturing and comparing provenance of ML experiments and their reproducibility using Jupyter Notebooks.},
	journal = {arXiv},
	author = {Samuel, Sheeba and Löffler, Frank and König-Ries, Birgitta},
	year = {2020},
	pages = {1--2},
}

@article{wang_exploring_2019,
	title = {Exploring {Regular} {Expression} {Evolution}},
	doi = {10.1109/SANER.2019.8667972},
	abstract = {Although there are tools to help developers understand the matching behaviors between a regular expression and a string, regular-expression related faults are still common. Learning developers' behavior through the change history of regular expressions can identify common edit patterns, which can inform the creation of mutation and repair operators to assist with testing and fixing regular expressions. In this work, we explore how regular expressions evolve over time, focusing on the characteristics of regular expression edits, the syntactic and semantic difference of the edits, and the feature changes of edits. Our exploration uses two datasets. First, we look at GitHub projects that have a regular expression in their current version and look back through the commit logs to collect the regular expressions' edit history. Second, we collect regular expressions composed by study participants during problem-solving tasks. Our results show that 1) 95\% of the regular expressions from GitHub are not edited, 2) most edited regular expressions have a syntactic distance of 4-6 characters from their predecessors, 3) over 50\% of the edits in GitHub tend to expand the scope of regular expression, and 4) the number of features used indicates the regular expression language usage increases over time. This work has implications for supporting regular expression repair and mutation to ensure test suite quality.},
	journal = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
	author = {Wang, Peipei and Bai, Gina R. and Stolee, Kathryn T.},
	year = {2019},
	note = {ISBN: 9781728105918},
	keywords = {Regular expressions, empirical studies, evolution},
	pages = {502--513},
}

@article{bai_exploring_2019,
	title = {Exploring tools and strategies used during regular expression composition tasks},
	volume = {2019-May},
	doi = {10.1109/ICPC.2019.00039},
	abstract = {Regular expressions are frequently found in programming projects. Studies have found that developers can accurately determine whether a string matches a regular expression. However, we still do not know the challenges associated with composing regular expressions. We conduct an exploratory case study to reveal the tools and strategies developers use during regular expression composition. In this study, 29 students are tasked with composing regular expressions that pass unit tests illustrating the intended behavior. The tasks are in Java and the Eclipse IDE was set up with JUnit tests. Participants had one hour to work and could use any Eclipse tools, web search, or web-based tools they desired. Screen-capture software recorded all interactions with browsers and the IDE. We analyzed the videos quantitatively by transcribing logs and extracting personas. Our results show that participants were 30\% successful (28 of 94 attempts) at achieving a 100\% pass rate on the unit tests. When participants used tools frequently, as in the case of the novice tester and the knowledgeable tester personas, or when they guess at a solution prior to searching, they are more likely to pass all the unit tests. We also found that compile errors often arise when participants searched for a result and copy/pasted the regular expression from another language into their Java files. These results point to future research into making regular expression composition easier for programmers, such as integrating visualization into the IDE to reduce context switching or providing language migration support when reusing regular expressions written in another language to reduce compile errors.},
	journal = {IEEE International Conference on Program Comprehension},
	author = {Bai, Gina R. and Clee, Brian and Shrestha, Nischal and Chapman, Carl and Wright, Cimone and Stolee, Kathryn T.},
	year = {2019},
	note = {ISBN: 9781728115191},
	keywords = {Exploratory study, Personas, Problem solving strategies, Regular expressions},
	pages = {197--208},
}

@article{davis_testing_2019,
	title = {Testing regex generalizability and its implications: {A} large-scale many-language measurement study},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.},
	journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
	author = {Davis, James C. and Moyer, Daniel and Kazerouni, Ayaan M. and Lee, Dongyoon},
	year = {2019},
	note = {ISBN: 9781728125084
Publisher: IEEE},
	keywords = {Data driven design, Empirical software engineering, Methods, Regular expressions},
	pages = {427--439},
}

@article{thiruvathukal_best_nodate,
	title = {Best {Practices} for {Reproducing} {Models} in {TensorFlow}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{park_empirical_2012,
	title = {An empirical study of supplementary bug fixes},
	issn = {21601852},
	doi = {10.1109/MSR.2012.6224298},
	abstract = {A recent study finds that errors of omission are harder for programmers to detect than errors of commission. While several change recommendation systems already exist to prevent or reduce omission errors during software development, there have been very few studies on why errors of omission occur in practice and how such errors could be prevented. In order to understand the characteristics of omission errors, this paper investigates a group of bugs that were fixed more than once in open source projects - those bugs whose initial patches were later considered incomplete and to which programmers applied supplementary patches. Our study on Eclipse JDT core, Eclipse SWT, and Mozilla shows that a significant portion of resolved bugs (22\% to 33\%) involves more than one fix attempt. Our manual inspection shows that the causes of omission errors are diverse, including missed porting changes, incorrect handling of conditional statements, or incomplete refactorings, etc. While many consider that missed updates to code clones often lead to omission errors, only a very small portion of supplementary patches (12\% in JDT, 25\% in SWT, and 9\% in Mozilla) have a content similar to their initial patches. This implies that supplementary change locations cannot be predicted by code clone analysis alone. Furthermore, 14\% to 15\% of files in supplementary patches are beyond the scope of immediate neighbors of their initial patch locations - they did not overlap with the initial patch locations nor had direct structural dependencies on them (e.g. calls, accesses, subtyping relations, etc.). These results call for new types of omission error prevention approaches that complement existing change recommendation systems. © 2012 IEEE.},
	journal = {IEEE International Working Conference on Mining Software Repositories},
	author = {Park, Jihun and Kim, Miryung and Ray, Baishakhi and Bae, Doo Hwan},
	year = {2012},
	note = {ISBN: 9781467317610
Publisher: IEEE},
	keywords = {bug fixes, empirical study, patches, software evolution},
	pages = {40--49},
}

@article{franco_comprehensive_2017,
	title = {A {Comprehensive} {Study} of {Real}-{World} {Numerical} {Bug} {Characteristics}},
	url = {https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=A+Comprehensive+Study+of+Real-World+Numerical+Bug+Characteristics&btnG=},
	journal = {Proceedings of the 32Nd IEEE/ACM International Conference on Automated Software Engineering},
	author = {Franco, Anthony Di and Guo, Hui and Rubio-gonzález, Cindy},
	year = {2017},
	note = {ISBN: 978-1-5386-2684-9},
	keywords = {empirical study, numerical bugs},
	pages = {509--519},
}

@article{thiruvathukal_best_nodate-1,
	title = {Best {Practices} for {Reproducing} {Models} in {TensorFlow}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{dolby_ariadne_2018,
	title = {Ariadne: {Analysis} for machine learning programs},
	doi = {10.1145/3211346.3211349},
	abstract = {Machine learning has transformed domains like vision and translation, and is now increasingly used in science, where the correctness of such code is vital. Python is popular for machine learning, in part because of its wealth of machine learning libraries, and is felt to make development faster; however, this dynamic language has less support for error detection at code creation time than tools like Eclipse. This is especially problematic for machine learning: given its statistical nature, code with subtle errors may run and produce results that look plausible but are meaningless. This can vitiate scientific results. We report on: applying a static framework, WALA, to machine learning code that uses TensorFlow. We have created static analysis for Python, a type system for tracking tensorsTensorflows core data structuresand a data flow analysis to track their usage. We report on how it was built and present some early results.},
	journal = {MAPL 2018 - Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, co-located with PLDI 2018},
	author = {Dolby, J. Julian and Shinnar, A. Avraham and Allain, A. Allison and Reinen, J. Jenna},
	year = {2018},
	note = {ISBN: 9781450358347},
	keywords = {Machine learning, Program analysis},
	pages = {1--10},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {14764687},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	pmid = {26017442},
	pages = {436--444},
}

@article{odena_tensorfuzz_2019,
	title = {{TensorFuzz}: {Debugging} neural networks with coverage-guided fuzzing},
	volume = {2019-June},
	abstract = {Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, wc develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms can provide this coverage metric for neural networks. We then combine these methods with techniques for property-based testing (PBT). In PBT, one asserts properties that a function should satisfy and the system automatically generates tests exercising those properties. We then apply this system to practical goals including (but not limited to) surfacing broken loss functions in popular GitHub repositories and making performance improvements to TensorFlow. Finally, we release an open source library called TensorFuzz that implements the described techniques.},
	journal = {36th International Conference on Machine Learning, ICML 2019},
	author = {Odena, Augustus and Olsson, Catherine and Andersen, David G. and Goodfellow, Ian},
	year = {2019},
	note = {ISBN: 9781510886988},
	pages = {8603--8613},
}

@article{lin_empirical_2009,
	title = {An empirical study on bug assignment automation using {Chinese} bug data},
	doi = {10.1109/ESEM.2009.5315994},
	abstract = {Bug assignment is an important step in bug life-cycle management. In large projects, this task would consume a substantial amount of human effort. To compare with the previous studies on automatic bug assignment in FOSS (Free/Open Source Software) projects, we conduct a case study on a proprietary software project in China. Our study consists of two experiments of automatic bug assignment, using Chinese text and the other non-text information of bug data respectively. Based on text data of the bug repository, the first experiment uses SVM to predict bug assignments and achieve accuracy close to that by human triagers. The second one explores the usefulness of non-text data in making such prediction. The main results from our study includes that text data are most useful data in the bug tracking system to triage bugs, and automation based on text data could effectively reduce the manual effort. © 2009 IEEE.},
	journal = {2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009},
	author = {Lin, Zhongpeng and Shu, Fengdi and Yang, Ye and Hu, Chenyong and Wang, Qing},
	year = {2009},
	note = {ISBN: 9781424448418
Publisher: IEEE},
	pages = {451--455},
}

@article{zhong_empirical_2015,
	title = {An empirical study on real bug fixes},
	volume = {1},
	issn = {02705257},
	doi = {10.1109/ICSE.2015.101},
	abstract = {Software bugs can cause significant financial loss and even the loss of human lives. To reduce such loss, developers devote substantial efforts to fixing bugs, which generally requires much expertise and experience. Various approaches have been proposed to aid debugging. An interesting recent research direction is automatic program repair, which achieves promising results, and attracts much academic and industrial attention. However, people also cast doubt on the effectiveness and promise of this direction. A key criticism is to what extent such approaches can fix real bugs. As only research prototypes for these approaches are available, it is infeasible to address the criticism by evaluating them directly on real bugs. Instead, in this paper, we design and develop BUGSTAT, a tool that extracts and analyzes bug fixes. With BUGSTAT's support, we conduct an empirical study on more than 9,000 real-world bug fixes from six popular Java projects. Comparing the nature of manual fixes with automatic program repair, we distill 15 findings, which are further summarized into four insights on the two key ingredients of automatic program repair: fault localization and faulty code fix. In addition, we provide indirect evidence on the size of the search space to fix real bugs and find that bugs may also reside in non-source files. Our results provide useful guidance and insights for improving the state-of-the-art of automatic program repair.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Zhong, Hao and Su, Zhendong},
	year = {2015},
	note = {ISBN: 9781479919345
Publisher: IEEE},
	pages = {913--923},
}

@article{zhang_empirical_2012,
	title = {An empirical study on factors impacting bug fixing time},
	issn = {10951350},
	doi = {10.1109/WCRE.2012.32},
	abstract = {Fixing bugs is an important activity of the software development process. A typical process of bug fixing consists of the following steps: 1) a user files a bug report, 2) the bug is assigned to a developer, 3) the developer fixes the bug, 4) changed code is reviewed and verified, and 5) the bug is resolved. Many studies have investigated the process of bug fixing. However, to the best of our knowledge, none has explicitly analyzed the interval between bug assignment and the time when bug fixing starts. After a bug assignment, some developers will immediately start fixing the bug while others will start bug fixing after a long period. We are blind on developer's delays when fixing bugs. This paper explores such delays of developers through an empirical study on three open source software systems. We examine factors affecting bug fixing time along three dimensions: bug reports, source code involved in the fix, and code changes that are required to fix the bug. We further compare different factors by descriptive logistic regression models. Our results can help development teams better understand factors behind delays, and then improve bug fixing process. © 2012 IEEE.},
	journal = {Proceedings - Working Conference on Reverse Engineering, WCRE},
	author = {Zhang, Feng and Khomh, Foutse and Zou, Ying and Hassan, Ahmed E.},
	year = {2012},
	note = {ISBN: 9780769548913
Publisher: IEEE},
	keywords = {bug fixing process, bug report, change request, empirical software engineering, fixing time, mylyn},
	pages = {225--234},
}

@article{zhang_predicting_2013,
	title = {Predicting bug-fixing time: {An} empirical study of commercial software projects},
	issn = {02705257},
	doi = {10.1109/ICSE.2013.6606654},
	abstract = {For a large and evolving software system, the project team could receive many bug reports over a long period of time. It is important to achieve a quantitative understanding of bug-fixing time. The ability to predict bug-fixing time can help a project team better estimate software maintenance efforts and better manage software projects. In this paper, we perform an empirical study of bug-fixing time for three CA Technologies projects. We propose a Markov-based method for predicting the number of bugs that will be fixed in future. For a given number of defects, we propose a method for estimating the total amount of time required to fix them based on the empirical distribution of bug-fixing time derived from historical data. For a given bug report, we can also construct a classification model to predict slow or quick fix (e.g., below or above a time threshold). We evaluate our methods using real maintenance data from three CA Technologies projects. The results show that the proposed methods are effective. © 2013 IEEE.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Zhang, Hongyu and Gong, Liang and Versteeg, Steve},
	year = {2013},
	note = {ISBN: 9781467330763
Publisher: IEEE},
	keywords = {Bugs, bug-fixing time, effort estimation, prediction, software maintenance},
	pages = {1042--1051},
}

@article{davis_testing_2019-1,
	title = {Testing regex generalizability and its implications: {A} large-scale many-language measurement study},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.},
	journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
	author = {Davis, James C. and Moyer, Daniel and Kazerouni, Ayaan M. and Lee, Dongyoon},
	year = {2019},
	note = {ISBN: 9781728125084
Publisher: IEEE},
	keywords = {Data driven design, Empirical software engineering, Methods, Regular expressions},
	pages = {427--439},
}

@article{islam_bug_2016,
	title = {Bug {Replication} in {Code} {Clones}: {An} {Empirical} {Study}},
	doi = {10.1109/saner.2016.78},
	abstract = {Cholesterol oxidase (ChOx) has been immobilized onto sol-gel derived nano-structured cerium oxide (NS-CeO 2 ) film deposited on indium-tin-oxide (ITO) coated glass substrate. Phase identification of sol-gel NS-CeO 2 film carried out using X-ray diffraction (XRD) yields reflection peak at 29.4° corresponding to (1 1 1) plane with oriented crystallite (34 nm) along c-axis normal to the substrate. Electrochemical studies reveal that NS-CeO 2 provides electroactive surface for the loading of ChOx and enhances electron transfer rate in the ChOx/NS-CeO 2 /ITO bioelectrode. The low value of Michaelis-Menten constant (K m ) obtained as 2.08 mM indicates enhanced ChOx affinity to cholesterol. The observed results show application of sol-gel derived NS-CeO 2 for biosensing without any functionalization. © 2008 Elsevier B.V. All rights reserved.},
	author = {Islam, Judith F. and Mondal, Manishankar and Roy, Chanchal K.},
	year = {2016},
	note = {ISBN: 9781509018550
Publisher: IEEE},
	pages = {68--78},
}

@article{asaduzzaman_bug_2012,
	title = {Bug introducing changes: {A} case study with {Android}},
	issn = {21601852},
	doi = {10.1109/MSR.2012.6224267},
	abstract = {Changes, a rather inevitable part of software development can cause maintenance implications if they introduce bugs into the system. By isolating and characterizing these bug introducing changes it is possible to uncover potential risky source code entities or issues that produce bugs. In this paper, we mine the bug introducing changes in the Android platform by mapping bug reports to the changes that introduced the bugs. We then use the change information to look for both potential problematic parts and dynamics in development that can cause maintenance implications. We believe that the results of our study can help better manage Android software development. © 2012 IEEE.},
	journal = {IEEE International Working Conference on Mining Software Repositories},
	author = {Asaduzzaman, Muhammad and Bullock, Michael C. and Roy, Chanchal K. and Schneider, Kevin A.},
	year = {2012},
	note = {ISBN: 9781467317610
Publisher: IEEE},
	keywords = {Bug, bug report, change log, fixes},
	pages = {116--119},
}

@article{tarar_automated_2020,
	title = {Automated summarization of bug reports to speed-up software development/maintenance process by using natural language processing ({NLP})},
	doi = {10.1109/ICCSE49874.2020.9201846},
	abstract = {Bug reports can provide a great deal of assistance for developers during the process of development. But due to the large size of bug repositories, it is sometimes difficult to take advantage of these artifacts in the available time. One way of helping developers to provide summaries of these reports and provide relevant details only. Once it's decided that this is the required report then one can study the details. As text mining technology advances, many substantial approaches have been proposed to generate optimized summaries for bug reports. In this paper, we have proposed an extractive based methodology for the generation of summaries of bug reports by using the sentence embedding. We achieved improved rouge-1 and rouge- 2 results than the previous state of the art systems for the bug report summary generation.},
	number = {Iccse},
	journal = {15th International Conference on Computer Science and Education, ICCSE 2020},
	author = {Tarar, M. Irtaza Nawaz and Ahmed, Faizan and Butt, Wasi Haider},
	year = {2020},
	note = {ISBN: 9781728172675},
	keywords = {Bug reports, Machine Learning, Natural Language Processing, Software Artifacts, Summarization},
	pages = {483--488},
}

@article{karpathy_software_2017,
	title = {Software 2.0},
	author = {Karpathy, Andrej},
	year = {2017},
	pages = {1--8},
}

@article{wan_are_2021,
	title = {Are {Machine} {Learning} {Cloud} {APIs} {Used} {Correctly} ?},
	number = {Ml},
	author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
	year = {2021},
}

@article{marta_approach_2007,
	title = {An {Approach} to {Software} {Testing} of {Machine} {Learning} {Applications} {Christian}},
	url = {http://www.psl.cs.columbia.edu/publications/pubs/Murphy-SEKE2007.pdf},
	abstract = {Some learn are not already known to human users. It is challenging no approach aimed at addressing this problem. We present our findings from testing implementations of two different ML ranking algorithms: Support Vector Machines 1..Introduction We investigate the problem of making machine learning (ML) applications dependable, focusing on software testing. Conventional software engineering processes is anomalies (henceforth " bugs ") in the ML applications of interest because there is no reliable indicate input. The general class of software systems with no reliable test oracle available is sometimes known as " non-testable programs " [1]. These ML applications fall describe determine no were optimal quality do not guarantee that an application implements or uses the algorithm correctly, and thus software testing is needed. Our testing, then, does not seek to determine whether an ML algorithm learns well, algorithm correctly implements the specification and fulfills In this paper, we describe our approach to testing ML applications, in particular those that implement ranking algorithms (a requirement of the real-world problem is possible only to show the presence of bugs but not their absence. Usually when input or output equivalence classes are applied to developing test cases, known in advance. Our research seeks to address the issue bugs, and how one can indeed know whether a test actually what Our approach for creating test cases consists of three facets: analyzing the problem domain and the corresponding real-world data sets; analyzing the algorithm as it is defined; and analyzing the implementation's is conventional, not novel, a number of issues arise when applying it to determining equivalence classes and We present our findings to date from two case studies: our first concerns the Martingale Boosting algorithm, [3] initially as a classification algorithm and then adapted by Long and others into a ranking algorithm called and Machines 2..Background},
	author = {Marta, Gail},
	year = {2007},
}

@article{herbold_smoke_2020,
	title = {Smoke testing for machine learning: {Simple} tests to discover severe defects},
	volume = {0},
	issn = {23318422},
	abstract = {Machine learning is nowadays a standard technique for data analysis within software applications. Software engineers need quality assurance techniques that are suitable for these new kinds of systems. Within this article, we discuss the question whether standard software testing techniques that have been part of textbooks since decades are also useful for the testing of machine learning software. Concretely, we try to determine generic smoke tests that can be used to assert that basic functions can be executed without crashing. We found that we can derive such tests using techniques similar to equivalence classes and boundary value analysis. Moreover, we found that these concepts can also be applied to hyperparameters, to further improve the quality of the smoke tests. Even though our approach is almost trivial, we were able to find bugs in all three machine learning libraries that we tested and severe bugs in two of the three libraries. This demonstrates that common software testing techniques are still valid in the age of machine learning and that they are suitable to find and prevent severe bugs, even in mature machine learning libraries.},
	number = {0},
	journal = {arXiv},
	author = {Herbold, Steffen and Haar, Tobias},
	year = {2020},
	keywords = {Boundary-value analysis, Classification, Combinatorial testing, Equivalence classes, Machine learning, Smoke testing, Software testing},
}

@article{abid_gradio_2019,
	title = {Gradio: {Hassle}-free sharing and testing of {ML} models in the wild},
	issn = {23318422},
	abstract = {Accessibility is a major challenge of machine learning (ML). Typical ML models are built by specialists and require specialized hardware/software as well as ML experience to validate. This makes it challenging for non-technical collaborators and endpoint users (e.g. physicians) to easily provide feedback on model development and to gain trust in ML. The accessibility challenge also makes collaboration more difficult and limits the ML researcher's exposure to realistic data and scenarios that occur in the wild. To improve accessibility and facilitate collaboration, we developed an open-source Python package, Gradio, which allows researchers to rapidly generate a visual interface for their ML models. Gradio makes accessing any ML model as easy as sharing a URL. Our development of Gradio is informed by interviews with a number of machine learning researchers who participate in interdisciplinary collaborations. Their feedback identified that Gradio should support a variety of interfaces and frameworks, allow for easy sharing of the interface, allow for input manipulation and interactive inference by the domain expert, as well as allow embedding the interface in iPython notebooks. We developed these features and carried out a case study to understand Gradio's usefulness and usability in the setting of a machine learning collaboration between a researcher and a cardiologist.},
	number = {Ml},
	journal = {arXiv},
	author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},
	year = {2019},
}

@article{hambali_software_2020,
	title = {{SOFTWARE} {TESTING} {CHALLENGES} {IN} {MACHINE} {LEARNING} {APPLICATION}},
	volume = {5},
	number = {November},
	author = {Hambali, Muhamad Adham and Nazeer, Ahmad and Arifin, Zainal},
	year = {2020},
	pages = {720--723},
}

@article{santhanam_engineering_2019,
	title = {Engineering reliable deep learning systems},
	volume = {3},
	issn = {23318422},
	abstract = {Recent progress in artificial intelligence (AI) using deep learning techniques has triggered its wide-scale use across a broad range of applications. These systems can already perform tasks such as natural language processing of voice and text, visual recognition, question-answering, recom-mendations and decision support. However, at the current level of maturity, the use of an AI component in mission-critical or safety-critical applications can have unexpected consequences. Consequently, serious concerns about relia-bility, repeatability, trust, and maintainability of AI applica-tions remain. As AI becomes pervasive despite its short-comings, more systematic ways of approaching AI software development and certification are needed. These fundamen-tal aspects establish the need for a discipline on "AI engi-neering". This paper presents the current perspective of rel-evant AI engineering concepts and some key challenges that need to be overcome to make significant progress in this important area.},
	journal = {arXiv},
	author = {Santhanam, P. and Farchi, Eitan and Pankratius, Victor},
	year = {2019},
	pages = {1--8},
}

@article{xie_testing_2020,
	title = {Testing {Coverage} {Criteria} for {Deep} {Forests}},
	doi = {10.1109/DSA.2019.00091},
	abstract = {In practice, many unknown errors have emerged in deep learning systems. One of the main reasons is that the behaviors of deep learning systems are unpredictable and difficult to test. Proper testing criteria are vitally important to evaluate the adequacy of testing deep learning systems. However, there is no testing criterion available for the deep forest, which is a deep learning model that has achieved good performance on small-scale data sets and low-computing-power platform projects. To address this problem, we propose a set of testing coverage criteria for deep forests in this paper. The set of testing coverage criteria is composed of multi-grained scanning node coverage (MGNC), multi-grained scanning leaf coverage (MGLC), cascade forest output coverage (CFOC) and cascade forest class coverage (CFCC).},
	journal = {Proceedings - 2019 6th International Conference on Dependable Systems and Their Applications, DSA 2019},
	author = {Xie, Ruilin and Cui, Zhanqi and Jia, Minghua and Wen, Yuan and Hao, Baoshui},
	year = {2020},
	note = {ISBN: 9781728160573
Publisher: IEEE},
	keywords = {cascade forest coverage, deep forest, multi-grained scanning coverage, testing coverage criteria},
	pages = {513--514},
}

@article{afzal_study_2020,
	title = {A {Study} on {Challenges} of {Testing} {Robotic} {Systems}},
	doi = {10.1109/ICST46399.2020.00020},
	abstract = {Robotic systems are increasingly a part of everyday life. Characteristics of robotic systems such as interaction with the physical world, and integration of hardware and software components, differentiate robotic systems from conventional software systems. Although numerous studies have investigated the challenges of software testing in practice, no such study has focused on testing of robotic systems. In this paper, we conduct a qualitative study to better understand the testing practices used by the robotics community, and identify the challenges faced by practitioners when testing their systems. We identify a total of 12 testing practices and 9 testing challenges from our participants' responses. We group these challenges into 3 major themes: Real-world complexities, Community and standards, and Component integration. We believe that further research on addressing challenges described with these three major themes can result in higher adoption of robotics testing practices, more testing automation, and higher-quality robotic systems.},
	journal = {Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation, ICST 2020},
	author = {Afzal, Afsoon and Goues, Claire Le and Hilton, Michael and Timperley, Christopher Steven},
	year = {2020},
	note = {ISBN: 9781728157771},
	keywords = {qualitative study, robotics testing, testing challenges},
	pages = {96--107},
}

@article{durelli_machine_2019,
	title = {Machine learning applied to software testing: {A} systematic mapping study},
	volume = {68},
	issn = {15581721},
	doi = {10.1109/TR.2019.2892517},
	abstract = {Software testing involves probing into the behavior of software systems to uncover faults. Most testing activities are complex and costly, so a practical strategy that has been adopted to circumvent these issues is to automate software testing. There has been a growing interest in applying machine learning (ML) to automate various software engineering activities, including testingrelated ones. In this paper, we set out to review the state-of-the art of howML has been explored to automate and streamline software testing and provide an overview of the research at the intersection of these two fields by conducting a systematic mapping study. We selected 48 primary studies. These selected studies were then categorized according to study type, testing activity, andMLalgorithm employed to automate the testing activity. The results highlight the most widely used ML algorithms and identify several avenues for future research. We found that ML algorithms have been used mainly for test-case generation, refinement, and evaluation. Also, ML has been used to evaluate test oracle construction and to predict the cost of testing-related activities. The results of this paper outline the ML algorithms that are most commonly used to automate software-testing activities, helping researchers to understand the current state of research concerning ML applied to software testing. We also found that there is a need for better empirical studies examining how ML algorithms have been used to automate software-testing activities.},
	number = {3},
	journal = {IEEE Transactions on Reliability},
	author = {Durelli, Vinicius H. S. and Durelli, Rafael S. and Borges, Simone S. and Endo, Andre T. and Eler, Marcelo M. and Dias, Diego R. C. and Guimarães, Marcelo P.},
	year = {2019},
	keywords = {Machine learning (ML), Software testing, Systematic mapping study},
	pages = {1189--1212},
}

@article{sherin_systematic_2019,
	title = {A systematic mapping study on testing of machine learning programs},
	issn = {23318422},
	abstract = {Context: Machine learning (ML) has made tremendous progress in the last few years leading to usage in mission-critical and safety-critical systems. This has led researchers to focus on the techniques for testing ML-enabled systems, and has been further emphasized by recent hazardous incidents (e.g., Tesla car accident). Objective: We aim to conduct a systematic mapping in the area of testing ML programs. We identify, analyze and classify the existing literature to provide an overview of the area. Methodology: We followed well-established guidelines of systematic mapping to develop a systematic protocol to identify and review the existing literature. We formulate three sets of research questions, define inclusion and exclusion criteria and systematically identify themes for the classification of existing techniques. We also report the quality of the published works using established assessment criteria. Results: we finally selected 37 papers out of 1654 based on our selection criteria up to January 2019. We analyze trends such as contribution facet, research facet, test approach, type of ML and the kind of testing with several other attributes. We also discuss the empirical evidence and reporting quality of selected papers. The data from the study is made publicly available for other researchers and practitioners. Conclusion: We present an overview of the area by answering several research questions. The area is growing rapidly, however, there is lack of enough empirical evidence to compare and assess the effectiveness of the techniques. More publicly available tools are required for use of practitioners and researchers. Further attention is needed on non-functional testing and testing of ML programs using reinforcement learning. We believe that this study can help researchers and practitioners to obtain an overview of the area and identify several sub-areas where more research is required.},
	journal = {arXiv},
	author = {Sherin, Salman and Khan, Muhammad Uzair and Iqbal, Muhammad Zohaib},
	year = {2019},
	keywords = {Deep learning, Machine learning, Software testing, Systematic mapping study},
}

@article{riccio_testing_2020,
	title = {Testing machine learning based systems: a systematic mapping},
	volume = {25},
	issn = {15737616},
	doi = {10.1007/s10664-020-09881-0},
	abstract = {Context:: A Machine Learning based System (MLS) is a software system including one or more components that learn how to perform a task from a given data set. The increasing adoption of MLSs in safety critical domains such as autonomous driving, healthcare, and finance has fostered much attention towards the quality assurance of such systems. Despite the advances in software testing, MLSs bring novel and unprecedented challenges, since their behaviour is defined jointly by the code that implements them and the data used for training them. Objective:: To identify the existing solutions for functional testing of MLSs, and classify them from three different perspectives: (1) the context of the problem they address, (2) their features, and (3) their empirical evaluation. To report demographic information about the ongoing research. To identify open challenges for future research. Method:: We conducted a systematic mapping study about testing techniques for MLSs driven by 33 research questions. We followed existing guidelines when defining our research protocol so as to increase the repeatability and reliability of our results. Results:: We identified 70 relevant primary studies, mostly published in the last years. We identified 11 problems addressed in the literature. We investigated multiple aspects of the testing approaches, such as the used/proposed adequacy criteria, the algorithms for test input generation, and the test oracles. Conclusions:: The most active research areas in MLS testing address automated scenario/input generation and test oracle creation. MLS testing is a rapidly growing and developing research area, with many open challenges, such as the generation of realistic inputs and the definition of reliable evaluation metrics and benchmarks.},
	number = {6},
	journal = {Empirical Software Engineering},
	author = {Riccio, Vincenzo and Jahangirova, Gunel and Stocco, Andrea and Humbatova, Nargiz and Weiss, Michael and Tonella, Paolo},
	year = {2020},
	note = {Publisher: Empirical Software Engineering},
	keywords = {Machine learning, Software testing, Systematic mapping, Systematic review},
	pages = {5193--5254},
}

@article{dilhara_understanding_2018,
	title = {Understanding {Software}-2 . 0 : {A} {Study} of {Machine} {Learning} library usage and evolution},
	volume = {1},
	number = {1},
	author = {Dilhara, Malinda and Ketkar, Ameya and Dig, Danny},
	year = {2018},
	pages = {1--41},
}

@article{chen_behavior_2019,
	title = {Behavior pattern-driven test case selection for deep neural networks},
	doi = {10.1109/AITest.2019.000-2},
	abstract = {With the widespread application of deep learning systems, the robustness of deep neural networks (DNNs) is received increasing attentions recently. By studying the distribution of neurons outputs in DNN models, we found that the behavior patterns of neurons are different for different kinds of DNNs' inputs, e.g. test cases generated by different adversarial attack techniques. In this paper, we extract the neuron behavior patterns of DNNs under different adversarial attack techniques, use them as the guidance for test case selection. Experimental results show that this method is more efficient than random technology.},
	number = {4},
	journal = {Proceedings - 2019 IEEE International Conference on Artificial Intelligence Testing, AITest 2019},
	author = {Chen, Yanshan and Wang, Ziyuan and Wang, Dong and Yao, Yongming and Chen, Zhenyu},
	year = {2019},
	note = {ISBN: 9781728104928
Publisher: IEEE},
	keywords = {Behavior pattern, Deep neural network, Test case prioritization, Test case selection},
	pages = {89--90},
}

@article{liem_oracle_2020,
	title = {Oracle {Issues} in {Machine} {Learning} and {Where} to {Find} {Them}},
	doi = {10.1145/3387940.3391490},
	abstract = {The rise in popularity of machine learning (ML), and deep learning in particular, has both led to optimism about achievements of artificial intelligence, as well as concerns about possible weaknesses and vulnerabilities of ML pipelines. Within the software engineering community, this has led to a considerable body of work on ML testing techniques, including white- and black-box testing for ML models. This means the oracle problem needs to be addressed. For supervised ML applications, oracle information is indeed available in the form of dataset 'ground truth', that encodes input data with corresponding desired output labels. However, while ground truth forms a gold standard, there still is no guarantee it is truly correct. Indeed, syntactic, semantic, and conceptual framing issues in the oracle may negatively affect the ML system's integrity. While syntactic issues may automatically be verified and corrected, the higher-level issues traditionally need human judgment and manual analysis. In this paper, we employ two heuristics based on information entropy and semantic analysis on well-known computer vision models and benchmark data from ImageNet. The heuristics are used to semi-automatically uncover potential higher-level issues in (i) the label taxonomy used to define the ground truth oracle (labels), and (ii) data encoding and representation. In doing this, beyond existing ML testing efforts, we illustrate the need for software engineering strategies that especially target and assess the oracle.},
	journal = {Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020},
	author = {Liem, Cynthia C. S. and Panichella, Annibale},
	year = {2020},
	note = {ISBN: 9781450379632},
	pages = {483--488},
}

@article{zhou_using_2020,
	title = {Using {Metamorphic} {Testing} to {Evaluate} {DNN} {Coverage} {Criteria}},
	doi = {10.1109/ISSREW51248.2020.00055},
	abstract = {Generating test cases and further evaluating their 'quality' are two critical topics in the area of Deep Neural Networks (DNNs). In this domain, different studies (e.g., [1], [2]) have reported that metamorphic testing (MT) serves as an effective test case generation method, where an initial set of source test cases is augmented with identified metamorphic relations (MRs) to produce the corresponding set of follow-up test cases. As a result, the fault detection effectiveness (and, hence, the 'quality') of the resulting test suite T, containing these source and follow-up test cases, will most likely be increased.},
	journal = {Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering Workshops, ISSREW 2020},
	author = {Zhou, Jinyi and Qiu, Kun and Zheng, Zheng and Chen, Tsong Yueh and Poon, Pak Lok},
	year = {2020},
	note = {ISBN: 9781728198705},
	keywords = {Cover, Deep Neural Networks, Metamorphic relations},
	pages = {147--148},
}

@article{tian_evaldnn_2020,
	title = {{EvalDNN}: {A} {Toolbox} for {Evaluating} {Deep} {Neural} {Network} {Models}},
	issn = {02705257},
	doi = {10.1145/3377812.3382133},
	abstract = {Recent studies have shown that the performance of deep learning models should be evaluated using various important metrics such as robustness and neuron coverage, besides the widely-used prediction accuracy metric. However, major deep learning frameworks currently only provide APIs to evaluate a model's accuracy. In order to comprehensively assess a deep learning model, framework users and researchers often need to implement new metrics by themselves, which is a tedious job. What is worse, due to the large number of hyper-parameters and inadequate documentation, evaluation results of some deep learning models are hard to reproduce, especially when the models and metrics are both new.To ease the model evaluation in deep learning systems, we have developed EvalDNN, a user-friendly and extensible toolbox supporting multiple frameworks and metrics with a set of carefully designed APIs. Using EvalDNN, evaluation of a pre-trained model with respect to different metrics can be done with a few lines of code. We have evaluated EvalDNN on 79 models from TensorFlow, Keras, GluonCV, and PyTorch. As a result of our effort made to reproduce the evaluation results of existing work, we release a performance benchmark of popular models, which can be a useful reference to facilitate future research. The tool and benchmark are available at https://github.com/yqtianust/EvalDNN and https://yqtianust.github.io/EvalDNN-benchmark/, respectively. A demo video of EvalDNN is available at: Https://youtu.be/v69bNJN2bJc.},
	journal = {Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020},
	author = {Tian, Yongqiang and Zeng, Zhihua and Wen, Ming and Liu, Yepang and Kuo, Tzu Yang and Cheung, Shing Chi},
	year = {2020},
	note = {ISBN: 9781450371223},
	keywords = {Deep Learning Model, Evaluation},
	pages = {45--48},
}

@article{ferdinandy_challenges_2020,
	title = {Challenges of machine learning model validation using correlated behaviour data: {Evaluation} of cross-validation strategies and accuracy measures},
	volume = {15},
	issn = {19326203},
	doi = {10.1371/journal.pone.0236092},
	abstract = {Automated monitoring of the movements and behaviour of animals is a valuable research tool. Recently, machine learning tools were applied to many species to classify units of behaviour. For the monitoring of wild species, collecting enough data for training models might be problematic, thus we examine how machine learning models trained on one species can be applied to another closely related species with similar behavioural conformation. We contrast two ways to calculate accuracies, termed here as overall and threshold accuracy, because the field has yet to define solid standards for reporting and measuring classification performances. We measure 21 dogs and 7 wolves, and find that overall accuracies are between 51 and 60\% for classifying 8 behaviours (lay, sit, stand, walk, trot, run, eat, drink) when training and testing data are from the same species and between 41 and 51\% when training and testing is cross-species. We show that using data from dogs to predict the behaviour of wolves is feasible. We also show that optimising the model for overall accuracy leads to similar overall and threshold accuracies, while optimizing for threshold accuracy leads to threshold accuracies well above 80\%, but yielding very low overall accuracies, often below the chance level. Moreover, we show that the most common method for dividing the data between training and testing data (random selection of test data) overestimates the accuracy of models when applied to data of new specimens. Consequently, we argue that for the most common goals of animal behaviour recognition overall accuracy should be the preferred metric. Considering, that often the goal is to collect movement data without other methods of observation, we argue that training data and testing data should be divided by individual and not randomly.},
	number = {7},
	journal = {PLoS ONE},
	author = {Ferdinandy, Bence and Gerencsér, Linda and Corrieri, Luca and Perez, Paula and Újváry, Dóra and Csizmadia, Gábor and Miklósi, Ádám},
	year = {2020},
	pmid = {32687528},
	note = {ISBN: 1111111111},
	pages = {1--14},
}

@article{wu_testing_2020,
	title = {Testing artificial intelligence system towards safety and robustness: {State} of the art},
	volume = {47},
	issn = {18199224},
	abstract = {With the increasing development of machine learning, conventional embedded systems cannot meet the requirement of current academic researches and industrial applications. Artificial Intelligence System (AIS) based on machine learning has been widely used in various safety-critical systems, such as machine vision, autonomous vehicles, collision avoidance system. Different from conventional embedded systems, AIS generates and updates control strategies through learning algorithms which make the control behaviors nondeterministic and bring about the test oracle problem in AIS testing procedure. There have been various testing approaches for AIS to guarantee the safety and robustness. However, few researches explain how to conduct AIS testing with a complete workflow systematically. This paper provides a comprehensive survey of existing testing techniques to detect the erroneous behaviors of AIS, and sums up the involved key steps and testing components in terms of test coverage criterion, test data generation, testing approach and common dataset. This literature review aims at organizing a standardized workflow and leading to a practicable insight and research trend towards AIS testing.},
	number = {3},
	journal = {IAENG International Journal of Computer Science},
	author = {Wu, Tingting and Dong, Yunwei and Dong, Zhiwei and Singa, Aziz and Chen, Xiong and Zhang, Yu},
	year = {2020},
	keywords = {Artificial intelligence system, Machine learning, Neural network, Testing, Verification},
	pages = {449--462},
}

@article{jha_data_2019,
	title = {Data {Infrastructure} for {Machine} {Learning}},
	volume = {7},
	doi = {10.22214/ijraset.2019.4133},
	abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
	number = {4},
	journal = {International Journal for Research in Applied Science and Engineering Technology},
	author = {Jha, Samridhi},
	year = {2019},
	pages = {740--742},
}

@book{gamma_design_1995,
	title = {Design patterns : elements of reusable object-oriented software},
	isbn = {0-201-63361-2},
	publisher = {Addison-Wesley},
	author = {Gamma, Erich},
	editor = {Gamma, Erich},
	year = {1995},
	keywords = {Computer software – Reusability, Object-oriented programming (Computer science), Software patterns},
}

@inproceedings{hazelwood_applied_2018,
	title = {Applied {Machine} {Learning} at {Facebook}: {A} {Datacenter} {Infrastructure} {Perspective}},
	isbn = {978-1-5386-3659-6},
	url = {https://research.fb.com/publications/applied-machine-learning-at-facebook-a-datacenter-infrastructure-perspective/},
	doi = {10.1109/HPCA.2018.00059},
	booktitle = {{HPCA}},
	author = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
	year = {2018},
	pmid = {23621169},
	note = {ISSN: 15300897},
}

@article{leesatapornwongsa_taxdc_2016,
	title = {{TaxDC}: {A} {Taxonomy} of {Non}-{Deterministic} {Concurrency} {Bugs} in {Datacenter} {Distributed} {Systems}},
	volume = {44},
	issn = {0163-5964},
	doi = {10.1145/2980024.2872374},
	abstract = {We present TaxDC, the largest and most comprehensive taxonomy of non-deterministic concurrency bugs in dis-tributed systems. We study 104 distributed concurrency (DC) bugs from four widely-deployed cloud-scale datacenter dis-tributed systems, Cassandra, Hadoop MapReduce, HBase and ZooKeeper. We study DC-bug characteristics along sev-eral axes of analysis such as the triggering timing condition and input preconditions, error and failure symptoms, and fix strategies, collectively stored as 2,083 classification labels in TaxDC database. We discuss how our study can open up many new research directions in combating DC bugs.},
	number = {2},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Leesatapornwongsa, Tanakorn and Lukman, Jeffrey F. and Lu, Shan and Gunawi, Haryadi S.},
	year = {2016},
	note = {ISBN: 9781450340915},
	keywords = {concurrency bugs, distributed systems, soft-},
	pages = {517--530},
}

@article{febrero_systematic_2014,
	title = {A systematic mapping study of software reliability modeling},
	volume = {56},
	issn = {09505849},
	url = {http://dx.doi.org/10.1016/j.infsof.2014.03.006},
	doi = {10.1016/j.infsof.2014.03.006},
	abstract = {Context Software Reliability (SR) is a highly active and dynamic research area. Published papers have approached this topic from various and heterogeneous points of view, resulting in a rich body of literature on this topic. The counterpart to this is the considerable complexity of this body of knowledge. Objective The objective of this study is to obtain a panorama and a taxonomy of Software Reliability Modeling (SRM). Method In order to do this, a Systematic Mapping Study (SMS) which analyzes and structures the literature on Software Reliability Modeling has been carried out. Results A total of 972 works were obtained as a result of the Systematic Mapping Study. On the basis of the more than 500 selected primary studies found, the results obtained show an increasing diversity of work. Conclusion Although it was discovered that Software Reliability Growth Models (SRGM) are still the most common modeling technique, it was also found that both the modeling based on static and architectural characteristics and the models based on Artificial Intelligence and automatic learning techniques are increasingly more apparent in literature. We have also observed that most Software Reliability Modeling efforts take place in the Pacific Rim area and in academic environments. Industrial initiatives are as yet marginal, and would appear to be primarily located in the USA. © 2014 Elsevier B.V. All rights reserved.},
	number = {8},
	journal = {Information and Software Technology},
	author = {Febrero, Felipe and Calero, Coral and Moraga, Ma Ángeles},
	year = {2014},
	note = {Publisher: Elsevier B.V.},
	keywords = {Modeling, Software Reliability, Systematic Mapping, Taxonomy},
	pages = {839--849},
}

@article{biesialska_big_2021,
	title = {Big {Data} analytics in {Agile} software development: {A} systematic mapping study},
	volume = {132},
	issn = {09505849},
	url = {https://doi.org/10.1016/j.infsof.2020.106448},
	doi = {10.1016/j.infsof.2020.106448},
	abstract = {Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.},
	number = {October 2020},
	journal = {Information and Software Technology},
	author = {Biesialska, Katarzyna and Franch, Xavier and Muntés-Mulero, Victor},
	year = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {Agile software development, Artificial intelligence, Data analytics, Literature review, Machine learning, Software analytics},
	pages = {106448},
}

@article{yang_how_2018,
	title = {How not to structure your database-backed web applications: {A} study of performance bugs in the wild},
	issn = {02705257},
	doi = {10.1145/3180155.3180194},
	abstract = {Many web applications use databases for persistent data storage, and using Object Relational Mapping (ORM) frameworks is a common way to develop such database-backed web applications. Unfortunately, developing efficient ORM applications is challenging, as the ORM framework hides the underlying database query generation and execution. This problem is becoming more severe as these applications need to process an increasingly large amount of persistent data. Recent research has targeted specific aspects of performance problems in ORM applications. However, there has not been any systematic study to identify common performance anti-patterns in real-world such applications, how they affect resulting application performance, and remedies for them. In this paper, we try to answer these questions through a comprehensive study of 12 representative real-world ORM applications. We generalize 9 ORM performance anti-patterns from more than 200 performance issues that we obtain by studying their bug-tracking systems and profiling their latest versions. To prove our point, we manually fix 64 performance issues in their latest versions and obtain a median speedup of 2× (and up to 39× max) with fewer than 5 lines of code change in most cases. Many of the issues we found have been confirmed by developers, and we have implemented ways to identify other code fragments with similar issues as well.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Yang, Junwen and Subramaniam, Pranav and Lu, Shan and Yan, Cong and Cheung, Alvin},
	year = {2018},
	note = {ISBN: 9781450356381},
	keywords = {Bug study, Database-backed applications, Object-relational mapping frameworks, Performance anti-patterns},
	pages = {800--810},
}

@article{thiruvathukal_concretizing_nodate,
	title = {Concretizing {Best} {Practices} using {Model} {Reconstruction} in {TensorFlow}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{guo_7_2017,
	title = {The 7 {Steps} of {Machine} {Learning} – {Towards} {Data} {Science}},
	abstract = {From detecting skin cancer, to sorting cucumbers, to detecting escalators in need of repairs, machine learning has granted computer systems entirely new abilities. But how does it really work under the hood? Let’s walk through a basic example, and use it as an excuse talk about the process of getting answers from your data using machine learning.},
	journal = {Towardsdatascience},
	author = {Guo, Yufeng},
	year = {2017},
	keywords = {BT-Machine Learning workflow},
	pages = {1--15},
}

@article{agrawal_cloudy_2019,
	title = {Cloudy with high chance of {DBMS}: {A} 10-year prediction for {Enterprise}-{Grade} {ML}},
	issn = {23318422},
	abstract = {Machine learning (ML) has proven itself in high-value web applications such as search ranking and is emerging as a powerful tool in a much broader range of enterprise scenarios including voice recognition and conversational understanding for customer support, autotuning for videoconferencing, inteligent feedback loops in large-scale sysops, manufacturing and autonomous vehicle management, complex financial predictions, just to name a few. Meanwhile, as the value of data is increasingly recognized and monetized, concerns about securing valuable data and risks to individual privacy have been growing. Consequently, rigorous data management has emerged as a key requirement in enterprise settings. How will these trends (ML growing popularity, and stricter data governance) intersect? What are the unmet requirements for applying ML in enterprise settings? What are the technical challenges for the DB community to solve? In this paper, we present our vision of how ML and database systems are likely to come together, and early steps we take towards making this vision a reality.},
	journal = {arXiv},
	author = {Agrawal, Ashvin and Chatterjee, Rony and Curino, Carlo and Floratou, Avrilia and Gowdal, Neha and Interlandi, Matteo and Jindal, Alekh and Karanasos, Kostantinos and Krishnan, Subru and Kroth, Brian and Leeka, Jyoti and Park, Kwanghyun and Patel, Hiren and Poppe, Olga and Psallidas, Fotis and Ramakrishnan, Raghu and Roy, Abhishek and Saur, Karla and Sen, Rathijit and Weimer, Markus and Wright, Travis and Zhu, Yiwen},
	year = {2019},
}

@book{kruchten_agile_2018,
	title = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}: 19th {International} {Conference}, {XP} 2018, {Porto}, {Portugal}, {May} 21–25, 2018, {Proceedings}},
	volume = {314},
	isbn = {978-3-319-91601-9},
	url = {http://link.springer.com/10.1007/978-3-319-91602-6},
	abstract = {This open access book constitutes the proceedings of the 19th International Conference on Agile Software Development, XP 2018, held in Porto, Portugal, in May 2018. XP is the premier agile software development conference combining research and practice, and XP 2018 provided a playful and informal environment to learn and trigger discussions around its main theme – make, inspect, adapt. The 21 papers presented in this volume were carefully reviewed and selected from 62 submissions. They were organized in topical sections named: agile requirements; agile testing; agile transformation; scaling agile; human-centric agile; and continuous experimentation.},
	author = {Kruchten, Philippe and Eds, François Coallier},
	year = {2018},
	doi = {10.1007/978-3-030-19034-7},
}

@article{bibliographies_102720_2019,
	title = {10/27/20 {Agenda}},
	author = {Bibliographies, Annotated and Engineering, Software and Learning, Machine and Study, Case},
	year = {2019},
}

@article{allamanis_survey_2017,
	title = {A survey of machine learning for big code and naturalness},
	volume = {51},
	issn = {23318422},
	abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit code's abundance of patterns. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss crosscutting and application-specific challenges and opportunities.},
	number = {4},
	journal = {arXiv},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	year = {2017},
	keywords = {Big Code, Code Naturalness, Machine Learning, Software Engineering Tools},
}

@article{xie_survey_2019,
	title = {A survey of machine learning techniques applied to software defined networking ({SDN}): {Research} issues and challenges},
	volume = {21},
	issn = {1553877X},
	doi = {10.1109/COMST.2018.2866942},
	abstract = {In recent years, with the rapid development of current Internet and mobile communication technologies, the infrastructure, devices and resources in networking systems are becoming more complex and heterogeneous. In order to efficiently organize, manage, maintain and optimize networking systems, more intelligence needs to be deployed. However, due to the inherently distributed feature of traditional networks, machine learning techniques are hard to be applied and deployed to control and operate networks. Software defined networking (SDN) brings us new chances to provide intelligence inside the networks. The capabilities of SDN (e.g., logically centralized control, global view of the network, software-based traffic analysis, and dynamic updating of forwarding rules) make it easier to apply machine learning techniques. In this paper, we provide a comprehensive survey on the literature involving machine learning algorithms applied to SDN. First, the related works and background knowledge are introduced. Then, we present an overview of machine learning algorithms. In addition, we review how machine learning algorithms are applied in the realm of SDN, from the perspective of traffic classification, routing optimization, quality of service/quality of experience prediction, resource management and security. Finally, challenges and broader perspectives are discussed.},
	number = {1},
	journal = {IEEE Communications Surveys and Tutorials},
	author = {Xie, Junfeng and Yu, F. Richard and Huang, Tao and Xie, Renchao and Liu, Jiang and Wang, Chenmeng and Liu, Yunjie},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Machine learning, Resource management, Software defined networking, Traffic classification},
	pages = {393--430},
}

@article{roh_survey_2018,
	title = {A survey on data collection for machine learning: {A} big data - {AI} integration perspective},
	volume = {4347},
	issn = {23318422},
	doi = {10.1109/tkde.2019.2946162},
	abstract = {Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.},
	number = {c},
	journal = {arXiv},
	author = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {Data acquisition, Data collection, Data labeling, Machine learning},
	pages = {1--20},
}

@article{this_rules_nodate,
	title = {Rules of {Machine} {Learning} : {Best} {Practices} for {ML} {Engineering}},
	abstract = {This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine?learned model, then you have the necessary background to read this document. Terminology},
	author = {This, Martin Zinkevich and Guide, Style},
}

@article{sugimura_building_2018,
	title = {Building a reproducible machine learning pipeline},
	issn = {23318422},
	abstract = {Reproducibility of modeling is a problem that exists for any machine learning practitioner, whether in industry or academia. The consequences of an irreproducible model can include significant financial costs, lost time, and even loss of personal reputation (if results prove unable to be replicated). This paper will first discuss the problems we have encountered while building a variety of machine learning models, and subsequently describe the framework we built to tackle the problem of model reproducibility. The framework is comprised of four main components (data, feature, scoring, and evaluation layers), which are themselves comprised of well defined transformations. This enables us to not only exactly replicate a model, but also to reuse the transformations across different models. As a result, the platform has dramatically increased the speed of both offline and online experimentation while also ensuring model reproducibility.},
	journal = {arXiv},
	author = {Sugimura, Peter and Hartl, Florian},
	year = {2018},
}

@article{parker_software_nodate,
	title = {Software 2.0},
	author = {Parker, David},
}

@article{arpteg_software_2018,
	title = {Software engineering challenges of deep learning},
	doi = {10.1109/SEAA.2018.00018},
	abstract = {Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.},
	journal = {Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018},
	author = {Arpteg, Anders and Brinne, Björn and Crnkovic-Friis, Luka and Bosch, Jan},
	year = {2018},
	note = {ISBN: 9781538673829
Publisher: IEEE},
	keywords = {Artificial intelligence, Deep learning, Machine learning, Software engineering challenges},
	pages = {50--59},
}

@article{shafiq_machine_2020,
	title = {Machine learning for software engineering: {A} systematic mapping},
	issn = {23318422},
	abstract = {Context: The software development industry is rapidly adopting machine learning for transitioning modern day software systems towards highly intelligent and self-learning systems. However, the full potential of machine learning for improving the software engineering life cycle itself is yet to be discovered, i.e., up to what extent machine learning can help reducing the effort/complexity of software engineering and improving the quality of resulting software systems. To date, no comprehensive study exists that explores the current state-of-the-art on the adoption of machine learning across software engineering life cycle stages. Objective: This article addresses the aforementioned problem and aims to present a state-of-the-art on the growing number of uses of machine learning in software engineering. Method: We conduct a systematic mapping study on applications of machine learning to software engineering following the standard guidelines and principles of empirical software engineering. Results: This study introduces a machine learning for software engineering (MLSE) taxonomy classifying the state-of-the-art machine learning techniques according to their applicability to various software engineering life cycle stages. Overall, 227 articles were rigorously selected and analyzed as a result of this study. Conclusion: From the selected articles, we explore a variety of aspects that should be helpful to academics and practitioners alike in understanding the potential of adopting machine learning techniques during software engineering projects.},
	journal = {arXiv},
	author = {Shafiq, Saad and Mashkoor, Atif and Mayr-Dorn, Christoph and Egyed, Alexander},
	year = {2020},
	keywords = {Machine learning, Software engineering, Systematic mapping},
}

@article{zhang_machine_2019,
	title = {Machine learning testing: {Survey}, landscapes and horizons},
	volume = {X},
	issn = {23318422},
	doi = {10.1109/tse.2019.2962027},
	abstract = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 128 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.},
	number = {X},
	journal = {arXiv},
	author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
	year = {2019},
	keywords = {Deep neural network, Machine learning, Software testing},
}

@article{lee_human---loop_2019,
	title = {A {Human}-in-the-loop {Perspective} on {AutoML}: {Milestones} and the {Road} {Ahead}},
	journal = {Data Engineering},
	author = {Lee, Doris Jung-Lin and Macke, Stephen and Xin, Doris and Lee, Angela and Huang, Silu and Parameswaran, Aditya},
	year = {2019},
	pages = {58},
}

@article{olukotun_designing_2018,
	title = {Designing {Computer} {Systems} for {Software} 2.0 ({Tutorial})},
	journal = {Nips 2018},
	author = {Olukotun, Kunle},
	year = {2018},
}

@article{zhang_ml_2003,
	title = {{ML} and {SE}},
	volume = {7491},
	number = {02},
	author = {ZHANG, DU and J.P.TSAI, JEFFREY},
	year = {2003},
	keywords = {minke whales, north atlantic, organochlorine pesticides, pcbs, stock delineation},
	pages = {87--119},
}

@article{carbin_overparameterization_2019,
	title = {Overparameterization: {A} connection between software 1.0 and software 2.0},
	volume = {136},
	issn = {18688969},
	doi = {10.4230/LIPIcs.SNAPL.2019.1},
	abstract = {A new ecosystem of machine-learning driven applications, titled Software 2.0, has arisen that integrates neural networks into a variety of computational tasks. Such applications include image recognition, natural language processing, and other traditional machine learning tasks. However, these techniques have also grown to include other structured domains, such as program analysis and program optimization for which novel, domain-specific insights mate with model design. In this paper, we connect the world of Software 2.0 with that of traditional software – Software 1.0 –through overparameterization: a program may provide more computational capacity and precision than is necessary for the task at hand. In Software 2.0, overparamterization – when a machine learning model has more parameters than datapoints in the dataset – arises as a contemporary understanding of the ability for modern, gradient-based learning methods to learn models over complex datasets with high-accuracy. Specifically, the more parameters a model has, the better it learns. In Software 1.0, the results of the approximate computing community show that traditional software is also overparameterized in that software often simply computes results that are more precise than is required by the user. Approximate computing exploits this overparameterization to improve performance by eliminating unnecessary, excess computation. For example, one – of many techniques – is to reduce the precision of arithmetic in the application. In this paper, we argue that the gap between available precision and that that is required for either Software 1.0 or Software 2.0 is a fundamental aspect of software design that illustrates the balance between software designed for general-purposes and domain-adapted solutions. A general-purpose solution is easier to develop and maintain versus a domain-adapted solution. However, that ease comes at the expense of performance. We show that the approximate computing community and the machine learning community have developed overlapping techniques to improve performance by reducing overparameterization. We also show that because of these shared techniques, questions, concerns, and answers on how to construct software can translate from one software variant to the other.},
	number = {1},
	journal = {Leibniz International Proceedings in Informatics, LIPIcs},
	author = {Carbin, Michael},
	year = {2019},
	note = {ISBN: 9783959771139},
	keywords = {Approximate computing, Machine learning, Software 2.0},
	pages = {1--13},
}

@article{nakamichi_requirements-driven_2020,
	title = {Requirements-driven method to determine quality characteristics and measurements for machine learning software and its evaluation},
	volume = {2020-Augus},
	issn = {23326441},
	doi = {10.1109/RE48521.2020.00036},
	abstract = {As the applications of machine learning algorithms in various fields are widely demanded, the development of machine learning software systems (MLS) is rapidly increasing. The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise. In this paper, we propose a requirements-driven method to determine the quality characteristics of the MLS. Major contributions of this paper include: (1) Extending the quality characteristics of ISO 25010, which defines the conventional software quality, to those unique to MLS; this paper also defines its measuring method. (2) A method to identify requirements, i.e., issues to be determined in the requirements definition, in order to derive the quality characteristics and measurement methods for MLS, since the quality characteristics and the measurement method depend on the goals of the system under development. In order to evaluate the proposed method, we carried out an empirical study of the quality characteristics and measurement methods related to functional correctness and the maturity of the MLS for the enterprise. Based on the study, we compare the quality characteristics and measurement methods derived by the proposed method with those suggested by developers, and demonstrate the effectiveness of the proposed method.},
	journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
	author = {Nakamichi, Koji and Ohashi, Kyoko and Namba, Isao and Yamamoto, Rieko and Aoyama, Mikio and Joeckel, Lisa and Siebert, Julien and Heidrich, Jens},
	year = {2020},
	note = {ISBN: 9781728174389},
	keywords = {machine learning, quality assurance, quality characteristics, quality measures, quality requirements, software quality model},
	pages = {260--270},
}

@article{rupprecht_ursprung_2019,
	title = {Ursprung: {Provenance} for large-scale analytics environments},
	issn = {07308078},
	doi = {10.1145/3299869.3320235},
	abstract = {Modern analytics has produced wonders, but reproducing and verifying these wonders is difficult. Data provenance helps to solve this problem by collecting information on how data is created and accessed. Although provenance collection techniques have been used successfully on a smaller scale, tracking provenance in large-scale analytics environments is challenging due to the scale of provenance generated and the heterogeneous domains. Without provenance, analysts struggle to keep track of and reproduce their analyses. We demonstrate Ursprung1, a provenance collection system specifically targeted at such environments. Ursprung transparently collects the minimal set of system-level provenance required to track the relationships between data and processes. To collect domain specific provenance, Ursprung enables users to specify capture rules to curate application-specific logs, intermediate results etc. To reduce storage overhead and accelerate queries, it uses event hierarchies to synthesize raw provenance into compact summaries.},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	author = {Rupprecht, Lukas and Lubbock, Alexander and Davis, James C. and Tyson, Darren and Arnold, Constantine and Bhagwat, Deepavali},
	year = {2019},
	note = {ISBN: 9781450356435},
	pages = {1989--1992},
}

@article{qin_prediction-guided_2020,
	title = {Prediction-{Guided} {Design} for {Software} {Systems}},
	abstract = {While software system development is commonly conducted with explicit rules, machine learning (ML) has been driving a revolution in modern system design. In this paper, we in- troduce a new prediction-guided paradigm, which leverages ML techniques to support decision-makings for the system itself. In the proposed design, the system would be automat- ically driven by various type of data, e.g., system workloads, user behaviors, and platform operations, etc. More impor- tantly, it brings a mindset of “proactive” to developers. Some significant issues can be thus eliminated before becoming catastrophe. In order to illustrate the benefits of the proposed paradigm, we present a project showcase, intelligent buffer management, which is used to achieve an optimal trade-off between having sufficiently large buffers to avoid failures and minimizing excess capacity in Microsoft Azure. It is designed in the prediction-guided paradigm to dynamically and proac- tively adjust the reserved buffer based on customer workload patterns and platform operations. The project not only sig- nificantly improves CFR (capacity fulfillment reliability) of tenant growth, but also reduces millions of dollars in COGS (cost of goods sold) for Microsoft. Prediction-Guided},
	number = {Ml},
	author = {Qin, Si and Xu, Yong and Zhou, Shandan and Lin, Qingwei and Zhang, Hongyu and Agarwal, Saurabh and Subramanian, Karthikeyan and Cortez, Eli and Miller, John and Cowdery, Chris and Kemburu, Shanti and Zhang, Dongmei and Moscibroda, Thomas},
	year = {2020},
	pages = {1--2},
}

@article{bajic_compute_nodate,
	title = {Compute substrate for {Software} 2 . 0 {Moore} ’ s {Law}},
	author = {Bajić, Ljubisa and Vasiljević, Jasmina},
}

@article{weidele_autoaiviz_2020,
	title = {{AutoAIViz}: {Opening} the blackbox of automated artificial intelligence with conditional parallel coordinates},
	doi = {10.1145/3377325.3377538},
	abstract = {Artificial Intelligence (AI) can now automate the algorithm selection, feature engineering, and hyperparameter tuning steps in a machine learning workflow. Commonly known as AutoML or AutoAI, these technologies aim to relieve data scientists from the tedious manual work. However, today's AutoAI systems often present only limited to no information about the process of how they select and generate model results. Thus, users often do not understand the process, neither do they trust the outputs. In this short paper, we provide a first user evaluation by 10 data scientists of an experimental system, AutoAIViz, that aims to visualize AutoAI's model generation process. We find that the proposed system helps users to complete the data science tasks, and increases their understanding, toward the goal of increasing trust in the AutoAI system.},
	journal = {International Conference on Intelligent User Interfaces, Proceedings IUI},
	author = {Weidele, Daniel Karl I. and Weisz, Justin D. and Oduor, Erick and Muller, Michael and Andres, Josh and Gray, Alexander and Wang, Dakuo},
	year = {2020},
	note = {ISBN: 9781450371186},
	keywords = {AutoAI, AutoML, democratizing AI, human-AI collaboration, parallel coordinates, visualization},
	pages = {308--312},
}

@article{tanaka_data_2015,
	title = {Data {Driven} {Debugging}},
	author = {Tanaka, Daigo},
	year = {2015},
	pages = {1--5},
}

@article{polyzotis_data_2017,
	title = {Data management challenges in production machine learning},
	volume = {Part F1277},
	issn = {07308078},
	doi = {10.1145/3035918.3054782},
	abstract = {The tutorial discusses data-management issues that arise in the context of machine learning pipelines deployed in production. Informed by our own experience with such largescale pipelines, we focus on issues related to understanding, validating, cleaning, and enriching training data. The goal of the tutorial is to bring forth these issues, draw connections to prior work in the database literature, and outline the open research questions that are not addressed by prior art.},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
	year = {2017},
	note = {ISBN: 9781450341974},
	pages = {1723--1726},
}

@article{alberti_deepdiva_2018,
	title = {{DeepDIVA}: {A} highly-functional python framework for reproducible experiments},
	volume = {2018-Augus},
	issn = {21676453},
	doi = {10.1109/ICFHR-2018.2018.00080},
	abstract = {We introduce DeepDIVA: An infrastructure designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality. Reproducing scientific results can be a frustrating experience, not only in document image analysis but in machine learning in general. Using DeepDIVA a researcher can either reproduce a given experiment or share their own experiments with others. Moreover, the framework offers a large range of functions, such as boilerplate code, keeping track of experiments, hyper-parameter optimization, and visualization of data and results. To demonstrate the effectiveness of this framework, this paper presents case studies in the area of handwritten document analysis where researchers benefit from the integrated functionality. DeepDIVA is implemented in Python and uses the deep learning framework PyTorch. It is completely open source, and accessible as Web Service through DIVAServices.},
	journal = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
	author = {Alberti, Michele and Pondenkandath, Vinaychandran and Wursch, Marcel and Ingold, Rolf and Liwicki, Marcus},
	year = {2018},
	note = {ISBN: 9781538658758},
	keywords = {Deep Learning, Framework, Hyper paramters Optimization, Machine Learning, Neural Networks, Open-source, Python, Reproducible Research},
	pages = {423--428},
}

@article{rezig_dagger_2020,
	title = {Dagger: {A} {Data} (not code) {Debugger}},
	url = {http://cidrdb.org/cidr2020/papers/p35-rezig-cidr20.pdf},
	abstract = {With the democratization of data science libraries and frameworks, most data scientists manage and generate their data analytics pipelines using a collection of scripts (e.g., Python, R). This marks a shift from traditional applications that communicate back and forth with a DBMS that stores and manages the application data. While code debuggers have reached impressive maturity over the past decades, they fall short in assisting users to explore data-driven what-if scenarios (e.g., split the training set into two and build two ML models). Those scenarios, while doable programmatically, are a substantial burden for users to manage themselves. Dagger (Data Debugger) is an end-to-end data debugger that abstracts key data-centric primitives to enable users to quickly identify and mitigate data-related problems in a given pipeline. Dagger was motivated by a series of interviews we conducted with data scientists across several organizations. A preliminary version of Dagger has been incorporated into Data Civilizer 2.0 to help physicians at the Massachusetts General Hospital process complex pipelines.},
	number = {1},
	journal = {\{CIDR\} 2020, 10th Conference on Innovative Data Systems Research, Amsterdam, The Netherlands, January 12-15, 2020, Online Proceedings},
	author = {Rezig, El Kindi and Cao, Lei and Simonini, Giovanni and Schoemans, Maxime and Madden, Samuel and Tang, Nan and Ouzzani, Mourad and Stonebraker, Michael},
	year = {2020},
}

@article{hall_editorial_2010,
	title = {Editorial: {Data} mining in software engineering},
	volume = {17},
	issn = {15737535},
	doi = {10.1007/s10515-010-0073-9},
	number = {4},
	journal = {Automated Software Engineering},
	author = {Hall, Robert J.},
	year = {2010},
	note = {ISBN: 1051501000739},
	pages = {373--374},
}

@article{wan_how_2019,
	title = {How does {Machine} {Learning} {Change} {Software} {Development} {Practices}?},
	volume = {PP},
	issn = {0098-5589},
	doi = {10.1109/tse.2019.2937083},
	abstract = {Adding an ability for a system to learn inherently adds non-determinism into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work features (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.},
	number = {c},
	journal = {IEEE Transactions on Software Engineering},
	author = {Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {1--1},
}

@article{vivanti_search-based_nodate,
	title = {Search-based {Data}-flow {Test} {Generation}},
	abstract = {Coverage criteria based on data-flow have long been discussed in the literature, yet to date they are still of surprising little practical relevance. This is in part because 1) manually writing a unit test for a data-flow aspect is more challenging than writing a unit test that simply covers a branch or statement, 2) there is a lack of tools to support data-flow testing, and 3) there is a lack of empirical evidence on how well data-flow testing scales in practice. To overcome these problems, we present 1) a search- based technique to automatically generate unit tests for data-flow criteria, 2) an implementation of this technique in the EVOSUITE test generation tool, and 3) a large empirical study applying this tool to the SF100 corpus of 100 open source Java projects. On average, the number of coverage objectives is three times as high as for branch coverage. However, the level of coverage achieved by EVOSUITE is comparable to other criteria, and the increase in size is only 15\%, leading to higher mutation scores. These results counter the common assumption that data-flow testing does not scale, and should help to re-establish data-flow testing as a viable alternative in practice.},
	author = {Vivanti, Mattia and Mis, Andre and Gorla, Alessandra and Fraser, Gordon},
	keywords = {-data-flow coverage, search based testing, unit testing},
}

@article{re_software_2018,
	title = {Software 2 . 0 and {Snorkel} : {Beyond} {Hand}-{Labeled} {Data}},
	volume = {11},
	number = {3},
	author = {Ré, Christopher},
	year = {2018},
	note = {ISBN: 9781450355520},
	pages = {3575},
}

@article{lwakatare_large-scale_2020,
	title = {Large-scale machine learning systems in real-world industrial settings: {A} review of challenges and solutions},
	volume = {127},
	issn = {09505849},
	doi = {10.1016/j.infsof.2020.106368},
	abstract = {Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.},
	number = {July},
	journal = {Information and Software Technology},
	author = {Lwakatare, Lucy Ellen and Raj, Aiswarya and Crnkovic, Ivica and Bosch, Jan and Olsson, Helena Holmström},
	year = {2020},
	keywords = {Challenges, Industrial settings, Machine learning systems, SLR, Software engineering, Solutions},
}

@article{oreilly_what_2012,
	title = {What is web 2.0?: {Design} patterns and business models for the next generation of software},
	abstract = {This paper was the first initiative to try to define Web2.0 and understand its implications for the next generation of software, looking at both design patterns and business modes. Web 2.0 is the network as platform, spanning all connected devices; Web 2.0 applications are those that make the most of the intrinsic advantages of that platform: delivering software as a continually-updated service that gets better the more people use it, consuming and remixing data from multiple sources, including individual users, while providing their own data and services in a form that allows remixing by others, creating network effects through an "architecture of participation," and going beyond the page metaphor of Web 1.0 to deliver rich user experiences.},
	number = {65},
	journal = {The Social Media Reader},
	author = {O'Reilly, Tim},
	year = {2012},
	note = {ISBN: 0814764053},
	keywords = {collective intelligence, data, long tail and, rich client, software as a service},
	pages = {32--52},
}

@article{vivek_exemplars_nodate,
	title = {Exemplars for {Machine} {Learning} : {Towards} {Software} {Engineering} and {Reproducibility}},
	author = {Vivek, Naveen and Chinnakotla, Akhil and Banna, Vishnu and Vegesana, Anirudh and Yan, Tristan and Davis, James and Lu, Yung-hsiang and Thiruvathukal, George K.},
	keywords = {best practices, computer vision, ducibility, engineers and scientists across, machine, machine learning, many industries are adopting, repro-, software engineering, tensorflow},
	pages = {2},
}

@article{microsoft_database_nodate,
	title = {Database {Systems} 2 . 0},
	author = {Microsoft, Johannes Gehrke},
	pages = {2399},
}

@article{gulzar_bigdebug_2016,
	title = {{BigDebug}: {Debugging} primitives for interactive big data processing in},
	volume = {14-22-May-},
	issn = {02705257},
	doi = {10.1145/2884781.2884813},
	abstract = {Developers use cloud computing platforms to process a large quantity of data in parallel when developing big data analytics. Debugging the massive parallel computations that run in today's datacenters is time consuming and error-prone. To address this challenge, we design a set of interactive, real-time debugging primitives for big data processing in Apache Spark, the next generation data-intensive scalable cloud computing platform. This requires rethinking the notion of step-through debugging in a traditional debugger such as gdb, because pausing the entire computation across distributed worker nodes causes significant delay and naively inspecting millions of records using a watchpoint is too time consuming for an end user. First, BIGDEBUG's simulated breakpoints and on-demand watchpoints allow users to selectively examine distributed, intermediate data on the cloud with little overhead. Second, a user can also pinpoint a crash-inducing record and selectively resume relevant sub-computations after a quick fix. Third, a user can determine the root causes of errors (or delays) at the level of individual records through a fine-grained data provenance capability. Our evaluation shows that BIGDEBUG scales to terabytes and its record-level tracing incurs less than 25\% overhead on average. It determines crash culprits orders of magnitude more accurately and provides up to 100\% time saving compared to the baseline replay debugger. The results show that BIGDEBUG supports debugging at interactive speeds with minimal performance impact.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Gulzar, Muhammad Ali and Interlandi, Matteo and Yoo, Seunghyun and Tetali, Sai Deep and Condie, Tyson and Millstein, Todd and Kim, Miryung},
	year = {2016},
	note = {ISBN: 9781450339001},
	keywords = {Big data analytics, Data-intensive scalable computing (DISC), Debugging, Fault localization and recovery, Interactive tools},
	pages = {784--795},
}

@article{camino_improving_2019,
	title = {Improving missing data imputation with deep generative models},
	issn = {23318422},
	abstract = {Datasets with missing values are very common on industry applications, and they can have a negative impact on machine learning models. Recent studies introduced solutions to the problem of imputing missing values based on deep generative models. Previous experiments with Generative Adversarial Networks and Variational Autoencoders showed interesting results in this domain, but it is not clear which method is preferable for different use cases. The goal of this work is twofold: we present a comparison between missing data imputation solutions based on deep generative models, and we propose improvements over those methodologies. We run our experiments using known real life datasets with different characteristics, removing values at random and reconstructing them with several imputation techniques. Our results show that the presence or absence of categorical variables can alter the selection of the best model, and that some models are more stable than others after similar runs with different random number generator seeds.},
	journal = {arXiv},
	author = {Camino, Ramiro D. and Hammerschmidt, Christian A. and State, Radu},
	year = {2019},
}

@article{menzies_five_2020,
	title = {The {Five} {Laws} of {SE} for {AI}},
	volume = {37},
	issn = {19374194},
	doi = {10.1109/MS.2019.2954841},
	abstract = {It is time to talk about software engineering (SE) for artificial intelligence (AI). As shown in Figure 1, industry is becoming increasingly dependent on AI software. Clearly, AI is useful for SE. But what about the other way around? How important is SE for AI? Many thought leaders in the AI industry are asking how to better develop and maintain AI software (see Figure 2).},
	number = {1},
	journal = {IEEE Software},
	author = {Menzies, Tim},
	year = {2020},
	pages = {81--85},
}

@book{horak_raslan_2013,
	title = {{RASLAN} 2013 {Recent} {Advances} in {Slavonic} {Natural} {Language} {Processing}},
	isbn = {978-80-263-0520-0},
	author = {Horak, P. Rychly A.},
	year = {2013},
}

@article{souza_workflow_2020,
	title = {Workflow provenance in the lifecycle of scientific machine learning},
	issn = {23318422},
	abstract = {Machine Learning (ML) has already fundamentally changed several businesses. More recently, it has also been profoundly impacting the computational science and engineering domains, like geoscience, climate science, and health science. In these domains, users need to perform comprehensive data analyses combining scientific data and ML models to provide for critical requirements, such as reproducibility, model explainability, and experiment data understanding. However, scientific ML is multidisciplinary, heterogeneous, and affected by the physical constraints of the domain, making such analyses even more challenging. In this work, we leverage workflow provenance techniques to build a holistic view to support the lifecycle of scientific ML. We contribute with (i) characterization of the lifecycle and taxonomy for data analyses; (ii) design principles to build this view, with a W3C PROV compliant data representation and a reference system architecture; and (iii) lessons learned after an evaluation in an Oil \& Gas case using an HPC cluster with 393 nodes and 946 GPUs. The experiments show that the principles enable queries that integrate domain semantics with ML models while keeping low overhead ({\textless}1\%), high scalability, and an order of magnitude of query acceleration under certain workloads against without our representation. MSC Codes 65Y05, 68P15},
	journal = {arXiv},
	author = {Souza, Renan and Azevedo, Leonardo G. and Lourenço, Vítor and Soares, Elton and Thiago, Raphael and Brandão, Rafael and Civitarese, Daniel and Brazil, Emilio Vital and Moreno, Marcio and Valduriez, Patrick and Mattoso, Marta and Cerqueira, Renato and Netto, Marco A. S.},
	year = {2020},
	keywords = {Artificial Intelligence, Data Science, Data lake, Design Principles, E-Science, Explainability, Lineage, Machine Learning Lifecycle, Provenance, Reproducibility, Scientific Machine Learning, Scientific Workflow, Taxonomy},
	pages = {1--21},
}

@article{rezig_data_2018,
	title = {Data civilizer 2.0: {A} holistic framework for data preparation and analytics},
	volume = {12},
	issn = {21508097},
	doi = {10.14778/3352063.3352108},
	abstract = {Data scientists spend over 80\% of their time (1) parameter-tuning machine learning models and (2) iterating between data cleaning and machine learning model execution. While there are existing efforts to support the first requirement, there is currently no integrated workflow system that couples data cleaning and machine learning development. The previous version of Data Civilizer was geared towards data cleaning and discovery using a set of pre-defined tools. In this paper, we introduce Data Civilizer 2.0, an end-to-end workflow system satisfying both requirements. In addition, this system also supports a sophisticated data debugger and a workflow visualization system. In this demo, we will show how we used Data Civilizer 2.0 to help scientists at the Massachusetts General Hospital build their cleaning and machine learning pipeline on their 30TB brain activity dataset.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Rezig, El Kindi and Cao, Lei and Stonebraker, Michael and Simonini, Giovanni and Tao, Wenbo and Madden, Samuel and Ouzzani, Mourad and Tang, Nan and Elmagarmid, Ahmed K.},
	year = {2018},
	pages = {1954--1957},
}

@article{ratner_role_2019,
	title = {The role of massively multi-task and weak supervision in software 2.0},
	abstract = {Over the last several years, machine learning models have reached new levels of empirical performance across a broad range of domains. Driven both by accuracy improvements and deployment advantages, many organizations have begun to shift to learning-centered software stacks—a new mode that has been called Software 2.0. This approach holds the promise of radically accelerating the construction, maintenance, and deployment of software systems, and opens up a broad research agenda around changes to hardware, systems, and interaction models. However, these approaches require one critical and often prohibitively expensive ingredient: labeled training data. We outline a vision for a Software 2.0 lifecycle centered around the idea that labeling training data can be the primary interface to Software 2.0 systems. In our envisioned approach, Software 2.0 stacks are programmed using weak supervision—i.e. noisier, programmatically-generated training data—which is specified at various levels of declarative abstraction and precision, and then combined using unsupervised statistical techniques. The codebase for Software 2.0 is also radically different: we envision labels for tens or hundreds of different tasks across an organization combined in a massively multitask central model, leading to amortization of labeling costs and new models of software reuse and development. Finally, we envision Software 2.0 stacks deployed by using collected training labels to supervise commodity model architectures over different servable feature sets. We outline the components of this lifecycle, and provide an interim report on Snorkel, our prototype Software 2.0 system, based on our experiences working on problems ranging from ad fraud to medical diagnostics with some of the world’s largest organizations.},
	journal = {CIDR 2019 - 9th Biennial Conference on Innovative Data Systems Research},
	author = {Ratner, Alexander and Hancock, Braden and Ré, Christopher},
	year = {2019},
}

@article{yao_taking_2018,
	title = {Taking the human out of learning applications: {A} survey on automated machine learning},
	issn = {23318422},
	abstract = {Machine learning techniques have deeply rooted in our everyday life. However, since it is knowledge- and labor-intensive to pursue good learning performance, humans are heavily involved in every aspect of machine learning. To make machine learning techniques easier to apply and reduce the demand for experienced human experts, automated machine learning (AutoML) has emerged as a hot topic with both industrial and academic interest. In this paper, we provide an up to date survey on AutoML. First, we introduce and define the AutoML problem, with inspiration from both realms of automation and machine learning. Then, we propose a general AutoML framework that not only covers most existing approaches to date, but also can guide the design for new methods. Subsequently, we categorize and review the existing works from two aspects, i.e., the problem setup and the employed techniques. The proposed framework and taxonomies provide a detailed analysis of AutoML approaches and explain the reasons underneath their successful applications. We hope this survey can serve as not only an insightful guideline for AutoML beginners but also an inspiration for future research.},
	journal = {arXiv},
	author = {Yao, Quanming and Wang, Mengshuo and Chen, Yuqiang and Dai, Wenyuan and Hu, Yi Qi and Li, Yu Feng and Tu, Wei Wei and Yang, Qiang and Yu, Yang},
	year = {2018},
	keywords = {Automated machine learning, Hyper-parameter optimization, Meta-learning, Neural architecture search, Transfer-learning},
	pages = {1--20},
}

@article{dam_towards_2019,
	title = {Towards effective {AI}-powered agile project management},
	doi = {10.1109/ICSE-NIER.2019.00019},
	abstract = {The rise of Artificial intelligence (AI) has the potential to significantly transform the practice of project management. Project management has a large socio-technical element with many uncertainties arising from variability in human aspects, e.g. customers' needs, developers' performance and team dynamics. AI can assist project managers and team members by automating repetitive, high-volume tasks to enable project analytics for estimation and risk prediction, providing actionable recommendations, and even making decisions. AI is potentially a game changer for project management in helping to accelerate productivity and increase project success rates. In this paper, we propose a framework where AI technologies can be leveraged to offer support for managing agile projects, which have become increasingly popular in the industry.},
	journal = {Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2019},
	author = {Dam, Hoa Khanh and Tran, Truyen and Grundy, John and Ghose, Aditya and Kamei, Yasutaka},
	year = {2019},
	note = {ISBN: 9781728117584},
	keywords = {Artificial Intelligence, Software Engineering},
	pages = {41--44},
}

@article{odegua_how_2020,
	title = {How {To} {Put} {Machine} {Learning} {Models} {Into} {Production}},
	volume = {28},
	number = {2},
	journal = {Stack Overflow},
	author = {Odegua, Rising},
	year = {2020},
	pages = {1--19},
}

@article{sekhon_towards_2019,
	title = {Towards improved testing for deep learning},
	doi = {10.1109/ICSE-NIER.2019.00030},
	abstract = {The growing use of deep neural networks in safety-critical applications makes it necessary to carry out adequate testing to detect and correct any incorrect behavior for corner case inputs before they can be actually used. Deep neural networks lack an explicit control-flow structure, making it impossible to apply to them traditional software testing criteria such as code coverage. In this paper, we examine existing testing methods for deep neural networks, the opportunities for improvement and the need for a fast, scalable, generalizable end-to-end testing method. We also propose a coverage criterion for deep neural networks that tries to capture all possible parts of the deep neural network's logic.},
	journal = {Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2019},
	author = {Sekhon, Jasmine and Fleming, Cody},
	year = {2019},
	note = {ISBN: 9781728117584},
	keywords = {Coverage criterion, Deep neural networks, Whitebox testing},
	pages = {85--88},
}

@phdthesis{islam_towards_2020,
	type = {Doctor of {Philosophy}},
	title = {Towards understanding the challenges faced by machine learning software developers and enabling automated solutions},
	url = {https://lib.dr.iastate.edu/etd/18149},
	language = {en},
	urldate = {2021-10-04},
	school = {Iowa State University},
	author = {Islam, Md Johirul},
	year = {2020},
	doi = {10.31274/etd-20200902-68},
}

@inproceedings{wan_bug_2017,
	address = {Buenos Aires, Argentina},
	title = {Bug {Characteristics} in {Blockchain} {Systems}: {A} {Large}-{Scale} {Empirical} {Study}},
	isbn = {978-1-5386-1544-7},
	shorttitle = {Bug {Characteristics} in {Blockchain} {Systems}},
	url = {http://ieeexplore.ieee.org/document/7962390/},
	doi = {10.1109/MSR.2017.59},
	abstract = {Bugs severely hurt blockchain system dependability. A thorough understanding of blockchain bug characteristics is required to design effective tools for preventing, detecting and mitigating bugs. We perform an empirical study on bug characteristics in eight representative open source blockchain systems. First, we manually examine 1,108 bug reports to understand the nature of the reported bugs. Second, we leverage card sorting to label the bug reports, and obtain ten bug categories in blockchain systems. We further investigate the frequency distribution of bug categories across projects and programming languages. Finally, we study the relationship between bug categories and bug ﬁxing time. The ﬁndings include: (1) semantic bugs are the dominant runtime bug category; (2) frequency distributions of bug types show similar trends across different projects and programming languages; (3) security bugs take the longest median time to be ﬁxed; (4) 35.71\% performance bugs are ﬁxed in more than one year; performance bugs take the longest average time to be ﬁxed.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Wan, Zhiyuan and Lo, David and Xia, Xin and Cai, Liang},
	month = may,
	year = {2017},
	pages = {413--424},
}

@inproceedings{vahabzadeh_empirical_2015,
	address = {Bremen, Germany},
	title = {An empirical study of bugs in test code},
	isbn = {978-1-4673-7532-0},
	url = {http://ieeexplore.ieee.org/document/7332456/},
	doi = {10.1109/ICSM.2015.7332456},
	abstract = {Testing aims at detecting (regression) bugs in production code. However, testing code is just as likely to contain bugs as the code it tests. Buggy test cases can silently miss bugs in the production code or loudly ring false alarms when the production code is correct. We present the ﬁrst empirical study of bugs in test code to characterize their prevalence and root cause categories. We mine the bug repositories and version control systems of 211 Apache Software Foundation (ASF) projects and ﬁnd 5,556 test-related bug reports. We (1) compare properties of test bugs with production bugs, such as active time and ﬁxing effort needed, and (2) qualitatively study 443 randomly sampled test bug reports in detail and categorize them based on their impact and root causes. Our results show that (1) around half of all the projects had bugs in their test code; (2) the majority of test bugs are false alarms, i.e., test fails while the production code is correct, while a minority of these bugs result in silent horrors, i.e., test passes while the production code is incorrect; (3) incorrect and missing assertions are the dominant root cause of silent horror bugs; (4) semantic (25\%), ﬂaky (21\%), environmentrelated (18\%) bugs are the dominant root cause categories of false alarms; (5) the majority of false alarm bugs happen in the exercise portion of the tests, and (6) developers contribute more actively to ﬁxing test bugs and test bugs are ﬁxed sooner compared to production bugs. In addition, we evaluate whether existing bug detection tools can detect bugs in test code.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Vahabzadeh, Arash and Fard, Amin Milani and Mesbah, Ali},
	month = sep,
	year = {2015},
	pages = {101--110},
}

@inproceedings{saha_empirical_2014,
	address = {Antwerp, Belgium},
	title = {An empirical study of long lived bugs},
	isbn = {978-1-4799-3752-3},
	url = {http://ieeexplore.ieee.org/document/6747164/},
	doi = {10.1109/CSMR-WCRE.2014.6747164},
	abstract = {Bug ﬁxing is a crucial part of software development and maintenance. A large number of bugs often indicate poor software quality since buggy behavior not only causes failures that may be costly but also has a detrimental effect on the user’s overall experience with the software product. The impact of long lived bugs can be even more critical since experiencing the same bug version after version can be particularly frustrating for user. While there are many studies that investigate factors affecting bug ﬁxing time for entire bug repositories, to the best of our knowledge, none of these studies investigates the extent and reasons of long lived bugs. In this paper, we analyzed long lived bugs from ﬁve different perspectives: their proportion, severity, assignment, reasons, as well as the nature of ﬁxes. Our study on four open-source projects shows that there are a considerable number of long lived bugs in each system and over 90\% of them adversely affect the user’s experience. The reasons of these long lived bugs are diverse including long assignment time, not understanding their importance in advance etc. However, many bug-ﬁxes were delayed without any speciﬁc reasons. Our analysis of bug ﬁxing changes further shows that many long lived bugs can be ﬁxed quickly through careful prioritization. We believe our results will help both developers and researchers to better understand factors behind delays, improve the overall bug ﬁxing process, and investigate analytical approaches for prioritizing bugs based on bug severity as well as expected bug ﬁxing effort. Index Terms—Bug tracking system, bug triaging, bug survival time I. INTRODUCTION Software development and maintenance is a complex process. Although developers and testers try their best to make their software error free, in practice software ships with bugs. The number of bugs in software is a signiﬁcant indicator of software quality since bugs can adversely affect users experience directly. Therefore, developers are generally very active in ﬁnding and removing bugs.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2014 {Software} {Evolution} {Week} - {IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering} ({CSMR}-{WCRE})},
	publisher = {IEEE},
	author = {Saha, Ripon K. and Khurshid, Sarfraz and Perry, Dewayne E.},
	month = feb,
	year = {2014},
	pages = {144--153},
}

@article{satija_blockene_nodate,
	title = {Blockene: {A} {High}-throughput {Blockchain} {Over} {Mobile} {Devices}},
	abstract = {We introduce Blockene, a blockchain that reduces resource usage at member nodes by orders of magnitude, requiring only a smartphone to participate in block validation and consensus. Despite being lightweight, Blockene provides a high throughput of transactions and scales to a large number of participants. Blockene consumes negligible battery and data in smartphones, enabling millions of users to participate in the blockchain without incentives, to secure transactions with their collective honesty. Blockene achieves these properties with a novel split-trust design based on delegating storage and gossip to untrusted nodes.},
	language = {en},
	author = {Satija, Sambhav and Mehra, Apurv and Singanamalla, Sudheesh and Grover, Karan and Sivathanu, Muthian and Chandran, Nishanth and Gupta, Divya and Lokam, Satya},
	pages = {17},
}

@article{nguyen_improving_2021,
	title = {Improving {Object} {Detection} by {Label} {Assignment} {Distillation}},
	url = {http://arxiv.org/abs/2108.10520},
	abstract = {Label assignment in object detection aims to assign targets, foreground or background, to sampled regions in an image. Unlike labeling for image classification, this problem is not well defined due to the object's bounding box. In this paper, we investigate the problem from a perspective of distillation, hence we call Label Assignment Distillation (LAD). Our initial motivation is very simple, we use a teacher network to generate labels for the student. This can be achieved in two ways: either using the teacher's prediction as the direct targets (soft label), or through the hard labels dynamically assigned by the teacher (LAD). Our experiments reveal that: (i) LAD is more effective than soft-label, but they are complementary. (ii) Using LAD, a smaller teacher can also improve a larger student significantly, while soft-label can't. We then introduce Co-learning LAD, in which two networks simultaneously learn from scratch and the role of teacher and student are dynamically interchanged. Using PAA-ResNet50 as a teacher, our LAD techniques can improve detectors PAA-ResNet101 and PAA-ResNeXt101 to \$46 {\textbackslash}rm AP\$ and \$47.5{\textbackslash}rm AP\$ on the COCO test-dev set. With a strong teacher PAA-SwinB, we improve the PAA-ResNet50 to \$43.9{\textbackslash}rm AP\$ with only {\textbackslash}1x schedule training, and PAA-ResNet101 to \$47.9{\textbackslash}rm AP\$, significantly surpassing the current methods. Our source code and checkpoints will be released at https://github.com/cybercore-co-ltd/CoLAD\_paper.},
	urldate = {2021-10-01},
	journal = {arXiv:2108.10520 [cs]},
	author = {Nguyen, Chuong H. and Nguyen, Thuy C. and Tang, Tuan N. and Phan, Nam L. H.},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.10520},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ge_ota_2021,
	title = {{OTA}: {Optimal} {Transport} {Assignment} for {Object} {Detection}},
	shorttitle = {{OTA}},
	url = {http://arxiv.org/abs/2103.14259},
	abstract = {Recent advances in label assignment in object detection mainly seek to independently define positive/negative training samples for each ground-truth (gt) object. In this paper, we innovatively revisit the label assignment from a global perspective and propose to formulate the assigning procedure as an Optimal Transport (OT) problem -- a well-studied topic in Optimization Theory. Concretely, we define the unit transportation cost between each demander (anchor) and supplier (gt) pair as the weighted summation of their classification and regression losses. After formulation, finding the best assignment solution is converted to solve the optimal transport plan at minimal transportation costs, which can be solved via Sinkhorn-Knopp Iteration. On COCO, a single FCOS-ResNet-50 detector equipped with Optimal Transport Assignment (OTA) can reach 40.7\% mAP under 1X scheduler, outperforming all other existing assigning methods. Extensive experiments conducted on COCO and CrowdHuman further validate the effectiveness of our proposed OTA, especially its superiority in crowd scenarios. The code is available at https://github.com/Megvii-BaseDetection/OTA.},
	urldate = {2021-10-01},
	journal = {arXiv:2103.14259 [cs]},
	author = {Ge, Zheng and Liu, Songtao and Li, Zeming and Yoshie, Osamu and Sun, Jian},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.14259},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liu_replicability_2020,
	title = {On the {Replicability} and {Reproducibility} of {Deep} {Learning} in {Software} {Engineering}},
	url = {http://arxiv.org/abs/2006.14244},
	abstract = {Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge. Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) replicability - whether the reported experimental result can be approximately reproduced in high probability with the same DL model and the same data; and (2) reproducibility - whether one reported experimental findings can be reproduced by new experiments with the same experimental protocol and DL model, but different sampled real-world data. Unlike traditional machine learning (ML) models, DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process. In this study, we conducted a literature review on 93 DL studies recently published in twenty SE journals or conferences. Our statistics show the urgency of investigating these two factors in SE. Moreover, we re-ran four representative DL models in SE. Experimental results show the importance of replicability and reproducibility, where the reported performance of a DL model could not be replicated for an unstable optimization process. Reproducibility could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data. It is therefore urgent for the SE community to provide a long-lasting link to a replication package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data.},
	urldate = {2021-09-29},
	journal = {arXiv:2006.14244 [cs]},
	author = {Liu, Chao and Gao, Cuiyun and Xia, Xin and Lo, David and Grundy, John and Yang, Xiaohu},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.14244},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{gladisch_experience_2019,
	title = {Experience {Paper}: {Search}-{Based} {Testing} in {Automated} {Driving} {Control} {Applications}},
	shorttitle = {Experience {Paper}},
	doi = {10.1109/ASE.2019.00013},
	abstract = {Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Gladisch, Christoph and Heinz, Thomas and Heinzemann, Christian and Oehlerking, Jens and von Vietinghoff, Anne and Pfitzer, Tim},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Measurement, Monitoring, Optimization, Software, Test pattern generators, Tools, automated driving, automated test generation, experience paper, search-based testing},
	pages = {26--37},
}

@article{islam_towards_nodate,
	title = {Towards understanding the challenges faced by machine learning software developers and enabling automated solutions},
	language = {en},
	author = {Islam, Johirul},
	pages = {162},
}

@article{zhou_centernet_2019,
	title = {{CenterNet}: {Objects} as {Points}},
	url = {http://arxiv.org/abs/1904.07850},
	abstract = {Detection identiﬁes objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefﬁcient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point — the center point of its bounding box. Our detector uses keypoint estimation to ﬁnd center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
	language = {en},
	urldate = {2021-09-22},
	journal = {arXiv:1904.07850 [cs]},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.07850},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ge_yolox_2021,
	title = {{YOLOX}: {Exceeding} {YOLO} {Series} in 2021},
	shorttitle = {{YOLOX}},
	url = {http://arxiv.org/abs/2107.08430},
	abstract = {In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3\% AP on COCO, surpassing NanoDet by 1.8\% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3\% AP on COCO, outperforming the current best practice by 3.0\% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0\% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8\% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX.},
	language = {en},
	urldate = {2021-09-22},
	journal = {arXiv:2107.08430 [cs]},
	author = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
	month = aug,
	year = {2021},
	note = {arXiv: 2107.08430},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{gkioxari_mesh_nodate,
	title = {Mesh {R}-{CNN}},
	abstract = {Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by ﬁrst predicting coarse voxel representations which are converted to meshes and reﬁned with a graph convolution network operating over the mesh’s vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes. Project page: https://gkioxari.github.io/meshrcnn/.},
	language = {en},
	author = {Gkioxari, Georgia and Malik, Jitendra and Johnson, Justin},
	pages = {15},
}

@article{tian_fcos_2020,
	title = {{FCOS}: {A} {Simple} and {Strong} {Anchor}-free {Object} {Detector}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {{FCOS}},
	url = {https://ieeexplore.ieee.org/document/9229517/},
	doi = {10.1109/TPAMI.2020.3032166},
	language = {en},
	urldate = {2021-09-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
	year = {2020},
	pages = {1--1},
}

@misc{noauthor_notitle_nodate,
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/0164121289900162?token=71D919BDA5333D78B51B87F17DBDA4B2A3214D8864E80C3F1774B6AF7B1DE0893FB7CFAA7B62969C98F0ADFA2D302459&originRegion=eu-west-1&originCreation=20210920153903},
	urldate = {2021-09-20},
}

@misc{noauthor_elsevier_nodate-1,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/0164121289900162?token=71D919BDA5333D78B51B87F17DBDA4B2A3214D8864E80C3F1774B6AF7B1DE0893FB7CFAA7B62969C98F0ADFA2D302459&originRegion=eu-west-1&originCreation=20210920153903},
	urldate = {2021-09-20},
}

@article{mchugh_interrater_2012,
	title = {Interrater reliability: the kappa statistic},
	volume = {22},
	issn = {1330-0962},
	shorttitle = {Interrater reliability},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/},
	abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen’s kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from −1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen’s suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
	number = {3},
	urldate = {2023-01-10},
	journal = {Biochemia Medica},
	author = {McHugh, Mary L.},
	month = oct,
	year = {2012},
	pmid = {23092060},
	pmcid = {PMC3900052},
	pages = {276--282},
}

@misc{trust_and_safety_professional_association_tspa_nodate,
	title = {{TSPA} {Job} {Board}},
	url = {http://web.archive.org/web/20221219211619/https://www.tspa.org/explore/job-board/},
	abstract = {Planning your next career move and seeking new opportunities in the field of trust and safety? Check out the job postings below and click on any link to apply. Have a job to list? Submit a job by filling out this form. T\&S employees at our corporate supporter companies receive free membership to TSPA. These},
	language = {en-US},
	urldate = {2022-11-04},
	journal = {Trust \& Safety Professional Association},
	author = {{Trust and Safety Professional Association}},
}

@misc{stanford_internet_observatory_trust_nodate,
	title = {Trust and {Safety} {Project}},
	url = {http://web.archive.org/web/20230105001534/https://cyber.fsi.stanford.edu/io/content/sio-trust-and-safety-project},
	language = {en},
	urldate = {2022-06-07},
	author = {{Stanford Internet Observatory}},
}

@misc{the_federation_federation_nodate,
	title = {The {Federation} - a statistics hub},
	url = {https://the-federation.info/},
	abstract = {Node list and statistics for The Federation and Fediverse},
	language = {en},
	urldate = {2022-10-26},
	author = {{The Federation}},
}

@misc{trust_and_safety_foundation_project_case_nodate,
	title = {Case {Studies}},
	url = {http://web.archive.org/web/20230101182825/https://trustandsafetyfoundation.org/case-studies/},
	abstract = {Featured Case Studies All Case Studies},
	language = {en-US},
	urldate = {2022-06-07},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
}

@misc{diaspora_foundation_diaspora_nodate,
	title = {diaspora* {Discourse}},
	url = {https://discourse.diasporafoundation.org/},
	abstract = {Project Discussions and Support},
	language = {en},
	urldate = {2022-04-29},
	journal = {diaspora* Discourse},
	author = {{Diaspora Foundation}},
}

@article{arangoHateSpeechDetection,
	title = {Hate {Speech} {Detection} is {Not} as {Easy} as {You} {May} {Think}: {A} {Closer} {Look} at {Model} {Validation}},
	abstract = {Hate speech is an important problem that is seriously affecting the dynamics and usefulness of online social communities. Large scale social platforms are currently investing important resources into automatically detecting and classifying hateful content, without much success. On the other hand, the results reported by state-of-the-art systems indicate that supervised approaches achieve almost perfect performance but only within specific datasets. In this work, we analyze this apparent contradiction between existing literature and actual applications. We study closely the experimental methodology used in prior work and their generalizability to other datasets. Our findings evidence methodological issues, as well as an important dataset bias. As a consequence, performance claims of the current state-of-the-art have become significantly overestimated. The problems that we have found are mostly related to data overfitting and sampling issues. We discuss the implications for current research and re-conduct experiments to give a more accurate picture of the current state-of-the art methods.},
	language = {en},
	author = {Arango, Aymé and Pérez, Jorge and Poblete, Barbara},
	pages = {9},
}

@misc{trust_and_safety_professional_association_senior_2022,
	title = {Senior {Security} {Engineer}, {Trust} \& {Safety}},
	url = {https://web.archive.org/web/20220808140939/https://www.tspa.org/job/senior-security-engineer-trust-safety/},
	abstract = {This content was reproduced from the employer’s website on July 3, 2022. Please visit their website below for the most up-to-date information about this position. The GitLab DevOps platform empowers 100,000+ organizations to deliver software faster and more efficiently. We are one of the world’s largest all-remote companies with 1,600+ team members and values that guide a culture where people embrace},
	language = {en-US},
	urldate = {2022-11-04},
	journal = {Trust \& Safety Professional Association},
	author = {{Trust and Safety Professional Association}},
	month = jul,
	year = {2022},
}

@misc{samuelson_how_2022,
	title = {How {Pinterest} built its {Trust} \& {Safety} team},
	url = {https://web.archive.org/web/20220912101106/https://medium.com/pinterest-engineering/how-pinterest-built-its-trust-safety-team-8d6c026dd4b9},
	language = {en},
	urldate = {2022-09-21},
	journal = {How Pinterest built its Trust \& Safety team},
	author = {Samuelson, Maisy},
	month = apr,
	year = {2022},
}

@misc{wikimedia_foundation_trust_2022,
	title = {Trust and {Safety}},
	url = {https://web.archive.org/web/20221017105630/http://meta.wikimedia.org/wiki/Trust_and_Safety},
	language = {en},
	urldate = {2022-02-25},
	author = {{Wikimedia Foundation}},
	month = feb,
	year = {2022},
}

@misc{modi_how_2022,
	title = {How our content abuse defense systems work to keep members safe},
	url = {https://web.archive.org/web/20221126095132/https://engineering.linkedin.com/blog/2022/how-our-content-abuse-defense-systems-work-to-keep-members-safe},
	abstract = {To create a safe and trusted experience for our members, our Trust \& Safety (TnS) team strives to keep content that violates our Professional Community Policies off of LinkedIn. In this blog post, we’ll provide insight into how we try to ensure conversations remain respectful and professional on our platform.},
	language = {en},
	urldate = {2022-09-21},
	journal = {LinkedIn Engineering},
	author = {Modi, Sanket},
	month = jan,
	year = {2022},
}

@misc{spectrum_labs_why_2022,
	title = {Why is {Trust} and {Safety} {Important}},
	url = {https://web.archive.org/web/20230108180811/https://www.spectrumlabsai.com/trust-and-safety},
	abstract = {Discover what Trust and Safety is, the metrics for tracking performance, and it can impact customer acquisition, engagement, and retention.},
	language = {en},
	urldate = {2022-02-25},
	author = {{Spectrum Labs}},
	year = {2022},
}

@misc{trust_and_safety_foundation_project_takedowns_2021,
	title = {Takedowns on {YouTube} skyrocket during pandemic as {AI} replaces human moderators (2020)},
	url = {https://web.archive.org/web/20220812124646/https://trustandsafetyfoundation.org/blog/takedowns-on-youtube-skyrocket-during-pandemic-as-ai-replaces-human-moderators-2020/},
	abstract = {Summary: YouTube has always faced an uphill battle when it comes to content moderation. A decade ago, the job was impossible to handle with only human moderators. According to stats […]},
	language = {en-US},
	urldate = {2022-11-09},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
	month = dec,
	year = {2021},
}

@misc{cloudflare_trust_2021,
	title = {Trust \& {Safety} {Engineering} {Team}},
	url = {https://web.archive.org/web/20220128033140/https://www.builtinaustin.com/job/engineer/software-engineer-trust-safety-engineering-team/76783},
	abstract = {Cloudflare is hiring for a Software Engineer - Trust \& Safety Engineering Team in Austin. Find more details about the job and how to apply at Built In Austin.},
	language = {en},
	urldate = {2022-06-11},
	journal = {Built In Austin},
	author = {{Cloudflare}},
	month = dec,
	year = {2021},
}

@misc{trust_and_safety_foundation_project_twitters_2021,
	title = {Twitter's self-deleting tweets feature creates new moderation problems (2020)},
	url = {http://web.archive.org/web/20220707045306/https://trustandsafetyfoundation.org/blog/twitters-self-deleting-tweets-feature-creates-new-moderation-problems-2020/},
	language = {en-US},
	urldate = {2022-11-07},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
	month = sep,
	year = {2021},
}

@misc{xie_building_2021,
	title = {Building a {Label}-{Based} {Enforcement} {Pipeline} for {Trust} \& {Safety}},
	url = {http://web.archive.org/web/20221221111741/https://medium.com/pinterest-engineering/building-a-label-based-enforcement-pipeline-for-trust-safety-4b05a409cb5d},
	abstract = {Sharon Xie{\textbar} Software Engineer, Trust \& Safety},
	language = {en},
	urldate = {2022-09-15},
	journal = {Pinterest Engineering Blog},
	author = {Xie, Sharon},
	month = may,
	year = {2021},
}

@misc{trust_and_safety_foundation_project_facebook_2021,
	title = {Facebook ad policies make it difficult for women’s health startups to advertise (2018)},
	url = {http://web.archive.org/web/20220707005633/https://trustandsafetyfoundation.org/blog/facebook-ad-policies-make-it-difficult-for-womens-health-startups-to-advertise-2018/},
	abstract = {Summary: Going back many years, there have been accusations that Facebook has a double standard when it comes to men’s and women’s health products in advertising. In 2018, a VentureBeat investigation highlighted […]},
	language = {en-US},
	urldate = {2022-11-09},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
	month = may,
	year = {2021},
}

@misc{trust_and_safety_foundation_project_youtubes_2021,
	title = {{YouTube}'s new policy on {Nazi} content results in removal of historical and education videos (2019)},
	url = {http://web.archive.org/web/20230101182807/https://trustandsafetyfoundation.org/blog/youtubes-new-policy-on-nazi-content-results-in-removal-of-historical-and-education-videos-2019/},
	language = {en-US},
	urldate = {2022-11-07},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
	month = mar,
	year = {2021},
}

@misc{khan_what_2020,
	title = {What is {Trust} and {Safety}?},
	url = {http://web.archive.org/web/20220707103612/https://blogs.gartner.com/akif-khan/what-is-trust-and-safety/},
	abstract = {Trust – noun.  firm belief in the reliability, truth, or ability of someone or something Safety – noun. the condition of being protected from or unlikely to cause danger, risk, or injury Together, these two words – trust and safety – describe the change in attitude and mindset that we have been seeing over the […]},
	language = {en},
	urldate = {2022-02-25},
	journal = {Akif Khan},
	author = {Khan, Akif},
	month = aug,
	year = {2020},
}

@misc{feerst_your_2019,
	title = {Your {Speech}, {Their} {Rules}: {Meet} the {People} {Who} {Guard} the {Internet}},
	shorttitle = {Your {Speech}, {Their} {Rules}},
	url = {http://web.archive.org/web/20221217212157/https://onezero.medium.com/your-speech-their-rules-meet-the-people-who-guard-the-internet-ab58fe6b9231},
	abstract = {Tech platform trust and safety employees are charged with policing the impossible. They open up to Medium’s head of trust and safety.},
	language = {en},
	urldate = {2022-06-07},
	journal = {OneZero},
	author = {Feerst, Alex},
	month = mar,
	year = {2019},
}

@misc{jeong_internet_2018,
	title = {The {Internet} of {Garbage} by {Sarah} {Jeong}},
	url = {http://web.archive.org/web/20221221094434/https://www.theverge.com/2018/8/28/17777330/internet-of-garbage-book-sarah-jeong-online-harassment},
	abstract = {"The internet is, and always has been, mostly garbage," argues Sarah Jeong in her book about the intractable problem of online harassment.},
	language = {en},
	urldate = {2022-07-18},
	journal = {The Verge},
	author = {Jeong, Sarah},
	month = aug,
	year = {2018},
}

@misc{lawson_mastodon_2018,
	title = {Mastodon and the challenges of abuse in a federated system},
	url = {http://web.archive.org/web/20230107102416/https://nolanlawson.com/2018/08/31/mastodon-and-the-challenges-of-abuse-in-a-federated-system/},
	abstract = {This post will probably only make sense to those deeply involved in Mastodon and the fediverse. So if that’s not your thing, or you’re not interested in issues of social media and safet…},
	language = {en},
	urldate = {2022-07-18},
	journal = {Read the Tea Leaves},
	author = {Lawson, Nolan},
	year = {2018},
}

@misc{hunt_trust_2017,
	title = {Trust and safety 101},
	url = {http://web.archive.org/web/20220819234233/https://www.csoonline.com/article/3206127/trust-and-safety-101.html},
	abstract = {Creating a trust and safety team, even if it consists of a small group of part-time employees, can pay dividends in brand equity and user trust.},
	language = {en},
	urldate = {2022-02-25},
	journal = {CSO Online},
	author = {Hunt, Pete},
	month = jul,
	year = {2017},
}

@misc{leong_consensual_2017,
	title = {Consensual {Software}: {How} to {Prioritize} {User} {Safety}},
	shorttitle = {Consensual {Software}},
	url = {http://web.archive.org/web/20230101040655/https://www.infoq.com/articles/consensual-software/},
	abstract = {This article covers how consensual software will help address online harassment and abuse vectors before they get out of hand. It also covers some features the GitHub Community \& Safety team has built and how we review features from other teams.},
	language = {en},
	urldate = {2022-06-03},
	journal = {InfoQ},
	author = {Leong, Danielle},
	month = may,
	year = {2017},
}

@misc{shi_what_2016,
	title = {What the {Heck} is {Trust} and {Safety}?},
	url = {http://web.archive.org/web/20221203164121/https://www.linkedin.com/pulse/what-heck-trust-safety-kenny-shi},
	abstract = {Here are some of the companies that have “Trust and Safety” initiatives: eBay, Airbnb, Twitter, Upwork, TaskRabbit, etc. What is Trust and Safety? How does it compare and contrast with fraud? When I google the word fraud, here is what it says: “wrongful or criminal deception intended to result in fi},
	language = {en},
	urldate = {2022-02-25},
	author = {Shi, Kenny},
	year = {2016},
}

@misc{gitlab_trust_nodate,
	title = {Trust \& {Safety} {Team}},
	url = {http://web.archive.org/web/20220530025357/https://about.gitlab.com/handbook/engineering/security/security-operations/trustandsafety/},
	abstract = {GitLab.com Trust \& Safety Team Overview},
	language = {en},
	urldate = {2022-06-07},
	journal = {GitLab},
	author = {{GitLab}},
}

@misc{leong_adding_2017,
	title = {Adding {Community} \& {Safety} checks to new features},
	url = {http://web.archive.org/web/20221209134254/https://github.blog/2017-01-31-community-and-safety-feature-reviews/},
	abstract = {With the continuous shipping nature at GitHub, it’s easy for the most well-intentioned feature to accidentally become the vector of abuse and harassment. The Community \& Safety engineering team focuses on building community management tools and maintaining user safety, but we also review new features our colleagues have written to ensure there are no accidental […]},
	language = {en-US},
	urldate = {2022-06-11},
	journal = {The GitHub Blog},
	author = {Leong, Danielle},
	month = jan,
	year = {2017},
}

@misc{galantino_trust_2019,
	title = {Trust \& {Safety} {Engineering} @ {GitHub}},
	url = {https://www.youtube.com/watch?v=UC3Y9rx1jFQ},
	abstract = {GitHub is the \#1 open source platform in the world, with over 30 million users working in over 100 million repositories. How do we protect our users from harassment while encouraging happy, healthy communities at such a large scale? In this session, I'll introduce the concept of Trust \& Safety work in online platforms, talk a bit about different models used to tackle this problem, and then walk through some engineering challenges and trade-offs faced at GitHub. 

Learning objective: User safety and privacy, just like security, needs to be built into the platform from the ground up. It is the job of every engineer writing user-facing code to understand and use these best practices.

Speaker:

Lexi Galantino
Community \& Safety Engineer, GitHub},
	urldate = {2022-06-11},
	author = {Galantino, Lexi},
	month = may,
	year = {2019},
}

@misc{aufranc_cnxsoft_ti_2012,
	title = {{TI} {Releases} {TI}-{RTOS}, a {Free} {Real} {Time} {Operating} {System} for {MCUs} - {CNX} {Software}},
	url = {https://www.cnx-software.com/2012/12/07/ti-releases-ti-rtos-a-free-real-time-operating-system-for-mcus/},
	abstract = {Texas Instruments announced TI-RTOS, a complete real-time operating system based on a preemptive multithreading kernel for its MCU platforms. TI-RTOSs},
	language = {en-US},
	urldate = {2023-01-09},
	journal = {CNX Software - Embedded Systems News},
	author = {Aufranc (CNXSoft), Jean-Luc},
	month = dec,
	year = {2012},
}

@inproceedings{poncelet_so_2022,
	title = {So {Many} {Fuzzers}, {So} {Little} {Time} : {Experience} from {Evaluating} {Fuzzers} on the {Contiki}-{NG} {Network} ({Hay}){Stack}},
	shorttitle = {So {Many} {Fuzzers}, {So} {Little} {Time}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:ri:diva-61138},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2023-01-06},
	author = {Poncelet, Clément and Sagonas, Konstantinos and Tsiftes, Nicolas},
	year = {2022},
}

@misc{noauthor_packetdrill_nodate,
	title = {packetdrill: {Scriptable} {Network} {Stack} {Testing}, from {Sockets} to {Packets} {\textbar} {USENIX}},
	shorttitle = {packetdrill},
	url = {https://www.usenix.org/conference/atc13/technical-sessions/presentation/cardwell},
	language = {en},
	urldate = {2023-01-06},
}

@misc{noauthor_afl_nodate,
	title = {{AFL}++ : {Combining} {Incremental} {Steps} of {Fuzzing} {Research} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/woot20/presentation/fioraldi},
	urldate = {2023-01-06},
}

@inproceedings{andronidis_snapfuzz_2022,
	title = {{SnapFuzz}: {An} {Efficient} {Fuzzing} {Framework} for {Network} {Applications}},
	shorttitle = {{SnapFuzz}},
	url = {http://arxiv.org/abs/2201.04048},
	doi = {10.1145/3533767.3534376},
	abstract = {In recent years, fuzz testing has benefited from increased computational power and important algorithmic advances, leading to systems that have discovered many critical bugs and vulnerabilities in production software. Despite these successes, not all applications can be fuzzed efficiently. In particular, stateful applications such as network protocol implementations are constrained by their low fuzzing throughput and the need to develop fuzzing harnesses that reset their state and isolate their side effects. In this paper, we present SnapFuzz, a novel fuzzing framework for network applications. SnapFuzz offers a robust architecture that transforms slow asynchronous network communication into fast synchronous communication, snapshots the target at the latest point at which it is safe to do so, speeds up all file operations by redirecting them to a custom in-memory filesystem, and removes the need for many fragile modifications, such as configuring time delays or writing clean-up scripts, together with several other improvements. Using SnapFuzz, we fuzzed five popular networking applications: LightFTP, TinyDTLS, Dnsmasq, LIVE555 and Dcmqrscp. We report impressive performance speedups of 62.8x, 41.2x, 30.6x, 24.6x, and 8.4x, respectively, with significantly simpler fuzzing harnesses in all cases. Through its performance advantage, SnapFuzz has also found 12 extra crashes compared to AFLNet in these applications.},
	urldate = {2023-01-06},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	author = {Andronidis, Anastasios and Cadar, Cristian},
	month = jul,
	year = {2022},
	note = {arXiv:2201.04048 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
	pages = {340--351},
}

@misc{noauthor_tcp-fuzz_nodate,
	title = {{TCP}-{Fuzz}: {Detecting} {Memory} and {Semantic} {Bugs} in {TCP} {Stacks} with {Fuzzing} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/atc21/presentation/zou},
	urldate = {2023-01-05},
}

@article{newman_operating_nodate,
	title = {An {Operating} {System} {Bug} {Exposes} 200 {Million} {Critical} {Devices}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/vxworks-vulnerabilities-urgent11/},
	abstract = {VxWorks is designed as a secure, "real-time" operating system for continuously functioning devices, like medical equipment, elevator controllers, or satellite modems.},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {Wired},
	author = {Newman, Lily Hay},
	note = {Section: tags},
	keywords = {critical infrastructure, iot, vulnerabilities},
}

@misc{noauthor_project_nodate,
	title = {Project {Memoria}},
	url = {https://www.forescout.com/research-labs/project-memoria/},
	abstract = {Project Memoria Securing TCP/IP Stacks Vedere Labs launched its Project Memoria initiative in 2020 with the mission of providing the cybersecurity community with the most extensive study to date of TCP/IP stacks security. Under Project Memoria, Forescout researchers collaborate with industry peers, universities and research institutes to analyze common mistakes associated with vulnerabilities in TCP/IP […]},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {Forescout},
}

@article{greenberg_hackers_nodate,
	title = {Hackers {Remotely} {Kill} a {Jeep} on the {Highway}—{With} {Me} in {It}},
	issn = {1059-1028},
	url = {https://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/},
	abstract = {I was driving 70 mph on the edge of downtown St. Louis when the exploit began to take hold.},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {Wired},
	author = {Greenberg, Andy},
	note = {Section: tags},
	keywords = {cars, chrysler, cybersecurity, defcon, safety, self-driving cars, threat level, vulnerabilities},
}

@misc{noauthor_amnesia33_nodate,
	title = {{AMNESIA}:33},
	shorttitle = {{AMNESIA}},
	url = {https://www.forescout.com/research-labs/amnesia33/},
	abstract = {AMNESIA:33 AMNESIA:33 Vedere Labs discovered 33 vulnerabilities impacting millions of IoT, OT and IT devices that present an immediate risk for organizations worldwide. Read Report 4 Critical Vulnerabilities 150+ Vendors Affected 1M+ IoT, OT \& IT Devices The Global Impact of AMNESIA:33 AMNESIA:33 is a set of 33 vulnerabilities that impact four open source TCP/IP […]},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {Forescout},
}

@misc{adams_iot_2021,
	title = {{IoT} device attacks double in the first half of 2021, and remote work may shoulder some of the blame},
	url = {https://www.techrepublic.com/article/iot-device-attacks-double-in-the-first-half-of-2021-and-remote-work-may-shoulder-some-of-the-blame/},
	abstract = {The smart home could be ripe for IoT device attacks as cybercriminals rake in record ransomware payments. Remote work may be responsible for the increase in attacks, Kaspersky says.},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {TechRepublic},
	author = {Adams, R. Dallon},
	month = sep,
	year = {2021},
}

@article{hahm_operating_2016,
	title = {Operating {Systems} for {Low}-{End} {Devices} in the {Internet} of {Things}: {A} {Survey}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Operating {Systems} for {Low}-{End} {Devices} in the {Internet} of {Things}},
	doi = {10.1109/JIOT.2015.2505901},
	abstract = {The Internet of Things (IoT) is projected to soon interconnect tens of billions of new devices, in large part also connected to the Internet. IoT devices include both high-end devices which can use traditional go-to operating systems (OSs) such as Linux, and low-end devices which cannot, due to stringent resource constraints, e.g., very limited memory, computational power, and power supply. However, large-scale IoT software development, deployment, and maintenance requires an appropriate OS to build upon. In this paper, we thus analyze in detail the specific requirements that an OS should satisfy to run on low-end IoT devices, and we survey applicable OSs, focusing on candidates that could become an equivalent of Linux for such devices, i.e., a one-size-fits-most, open source OS for low-end IoT devices.},
	number = {5},
	journal = {IEEE Internet of Things Journal},
	author = {Hahm, Oliver and Baccelli, Emmanuel and Petersen, Hauke and Tsiftes, Nicolas},
	month = oct,
	year = {2016},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Embedded software, Hardware, Internet of Things (IoT), Internet of things, Operating systems, Random access memory, Standards, low-power electronics, operating system (OS)},
	pages = {720--734},
}

@misc{social_web_working_group_activitypub_2018,
	title = {{ActivityPub}},
	url = {https://w3c.github.io/activitypub/#security-considerations},
	urldate = {2023-01-02},
	author = {{Social Web Working Group}},
	month = jan,
	year = {2018},
}

@inproceedings{Wittern2016JSPackageEcosystem,
	address = {Austin Texas},
	title = {A look at the dynamics of the {JavaScript} package ecosystem},
	isbn = {978-1-4503-4186-8},
	url = {https://dl.acm.org/doi/10.1145/2901739.2901743},
	doi = {10.1145/2901739.2901743},
	abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node.js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem’s growth and activity, into conﬂicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
	language = {en},
	urldate = {2022-06-10},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
	month = may,
	year = {2016},
	pages = {351--361},
}

@article{aslan_comprehensive_2020,
	title = {A {Comprehensive} {Review} on {Malware} {Detection} {Approaches}},
	doi = {10.1109/ACCESS.2019.2963724},
	abstract = {According to the recent studies, malicious software (malware) is increasing at an alarming rate, and some malware can hide in the system by using different obfuscation techniques. In order to protect computer systems and the Internet from the malware, the malware needs to be detected before it affects a large number of systems. Recently, there have been made several studies on malware detection approaches. However, the detection of malware still remains problematic. Signature-based and heuristic-based detection approaches are fast and efficient to detect known malware, but especially signature-based detection approach has failed to detect unknown malware. On the other hand, behavior-based, model checking-based, and cloud-based approaches perform well for unknown and complicated malware; and deep learning-based, mobile devices-based, and IoT-based approaches also emerge to detect some portion of known and unknown malware. However, no approach can detect all malware in the wild. This shows that to build an effective method to detect malware is a very challenging task, and there is a huge gap for new studies and methods. This paper presents a detailed review on malware detection approaches and recent detection methods which use these approaches. Paper goal is to help researchers to have a general idea of the malware detection approaches, pros and cons of each detection approach, and methods that are used in these approaches.},
	journal = {IEEE Access},
	author = {Aslan, Omer and Samet, Refik},
	year = {2020},
	keywords = {Computer viruses, Cyber security, Encryption, Feature extraction, Internet, malware classification, malware detection approaches, malware features},
}

@article{oikonomou_contiki-ng_2022,
	title = {The {Contiki}-{NG} open source operating system for next generation {IoT} devices},
	volume = {18},
	issn = {2352-7110},
	url = {https://www.softxjournal.com/article/S2352-7110(22)00062-0/fulltext},
	doi = {10.1016/j.softx.2022.101089},
	language = {English},
	urldate = {2022-12-26},
	journal = {SoftwareX},
	author = {Oikonomou, George and Duquennoy, Simon and Elsts, Atis and Eriksson, Joakim and Tanaka, Yasuyuki and Tsiftes, Nicolas},
	month = jun,
	year = {2022},
	note = {Publisher: Elsevier},
	keywords = {Contiki-NG, Internet of Things, Resource-Constrained Devices},
}

@misc{dabic_sampling_2021,
	title = {Sampling {Projects} in {GitHub} for {MSR} {Studies}},
	url = {http://arxiv.org/abs/2103.04682},
	abstract = {Almost every Mining Software Repositories (MSR) study requires, as first step, the selection of the subject software repositories. These repositories are usually collected from hosting services like GitHub using specific selection criteria dictated by the study goal. For example, a study related to licensing might be interested in selecting projects explicitly declaring a license. Once the selection criteria have been defined, utilities such as the GitHub APIs can be used to "query" the hosting service. However, researchers have to deal with usage limitations imposed by these APIs and a lack of required information. For example, the GitHub search APIs allow 30 requests per minute and, when searching repositories, only provide limited information (e.g., the number of commits in a repository is not included). To support researchers in sampling projects from GitHub, we present GHS (GitHub Search), a dataset containing 25 characteristics (e.g., number of commits, license, etc.) of 735,669 repositories written in 10 programming languages. The set of characteristics has been derived by looking for frequently used project selection criteria in MSR studies and the dataset is continuously updated to (i) always provide fresh data about the existing projects, and (ii) increase the number of indexed projects. The GHS dataset can be queried through a web application we built that allows to set many combinations of selection criteria needed for a study and download the information of matching repositories: https://seart-ghs.si.usi.ch.},
	language = {en},
	urldate = {2022-12-23},
	publisher = {arXiv},
	author = {Dabic, Ozren and Aghajani, Emad and Bavota, Gabriele},
	month = mar,
	year = {2021},
	note = {arXiv:2103.04682 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{bui_vul4j_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{Vul4J}: a dataset of reproducible {Java} vulnerabilities geared towards the study of program repair techniques},
	isbn = {978-1-4503-9303-4},
	shorttitle = {{Vul4J}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528482},
	doi = {10.1145/3524842.3528482},
	abstract = {In this work we present Vul4J, a Java vulnerability dataset where each vulnerability is associated to a patch and, most importantly, to a Proof of Vulnerability (PoV) test case. We analyzed 1803 fix commits from 912 real-world vulnerabilities in the Project KB knowledge base to extract the reproducible vulnerabilities, i.e., vulnerabilities that can be triggered by one or more PoV test cases. To this aim, we ran the test suite of the application in both, the vulnerable and secure versions, to identify the corresponding PoVs. Furthermore, if no PoV test case was spotted, then we wrote it ourselves. As a result, Vul4J includes 79 reproducible vulnerabilities from 51 open-source projects, spanning 25 different Common Weakness Enumeration (CWE) types. To the extent of our knowledge, this is the first dataset of its kind created for Java. Particularly, it targets the study of Automated Program Repair (APR) tools, where PoVs are often necessary in order to identify plausible patches. We made our dataset and related tools publically available on GitHub.},
	language = {en},
	urldate = {2022-12-23},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Bui, Quang-Cuong and Scandariato, Riccardo and Ferreyra, Nicolás E. Díaz},
	month = may,
	year = {2022},
	pages = {464--468},
}

@inproceedings{buchkova_dasea_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{DaSEA}: a dataset for software ecosystem analysis},
	isbn = {978-1-4503-9303-4},
	shorttitle = {{DaSEA}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528004},
	doi = {10.1145/3524842.3528004},
	abstract = {Software package managers facilitate reuse and rapid construction of software systems. Since evermore software is distributed via package managers, researchers and practitioners require explicit data of software dependency networks that are opaquely formed by dependency relations between software packages. To reason about increasingly complex software products and ecosystems, researchers and practitioners rely either on publicly available datasets like the seemingly unattended libraries.io [14] or they mine problem-specific data from software ecosystems repeatedly and non-transparently. Therefore, we present the DaSEA dataset, which contains metadata of software packages, their versions, and dependencies from multiple ecosystems (currently six programming languages and five operating system package managers). Alongside the dataset, we provide an extensible open-source tool under the same name that is used to create updated versions of the DaSEA dataset allowing studies of evolution of software ecosystems.},
	language = {en},
	urldate = {2022-12-23},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Buchkova, Petya and Hinnerskov, Joakim Hey and Olsen, Kasper and Pfeiffer, Rolf-Helge},
	month = may,
	year = {2022},
	pages = {388--392},
}

@inproceedings{zacchiroli_large-scale_2022,
	address = {Pittsburgh Pennsylvania},
	title = {A large-scale dataset of (open source) license text variants},
	isbn = {978-1-4503-9303-4},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528491},
	doi = {10.1145/3524842.3528491},
	abstract = {We introduce a large-scale dataset of the complete texts of free/open source software (FOSS) license variants. To assemble it we have collected from the Software Heritage archive—the largest publicly available archive of FOSS source code with accompanying development history—all versions of files whose names are commonly used to convey licensing terms to software users and developers. The dataset consists of 6.5 million unique license files that can be used to conduct empirical studies on open source licensing, training of automated license classifiers, natural language processing (NLP) analyses of legal texts, as well as historical and phylogenetic studies on FOSS licensing.},
	language = {en},
	urldate = {2022-12-23},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Zacchiroli, Stefano},
	month = may,
	year = {2022},
	pages = {757--761},
}

@misc{grotov_large-scale_2022,
	title = {A {Large}-{Scale} {Comparison} of {Python} {Code} in {Jupyter} {Notebooks} and {Scripts}},
	url = {http://arxiv.org/abs/2203.16718},
	abstract = {In recent years, Jupyter notebooks have grown in popularity in several domains of software engineering, such as data science, machine learning, and computer science education. Their popularity has to do with their rich features for presenting and visualizing data, however, recent studies show that notebooks also share a lot of drawbacks: high number of code clones, low reproducibility, etc. In this work, we carry out a comparison between Python code written in Jupyter Notebooks and in traditional Python scripts. We compare the code from two perspectives: structural and stylistic. In the first part of the analysis, we report the difference in the number of lines, the usage of functions, as well as various complexity metrics. In the second part, we show the difference in the number of stylistic issues and provide an extensive overview of the 15 most frequent stylistic issues in the studied mediums. Overall, we demonstrate that notebooks are characterized by the lower code complexity, however, their code could be perceived as more entangled than in the scripts. As for the style, notebooks tend to have 1.4 times more stylistic issues, but at the same time, some of them are caused by specific coding practices in notebooks and should be considered as false positives. With this research, we want to pave the way to studying specific problems of notebooks that should be addressed by the development of notebook-specific tools, and provide various insights that can be useful in this regard.},
	language = {en},
	urldate = {2022-12-23},
	publisher = {arXiv},
	author = {Grotov, Konstantin and Titov, Sergey and Sotnikov, Vladimir and Golubev, Yaroslav and Bryksin, Timofey},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16718 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{hartel_operationalizing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Operationalizing threats to {MSR} studies by simulation-based testing},
	isbn = {978-1-4503-9303-4},
	url = {https://dl.acm.org/doi/10.1145/3524842.3527960},
	doi = {10.1145/3524842.3527960},
	abstract = {Quantitative studies on the border between Mining Software Repository (MSR) and Empirical Software Engineering (ESE) apply data analysis methods, like regression modeling, statistic tests or correlation analysis, to commits or pulls to better understand the software development process. Such studies assure the validity of the reported results by following a sound methodology. However, with increasing complexity, parts of the methodology can still go wrong. This may result in MSR/ESE studies with undetected threats to validity. In this paper, we propose to systematically protect against threats by operationalizing their treatment using simulations. A simulation substitutes observed and unobserved data, related to an MSR/ESE scenario, with synthetic data, carefully defined according to plausible assumptions on the scenario. Within a simulation, unobserved data becomes transparent, which is the key difference to a real study, necessary to detect threats to an analysis methodology. Running an analysis methodology on synthetic data may detect basic technical bugs and misinterpretations, but it also improves the trust in the methodology. The contribution of a simulation is to operationalize testing the impact of important assumptions. Assumptions still need to be rated for plausibility. We evaluate simulation-based testing by operationalizing undetected threats in the context of four published MSR/ESE studies. We recommend that future research uses such more systematic treatment of threats, as a contribution against the reproducibility crisis.},
	language = {en},
	urldate = {2022-12-23},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Härtel, Johannes and Lämmel, Ralf},
	month = may,
	year = {2022},
	pages = {86--97},
}

@article{obie_violation_nodate,
	title = {On the {Violation} of {Honesty} in {Mobile} {Apps}: {Automated} {Detection} and {Categories}},
	abstract = {Human values such as integrity, privacy, curiosity, security, and honesty are guiding principles for what people consider important in life. Such human values may be violated by mobile software applications (apps), and the negative effects of such human value violations can be seen in various ways in society. In this work, we focus on the human value of honesty. We present a model to support the automatic identification of violations of the value of honesty from app reviews from an end-user perspective. Beyond the automatic detection of honesty violations by apps, we also aim to better understand different categories of honesty violations expressed by users in their app reviews. The result of our manual analysis of our honesty violations dataset shows that honesty violations can be characterised into ten categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. Based on these results, we argue for a conscious effort in developing more honest software artefacts including mobile apps, and the promotion of honesty as a key value in software development practices. Furthermore, we discuss the role of app distribution platforms as enforcers of ethical systems supporting human values, and highlight some proposed next steps for human values in software engineering (SE) research.},
	language = {en},
	author = {Obie, Humphrey O and Ilekura, Idowu and Du, Hung},
}

@inproceedings{thomas_sok_2021,
	title = {{SoK}: {Hate}, {Harassment}, and the {Changing} {Landscape} of {Online} {Abuse}},
	shorttitle = {{SoK}},
	url = {https://ieeexplore.ieee.org/document/9519435},
	doi = {10.1109/SP40001.2021.00028},
	abstract = {We argue that existing security, privacy, and antiabuse protections fail to address the growing threat of online hate and harassment. In order for our community to understand and address this gap, we propose a taxonomy for reasoning about online hate and harassment. Our taxonomy draws on over 150 interdisciplinary research papers that cover disparate threats ranging from intimate partner violence to coordinated mobs. In the process, we identify seven classes of attacks—such as toxic content and surveillance—that each stem from different attacker capabilities and intents. We also provide longitudinal evidence from a three-year survey that hate and harassment is a pervasive, growing experience for online users, particularly for at-risk communities like young adults and people who identify as LGBTQ+. Responding to each class of hate and harassment requires a unique strategy and we highlight five such potential research directions that ultimately empower individuals, communities, and platforms to do so.},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Thomas, Kurt and Akhawe, Devdatta and Bailey, Michael and Boneh, Dan and Bursztein, Elie and Consolvo, Sunny and Dell, Nicola and Durumeric, Zakir and Kelley, Patrick Gage and Kumar, Deepak and McCoy, Damon and Meiklejohn, Sarah and Ristenpart, Thomas and Stringhini, Gianluca},
	month = may,
	year = {2021},
	note = {ISSN: 2375-1207},
	keywords = {Cognition, Computer security, Distance measurement, Privacy, Social networking (online), Taxonomy, at-risk, emerging-threats, harassment, hate},
	pages = {247--267},
}

@misc{tunstall_announcing_nodate,
	title = {Announcing {Evaluation} on the {Hub}},
	url = {https://huggingface.co/blog/eval-on-the-hub},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-12-19},
	author = {Tunstall, Lewis and Thakur, Abhishek and Thrush, Tristan and Luccioni, Sasha and Werra, Leandro von and Rajani, Nazneen and Piktus, Ola and Sanseviero, Omar and Kiela, Douwe},
}

@misc{koscik_identifying_2018,
	title = {Identifying {Abuse} {Vectors}},
	url = {https://web.archive.org/web/20220818200307/https://spinecone.gitbooks.io/identifying-abuse-vectors/content/},
	urldate = {2022-06-02},
	author = {Koscik, Terian},
	year = {2018},
}

@misc{noauthor_snpsfuzzer_nodate,
	title = {{SNPSFuzzer}: {A} {Fast} {Greybox} {Fuzzer} for {Stateful} {Network} {Protocols} using {Snapshots}},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{liang_pruning_2021,
	title = {Pruning and quantization for deep neural network acceleration: {A} survey},
	volume = {461},
	issn = {0925-2312},
	shorttitle = {Pruning and quantization for deep neural network acceleration},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221010894},
	doi = {10.1016/j.neucom.2021.07.045},
	abstract = {Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.},
	language = {en},
	urldate = {2022-12-09},
	journal = {Neurocomputing},
	author = {Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
	month = oct,
	year = {2021},
	keywords = {Convolutional neural network, Low-bit mathematics, Neural network acceleration, Neural network pruning, Neural network quantization},
	pages = {370--403},
}

@misc{zhuang_comprehensive_2020,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	url = {http://arxiv.org/abs/1911.02685},
	doi = {10.48550/arXiv.1911.02685},
	abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jun,
	year = {2020},
	note = {arXiv:1911.02685 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{strubell_energy_2020,
	title = {Energy and {Policy} {Considerations} for {Modern} {Deep} {Learning} {Research}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7123},
	doi = {10.1609/aaai.v34i09.7123},
	abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
	language = {en},
	number = {09},
	urldate = {2022-12-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = apr,
	year = {2020},
	note = {Number: 09},
	pages = {13693--13696},
}

@article{thompson_facebook_2022,
	chapter = {Technology},
	title = {Facebook {Failed} to {Stop} {Ads} {Threatening} {Election} {Workers}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2022/12/01/technology/facebook-ads-threats.html},
	abstract = {The ads, submitted by researchers, were rejected by YouTube and TikTok.},
	language = {en-US},
	urldate = {2022-12-05},
	journal = {The New York Times},
	author = {Thompson, Stuart A.},
	month = dec,
	year = {2022},
	keywords = {Computers and the Internet, Facebook Inc, Global Witness, Meta Platforms Inc, Midterm Elections (2022), New York University, Online Advertising, Right-Wing Extremism and Alt-Right, Rumors and Misinformation, Social Media, Threats and Threatening Messages, TikTok (ByteDance), YouTube.com},
}

@article{thompson_reflections_1984,
	title = {Reflections on trusting trust},
	volume = {27},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/358198.358210},
	doi = {10.1145/358198.358210},
	abstract = {To what extent should one trust a statement that a program is free of Trojan horses? Perhaps it is more important to trust the people who wrote the software.},
	language = {en},
	number = {8},
	urldate = {2022-12-05},
	journal = {Communications of the ACM},
	author = {Thompson, Ken},
	month = aug,
	year = {1984},
	pages = {761--763},
}

@misc{team_product_2022,
	title = {Product {Surveys}: {The} {Do}'s and {Don}'ts},
	shorttitle = {Product {Surveys}},
	url = {https://trustmary.com/surveys/product-surveys-dos-and-donts/},
	abstract = {Being able to create product surveys that are on point is not a walk in the park. This article will help you tackle the pain points.},
	language = {en-US},
	urldate = {2022-11-06},
	journal = {Trustmary},
	author = {team, Trustmary},
	month = feb,
	year = {2022},
}

@article{noauthor_surviving_nodate,
	title = {Surviving {Software} {Dependencies}},
	language = {en},
	pages = {24},
}

@article{potvin_why_2016,
	title = {Why {Google} stores billions of lines of code in a single repository},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2854146},
	doi = {10.1145/2854146},
	abstract = {Google's monolithic repository provides a common source of truth for tens of thousands of developers around the world.},
	language = {en},
	number = {7},
	urldate = {2022-12-05},
	journal = {Communications of the ACM},
	author = {Potvin, Rachel and Levenberg, Josh},
	month = jun,
	year = {2016},
	pages = {78--87},
}

@article{pike_interpreting_2005,
	title = {Interpreting the {Data}: {Parallel} {Analysis} with {Sawzall}},
	volume = {13},
	issn = {1058-9244, 1875-919X},
	shorttitle = {Interpreting the {Data}},
	url = {http://www.hindawi.com/journals/sp/2005/962135/abs/},
	doi = {10.1155/2005/962135},
	abstract = {Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new procedural programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design – including the separation into two phases, the form of the programming language, and the properties of the aggregators – exploits the parallelism inherent in having data and computation distributed across many machines.},
	language = {en},
	number = {4},
	urldate = {2022-12-05},
	journal = {Scientific Programming},
	author = {Pike, Rob and Dorward, Sean and Griesemer, Robert and Quinlan, Sean},
	year = {2005},
	pages = {277--298},
}

@article{winters_software_nodate,
	title = {Software {Engineering} at {Google}},
	language = {en},
	author = {Winters, Titus and Manshreck, Tom and Wright, Hyrum},
	pages = {602},
}

@book{noauthor_notitle_nodate,
}

@book{schorlemmer_sok_2022,
	title = {Sok: {Analysis} of {Software} {Supply} {Chain} {Security} by {Establishing} {Secure} {Design} {Properties}},
	volume = {1},
	isbn = {978-1-4503-9885-5},
	publisher = {Association for Computing Machinery},
	author = {Schorlemmer, Taylor R and Davis, James C},
	year = {2022},
	doi = {10.1145/3560835.3564556},
	note = {Publication Title: Proceedings of the 2022 ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses (SCORED '22), November 11, 2022, Los Angeles, CA, USA
Issue: 1},
	keywords = {Software Supply Chain Attacks, Security Properties},
}

@book{gopalakrishna__nodate,
	title = {“ {If} security is required ”: {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	volume = {1},
	isbn = {978-1-4503-9332-4},
	publisher = {Association for Computing Machinery},
	author = {Gopalakrishna, Nikhil Krishna and Bland, Forrest Lee and Davis, James C},
	doi = {10.1145/3528227.3528565},
	note = {Publication Title: 4th International Workshop on Software Engineering Research and Practice for the IoT (SERP4IoT'22), May 19, 2022, Pittsburgh, PA, USA
Issue: 1},
	keywords = {Internet of Things, Machine Learning, Security and, cyber-, internet of things, machine learning, security and privacy},
}

@book{newman_sigstore_nodate,
	title = {Sigstore : {Software} {Signing} for {Everybody}},
	volume = {1},
	isbn = {978-1-4503-9450-5},
	publisher = {Association for Computing Machinery},
	author = {Newman, Zachary and Meyers, John Speed},
	doi = {10.1145/3548606.3560596},
	note = {Publication Title: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (CCS '22), November 7â•ﬁ11, 2022, Los Angeles, CA, USA
Issue: 1},
	keywords = {2022, acm reference format, and santiago torres-arias, code signing, distributed systems, john speed meyers, security, software transparency, zachary newman},
}


@article{whitten_why_2005,
	title = {Why {Johnny} {Can} ’ t {Encrypt}},
	volume = {1999},
	abstract = {U ER ERRORS CAUSE OR CONTRIBUTE TO MOST COMPUTER SECURITY FAILURES, yet user interfaces for security still tend to be clumsy, confusing, or near nonexistent. Is this simply because of a failure to apply standard user interface design techniques to security? We argue that, on the contrary, effective security requires a different usability standard, and that it will not be achieved through the user interface design techniques appropriate to other types of consumer software.1 To test this hypothesis, we performed a case study of a security program that does have a good user interface by general standards: PGP 5.0. Our case study used a cognitive walkthrough analysis together with a laboratory user test to evaluate whether PGP 5.0 can be used successfully by cryptography novices to achieve effective electronic mail security. The analysis found a number of user interface design flaws that may contribute to security failures, and the user test demonstrated that when our test participants were given 90 minutes in which to sign and encrypt a message using PGP 5.0, the majority of them were unable to do so successfully. We conclude that PGP 5.0 is not usable enough to provide effective security for most computer users, despite its attractive graphical user interface, supporting our hypothesis that user interface design for effective security remains an open problem. We close with a brief description of our continuing work on the development and application of user interface design principles and techniques for security.},
	number = {October},
	journal = {Security},
	author = {Whitten, a and Tygar, Jd},
	year = {2005},
	note = {ISBN: 05960082799780596008277},
	keywords = {definition, deriving usability standard for pgp, why johnny can't encrypt},
	pages = {679--702},
}

@misc{noauthor_notitle_nodate-1,
}

@inproceedings{anandayuvaraj_reflecting_2022,
	title = {Reflecting on recurring failures in {IoT} development},
	doi = {https://doi.org/10.1145/3551349.3559545},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Anandayuvaraj, Dharun and Davis, James C},
	year = {2022},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@techreport{lella_enisa_2021,
	title = {{ENISA} threat landscape for supply chain attacks.},
	url = {https://data.europa.eu/doi/10.2824/168593},
	abstract = {This report aims at mapping and studying the supply chain attacks that were discovered from January 2020 to early July 2021. Based on the trends and patterns observed, supply chain attacks increased in number and sophistication in the year 2020 and this trend is continuing in 2021, posing an increasing risk for organizations. It is estimated that there will be four times more supply chain attacks in 2021 than in 2020. With half of the attacks being attributed to Advanced Persistence Threat (APT) actors, their complexity and resources greatly exceed the more common non-targeted attacks, and, therefore, there is an increasing need for new protective methods that incorporate suppliers in order to guarantee that organizations remain secure.},
	language = {en},
	urldate = {2022-06-21},
	institution = {European Union Agency for Cybersecurity},
	editor = {Lella, Ifigeneia and Theocharidou, Marianthi and Tsekmezoglou, Eleni and Malatras, Apostolos},
	month = jul,
	year = {2021},
	keywords = {Government},
}

@techreport{enduring_security_framework_securing_2022,
	title = {Securing the {Software} {Supply} {Chain}: {Recommended} {Practices} {Guide} for {Customers}},
	url = {https://media.defense.gov/2022/Nov/17/2003116445/-1/-1/0/ESF_SECURING_THE_SOFTWARE_SUPPLY_CHAIN_CUSTOMER.PDF},
	institution = {Cybersecurity and Infrastructure Security Agency},
	author = {Enduring Security Framework},
	month = oct,
	year = {2022},
}

@misc{noauthor_supply-chain_nodate,
	title = {Supply-chain {Levels} for {Software} {Artifacts}},
	url = {http://slsa.dev/},
	abstract = {Security framework to ensure software supply chain integrity},
	language = {en},
	urldate = {2022-11-29},
	journal = {SLSA},
}

@misc{white_house_executive_2021,
	title = {Executive {Order} on {Improving} the {Nation}'s {Cybersecurity}},
	url = {https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/},
	abstract = {By the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:Section 1.},
	language = {en-US},
	urldate = {2022-11-29},
	journal = {The White House},
	author = {White House},
	month = may,
	year = {2021},
}

@misc{cimpanu_microsoft_2020,
	title = {Microsoft, {FireEye} confirm {SolarWinds} supply chain attack},
	url = {https://www.zdnet.com/article/microsoft-fireeye-confirm-solarwinds-supply-chain-attack/},
	abstract = {Known victims so far include the US Treasury, the US NTIA, and FireEye itself.},
	language = {en},
	urldate = {2022-11-29},
	journal = {ZDNET},
	author = {Cimpanu, Catalin},
	month = dec,
	year = {2020},
}

@misc{microsoft_security_response_center_customer_2020,
	title = {Customer {Guidance} on {Recent} {Nation}-{State} {Cyber} {Attacks} – {Microsoft} {Security} {Response} {Center}},
	url = {https://msrc-blog.microsoft.com/2020/12/13/customer-guidance-on-recent-nation-state-cyber-attacks/},
	language = {en-US},
	urldate = {2022-11-29},
	author = {{Microsoft Security Response Center}},
	month = dec,
	year = {2020},
}

@techreport{nissen_deliver_2018,
	title = {Deliver {Uncompromised}: {A} {Strategy} for {Supply} {Chain} {Security} and {Resilience}},
	url = {https://www.mitre.org/sites/default/files/2021-11/prs-18-2417-deliver-uncompromised-MITRE-study-26AUG2019.pdf},
	language = {en},
	institution = {The MITRE Center for Technology \& National Security},
	author = {Nissen, Christopher and Gronager, John and Metzger, Robert and Rishikof, Harvey},
	month = aug,
	year = {2018},
	pages = {56},
}

@misc{noauthor_deliver_2018,
	title = {Deliver {Uncompromised}: {A} {Strategy} for {Supply} {Chain} {Security} and {Resilience} in {Response} to the {Changing} {Character} of {War}},
	shorttitle = {Deliver {Uncompromised}},
	url = {https://www.mitre.org/news-insights/publication/deliver-uncompromised-strategy-supply-chain-security-and-resilience},
	abstract = {The nature of warfare is changing, bringing new threats to the defense supply chain that must be addressed. This report examines options that span legislation and regulation, policy and adminis¬tration, acquisition and oversight, programs and technology.},
	language = {en},
	urldate = {2022-11-29},
	journal = {MITRE},
	month = aug,
	year = {2018},
}

@article{kim_promoting_2022,
	title = {Promoting {Online} {Civility} {Through} {Platform} {Architecture}},
	volume = {1},
	copyright = {Copyright (c) 2022 Journal of Online Trust and Safety},
	issn = {2770-3142},
	url = {https://tsjournal.org/index.php/jots/article/view/54},
	doi = {10.54501/jots.v1i4.54},
	abstract = {This study tests whether the architecture of a social media platform can encourage conversations among users to be more civil. It was conducted in collaboration with Nextdoor, a networking platform for neighbors within a defined geographic area. The study involved: (1) prompting users to move popular posts from the neighborhood-wide feed to new groups dedicated to the topic and (2) an experiment that randomized the announcement of community guidelines to members who join those newly formed groups. We examined the impact of each intervention on the level of civility, moral values reflected in user comments, and user’s submitted reports of inappropriate content. In a large quantitative analysis of comments posted to Nextdoor, the results indicate that platform architecture can shape the civility of conversations. Comments within groups were more civil and less frequently reported to Nextdoor moderators than the comments on the neighborhood-wide posts. In addition, comments in groups where new members were shown guidelines were less likely to be reported to moderators and were expressed in a more morally virtuous tone than comments in groups where new members were not presented with guidelines. This research demonstrates the importance of considering the design, structure, and affordance of the online environment when online platforms seek to promote civility and other pro-social behaviors.},
	language = {en},
	number = {4},
	urldate = {2022-11-22},
	journal = {Journal of Online Trust and Safety},
	author = {Kim, Jisu and McDonald, Curtis and Meosky, Paul and Katsaros, Matthew and Tyler, Tom},
	month = sep,
	year = {2022},
	note = {Number: 4},
	keywords = {Social media},
}

@incollection{taibi_resem_2022,
	address = {Cham},
	title = {Resem: {Searching} {Regular} {Expression} {Patterns} with {Semantics} and {Input}/{Output} {Examples}},
	volume = {13709},
	isbn = {978-3-031-21387-8 978-3-031-21388-5},
	shorttitle = {Resem},
	url = {https://link.springer.com/10.1007/978-3-031-21388-5_35},
	abstract = {Regular expression is widely known as a powerful and generalpurpose text processing tool for programming. Though the regular expression is highly versatile, there are various diﬃculties in using them. One promising approach to reduce the burden of the pattern composition is reuse by referring to past usages. Still, several source code-specialized search engines have been proposed, they are not suitable for the scenario of reusing regular expression patterns. The purpose of this study is the eﬃcient reuse of regular expression patterns. To achieve the purpose, we propose a usage retrieval system Resem specialized in regular expression patterns. Resem adopts two key features: search by semantics and collecting input/output examples. Resem will smoothly connect what to do to how to do in the implementation process of string manipulation.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer International Publishing},
	author = {Takeshige, Hiroki and Matsumoto, Shinsuke and Kusumoto, Shinji},
	editor = {Taibi, Davide and Kuhrmann, Marco and Mikkonen, Tommi and Klünder, Jil and Abrahamsson, Pekka},
	year = {2022},
	doi = {10.1007/978-3-031-21388-5_35},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {511--517},
}

@inproceedings{padhye_jqf_2019,
	address = {New York, NY, USA},
	series = {{ISSTA} 2019},
	title = {{JQF}: coverage-guided property-based testing in {Java}},
	isbn = {9781450362245},
	shorttitle = {{JQF}},
	url = {https://doi.org/10.1145/3293882.3339002},
	doi = {10.1145/3293882.3339002},
	abstract = {We present JQF, a platform for performing coverage-guided fuzz testing in Java. JQF is designed both for practitioners, who wish to find bugs in Java programs, as well as for researchers, who wish to implement new fuzzing algorithms. Practitioners write QuickCheck-style test methods that take inputs as formal parameters. JQF instruments the test program's bytecode and continuously executes tests using inputs that are generated in a coverage-guided fuzzing loop. JQF's input-generation mechanism is extensible. Researchers can implement custom fuzzing algorithms by extending JQF's Guidance interface. A Guidance instance responds to code coverage events generated during the execution of a test case, such as function calls and conditional jumps, and provides the next input. We describe several guidances that currently ship with JQF, such as: semantic fuzzing with Zest, binary fuzzing with AFL, and complexity fuzzing with PerfFuzz. JQF is a mature tool that is open-source and publicly available. At the time of writing, JQF has been successful in discovering 42 previously unknown bugs in widely used open-source software such as OpenJDK, Apache Commons, and the Google Closure Compiler.},
	urldate = {2022-11-21},
	publisher = {Association for Computing Machinery},
	author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik},
	month = jul,
	year = {2019},
	keywords = {Coverage-guided fuzzing, QuickCheck, property-based testing},
	pages = {398--401},
}

@misc{noauthor_kahns_2016,
	title = {Kahn's algorithm for {Topological} {Sorting}},
	url = {https://www.geeksforgeeks.org/topological-sorting-indegree-based-solution/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-us},
	urldate = {2022-11-21},
	journal = {GeeksforGeeks},
	month = apr,
	year = {2016},
	note = {Section: Graph},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-11-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@article{kotti_impact_2022,
	title = {Impact of {Software} {Engineering} {Research} in {Practice}: {A} {Patent} and {Author} {Survey} {Analysis}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {Impact of {Software} {Engineering} {Research} in {Practice}},
	url = {http://arxiv.org/abs/2204.03366},
	doi = {10.1109/TSE.2022.3208210},
	abstract = {Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4,354 SE patents citing 1,690 SE papers published in four leading SE venues between 1975-2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and awarded publications, achieving 26\% response rate. Overall, researchers have equipped practitioners with various tools, processes, and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited funding resources or unfavorable cost-benefit trade-off of the proposed solutions. The need for higher SE research funding could be corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its definition. Therefore, academia and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic.},
	urldate = {2022-11-18},
	journal = {IEEE Transactions on Software Engineering},
	author = {Kotti, Zoe and Gousios, Georgios and Spinellis, Diomidis},
	year = {2022},
	note = {arXiv:2204.03366 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {1--19},
}

@inproceedings{schumilo_nyx-net_2022,
	address = {New York, NY, USA},
	series = {{EuroSys} '22},
	title = {Nyx-net: network fuzzing with incremental snapshots},
	isbn = {9781450391627},
	shorttitle = {Nyx-net},
	url = {https://doi.org/10.1145/3492321.3519591},
	doi = {10.1145/3492321.3519591},
	abstract = {Coverage-guided fuzz testing ("fuzzing") has become mainstream and we have observed lots of progress in this research area recently. However, it is still challenging to efficiently test network services with existing coverage-guided fuzzing methods. In this paper, we introduce the design and implementation of Nyx-Net, a novel snapshot-based fuzzing approach that can successfully fuzz a wide range of targets spanning servers, clients, games, and even Firefox's Inter-Process Communication (IPC) interface. Compared to state-of-the-art methods, Nyx-Net improves test throughput by up to 300x and coverage found by up to 70\%. Additionally, Nyx-Net is able to find crashes in two of ProFuzzBench's targets that no other fuzzer found previously. When using Nyx-Net to play the game Super Mario, Nyx-Net shows speedups of 10--30x compared to existing work. Moreover, Nyx-Net is able to find previously unknown bugs in servers such as Lighttpd, clients such as MySQL client, and even Firefox's IPC mechanism---demonstrating the strength and versatility of the proposed approach. Lastly, our prototype implementation was awarded a \$20.000 bug bounty for enabling fuzzing on previously unfuzzable code in Firefox and solving a long-standing problem at Mozilla.},
	urldate = {2022-11-17},
	publisher = {Association for Computing Machinery},
	author = {Schumilo, Sergej and Aschermann, Cornelius and Jemmett, Andrea and Abbasi, Ali and Holz, Thorsten},
	month = mar,
	year = {2022},
	keywords = {fuzzing, software security, testing},
	pages = {166--180},
}

@inproceedings{chen_sfuzz_2022,
	address = {New York, NY, USA},
	series = {{CCS} '22},
	title = {{SFuzz}: {Slice}-based {Fuzzing} for {Real}-{Time} {Operating} {Systems}},
	isbn = {9781450394505},
	shorttitle = {{SFuzz}},
	url = {https://doi.org/10.1145/3548606.3559367},
	doi = {10.1145/3548606.3559367},
	abstract = {Real-Time Operating System (RTOS) has become the main category of embedded systems. It is widely used to support tasks requiring real-time response such as printers and switches. The security of RTOS has been long overlooked as it was running in special environments isolated from attackers. However, with the rapid development of IoT devices, tremendous RTOS devices are connected to the public network. Due to the lack of security mechanisms, these devices are extremely vulnerable to a wide spectrum of attacks. Even worse, the monolithic design of RTOS combines various tasks and services into a single binary, which hinders the current program testing and analysis techniques working on RTOS. In this paper, we propose SFuzz, a novel slice-based fuzzer, to detect security vulnerabilities in RTOS. Our insight is that RTOS usually divides a complicated binary into many separated but single-minded tasks. Each task accomplishes a particular event in a deterministic way and its control flow is usually straightforward and independent. Therefore, we identify such code from the monolithic RTOS binary and synthesize a slice for effective testing. Specifically, SFuzz first identifies functions that handle user input, constructs call graphs that start from callers of these functions, and leverages forward slicing to build the execution tree based on the call graphs and pruning the paths independent of external inputs. Then, it detects and handles roadblocks within the coarse-grain scope that hinder effective fuzzing, such as instructions unrelated to the user input. And then, it conducts coverage-guided fuzzing on these code snippets. Finally, SFuzz leverages forward and backward slicing to track and verify each path constraint and determine whether a bug discovered in the fuzzer is a real vulnerability. SFuzz successfully discovered 77 zero-day bugs on 35 RTOS samples, and 67 of them have been assigned CVE or CNVD IDs. Our empirical evaluation shows that SFuzz outperforms the state-of-the-art tools (e.g., UnicornAFL) on testing RTOS.},
	urldate = {2022-11-17},
	publisher = {Association for Computing Machinery},
	author = {Chen, Libo and Cai, Quanpu and Ma, Zhenbang and Wang, Yanhao and Hu, Hong and Shen, Minghang and Liu, Yue and Guo, Shanqing and Duan, Haixin and Jiang, Kaida and Xue, Zhi},
	month = nov,
	year = {2022},
	keywords = {concolic execution, rtos, slice-based fuzzing, taint analysis},
	pages = {485--498},
}

@inproceedings{cappos_look_2008,
	address = {New York, NY, USA},
	series = {{CCS} '08},
	title = {A look in the mirror: attacks on package managers},
	isbn = {978-1-59593-810-7},
	shorttitle = {A look in the mirror},
	url = {https://doi.org/10.1145/1455770.1455841},
	doi = {10.1145/1455770.1455841},
	abstract = {This work studies the security of ten popular package managers. These package managers use different security mechanisms that provide varying levels of usability and resilience to attack. We find that, despite their existing security mechanisms, all of these package managers have vulnerabilities that can be exploited by a man-in-the-middle or a malicious mirror. While all current package managers suffer from vulnerabilities, their security is also positively or negatively impacted by the distribution's security practices. Weaknesses in package managers are more easily exploited when distributions use third-party mirrors as official mirrors. We were successful in using false credentials to obtain an official mirror on all five of the distributions we attempted. We also found that some security mechanisms that control where a client obtains metadata and packages from may actually decrease security. We analyze current package managers to show that by exploiting vulnerabilities, an attacker with a mirror can compromise or crash hundreds to thousands of clients weekly. The problems we disclose are now being corrected by many different package manager maintainers.},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the 15th {ACM} conference on {Computer} and communications security},
	publisher = {Association for Computing Machinery},
	author = {Cappos, Justin and Samuel, Justin and Baker, Scott and Hartman, John H.},
	month = oct,
	year = {2008},
	keywords = {mirrors, package management, replay attack},
	pages = {565--574},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-11-15},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{newman_sigstore_2022,
	address = {New York, NY, USA},
	series = {{CCS} '22},
	title = {Sigstore: {Software} {Signing} for {Everybody}},
	isbn = {978-1-4503-9450-5},
	shorttitle = {Sigstore},
	url = {https://doi.org/10.1145/3548606.3560596},
	doi = {10.1145/3548606.3560596},
	abstract = {Software supply chain compromises are on the rise. From the effects of XCodeGhost to SolarWinds, hackers have identified that targeting weak points in the supply chain allows them to compromise high-value targets such as U.S. government agencies and corporate targets such as Google and Microsoft. Software signing, a promising mitigation for many of these attacks, has seen limited adoption in open-source and enterprise ecosystems. In this paper, we propose Sigstore, a system to provide widespread software signing capabilities. To do so, we designed the system to provide baseline artifact signing capabilities that minimize the adoption barrier for developers. To this end, Sigstore leverages three distinct mechanisms: First, it uses a protocol similar to ACME to authenticate developers through OIDC, tying signatures to existing and widely-used identities. Second, it enables developers to use ephemeral keys to sign their artifacts, reducing the inconvenience and risk of key management. Finally, Sigstore enables user authentication by means of artifact and identity logs, bringing transparency to software signatures. Sigstore is quickly becoming a critical piece of Internet infrastructure with more than 2.2M signatures over critical software such as Kubernetes and Distroless.},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the 2022 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Newman, Zachary and Meyers, John Speed and Torres-Arias, Santiago},
	month = nov,
	year = {2022},
	keywords = {code signing, distributed systems, security, software transparency},
	pages = {2353--2367},
}

@inproceedings{kariyappa_maze_2021,
	address = {Nashville, TN, USA},
	title = {{MAZE}: {Data}-{Free} {Model} {Stealing} {Attack} {Using} {Zeroth}-{Order} {Gradient} {Estimation}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{MAZE}},
	url = {https://ieeexplore.ieee.org/document/9577631/},
	doi = {10.1109/CVPR46437.2021.01360},
	abstract = {High quality Machine Learning (ML) models are often considered valuable intellectual property by companies. Model Stealing (MS) attacks allow an adversary with blackbox access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset necessary to query the target model. In this paper, we show that preventing access to the target dataset is not an adequate defense to protect a model. We propose MAZE – a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created using a generative model to perform MS.},
	language = {en},
	urldate = {2022-11-08},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kariyappa, Sanjay and Prakash, Atul and Qureshi, Moinuddin K},
	month = jun,
	year = {2021},
	pages = {13809--13818},
}

@misc{li_data_2022,
	title = {Data {Stealing} {Attack} on {Medical} {Images}: {Is} it {Safe} to {Export} {Networks} from {Data} {Lakes}?},
	shorttitle = {Data {Stealing} {Attack} on {Medical} {Images}},
	url = {http://arxiv.org/abs/2206.03391},
	abstract = {In privacy-preserving machine learning, it is common that the owner of the learned model does not have any physical access to the data. Instead, only a secured remote access to a data lake is granted to the model owner without any ability to retrieve data from the data lake. Yet, the model owner may want to export the trained model periodically from the remote repository and a question arises whether this may cause is a risk of data leakage. In this paper, we introduce the concept of data stealing attack during the export of neural networks. It consists in hiding some information in the exported network that allows the reconstruction outside the data lake of images initially stored in that data lake. More precisely, we show that it is possible to train a network that can perform lossy image compression and at the same time solve some utility tasks such as image segmentation. The attack then proceeds by exporting the compression decoder network together with some image codes that leads to the image reconstruction outside the data lake. We explore the feasibility of such attacks on databases of CT and MR images, showing that it is possible to obtain perceptually meaningful reconstructions of the target dataset, and that the stolen dataset can be used in turns to solve a broad range of tasks. Comprehensive experiments and analyses show that data stealing attacks should be considered as a threat for sensitive imaging data sources.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Li, Huiyu and Ayache, Nicholas and Delingette, Hervé},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03391 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{orekondy_prediction_2020,
	title = {Prediction {Poisoning}: {Towards} {Defenses} {Against} {DNN} {Model} {Stealing} {Attacks}},
	shorttitle = {Prediction {Poisoning}},
	url = {http://arxiv.org/abs/1906.10908},
	abstract = {High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85\${\textbackslash}times\$ with minimal impact on the utility for benign users.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Orekondy, Tribhuvanesh and Schiele, Bernt and Fritz, Mario},
	month = mar,
	year = {2020},
	note = {arXiv:1906.10908 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{juuti_prada_2019,
	title = {{PRADA}: {Protecting} {Against} {DNN} {Model} {Stealing} {Attacks}},
	shorttitle = {{PRADA}},
	doi = {10.1109/EuroSP.2019.00044},
	abstract = {Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.},
	booktitle = {2019 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	author = {Juuti, Mika and Szyller, Sebastian and Marchal, Samuel and Asokan, N.},
	month = jun,
	year = {2019},
	keywords = {Adversarial machine learning, Business, Computational modeling, Data mining, Mathematical model, Neural networks, Predictive models, Training, deep neural network, model extraction, model stealing},
	pages = {512--527},
}

@inproceedings{laorden_threat_2010,
	address = {Berlin, Heidelberg},
	series = {Advances in {Intelligent} and {Soft} {Computing}},
	title = {A {Threat} {Model} {Approach} to {Threats} and {Vulnerabilities} in {On}-line {Social} {Networks}},
	abstract = {On-line Social Networks (OSN) have become one of the most used Internet services. However, as happens with every new technology, they are prone to several security issues. Despite privacy concerns begin to emerge, there are still other dangerous vulnerabilities that affect security and threaten organisations and users assets. In this paper, we present the first Threat Modelling approach in Online Social Networks that intends to identify the threats and vulnerabilities that can be exploited. Next, we define what we call the Circle of Risk (CoR), a graphical definition of every security aspect involved in the threat modelling.},
	language = {en},
	booktitle = {Computational {Intelligence} in {Security} for {Information} {Systems} 2010},
	publisher = {Springer},
	author = {Laorden, Carlos and Sanz, Borja and Alvarez, Gonzalo and Bringas, Pablo G.},
	editor = {Herrero, Álvaro and Corchado, Emilio and Redondo, Carlos and Alonso, Ángel},
	year = {2010},
	keywords = {On-line Social Networks, privacy, threat modelling, web security},
}

@misc{wikipedia_fediverse_2022,
	title = {Fediverse},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Fediverse&oldid=1117783694},
	abstract = {The fediverse (a portmanteau of "federation" and "universe") is an ensemble of federated (i.e. interconnected) servers that are used for web publishing (i.e. social networking, microblogging, blogging, or websites) and file hosting, but which, while independently hosted, can communicate with each other. 
On different servers (instances), users can create so-called identities. These identities are able to communicate over the boundaries of the instances because the software running on the servers supports one or more communication protocols that follow an open standard. As an identity on the fediverse, users are able to post text and other media, or to follow posts by other identities. In some cases, users can show or share data (video, audio, text, and other files) publicly or to a selected group of identities and allow other identities to edit other users' data (such as a calendar or an address book).},
	language = {en},
	urldate = {2022-10-26},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1117783694},
}

@misc{wikipedia_activity_2022,
	title = {Activity stream},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Activity_stream&oldid=1109477763},
	abstract = {An activity stream is a list of recent activities performed by an individual, typically on a single website. For example, Facebook's News Feed is an activity stream. Since the introduction of the News Feed on September 6, 2006, other major websites have introduced similar implementations for their own users. Since the proliferation of activity streams on websites, there have been calls to standardize the format so that websites could interact with a stream provided by another website. The Activity Streams project, for example, is an effort to develop an activity stream protocol to syndicate activities across social web applications. Several major websites with activity stream implementations have already opened up their activity streams to developers to use, including Facebook and MySpace.Though activity stream arises from social networking, nowadays it has become an essential part of business software. Enterprise social software is used in different types of companies to organize their internal communication and acts as an addition to traditional corporate intranet. Collaboration software like Jive Software, Yammer, and Chatter offer activity stream as a separate product. At the same time other software providers such as tibbr, Central Desktop, and Wrike offer activity stream as an integrated part of their collaboration software solution.Activity streams come in two different variations:

Generic feeds: all users see the same content in the activity stream.
Personalised feeds: each user gets bespoke items as well as custom ranking of each element in the feed.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = sep,
	year = {2022},
	note = {Page Version ID: 1109477763},
}

@misc{wikipedia_comparison_2022,
	title = {Comparison of microblogging and similar services},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Comparison_of_microblogging_and_similar_services&oldid=1085241013},
	abstract = {The tables below compare general and technical information for some notable active microblogging services, and also social network services that have status updates.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = apr,
	year = {2022},
	note = {Page Version ID: 1085241013},
}

@article{marwick_far-right_2022,
	title = {Far-{Right} {Online} {Radicalization}: {A} {Review} of the {Literature}},
	shorttitle = {Far-{Right} {Online} {Radicalization}},
	url = {https://citap.pubpub.org/pub/jq7l6jny/release/1},
	doi = {10.21428/bfcb0bff.e9492a11},
	abstract = {This literature review examines cross-disciplinary work on radicalization to situate, historicize, frame, and better understand the present concerns around online radicalization and far-right extremist and fringe movements. We find that research on radicalization is inextricably linked to the post-9/11 context in which it emerged, and as a result is overly focused on studying the other. Applying this research to the spread of far-right ideas online does not account for the ways in which the far-right’s endorsement of white supremacy and racism holds historical, normative precedent in the United States. Further, radicalization research is rife with uncertainties, ranging from definitional ambiguity to an inability to identify any simplistic, causal models capable of fully explaining the conditions under which radicalization occurs. Instead, there are multiple possible pathways to radicalization, and while the internet does not cause individuals to adopt far-right extremist or fringe beliefs, some technological affordances may aid adoption of these beliefs through gradual processes of socialization. We conclude that the term “radicalization” does not serve as a useful analytical frame for studying the spread of far-right and fringe ideas online. Instead, potential analytical frameworks better suited to studying these phenomena include theories prominent in the study of online communities, conversion, mainstreaming, and sociotechnical theories of media effects.},
	language = {en},
	urldate = {2022-11-04},
	journal = {The Bulletin of Technology \& Public Life},
	author = {Marwick, Alice and Clancy, Benjamin and Furl, Katherine},
	month = may,
	year = {2022},
}

@misc{masnick_hey_2022,
	title = {Hey {Elon}: {Let} {Me} {Help} {You} {Speed} {Run} {The} {Content} {Moderation} {Learning} {Curve}},
	url = {https://www.techdirt.com/2022/11/02/hey-elon-let-me-help-you-speed-run-the-content-moderation-learning-curve/},
	abstract = {It’s kind of a rite of passage for any new social media network. They show up, insist that they’re the “platform for free speech” without quite understanding what that actua…},
	language = {en-US},
	urldate = {2022-11-04},
	journal = {Techdirt},
	author = {Masnick, Mike},
	month = nov,
	year = {2022},
}

@misc{singhalSoKContentModeration2022,
	title = {{SoK}: {Content} {Moderation} in {Social} {Media}, from {Guidelines} to {Enforcement}, and {Research} to {Practice}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/2206.14855},
	doi = {10.48550/arXiv.2206.14855},
	abstract = {To counter online abuse and misinformation, social media platforms have been establishing content moderation guidelines and employing various moderation policies. The goal of this paper is to study these community guidelines and moderation practices, as well as the relevant research publications to identify the research gaps, differences in moderation techniques, and challenges that should be tackled by the social media platforms and the research community at large. In this regard, we study and analyze in the US jurisdiction the fourteen most popular social media content moderation guidelines and practices, and consolidate them. We then introduce three taxonomies drawn from this analysis as well as covering over one hundred interdisciplinary research papers about moderation strategies. We identified the differences between the content moderation employed in mainstream social media platforms compared to fringe platforms. We also highlight the implications of Section 230, the need for transparency and opacity in content moderation, why platforms should shift from a one-size-fits-all model to a more inclusive model, and lastly, we highlight why there is a need for a collaborative human-AI system.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Singhal, Mohit and Ling, Chen and Paudel, Pujan and Thota, Poojitha and Kumarswamy, Nihal and Stringhini, Gianluca and Nilizadeh, Shirin},
	month = oct,
	year = {2022},
	note = {arXiv:2206.14855 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Social and Information Networks},
}

@misc{vogels_online_2021,
	title = {Online harassment occurs most often on social media, but strikes in other places, too},
	url = {https://www.pewresearch.org/fact-tank/2021/02/16/online-harassment-occurs-most-often-on-social-media-but-strikes-in-other-places-too/},
	abstract = {Three-quarters of U.S. adults who have recently faced some kind of online harassment say it happened on social media.},
	language = {en-US},
	urldate = {2022-11-02},
	journal = {Pew Research Center},
	author = {Vogels, Emily a},
	month = feb,
	year = {2021},
}

@article{memon_role_2018,
	title = {The role of online social networking on deliberate self-harm and suicidality in adolescents: {A} systematized review of literature},
	volume = {60},
	issn = {0019-5545},
	shorttitle = {The role of online social networking on deliberate self-harm and suicidality in adolescents},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6278213/},
	doi = {10.4103/psychiatry.IndianJPsychiatry_414_17},
	abstract = {Social media use by minors has significantly increased and has been linked to depression and suicidality. Simultaneously, age-adjusted suicide rates have steadily increased over the past decade in the United States with suicide being the second most common cause of death in youth. Hence, the increase in suicide rate parallels the simultaneous increase in social media use. In addition, the rate of nonsuicidal self-injury ranges between 14\% and 21\% among young people. Evidence suggests that self-harming youth is more active on online social networks than youth who do not engage in self-harm behavior. The role of online social networking on deliberates self-harm and suicidality in adolescents with a focus on negative influence was assessed by conducting a systematized literature review. A literature search on “PubMed” and “Ovid Medline” using a combination of MeSH terms yielded nine articles for data extraction satisfying predefined inclusion/exclusion criteria. It was found that social networking websites are utilized by suicidal and self-harming youth as a medium to communicate with and to seek social support from other users. Online social networking also leads to increased exposure to and engagement in self-harm behavior due to users receiving negative messages promoting self-harm, emulating self-injurious behavior of others, and adopting self-harm practices from shared videos. Greater time spent on social networking websites led to higher psychological distress, an unmet need for mental health support, poor self-rated mental health, and increased suicidal ideation. In conclusion, greater time spent on online social networking promotes self-harm behavior and suicidal ideation in vulnerable adolescents.},
	number = {4},
	urldate = {2022-11-02},
	journal = {Indian Journal of Psychiatry},
	author = {Memon, Aksha M. and Sharma, Shiva G. and Mohite, Satyajit S. and Jain, Shailesh},
	year = {2018},
	pmid = {30581202},
	pmcid = {PMC6278213},
	pages = {384--392},
}

@misc{vogels_state_2021,
	title = {The {State} of {Online} {Harassment}},
	url = {https://www.pewresearch.org/internet/2021/01/13/the-state-of-online-harassment/},
	abstract = {Roughly four-in-ten Americans have experienced online harassment, with half of this group citing politics as the reason they think they were targeted. Growing shares face more severe online abuse such as sexual harassment or stalking},
	language = {en-US},
	urldate = {2022-11-02},
	journal = {Pew Research Center: Internet, Science \& Tech},
	author = {Vogels, Emily A.},
	month = jan,
	year = {2021},
}

@misc{international_standards_organization_iso_2018,
	title = {{ISO} 31000:2018(en), {Risk} management — {Guidelines}},
	url = {https://www.iso.org/obp/ui/#iso:std:iso:31000},
	urldate = {2022-07-07},
	author = {{International Standards Organization}},
	year = {2018},
}

@inproceedings{xiao_integrated_2003,
	title = {Integrated {TCP}/{IP} protocol software testing for vulnerability detection},
	doi = {10.1109/ICCNMC.2003.1243061},
	abstract = {Many security holes stem from the defects in network protocol implementations. This paper presents an industry best practice of integrated TCP/IP network protocol testing that targets software robustness vulnerabilities. The deployed test system consists of a versatile test engine, a protocol data unit generator and a few auxiliary tools. The specially designed kernel test engine supporting IP/TCP/UDP as carrier protocols drives predefined fault-injected PDU (protocol data unit) to the network unit under test. Its novel callback mechanism and virtual network device connection capability cost-effectively enhance user controlled testing intelligence for verifying protocols with complicated state transitions. The PDU generator aims to provide a systematic solution for rapid test case creation, which is based on new strengthened BNF (Backus-Naur form) language for protocol specification mutation and fault injection. Established on this system, we propose an integrated industry test environment for network protocol code assessment. Initial experiments and case studies with multicast protocols unveiled several robustness violations, which have significant security impacts.},
	booktitle = {2003 {International} {Conference} on {Computer} {Networks} and {Mobile} {Computing}, 2003. {ICCNMC} 2003.},
	author = {Xiao, Shu and Deng, Lijun and Li, Sheng and Wang, Xiangrong},
	month = oct,
	year = {2003},
	keywords = {Best practices, Computer industry, Data security, Engines, IP networks, Protocols, Robustness, Software testing, System testing, TCPIP},
	pages = {311--319},
}

@inproceedings{corteggiani_inception_2018,
	title = {Inception: \{{System}-{Wide}\} {Security} {Testing} of \{{Real}-{World}\} {Embedded} {Systems} {Software}},
	isbn = {9781939133045},
	shorttitle = {Inception},
	url = {https://www.usenix.org/conference/usenixsecurity18/presentation/corteggiani},
	urldate = {2022-10-27},
	author = {Corteggiani, Nassim and Camurati, Giovanni and Francillon, Aurélien},
	year = {2018},
	pages = {309--326},
}

@inproceedings{gros_refuzz_2022,
	address = {Nagasaki Japan},
	title = {{ReFuzz} - {Structure} {Aware} {Fuzzing} of the {Resilient} {File} {System} ({ReFS})},
	isbn = {978-1-4503-9140-5},
	url = {https://dl.acm.org/doi/10.1145/3488932.3523260},
	doi = {10.1145/3488932.3523260},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {Proceedings of the 2022 {ACM} on {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Groß, Tobias and Schleier, Tobias and Müller, Tilo},
	month = may,
	year = {2022},
	pages = {589--601},
}

@inproceedings{aschermann_redqueen_2019,
	address = {San Diego, CA},
	title = {{REDQUEEN}: {Fuzzing} with {Input}-to-{State} {Correspondence}},
	isbn = {978-1-891562-55-6},
	shorttitle = {{REDQUEEN}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_04A-2_Aschermann_paper.pdf},
	doi = {10.14722/ndss.2019.23371},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Aschermann, Cornelius and Schumilo, Sergej and Blazytko, Tim and Gawlik, Robert and Holz, Thorsten},
	year = {2019},
}

@inproceedings{ma_you_2021,
	address = {Chicago IL USA},
	title = {“{You} have said too much”: {Java}-like verbosity anti-patterns in {Python} codebases},
	isbn = {978-1-4503-9089-7},
	shorttitle = {“{You} have said too much”},
	url = {https://dl.acm.org/doi/10.1145/3484272.3484960},
	doi = {10.1145/3484272.3484960},
	abstract = {As a popular language for teaching introductory programming, Java can profoundly influence beginner programmers with its coding style and idioms. Despite its many advantages, the paradigmatic coding style in Java is often described as verbose. As a result, when writing code in more concise languages, such programmers tend to emulate the familiar Java coding idioms, thus neglecting to take advantage of the more succinct counterparts in those languages. As a result of such verbosity, not only the overall code quality suffers, but the verbose non-idiomatic patterns also render code hard to understand and maintain. In this paper, we study the incidences of Java-like verbosity as they occur in Python codebases. We present a collection of Java-Like Verbosity Anti-patterns and our pilot study of their presence in representative open-source Python codebases. We discuss our findings as a call for action to computing educators, particularly those who work with introductory students. We need novel pedagogical interventions that encourage budding programmers to write concise idiomatic code in any language.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the 2021 {ACM} {SIGPLAN} {International} {Symposium} on {SPLASH}-{E}},
	publisher = {ACM},
	author = {Ma, Yuzhi and Tilevich, Eli},
	month = oct,
	year = {2021},
	pages = {13--18},
}

@article{Jiang2022PTMSupplyChain,
	title = {An {Empirical} {Study} of {Artifacts} and {Security} {Risks} in the {Pre}-trained {Model} {Supply} {Chain}},
	abstract = {Deep neural networks achieve state-of-the-art performance on many tasks, but require increasingly complex architectures and costly training procedures. Engineers can reduce costs by reusing a pre-trained model (PTM) and fine-tuning it for their own tasks. To facilitate software reuse, engineers collaborate around model hubs, collections of PTMs and datasets organized by problem domain. Although model hubs are now comparable in popularity and size to other software ecosystems, the associated PTM supply chain has not yet been examined from a software engineering perspective.},
	language = {en},
	journal = {Los Angeles},
	author = {Jiang, Wenxin and Synovic, Nicholas and Sethi, Rohan},
	year = {2022},
	pages = {10},
}

@misc{noauthor_biggest_nodate,
	title = {Biggest social media platforms 2022},
	url = {https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/},
	abstract = {Facebook, YouTube, and WhatsApp are the most popular social networks worldwide, each with at least two billion active users.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Statista},
}

@article{wong_be_2017,
	title = {Be more familiar with our enemies and pave the way forward: {A} review of the roles bugs played in software failures},
	volume = {133},
	issn = {0164-1212},
	shorttitle = {Be more familiar with our enemies and pave the way forward},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121217301334},
	doi = {10.1016/j.jss.2017.06.069},
	abstract = {There has been an increasing frequency of failures due to defective software that cost millions of dollars. Recent high profile incidents have drawn increased attention to the risks of failed software systems to the public. Yet aside from the Therac-25 case, very few incidents of software failure causing humans harm have been proven and widely reported. With increased government oversight and the expanded use of social networking for real time reporting of problems, we are only beginning to understand the potential for major injury or death related to software failures. However, debugging defective software can be costly and time consuming. Moreover, undetected bugs could induce great harm to the public when software systems are applied in safety-critical areas, such as consumer products, public infrastructure, transportation systems, etc. Therefore, it is vital that we remove these bugs as early as possible. To gain more understanding of the nature of these bugs, we review the reported software failures that have impacted the health, safety, and welfare of the public. A focus on lessons learned and implications for future software systems is also provided which acts as guidelines for engineers to improve the quality of their products and avoid similar failures from happening.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Journal of Systems and Software},
	author = {Wong, W. Eric and Li, Xuelin and Laplante, Philip A.},
	month = nov,
	year = {2017},
	keywords = {Accidents, Bugged software systems, Lessons learned, Mishaps, Software failures},
	pages = {68--94},
}

@article{neumann_risks_nodate,
	title = {The {RISKS} {Digest}},
	url = {http://catless.ncl.ac.uk/Risks/},
	abstract = {The web page of the RISKS digest moderated by Peter G. Neumman of SRI},
	language = {en},
	urldate = {2022-10-26},
	journal = {The RISKS Digest},
	author = {Neumann, Peter G.},
}

@inproceedings{li_testing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Testing machine learning systems in industry: an empirical study},
	isbn = {978-1-4503-9226-6},
	shorttitle = {Testing machine learning systems in industry},
	url = {https://dl.acm.org/doi/10.1145/3510457.3513036},
	doi = {10.1145/3510457.3513036},
	language = {en},
	urldate = {2022-10-24},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Li, Shuyue and Guo, Jiaqi and Lou, Jian-Guang and Fan, Ming and Liu, Ting and Zhang, Dongmei},
	month = may,
	year = {2022},
	pages = {263--272},
}

@inproceedings{wang_profactory_2022,
	title = {\{{ProFactory}\}: {Improving} \{{IoT}\} {Security} via {Formalized} {Protocol} {Customization}},
	isbn = {9781939133311},
	shorttitle = {\{{ProFactory}\}},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/wang-fei},
	urldate = {2022-10-21},
	author = {Wang, Fei and Wu, Jianliang and Nan, Yuhong and Aafer, Yousra and Zhang, Xiangyu and Xu, Dongyan and Payer, Mathias},
	year = {2022},
	pages = {3879--3896},
}

@article{noauthor_extraction_nodate,
	title = {Extraction and {Management} of {Rationale}},
	language = {en},
	pages = {3},
}

@article{li_unifuzz_nodate,
	title = {{UNIFUZZ}: {A} {Holistic} and {Pragmatic} {Metrics}-{Driven} {Platform} for {Evaluating} {Fuzzers}},
	abstract = {A ﬂurry of fuzzing tools (fuzzers) have been proposed in the literature, aiming at detecting software vulnerabilities effectively and efﬁciently. To date, it is however still challenging to compare fuzzers due to the inconsistency of the benchmarks, performance metrics, and/or environments for evaluation, which buries the useful insights and thus impedes the discovery of promising fuzzing primitives. In this paper, we design and develop UNIFUZZ, an open-source and metrics-driven platform for assessing fuzzers in a comprehensive and quantitative manner. Speciﬁcally, UNIFUZZ to date has incorporated 35 usable fuzzers, a benchmark of 20 real-world programs, and six categories of performance metrics. We ﬁrst systematically study the usability of existing fuzzers, ﬁnd and ﬁx a number of ﬂaws, and integrate them into UNIFUZZ. Based on the study, we propose a collection of pragmatic performance metrics to evaluate fuzzers from six complementary perspectives. Using UNIFUZZ, we conduct in-depth evaluations of several prominent fuzzers including AFL [1], AFLFast [2], Angora [3], Honggfuzz [4], MOPT [5], QSYM [6], T-Fuzz [7] and VUzzer64 [8]. We ﬁnd that none of them outperforms the others across all the target programs, and that using a single metric to assess the performance of a fuzzer may lead to unilateral conclusions, which demonstrates the signiﬁcance of comprehensive metrics. Moreover, we identify and investigate previously overlooked factors that may signiﬁcantly affect a fuzzer’s performance, including instrumentation methods and crash analysis tools. Our empirical results show that they are critical to the evaluation of a fuzzer. We hope that our ﬁndings can shed light on reliable fuzzing evaluation, so that we can discover promising fuzzing primitives to effectively facilitate fuzzer designs in the future.},
	language = {en},
	author = {Li, Yuwei and Ji, Shouling and Chen, Yuan and Liang, Sizhuang and Lee, Wei-Han and Chen, Yueyao and Lyu, Chenyang and Wu, Chunming and Beyah, Raheem and Cheng, Peng and Lu, Kangjie and Wang, Ting},
	pages = {18},
}

@misc{zhou_unifuzz_2020,
	title = {{UniFuzz}: {Optimizing} {Distributed} {Fuzzing} via {Dynamic} {Centralized} {Task} {Scheduling}},
	shorttitle = {{UniFuzz}},
	url = {http://arxiv.org/abs/2009.06124},
	doi = {10.48550/arXiv.2009.06124},
	abstract = {Fuzzing is one of the most efficient technology for vulnerability detection. Since the fuzzing process is computing-intensive and the performance improved by algorithm optimization is limited, recent research seeks to improve fuzzing performance by utilizing parallel computing. However, parallel fuzzing has to overcome challenges such as task conflicts, scalability in a distributed environment, synchronization overhead, and workload imbalance. In this paper, we design and implement UniFuzz, a distributed fuzzing optimization based on a dynamic centralized task scheduling. UniFuzz evaluates and distributes seeds in a centralized manner to avoid task conflicts. It uses a "request-response" scheme to dynamically distribute fuzzing tasks, which avoids workload imbalance. Besides, UniFuzz can adaptively switch the role of computing cores between evaluating, and fuzzing, which avoids the potential bottleneck of seed evaluation. To improve synchronization efficiency, UniFuzz shares different fuzzing information in a different way according to their characteristics, and the average overhead of synchronization is only about 0.4{\textbackslash}\%. We evaluated UniFuzz with real-world programs, and the results show that UniFuzz outperforms state-of-the-art tools, such as AFL, PAFL and EnFuzz. Most importantly, the experiment reveals a counter-intuitive result that parallel fuzzing can achieve a super-linear acceleration to the single-core fuzzing. We made a detailed explanation and proved it with additional experiments. UniFuzz also discovered 16 real-world vulnerabilities.},
	urldate = {2022-10-20},
	author = {Zhou, Xu and Wang, Pengfei and Liu, Chenyifan and Yue, Tai and Liu, Yingying and Song, Congxi and Lu, Kai and Yin, Qidi},
	month = sep,
	year = {2020},
	note = {arXiv:2009.06124 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{noauthor_notitle_nodate,
}

@inproceedings{bohme_coverage-based_2016,
	address = {New York, NY, USA},
	series = {{CCS} '16},
	title = {Coverage-based {Greybox} {Fuzzing} as {Markov} {Chain}},
	isbn = {9781450341394},
	url = {https://doi.org/10.1145/2976749.2978428},
	doi = {10.1145/2976749.2978428},
	abstract = {Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few "high-frequency" paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule. We implemented the exponential schedule by extending AFL. In 24 hours, AFLFAST exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFAST produces at least an order of magnitude more unique crashes than AFL.},
	urldate = {2022-10-20},
	publisher = {Association for Computing Machinery},
	author = {Böhme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik},
	month = oct,
	year = {2016},
	keywords = {foundations, fuzzing, software security, testing efficiency, vulnerability detection},
	pages = {1032--1043},
}

@inproceedings{davis_impact_2018,
	address = {Lake Buena Vista FL USA},
	title = {The impact of regular expression denial of service ({ReDoS}) in practice: an empirical study at the ecosystem scale},
	isbn = {978-1-4503-5573-5},
	shorttitle = {The impact of regular expression denial of service ({ReDoS}) in practice},
	url = {https://dl.acm.org/doi/10.1145/3236024.3236027},
	doi = {10.1145/3236024.3236027},
	abstract = {Regular expressions (regexes) are a popular and powerful means of automatically manipulating text. Regexes are also an understudied denial of service vector (ReDoS). If a regex has super-linear worst-case complexity, an attacker may be able to trigger this complexity, exhausting the victim’s CPU resources and causing denial of service. Existing research has shown how to detect these superlinear regexes, and practitioners have identified super-linear regex anti-pattern heuristics that may lead to such complexity. In this paper, we empirically study three major aspects of ReDoS that have hitherto been unexplored: the incidence of super-linear regexes in practice, how they can be prevented, and how they can be repaired. In the ecosystems of two of the most popular programming languages Ð JavaScript and Python ś we detected thousands of super-linear regexes affecting over 10,000 modules across diverse application domains. We also found that the conventional wisdom for super-linear regex anti-patterns has few false negatives but many false positives; these anti-patterns appear to be necessary, but not sufficient, signals of super-linear behavior. Finally, we found that when faced with a super-linear regex, developers favor revising it over truncating input or developing a custom parser, regardless of whether they had been shown examples of all three fix strategies. These findings motivate further research into ReDoS, since many modules are vulnerable to it and existing mechanisms to avoid it are insufficient. We believe that ReDoS vulnerabilities are a larger threat in practice than might have been guessed.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Davis, James C. and Coghlan, Christy A. and Servant, Francisco and Lee, Dongyoon},
	month = oct,
	year = {2018},
	pages = {246--256},
}

@inproceedings{zou_tcp-fuzz_2021,
	title = {\{{TCP}-{Fuzz}\}: {Detecting} {Memory} and {Semantic} {Bugs} in \{{TCP}\} {Stacks} with {Fuzzing}},
	isbn = {978-1-939133-23-6},
	shorttitle = {\{{TCP}-{Fuzz}\}},
	url = {https://www.usenix.org/conference/atc21/presentation/zou},
	language = {en},
	urldate = {2022-10-19},
	author = {Zou, Yong-Hao and Bai, Jia-Ju and Zhou, Jielong and Tan, Jianfeng and Qin, Chenggang and Hu, Shi-Min},
	year = {2021},
	pages = {489--502},
}

@inproceedings{abdallah_tasharok_2022,
	title = {{TASHAROK}: {Using} {Mechanism} {Design} for {Enhancing} {Security} {Resource} {Allocation} in {Interdependent} {Systems}},
	shorttitle = {{TASHAROK}},
	doi = {10.1109/SP46214.2022.9833591},
	abstract = {We consider interdependent systems managed by multiple defenders that are under the threat of stepping-stone attacks. We model such systems via game-theoretic models and incorporate the effect of behavioral probability weighting that is used to model biases in human decision-making, as descended from the field of behavioral economics. We then incorporate into our framework called TASHAROK, two types of tax-based mechanisms for such interdependent security games where the central regulator incentivizes defenders to invest well in securing their assets so as to achieve the socially optimal outcome. We first show that due to the nature of our interdependent security game, no reliable tax-based mechanism can incentivize the socially optimal investment profile while maintaining a weakly balanced budget. We then show the effect of behavioral probability weighting bias on the amount of taxes paid by defenders, and prove that higher biases make defenders pay more taxes under the two mechanisms. We then explore voluntary participation in tax-based mechanisms. To evaluate our mechanisms, we use four representative real-world interdependent systems where we compare the game-theoretic optimal investments to the socially optimal investments under the two mechanisms. We show that the mechanisms yield higher decrease in the social cost for behavioral decision-makers compared to rational decision-makers.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Abdallah, Mustafa and Woods, Daniel and Naghizadeh, Parinaz and Khalil, Issa and Cason, Timothy and Sundaram, Shreyas and Bagchi, Saurabh},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	keywords = {Attack graphs., Behavioral decision-making, Behavioral sciences, Costs, Decision making, Finance, Games, Interdependent systems, Mechanism design, Regulators, Security, Security games},
	pages = {249--266},
}

@inproceedings{addison_controlling_2002,
	address = {ZAF},
	series = {{SAICSIT} '02},
	title = {Controlling software project risks: an empirical study of methods used by experienced project managers},
	isbn = {978-1-58113-596-1},
	shorttitle = {Controlling software project risks},
	abstract = {The failure rate of software projects has been proven to be very high, and the incidence of failure is becoming worse as more companies venture into software development. Risk management is a collection of methods aimed at minimising or reducing the effects of project failure. This research report has focused on experienced project manager's perceptions of software project risks and controls. It reports on the more significant risks and controls that are utilised to reduce the occurrence of the risk factors, or minimise the impact of various risks. Risk factors involved in software projects along with controls to mitigate these factors were identified in the literature. These were then used in an empirical study to determine their importance and frequency of occurrence. The effectiveness of various controls to reduce the occurrence of risk factors was also identified and discussed. Experienced project managers were found to use certain controls more than inexperienced project managers, particularly 'assign responsibilities to team members' and 'stabilise requirements and specifications'.},
	urldate = {2022-06-17},
	booktitle = {Proceedings of the 2002 annual research conference of the {South} {African} institute of computer scientists and information technologists on {Enablement} through technology},
	publisher = {South African Institute for Computer Scientists and Information Technologists},
	author = {Addison, Tom and Vallabh, Seema},
	month = sep,
	year = {2002},
	keywords = {control methods, management, risks},
	pages = {128--140},
}

@inproceedings{cao_what_2022,
	title = {What the {Fork}? {Finding} and {Analyzing} {Malware} in {GitHub} {Forks}},
	shorttitle = {What the {Fork}?},
	url = {https://www.ndss-symposium.org/ndss-paper/auto-draft-275/},
	language = {en-US},
	urldate = {2022-10-17},
	author = {Cao, Alan and Dolan-Gavitt, Brendan},
	month = apr,
	year = {2022},
}

@inproceedings{trabelsi_abusing_2013,
	title = {Abusing social networks with abuse reports: {A} coalition attack for social networks},
	shorttitle = {Abusing social networks with abuse reports},
	abstract = {In Social Network websites, the users can report the bad behaviors of other users. In order to do so, they can create a kind of escalation ticket called abuse report in which they detail the infraction made by the “bad” user and help the website moderator to decide on a penalty. Today Social Networks count billions of users, the handling of the abuse reports is no more executed manually by moderators; they currently rely on some algorithms that automatically block the “bad” users until a moderator takes care of the case. In this paper we purport to demonstrate how such algorithms are maliciously used by attackers to illegally block innocent victims. We also propose to automate such an attack to demonstrate the big damage that can be caused in current social network websites. We also took the case study of Facebook as proof of concept.},
	booktitle = {2013 {International} {Conference} on {Security} and {Cryptography} ({SECRYPT})},
	author = {Trabelsi, Slim and Bouafif, Hana},
	month = jul,
	year = {2013},
	keywords = {Abuse Report, Attack, Browsers, Coalition, Computer crime, DoS, Facebook, Organizations, Servers, Social Networks, Tin},
	pages = {1--6},
}

@article{schelter_automatically_nodate,
	title = {Automatically {Tracking} {Metadata} and {Provenance} of {Machine} {Learning} {Experiments}},
	abstract = {We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workﬂow, and provides a basis for comparability and repeatability of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.},
	language = {en},
	author = {Schelter, Sebastian and Böse, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
	pages = {8},
}

@article{yang_complex_2022,
	title = {Complex {Python} {Features} in the {Wild}},
	abstract = {While Python is increasingly popular, program analysis tooling for Python is lagging. This is due, in part, to complex features of the Python language—features with difficult to understand and model semantics. Besides the “usual suspects”, reflection and dynamic execution, complex Python features include context managers, decorators, and generators, among others. This paper explores how often and in what ways developers use certain complex features. We analyze over 3 million Python files mined from GitHub to address three research questions: (i) How often do developers use certain complex Python features? (ii) In what ways do developers use these features? (iii) Does use of complex features increase or decrease over time? Our findings show that usage of dynamic features that pose a threat to static analysis is infrequent. On the other hand, usage of context managers and decorators is surprisingly widespread. Our actionable result is a list of Python features that any “minimal syntax” ought to handle in order to capture developers’ use of the Python language. We hope that understanding the usage of Python features will help tool-builders improve Python tools, which can in turn lead to more correct, secure, and performant Python code.},
	language = {en},
	author = {Yang, Yi and Milanova, Ana and Hirzel, Martin},
	year = {2022},
	pages = {12},
}

@misc{baudart_mining_2020,
	title = {Mining {Documentation} to {Extract} {Hyperparameter} {Schemas}},
	url = {http://arxiv.org/abs/2006.16984},
	abstract = {AI automation tools need machine-readable hyperparameter schemas to define their search spaces. At the same time, AI libraries often come with good human-readable documentation. While such documentation contains most of the necessary information, it is unfortunately not ready to consume by tools. This paper describes how to automatically mine Python docstrings in AI libraries to extract JSON Schemas for their hyperparameters. We evaluate our approach on 119 transformers and estimators from three different libraries and find that it is effective at extracting machine-readable schemas. Our vision is to reduce the burden to manually create and maintain such schemas for AI automation tools and broaden the reach of automation to larger libraries and richer schemas.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Baudart, Guillaume and Kirchner, Peter D. and Hirzel, Martin and Kate, Kiran},
	month = jul,
	year = {2020},
	note = {arXiv:2006.16984 [cs, stat]},
	keywords = {Computer Science - Databases, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{shi_evaluation_2022,
	title = {On the {Evaluation} of {Neural} {Code} {Summarization}},
	url = {http://arxiv.org/abs/2107.07112},
	doi = {10.1145/3510003.3510060},
	abstract = {Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from -18{\textbackslash}\% to +25{\textbackslash}\%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Chen, Junjie and Han, Shi and Zhang, Hongyu and Zhang, Dongmei and Sun, Hongbin},
	month = may,
	year = {2022},
	note = {arXiv:2107.07112 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	pages = {1597--1608},
}

@inproceedings{shi_evaluation_2022-1,
	title = {On the {Evaluation} of {Neural} {Code} {Summarization}},
	url = {http://arxiv.org/abs/2107.07112},
	doi = {10.1145/3510003.3510060},
	abstract = {Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from -18{\textbackslash}\% to +25{\textbackslash}\%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Chen, Junjie and Han, Shi and Zhang, Hongyu and Zhang, Dongmei and Sun, Hongbin},
	month = may,
	year = {2022},
	note = {arXiv:2107.07112 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	pages = {1597--1608},
}

@inproceedings{baudart_pipeline_2021,
	title = {Pipeline {Combinators} for {Gradual} {AutoML}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/a3b36cb25e2e0b93b5f334ffb4e4064e-Abstract.html},
	abstract = {Automated machine learning (AutoML) can make data scientists more productive.  But if machine learning is totally automated, that leaves no room for data scientists to apply their intuition.  Hence, data scientists often prefer not total but gradual automation, where they control certain choices and AutoML explores the rest.  Unfortunately, gradual AutoML is cumbersome with state-of-the-art tools, requiring large non-compositional code changes.  More concise compositional code can be achieved with combinators, a powerful concept from functional programming.  This paper introduces a small set of orthogonal combinators for composing machine-learning operators into pipelines.  It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers.  On that foundation, this paper presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study.},
	urldate = {2022-10-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Baudart, Guillaume and Hirzel, Martin and Kate, Kiran and Ram, Parikshit and Shinnar, Avi and Tsay, Jason},
	year = {2021},
	pages = {19705--19718},
}

@misc{zhu_automatic_2019,
	title = {Automatic {Code} {Summarization}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Automatic {Code} {Summarization}},
	url = {http://arxiv.org/abs/1909.04352},
	abstract = {Background: During software maintenance and development, the comprehension of program code is key to success. High-quality comments can help us better understand programs, but they're often missing or outmoded in today's programs. Automatic code summarization is proposed to solve these problems. During the last decade, huge progress has been made in this field, but there is a lack of an up-to-date survey. Aims: We studied publications concerning code summarization in the field of program comprehension to investigate state-of-the-art approaches. By reading and analyzing relevant articles, we aim at obtaining a comprehensive understanding of the current status of automatic code summarization. Method: In this paper, we performed a systematic literature review over the automatic source code summarization field. Furthermore, we synthesized the obtained data and investigated different approaches. Results: We successfully collected and analyzed 41 selected studies from the different research communities. We exhaustively investigated and described the data extraction techniques, description generation methods, evaluation methods and relevant artifacts of those works. Conclusions: Our systematic review provides an overview of the state of the art, and we also discuss further research directions. By fully elaborating current approaches in the field, our work sheds light on future research directions of program comprehension and comment generation.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Zhu, Yuxiang and Pan, Minxue},
	month = oct,
	year = {2019},
	note = {arXiv:1909.04352 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{zhu_automatic_2019-1,
	title = {Automatic {Code} {Summarization}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Automatic {Code} {Summarization}},
	url = {http://arxiv.org/abs/1909.04352},
	abstract = {Background: During software maintenance and development, the comprehension of program code is key to success. High-quality comments can help us better understand programs, but they're often missing or outmoded in today's programs. Automatic code summarization is proposed to solve these problems. During the last decade, huge progress has been made in this field, but there is a lack of an up-to-date survey. Aims: We studied publications concerning code summarization in the field of program comprehension to investigate state-of-the-art approaches. By reading and analyzing relevant articles, we aim at obtaining a comprehensive understanding of the current status of automatic code summarization. Method: In this paper, we performed a systematic literature review over the automatic source code summarization field. Furthermore, we synthesized the obtained data and investigated different approaches. Results: We successfully collected and analyzed 41 selected studies from the different research communities. We exhaustively investigated and described the data extraction techniques, description generation methods, evaluation methods and relevant artifacts of those works. Conclusions: Our systematic review provides an overview of the state of the art, and we also discuss further research directions. By fully elaborating current approaches in the field, our work sheds light on future research directions of program comprehension and comment generation.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Zhu, Yuxiang and Pan, Minxue},
	month = oct,
	year = {2019},
	note = {arXiv:1909.04352 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{rak-amnouykit_raise_2022,
	address = {Virtual South Korea},
	title = {The raise of machine learning hyperparameter constraints in {Python} code},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534400},
	doi = {10.1145/3533767.3534400},
	abstract = {Machine-learning operators often have correctness constraints that cut across multiple hyperparameters and/or data. Violating these constraints causes the operator to raise runtime exceptions, but those are usually documented only informally or not at all. This paper presents the first interprocedural weakest-precondition analysis for Python to extract hyperparameter constraints. The analysis is mostly static, but to make it tractable for typical Python idioms in machine-learning libraries, it selectively switches to the concrete domain for some cases. This paper demonstrates the analysis by extracting hyperparameter constraints for 181 operators from a total of 8 ML libraries, where it achieved high precision and recall and found real bugs. Our technique advances static analysis for Python and is a step towards safer and more robust machine learning.},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Rak-amnouykit, Ingkarat and Milanova, Ana and Baudart, Guillaume and Hirzel, Martin and Dolby, Julian},
	month = jul,
	year = {2022},
	pages = {580--592},
}

@inproceedings{rak-amnouykit_extracting_2021,
	address = {Virtual, United States},
	title = {Extracting {Hyperparameter} {Constraints} from {Code}},
	url = {https://hal.archives-ouvertes.fr/hal-03401683},
	abstract = {Machine-learning operators often have correctness constraints that cut across multiple hyperparameters and/or data. Violating these constraints causes runtime exceptions, but they are usually documented only informally or not at all. This paper presents a weakest precondition analysis for Python code. We demonstrate our analysis by extracting hyperparameter constraints for 45 sklearn operators. Our analysis is a step towards safer and more robust machine learning.},
	urldate = {2022-10-13},
	booktitle = {{ICLR} {Workshop} on {Security} and {Safety} in {Machine} {Learning} {Systems}},
	author = {Rak-Amnouykit, Ingkarat and Milanova, Ana and Baudart, Guillaume and Hirzel, Martin and Dolby, Julian},
	month = may,
	year = {2021},
}

@inproceedings{niesler_hera_2021,
	address = {Virtual},
	title = {{HERA}: {Hotpatching} of {Embedded} {Real}-time {Applications}},
	isbn = {978-1-891562-66-2},
	shorttitle = {{HERA}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6B-2_24159_paper.pdf},
	doi = {10.14722/ndss.2021.24159},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Proceedings 2021 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Niesler, Christian and Surminski, Sebastian and Davi, Lucas},
	year = {2021},
}

@article{noauthor_so_nodate,
	title = {So {Many} {Fuzzers}, {So} {Little} {Time}},
	language = {en},
	pages = {11},
}

@article{jero_automated_nodate,
	title = {Automated {Attack} {Discovery} in {TCP} {Congestion} {Control} {Using} a {Model}-guided {Approach}},
	abstract = {One of the most important goals of TCP is to ensure fairness and prevent congestion collapse by implementing congestion control. Various attacks against TCP congestion control have been reported over the years, most of which have been discovered through manual analysis. In this paper, we propose an automated method that combines the generality of implementation-agnostic fuzzing with the precision of runtime analysis to ﬁnd attacks against implementations of TCP congestion control. It uses a model-guided approach to generate abstract attack strategies, by leveraging a state machine model of TCP congestion control to ﬁnd vulnerable state machine paths that an attacker could exploit to increase or decrease the throughput of a connection to his advantage. These abstract strategies are then mapped to concrete attack strategies, which consist of sequences of actions such as injection or modiﬁcation of acknowledgements and a logical time for injection. We design and implement a virtualized platform, TCPWN, that consists of a a proxy-based attack injector and a TCP congestion control state tracker that uses only network trafﬁc to create and inject these concrete attack strategies. We evaluated 5 TCP implementations from 4 Linux distributions and Windows 8.1. Overall, we found 11 classes of attacks, of which 8 are new.},
	language = {en},
	author = {Jero, Samuel and Hoque, Endadul and Choffnes, David and Mislove, Alan and Nita-Rotaru, Cristina},
	pages = {15},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/},
	urldate = {2022-10-07},
}

@inproceedings{corteggiani_inception_2018-1,
	title = {Inception: \{{System}-{Wide}\} {Security} {Testing} of \{{Real}-{World}\} {Embedded} {Systems} {Software}},
	isbn = {978-1-939133-04-5},
	shorttitle = {Inception},
	url = {https://www.usenix.org/conference/usenixsecurity18/presentation/corteggiani},
	language = {en},
	urldate = {2022-10-07},
	author = {Corteggiani, Nassim and Camurati, Giovanni and Francillon, Aurélien},
	year = {2018},
	pages = {309--326},
}

@misc{noauthor_firehydrant_nodate,
	title = {{FireHydrant}: {Incident} management for every developer},
	url = {https://firehydrant.com/},
	abstract = {FireHydrant helps teams more easily maintain service catalogs, respond to incidents, communicate through status pages, and learn with retrospectives.},
	language = {en},
	urldate = {2022-10-06},
	journal = {Incident management for every developer},
}

@inproceedings{michael_regexes_2019,
	title = {Regexes are {Hard}: {Decision}-{Making}, {Difficulties}, and {Risks} in {Programming} {Regular} {Expressions}},
	shorttitle = {Regexes are {Hard}},
	doi = {10.1109/ASE.2019.00047},
	abstract = {Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Michael, Louis G. and Donohue, James and Davis, James C. and Lee, Dongyoon and Servant, Francisco},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Decision making, Face, Interviews, Programming profession, Software, Tools, regular expressions, developer process, qualitative research},
	pages = {415--426},
}

@inproceedings{kapur_towards_2019,
	address = {Montreal, QC, Canada},
	title = {Towards a {Knowledge} {Warehouse} and {Expert} {System} for the {Automation} of {SDLC} {Tasks}},
	isbn = {978-1-72813-393-5},
	url = {https://ieeexplore.ieee.org/document/8812829/},
	doi = {10.1109/ICSSP.2019.00011},
	abstract = {Cost of a skilled and competent software developer is high, and it is desirable to minimize dependency on such costly human resources. One of the ways to minimize such costs is via automation of various software development tasks.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Software} and {System} {Processes} ({ICSSP})},
	publisher = {IEEE},
	author = {Kapur, Ritu and Sodhi, Balwinder},
	month = may,
	year = {2019},
	pages = {5--8},
}

@article{boenisch_systematic_2021,
	title = {A {Systematic} {Review} on {Model} {Watermarking} for {Neural} {Networks}},
	volume = {4},
	issn = {2624-909X},
	url = {http://arxiv.org/abs/2009.12153},
	doi = {10.3389/fdata.2021.729663},
	abstract = {Machine learning (ML) models are applied in an increasing variety of domains. The availability of large amounts of data and computational resources encourages the development of ever more complex and valuable models. These models are considered intellectual property of the legitimate parties who have trained them, which makes their protection against stealing, illegitimate redistribution, and unauthorized application an urgent need. Digital watermarking presents a strong mechanism for marking model ownership and, thereby, offers protection against those threats. This work presents a taxonomy identifying and analyzing different classes of watermarking schemes for ML models. It introduces a unified threat model to allow structured reasoning on and comparison of the effectiveness of watermarking methods in different scenarios. Furthermore, it systematizes desired security requirements and attacks against ML model watermarking. Based on that framework, representative literature from the field is surveyed to illustrate the taxonomy. Finally, shortcomings and general limitations of existing approaches are discussed, and an outlook on future research directions is given.},
	urldate = {2022-09-27},
	journal = {Frontiers in Big Data},
	author = {Boenisch, Franziska},
	month = nov,
	year = {2021},
	note = {arXiv:2009.12153 [cs]},
	keywords = {A.1, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Multimedia, I.2},
	pages = {729663},
}

@article{sawant_learning-based_nodate,
	title = {Learning-based {Identiﬁcation} of {Coding} {Best} {Practices} from {Software} {Documentation}},
	abstract = {Automatic identiﬁcation of coding best practices can scale the development of code and application analyzers. We present Doc2BP, a deep learning tool to identify coding best practices in software documentation. Natural language descriptions are mapped to an informative embedding space, optimized under the dual objectives of binary and few shot classiﬁcation. The binary objective powers general classiﬁcation into known best practice categories using a deep learning classiﬁer. The few shot objective facilitates example-based classiﬁcation into novel categories by matching embeddings with user-provided examples at run-time, without having to retrain the underlying model. We analyze the effects of manually and synthetically labeled examples, context, and cross-domain information.},
	language = {en},
	author = {Sawant, Neela and Sengamedu, Srinivasan H},
	pages = {10},
}

@misc{dellavecchia_how_2022,
	title = {How a {Rogue} {Developer} {Ruined} {Millions} of {Software} (happened this weekend)},
	url = {https://medium.com/@anthonyjdella/how-a-rogue-developer-ruined-millions-of-software-happened-this-weekend-8602af1f8e07},
	abstract = {TLDR: A software developer who made some highly used open-source software, decided to go rogue and inject a bug into his software, making…},
	language = {en},
	urldate = {2022-05-08},
	journal = {Medium},
	author = {Dellavecchia, Anthony},
	month = jan,
	year = {2022},
}

@inproceedings{Zimmermann2019SecurityThreatsinNPMEcosystem,
	title = {Small {World} with {High} {Risks}: {A} {Study} of {Security} {Threats} in the npm {Ecosystem}},
	abstract = {The popularity of JavaScript has lead to a large ecosystem of third-party packages available via the npm software package registry. The open nature of npm has boosted its growth, providing over 800,000 free and reusable software packages. Unfortunately, this open nature also causes security risks, as evidenced by recent incidents of single packages that broke or attacked software running on millions of computers. This paper studies security risks for users of npm by systematically analyzing dependencies between packages, the maintainers responsible for these packages, and publicly reported security issues. Studying the potential for running vulnerable or malicious code due to third-party dependencies, we ﬁnd that individual packages could impact large parts of the entire ecosystem. Moreover, a very small number of maintainer accounts could be used to inject malicious code into the majority of all packages, a problem that has been increasing over time. Studying the potential for accidentally using vulnerable code, we ﬁnd that lack of maintenance causes many packages to depend on vulnerable code, even years after a vulnerability has become public. Our results provide evidence that npm suffers from single points of failure and that unmaintained packages threaten large code bases. We discuss several mitigation techniques, such as trusted maintainers and total ﬁrst-party security, and analyze their potential effectiveness.},
	booktitle = {{USENIX} {Security} {Symposium}},
	author = {Zimmermann, Markus and Staicu, Cristian-Alexandru and Pradel, Michael},
	year = {2019},
}

@article{wang_backdoor_2022,
	title = {Backdoor {Attacks} {Against} {Transfer} {Learning} {With} {Pre}-{Trained} {Deep} {Learning} {Models}},
	volume = {15},
	issn = {1939-1374},
	doi = {10.1109/TSC.2020.3000900},
	abstract = {Transfer learning provides an effective solution for feasibly and fast customize accurate Student models, by transferring the learned knowledge of pre-trained Teacher models over large datasets via fine-tuning. Many pre-trained Teacher models used in transfer learning are publicly available and maintained by public platforms, increasing their vulnerability to backdoor attacks. In this article, we demonstrate a backdoor threat to transfer learning tasks on both image and time-series data leveraging the knowledge of publicly accessible Teacher models, aimed at defeating three commonly adopted defenses: pruning-based, retraining-based and input pre-processing-based defenses. Specifically, (\${\textbackslash}mathcal A\$A) ranking-based selection mechanism to speed up the backdoor trigger generation and perturbation process while defeating pruning-based and/or retraining-based defenses. (\${\textbackslash}mathcal B\$B) autoencoder-powered trigger generation is proposed to produce a robust trigger that can defeat the input pre-processing-based defense, while guaranteeing that selected neuron(s) can be significantly activated. (\${\textbackslash}mathcal C\$C) defense-aware retraining to generate the manipulated model using reverse-engineered model inputs. We launch effective misclassification attacks on Student models over real-world images, brain Magnetic Resonance Imaging (MRI) data and Electrocardiography (ECG) learning systems. The experiments reveal that our enhanced attack can maintain the 98.4 and 97.2 percent classification accuracy as the genuine model on clean image and time series inputs while improving \$27.9\%-100\%\$27.9\%-100\% and \$27.1\%-56.1\%\$27.1\%-56.1\% attack success rate on trojaned image and time series inputs respectively in the presence of pruning-based and/or retraining-based defenses.},
	number = {3},
	journal = {IEEE Transactions on Services Computing},
	author = {Wang, Shuo and Nepal, Surya and Rudolph, Carsten and Grobler, Marthie and Chen, Shangyu and Chen, Tianle},
	year = {2022},
	keywords = {Biological system modeling, Data models, Electrocardiography, Learning systems, Task analysis, Training, Web service, backdoor attack, deep neural network, pre-trained model, transfer learning},
	pages = {1526--1539},
}

@inproceedings{Liu2022LoneNeuron,
	address = {Los Angeles},
	title = {{LoneNeuron}: a {Highly}-{Effective} {Feature}-{Domain} {Neural} {Trojan} {Using} {Invisible} and {Polymorphic} {Watermarks}},
	abstract = {The wide adoption of deep neural networks (DNNs) in real-world applications raises increasing security concerns. Neural Trojans embedded in pre-trained neural networks are a harmful attack against the DNN model supply chain. They generate false outputs when certain stealthy triggers appear in the inputs. While data-poisoning attacks have been well studied in the literature, code-poisoning and model-poisoning backdoors only start to attract attention until recently. We present a novel model-poisoning neural Trojan, namely LoneNeuron, which responds to feature-domain patterns that transform into invisible, sample-specific, and polymorphic pixel-domain watermarks. With high attack specificity, LoneNeuron achieves a 100\% attack success rate, while not affecting the main task performance. With LoneNeuron’s unique watermark polymorphism property, the same feature-domain trigger is resolved to multiple watermarks in the pixel domain, which further improves watermark randomness, stealthiness, and resistance against Trojan detection. Extensive experiments show that LoneNeuron could escape stateof-the-art Trojan detectors. LoneNeuron is also the first effective backdoor attack against vision transformers (ViTs).},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Liu, Zeyan and Li, Fengjun and Li, Zhu and Luo, Bo},
	year = {2022},
}

@inproceedings{Decan2018SecurityVulnerabilitiesinNPMDependencyNetwork,
	title = {On the impact of security vulnerabilities in the npm package dependency network},
	abstract = {Security vulnerabilities are among the most pressing problems in open source software package libraries. It may take a long time to discover and fix vulnerabilities in packages. In addition, vulnerabilities may propagate to dependent packages, making them vulnerable too. This paper presents an empirical study of nearly 400 security reports over a 6-year period in the npm dependency network containing over 610k JavaScript packages. Taking into account the severity of vulnerabilities, we analyse how and when these vulnerabilities are discovered and fixed, and to which extent they affect other packages in the packaging ecosystem in presence of dependency constraints. We report our findings and provide guidelines for package maintainers and tool developers to improve the process of dealing with security issues.},
	booktitle = {International {Conf.} on {Mining} {SW} {Repositories}},
	author = {Decan, Alexandre and Mens, Tom and Constantinou, Eleni},
	year = {2018},
}

@misc{Chakraborty2018AdversarialAtatcksanddefences,
	title = {Adversarial {Attacks} and {Defences}: {A} {Survey}},
	url = {https://arxiv.org/abs/1810.00069},
	abstract = {Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.},
	publisher = {arXiv},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_multi-modal_2020,
	address = {London UK},
	title = {Multi-modal synthesis of regular expressions},
	isbn = {978-1-4503-7613-6},
	url = {https://dl.acm.org/doi/10.1145/3385412.3385988},
	doi = {10.1145/3385412.3385988},
	abstract = {In this paper, we propose a multi-modal synthesis technique for automatically constructing regular expressions (regexes) from a combination of examples and natural language. Using multiple modalities is useful in this context because natural language alone is often highly ambiguous, whereas examples in isolation are often not sufficient for conveying user intent. Our proposed technique first parses the English description into a so-called hierarchical sketch that guides our programming-by-example (PBE) engine. Since the hierarchical sketch captures crucial hints, the PBE engine can leverage this information to both prioritize the search as well as make useful deductions for pruning the search space.},
	language = {en},
	urldate = {2022-09-21},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Chen, Qiaochu and Wang, Xinyu and Ye, Xi and Durrett, Greg and Dillig, Isil},
	month = jun,
	year = {2020},
	pages = {487--502},
}

@inproceedings{Huang2011AdversarialML,
	title = {Adversarial {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/2046684.2046692},
	doi = {10.1145/2046684.2046692},
	abstract = {In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging ﬁeld of study: adversarial machine learning—the study of eﬀective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-speciﬁc factors that limit an adversary’s capabilities; introduce two models for modeling an adversary’s capabilities; explore the limits of an adversary’s knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.},
	booktitle = {{ACM} workshop on {Security} and {Artificial} {Intelligence}},
	publisher = {IEEE},
	author = {Huang, Ling and Joseph, Anthony D and Nelson, Blaine and Rubinstein, Benjamin I P and Tygar, J D},
	year = {2011},
	pages = {43--58},
}

@article{han_pre-trained_2021,
	title = {Pre-trained models: {Past}, present and future},
	abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
	journal = {AI Open},
	author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
	year = {2021},
}

@inproceedings{Sanyal2022ModelStealing,
	title = {Towards {Data}-{Free} {Model} {Stealing} in a {Hard} {Label} {Setting}},
	abstract = {Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework1 that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim’s gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-ofthe-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Sanyal, Sunandini and Addepalli, Sravanti and Babu, R Venkatesh},
	year = {2022},
	pages = {15284--15293},
}

@article{Saha2020BackdoorAttacks,
	title = {Hidden {Trigger} {Backdoor} {Attacks}},
	volume = {34},
	doi = {10.1609/aaai.v34i07.6871},
	abstract = {With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a speciﬁc small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classiﬁcation settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},
	year = {2020},
	pages = {11957--11965},
}

@inproceedings{Ohm2020ReviewofOpenSourceSSCAttacks,
	title = {Backstabber’s {Knife} {Collection}: {A} {Review} of {Open} {Source} {Software} {Supply} {Chain} {Attacks}},
	abstract = {A software supply chain attack is characterized by the injection of malicious code into a software package in order to compromise dependent systems further down the chain. Recent years saw a number of supply chain attacks that leverage the increasing use of open source during software development, which is facilitated by dependency managers that automatically resolve, download and install hundreds of open source packages throughout the software life cycle. Even though many approaches for detection and discovery of vulnerable packages exist, no prior work has focused on malicious packages. This paper presents a dataset as well as analysis of 174 malicious software packages that were used in real-world attacks on open source software supply chains and which were distributed via the popular package repositories npm, PyPI, and RubyGems. Those packages, dating from November 2015 to November 2019, were manually collected and analyzed. This work is meant to facilitate the future development of preventive and detective safeguards by open source and research communities.},
	booktitle = {Detection of {Intrusions} and {Malware}, and {Vulnerability} {Assessment}},
	publisher = {Springer},
	author = {Ohm, Marc and Plate, Henrik and Sykosch, Arnold and Meier, Michael},
	editor = {Maurice, Clémentine and Bilge, Leyla and Stringhini, Gianluca and Neves, Nuno},
	year = {2020},
	keywords = {Application security, Malware, Software supply chain, Taxonomy, attacks},
}

@inproceedings{Du2013SSCRiskManagement,
	title = {Towards {An} {Analysis} of {Software} {Supply} {Chain} {Risk} {Management}},
	volume = {1},
	abstract = {Nowadays, software supply chain participants have become international distributors, which make software supply chain more and more complex. This complexity makes manager understand, acquire, monitor and manage software supply chain products and processes more difficult than ever, and then relevant security problems happen, such as software with security holes. But most of security problems are different from other supply chains. Therefore, based on system’s perspective and current several analysis methods of software supply chain, the paper analyzes and summarizes software supply chain risks, software supply chain risk management methods, and puts forward some basic risk management practices to protect software supply chain’s security. Finally, the paper discusses the future research direction to software supply chain.},
	booktitle = {Proceedings of the {World} {Congress} on {Engineering} and {Computer} {Science}},
	author = {Du, Shixian and Lu, Tianbo and Zhao, Lingling and Xu, Bing and Guo, Xiaobo and Yang, Hongyu},
	year = {2013},
}

@article{Akhtar2018AdversarialAttacksonDLinCV,
	title = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	volume = {6},
	abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	journal = {IEEE Access},
	author = {Akhtar, Naveed and Mian, Ajmal},
	year = {2018},
	keywords = {Computational modeling, Computer vision, Deep learning, Machine learning, Neural networks, Perturbation methods, Predictive models, Task analysis, adversarial learning, adversarial perturbation, black-box attack, perturbation detection, white-box attack},
	pages = {14410--14430},
}

@inproceedings{xu_fuzzing_2019,
	title = {Fuzzing {File} {Systems} via {Two}-{Dimensional} {Input} {Space} {Exploration}},
	doi = {10.1109/SP.2019.00035},
	abstract = {File systems, a basic building block of an OS, are too big and too complex to be bug free. Nevertheless, file systems rely on regular stress-testing tools and formal checkers to find bugs, which are limited due to the ever-increasing complexity of both file systems and OSes. Thus, fuzzing, proven to be an effective and a practical approach, becomes a preferable choice, as it does not need much knowledge about a target. However, three main challenges exist in fuzzing file systems: mutating a large image blob that degrades overall performance, generating image-dependent file operations, and reproducing found bugs, which is difficult for existing OS fuzzers. Hence, we present JANUS, the first feedback-driven fuzzer that explores the two-dimensional input space of a file system, i.e., mutating metadata on a large image, while emitting image-directed file operations. In addition, JANUS relies on a library OS rather than on traditional VMs for fuzzing, which enables JANUS to load a fresh copy of the OS, thereby leading to better reproducibility of bugs. We evaluate JANUS on eight file systems and found 90 bugs in the upstream Linux kernel, 62 of which have been acknowledged. Forty-three bugs have been fixed with 32 CVEs assigned. In addition, JANUS achieves higher code coverage on all the file systems after fuzzing 12 hours, when compared with the state-of-the-art fuzzer Syzkaller for fuzzing file systems. JANUS visits 4.19x and 2.01x more code paths in Btrfs and ext4, respectively. Moreover, JANUS is able to reproduce 88-100\% of the crashes, while Syzkaller fails on all of them.},
	booktitle = {2019 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Xu, Wen and Moon, Hyungon and Kashyap, Sanidhya and Tseng, Po-Ning and Kim, Taesoo},
	month = may,
	year = {2019},
	note = {ISSN: 2375-1207},
	keywords = {Computer bugs, File systems, Filesystem, Fuzzing, Kernel, Libraries, Linux, Metadata},
	pages = {818--834},
}

@article{humphrey_characterizing_1988,
	title = {Characterizing the software process: a maturity framework},
	volume = {5},
	issn = {1937-4194},
	shorttitle = {Characterizing the software process},
	doi = {10.1109/52.2014},
	abstract = {A description is given of a software-process maturity framework that has been developed to provide the US Department of Defense with a means to characterize the capabilities of software-development organizations. This software-development process-maturity model reasonably represents the actual ways in which software-development organizations improve. It provides a framework for assessing these organizations and identifying the priority areas for immediate improvement. It also helps identify those places where advanced technology can be most valuable in improving the software-development process. The framework can be used by any software organization to assess its own capabilities and identify the most important areas for improvement.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Software},
	author = {Humphrey, W.S.},
	month = mar,
	year = {1988},
	note = {Conference Name: IEEE Software},
	keywords = {Computer industry, Costs, Industrial control, Job shop scheduling, Metals industry, Process control, Software engineering, Software maintenance, Software measurement, Software quality},
	pages = {73--79},
}

@inproceedings{mayer_model_2009,
	title = {A model to assess the maturity level of the {Risk} {Management} process in information security},
	doi = {10.1109/INMW.2009.5195935},
	abstract = {The Risk Management (RM) process comprises coordinated activities aimed at guiding and controlling an organization as far as risks are concerned. These activities encompass the definition of the context of analysis, assessment, treatment, acceptance, as well as the communication and the monitoring of information security risks. Organizations should implement RM in a consistent, systematic manner in order to achieve compliance with current laws, standards and regulations, and also meet mandatory requirements for the certification of an Information Security Management System. However, in the context of information security, no reference was found in literature for a model to assess the maturity level of an RM process. In order to overcome this problem, this study describes the structure of a model for the assessment of the maturity level of the RM process in the realm of Information Security. The designed model basically consists of a set of best practices, totally aligned with standard ISO/IEC 27005 and comprised of: (1) three stages; (2) five maturity levels; (3) forty-three control objectives; (4) one control map; (5) one assessment instrument relative to the maturity level of the activities of the RM process; (6) an accountability matrix relative to each activity of the process and also a (7) risk scorecard.},
	booktitle = {2009 {IFIP}/{IEEE} {International} {Symposium} on {Integrated} {Network} {Management}-{Workshops}},
	author = {Mayer, Janice and Lemes Fagundes, Leonardo},
	month = jun,
	year = {2009},
	keywords = {Communication system control, Context, IEC standards, ISO standards, Information Security, Information analysis, Information security, Maturity Model, Monitoring, Risk Management, Risk analysis, Risk management, Standards organizations},
	pages = {61--70},
}

@article{Wang2022EvilModel2,
	title = {{EvilModel} 2.0: {Bringing} {Neural} {Network} {Models} into {Malware} {Attacks}},
	doi = {10.1016/j.cose.2022.102807},
	journal = {Computers \& Security},
	author = {Wang, Zhi and Liu, Chaoge and Cui, Xiang and Yin, Jie and Wang, Xutong},
	year = {2022},
}

@inproceedings{chapman_exploring_2016,
	address = {Saarbrücken Germany},
	title = {Exploring regular expression usage and context in {Python}},
	isbn = {978-1-4503-4390-9},
	url = {https://dl.acm.org/doi/10.1145/2931037.2931073},
	doi = {10.1145/2931037.2931073},
	abstract = {Due to the popularity and pervasive use of regular expressions, researchers have created tools to support their creation, validation, and use. However, little is known about the context in which regular expressions are used, the features that are most common, and how behaviorally similar regular expressions are to one another.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 25th {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Chapman, Carl and Stolee, Kathryn T.},
	month = jul,
	year = {2016},
	pages = {282--293},
}

@techreport{chaganti_stegomalware_2021,
	type = {preprint},
	title = {Stegomalware: {A} {Systematic} {Survey} of {Malware} {Hiding} and {Detection} in {Images}, {Machine} {Learning} {Models} and {Research} {Challenges}},
	shorttitle = {Stegomalware},
	url = {https://www.techrxiv.org/articles/preprint/Stegomalware_A_Systematic_Survey_of_Malware_Hiding_and_Detection_in_Images_Machine_Learning_Models_and_Research_Challenges/16755457/1},
	abstract = {Malware distribution to the victim network is commonly performed through ﬁle attachments in phishing email or downloading illegitimate ﬁles from the internet, when the victim interacts with the source of infection. To detect and prevent the malware distribution in the victim machine, the existing end device security applications may leverage sophisticated techniques such as signature-based or anomaly-based, machine learning techniques. The well-known ﬁle formats Portable Executable (PE) for Windows and Executable and Linkable Format (ELF) for Linux based operating system are used for malware analysis and the malware detection capabilities of these ﬁles has been well advanced for real time detection. But the malware payload hiding in multimedia like cover images using steganography detection has been a challenge for enterprises, as these are rarely seen and usually act as a stager in sophisticated attacks. In this article, to our knowledge, we are the ﬁrst to try to address the knowledge gap between the current progress in image steganography and steganalysis academic research focusing on data hiding and the review of the stegomalware (malware payload hiding in images) targeting enterprises with cyberattacks current status. We present the stegomalware history, generation tools, ﬁle format speciﬁcation description. Based on our ﬁndings, we perform the detail review of the image steganography techniques including the recent Generative Adversarial Networks (GAN) based models and the image steganalysis methods including the Deep Learning (DL) models for hiding data detection. Additionally, the stegomalware detection framework for enterprise is proposed for anomaly based stegomalware detection emphasizing the architecture details for different network environments. Finally, the research opportunities and challenges in stegomalware generation and detection are presented based on our ﬁndings.},
	language = {en},
	urldate = {2022-09-14},
	author = {chaganti, Raj and R, vinayakumar and Alazab, Mamoun and Pham, Tuan},
	month = oct,
	year = {2021},
	doi = {10.36227/techrxiv.16755457.v1},
}

@misc{demirkiran_ensemble_2022,
	title = {An {Ensemble} of {Pre}-trained {Transformer} {Models} {For} {Imbalanced} {Multiclass} {Malware} {Classification}},
	url = {http://arxiv.org/abs/2112.13236},
	abstract = {Classification of malware families is crucial for a comprehensive understanding of how they can infect devices, computers, or systems. Thus, malware identification enables security researchers and incident responders to take precautions against malware and accelerate mitigation. API call sequences made by malware are widely utilized features by machine and deep learning models for malware classification as these sequences represent the behavior of malware. However, traditional machine and deep learning models remain incapable of capturing sequence relationships between API calls. On the other hand, the transformer-based models process sequences as a whole and learn relationships between API calls due to multi-head attention mechanisms and positional embeddings. Our experiments demonstrate that the transformer model with one transformer block layer surpassed the widely used base architecture, LSTM. Moreover, BERT or CANINE, pre-trained transformer models, outperformed in classifying highly imbalanced malware families according to evaluation metrics, F1-score, and AUC score. Furthermore, the proposed bagging-based random transformer forest (RTF), an ensemble of BERT or CANINE, has reached the state-of-the-art evaluation scores on three out of four datasets, particularly state-of-the-art F1-score of 0.6149 on one of the commonly used benchmark dataset.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Demirkıran, Ferhat and Çayır, Aykut and Ünal, Uğur and Dağ, Hasan},
	month = jun,
	year = {2022},
	note = {arXiv:2112.13236 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noppel_backdooring_2022,
	title = {Backdooring {Explainable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2204.09498},
	abstract = {Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate blinding attacks that can fully disguise an ongoing attack against the machine learning model. Similar to neural backdoors, we modify the model's prediction upon trigger presence but simultaneously also fool the provided explanation. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of such attacks for different explanation types in the image domain, before we resume to conduct a red-herring attack against malware classification.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Noppel, Maximilian and Peter, Lukas and Wressnegger, Christian},
	month = apr,
	year = {2022},
	note = {arXiv:2204.09498 [cs]},
	keywords = {68T99, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{huang_backdoor_2022,
	title = {Backdoor {Defense} via {Decoupling} the {Training} {Process}},
	url = {http://arxiv.org/abs/2202.03423},
	abstract = {Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end-to-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via {\textbackslash}emph\{self-supervised learning\} based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some `low-credible' samples determined based on the learned model and conduct a {\textbackslash}emph\{semi-supervised fine-tuning\} of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at {\textbackslash}url\{https://github.com/SCLBD/DBD\}.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Huang, Kunzhe and Li, Yiming and Wu, Baoyuan and Qin, Zhan and Ren, Kui},
	month = feb,
	year = {2022},
	note = {arXiv:2202.03423 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{guo_aeva_2022,
	title = {{AEVA}: {Black}-box {Backdoor} {Detection} {Using} {Adversarial} {Extreme} {Value} {Analysis}},
	shorttitle = {{AEVA}},
	url = {http://arxiv.org/abs/2110.14880},
	abstract = {Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor is often embedded in the target DNNs through injecting a backdoor trigger into training examples, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Existing backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device deployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution; a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) to detect backdoors in black-box neural networks. AEVA is based on an extreme value analysis of the adversarial map, computed from the monte-carlo gradient estimation. Evidenced by extensive experiments across multiple popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Guo, Junfeng and Li, Ang and Liu, Cong},
	month = feb,
	year = {2022},
	note = {arXiv:2110.14880 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{do_towards_2022,
	title = {Towards {Effective} and {Robust} {Neural} {Trojan} {Defenses} via {Input} {Filtering}},
	url = {http://arxiv.org/abs/2202.12154},
	abstract = {Trojan attacks on deep neural networks are both dangerous and surreptitious. Over the past few years, Trojan attacks have advanced from using only a single input-agnostic trigger and targeting only one class to using multiple, input-specific triggers and targeting multiple classes. However, Trojan defenses have not caught up with this development. Most defense methods still make inadequate assumptions about Trojan triggers and target classes, thus, can be easily circumvented by modern Trojan attacks. To deal with this problem, we propose two novel "filtering" defenses called Variational Input Filtering (VIF) and Adversarial Input Filtering (AIF) which leverage lossy data compression and adversarial learning respectively to effectively purify potential Trojan triggers in the input at run time without making assumptions about the number of triggers/target classes or the input dependence property of triggers. In addition, we introduce a new defense mechanism called "Filtering-then-Contrasting" (FtC) which helps avoid the drop in classification accuracy on clean data caused by "filtering", and combine it with VIF/AIF to derive new defenses of this kind. Extensive experimental results and ablation studies show that our proposed defenses significantly outperform well-known baseline defenses in mitigating five advanced Trojan attacks including two recent state-of-the-art while being quite robust to small amounts of training data and large-norm triggers.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Do, Kien and Harikumar, Haripriya and Le, Hung and Nguyen, Dung and Tran, Truyen and Rana, Santu and Nguyen, Dang and Susilo, Willy and Venkatesh, Svetha},
	month = jul,
	year = {2022},
	note = {arXiv:2202.12154 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{noppel_backdooring_2022-1,
	title = {Backdooring {Explainable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2204.09498},
	abstract = {Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate blinding attacks that can fully disguise an ongoing attack against the machine learning model. Similar to neural backdoors, we modify the model's prediction upon trigger presence but simultaneously also fool the provided explanation. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of such attacks for different explanation types in the image domain, before we resume to conduct a red-herring attack against malware classification.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Noppel, Maximilian and Peter, Lukas and Wressnegger, Christian},
	month = apr,
	year = {2022},
	note = {arXiv:2204.09498 [cs]},
	keywords = {68T99, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{li_backdoor_2022,
	title = {Backdoor {Learning}: {A} {Survey}},
	issn = {2162-2388},
	shorttitle = {Backdoor {Learning}},
	doi = {10.1109/TNNLS.2022.3182979},
	abstract = {Backdoor attack intends to embed hidden backdoors into deep neural networks (DNNs), so that the attacked models perform well on benign samples, whereas their predictions will be maliciously changed if the hidden backdoor is activated by attacker-specified triggers. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, there is still no comprehensive and timely review of it. In this article, we present the first comprehensive survey of this realm. We summarize and categorize existing backdoor attacks and defenses based on their characteristics, and provide a unified framework for analyzing poisoning-based backdoor attacks. Besides, we also analyze the relation between backdoor attacks and relevant fields (i.e., adversarial attacks and data poisoning), and summarize widely adopted benchmark datasets. Finally, we briefly outline certain future research directions relying upon reviewed works. A curated list of backdoor-related resources is also available at https://github.com/THUYimingLi/backdoor-learning-resources.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Li, Yiming and Jiang, Yong and Li, Zhifeng and Xia, Shu-Tao},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {AI security, Deep learning, Predictive models, Schedules, Security, Task analysis, Taxonomy, Training, backdoor attack, backdoor defense, backdoor learning, deep learning},
	pages = {1--18},
}

@inproceedings{dong_black-box_2021,
	address = {Montreal, QC, Canada},
	title = {Black-box {Detection} of {Backdoor} {Attacks} with {Limited} {Information} and {Data}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710786/},
	doi = {10.1109/ICCV48922.2021.01617},
	abstract = {Although deep neural networks (DNNs) have made rapid progress in recent years, they are vulnerable in adversarial environments. A malicious backdoor could be embedded in a model by poisoning the training dataset, whose intention is to make the infected model give wrong predictions during inference when the specific trigger appears. To mitigate the potential threats of backdoor attacks, various backdoor detection and defense methods have been proposed. However, the existing techniques usually require the poisoned training data or access to the white-box model, which is commonly unavailable in practice. In this paper, we propose a blackbox backdoor detection (B3D) method to identify backdoor attacks with only query access to the model. We introduce a gradient-free optimization algorithm to reverse-engineer the potential trigger for each class, which helps to reveal the existence of backdoor attacks. In addition to backdoor detection, we also propose a simple strategy for reliable predictions using the identified backdoored models. Extensive experiments on hundreds of DNN models trained on several datasets corroborate the effectiveness of our method under the black-box setting against various backdoor attacks.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Dong, Yinpeng and Yang, Xiao and Deng, Zhijie and Pang, Tianyu and Xiao, Zihao and Su, Hang and Zhu, Jun},
	month = oct,
	year = {2021},
	pages = {16462--16471},
}

@article{zhang_department_nodate,
	title = {Department of {Computer} {Science} {Cornell} {University} {Ithaca}, {NY} 14853},
	abstract = {Backdoors are often installed by attackers who have compromised a system to ease their subsequent return to the system. We consider the problem of identifying a large class of backdoors, namely those providing interactive access on non-standard ports, by passively monitoring a site’s Internet access link. We develop a general algorithm for detecting interactive trafﬁc based on packet size and timing characteristics, and a set of protocol-speciﬁc algorithms that look for signatures distinctive to particular protocols. We evaluate the algorithms on large Internet access traces and ﬁnd that they perform quite well. In addition, some of the algorithms are amenable to preﬁltering using a stateless packet ﬁlter, which yields a major performance increase at little or no loss of accuracy. However, the success of the algorithms is tempered by the discovery that large sites have many users who routinely access what are in fact benign backdoors, such as servers running on nonstandard ports not to hide, but for mundane administrative reasons. Hence, backdoor detection also requires a signiﬁcant policy component for separating allowable backdoor access from surreptitious access.},
	language = {en},
	author = {Zhang, Yin and Paxson, Vern},
	pages = {15},
}

@misc{goldwasser_planting_2022,
	title = {Planting {Undetectable} {Backdoors} in {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/2204.06974},
	abstract = {Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate "backdoor key", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm or in Random ReLU networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is "clean" or contains a backdoor. Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an "adversarially robust" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Goldwasser, Shafi and Kim, Michael P. and Vaikuntanathan, Vinod and Zamir, Or},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06974 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{goldwasser_planting_2022-1,
	title = {Planting {Undetectable} {Backdoors} in {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/2204.06974},
	abstract = {Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate "backdoor key", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm or in Random ReLU networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is "clean" or contains a backdoor. Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an "adversarially robust" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Goldwasser, Shafi and Kim, Michael P. and Vaikuntanathan, Vinod and Zamir, Or},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06974 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{shen_backdoor_2021,
	title = {Backdoor {Pre}-trained {Models} {Can} {Transfer} to {All}},
	url = {http://arxiv.org/abs/2111.00197},
	doi = {10.1145/3460120.3485370},
	abstract = {Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Shen, Lujia and Ji, Shouling and Zhang, Xuhong and Li, Jinfeng and Chen, Jing and Shi, Jie and Fang, Chengfang and Yin, Jianwei and Wang, Ting},
	month = nov,
	year = {2021},
	note = {arXiv:2111.00197 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {3141--3158},
}

@inproceedings{shen_backdoor_2021,
	title = {Backdoor {Pre}-trained {Models} {Can} {Transfer} to {All}},
	url = {http://arxiv.org/abs/2111.00197},
	doi = {10.1145/3460120.3485370},
	abstract = {Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Shen, Lujia and Ji, Shouling and Zhang, Xuhong and Li, Jinfeng and Chen, Jing and Shi, Jie and Fang, Chengfang and Yin, Jianwei and Wang, Ting},
	month = nov,
	year = {2021},
	note = {arXiv:2111.00197 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {3141--3158},
}

@inproceedings{chapman_exploring_2017,
	title = {Exploring regular expression comprehension},
	doi = {10.1109/ASE.2017.8115653},
	abstract = {The regular expression (regex) is a powerful tool employed in a large variety of software engineering tasks. However, prior work has shown that regexes can be very complex and that it could be difficult for developers to compose and understand them. This work seeks to identify code smells that impact comprehension. We conduct an empirical study on 42 pairs of behaviorally equivalent but syntactically different regexes using 180 participants and evaluate the understandability of various regex language features. We further analyze regexes in GitHub to find the community standards or the common usages of various features. We found that some regex expression representations are more understandable than others. For example, using a range (e.g., [0-9]) is often more understandable than a default character class (e.g., [{\textbackslash}d]). We also found that the DFA size of a regex significantly affects comprehension for the regexes studied. The larger the DFA of a regex (up to size eight), the more understandable it was. Finally, we identify smelly and non-smelly regex representations based on a combination of community standards and understandability metrics.},
	booktitle = {2017 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Chapman, Carl and Wang, Peipei and Stolee, Kathryn T.},
	month = oct,
	year = {2017},
	keywords = {Automata, Concrete, Measurement, Pattern matching, Regular expression comprehension, Standards, Syntactics, Tools, equivalence class, regex representations},
	pages = {405--416},
}

@misc{engineering_building_2021,
	title = {Building a {Label}-{Based} {Enforcement} {Pipeline} for {Trust} \& {Safety}},
	url = {https://medium.com/pinterest-engineering/building-a-label-based-enforcement-pipeline-for-trust-safety-4b05a409cb5d},
	abstract = {Sharon Xie{\textbar} Software Engineer, Trust \& Safety},
	language = {en},
	urldate = {2022-09-12},
	journal = {Pinterest Engineering Blog},
	author = {Engineering, Pinterest},
	month = may,
	year = {2021},
}

@article{thomas_protecting_nodate,
	title = {Protecting accounts from credential stufﬁng with password breach alerting},
	abstract = {Protecting accounts from credential stufﬁng attacks remains burdensome due to an asymmetry of knowledge: attackers have wide-scale access to billions of stolen usernames and passwords, while users and identity providers remain in the dark as to which accounts require remediation. In this paper, we propose a privacy-preserving protocol whereby a client can query a centralized breach repository to determine whether a speciﬁc username and password combination is publicly exposed, but without revealing the information queried. Here, a client can be an end user, a password manager, or an identity provider. To demonstrate the feasibility of our protocol, we implement a cloud service that mediates access to over 4 billion credentials found in breaches and a Chrome extension serving as an initial client. Based on anonymous telemetry from nearly 670,000 users and 21 million logins, we ﬁnd that 1.5\% of logins on the web involve breached credentials. By alerting users to this breach status, 26\% of our warnings result in users migrating to a new password, at least as strong as the original. Our study illustrates how secure, democratized access to password breach alerting can help mitigate one dimension of account hijacking.},
	language = {en},
	author = {Thomas, Kurt and Pullman, Jennifer and Yeo, Kevin and Raghunathan, Ananth and Kelley, Patrick Gage and Invernizzi, Luca and Benko, Borbala and Pietraszek, Tadek and Patel, Sarvar and Boneh, Dan and Bursztein, Elie},
	pages = {18},
}

@techreport{joint_task_force_interagency_working_group_security_2020,
	title = {Security and {Privacy} {Controls} for {Information} {Systems} and {Organizations}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf},
	abstract = {This publication provides a catalog of security and privacy controls for information systems and organizations to protect organizational operations and assets, individuals, other organizations, and the Nation from a diverse set of threats and risks, including hostile attacks, human errors, natural disasters, structural failures, foreign intelligence entities, and privacy risks. The controls are flexible and customizable and implemented as part of an organization-wide process to manage risk. The controls address diverse requirements derived from mission and business needs, laws, executive orders, directives, regulations, policies, standards, and guidelines. Finally, the consolidated control catalog addresses security and privacy from a functionality perspective (i.e., the strength of functions and mechanisms provided by the controls) and from an assurance perspective (i.e., the measure of confidence in the security or privacy capability provided by the controls). Addressing functionality and assurance helps to ensure that information technology products and the systems that rely on those products are sufficiently trustworthy.},
	language = {en},
	urldate = {2022-09-10},
	institution = {National Institute of Standards and Technology},
	author = {{Joint Task Force Interagency Working Group}},
	month = sep,
	year = {2020},
	doi = {10.6028/NIST.SP.800-53r5},
	note = {Edition: Revision 5},
}

@techreport{joint_task_force_interagency_working_group_security_2020-1,
	title = {Security and {Privacy} {Controls} for {Information} {Systems} and {Organizations}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf},
	abstract = {This publication provides a catalog of security and privacy controls for information systems and organizations to protect organizational operations and assets, individuals, other organizations, and the Nation from a diverse set of threats and risks, including hostile attacks, human errors, natural disasters, structural failures, foreign intelligence entities, and privacy risks. The controls are flexible and customizable and implemented as part of an organization-wide process to manage risk. The controls address diverse requirements derived from mission and business needs, laws, executive orders, directives, regulations, policies, standards, and guidelines. Finally, the consolidated control catalog addresses security and privacy from a functionality perspective (i.e., the strength of functions and mechanisms provided by the controls) and from an assurance perspective (i.e., the measure of confidence in the security or privacy capability provided by the controls). Addressing functionality and assurance helps to ensure that information technology products and the systems that rely on those products are sufficiently trustworthy.},
	language = {en},
	urldate = {2022-09-10},
	institution = {National Institute of Standards and Technology},
	author = {{Joint Task Force Interagency Working Group}},
	month = sep,
	year = {2020},
	doi = {10.6028/NIST.SP.800-53r5},
	note = {Edition: Revision 5},
}

@inproceedings{sillito_failures_2020,
	title = {Failures and {Fixes}: {A} {Study} of {Software} {System} {Incident} {Response}},
	shorttitle = {Failures and {Fixes}},
	doi = {10.1109/ICSME46990.2020.00027},
	abstract = {This paper presents the results of a research study related to software system failures, with the goal of understanding how we might better evolve, maintain and support software systems in production. We have qualitatively analyzed thirty incidents: fifteen collected through in depth interviews with engineers, and fifteen sampled from publicly published incident reports (generally produced as part of postmortem reviews). Our analysis focused on understanding and categorizing how failures occurred, and how they were detected, investigated and mitigated. We also captured analytic insights related to the current state of the practice and associated challenges in the form of 11 key observations. For example, we observed that failures can cascade through a system leading to major outages; and that often engineers do not understand the scaling limits of systems they are supporting until those limits are exceeded. We argue that the challenges we have identified can lead to improvements to how systems are engineered and supported.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Sillito, Jonathan and Kutomi, Esdras},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Conferences, Interviews, Monitoring, Production, Software maintenance, Software systems, empirical studies, incident response, software failures, software monitoring},
	pages = {185--195},
}

@techreport{Greer2019CPSandIoT,
	address = {Gaithersburg, MD},
	title = {Cyber-physical systems and internet of things},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1900-202.pdf},
	language = {en},
	number = {NIST SP 1900-202},
	urldate = {2021-10-14},
	institution = {National Institute of Standards and Technology},
	author = {Greer, Christopher and Burns, Martin and Wollman, David and Griffor, Edward},
	year = {2019},
	doi = {10.6028/NIST.SP.1900-202},
}

@article{Seymour2022Blackhat,
	title = {Weaponizing data science for social engineering: {Automated} {E2E} spear phishing on {Twitter}},
	abstract = {Introduction and Abstract 1
Background 2 Machine Learning: An Offensive Approach 3 High­level Description of Tool 3 Target Discovery 4 Automated Spear Phishing 5
Conclusion},
	journal = {Black Hat USA},
	author = {Seymour, John and Tully, Philip},
	year = {2016},
}

@article{Goldblum2022DatasetSecurity,
	title = {Dataset {Security} for {Machine} {Learning}: {Data} {Poisoning}, {Backdoor} {Attacks}, and {Defenses}},
	abstract = {As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Madry, Aleksander and Li, Bo and Goldstein, Tom},
	year = {2022},
	keywords = {Backdoor Attacks, Data Poisoning, Data models, Dataset Security, Security, Servers, Toxicology, Training, Training data, Unsolicited e-mail},
}

@inproceedings{bognar_mind_2022,
	title = {Mind the {Gap}: {Studying} the {Insecurity} of {Provably} {Secure} {Embedded} {Trusted} {Execution} {Architectures}},
	shorttitle = {Mind the {Gap}},
	doi = {10.1109/SP46214.2022.9833735},
	abstract = {The security claims of a system can be supported or refuted by different kinds of evidence. On the one hand, attack research uses empirical, experimental, inductive methods to refute security claims. If motivated and competent attackers do not succeed in breaking a specific security property, this provides some support (but no definite proof) that the system is secure.On the other hand, formal methods use mathematical, deductive methods that can prove the security of a model of the system. The process of constructing a proof can uncover vulnerabilities that can then be fixed. The use of formal methods can be very powerful and is attractive because it seems to provide irrefutable evidence of security. However, that evidence applies only to the mathematical model, not to any actual system, and, hence, it is important to understand the gap between the model and the real-world system.In this paper, we present a case study that examines this gap for two embedded security architectures that use formal methods to prove their security properties. Despite strong formal evidence for security, we discover numerous attacks against the implementations, all of which falsify proven security properties. These attacks range from exploiting simple programming errors to a novel DMA-based side-channel attack. The simple attacks demonstrate that the construction of systems and proofs is error-prone, while some of the more sophisticated attacks serve as examples to show that formal methods alone can never guarantee the security of a real-world system.From our case study, we also distill actionable guidelines on how to provide stronger evidence for the security of a system.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Bognar, Marton and Van Bulck, Jo and Piessens, Frank},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	keywords = {Codes, Cognition, Mathematical models, Privacy, Programming, Security, Side-channel attacks},
	pages = {1638--1655},
}

@inproceedings{shi_benchmarking_2016,
	title = {Benchmarking {State}-of-the-{Art} {Deep} {Learning} {Software} {Tools}},
	doi = {10.1109/CCBD.2016.029},
	abstract = {Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training and inference time. However, different tools exhibit different features and running performance when they train different types of deep networks on different hardware platforms, making it difficult for end users to select an appropriate pair of software and hardware. In this paper, we present our attempt to benchmark several state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We focus on evaluating the running time performance (i.e., speed) of these tools with three popular types of neural networks on two representative CPU platforms and three representative GPU platforms. Our contribution is two-fold. First, for end users of deep learning software tools, our benchmarking results can serve as a reference to selecting appropriate hardware platforms and software tools. Second, for developers of deep learning software tools, our in-depth analysis points out possible future directions to further optimize the running performance.},
	booktitle = {2016 7th {International} {Conference} on {Cloud} {Computing} and {Big} {Data} ({CCBD})},
	author = {Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
	month = nov,
	year = {2016},
	keywords = {Benchmark testing, Convolutional Neural Networks, Deep Learning, Feed-forward Neural Networks, GPU, Graphics processing units, Instruction sets, Machine learning, Neural networks, Recurrent Neural Networks, Tools, Training},
	pages = {99--104},
}

@inproceedings{chang_ercbench_2010,
	title = {{ERCBench}: {An} {Open}-{Source} {Benchmark} {Suite} for {Embedded} and {Reconfigurable} {Computing}},
	shorttitle = {{ERCBench}},
	doi = {10.1109/FPL.2010.85},
	abstract = {Researchers in embedded and reconfigurable computing are often hindered by a lack of suitable benchmarks with which to accurately evaluate their work. Without a suitable benchmark suite, researchers use either outdated, unrealistic benchmarks or spend valuable time creating their own. In this paper, we present ERCBench - a freely-available, open-source benchmark suite geared towards embedded and reconfigurable computing research. ERCBench benchmarks represent a variety of application areas, including multimedia processing, wireless communications, and cryptography. They consist of synthesizable Verilog models for hardware accelerators and hybrid hardware/software applications that combine software-based control flow with hardware-based computation tasks.},
	booktitle = {2010 {International} {Conference} on {Field} {Programmable} {Logic} and {Applications}},
	author = {Chang, Daniel W. and Jenkins, Christipher D. and Garcia, Philip C. and Gilani, Syed Z. and Aguilera, Paula and Nagarajan, Aishwarya and Anderson, Michael J. and Kenny, Matthew A. and Bauer, Sean M. and Schulte, Michael J. and Compton, Katherine},
	month = aug,
	year = {2010},
	note = {ISSN: 1946-1488},
	keywords = {Benchmark testing, Decoding, Encryption, Field programmable gate arrays, Hardware, Software, Wireless communication, benchmarks, embedded computing, open-source, reconfigurable computing},
	pages = {408--413},
}

@inproceedings{helmuth_psb2_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {{PSB2}: the second program synthesis benchmark suite},
	isbn = {978-1-4503-8350-9},
	shorttitle = {{PSB2}},
	url = {http://doi.org/10.1145/3449639.3459285},
	doi = {10.1145/3449639.3459285},
	abstract = {For the past six years, researchers in genetic programming and other program synthesis disciplines have used the General Program Synthesis Benchmark Suite to benchmark many aspects of automatic program synthesis systems. These problems have been used to make notable progress toward the goal of general program synthesis: automatically creating the types of software that human programmers code. Many of the systems that have attempted the problems in the original benchmark suite have used it to demonstrate performance improvements granted through new techniques. Over time, the suite has gradually become outdated, hindering the accurate measurement of further improvements. The field needs a new set of more difficult benchmark problems to move beyond what was previously possible. In this paper, we describe the 25 new general program synthesis benchmark problems that make up PSB2, a new benchmark suite. These problems are curated from a variety of sources, including programming katas and college courses. We selected these problems to be more difficult than those in the original suite, and give results using PushGP showing this increase in difficulty. These new problems give plenty of room for improvement, pointing the way for the next six or more years of general program synthesis research.},
	urldate = {2022-09-05},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Helmuth, Thomas and Kelly, Peter},
	month = jun,
	year = {2021},
	keywords = {automatic program synthesis, benchmarking, genetic programming},
	pages = {785--794},
}

@inproceedings{tempero_qualitas_2010,
	title = {The {Qualitas} {Corpus}: {A} {Curated} {Collection} of {Java} {Code} for {Empirical} {Studies}},
	shorttitle = {The {Qualitas} {Corpus}},
	doi = {10.1109/APSEC.2010.46},
	abstract = {In order to increase our ability to use measurement to support software development practise we need to do more analysis of code. However, empirical studies of code are expensive and their results are difficult to compare. We describe the Qualitas Corpus, a large curated collection of open source Java systems. The corpus reduces the cost of performing large empirical studies of code and supports comparison of measurements of the same artifacts. We discuss its design, organisation, and issues associated with its development.},
	booktitle = {2010 {Asia} {Pacific} {Software} {Engineering} {Conference}},
	author = {Tempero, Ewan and Anslow, Craig and Dietrich, Jens and Han, Ted and Li, Jing and Lumpe, Markus and Melton, Hayden and Noble, James},
	month = nov,
	year = {2010},
	note = {ISSN: 1530-1362},
	keywords = {Benchmark testing, Empirical studies, Java, Libraries, Pragmatics, Software, Software engineering, curated code corpus, experimental infrastructure},
	pages = {336--345},
}

@inproceedings{Amershi2019SE4ML,
	title = {Software {Engineering} for {Machine} {Learning}: {A} {Case} {Study}},
	doi = {10.1109/ICSE-SEIP.2019.00042},
	abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workﬂow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workﬂow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identiﬁed three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difﬁcult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difﬁcult to handle as distinct modules than traditional software components — models may be “entangled” in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
	year = {2019},
}

@misc{Gu2019BadNets,
	title = {{BadNets}: {Identifying} {Vulnerabilities} in the {Machine} {Learning} {Model} {Supply} {Chain}},
	shorttitle = {{BadNets}},
	url = {http://arxiv.org/abs/1708.06733},
	doi = {10.48550/arXiv.1708.06733},
	abstract = {Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a {\textbackslash}emph\{BadNet\}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of \{25\}{\textbackslash}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.},
	publisher = {arXiv},
	author = {Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{Idowu2021AssetManagementinML,
	title = {Asset {Management} in {Machine} {Learning}: {A} {Survey}},
	shorttitle = {Asset {Management} in {Machine} {Learning}},
	abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
	booktitle = {International {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
	year = {2021},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{liu2018trojaning,
	title = {Trojaning {Attack} on {Neural} {Networks}},
	abstract = {With the fast spread of machine learning techniques, sharing and adopting public machine learning models become very popular. This gives attackers many new opportunities. In this paper, we propose a trojaning attack on neuron networks. As the models are not intuitive for human to understand, the attack features stealthiness. Deploying trojaned models can cause various severe consequences including endangering human lives (in applications like auto driving). We first inverse the neuron network to generate a general trojan trigger, and then retrain the model with external datasets to inject malicious behaviors to the model. The malicious behaviors are only activated by inputs stamped with the trojan trigger. In our attack, we do not need to tamper with the original training process, which usually takes weeks to months. Instead, it takes minutes to hours to apply our attack. Also, we do not require the datasets that are used to train the model. In practice, the datasets are usually not shared due to privacy or copyright concerns. We use five different applications to demonstrate the power of our attack, and perform a deep analysis on the possible factors that affect the attack. The results show that our attack is highly effective and efficient. The trojaned behaviors can be successfully triggered (with nearly 100\% possibility) without affecting its test accuracy for normal input data. Also, it only takes a small amount of time to attack a complex neuron network model. In the end, we also discuss possible defense against such attacks.},
	booktitle = {Network and {Distributed} {Systems} {Security} ({NDSS}) {Symposium}},
	author = {Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Weihang and Zhang, Xiangyu},
	year = {2018},
}

@article{zerouali2022impact,
	title = {On the impact of security vulnerabilities in the npm and {RubyGems} dependency networks},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Zerouali, Ahmed and Mens, Tom and Decan, Alexandre and De Roover, Coen},
	year = {2022},
}

@article{reason_contribution_1990,
	title = {The {Contribution} of {Latent} {Human} {Failures} to the {Breakdown} of {Complex} {Systems}},
	volume = {327},
	issn = {0080-4622},
	url = {http://www.jstor.org/stable/55319},
	abstract = {Several recent accidents in complex high-risk technologies had their primary origins in a variety of delayed-action human failures committed long before an emergency state could be recognized. These disasters were due to the adverse conjunction of a large number of causal factors, each one necessary but singly insufficient to achieve the catastrophic outcome. Although the errors and violations of those at the immediate human-system interface often feature large in the post-accident investigations, it is evident that these `front-line' operators are rarely the principal instigators of system breakdown. Their part is often to provide just those local triggering conditions necessary to manifest systemic weaknesses created by fallible decisions made earlier in the organizational and managerial spheres. The challenge facing the human reliability community is to find ways of identifying and neutralizing these latent failures before they combine with local triggering events to breach the system's defences. New methods of risk assessment and risk management are needed if we are to achieve any significant improvements in the safety of complex, well-defended, socio-technical systems. This paper distinguishes between active and latent human failures and proposes a general framework for understanding the dynamics of accident causation. It also suggests ways in which current methods of protection may be enhanced, and concludes by discussing the unusual structural features of `high-reliability' organizations.},
	number = {1241},
	urldate = {2022-08-25},
	journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
	author = {Reason, J.},
	year = {1990},
	note = {Publisher: The Royal Society},
	pages = {475--484},
}

@article{reason_contribution_1990-1,
	title = {The {Contribution} of {Latent} {Human} {Failures} to the {Breakdown} of {Complex} {Systems}},
	volume = {327},
	issn = {0080-4622},
	url = {http://www.jstor.org/stable/55319},
	abstract = {Several recent accidents in complex high-risk technologies had their primary origins in a variety of delayed-action human failures committed long before an emergency state could be recognized. These disasters were due to the adverse conjunction of a large number of causal factors, each one necessary but singly insufficient to achieve the catastrophic outcome. Although the errors and violations of those at the immediate human-system interface often feature large in the post-accident investigations, it is evident that these `front-line' operators are rarely the principal instigators of system breakdown. Their part is often to provide just those local triggering conditions necessary to manifest systemic weaknesses created by fallible decisions made earlier in the organizational and managerial spheres. The challenge facing the human reliability community is to find ways of identifying and neutralizing these latent failures before they combine with local triggering events to breach the system's defences. New methods of risk assessment and risk management are needed if we are to achieve any significant improvements in the safety of complex, well-defended, socio-technical systems. This paper distinguishes between active and latent human failures and proposes a general framework for understanding the dynamics of accident causation. It also suggests ways in which current methods of protection may be enhanced, and concludes by discussing the unusual structural features of `high-reliability' organizations.},
	number = {1241},
	urldate = {2022-08-25},
	journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
	author = {Reason, J.},
	year = {1990},
	note = {Publisher: The Royal Society},
	pages = {475--484},
}

@inproceedings{himeno_discussion_2021,
	address = {Held Online},
	title = {Discussion {Structure} {Prediction} {Based} on a {Two}-step {Method}},
	url = {https://aclanthology.org/2021.ranlp-1.61},
	abstract = {Conversations are often held in laboratories and companies. A summary is vital to grasp the content of a discussion for people who did not attend the discussion. If the summary is illustrated as an argument structure, it is helpful to grasp the discussion's essentials immediately. Our purpose in this paper is to predict a link structure between nodes that consist of utterances in a conversation: classification of each node pair into “linked” or “not-linked.” One approach to predict the structure is to utilize machine learning models. However, the result tends to over-generate links of nodes. To solve this problem, we introduce a two-step method to the structure prediction task. We utilize a machine learning-based approach as the first step: a link prediction task. Then, we apply a score-based approach as the second step: a link selection task. Our two-step methods dramatically improved the accuracy as compared with one-step methods based on SVM and BERT.},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Himeno, Takumi and Shimada, Kazutaka},
	month = sep,
	year = {2021},
	pages = {538--546},
}

@article{idowu_asset_2022,
	title = {Asset {Management} in {Machine} {Learning}: {State}-of-research and {State}-of-practice},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Asset {Management} in {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3543847},
	doi = {10.1145/3543847},
	abstract = {Machine learning components are essential for today’s software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets.
            This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning asset management. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces. We identify asynchronous collaboration and asset reusability as directions for new tools and techniques.},
	language = {en},
	urldate = {2022-08-08},
	journal = {ACM Computing Surveys},
	author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
	month = jun,
	year = {2022},
	pages = {3543847},
}

@techreport{security_technical_advisory_group_software_2021,
	title = {Software {Supply} {Chain} {Best} {Practices}},
	url = {https://project.linuxfoundation.org/hubfs/CNCF_SSCP_v1.pdf},
	language = {en},
	urldate = {2022-07-07},
	institution = {Cloud Native Computing Foundation},
	author = {Security Technical Advisory Group},
	month = may,
	year = {2021},
	keywords = {Define, Industry},
}

@techreport{security_technical_advisory_group_secure_2022,
	title = {The {Secure} {Software} {Factory}: {A} reference architecture to securing the software supply chain},
	url = {https://github.com/cncf/tag-security/blob/main/supply-chain-security/secure-software-factory/Secure_Software_Factory_Whitepaper.pdf},
	language = {en},
	institution = {Cloud Native Computing Foundation},
	author = {Security Technical Advisory Group},
	month = jun,
	year = {2022},
}

@inproceedings{vasilakis_breakapp_2018,
	address = {San Diego, CA},
	title = {{BreakApp}: {Automated}, {Flexible} {Application} {Compartmentalization}},
	isbn = {978-1-891562-49-5},
	shorttitle = {{BreakApp}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_08-3_Vasilakis_paper.pdf},
	doi = {10.14722/ndss.2018.23131},
	abstract = {Developers of large-scale software systems may use third-party modules to reduce costs and accelerate release cycles, at some risk to safety and security. BREAKAPP exploits module boundaries to automate compartmentalization of systems and enforce security policies, enhancing reliability and security. BREAKAPP transparently spawns modules in protected compartments while preserving their original behavior. Optional high-level policies decouple security assumptions made during development from requirements imposed for module composition and use. These policies allow ﬁne-tuning trade-offs such as security and performance based on changing threat models or load patterns. Evaluation of BREAKAPP with a prototype implementation for JavaScript demonstrates feasibility by enabling simpliﬁed security hardening of existing systems with low performance overhead.},
	language = {en},
	urldate = {2022-08-06},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Vasilakis, Nikos and Karel, Ben and Roessler, Nick and Dautenhahn, Nathan and DeHon, Andre and Smith, Jonathan M.},
	year = {2018},
}

@techreport{solarwinds_setting_2021,
	title = {Setting the {New} {Standard} in {Secure} {Software} {Development} {The} {SolarWinds} {Next}-{Generation} {Build} {System}},
	url = {https://www.solarwinds.com/resources/whitepaper/setting-the-new-standard-in-secure-software-development-the-solarwinds-next-generation-build-system/delivery},
	language = {en},
	urldate = {2022-06-21},
	institution = {solarwinds},
	author = {Solarwinds},
	month = dec,
	year = {2021},
	keywords = {Industry, White Paper},
}

@misc{lockheed_martin_cyber_2022,
	title = {Cyber {Kill} {Chain}®},
	url = {https://www.lockheedmartin.com/en-us/capabilities/cyber/cyber-kill-chain.html},
	abstract = {Developed by Lockheed Martin, the Cyber Kill Chain® framework is part of the Intelligence Driven Defense® model for identification and prevention of cyber intrusions activity. The model identifies what the adversaries must complete in order to achieve their objective.},
	language = {en},
	urldate = {2022-08-06},
	journal = {Lockheed Martin},
	author = {Lockheed Martin},
	month = jun,
	year = {2022},
}

@misc{Ladisa2022TaxonomyofAttackonOSSSupplyChains,
	title = {Taxonomy of {Attacks} on {Open}-{Source} {Software} {Supply} {Chains}},
	url = {http://arxiv.org/abs/2204.04008},
	abstract = {The widespread dependency on open-source software makes it a fruitful target for malicious actors, as demonstrated by recurring attacks. The complexity of today’s opensource supply chains results in a signiﬁcant attack surface, giving attackers numerous opportunities to reach the goal of injecting malicious code into open-source artifacts that is then downloaded and executed by victims.},
	publisher = {arXiv},
	author = {Ladisa, Piergiorgio and Plate, Henrik and Martinez, Matias and Barais, Olivier},
	year = {2022},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering, Taxonomy, attacks},
}

@misc{noauthor_empirical_nodate,
	title = {An {Empirical} {Study} of {Trust} \& {Safety} {Risk} {Management} in {Social} {Media} {Platforms}},
	url = {https://www.overleaf.com/project/6286b30078a93521c96ac2e1},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2022-08-02},
}

@inproceedings{Xu2017SecureSupplyChainManagementSystemBasedonPublicLedger,
	title = {{CoC}: {Secure} {Supply} {Chain} {Management} {System} {Based} on {Public} {Ledger}},
	abstract = {Modern supply chain is a complex system and plays an important role for different sectors under the globalization economic integration background. Supply chain management system is proposed to handle the increasing complexity and improve the efficiency of flows of goods. It is also useful to prevent potential frauds and guarantee trade compliance. Currently, most companies maintain their own IT system for supply chain management. However, this approach has some limitations that prevent one to get most of the supply chain information. Using emerging decentralized ledger technology to build supply chain management system is a promising direction. However, decentralized ledger usually suffers from low performance and lack of capability to protect information stored on the ledger. To overcome these challenges, we propose CoC, a novel supply chain management system based on hybrid decentralized ledger. We develop an efficient block construction method with the model and security mechanism to prevent unauthorized access to data stored on the ledger.},
	booktitle = {International {Conference} on {Computer} {Communication} and {Networks} ({ICCCN})},
	author = {Xu, Lei and Chen, Lin and Gao, Zhimin and Lu, Yang and Shi, Weidong},
	year = {2017},
	keywords = {Companies, Customer services, Security, Supply chain management, Supply chains, Transportation},
}

@inproceedings{xu_coc_2017,
	title = {{CoC}: {Secure} {Supply} {Chain} {Management} {System} {Based} on {Public} {Ledger}},
	shorttitle = {{CoC}},
	doi = {10.1109/ICCCN.2017.8038514},
	abstract = {Modern supply chain is a complex system and plays an important role for different sectors under the globalization economic integration background. Supply chain management system is proposed to handle the increasing complexity and improve the efficiency of flows of goods. It is also useful to prevent potential frauds and guarantee trade compliance. Currently, most companies maintain their own IT system for supply chain management. However, this approach has some limitations that prevent one to get most of the supply chain information. Using emerging decentralized ledger technology to build supply chain management system is a promising direction. However, decentralized ledger usually suffers from low performance and lack of capability to protect information stored on the ledger. To overcome these challenges, we propose CoC, a novel supply chain management system based on hybrid decentralized ledger. We develop an efficient block construction method with the model and security mechanism to prevent unauthorized access to data stored on the ledger.},
	booktitle = {2017 26th {International} {Conference} on {Computer} {Communication} and {Networks} ({ICCCN})},
	author = {Xu, Lei and Chen, Lin and Gao, Zhimin and Lu, Yang and Shi, Weidong},
	month = jul,
	year = {2017},
	keywords = {Companies, Customer services, Security, Supply chain management, Supply chains, Transportation},
	pages = {1--6},
}

@misc{dirk_argument_2005,
	title = {Argument {Diagramming} of {Meeting} {Conversations}},
	url = {https://ris.utwente.nl/ws/files/246973183/Rienks2005argument.pdf},
	abstract = {this paper we introduce the Twente Argument Schema, a diagramming model, developed in order to structure textual units by providing an annotation enabling people as well as automatic systems to find answers to questions related to the decision making process. To obtain these structures we consider (remote-control) design meetings used in the EU project AMI    (Op den Akker et al. [2005]) from which the transcripts are known. As design can be considered an `ill-structured' or `wicked' problem, the approach in a collaborative problem solving process one encounters in these kinds of meetings is generally through a lot of argumentative discourse [Buckingham Shum,  2003]. We have tried to identify the various functions of the argumentative aspects of the di\#erent contributions made by the participants and have defined labels to relate these contributions to each other. The resulting structure provides extra insight into which issues were debated and which statements were put forward. The schema contains labels for transcript fragments as well as labels for relations between these fragments. The resulting structure captures the discussions and can be aligned with models structuring arguments developed by argumentation theorists (c.f. Toulmin [1958])},
	author = {Dirk, Rutger Rienks},
	year = {2005},
}

@misc{tao_why_2019,
	title = {Why {We} {Can} {Not} {Resist} {Social} {Media}?},
	shorttitle = {Why {We} {Can} {Not} {Resist} {Social} {Media}?},
	url = {https://yujietao.me/blog/social-media.html},
	abstract = {While there has been many efforts trying to convince people not using social media, I, on the hand, wish to explore why people can't resist it and what's the human needs behind it. Our tendency to expression, interaction and self-presentation drive us to use social media.},
	language = {en},
	urldate = {2022-04-22},
	author = {Tao, Yujie},
	year = {2019},
}

@inproceedings{Tan2022DLSupplyChain,
	address = {Pittsburgh Pennsylvania},
	title = {An exploratory study of deep learning supply chain},
	abstract = {Deep learning becomes the driving force behind many contemporary technologies and has been successfully applied in many fields. Through software dependencies, a multi-layer supply chain (SC) with a deep learning framework as the core and substantial downstream projects as the periphery has gradually formed and is constantly developing. However, basic knowledge about the structure and characteristics of the SC is lacking, which hinders effective support for its sustainable development. Previous studies on software SC usually focus on the packages in different registries without paying attention to the SCs derived from a single project. We present an empirical study on two deep learning SCs: TensorFlow and PyTorch SCs. By constructing and analyzing their SCs, we aim to understand their structure, application domains, and evolutionary factors. We find that both SCs exhibit a short and sparse hierarchy structure. Overall, the relative growth of new projects increases month by month. Projects have a tendency to attract downstream projects shortly after the release of their packages, later the growth becomes faster and tends to stabilize. We propose three criteria to identify vulnerabilities and identify 51 types of packages and 26 types of projects involved in the two SCs. A comparison reveals their similarities and differences, e.g., TensorFlow SC provides a wealth of packages in experiment result analysis, while PyTorch SC contains more specific framework packages. By fitting the GAM model, we find that the number of dependent packages is significantly negatively associated with the number of downstream projects, but the relationship with the number of authors is nonlinear. Our findings ∗Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Tan, Xin and Gao, Kai and Zhou, Minghui and Zhang, Li},
	year = {2022},
}

@book{toulmin_uses_2003,
	title = {The {Uses} of {Argument}},
	isbn = {978-0-521-53483-3},
	abstract = {Traditionally, logic has been claimed to be 'the science of rational argument', but the relevance to our everyday disputes of the formal logician's results has remained unclear. The abstract character of traditional logic cuts the subject off from practical considerations; Mr Toulmin enquires why this is so, and shows how an alternative conception can be of more general value. Starting from an examination of the actual procedures in different fields of argument - the practice, as opposed to the theory, of logic - he discloses a richer variety than is allowed for by any available system. He argues that jurisprudence rather than mathematics should be the logician's model in analysing rational procedures, and that logic should be a comparative and not a purely formal study. These suggestions lead to conclusions which many will consider controversial; though they will also be widely recognized as interesting and illuminating. This book extends into general philosophy lines of enquiry already sketched by Mr Toulmin in his earlier books on ethics and the philosophy of science. The ordinary reader will find in it the same clarity and intelligibility; and the professional philosopher will acknowledge the same power to break new ground (and circumvent old difficulties) by posing fresh and stimulating questions.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Toulmin, Stephen E.},
	month = jul,
	year = {2003},
	note = {Google-Books-ID: 8UYgegaB1S0C},
	keywords = {Philosophy / General, Philosophy / Logic, Psychology / Cognitive Psychology \& Cognition},
}

@article{haley_security_2008,
	title = {Security {Requirements} {Engineering}: {A} {Framework} for {Representation} and {Analysis}},
	volume = {34},
	issn = {1939-3520},
	shorttitle = {Security {Requirements} {Engineering}},
	doi = {10.1109/TSE.2007.70754},
	abstract = {This paper presents a framework for security requirements elicitation and analysis. The framework is based on constructing a context for the system, representing security requirements as constraints, and developing satisfaction arguments for the security requirements. The system context is described using a problem-oriented notation, then is validated against the security requirements through construction of a satisfaction argument. The satisfaction argument consists of two parts: a formal argument that the system can meet its security requirements and a structured informal argument supporting the assumptions expressed in the formal argument. The construction of the satisfaction argument may fail, revealing either that the security requirement cannot be satisfied in the context or that the context does not contain sufficient information to develop the argument. In this case, designers and architects are asked to provide additional design information to resolve the problems. We evaluate the framework by applying it to a security requirements analysis within an air traffic control technology evaluation project.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Haley, Charles and Laney, Robin and Moffett, Jonathan and Nuseibeh, Bashar},
	month = jan,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Air traffic control, Application software, Computer Society, Computer security, Credit cards, Data security, Information security, Internet, Requirements/Specifications, Security, Software engineering, Software/Software Engineering, Statistics},
	pages = {133--153},
}

@article{kelly_systematic_2004,
	title = {A {Systematic} {Approach} to {Safety} {Case} {Management}},
	volume = {113},
	issn = {0096-736X},
	url = {https://www.jstor.org/stable/44699541},
	abstract = {In Europe, over recent years, there has been a marked shift in the regulatory approach to ensuring system safety. Whereas compliance with prescriptive safety codes and standards was previously the norm, the responsibility has now shifted back onto the developers and operators to construct and present well reasoned arguments that their systems achieve acceptable levels of safety. These arguments (together with supporting evidence) are typically referred to as a "safety case". This paper describes the role and purpose of a safety case (as defined by current safety and regulatory standards). Safety arguments within safety cases are often poorly communicated. This paper presents a technique called GSN (Goal Structuring Notation) that is increasingly being used in safety-critical industries to improve the structure, rigor, and clarity of safety arguments. Based upon the GSN approach, the paper also describes how an evolutionary and systematic approach to safety case construction, in step with system development, can be facilitated.},
	urldate = {2022-07-27},
	journal = {SAE Transactions},
	author = {Kelly, Tim},
	year = {2004},
	note = {Publisher: SAE International},
	pages = {257--266},
}

@inproceedings{franqueira_risk_2011,
	title = {Risk and argument: {A} risk-based argumentation method for practical security},
	shorttitle = {Risk and argument},
	doi = {10.1109/RE.2011.6051659},
	abstract = {When showing that a software system meets certain security requirements, it is often necessary to work with formal and informal descriptions of the system behavior, vulnerabilities, and threats from potential attackers. In earlier work, Haley et al. [1] showed structured argumentation could deal with such mixed descriptions. However, incomplete and uncertain information, and limited resources force practitioners to settle for good-enough security. To deal with these conditions of practice, we extend the method of Haley et al. with risk assessment. The proposed method, RISA (RIsk assessment in Security Argumentation), uses public catalogs of security expertise to support the risk assessment, and to guide the security argumentation in identifying rebuttals and mitigations for security requirements satisfaction. We illustrate RISA with a realistic example of PIN Entry Device.},
	booktitle = {2011 {IEEE} 19th {International} {Requirements} {Engineering} {Conference}},
	author = {Franqueira, Virginia N. L. and Tun, Thein Than and Yu, Yijun and Wieringa, Roel and Nuseibeh, Bashar},
	month = aug,
	year = {2011},
	note = {ISSN: 2332-6441},
	keywords = {Argumentation, Catalogs, Common Attack Pattern Enumeration and Classification (CAPEC), Common Weakness Enumeration (CWE), Context, Cryptography, Risk Assessment, Risk management, Security Requirements, Software, Systematics},
	pages = {239--248},
}

@article{krigsman_annual_2009,
	title = {Annual cost of {IT} failure: \$6.2 trillion},
	url = {https://www.zdnet.com/article/annual-cost-of-it-failure-6-2-trillion/},
	journal = {ZDNet},
	author = {Krigsman, Michael},
	year = {2009},
}

@techreport{noauthor_notitle_nodate,
}

@article{kelly_systematic_2004-1,
	title = {A {Systematic} {Approach} to {Safety} {Case} {Management}},
	volume = {113},
	issn = {0096-736X},
	url = {https://www.jstor.org/stable/44699541},
	abstract = {In Europe, over recent years, there has been a marked shift in the regulatory approach to ensuring system safety. Whereas compliance with prescriptive safety codes and standards was previously the norm, the responsibility has now shifted back onto the developers and operators to construct and present well reasoned arguments that their systems achieve acceptable levels of safety. These arguments (together with supporting evidence) are typically referred to as a "safety case". This paper describes the role and purpose of a safety case (as defined by current safety and regulatory standards). Safety arguments within safety cases are often poorly communicated. This paper presents a technique called GSN (Goal Structuring Notation) that is increasingly being used in safety-critical industries to improve the structure, rigor, and clarity of safety arguments. Based upon the GSN approach, the paper also describes how an evolutionary and systematic approach to safety case construction, in step with system development, can be facilitated.},
	urldate = {2022-07-27},
	journal = {SAE Transactions},
	author = {Kelly, Tim},
	year = {2004},
	note = {Publisher: SAE International},
	pages = {257--266},
}

@book{jurafsky2000speech,
	title = {Speech \& language processing},
	publisher = {Pearson Education India},
	author = {Jurafsky, Dan},
	year = {2000},
}

@misc{noauthor_csrb_nodate,
	title = {{CSRB} {\textbar} {CISA}},
	url = {https://www.cisa.gov/cyber-safety-review-board},
	urldate = {2022-07-27},
}

@article{carroll_making_2015,
	title = {Making {Sense} of {Ambiguity} through {Dialogue} and {Collaborative} {Action}},
	volume = {23},
	issn = {09660879},
	shorttitle = {Making {Sense} of {Ambiguity} through {Dialogue} and {Collaborative} {Action}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1468-5973.12075},
	doi = {10.1111/1468-5973.12075},
	language = {en},
	number = {2},
	urldate = {2022-05-23},
	journal = {Journal of Contingencies and Crisis Management},
	author = {Carroll, John S.},
	month = jun,
	year = {2015},
	pages = {59--65},
}

@misc{noauthor_azure_nodate,
	title = {Azure status history {\textbar} {Microsoft} {Azure}},
	url = {https://status.azure.com/en-us/status/history/},
	urldate = {2022-07-27},
}

@article{booch1996unified,
	title = {The unified modeling language},
	volume = {14},
	number = {13},
	journal = {Unix Review},
	author = {Booch, Grady and Jacobson, Ivar and Rumbaugh, James and {others}},
	year = {1996},
	pages = {5},
}

@inproceedings{montes_discrepancies_2022,
	title = {Discrepancies among {Pre}-trained {Deep} {Neural} {Networks}: {A} {New} {Threat} to {Model} {Zoo} {Reliability}},
	booktitle = {{ESEC}/{FSE}-{IVR}},
	author = {Montes, Diego and Pongpatapee, Peerapatanapokin and Schultz, Jeff and Guo, Chengjun and Jiang, Wenxin and Davis, James},
	year = {2022},
}

@misc{noauthor_google_nodate,
	title = {Google {Cloud} {Service} {Health}},
	url = {https://status.cloud.google.com/summary},
	urldate = {2022-07-27},
}

@inproceedings{amusuo2022SoftwareFailureAnalysis,
	title = {Reflections on software failure analysis},
	booktitle = {{ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} — {Ideas}, {Visions}, and {Reflections} track ({ESEC}/{FSE}-{IVR})},
	author = {Amusuo, Paschal and Sharma, Aishwarya and Rao, Siddharth R and Vincent, Abbey and Davis, James C},
	year = {2022},
}

@techreport{anandayuvaraj2022FailureAwareSDLC,
	title = {Towards a failure-aware {SDLC} for internet of things},
	url = {https://arxiv.org/abs/2206.13562},
	author = {Anandayuvaraj, Dharun and Thulluri, Pujita and Figueroa, Jutin and Shandilya, Harshit and Davis, James C},
	year = {2022},
}

@article{birks2008memoing,
	title = {Memoing in qualitative research: {Probing} data and processes},
	volume = {13},
	number = {1},
	journal = {Journal of research in nursing},
	author = {Birks, Melanie and Chapman, Ysanne and Francis, Karen},
	year = {2008},
	note = {Publisher: Sage Publications Sage UK: London, England},
	pages = {68--75},
}

@article{holton2007coding,
	title = {The coding process and its challenges},
	volume = {3},
	journal = {The Sage handbook of grounded theory},
	author = {Holton, Judith A},
	year = {2007},
	pages = {265--289},
}

@article{landis1977application,
	title = {An application of hierarchical kappa-type statistics in the assessment of majority agreement among multiple observers},
	journal = {Biometrics. Journal of the International Biometric Society},
	author = {Landis, J Richard and Koch, Gary G},
	year = {1977},
	note = {Publisher: JSTOR},
	pages = {363--374},
}

@article{cohen1960coefficient,
	title = {A coefficient of agreement for nominal scales},
	volume = {20},
	number = {1},
	journal = {Educational and psychological measurement},
	author = {Cohen, Jacob},
	year = {1960},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {37--46},
}

@article{jaradat2015complex,
	title = {Complex system governance requires systems thinking-how to find systems thinkers},
	volume = {6},
	number = {1-2},
	journal = {International Journal of System of Systems Engineering},
	author = {Jaradat, Raed M},
	year = {2015},
	note = {Publisher: Inderscience Publishers},
	pages = {53--70},
}

@article{davis2020lake,
	title = {The {Lake} {Urmia} vignette: a tool to assess understanding of complexity in socio-environmental systems},
	volume = {36},
	number = {2},
	journal = {System Dynamics Review},
	author = {Davis, Kirsten and Ghaffarzadegan, Navid and Grohs, Jacob and Grote, Dustin and Hosseinichimeh, Niyousha and Knight, David and Mahmoudi, Hesam and Triantis, Konstantinos},
	year = {2020},
	note = {Publisher: Wiley Online Library},
	pages = {191--222},
}

@book{burns_autonomy_2018,
	edition = {1},
	title = {Autonomy: {The} {Quest} to {Build} the {Driverless} {Car}―{And} {How} {It} {Will} {Reshape} {Our} {World}},
	isbn = {978-0-06-266112-8},
	language = {English},
	publisher = {Ecco},
	author = {Burns, Lawrence and Shulgan, Christopher},
	year = {2018},
}

@article{christensen_risk_2003,
	title = {Risk terminology—a platform for common understanding and better communication},
	volume = {103},
	issn = {0304-3894},
	url = {https://www.sciencedirect.com/science/article/pii/S0304389403000396},
	doi = {10.1016/S0304-3894(03)00039-6},
	abstract = {The sciences analyzing and describing risks are relatively new and developing, and the associated terminologies are developing as well. This has led to ambiguity in the use of terms, both between different risk sciences and between the different parties involved in risk debates. Only recently, major vocabularies have been compiled by authoritative agencies. Some of these vocabularies are examined and explained based on a division into fundamental and action oriented risk terms. Fundamental terms are associated with description and characterization of the chemical, biological and physical processes leading from risk source(s) to possible consequences/effects. The approach to these terms is based on a cause–effect skeleton. The action oriented terms cover administrative, scientific, sociological, etc. processes associated with the work of identifying, characterizing, regulating and communicating risks in the society, and their internal connection and iterative character have been illustrated. Focus is laid on engineering and toxicological risks, but to some extent, the thoughts presented may be extrapolated to other areas. Differences in applied terminology probably cannot be eliminated, but they can be identified and clarified for better understanding. With the present paper, the authors hope to contribute to reducing the probability of derailing risk discussions from the risk issue itself.},
	language = {en},
	number = {3},
	urldate = {2022-07-25},
	journal = {Journal of Hazardous Materials},
	author = {Christensen, Frans Møller and Andersen, Ole and Duijm, Nijs Jan and Harremoës, Poul},
	month = oct,
	year = {2003},
	keywords = {Cause–effect, Risk analysis, Risk assessment, Risk management, Risk terminology},
	pages = {181--203},
}

@inproceedings{basili_lessons_2002,
	address = {Orlando, FL, USA},
	title = {Lessons learned from 25 years of process improvement: the rise and fall of the {NASA} software engineering laboratory},
	isbn = {978-1-58113-472-8},
	shorttitle = {Lessons learned from 25 years of process improvement},
	url = {https://ieeexplore.ieee.org/document/1007957},
	doi = {10.1109/ICSE.2002.1007957},
	abstract = {For 25 years the NASA/GSFC Software Engineering Laboratory (SEL) has been a major resource in software process improvement activities. But due to a changing climate at NASA, agency reorganization, and budget cuts, the SEL has lost much of its impact. In this paper we describe the history of the SEL and give some lessons learned on what we did right, what we did wrong, and what others can learn from our experiences. We briefly describe the research that was conducted by the SEL, describe how we evolved our understanding of software process improvement, and provide a set of lessons learned and hypotheses that should enable future groups to learn from and improve on our quarter century of experiences.},
	language = {en},
	urldate = {2022-07-25},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Software} {Engineering}. {ICSE} 2002},
	publisher = {ACM},
	author = {Basili, V.R. and McGarry, F.E. and Pajerski, R. and Zelkowitz, M.V.},
	year = {2002},
	pages = {69--79},
}

@article{money_understanding_2021,
	title = {Understanding {Failed} {Software} {Projects} through {Forensic} {Analysis}},
	issn = {0887-4417, 2380-2057},
	url = {https://www.tandfonline.com/doi/full/10.1080/08874417.2021.1950076},
	doi = {10.1080/08874417.2021.1950076},
	abstract = {Software project failure has cost trillions of dollars over the past fifty years. The many reasons for failure are widely discussed in the project management and software development literatures. This paper assesses methods for software project forensic analysis to develop principles and practices that would provide a comprehensive and more complete understanding of the why, when, where, how, and what causes software project failure. This paper discusses the approaches followed to determine the causes of failures, examines the issues and challenges of assessing failures, and proposes a structured approach (forensic analysis) for determining the causes of software project failures.},
	language = {en},
	urldate = {2022-07-24},
	journal = {Journal of Computer Information Systems},
	author = {Money, William H. and Kaisler, Stephen H. and Cohen, Stephen J.},
	month = jul,
	year = {2021},
	pages = {1--14},
}

@article{chillarege1992orthogonal,
	title = {Orthogonal defect classification-a concept for in-process measurements},
	volume = {18},
	number = {11},
	journal = {IEEE Transactions on software Engineering},
	author = {Chillarege, Ram and Bhandari, Inderpal S and Chaar, Jarir K and Halliday, Michael J and Moebus, Diane S and Ray, Bonnie K and Wong, Man-Yuen},
	year = {1992},
	pages = {943--956},
}

@inproceedings{seaman2008defect,
	title = {Defect categorization: making use of a decade of widely varying historical data},
	booktitle = {Proceedings of the {Second} {ACM}-{IEEE} international symposium on {Empirical} software engineering and measurement},
	author = {Seaman, Carolyn B and Shull, Forrest and Regardie, Myrna and Elbert, Denis and Feldmann, Raimund L and Guo, Yuepu and Godfrey, Sally},
	year = {2008},
	pages = {149--157},
}

@phdthesis{stringfellow2010accident,
	title = {Accident analysis and hazard analysis for human and organizational factors},
	school = {Massachusetts Institute of Technology},
	author = {Stringfellow, Margaret Virgina},
	year = {2010},
}

@article{nelson2008stamp,
	title = {A {STAMP} analysis of the {LEX} {COMAIR} 5191 accident},
	journal = {Master's thesis, Lund},
	author = {Nelson, Paul S},
	year = {2008},
	note = {Publisher: Citeseer},
}

@article{arnold2009qualitative,
	title = {A qualitative comparative analysis of {SOAM} and {STAMP} in {ATM} occurrence investigation},
	journal = {Master's Thesis, Lund University, Lund, Sweden},
	author = {Arnold, Richard},
	year = {2009},
}

@book{ohno1988toyota,
	title = {Toyota production system: beyond large-scale production},
	publisher = {Productivity press},
	author = {Ohno, Taiichi and Bodek, Norman},
	year = {1988},
}

@techreport{ellison_evaluating_2010,
	title = {Evaluating and {Mitigating} {Software} {Supply} {Chain} {Security} {Risks}},
	url = {https://apps.dtic.mil/sti/citations/ADA522538},
	abstract = {The Department of Defense DoD is concerned that security vulnerabilities could be inserted into software that has been developed outside of the DoDs supervision or control. This report presents an initial analysis of how to evaluate and mitigate the risk that such unauthorized insertions have been made. The analysis is structured in terms of actions that should be taken in each phase of the DoD acquisition life cycle},
	language = {en},
	urldate = {2022-06-21},
	institution = {CARNEGIE-MELLON UNIV PITTSBURGH PA SOFTWARE ENGINEERING INST},
	author = {Ellison, Robert J. and Goodenough, John B. and Weinstock, Charles B. and Woody, Carol},
	month = may,
	year = {2010},
	note = {Section: Technical Reports},
	keywords = {Government},
}

@techreport{boyens_cybersecurity_2022,
	title = {Cybersecurity {Supply} {Chain} {Risk} {Management} {Practices} for {Systems} and {Organizations}},
	url = {https://csrc.nist.gov/publications/detail/sp/800-161/rev-1/final},
	abstract = {Organizations are concerned about the risks associated with products and services that may potentially contain malicious functionality, are counterfeit, or are vulnerable due to poor manufacturing and development practices within the supply chain. These risks are associated with an enterprise’s decreased visibility into and understanding of how the technology they acquire is developed, integrated, and deployed or the processes, procedures, standards, and practices used to ensure the security, resilience, reliability, safety, integrity, and quality of the products and services.  This publication provides guidance to organizations on identifying, assessing, and mitigating cybersecurity risks throughout the supply chain at all levels of their organizations. The publication integrates cybersecurity supply chain risk management (C-SCRM) into risk management activities by applying a multilevel, C-SCRM-specific approach, including guidance on the development of C-SCRM strategy implementation...},
	language = {en},
	number = {NIST Special Publication (SP) 800-161 Rev. 1},
	urldate = {2022-06-16},
	institution = {National Institute of Standards and Technology},
	author = {Boyens, Jon and Smith, Angela and Bartol, Nadya and Winkler, Kris and Holbrook, Alex and Fallon, Matthew},
	month = may,
	year = {2022},
	doi = {10.6028/NIST.SP.800-161r1},
	keywords = {Government},
}

@incollection{Leveson2009SWSystemSafety,
	title = {Software {System} {Safety}},
	booktitle = {Safety {Design} for {Space} {Systems}},
	publisher = {Elsevier},
	author = {Leveson, Nancy G. and Weiss, Kathryn Anne},
	year = {2009},
}

@misc{Hata2022SWSupplyChainMapHowReuseNetworksExpand,
	title = {Software {Supply} {Chain} {Map}: {How} {Reuse} {Networks} {Expand}},
	url = {http://arxiv.org/abs/2204.06531},
	abstract = {Clone-and-own is a typical code reuse approach because of its simplicity and efficiency. Cloned software components are maintained independently by a new owner. These clone-and-own operations can be occurred sequentially, that is, cloned components can be cloned again and owned by other new owners on the supply chain. In general, code reuse is not documented well, consequently, appropriate changes like security patches cannot be propagated to descendant software projects. However, the OpenChain Project defined identifying and tracking source code reuses as responsibilities of FLOSS software staffs. Hence supporting source code reuse awareness is in a real need. This paper studies software reuse relations in FLOSS ecosystem. Technically, clone-and-own reuses of source code can be identified by file-level clone set detection. Since change histories are associated with files, we can determine origins and destinations in reusing across multiple software by considering times. By building software supply chain maps, we find that clone-and-own is prevalent in FLOSS development, and set of files are reused widely and repeatedly. These observations open up future challenges of maintaining and tracking global software genealogies.},
	publisher = {arXiv},
	author = {Hata, Hideaki and Ishio, Takashi},
	year = {2022},
	keywords = {Computer Science - Software Engineering},
}

@misc{hata_software_2022,
	title = {Software {Supply} {Chain} {Map}: {How} {Reuse} {Networks} {Expand}},
	shorttitle = {Software {Supply} {Chain} {Map}},
	url = {http://arxiv.org/abs/2204.06531},
	abstract = {Clone-and-own is a typical code reuse approach because of its simplicity and efficiency. Cloned software components are maintained independently by a new owner. These clone-and-own operations can be occurred sequentially, that is, cloned components can be cloned again and owned by other new owners on the supply chain. In general, code reuse is not documented well, consequently, appropriate changes like security patches cannot be propagated to descendant software projects. However, the OpenChain Project defined identifying and tracking source code reuses as responsibilities of FLOSS software staffs. Hence supporting source code reuse awareness is in a real need. This paper studies software reuse relations in FLOSS ecosystem. Technically, clone-and-own reuses of source code can be identified by file-level clone set detection. Since change histories are associated with files, we can determine origins and destinations in reusing across multiple software by considering times. By building software supply chain maps, we find that clone-and-own is prevalent in FLOSS development, and set of files are reused widely and repeatedly. These observations open up future challenges of maintaining and tracking global software genealogies.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Hata, Hideaki and Ishio, Takashi},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06531 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{aldiabat_data_2018,
	title = {Data {Saturation}: {The} {Mysterious} {Step} {In} {Grounded} {Theory} {Method}},
	issn = {2160-3715, 1052-0147},
	shorttitle = {Data {Saturation}},
	url = {https://nsuworks.nova.edu/tqr/vol23/iss1/18/},
	doi = {10.46743/2160-3715/2018.2994},
	abstract = {The aim of this paper is to provide a discussion that is broad in both depth and breadth, about the concept of data saturation in Grounded Theory. It is expected that this knowledge will provide a helpful resource for (a) the novice researcher using a Grounded Theory approach, or for (b) graduate students currently enrolled in a qualitative research course, and for (c) instructors who teach or supervise qualitative research projects. The following topics are discussed in this paper: (1) definition of data saturation in Grounded Theory (GT); (2) factors pertaining to data saturation; (3) factors that hinder data saturation; (4) the relationship between theoretical sampling and data saturation; (5) the relationship between constant comparative and data saturation; and (6) illustrative examples of strategies used during data collection to maximize the components of rigor that Yonge and Stewin (1988) described as Credibility, Transferability or Fittingness, Dependability or Auditability, and Confirmability.},
	language = {en},
	urldate = {2022-07-21},
	journal = {The Qualitative Report},
	author = {Aldiabat, Khaldoun and Le Navenec, Carole-Lynne},
	month = jan,
	year = {2018},
}

@misc{allspaw_blameless_2012,
	title = {Blameless {PostMortems} and a {Just} {Culture}},
	url = {https://www.etsy.com/codeascraft/blameless-postmortems},
	urldate = {2022-07-20},
	author = {Allspaw, John},
	year = {2012},
}

@article{kuhn2010practical,
	title = {Practical combinatorial testing},
	volume = {800},
	number = {142},
	journal = {NIST special Publication},
	author = {Kuhn, D Richard and Kacker, Raghu N and Lei, Yu and {others}},
	year = {2010},
	pages = {142},
}

@book{rosenthal2020chaos,
	title = {Chaos engineering: system resiliency in practice},
	publisher = {O'Reilly Media},
	author = {Rosenthal, Casey and Jones, Nora},
	year = {2020},
}

@article{riesener_derivation_2020,
	title = {Derivation of description features for engineering change request by aid of latent dirichlet allocation},
	volume = {1},
	issn = {2633-7762},
	url = {https://www.cambridge.org/core/journals/proceedings-of-the-design-society-design-conference/article/derivation-of-description-features-for-engineering-change-request-by-aid-of-latent-dirichlet-allocation/810E17C0CF8762F28069F43AD7692E50},
	doi = {10.1017/dsd.2020.98},
	abstract = {Complex products and shorter development cycles lead to an increasing number of engineering changes. In order to be able to process these changes more effectively and efficiently, this paper develops a description model as a first step towards a data driven approach of processing engineering change requests. The description model is systematically derived from literature using text mining and natural language processing techniques. An example of the application is given by an automated classification based on similarity calculations between new and historic engineering change requests.},
	language = {en},
	urldate = {2022-07-20},
	journal = {Proceedings of the Design Society: DESIGN Conference},
	author = {Riesener, M. and Dölle, C. and Mendl-Heinisch, M. and Schuh, G. and Keuper, A.},
	month = may,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	keywords = {automated classification, data mining, engineering change, latent dirichlet allocation, product development},
	pages = {697--706},
}

@misc{Aryal2022SurveyonAdvAttacks4MalwareAnalysis,
	title = {A {Survey} on {Adversarial} {Attacks} for {Malware} {Analysis}},
	url = {http://arxiv.org/abs/2111.08223},
	abstract = {Machine learning has witnessed tremendous growth in its adoption and advancement in the last decade. The evolution of machine learning from traditional algorithms to modern deep learning architectures has shaped the way today's technology functions. Its unprecedented ability to discover knowledge/patterns from unstructured data and automate the decision-making process led to its application in wide domains. High flying machine learning arena has been recently pegged back by the introduction of adversarial attacks. Adversaries are able to modify data, maximizing the classification error of the models. The discovery of blind spots in machine learning models has been exploited by adversarial attackers by generating subtle intentional perturbations in test samples. Increasing dependency on data has paved the blueprint for ever-high incentives to camouflage machine learning models. To cope with probable catastrophic consequences in the future, continuous research is required to find vulnerabilities in form of adversarial and design remedies in systems. This survey aims at providing the encyclopedic introduction to adversarial attacks that are carried out against malware detection systems. The paper will introduce various machine learning techniques used to generate adversarial and explain the structure of target files. The survey will also model the threat posed by the adversary and followed by brief descriptions of widely accepted adversarial algorithms. Work will provide a taxonomy of adversarial evasion attacks on the basis of attack domain and adversarial generation techniques. Adversarial evasion attacks carried out against malware detectors will be discussed briefly under each taxonomical headings and compared with concomitant researches. Analyzing the current research challenges in an adversarial generation, the survey will conclude by pinpointing the open future research directions.},
	publisher = {arXiv},
	author = {Aryal, Kshitiz and Gupta, Maanak and Abdelsalam, Mahmoud},
	year = {2022},
	keywords = {Computer Science - Cryptography and Security},
}

@article{hosny_modelhubai_nodate,
	title = {{ModelHub}.{AI}: {Dissemination} {Platform} for {Deep} {Learning} {Models}},
	abstract = {Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present ModelHub.AI (www.modelhub.ai), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on ModelHub.AI and allows both researchers and developers to utilize models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.},
	language = {en},
	author = {Hosny, Ahmed and Schwier, Michael and Berger, Christoph and Örnek, Evin P and Turan, Mehmet and Tran, Phi V and Isensee, Fabian and Maier-Hein, Klaus H and McKinley, Richard and Lu, Michael T and Hoffmann, Udo and Menze, Bjoern and Bakas, Spyridon and Fedorov, Andriy and Aerts, Hugo JWL},
	pages = {12},
}

@techreport{graham-cumming_details_2019,
	title = {Details of the {Cloudflare} outage on {July} 2, 2019},
	url = {https://web.archive.org/web/20190712160002/https://blog.cloudflare.com/ details-of-the-cloudflare-outage-on-july-2-2019/},
	author = {Graham-Cumming, John},
	year = {2019},
}

@article{kelly_dexcom_2019,
	title = {Dexcom {CEO} on remote monitoring outage: ‘{No} excuses. {We} can do better’},
	url = {https://www.medtechdive.com/news/dexcom-ceo-on-remote-monitoring-outage-no-excuses-we-can-do-better/569122/},
	journal = {MedTechDive},
	author = {Kelly, Susan},
	year = {2019},
}

@misc{dexcom_dexcom_2019,
	title = {Dexcom {Status} {Updates}},
	author = {Dexcom},
	year = {2019},
}

@techreport{nehman_cloudflares_2018,
	title = {Cloudflare’s plan to protect the whole internet comes into focus},
	url = {https://www.wired.com/story/cloudflare-spectrum-iot-protection/},
	author = {Nehman, Lily},
	year = {2018},
}

@article{abdul2019comprehensive,
	title = {A comprehensive study of security and privacy guidelines, threats, and countermeasures: {An} {IoT} perspective},
	volume = {8},
	number = {2},
	journal = {Journal of Sensor and Actuator Networks},
	author = {Abdul-Ghani, Hezam Akram and Konstantas, Dimitri},
	year = {2019},
	note = {Publisher: MDPI},
	pages = {22},
}

@article{ayewah_using_2008,
	title = {Using {Static} {Analysis} to {Find} {Bugs}},
	volume = {25},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/4602670/},
	doi = {10.1109/MS.2008.130},
	language = {en},
	number = {5},
	urldate = {2022-07-19},
	journal = {IEEE Software},
	author = {Ayewah, Nathaniel and Pugh, William and Hovemeyer, David and Morgenthaler, J. David and Penix, John},
	month = sep,
	year = {2008},
	pages = {22--29},
}

@inproceedings{johnson_why_2013,
	address = {San Francisco, CA, USA},
	title = {Why don't software developers use static analysis tools to find bugs?},
	isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
	url = {http://ieeexplore.ieee.org/document/6606613/},
	doi = {10.1109/ICSE.2013.6606613},
	abstract = {Using static analysis tools for automating code inspections can be beneﬁcial for software engineers. Such tools can make ﬁnding bugs, or software defects, faster and cheaper than manual inspections. Despite the beneﬁts of using static analysis tools to ﬁnd bugs, research suggests that these tools are underused. In this paper, we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneﬁcial, false positives and the way in which the warnings are presented, among other things, are barriers to use. We discuss several implications of these results, such as the need for an interactive mechanism to help developers ﬁx defects.},
	language = {en},
	urldate = {2022-07-19},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
	month = may,
	year = {2013},
	pages = {672--681},
}

@article{cryst_introducing_2021,
	title = {Introducing the {Journal} of {Online} {Trust} and {Safety}},
	volume = {1},
	copyright = {Copyright (c) 2021},
	issn = {2770-3142},
	url = {https://tsjournal.org/index.php/jots/article/view/8},
	abstract = {Introducing the Journal of Online Trust and Safety},
	language = {en},
	number = {1},
	urldate = {2022-07-18},
	journal = {Journal of Online Trust and Safety},
	author = {Cryst, Elena and Grossman, Shelby and Hancock, Jeff and Stamos, Alex and Thiel, David},
	month = oct,
	year = {2021},
	note = {Number: 1},
	keywords = {open access, scholarly publishing, trust and safety},
}

@article{schon1968reflective,
	title = {The reflective practitioner},
	journal = {New York},
	author = {Schön, Donald A},
	year = {1968},
}

@article{ma_business_2007,
	title = {The business model of "{Software}-as-a-{Service}"},
	abstract = {In the recent years, the emergence of the Softwareas-a-Service (SaaS) business model has attracted great attentions from both researchers and practitioners. Under the SaaS business model, vendors deliver on-demand information processing services to user firms, and thus offering computing utility rather than the standalone software itself. The SaaS has become an attractive alternative to the traditional software delivery model, which typically requires users to purchase, install, and maintain software systems by themselves. In this work, we propose an analytical model to study the competition between the SaaS and the traditional COTS (Commercial off-the-shelf) solution for software applications. The competitive model considers heterogeneous users who differ in terms of their transaction volume, while the SaaS and COTS vendors differ in terms of their pricing structure, setup cost, and system customization. We conclude that when commercial software becomes more open, modulated, and standardized, the SaaS business model will take a significant market share. In the extreme case, it may dominate the whole software industry and drives the traditional software out of the market. We also show that it is never optimal for the SaaS vendors to exert their full lock-in power through harsh software contracts. Under certain conditions, we suggest SaaS vendors to offer their existing users an easy exit option rather than to establish switching barriers to lock them in.},
	language = {en},
	journal = {IEEE international conference on services computing},
	author = {Ma, Dan},
	year = {2007},
	pages = {9},
}

@book{brooks1995mythical,
	title = {The mythical man-month: essays on software engineering},
	publisher = {Pearson Education},
	author = {Brooks Jr, Frederick P},
	year = {1995},
}

@inproceedings{menzies_automated_2008,
	address = {Beijing, China},
	title = {Automated severity assessment of software defect reports},
	isbn = {978-1-4244-2613-3},
	url = {http://ieeexplore.ieee.org/document/4658083/},
	doi = {10.1109/ICSM.2008.4658083},
	abstract = {In mission critical systems, such as those developed by NASA, it is very important that the test engineers properly recognize the severity of each issue they identify during testing. Proper severity assessment is essential for appropriate resource allocation and planning for fixing activities and additional testing. Severity assessment is strongly influenced by the experience of the test engineers and by the time they spend on each issue.},
	language = {en},
	urldate = {2022-07-18},
	booktitle = {2008 {IEEE} {International} {Conference} on {Software} {Maintenance}},
	publisher = {IEEE},
	author = {Menzies, Tim and Marcus, Andrian},
	month = sep,
	year = {2008},
	pages = {346--355},
}

@inproceedings{eckhardt_are_2016,
	address = {Austin Texas},
	title = {Are "non-functional" requirements really non-functional?: an investigation of non-functional requirements in practice},
	isbn = {978-1-4503-3900-1},
	shorttitle = {Are "non-functional" requirements really non-functional?},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884788},
	doi = {10.1145/2884781.2884788},
	abstract = {Non-functional requirements (NFRs) are commonly distinguished from functional requirements by diﬀerentiating how the system shall do something in contrast to what the system shall do. This distinction is not only prevalent in research, but also inﬂuences how requirements are handled in practice. NFRs are usually documented separately from functional requirements, without quantitative measures, and with relatively vague descriptions. As a result, they remain diﬃcult to analyze and test. Several authors argue, however, that many so-called NFRs actually describe behavioral properties and may be treated the same way as functional requirements. In this paper, we empirically investigate this point of view and aim to increase our understanding on the nature of NFRs addressing system properties. We report on the classiﬁcation of 530 NFRs extracted from 11 industrial requirements speciﬁcations and analyze to which extent these NFRs describe system behavior. Our results suggest that most “non-functional” requirements are not non-functional as they describe behavior of a system. Consequently, we argue that many so-called NFRs can be handled similarly to functional requirements.},
	language = {en},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Eckhardt, Jonas and Vogelsang, Andreas and Fernández, Daniel Méndez},
	month = may,
	year = {2016},
	pages = {832--842},
}

@inproceedings{crosby2003denial,
	title = {Denial of service via algorithmic complexity attacks},
	booktitle = {12th {USENIX} security symposium ({USENIX} security 03)},
	author = {Crosby, Scott A and Wallach, Dan S},
	year = {2003},
}

@misc{rungeler_integration_2015,
	title = {Integration of the {Packetdrill} {Testing} {Tool} in {INET}},
	url = {http://arxiv.org/abs/1509.03127},
	abstract = {Google released in 2013 a script-based tool called packetdrill, which allows to test transport protocols like UDP and TCP on Linux and BSD-based operating systems. The scripts deﬁning a test-case allow to inject packets to the implementation under test, perform operations at the API controlling the transport protocol and verify the sending of packets, all at speciﬁed times. This paper describes a port of packetdrill to the INET framework for the OMNeT++ simulation environment providing a simple and powerful method of testing the transport protocols implemented in INET.},
	language = {en},
	urldate = {2022-07-17},
	publisher = {arXiv},
	author = {Rüngeler, Irene and Tüxen, Michael},
	month = sep,
	year = {2015},
	note = {Number: arXiv:1509.03127
arXiv:1509.03127 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture, Computer Science - Performance},
}

@inproceedings{goel_directed_2022,
	title = {Directed {Acyclic} {Graph}-based {Neural} {Networks} for {Tunable} {Low}-{Power} {Computer} {Vision}},
	abstract = {Processing visual data on mobile devices has many applications, e.g., emergency response and tracking. State-of-the-art computer vision techniques rely on large Deep Neural Networks (DNNs) that are usually too power-hungry to be deployed on resource-constrained edge devices. Many techniques improve DNN efficiency of DNNs by compromising accuracy. However, the accuracy and efficiency of these techniques cannot be adapted for diverse edge applications with different hardware constraints and accuracy requirements. This paper demonstrates that a recent, efficient tree-based DNN architecture, called the hierarchical DNN, can be converted into a Directed Acyclic Graph-based (DAG) architecture to provide tunable accuracy-efficiency tradeoff options. We propose a systematic method that identifies the connections that must be added to convert the tree to a DAG to improve accuracy. We conduct experiments on popular edge devices and show that increasing the connectivity of the DAG improves the accuracy to within 1\% of the existing high accuracy techniques. Our approach requires 93\% less memory, 43\% less energy, and 49\% fewer operations than the high accuracy techniques, thus providing more accuracy-efficiency configurations.},
	language = {en},
	booktitle = {{ACM}/{IEEE} international symposium on low power electronics and design ({ISLPED})},
	author = {Goel, Abhinav and Tung, Caleb and Eliopoulos, Nick and Hu, Xiao and Thiruvathukal, George K and Davis, James C and Lu, Yung-Hsiang},
	year = {2022},
	pages = {6},
}

@inproceedings{synovic_snapshot_2022,
	title = {Snapshot {Metrics} {Are} {Not} {Enough}: {Analyzing} {Software} {Repositories} with {Longitudinal} {Metrics}},
	abstract = {Software metrics capture information about software development processes and products. These metrics support decision-making, e.g., in team management or dependency selection. However, existing metrics tools measure only a snapshot of a software project. Little attention has been given to enabling engineers to reason about metric trends over time—longitudinal metrics that give insight about process, not just product. In this work, we present PRIME (PRocess MEtrics), a tool for computing and visualizing process metrics. The currently-supported metrics include productivity, issue density, issue spoilage, and bus factor. We illustrate the value of longitudinal data and conclude with a research agenda. The tool’s demo video can be watched at https://youtu.be/YigEHy3 JCo. The source code can be found at github.com/SoftwareSystemsLaboratory/prime.},
	language = {en},
	booktitle = {{ASE}-{Tools}},
	author = {Synovic, Nicholas and Hyatt, Matt and Sethi, Rohan and Thota, Sohini and Miller, Allan J and Jiang, Wenxin and Amobi, Emmanuel S and Pinderski, Austin and Laufer, Konstantin and Hayward, Nicholas J and Klingensmith, Neil and Davis, James C and Thiruvathukal, George K},
	year = {2022},
}

@article{runeson_guidelines_2009,
	title = {Guidelines for conducting and reporting case study research in software engineering},
	volume = {14},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-008-9102-8},
	doi = {10.1007/s10664-008-9102-8},
	abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
	language = {en},
	number = {2},
	urldate = {2022-07-13},
	journal = {Empirical Software Engineering},
	author = {Runeson, Per and Höst, Martin},
	month = apr,
	year = {2009},
	pages = {131--164},
}

@misc{paul_graham_paulg_hypothesis_2017,
	type = {Tweet},
	title = {A hypothesis about how you might discover the next {Twitter}. https://t.co/{ral5ns6S7p}},
	url = {https://mobile.twitter.com/paulg/status/898919987657805825},
	language = {en},
	urldate = {2022-07-13},
	journal = {Twitter},
	author = {{Paul Graham [@paulg]}},
	month = aug,
	year = {2017},
}

@inproceedings{anandayuvaraj_reflecting_2022,
	title = {Reflecting on {Recurring} {Failures} in {IoT} {Development}},
	abstract = {As IoT systems are given more responsibility and autonomy, they offer greater benefits, but also carry greater risks. We believe this trend invigorates an old challenge of software engineering: how to develop high-risk software-intensive systems safely and securely under market pressures? As a first step, we conducted a systematic analysis of recent IoT failures to identify engineering challenges. We collected and analyzed 22 news reports and studied the sources, impacts, and repair strategies of failures in IoT systems. We observed failure trends both within and across application domains. We also observed that failure themes have persisted over time. To alleviate these trends, we outline a research agenda toward a Failure-Aware Software Development Life Cycle for IoT development. We propose an encyclopedia of failures and an empirical basis for system postmortems, complemented by appropriate automated tools.},
	language = {en},
	booktitle = {Automated {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results} ({ASE}-{NIER})},
	author = {Anandayuvaraj, Dharun and Davis, James C},
	year = {2022},
	pages = {6},
}

@inproceedings{anvik_are_2020,
	address = {New York, NY, USA},
	title = {Are {Automatic} {Bug} {Report} {Summarizers} {Missing} the {Target}?},
	isbn = {978-1-4503-7963-2},
	url = {https://doi.org/10.1145/3387940.3392236},
	abstract = {Bug reports can be lengthy due to long descriptions and long conversation threads. Automatic summarization of the text in a bug report can reduce the time spent by software project members on understanding the content of a bug report. Quality of the bug report summaries have been historically evaluated using human-created gold-standard summaries. However, we believe this is not a good practice for two reasons. First, we observed high disagreement levels in the annotated summaries and the number of annotators to create gold-standard summaries was lower than the established value for stable annotation. We believe that creating a fixed summary length of 25\% of the word count of the corresponding bug report is not suitable for every time when a person refers to a bug report. Therefore, we propose an automatic sentence annotation method and an interface to customize the presented summary.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {Association for Computing Machinery},
	author = {Anvik, John and Galappaththi, Akalanka},
	month = jun,
	year = {2020},
	keywords = {bug report summarization, gold-standard summary, human annotation, software bug reports},
	pages = {149--152},
}

@article{nadia_engineering_2006,
	title = {Engineering change request management in a new product development process},
	volume = {9},
	issn = {1460-1060},
	url = {https://doi.org/10.1108/14601060610639999},
	doi = {10.1108/14601060610639999},
	abstract = {Purpose – The objective of this research was to compare the behavior of two methods of managing an engineering change request (ECR) process, namely, perform changes as they occur or in a batch. Design/methodology/approach – This comparison was accomplished by creating a computer model of a new product development (NPD) process and simulating ECR management. The model connects process design and process characteristics (teamwork, parallel activities) to process outcomes (development time, effort). The first method executes the ECR promptly and the rework is done as soon as the ECR is initiated. In the second method, ECRs are batched; in other words, a number of them are accumulated, and processing of the ECRs takes place when a batch of a certain size has accumulated. Thus, the change requests are grouped into a batch, and then, the section(s) of the process to effect the change(s) is (are) reworked. Findings – Batching ECRs was found to be superior to doing them one at a time. Research limitations/implications – Future work should focus on refining the computer model and differentiating ECRs by assigning priorities to incoming ECRs. Practical implications – For product development managers, processing ECRs in batches is preferable than attending to them on an individual basis. Nevertheless, in some situations ECRs require immediate attention. A mechanism will always be needed to deal with situations directly. Also, in terms of batching, ECRs could be processed in groups on a periodic basis. Periodically performing ECRs due to new design versions or prototypes in a timely manner is a good compromise between a random batch mode and doing them individually. Originality/value – The paper shows that batch processing is superior to executing ECRs promptly as they are received. This result has been shown through the use of a computer model of NPD. To the authors' knowledge, no other studies have used computer modeling to study this problem.},
	number = {1},
	urldate = {2022-07-12},
	journal = {European Journal of Innovation Management},
	author = {Nadia, Bhuiyan and Gregory, Gatard and Vince, Thomson},
	month = jan,
	year = {2006},
	note = {Publisher: Emerald Group Publishing Limited},
	keywords = {Change management, Product development},
	pages = {5--19},
}

@inproceedings{da_cunha_decision-making_2016,
	title = {Decision-{Making} in {Software} {Project} {Management}: {A} {Qualitative} {Case} {Study} of a {Private} {Organization}},
	shorttitle = {Decision-{Making} in {Software} {Project} {Management}},
	doi = {10.1145/2897586.2897598},
	abstract = {Context: In software project management, the decision-making process is a complex set of tasks largely based on specific knowledge and individual cultural background, as well as human relations. The factors that affect the decisions of the software project managers (SPMs) and their potential consequences require attention because project delays and failures are usually related to a series of poor decisions. Objective: To understand how SPMs make decisions based on how they interpret their experiences in the workplace, and also to identify antecedents and consequences of those decisions in order to increase the effectiveness of project management. Method: Semi-structured interviews were carried out with SPMs within a Brazilian large private organization. The data was analyzed using techniques from grounded theory approach. Results: We found that decision-making in software project management is based on knowledge sharing in which the SPM acts as a facilitator before making decisions. This phenomenon is influenced by individual factors, such as experience, communication, negotiation, self-control and systemic view of the project and by contextual factors such as the autonomy of the SPM and team members' technical competence. Also, these factors are mediated by cognitive biases. Conclusions: Due to the uncertainty and dynamism inherent in software projects, the SPMs focus on making, monitoring and adjusting decisions in an argument-driven way.},
	booktitle = {2016 {IEEE}/{ACM} {Cooperative} and {Human} {Aspects} of {Software} {Engineering} ({CHASE})},
	author = {da Cunha, José Adson O.G. and da Silva, Fabio Q. B. and de Moura, Hermano P. and Vasconcellos, Francisco J.S.},
	month = may,
	year = {2016},
	keywords = {Biological system modeling, Context, Decision making, Decision-Making, Grounded Theory, Interviews, Organizations, Project management, Software, Software Project Management},
	pages = {26--32},
}

@misc{zhuang_comprehensive_2020,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	url = {http://arxiv.org/abs/1911.02685},
	doi = {10.48550/arXiv.1911.02685},
	abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jun,
	year = {2020},
	note = {arXiv:1911.02685 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gou_knowledge_2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {http://arxiv.org/abs/2006.05525},
	doi = {10.1007/s11263-021-01453-z},
	abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation eﬀectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are brieﬂy reviewed and comments on future research are discussed and forwarded.},
	language = {en},
	number = {6},
	urldate = {2022-07-11},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen John and Tao, Dacheng},
	month = jun,
	year = {2021},
	note = {arXiv:2006.05525 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1789--1819},
}

@article{microsoft_microsoft_2021,
	title = {Microsoft {Digital} {Defense} {Report} {OCTOBER} 2021},
	url = {https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RWMFIi},
	language = {en},
	author = {Microsoft},
	year = {2021},
	keywords = {Industry},
	pages = {134},
}

@techreport{center_for_security_and_emerging_technology_poison_2021,
	title = {Poison in the {Well}: {Securing} the {Shared} {Resources} of {Machine} {Learning}},
	shorttitle = {Poison in the {Well}},
	url = {https://cset.georgetown.edu/publication/poison-in-the-well/},
	abstract = {Modern machine learning often relies on open-source datasets, pretrained models, and machine learning libraries from across the internet, but are those resources safe to use? Previously successful digital supply chain attacks against cyber infrastructure suggest the answer may be no. This report introduces policymakers to these emerging threats and provides recommendations for how to secure the machine learning supply chain.},
	urldate = {2022-07-08},
	institution = {Center for Security and Emerging Technology},
	author = {{Center for Security and Emerging Technology} and Lohn, Andrew},
	month = jun,
	year = {2021},
	doi = {10.51593/2020CA013},
}

@techreport{technology_minimum_2006,
	title = {Minimum {Security} {Requirements} for {Federal} {Information} and {Information} {Systems}},
	url = {https://csrc.nist.gov/publications/detail/fips/200/final},
	abstract = {FIPS 200 is the second standard that was specified by the Information Technology Management Reform Act of 1996 (FISMA). It is an integral part of the risk management framework that the National Institute of Standards and Technology (NIST) has developed to assist federal agencies in providing levels of information security based on levels of risk. FIPS 200 specifies minimum security requirements for federal information and information systems and a risk-based process for selecting the security controls necessary to satisfy the minimum requirements.},
	language = {en},
	number = {Federal Information Processing Standard (FIPS) 200},
	urldate = {2022-06-23},
	institution = {U.S. Department of Commerce},
	author = {Technology, National Institute of Standards and},
	month = mar,
	year = {2006},
	doi = {10.6028/NIST.FIPS.200},
	keywords = {Government},
}

@techreport{microsoft_3_2021,
	title = {3 ways to mitigate risk when using private package feeds},
	url = {https://azure.microsoft.com/mediahandler/files/resourcefiles/3-ways-to-mitigate-risk-using-private-package-feeds/3%20Ways%20to%20Mitigate%20Risk%20When%20Using%20Private%20Package%20Feeds%20-%20v1.0.pdf},
	abstract = {Software today has become an assembly of components from a wide range of sources. Many organizations use public package feeds to take advantage of the open ecosystems they offer. Projects that consume packages from multiple public and private feeds may be exposed to supply chain vulnerabilities. 

This white paper discusses configurations that can introduce risk in your software supply chain, and how to mitigate these risks.},
	language = {en},
	urldate = {2022-06-21},
	institution = {Microsoft},
	author = {Microsoft},
	month = mar,
	year = {2021},
	keywords = {Industry, White Paper},
	pages = {15},
}

@misc{noauthor_introduction_2022,
	title = {Introduction},
	url = {http://slsa.dev/spec/v0.1/index},
	abstract = {Security framework to ensure software supply chain integrity},
	language = {en},
	urldate = {2022-07-07},
	journal = {SLSA},
	month = jul,
	year = {2022},
}

@techreport{cybersecurity_enisa_2021,
	type = {Report/{Study}},
	title = {{ENISA} {Threat} {Landscape} 2021},
	url = {https://www.enisa.europa.eu/publications/enisa-threat-landscape-2021},
	abstract = {This is the ninth edition of the ENISA Threat Landscape (ETL) report, an annual report on the status of the cybersecurity threat landscape that identifies prime threats, major trends observed with respect to threats, threat actors and attack techniques, and also describes relevant mitigation measures. In the process of constantly improving our methodology for the development of threat landscapes, this year’s work has been supported by a newly formatted ENISA ad hoc Working Group on Cybersecurity Threat Landscapes (CTL). In this report we discuss the first 8 cybersecurity threat categories. Supply chain threats, the 9th category, were analysed in detail, in a dedicated ENISA report.},
	language = {en},
	urldate = {2022-06-21},
	institution = {ENISA},
	author = {Cybersecurity., European Union Agency for},
	editor = {Lella, Ifigeneia and Tsekmezoglou, Eleni and Malatras, Apostolos},
	collaborator = {Ardagna, Claudio and Corbiaux, Stephen and Sfakianakis, Andreas and Douligeris, Christos},
	month = oct,
	year = {2021},
	keywords = {Government},
}

@misc{khandelwal_ccleaner_2018,
	title = {{CCleaner} {Attack} {Timeline}—{Here}'s {How} {Hackers} {Infected} 2.3 {Million} {PCs}},
	url = {https://thehackernews.com/2018/04/ccleaner-malware-attack.html},
	abstract = {Here's a brief timeline of CCleaner supply chain malware attack, explaining how and when hackers infected millions of computers.},
	language = {en},
	urldate = {2022-06-24},
	journal = {The Hacker News},
	author = {Khandelwal, Swati},
	month = apr,
	year = {2018},
	note = {Section: Article},
	keywords = {attacks},
}

@misc{ii_compromised_2018,
	title = {Compromised npm {Package}: event-stream},
	shorttitle = {Compromised npm {Package}},
	url = {https://medium.com/intrinsic-blog/compromised-npm-package-event-stream-d47d08605502},
	abstract = {Ownership of a popular npm package, event-stream, was transferred by the original author to a malicious user.},
	language = {en},
	urldate = {2022-06-24},
	journal = {intrinsic},
	author = {II, Thomas Hunter},
	month = nov,
	year = {2018},
	keywords = {attacks},
}

@misc{microsoft_defender_security_research_team_new_2017,
	title = {New ransomware, old techniques: {Petya} adds worm capabilities},
	shorttitle = {New ransomware, old techniques},
	url = {https://www.microsoft.com/security/blog/2017/06/27/new-ransomware-old-techniques-petya-adds-worm-capabilities/},
	abstract = {On June 27, 2017 reports of a ransomware infection began spreading across Europe. We saw the first infections in Ukraine, where more than 12,500 machines encountered the threat. We then observed infections in another 64 countries, including Belgium, Brazil, Germany, Russia, and the United States. The trend towards increasingly sophisticated malware behavior, highlighted by the…},
	language = {en-US},
	urldate = {2022-06-24},
	journal = {Microsoft Security Blog},
	author = {Microsoft Defender Security Research Team},
	month = jun,
	year = {2017},
	keywords = {attacks},
}

@techreport{ensor_shifting_2021,
	title = {Shifting left on security - {Securing} software supply chains},
	url = {https://cloud.google.com/solutions/shifting-left-on-security},
	language = {en},
	institution = {Google Cloud},
	author = {Ensor, Mike and Stevens, Drew},
	month = feb,
	year = {2021},
	keywords = {Industry, White Paper},
	pages = {36},
}

@misc{risktec_risk-based_nodate,
	title = {Risk-based decision making},
	url = {https://risktec.tuv.com/risktec-knowledge-bank/quantitative-probabilistic-risk-assessment/risk-based-decision-making/},
	abstract = {Risk-based decision making evolves around assessing the need to make choices that are often limited by external reasons for the 'best' result},
	language = {en-GB},
	urldate = {2022-07-07},
	author = {{Risktec}},
}

@incollection{lu_risk_2012,
	address = {Berlin, Heidelberg},
	series = {Intelligent {Systems} {Reference} {Library}},
	title = {Risk {Management} in {Decision} {Making}},
	isbn = {978-3-642-25755-1},
	url = {https://doi.org/10.1007/978-3-642-25755-1_1},
	abstract = {Organizational decision making often occurs in the face of uncertainty about whether a decision maker’s choices will lead to benefit or disaster. Risk is the potential that a decision will lead to a loss or an undesirable outcome. In fact, almost any human decision carries some risk, but some decisions are much more risky than others. Risk and decision making are two inter-related factors in organizational management, and they are both related to various uncertainties.},
	language = {en},
	urldate = {2022-07-07},
	booktitle = {Handbook on {Decision} {Making}: {Vol} 2: {Risk} {Management} in {Decision} {Making}},
	publisher = {Springer},
	author = {Lu, Jie and Jain, Lakhmi C. and Zhang, Guangquan},
	editor = {Lu, Jie and Jain, Lakhmi C. and Zhang, Guangquan},
	year = {2012},
	doi = {10.1007/978-3-642-25755-1_1},
	keywords = {Grassland Fire, Group Decision Support System, Risk Management, Risk Management Process, Supply Chain},
	pages = {3--7},
}

@article{defranco_content_2017,
	title = {A content analysis process for qualitative software engineering research},
	volume = {13},
	issn = {1614-5054},
	url = {https://doi.org/10.1007/s11334-017-0287-0},
	doi = {10.1007/s11334-017-0287-0},
	abstract = {A review of the qualitative research methods discussed in papers that study software engineering teams showed most of those papers did not follow a systematic process during the qualitative analysis. This finding is concerning as this deficiency in research analysis procedure may reduce the validity and/or completeness of the qualitative results. Such a lack of rigor may be a result of qualitative research not being as firmly established in software engineering as quantitative research methodologies. In engineering research, quantitative methods are typically more prevalent and qualitative analysis is part of a mixed-method analysis process. However, when researching teams, where human activity is abundant, qualitative analysis may need to take precedence. In this paper, we focus on the qualitative analysis method called content analysis with the goal of presenting a rigorous process for content analysis in the context of software engineering. We then present and demonstrate the use of that content analysis process for software engineering researchers using two examples. An analysis of 215 articles that were a result of a mapping study on software engineering team research is presented. Those papers were analyzed to determine which utilized a qualitative data analysis method in their research in addition to the rigor and type of qualitative analysis performed. We ultimately included 23 papers in this study. We present a mapping study and a content analysis process that include a straightforward way to select, code, and present data in both inductive and deductive studies. We demonstrated the process using the keywords from the papers included in this study as well as on a second data set that utilized responses from structured interview transcripts from practicing software engineers. The first dataset also resulted in a taxonomy to categorize software engineering team research. We presented and demonstrated a content analysis process in terms of software engineering in order to improve future qualitative software engineering research that would benefit from systematic content analysis.},
	language = {en},
	number = {2},
	urldate = {2022-07-07},
	journal = {Innovations in Systems and Software Engineering},
	author = {DeFranco, Joanna F. and Laplante, Phillip A.},
	month = sep,
	year = {2017},
	keywords = {Content analysis, Mapping study, Qualitative research, Software engineering teams},
	pages = {129--141},
}

@incollection{borissova_overview_2021,
	address = {Cham},
	series = {Studies in {Computational} {Intelligence}},
	title = {An {Overview} of {Multi}-criteria {Decision} {Making} {Models} and {Software} {Systems}},
	isbn = {978-3-030-72284-5},
	url = {https://doi.org/10.1007/978-3-030-72284-5_15},
	abstract = {The article describes an overview of proposed multi-criteria decision making models and their applications. Several research directions are discussed including multi-criteria models with a priori and exact articulation of DM preferences; models with fuzzy preferences; models with interactive participation of DM; models based on bi-level optimization, group decision making models and software systems for decision making. This overview concerns the latest achievements in multi-criteria decision making of the scientists from Bulgarian Academy of Sciences.},
	language = {en},
	urldate = {2022-07-07},
	booktitle = {Research in {Computer} {Science} in the {Bulgarian} {Academy} of {Sciences}},
	publisher = {Springer International Publishing},
	author = {Borissova, Daniela},
	editor = {Atanassov, Krassimir T.},
	year = {2021},
	doi = {10.1007/978-3-030-72284-5_15},
	keywords = {Bi-level optimization, Decision making, Group decision making, Multi-criteria methods, Software systems},
	pages = {305--323},
}

@article{jhaver_online_2018,
	title = {Online {Harassment} and {Content} {Moderation}: {The} {Case} of {Blocklists}},
	volume = {25},
	issn = {1073-0516, 1557-7325},
	shorttitle = {Online {Harassment} and {Content} {Moderation}},
	url = {https://dl.acm.org/doi/10.1145/3185593},
	doi = {10.1145/3185593},
	abstract = {Online harassment is a complex and growing problem. On Twitter, one mechanism people use to avoid harassment is the
              blocklist
              , a list of accounts that are preemptively blocked from interacting with a subscriber. In this article, we present a rich description of Twitter blocklists – why they are needed, how they work, and their strengths and weaknesses in practice. Next, we use blocklists to interrogate online harassment – the forms it takes, as well as tactics used by harassers. Specifically, we interviewed both people who use blocklists to protect themselves, and people who are blocked by blocklists. We find that users are not adequately protected from harassment, and at the same time, many people feel that they are blocked unnecessarily and unfairly. Moreover, we find that not all users agree on what constitutes harassment. Based on our findings, we propose design interventions for social network sites with the aim of protecting people from harassment, while preserving freedom of speech.},
	language = {en},
	number = {2},
	urldate = {2022-07-07},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Jhaver, Shagun and Ghoshal, Sucheta and Bruckman, Amy and Gilbert, Eric},
	month = apr,
	year = {2018},
	pages = {1--33},
}

@inproceedings{alfantoukh_multi-stakeholder_2018,
	title = {Multi-{Stakeholder} {Consensus} {Decision}-{Making} {Framework} {Based} on {Trust}: {A} {Generic} {Framework}},
	shorttitle = {Multi-{Stakeholder} {Consensus} {Decision}-{Making} {Framework} {Based} on {Trust}},
	doi = {10.1109/CIC.2018.00012},
	abstract = {The decision-making process is one we encounter in every aspect of our lives. Decision-making becomes more challenging when dealing with multi-stakeholder decisions due to the existence of conflicts among them and the diversity in their expertise. As a result, the influence among them, which is represented by trust, is considered to be an important criterion when one is making a final decision. Such trust is a result of the interactions among those stakeholders. Rating is one of the methods that stakeholders use in their interactions to express their opinions of one another. It requires a decision that is agreed upon by everyone; of course, it might take several rounds to reach a final consensus decision. In this research study, we built a consensus decision-making framework based on trust. The trust framework has been proposed previously and is based on measurement theory. Then, we developed software to simulate the decision-making scenarios to study the rating convergences in these decision-making rounds and to investigate their convergences with and without trust. This simulator was designed to emulate humans' behaviors from a social science perspective. Our result showed that measurement theory-based trust is useful in the consensus-creating process, as it decreases the number of necessary rounds and even creates a consensus when an extreme conflict in preferences exists.},
	booktitle = {2018 {IEEE} 4th {International} {Conference} on {Collaboration} and {Internet} {Computing} ({CIC})},
	author = {Alfantoukh, Lina and Ruan, Yefeng and Durresi, Arjan},
	month = oct,
	year = {2018},
	keywords = {Big Data, Collaboration, Computational modeling, Decision making, Decision-Making, Trust, Multistakeholder, Collaboration, big data, History, Mathematical model, Stakeholders},
	pages = {472--479},
}

@article{li_risk_2012,
	title = {Risk {Decision} {Making} {Based} on {Decision}-theoretic {Rough} {Set}: {A} {Three}-way {View} {Decision} {Model}},
	copyright = {Copyright Taylor and Francis Group, LLC},
	shorttitle = {Risk {Decision} {Making} {Based} on {Decision}-theoretic {Rough} {Set}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/18756891.2011.9727759},
	abstract = {Rough set theory has witnessed great success in data mining and knowledge discovery, which provides a good support for decision making on a certain data. However, a practical decision problem alway...},
	language = {en},
	urldate = {2022-07-07},
	journal = {International Journal of Computational Intelligence Systems},
	author = {Li, Huaxiong and Zhou, Xianzhong},
	month = mar,
	year = {2012},
	note = {Publisher: Taylor \& Francis Group},
}

@phdthesis{alfantoukh_multi-stakeholder_2019,
	type = {Thesis},
	title = {Multi-{Stakeholder} {Consensus} {Decision}-{Making} {Framework} {Based} on {Trust} and {Risk}},
	url = {https://scholarworks.iupui.edu/handle/1805/18885},
	abstract = {This thesis combines human and machine intelligence for consensus decision-making, and it contains four interrelated research areas. Before presenting the four research areas, this thesis presents a literature review on decision-making using two criteria: trust and risk. The analysis involves studying the individual and the multi-stakeholder decision-making. Also, it explores the relationship between trust and risk to provide insight on how to apply them when making any decision. This thesis presents a grouping procedure of the existing trust-based multi-stakeholder decision-making schemes by considering the group decision-making process and models. In the first research area, this thesis presents the foundation of building multi-stakeholder consensus decision-making (MSCDM). This thesis describes trust-based multi-stakeholder decision-making for water allocation to help the participants select a solution that comes from the best model. Several criteria are involved when deciding on a solution such as trust, damage, and benefit. This thesis considers Jain's fairness index as an indicator of reaching balance or equality for the stakeholder's needs. The preferred scenario is when having a high trust, low damages and high benefits. The worst scenario involves having low trust, high damage, and low benefit. The model is dynamic by adapting to the changes over time. The decision to select is the solution that is fair for almost everyone. In the second research area, this thesis presents a MSCDM, which is a generic framework that coordinates the decision-making rounds among stakeholders based on their influence toward each other, as represented by the trust relationship among them. This thesis describes the MSCDM framework that helps to find a decision the stakeholders can agree upon. Reaching a consensus decision might require several rounds where stakeholders negotiate by rating each other. This thesis presents the results of implementing MSCDM and evaluates the effect of trust on the consensus achievement and the reduction in the number of rounds needed to reach the final decision. This thesis presents Rating Convergence in the implemented MSCDM framework, and such convergence is a result of changes in the stakeholders' rating behavior in each round. This thesis evaluates the effect of trust on the rating changes by measuring the distance of the choices made by the stakeholders. Trust is useful in decreasing the distances. In the third research area, this thesis presents Rating Convergence in the implemented MSCDM framework, and such convergence is a result of changes in stakeholders' rating behavior in each round. This thesis evaluates the effect of trust on the rating changes by measuring the perturbation in the rating matrix. Trust is useful in increasing the rating matrix perturbation. Such perturbation helps to decrease the number of rounds. Therefore, trust helps to increase the speed of agreeing upon the same decision through the influence. In the fourth research area, this thesis presents Rating Aggregation operators in the implemented MSCDM framework. This thesis addresses the need for aggregating the stakeholders' ratings while they negotiate on the round of decisions to compute the consensus achievement. This thesis presents four aggregation operators: weighted sum (WS), weighted product (WP), weighted product similarity measure (WPSM), and weighted exponent similarity measure (WESM). This thesis studies the performance of those aggregation operators in terms of consensus achievement and the number of rounds needed. The consensus threshold controls the performance of these operators. The contribution of this thesis lays the foundation for developing a framework for MSCDM that facilitates reaching the consensus decision by accounting for the stakeholders' influences toward one another. Trust represents the influence.},
	language = {en},
	urldate = {2022-07-07},
	author = {Alfantoukh, Lina Abdulaziz},
	month = may,
	year = {2019},
	doi = {10.7912/C2/2360},
	note = {Accepted: 2019-04-18T14:20:27Z},
}

@article{aven_decision_2007,
	title = {A decision framework for risk management, with application to the offshore oil and gas industry},
	volume = {92},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832006000093},
	doi = {10.1016/j.ress.2005.12.009},
	abstract = {In this paper we present and discuss a decision framework for risk management. The framework comprises the basic elements: problem definition (challenges, goals and alternatives), stakeholders, concerns that affect the consequence analyses and the value judgments related to these consequences and analyses (frame conditions and constraints), identification of which consequence analyses to execute and the execution of these, managerial review and judgement, and the decision. The framework has novel aspects on the way of classifying the decision situations and characterising risks. The classification is based on the two dimensions, expected consequences, and uncertainties. Our starting point is the offshore oil and gas industry, but our framework and discussion is to a large extent general and could also be applied in other areas. An example is outlined to illustrate the use of the framework.},
	language = {en},
	number = {4},
	urldate = {2022-07-07},
	journal = {Reliability Engineering \& System Safety},
	author = {Aven, T. and Vinnem, J. E. and Wiencke, H. S.},
	month = apr,
	year = {2007},
	keywords = {ALARP, Decision framework, Risk and uncertainty},
	pages = {433--448},
}

@inproceedings{liu_stegonet_2020,
	address = {Austin USA},
	title = {{StegoNet}: {Turn} {Deep} {Neural} {Network} into a {Stegomalware}},
	isbn = {978-1-4503-8858-0},
	shorttitle = {{StegoNet}},
	url = {https://dl.acm.org/doi/10.1145/3427228.3427268},
	doi = {10.1145/3427228.3427268},
	abstract = {Deep Neural Networks (DNNs) are now presenting human-level performance on many real-world applications, and DNN-based intelligent services are becoming more and more popular across all aspects of our lives. Unfortunately, the ever-increasing DNN service implies a dangerous feature which has not yet been well studied–allowing the marriage of existing malware and DNN model for any pre-defined malicious purpose. In this paper, we comprehensively investigate how to turn DNN into a new breed evasive self-contained stegomalware, namely StegoNet, using model parameter as a novel payload injection channel, with no service quality degradation (i.e. accuracy) and the triggering event connected to the physical world by specified DNN inputs. A series of payload injection techniques which take advantage of a variety of unique neural network natures like complex structure, high error resilience capability and huge parameter size, are developed for both uncompressed models (with model redundancy) and deeply compressed models tailored for resource-limited devices (no model redundancy), including LSB substitution, resilience training, value mapping, and sign-mapping. We also proposed a set of triggering techniques like logits trigger, rank trigger and fine-tuned rank trigger to trigger StegoNet by specific physical events under realistic environment variations. We implement the StegoNet prototype on Nvidia Jetson TX2 testbed. Extensive experimental results and discussions on the evasiveness, integrity of proposed payload injection techniques, and the reliability and sensitivity of the triggering techniques, well demonstrate the feasibility and practicality of StegoNet.},
	language = {en},
	urldate = {2022-07-07},
	booktitle = {Annual {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Liu, Tao and Liu, Zihao and Liu, Qi and Wen, Wujie and Xu, Wenyao and Li, Ming},
	month = dec,
	year = {2020},
	pages = {928--938},
}

@inproceedings{Wang2021EvilModel,
	title = {{EvilModel}: {Hiding} {Malware} {Inside} of {Neural} {Network} {Models}},
	abstract = {Delivering malware covertly and evasively is critical to advanced malware campaigns. In this paper, we present a new method to covertly and evasively deliver malware through a neural network model. Neural network models are poorly explainable and have a good generalization ability. By embedding malware in neurons, the malware can be delivered covertly, with minor or no impact on the performance of neural network. Meanwhile, because the structure of the neural network model remains unchanged, it can pass the security scan of anti-virus engines. Experiments show that 36.9MB of malware can be embedded in a 178MB-AlexNet model within 1\% accuracy loss, and no suspicion is raised by anti-virus engines in VirusTotal, which verifies the feasibility of this method. With the widespread application of artificial intelligence, utilizing neural networks for attacks becomes a forwarding trend. We hope this work can provide a reference scenario for the defense on neural network-assisted attacks.},
	booktitle = {{IEEE} {Symposium} on {Computers} and {Communications} ({ISCC})},
	author = {Wang, Zhi and Liu, Chaoge and Cui, Xiang},
	year = {2021},
	keywords = {Artificial Intelligence, Artificial intelligence, Computational modeling, Computer security, Computer viruses, Malware, Market research, Neural Networks, Neurons, Steganography},
}

@article{goldhaber_exploring_2020,
	title = {Exploring the {Impact} of {Student} {Teaching} {Apprenticeships} on {Student} {Achievement} and {Mentor} {Teachers}},
	volume = {13},
	issn = {1934-5747},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7252591/},
	doi = {10.1080/19345747.2019.1698087},
	abstract = {We exploit within-teacher variation in the years that math and reading teachers in grades 4–8 host an apprentice (“student teacher”) in Washington State to estimate the causal effect of these apprenticeships on student achievement, both during the apprenticeship and afterwards. While the average causal effect of hosting a student teacher on student performance in the year of the apprenticeship is indistinguishable from zero in both math and reading, hosting a student teacher is found to have modest positive impacts on student math and reading achievement in a teacher’s classroom in following years. These findings suggest that schools and districts can participate in the student teaching process without fear of short-term decreases in student test scores while potentially gaining modest long-term test score increases.},
	number = {2},
	urldate = {2022-07-05},
	journal = {Journal of Research on Educational Effectiveness},
	author = {Goldhaber, Dan and Krieg, John M. and Theobald, Roddy},
	month = feb,
	year = {2020},
	pmid = {32537041},
	pmcid = {PMC7252591},
	pages = {213--234},
}

@article{quew-jones_enhancing_2022,
	title = {Enhancing apprenticeships within the {Higher} {Education} curriculum – an {Action} {Learning} and {Action} {Research} study},
	issn = {1476-7333, 1476-7341},
	url = {https://www.tandfonline.com/doi/full/10.1080/14767333.2022.2056135},
	doi = {10.1080/14767333.2022.2056135},
	abstract = {This Action Learning (AL)/Action Research study (AS) explores the practice of Action Learning (AL) to further higher education (H.E.) apprenticeships by collaboration between University Provider (UP) and employer. AL members aim to address complexity, bridging the gap between management education delivered by a work-based learning (WBL) apprenticeship course and translating into the apprentices’ workplace. Set members followed a systematic cycle of planning, action, observing and reﬂecting. This demonstrates how AL, as a methodology, supports apprenticeship ambassadors (who lead apprenticeships in their organisations) and UPs to solve complex problems through inquiry and critical reﬂection to enhance the apprenticeship curriculum. The principal ﬁndings from AL to cultivate stronger collaboration were clarity of WBL, value proposition and ownership expectation; support of translation of theory into practice; empowering the apprenticeship mindset and professional identity; and senior management buy-in.},
	language = {en},
	urldate = {2022-07-05},
	journal = {Action Learning: Research and Practice},
	author = {Quew-Jones, Rebecca},
	month = mar,
	year = {2022},
	pages = {1--19},
}

@misc{merten_identification_2017,
	type = {Dissertation},
	title = {Identification of {Software} {Features} in {Issue} {Tracking} {System} {Data}},
	copyright = {info:eu-repo/semantics/openAccess},
	url = {https://archiv.ub.uni-heidelberg.de/volltextserver/22655/},
	abstract = {The knowledge of Software Features (SFs) is vital for software developers and requirements specialists during all software engineering phases: to understand and derive software requirements, to plan and prioritize implementation tasks, to update documentation, or to test whether the final product correctly implements the requested SF. In most software projects, SFs are managed in conjunction with other information such as bug reports, programming tasks, or refactoring tasks with the aid of Issue Tracking Systems (ITSs). Hence ITSs contains a variety of information that is only partly related to SFs.
In practice, however, the usage of ITSs to store SFs comes with two major problems: (1) ITSs are neither designed nor used as documentation systems. Therefore, the data inside an ITS is often uncategorized and SF descriptions are concealed in rather lengthy. (2) Although an SF is often requested in a single sentence, related information can be scattered among many issues. E.g. implementation tasks related to an SF are often reported in additional issues. Hence, the detection of SFs in ITSs is complicated: a manual search for the SFs implies reading, understanding and exploiting the Natural Language (NL) in many issues in detail. This is cumbersome and labor intensive, especially if related information is spread over more than one issue.
This thesis investigates whether SF detection can be supported automatically. First the problem is analyzed: (i) An empirical study shows that requests for important SFs reside in ITSs, making ITSs a good tar- get for SF detection. (ii) A second study identifies characteristics of the information and related NL in issues. These characteristics repre- sent opportunities as well as challenges for the automatic detection of SFs.
Based on these problem studies, the Issue Tracking Software Feature Detection Method (ITSoFD), is proposed. The method has two main components and includes an approach to preprocess issues. Both components address one of the problems associated with storing SFs in ITSs. ITSoFD is validated in three solution studies: (I) An empirical study researches how NL that describes SFs can be detected with techniques from Natural Language Processing (NLP) and Machine Learning. Issues are parsed and different characteristics of the issue and its NL are extracted. These characteristics are used to clas- sify the issue’s content and identify SF description candidates, thereby approaching problem (1). (II) An empirical study researches how issues that carry information potentially related to an SF can be detected with techniques from NLP and Information Retrieval. Characteristics of the issue’s NL are utilized to create a traceability network
 vii
of related issues, thereby approaching problem (2). (III) An empirical study researches how NL data in issues can be preprocessed using heuristics and hierarchical clustering. Code, stack traces, and other technical information is separated from NL. Heuristics are used to identify candidates for technical information and clustering improves the heuristic’s results. The technique can be applied to support components, I. and II.},
	language = {eng},
	urldate = {2022-07-06},
	author = {Merten, Thorsten},
	year = {2017},
	doi = {10.11588/heidok.00022655},
}

@article{kutomi_supporting_nodate,
	title = {Supporting {Support} {Engineers}},
	abstract = {The steady and uninterrupted availability of systems is essential for the mission of many companies and other organizations. This responsibility relies mostly upon support engineers, who are responsible to respond to incidents. Incident response is a unique type of task in software engineering, given it carries distinguishing characteristics like risks, pressure, incomplete information and urgency. Despite the importance of this task for many organizations, little can be found in the literature about the incident response task and model. To ﬁll the gap, we created a theoretical foundation to foster research on incident response. We conducted an interview study, asking 12 support engineers about their experiences dealing with outages, service degradation, and other incidents that demanded an urgent response. We used our 22 collected cases to identify important concepts of incidents and their dimensions, and created an ontology of incidents and a model of the incident response. To validate the usefulness of our results, we analyzed our incidents based on our ontology and model, providing some insights related to detection of incidents, investigation and the hand over process. We also provide analytical insights related to the prevention of resource limitation incidents. Finally, we validate the usefulness of our research by proposing an improvement on monitoring tools used by support engineers.},
	language = {en},
	author = {Kutomi, Esdras},
	pages = {69},
}

@inproceedings{ko_design_2011,
	address = {New York, NY, USA},
	series = {{iConference} '11},
	title = {Design, discussion, and dissent in open bug reports},
	isbn = {978-1-4503-0121-3},
	url = {https://doi.org/10.1145/1940761.1940776},
	doi = {10.1145/1940761.1940776},
	abstract = {While studies have considered computer-mediated decision-making in several domains, few have considered the unique challenges posed in software design. To address this gap, a qualitative study of 100 contentious open source bug reports was performed. The results suggest that the immeasurability of many software qualities and conflicts between achieving original design intent and serving changing user needs led to a high reliance on anecdote, speculation, and generalization. The visual presentation of threaded discussions aggravated these problems making it difficult to view design proposals and comparative critiques. The results raise several new questions about the interaction between authority and evidence in online design discussions.},
	urldate = {2022-07-05},
	booktitle = {Proceedings of the 2011 {iConference}},
	publisher = {Association for Computing Machinery},
	author = {Ko, Amy J. and Chilana, Parmit K.},
	month = feb,
	year = {2011},
	keywords = {Bugzilla, change requests, design rationale, open source},
	pages = {106--113},
}

@misc{sillito_failures_2020,
	title = {Failures and {Fixes}: {A} {Study} of {Software} {System} {Incident} {Response}},
	shorttitle = {Failures and {Fixes}},
	url = {http://arxiv.org/abs/2008.11192},
	abstract = {This paper presents the results of a research study related to software system failures, with the goal of understanding how we might better evolve, maintain and support software systems in production. We have qualitatively analyzed thirty incidents: fifteen collected through in depth interviews with engineers, and fifteen sampled from publicly published incident reports (generally produced as part of postmortem reviews). Our analysis focused on understanding and categorizing how failures occurred, and how they were detected, investigated and mitigated. We also captured analytic insights related to the current state of the practice and associated challenges in the form of 11 key observations. For example, we observed that failures can cascade through a system leading to major outages; and that often engineers do not understand the scaling limits of systems they are supporting until those limits are exceeded. We argue that the challenges we have identified can lead to improvements to how systems are engineered and supported.},
	urldate = {2022-07-05},
	publisher = {arXiv},
	author = {Sillito, Jonathan and Kutomi, Esdras},
	month = aug,
	year = {2020},
	note = {Number: arXiv:2008.11192
arXiv:2008.11192 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{nahar_collaboration_2022,
	title = {Collaboration {Challenges} in {Building} {ML}-{Enabled} {Systems}: {Communication}, {Documentation}, {Engineering}, and {Process}},
	abstract = {The introduction of machine learning (ML) components in software projects has created the need for software engineers to collaborate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through interviews with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common collaboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. We find that most of these challenges center around communication, documentation, engineering, and process, and collect recommendations to address these challenges.},
	language = {en},
	author = {Nahar, Nadia and Zhou, Shurui and Lewis, Grace and Kästner, Christian},
	year = {2022},
	pages = {13},
}

@article{ghorbani_interpretation_2019,
	title = {Interpretation of {Neural} {Networks} {Is} {Fragile}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4252},
	doi = {10.1609/aaai.v33i01.33013681},
	abstract = {In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientiﬁc robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. inﬂuence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.},
	language = {en},
	urldate = {2022-07-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
	month = jul,
	year = {2019},
	pages = {3681--3688},
}

@inproceedings{torres-arias_-toto_2019,
	title = {in-toto: {Providing} farm-to-table guarantees for bits and bytes},
	isbn = {978-1-939133-06-9},
	shorttitle = {in-toto},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/torres-arias},
	language = {en},
	urldate = {2022-07-01},
	author = {Torres-Arias, Santiago and Afzali, Hammad and Kuppusamy, Trishank Karthik and Curtmola, Reza and Cappos, Justin},
	year = {2019},
	pages = {1393--1410},
}

@book{reinders_data_2021,
	address = {Berkeley, CA},
	title = {Data {Parallel} {C}++: {Mastering} {DPC}++ for {Programming} of {Heterogeneous} {Systems} using {C}++ and {SYCL}},
	isbn = {978-1-4842-5573-5 978-1-4842-5574-2},
	shorttitle = {Data {Parallel} {C}++},
	url = {http://link.springer.com/10.1007/978-1-4842-5574-2},
	language = {en},
	urldate = {2022-06-29},
	publisher = {Apress},
	author = {Reinders, James and Ashbaugh, Ben and Brodman, James and Kinsner, Michael and Pennycook, John and Tian, Xinmin},
	year = {2021},
	doi = {10.1007/978-1-4842-5574-2},
}

@inproceedings{chida2022lookaheads,
	title = {On lookaheads in regular expressions with backreferences},
	booktitle = {7th international conference on formal structures for computation and deduction},
	author = {Chida, Nariyoshi and Terauchi, Tachio},
	year = {2022},
}

@techreport{noauthor_dod_2015,
	title = {{DoD} {Cyber} {Strategy}},
	year = {2015},
}

@inproceedings{votipka_hacked_2021,
	address = {San Francisco, CA, USA},
	title = {{HackEd}: {A} {Pedagogical} {Analysis} of {Online} {Vulnerability} {Discovery} {Exercises}},
	isbn = {978-1-72818-934-5},
	shorttitle = {{HackEd}},
	url = {https://ieeexplore.ieee.org/document/9519464/},
	doi = {10.1109/SP40001.2021.00092},
	abstract = {Hacking exercises are a common tool for security education, but there is limited investigation of how they teach security concepts and whether they follow pedagogical best practices. This paper enumerates the pedagogical practices of 31 popular online hacking exercises. Speciﬁcally, we derive a set of pedagogical dimensions from the general learning sciences and educational literature, tailored to hacking exercises, and review whether and how each exercise implements each pedagogical dimension. In addition, we interview the organizers of 15 exercises to understand challenges and tradeoffs that may occur when choosing whether and how to implement each dimension.},
	language = {en},
	urldate = {2022-06-27},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Votipka, Daniel and Zhang, Eric and Mazurek, Michelle L.},
	month = may,
	year = {2021},
	pages = {1268--1285},
}

@misc{noauthor_5_2012,
	title = {5 {Reasons} {Software} {Projects} {Fail}: {The} {Role} of {Requirements}},
	shorttitle = {5 {Reasons} {Software} {Projects} {Fail}},
	url = {https://argondigital.com/blog/product-management/5-reasons-software-projects-fail-hint-its-often-due-to-incomplete-incorrect-requirements/},
	abstract = {Business analysts, in requirements gathering and accurate, complete requirements definition, can significantly impact whether software projects succeed or fail.},
	language = {en-US},
	urldate = {2022-02-04},
	journal = {ArgonDigital {\textbar} Making Technology a Strategic Advantage},
	month = mar,
	year = {2012},
}

@inproceedings{vetterl_counting_2019,
	title = {Counting {Outdated} {Honeypots}: {Legal} and {Useful}},
	shorttitle = {Counting {Outdated} {Honeypots}},
	doi = {10.1109/SPW.2019.00049},
	abstract = {Honeypots are intended to be covert and so little is known about how many are deployed or who is using them. We used protocol deviations at the SSH transport layer to fingerprint Kippo and Cowrie, the two most popular medium interaction SSH honeypots. Several Internet-wide scans over a one year period revealed the presence of thousands of these honeypots. Sending specific commands revealed their patch status and showed that many systems were not up to date: a quarter or more were not fully updated and by the time of our last scan 20\% of honeypots were still running Kippo, which had last been updated several years earlier. However, our paper reporting these results was rejected from a major conference on the basis that our interactions with the honeypots were illegal and hence the research was unethical. We later published a much redacted account of our research which described the fingerprinting but omitted the results we had gained from the issuing of commands to check the patch status. In the present work we provide the missing results, but start with an extended ethical justification for our research and a detailed legal analysis to show why we did not infringe cybersecurity laws.},
	booktitle = {2019 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Vetterl, Alexander and Clayton, Richard and Walden, Ian},
	month = may,
	year = {2019},
	keywords = {Authorization, Computer security, Internet, Law, Protocols, ethical issues, ethics, honeypots, intrusion detection, measurement, unauthorised access},
	pages = {224--229},
}

@article{symonds_innovating_2017,
	title = {Innovating the {Prioritization} of {Cyber} {Defense}},
	volume = {16},
	issn = {1445-3312},
	url = {http://www.jstor.org/stable/26502753},
	abstract = {The U.S. Department of Defense (DoD) faces a monumental undertaking in protecting the infrastructure that underpins the entirety of its operations: It must identify and prioritize key terrain to dynamically defend. This paper will examine the criteria to identify critical information systems and infrastructure, will review the process to identify key terrain in cyberspace, and will offer a recommendation on how to more effectively prioritize network defender operations using data analytics.},
	number = {2},
	urldate = {2022-02-19},
	journal = {Journal of Information Warfare},
	author = {Symonds, R},
	year = {2017},
	note = {Publisher: Peregrine Technical Solutions},
	pages = {12--18},
}

@inproceedings{sobb_assessment_2020,
	title = {Assessment of {Cyber} {Security} {Implications} of {New} {Technology} {Integrations} into {Military} {Supply} {Chains}},
	doi = {10.1109/SPW50608.2020.00038},
	abstract = {Military supply chains play a critical role in the acquisition and movement of goods for defence purposes. The disruption of these supply chain processes can have potentially devastating affects to the operational capability of military forces. The introduction and integration of new technologies into defence supply chains can serve to increase their effectiveness. However, the benefits posed by these technologies may be outweighed by significant consequences to the cyber security of the entire defence supply chain. Supply chains are complex Systems of Systems, and the introduction of an insecure technology into such a complex ecosystem may induce cascading system-wide failure, and have catastrophic consequences to military mission assurance. Subsequently, there is a need for an evaluative process to determine the extent to which a new technology will affect the cyber security of military supply chains. This work proposes a new model, the Military Supply Chain Cyber Implications Model (M-SCCIM), that serves to aid military decision makers in understanding the potential cyber security impact of introducing new technologies to supply chains. M-SCCIM is a multiphase model that enables understanding of cyber security and supply chain implications through the lenses of theoretical examinations, pilot applications and system wide implementations.},
	booktitle = {2020 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Sobb, Theresa May and Turnbull, Benjamin},
	month = may,
	year = {2020},
	keywords = {Computer crime, Lenses, Power system faults, Power system protection, Privacy, Security, Supply chains, Systems-of-systems, cyber, cyber-security.framework, military, supply chain},
	pages = {128--135},
}

@inproceedings{spooner_navigating_2018,
	address = {San Francisco, CA},
	title = {Navigating the {Insider} {Threat} {Tool} {Landscape}: {Low} {Cost} {Technical} {Solutions} to {Jump} {Start} an {Insider} {Threat} {Program}},
	isbn = {978-1-5386-8276-0},
	shorttitle = {Navigating the {Insider} {Threat} {Tool} {Landscape}},
	url = {https://ieeexplore.ieee.org/document/8424656/},
	doi = {10.1109/SPW.2018.00040},
	language = {en},
	urldate = {2022-06-27},
	booktitle = {2018 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	publisher = {IEEE},
	author = {Spooner, Derrick and Silowash, George and Costa, Daniel and Albrethsen, Michael},
	month = may,
	year = {2018},
	pages = {247--257},
}

@inproceedings{sion_automated_2021,
	address = {Atlanta, GA, USA},
	title = {Automated {Threat} {Analysis} and {Management} in a {Continuous} {Integration} {Pipeline}},
	isbn = {978-1-66543-170-5},
	url = {https://ieeexplore.ieee.org/document/9652652/},
	doi = {10.1109/SecDev51306.2021.00021},
	abstract = {Security and privacy threat modeling is commonly applied to systematically identify and address design-level security and privacy concerns in the early stages of architecture and design. Identifying and resolving these threats should remain a continuous concern during the development lifecycle. Especially with contemporary agile development practices, a single-shot upfront analysis becomes quickly outdated. Despite it being explicitly recommended by experts, existing threat modeling approaches focus largely on early development phases and provide limited support during later implementation phases.},
	language = {en},
	urldate = {2022-06-27},
	booktitle = {2021 {IEEE} {Secure} {Development} {Conference} ({SecDev})},
	publisher = {IEEE},
	author = {Sion, Laurens and Van Landuyt, Dimitri and Yskout, Koen and Verreydt, Stef and Joosen, Wouter},
	month = oct,
	year = {2021},
	pages = {30--37},
}

@article{frik_privacy_nodate,
	title = {Privacy and {Security} {Threat} {Models} and {Mitigation} {Strategies} of {Older} {Adults}},
	abstract = {Older adults (65+) are becoming primary users of emerging smart systems, especially in health care. However, these technologies are often not designed for older users and can pose serious privacy and security concerns due to their novelty, complexity, and propensity to collect and communicate vast amounts of sensitive information. Efforts to address such concerns must build on an in-depth understanding of older adults’ perceptions and preferences about data privacy and security for these technologies, and accounting for variance in physical and cognitive abilities. In semi-structured interviews with 46 older adults, we identiﬁed a range of complex privacy and security attitudes and needs speciﬁc to this population, along with common threat models, misconceptions, and mitigation strategies. Our work adds depth to current models of how older adults’ limited technical knowledge, experience, and age-related declines in ability amplify vulnerability to certain risks; we found that health, living situation, and ﬁnances play a notable role as well. We also found that older adults often experience usability issues or technical uncertainties in mitigating those risks—and that managing privacy and security concerns frequently consists of limiting or avoiding technology use. We recommend educational approaches and usable technical protections that build on seniors’ preferences.},
	language = {en},
	author = {Frik, Alisa and Nurgalieva, Leysan and Bernd, Julia and Lee, Joyce S and Schaub, Florian and Egelman, Serge},
	pages = {21},
}

@article{fulton_effect_nodate,
	title = {The {Effect} of {Entertainment} {Media} on {Mental} {Models} of {Computer} {Security}},
	abstract = {When people inevitably need to make decisions about their computer-security posture, they rely on their mental models of threats and potential targets. Research has demonstrated that these mental models, which are often incomplete or incorrect, are informed in part by ﬁctional portrayals in television and ﬁlm. Inspired by prior research in public health demonstrating that efforts to ensure accuracy in the portrayal of medical situations has had an overall positive effect on public medical knowledge, we explore the relationship between computer security and ﬁctional television and ﬁlm. We report on a semistructured interview study (n=19) investigating what users have learned about computer security from mass media and how they evaluate what is and is not realistic within ﬁctional portrayals. In addition to conﬁrming prior ﬁndings that television and ﬁlm shape users’ mental models of security, we identify speciﬁc misconceptions that appear to align directly with common ﬁctional tropes. We identify speciﬁc proxies that people use to evaluate realism and examine how they inﬂuence these misconceptions. We conclude with recommendations for security researchers as well as creators of ﬁctional media when considering how to improve people’s understanding of computer-security concepts and behaviors.},
	language = {en},
	author = {Fulton, Kelsey R and Gelles, Rebecca and McKay, Alexandra and Roberts, Richard and Abdi, Yasmin and Mazurek, Michelle L},
	pages = {19},
}

@article{filar_ask_nodate,
	title = {Ask {Me} {Anything}: {A} {Conversational} {Interface} to {Augment} {Information} {Security} {Workers}},
	abstract = {Security products often create more problems than they solve, drowning users in alerts without providing the context required to remediate threats. This challenge is compounded by a lack of experienced personnel and security tools with complex interfaces. These interfaces require users to become domain experts or rely on repetitive, time consuming tasks to turn this data deluge into actionable intelligence. In this paper we present Artemis, a conversational interface to endpoint detection and response (EDR) event data. Artemis leverages dialog to drive the automation of complex tasks and reduce the need to learn a structured query language. Designed to empower inexperienced and junior security workers to better understand their security environment, Artemis provides an intuitive platform to ask questions of alert data as users are guided through triage and hunt workﬂows. In this paper, we will discuss our user-centric design methodology, feedback from user interviews, and the design requirements generated upon completion of our study. We will also present core functionality, ﬁndings from scenario-based testing, and future research for the Artemis platform.},
	language = {en},
	author = {Filar, Bobby and Seymour, Richard J and Park, Matthew},
	pages = {8},
}

@article{busse_replication_nodate,
	title = {Replication: {No} {One} {Can} {Hack} {My} {Mind} {Revisiting} a {Study} on {Expert} and {Non}-{Expert} {Security} {Practices} and {Advice}},
	abstract = {A 2015 study by Iulia Ion, Rob Reeder, and Sunny Consolvo examined the self-reported security behavior of security experts and non-experts. They also analyzed what kind of security advice experts gave to non-experts and how realistic and effective they think typical advice is.},
	language = {en},
	author = {Busse, Karoline and Schäfer, Julia and Smith, Matthew},
	pages = {21},
}

@misc{anton_devil_2019,
	title = {Devil in the {Detail}: {Attack} {Scenarios} in {Industrial} {Applications}},
	shorttitle = {Devil in the {Detail}},
	url = {http://arxiv.org/abs/1905.10292},
	doi = {10.48550/arXiv.1905.10292},
	abstract = {In the past years, industrial networks have become increasingly interconnected and opened to private or public networks. This leads to an increase in efficiency and manageability, but also increases the attack surface. Industrial networks often consist of legacy systems that have not been designed with security in mind. In the last decade, an increase in attacks on cyber-physical systems was observed, with drastic consequences on the physical work. In this work, attack vectors on industrial networks are categorised. A real-world process is simulated, attacks are then introduced. Finally, two machine learning-based methods for time series anomaly detection are employed to detect the attacks. Matrix Profiles are employed more successfully than a predictor Long Short-Term Memory network, a class of neural networks.},
	urldate = {2022-06-27},
	publisher = {arXiv},
	author = {Anton, Simon D. Duque and Hafner, Alexander and Schotten, Hans Dieter},
	month = may,
	year = {2019},
	note = {Number: arXiv:1905.10292
arXiv:1905.10292 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{alqhatani_there_nodate,
	title = {“{There} is nothing that {I} need to keep secret”: {Sharing} {Practices} and {Concerns} of {Wearable} {Fitness} {Data}},
	abstract = {There has been increasing use of commercial wearable devices for tracking fitness-related activities in the past few years. These devices sense and collect a variety of personal health and fitness data, which can be shared by users with different audiences. Yet, little is known about users’ practices for sharing information collected by these devices, and the concerns they have when disclosing this information across a variety of platforms. In this study, we conducted 30 semi-structured interviews with wearable fitness device users to understand their sharing intentions and practices, and to examine what they do to manage their privacy. We describe a set of common goals for sharing health and fitness information, which then influence users’ choices of the recipients and the specific practices they employ to share that information. Our findings indicate that participants were primarily concerned about acceptable norms and selfpresentation rather than the sensitivity of the information. Our results provide a set of common goals and practices which can inspire new applications and help improve existing platforms for sharing sensed fitness information.},
	language = {en},
	author = {Alqhatani, Abdulmajeed and Lipford, Heather Richter},
	pages = {15},
}

@inproceedings{everson_network_2020,
	title = {Network {Attack} {Surface} {Simplification} for {Red} and {Blue} {Teams}},
	doi = {10.1109/SecDev45635.2020.00027},
	abstract = {Network port scans are a key first step to developing a true understanding of a network-facing attack surface. However in large-scale networks, the data resulting from such scans can be too numerous for Red Teams to process for manual and semiautomatic testing. Indiscriminate port scans can also compromise a Red Team seeking to quickly gain a foothold on a network. A large attack surface can even complicate Blue Team activities like threat hunting. In this paper we provide a cluster analysis methodology designed to group similar hosts to reduce security team workload and Red Team observability. We also measure the Internet-facing network attack surface of 13 organizations by clustering their hosts based on similarity. Through a case study we demonstrate how the output of our clustering technique provides new insight to both Red and Blue Teams, allowing them to quickly identify potential high-interest points on the attack surface.},
	booktitle = {2020 {IEEE} {Secure} {Development} ({SecDev})},
	author = {Everson, Douglas and Cheng, Long},
	month = sep,
	year = {2020},
	keywords = {Cluster Analysis, Clustering algorithms, Complexity theory, Network Attack Surface, Organizations, Shape, Software algorithms, Surface treatment, Tools},
	pages = {74--80},
}

@inproceedings{acosta_cybersecurity_2020,
	title = {Cybersecurity {Deception} {Experimentation} {System}},
	doi = {10.1109/SecDev45635.2020.00022},
	abstract = {Cybersecurity deception research has had many successes in recent years. While early cyber deception systems (e.g., honeypots) were largely static, recent approaches leverage computational game theory and machine learning techniques to allow for dynamic deception strategies that can potentially observe, react, and adapt to an adversary in both the short and long term. However, applying these theoretical models and algorithms in real-world settings poses additional considerations that are not always apparent in theory. Currently, testbeds and experimentation platforms for dynamic deception are lacking, limiting the ability of researchers and analysts to test and evaluate these approaches using realistic scenarios and data. Honeypots are a technology where these dynamic deception methods can have a great impact on effectiveness. The basic technology to mimic nodes and network services has been used for several decades and is effective against less experienced adversaries, but is less useful against sophisticated intruders. Using adaptation, behavior-based model development and reasoning and other artificial intelligence techniques have the potential to make honeypots much more effective against experienced adversaries by making them less predictable and more targeted. Before these novel technologies can be used in the real world, it is critical that they are tested and validated on realistic systems and in realistic settings. We present the Cybersecurity Deception Experimentation System (CDES) that extends the Common Open Research Emulator (CORE) to provide a platform that is capable of evaluating dynamic deception algorithms. We also provide three case studies that demonstrate how CDES can be used to practically implement dynamic honeypots in increasingly complex scenarios and discuss some nuances of each implementation.},
	booktitle = {2020 {IEEE} {Secure} {Development} ({SecDev})},
	author = {Acosta, Jaime C. and Basak, Anjon and Kiekintveld, Christopher and Leslie, Nandi and Kamhoua, Charles},
	month = sep,
	year = {2020},
	keywords = {Computer security, Cyber Deception, Cybersecurity, Emulation, Graphical user interfaces, Monitoring, Network Security, Sockets, Software, Switches, Testbed, Tools},
	pages = {34--40},
}

@article{obada-obieh_challenges_nodate,
	title = {Challenges and {Threats} of {Mass} {Telecommuting}: {A} {Qualitative} {Study} of {Workers}},
	abstract = {This paper reports the security and privacy challenges and threats that people experience while working from home. We conducted semi-structured interviews with 24 participants working from home in the three weeks preceding the study. We asked questions related to participants’ challenges with telecommuting. Our results suggest that participants experienced challenges, threats, and potential outcomes of threats associated with the technological, human, organizational, and environmental dimensions. We also discovered two threat models: one in which the employer’s asset is at stake and another in which the employee’s privacy is compromised. We believe these insights can lead to better support for employees and possibly reduce cyber-attacks associated with telecommuting during the pandemic and beyond.},
	language = {en},
	author = {Obada-Obieh, Borke and Huang, Yue and Beznosov, Konstantin},
	pages = {21},
}

@article{brink_security_nodate,
	title = {{SECURITY} {AWARENESS} {TRAINING}: {SMALL} {INVESTMENT}, {LARGE} {REDUCTION} {IN} {RISK}},
	language = {en},
	author = {Brink, Derek E},
	pages = {14},
}

@techreport{cichonski_computer_2012,
	title = {Computer {Security} {Incident} {Handling} {Guide} : {Recommendations} of the {National} {Institute} of {Standards} and {Technology}},
	shorttitle = {Computer {Security} {Incident} {Handling} {Guide}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf},
	abstract = {Computer security incident response has become an important component of information technology (IT) programs. Because performing incident response effectively is a complex undertaking, establishing a successful incident response capability requires substantial planning and resources. This publication assists organizations in establishing computer security incident response capabilities and handling incidents efficiently and effectively. This publication provides guidelines for incident handling, particularly for analyzing incident-related data and determining the appropriate response to each incident. The guidelines can be followed independently of particular hardware platforms, operating systems, protocols, or applications.},
	language = {en},
	number = {NIST SP 800-61r2},
	urldate = {2022-04-12},
	institution = {National Institute of Standards and Technology},
	author = {Cichonski, Paul and Millar, Tom and Grance, Tim and Scarfone, Karen},
	month = aug,
	year = {2012},
	doi = {10.6028/NIST.SP.800-61r2},
	pages = {NIST SP 800--61r2},
}

@article{pennington_getting_nodate,
	title = {Getting {Started} with {ATT}\&{CK}},
	language = {en},
	author = {Pennington, Adam},
	pages = {45},
}

@article{noauthor_business_2015,
	title = {Business {Blackout}: {The} insurance implications of a cyber attack on the {US} power grid},
	language = {en},
	year = {2015},
	pages = {68},
}

@techreport{noauthor_ibm_nodate,
	title = {{IBM} {Cyber} {Report2022}},
}

@article{noauthor_cost_nodate,
	title = {Cost of a {Data} {Breach} {Report} 2020},
	language = {en},
	pages = {82},
}

@book{stadtler_supply_2008,
	address = {Berlin, Heidelberg},
	edition = {4th},
	title = {Supply {Chain} {Management} and {Advanced} {Planning}},
	isbn = {978-3-540-74511-2},
	url = {http://link.springer.com/10.1007/978-3-540-74512-9},
	abstract = {What is the essence of Supply Chain Management (SCM)? How does it relate to Advanced Planning? In which sense are the underlying planning concepts “advanced”? What are the origins of SCM? These as well as related questions will be answered in this chapter.},
	language = {en},
	urldate = {2022-06-24},
	publisher = {Springer Berlin Heidelberg},
	editor = {Stadtler, Hartmut and Kilger, Christoph},
	year = {2008},
	doi = {10.1007/978-3-540-74512-9},
}

@article{lamb_reproducible_2022,
	title = {Reproducible {Builds}: {Increasing} the {Integrity} of {Software} {Supply} {Chains}},
	volume = {39},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Reproducible {Builds}},
	url = {https://ieeexplore.ieee.org/document/9403390/},
	doi = {10.1109/MS.2021.3073045},
	number = {2},
	urldate = {2022-06-22},
	journal = {IEEE Software},
	author = {Lamb, Chris and Zacchiroli, Stefano},
	month = mar,
	year = {2022},
	pages = {62--70},
}

@techreport{paulsen_criticality_2018,
	title = {Criticality {Analysis} {Process} {Model}: {Prioritizing} {Systems} and {Components}},
	shorttitle = {Criticality {Analysis} {Process} {Model}},
	url = {https://csrc.nist.gov/publications/detail/nistir/8179/final},
	abstract = {In the modern world, where complex systems and systems-of-systems are integral to the functioning of society and businesses, it is increasingly important to be able to understand and manage risks that these systems and components may present to the missions that they support. However, in the world of finite resources, it is not possible to apply equal protection to all assets. This publication describes a comprehensive Criticality Analysis Process Model – a structured method of prioritizing programs, systems, and components based on their importance to the goals of an organization and the impact that their inadequate operation or loss may present to those goals. A criticality analysis can help organizations identify and better understand the systems, subsystems, components, and subcomponents that are most essential to their operations and the environment in which they operate. That understanding facilitates better decision making related to the management of an organization’s...},
	language = {en},
	number = {NIST Internal or Interagency Report (NISTIR) 8179},
	urldate = {2022-06-17},
	institution = {National Institute of Standards and Technology},
	author = {Paulsen, Celia and Boyens, Jon and Bartol, Nadya and Winkler, Kris},
	month = apr,
	year = {2018},
	doi = {10.6028/NIST.IR.8179},
}

@incollection{maurice_backstabbers_2020,
	address = {Cham},
	title = {Backstabber’s {Knife} {Collection}: {A} {Review} of {Open} {Source} {Software} {Supply} {Chain} {Attacks}},
	volume = {12223},
	isbn = {978-3-030-52682-5 978-3-030-52683-2},
	shorttitle = {Backstabber’s {Knife} {Collection}},
	url = {http://link.springer.com/10.1007/978-3-030-52683-2_2},
	abstract = {A software supply chain attack is characterized by the injection of malicious code into a software package in order to compromise dependent systems further down the chain. Recent years saw a number of supply chain attacks that leverage the increasing use of open source during software development, which is facilitated by dependency managers that automatically resolve, download and install hundreds of open source packages throughout the software life cycle. Even though many approaches for detection and discovery of vulnerable packages exist, no prior work has focused on malicious packages. This paper presents a dataset as well as analysis of 174 malicious software packages that were used in real-world attacks on open source software supply chains and which were distributed via the popular package repositories npm, PyPI, and RubyGems. Those packages, dating from November 2015 to November 2019, were manually collected and analyzed. This work is meant to facilitate the future development of preventive and detective safeguards by open source and research communities.},
	language = {en},
	urldate = {2022-06-23},
	booktitle = {Detection of {Intrusions} and {Malware}, and {Vulnerability} {Assessment}},
	publisher = {Springer International Publishing},
	author = {Ohm, Marc and Plate, Henrik and Sykosch, Arnold and Meier, Michael},
	editor = {Maurice, Clémentine and Bilge, Leyla and Stringhini, Gianluca and Neves, Nuno},
	year = {2020},
	doi = {10.1007/978-3-030-52683-2_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {23--43},
}

@misc{kaczorowski_secure_2020,
	title = {Secure at every step: {What} is software supply chain security and why does it matter?},
	shorttitle = {Secure at every step},
	url = {https://github.blog/2020-09-02-secure-your-software-supply-chain-and-protect-against-supply-chain-threats-github-blog/},
	abstract = {The most important way to protect supply chain threats - scan code for security vulnerabilities, learn how to find vulnerabilities in code, and quickly patch them with dynamic code analysis tools.},
	language = {en-US},
	urldate = {2022-06-21},
	journal = {The GitHub Blog},
	author = {Kaczorowski, Maya},
	month = sep,
	year = {2020},
}

@misc{noauthor_3_nodate,
	title = {3 {Ways} to {Mitigate} {Risk} {When} {Using} {Private} {Package} {Feeds}},
	url = {https://azure.microsoft.com/en-us/resources/3-ways-to-mitigate-risk-using-private-package-feeds/},
	abstract = {Software today has become an assembly of components from a wide range of sources. Many organizations use public package feeds to take advantage of the open ecosystems they offer. Projects that consume packages from multiple public and private feeds may be exposed to supply chain vulnerabilitie...},
	language = {en},
	urldate = {2022-06-21},
}

@techreport{noauthor_shifting_nodate,
	title = {Shifting left on security: {Securing} software supply chains {\textbar} {Solutions}},
	shorttitle = {Shifting left on security},
	url = {https://cloud.google.com/solutions/shifting-left-on-security},
	language = {en},
	urldate = {2022-06-21},
}

@inproceedings{pletea_security_2014,
	address = {New York, NY, USA},
	series = {{MSR} 2014},
	title = {Security and emotion: sentiment analysis of security discussions on {GitHub}},
	isbn = {978-1-4503-2863-0},
	shorttitle = {Security and emotion},
	url = {https://doi.org/10.1145/2597073.2597117},
	doi = {10.1145/2597073.2597117},
	abstract = {Application security is becoming increasingly prevalent during software and especially web application development. Consequently, countermeasures are continuously being discussed and built into applications, with the goal of reducing the risk that unauthorized code will be able to access, steal, modify, or delete sensitive data. In this paper we gauged the presence and atmosphere surrounding security-related discussions on GitHub, as mined from discussions around commits and pull requests. First, we found that security related discussions account for approximately 10\% of all discussions on GitHub. Second, we found that more negative emotions are expressed in security-related discussions than in other discussions. These findings confirm the importance of properly training developers to address security concerns in their applications as well as the need to test applications thoroughly for security vulnerabilities in order to reduce frustration and improve overall project atmosphere.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 11th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Pletea, Daniel and Vasilescu, Bogdan and Serebrenik, Alexander},
	month = may,
	year = {2014},
	keywords = {GitHub, Security, mining challenge, sentiment analysis},
	pages = {348--351},
}

@inproceedings{dors_reflective_2020,
	title = {Reflective {Practice} in {Software} {Development} {Studios}: {Findings} from an {Ethnographic} {Study}},
	shorttitle = {Reflective {Practice} in {Software} {Development} {Studios}},
	doi = {10.1109/CSEET49119.2020.9206217},
	abstract = {Over the last two decades, software educators have adopted new approaches, techniques, and tools for practical learning. Previous research has found that studio-based learning, which has been in use by some design and architecture courses, is suitable for learning the practical aspects of software engineering. The studies available recognize the presence of reflective practice in software development studios; however, they do not show empirical evidence of its contribution to learning. The goal of this study is to understand the use of reflective practice in software development studios and its contributions to the practical learning of software engineering. The qualitative data were collected using an ethnographic method through participant observation and from written students' self-reflections, which were analyzed using Cycle Coding Method supported by Atlas.ti Qualitative Data Analysis tool. The findings suggest that reflective practice promotes the emergence of new ideas while contributing to the development of skills that are valuable to software engineering professional practice. This research presents a qualitative look at how these are developed in the context of a particular software development studio.},
	booktitle = {2020 {IEEE} 32nd {Conference} on {Software} {Engineering} {Education} and {Training} ({CSEE}\&{T})},
	author = {Dors, Tania Mara and Van Amstel, Frederick M. C. and Binder, Fabio and Reinehr, Sheila and Malucelli, Andreia},
	month = nov,
	year = {2020},
	note = {ISSN: 2377-570X},
	keywords = {Computer Science Practical Learning, Computer architecture, Education, Reflection, Reflective Practice, Software Engineering Education, Software Engineering Practical Skills Development, Software Practice, Software design, Software engineering, Studio-based Learning, Tools},
	pages = {1--10},
}

@inproceedings{Zahan2022WeakLinksinNPMSupplyChain,
	title = {What are {Weak} {Links} in the npm {Supply} {Chain}?},
	abstract = {Modern software development frequently uses third-party packages, raising the concern of supply chain security attacks. Many attackers target popular package managers, like npm, and their users with supply chain attacks. In 2021 there was a 650\% year-on-year growth in security attacks by exploiting Open Source Software's supply chain. Proactive approaches are needed to predict package vulnerability to high-risk supply chain attacks. The goal of this work is to help software developers and security specialists in measuring npm supply chain weak link signals to prevent future supply chain attacks by empirically studying npm package metadata. In this paper, we analyzed the metadata of 1.63 million JavaScript npm packages. We propose six signals of security weaknesses in a software supply chain, such as the presence of install scripts, maintainer accounts associated with an expired email domain, and inactive packages with inactive maintainers. One of our case studies identified 11 malicious packages from the install scripts signal. We also found 2,818 maintainer email addresses associated with expired domains, allowing an attacker to hijack 8,494 packages by taking over the npm accounts. We obtained feedback on our weak link signals through a survey responded to by 470 npm package developers. The majority of the developers supported three out of our six proposed weak link signals. The developers also indicated that they would want to be notified about weak links signals before using third-party packages. Additionally, we discussed eight new signals suggested by package developers.},
	booktitle = {{ICSE}},
	author = {Zahan, Nusrat and Zimmermann, Tom and Godefroid, Patrice and Murphy, Brendan and Maddila, Chandra and Williams, Laurie},
	year = {2022},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{yan_learning_2020,
	title = {Learning {From} {Design} {Failure}, {Collaboratively}},
	abstract = {Effective design requires iterative cycles of learning from failure, where design teams evaluate their designs to identify problems and make iterative improvement. However, existing learning from failure models do not always take collaborative process, a key aspect of design, into account. The ultimate goal is to design tools to better support young design teams’ collaborative learning from design failure process in an after-school design club with students in grades 4-6. In this article, we examine how teams use discourse to learn from design failure collaboratively by analyzing video data and group artifacts. Our findings suggest that the specific communication process can support or hinder the learning from failure in design contexts and that there’s a need to support team’s regulation process so they can benefit from their design failure.},
	language = {en},
	journal = {ICLS},
	author = {Yan, Shulong and Borge, Marcela},
	year = {2020},
	pages = {8},
}

@inproceedings{rashidi_you_2018,
	title = {"{You} don't want to be the next meme": {College} {Students}' {Workarounds} to {Manage} {Privacy} in the {Era} of {Pervasive} {Photography}},
	isbn = {978-1-939133-10-6},
	shorttitle = {"{You} don't want to be the next meme"},
	url = {https://www.usenix.org/conference/soups2018/presentation/rashidi},
	language = {en},
	urldate = {2022-06-17},
	author = {Rashidi, Yasmeen and Ahmed, Tousif and Patel, Felicia and Fath, Emily and Kapadia, Apu and Nippert-Eng, Christena and Su, Norman Makoto},
	year = {2018},
	pages = {143--157},
}

@inproceedings{pham_aflnet_2020,
	title = {{AFLNET}: {A} {Greybox} {Fuzzer} for {Network} {Protocols}},
	shorttitle = {{AFLNET}},
	doi = {10.1109/ICST46399.2020.00062},
	abstract = {Server fuzzing is difficult. Unlike simple command-line tools, servers feature a massive state space that can be traversed effectively only with well-defined sequences of input messages. Valid sequences are specified in a protocol. In this paper, we present AFLNET, the first greybox fuzzer for protocol implementations. Unlike existing protocol fuzzers, AFLNET takes a mutational approach and uses state-feedback to guide the fuzzing process. AFLNET is seeded with a corpus of recorded message exchanges between the server and an actual client. No protocol specification or message grammars are required. AFLNET acts as a client and replays variations of the original sequence of messages sent to the server and retains those variations that were effective at increasing the coverage of the code or state space. To identify the server states that are exercised by a message sequence, AFLNET uses the server's response codes. From this feedback, AFLNET identifies progressive regions in the state space, and systematically steers towards such regions. The case studies with AFLNET on two popular protocol implementations demonstrate a substantial performance boost over the state-of the-art. AFLNET discovered two new CVEs which are classified as critical (CVSS score CRITICAL 9.8).},
	booktitle = {2020 {IEEE} 13th {International} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	author = {Pham, Van-Thuan and Böhme, Marcel and Roychoudhury, Abhik},
	month = oct,
	year = {2020},
	note = {ISSN: 2159-4848},
	keywords = {Computer bugs, Data models, Fuzzing, Protocols, Security, Servers, Tools},
	pages = {460--465},
}

@misc{natella_stateafl_2021,
	title = {{StateAFL}: {Greybox} {Fuzzing} for {Stateful} {Network} {Servers}},
	shorttitle = {{StateAFL}},
	url = {http://arxiv.org/abs/2110.06253},
	doi = {10.48550/arXiv.2110.06253},
	abstract = {Fuzzing network servers is a technical challenge, since the behavior of the target server depends on its state over a sequence of multiple messages. Existing solutions are costly and difficult to use, as they rely on manually-customized artifacts such as protocol models, protocol parsers, and learning frameworks. The aim of this work is to develop a greybox fuzzer for network servers that only relies on lightweight analysis of the target program, with no manual customization, in a similar way to what the AFL fuzzer achieved for stateless programs. The proposed fuzzer instruments the target server at compile-time, to insert probes on memory allocations and network I/O operations. At run-time, it infers the current protocol state of the target by analyzing snapshots of long-lived memory areas, and incrementally builds a protocol state machine for guiding fuzzing. The experimental results show that the fuzzer can be applied with no manual customization on a large set of network servers for popular protocols, and that it can achieve comparable, or even better code coverage than customized fuzzing. Moreover, our qualitative analysis shows that states inferred from memory better reflect the server behavior than only using response codes from messages.},
	urldate = {2022-06-17},
	publisher = {arXiv},
	author = {Natella, Roberto},
	month = oct,
	year = {2021},
	note = {Number: arXiv:2110.06253
arXiv:2110.06253 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Operating Systems, Computer Science - Software Engineering},
}

@misc{congdon1999techniques,
	title = {Techniques and recommendations for implementing valuable postmortems in software development projects},
	publisher = {May},
	author = {Congdon, Gloria C},
	year = {1999},
}

@article{norman1990commentary,
	title = {Commentary: {Human} error and the design of computer systems},
	volume = {33},
	number = {1},
	journal = {Communications of the ACM},
	author = {Norman, Donald A},
	year = {1990},
	note = {Publisher: Association for Computing Machinery, Inc.},
	pages = {4--7},
}

@article{tchankova_risk_2002,
	title = {Risk identification – basic stage in risk management},
	volume = {13},
	issn = {0956-6163},
	url = {https://doi.org/10.1108/09566160210431088},
	doi = {10.1108/09566160210431088},
	abstract = {In this paper risk identification is investigated as a basic stage in risk management. The risk identification phase as the first stage in the risk management process is presented and its leading role for effective risk management is proved. The basic terms that are necessary for building of the frame approach for risk identification are defined: sources of risk‐hazard, factor‐peril‐resources exposed to risk. A classification of risk sources – physical, social, political, operational, economic, legal and cognitive environment – is proposed. It allows covering all types of risk facing the organisation. A grouping of the resources exposed to risk such as physical, human, and financial resources is introduced. It is based on a practical consideration of the risk situations in the organisations.},
	number = {3},
	urldate = {2022-06-16},
	journal = {Environmental Management and Health},
	author = {Tchankova, Lubka},
	month = jan,
	year = {2002},
	note = {Publisher: MCB UP Ltd},
	keywords = {Risk, Risk management},
	pages = {290--297},
}

@inproceedings{gunawi2014bugs,
	title = {What bugs live in the cloud? a study of 3000+ issues in cloud systems},
	booktitle = {Proceedings of the {ACM} symposium on cloud computing ({SOCC})},
	author = {Gunawi, Haryadi S and Hao, Mingzhe and Leesatapornwongsa, Tanakorn and Patana-anake, Tiratat and Do, Thanh and Adityatama, Jeffry and Eliazar, Kurnia J and Laksono, Agung and Lukman, Jeffrey F and Martin, Vincentius and {others}},
	year = {2014},
	pages = {1--14},
}

@article{mazuera2019android,
	title = {The {Android} {OS} stack and its vulnerabilities: an empirical study},
	volume = {24},
	number = {4},
	journal = {Empirical Software Engineering},
	author = {Mazuera-Rozo, Alejandro and Bautista-Mora, Jairo and Linares-Vásquez, Mario and Rueda, Sandra and Bavota, Gabriele},
	year = {2019},
	note = {Publisher: Springer},
	pages = {2056--2101},
}

@inproceedings{li2021understanding,
	title = {Understanding and detecting performance bugs in markdown compilers},
	booktitle = {2021 36th {IEEE}/{ACM} international conference on automated software engineering ({ASE})},
	author = {Li, Penghui and Liu, Yinxi and Meng, Wei},
	year = {2021},
	note = {tex.organization: IEEE},
	pages = {892--904},
}

@inproceedings{chen2019understanding,
	title = {Understanding exception-related bugs in large-scale cloud systems},
	booktitle = {2019 34th {IEEE}/{ACM} international conference on automated software engineering ({ASE})},
	author = {Chen, Haicheng and Dou, Wensheng and Jiang, Yanyan and Qin, Feng},
	year = {2019},
	note = {tex.organization: IEEE},
	pages = {339--351},
}

@inproceedings{wang_argulens_2020,
	address = {New York, NY, USA},
	title = {{ArguLens}: {Anatomy} of {Community} {Opinions} {On} {Usability} {Issues} {Using} {Argumentation} {Models}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {{ArguLens}},
	url = {https://doi.org/10.1145/3313831.3376218},
	abstract = {In open-source software (OSS), the design of usability is often influenced by the discussions among community members on platforms such as issue tracking systems (ITSs). However, digesting the rich information embedded in issue discussions can be a major challenge due to the vast number and diversity of the comments. We propose and evaluate ArguLens, a conceptual framework and automated technique leveraging an argumentation model to support effective understanding and consolidation of community opinions in ITSs. Through content analysis, we anatomized highly discussed usability issues from a large, active OSS project, into their argumentation components and standpoints. We then experimented with supervised machine learning techniques for automated argument extraction. Finally, through a study with experienced ITS users, we show that the information provided by ArguLens supported the digestion of usability-related opinions and facilitated the review of lengthy issues. ArguLens provides the direction of designing valuable tools for high-level reasoning and effective discussion about usability.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Wenting and Arya, Deeksha and Novielli, Nicole and Cheng, Jinghui and Guo, Jin L.C.},
	month = apr,
	year = {2020},
	keywords = {argumentation analysis, issue discussion analysis, online communities, open source software, usability},
	pages = {1--14},
}

@misc{noauthor_managed_nodate,
	title = {Managed {Detection} and {Response} ({MDR}) {Services} {\textbar} {Mandiant}},
	url = {https://www.mandiant.com/advantage/managed-defense},
	abstract = {Mandiant's Managed Detection and Response (MDR) service defends your business across endpoint, network, cloud, email, and operational technology.},
	language = {en},
	urldate = {2022-06-14},
}

@article{infocyte_wwwinfocytecom_nodate,
	title = {www.infocyte.com @{InfocyteInc}},
	language = {en},
	author = {Infocyte, Reserved and Hunt, Infocyte},
	pages = {2},
}

@misc{noauthor_falcon_2022,
	title = {Falcon {Overwatch}: {Managed} \& {Proactive} {Threat} {Hunting} {\textbar} {CrowdStrike}},
	shorttitle = {Falcon {Overwatch}},
	url = {https://www.crowdstrike.com/endpoint-security-products/falcon-overwatch-threat-hunting/},
	abstract = {Falcon OverWatch is a managed hunting service to help prioritize \& respond to threats created by security professionals. View the benefits of OverWatch here!},
	language = {en},
	urldate = {2022-02-19},
	journal = {crowdstrike.com},
	month = feb,
	year = {2022},
}

@misc{noauthor_detection_2022,
	title = {Detection and {Prevention} {\textbar} {CISA}},
	url = {https://www.cisa.gov/detection-and-prevention},
	urldate = {2022-06-08},
	month = jun,
	year = {2022},
}

@phdthesis{celikAUTOMATEDIOTSECURITY2019,
	title = {Automated {IoT} {Security} and {Privacy} {ANalysis}},
	language = {en},
	school = {The Pennsylvania State University},
	author = {Celik, Zeynel Berkay},
	year = {2019},
}

@misc{noauthor_all_2022,
	title = {All {Cyber} {Mission} {Force} {Teams} {Achieve} {Initial} {Operating} {Capability}},
	url = {https://www.defense.gov/News/News-Stories/Article/Article/984663/all-cyber-mission-force-teams-achieve-initial-operating-capability/},
	abstract = {All 133 of U.S. Cyber Command’s Cyber Mission Force teams have reached a threshold level of initial operating capacity and can execute their fundamental mission, Cybercom officials announced.},
	language = {en-US},
	urldate = {2022-06-08},
	journal = {U.S. Department of Defense},
	month = jun,
	year = {2022},
}

@misc{mcclanahan_169th_2019,
	title = {169th {Cyber} {Protection} {Team} – {Highly} {Capable}, {Always} {Ready}},
	url = {https://news.maryland.gov/ng/2019/11/06/169th-cyber-protection-team-highly-capable-always-ready/},
	language = {en-US},
	urldate = {2022-02-19},
	journal = {Maryland National Guard News},
	author = {McClanahan, Sarah M.},
	month = nov,
	year = {2019},
}

@misc{noauthor_threat_nodate,
	title = {Threat {Hunting}},
	url = {https://www.boozallen.com/expertise/cybersecurity/threat-hunting.html},
	abstract = {Our cyber threat hunting services can find advanced persistent threats to reduce operational, financial risk.},
	language = {en},
	urldate = {2022-02-19},
}

@article{trent_modelling_2019,
	title = {Modelling the {Cognitive} {Work} of {Cyber} {Protection} {Teams}},
	volume = {4},
	issn = {2474-2120},
	url = {http://www.jstor.org/stable/26623071},
	abstract = {Cyber Protection Teams (CPTs) defend our Nation’s critical military networks. While Cyber Security Service Providers are responsible for the continuous monitoring and vulnerability patching of networks, CPTs perform threat-oriented missions to defeat adversaries within and through cyberspace. The research we report here provides a descriptive workflow of cyber defense in CPTs as well as a prescriptive work model that all CPTs should be capable of executing. This paper describes how these models were developed and used to assess technologies and performance of CPTs. Such models offer a variety of benefits to practitioner and research communities, particularly when the domain of practice is closed to most researchers. This project demonstrates the need for continual curation of CPT work models as well as the need for models of work for the other types of cyber teams (i.e. Mission and Support) in the Cyber Mission Force.},
	number = {1},
	urldate = {2022-02-19},
	journal = {The Cyber Defense Review},
	author = {Trent, Stoney and Hoffman, Robert R. and Merritt, David and Smith, Sarah},
	year = {2019},
	note = {Publisher: Army Cyber Institute},
	pages = {125--136},
}

@misc{noauthor_ctir_nodate,
	title = {{CTIR} - {Threat} {Hunting} {\textbar}{\textbar} {Cisco} {Talos} {Intelligence} {Group} - {Comprehensive} {Threat} {Intelligence}},
	url = {https://talosintelligence.com/incident_response/hunting},
	urldate = {2022-06-14},
}

@misc{noauthor_executive_2021,
	title = {Executive {Order} on {Improving} the {Nation}'s {Cybersecurity}},
	url = {https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/},
	abstract = {By the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:Section 1.},
	language = {en-US},
	urldate = {2022-06-14},
	journal = {The White House},
	month = may,
	year = {2021},
}

@incollection{lewkowicz_supporting_2010,
	address = {London},
	title = {Supporting {Reflection} in {Software} {Development} with {Everyday} {Working} {Tools}},
	isbn = {978-1-84996-210-0 978-1-84996-211-7},
	url = {http://link.springer.com/10.1007/978-1-84996-211-7_9},
	abstract = {Through their day-to-day usage collaboration tools collect data on the work process. These data can be used to aid participants’ retrospective reflection on the process. The paper shows how this can be done in software development project work. Through a case study we demonstrate how retrospective reflection was conducted by use of an industry approach to project retrospectives combined with the examination of historical data in Trac, an issue tracker. The data helped the team reconstruct the project trajectory by aiding the recall of significant events, leading to a shift in the team’s perspective on the project. The success of the toolaided retrospective reflection is attributed to its organization as well as the type of historical data examined through the tool and the tool features for navigating the data. These insights can be used to help project teams determine the potential of their tools to aid retrospective reflection.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of {COOP} 2010},
	publisher = {Springer London},
	author = {Krogstie, Birgit and Divitini, Monica},
	editor = {Lewkowicz, Myriam and Hassanaly, Parina and Wulf, Volker and Rohde, Markus},
	year = {2010},
	doi = {10.1007/978-1-84996-211-7_9},
	pages = {141--162},
}

@book{beyer2016site,
	title = {Site reliability engineering: {How} {Google} runs production systems},
	publisher = {" O'Reilly Media, Inc."},
	author = {Beyer, Betsy and Jones, Chris and Petoff, Jennifer and Murphy, Niall Richard},
	year = {2016},
}

@techreport{noauthor_isoiecieee_2017,
	title = {{ISO}/{IEC}/{IEEE} {International} {Standard} - {Systems} and software engineering -- {Software} life cycle processes},
	url = {https://ieeexplore.ieee.org/document/8100771/},
	language = {en},
	number = {12207:2017(E)},
	urldate = {2022-01-19},
	institution = {IEEE},
	year = {2017},
	doi = {10.1109/IEEESTD.2017.8100771},
	note = {ISBN: 9781504442534},
}

@techreport{ISO90003,
	address = {Geneva, CH},
	type = {Standard},
	title = {{ISO}/{IEC}/{IEEE} 90003:2018 {Software} engineering — {Guidelines} for the application of {ISO} 9001:2015 to computer software},
	institution = {International Organization for Standardization},
	year = {2018},
	note = {Volume: 2018
tex.key: ISO 90003:2018(E)},
}

@techreport{ISO9001,
	address = {Geneva, CH},
	type = {Standard},
	title = {{ISO} 9001: {Quality} management systems-requirements},
	institution = {International Organization for Standardization},
	year = {2015},
	note = {Volume: 2015
tex.key: ISO 9001:2015(E)},
}

@techreport{noauthor_ieee_2014,
	title = {{IEEE} {Standard} for {Software} {Quality} {Assurance} {Processes}},
	abstract = {Abstract: Requirements for initiating, planning, controlling, and executing the Software Quality Assurance processes of a software development or maintenance project are established in this standard. This standard is harmonized with the software life cycle process of ISO/IEC/IEEE 12207:2008 and the information content requirements of ISO/IEC/IEEE 15289:2011.},
	language = {en},
	year = {2014},
	pages = {138},
}

@book{humphrey1995discipline,
	title = {A discipline for software engineering},
	publisher = {Pearson Education India},
	author = {Humphrey, Watts S},
	year = {1995},
}

@article{lyytinen_learning_1999,
	title = {Learning failure in information systems development},
	volume = {9},
	issn = {1350-1917, 1365-2575},
	url = {http://doi.wiley.com/10.1046/j.1365-2575.1999.00051.x},
	doi = {10.1046/j.1365-2575.1999.00051.x},
	abstract = {Information systems development is a high-risk undertaking, and failures remain common despite advances in development tools and technologies. In this paper, we argue that one reason for this is the collapse of organizational intelligence required to deal with the complexities of systems development. Organizations fail to learn from their experience in systems development because of limits of organizational intelligence, disincentives for learning, organizational designs and educational barriers. Not only have many organizations failed to learn, but they have also learned to fail. Over time they accept and expect poor performance while creating organizational myths that perpetuate short-term optimization. This paper illustrates learning failure in systems development and recommends tactics for overcoming it.},
	language = {en},
	number = {2},
	urldate = {2022-02-14},
	journal = {Information Systems Journal},
	author = {Lyytinen, Kalle and Robey, Daniel},
	month = apr,
	year = {1999},
	pages = {85--101},
}

@inproceedings{buttler_rethinking_2012,
	address = {Graz, Austria},
	title = {Rethinking lessons learned capturing: using storytelling, root cause analysis, and collaboration engineering to capture lessons learned about project management},
	isbn = {978-1-4503-1242-4},
	shorttitle = {Rethinking lessons learned capturing},
	url = {http://dl.acm.org/citation.cfm?doid=2362456.2362460},
	doi = {10.1145/2362456.2362460},
	abstract = {Lessons learned are one way to retain experience and knowledge in project-based organizations, helping them to prevent reinventing the wheel or repeating past mistakes. However, there are several challenges that make capturing lessons learned a challenging endeavor. These include capturing knowledge about project management, allowing learning from mistakes, and handling the group processes within the project team. We introduce a novel approach combining elements from storytelling, root cause analysis, and collaboration engineering to address these challenges, and report on ﬁrst experiences utilizing this approach in a project in the oil and gas industry.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Knowledge} {Management} and {Knowledge} {Technologies} - i-{KNOW} '12},
	publisher = {ACM Press},
	author = {Buttler, Tanja and Lukosch, Stephan},
	year = {2012},
	pages = {1},
}

@article{baaz_appreciating_2010,
	title = {Appreciating {Lessons} {Learned}},
	volume = {27},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/5339121/},
	doi = {10.1109/MS.2009.198},
	language = {en},
	number = {4},
	urldate = {2022-06-14},
	journal = {IEEE Software},
	author = {Baaz, Anders and Holmberg, Lena and Nilsson, Agneta and Olsson, Helena Holmström and Sandberg, Anna Börjesson},
	month = jul,
	year = {2010},
	pages = {72--79},
}

@phdthesis{krogstie2010work,
	title = {The work-reflection-learning cycle in software engineering student projects: {Use} of collaboration tools},
	school = {Citeseer},
	author = {Krogstie, Birgit Rognebakke},
	year = {2010},
}

@misc{noauthor_california_2018,
	title = {California {Consumer} {Privacy} {Act} ({CCPA})},
	url = {https://oag.ca.gov/privacy/ccpa},
	abstract = {The California Consumer Privacy Act of 2018 (CCPA) gives consumers more control over the personal information that businesses collect about them and the CCPA regulations provide guidance on how to implement the law.},
	language = {en},
	urldate = {2022-06-13},
	journal = {State of California - Department of Justice - Office of the Attorney General},
	month = oct,
	year = {2018},
}

@inproceedings{tondel_using_2020,
	address = {Bari Italy},
	title = {Using {Situational} and {Narrative} {Analysis} for {Investigating} the {Messiness} of {Software} {Security}},
	isbn = {978-1-4503-7580-1},
	url = {https://dl.acm.org/doi/10.1145/3382494.3422162},
	doi = {10.1145/3382494.3422162},
	abstract = {Background: Software engineering work and its context often has characteristics of what in social science is termed ‘messy’; it has ephemeral and irregular qualities. This puts high demands on researchers doing inquiry and analysis. Aims: This paper aims to show what a combination of situational analysis (SA) and narrative analysis (NA) can bring to qualitative software engineering research, and in particular for situations characterised by mess. Method: SA and NA were applied to a case study on software security. Results: We found that these analysis methods helped us gain new insights and understandings and a broader perspective of the situation we are studying. Additionally, the methods helped collaboration in the analysis. Conclusion: We recommend applying and studying these and similar combinations of analysis approaches further.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 14th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {ACM},
	author = {Tøndel, Inger Anne and Cruzes, Daniela Soares and Jaatun, Martin Gilje},
	month = oct,
	year = {2020},
	pages = {1--6},
}

@inproceedings{noauthor_grounded_2015,
	address = {Florence, Italy},
	title = {Grounded {Theory} in {Software} {Engineering} {Research}: {A} {Critical} {Review} and {Guidelines}},
	isbn = {978-1-4799-1934-5},
	abstract = {Grounded Theory (GT) has proved an extremely useful research approach in several fields including medical sociology, nursing, education and management theory. However, GT is a complex method based on an inductive paradigm that is fundamentally different from the traditional hypothetico-deductive research model. As there are at least three variants of GT, some ostensibly GT research suffers from method slurring, where researchers adopt an arbitrary subset of GT practices that are not recognizable as GT. In this paper, we describe the variants of GT and identify the core set of GT practices. We then analyze the use of grounded theory in software engineering. We carefully and systematically selected 98 articles that mention GT, of which 52 explicitly claim to use GT, with the other 46 using GT techniques only. Only 16 articles provide detailed accounts of their research procedures. We offer guidelines to improve the quality of both conducting and reporting GT studies. The latter is an important extension since current GT guidelines in software engineering do not cover the reporting process, despite good reporting being necessary for evaluating a study and informing subsequent research.},
	language = {en},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	year = {2015},
}

@article{corbin1990grounded,
	title = {Grounded theory research: {Procedures}, canons, and evaluative criteria},
	volume = {13},
	number = {1},
	journal = {Qualitative sociology},
	author = {Corbin, Juliet M and Strauss, Anselm},
	year = {1990},
	note = {Number: 1
Publisher: Springer},
	pages = {3--21},
}

@misc{orchilles_cyber_2022,
	title = {Cyber {Kill} {Chain}, {MITRE} {ATT}\&{CK}, and {Purple} {Team} {\textbar} {SANS} {Institute}},
	url = {https://www.sans.org/blog/cyber-kill-chain-mitre-attack-purple-team/},
	urldate = {2022-06-08},
	author = {Orchilles, Jorge},
	month = mar,
	year = {2022},
}

@misc{cycraft_technologycorp_cycraft_2022,
	title = {{CyCraft} {Classroom}: {MITRE} {ATT}\&{CK} vs. {Cyber} {Kill} {Chain} vs. {Diamond} {Model}},
	shorttitle = {{CyCraft} {Classroom}},
	url = {https://medium.com/cycraft/cycraft-classroom-mitre-att-ck-vs-cyber-kill-chain-vs-diamond-model-1cc8fa49a20f},
	abstract = {In cybersecurity, there have been several approaches used to track and analyze the various characteristics of cyber intrusions by advanced…},
	language = {en},
	urldate = {2022-06-08},
	journal = {CyCraft},
	author = {{CyCraft TechnologyCorp}},
	month = feb,
	year = {2022},
}

@misc{noauthor_cyber_2022,
	title = {Cyber {Threat} {Hunting} {Solutions} {\textbar} {IBM}},
	url = {https://www.ibm.com/qradar/threat-hunting},
	abstract = {Incorporate IBM Security cyber threat hunting solutions into your security strategy to counter and mitigate threats more quickly.},
	language = {en-us},
	urldate = {2022-06-02},
	month = jun,
	year = {2022},
}

@misc{noauthor_threat_2022,
	title = {Threat {Hunting} {Tools}},
	url = {https://www.cyrebro.io/threat-hunting/},
	abstract = {Deal with Potential Challenges Via CYREBRO's Threat Hunting Tools to detect, block and protect from various cyber threats.},
	language = {en-US},
	urldate = {2022-06-02},
	journal = {CYREBRO},
	month = jun,
	year = {2022},
}

@article{curtis_software_1986,
	title = {Software psychology: {The} need for an interdisciplinary program},
	volume = {74},
	issn = {0018-9219},
	shorttitle = {Software psychology},
	url = {http://ieeexplore.ieee.org/document/1457864/},
	doi = {10.1109/PROC.1986.13596},
	language = {en},
	number = {8},
	urldate = {2022-05-06},
	journal = {Proceedings of the IEEE},
	author = {Curtis, B. and Soloway, E.M. and Brooks, R.E. and Black, J.B. and Ehrlich, K. and Ramsey, H.R.},
	year = {1986},
	note = {Number: 8},
	pages = {1092--1106},
}

@article{inayat_systematic_2015,
	title = {A systematic literature review on agile requirements engineering practices and challenges},
	volume = {51},
	issn = {07475632},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S074756321400569X},
	doi = {10.1016/j.chb.2014.10.046},
	abstract = {Unlike traditional software development methods, agile methods are marked by extensive collaboration, i.e. face-to-face communication. Although claimed to be beneﬁcial, the software development community as a whole is still unfamiliar with the role of the requirements engineering practices in agile methods. The term ‘‘agile requirements engineering’’ is used to deﬁne the ‘‘agile way’’ of planning, executing and reasoning about requirements engineering activities. Moreover, not much is known about the challenges posed by collaboration-oriented agile way of dealing with requirements engineering activities. Our goal is to map the evidence available about requirements engineering practices adopted and challenges faced by agile teams in order to understand how traditional requirements engineering issues are resolved using agile requirements engineering. We conducted a systematic review of literature published between 2002 and June 2013 and identiﬁed 21 papers, that discuss agile requirements engineering. We formulated and applied speciﬁc inclusion and exclusion criteria in two distinct rounds to determine the most relevant studies for our research goal. The review identiﬁed 17 practices of agile requirements engineering, ﬁve challenges traceable to traditional requirements engineering that were overcome by agile requirements engineering, and eight challenges posed by the practice of agile requirements engineering. However, our ﬁndings suggest that agile requirements engineering as a research context needs additional attention and more empirical results are required to better understand the impact of agile requirements engineering practices e.g. dealing with non-functional requirements and self-organising teams.},
	language = {en},
	urldate = {2022-05-06},
	journal = {Computers in Human Behavior},
	author = {Inayat, Irum and Salim, Siti Salwah and Marczak, Sabrina and Daneva, Maya and Shamshirband, Shahaboddin},
	month = oct,
	year = {2015},
	pages = {915--929},
}

@article{ambreen_empirical_2018,
	title = {Empirical research in requirements engineering: trends and opportunities},
	volume = {23},
	issn = {0947-3602, 1432-010X},
	shorttitle = {Empirical research in requirements engineering},
	url = {http://link.springer.com/10.1007/s00766-016-0258-2},
	doi = {10.1007/s00766-016-0258-2},
	language = {en},
	number = {1},
	urldate = {2022-05-06},
	journal = {Requirements Engineering},
	author = {Ambreen, Talat and Ikram, Naveed and Usman, Muhammad and Niazi, Mahmood},
	month = mar,
	year = {2018},
	note = {Number: 1},
	pages = {63--95},
}

@article{dermeval_applications_2016,
	title = {Applications of ontologies in requirements engineering: a systematic review of the literature},
	volume = {21},
	issn = {0947-3602, 1432-010X},
	shorttitle = {Applications of ontologies in requirements engineering},
	url = {http://link.springer.com/10.1007/s00766-015-0222-6},
	doi = {10.1007/s00766-015-0222-6},
	language = {en},
	number = {4},
	urldate = {2022-05-06},
	journal = {Requirements Engineering},
	author = {Dermeval, Diego and Vilela, Jéssyka and Bittencourt, Ig Ibert and Castro, Jaelson and Isotani, Seiji and Brito, Patrick and Silva, Alan},
	month = nov,
	year = {2016},
	note = {Number: 4},
	pages = {405--437},
}

@article{murphy_primacy_2006,
	title = {Primacy and {Recency} {Effects} on {Clicking} {Behavior}},
	volume = {11},
	issn = {1083-6101, 1083-6101},
	url = {https://academic.oup.com/jcmc/article/11/2/522-535/4617731},
	doi = {10.1111/j.1083-6101.2006.00025.x},
	language = {en},
	number = {2},
	urldate = {2022-04-30},
	journal = {Journal of Computer-Mediated Communication},
	author = {Murphy, Jamie and Hofacker, Charles and Mizerski, Richard},
	month = jan,
	year = {2006},
	note = {Number: 2},
	pages = {522--535},
}

@article{bjork_recency-sensitive_1974,
	title = {Recency-sensitive retrieval processes in long-term free recall},
	volume = {6},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0010028574900097},
	doi = {10.1016/0010-0285(74)90009-7},
	language = {en},
	number = {2},
	urldate = {2022-04-30},
	journal = {Cognitive Psychology},
	author = {Bjork, Robert A. and Whitten, William B.},
	month = apr,
	year = {1974},
	note = {Number: 2},
	pages = {173--189},
}

@article{murdock_serial_1962,
	title = {The serial position effect of free recall.},
	volume = {64},
	issn = {0022-1015},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0045106},
	doi = {10.1037/h0045106},
	language = {en},
	number = {5},
	urldate = {2022-04-30},
	journal = {Journal of Experimental Psychology},
	author = {Murdock, Bennet B.},
	month = nov,
	year = {1962},
	note = {Number: 5},
	pages = {482--488},
}

@article{bjork_recency-sensitive_1974-1,
	title = {Recency-sensitive retrieval processes in long-term free recall},
	volume = {6},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0010028574900097},
	doi = {10.1016/0010-0285(74)90009-7},
	language = {en},
	number = {2},
	urldate = {2022-04-30},
	journal = {Cognitive Psychology},
	author = {Bjork, Robert A. and Whitten, William B.},
	month = apr,
	year = {1974},
	note = {Number: 2},
	pages = {173--189},
}

@misc{noauthor_cyber_2022-1,
	title = {Cyber {Security} {Analyst} {Demographics} and {Statistics} [2022]: {Number} {Of} {Cyber} {Security} {Analysts} {In} {The} {US}},
	url = {https://www.zippia.com/cyber-security-analyst-jobs/demographics/},
	urldate = {2022-04-12},
	month = apr,
	year = {2022},
}

@techreport{cichonski_computer_2012,
	title = {Computer {Security} {Incident} {Handling} {Guide} : {Recommendations} of the {National} {Institute} of {Standards} and {Technology}},
	shorttitle = {Computer {Security} {Incident} {Handling} {Guide}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf},
	abstract = {Computer security incident response has become an important component of information technology (IT) programs. Because performing incident response effectively is a complex undertaking, establishing a successful incident response capability requires substantial planning and resources. This publication assists organizations in establishing computer security incident response capabilities and handling incidents efficiently and effectively. This publication provides guidelines for incident handling, particularly for analyzing incident-related data and determining the appropriate response to each incident. The guidelines can be followed independently of particular hardware platforms, operating systems, protocols, or applications.},
	language = {en},
	number = {NIST SP 800-61r2},
	urldate = {2022-04-12},
	institution = {National Institute of Standards and Technology},
	author = {Cichonski, Paul and Millar, Tom and Grance, Tim and Scarfone, Karen},
	month = aug,
	year = {2012},
	doi = {10.6028/NIST.SP.800-61r2},
	note = {Issue: NIST SP 800-61r2},
	pages = {NIST SP 800--61r2},
}

@article{pennington_getting_nodate,
	title = {Getting {Started} with {ATT}\&{CK}},
	language = {en},
	author = {Pennington, Adam},
	pages = {45},
}

@article{noauthor_business_2015,
	title = {Business {Blackout}: {The} insurance implications of a cyber attack on the {US} power grid},
	language = {en},
	year = {2015},
	pages = {68},
}

@book{noauthor_software_2014,
	title = {Software {Engineering} {Body} of {Knowledge}},
	isbn = {0-7695-5166-1},
	publisher = {IEEE},
	year = {2014},
}

@misc{noauthor_requirements_2022,
	type = {Journal},
	title = {Requirements {Engineering}},
	url = {https://www.springer.com/journal/766},
	abstract = {The journal provides a focus for the dissemination of new results about the elicitation, representation and validation of requirements of software intensive ...},
	language = {en},
	urldate = {2022-03-27},
	journal = {Springer},
	month = mar,
	year = {2022},
}

@techreport{noauthor_strategies_2011,
	address = {1788 Wilmington Pike Glen Mills, PA 19342 USA Phone: +1-484-450-0100},
	title = {Strategies for {Project} {Recovery}},
	url = {www.pmsolutions.com},
	urldate = {2022-03-26},
	institution = {PM Solutions},
	year = {2011},
}

@article{brink_security_nodate,
	title = {{SECURITY} {AWARENESS} {TRAINING}: {SMALL} {INVESTMENT}, {LARGE} {REDUCTION} {IN} {RISK}},
	language = {en},
	author = {Brink, Derek E},
	pages = {14},
}

@article{brink_quantifying_nodate,
	title = {Quantifying the {Value} of {Time} in {Cyber}-{Threat} {Detection} and {Response}},
	language = {en},
	author = {Brink, Derek E},
	pages = {9},
}

@phdthesis{emami2020informing,
	title = {Informing privacy and security decision making in an {IoT} world},
	school = {Carnegie Mellon University},
	author = {Emami-Naeini, Pardis},
	year = {2020},
}

@article{brink_quantifying_nodate-1,
	title = {Quantifying the {Value} of {Time} in {Cyber}-{Threat} {Detection} and {Response}},
	language = {en},
	author = {Brink, Derek E},
	pages = {9},
}

@article{dreyfus_socrates_nodate,
	title = {From {Socrates} to {Expert} {Systems}},
	language = {en},
	author = {Dreyfus, Hubert L and Dreyfus, Stuart E},
	pages = {17},
}

@article{nosco_industrial_2020,
	title = {The {Industrial} {Age} of {Hacking}},
	abstract = {There is a cognitive bias in the hacker community to select a piece of software and invest signiﬁcant human resources into ﬁnding bugs in that software without any prior indication of success. We label this strategy depth-ﬁrst search and propose an alternative: breadth-ﬁrst search. In breadthﬁrst search, humans perform minimal work to enable automated analysis on a range of targets before committing additional time and effort to research any particular one. We present a repeatable human study that leverages teams of varying skill while using automation to the greatest extent possible. Our goal is a process that is effective at ﬁnding bugs; has a clear plan for the growth, coaching, and efﬁcient use of team members; and supports measurable, incremental progress. We derive an assembly-line process that improves on what was once intricate, manual work. Our work provides evidence that the breadth-ﬁrst approach increases the effectiveness of teams.},
	language = {en},
	author = {Nosco, Tim and Ziegler, Jared and Clark, Zechariah},
	month = aug,
	year = {2020},
	pages = {19},
}

@article{trent_modelling_2019-1,
	title = {Modelling the {Cognitive} {Work} of {Cyber} {Protection} {Teams}},
	volume = {4},
	issn = {2474-2120},
	url = {http://www.jstor.org/stable/26623071},
	abstract = {Cyber Protection Teams (CPTs) defend our Nation’s critical military networks. While Cyber Security Service Providers are responsible for the continuous monitoring and vulnerability patching of networks, CPTs perform threat-oriented missions to defeat adversaries within and through cyberspace. The research we report here provides a descriptive workflow of cyber defense in CPTs as well as a prescriptive work model that all CPTs should be capable of executing. This paper describes how these models were developed and used to assess technologies and performance of CPTs. Such models offer a variety of benefits to practitioner and research communities, particularly when the domain of practice is closed to most researchers. This project demonstrates the need for continual curation of CPT work models as well as the need for models of work for the other types of cyber teams (i.e. Mission and Support) in the Cyber Mission Force.},
	number = {1},
	urldate = {2022-02-19},
	journal = {The Cyber Defense Review},
	author = {Trent, Stoney and Hoffman, Robert R. and Merritt, David and Smith, Sarah},
	year = {2019},
	note = {Number: 1
Publisher: Army Cyber Institute},
	pages = {125--136},
}

@article{symonds_innovating_2017,
	title = {Innovating the {Prioritization} of {Cyber} {Defense}},
	volume = {16},
	issn = {1445-3312},
	url = {http://www.jstor.org/stable/26502753},
	abstract = {The U.S. Department of Defense (DoD) faces a monumental undertaking in protecting the infrastructure that underpins the entirety of its operations: It must identify and prioritize key terrain to dynamically defend. This paper will examine the criteria to identify critical information systems and infrastructure, will review the process to identify key terrain in cyberspace, and will offer a recommendation on how to more effectively prioritize network defender operations using data analytics.},
	number = {2},
	urldate = {2022-02-19},
	journal = {Journal of Information Warfare},
	author = {Symonds, R},
	year = {2017},
	note = {Number: 2
Publisher: Peregrine Technical Solutions},
	pages = {12--18},
}

@misc{noauthor_threat_2022-1,
	title = {Threat {Hunting}},
	url = {https://www.boozallen.com/expertise/cybersecurity/threat-hunting.html},
	abstract = {Our cyber threat hunting services can find advanced persistent threats to reduce operational, financial risk.},
	language = {en},
	urldate = {2022-02-19},
	month = feb,
	year = {2022},
}

@article{noauthor_cost_nodate,
	title = {Cost of a {Data} {Breach} {Report} 2020},
	language = {en},
	pages = {82},
}

@techreport{noauthor_2019_2020,
	title = {2019 {TOP} {THREAT} {DETECTION} {TRENDS} {SURVEY}},
	urldate = {2022-02-18},
	institution = {Attivo Networks},
	year = {2020},
}

@misc{milea_hypothesis_2017,
	title = {Hypothesis in {Threat} {Hunting}},
	url = {https://medium.com/@demetriom/hypothesis-in-threat-hunting-4bea5446e34c},
	abstract = {Today’s threat landscape requires organizations to operate more proactively to keep up with advanced and persistent threats. There is no…},
	language = {en},
	urldate = {2022-02-18},
	journal = {Medium},
	author = {Milea, Demetrio},
	month = jul,
	year = {2017},
}

@phdthesis{bynum_cyber_2019,
	title = {{CYBER} {THREAT} {HUNTING}},
	school = {Utica College},
	author = {Bynum, Jon R},
	month = aug,
	year = {2019},
}

@article{gallagher_new_nodate,
	title = {New {Me}: {Understanding} {Expert} and {Non}-{Expert} {Perceptions} and {Usage} of the {Tor} {Anonymity} {Network}},
	abstract = {Proper use of an anonymity system requires adequate understanding of how it functions. Yet, there is surprisingly little research that looks into user understanding and usage of anonymity software. Improper use stemming from a lack of suﬃcient knowledge of the system has the potential to lead to deanonymization, which may hold severe personal consequences for the user. We report on the understanding and the use of the Tor anonymity system. Via semistructured interviews with 17 individuals (6 experts and 11 non-experts) we found that experts and non-experts view, understand, and use Tor in notably diﬀerent ways. Moreover, both groups exhibit behavior as well as gaps in understanding that could potentially compromise anonymity. Based on these ﬁndings, we provide several suggestions for improving the user experience of Tor to facilitate better user understanding of its operation, threat model, and limitations.},
	language = {en},
	author = {Gallagher, Kevin and Patil, Sameer and Memon, Nasir},
	pages = {15},
}

@inproceedings{mohanani_requirements_2014,
	address = {Hyderabad India},
	title = {Requirements fixation},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568235},
	doi = {10.1145/2568225.2568235},
	abstract = {There is a broad consensus that understanding system desiderata (requirements) and design creativity are both important for software engineering success. However, little research has addressed the relationship between design creativity and the way requirements are framed or presented. This paper therefore aims to investigate the possibility that the way desiderata are framed or presented can affect design creativity. Forty two participants took part in a randomized control trial where one group received desiderata framed as “requirements” while the other received desiderata framed as “ideas”. Participants produced design concepts which were judged for originality. Participants who received requirements framing produced significantly less original designs than participants who received ideas framing (MannWhitney U=116.5, p=0.004). We conclude that framing desiderata as “requirements” may cause requirements fixation where designers’ preoccupation with satisfying explicit requirements inhibits their creativity.},
	language = {en},
	urldate = {2022-02-15},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Mohanani, Rahul and Ralph, Paul and Shreeve, Ben},
	month = may,
	year = {2014},
	pages = {895--906},
}

@article{long_scalable_nodate,
	title = {Scalable {Methods} for {Conducting} {Cyber} {Threat} {Hunt} {Operations}},
	author = {Long, Michael C},
}

@article{lee_who_2016,
	title = {The {Who}, {What}, {Where}, {When}, {Why} and {How} of {Effective} {Threat} {Hunting}},
	language = {en},
	author = {Lee, Written Robert M and Lee, Rob},
	month = feb,
	year = {2016},
	pages = {14},
}

@article{lee_generating_nodate,
	title = {Generating {Hypotheses} for {Successful} {Threat} {Hunting}},
	language = {en},
	author = {Lee, Robert M and Bianco, David},
	pages = {12},
}

@misc{bianco_simple_nodate,
	type = {Blog},
	title = {A {Simple} {Hunting} {Maturity} {Model}},
	url = {http://detect-respond.blogspot.com/2015/10/a-simple-hunting-maturity-model.html},
	abstract = {UPDATE 2015-10-15:  I received some questions about the roles of automation in HMM0 and HMM4, which I addressed in a new section.  Also, I n...},
	language = {en},
	urldate = {2022-02-15},
	journal = {Enterprise Detection and Response},
	author = {Bianco, David},
	month = oct,
}

@article{ko_practical_2015,
	title = {A practical guide to controlled experiments of software engineering tools with human participants},
	volume = {20},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-013-9279-3},
	doi = {10.1007/s10664-013-9279-3},
	abstract = {Empirical studies, often in the form of controlled experiments, have been widely adopted in software engineering research as a way to evaluate the merits of new software engineering tools. However, controlled experiments involving human participants actually using new tools are still rare, and when they are conducted, some have serious validity concerns. Recent research has also shown that many software engineering researchers view this form of tool evaluation as too risky and too difficult to conduct, as they might ultimately lead to inconclusive or negative results. In this paper, we aim both to help researchers minimize the risks of this form of tool evaluation, and to increase their quality, by offering practical methodological guidance on designing and running controlled experiments with developers. Our guidance fills gaps in the empirical literature by explaining, from a practical perspective, options in the recruitment and selection of human participants, informed consent, experimental procedures, demographic measurements, group assignment, training, the selecting and design of tasks, the measurement of common outcome variables such as success and time on task, and study debriefing. Throughout, we situate this guidance in the results of a new systematic review of the tool evaluations that were published in over 1,700 software engineering papers published from 2001 to 2011.},
	language = {en},
	number = {1},
	urldate = {2022-02-15},
	journal = {Empirical Software Engineering},
	author = {Ko, Andrew J. and LaToza, Thomas D. and Burnett, Margaret M.},
	month = feb,
	year = {2015},
	note = {Number: 1},
	pages = {110--141},
}

@article{lee_sans_2018,
	title = {{SANS} 2018 {Threat} {Hunting} {Survey} {Results}},
	language = {en},
	author = {Lee, Robert M and Lee, Rob T},
	year = {2018},
	pages = {20},
}

@article{lee_hunter_2017,
	title = {The {Hunter} {Strikes} {Back}: {The} {SANS} 2017 {Threat} {Hunting} {Survey}},
	language = {en},
	author = {Lee, Written Rob and Lee, Robert M},
	year = {2017},
	pages = {26},
}

@article{morse_analytic_2015,
	title = {Analytic {Strategies} and {Sample} {Size}},
	volume = {25},
	issn = {1049-7323},
	url = {https://doi.org/10.1177/1049732315602867},
	doi = {10.1177/1049732315602867},
	number = {10},
	urldate = {2022-02-09},
	journal = {Qualitative Health Research},
	author = {Morse, Janice M.},
	month = oct,
	year = {2015},
	note = {Number: 10
Publisher: SAGE Publications Inc},
	pages = {1317--1318},
}

@article{guest_how_2006,
	title = {How {Many} {Interviews} {Are} {Enough}?: {An} {Experiment} with {Data} {Saturation} and {Variability}},
	volume = {18},
	issn = {1525-822X, 1552-3969},
	shorttitle = {How {Many} {Interviews} {Are} {Enough}?},
	url = {http://journals.sagepub.com/doi/10.1177/1525822X05279903},
	doi = {10.1177/1525822X05279903},
	abstract = {Guidelines for determining nonprobabilistic sample sizes are virtually nonexistent. Purposive samples are the most commonly used form of nonprobabilistic sampling, and their size typically relies on the concept of “saturation,” or the point at which no new information or themes are observed in the data. Although the idea of saturation is helpful at the conceptual level, it provides little practical guidance for estimating sample sizes, prior to data collection, necessary for conducting quality research. Using data from a study involving sixty in-depth interviews with women in two West African countries, the authors systematically document the degree of data saturation and variability over the course of thematic analysis. They operationalize saturation and make evidence-based recommendations regarding nonprobabilistic sample sizes for interviews. Based on the data set, they found that saturation occurred within the first twelve interviews, although basic elements for metathemes were present as early as six interviews. Variability within the data followed similar patterns.},
	language = {en},
	number = {1},
	urldate = {2022-02-09},
	journal = {Field Methods},
	author = {Guest, Greg and Bunce, Arwen and Johnson, Laura},
	month = feb,
	year = {2006},
	note = {Number: 1},
	pages = {59--82},
}

@article{morse_determining_2000,
	title = {Determining {Sample} {Size}},
	volume = {10},
	issn = {1049-7323},
	url = {https://doi.org/10.1177/104973200129118183},
	doi = {10.1177/104973200129118183},
	number = {1},
	urldate = {2022-02-09},
	journal = {Qualitative Health Research},
	author = {Morse, Janice M.},
	month = jan,
	year = {2000},
	note = {Number: 1
Publisher: SAGE Publications Inc},
	pages = {3--5},
}

@incollection{anderson_systems_nodate,
	address = {Cambridge, Massachusetts},
	title = {Systems thinking basics},
	booktitle = {Systems {Thinking} {Basics}},
	publisher = {Pegasus Communications, Inc.},
	author = {Anderson, Virginia and Johnson, Lauren},
}

@article{aronson_overview_nodate,
	title = {Overview of {Systems} {Thinking}},
	language = {en},
	author = {Aronson, Daniel},
	pages = {3},
}

@article{kim_introduction_nodate,
	title = {Introduction to {Systems} {Thinking}},
	language = {en},
	author = {Kim, Daniel H},
	pages = {21},
}

@article{arnold_definition_2015,
	title = {A {Definition} of {Systems} {Thinking}: {A} {Systems} {Approach}},
	volume = {44},
	issn = {18770509},
	shorttitle = {A {Definition} of {Systems} {Thinking}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915002860},
	doi = {10.1016/j.procs.2015.03.050},
	abstract = {This paper proposes a definition of systems thinking for use in a wide variety of disciplines, with particular emphasis on the development and assessment of systems thinking educational efforts. The definition was derived from a review of the systems thinking literature combined with the application of systems thinking to itself. Many different definitions of systems thinking can be found throughout the systems community, but key components of a singular definition can be distilled from the literature. This researcher considered these components both individually and holistically, then proposed a new definition of systems thinking that integrates these components as a system. The definition was tested for fidelity against a System Test and against three widely accepted system archetypes. Systems thinking is widely believed to be critical in handling the complexity facing the world in the coming decades; however, it still resides in the educational margins. In order for this important skill to receive mainstream educational attention, a complete definition is required. Such a definition has not yet been established. This research is an attempt to rectify this deficiency by providing such a definition.},
	language = {en},
	urldate = {2022-02-05},
	journal = {Procedia Computer Science},
	author = {Arnold, Ross D. and Wade, Jon P.},
	year = {2015},
	pages = {669--678},
}

@misc{noauthor_empirical_2022,
	title = {Empirical {Standards}},
	copyright = {CC0-1.0},
	url = {https://github.com/acmsigsoft/EmpiricalStandards},
	abstract = {Tools and standards for conducting and evaluating research in software engineering},
	urldate = {2022-02-05},
	publisher = {ACM Special Interest Group on Software Engineering},
	month = feb,
	year = {2022},
	note = {original-date: 2020-09-16T15:54:52Z},
	keywords = {empirical-standards, research, sigsoft, softwareengineering, standards},
}

@inproceedings{veras_errors_2010,
	address = {San Jose, CA, USA},
	title = {Errors on {Space} {Software} {Requirements}: {A} {Field} {Study} and {Application} {Scenarios}},
	isbn = {978-1-4244-9056-1},
	shorttitle = {Errors on {Space} {Software} {Requirements}},
	url = {http://ieeexplore.ieee.org/document/5635117/},
	doi = {10.1109/ISSRE.2010.30},
	abstract = {This paper presents a field study on real errors found in space software requirements documents. The goal is to understand and characterize the most frequent types of requirement problems in this critical application domain. To classify the software requirement errors analyzed we initially used a well-known existing taxonomy that was later extended in order to allow a more thorough analysis. The results of the study show a high rate of requirement errors (9.5 errors per each 100 requirements), which is surprising if we consider that the focus of the work is critical embedded software. Besides the characterization of the most frequent types of errors, the paper also proposes a set of operators that define how to inject realistic errors in requirement documents. This may be used in several scenarios, including: evaluating and training reviewers, estimating the number of requirement errors in real specifications, defining checklists for quick requirement verification, and defining benchmarks for requirements specifications.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {2010 {IEEE} 21st {International} {Symposium} on {Software} {Reliability} {Engineering}},
	publisher = {IEEE},
	author = {Veras, Paulo C. and Villani, Emilia and Ambrosio, Ana Maria and Silva, Nuno and Vieira, Marco and Madeira, Henrique},
	month = nov,
	year = {2010},
	pages = {61--70},
}

@article{darlington_current_2002,
	title = {Current research in the engineering design requirement},
	volume = {216},
	issn = {0954-4054, 2041-2975},
	url = {http://journals.sagepub.com/doi/10.1243/0954405021520049},
	doi = {10.1243/0954405021520049},
	abstract = {The design requirement is a description of the desired solution to a problem. In engineering design, as in all other, a clear expression of a well-formulated design goal is vital for successful and e cient completion of the design task. The nature of the design requirement and the processes by which it is achieved have been the subject of a wide variety of research. The purpose of the paper is twofold. Firstly, it sets out to collate and discuss representative research in this area in order to give an overview of the current scope of the work. Secondly, it seeks to draw a comparison with the task of developing the design requirement for software and information systems and to initiate a discussion that considers to what extent the substantial body of research in software requirements engineering might help to give an understanding of the design requirement for the engineering design domain. A tentative characterization of the diVerences between the tasks in the two domains is presented, and representative papers from requirements engineering are used to suggest areas of overlap as a starting point for further investigation.},
	language = {en},
	number = {3},
	urldate = {2022-02-04},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
	author = {Darlington, M J and Culley, S J},
	month = mar,
	year = {2002},
	note = {Number: 3},
	pages = {375--388},
}

@incollection{easterbrook_selecting_2008,
	address = {London},
	title = {Selecting {Empirical} {Methods} for {Software} {Engineering} {Research}},
	isbn = {978-1-84800-043-8 978-1-84800-044-5},
	url = {http://link.springer.com/10.1007/978-1-84800-044-5_11},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Guide to {Advanced} {Empirical} {Software} {Engineering}},
	publisher = {Springer London},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	year = {2008},
	doi = {10.1007/978-1-84800-044-5_11},
	pages = {285--311},
}

@incollection{ralph_comparing_2010,
	address = {Berlin, Heidelberg},
	title = {Comparing {Two} {Software} {Design} {Process} {Theories}},
	volume = {6105},
	isbn = {978-3-642-13334-3 978-3-642-13335-0},
	url = {http://link.springer.com/10.1007/978-3-642-13335-0_10},
	abstract = {This paper explores an ongoing conflict concerning the nature of software design. This conflict manifests itself as antagonism between managers and developers, debates about agile vs. plan-driven methodologies and aspiring developers’ dissatisfaction with their courses. One side views design as a plandriven information processing task involving rational decision-making (the Reason-Centric Perspective), while the other views design as an improvised, creative task involving naturalized decision-making (Action-Centric Perspective). Each perspective includes an epistemology, theory of human action and a software design process theory (an explanation of how software is created in practice). This paper reports the results of an exploratory questionnaire study that comparatively and empirically evaluated the two process theories. Results clearly favor the Action-Centric process theory: the Sensemaking-CoevolutionImplementation Framework.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Global {Perspectives} on {Design} {Science} {Research}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ralph, Paul},
	editor = {Winter, Robert and Zhao, J. Leon and Aier, Stephan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	year = {2010},
	doi = {10.1007/978-3-642-13335-0_10},
	note = {Series Editors: \_:n2166
Series Title: Lecture Notes in Computer Science},
	pages = {139--153},
}

@inproceedings{lutz_analyzing_1992,
	address = {San Diego, CA, USA},
	title = {Analyzing software requirements errors in safety-critical, embedded systems},
	isbn = {978-0-8186-3120-7},
	url = {http://ieeexplore.ieee.org/document/324825/},
	doi = {10.1109/ISRE.1993.324825},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {[1993] {Proceedings} of the {IEEE} {International} {Symposium} on {Requirements} {Engineering}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Lutz, R.R.},
	year = {1992},
	pages = {126--133},
}

@article{iqbal_requirements_2020,
	title = {Requirements engineering issues causing software development outsourcing failure},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0229785},
	doi = {10.1371/journal.pone.0229785},
	abstract = {Software development outsourcing is becoming more and more famous because of the advantages like cost abatement, process enhancement, and coping with the scarcity of needed resources. Studies confirm that unfortunately a large proportion of the software development outsourcing projects fails to realize anticipated benefits. Investigations into the failures of such projects divulge that in several cases software development outsourcing projects are failed because of the issues that are associated with requirements engineering process. The objective of this study is the identification and the ranking of the commonly occurring issues of the requirements engineering process in the case of software development outsourcing. For this purpose, contemporary literature has been assessed rigorously, issues faced by practitioners have been identified and three questionnaire surveys have been organized by involving experienced software development outsourcing practitioners. The Delphi technique, cut-off value method and 50\% rule have also been employed. The study explores 150 issues (129 issues from literature and 21 from industry) of requirements engineering process for software development outsourcing, groups the 150 issues into 7 identified categories and then extricates 43 customarily or commonly arising issues from the 150 issues. Founded on ‘frequency of occurrence’ the 43 customarily arising issues have been ranked with respect to respective categories (category-wise ranking) and with respect to all the categories (overall ranking). Categories of the customarily arising issues have also been ranked. The issues’ identification and ranking contribute to design proactive software project management plan for dealing with software development outsourcing failures and attaining conjectured benefits of the software development outsourcing.},
	language = {en},
	number = {4},
	urldate = {2022-02-04},
	journal = {PLOS ONE},
	author = {Iqbal, Javed and Ahmad, Rodina B. and Khan, Muzafar and {Fazal-e-Amin} and Alyahya, Sultan and Nizam Nasir, Mohd Hairul and Akhunzada, Adnan and Shoaib, Muhammad},
	editor = {Xin, Baogui},
	month = apr,
	year = {2020},
	note = {Number: 4},
	pages = {e0229785},
}

@inproceedings{sudin_role_2010,
	title = {The {Role} of a {Specification} in the {Design} {Process}: {A} {Case} {Study}},
	language = {en},
	author = {Sudin, M N and Ahmed-Kristensen, S and Andreasen, M M},
	month = may,
	year = {2010},
	pages = {10},
}

@article{kiritani_success_2015,
	title = {The {Success} or {Failure} of the {Requirements} {Definition} and {Study} of the {Causation} of the {Quantity} of {Trust} {Existence} {Between} {Stakeholders}},
	volume = {64},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915026113},
	doi = {10.1016/j.procs.2015.08.476},
	abstract = {Requirements definition that is an important work process for a project and may determine the success or failure of system development project tends to draw an ambiguous conclusion, which will lead directly to the failure of such a system construction. To optimize the requirements definition process we present a model that the trust management process is integrated into the requirements definition process, as measures to minimize the gap between requirements caused due to a lack or discrepancy in communication that is produced easily in requirements definition and to use a negotiation method for solving problems in the following processes provided by the gap between requirements including tacit ones. We discuss a matter that building trust between the stakeholders in the requirements definition process is effective to optimize the requirements definition, which has been produced by the special characteristics of Japanese firms in the information system development, and we also describe the necessity and effectiveness of information system in Japan.},
	language = {en},
	urldate = {2022-02-04},
	journal = {Procedia Computer Science},
	author = {Kiritani, Keisuke and Ohashi, Masakazu},
	year = {2015},
	pages = {153--160},
}

@misc{noauthor_5_2012,
	title = {5 {Reasons} {Software} {Projects} {Fail}: {The} {Role} of {Requirements}},
	shorttitle = {5 {Reasons} {Software} {Projects} {Fail}},
	url = {https://argondigital.com/blog/product-management/5-reasons-software-projects-fail-hint-its-often-due-to-incomplete-incorrect-requirements/},
	abstract = {Business analysts, in requirements gathering and accurate, complete requirements definition, can significantly impact whether software projects succeed or fail.},
	language = {en-US},
	urldate = {2022-02-04},
	journal = {ArgonDigital {\textbar} Making Technology a Strategic Advantage},
	month = mar,
	year = {2012},
}

@article{ralph_sensemaking-coevolution-implementation_2015,
	title = {The {Sensemaking}-{Coevolution}-{Implementation} {Theory} of software design},
	volume = {101},
	issn = {01676423},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642314005395},
	doi = {10.1016/j.scico.2014.11.007},
	language = {en},
	urldate = {2022-02-04},
	journal = {Science of Computer Programming},
	author = {Ralph, Paul},
	month = apr,
	year = {2015},
	pages = {21--41},
}

@article{vessey_expertise_1985,
	title = {Expertise in debugging computer programs: {A} process analysis},
	volume = {23},
	issn = {00207373},
	shorttitle = {Expertise in debugging computer programs},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020737385800547},
	doi = {10.1016/S0020-7373(85)80054-7},
	language = {en},
	number = {5},
	urldate = {2022-01-29},
	journal = {International Journal of Man-Machine Studies},
	author = {Vessey, Iris},
	month = nov,
	year = {1985},
	note = {Number: 5},
	pages = {459--494},
}

@article{spohrer_novice_1986,
	title = {Novice mistakes: are the folk wisdoms correct?},
	volume = {29},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Novice mistakes},
	url = {https://dl.acm.org/doi/10.1145/6138.6145},
	doi = {10.1145/6138.6145},
	abstract = {An evaluation of two folk wisdoms serves to elucidate the underlying or "deep-structure" reasons for novice errors.},
	language = {en},
	number = {7},
	urldate = {2022-01-29},
	journal = {Communications of the ACM},
	author = {Spohrer, James C. and Soloway, Elliot},
	month = jul,
	year = {1986},
	note = {Number: 7},
	pages = {624--632},
}

@article{litzinger_engineering_2011,
	title = {Engineering {Education} and the {Development} of {Expertise}},
	volume = {100},
	issn = {10694730},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.2168-9830.2011.tb00006.x},
	doi = {10.1002/j.2168-9830.2011.tb00006.x},
	abstract = {BACKGROUND Although engineering education has evolved in ways that improve the readiness of graduates to meet the challenges of the twenty-first century, national and international organizations continue to call for change. Future changes in engineering education should be guided by research on expertise and the learning processes that support its development.
PURPOSE The goals of this paper are: to relate key findings from studies of the development of expertise to engineering education, to summarize instructional practices that are consistent with these findings, to provide examples of learning experiences that are consistent with these instructional practices, and finally, to identify challenges to implementing such learning experiences in engineering programs. SCOPE/METHOD The research synthesized for this article includes that on the development of expertise, students’ approaches to learning, students’ responses to instructional practices, and the role of motivation in learning. In addition, literature on the dominant teaching and learning practices in engineering education is used to frame some of the challenges to implementing alternative approaches to learning.
CONCLUSION Current understanding of expertise, and the learning processes that develop it, indicates that engineering education should encompass a set of learning experiences that allow students to construct deep conceptual knowledge, to develop the ability to apply key technical and professional skills fluently, and to engage in a number of authentic engineering projects. Engineering curricula and teaching methods are often not well aligned with these goals. Curriculum-level instructional design processes should be used to design and implement changes that will improve alignment.},
	language = {en},
	number = {1},
	urldate = {2022-01-29},
	journal = {Journal of Engineering Education},
	author = {Litzinger, Thomas and Lattuca, Lisa R. and Hadgraft, Roger and Newstetter, Wendy},
	month = jan,
	year = {2011},
	note = {Number: 1},
	pages = {123--150},
}

@book{noauthor_how_2000,
	address = {Washington, D.C.},
	title = {How {People} {Learn}: {Brain}, {Mind}, {Experience}, and {School}: {Expanded} {Edition}},
	isbn = {978-0-309-07036-2},
	shorttitle = {How {People} {Learn}},
	url = {http://www.nap.edu/catalog/9853},
	language = {en},
	urldate = {2022-01-29},
	publisher = {National Academies Press},
	month = aug,
	year = {2000},
	doi = {10.17226/9853},
	note = {Pages: 9853},
}

@book{michelfelder_philosophy_2013,
	address = {Dordrecht},
	series = {Philosophy of {Engineering} and {Technology}},
	title = {Philosophy and {Engineering}: {Reflections} on {Practice}, {Principles} and {Process}},
	volume = {15},
	isbn = {978-94-007-7761-3 978-94-007-7762-0},
	shorttitle = {Philosophy and {Engineering}},
	url = {http://link.springer.com/10.1007/978-94-007-7762-0},
	language = {en},
	urldate = {2022-01-25},
	publisher = {Springer Netherlands},
	editor = {Michelfelder, Diane P and McCarthy, Natasha and Goldberg, David E.},
	year = {2013},
	doi = {10.1007/978-94-007-7762-0},
}

@incollection{ralph_proposal_2009,
	address = {Berlin, Heidelberg},
	title = {A {Proposal} for a {Formal} {Definition} of the {Design} {Concept}},
	volume = {14},
	isbn = {978-3-540-92965-9 978-3-540-92966-6},
	url = {http://link.springer.com/10.1007/978-3-540-92966-6_6},
	abstract = {A clear and unambiguous definition of the design concept would be useful for developing a cumulative tradition for research on design. In this article we suggest a formal definition for the concept design and propose a conceptual model linking concepts related to design projects. The definition of design incorporates seven elements: agent, object, environment, goals, primitives, requirements and constraints. The design project conceptual model is based on the view that projects are temporal trajectories of work systems that include human agents who work to design systems for stakeholders, and use resources and tools to accomplish this task. We demonstrate how these two suggestions can be useful by showing that 1) the definition of design can be used to classify design knowledge and 2) the conceptual model can be used to classify design approaches.},
	language = {en},
	urldate = {2022-01-24},
	booktitle = {Design {Requirements} {Engineering}: {A} {Ten}-{Year} {Perspective}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ralph, Paul and Wand, Yair},
	editor = {Lyytinen, Kalle and Loucopoulos, Pericles and Mylopoulos, John and Robinson, Bill and van der Aalst, Will and Mylopoulos, John and Sadeh, Norman M. and Shaw, Michael J. and Szyperski, Clemens},
	year = {2009},
	doi = {10.1007/978-3-540-92966-6_6},
	note = {Series Editors: \_:n2089
Series Title: Lecture Notes in Business Information Processing},
	pages = {103--136},
}

@article{milajerdi_poirot_2019,
	title = {{POIROT}: {Aligning} {Attack} {Behavior} with {Kernel} {Audit} {Records} for {Cyber} {Threat} {Hunting}},
	shorttitle = {{POIROT}},
	url = {http://arxiv.org/abs/1910.00056},
	doi = {10.1145/3319535.3363217},
	abstract = {Cyber threat intelligence (CTI) is being used to search for indicators of attacks that might have compromised an enterprise network for a long time without being discovered. To have a more effective analysis, CTI open standards have incorporated descriptive relationships showing how the indicators or observables are related to each other. However, these relationships are either completely overlooked in information gathering or not used for threat hunting. In this paper, we propose a system, called POIROT, which uses these correlations to uncover the steps of a successful attack campaign. We use kernel audits as a reliable source that covers all causal relations and information flows among system entities and model threat hunting as an inexact graph pattern matching problem. Our technical approach is based on a novel similarity metric which assesses an alignment between a query graph constructed out of CTI correlations and a provenance graph constructed out of kernel audit log records. We evaluate POIROT on publicly released real-world incident reports as well as reports of an adversarial engagement designed by DARPA, including ten distinct attack campaigns against different OS platforms such as Linux, FreeBSD, and Windows. Our evaluation results show that POIROT is capable of searching inside graphs containing millions of nodes and pinpoint the attacks in a few minutes, and the results serve to illustrate that CTI correlations could be used as robust and reliable artifacts for threat hunting.},
	urldate = {2022-01-23},
	journal = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
	author = {Milajerdi, Sadegh M. and Eshete, Birhanu and Gjomemo, Rigel and Venkatakrishnan, V. N.},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.00056},
	keywords = {Computer Science - Cryptography and Security},
	pages = {1795--1812},
}

@article{rodeghero_please_2021,
	title = {Please {Turn} {Your} {Cameras} {On}: {Remote} {Onboarding} of {Software} {Developers} during a {Pandemic}},
	shorttitle = {Please {Turn} {Your} {Cameras} {On}},
	url = {http://arxiv.org/abs/2011.08130},
	abstract = {The COVID-19 pandemic has impacted the way that software development teams onboard new hires. Previously, most software developers worked in physical offices and new hires onboarded to their teams in the physical office, following a standard onboarding process. However, when companies transitioned employees to work from home due to the pandemic, there was little to no time to develop new onboarding procedures. In this paper, we present a survey of 267 new hires at Microsoft that onboarded to software development teams during the pandemic. We explored their remote onboarding process, including the challenges that the new hires encountered and their social connectedness with their teams. We found that most developers onboarded remotely and never had an opportunity to meet their teammates in person. This leads to one of the biggest challenges faced by these new hires, building a strong social connection with their team. We use these results to provide recommendations for onboarding remote hires.},
	urldate = {2022-01-20},
	journal = {arXiv:2011.08130 [cs]},
	author = {Rodeghero, Paige and Zimmermann, Thomas and Houck, Brian and Ford, Denae},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.08130},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
}

@inproceedings{ju_case_2021,
	title = {A {Case} {Study} of {Onboarding} in {Software} {Teams}: {Tasks} and {Strategies}},
	shorttitle = {A {Case} {Study} of {Onboarding} in {Software} {Teams}},
	doi = {10.1109/ICSE43902.2021.00063},
	abstract = {Developers frequently move into new teams or environments across software companies. Their onboarding experience is correlated with productivity, job satisfaction, and other short-term and long-term outcomes. The majority of the onboarding process comprises engineering tasks such as fixing bugs or implementing small features. Nevertheless, we do not have a systematic view of how tasks influence onboarding. In this paper, we present a case study of Microsoft, where we interviewed 32 developers moving into a new team and 15 engineering managers onboarding a new developer into their team - to understand and characterize developers' onboarding experience and expectations in relation to the tasks performed by them while onboarding. We present how tasks interact with new developers through three representative themes: learning, confidence building, and socialization. We also discuss three onboarding strategies as inferred from the interviews that managers commonly use unknowingly, and discuss their pros and cons and offer situational recommendations. Furthermore, we triangulate our interview findings with a developer survey (N = 189) and a manager survey (N = 37) and find that survey results suggest that our findings are representative and our recommendations are actionable. Practitioners could use our findings to improve their onboarding processes, while researchers could find new research directions from this study to advance the understanding of developer onboarding. Our research instruments and anonymous data are available at https://zenodo.org/record/4455937\#.YCOQCs 0lFd.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Ju, An and Sajnani, Hitesh and Kelly, Scot and Herzig, Kim},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Instruments, Interviews, Productivity, Software, Software engineering, Systematics, Task analysis, confidence, learning, onboarding, socialization, software development teams, teams},
	pages = {613--623},
}

@article{baltes_towards_2018,
	title = {Towards a {Theory} of {Software} {Development} {Expertise}},
	url = {http://arxiv.org/abs/1807.06087},
	doi = {10.1145/3236024.3236061},
	abstract = {Software development includes diverse tasks such as implementing new features, analyzing requirements, and fixing bugs. Being an expert in those tasks requires a certain set of skills, knowledge, and experience. Several studies investigated individual aspects of software development expertise, but what is missing is a comprehensive theory. We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance. Our theory currently focuses on programming, but already provides valuable insights for researchers, developers, and employers. The theory describes important properties of software development expertise and which factors foster or hinder its formation, including how developers' performance may decline over time. Moreover, our quantitative results show that developers' expertise self-assessments are context-dependent and that experience is not necessarily related to expertise.},
	urldate = {2022-01-20},
	journal = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	author = {Baltes, Sebastian and Diehl, Stephan},
	month = oct,
	year = {2018},
	note = {arXiv: 1807.06087},
	keywords = {Computer Science - Software Engineering},
	pages = {187--200},
}

@article{lin_attack_nodate,
	title = {Attack {Tactic} {Labeling} for {Cyber} {Threat} {Hunting}},
	abstract = {Recently, the cyber attack has become more complex and targeted, making traditional security defense mechanisms based on the “Indicator of Compromise” ineffective. Furthermore, fail to consider attack kill chain may lead to a high false-positive rate for attack detection. To trace hackers’ behaviors and footprints, it is crucial to provide additional information such as attack tactics, techniques, and procedures in detecting attacks.},
	language = {en},
	author = {Lin, Sheng-Xiang and Li, Zong-Jyun and Chen, Tzu-Yang and Wu, Dong-Jie},
	pages = {7},
}

@inproceedings{sree_artificial_2021,
	title = {Artificial {Intelligence} {Based} {Predictive} {Threat} {Hunting} {In} {The} {Field} of {Cyber} {Security}},
	doi = {10.1109/GCAT52182.2021.9587507},
	abstract = {Artificial intelligence (AI) is a broad field of computer science that focuses on designing smart machines capable of performing tasks typically requiring human intelligence. Despite the fact that security solutions are growing progressively modern and stable, cyberattacks are still evolving and are at their extreme. The main reason is that conventional methods of malware detection fail. Cyber attackers are actively developing new ways to prevent defence programmes from infecting malware networks and servers. Most anti-malware and antivirus applications currently use signature-based detection to identify attacks, which is unsuccessful in detecting new threats. This is where Artificial Intelligence is most handy. The standardised models for threatened hunting and performance quantification from the start of hazard hunting to the end still allow methodological rigour and completeness to be studied remain undefined. The organised practise of hazard hunts seeks to disclose the presence of TTP in the field of detection that has not already been detected. In this study, a realistic and comprehensive model is outlined to detect attackers in six stages: aim, scale, equipment, planning, execution and input. This study describes Threat Hunting in an ecosystem as the constructive, analyst-driven scanning mechanism for attackers TTP. The model has been checked for real-world data sets using a variety of threats. The effectiveness and practicality of this research have been shown with and without a blueprint through danger hunts. In addition, the article presents an analysis of the concept of threat hunting based on data from Ukrainian electricity grid attacks in an online environment to highlight the effects of this model on threat hunting in a simulated environment. The findings of this analysis include an effective and repetitive way to search for and quantify honesty, coverage and rigour.},
	booktitle = {2021 2nd {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	author = {Sree, Vaddi Sowmya and Koganti, Chaitna Sri and Kalyana, Srinivas K and Anudeep, P.},
	month = oct,
	year = {2021},
	keywords = {Analytical models, Biological system modeling, Data models, Ecosystems, Hazards, Joint Targetting Cycle, Systematics, Threa tHunting Model, Threathunting, Tools},
	pages = {1--6},
}

@inproceedings{horta_neto_cyber_2020,
	title = {Cyber {Threat} {Hunting} {Through} {Automated} {Hypothesis} and {Multi}-{Criteria} {Decision} {Making}},
	doi = {10.1109/BigData50022.2020.9378213},
	abstract = {There are sophisticated cyber attacks that pose a high risk to institutions, especially when they are carefully planned and victims are unable to identify them. This is a preliminary result of executing the high-level cyber threat hunting through automated hypothesis-making and multi-criteria decision making using the binary attack-chaining tables identified in the networks. Firstly, the concepts required for threats modeling and the process of knowledge discovery in databases focused on high-level threat hunting were introduced. After, the knowledge discovered was used in an experiment that applied and evaluated the effectiveness of machine learning and decision-making algorithms in the method proposed to prioritize hypotheses in the screening phase. In addition, an automated hypothesis-making method to be used in production environments was also proposed. Finally, the results achieved in the experiment demonstrated that high-level threat hunting is a viable and more efficient alternative compared to manual process.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Horta Neto, Antonio José and Fernandes Pereira dos Santos, Anderson},
	month = dec,
	year = {2020},
	keywords = {Big Data, Decision making, Knowledge discovery, Machine learning, Machine learning algorithms, Manuals, Production, decision-making, knowledge discovery in databases, machine learning, threat hunting, threat intelligence},
	pages = {1823--1830},
}

@inproceedings{miazi_design_2017,
	title = {The {Design} of {Cyber} {Threat} {Hunting} {Games}: {A} {Case} {Study}},
	shorttitle = {The {Design} of {Cyber} {Threat} {Hunting} {Games}},
	doi = {10.1109/ICCCN.2017.8038527},
	abstract = {Cyber Threat Hunting is an emerging cyber security activity. Recent studies show that, although similar actions like threat hunting are being actively practiced in some organization, security administrator and policy makers are far from being satisfied with their effectiveness. Most security professionals lack expertise in data analytics while most people with data analytics skills lack security knowledge. To understand the necessity of threat hunting education at university level, we organized a \textit{Threat Hunting Competition} on campus with generated logs. In this paper, we identify skills needed for cyber threat hunting, describe the data generation process as well as the usage of logs to teach threat hunting at universities.},
	booktitle = {2017 26th {International} {Conference} on {Computer} {Communication} and {Networks} ({ICCCN})},
	author = {Miazi, Md Nazmus Sakib and Pritom, Mir Mehedi A. and Shehab, Mohamed and Chu, Bill and Wei, Jinpeng},
	month = jul,
	year = {2017},
	keywords = {Companies, Computer security, Data analysis, Forensics, Games},
	pages = {1--6},
}

@article{karuna_automating_2021,
	title = {Automating {Cyber} {Threat} {Hunting} {Using} {NLP}, {Automated} {Query} {Generation}, and {Genetic} {Perturbation}},
	url = {http://arxiv.org/abs/2104.11576},
	abstract = {Scaling the cyber hunt problem poses several key technical challenges. Detecting and characterizing cyber threats at scale in large enterprise networks is hard because of the vast quantity and complexity of the data that must be analyzed as adversaries deploy varied and evolving tactics to accomplish their goals. There is a great need to automate all aspects, and, indeed, the workflow of cyber hunting. AI offers many ways to support this. We have developed the WILEE system that automates cyber threat hunting by translating high-level threat descriptions into many possible concrete implementations. Both the (high-level) abstract and (low-level) concrete implementations are represented using a custom domain specific language (DSL). WILEE uses the implementations along with other logic, also written in the DSL, to automatically generate queries to confirm (or refute) any hypotheses tied to the potential adversarial workflows represented at various layers of abstraction.},
	urldate = {2022-01-19},
	journal = {arXiv:2104.11576 [cs]},
	author = {Karuna, Prakruthi and Hemberg, Erik and O'Reilly, Una-May and Rutar, Nick},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.11576},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@inproceedings{agarwal_cyber_2021,
	title = {Cyber {Security} {Model} for {Threat} {Hunting}},
	doi = {10.1109/ICRITO51393.2021.9596199},
	abstract = {Data privacy and encryption will still be top security priorities. Threat controls are countermeasures or safeguards used to reduce the chances that a threat will exploit a vulnerability as there is also a lack of understanding and a systematic model on which to base threat hunting operations and quantifying their effectiveness from the start of a threat hunt engagement to the end, as well as analytic rigour and completeness analysis. Threat hunting is a systematic method that aims to discover the location of attacker tactics, techniques, and procedures (TTP) in an area that has not yet been detected by current detection technologies. Using six stages: purpose, scope, equip, plan review, execute, and feedback, this research outlines a survey on this research.},
	booktitle = {2021 9th {International} {Conference} on {Reliability}, {Infocom} {Technologies} and {Optimization} ({Trends} and {Future} {Directions}) ({ICRITO})},
	author = {Agarwal, Anchit and Walia, Himdweep and Gupta, Himanshu},
	month = sep,
	year = {2021},
	keywords = {Analytical models, Computer crime, Cyber Security, Data Encryption, Data privacy, Encryption, Market research, Reliability, Systematics, TTP-Based Hunting, Threat Hunting},
	pages = {1--8},
}

@inproceedings{gao_system_2021,
	title = {A {System} for {Efficiently} {Hunting} for {Cyber} {Threats} in {Computer} {Systems} {Using} {Threat} {Intelligence}},
	doi = {10.1109/ICDE51399.2021.00309},
	abstract = {Log-based cyber threat hunting has emerged as an important solution to counter sophisticated cyber attacks. However, existing approaches require non-trivial efforts of manual query construction and have overlooked the rich external knowledge about threat behaviors provided by open-source Cyber Threat Intelligence (OSCTI). To bridge the gap, we build ThreatRaptor, a system that facilitates cyber threat hunting in computer systems using OSCTI. Built upon mature system auditing frameworks, ThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP pipeline that extracts structured threat behaviors from unstructured OSCTI text, (2) a concise and expressive domain-specific query language, TBQL, to hunt for malicious system activities, (3) a query synthesis mechanism that automatically synthesizes a TBQL query from the extracted threat behaviors, and (4) an efficient query execution engine to search the big system audit logging data.},
	booktitle = {2021 {IEEE} 37th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Gao, Peng and Shao, Fei and Liu, Xiaoyuan and Xiao, Xusheng and Liu, Haoyuan and Qin, Zheng and Xu, Fengyuan and Mittal, Prateek and Kulkarni, Sanjeev R. and Song, Dawn},
	month = apr,
	year = {2021},
	note = {ISSN: 2375-026X},
	keywords = {Conferences, Data engineering, Data mining, Database languages, Manuals, Open source software, Pipelines, n/a},
	pages = {2705--2708},
}

@book{garcia-alfaro_security_2021,
	address = {Cham},
	series = {Lecture {Notes} of the {Institute} for {Computer} {Sciences}, {Social} {Informatics} and {Telecommunications} {Engineering}},
	title = {Security and {Privacy} in {Communication} {Networks}: 17th {EAI} {International} {Conference}, {SecureComm} 2021, {Virtual} {Event}, {September} 6–9, 2021, {Proceedings}, {Part} {I}},
	volume = {398},
	isbn = {978-3-030-90018-2 978-3-030-90019-9},
	shorttitle = {Security and {Privacy} in {Communication} {Networks}},
	url = {https://link.springer.com/10.1007/978-3-030-90019-9},
	language = {en},
	urldate = {2022-01-19},
	publisher = {Springer International Publishing},
	editor = {Garcia-Alfaro, Joaquin and Li, Shujun and Poovendran, Radha and Debar, Hervé and Yung, Moti},
	year = {2021},
	doi = {10.1007/978-3-030-90019-9},
}

@inproceedings{gao_enabling_2021,
	title = {Enabling {Efficient} {Cyber} {Threat} {Hunting} {With} {Cyber} {Threat} {Intelligence}},
	doi = {10.1109/ICDE51399.2021.00024},
	abstract = {Log-based cyber threat hunting has emerged as an important solution to counter sophisticated attacks. However, existing approaches require non-trivial efforts of manual query construction and have overlooked the rich external threat knowledge provided by open-source Cyber Threat Intelligence (OSCTI). To bridge the gap, we propose ThreatRaptor, a system that facilitates threat hunting in computer systems using OSCTI. Built upon system auditing frameworks, ThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP pipeline that extracts structured threat behaviors from unstructured OSCTI text, (2) a concise and expressive domain-specific query language, TBQL, to hunt for malicious system activities, (3) a query synthesis mechanism that automatically synthesizes a TBQL query for hunting, and (4) an efficient query execution engine to search the big audit logging data. Evaluations on a broad set of attack cases demonstrate the accuracy and efficiency of ThreatRaptor in practical threat hunting.},
	booktitle = {2021 {IEEE} 37th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Gao, Peng and Shao, Fei and Liu, Xiaoyuan and Xiao, Xusheng and Qin, Zheng and Xu, Fengyuan and Mittal, Prateek and Kulkarni, Sanjeev R. and Song, Dawn},
	month = apr,
	year = {2021},
	note = {ISSN: 2375-026X},
	keywords = {Conferences, Data engineering, Data mining, Database languages, Manuals, Open source software, Pipelines, n/a},
	pages = {193--204},
}

@article{ajmal_last_2021,
	title = {Last {Line} of {Defense}: {Reliability} {Through} {Inducing} {Cyber} {Threat} {Hunting} {With} {Deception} in {SCADA} {Networks}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Last {Line} of {Defense}},
	doi = {10.1109/ACCESS.2021.3111420},
	abstract = {There exists a gap between existing security mechanisms and their ability to detect advancing threats. Antivirus and EDR (End Point Detection and Response) aim to detect and prevent threats; such security mechanisms are reactive. This approach did not prove to be effective in protecting against stealthy attacks. SCADA (Supervisory Control and Data Acquisition) security is crucial for any country. However, SCADA is always an easy target for adversaries due to a lack of security for heterogeneous devices. An attack on SCADA is mainly considered a national-level threat. Recent research on SCADA security has not considered “unknown threats,” which has left a gap in security. The proactive approach, such as threat hunting, is the need of the hour. In this research, we investigated that threat hunting in conjunction with cyber deception and kill chain has countervailing effects on detecting SCADA threats and mitigating them. We have used the concept of “decoy farm” in the SCADA network, where all attacks are engaged. Moreover, we present a novel threat detection and prevention approach for SCADA, focusing on unknown threats. To test the effectiveness of approach, we emulated several SCADA, Linux and Windows based attacks on a simulated SCADA network. We have concluded that our approach detects and prevents the attacker before using the current reactive approach and security mechanism for SCADA with enhanced protection for heterogeneous devices. The results and experiments show that the proposed threat hunting approach has significantly improved the threat detection ability.},
	journal = {IEEE Access},
	author = {Ajmal, Abdul Basit and Alam, Masoom and Khaliq, Awais Abdul and Khan, Shawal and Qadir, Zakria and Mahmud, M. A. Parvez},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Industrial Internet of Things (IIoT), Licenses, Open source software, Process control, Protocols, SCADA systems, Security, Threat hunting, Tools, cyber deception, decoys, honeypots, indicators of compromise (IOC), supervisory control and data acquisition (SCADA)},
	pages = {126789--126800},
}

@article{johnson_long-term_2017,
	title = {Long-term follow-up of psilocybin-facilitated smoking cessation},
	volume = {43},
	issn = {0095-2990, 1097-9891},
	url = {https://www.tandfonline.com/doi/full/10.3109/00952990.2016.1170135},
	doi = {10.3109/00952990.2016.1170135},
	abstract = {Background—A recent open-label pilot study (N=15) found that two to three moderate to high doses (20 and 30 mg/70 kg) of the serotonin 2A receptor agonist psilocybin, in combination with cognitive behavioral therapy (CBT) for smoking cessation, resulted in substantially higher 6month smoking abstinence rates than are typically observed with other medications or CBT alone.
Objectives—To assess long-term effects of a psilocybin-facilitated smoking cessation program at ≥12 months after psilocybin administration.
Methods—The present report describes biologically verified smoking abstinence outcomes of the previous pilot study at ≥12 months, and related data on subjective effects of psilocybin.
Results—All 15 participants completed a 12-month follow-up, and 12 (80\%) returned for a longterm (≥16 months) follow-up, with a mean interval of 30 months (range = 16 – 57 months) between target-quit date (i.e., first psilocybin session) and long-term follow-up. At 12-month follow-up, 10 participants (67\%) were confirmed as smoking abstinent. At long-term follow-up, nine participants (60\%) were confirmed as smoking abstinent. At 12-month follow-up 13 participants (86.7\%) rated their psilocybin experiences among the 5 most personally meaningful and spiritually significant experiences of their lives.
Conclusion—These results suggest that in the context of a structured treatment program, psilocybin holds considerable promise in promoting long-term smoking abstinence. The present study adds to recent and historical evidence suggesting high success rates when using classic psychedelics in the treatment of addiction. Further research investigating psilocybin-facilitated treatment of substance use disorders is warranted.},
	language = {en},
	number = {1},
	urldate = {2021-12-12},
	journal = {The American Journal of Drug and Alcohol Abuse},
	author = {Johnson, Matthew W. and Garcia-Romeu, Albert and Griffiths, Roland R.},
	month = jan,
	year = {2017},
	note = {Number: 1},
	pages = {55--60},
}

@article{roberson_color_nodate,
	title = {Color categories: {Confirmation} of the relativity hypothesis.},
	abstract = {The question of whether language affects our categorization of perceptual continua is of particular interest for the domain of color where constraints on categorization have been proposed both within the visual system and in the visual environment. Recent research (Roberson et al., 2000; Roberson et al., in press) found substantial evidence of cognitive color differences between different language communities, but concerns remained as to how representative might be a tiny, extremely remote community. The present study replicates and extends previous findings using additional paradigms among a larger community in a different visual environment. Adult semi-nomadic tribesmen in Southern Africa carried out similarity judgments, short-term memory and long-term learning tasks. They showed different cognitive organization of color to both English and another language with the five color terms. Moreover, Categorical Perception effects were found to differ even between languages with broadly similar color categories. The results provide further evidence of the tight relationship between language and cognition.},
	language = {en},
	author = {Roberson, Debi and Davidoff, Jules},
	pages = {70},
}

@article{van_dongen_cumulative_2003,
	title = {The {Cumulative} {Cost} of {Additional} {Wakefulness}: {Dose}-{Response} {Effects} on {Neurobehavioral} {Functions} and {Sleep} {Physiology} {From} {Chronic} {Sleep} {Restriction} and {Total} {Sleep} {Deprivation}},
	volume = {26},
	issn = {1550-9109, 0161-8105},
	shorttitle = {The {Cumulative} {Cost} of {Additional} {Wakefulness}},
	url = {https://academic.oup.com/sleep/article-lookup/doi/10.1093/sleep/26.2.117},
	doi = {10.1093/sleep/26.2.117},
	abstract = {Objectives: To inform the debate over whether human sleep can be chronically reduced without consequences, we conducted a doseresponse chronic sleep restriction experiment in which waking neurobehavioral and sleep physiological functions were monitored and compared to those for total sleep deprivation. Design: The chronic sleep restriction experiment involved randomization to one of three sleep doses (4 h, 6 h, or 8 h time in bed per night), which were maintained for 14 consecutive days. The total sleep deprivation experiment involved 3 nights without sleep (0 h time in bed). Each study also involved 3 baseline (pre-deprivation) days and 3 recovery days. Setting: Both experiments were conducted under standardized laboratory conditions with continuous behavioral, physiological and medical monitoring. Participants: A total of n = 48 healthy adults (ages 21–38) participated in the experiments. Interventions: Nocturnal sleep periods were restricted to 8 h, 6 h or 4 h per day for 14 days, or to 0 h for 3 days. All other sleep was prohibited. Results: Chronic restriction of sleep periods to 4 h or 6 h per night over 14 consecutive days resulted in significant cumulative, dose-dependent deficits in cognitive performance on all tasks. Subjective sleepiness ratings showed an acute response to sleep restriction but only small further increases on subsequent days, and did not significantly differentiate the 6 h and 4 h conditions. Polysomnographic variables and δ power in the nonREM sleep EEG—a putative marker of sleep homeostasis—displayed an acute response to sleep restriction with negligible further changes across the 14 restricted nights. Comparison of chronic sleep restriction to total sleep deprivation showed that the latter resulted in disproportionately large waking neurobehavioral and sleep δ power responses relative to how much sleep was lost. A statistical model revealed that, regardless of the mode of sleep deprivation, lapses in behavioral alertness were nearlinearly related to the cumulative duration of wakefulness in excess of 15.84 h (s.e. 0.73 h). Conclusions: Since chronic restriction of sleep to 6 h or less per night produced cognitive performance deficits equivalent to up to 2 nights of total sleep deprivation, it appears that even relatively moderate sleep restriction can seriously impair waking neurobehavioral functions in healthy adults. Sleepiness ratings suggest that subjects were largely unaware of these increasing cognitive deficits, which may explain why the impact of chronic sleep restriction on waking cognitive functions is often assumed to be benign. Physiological sleep responses to chronic restriction did not mirror waking neurobehavioral responses, but cumulative wakefulness in excess of a 15.84 h predicted performance lapses across all four experimental conditions. This suggests that sleep debt is perhaps best understood as resulting in additional wakefulness that has a neurobiological “cost” which accumulates over time.},
	language = {en},
	number = {2},
	urldate = {2021-12-12},
	journal = {Sleep},
	author = {Van Dongen, Hans P.A. and Maislin, Greg and Mullington, Janet M. and Dinges, David F.},
	month = mar,
	year = {2003},
	note = {Number: 2},
	pages = {117--126},
}

@article{georgiadou_assessing_2021,
	title = {Assessing {MITRE} {ATT}\&{CK} {Risk} {Using} a {Cyber}-{Security} {Culture} {Framework}},
	volume = {21},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/9/3267},
	doi = {10.3390/s21093267},
	abstract = {The MITRE ATT\&CK (Adversarial Tactics, Techniques, and Common Knowledge) Framework provides a rich and actionable repository of adversarial tactics, techniques, and procedures. Its innovative approach has been broadly welcomed by both vendors and enterprise customers in the industry. Its usage extends from adversary emulation, red teaming, behavioral analytics development to a defensive gap and SOC (Security Operations Center) maturity assessment. While extensive research has been done on analyzing speciﬁc attacks or speciﬁc organizational culture and human behavior factors leading to such attacks, a holistic view on the association of both is currently missing. In this paper, we present our research results on associating a comprehensive set of organizational and individual culture factors (as described on our developed cyber-security culture framework) with security vulnerabilities mapped to speciﬁc adversary behavior and patterns utilizing the MITRE ATT\&CK framework. Thus, exploiting MITRE ATT\&CK’s possibilities towards a scientiﬁc direction that has not yet been explored: security assessment and defensive design, a step prior to its current application domain. The suggested cyber-security culture framework was originally designed to aim at critical infrastructures and, more speciﬁcally, the energy sector. Organizations of these domains exhibit a co-existence and strong interaction of the IT (Information Technology) and OT (Operational Technology) networks. As a result, we emphasize our scientiﬁc effort on the hybrid MITRE ATT\&CK for Enterprise and ICS (Industrial Control Systems) model as a broader and more holistic approach. The results of our research can be utilized in an extensive set of applications, including the efﬁcient organization of security procedures as well as enhancing security readiness evaluation results by providing more insights into imminent threats and security risks.},
	language = {en},
	number = {9},
	urldate = {2021-12-10},
	journal = {Sensors},
	author = {Georgiadou, Anna and Mouzakitis, Spiros and Askounis, Dimitris},
	month = may,
	year = {2021},
	note = {Number: 9},
	pages = {3267},
}

@article{araujo_evidential_2021,
	title = {Evidential {Cyber} {Threat} {Hunting}},
	url = {http://arxiv.org/abs/2104.10319},
	abstract = {A formal cyber reasoning framework for automating the threat hunting process is described. The new cyber reasoning methodology introduces an operational semantics that operates over three subspaces -- knowledge, hypothesis, and action -- to enable human-machine co-creation of threat hypotheses and protective recommendations. An implementation of this framework shows that the approach is practical and can be used to generalize evidence-based multi-criteria threat investigations.},
	urldate = {2021-11-14},
	journal = {arXiv:2104.10319 [cs]},
	author = {Araujo, Frederico and Kirat, Dhilung and Shu, Xiaokui and Taylor, Teryl and Jang, Jiyong},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.10319},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{ebrahimi_national_nodate,
	title = {National {Guard} {Cyber} {Protection} {Teams} as a {Response} to {Cybersecurity} {Threats}},
	language = {en},
	author = {Ebrahimi, Alex and Leithner, Anika and Lowham, Elizabeth A and Tiscareño, Samantha and Battle, Martin and Elmjouie, Maria and Vieira, Madalyn and Watkins, Jake},
	pages = {70},
}

@inproceedings{wafula_carve_2019,
	title = {{CARVE}: {A} {Scientific} {Method}-{Based} {Threat} {Hunting} {Hypothesis} {Development} {Model}},
	shorttitle = {{CARVE}},
	doi = {10.1109/EIT.2019.8833792},
	abstract = {A threat hunting exercise is a hypothesis driven exploratory and explanatory research process, the exercise is inherently scientific in nature and lends itself to the application of the scientific method of hypothesis development. The exercise commences with exploratory steps in the threat hypothesis phase to develop a logical argument asserting an existential threat, then follows with explanatory steps in the threat hunt phase to validate the argument. To deem a threat credible, that is, valid and relevant, a threat hunting hypothesis must establish a correlational and causal relationship between the asserted threat and a targeted asset, the hypothesis must adhere to the constructs of the scientific method for the exercise to be defined and measured objectively, and yield valuable and repeatable outcomes. Lack of adherence to the scientific method increases the frequency of invalid and/or irrelevant propositions in threat hypotheses, which diminishes Return on Investment (ROI) in cybersecurity defensive efforts due to wasted cycles of threat hunting exercises. This paper proposes a scientific method-based model, Collect Analyze Relate Validate Establish (CARVE), which can be used to develop valid and relevant threat hunting hypotheses in the context of a given organization's information system and environment. The CARVE model is defined by the following five steps: Collect, Analyze, Relate, Validate, and Establish. The effectiveness of the model is demonstrated using a case study based on the technical alert United States Computer Emergency Readiness Team (US CERT) TA17-293A.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Electro} {Information} {Technology} ({EIT})},
	author = {Wafula, Kevin and Wang, Yong},
	month = may,
	year = {2019},
	note = {ISSN: 2154-0373},
	keywords = {Analytical models, CARVE Model, Computer security, Correlation, Credible Threat, Information systems, Intelligent sensors, Organizations, Threat, Threat Hunting Hypothesis},
	pages = {1--6},
}

@inproceedings{rasheed_threat_2017,
	title = {Threat {Hunting} {Using} {GRR} {Rapid} {Response}},
	doi = {10.1109/ICTCS.2017.22},
	abstract = {Cybercrimes have evolved, and their tactics and techniques are increasingly changing with an alerting pace. This calls for a change in the mindset used to implement security measures, by adopting the approach of continuously and constantly looking for attacks that pass through the deployed security solutions. This approach of searching through the networks for any evidence on threat activity, rather waiting for a breach notification is referred to as cyber threat hunting. This paper discusses the deployment of threat hunting process using GRR Rapid Response. Two experiments were conducted, in which, both remote code execution, client side exploits are tested, and successful exploitation was used to configure a backdoor to the victim's system to achieve persistence. The experiments show that threat hunting can be achieved by the study of the monitored system's normal patterns of behavior, which will help identify the indications and thresholds that can be used in threat hunting.},
	booktitle = {2017 {International} {Conference} on {New} {Trends} in {Computing} {Sciences} ({ICTCS})},
	author = {Rasheed, Hussein and Hadi, Ali and Khader, Mariam},
	month = oct,
	year = {2017},
	keywords = {Computer crime, Firewalls (computing), GRR Rapid Response, IoC, Monitoring, Organizations, Response, Servers, Threat Hunting, Threat Intelligence, Tools},
	pages = {155--160},
}

@phdthesis{alharbi_security_2021,
	address = {United States -- California},
	type = {Ph.{D}.},
	title = {A {Security} {Operation} {Center} {Maturity} {Model} ({SOC}-{MM}) in the {Context} of {Newly} {Emerging} {Cyber} {Threats}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2447834861/abstract/49F3D76000004605PQ/1},
	abstract = {Cyberattacks and threats from hardware and software system components are rapidly emerging as one of the biggest challenges that today’s businesses, government agencies, and individuals face. As a result, organizations are continuously trying to adapt and update their information assets.
One of the most popular preventive methods against these malicious attacks is establishing of a security operations center (SOC). SOCs are responsible for protecting mission-critical information, as well as detecting and responding to attacks; they are also tasked with planning and creating contingency procedures to meet not only known challenges but also emerging and as yet undiscovered ones. However, organizations are faced with a real challenge: The absence of a holistic framework and guidance on how to establish an SOC, which makes setting up an SOC a complex task. Each organization is trying to reinvent the wheel, which has led to a diversity of implementation forms. Consequently, without a consistent method or process to follow, there is no way to ensure that the essential attributes for establishing an SOC are met. There are also no clear mechanisms for determining the maturity level and capabilities of these SOCs.
To address these gaps, this research attempted to: formulate an up-to-date definition of SOC and identify SOC’s essential attributes, develop an SOC maturity model (SOC-MM) based upon the context of newly emerging cyber threats, design an SOC-MM tool that can identify current level of maturity in organization and provides recommendations on how to reach the next maturity level, and finally implement and evaluate the SOC-MM and SOC-MM tool through a case study approach to prove its effectiveness for organizations.
Primarily, a design since research approach was adopted, which is a proactive research methodology. This approach is appropriate for research areas that are not clearly defined, as in the case of SOCs. Following the DSR process, the research steps for this study were as follow: the problem was defined through literature review, the research objectives were then listed, then interview and survey questions were created based on information gathered from the literature review. Interviews and surveys were then conducted with SOC experts and CSOs to formulate SOC's definition, the attributes, and the SOC-MM. From the designed SOC-MM, the tool was created, then was sent to an organization to be evaluated along with the SOC-MM.
There is a lot of possible beneficiary of the proposed model and tool, such as in organization as a self-assessment tool, where they can use the tool to measure their level of maturity and work on the recommendations provided for them to enhance their SOC. Another method of implication is in consultancy companies as a part of their services that they provide for their customers; the model proposed provides new insights that can help determine maturity through different landscapes than these models. Furthermore, it can be used in the organization's audit department, where they can use the model as a part of the performance evaluation and provide the SOC with the proposed recommendation to ensure continued improvement of the SOC.
Keywords: Cyberattacks, security operations center (SOC), maturity model (MM), newly emerging cyber threats, security operations center maturity model (SOC-MM).},
	language = {English},
	urldate = {2021-11-08},
	school = {The Claremont Graduate University},
	author = {Alharbi, Norah},
	month = nov,
	year = {2021},
	note = {ISBN: 9798672151229},
	keywords = {Cyberattacks, Cybersecurity, Maturity model, Security operations center, Security operations center maturity model},
}

@article{chamkar_human_2021,
	title = {{THE} {HUMAN} {FACTOR} {CAPABILITIES} {IN} {SECURITY} {OPERATION} {CENTER} ({SOC})},
	issn = {0736-6981, 1936-1009},
	url = {https://www.tandfonline.com/doi/full/10.1080/07366981.2021.1977026},
	doi = {10.1080/07366981.2021.1977026},
	abstract = {The human factor is considered the weakest link in cybersecurity and inside the Security Operation Centers (SOC) and it represents the most important component at the same time. Human factor capabilities and challenges attracted the attention of researchers to address how these challenges can be reduced or mitigated. However, these research papers do not consider the complexity, unpredictability, interdependent and evolving nature of the SOC systems. This study aims to explore the human capabilities and weaknesses inside the Security Operation Centre. To this end, we employed survey bases questionaries alongside the daily observation of SOC analysts and interviews with SOC experts. Forty SOC analysts and five experts conducted the survey. The finding of this study will help SOC managers and SOC designers better understand the challenges faced by the SOC analysts and take into account the interdependent and evolving nature of the Security Operation Centers.},
	language = {en},
	urldate = {2021-11-08},
	journal = {EDPACS},
	author = {Chamkar, Samir Achraf and Maleh, Yassine and Gherabi, Noreddine},
	month = oct,
	year = {2021},
	pages = {1--14},
}

@inproceedings{majid_success_2019,
	address = {Bandung, Indonesia},
	title = {Success {Factors} for {Cyber} {Security} {Operation} {Center} ({SOC}) {Establishment}},
	isbn = {978-1-63190-198-0},
	url = {http://eudl.eu/doi/10.4108/eai.18-7-2019.2287841},
	doi = {10.4108/eai.18-7-2019.2287841},
	abstract = {The boundless in the digital world is one of the terms used to describe the present state where everything depends mostly on the use of technology. The increased dependency on these technology services has indirectly increased the risk of threats and cyber-attacks. One of the popular solutions to defend against these threats is by implementing the Cyber Security Operation Center (SOC) to monitor, track and handle the cyber incidents. However, there are a number of factors that affect the success of the SOC. Therefore, this paper aims to highlight the importance of the human, process and technology factors towards the establishment of SOC. A comparison of the previous establishment of SOC from the literature is made. The inputs from the literature come from the journal, proceeding, report starting from the year 2011 until 2018. From the result of the comparison, it presents the requirement of human, process, and technology to make sure the SOC work efficiently to defend against the cyber-attack.},
	language = {en},
	urldate = {2021-11-08},
	booktitle = {Proceedings of the {Proceedings} of the 1st {International} {Conference} on {Informatics}, {Engineering}, {Science} and {Technology}, {INCITEST} 2019, 18 {July} 2019, {Bandung}, {Indonesia}},
	publisher = {EAI},
	author = {Majid, M. and Ariffi, K},
	year = {2019},
}

@article{cothier_timeliness_1986,
	title = {Timeliness and {Measures} of {Effectiveness} in {Command} and {Control}},
	volume = {16},
	issn = {0018-9472},
	url = {http://ieeexplore.ieee.org/document/4309003/},
	doi = {10.1109/TSMC.1986.4309003},
	abstract = {A methodology for assessing the effectiveness of command, control, and communications systems is extended to include timeliness. The assessment is based on comparing the properties of the system to the mission requirements when both are expressed as loci in a commensurate space of measures of performance. The methodology for evaluating measures of effectiveness is illustrated through application to an idealized fire support system.},
	language = {en},
	number = {6},
	urldate = {2021-11-05},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Cothier, Philippe H. and Levis, Alexander H.},
	year = {1986},
	note = {Number: 6},
	pages = {844--853},
}

@article{handley_incorporating_nodate,
	title = {Incorporating {Heterogeneity} in {Command} {Center} {Interactions}},
	abstract = {One of the many complexities of multinational coalition operations stems from differences in culture, military procedures, and command and control processes between the cooperating command centers. These differences can effect the interactions between decision makers of different command centers and can affect the outcome of the coalition operation. A model can be used to study the effect on coalition performance due to interactions between heterogeneous command centers. A coalition model, composed of individual models of the five-stage interacting decision maker model, which has been modified to include subjective parameters, was used in a virtual experiment. The subjective parameters included in the decision maker model can be any attribute that characterizes the heterogeneity of the decision makers. In this case, the parameters of power distance and uncertainty avoidance were used, two of Hofstede’s [1991] cultural dimensions. Differences in these values across the cooperating command centers can cause communication and coordination difficulties. The accuracy and timeliness of the coalition’s response was used to evaluate its performance as a function of heterogeneity. Including the presence of heterogeneity in the coalition model, through the use of subjective parameters, is the first step in formalizing the process for developing adaptive coalition architectures.},
	language = {en},
	author = {Handley, Holly A H and Levis, Alexander H},
	pages = {15},
}

@article{gonda_creating_2013,
	title = {Creating {A} {Collaborative} {Virtual} {Command} {Center} {Among} {Four} {Separate} {Organizations} {In} {The} {United} {States} {Army}: {An} {Exploratory} {Case} {Study}},
	abstract = {While individual leadership skills are an important factor in transforming organizations, leading through a common mission and shared purpose of collaborative and efficient interaction requires leaders to participate in different ways. The purpose of this study was to determine how to create a collaborative and efficient virtual Command Center within the new leadership structure of a Life Cycle Management Command within the United States Army. This case study employed a variety of organization development tools to bring together leadership, change agents, and customers to create a new vision and strategic plan of how to operate effectively. Leadership used the results of this study to implement the structural changes and new processes required for a successful transformation to a virtual business.},
	language = {en},
	author = {Gonda, Teresa and Kohnke, Anne},
	year = {2013},
	pages = {20},
}

@article{tosh_elements_2020,
	title = {Elements of an {Effective} {Incident} {Command} {Center}},
	volume = {95},
	issn = {00256196},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0025619620306480},
	doi = {10.1016/j.mayocp.2020.06.026},
	language = {en},
	number = {9},
	urldate = {2021-11-04},
	journal = {Mayo Clinic Proceedings},
	author = {Tosh, Pritish K. and Bucks, Colin M. and O’Horo, John C. and DeMartino, Erin S. and Johnson, Jay M. and Callies, Byron I.},
	month = sep,
	year = {2020},
	note = {Number: 9},
	pages = {S3--S7},
}

@inproceedings{sundaramurthy_tale_2014,
	address = {Scottsdale, Arizona, USA},
	title = {A {Tale} of {Three} {Security} {Operation} {Centers}},
	isbn = {978-1-4503-3152-4},
	url = {http://dl.acm.org/citation.cfm?doid=2663887.2663904},
	doi = {10.1145/2663887.2663904},
	abstract = {Security researchers have been trying to understand functioning of a security operation center (SOC) and how security analysts perform their job. This eﬀort is motivated by the fact that security monitoring and analysis is not just a technical problem. Researchers must take into consideration the human and organizational factors for their research ideas to succeed. Much work towards this direction has been through interviews of security analysts in SOCs. Interviews, however useful, will not be always possible as analysts work in a high-stress and time constrained environment. Thus the understanding of operational challenges through interviews is quite shallow. There is also an issue of trust that limits the amount of information an analyst shares with an interviewing researcher. In our work, we take an anthropological approach to address this problem. Students with Computer Science background get trained in anthropological methods by an anthropologist and are embedded as security analysts in operation centers. Embedded students perform the same job as an analyst and see the operational world from the view point of an analyst. Through reﬂection on the observations made by the students we gain a holistic perspective of the challenges in operation centers. In this paper we report preliminary results on the ongoing ﬁeldwork at two corporate and a University SOC.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 2014 {ACM} {Workshop} on {Security} {Information} {Workers} - {SIW} '14},
	publisher = {ACM Press},
	author = {Sundaramurthy, Sathya Chandran and Case, Jacob and Truong, Tony and Zomlot, Loai and Hoffmann, Marcel},
	year = {2014},
	pages = {43--50},
}

@inproceedings{jacobs_classification_2013,
	address = {Johannesburg, South Africa},
	title = {Classification of {Security} {Operation} {Centers}},
	isbn = {978-1-4799-0808-0},
	url = {http://ieeexplore.ieee.org/document/6641054/},
	doi = {10.1109/ISSA.2013.6641054},
	abstract = {Security Operation Centers (SOCs) are a necessary service for organisations that want to address compliance and threat management. While there are frameworks in existence that addresses the technology aspects of these services, a holistic framework addressing processes, staffing and technology currently do not exist. Additionally, it would be useful for organizations and constituents considering building, buying or selling these services to measure the effectiveness and maturity of the provided services. In this paper, we propose a classification and rating scheme for SOC services, evaluating both the capabilities and the maturity of the services offered.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {2013 {Information} {Security} for {South} {Africa}},
	publisher = {IEEE},
	author = {Jacobs, Pierre and Arnab, Alapan and Irwin, Barry},
	month = aug,
	year = {2013},
	pages = {1--7},
}

@article{cho_capturing_2020,
	title = {Capturing {Tacit} {Knowledge} in {Security} {Operation} {Centers}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9007685/},
	doi = {10.1109/ACCESS.2020.2976076},
	abstract = {The use of tacit knowledge has previously been shown to help expedite problem-solving procedures in the setting of medical emergency responses, as individuals can use past experiences in present and future challenges. However, there is a lack of understanding in its application in IT and socio-technical management. This paper examines the thought processes observed in Security Operational Centre (SOC) analysts facing threat events to lay the groundwork for tacit knowledge management in SOCs. Based on Sternberg’s ﬁeldwork in tacit knowledge, we conducted semi-structured interviews with ten analysts to explore the key artefacts and individual traits that aid their approach to communication, and to examine the thought processes under hypothetical incident handling scenarios. The results highlight a unanimous pursuit of Root Cause Analysis (RCA) upon the outbreak of an incident and stages of decision-making when escalating to third party support providers. Using Business Process Modelling and Notation (BPMN), we show the procedural elements of tacit knowledge from several scenarios. The results also suggest that simulation environments and physical proximity with analysts and vendors can facilitate the transfer of tacit knowledge more effectively in SOCs.},
	language = {en},
	urldate = {2021-11-04},
	journal = {IEEE Access},
	author = {Cho, Selina Y. and Happa, Jassim and Creese, Sadie},
	year = {2020},
	pages = {42021--42041},
}

@article{noauthor_toward_2022,
	title = {Toward the {Cyber}-{Physical} {Frontier}: {Reflections} and {Directions}},
	language = {en},
	year = {2022},
	pages = {5},
}

@article{goldberg_alternative_nodate,
	title = {An {Alternative} "{Description} of {Personality}": {The} {Big}-{Five} {Factor} {Structure}},
	language = {en},
	author = {Goldberg, Lewis R},
	pages = {14},
}

@article{maznevski_cultural_2002,
	title = {Cultural {Dimensions} at the {Individual} {Level} of {Analysis}: {The} {Cultural} {Orientations} {Framework}},
	volume = {2},
	issn = {1470-5958, 1741-2838},
	shorttitle = {Cultural {Dimensions} at the {Individual} {Level} of {Analysis}},
	url = {http://journals.sagepub.com/doi/10.1177/147059580223001},
	doi = {10.1177/147059580223001},
	abstract = {This article describes a theoretically-grounded framework of cultural dimensions conceptualized and operationalized at the individual level of analysis, based on the work of anthropologists Kluckhohn and Strodtbeck. We present empirical data gathered from five countries – Canada, Mexico, the Netherlands, Taiwan, and the United States – to assess the validity of the framework. We then use the results to explore how the cultural orientations framework can add insight and new perspectives to critical questions in cross cultural management research.},
	language = {en},
	number = {3},
	urldate = {2021-10-14},
	journal = {International Journal of Cross Cultural Management},
	author = {Maznevski, Martha L. and Gomez, Carolina B. and DiStefano, Joseph J. and Noorderhaven, Niels G. and Wu, Pei-Chuan},
	month = dec,
	year = {2002},
	note = {Number: 3},
	pages = {275--295},
}

@article{kostova_interplay_nodate,
	title = {On the {Interplay} between {Requirements}, {Engineering}, and {Artiﬁcial} {Intelligence}},
	abstract = {With this paper, we present our reﬂections on the issues that are faced by the Requirements Engineering academic discipline and practice. The current reality of the Artiﬁcial Intelligence and Machine Learning hype penetrating from research into all industry sectors and all phases of system design and development is a transformative shift that inﬂuences the way Requirements Engineering is conducted and the nature of the systems that are engineered. We identify two sides of this transformation with regards to the Requirements Engineering discipline: (1) Artiﬁcial Intelligence tools are used more and more during the Requirements Engineering process, (2) the Requirements Engineering process for systems that include Artiﬁcial Intelligence is diﬀerent. By identifying and framing these changes, we pose questions about what it means to engineer requirements. Our analysis asks more questions than it answers. We hope to engage the Requirements Engineering academic community in a larger conversation about the role of Requirements Engineering in the changing world and about a possible new vision of engineering becoming secondary to requirements in Requirements Engineering.},
	language = {en},
	author = {Kostova, Blagovesta and Gurses, Seda and Delft, TU and Leuven, KU and Wegmann, Alain},
	pages = {5},
}

@inproceedings{gladisch_experience_2019,
	title = {Experience {Paper}: {Search}-{Based} {Testing} in {Automated} {Driving} {Control} {Applications}},
	shorttitle = {Experience {Paper}},
	doi = {10.1109/ASE.2019.00013},
	abstract = {Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Gladisch, Christoph and Heinz, Thomas and Heinzemann, Christian and Oehlerking, Jens and von Vietinghoff, Anne and Pfitzer, Tim},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Measurement, Monitoring, Optimization, Software, Test pattern generators, Tools, automated driving, automated test generation, experience paper, search-based testing},
	pages = {26--37},
}

@inproceedings{wang_exploratory_2021,
	address = {Athens Greece},
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468559},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	month = aug,
	year = {2021},
	pages = {20--31},
}

@inproceedings{hutchinson_model-driven_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Model-driven engineering practices in industry},
	isbn = {978-1-4503-0445-0},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985882},
	doi = {10.1145/1985793.1985882},
	abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some ‘lessons learned’, in particular the importance of complex organizational, managerial and social factors – as opposed to simple technical factors – in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
	language = {en},
	urldate = {2021-09-14},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Hutchinson, John and Rouncefield, Mark and Whittle, Jon},
	month = may,
	year = {2011},
	pages = {633--642},
}

@article{bei_cyber_2011,
	title = {Cyber defense competition: a tale of two teams},
	abstract = {Collegiate Cyber Defense Competitions have recently grown in popularity as a means of providing real-world experiences to students learning computer security at the college level. Preparation and training for these competitions focuses students on essential skills needed to defend networks against real threats and better prepares them for the problems and conditions they may encounter outside the protection of university run labs. This paper highlights the benefits of Cyber Defense Competitions and documents the experiences of two teams that trained and competed in the Northwest regional cyber defense competition. Both teams benefited from participating in the competition with students expressing positive learning experiences. Recommendations for other schools that may be interested in competing or setting up in-house cyber defense exercises will be presented.},
	language = {en},
	author = {Bei, Yan and Kesterson, Robert and Gwinnup, Kyle and Taylor, Carol},
	year = {2011},
	pages = {7},
}

@article{carlin_developing_2010,
	title = {Developing the {Cyber} {Defenders} of {Tomorrow} {With} {Regional} {Collegiate} {Cyber} {Defense} {Competitions} ({CCDC})},
	abstract = {With the projected higher demand for Network Systems Analysts and increasing computer crime, network security specialists are an organization’s ﬁrst line of defense. The principle function of this paper is to provide the evolution of Collegiate Cyber Defense Competitions (CCDC), event planning required, soliciting sponsors, recruiting personnel for the operations, red, white and blue teams. Information on one school’s preparation will be provided with a review of what could have been improved to prepare their team for the competition.},
	language = {en},
	author = {Carlin, Anna and Manson, Daniel P and Zhu, Jake},
	year = {2010},
	pages = {10},
}

@article{werlinger_integrated_2009,
	title = {An integrated view of human, organizational, and technological challenges of {IT} security management},
	volume = {17},
	issn = {0968-5227},
	url = {https://www.emerald.com/insight/content/doi/10.1108/09685220910944722/full/html},
	doi = {10.1108/09685220910944722},
	abstract = {Purpose – The purpose of this study is to determine the main challenges that IT security practitioners face in their organizations, including the interplay among human, organizational, and technological factors.},
	language = {en},
	number = {1},
	urldate = {2021-09-02},
	journal = {Information Management \& Computer Security},
	author = {Werlinger, Rodrigo and Hawkey, Kirstie and Beznosov, Konstantin},
	editor = {Furnell, Steven M.},
	month = mar,
	year = {2009},
	note = {Number: 1},
	pages = {4--19},
}

@article{werlinger_preparation_2010,
	title = {Preparation, detection, and analysis: the diagnostic work of {IT} security incident response},
	volume = {18},
	issn = {0968-5227},
	shorttitle = {Preparation, detection, and analysis},
	url = {https://www.emerald.com/insight/content/doi/10.1108/09685221011035241/full/html},
	doi = {10.1108/09685221011035241},
	abstract = {Purpose – The purpose of this paper is to examine security incident response practices of information technology (IT) security practitioners as a diagnostic work process, including the preparation phase, detection, and analysis of anomalies.},
	language = {en},
	number = {1},
	urldate = {2021-09-02},
	journal = {Information Management \& Computer Security},
	author = {Werlinger, Rodrigo and Muldner, Kasia and Hawkey, Kirstie and Beznosov, Konstantin},
	editor = {Furnell, Steven M.},
	month = mar,
	year = {2010},
	note = {Number: 1},
	pages = {26--42},
}

@article{werlinger_security_2008,
	title = {Security {Practitioners} in {Context}: {Their} {Activities} and {Interactions}},
	abstract = {This study develops the context of interactions of IT security practitioners. Preliminary qualitative analysis of 22 interviews (to date) and participatory observation has identified eight different types of activities that require interactions between security practitioners and different stakeholders. Our analysis shows that the tools used by our participants do not provide sufficient support for their complex security tasks, including the interactions with other stakeholders. We provide recommendations to improve tool support for security practitioners.},
	language = {en},
	author = {Werlinger, Rodrigo and Hawkey, Kirstie and Beznosov, Konstantin},
	year = {2008},
	pages = {6},
}

@article{vieane_task_2017,
	title = {Task {Interruptions} {Undermine} {Cyber} {Defense}},
	volume = {61},
	issn = {2169-5067, 1071-1813},
	url = {http://journals.sagepub.com/doi/10.1177/1541931213601576},
	doi = {10.1177/1541931213601576},
	abstract = {Computer network defense analysts engage a difficult, though critical, task in cyber defense. Anecdotally, these operators complain of frequent task interruptions while they are performing their duties. The goal for the current study was to investigate the effect of a commonly reported interruption, answering email, on accuracy and completion times in a simulated network analyst task. During task trials, participants were interrupted by emails between alert investigations, during alert investigations, or not at all (control). The results indicated that email interruptions increased alert completion times regardless of when they occurred, but interruptions that occurred during an alert investigation also reduced the accuracy of subsequent judgments about alert threat. Overall, the results suggest that task interruptions can potentially undermine cyber defense, and steps should be taken to better quantify and mitigate this threat.},
	language = {en},
	number = {1},
	urldate = {2021-09-01},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Vieane, Alex and Funke, Gregory and Greenlee, Eric and Mancuso, Vincent and Borghetti, Brett and Miller, Brent and Menke, Lauren and Brown, Rebecca and Foroughi, Cyrus K. and Boehm-Davis, Deborah},
	month = sep,
	year = {2017},
	note = {Number: 1},
	pages = {375--379},
}

@article{sawyer_hacking_2018,
	title = {Hacking the {Human}: {The} {Prevalence} {Paradox} in {Cybersecurity}},
	volume = {60},
	issn = {0018-7208, 1547-8181},
	shorttitle = {Hacking the {Human}},
	url = {http://journals.sagepub.com/doi/10.1177/0018720818780472},
	doi = {10.1177/0018720818780472},
	abstract = {Objective:
              This work assesses the efficacy of the “prevalence effect” as a form of cyberattack in human-automation teaming, using an email task.
            
            
              Background:
              Under the prevalence effect, rare signals are more difficult to detect, even when taking into account their proportionally low occurrence. This decline represents diminished human capability to both detect and respond. As signal probability (SP) approaches zero, accuracy exhibits logarithmic decay. Cybersecurity, a context in which the environment is entirely artificial, provides an opportunity to manufacture conditions enhancing or degrading human performance, such as prevalence effects. Email cybersecurity prevalence effects have not previously been demonstrated, nor intentionally manipulated.
            
            
              Method:
              The Email Testbed (ET) provides a simulation of a clerical email work involving messages containing sensitive personal information. Using the ET, participants were presented with 300 email interactions and received cyberattacks at rates of either 1\%, 5\%, or 20\%.
            
            
              Results:
              Results demonstrated the existence and power of prevalence effects in email cybersecurity. Attacks delivered at a rate of 1\% were significantly more likely to succeed, and the overall pattern of accuracy across declining SP exhibited logarithmic decay.
            
            
              Application:
              These findings suggest a “prevalence paradox” within human-machine teams. As automation reduces attack SP, the human operator becomes increasingly likely to fail in detecting and reporting attacks that remain. In the cyber realm, the potential to artificially inflict this state on adversaries, hacking the human operator rather than algorithmic defense, is considered. Specific and general information security design countermeasures are offered.},
	language = {en},
	number = {5},
	urldate = {2021-09-01},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	author = {Sawyer, Ben D. and Hancock, Peter A.},
	month = aug,
	year = {2018},
	note = {Number: 5},
	pages = {597--609},
}

@article{martin_signal_2018,
	title = {Signal {Detection} {Theory} ({SDT}) {Is} {Effective} for {Modeling} {User} {Behavior} {Toward} {Phishing} and {Spear}-{Phishing} {Attacks}},
	volume = {60},
	issn = {0018-7208, 1547-8181},
	url = {http://journals.sagepub.com/doi/10.1177/0018720818789818},
	doi = {10.1177/0018720818789818},
	abstract = {Objective: To examine the utility of equal-variance signal detection theory (EVSDT) for evaluating and understanding human detection of phishing and spear-phishing e-mail scams.
Background: Although the majority of cybersecurity breaches are due to erroneous responses to deceptive phishing e-mails, it is unclear how best to quantify performance in this context. In particular, it is unclear whether equal variances can safely be assumed in the SDT model, or, relatedly, whether degree of targeting, or threat level, primarily affects mean separation or evidence variability.
Method: Through an online inbox simulation, the present research found that differences in susceptibility to phishing and spear-phishing e-mails could be carefully quantified with respect to detection accuracy and response bias through the use of an EVSDT framework.
Results: The results indicated that EVSDT-based point metrics are effective for modeling and measuring phishing susceptibility in the inbox task, without the need for parameter estimation or model comparison involving unequal-variance SDT (UVSDT). Threat level modulated mean separation, with no effects on signal variances.
Conclusion: These findings support the viability of using EVSDT to initially assess and subsequently monitor training effectiveness for phishing susceptibility, thereby providing measures that are superior to more intuitive metrics, which typically confound an individual’s bias and accuracy. Effects of threat level mapped clearly onto distribution means with no effect on variances, suggesting phishing susceptibility primarily reflects temporally stable discriminative characteristics of observers. Notably, results indicated that people are particularly poor at identifying spear-phishing e-mail threats (demonstrating only 40\% accuracy).},
	language = {en},
	number = {8},
	urldate = {2021-09-01},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	author = {Martin, Jaclyn and Dubé, Chad and Coovert, Michael D.},
	month = dec,
	year = {2018},
	note = {Number: 8},
	pages = {1179--1191},
}

@inproceedings{conti_attacking_2005,
	address = {Pittsburgh, Pennsylvania},
	title = {Attacking information visualization system usability overloading and deceiving the human},
	isbn = {978-1-59593-178-8},
	url = {http://portal.acm.org/citation.cfm?doid=1073001.1073010},
	doi = {10.1145/1073001.1073010},
	abstract = {Information visualization is an effective way to easily comprehend large amounts of data. For such systems to be truly effective, the information visualization designer must be aware of the ways in which their system may be manipulated and protect their users from attack. In addition, users should be aware of potential attacks in order to minimize or negate their effect. These attacks target the information visualization system as well as the perceptual, cognitive and motor capabilities of human end users. To identify and help counter these attacks we present a framework for information visualization system security analysis, a taxonomy of visualization attacks and technology independent principles for countering malicious visualizations. These themes are illustrated with case studies and working examples from the network security visualization domain, but are widely applicable to virtually any information visualization system.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the 2005 symposium on {Usable} privacy and security  - {SOUPS} '05},
	publisher = {ACM Press},
	author = {Conti, Gregory and Ahamad, Mustaque and Stasko, John},
	year = {2005},
	pages = {89--100},
}

@article{nagy_empirical_nodate,
	title = {An empirical study on current models for reasoning about digital evidence},
	abstract = {The forensic process relies on the scientific method to scrutinize recovered evidence that either supports or negates an investigative hypothesis. Currently, analysis of digital evidence remains highly subjective to the forensic practitioner. Digital forensics is in need of a deterministic approach to obtain the most judicious conclusions from evidence. The objective of this paper is to examine current methods of digital evidence analysis. It describes the mechanisms for which these processes may be carried out, and discusses the key obstacles presented by each. Lastly, it concludes with suggestions for further improvement of the digital forensic process as a whole.},
	language = {en},
	author = {Nagy, Stefan and Palmer, Imani and Sundaramurthy, Sathya Chandran and Ou, Xinming and Campbell, Roy},
	pages = {7},
}

@incollection{li_android_2017,
	address = {Cham},
	title = {Android {Malware} {Clustering} {Through} {Malicious} {Payload} {Mining}},
	volume = {10453},
	isbn = {978-3-319-66331-9 978-3-319-66332-6},
	url = {http://link.springer.com/10.1007/978-3-319-66332-6_9},
	abstract = {Clustering has been well studied for desktop malware analysis as an eﬀective triage method. Conventional similarity-based clustering techniques, however, cannot be immediately applied to Android malware analysis due to the excessive use of third-party libraries in Android application development and the widespread use of repackaging in malware development. We design and implement an Android malware clustering system through iterative mining of malicious payload and checking whether malware samples share the same version of malicious payload. Our system utilizes a hierarchical clustering technique and an eﬃcient bit-vector format to represent Android apps. Experimental results demonstrate that our clustering approach achieves precision of 0.90 and recall of 0.75 for Android Genome malware dataset, and average precision of 0.98 and recall of 0.96 with respect to manually veriﬁed ground-truth.},
	language = {en},
	urldate = {2021-08-31},
	booktitle = {Research in {Attacks}, {Intrusions}, and {Defenses}},
	publisher = {Springer International Publishing},
	author = {Li, Yuping and Jang, Jiyong and Hu, Xin and Ou, Xinming},
	editor = {Dacier, Marc and Bailey, Michael and Polychronakis, Michalis and Antonakakis, Manos},
	year = {2017},
	doi = {10.1007/978-3-319-66332-6_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {192--214},
}

@article{sundaramurthy_anthropological_2014,
	title = {An {Anthropological} {Approach} to {Studying} {CSIRTs}},
	volume = {12},
	issn = {1558-4046},
	doi = {10.1109/MSP.2014.84},
	abstract = {The ethnographic method of participant observation can help researchers better understand the challenges computer security incident response teams face by illuminating underlying assumptions and tacit practices that shape how tools are actually used in different contexts.},
	number = {5},
	journal = {IEEE Security Privacy},
	author = {Sundaramurthy, Sathya Chandran and McHugh, John and Ou, Xinming Simon and Rajagopalan, S. Raj and Wesch, Michael},
	month = sep,
	year = {2014},
	note = {Number: 5
Conference Name: IEEE Security Privacy},
	keywords = {Behavioral science, Computer security, Context awareness, Cultural differences, System-on-chip, communication/networking and information technology, computer systems organization, computing milieux, management of computing and information systems, network-level security and protection, security},
	pages = {52--60},
}

@article{sundaramurthy_humans_2017,
	title = {Humans {Are} {Dynamic} - {Our} {Tools} {Should} {Be} {Too}},
	volume = {21},
	issn = {1941-0131},
	doi = {10.1109/MIC.2017.52},
	abstract = {Security Operation Centers (SOCs) are being operated by universities, government agencies, and corporations to defend their enterprise networks and identify and thwart malicious behaviors in both networks and hosts. The success of a SOC depends on combining good tools and processes with efficient and effective analysts. During four years of anthropological fieldwork methods to study SOCs, the authors discovered that successful SOC innovations must resolve multiple internal and external conflicts to be effective and efficient. This discovery, guided by activity theory (AT) as a framework for analyzing the fieldwork data, enabled them understand these realities. Their research indicates conflict resolution is a prerequisite for continuous improvement of SOCs in both human and technological aspects. Failure to do so can lead to adverse effects, such as analyst burnout and reduction in overall effectiveness.},
	number = {3},
	journal = {IEEE Internet Computing},
	author = {Sundaramurthy, Sathya Chandran and Wesch, Michael and Ou, Xinming and McHugh, John and Rajagopalan, S. Raj and Bardas, Alexandru G.},
	month = may,
	year = {2017},
	note = {Number: 3
Conference Name: IEEE Internet Computing},
	keywords = {Computer security, Creativity, Human factors, Internet/Web technologies, Malware, SOC, Technological innovation, activity theory, security and privacy, security operation center, usable security},
	pages = {40--46},
}

@article{nyre-yu_observing_2019,
	title = {Observing {Cyber} {Security} {Incident} {Response}: {Qualitative} {Themes} {From} {Field} {Research}},
	volume = {63},
	issn = {2169-5067, 1071-1813},
	shorttitle = {Observing {Cyber} {Security} {Incident} {Response}},
	url = {http://journals.sagepub.com/doi/10.1177/1071181319631016},
	doi = {10.1177/1071181319631016},
	abstract = {Cyber security increasingly focuses on the challenges faced by network defenders. Cultural and security-driven sentiments about external observation, as well as publication concerns, limit the ability of researchers to understand the context surrounding incident response. Context awareness is crucial to inform design and engineering. Furthermore, these perspectives can be heavily influenced by the targeted sector or industry of the research. Together, a lack of broad contextual understanding may be biasing approaches to improving operations, and driving faulty assumptions in cyber teams. A qualitative field study was conducted in three computer security incident response teams (CSIRTs) and included perspectives of government, academia, and private sector teams. Themes emerged and provide insights across multiple aspects of incident response, including information sharing, organization, learning, and automation. The need to focus on vertical integration of issues at different levels of the incident response system is also discussed. Future research will build upon these results, using them to inform technology advancement in CSIR settings.},
	language = {en},
	number = {1},
	urldate = {2021-08-31},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Nyre-Yu, Megan and Gutzwiller, Robert S. and Caldwell, Barrett S.},
	month = nov,
	year = {2019},
	note = {Number: 1},
	pages = {437--441},
}

@article{chandran_turning_nodate,
	title = {Turning {Contradictions} into {Innovations} or: {How} {We} {Learned} to {Stop} {Whining} and {Improve} {Security} {Operations}},
	abstract = {Eﬀorts to improve the eﬃciency of security operation centers (SOCs) have emphasized building tools for analysts or understanding the human and organizational factors involved. The importance of viewing the viability of a solution from multiple perspectives has been largely ignored. Multiple perspectives arise because of inherent conﬂicts among the objectives a SOC has to meet and diﬀerences between the goals of the parties involved. During the 3.5 years that we have used anthropological ﬁeldwork methods to study SOCs, we discovered that successful SOC innovations must resolve these conﬂicts to be eﬀective in improving operational eﬃciency. This discovery was guided by Activity Theory (AT), which provided a framework for analyzing our ﬁeldwork data. We use the version of AT proposed by Engestr¨om to model SOC operations. Template analysis, a qualitative data analysis technique, guided by AT validated the existence of contradictions in SOCs. The same technique was used to elicit from the data concrete contradictions and how they were resolved. Our analysis provide evidence of the importance of conﬂict resolution as a prerequisite for operations improvement. AT enabled us to understand why some of our innovations worked in the SOCs we studied (and why others failed). AT helps us see a potentially successful and repeatable mechanism for introducing new technologies to future SOCs. Understanding and supporting all of the spoken and unspoken requirements of SOC analysts and managers appears to be the only way to get new technologies accepted and used in SOCs.},
	language = {en},
	author = {Chandran, Sathya and McHugh, John and Ou, Xinming},
	pages = {16},
}

@article{spring_human_nodate,
	title = {Human decision-making in computer security incident response},
	language = {en},
	author = {Spring, Jonathan Michael and London, University College},
	pages = {319},
}

@article{bettenburg_what_nodate,
	title = {What {Makes} a {Good} {Bug} {Report}?},
	abstract = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to ﬁnd out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are at the same time most difﬁcult to provide for users. Such insight is helpful to design new bug tracking tools that guide users at collecting and providing more helpful information.},
	language = {en},
	author = {Bettenburg, Nicolas and Just, Sascha and Schröter, Adrian},
	pages = {11},
}

@inproceedings{rahaman_program_2017,
	title = {Program {Analysis} of {Cryptographic} {Implementations} for {Security}},
	doi = {10.1109/SecDev.2017.23},
	abstract = {Cryptographic implementation errors in popular open source libraries (e.g., OpenSSL, GnuTLS, BotanTLS, etc.) and the misuses of cryptographic primitives (e.g., as in Juniper Network) have been the major source of vulnerabilities in the wild. These serious problems prompt the need for new compile-time security checking. Such security enforcements demand the study of various cryptographic properties and their mapping into enforceable program analysis rules. We refer to this new security approach as cryptographic program analysis (CPA). In this paper, we show how cryptographic program analysis can be performed effectively andits security applications. Specifically, we systematically investigate different threat categories on various cryptographicimplementations and their usages. Then, we derive varioussecurity rules, which are enforceable by program analysistools during code compilation. We also demonstrate the capabilities of static taint analysis to enforce most of these security rules and provide a prototype implementation. We point out promising future research and development directions in this new area of cryptographic program analysis.},
	booktitle = {2017 {IEEE} {Cybersecurity} {Development} ({SecDev})},
	author = {Rahaman, Sazzadur and Yao, Danfeng},
	month = sep,
	year = {2017},
	keywords = {Ciphers, Cryptographic Program Analysis, Cryptography, Encryption, Libraries, Program Analysis, Security, Side-channel attacks, Tools},
	pages = {61--68},
}

@article{akhawe_alice_nodate,
	title = {Alice in {Warningland}: {A} {Large}-{Scale} {Field} {Study} of {Browser} {Security} {Warning} {Effectiveness}},
	abstract = {We empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature. We used Mozilla Firefox and Google Chrome’s in-browser telemetry to observe over 25 million warning impressions in situ. During our ﬁeld study, users continued through a tenth of Mozilla Firefox’s malware and phishing warnings, a quarter of Google Chrome’s malware and phishing warnings, and a third of Mozilla Firefox’s SSL warnings. This demonstrates that security warnings can be effective in practice; security experts and system architects should not dismiss the goal of communicating security information to end users. We also ﬁnd that user behavior varies across warnings. In contrast to the other warnings, users continued through 70.2\% of Google Chrome’s SSL warnings. This indicates that the user experience of a warning can have a signiﬁcant impact on user behavior. Based on our ﬁndings, we make recommendations for warning designers and researchers.},
	language = {en},
	author = {Akhawe, Devdatta and Felt, Adrienne Porter},
	pages = {17},
}

@phdthesis{cetin_increasing_2021,
	title = {Increasing the {Impact} of {Voluntary} {Action} {Against} {Cybercrime}},
	url = {http://resolver.tudelft.nl/uuid:ad5d9147-b3ef-4708-b954-142b00820499},
	language = {en},
	urldate = {2021-08-25},
	school = {Delft University of Technology},
	author = {Çetin, F.O.},
	month = aug,
	year = {2021},
	doi = {10.4233/UUID:AD5D9147-B3EF-4708-B954-142B00820499},
	doi = {10.4233/UUID:AD5D9147-B3EF-4708-B954-142B00820499},
}

@inproceedings{schechter_emperors_2007,
	address = {Berkeley, CA},
	title = {The {Emperor}'s {New} {Security} {Indicators}},
	isbn = {978-0-7695-2848-9},
	url = {http://ieeexplore.ieee.org/document/4223213/},
	doi = {10.1109/SP.2007.35},
	abstract = {We evaluate website authentication measures that are designed to protect users from man-in-the-middle, ‘phishing’, and other site forgery attacks. We asked 67 bank customers to conduct common online banking tasks. Each time they logged in, we presented increasingly alarming clues that their connection was insecure. First, we removed HTTPS indicators. Next, we removed the participant’s site-authentication image—the customer-selected image that many websites now expect their users to verify before entering their passwords. Finally, we replaced the bank’s password-entry page with a warning page. After each clue, we determined whether participants entered their passwords or withheld them.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {2007 {IEEE} {Symposium} on {Security} and {Privacy} ({SP} '07)},
	publisher = {IEEE},
	author = {Schechter, Stuart E. and Dhamija, Rachna and Ozment, Andy and Fischer, Ian},
	month = may,
	year = {2007},
	pages = {51--65},
}

@article{wash_out_nodate,
	title = {Out of the {Loop}: {How} {Automated} {Software} {Updates} {Cause} {Unintended} {Security} {Consequences}},
	abstract = {When security updates are not installed, or installed slowly, end users are at an increased risk for harm. To improve security, software designers have endeavored to remove the user from the software update loop. However, user involvement in software updates remains necessary; not all updates are wanted, and required reboots can negatively impact users. We used a multi-method approach to collect interview, survey, and computer log data from 37 Windows 7 users. We compared what the users think is happening on their computers (interview and survey data), what users want to happen on their computer (interview and survey data), and what was actually going on (log data). We found that 28 out of our 37 participants had a misunderstanding about what was happening on their computer, and that over half of the participants could not execute their intentions for computer management.},
	language = {en},
	author = {Wash, Rick and Rader, Emilee and Vaniea, Kami and Rizor, Michelle},
	pages = {16},
}

@article{sunshine_crying_nodate,
	title = {Crying {Wolf}: {An} {Empirical} {Study} of {SSL} {Warning} {Eﬀectiveness}},
	abstract = {Web users are shown an invalid certiﬁcate warning when their browser cannot validate the identity of the websites they are visiting. While these warnings often appear in benign situations, they can also signal a man-in-the-middle attack. We conducted a survey of over 400 Internet users to examine their reactions to and understanding of current SSL warnings. We then designed two new warnings using warnings science principles and lessons learned from the survey. We evaluated warnings used in three popular web browsers and our two warnings in a 100participant, between-subjects laboratory study. Our warnings performed signiﬁcantly better than existing warnings, but far too many participants exhibited dangerous behavior in all warning conditions. Our results suggest that, while warnings can be improved, a better approach may be to minimize the use of SSL warnings altogether by blocking users from making unsafe connections and eliminating warnings in benign situations.},
	language = {en},
	author = {Sunshine, Joshua and Egelman, Serge and Almuhimedi, Hazim and Atri, Neha and Cranor, Lorrie Faith},
	pages = {34},
}

@article{cheung_effectiveness_nodate,
	title = {Effectiveness of {Cybersecurity} {Competitions}},
	abstract = {There has been a heightened interest among U.S. government agencies to fund cybersecurity workforce development. These efforts include offering universities funding for student scholarships, funding for building capacity in cybersecurity education, as well as sponsoring cybersecurity competitions, games, and outreach programs. This paper examines the effectiveness of cybersecurity competitions in educating students. Our study shows that though competitions do pique students’ interest, the effectiveness of this approach in producing more high quality professionals can be limited. One reason is that the knowledge barrier to compete in these competitions is high. To be successful, students have to be proﬁcient in operating systems, application services, software engineering, system administration and networking. Many Computer Science and Information Technology students do not feel qualiﬁed, and consequently this reduces participation from a wider student audience. Our approach takes aims at lowering this barrier to entry. We employ a hands-on learning methodology where students attend lectures on background knowledge on weekdays and practice what they learn in weekend workshops. A virtual networking environment is provided for students to practice network defense in the workshops and on their own time.},
	language = {en},
	author = {Cheung, Ronald S and Cohen, Joseph Paul and Lo, Henry Z and Elia, Fabio and Carrillo-Marquez, Veronica},
	pages = {5},
}

@article{fagan_why_nodate,
	title = {Why {Do} {They} {Do} {What} {They} {Do}?},
	abstract = {Usable security researchers have long been interested in what users do to keep their devices and data safe and how that compares to recommendations. Additionally, experts have long debated and studied the psychological underpinnings and motivations for users to do what they do, especially when such behavior is seen as risky, at least to experts. This study investigates user motivations through a survey conducted on Mechanical Turk, which resulted in responses from 290 participants. We use a rational decision model to guide our design, as well as current thought on human motivation in general and in the realm of computer security. Through quantitative and qualitative analysis, we identify key gaps in perception between those who follow common security advice (i.e., update software, use a password manager, use 2FA, change passwords) and those who do not and help explain participants’ motivations behind their decisions. Additionally, we ﬁnd that social considerations are trumped by individualized rationales.},
	language = {en},
	author = {Fagan, Michael and Khan, Mohammad Maiﬁ Hasan},
	pages = {18},
}

@article{cetin_understanding_2016,
	title = {Understanding the role of sender reputation in abuse reporting and cleanup},
	volume = {2},
	issn = {2057-2085, 2057-2093},
	url = {https://academic.oup.com/cybersecurity/article-lookup/doi/10.1093/cybsec/tyw005},
	doi = {10.1093/cybsec/tyw005},
	abstract = {Motivation: Participants on the front lines of abuse reporting have a variety of options to notify intermediaries and resource owners about abuse of their systems and services. These can include emails to personal messages to blacklists to machine-generated feeds. Recipients of these reports have to voluntarily act on this information. We know remarkably little about the factors that drive higher response rates to abuse reports. One such factor is the reputation of the sender. In this article, we present the ﬁrst randomized controlled experiment into sender reputation. We used a private datafeed of Asprox-infected websites to issue notiﬁcations from three senders with different reputations: an individual, a university and an established anti-malware organization.},
	language = {en},
	number = {1},
	urldate = {2021-08-24},
	journal = {Journal of Cybersecurity},
	author = {Çetin, Orçun and Hanif Jhaveri, Mohammad and Gañán, Carlos and van Eeten, Michel and Moore, Tyler},
	month = dec,
	year = {2016},
	note = {Number: 1},
	pages = {83--98},
}

@article{almuhimedi_your_nodate,
	title = {Your {Reputation} {Precedes} {You}: {History}, {Reputation}, and the {Chrome} {Malware} {Warning}},
	abstract = {Several web browsers, including Google Chrome and Mozilla Firefox, use malware warnings to stop people from visiting infectious websites. However, users can choose to click through (i.e., ignore) these malware warnings. In Google Chrome, users click through a ﬁfth of malware warnings on average. We investigate factors that may contribute to why people ignore such warnings. First, we examine ﬁeld data to see how browsing history aﬀects click-through rates. We ﬁnd that users consistently heed warnings about websites that they have not visited before. However, users respond unpredictably to warnings about websites that they have previously visited. On some days, users ignore more than half of warnings about websites they’ve visited in the past. Next, we present results of an online, survey-based experiment that we ran to gain more insight into the eﬀects of reputation on warning adherence. Participants said that they trusted high-reputation websites more than the warnings; however, their responses suggest that a notable minority of people could be swayed by providing more information. We provide recommendations for warning designers and pose open questions about the design of malware warnings.},
	language = {en},
	author = {Almuhimedi, Hazim and Felt, Adrienne Porter and Reeder, Robert W and Consolvo, Sunny},
	pages = {16},
}

@article{urban_notice_nodate,
	title = {Notice and {Takedown} in {Everyday} {Practice}},
	language = {en},
	author = {Urban, Jennifer M and Karaganis, Joe and Schofield, Brianna L},
	pages = {182},
}

@article{ukrop_will_2020,
	title = {Will {You} {Trust} {This} {TLS} {Certificate}?: {Perceptions} of {People} {Working} in {IT} ({Extended} {Version})},
	volume = {1},
	issn = {2692-1626, 2576-5337},
	shorttitle = {Will {You} {Trust} {This} {TLS} {Certificate}?},
	url = {https://dl.acm.org/doi/10.1145/3419472},
	doi = {10.1145/3419472},
	abstract = {Flawed TLS certificates are not uncommon on the Internet. While they signal a potential issue, in most cases they have benign causes (e.g., misconfiguration or even deliberate deployment). This adds fuzziness to the decision on whether to trust a connection or not. Little is known about perceptions of flawed certificates by IT professionals, even though their decisions impact high numbers of end users. Moreover, it is unclear how much the content of error messages and documentation influences these perceptions.
            To shed light on these issues, we observed 75 attendees of an industrial IT conference investigating different certificate validation errors. We also analyzed the influence of reworded error messages and redesigned documentation. We find that people working in IT have very nuanced opinions, with trust decisions being far from binary. The self-signed and the name-constrained certificates seem to be over-trusted (the latter also being poorly understood). We show that even small changes in existing error messages can positively influence resource use, comprehension, and trust assessment. At the end of the article, we summarize lessons learned from conducting usable security studies with IT professionals.},
	language = {en},
	number = {4},
	urldate = {2021-08-24},
	journal = {Digital Threats: Research and Practice},
	author = {Ukrop, Martin and Kraus, Lydia and Matyas, Vashek},
	month = dec,
	year = {2020},
	note = {Number: 4},
	pages = {1--29},
}

@article{zeng_fixing_nodate,
	title = {Fixing {HTTPS} {Misconﬁgurations} at {Scale}: {An} {Experiment} with {Security} {Notiﬁcations}},
	abstract = {HTTPS is vital to protecting the security and privacy of users on the Internet. As the cryptographic algorithms and standards underlying HTTPS evolve to meet emerging threats, website owners are responsible for updating and maintaining their HTTPS conﬁgurations. In practice, millions of hosts have misconﬁgured and insecure conﬁgurations. In addition to presenting security and privacy risks, misconﬁgurations can harm user experience on the web, when browsers show warnings for deprecated and outdated protocols.},
	language = {en},
	author = {Zeng, Eric and Li, Frank and Stark, Emily},
	pages = {19},
}

@article{vasek_malware_nodate,
	title = {Do malware reports expedite cleanup? {An} experimental study},
	abstract = {Web-based malware is pervasive. Miscreants compromise insecure hosts or even set up dedicated servers to distribute malware to unsuspecting users. This scourge is mainly fought by the voluntary action of private actors who detect and report infections to affected site owners, hosting providers and registrars. In this paper we describe an experiment to assess whether sending reports to affected parties makes a measurable difference in cleaning up malware. Using community reports of malware submitted to StopBadware over two months in Fall 2011, we ﬁnd evidence that detailed notices are immediately effective: 32\% of malware-distributing websites are cleaned within one day of sending a notice, compared to just 13\% of sites not receiving a notice. The improved cleanup rate holds for longer periods, too – 62\% of websites receiving a detailed notice were cleaned up after 16 days, compared to 45\% of websites not receiving a notice. It turns out that including details describing the compromise is essential for the notice to work – sending reports with minimal descriptions of the malware was found to be roughly as effective as not sending reports at all. Furthermore, we present evidence that sending multiple notices from two sources is not helpful. Instead, only the ﬁrst transmitted notice makes a difference.},
	language = {en},
	author = {Vasek, Marie and Moore, Tyler},
	pages = {8},
}

@inproceedings{stock_didnt_2018,
	address = {San Diego, CA},
	title = {Didn't {You} {Hear} {Me}? - {Towards} {More} {Successful} {Web} {Vulnerability} {Notifications}},
	isbn = {978-1-891562-49-5},
	shorttitle = {Didn't {You} {Hear} {Me}?},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01B-1_Stock_paper.pdf},
	doi = {10.14722/ndss.2018.23171},
	abstract = {After treating the notiﬁcation of vulnerable parties as mere side-notes in research, the security community has recently put more focus on how to conduct vulnerability disclosure at scale. The ﬁrst works in this area have shown that while notiﬁcations are helpful to a signiﬁcant fraction of operators, the vast majority of systems remain unpatched. In this paper, we build on these previous works, aiming to understand why the effects are not more signiﬁcant. To that end, we report on a notiﬁcation experiment targeting more than 24,000 domains, which allowed us to analyze what technical and human aspects are roadblocks to a successful campaign. As part of this experiment, we explored potential alternative notiﬁcation channels beyond email, including social media and phone. In addition, we conducted an anonymous survey with the notiﬁed operators, investigating their perspectives on our notiﬁcations. We show the pitfalls of email-based communications, such as the impact of anti-spam ﬁlters, the lack of trust by recipients, and the hesitation in ﬁxing vulnerabilities despite awareness. However, our exploration of alternative communication channels did not suggest a more promising medium. Seeing these results, we pinpoint future directions in improving security notiﬁcations.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Stock, Ben and Pellegrino, Giancarlo and Li, Frank and Backes, Michael and Rossow, Christian},
	year = {2018},
}

@article{stock_hey_nodate,
	title = {Hey, {You} {Have} a {Problem}: {On} the {Feasibility} of {Large}-{Scale} {Web} {Vulnerability} {Notiﬁcation}},
	abstract = {Large-scale discovery of thousands of vulnerable Web sites has become a frequent event, thanks to recent advances in security research and the rise in maturity of Internet-wide scanning tools. The issues related to disclosing the vulnerability information to the affected parties, however, have only been treated as a side note in prior research.},
	language = {en},
	author = {Stock, Ben and Pellegrino, Giancarlo and Rossow, Christian},
	pages = {19},
}

@inproceedings{li_remedying_2016,
	address = {Montréal Québec Canada},
	title = {Remedying {Web} {Hijacking}: {Notification} {Effectiveness} and {Webmaster} {Comprehension}},
	isbn = {978-1-4503-4143-1},
	shorttitle = {Remedying {Web} {Hijacking}},
	url = {https://dl.acm.org/doi/10.1145/2872427.2883039},
	doi = {10.1145/2872427.2883039},
	abstract = {As miscreants routinely hijack thousands of vulnerable web servers weekly for cheap hosting and trafﬁc acquisition, security services have turned to notiﬁcations both to alert webmasters of ongoing incidents as well as to expedite recovery. In this work we present the ﬁrst large-scale measurement study on the effectiveness of combinations of browser, search, and direct webmaster notiﬁcations at reducing the duration a site remains compromised. Our study captures the life cycle of 760,935 hijacking incidents from July, 2014–June, 2015, as identiﬁed by Google Safe Browsing and Search Quality. We observe that direct communication with webmasters increases the likelihood of cleanup by over 50\% and reduces infection lengths by at least 62\%. Absent this open channel for communication, we ﬁnd browser interstitials—while intended to alert visitors to potentially harmful content—correlate with faster remediation. As part of our study, we also explore whether webmasters exhibit the necessary technical expertise to address hijacking incidents. Based on appeal logs where webmasters alert Google that their site is no longer compromised, we ﬁnd 80\% of operators successfully clean up symptoms on their ﬁrst appeal. However, a sizeable fraction of site owners do not address the root cause of compromise, with over 12\% of sites falling victim to a new attack within 30 days. We distill these ﬁndings into a set of recommendations for improving web security and best practices for webmasters.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the 25th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Li, Frank and Ho, Grant and Kuan, Eric and Niu, Yuan and Ballard, Lucas and Thomas, Kurt and Bursztein, Elie and Paxson, Vern},
	month = apr,
	year = {2016},
	pages = {1009--1019},
}

@article{li_youve_nodate,
	title = {You’ve {Got} {Vulnerability}: {Exploring} {Effective} {Vulnerability} {Notiﬁcations}},
	abstract = {Security researchers can send vulnerability notiﬁcations to take proactive measures in securing systems at scale. However, the factors affecting a notiﬁcation’s efﬁcacy have not been deeply explored. In this paper, we report on an extensive study of notifying thousands of parties of security issues present within their networks, with an aim of illuminating which fundamental aspects of notiﬁcations have the greatest impact on efﬁcacy. The vulnerabilities used to drive our study span a range of protocols and considerations: exposure of industrial control systems; apparent ﬁrewall omissions for IPv6-based services; and exploitation of local systems in DDoS ampliﬁcation attacks. We monitored vulnerable systems for several weeks to determine their rate of remediation. By comparing with experimental controls, we analyze the impact of a number of variables: choice of party to contact (WHOIS abuse contacts versus national CERTs versus US-CERT), message verbosity, hosting an information website linked to in the message, and translating the message into the notiﬁed party’s local language. We also assess the outcome of the emailing process itself (bounces, automated replies, human replies, silence) and characterize the sentiments and perspectives expressed in both the human replies and an optional anonymous survey that accompanied our notiﬁcations.},
	language = {en},
	author = {Li, Frank and Durumeric, Zakir and Czyz, Jakub and Karami, Mohammad and Bailey, Michael and McCoy, Damon and Savage, Stefan and Paxson, Vern},
	pages = {19},
}

@inproceedings{cetin_tell_2019,
	title = {Tell {Me} {You} {Fixed} {It}: {Evaluating} {Vulnerability} {Notifications} via {Quarantine} {Networks}},
	shorttitle = {Tell {Me} {You} {Fixed} {It}},
	doi = {10.1109/EuroSP.2019.00032},
	abstract = {Mechanisms for large-scale vulnerability notifications have been confronted with disappointing remediation rates. It has proven difficult to reach the relevant party and, once reached, to incentivize them to act. We present the first empirical study of a potentially more effective mechanism: quarantining the vulnerable resource until it is remediated. We have measured the remediation rates achieved by a medium-sized ISP for 1, 688 retail customers running open DNS resolvers or Multicast DNS services. These servers can be abused in UDP-based amplification attacks. We assess the effectiveness of quarantining by comparing remediation with two other groups: one group which was notified but not quarantined and another group where no action was taken. We find very high remediation rates for the quarantined users, 87\%, even though they can self-release from the quarantine environment. Of those who received the email-only notification, 76\% remediated. Surprisingly, over half of the customers who were not notified at all also remediated, though this is tied to the fact that many observations of vulnerable servers are transient. All in all, quarantining appears more effective than other notification and remediation mechanisms, but it is also clear that it can not be deployed as a general solution for Internet-wide notifications.},
	booktitle = {2019 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS} {P})},
	author = {Çetin, Orçun and Gañán, Carlos and Altena, Lisette and Tajalizadehkhoob, Samaneh and van Eeten, Michel},
	month = jun,
	year = {2019},
	keywords = {IP networks, Internet, Multicast protocols, Network security, Quarantine networks, Transport protocols, Vulnerability notifications},
	pages = {326--339},
}

@article{cetin_let_nodate,
	title = {Let {Me} {Out}! {Evaluating} the {Effectiveness} of {Quarantining} {Compromised} {Users} in {Walled} {Gardens}},
	abstract = {In the ﬁght to clean up malware-infected machines, notiﬁcations from Internet Service Providers (ISPs) to their customers play a crucial role. Since stand-alone notiﬁcations are routinely ignored, some ISPs have invested in a potentially more effective mechanism: quarantining customers in so-called walled gardens. We present the ﬁrst empirical study on user behavior and remediation effectiveness of quarantining infected machines in broadband networks. We analyzed 1, 736 quarantining actions involving 1, 208 retail customers of a medium-sized ISP in the period of April-October 2017. The ﬁrst two times they are quarantined, users can easily release themselves from the walled garden and around two-thirds of them use this option. Notwithstanding this easy way out, we ﬁnd that 71\% of these users have actually cleaned up the infection during their ﬁrst quarantine period and, of the recidivists, 48\% are cleaned after their second quarantining. Users who do not self-release either contact customer support (30\%) or are released automatically after 30 days (3\%). They have even higher cleanup rates. Reinfection rates are quite low and most users get quarantined only once. Users that remain infected spend less time in the walled garden during subsequent quarantining events, without a major drop in cleanup rates. This suggests there are positive learning effects, rather than mere habituation to being notiﬁed and self-releasing from the walled garden. In the communications with abuse and support staff, a fraction of quarantined users ask for additional help, request a paid technician, voice frustration about being cut off, or threaten to cancel their subscriptions. All in all, walled gardens seem to be a relatively effective and usable mechanism to improve the security of end users. We reﬂect on our main ﬁndings in terms of how to advance this industry best practice for botnet mitigation by ISPs.},
	language = {en},
	author = {Çetin, Orçun and Altena, Lisette and Gañán, Carlos and van Eeten, Michel},
	pages = {14},
}

@article{cetin_make_nodate,
	title = {Make {Notiﬁcations} {Great} {Again}: {Learning} {How} to {Notify} in the {Age} of {Large}-{Scale} {Vulnerability} {Scanning}},
	abstract = {As large-scale vulnerability detection becomes more feasible, it also increases the urgency to ﬁnd effective largescale notiﬁcation mechanisms to inform the affected parties. Researchers, CERTs, security companies and other organizations with vulnerability data have a variety of options to identify, contact and communicate with the actors responsible for the affected system or service. A lot of things can – and do – go wrong. It might be impossible to identify the appropriate recipient of the notiﬁcation, the message might not be trusted by the recipient, it might be overlooked or ignored or misunderstood. Such problems multiply as the volume of notiﬁcations increases. In this paper, we undertake several large-scale notiﬁcation campaigns for a vulnerable conﬁguration of authoritative nameservers. We investigate three issues: What is the most effective way to reach the affected parties? What communication path mobilizes the strongest incentive for remediation? And ﬁnally, what is the impact of providing recipients a mechanism to actively demonstrate the vulnerability for their own system, rather than sending them the standard static notiﬁcation message. We ﬁnd that retrieving contact information at scale is highly problematic, though there are different degrees of failure for different mechanisms. For those parties who are reached, notiﬁcation signiﬁcantly increases remediation rates. Reaching out to nameserver operators directly had better results than going via their customers, the domain owners. While the latter, in principle, have a stronger incentive to care and their request for remediation would trigger the commercial incentive of the operator to keep its customers happy, this communication path turned out to have slightly worse remediation rates. Finally, we ﬁnd no evidence that vulnerability demonstrations did better than static messages. In fact, few recipients engaged with the demonstration website.},
	language = {en},
	author = {Cetin, Orcun and Ganan, Carlos and Korczynski, Maciej and van Eeten, Michel},
	pages = {23},
}

@article{pearman_why_nodate,
	title = {Why people (don’t) use password managers effectively},
	abstract = {Security experts often recommend using passwordmanagement tools that both store passwords and generate random passwords. However, research indicates that only a small fraction of users use password managers with password generators. Past studies have explored factors in the adoption of password managers using surveys and online store reviews. Here we describe a semi-structured interview study with 30 participants that allows us to provide a more comprehensive picture of the mindsets underlying adoption and effective use of password managers and password-generation features. Our participants include users who use no password-speciﬁc tools at all, those who use password managers built into browsers or operating systems, and those who use separately installed password managers. Furthermore, past ﬁeld data has indicated that users of built-in, browser-based password managers more often use weak and reused passwords than users of separate password managers that have password generation available by default. Our interviews suggest that users of built-in password managers may be driven more by convenience, while users of separately installed tools appear more driven by security. We advocate tailored designs for these two mentalities and provide actionable suggestions to induce effective password manager usage.},
	language = {en},
	author = {Pearman, Sarah and Zhang, Shikun Aerin and Bauer, Lujo and Christin, Nicolas and Cranor, Lorrie Faith},
	pages = {21},
}

@article{ray_why_nodate,
	title = {Why {Older} {Adults} ({Don}’t) {Use} {Password} {Managers}},
	abstract = {Password managers (PMs) are considered highly effective tools for increasing security, and a recent study by Pearman et al. (SOUPS’19) highlighted the motivations and barriers to adopting PMs. We expand these ﬁndings by replicating Pearman et al.’s protocol and interview instrument applied to a sample of strictly older adults ({\textgreater}60 years of age), as the prior work focused on a predominantly younger cohort. We conducted n = 26 semi-structured interviews with PM users, built-in browser/operating system PM users, and nonPM users. The average participant age was 70.4 years. Using the same codebook from Pearman et al., we showcase differences and similarities in PM adoption between the samples, including fears of a single point of failure and the importance of having control over one’s private information. Meanwhile, older adults were found to have higher mistrust of cloud storage of passwords and cross-device synchronization. We also highlight PM adoption motivators for older adults, including the power of recommendations from family members and the importance of education and outreach to improve familiarity.},
	language = {en},
	author = {Ray, Hirak and Wolf, Flynn and Kuber, Ravi and Aviv, Adam J},
	pages = {18},
}

@inproceedings{tiefenau_usability_2019,
	address = {London United Kingdom},
	title = {A {Usability} {Evaluation} of {Let}'s {Encrypt} and {Certbot}: {Usable} {Security} {Done} {Right}},
	isbn = {978-1-4503-6747-9},
	shorttitle = {A {Usability} {Evaluation} of {Let}'s {Encrypt} and {Certbot}},
	url = {https://dl.acm.org/doi/10.1145/3319535.3363220},
	doi = {10.1145/3319535.3363220},
	abstract = {The correct configuration of HTTPS is a complex set of tasks, which many administrators have struggled with in the past. Let’s Encrypt and Electronic Frontier Foundation’s Certbot aim to improve the TLS ecosystem by offering free trusted certificates (Let’s Encrypt) and by providing user-friendly support to configure and harden TLS (Certbot). Although adoption rates have increased, to date, there has been only a little scientific evidence of the actual usability and security benefits of this semi-automated approach. Therefore, we conducted a randomized control trial to evaluate the usability of Let’s Encrypt and Certbot in comparison to the traditional certificate authority approach. We performed a within-subjects lab study with 31 participants. The study sheds light on the security and usability enhancements that Let’s Encrypt and Certbot provide. We highlight how usability improvements aimed at administrators can have a large impact on security and discuss takeaways for Certbot and other security-related tasks that experts struggle with.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Tiefenau, Christian and von Zezschwitz, Emanuel and Häring, Maximilian and Krombholz, Katharina and Smith, Matthew},
	month = nov,
	year = {2019},
	pages = {1971--1988},
}

@inproceedings{bock_geneva_2019,
	address = {London United Kingdom},
	title = {Geneva: {Evolving} {Censorship} {Evasion} {Strategies}},
	isbn = {978-1-4503-6747-9},
	shorttitle = {Geneva},
	url = {https://dl.acm.org/doi/10.1145/3319535.3363189},
	doi = {10.1145/3319535.3363189},
	abstract = {Researchers and censoring regimes have long engaged in a cat-andmouse game, leading to increasingly sophisticated Internet-scale censorship techniques and methods to evade them. In this paper, we take a drastic departure from the previously manual evadedetect cycle by developing techniques to automate the discovery of censorship evasion strategies. We present Geneva, a novel genetic algorithm that evolves packet-manipulation-based censorship evasion strategies against nation-state level censors. Geneva composes, mutates, and evolves sophisticated strategies out of four basic packet manipulation primitives (drop, tamper headers, duplicate, and fragment). With experiments performed both in-lab and against several real censors (in China, India, and Kazakhstan), we demonstrate that Geneva is able to quickly and independently re-derive most strategies from prior work, and derive novel subspecies and altogether new species of packet manipulation strategies. Moreover, Geneva discovers successful strategies that prior work posited were not effective, and evolves extinct strategies into newly working variants. We analyze the novel strategies Geneva creates to infer previously unknown behavior in censors. Geneva is a first step towards automating censorship evasion; to this end, we have made our code and data publicly available.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Bock, Kevin and Hughey, George and Qiang, Xiao and Levin, Dave},
	month = nov,
	year = {2019},
	pages = {2199--2214},
}

@article{erlenhov_empirical_2020,
	title = {An {Empirical} {Study} of {Bots} in {Software} {Development} -- {Characteristics} and {Challenges} from a {Practitioner}'s {Perspective}},
	url = {http://arxiv.org/abs/2005.13969},
	doi = {10.1145/3368089.3409680},
	abstract = {Software engineering bots – automated tools that handle tedious tasks – are increasingly used by industrial and open source projects to improve developer productivity. Current research in this area is held back by a lack of consensus of what software engineering bots (DevBots) actually are, what characteristics distinguish them from other tools, and what benefits and challenges are associated with DevBot usage. In this paper we report on a mixed-method empirical study of DevBot usage in industrial practice. We report on findings from interviewing 21 and surveying a total of 111 developers. We identify three different personas among DevBot users (focusing on autonomy, chat interfaces, and “smartness”), each with different definitions of what a DevBot is, why developers use them, and what they struggle with. We conclude that future DevBot research should situate their work within our framework, to clearly identify what type of bot the work targets, and what advantages practitioners can expect. Further, we find that there currently is a lack of generalpurpose “smart” bots that go beyond simple automation tools or chat interfaces. This is problematic, as we have seen that such bots, if available, can have a transformative effect on the projects that use them.},
	language = {en},
	urldate = {2021-08-24},
	journal = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	author = {Erlenhov, Linda and Neto, Francisco Gomes de Oliveira and Leitner, Philipp},
	month = nov,
	year = {2020},
	note = {arXiv: 2005.13969},
	keywords = {Computer Science - Software Engineering},
	pages = {445--455},
}

@article{utz_informed_2019,
	title = {({Un})informed {Consent}: {Studying} {GDPR} {Consent} {Notices} in the {Field}},
	shorttitle = {({Un})informed {Consent}},
	url = {http://arxiv.org/abs/1909.02638},
	doi = {10.1145/3319535.3354212},
	abstract = {Since the adoption of the General Data Protection Regulation (GDPR) in May 2018 more than 60 \% of popular websites in Europe display cookie consent notices to their visitors. This has quickly led to users becoming fatigued with privacy notifications and contributed to the rise of both browser extensions that block these banners and demands for a solution that bundles consent across multiple websites or in the browser. In this work, we identify common properties of the graphical user interface of consent notices and conduct three experiments with more than 80,000 unique users on a German website to investigate the influence of notice position, type of choice, and content framing on consent. We find that users are more likely to interact with a notice shown in the lower (left) part of the screen. Given a binary choice, more users are willing to accept tracking compared to mechanisms that require them to allow cookie use for each category or company individually. We also show that the widespread practice of nudging has a large effect on the choices users make. Our experiments show that seemingly small implementation decisions can substantially impact whether and how people interact with consent notices. Our findings demonstrate the importance for regulation to not just require consent, but also provide clear requirements or guidance for how this consent has to be obtained in order to ensure that users can make free and informed choices.},
	language = {en},
	urldate = {2021-08-24},
	journal = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
	author = {Utz, Christine and Degeling, Martin and Fahl, Sascha and Schaub, Florian and Holz, Thorsten},
	month = nov,
	year = {2019},
	note = {arXiv: 1909.02638},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	pages = {973--990},
}

@article{maass_effective_nodate,
	title = {Effective {Notiﬁcation} {Campaigns} on the {Web}: {A} {Matter} of {Trust}, {Framing}, and {Support}},
	abstract = {Misconﬁgurations and outdated software are a major cause of compromised websites and data leaks. Past research has proposed and evaluated sending automated security notiﬁcations to the operators of misconﬁgured websites, but encountered issues with reachability, mistrust, and a perceived lack of importance. In this paper, we seek to understand the determinants of effective notiﬁcations. We identify a data protection misconﬁguration that affects 12.7 \% of the 1.3 million websites we scanned and opens them up to legal liability. Using a subset of 4754 websites, we conduct a multivariate randomized controlled notiﬁcation experiment, evaluating contact medium, sender, and framing of the message. We also include a link to a public web-based self-service tool that is run by us in disguise and conduct an anonymous survey of the notiﬁed website owners (N=477) to understand their perspective.},
	language = {en},
	author = {Maass, Max and Stöver, Alina and Pridöhl, Henning and Bretthauer, Sebastian and Herrmann, Dominik and Hollick, Matthias and Spiecker, Indra},
	pages = {18},
}

@article{Liu2022MLPrivacy,
	title = {When {Machine} {Learning} {Meets} {Privacy}: {A} {Survey} and {Outlook}},
	abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.},
	journal = {ACM Computing Surveys},
	author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
	year = {2022},
}

@article{kasi_post_2008,
	title = {The post mortem paradox: a {Delphi} study of {IT} specialist perceptions},
	volume = {17},
	issn = {0960-085X, 1476-9344},
	url = {https://www.tandfonline.com/doi/full/10.1057/palgrave.ejis.3000727},
	doi = {10.1057/palgrave.ejis.3000727},
	abstract = {While post mortem evaluation (PME) has long been advocated as a means of improving development practices by learning from IT project failures, few organizations conduct PMEs. The purpose of the study is to explain this discrepancy between theory and practice. This paper integrates findings from a Delphi study of what experienced practitioners perceive as the most important barriers to conducting PMEs with insights from organizational learning theory. The results suggest that there are critical tensions between development practices and learning contexts in many organizations, and adopting PMEs in these cases is likely to reinforce organizational learning dysfunctions rather than improve current development practices. Based on these findings, we argue that the PME literature has underestimated the limits to learning in most IT organizations and we propose to explore paradoxical thinking to help researchers frame continued inquiry into PME and to help managers overcome learning dysfunctions as they push for more widespread use of PMEs.},
	language = {en},
	number = {1},
	urldate = {2022-02-14},
	journal = {European Journal of Information Systems},
	author = {Kasi, Vijay and Keil, Mark and Mathiassen, Lars and Pedersen, Keld},
	month = feb,
	year = {2008},
	keywords = {DA\_NSFGRFP},
	pages = {62--78},
}

@book{petroski_design_1994,
	title = {Design paradigms: {Case} histories of error and judgment in engineering},
	publisher = {Cambridge University Press},
	author = {Petroski, Henry},
	year = {1994},
	keywords = {DA\_NSFGRFP, IoTFailurePaper},
}

@inproceedings{dias_brief_2018,
	address = {Vasteras},
	title = {A {Brief} {Overview} of {Existing} {Tools} for {Testing} the {Internet}-of-{Things}},
	isbn = {978-1-5386-6352-3},
	url = {https://ieeexplore.ieee.org/document/8411738/},
	doi = {10.1109/ICSTW.2018.00035},
	abstract = {Systems are error-prone. Big systems have lots of errors. The Internet-of-Things poses us one of the biggest and widespread systems, where errors directly impact people’s lives. Testing and validating is how one deals with errors; but testing and validating a planetary-scale, heterogeneous, and evergrowing ecosystem has its own challenges and idiosyncrasies. As of today, the solutions available for testing these systems are insufﬁcient and fragmentary. In this paper we provide an overview on test approaches, tools and methodologies for the Internet-of-Things, its software and its devices. Our conclusion is that we are still lagging behind on the best practices and lessons learned from the Software Engineering community in the past decades.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2018 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	publisher = {IEEE},
	author = {Dias, Joao Pedro and Couto, Flavio and Paiva, Ana C.R. and Ferreira, Hugo Sereno},
	month = apr,
	year = {2018},
	keywords = {DA\_NSFGRFP},
	pages = {104--109},
}

@techreport{paing_quertci_2022,
	title = {{QuerTCI}: {A} {Tool} {Integrating} {GitHub} {Issue} {Querying} with {Comment} {Classification}},
	shorttitle = {{QuerTCI}},
	url = {http://arxiv.org/abs/2202.08761},
	abstract = {Issue tracking systems enable users and developers to comment on problems plaguing a software system. Empirical Software Engineering (ESE) researchers study (open-source) project issues and the comments and threads within to discover -- among others -- challenges developers face when, e.g., incorporating new technologies, platforms, and programming language constructs. However, issue discussion threads accumulate over time and thus can become unwieldy, hindering any insight that researchers may gain. While existing approaches alleviate this burden by classifying issue thread comments, there is a gap between searching popular open-source software repositories (e.g., those on GitHub) for issues containing particular keywords and feeding the results into a classification model. In this paper, we demonstrate a research infrastructure tool called QuerTCI that bridges this gap by integrating the GitHub issue comment search API with the classification models found in existing approaches. Using queries, ESE researchers can retrieve GitHub issues containing particular keywords, e.g., those related to a certain programming language construct, and subsequently classify the kinds of discussions occurring in those issues. Using our tool, our hope is that ESE researchers can uncover challenges related to particular technologies using certain keywords through popular open-source repositories more seamlessly than previously possible. A tool demonstration video may be found at: https://youtu.be/fADKSxn0QUk.},
	number = {arXiv:2202.08761},
	urldate = {2022-06-11},
	institution = {arXiv},
	author = {Paing, Ye and Vélez, Tatiana Castro and Khatchadourian, Raffi},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.08761},
	note = {arXiv:2202.08761 [cs]
type: article},
	keywords = {Computer Science - Software Engineering},
}

@misc{leong_consensual_2022,
	title = {Consensual {Software}},
	copyright = {MIT},
	url = {https://github.com/consensualsoftware/consensual_software},
	abstract = {the source code for the consensual software website},
	urldate = {2022-06-11},
	publisher = {Consensual Software},
	author = {Leong, Danielle},
	month = may,
	year = {2022},
	note = {original-date: 2017-05-24T05:33:32Z},
}

@misc{journal_of_online_trust_and_safety_journal_2022,
	title = {Journal of {Online} {Trust} and {Safety}},
	url = {https://tsjournal.org/index.php/jots},
	language = {en-US},
	urldate = {2022-06-07},
	author = {{Journal of Online Trust and Safety}},
	year = {2022},
	keywords = {open access, scholarly publishing, trust and safety},
}

@article{kumari_requirements_2010,
	title = {Requirements {Analysis} for {Privacy} in {Social} {Networks}},
	abstract = {The rise and growth of social networks can be seen as empowering the user to change website’s content by posting information using minimal technical knowledge. But, this empowerment has resulted in loads of sensitive data being let out unprotected in the public domain. To ensure user privacy, we need to understand privacy requirements relevant to social networks, per se. In this paper, we address this problem. We identify all the major stakeholders and their assets. We also look into the various aspects of user data that these stakeholders can be interested in. We then show how interests of various stakeholders can conﬂict and become threats for them. To counter these threats, we present a set of system requirements mapped to the respective privacy requirements.},
	language = {en},
	author = {Kumari, Prachi},
	year = {2010},
	pages = {13},
}

@article{ivaturi_taxonomy_2011,
	title = {A {Taxonomy} for {Social} {Engineering} attacks},
	abstract = {As the technology to secure information improves, hackers will employ less technical means to get access to unauthorized data. The use of Social Engineering as a non tech method of hacking has been increasingly used during the past few years. There are different types of social engineering methods reported but what is lacking is a unifying effort to understand these methods in the aggregate. This paper aims to classify these methods through taxonomy so that organizations can gain a better understanding of these attack methods and accordingly be vigilant against them.},
	language = {en},
	author = {Ivaturi, Koteswara and Janczewski, Lech},
	year = {2011},
	pages = {12},
}

@article{rasmussen1997risk,
	title = {Risk management in a dynamic society: a modelling problem},
	volume = {27},
	number = {2-3},
	journal = {Safety science},
	author = {Rasmussen, Jens},
	year = {1997},
	note = {Publisher: Elsevier},
	pages = {183--213},
}

@article{hudson1994tripod,
	title = {Tripod {Delta}: {Proactive} approach to enhanced safety},
	volume = {46},
	number = {01},
	journal = {Journal of petroleum technology},
	author = {Hudson, PTW and Reason, JT and Bentley, PD and Primrose, M},
	year = {1994},
	note = {Publisher: OnePetro},
	pages = {58--62},
}

@book{reason2016managing,
	title = {Managing the risks of organizational accidents},
	publisher = {Routledge},
	author = {Reason, James},
	year = {2016},
}

@book{leveson2016engineering,
	title = {Engineering a safer world: {Systems} thinking applied to safety},
	publisher = {The MIT Press},
	author = {Leveson, Nancy G},
	year = {2016},
}

@book{norman2013design,
	title = {The design of everyday things: {Revised} and expanded edition},
	publisher = {Basic books},
	author = {Norman, Don},
	year = {2013},
}

@inproceedings{ji_model-reuse_2018,
	address = {Toronto Canada},
	title = {Model-{Reuse} {Attacks} on {Deep} {Learning} {Systems}},
	isbn = {978-1-4503-5693-0},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243757},
	doi = {10.1145/3243734.3243757},
	abstract = {Many of today’s machine learning (ML) systems are built by reusing an array of, often pre-trained, primitive models, each fulfilling distinct functionality (e.g., feature extraction). The increasing use of primitive models significantly simplifies and expedites the development cycles of ML systems. Yet, because most of such models are contributed and maintained by untrusted sources, their lack of standardization or regulation entails profound security implications, about which little is known thus far.},
	language = {en},
	urldate = {2022-06-10},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Ji, Yujie and Zhang, Xinyang and Ji, Shouling and Luo, Xiapu and Wang, Ting},
	month = oct,
	year = {2018},
	pages = {349--363},
}

@techreport{Szegedy2014IntriguingPropertiesofNN,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	number = {arXiv:1312.6199},
	institution = {arXiv},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{zhang_adversarial_2020,
	title = {Adversarial {Attacks} on {Deep}-learning {Models} in {Natural} {Language} {Processing}: {A} {Survey}},
	volume = {11},
	issn = {2157-6904, 2157-6912},
	shorttitle = {Adversarial {Attacks} on {Deep}-learning {Models} in {Natural} {Language} {Processing}},
	url = {https://dl.acm.org/doi/10.1145/3374217},
	doi = {10.1145/3374217},
	abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named
              adversarial examples
              . These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.},
	language = {en},
	number = {3},
	urldate = {2022-06-10},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
	month = jun,
	year = {2020},
	pages = {1--41},
}

@article{sigfridsson_mixing_2008,
	title = {Mixing research methods to unveil work practices of dispersed {Open} {Source} communities: lessons learned from the {PyPy} study},
	shorttitle = {Mixing research methods to unveil work practices of dispersed {Open} {Source} communities},
	abstract = {Open Source software development projects are based on collaboration within dispersed, multifaceted, and volunteer-based communities. Studying the work practices of such a community requires adherence to a plethora of methodological approaches and employment of several different research methods. In this paper we present a study of an Open Source community called PyPy. The focus of the paper is on the evolution of the study itself and how we have utilized several different research methods - including participant observation, virtual ethnography, and electronic questionnaires - to unveil the work practices of this community. We will conclude by discussing the issues we have experienced whilst doing this and some relevant lessons learned regarding studying Open Source communities.},
	author = {Sigfridsson, Anders and Sheehan, Anne and Avram, Gabriela},
	month = jan,
	year = {2008},
}

@article{Hu2021BlockChainBasedPubliceEcosystemforAuditingSecurityofSWApplications,
	title = {Blockchain-based public ecosystem for auditing security of software applications},
	journal = {Computing},
	author = {Hu, Qinwen and Asghar, Muhammad Rizwan and Zeadally, Sherali},
	year = {2021},
}

@techreport{Taylor2020SpellBound,
	title = {{SpellBound}: {Defending} {Against} {Package} {Typosquatting}},
	url = {http://arxiv.org/abs/2003.03471},
	abstract = {Package managers for software repositories based on a single programming language are very common. Examples include npm (JavaScript), and PyPI (Python). These tools encourage code reuse, making it trivial for developers to import external packages. Unfortunately, repositories' size and the ease with which packages can be published facilitates the practice of typosquatting: the uploading of a package with name similar to that of a highly popular package, typically with the aim of capturing some of the popular package's installs. Typosquatting has serious negative implications, resulting in developers importing malicious packages, or -- as we show -- code clones which do not incorporate recent security updates. In order to tackle this problem, we present SpellBound, a tool for identifying and reporting potentially erroneous imports to developers. SpellBound implements a novel typosquatting detection technique, based on an in-depth analysis of npm and PyPI. Our technique leverages a model of lexical similarity between names, and further incorporates the notion of package popularity. This approach flags cases where unknown/scarcely used packages would be installed in place of popular ones with similar names, before installation occurs. We evaluated SpellBound on both npm and PyPI, with encouraging results: SpellBound flags typosquatting cases while generating limited warnings (0.5\% of total package installs), and low overhead (only 2.5\% of package install time). Furthermore, SpellBound allowed us to confirm known cases of typosquatting and discover one high-profile, unknown case of typosquatting that resulted in a package takedown by the npm security team.},
	number = {arXiv:2003.03471},
	institution = {arXiv},
	author = {Taylor, Matthew and Vaidya, Ruturaj K. and Davidson, Drew and De Carli, Lorenzo and Rastogi, Vaibhav},
	year = {2020},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{combe_docker_2016,
	title = {To {Docker} or {Not} to {Docker}: {A} {Security} {Perspective}},
	volume = {3},
	issn = {2325-6095},
	shorttitle = {To {Docker} or {Not} to {Docker}},
	doi = {10.1109/MCC.2016.100},
	abstract = {The need for ever-shorter development cycles, continuous delivery, and cost savings in cloud-based infrastructures led to the rise of containers, which are more flexible than virtual machines and provide near-native performance. Among all container solutions, Docker, a complete packaging and software delivery tool, currently leads the market. This article gives an overview of the container ecosystem and discusses the Docker environment's security implications through realistic use cases. The authors define an adversary model, point out several vulnerabilities affecting current Docker usage, and discuss further research directions.},
	number = {5},
	journal = {IEEE Cloud Computing},
	author = {Combe, Theo and Martin, Antony and Di Pietro, Roberto},
	month = sep,
	year = {2016},
	note = {Conference Name: IEEE Cloud Computing},
	keywords = {Cloud computing, Computer security, Containers, Cost benefit analysis, Docker, Linux, Product life cycle management, Virtual networks, cloud computing, containers, security, virtualization},
	pages = {54--62},
}

@inproceedings{nadeem_automatic_2021,
	title = {Automatic {Issue} {Classifier}: {A} {Transfer} {Learning} {Framework} for {Classifying} {Issue} {Reports}},
	shorttitle = {Automatic {Issue} {Classifier}},
	url = {http://arxiv.org/abs/2202.06149},
	doi = {10.1109/ISSREW53611.2021.00113},
	abstract = {Issue tracking systems are used in the software industry for the facilitation of maintenance activities that keep the software robust and up to date with ever-changing industry requirements. Usually, users report issues that can be categorized into different labels such as bug reports, enhancement requests, and questions related to the software. Most of the issue tracking systems make the labelling of these issue reports optional for the issue submitter, which leads to a large number of unlabeled issue reports. In this paper, we present a state-of-the-art method to classify the issue reports into their respective categories i.e. bug, enhancement, and question. This is a challenging task because of the common use of informal language in the issue reports. Existing studies use traditional natural language processing approaches adopting key-word based features, which fail to incorporate the contextual relationship between words and therefore result in a high rate of false positives and false negatives. Moreover, previous works utilize a uni-label approach to classify the issue reports however, in reality, an issue-submitter can tag one issue report with more than one label at a time. This paper presents our approach to classify the issue reports in a multi-label setting. We use an off-the-shelf neural network called RoBERTa and fine-tune it to classify the issue reports. We validate our approach on issue reports belonging to numerous industrial projects from GitHub. We were able to achieve promising F-1 scores of 81\%, 74\%, and 80\% for bug reports, enhancements, and questions, respectively. We also develop an industry tool called Automatic Issue Classifier (AIC), which automatically assigns labels to newly reported issues on GitHub repositories with high accuracy.},
	urldate = {2022-06-09},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Nadeem, Anas and Sarwar, Muhammad Usman and Malik, Muhammad Zubair},
	month = oct,
	year = {2021},
	note = {arXiv:2202.06149 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	pages = {421--426},
}

@article{heck_framework_2017,
	title = {A framework for quality assessment of just-in-time requirements: the case of open source feature requests},
	volume = {22},
	issn = {1432-010X},
	shorttitle = {A framework for quality assessment of just-in-time requirements},
	url = {https://doi.org/10.1007/s00766-016-0247-5},
	doi = {10.1007/s00766-016-0247-5},
	abstract = {Until now, quality assessment of requirements has focused on traditional up-front requirements. Contrasting these traditional requirements are just-in-time (JIT) requirements, which are by definition incomplete, not specific and might be ambiguous when initially specified, indicating a different notion of "correctness." We analyze how the assessment of JIT requirements quality should be performed based on the literature of traditional and JIT requirements. Based on that analysis, we have designed a quality framework for JIT requirements and instantiated it for feature requests in open source projects. We also indicate how the framework can be instantiated for other types of JIT requirements. We have performed an initial evaluation of our framework for feature requests with eight practitioners from the Dutch agile community, receiving overall positive feedback. Subsequently, we have used our framework to assess 550 feature requests originating from three Open Source Software systems (Netbeans, ArgoUML and Mylyn Tasks). In doing so, we obtain a view on the feature request quality for the three open source projects. The value of our framework is threefold: (1) it gives an overview of quality criteria that are applicable to feature requests (at creation time or JIT); (2) it serves as a structured basis for teams that need to assess the quality of their JIT requirements; and (3) it provides a way to get an insight into the quality of JIT requirements in existing projects.},
	language = {en},
	number = {4},
	urldate = {2022-06-09},
	journal = {Requirements Engineering},
	author = {Heck, Petra and Zaidman, Andy},
	month = nov,
	year = {2017},
	keywords = {Feature request, Just-in-time requirement, Open source, Quality assessment, Quality framework},
	pages = {453--473},
}

@inproceedings{razavian_cnn_2014,
	title = {{CNN} {Features} {Off}-the-{Shelf}: {An} {Astounding} {Baseline} for {Recognition}},
	shorttitle = {{CNN} {Features} {Off}-the-{Shelf}},
	doi = {10.1109/CVPRW.2014.131},
	abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
	month = jun,
	year = {2014},
	note = {ISSN: 2160-7516},
	keywords = {Birds, Feature extraction, Image recognition, Support vector machines, Training, Vectors, Visualization},
	pages = {512--519},
}

@article{ralph_sensemaking-coevolution-implementation_2015,
	series = {Towards general theories of software engineering},
	title = {The {Sensemaking}-{Coevolution}-{Implementation} {Theory} of software design},
	volume = {101},
	issn = {0167-6423},
	url = {https://www.sciencedirect.com/science/article/pii/S0167642314005395},
	doi = {10.1016/j.scico.2014.11.007},
	abstract = {Following calls for greater theory development in software engineering, this paper formulates a process theory of software development practice. Sensemaking-Coevolution-Implementation Theory explains how complex software systems are created by cohesive software development teams in organizations. It posits that an independent agent (the development team) creates a software system by alternating between three categories of activities: making sense of an ambiguous context, mutually refining schemas of the context and design space, and manifesting their understanding of the design space in a technological artifact. This theory development paper defines, illustrates and conceptually evaluates Sensemaking-Coevolution-Implementation Theory. It grounds the theory's concepts and relationships in existing software engineering, information systems development and interdisciplinary design literature.},
	language = {en},
	urldate = {2022-06-09},
	journal = {Science of Computer Programming},
	author = {Ralph, Paul},
	month = apr,
	year = {2015},
	keywords = {Coevolution, Design, General theory, Process theory, Theory development},
	pages = {21--41},
}

@techreport{Kurita2020WeightPoisoningPTM,
	title = {Weight {Poisoning} {Attacks} on {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2004.06660},
	abstract = {Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/neulab/RIPPLe.},
	institution = {arXiv},
	author = {Kurita, Keita and Michel, Paul and Neubig, Graham},
	year = {2020},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bi_first_2021,
	title = {A {First} {Look} at {Accessibility} {Issues} in {Popular} {GitHub} {Projects}},
	doi = {10.1109/ICSME52107.2021.00041},
	abstract = {Accessibility design elements allow people to access software products and services independent of their different abilities. However, accessibility is challenging to handle and whether accessibility is widely considered in software projects is unclear. In this work, we aim to understand if accessibility is a prevalent consideration in practice, what accessibility issues are discussed in GitHub projects, what potential reasons cause accessibility issues, and what solutions (e.g., tools and standards) are applied for addressing accessibility issues. In this work, we collect 11,820 accessibility issues and their threads discussed by developers in popular GitHub projects. We manually analyzed and grouped the collected accessibility issues into seven categories. The results of our study uncover that accessibility is widely discussed in general projects, and the potential reasons that cause accessibility issues are because developers are not aware of the importance of accessibility and they lack knowledge about accessibility concerns, standards, and existing tools. Our results and findings can enhance and improve developers' knowledge and awareness when they conduct accessibility-relevant design or incorporate accessibility elements into their projects.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Bi, Tingting and Xia, Xin and Lo, David and Aleti, Aldeida},
	month = sep,
	year = {2021},
	note = {ISSN: 2576-3148},
	keywords = {Accessibility Issues, Conferences, Empirical Study, Mining Repository, Software development management, Software maintenance, Standards, Tools},
	pages = {390--401},
}

@inproceedings{knauss_vissuelizer_2013,
	title = {V:{Issue}:lizer: {Exploring} requirements clarification in online communication over time},
	shorttitle = {V},
	doi = {10.1109/ICSE.2013.6606709},
	abstract = {This demo introduces V:ISSUE:LIZER, a tool for exploring online communication and analyzing clarification of requirements over time. V:Issue:lizer supports managers and developers to identify requirements with insufficient shared understanding, to analyze communication problems, and to identify developers that are knowledgeable about domain or project related issues through visualizations. Our preliminary evaluation shows that V:Issue:lizer offers managers valuable information for their decision making. (Demo video: http://youtu.be/Oy3xvzjy3BQ).},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Knauss, Eric and Damian, Daniela},
	month = may,
	year = {2013},
	note = {ISSN: 1558-1225},
	keywords = {Collaboration, Data visualization, Knowledge engineering, Social network services, Software, Trajectory, Visualization, communication of requirements, distributed requirements engineering, requirements clarification patterns},
	pages = {1327--1330},
}

@article{biggio_poisoning_nodate,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM’s test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM’s decision function due to malicious input and use this ability to construct malicious data.},
	language = {en},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	pages = {8},
}

@inproceedings{mashiyat_using_2014,
	address = {New York, NY, USA},
	series = {{RSSE} 2014},
	title = {Using developer conversations to resolve uncertainty in software development: a position paper},
	isbn = {978-1-4503-2845-6},
	shorttitle = {Using developer conversations to resolve uncertainty in software development},
	url = {https://doi.org/10.1145/2593822.2593823},
	doi = {10.1145/2593822.2593823},
	abstract = {Software development is a social process: tasks such as implementing a requirement or fixing a bug typically spark conversations between the stakeholders of a software project, where they identify points of uncertainty in the solution space and explore proposals to resolve them. Due to the fluid nature of these interactions, it is hard for project managers to maintain an overall understanding of the state of the discussion and to know when and how to intervene. We propose an approach for extracting the uncertainty information from developer conversations in order to provide managers with analytics. Using these allows us to recommend specific actions that managers can take to better facilitate the resolution of uncertainty.},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Recommendation} {Systems} for {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Mashiyat, Ahmed Shah and Famelis, Michalis and Salay, Rick and Chechik, Marsha},
	month = jun,
	year = {2014},
	keywords = {Uncertainty management, natural language processing},
	pages = {1--5},
}

@article{chen_survey_2016,
	title = {A survey on the use of topic models when mining software repositories},
	volume = {21},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-015-9402-8},
	doi = {10.1007/s10664-015-9402-8},
	abstract = {Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) techniques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineering research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models. We find that i) most studies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task.},
	language = {en},
	number = {5},
	urldate = {2022-06-09},
	journal = {Empirical Software Engineering},
	author = {Chen, Tse-Hsun and Thomas, Stephen W. and Hassan, Ahmed E.},
	month = oct,
	year = {2016},
	keywords = {LDA, LSI, Survey, Topic modeling},
	pages = {1843--1919},
}

@techreport{hosseini_deceiving_2017,
	title = {Deceiving {Google}'s {Cloud} {Video} {Intelligence} {API} {Built} for {Summarizing} {Videos}},
	url = {http://arxiv.org/abs/1703.09793},
	abstract = {Despite the rapid progress of the techniques for image classification, video annotation has remained a challenging task. Automated video annotation would be a breakthrough technology, enabling users to search within the videos. Recently, Google introduced the Cloud Video Intelligence API for video analysis. As per the website, the system can be used to "separate signal from noise, by retrieving relevant information at the video, shot or per frame" level. A demonstration website has been also launched, which allows anyone to select a video for annotation. The API then detects the video labels (objects within the video) as well as shot labels (description of the video events over time). In this paper, we examine the usability of the Google's Cloud Video Intelligence API in adversarial environments. In particular, we investigate whether an adversary can subtly manipulate a video in such a way that the API will return only the adversary-desired labels. For this, we select an image, which is different from the video content, and insert it, periodically and at a very low rate, into the video. We found that if we insert one image every two seconds, the API is deceived into annotating the video as if it only contained the inserted image. Note that the modification to the video is hardly noticeable as, for instance, for a typical frame rate of 25, we insert only one image per 50 video frames. We also found that, by inserting one image per second, all the shot labels returned by the API are related to the inserted image. We perform the experiments on the sample videos provided by the API demonstration website and show that our attack is successful with different videos and images.},
	number = {arXiv:1703.09793},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Hosseini, Hossein and Xiao, Baicen and Poovendran, Radha},
	month = mar,
	year = {2017},
	note = {arXiv:1703.09793 [cs]
version: 2
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{biggio_poisoning_nodate-1,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM’s test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM’s decision function due to malicious input and use this ability to construct malicious data.},
	language = {en},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	pages = {8},
}

@inproceedings{viviani_structure_2018,
	title = {The {Structure} of {Software} {Design} {Discussions}},
	abstract = {As a software system is iteratively developed, software developers engage in many discussions, often through written forms. Some of these discussions occur on pull requests and include information about the design of the system to which the code in the pull request is being contributed. Although previous work has shown that design is discussed in forums like pull requests, little is known about the form and content of the discussion about design. In this paper, we report on an in-depth analysis of three pull requests to better understand the form and content of design information in pull request discussions to enable the development of tools to help humans access this information.},
	booktitle = {2018 {IEEE}/{ACM} 11th {International} {Workshop} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering} ({CHASE})},
	author = {Viviani, Giovanni and Janik-Jones, Calahan and Famelis, Michalis and Murphy, Gail},
	month = may,
	year = {2018},
	note = {ISSN: 2574-1837},
	keywords = {Conferences, Encoding, Rails, Security, Software design, Tools, design, empirical, natural language},
	pages = {104--107},
}

@inproceedings{gopalakrishna_if_2022,
	title = {"{If} security is required'": {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	language = {en},
	author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C.},
	year = {2022},
	pages = {8},
}

@techreport{khalajzadeh_how_2022,
	title = {How are {Diverse} {End}-user {Human}-centric {Issues} {Discussed} on {GitHub}?},
	url = {http://arxiv.org/abs/2201.05927},
	abstract = {Many software systems fail to meet the needs of the diverse end-users in society and are prone to pose problems, such as accessibility and usability issues. Some of these problems (partially) stem from the failure to consider the characteristics, limitations, and abilities of diverse end-users during software development. We refer to this class of problems as human-centric issues. Despite their importance, there is a limited understanding of the types of human-centric issues encountered by developers. In-depth knowledge of these human-centric issues is needed to design software systems that better meet their diverse end-users' needs. This paper aims to provide insights for the software development and research communities on which human-centric issues are a topic of discussion for developers on GitHub. We conducted an empirical study by extracting and manually analysing 1,691 issue comments from 12 diverse projects, ranging from small to large-scale projects, including projects designed for challenged end-users, e.g., visually impaired and dyslexic users. Our analysis shows that eight categories of human-centric issues are discussed by developers. These include Inclusiveness, Privacy \& Security, Compatibility, Location \& Language, Preference, Satisfaction, Emotional Aspects, and Accessibility. Guided by our findings, we highlight some implications and possible future paths to further understand and incorporate human-centric issues in software development to be able to design software that meets the needs of diverse end users in society.},
	number = {arXiv:2201.05927},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Khalajzadeh, Hourieh and Shahin, Mojtaba and Obie, Humphrey O. and Grundy, John},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2201.05927},
	note = {arXiv:2201.05927 [cs]
type: article},
	keywords = {Computer Science - Software Engineering},
}

@article{pfefferkorn_content-oblivious_2022,
	title = {Content-{Oblivious} {Trust} and {Safety} {Techniques}: {Results} from a {Survey} of {Online} {Service} {Providers}},
	volume = {1},
	copyright = {Copyright (c) 2022 Journal of Online Trust and Safety},
	issn = {2770-3142},
	shorttitle = {Content-{Oblivious} {Trust} and {Safety} {Techniques}},
	url = {https://tsjournal.org/index.php/jots/article/view/14},
	doi = {10.54501/jots.v1i2.14},
	abstract = {We present the results of a survey about the trust and safety techniques of a group of online service providers that collectively serve billions of users. We classify techniques that require the provider to be able to access the contents of users’ files and communications at will as content dependent, and content oblivious otherwise. We find that more providers use abuse-reporting features (which are content oblivious) than other abuse-detection techniques, but that participants’ abuse-reporting tools do not consistently cover the types of abuse that users may encounter. We also find that, despite strong consensus among participating providers that automated content scanning (which is content dependent) is the most useful means of detecting child sex abuse imagery, they do not consider it to be nearly as useful for other kinds of abuse. These results indicate that content-dependent techniques do not constitute a silver bullet to protect users against abuse. They also demonstrate that the impact of end-to-end encryption (which, controversially, impedes outside access to user content) on abuse detection may vary by abuse type. These findings have implications for policy debates over the regulation of online service providers’ anti-abuse obligations and their use of end-to-end encryption.},
	language = {en},
	number = {2},
	urldate = {2022-06-07},
	journal = {Journal of Online Trust and Safety},
	author = {Pfefferkorn, Riana},
	month = feb,
	year = {2022},
	note = {Number: 2},
	keywords = {trust and safety},
}

@article{journal_of_online_trust_and_safety_second_2022,
	title = {The {Second} {Issue}},
	volume = {1},
	shorttitle = {Vol. 1 {No}. 2 (2022)},
	url = {https://tsjournal.org/index.php/jots/issue/view/2},
	language = {en-US},
	number = {2},
	urldate = {2022-06-07},
	author = {{Journal of Online Trust and Safety}},
	year = {2022},
	keywords = {open access, scholarly publishing, trust and safety},
}

@article{bentahar_taxonomy_2010,
	title = {A taxonomy of argumentation models used for knowledge representation},
	volume = {33},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-010-9154-1},
	doi = {10.1007/s10462-010-9154-1},
	abstract = {Understanding argumentation and its role in human reasoning has been a continuous subject of investigation for scholars from the ancient Greek philosophers to current researchers in philosophy, logic and artificial intelligence. In recent years, argumentation models have been used in different areas such as knowledge representation, explanation, proof elaboration, commonsense reasoning, logic programming, legal reasoning, decision making, and negotiation. However, these models address quite specific needs and there is need for a conceptual framework that would organize and compare existing argumentation-based models and methods. Such a framework would be very useful especially for researchers and practitioners who want to select appropriate argumentation models or techniques to be incorporated in new software systems with argumentation capabilities. In this paper, we propose such a conceptual framework, based on taxonomy of the most important argumentation models, approaches and systems found in the literature. This framework highlights the similarities and differences between these argumentation models. As an illustration of the practical use of this framework, we present a case study which shows how we used this framework to select and enrich an argumentation model in a knowledge acquisition project which aimed at representing argumentative knowledge contained in texts critiquing military courses of action.},
	language = {en},
	number = {3},
	urldate = {2022-06-07},
	journal = {Artificial Intelligence Review},
	author = {Bentahar, Jamal and Moulin, Bernard and Bélanger, Micheline},
	month = mar,
	year = {2010},
	keywords = {Argumentation models, Argumentation theory, Courses of action, Knowledge representation},
	pages = {211--259},
}

@inproceedings{veselsky_establishing_2022,
	title = {Establishing {Trust} in {Vehicle}-to-{Vehicle} {Coordination}: {A} {Sensor} {Fusion} {Approach}},
	abstract = {Autonomous vehicles (AVs) use diverse sensors to understand their surroundings as they continually make safetycritical decisions. However, establishing trust with other AVs is a key prerequisite because safety-critical decisions cannot be made based on data shared from untrusted sources. Existing protocols require an infrastructure network connection and a third-party root of trust to establish a secure channel, which are not always available.},
	language = {en},
	author = {Veselsky, Jakob and West, Jack and Isaac, Ahlgren and Goel, Abhinav and Jiang, Wenxin and Lee, Kyuin and Kim, Younghyun and Davis, James C. and Thiruvathukal, George and Klingensmith, Neil},
	year = {2022},
	pages = {6},
}

@inproceedings{10.1145/3508396.3517075,
	address = {New York, NY, USA},
	series = {{HotMobile} '22},
	title = {Poster: {Establishing} trust in vehicle-to-vehicle coordination: {A} sensor fusion approach},
	isbn = {978-1-4503-9218-1},
	url = {https://doi.org/10.1145/3508396.3517075},
	doi = {10.1145/3508396.3517075},
	abstract = {As we add more autonomous and semi-autonomous vehicles (AVs) to our roads, their effects on passenger and pedestrian safety are becoming more important. Despite extensive testing before deployment, AV systems are not perfect at identifying hazards in the roadway. Although a particular AV’s sensors and software may not be 100\% accurate at identifying hazards, there is an untapped pool of information held by other AVs in the vicinity that could be used to quickly and accurately identify roadway hazards before they present a safety threat.},
	booktitle = {Proceedings of the 23rd annual international workshop on mobile computing systems and applications ({HotMobile})},
	publisher = {Association for Computing Machinery},
	author = {Veselsky, Jakob and West, Jack and Ahlgren, Isaac and Thiruvathukal, George K. and Klingensmith, Neil and Goel, Abhinav and Jiang, Wenxin and Davis, James C. and Lee, Kyuin and Kim, Younghyun},
	year = {2022},
	note = {Number of pages: 1
Place: Tempe, Arizona},
	pages = {128},
}

@inproceedings{barlas_exploiting_2022,
	title = {Exploiting {Input} {Sanitization} for {Regex} {Denial} of {Service}},
	abstract = {Web services use server-side input sanitization to guard against harmful input. Some web services publish their sanitization logic to make their client interface more usable, e.g., allowing clients to debug invalid requests locally. However, this usability practice poses a security risk. Specifically, services may share the regexes they use to sanitize input strings — and regex-based denial of service (ReDoS) is an emerging threat. Although prominent service outages caused by ReDoS have spurred interest in this topic, we know little about the degree to which live web services are vulnerable to ReDoS.},
	language = {en},
	author = {Barlas, Efe and Du, Xin and Davis, James C},
	year = {2022},
	pages = {13},
}

@inproceedings{xu_empirical_2022,
	title = {An {Empirical} {Study} on the {Impact} of {Deep} {Parameters} on {Mobile} {App} {Energy} {Usage}},
	abstract = {Improving software performance through conﬁguration parameter tuning is a common activity during software maintenance. Beyond traditional performance metrics like latency, mobile app developers are interested in reducing app energy usage. Some mobile apps have centralized locations for parameter tuning, similar to databases and operating systems, but it is common for mobile apps to have hundreds of parameters scattered around the source code. The correlation between these “deep” parameters and app energy usage is unclear. Researchers have studied the energy effects of deep parameters in speciﬁc modules, but we lack a systematic understanding of the energy impact of mobile deep parameters.},
	language = {en},
	author = {Xu, Qiang and Davis, James C and Hu, Y Charlie and Jindal, Abhilash},
	year = {2022},
	pages = {12},
}

@misc{xu_qiang_replication_2022,
	title = {Replication package for paper "{An} {Empirical} {Study} on the {Impact} of {Deep} {Parameters} on {Mobile} {App} {Energy} {Usage}" ({SANER} 2022)},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/5823364},
	abstract = {Improving software performance through conﬁguration parameter tuning is a common activity during software maintenance. Beyond traditional performance metrics like latency, mobile app developers are interested in reducing app energy usage. Some mobile apps have centralized locations for parameter tuning, similar to databases and operating systems, but it is common for mobile apps to have hundreds of parameters scattered around the source code. The correlation between these “deep” parameters and app energy usage is unclear. Researchers have studied the energy effects of deep parameters in speciﬁc modules, but we lack a systematic understanding of the energy impact of mobile deep parameters.},
	urldate = {2022-06-03},
	publisher = {Zenodo},
	author = {Xu, Qiang and Davis, James C. and Hu, Y. Charlie and Jindal, Abhilash},
	month = jan,
	year = {2022},
	doi = {10.5281/ZENODO.5823364},
	note = {Language: en},
	keywords = {Android, configuration, deep parameter, energy consumption, mobile app},
}

@techreport{higuera_software_1996,
	title = {Software {Risk} {Management}.},
	url = {https://apps.dtic.mil/sti/citations/ADA310913},
	abstract = {This paper presents a holistic vision of the risk-based methodologies for Software Risk Management SRM developed at the Software Engineering Institute SEI. SRM methodologies address the entire life cycle of software acquisition, development, and maintenance. This paper is driven by the premise that the ultimate efficacy of the developed methodologies and tools for software engineering is to buy smarter, manage more effectively, identify opportunities for continuous improvement, use available information and databases more efficiently, improve industry, raise the communitys playing field, and review and evaluate progress. The methodologies are based on seven management principles shared product vision, teamwork, global perspective, forward-looking view, open communication, integrated management, and continuous process.},
	language = {en},
	urldate = {2022-06-03},
	institution = {CARNEGIE-MELLON UNIV PITTSBURGH PA SOFTWARE ENGINEERING INST},
	author = {Higuera, Ronald P. and Haimes, Yacov Y.},
	month = jun,
	year = {1996},
	note = {Section: Technical Reports},
}

@inproceedings{cheng_anyone_2017,
	address = {New York, NY, USA},
	series = {{CSCW} '17},
	title = {Anyone {Can} {Become} a {Troll}: {Causes} of {Trolling} {Behavior} in {Online} {Discussions}},
	isbn = {978-1-4503-4335-0},
	shorttitle = {Anyone {Can} {Become} a {Troll}},
	url = {https://doi.org/10.1145/2998181.2998213},
	doi = {10.1145/2998181.2998213},
	abstract = {In online communities, antisocial behavior such as trolling disrupts constructive discussion. While prior work suggests that trolling behavior is confined to a vocal and antisocial minority, we demonstrate that ordinary people can engage in such behavior as well. We propose two primary trigger mechanisms: the individual's mood, and the surrounding context of a discussion (e.g., exposure to prior trolling behavior). Through an experiment simulating an online discussion, we find that both negative mood and seeing troll posts by others significantly increases the probability of a user trolling, and together double this probability. To support and extend these results, we study how these same mechanisms play out in the wild via a data-driven, longitudinal analysis of a large online news discussion community. This analysis exposes temporal mood effects, and explores long range patterns of repeated exposure to trolling. A predictive model of trolling behavior reveals that mood and discussion context together can explain trolling behavior better than an individual's history of trolling. These results combine to suggest that ordinary people can, under the right circumstances, behave like trolls.},
	urldate = {2022-06-02},
	booktitle = {Proceedings of the 2017 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Justin and Bernstein, Michael and Danescu-Niculescu-Mizil, Cristian and Leskovec, Jure},
	month = feb,
	year = {2017},
	keywords = {antisocial behavior, online communities, trolling},
	pages = {1217--1230},
}

@inproceedings{stol2016grounded,
	title = {Grounded theory in software engineering research: a critical review and guidelines},
	booktitle = {Proceedings of the 38th international conference on software engineering},
	author = {Stol, Klaas-Jan and Ralph, Paul and Fitzgerald, Brian},
	year = {2016},
	pages = {120--131},
}

@inproceedings{sung2022settle,
	title = {How to settle the {ReDoS} problem: {Back} to the classical automata theory},
	booktitle = {Implementation and application of automata: 26th international conference, {CIAA} 2022, rouen, france, june 28–{July} 1, 2022, proceedings},
	author = {Sung, Sicheol and Cheon, Hyunjoon and Han, Yo-Sub},
	note = {tex.organization: Springer Nature},
	pages = {34},
}

@misc{moffat_risk_2022,
	title = {The {Risk} {Landscape} {\textbar} {Risk}-{First}},
	url = {https://riskfirst.org//risks/Risk-Landscape},
	abstract = {A way to think about the risks you face on a software project.},
	language = {en},
	urldate = {2022-05-31},
	journal = {Risk-First Software Development},
	author = {Moffat, Rob},
	year = {2022},
}

@article{raval_risk_2017,
	title = {Risk {Landscape} of {Autonomous} {Cars}},
	volume = {56},
	issn = {0736-6981},
	url = {https://doi.org/10.1080/07366981.2017.1355099},
	doi = {10.1080/07366981.2017.1355099},
	abstract = {This article presents a study of how automobile risks are changing over time as the level of automation shifts from the traditional to semi-autonomous to fully autonomous car. The study involves the concept of comparative advantage between the human driver and the driverless car to determine how the roles between the two have been changing and, in turn, how the change affects attendant risks. To develop a risk landscape across changes in levels of automation, the roles of human driver and driverless car are matched with the five levels of automation used to describe the phases of development of the driverless car. We conclude that the costs associated with fatal accidents will decrease sharply over time as the driverless cars are deployed on the road. And we offer implications for policy, regulation, and insurance based on our analysis of the changing risk landscape.},
	number = {3},
	urldate = {2022-05-31},
	journal = {EDPACS},
	author = {Raval, Vasant and Dentlinger, Michael J.},
	month = sep,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/07366981.2017.1355099},
	pages = {1--18},
}

@article{oehmen_risk_2020,
	title = {Risk {Management} in {Product} {Development}: {Risk} {Identification}, {Assessment}, and {Mitigation} - {A} {Literature} {Review}},
	volume = {1},
	issn = {2633-7762},
	shorttitle = {{RISK} {MANAGEMENT} {IN} {PRODUCT} {DEVELOPMENT}},
	url = {https://www.cambridge.org/core/journals/proceedings-of-the-design-society-design-conference/article/risk-management-in-product-development-risk-identification-assessment-and-mitigation-a-literature-review/737BD10A6D55576D5A813BE37D36A22E#},
	doi = {10.1017/dsd.2020.27},
	abstract = {This paper reviews the literature on risk management practices and methods in product design and development. Based on an expert workshop by the Risk Management Processes and Methods in Design Special Interest Group within the Design Society and literature review, three key areas are discussed: risk identification, assessment, and mitigation. In each area, researchers have described practices that are used in product development organizations, proposed new methods to support risk management processes and decision-making, and generated evidence to evaluate the effectiveness of these activities.},
	language = {en},
	urldate = {2022-05-11},
	journal = {Proceedings of the Design Society: DESIGN Conference},
	author = {Oehmen, J. and Guenther, A. and Herrmann, J. W. and Schulte, J. and Willumsen, P.},
	month = may,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	keywords = {design management, literature review, product development, risk management, uncertainty},
	pages = {657--666},
}

@inproceedings{cramer_draft_2023,
	title = {[{DRAFT}] {An} {Empirical} {Study} of {Trust} \& {Safety} {Risks} in {Social} {Media} {Platforms}},
	shorttitle = {T\&{S} {Risks} in {SMPs}},
	author = {Cramer, Geoffrey},
	month = may,
	year = {2023},
}

@inproceedings{hasan_why_2014,
	address = {Doha, Qatar},
	title = {Why are {You} {Taking} this {Stance}? {Identifying} and {Classifying} {Reasons} in {Ideological} {Debates}},
	shorttitle = {Why are {You} {Taking} this {Stance}?},
	url = {https://aclanthology.org/D14-1083},
	doi = {10.3115/v1/D14-1083},
	urldate = {2022-05-27},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hasan, Kazi Saidul and Ng, Vincent},
	month = oct,
	year = {2014},
	pages = {751--762},
}

@inproceedings{boltuzic_identifying_2015,
	address = {Denver, CO},
	title = {Identifying {Prominent} {Arguments} in {Online} {Debates} {Using} {Semantic} {Textual} {Similarity}},
	url = {https://aclanthology.org/W15-0514},
	doi = {10.3115/v1/W15-0514},
	urldate = {2022-05-27},
	booktitle = {Proceedings of the 2nd {Workshop} on {Argumentation} {Mining}},
	publisher = {Association for Computational Linguistics},
	author = {Boltužić, Filip and Šnajder, Jan},
	month = jun,
	year = {2015},
	pages = {110--115},
}

@article{wang_how_2022,
	title = {How {Do} {Open} {Source} {Software} {Contributors} {Perceive} and {Address} {Usability}?: {Valued} {Factors}, {Practices}, and {Challenges}},
	volume = {39},
	issn = {1937-4194},
	shorttitle = {How {Do} {Open} {Source} {Software} {Contributors} {Perceive} and {Address} {Usability}?},
	doi = {10.1109/MS.2020.3009514},
	abstract = {Given the recent changes in the open source software (OSS) landscape, we examined OSS contributors’ current valued factors, practices, and challenges concerning usability. Our survey provides insights for OSS practitioners and tool designers to promote a user-centric mindset and improve usability practice in OSS communities.},
	number = {1},
	journal = {IEEE Software},
	author = {Wang, Wenting and Cheng, Jinghui and Guo, Jin L.C.},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Software},
	keywords = {Graphical user interfaces, Guidelines, Open source software, Usability},
	pages = {76--83},
}

@inproceedings{cheng_how_2018,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '18},
	title = {How {Do} the {Open} {Source} {Communities} {Address} {Usability} and {UX} {Issues}? {An} {Exploratory} {Study}},
	isbn = {978-1-4503-5621-3},
	shorttitle = {How {Do} the {Open} {Source} {Communities} {Address} {Usability} and {UX} {Issues}?},
	url = {https://doi.org/10.1145/3170427.3188467},
	doi = {10.1145/3170427.3188467},
	abstract = {Usability and user experience (UX) issues are often not well emphasized and addressed in open source software (OSS) development. There is an imperative need for supporting OSS communities to collaboratively identify, understand, and fix UX design issues in a distributed environment. In this paper, we provide an initial step towards this effort and report on an exploratory study that investigated how the OSS communities currently reported, discussed, negotiated, and eventually addressed usability and UX issues. We conducted in-depth qualitative analysis of selected issue tracking threads from three OSS projects hosted on GitHub. Our findings indicated that discussions about usability and UX issues in OSS communities were largely influenced by the personal opinions and experiences of the participants. Moreover, the characteristics of the community may have greatly affected the focus of such discussion.},
	urldate = {2022-05-26},
	booktitle = {Extended {Abstracts} of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Jinghui and Guo, Jin L.C.},
	month = apr,
	year = {2018},
	keywords = {issue tracking, open source community, open source software development, usability, user experience},
	pages = {1--6},
}

@inproceedings{hellman_facilitating_2021,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '21},
	title = {Facilitating {Asynchronous} {Participatory} {Design} of {Open} {Source} {Software}: {Bringing} {End} {Users} into the {Loop}},
	isbn = {978-1-4503-8095-9},
	shorttitle = {Facilitating {Asynchronous} {Participatory} {Design} of {Open} {Source} {Software}},
	url = {https://doi.org/10.1145/3411763.3451643},
	doi = {10.1145/3411763.3451643},
	abstract = {As open source software (OSS) becomes increasingly mature and popular, there are significant challenges with properly accounting for usability concerns for the diverse end users. Participatory design, where multiple stakeholders collaborate on iterating the design, can be an efficient way to address the usability concerns for OSS projects. However, barriers such as a code-centric mindset and insufficient tool support often prevent OSS teams from effectively including end users in participatory design methods. This paper proposes preliminary contributions to this problem through the user-centered exploration of (1) a set of design guidelines that capture the needs of OSS participatory design tools, (2) two personas that represent the characteristics of OSS designers and end users, and (3) a low-fidelity prototype tool for end user involvement in OSS projects. This work paves the road for future studies about tool design that would eventually help improve OSS usability.},
	urldate = {2022-05-26},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hellman, Jazlyn and Cheng, Jinghui and Guo, Jin L.C.},
	month = may,
	year = {2021},
	keywords = {asynchronous collaboration, open source software, participatory design, usability},
	pages = {1--7},
}

@inproceedings{gill_privacy_2011,
	address = {New York, NY, USA},
	series = {{CHI} '11},
	title = {Privacy dictionary: a linguistic taxonomy of privacy for content analysis},
	isbn = {978-1-4503-0228-9},
	shorttitle = {Privacy dictionary},
	url = {https://doi.org/10.1145/1978942.1979421},
	doi = {10.1145/1978942.1979421},
	abstract = {Privacy is frequently a key concern relating to technology and central to HCI research, yet it is notoriously difficult to study in a naturalistic way. In this paper we describe and evaluate a dictionary of privacy designed for content analysis, derived using prototype theory and informed by traditional theoretical approaches to privacy. We evaluate our dictionary categories alongside privacy-related categories from an existing content analysis tool, LIWC, using verbal discussions of privacy issues from a variety of technology and non-technology contexts. We find that our privacy dictionary is better able to distinguish between privacy and non-privacy language, and is less context-dependent than LIWC. However, the more general LIWC categories are able to describe a greater amount of variation in our data. We discuss possible improvements to the privacy dictionary and note future work.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gill, Alastair J. and Vasalou, Asimina and Papoutsi, Chrysanthi and Joinson, Adam N.},
	month = may,
	year = {2011},
	keywords = {content analysis, language, privacy, privacy dictionary},
	pages = {3227--3236},
}

@inproceedings{rastkar_summarizing_2010,
	title = {Summarizing software artifacts: a case study of bug reports},
	volume = {1},
	shorttitle = {Summarizing software artifacts},
	doi = {10.1145/1806799.1806872},
	abstract = {Many software artifacts are created, maintained and evolved as part of a software development project. As software developers work on a project, they interact with existing project artifacts, performing such activities as reading previously filed bug reports in search of duplicate reports. These activities often require a developer to peruse a substantial amount of text. In this paper, we investigate whether it is possible to summarize software artifacts automatically and effectively so that developers could consult smaller summaries instead of entire artifacts. To provide focus to our investigation, we consider the generation of summaries for bug reports. We found that existing conversation-based generators can produce better results than random generators and that a generator trained specifically on bug reports can perform statistically better than existing conversation-based generators. We demonstrate that humans also find these generated summaries reasonable indicating that summaries might be used effectively for many tasks.},
	booktitle = {2010 {ACM}/{IEEE} 32nd {International} {Conference} on {Software} {Engineering}},
	author = {Rastkar, Sarah and Murphy, Gail C. and Murray, Gabriel},
	month = may,
	year = {2010},
	note = {ISSN: 1558-1225},
	keywords = {Computer bugs, Electromagnetic compatibility, Electronic mail, Humans, Programming, Software, Training, human-centric software engineering, machine learning},
	pages = {505--514},
}

@inproceedings{ortu_mining_2018,
	address = {New York, NY, USA},
	series = {{PROMISE}'18},
	title = {Mining {Communication} {Patterns} in {Software} {Development}: {A} {GitHub} {Analysis}},
	isbn = {978-1-4503-6593-2},
	shorttitle = {Mining {Communication} {Patterns} in {Software} {Development}},
	url = {https://doi.org/10.1145/3273934.3273943},
	doi = {10.1145/3273934.3273943},
	abstract = {Background: Studies related to human factors in software engineering are providing insightful information on the emotional state of contributors and the impact this has on the code. The open source software development paradigm involves different roles, and previous studies about emotions in software development have not taken into account what different roles might play when people express their feelings. Aim: We present an analysis of issues and commits on five GitHub projects distinguishing contributors between users and developers, and between one-commit and multi-commit developers. Method: We analyzed more than 650K comments from 130K issues of 64K contributors. We calculated emotions (love, joy, anger, sadness) and politeness of the comments related to the issues of the considered projects and introduced the definition of contributor fan-in and fan-out. Results: Results show that users and developers communicate differently as well as multi-commit developers and one-commit developers do. Conclusions: We provide empirical evidence that one-commit developers are more active and more polite in posting comments. Multi-commit developers are less active in posting comments, and while commenting, they are less polite than when commented.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Predictive} {Models} and {Data} {Analytics} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ortu, Marco and Hall, Tracy and Marchesi, Michele and Tonelli, Roberto and Bowes, David and Destefanis, Giuseppe},
	month = oct,
	year = {2018},
	keywords = {data analytics, human factors, software engineering},
	pages = {70--79},
}

@inproceedings{ding_entity-level_2018,
	address = {New York, NY, USA},
	series = {{SEmotion} '18},
	title = {Entity-level sentiment analysis of issue comments},
	isbn = {978-1-4503-5751-7},
	url = {https://doi.org/10.1145/3194932.3194935},
	doi = {10.1145/3194932.3194935},
	abstract = {Emotions and sentiment of software developers can largely influence the software productivity and quality. However, existing work on emotion mining and sentiment analysis is still in the early stage in software engineering in terms of accuracy, the size of datasets used and the specificity of the analysis. In this work, we are concerned with conducting entity-level sentiment analysis. We first build a manually labeled dataset containing 3,000 issue comments selected from 231,732 issue comments collected from 10 open source projects in GitHub. Then we design and develop SentiSW, an entity-level sentiment analysis tool consisting of sentiment classification and entity recognition, which can classify issue comments into {\textless}sentiment, entity{\textgreater} tuples. We evaluate the sentiment classification using ten-fold cross validation, and it achieves 68.71\% mean precision, 63.98\% mean recall and 77.19\% accuracy, which is significantly higher than existing tools. We evaluate the entity recognition by manually annotation and it achieves a 75.15\% accuracy.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Emotion} {Awareness} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Jin and Sun, Hailong and Wang, Xu and Liu, Xudong},
	month = jun,
	year = {2018},
	keywords = {entity recognition, entity-level sentiment analysis, open source software project, sentiment classification},
	pages = {7--13},
}

@inproceedings{arya_analysis_2019,
	address = {Montreal, Quebec, Canada},
	series = {{ICSE} '19},
	title = {Analysis and detection of information types of open source software issue discussions},
	url = {https://doi.org/10.1109/ICSE.2019.00058},
	doi = {10.1109/ICSE.2019.00058},
	abstract = {Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Arya, Deeksha and Wang, Wenting and Guo, Jin L. C. and Cheng, Jinghui},
	month = may,
	year = {2019},
	keywords = {collaborative software engineering, issue discussion analysis, issue tracking system},
	pages = {454--464},
}

@inproceedings{destefanis_measuring_2018,
	address = {New York, NY, USA},
	series = {{SEmotion} '18},
	title = {On measuring affects of github issues' commenters},
	isbn = {978-1-4503-5751-7},
	url = {https://doi.org/10.1145/3194932.3194936},
	doi = {10.1145/3194932.3194936},
	abstract = {In this study, we analyzed issues and comments on GitHub projects and built collaboration networks dividing contributors into two categories: users and commenters. We identified as commenters those users who only post comments without posting any issues nor committing changes in the source code. Since previous studies showed that there is a link between a positive environment (regarding affectiveness) and productivity, our goal was to investigate commenters' contribution to the project concerning affectiveness. We analyzed more than 370K comments from 100K issues of 25K contributors from 3 open source projects. We then calculated and compared the affectiveness of the issues' comments written by users and commenters in terms of sentiment, politeness, and emotions. We provide empirical evidence that commenters are less polite, less positive and in general they express a lower level of emotions in their comments than users. Our results also confirm that GitHub's contributors consist of different groups which behave differently, and this provides useful information for future studies in the field.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Emotion} {Awareness} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Destefanis, Giuseppe and Ortu, Marco and Bowes, David and Marchesi, Michele and Tonelli, Roberto},
	month = jun,
	year = {2018},
	keywords = {human factors, mining software repositories, software engineering},
	pages = {14--19},
}

@inproceedings{imtiaz_sentiment_2018,
	address = {New York, NY, USA},
	series = {{SEmotion} '18},
	title = {Sentiment and politeness analysis tools on developer discussions are unreliable, but so are people},
	isbn = {978-1-4503-5751-7},
	url = {https://doi.org/10.1145/3194932.3194938},
	doi = {10.1145/3194932.3194938},
	abstract = {Many software engineering researchers use sentiment and politeness analysis tools to study the emotional environment within collaborative software development. However, papers that use these tools rarely establish their reliability. In this paper, we evaluate popular existing tools for sentiment and politeness detection over a dataset of 589 manually rated GitHub comments that represent developer discussions. We also develop a coding scheme on how to quantify politeness for conversational texts found on collaborative platforms. We find that not only do the tools have a low agreement with human ratings on sentiment and politeness, human raters also have a low agreement among themselves.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Emotion} {Awareness} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Imtiaz, Nasif and Middleton, Justin and Girouard, Peter and Murphy-Hill, Emerson},
	month = jun,
	year = {2018},
	keywords = {affect analysis, developer discussion, github, politeness, sentiment},
	pages = {55--61},
}

@inproceedings{ortu_empirical_2019,
	title = {Empirical {Analysis} of {Affect} of {Merged} {Issues} on {GitHub}},
	doi = {10.1109/SEmotion.2019.00017},
	abstract = {Pull-request based workflows are popular trends of modern software development platform such as GitHub. A pull-request notifies other developers that new changes are proposed, a code review process follows the pull-request that may be merged in the main branch if other developers accept the changes. Many factors influence the acceptance of pull-requests. Since open source software is based on collaboration, it is essential to discover how the affect expressed by developer discussing pull-request issues, namely how they collaborate, influences the acceptance of the pull-request proposed. In this study we analysed the relations with the affect expressed in pull-request issues'comments and whether an issue is merged in the main branch or not. We focused on pull-request issues and we found that issues with higher level anger, sadness, arousal and valence are less likely to be merged while issues with higher level of valence, joy are more likely to be merged. Positive affect indicates a good collaboration environment, and our finding shows that this healthy collaboration is likely to increase the acceptance of pull-requests.},
	booktitle = {2019 {IEEE}/{ACM} 4th {International} {Workshop} on {Emotion} {Awareness} in {Software} {Engineering} ({SEmotion})},
	author = {Ortu, Marco and Marchesi, Michele and Tonelli, Roberto},
	month = may,
	year = {2019},
	keywords = {Collaboration, Conferences, Data mining, Open source software, Productivity, Software engineering, human aspect, software engineering},
	pages = {46--48},
}

@inproceedings{cramer_draft_2023-1,
	title = {[{DRAFT}] {An} {Empirical} {Study} of {Trust} \& {Safety} {Risks} in {Social} {Media} {Platforms}},
	shorttitle = {T\&{S} {Risks} in {SMPs}},
	author = {Cramer, Geoffrey},
	month = may,
	year = {2023},
}

@misc{wikipedia_plan--check-act_2022,
	title = {Plan-do-check-act},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=PDCA&oldid=1089529583},
	abstract = {PDCA (plan–do–check–act or plan–do–check–adjust) is an iterative design and management method used in business for the control and continual improvement of processes and products. It is also known as the Deming circle/cycle/wheel, the Shewhart cycle, the control circle/cycle, or plan–do–study–act (PDSA). Another version of this PDCA cycle is OPDCA. The added "O" stands for observation or as some versions say: "Observe the current condition." This emphasis on observation and current condition has currency with the literature on lean manufacturing and the Toyota Production System. The PDCA cycle, with Ishikawa's changes, can be traced back to S. Mizuno of the Tokyo Institute of Technology in 1959.},
	language = {en},
	urldate = {2022-05-24},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = may,
	year = {2022},
	note = {Page Version ID: 1089529583},
}

@inproceedings{tsay_influence_2014,
	address = {New York, NY, USA},
	series = {{ICSE} 2014},
	title = {Influence of social and technical factors for evaluating contribution in {GitHub}},
	isbn = {978-1-4503-2756-5},
	url = {https://doi.org/10.1145/2568225.2568315},
	doi = {10.1145/2568225.2568315},
	abstract = {Open source software is commonly portrayed as a meritocracy, where decisions are based solely on their technical merit. However, literature on open source suggests a complex social structure underlying the meritocracy. Social work environments such as GitHub make the relationships between users and between users and work artifacts transparent. This transparency enables developers to better use information such as technical value and social connections when making work decisions. We present a study on open source software contribution in GitHub that focuses on the task of evaluating pull requests, which are one of the primary methods for contributing code in GitHub. We analyzed the association of various technical and social measures with the likelihood of contribution acceptance. We found that project managers made use of information signaling both good technical contribution practices for a pull request and the strength of the social connection between the submitter and project manager when evaluating pull requests. Pull requests with many comments were much less likely to be accepted, moderated by the submitter's prior interaction in the project. Well-established projects were more conservative in accepting pull requests. These findings provide evidence that developers use both technical and social information when evaluating potential contributions to open source software projects.},
	urldate = {2022-05-24},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Tsay, Jason and Dabbish, Laura and Herbsleb, James},
	month = may,
	year = {2014},
	keywords = {GitHub, contribution, open source, signaling theory, social computing, social media, transparency},
	pages = {356--366},
}

@article{stromer-galley_measuring_2020,
	title = {Measuring {Deliberation}’s {Content}: {A} {Coding} {Scheme}},
	volume = {3},
	issn = {2634-0488},
	shorttitle = {Measuring {Deliberation}’s {Content}},
	url = {https://delibdemjournal.org/article/id/331/},
	doi = {10.16997/jdd.50},
	abstract = {This paper details a content analysis scheme to measure the quality of political deliberation in face-to-face and online groups. Much of deliberation research studies the outcomes of deliberation, but there has been a lack of analysis of what groups actually do when tasked with deliberating. The coding scheme was developed out of the theoretical literature on deliberation and further enhanced by the empirical literature on small groups, deliberation, online political talk, and conversation analysis. Strict standards for creating coding schemes were followed to ensure a valid and reliable coding process. Results of the coding of deliberations on the topic of public schools suggest that participants produced a fairly high level of reasoned opinion expression, but not necessarily on the topic which they were asked to deliberate. It is hoped that the code scheme can be utilized by practitioners and researchers of political and social deliberations.},
	language = {en},
	number = {1},
	urldate = {2022-05-24},
	journal = {Journal of Deliberative Democracy},
	author = {Stromer-Galley, Jennifer},
	month = may,
	year = {2020},
	note = {Number: 1
Publisher: University of Westminster Press},
}

@article{giles_microanalysis_2015,
	title = {Microanalysis {Of} {Online} {Data}: {The} methodological development of “digital {CA}”},
	volume = {7},
	issn = {2211-6958},
	shorttitle = {Microanalysis {Of} {Online} {Data}},
	url = {https://www.sciencedirect.com/science/article/pii/S2211695814000464},
	doi = {10.1016/j.dcm.2014.12.002},
	abstract = {This paper introduces the work of the MOOD (Microanalysis Of Online Data) network, an interdisciplinary association of academic researchers exploring ways of conducting close qualitative analyses of online interaction. Despite the fact that much online interaction meets the criteria for ‘conversation’, conversation analysis (CA) has only recently begun to grow and flourish as a methodology for analysing the overwhelming quantity of material that in many cases sits in archive form, visible to millions, on the Internet. We discuss the development of methods that are inherently suited for subjecting online interaction to the kind of rigorous analysis that conversation analysts have applied to talk of all kinds for several decades. We go on to explore the fundamental challenges that online data pose for CA, the value of many CA techniques for online analysis, and the possibilities of developing bespoke modes of analysis that are crafted for use with specific forms of online data (e.g. ‘tweets’ on Twitter).},
	language = {en},
	urldate = {2022-05-24},
	journal = {Discourse, Context \& Media},
	author = {Giles, David and Stommel, Wyke and Paulus, Trena and Lester, Jessica and Reed, Darren},
	month = mar,
	year = {2015},
	keywords = {Computer-mediated communication, Conversation analysis, Digital media, Online discussion, Online interaction},
	pages = {45--51},
}

@article{meredith_analysing_2017,
	title = {Analysing technological affordances of online interactions using conversation analysis},
	volume = {115},
	issn = {0378-2166},
	url = {https://www.sciencedirect.com/science/article/pii/S0378216617301558},
	doi = {10.1016/j.pragma.2017.03.001},
	abstract = {The use of conversation analysis (CA) as a method for analysing the interactional practices of online communication has been growing in recent years (Giles et al., 2015). A key challenge for analysing online communication is the varied platforms through which interaction can occur. This paper demonstrates how using CA and the concept of affordances (Hutchby, 2001) can provide a lens through which to analyse not only the interaction, but also the technological context of that interaction. A corpus of instant messaging chats, captured from Facebook chat using screen-capture software, is used as a case study to demonstrate how the concept of affordances can be used alongside CA analysis to address the role of technology in the interaction. Two key interactional practices – turn adjacency and openings – are analysed to show the insights that CA can offer for providing an in-depth analysis of online interaction. By using affordances as a lens through which CA analysis can be refracted, scholars using ‘digital CA’ can better develop an understanding of patterns of interaction across different interactional platforms.},
	language = {en},
	urldate = {2022-05-24},
	journal = {Journal of Pragmatics},
	author = {Meredith, Joanne},
	month = jul,
	year = {2017},
	keywords = {Conversation analysis, Instant messaging, Online interaction, Screen-capture, Technological affordances},
	pages = {42--55},
}

@article{lewinski_developing_2019,
	title = {Developing {Methods} {That} {Facilitate} {Coding} and {Analysis} of {Synchronous} {Conversations} via {Virtual} {Environments}},
	volume = {18},
	issn = {1609-4069},
	url = {https://doi.org/10.1177/1609406919842443},
	doi = {10.1177/1609406919842443},
	abstract = {Programs via the Internet are uniquely positioned to capture qualitative data. One reason is because the Internet facilitates the creation of a community of similar individuals who can exchange information and support related to living with a chronic illness. Synchronous conversations via the Internet can provide insight into real-time social interaction and the exchange of social support. One way to analyze interactions among individuals is by using qualitative methods such as content, conversation, or discourse analysis. This manuscript describes how we used content analysis with aspects from conversation and discourse analysis to analyze synchronous conversations via the Internet to describe what individuals talk about and how individuals talk in an Internet-mediated interaction. With the increase in Internet interventions that facilitate collection of real-time conversational data, this article provides insight into how combining qualitative methods can facilitate the coding and analysis of these complex data.},
	language = {en},
	urldate = {2022-05-24},
	journal = {International Journal of Qualitative Methods},
	author = {Lewinski, Allison A. and Anderson, Ruth A. and Vorderstrasse, Allison A. and Johnson, Constance M.},
	month = jan,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	keywords = {Internet, content analysis, qualitative analysis, qualitative research methods, synchronous conversations, virtual environments},
	pages = {1609406919842443},
}

@article{basili_software_1992,
	title = {Software {Modeling} and {Measurement}: {The} {Goal}/{Question}/{Metric} {Paradigm}},
	shorttitle = {Software {Modeling} and {Measurement}},
	url = {https://drum.lib.umd.edu/handle/1903/7538},
	language = {en\_US},
	urldate = {2022-05-24},
	author = {Basili, Victor R.},
	month = sep,
	year = {1992},
	note = {Accepted: 2008-02-13T17:12:42Z},
}

@article{carroll_incident_1995,
	title = {Incident {Reviews} in {High}-{Hazard} {Industries}: {Sense} {Making} and {Learning} {Under} {Ambiguity} and {Accountability}},
	volume = {9},
	issn = {1087-0172},
	shorttitle = {Incident {Reviews} in {High}-{Hazard} {Industries}},
	url = {http://journals.sagepub.com/doi/10.1177/108602669500900203},
	doi = {10.1177/108602669500900203},
	abstract = {Learning from practical experience is of greater importance in more complex work environments. In high-hazard industries, complexity, tight coupling, and invisibility make safe operation and learning from experience particularly difficult. There is growing recognition that further improvement is needed and that it will require more than incremental improvement and exchange of "best practices." This article describes how organization members make sense of practical experience in one high-hazard industry\&mdash;nuclear power\&mdash;and how their sense-making affects their decisions and actions. The author discusses four factors that can limit the effectiveness of the interpretive process: root cause seduction, sharp-end focus, solution-driven search, and account acceptability. He then examines the impact that myopic interpretations can have on operating performance by placing incident reviews within the organizational learning process, and he closes with suggestions for a cross-disciplinary research agenda.},
	language = {en},
	number = {2},
	urldate = {2022-05-23},
	journal = {Industrial \& Environmental Crisis Quarterly},
	author = {Carroll, John S.},
	month = jun,
	year = {1995},
	pages = {175--197},
}

@article{carroll_organizational_1998,
	title = {Organizational {Learning} {Activities} in {High}-hazard {Industries}: {The} {Logics} {Underlying} {Self}-{Analysis}},
	volume = {35},
	issn = {0022-2380, 1467-6486},
	shorttitle = {Organizational {Learning} {Activities} in {High}-hazard {Industries}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1467-6486.00116},
	doi = {10.1111/1467-6486.00116},
	abstract = {Organizational learning takes place through activities performed by individuals, groups, and organizations as they gather and digest information, imagine and plan new actions, and implement change. I examine the learning practices of companies in two industries ± nuclear power plants and chemical process plants ± that must manage safety as a major component of operations, and therefore must learn from precursors and near-misses rather than exclusively by trial-anderror. Speci®cally, I analyse the linked assumptions or logics underlying incident reviews, root cause analysis teams, and self-analysis programmes. These logics arise from occupational and hierarchical groups that work on dierent problems in dierent ways ± for example, anticipation and resilience, ®xing and learning, concrete and abstract. In organizations with fragmentary, myopic and disparate understandings of how the work is accomplished, there are likely to be more failures to learn from operating experience, recurrent problems, and cyclical crises. Enhanced learning requires ways to broaden and bring together disparate logics.},
	language = {en},
	number = {6},
	urldate = {2022-05-23},
	journal = {Journal of Management Studies},
	author = {Carroll, John S.},
	month = nov,
	year = {1998},
	pages = {699--717},
}

@misc{noauthor_docs_nodate,
	title = {Docs home {\textbar} {Semgrep}},
	url = {https://semgrep.dev/docs/},
	abstract = {Read the documentation and get started with Semgrep. A fast, open-source, static analysis tool for finding bugs and enforcing code standards at editor, commit, and CI time.},
	urldate = {2022-05-19},
}

@misc{noauthor_codeql_nodate,
	title = {{CodeQL}},
	url = {https://codeql.github.com/},
	urldate = {2022-05-21},
}

@inproceedings{guo_caspar_2020,
	title = {Caspar: {Extracting} and {Synthesizing} {User} {Stories} of {Problems} from {App} {Reviews}},
	shorttitle = {Caspar},
	abstract = {A user's review of an app often describes the user's interactions with the app. These interactions, which we interpret as mini stories, are prominent in reviews with negative ratings. In general, a story in an app review would contain at least two types of events: user actions and associated app behaviors. Being able to identify such stories would enable an app's developer in better maintaining and improving the app's functionality and enhancing user experience. We present Caspar, a method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. By extending and applying natural language processing techniques, Caspar extracts ordered events from app reviews, classifies them as user actions or app problems, and synthesizes action-problem pairs. Our evaluation shows that Caspar is effective in finding action-problem pairs from reviews. First, Caspar classifies the events with an accuracy of 82.0\% on manually labeled data. Second, relative to human evaluators, Caspar extracts event pairs with 92.9\% precision and 34.2\% recall. In addition, we train an inference model on the extracted action-problem pairs that automatically predicts possible app problems for different use cases. Preliminary evaluation shows that our method yields promising results. Caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Guo, Hui and Singh, Munindar P.},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	keywords = {Natural language processing, Predictive models, Software engineering, app review analysis, event extraction, event inference, natural language processing, requirements},
	pages = {628--640},
}

@misc{stoch_how_2012,
	title = {How social media can benefit small businesses},
	abstract = {Have you ever wondered what Twitter, Facebook, LinkedIn and
Pinterest can bring to your company? Alison Coleman meets the
firms that have used social media to their advantage},
	language = {en},
	author = {Stoch, David},
	month = aug,
	year = {2012},
}

@article{kashmar_access_2021,
	title = {Access {Control} in {Cybersecurity} and {Social} {Media}},
	abstract = {Social media networks and their applications (e.g. Facebook, Twitter...) are a current phenomenon with a great impact on several aspects such as, personal, commercial, political, etc. This media is vulnerable to various forms of attacks and threats due to the heterogeneity of networks, diversity of applications and platforms, and the level of users' awareness and intentions. As network technologies and their various applications evolve, the way people interact with them changes. Thus, the main concern for all social media users is to protect their data from any type of illegal access. With network and application developments, the concept of controlling access evolves in various stages. It begins with the implementation of the principles of information security (confidentiality, authentication ...), then by finding various access control (AC) models to enforce security policy in this field. For cybersecurity and social media, various methods are developed based on conventional AC models: Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role Based Access Control (RBAC), Organization Based Access Control (OrBAC), and others.
In this chapter, we highlight the various types of cybercriminal attacks in social media networks. We then introduce the challenges faced for controlling users’ access and the importance of the AC concept for cybersecurity and social media. We will also review the common AC models and the AC methods that are proposed to enhance privacy issues in social networks. Based on these methods, we will conclude our chapter by analyzing them to know their efficiency in such media and their adaptability for any future requirements.},
	journal = {Access Control in Cybersecurity and Social Media},
	author = {Kashmar, Nadine and Adda, Mehdi and Ibrahim, H. and Atieh, Mirna},
	month = feb,
	year = {2021},
}

@article{massey_where_2016,
	title = {Where {Do} {U}.{S}. {Adults} {Who} {Do} {Not} {Use} the {Internet} {Get} {Health} {Information}? {Examining} {Digital} {Health} {Information} {Disparities} {From} 2008 to 2013},
	volume = {21},
	issn = {1081-0730},
	shorttitle = {Where {Do} {U}.{S}. {Adults} {Who} {Do} {Not} {Use} the {Internet} {Get} {Health} {Information}?},
	url = {https://doi.org/10.1080/10810730.2015.1058444},
	doi = {10.1080/10810730.2015.1058444},
	abstract = {With more people turning to the Internet for health information, a few questions remain: Which populations represent the remaining few who have never used the Internet, and where do they go for health information? The purpose of this study is to describe population characteristics and sources of health information among U.S. adults who do not use the Internet. Data from 3 iterations of the Health Information National Trends Survey (n = 1,722) are used to examine trends in health information sources. Weighted predicted probabilities demonstrate changes in information source over time. Older adults, minority populations, and individuals with low educational attainment represent a growing percentage of respondents who have looked for health information but have never used the Internet, highlighting trends in digital information disparities. However, 1 in 10 respondents who have never used the Internet also indicate that the Internet was their first source of health information, presumably through surrogates. Findings highlight digital disparities in information seeking and the complex nature of online information seeking. Future research should examine how individuals conceptualize information sources, measure skills related to evaluating information and sources, and investigate the social nature of information seeking. Health care organizations and public health agencies can leverage the multifaceted nature of information seeking to better develop information resources to increase information access by vulnerable populations.},
	number = {1},
	urldate = {2022-05-20},
	journal = {Journal of Health Communication},
	author = {Massey, Philip M.},
	month = jan,
	year = {2016},
	pmid = {26166484},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10810730.2015.1058444},
	pages = {118--124},
}

@article{ishimatsu2010modeling,
	title = {Modeling and hazard analysis using {STPA}},
	author = {Ishimatsu, Takuto and Leveson, Nancy G and Thomas, John and Katahira, Masa and Miyamoto, Yuko and Nakao, Haruka},
	year = {2010},
	note = {Publisher: International Association for the Advancement of Space Safety (IAASS)},
	keywords = {IoTFailurePaper},
}

@phdthesis{song2012applying,
	title = {Applying system-theoretic accident model and processes ({STAMP}) to hazard analysis},
	author = {Song, Yao},
	year = {2012},
	keywords = {IoTFailurePaper},
}

@article{reifer_software_1979,
	title = {Software {Failure} {Modes} and {Effects} {Analysis}},
	volume = {R-28},
	issn = {1558-1721},
	doi = {10.1109/TR.1979.5220578},
	abstract = {This concept paper discusses the possible use of failure modes and effects analysis (FMEA) as a means to produce more reliable software. FMEA is a fault avoidance technique whose objective is to identify hazards in requirements that have the potential to either endanger mission success or significantly impact life-cycle costs. FMEA techniques can be profitably applied during the analysis stage to identify potential hazards in requirements and design. As hazards are identified, software defenses can be developed using fault tolerant or self-checking techniques to reduce the probability of their occurrence once the program is implemented. Critical design features can also be demonstrated a priori analytically using proof of correctness techniques prior to their implementation if warranted by cost and criticality.},
	number = {3},
	journal = {IEEE Transactions on Reliability},
	author = {Reifer, Donald J.},
	month = aug,
	year = {1979},
	keywords = {Costs, Failure analysis, Fault tolerance, Fault tolerant software, Hazards, IoTFailurePaper, Military standards, Missiles, Self checking software, Software failure modes and effects analysis, Software performance, Software reliability, Software safety, Software standards, Software testing},
	pages = {247--249},
}

@book{fairbanks_just_2010,
	title = {Just {Enough} {Software} {Architecture}: {A} {Risk}-driven {Approach}},
	isbn = {978-0-9846181-0-1},
	shorttitle = {Just {Enough} {Software} {Architecture}},
	abstract = {This is a practical guide for software developers, and different than other software architecture books. Here's why:It teaches risk-driven architecting. There is no need for meticulous designs when risks are small, nor any excuse for sloppy designs when risks threaten your success. This book describes a way to do just enough architecture. It avoids the one-size-fits-all process tar pit with advice on how to tune your design effort based on the risks you face.It democratizes architecture. This book seeks to make architecture relevant to all software developers. Developers need to understand how to use constraints as guiderails that ensure desired outcomes, and how seemingly small changes can affect a system's properties.It cultivates declarative knowledge. There is a difference between being able to hit a ball and knowing why you are able to hit it, what psychologists refer to as procedural knowledge versus declarative knowledge. This book will make you more aware of what you have been doing and provide names for the concepts.It emphasizes the engineering. This book focuses on the technical parts of software development and what developers do to ensure the system works not job titles or processes. It shows you how to build models and analyze architectures so that you can make principled design tradeoffs. It describes the techniques software designers use to reason about medium to large sized problems and points out where you can learn specialized techniques in more detail.It provides practical advice. Software design decisions influence the architecture and vice versa. The approach in this book embraces drill-down/pop-up behavior by describing models that have various levels of abstraction, from architecture to data structure design.},
	language = {en},
	publisher = {Marshall \& Brainerd},
	author = {Fairbanks, George},
	year = {2010},
	note = {Google-Books-ID: ITsWdAAzVYMC},
	keywords = {Computers / Software Development \& Engineering / General, Computers / Software Development \& Engineering / Quality Assurance \& Testing, Computers / Software Development \& Engineering / Systems Analysis \& Design, Computers / Software Development \& Engineering / Tools, IoTFailurePaper},
}

@inproceedings{lee_smart_2016,
	address = {New York, NY, USA},
	series = {{APSys} '16},
	title = {Smart and {Secure}: {Preserving} {Privacy} in {Untrusted} {Home} {Routers}},
	isbn = {978-1-4503-4265-0},
	shorttitle = {Smart and {Secure}},
	url = {https://doi.org/10.1145/2967360.2967380},
	doi = {10.1145/2967360.2967380},
	abstract = {Recently, wireless home routers increasingly become smart. While these smart routers provide rich functionalities to users, they also raise security concerns. Since a smart home router may process and store personal data for users, once compromised, these sensitive information will be exposed. Unfortunately, current operating systems on home routers are far from secure. As a consequence, users are facing a difficult tradeoff between functionality and privacy risks. This paper attacks this dilemma with a novel SEAL architecture for home routers. SEAL leverages the ARM TrustZone technology to divide a conventional router OS (i.e., Linux) in a non-secure/normal world. All sensitive user data are shielded from the normal world using encryption. Modules (called applets) that process the sensitive data are located in a secure world and confined in secure sandboxes provided by a tiny secure OS. We report the system design of SEAL and our preliminary implementation and evaluation results.},
	urldate = {2022-05-20},
	booktitle = {Proceedings of the 7th {ACM} {SIGOPS} {Asia}-{Pacific} {Workshop} on {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lee, Seung-seob and Shi, Hang and Tan, Kun and Liu, Yunxin and Lee, SuKyoung and Cui, Yong},
	month = aug,
	year = {2016},
	pages = {1--8},
}

@inproceedings{olson_border_2015,
	title = {Border control: {Sandboxing} accelerators},
	shorttitle = {Border control},
	doi = {10.1145/2830772.2830819},
	abstract = {As hardware accelerators proliferate, there is a desire to logically integrate them more tightly with CPUs through interfaces such as shared virtual memory. Although this integration has programmability and performance benefits, it may also have serious security and fault isolation implications, especially when accelerators are designed by third parties. Unchecked, accelerators could make incorrect memory accesses, causing information leaks, data corruption, or crashes not only for processes running on the accelerator, but for the rest of the system as well. Unfortunately, current security solutions are insufficient for providing memory protection from tightly integrated untrusted accelerators. We propose Border Control, a sandboxing mechanism which guarantees that the memory access permissions in the page table are respected by accelerators, regardless of design errors or malicious intent. Our hardware implementation of Border Control provides safety against improper memory accesses with a space overhead of only 0.006\% of system physical memory per accelerator. We show that when used with a current highly demanding accelerator, this initial Border Control implementation has on average a 0.15\% runtime overhead relative to the unsafe baseline.},
	booktitle = {2015 48th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Olson, Lena E. and Power, Jason and Hill, Mark D. and Wood, David A.},
	month = dec,
	year = {2015},
	note = {ISSN: 2379-3155},
	keywords = {Computer bugs, Graphics processing units, Hardware, Libraries, Safety, Security, accelerators, hardware sandboxing, memory protection},
	pages = {470--481},
}

@inproceedings{narayan_retrofitting_2020,
	title = {Retrofitting {Fine} {Grain} {Isolation} in the {Firefox} {Renderer}},
	isbn = {978-1-939133-17-5},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/narayan},
	language = {en},
	urldate = {2022-05-20},
	author = {Narayan, Shravan and Disselkoen, Craig and Garfinkel, Tal and Froyd, Nathan and Rahm, Eric and Lerner, Sorin and Shacham, Hovav and Stefan, Deian},
	year = {2020},
	pages = {699--716},
}

@inproceedings{abbasi_challenges_2019,
	title = {Challenges in {Designing} {Exploit} {Mitigations} for {Deeply} {Embedded} {Systems}},
	doi = {10.1109/EuroSP.2019.00013},
	abstract = {Memory corruption vulnerabilities have been around for decades and rank among the most prevalent vulnerabilities in embedded systems. Yet this constrained environment poses unique design and implementation challenges that significantly complicate the adoption of common hardening techniques. Combined with the irregular and involved nature of embedded patch management, this results in prolonged vulnerability exposure windows and vulnerabilities that are relatively easy to exploit. Considering the sensitive and critical nature of many embedded systems, this situation merits significant improvement. In this work, we present the first quantitative study of exploit mitigation adoption in 42 embedded operating systems, showing the embedded world to significantly lag behind the general-purpose world. To improve the security of deeply embedded systems, we subsequently present μArmor, an approach to address some of the key gaps identified in our quantitative analysis. μArmor raises the bar for exploitation of embedded memory corruption vulnerabilities, while being adoptable on the short term without incurring prohibitive extra performance or storage costs.},
	booktitle = {2019 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS} {P})},
	author = {Abbasi, Ali and Wetzels, Jos and Holz, Thorsten and Etalle, Sandro},
	month = jun,
	year = {2019},
	keywords = {Embedded System, Embedded systems, Exploit Mitigation, Exploiting, Hardware, Linux, Real-time systems, Security, Statistical analysis},
	pages = {31--46},
}

@techreport{yu_building_2022,
	title = {Building {Embedded} {Systems} {Like} {It}'s 1996},
	url = {http://arxiv.org/abs/2203.06834},
	abstract = {Embedded devices are ubiquitous. However, preliminary evidence shows that attack mitigations protecting our desktops/servers/phones are missing in embedded devices, posing a significant threat to embedded security. To this end, this paper presents an in-depth study on the adoption of common attack mitigations on embedded devices. Precisely, it measures the presence of standard mitigations against memory corruptions in over 10k Linux-based firmware of deployed embedded devices. The study reveals that embedded devices largely omit both user-space and kernel-level attack mitigations. The adoption rates on embedded devices are multiple times lower than their desktop counterparts. An equally important observation is that the situation is not improving over time. Without changing the current practices, the attack mitigations will remain missing, which may become a bigger threat in the upcoming IoT era. Throughout follow-up analyses, we further inferred a set of factors possibly contributing to the absence of attack mitigations. The exemplary ones include massive reuse of non-protected software, lateness in upgrading outdated kernels, and restrictions imposed by automated building tools. We envision these will turn into insights towards improving the adoption of attack mitigations on embedded devices in the future.},
	number = {arXiv:2203.06834},
	urldate = {2022-05-20},
	institution = {arXiv},
	author = {Yu, Ruotong and Del Nin, Francesca and Zhang, Yuchen and Huang, Shan and Kaliyar, Pallavi and Zakto, Sarah and Conti, Mauro and Portokalidis, Georgios and Xu, Jun},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.06834},
	note = {arXiv:2203.06834 [cs]
type: article},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{noauthor_construction_nodate,
	title = {Construction site accident analysis using text mining and natural language processing techniques {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0926580518306137?token=5A7FAE4AA4A7FAD79C6BDA08C3235E445BA29FB4C4D938F55BB1785218950E25F04A2CCFB864DD32BF7E209409C59011&originRegion=us-east-1&originCreation=20220519230633},
	language = {en},
	urldate = {2022-05-19},
	doi = {10.1016/j.autcon.2018.12.016},
}

@article{zhang_construction_2019,
	title = {Construction site accident analysis using text mining and natural language processing techniques},
	volume = {99},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580518306137},
	doi = {10.1016/j.autcon.2018.12.016},
	abstract = {Workplace safety is a major concern in many countries. Among various industries, construction sector is identified as the most hazardous work place. Construction accidents not only cause human sufferings but also result in huge financial loss. To prevent reoccurrence of similar accidents in the future and make scientific risk control plans, analysis of accidents is essential. In construction industry, fatality and catastrophe investigation summary reports are available for the past accidents. In this study, text mining and natural language process (NLP) techniques are applied to analyze the construction accident reports. To be more specific, five baseline models, support vector machine (SVM), linear regression (LR), K-nearest neighbor (KNN), decision tree (DT), Naive Bayes (NB) and an ensemble model are proposed to classify the causes of the accidents. Besides, Sequential Quadratic Programming (SQP) algorithm is utilized to optimize weight of each classifier involved in the ensemble model. Experiment results show that the optimized ensemble model outperforms rest models considered in this study in terms of average weighted F1 score. The result also shows that the proposed approach is more robust to cases of low support. Moreover, an unsupervised chunking approach is proposed to extract common objects which cause the accidents based on grammar rules identified in the reports. As harmful objects are one of the major factors leading to construction accidents, identifying such objects is extremely helpful to mitigate potential risks. Certain limitations of the proposed methods are discussed and suggestions and future improvements are provided.},
	language = {en},
	urldate = {2022-05-19},
	journal = {Automation in Construction},
	author = {Zhang, Fan and Fleyeh, Hasan and Wang, Xinru and Lu, Minghui},
	month = mar,
	year = {2019},
	keywords = {Construction site accident analysis, Machine learning, Natural language processing, Optimization, Sequential quadratic programming, Text mining},
	pages = {238--248},
}

@article{machiry_c_2022,
	title = {C to checked {C} by 3c},
	volume = {6},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3527322},
	doi = {10.1145/3527322},
	abstract = {ARAVIND MACHIRY, Purdue University, USA JOHN KASTNER, Amazon, USA MATT MCCUTCHEN, Amazon, USA AARON ELINE, Amazon, USA KYLE HEADLEY, Amazon, USA MICHAEL HICKS, Amazon, USA Owing to the continued use of C (and C++), spatial safety violations (e.g., buffer overflows) still constitute one of today’s most dangerous and prevalent security vulnerabilities. To combat these violations, Checked C extends C with bounds-enforced checked pointer types. Checked C is essentially a gradually typed spatially safe CÐchecked pointers are backwards-binary compatible with legacy pointers, and the language allows them to be added piecemeal, rather than necessarily all at once, so that safety retrofitting can be incremental. This paper presents a semi-automated process for porting a legacy C program to Checked C. The process centers on 3C, a static analysis-based annotation tool. 3C employs two novel static analysis algorithmsÐtyp3c and boun3cÐto annotate legacy pointers as checked pointers, and to infer array bounds annotations for pointers that need them. 3C performs a root cause analysis to direct a human developer to code that should be refactored; once done, 3C can be re-run to infer further annotations (and updated root causes). Experiments on 11 programs totaling 319KLoC show 3C to be effective at inferring checked pointer types, and experience with previously and newly ported code finds 3C works well when combined with human-driven refactoring. CCS Concepts: • Software and its engineering → Translator writing systems and compiler generators; Software maintenance tools; • Security and privacy → Formal security models.},
	language = {en},
	number = {OOPSLA1},
	urldate = {2022-05-19},
	journal = {Proceedings of the ACM on Programming Languages (OOPSLA)},
	author = {Machiry, Aravind and Kastner, John and McCutchen, Matt and Eline, Aaron and Headley, Kyle and Hicks, Michael},
	month = apr,
	year = {2022},
	pages = {1--29},
}

@inproceedings{elliott_checked_2018,
	address = {Cambridge, MA},
	title = {Checked {C}: {Making} {C} {Safe} by {Extension}},
	isbn = {978-1-5386-7662-2},
	shorttitle = {Checked {C}},
	url = {https://ieeexplore.ieee.org/document/8543387/},
	doi = {10.1109/SecDev.2018.00015},
	abstract = {This paper presents Checked C, an extension to C designed to support spatial safety, implemented in Clang and LLVM. Checked C’s design is distinguished by its focus on backward-compatibility, incremental conversion, developer control, and enabling highly performant code. Like past approaches to a safer C, Checked C employs a form of checked pointer whose accesses can be statically or dynamically verified. Performance evaluation on a set of standard benchmark programs shows overheads to be relatively low. More interestingly, Checked C introduces the notions of a checked region and bounds-safe interfaces.},
	language = {en},
	urldate = {2022-05-19},
	booktitle = {2018 {IEEE} {Cybersecurity} {Development} ({SecDev})},
	publisher = {IEEE},
	author = {Elliott, Archibald Samuel and Ruef, Andrew and Hicks, Michael and Tarditi, David},
	month = sep,
	year = {2018},
	pages = {53--60},
}

@book{rob_moffatt_risk-first_2019,
	title = {Risk-{First} {Software} {Development}: {Volume} 1: {The} {Menagerie}},
	isbn = {1-71749-185-5},
	shorttitle = {Risk-{First} {Software} {Development}},
	url = {https://riskfirst.org},
	author = {{Rob Moffatt}},
	year = {2019},
}

@article{oukemeni_ipam_2019,
	title = {{IPAM}: {Information} {Privacy} {Assessment} {Metric} in {Microblogging} {Online} {Social} {Networks}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{IPAM}},
	doi = {10.1109/ACCESS.2019.2932899},
	abstract = {A large amount of sensitive data is transferred, stored, processed, and analyzed daily in Online Social Networks (OSNs). Thus, an effective and efficient evaluation of the privacy level provided in such services is necessary to meet user expectations and comply with the requirement of the applicable laws and regulations. Several prior works have proposed mechanisms for evaluating and calculating privacy scores in OSNs. However, current models are system-specific and assess privacy only from the user's perspective. There is still a lack of a universal model that can quantify the level of privacy and compare between different systems. In this paper, we propose a generic framework to (i) guide the development of privacy metrics and (ii) to measure and assess the privacy level of OSNs, more specifically microblogging systems. The present study develops an algorithmic model to compute privacy scores based on the impact of privacy and security requirements, accessibility, and difficulty of information extraction. The proposed framework aims to provide users as well as system providers with a measure of how much the investigated system is protecting privacy. It allows also comparing the privacy protection level between different systems. The privacy score framework has been tested using real microblogging social networks and the results show the potential of the proposed privacy scoring framework.},
	journal = {IEEE Access},
	author = {Oukemeni, Samia and Rifà-Pous, Helena and Marquès Puig, Joan Manuel},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Data privacy, Law, Measurement, Online social networks, Privacy, Security, Social networking (online), information privacy protection, information security, metrics, microbloggings, privacy measurements},
	pages = {114817--114836},
}

@incollection{page_social_2022,
	address = {Cham},
	title = {Social {Media} and {Privacy}},
	isbn = {978-3-030-82786-1},
	url = {https://doi.org/10.1007/978-3-030-82786-1_7},
	abstract = {With the popularity of social media, researchers and designers must consider a wide variety of privacy concerns while optimizing for meaningful social interactions and connection. While much of the privacy literature has focused on information disclosures, the interpersonal dynamics associated with being on social media make it important for us to look beyond informational privacy concerns to view privacy as a form of interpersonal boundary regulation. In other words, attaining the right level of privacy on social media is a process of negotiating how much, how little, or when we desire to interact with others, as well as the types of information we choose to share with them or allow them to share about us. We propose a framework for how researchers and practitioners can think about privacy as a form of interpersonal boundary regulation on social media by introducing five boundary types (i.e., relational, network, territorial, disclosure, and interactional) social media users manage. We conclude by providing tools for assessing privacy concerns in social media, as well as noting several challenges that must be overcome to help people to engage more fully and stay on social media.},
	language = {en},
	urldate = {2022-05-17},
	booktitle = {Modern {Socio}-{Technical} {Perspectives} on {Privacy}},
	publisher = {Springer International Publishing},
	author = {Page, Xinru and Berrios, Sara and Wilkinson, Daricia and Wisniewski, Pamela J.},
	editor = {Knijnenburg, Bart P. and Page, Xinru and Wisniewski, Pamela and Lipford, Heather Richter and Proferes, Nicholas and Romano, Jennifer},
	year = {2022},
	doi = {10.1007/978-3-030-82786-1_7},
	pages = {113--147},
}

@article{tiedeman_post-mortems-methodology_1990,
	title = {Post-mortems-methodology and experiences},
	volume = {8},
	issn = {1558-0008},
	doi = {10.1109/49.46869},
	abstract = {A methodology for preparing and conducting post-mortems for software development projects is presented. Based on the continuous process improvement concept, post-mortems are systematic reviews of a product's quality and the quality of the associated processes that produce it. The intent is to learn both what worked well and what could be improved. Three types of postmortem have evolved, each focusing on particular aspects of the software development process. The planning postmortem covers requirements specification and work program definition. The design verification postmortem includes design, unit test, system verification, and first office application activities. The field post-mortem focuses on actual field experience. The post-mortem methodology has been in use for several years and has been found to be a very effective organizational learning tool and source for many software quality improvement ideas. Specific examples of what has been and can be learned are included. The evolution of the post-mortem is discussed, along with possible metrics for evaluating the effectiveness of postmortems.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Tiedeman, M.J.},
	month = feb,
	year = {1990},
	note = {Conference Name: IEEE Journal on Selected Areas in Communications},
	keywords = {Application software, Electrical capacitance tomography, Electronic switching systems, Feedback, IoTFailurePaper, Programming, Software quality, Spine, Switches, Switching systems, System testing},
	pages = {176--180},
}

@article{dingsoyr_postmortem_2005,
	title = {Postmortem reviews: purpose and approaches in software engineering},
	volume = {47},
	issn = {09505849},
	shorttitle = {Postmortem reviews},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584904001296},
	doi = {10.1016/j.infsof.2004.08.008},
	abstract = {Conducting postmortems is a simple and practical method for organizational learning. Yet, not many companies have implemented such practices, and in a survey, few expressed satisfaction with how postmortems were conducted. In this article, we discuss the importance of postmortem reviews as a method for knowledge sharing in software projects, and give an overview of known such processes in the field of software engineering. In particular, we present three lightweight methods for conducting postmortems found in the literature, and discuss what criteria companies should use in defining their way of conducting postmortems.},
	language = {en},
	number = {5},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Dingsøyr, Torgeir},
	month = mar,
	year = {2005},
	keywords = {IoTFailurePaper},
	pages = {293--303},
}

@incollection{diaz-herrera_falling_1994,
	address = {Berlin/Heidelberg},
	title = {Falling down is part of growing {Up}; the study of failure and the {Software} {Engineering} community},
	volume = {750},
	isbn = {978-3-540-57461-3},
	url = {http://link.springer.com/10.1007/BFb0017636},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Software {Engineering} {Education}},
	publisher = {Springer-Verlag},
	author = {Dalcher, Darren},
	editor = {Díaz-Herrera, Jorge L.},
	year = {1994},
	doi = {10.1007/BFb0017636},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {IoTFailurePaper},
	pages = {489--496},
}

@book{cusumano1998microsoft,
	title = {Microsoft secrets: how the world's most powerful software company creates technology, shapes markets, and manages people},
	publisher = {Simon and Schuster},
	author = {Cusumano, Michael A and Selby, Richard W},
	year = {1998},
	keywords = {IoTFailurePaper},
}

@book{saldana_coding_2021,
	title = {The coding manual for qualitative researchers},
	publisher = {SAGE Publications, Inc},
	author = {Saldaña, Johnny},
	year = {2021},
}

@inproceedings{tiganoaia_risks_2017,
	title = {The risks in the social networks — {An} exploratory study},
	volume = {2},
	doi = {10.1109/IDAACS.2017.8095232},
	abstract = {While social networking has gone on almost as long as societies themselves have existed, the unparalleled potential of the Web to facilitate such connections has led to an exponential and ongoing expansion of that phenomenon. In addition to social media platforms, the capacity for social interaction and collaboration is increasingly built into business applications [1]. The risks in the online space and particularly in social networks represent one of the most important and opened issue of our time. The paper is an exploratory study with the focus point related to an original research based on a questionnaire with more that 300 of respondents. Topics about the risks in the social networks such as: the security in the social networks, incidents, viruses, worms, spam, the level of security regarding the applications used on social networks, etc were analysed. The findings of this research provide a basis that is useful to understand the importance of the prevention and treatment of risks in the social networks. Regarding the management perspective of an organization, this paper highlights the necessity of the investments in companies security against cyber risks. The paper also provides useful findings in the field of cyber risks for the online community.},
	booktitle = {2017 9th {IEEE} {International} {Conference} on {Intelligent} {Data} {Acquisition} and {Advanced} {Computing} {Systems}: {Technology} and {Applications} ({IDAACS})},
	author = {Tiganoaia, Bogdan and Cernian, Alexandra and Niculescu, Andrei},
	month = sep,
	year = {2017},
	keywords = {Computer security, Education, Social network services, Terrorism, questionnaire, research, risks, social networks},
	pages = {974--977},
}

@techreport{carr_taxonomy-based_1993,
	title = {Taxonomy-{Based} {Risk} {Identification}},
	url = {https://apps.dtic.mil/sti/citations/ADA266992},
	abstract = {This report describes a method for facilitating the systematic and repeatable identification of risks associated with the development of a software-dependent project. This method, derived from published literature and previous experience in developing software, was tested in active government- funded defense and civilian software development projects for both its usefulness and for improving the method itself. Results of the field tests encouraged the claim that the described method is useful, usable, and efficient. The report concludes with some macro-level lessons learned from the field tests and brief overview of future work in establishing risk management on a firm footing in software development projects.},
	language = {en},
	urldate = {2022-05-11},
	institution = {CARNEGIE-MELLON UNIV PITTSBURGH PA SOFTWARE ENGINEERING INST},
	author = {Carr, Marvin J. and Konda, Suresh L. and Monarch, Ira and Ulrich, F. C. and Walker, Clay F.},
	month = jun,
	year = {1993},
	note = {Section: Technical Reports},
}

@article{boehm_software_1991,
	title = {Software risk management: principles and practices},
	volume = {8},
	issn = {1937-4194},
	shorttitle = {Software risk management},
	doi = {10.1109/52.62930},
	abstract = {The emerging discipline of software risk management is described. It is defined as an attempt to formalize the risk-oriented correlates of success into a readily applicable set of principles and practices. Its objectives are to identify, address, and eliminate risk items before they become either threats to successful software operation or major sources of software rework. The basic concepts are set forth, and the major steps and techniques involved in software risk management are explained. Suggestions for implementing risk management are provided.{\textless}{\textgreater}},
	number = {1},
	journal = {IEEE Software},
	author = {Boehm, B.W.},
	month = jan,
	year = {1991},
	note = {Conference Name: IEEE Software},
	keywords = {Bridges, Disaster management, Energy resolution, Frequency, Project management, Risk management, Signal resolution, Software prototyping, Tides, Virtual prototyping},
	pages = {32--41},
}

@book{reis_handbook_2000,
	title = {Handbook of {Research} {Methods} in {Social} and {Personality} {Psychology}},
	isbn = {978-0-521-55903-4},
	abstract = {This sourcebook covers conceptual and practical issues in research design, methods of research and statistical approaches in social and personality psychology. The primary purpose of the handbook is to provide readable yet comprehensive chapters on the range of methods and tools used by researchers in social and personality psychology. In addition, it should alert researchers to methodological possibilities they may not have thought of. Innovative research methods work best when they allow researchers to ask theoretically driven questions that could not have been asked previously, thereby enhancing the quality and depth of their empirical knowledge base. With the help of this text, both new and established social psychologists should learn about appropriate uses of each method and the opportunities they provide for expanding knowledge.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Reis, Harry T. and Reis, Harry T. and Judd, Charles M.},
	month = mar,
	year = {2000},
	note = {Google-Books-ID: j7aawGLbtEoC},
	keywords = {Psychology / Personality, Psychology / Social Psychology},
}

@article{kish-gephart_bad_2010,
	title = {Bad apples, bad cases, and bad barrels: {Meta}-analytic evidence about sources of unethical decisions at work.},
	volume = {95},
	issn = {1939-1854, 0021-9010},
	shorttitle = {Bad apples, bad cases, and bad barrels},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0017103},
	doi = {10.1037/a0017103},
	abstract = {As corporate scandals proliferate, practitioners and researchers alike need a cumulative, quantitative understanding of the antecedents associated with unethical decisions in organizations. In this metaanalysis, the authors draw from over 30 years of research and multiple literatures to examine individual (“bad apple”), moral issue (“bad case”), and organizational environment (“bad barrel”) antecedents of unethical choice. Findings provide empirical support for several foundational theories and paint a clearer picture of relationships characterized by mixed results. Structural equation modeling revealed the complexity (multidetermined nature) of unethical choice, as well as a need for research that simultaneously examines different sets of antecedents. Moderator analyses unexpectedly uncovered better prediction of unethical behavior than of intention for several variables. This suggests a need to more strongly consider a new “ethical impulse” perspective in addition to the traditional “ethical calculus” perspective. Results serve as a data-based foundation and guide for future theoretical and empirical development in the domain of behavioral ethics.},
	language = {en},
	number = {1},
	urldate = {2022-05-10},
	journal = {Journal of Applied Psychology},
	author = {Kish-Gephart, Jennifer J. and Harrison, David A. and Treviño, Linda Klebe},
	month = jan,
	year = {2010},
	pages = {1--31},
}

@inproceedings{almudahi_social_2022,
	title = {Social {Media} {Privacy} {Issues}, {Threats}, and {Risks}},
	doi = {10.1109/WiDS-PSU54548.2022.00043},
	abstract = {General knownledge presumes that social media users are increasing. As of October 2021, more than 4.5 billion people are using social networks. Popularly utilized for communication, knowledge participation, thoughts communication, videos, building networks, images, and so on. However, there is still a lack of knowledge of the consequences associated with the use of said programs, and the growth of users increases the possible vulnerabilities and attacks. This paper proposes some suggestions for mitigation of risks and threats when people publish anything online. The authors' contribution provided in this paper is the suggested solutions for each of the listed and discussed threats and risks.},
	booktitle = {2022 {Fifth} {International} {Conference} of {Women} in {Data} {Science} at {Prince} {Sultan} {University} ({WiDS} {PSU})},
	author = {AlMudahi, Gahadh Faisal and AlSwayeh, Lama Khalid and AlAnsary, Sara Ahmed and Latif, Rabia},
	month = mar,
	year = {2022},
	keywords = {Cross-Site Scripting, Data privacy, Data science, Knowledge engineering, Media, Social networking (online), Videos, identity theft, location disclosure, malware, phishing, ransomware, risks, social media, social media threats},
	pages = {155--159},
}

@book{senge_fifth_2006,
	title = {The {Fifth} {Discipline}: {The} {Art} and {Practice} of the {Learning} {Organization}},
	isbn = {978-0-385-51725-6},
	shorttitle = {The {Fifth} {Discipline}},
	abstract = {Completely Updated and RevisedThis revised edition of Peter Senge's bestselling classic, The Fifth Discipline, is based on fifteen years of experience in putting the book's ideas into practice. As Senge makes clear, in the long run the only sustainable competitive advantage is your organization's ability to learn faster than the competition. The leadership stories in the book demonstrate the many ways that the core ideas in The Fifth Discipline, many of which seemed radical when first published in 1990, have become deeply integrated into people's ways of seeing the world and their managerial practices. In The Fifth Discipline, Senge describes how companies can rid themselves of the learning “disabilities” that threaten their productivity and success by adopting the strategies of learning organizations—ones in which new and expansive patterns of thinking are nurtured, collective aspiration is set free, and people are continually learning how to create results they truly desire. The updated and revised Currency edition of this business classic contains over one hundred pages of new material based on interviews with dozens of practitioners at companies like BP, Unilever, Intel, Ford, HP, Saudi Aramco, and organizations like Roca, Oxfam, and The World Bank. It features a new Foreword about the success Peter Senge has achieved with learning organizations since the book's inception, as well as new chapters on Impetus (getting started), Strategies, Leaders' New Work, Systems Citizens, and Frontiers for the Future. Mastering the disciplines Senge outlines in the book will:• Reignite the spark of genuine learning driven by people focused on what truly matters to them• Bridge teamwork into macro-creativity• Free you of confining assumptions and mindsets• Teach you to see the forest and the trees• End the struggle between work and personal time},
	language = {en},
	publisher = {Doubleday/Currency},
	author = {Senge, Peter M.},
	year = {2006},
	keywords = {Business \& Economics / Business Communication / General, Business \& Economics / Leadership, Business \& Economics / Management, Business \& Economics / Organizational Development},
}

@inproceedings{persia_survey_2017,
	title = {A {Survey} of {Online} {Social} {Networks}: {Challenges} and {Opportunities}},
	shorttitle = {A {Survey} of {Online} {Social} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8102989},
	doi = {10.1109/IRI.2017.74},
	abstract = {Online Social Networks (OSNs) have become fundamental parts of our online lives, and their popularity is increasing at a surprising rate every day. However, besides the revolution the OSNs have generated in social networking, they have also introduced some issues; first, since the amount of multimedia data on the Internet is growing continuously, it is extremely important for users not only to share multimedia content with each other, but also to receive the specific content they are interested in; second, OSNs have introduced new threats to their users due to their attractiveness, the ever-increasing number of users, and the massive amount of personal information they share. For such reasons, in this paper we propose a survey of online social networks, which can hopefully support both researchers and social network users. More specifically, we focus our attention on the most relevant research challenges regarding semantics and security.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Information} {Reuse} and {Integration} ({IRI})},
	author = {Persia, Fabio and D'Auria, Daniela},
	month = aug,
	year = {2017},
	keywords = {Online Social Networks, Phishing Attacks, Semantics, Sentiment Detection, Social network services, Spamming, Support vector machines, Sybil Attacks, Tools, Videos},
	pages = {614--620},
}

@article{gelles_boeing_2019,
	chapter = {Business},
	title = {Boeing 737 {Max}: {What}’s {Happened} {After} the 2 {Deadly} {Crashes}},
	issn = {0362-4331},
	shorttitle = {Boeing 737 {Max}},
	url = {https://www.nytimes.com/interactive/2019/business/boeing-737-crashes.html},
	abstract = {Boeing remains under intense scrutiny nearly one year after the first Max jet was involved in a fatal accident.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Gelles, David},
	month = mar,
	year = {2019},
	keywords = {Boeing 737 Max Groundings and Safety Concerns (2019), Boeing Company},
}

@article{zetter_inside_2016,
	title = {Inside the {Cunning}, {Unprecedented} {Hack} of {Ukraine}'s {Power} {Grid}},
	issn = {1059-1028},
	url = {https://www.wired.com/2016/03/inside-cunning-unprecedented-hack-ukraines-power-grid/},
	abstract = {The hack on Ukraine's power grid was a first-of-its-kind attack that sets an ominous precedent for the security of power grids everywhere.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Zetter, Kim},
	year = {2016},
	keywords = {critical infrastructure, hacks},
}

@article{greenberg_hackers_2015,
	title = {Hackers {Cut} a {Corvette}'s {Brakes} {Via} a {Common} {Car} {Gadget}},
	issn = {1059-1028},
	url = {https://www.wired.com/2015/08/hackers-cut-corvettes-brakes-via-common-car-gadget/},
	abstract = {The free dongles that insurance companies ask customers to plug into their dashes could expose your car to hackers.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Greenberg, Andy},
	year = {2015},
	keywords = {car hacking, mobile, uber},
}

@article{greenberg_hackers_2017,
	title = {Hackers {Remotely} {Kill} a {Jeep} on the {Highway}—{With} {Me} in {It}},
	issn = {1059-1028},
	url = {https://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/},
	abstract = {I was driving 70 mph on the edge of downtown St. Louis when the exploit began to take hold.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Greenberg, Andy},
	year = {2017},
}

@article{barrett_your_2016,
	title = {Your {DVR} {Didn}'t {Take} {Down} the {Internet}},
	issn = {1059-1028},
	url = {https://www.wired.com/2016/10/internet-outage-webcam-dvr-botnet/},
	abstract = {Repeat after us. Friday's botnet was not your fault.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Barrett, Brian},
	year = {2016},
	keywords = {iot, security},
}

@article{dreyfuss_hackers_2019,
	title = {Hackers {Found} a {Freaky} {New} {Way} to {Kill} {Your} {Car}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/car-hacking-biometric-database-security-roundup/},
	abstract = {Mueller report fallout, a biometrics database, and more of the week's top security news.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Dreyfuss, Emily},
	year = {2019},
	keywords = {biometrics, cybersecurity, security roundup},
}

@inproceedings{ding_iotsafe_2021,
	title = {{IoTSafe}: {Enforcing} {Safety} and {Security} {Policy} with {Real} {IoT} {Physical} {Interaction} {Discovery}},
	shorttitle = {{IoTSafe}},
	url = {https://www.ndss-symposium.org/ndss-paper/iotsafe-enforcing-safety-and-security-policy-with-real-iot-physical-interaction-discovery/},
	language = {en-US},
	urldate = {2021-09-22},
	booktitle = {Proceedings 2021 {Network} and {Distributed} {System} {Security} {Symposium}},
	author = {Ding, Wenbo},
	year = {2021},
}

@article{yamauchi_anomaly_2020,
	title = {Anomaly {Detection} in {Smart} {Home} {Operation} {From} {User} {Behaviors} and {Home} {Conditions}},
	volume = {66},
	issn = {1558-4127},
	doi = {10.1109/TCE.2020.2981636},
	abstract = {As several home appliances, such as air conditioners, heaters, and refrigerators, were connecting to the Internet, they became targets of cyberattacks, which cause serious problems such as compromising safety and even harming users. We have proposed a method to detect such attacks based on user behavior. This method models user behavior as sequences of user events including operation of home IoT (Internet of Things) devices and other monitored activities. Considering users behave depending on the condition of the home such as time and temperature, our method learns event sequences for each condition. To mitigate the impact of events of other users in the home included in the monitored sequence, our method generates multiple event sequences by removing some events and learning the frequently observed sequences. For evaluation, we constructed an experimental network of home IoT devices and recorded time data for four users entering/leaving a room and operating devices. We obtained detection ratios exceeding 90\% for anomalous operations with less than 10\% of misdetections when our method observed event sequences related to the operation. In this article, we also discuss the effectiveness of our method by comparing with a method learning users' behavior by Hidden Markov Models.},
	number = {2},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Yamauchi, Masaaki and Ohsita, Yuichi and Murata, Masayuki and Ueda, Kensuke and Kato, Yoshiaki},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Consumer Electronics},
	keywords = {Anomaly detection, Heating systems, Hidden Markov models, Internet of Things, Monitoring, Smart homes, Temperature measurement, Temperature sensors, behavior pattern, consumer electronics, cybersecurity, operation by attackers, smart home},
	pages = {183--192},
}

@inproceedings{chaki_conflict_2020,
	title = {A {Conflict} {Detection} {Framework} for {IoT} {Services} in {Multi}-resident {Smart} {Homes}},
	doi = {10.1109/ICWS49710.2020.00036},
	abstract = {We propose a novel framework to detect conflicts among IoT services in a multi-resident smart home. A novel IoT conflict model is proposed considering the functional and non-functional properties of IoT services. We design a conflict ontology that formally represents different types of conflicts. A hybrid conflict detection algorithm is proposed by combining both knowledge-driven and data-driven approaches. Experimental results on real-world datasets show the efficiency of the proposed approach.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Chaki, Dipankar and Bouguettaya, Athman and Mistry, Sajib},
	month = oct,
	year = {2020},
	keywords = {Conferences, Detection algorithms, Internet of Things, IoT services, Ontologies, Smart homes, Web services, conflict detection, conflict ontology, formal conflict model, multi-resident smart home},
	pages = {224--231},
}

@inproceedings{wang_charting_2019,
	address = {New York, NY, USA},
	series = {{CCS} '19},
	title = {Charting the {Attack} {Surface} of {Trigger}-{Action} {IoT} {Platforms}},
	isbn = {978-1-4503-6747-9},
	url = {https://doi.org/10.1145/3319535.3345662},
	doi = {10.1145/3319535.3345662},
	abstract = {Internet of Things (IoT) deployments are becoming increasingly automated and vastly more complex. Facilitated by programming abstractions such as trigger-action rules, end-users can now easily create new functionalities by interconnecting their devices and other online services. However, when multiple rules are simultaneously enabled, complex system behaviors arise that are difficult to understand or diagnose. While history tells us that such conditions are ripe for exploitation, at present the security states of trigger-action IoT deployments are largely unknown. In this work, we conduct a comprehensive analysis of the interactions between trigger-action rules in order to identify their security risks. Using IFTTT as an exemplar platform, we first enumerate the space of inter-rule vulnerabilities that exist within trigger-action platforms. To aid users in the identification of these dangers, we go on to present iRuler, a system that performs Satisfiability Modulo Theories (SMT) solving and model checking to discover inter-rule vulnerabilities within IoT deployments. iRuler operates over an abstracted information flow model that represents the attack surface of an IoT deployment, but we discover in practice that such models are difficult to obtain given the closed nature of IoT platforms. To address this, we develop methods that assist in inferring trigger-action information flows based on Natural Language Processing. We develop a novel evaluative methodology for approximating plausible real-world IoT deployments based on the installation counts of 315,393 IFTTT applets, determining that 66\% of the synthetic deployments in the IFTTT ecosystem exhibit the potential for inter-rule vulnerabilities. Combined, these efforts provide the insight into the real-world dangers of IoT deployment misconfigurations.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qi and Datta, Pubali and Yang, Wei and Liu, Si and Bates, Adam and Gunter, Carl A.},
	month = nov,
	year = {2019},
	keywords = {formal methods, information flow, inter-rule vulnerability, nlp, trigger-action iot platform},
	pages = {1439--1453},
}

@inproceedings{shehata_managing_2007,
	title = {Managing {Policy} {Interactions} in {KNX}-{Based} {Smart} {Homes}},
	volume = {2},
	doi = {10.1109/COMPSAC.2007.139},
	abstract = {Smart homes have enjoyed increasing popularity in recent years. In order for them to further expand their market share, users need to be able to fully control devices. Policies are one way for users to achieve such flexible control of devices. However, user policies often tend to interact in unwanted ways leading to unexpected behavior of devices. This paper describes the design of a run-time policy interaction management module (PIMM) that serves as manager for detecting and resolving interactions between user policies in KNX-based smart homes. This module extends the traditional KNX networking system with the ability to manage policy interactions. The module operates in the run-time S-mode of the KNX network and works as part of the engineering tool software (ETS) used to configure and control the operation of the KNX network in smart homes. The proposed module serves as the first of its kind that can be implemented inside the KNX networking system to detect and resolve unwanted policies interactions.},
	booktitle = {31st {Annual} {International} {Computer} {Software} and {Applications} {Conference} ({COMPSAC} 2007)},
	author = {Shehata, Mohamed and Eberlein, Armin and Fapojuwo, Abraham O.},
	month = jul,
	year = {2007},
	note = {ISSN: 0730-3157},
	keywords = {Application software, Control systems, Drives, Engineering management, Home computing, Runtime, Smart homes, Software tools, Telecommunication control, Telecommunication services},
	pages = {367--378},
}

@inproceedings{chaki_dynamic_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Dynamic {Conflict} {Resolution} of {IoT} {Services} in {Smart} {Homes}},
	isbn = {978-3-030-91431-8},
	doi = {10.1007/978-3-030-91431-8_23},
	abstract = {We propose a novel conflict resolution framework for IoT services in multi-resident smart homes. The proposed framework employs a preference extraction model based on a temporal proximity strategy. We design a preference aggregation model using a matrix factorization-based approach (i.e., singular value decomposition). The concepts of current resident item matrix and ideal resident item matrix are introduced as key criteria to cater to the conflict resolution framework. Finally, a set of experiments on real-world datasets are conducted to show the effectiveness of the proposed approach.},
	language = {en},
	booktitle = {Service-{Oriented} {Computing}},
	publisher = {Springer International Publishing},
	author = {Chaki, Dipankar and Bouguettaya, Athman},
	editor = {Hacid, Hakim and Kao, Odej and Mecella, Massimo and Moha, Naouel and Paik, Hye-young},
	year = {2021},
	keywords = {Conflict resolution, IoT service, Multi-resident smart home, Preference aggregation, Preference extraction},
	pages = {368--384},
}

@techreport{mccall_safetap_2021,
	type = {report},
	title = {{SafeTAP}: {An} {Efficient} {Incremental} {Analyzer} for {Trigger}-{Action} {Programs}},
	shorttitle = {{SafeTAP}},
	url = {https://kilthub.cmu.edu/articles/report/SafeTAP_An_Efficient_Incremental_Analyzer_for_Trigger-Action_Programs/14792271/1},
	abstract = {Home automation rules that allow users to connect smart home devices using trigger-action programs (TAP) can interact in subtle and unexpected ways. Determining whether these rules are free of undesirable behavior is challenging; so researchers have developed tools to analyze rules and assist users. However, it is unclear whether users need such tools, and what help they need from such tools. To answer this question, we performed a user study where half of the participants were given our custom analysis tool SafeTAP and the other half were not. We found that users are not good at finding issues in their TAP rules, despite perceiving such tasks as easy. The user study also indicates that users would like to check their rules every time they make rule changes. Therefore, we designed a novel incremental symbolic model checking (SMC) algorithm, which extends the basic SMC algorithm of SafeTAP. SafeTAPΔ only performs analysis caused by the addition or removal of rules and reports only new violations that have not already been reported to the user. We evaluate the performance of SafeTAPΔ and show that incremental checking on average improves the performance by 6X when adding new rules.},
	language = {en},
	urldate = {2022-05-06},
	institution = {Carnegie Mellon University},
	author = {McCall, McKenna and Shezan, Faysal Hossain and Bichhawat, Abhishek and Cobb, Camille and Jia, Limin and Tian, Yuan and Grace, Cooper and Yang, Mitchell},
	month = jun,
	year = {2021},
	doi = {10.1184/R1/14792271.v1},
}

@inproceedings{chaki_adaptive_2021,
	title = {Adaptive {Priority}-based {Conflict} {Resolution} of {IoT} {Services}},
	doi = {10.1109/ICWS53863.2021.00091},
	abstract = {We propose a novel conflict resolution framework for IoT services in multi-resident smart homes. An adaptive priority model is developed considering the residents' contextual factors (e.g., age, illness, impairment). The proposed priority model is designed using the concept of the analytic hierarchy process. A set of experiments on real-world datasets are conducted to show the efficiency of the proposed approach.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Chaki, Dipankar and Bouguettaya, Athman},
	month = sep,
	year = {2021},
	keywords = {Adaptation models, Adaptive priority, Analytic hierarchy process, Analytical models, Conferences, Conflict resolution, Internet of Things, IoT service, Multi-resident smart home, Smart homes, Web services},
	pages = {663--668},
}

@incollection{yuan_deresolver_2021,
	address = {New York, NY, USA},
	title = {{DeResolver}: a decentralized negotiation and conflict resolution framework for smart city services},
	isbn = {978-1-4503-8353-0},
	shorttitle = {{DeResolver}},
	url = {https://doi.org/10.1145/3450267.3450538},
	abstract = {As various smart services are increasingly deployed in modern cities, many unexpected conflicts arise due to various physical world couplings. Existing solutions for conflict resolution often rely on centralized control to enforce predetermined and fixed priorities of different services, which is challenging due to the inconsistent and private objectives of the services. Also, the centralized solutions miss opportunities to more effectively resolve conflicts according to their spatiotemporal locality of the conflicts. To address this issue, we design a decentralized negotiation and conflict resolution framework named DeResolver, which allows services to resolve conflicts by communicating and negotiating with each other to reach a Pareto-optimal agreement autonomously and efficiently. Our design features a two-level semi-supervised learning-based algorithm to predict acceptable proposals and their rankings of each opponent through the negotiation. Our design is evaluated with a smart city case study of three services: intelligent traffic light control, pedestrian service, and environmental control. In this case study, a data-driven evaluation is conducted using a large data set consisting of the GPS locations of 246 surveillance cameras and an automatic traffic monitoring system with more than 3 million records per day to extract real-world vehicle routes. The evaluation results show that our solution achieves much more balanced results, i.e., only increasing the average waiting time of vehicles, the measurement metric of intelligent traffic light control service, by 6.8\% while reducing the weighted sum of air pollutant emission, measured for environment control service, by 12.1\%, and the pedestrian waiting time, the measurement metric of pedestrian service, by 33.1\%, compared to priority-based solution.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the {ACM}/{IEEE} 12th {International} {Conference} on {Cyber}-{Physical} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yukun and Ma, Meiyi and Han, Songyang and Zhang, Desheng and Miao, Fei and Stankovic, John and Lin, Shan},
	month = may,
	year = {2021},
	keywords = {conflicts across services, decentralized resolution, multiple services negotiation, smart services},
	pages = {98--109},
}

@inproceedings{ma_detection_2016,
	title = {Detection of {Runtime} {Conflicts} among {Services} in {Smart} {Cities}},
	doi = {10.1109/SMARTCOMP.2016.7501688},
	abstract = {The populations of large cities around the world are growing rapidly. Cities are beginning to address this problem by implementing significant sensing and actuation infrastructure and building services on this infrastructure. However, as the density of sensing and actuation increases and as the complexities of services grow there is an increasing potential for conflicts across Smart City services. These conflicts can cause unsafe situations and disrupt the benefits that the services were originally intended to provide. Although some of the conflicts can be detected and avoided during designing the services, many can still occur unpredictably during runtime. This paper carefully defines and enumerates the main issues regarding the detection and resolution of runtime conflicts in smart cities. In particular, it focuses on conflicts that arise across services. This issue is becoming more and more important as Smart City designs attempt to integrate services from different domains (transportation, energy, public safety, emergency, medical, and many others). Research challenges are identified and then addressed that deal with uncertainty, dynamism, real-time, mobility and spatio-temporal availability, duration and scale of effect, efficiency, and ownership. A watchdog architecture is also described that oversees the services operating in a Smart City. This watchdog solution detects and resolves conflicts, it learns and adapts, and it provides additional inputs to decision making aspects of services. Using data from a Smart City dataset, an emulated set of services and activities using those services are created to perform a conflict analysis. A second analysis hypothesizes 41 future services across 5 domains. Both of these evaluations demonstrate the high probability of conflicts in smart cities of the future.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Ma, M. and Preum, S. Masud and Tarneberg, W. and Ahmed, M. and Ruiters, M. and Stankovic, J.},
	month = may,
	year = {2016},
	keywords = {Roads, Safety, Smart cities, Uncertainty, Vehicles},
	pages = {1--10},
}

@inproceedings{alhanahnah_scalable_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {Scalable analysis of interaction threats in {IoT} systems},
	isbn = {978-1-4503-8008-9},
	url = {https://doi.org/10.1145/3395363.3397347},
	doi = {10.1145/3395363.3397347},
	abstract = {The ubiquity of Internet of Things (IoT) and our growing reliance on IoT apps are leaving us more vulnerable to safety and security threats than ever before. Many of these threats are manifested at the interaction level, where undesired or malicious coordinations between apps and physical devices can lead to intricate safety and security issues. This paper presents IoTCOM, an approach to automatically discover such hidden and unsafe interaction threats in a compositional and scalable fashion. It is backed with auto-mated program analysis and formally rigorous violation detection engines. IoTCOM relies on program analysis to automatically infer the relevant app’s behavior. Leveraging a novel strategy to trim the extracted app’s behavior prior to translating them to analyzable formal specifications,IoTCOM mitigates the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IoTCOM’s ability to effectively detect a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown, and to significantly outperform the existing techniques in terms of scalability.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Alhanahnah, Mohannad and Stevens, Clay and Bagheri, Hamid},
	month = jul,
	year = {2020},
	keywords = {Formal Verification, Interaction Threats, IoT Safety},
	pages = {272--285},
}

@article{sun_conflict_2015,
	title = {Conflict {Detection} {Scheme} {Based} on {Formal} {Rule} {Model} for {Smart} {Building} {Systems}},
	volume = {45},
	issn = {2168-2305},
	doi = {10.1109/THMS.2014.2364613},
	abstract = {Smart building systems can provide flexible and configurational sensing and controlling operations according to users' requirements. As the number and the complexity of service rules customized by users have significantly increased, there is an increasing danger of conflict during the interaction process between users and the system. To address this issue, we propose a new rule conflict detection scheme tailored for the smart building system. First, we present a formal rule model UTEA based on User, Triggers, Environment entities, and Actuators. This model can handle not only controlled devices with discrete status but also real-valued environmental data such as temperature and humidity. In addition, this model takes multiple users with different authorities into account. Second, we define 11 rule relations and further classify conflicts into five categories. Third, we implement a rule storage system for detecting conflicts and design a conflict detection algorithm, which can detect the conflict between two rules as well as cycle conflict/multicross contradiction among multiple rules. We evaluated our scheme in a real smart building system with more than 30 000 service rules. The experiment results show that our scheme improves the performance in terms of error/missed-detection rates and running time.},
	number = {2},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Sun, Yan and Wang, Xukai and Luo, Hong and Li, Xiangyang},
	month = apr,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Human-Machine Systems},
	keywords = {Actuators, Conflict detection, Humidity, Intelligent sensors, Smart buildings, Sun, Temperature sensors, rule model, service, smart building system},
	pages = {215--227},
}

@inproceedings{ding_safety_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {On the {Safety} of {IoT} {Device} {Physical} {Interaction} {Control}},
	isbn = {978-1-4503-5693-0},
	url = {https://doi.org/10.1145/3243734.3243865},
	doi = {10.1145/3243734.3243865},
	abstract = {Emerging Internet of Things (IoT) platforms provide increased functionality to enable human interaction with the physical world in an autonomous manner. The physical interaction features of IoT platforms allow IoT devices to make an impact on the physical environment. However, such features also bring new safety challenges, where attackers can leverage stealthy physical interactions to launch attacks against IoT systems. In this paper, we propose a framework called IoTMon that discovers any possible physical interactions and generates all potential interaction chains across applications in the IoT environment. IoTMon also includes an assessment of the safety risk of each discovered inter-app interaction chain based on its physical influence. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of IoTMon and present a comprehensive system evaluation on the Samsung SmartThings platform. We study 185 official SmartThings applications and find they can form 162 hidden inter-app interaction chains through physical surroundings. In particular, our experiment reveals that 37 interaction chains are highly risky and could be potentially exploited to impact the safety of the IoT{\textasciitilde}environment.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Wenbo and Hu, Hongxin},
	month = oct,
	year = {2018},
	keywords = {internet of things, physical interaction control, safety},
	pages = {832--846},
}

@article{yu_tapinspector_2021,
	title = {{TAPInspector}: {Safety} and {Liveness} {Verification} of {Concurrent} {Trigger}-{Action} {IoT} {Systems}},
	shorttitle = {{TAPInspector}},
	url = {http://arxiv.org/abs/2102.01468},
	abstract = {Trigger-action programming (TAP) is a popular end-user programming framework that can simplify the Internet of Things (IoT) automation with simple trigger-action rules. However, it also introduces new security and safety threats. A lot of advanced techniques have been proposed to address this problem. Rigorously reasoning about the security of a TAP-based IoT system requires a well-defined model and verification method both against rule semantics and physical-world states, e.g., concurrency, rule latency, and connection-based interactions, which has been missing until now. This paper presents TAPInspector, a novel system to detect vulnerabilities in concurrent TAP-based IoT systems using model checking. It automatically extracts TAP rules from IoT apps, translates them into a hybrid model with model slicing and state compression, and performs model checking with various safety and liveness properties. Our experiments corroborate that TAPInspector is effective: it identifies 533 violations with 9 new types of violations from 1108 real-world market IoT apps and is 60000 times faster than the baseline without optimization at least.},
	urldate = {2022-05-06},
	journal = {arXiv:2102.01468 [cs]},
	author = {Yu, Yinbo and Liu, Jiajia},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.01468},
	keywords = {C.2.0, Computer Science - Cryptography and Security, Computer Science - Human-Computer Interaction, Computer Science - Networking and Internet Architecture, Computer Science - Software Engineering, D.2.4, F.3.1},
}

@article{ozmen_discovering_2021,
	title = {Discovering {Physical} {Interaction} {Vulnerabilities} in {IoT} {Deployments}},
	url = {http://arxiv.org/abs/2102.01812},
	abstract = {Internet of Things (IoT) applications drive the behavior of IoT deployments according to installed sensors and actuators. It has recently been shown that IoT deployments are vulnerable to physical interactions, caused by design flaws or malicious intent, that can have severe physical consequences. Yet, extant approaches to securing IoT do not translate the app source code into its physical behavior to evaluate physical interactions. Thus, IoT consumers and markets do not possess the capability to assess the safety and security risks these interactions present. In this paper, we introduce the IoTSeer security service for IoT deployments, which uncovers undesired states caused by physical interactions. IoTSeer operates in four phases (1) translation of each actuation command and sensor event in an app source code into a hybrid I/O automaton that defines an app's physical behavior, (2) combining apps in a novel composite automaton that represents the joint physical behavior of interacting apps, (3) applying grid-based testing and falsification to validate whether an IoT deployment conforms to desired physical interaction policies, and (4) identification of the root cause of policy violations and proposing patches that guide users to prevent them. We use IoTSeer in an actual house with 13 actuators and six sensors with 37 apps and demonstrate its effectiveness and performance.},
	urldate = {2022-05-06},
	journal = {arXiv:2102.01812 [cs]},
	author = {Ozmen, Muslum Ozgur and Li, Xuansong and Chu, Andrew Chun-An and Celik, Z. Berkay and Hoxha, Bardh and Zhang, Xiangyu},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.01812},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Formal Languages and Automata Theory},
}

@incollection{stevens_comparing_2020,
	address = {New York, NY, USA},
	title = {Comparing formal models of {IoT} app coordination analysis},
	isbn = {978-1-4503-8126-0},
	url = {https://doi.org/10.1145/3416507.3423188},
	abstract = {The rising popularity of the Internet-of-Things (IoT) devices has driven their increasing adoption in various settings, such as modern homes. IoT systems integrate such physical devices with third-party apps, which can coordinate in arbitrary ways. However, malicious or undesired coordination can lead to serious vulnerabilities. This paper explores two different ways, i.e., a commonly-used state-based approach and a holistic, rule-based approach, to formally model app coordination and the safety and security thereof in the context of IoT platforms. The less common rule-base approach allows for a smaller, more scalable model. We realize both modeling approaches using bounded model checking with Alloy to automatically identify potential cases where apps exhibit coordination relationships. We evaluate the effectiveness of the modeling approaches by checking a corpus of real-world IoT apps of Samsung SmartThings and IFTTT. The experimental results demonstrate that our rule-based modeling leads to a more scalable analysis.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the 3rd {ACM} {SIGSOFT} {International} {Workshop} on {Software} {Security} from {Design} to {Deployment}},
	publisher = {Association for Computing Machinery},
	author = {Stevens, Clay and Alhanahnah, Mohannad and Yan, Qiben and Bagheri, Hamid},
	month = nov,
	year = {2020},
	keywords = {Coordination Threats, Formal Verification, IoT Safety},
	pages = {3--10},
}

@inproceedings{chi_cross-app_2020,
	title = {Cross-{App} {Interference} {Threats} in {Smart} {Homes}: {Categorization}, {Detection} and {Handling}},
	shorttitle = {Cross-{App} {Interference} {Threats} in {Smart} {Homes}},
	doi = {10.1109/DSN48063.2020.00056},
	abstract = {Internet of Thing platforms prosper home automation applications (apps). Prior research concerns intra-app security. Our work reveals that automation apps, even secured individually, still cause a family of threats when they interplay, termed as Cross-App Interference (CAI) threats. We systematically categorize such threats and encode them using satisfiability modulo theories (SMT). We present HomeGuard, a system for detecting and handling CAI threats in real deployments. A symbolic executor is built to extract rule semantics, and instrumentation is utilized to capture configuration during app installation. Rules and configuration are checked against SMT models, the solutions of which indicate the existence of corresponding CAI threats. We further combine app functionalities, device attributes and CAI types to label the risk level of CAI instances. In our evaluation, HomeGuard discovers 663 CAI instances from 146 SmartThings market apps, imposing minor latency upon app installation and no runtime overhead.},
	booktitle = {2020 50th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	author = {Chi, Haotian and Zeng, Qiang and Du, Xiaojiang and Yu, Jiaping},
	month = jun,
	year = {2020},
	note = {ISSN: 1530-0889},
	keywords = {Actuators, Automation, Safety, Semantics, Smart homes, Temperature sensors},
	pages = {411--423},
}

@inproceedings{jin_kang_iotbox_2021,
	title = {{IoTBox}: {Sandbox} {Mining} to {Prevent} {Interaction} {Threats} in {IoT} {Systems}},
	shorttitle = {{IoTBox}},
	doi = {10.1109/ICST49551.2021.00029},
	abstract = {Internet of Things (IoT) apps provide great convenience but exposes us to new safety threats. Unlike traditional software systems, threats may emerge from the joint behavior of multiple apps. While prior studies use handcrafted safety and security policies to detect these threats, these policies may not anticipate all usages of the devices and apps in a smart home, causing false alarms. In this study, we propose to use the technique of mining sandboxes for securing an IoT environment. After a set of behaviors are analyzed from a bundle of apps and devices, a sandbox is deployed, which enforces that previously unseen behaviors are disallowed. Hence, the execution of malicious behavior, introduced from software updates or obscured through methods to hinder program analysis, is blocked.While sandbox mining techniques have been proposed for Android apps, we show and discuss why they are insufficient for detecting malicious behavior in a more complex IoT system. We prototype IoTBox to address these limitations. IoTBox explores behavior through a formal model of a smart home. In our empirical evaluation to detect malicious code changes, we find that IoTBox achieves substantially higher precision and recall compared to existing techniques for mining sandboxes.},
	booktitle = {2021 14th {IEEE} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Jin Kang, Hong and Qin Sim, Sheng and Lo, David},
	month = apr,
	year = {2021},
	note = {ISSN: 2159-4848},
	keywords = {Conferences, Malware, Prototypes, Safety, Smart homes, Software systems, Software testing, interaction threats, internet of things, mining sandboxes},
	pages = {182--193},
}

@inproceedings{ma_cityguard_2017,
	address = {New York, NY, USA},
	series = {{IoTDI} '17},
	title = {{CityGuard}: {A} {Watchdog} for {Safety}-{Aware} {Conflict} {Detection} in {Smart} {Cities}},
	isbn = {978-1-4503-4966-6},
	shorttitle = {{CityGuard}},
	url = {https://doi.org/10.1145/3054977.3054989},
	doi = {10.1145/3054977.3054989},
	abstract = {Nowadays, increasing number of smart services are being developed and deployed in cities around the world. IoT platforms have emerged to integrate smart city services and city resources, and thus improve city performance in the domains of transportation, emergency, environment, public safety, etc. Despite the increasing intelligence of smart services and the sophistication of platforms, the safety issues in smart cities are not addressed adequately, especially the safety issues arising from the integration of smart services. Therefore, CityGuard, a safety-aware watchdog architecture is developed. To the best of our knowledge, it is the first architecture that detects and resolves conflicts among actions of different services considering both safety and performance requirements. Prior to developing CityGuard, safety and performance requirements and a spectrum of conflicts are specified. Sophisticated models are used to analyze secondary effects, and detect device and environmental conflicts. A simulation based on New York City is used for the evaluation. The results show that CityGuard (i) identifies unsafe actions and thus helps to prevent the city from safety hazards, (ii) detects and resolves two major types of conflicts, i.e., device and environmental conflicts, and (iii) improves the overall city performance.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
	month = apr,
	year = {2017},
	keywords = {City Safety, City Simulation, Conflict Detection, Smart City},
	pages = {259--270},
}

@misc{international_organization_for_standardization_iso_2018,
	title = {{ISO} 26262-1:2018},
	shorttitle = {{ISO} 26262-1},
	url = {https://www.iso.org/standard/68383.html},
	abstract = {This document is intended to be applied to safety-related systems that include one or more electrical and/or electronic (E/E) systems and that are installed in series production road vehicles, excluding mopeds. This document does not address unique E/E systems in special vehicles such as E/E systems designed for drivers with disabilities.

NOTE Other dedicated application-specific safety standards exist and can complement the ISO 26262 series of standards or vice versa.

Systems and their components released for production, or systems and their components already under development prior to the publication date of this document, are exempted from the scope of this edition. This document addresses alterations to existing systems and their components released for production prior to the publication of this document by tailoring the safety lifecycle depending on the alteration. This document addresses integration of existing systems not developed according to this document and systems developed according to this document by tailoring the safety lifecycle.

This document addresses possible hazards caused by malfunctioning behaviour of safety-related E/E systems, including interaction of these systems. It does not address hazards related to electric shock, fire, smoke, heat, radiation, toxicity, flammability, reactivity, corrosion, release of energy and similar hazards, unless directly caused by malfunctioning behaviour of safety-related E/E systems.

This document describes a framework for functional safety to assist the development of safety-related E/E systems. This framework is intended to be used to integrate functional safety activities into a company-specific development framework. Some requirements have a clear technical focus to implement functional safety into a product; others address the development process and can therefore be seen as process requirements in order to demonstrate the capability of an organization with respect to functional safety.

This document defines the vocabulary of terms used in the ISO 26262 series of standards.},
	language = {en},
	urldate = {2022-05-04},
	journal = {ISO},
	author = {{International Organization for Standardization}},
	month = dec,
	year = {2018},
}

@article{wong_trust_2014,
	title = {Trust and {Privacy} {Exploitation} in {Online} {Social} {Networks}},
	volume = {16},
	issn = {1941-045X},
	doi = {10.1109/MITP.2014.79},
	abstract = {Online social networks have been typically created for convenience–so they haven't been built from the ground up with security in mind. They often have confusing privacy settings and are susceptible to various kinds of attacks that exploit users' trust and privacy. In this article, the authors discuss security pitfalls in today's social networks, briefly introducing common attack methods. They implemented a proof-of-concept Facebook app, which is actually a harmless malware that uses common attack methods to demonstrate the vulnerability of online social networks. Although today's online social networks commonly offer users a variety of security settings, users tend to trust the information obtained from online social networks regardless of the settings. This kind of user mentality can be more crucial than technical aspects in determining the level of security in online social networks.},
	number = {5},
	journal = {IT Professional},
	author = {Wong, Kaze and Wong, Angus and Yeung, Alan and Fan, Wei and Tang, Su-Kit},
	month = sep,
	year = {2014},
	note = {Conference Name: IT Professional},
	keywords = {Computer security, Facebook, Games, Information networks, Internet/Web technologies, Malware, Mobile communication, Privacy, Social network services, Trust management, Web and Internet services, information network, mobile, networking, privacy, security},
	pages = {28--33},
}

@article{tang_decision-making_2021,
	title = {Decision-{Making} {Principles} for {Better} {Software} {Design} {Decisions}},
	volume = {38},
	issn = {1937-4194},
	doi = {10.1109/MS.2021.3102358},
	abstract = {Software design is about making decisions. The quality of design decisions influences the quality of software design. This article describes nine decision-making principles to give software designers a systematic approach for decision making.},
	number = {6},
	journal = {IEEE Software},
	author = {Tang, Antony and Kazman, Rick},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Software},
	keywords = {Decision making, Software design, Software development management, Systematics},
	pages = {98--102},
}

@article{fatima_improving_2019,
	title = {Improving {Software} {Requirements} {Reasoning} by {Novices}: {A} {Story}-{Based} {Approach}},
	shorttitle = {Improving {Software} {Requirements} {Reasoning} by {Novices}},
	doi = {10.1049/iet-sen.2018.5379},
	abstract = {Context: Requirements-elicitation is one of the essential steps towards software design and construction. Business analysts and stakeholders often face challenges in gathering or conveying key software requirements. There are many methods, and tools designed by researchers and practitioners, but with the invention of new technologies, there appears to be a need to make requirements gathering and design-rationale process more efficient. Storytelling is an emerging concept and researchers are witnessing its effectiveness in education, community-building, information system, and requirement elicitation. 

Objective: Objectives of this study are to i) devise a method for requirements elicitation and improving design-rationales using story-based technique; ii) evaluate the effectiveness of the aforementioned proposed activity.

Methodology: To answer the research objectives, we have i) conducted open-ended interviews to get feedback on our proposed method; ii) case requirement from a running project to map how this method can be useful; and iii) performed empirical evaluation of the proposed card-based activity.

Result: i) Our regression model has shown that participants' perception regarding the ease of use and the fun in the game has an ultimate effect on requirements elicitation through enhancing user's desire to play the game, hence, increasing the collaborative learning outcomes of the game; ii) Our results have shown that using team-based activities helps the less-experienced designers to argue through design rationales and better elicit software requirements. Our results have reinforced the finding that using game-based solutions not only enhances communication and develops trust between stakeholders but also helps in motivating participants of requirements activity; iii) Initial results (from interview and empirical evaluation) for the proposed technique and method show positive results. Improvement in the process and activity as suggested by the participants will be accommodated in future studies.},
	journal = {IET Software},
	author = {Fatima, Rubia and Yasin Chouhan, Affan and Liu, Lin and Wang, Jianmin and Afzal, Wasif and Yasin, Atif},
	month = jul,
	year = {2019},
}

@article{lee_design_1997,
	title = {Design rationale systems: understanding the issues},
	volume = {12},
	issn = {2374-9407},
	shorttitle = {Design rationale systems},
	doi = {10.1109/64.592267},
	abstract = {Most current design rationale systems fail to consider practical concerns, such as cost-effective use and smooth integration. The author identifies seven technical and business issues and describes their implications.},
	number = {3},
	journal = {IEEE Expert},
	author = {Lee, J.},
	month = may,
	year = {1997},
	note = {Conference Name: IEEE Expert},
	keywords = {Collaboration, Collaborative tools, Costs, Decision making, Design engineering, Documentation, Problem-solving, Project management, Tail, Writing},
	pages = {78--85},
}

@article{dorst_creativity_2001,
	title = {Creativity in the design process: co-evolution of problem–solution},
	volume = {22},
	issn = {0142-694X},
	shorttitle = {Creativity in the design process},
	url = {https://www.sciencedirect.com/science/article/pii/S0142694X01000096},
	doi = {10.1016/S0142-694X(01)00009-6},
	abstract = {Empirical data on design processes were obtained from a set of protocol studies of nine experienced industrial designers, whose designs were evaluated on overall quality and on a variety of aspects including creativity. From the protocol data we identify aspects of creativity in design related to the formulation of the design problem and to the concept of originality. We also apply our observations to a model of creative design as the co-evolution of problem/solution spaces, and confirm the general validity of the model. We propose refinements to the co-evolution model, and suggest relevant new concepts of ‘default’ and ‘surprise’ problem/solution spaces.},
	language = {en},
	number = {5},
	urldate = {2022-05-04},
	journal = {Design Studies},
	author = {Dorst, Kees and Cross, Nigel},
	month = sep,
	year = {2001},
	keywords = {co-evolution, creative design, design process, product design},
	pages = {425--437},
}

@article{falessi_design_2006,
	title = {Design decision rationale: experiences and steps ahead towards systematic use},
	volume = {31},
	issn = {0163-5948},
	shorttitle = {Design decision rationale},
	url = {https://dl.acm.org/doi/10.1145/1163514.1178642},
	doi = {10.1145/1163514.1178642},
	abstract = {Design decisions crucially influence the success of every software project. While the resulting design is typically documented quite well, the situation is usually different for the underlying rationale and decision-making process. Despite being recognized as a helpful approach in general, the explicit documentation of Design Decision Rationale (DDR) is not yet largely utilized due to some inhibitors (e.g., additional documentation effort). Experience with other qualities, e.g. software reusability, evidently shows that an improvement of these qualities only pays off on a large scale and therefore has to be pursued in a strategic, pre-planned, and carefully focused way. In this paper we argue that this also has to be considered for documenting DDR. To this end the paper presents: (i) the Decision, Goal, and Alternatives (DGA) DDR framework, (ii) experience in dealing with DGA, (iii) motivators and inhibitors of using DDR, and (iv) an approach for systematic DDR use that follows value-based software engineering principles.},
	language = {en},
	number = {5},
	urldate = {2022-05-04},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Falessi, Davide and Becker, Martin and Cantone, Giovanni},
	month = sep,
	year = {2006},
	pages = {2},
}

@inproceedings{jarczyk_design_1992,
	title = {Design {Rationale} for {Software} {Engineering}: {A} {Survey}},
	shorttitle = {Design {Rationale} for {Software} {Engineering}},
	abstract = {We provide an introduction to what design rationale is and why it is important in software engineering. We look at the recent history of argumentation methods. We survey a number of the major systems developed for the support of design rationale, comparing their features and discussing their differences. We look at advantages and disadvantages of the various approaches to design rationale with special attention paid to how they can be used in the process of software engineering. We conclude with a discussion of some open issues which are important for the inclusion of},
	publisher = {Press},
	author = {Jarczyk, Alex P. J. and Löffler, Peter and Iii, Frank M. Shipman},
	year = {1992},
	pages = {577--586},
}

@inproceedings{razavian_reflective_2016,
	title = {Reflective {Approach} for {Software} {Design} {Decision} {Making}},
	doi = {10.1109/QRASA.2016.8},
	abstract = {Good software design practice is difficult to define and teach. Despite the many software design methods and processes that are available, the quality of software design relies on human factors. We notice from literature and our own experiments that some of these factors concern design reasoning and reflection. In this paper, we propose a reflective approach to software design decision making. The approach is built upon Two-Minds model and is enabled by a set of problem-generic reflective questions. We illustrate its usefulness in design sessions with an example taken from preliminary experimentation.},
	booktitle = {2016 {Qualitative} {Reasoning} about {Software} {Architectures} ({QRASA})},
	author = {Razavian, Maryam and Tang, Antony and Capilla, Rafael and Lago, Patricia},
	month = apr,
	year = {2016},
	keywords = {Cognition, Context, Decision making, Object oriented modeling, Problem-solving, Software design},
	pages = {19--26},
}

@article{tang_improving_2018,
	title = {Improving software design reasoning–{A} reminder card approach},
	volume = {144},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121218301043},
	doi = {10.1016/j.jss.2018.05.019},
	abstract = {Software designers have been known to think naturalistically. This means that there may be inadequate rational thinking during software design. In the past two decades, many research works suggested that designers need to produce design rationale. However, design rationale can be produced to retrofit naturalistic decisions, which means that design decisions may still not be well reasoned. Through a controlled experiment, we studied design reasoning and design rationale by asking participants to carry out a group design. As treatment, we provided 6 out of 12 student teams with a set of reasoning reminder cards to see how they compare with teams without the reminder cards. Additionally, we performed the same experiment with 2 teams of professionals who used the reminder cards, and compared the results with 3 teams of professionals. The experimental results show that both professionals and students who were equipped with the reasoning reminder cards reasoned more with their design. Second, the more a team discusses design reasoning, the more design rationale they find.},
	language = {en},
	urldate = {2022-05-04},
	journal = {Journal of Systems and Software},
	author = {Tang, Antony and Bex, Floris and Schriek, Courtney and van der Werf, Jan Martijn E. M.},
	month = oct,
	year = {2018},
	pages = {22--40},
}

@misc{github_safety_2018,
	title = {[{Safety}] {Ability} to block everyone following a certain account},
	url = {https://github.com/mastodon/mastodon/issues/8238},
	abstract = {Use case: find the main dickheads in a toxic group and block all of their crew in one swift move. Source I searched or browsed the repo’s other issues to ensure this is not a duplicate.},
	language = {en},
	urldate = {2022-05-04},
	journal = {mastodon/mastodon},
	author = {{GitHub}},
	month = aug,
	year = {2018},
	note = {Issue \#8238},
}

@misc{international_electrotechnical_commission_iec_2010,
	title = {{IEC} 61508-1:2010 {\textbar} {IEC} {Webstore} {\textbar} functional safety, smart city},
	url = {https://webstore.iec.ch/publication/5515},
	abstract = {IEC 61508-1:2010 covers those aspects to be considered when electrical/electronic/programmable electronic (E/E/PE) systems are used to carry out safety functions. A major objective of this standard is to facilitate the development of product and application sector international standards by the technical committees responsible for the product or application sector. This will allow all the relevant factors, associated with the product or application, to be fully taken into account and thereby meet the specific needs of users of the product and the application sector. A second objective of this standard is to enable the development of E/E/PE safety-related systems where product or application sector international standards do not exist. This second edition cancels and replaces the first edition published in 1998. This edition constitutes a technical revision. It has been subject to a thorough review and incorporates many comments received at the various revision stages. It has the status of a basic safety publication according to IEC Guide 104.},
	urldate = {2022-05-03},
	journal = {IEC Webstore},
	author = {{International Electrotechnical Commission}},
	month = apr,
	year = {2010},
}

@article{corbin1990grounded,
	title = {Grounded theory research: {Procedures}, canons, and evaluative criteria},
	volume = {13},
	number = {1},
	journal = {Qualitative sociology},
	author = {Corbin, Juliet M and Strauss, Anselm},
	year = {1990},
	note = {Publisher: Springer},
	pages = {3--21},
}

@article{zannier_model_2007,
	series = {Qualitative {Software} {Engineering} {Research}},
	title = {A model of design decision making based on empirical results of interviews with software designers},
	volume = {49},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584907000122},
	doi = {10.1016/j.infsof.2007.02.010},
	abstract = {Despite the impact of design decisions on software design, we have little understanding about how design decisions are made. This hinders our ability to provide design metrics, processes and training that support inherent design work. By interviewing 25 software designers and using content analysis and explanation building as our analysis technique, we provide qualitative and quantitative results that highlight aspects of rational and naturalistic decision making in software design. Our qualitative multi-case study results in a model of design decision making to answer the question: how do software designers make design decisions? We find the structure of the design problem determines the aspects of rational and naturalistic decision making used. The more structured the design decision, the less a designer considers options.},
	language = {en},
	number = {6},
	urldate = {2022-05-03},
	journal = {Information and Software Technology},
	author = {Zannier, Carmen and Chiasson, Mike and Maurer, Frank},
	month = jun,
	year = {2007},
	keywords = {Design decision, Interviewing, Naturalistic decision making, Rational decision making},
	pages = {637--653},
}

@inproceedings{zannier_qualitative_2005,
	title = {A qualitative empirical evaluation of design decisions},
	abstract = {In this paper, we motivate examining software design decision making and provide the process by which the examination will occur. The objective is to provide qualitative results indicative of rational or naturalistic software design decision making. In a rational decision a decision maker evaluates decision alternatives and potential outcomes for each alternative using a utility function and probabilities of the outcome of each alternative. The utility function assigns a value to each possible alternative based on its outcome. The goal of rational decision making is selecting the optimal alternative. A naturalistic decision manifests itself in dynamic and continually changing conditions, embodies real-time reactions to these changes, embraces ill-defined tasks, and has a goal of selecting a satisfactory alternative. The proposed empirical qualitative study consists of inductive and deductive interviewing and deductive observations.},
	booktitle = {in: {Workshop} on {Human} \& {Social} {Factors} of {Soft}. {Eng}},
	publisher = {ACM Press},
	author = {Zannier, Carmen and Maurer, Frank},
	year = {2005},
}

@article{christiaans_accessing_2010,
	series = {Special {Issue} {Studying} {Professional} {Software} {Design}},
	title = {Accessing decision-making in software design},
	volume = {31},
	issn = {0142-694X},
	url = {https://www.sciencedirect.com/science/article/pii/S0142694X10000670},
	doi = {10.1016/j.destud.2010.09.005},
	abstract = {This paper presents an analysis of software design protocols as one of the contributions to the 2010 international workshop ‘Studying Professional Software Design’. The aim of the study described here is to analyse the design process of software designers and to compare the results with that of product designers, an area familiar to the authors. Decision-making is the main focus of this study. A descriptive model of decision-making, developed by the authors, has been used to analyse the protocols of the three software design teams. The results give insight in how software designers process their activities, on the influence of individual or team differences, and what the consequences for their outcomes are.},
	language = {en},
	number = {6},
	urldate = {2022-05-03},
	journal = {Design Studies},
	author = {Christiaans, Henri and Almendra, Rita Assoreira},
	month = nov,
	year = {2010},
	keywords = {decision-making, design behaviour, design cognition, design process},
	pages = {641--662},
}

@article{tang_what_2010,
	series = {Special {Issue} {Studying} {Professional} {Software} {Design}},
	title = {What makes software design effective?},
	volume = {31},
	issn = {0142-694X},
	url = {https://www.sciencedirect.com/science/article/pii/S0142694X10000669},
	doi = {10.1016/j.destud.2010.09.004},
	abstract = {Software design is a complex cognitive process in which decision making plays a major role, but our understanding of how decisions are made is limited, especially with regards to reasoning with design problems and formulation of design solutions. In this research, we have observed software designers at work and have analysed how they make decisions during design. We report on how factors such as design planning, design context switching, problem-solution co-evolution and the application of reasoning techniques influence software design effectiveness.},
	language = {en},
	number = {6},
	urldate = {2022-05-03},
	journal = {Design Studies},
	author = {Tang, Antony and Aleti, Aldeida and Burge, Janet and van Vliet, Hans},
	month = nov,
	year = {2010},
	keywords = {decision making, design effectiveness, design reasoning, software design},
	pages = {614--640},
}

@inproceedings{falessi_documenting_2006,
	address = {New York, NY, USA},
	series = {{ISESE} '06},
	title = {Documenting design decision rationale to improve individual and team design decision making: an experimental evaluation},
	isbn = {978-1-59593-218-1},
	shorttitle = {Documenting design decision rationale to improve individual and team design decision making},
	url = {https://doi.org/10.1145/1159733.1159755},
	doi = {10.1145/1159733.1159755},
	abstract = {Individual and team decision-making have crucial influence on the level of success of every software project. Even though several studies were already conducted, which concerned design decision rationale documentation approaches, a few of them focused on performances and evaluated them in laboratory. This paper proposes a technique to document design decision rationale, and evaluates experimentally the impact such a technique has on effectiveness and efficiency of individual/team decision-making in presence of requirement changes. The study was conducted as a controlled experiment. Fifty post-graduate Master students performed in the role of experiment subjects. Documented design decisions regarding the Ambient Intelligence paradigm constituted the experiment objects. Main results of the experiment show that, for both individual and team-based decision-making, effectiveness significantly improves, while efficiency remains unaltered, when decision-makers are allowed to use, rather not use, the proposed design rationale documentation technique.},
	urldate = {2022-05-03},
	booktitle = {Proceedings of the 2006 {ACM}/{IEEE} international symposium on {Empirical} software engineering},
	publisher = {Association for Computing Machinery},
	author = {Falessi, Davide and Cantone, Giovanni and Becker, Martin},
	month = sep,
	year = {2006},
	keywords = {design decision rationale, experimental evaluation, individual and team decision-making},
	pages = {134--143},
}

@article{boudette_tesla_2021,
	chapter = {Business},
	title = {Tesla {Says} {Autopilot} {Makes} {Its} {Cars} {Safer}. {Crash} {Victims} {Say} {It} {Kills}.},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2021/07/05/business/tesla-autopilot-lawsuits-safety.html},
	abstract = {A California family that lost a 15-year-old boy when a Tesla hit its pickup truck is suing the company, claiming its Autopilot system was partly responsible.},
	language = {en-US},
	urldate = {2022-05-02},
	journal = {The New York Times},
	author = {Boudette, Neal E.},
	month = jul,
	year = {2021},
	keywords = {Automobile Safety Features and Defects, Deaths (Fatalities), Driverless and Semiautonomous Vehicles, Electric and Hybrid Vehicles, Maldonado, Jovani (d 2019), Musk, Elon, National Highway Traffic Safety Administration, Suits and Litigation (Civil), Tesla Motors Inc, Traffic Accidents and Safety},
}

@article{noauthor_tesla_nodate,
	title = {Tesla {Says} {Autopilot} {Makes} {Its} {Cars} {Safer}. {Crash} {Victims} {Say} {It} {Kills}. - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/07/05/business/tesla-autopilot-lawsuits-safety.html},
	urldate = {2022-01-03},
}

@article{ozkan2020expectations,
	title = {Expectations and experiences of short-term study abroad leadership teams},
	volume = {2},
	number = {1},
	journal = {Journal of International Engineering Education},
	author = {Ozkan, Desen S and Davis, Kirsten A and Davis, James C and James, Matthew and Murzi, Homero and Knight, David B},
	year = {2020},
	pages = {1},
}

@misc{noauthor_software_2021,
	title = {Software {Bill} of {Materials} {Elements} and {Considerations}},
	url = {https://www.federalregister.gov/documents/2021/06/02/2021-11592/software-bill-of-materials-elements-and-considerations},
	abstract = {The Executive Order on Improving the Nation's Cybersecurity directs the Department of Commerce, in coordination with the National Telecommunications and Information Administration (NTIA), to publish the minimum elements for a Software Bill of Materials (SBOM). Through this Notice, following from...},
	urldate = {2022-04-29},
	journal = {Federal Register},
	month = jun,
	year = {2021},
}

@article{kayes_privacy_2017,
	title = {Privacy and security in online social networks: {A} survey},
	volume = {3-4},
	issn = {2468-6964},
	shorttitle = {Privacy and security in online social networks},
	url = {https://www.sciencedirect.com/science/article/pii/S2468696417300332},
	doi = {10.1016/j.osnem.2017.09.001},
	abstract = {Online social networks (OSN) are a permanent presence in today’s personal and professional lives of a huge segment of the population, with direct consequences to offline activities. Built on a foundation of trust – users connect to other users with common interests or overlapping personal trajectories – online social networks and the associated applications extract an unprecedented volume of personal information. Unsurprisingly, serious privacy and security risks emerged, positioning themselves along two main types of attacks: attacks that exploit the implicit trust embedded in declared social relationships; and attacks that harvest user’s personal information for ill-intended use. This article provides an overview of the privacy and security issues that emerged so far in OSNs. We introduce a taxonomy of privacy and security attacks in OSNs, we overview existing solutions to mitigate those attacks, and outline challenges still to overcome.},
	language = {en},
	urldate = {2022-04-28},
	journal = {Online Social Networks and Media},
	author = {Kayes, Imrul and Iamnitchi, Adriana},
	month = oct,
	year = {2017},
	keywords = {Online social networks, Privacy, Security},
	pages = {1--21},
}

@misc{noauthor_details_2019,
	title = {Details of the {Cloudflare} outage on {July} 2, 2019},
	url = {http://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/},
	abstract = {Almost nine years ago, Cloudflare was a tiny company and I was a customer not an employee. Cloudflare had launched a month earlier and one day alerting told me that my little site, jgc.org, didn’t seem to have working DNS any more.},
	language = {en},
	urldate = {2022-04-28},
	journal = {The Cloudflare Blog},
	month = jul,
	year = {2019},
}

@misc{noauthor_cloudflare_nodate,
	title = {Cloudflare, {Inc}. ({NET}) {Stock} {Price}, {News}, {Quote} \& {History} - {Yahoo} {Finance}},
	url = {https://finance.yahoo.com/quote/NET/},
	urldate = {2022-04-28},
}

@misc{noauthor_dexcom_nodate,
	title = {{DexCom}, {Inc}. ({DXCM}) {Stock} {Price}, {News}, {Quote} \& {History} - {Yahoo} {Finance}},
	url = {https://finance.yahoo.com/quote/DXCM/},
	abstract = {Find the latest DexCom, Inc. (DXCM) stock quote, history, news and other vital information to help you with your stock trading and investing.},
	language = {en-US},
	urldate = {2022-04-28},
}

@misc{noauthor_consultations_nodate,
	title = {Consultations - {Center} for {Instructional} {Excellence} - {Purdue} {University}},
	url = {https://www.purdue.edu/cie/teachingresources/consultations.html},
	urldate = {2022-04-26},
}

@inproceedings{marshall2014vertically,
	title = {The vertically integrated projects ({VIP}) program: leveraging faculty research interests to transform undergraduate {STEM} education},
	booktitle = {Transforming institutions: 21st century undergraduate {STEM} education conference},
	author = {Marshall, Stephen and Coyle, Edward and Krogmeier, James V and Abler, Randal T and Johnson, Amos and Gilchrist, Brian E},
	year = {2014},
}

@book{lamport2002specifying,
	title = {Specifying systems: {The} {TLA}+ language and tools for hardware and software engineers},
	volume = {388},
	publisher = {Addison-Wesley Boston},
	author = {Lamport, Leslie},
	year = {2002},
}

@misc{holzmann2004holzmann,
	title = {Holzmann. {The} {SPIN} model {Checker}—{Primer} and reference manual},
	publisher = {Addison-Wesley, Reading},
	author = {Holzmann, Gerard J},
	year = {2004},
}

@book{abrial2010modeling,
	title = {Modeling in {Event}-{B}: system and software engineering},
	publisher = {Cambridge University Press},
	author = {Abrial, Jean-Raymond},
	year = {2010},
}

@misc{noauthor_notitle_nodate,
	url = {https://www.acm.org/code-of-ethics},
}

@article{bail_social-media_2022,
	title = {Social-media reform is flying blind},
	volume = {603},
	copyright = {2022 Nature},
	url = {https://www.nature.com/articles/d41586-022-00805-0},
	doi = {10.1038/d41586-022-00805-0},
	abstract = {Redesigning social media to improve society requires a new platform for research.},
	language = {en},
	number = {7903},
	urldate = {2022-04-26},
	journal = {Nature},
	author = {Bail, Chris},
	month = mar,
	year = {2022},
	note = {Bandiera\_abtest: a
Cg\_type: World View
Number: 7903
Publisher: Nature Publishing Group
Subject\_term: Media, Communication, Society},
	keywords = {Communication, Media, Society},
	pages = {766--766},
}

@inproceedings{wang_privacy_2015,
	title = {Privacy threat modeling framework for online social networks},
	doi = {10.1109/CTS.2015.7210449},
	abstract = {Online social networks (OSNs) provide services for people to connect and share information. Social networking sites contain huge amount of personal information such as user profiles, user relations, and user activities. Most of the information is personal and sensitive in nature and hence disclosure of this information may cause harassment, financial loss, and even identity theft. Thus, protecting user privacy in online social networks is essential. Many threats and attacks have been found in social networks. However, there is lack of a threat model to study privacy issues in online social networks. This paper presents a privacy threat model for online social networks. The threat model includes four components, online social networking sites, third party service providers, genuine social network users, and malicious users. Threats and vulnerabilities are analyzed from six security aspects, i.e., hardware, operating systems, OSN privacy policies, user privacy settings, user relations, and user data. The paper further summarizes and analyzes the existing threats and attacks using the proposed model.},
	booktitle = {2015 {International} {Conference} on {Collaboration} {Technologies} and {Systems} ({CTS})},
	author = {Wang, Yong and Nepali, Raj Kumar},
	month = jun,
	year = {2015},
	keywords = {Data privacy, Facebook, Operating systems, Organizations, Privacy, Security, countermeasures, online social networks, privacy threat modeling, privacy threats and attacks},
	pages = {358--363},
}

@inproceedings{kumar_risk_2016,
	title = {Risk analysis of online social networks},
	doi = {10.1109/CCAA.2016.7813833},
	abstract = {Social networks are used as the communication network among the groups of people. In past few years several fields use the structure of social network and utilize it. But there are many threats and challenges associated with these networks. These threats disclose the personal information of the users and use that information for unauthorized and malicious purposes. One of the great revolutions in the field of online social networking is online health social network, which is a sub branch of ordinary social network. OHSNs are used for enhancement in the field of health care issues and improvise the better solution for those issues. In this paper we discuss security issues and privacy risk which harm the online social network user. In addition we survey the existing defense mechanism and also propose the better protection and security ideas for online social network users. Furthermore we advise future research directions.},
	booktitle = {2016 {International} {Conference} on {Computing}, {Communication} and {Automation} ({ICCCA})},
	author = {Kumar, Horesh and Jain, Shruti and Srivastava, Ritesh},
	month = apr,
	year = {2016},
	keywords = {Diseases, Facebook, Internet, Online Social Networks, Privacy, Security, Threats, Uniform resource locators},
	pages = {846--851},
}

@inproceedings{greschbach_devil_2012,
	title = {The devil is in the metadata — {New} privacy challenges in {Decentralised} {Online} {Social} {Networks}},
	doi = {10.1109/PerComW.2012.6197506},
	abstract = {Decentralised Online Social Networks (DOSN) are evolving as a promising approach to mitigate design-inherent privacy flaws of logically centralised services such as Facebook, Google+ or Twitter. A common approach to build a DOSN is to use a peer-to-peer architecture. While the absence of a single point of data aggregation strikes the most powerful attacker from the list of adversaries, the decentralisation also removes some privacy protection afforded by the central party's intermediation of all communication. As content storage, access right management, retrieval and other administrative tasks of the service become the obligation of the users, it is non-trivial to hide the metadata of objects and information flows, even when the content itself is encrypted. Such metadata is, deliberately or as a side effect, hidden by the provider in a centralised system. In this work, we aim to identify the dangers arising or made more severe from decentralisation, and show how inferences from metadata might invade users' privacy. Furthermore, we discuss general techniques to mitigate or solve the identified issues.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops}},
	author = {Greschbach, Benjamin and Kreitz, Gunnar and Buchegger, Sonja},
	month = mar,
	year = {2012},
	keywords = {Access control, Data privacy, Encryption, Peer to peer computing, Privacy, Social network services},
	pages = {333--339},
}

@article{hasib_threats_2009,
	title = {Threats of online social networks},
	abstract = {In the recent years, we have witnessed a dramatic rise in popularity of online social networking services, with several Social Network Sites (SNSs) such as Myspace, Facebook, Blogger, You Tube, Yahoo! Groups etc are now among the most visited websites globally. However, since such forums are relatively easy to access and the users are often not aware of the size and the nature of the audience accessing their profiles, they often reveal more information which is not appropriate to a public forum. As a result, such commercial and social site may often generate a number of privacy and security related threats for the members. This paper highlights the commercial and social benefits of safe and well-informed use of SNSs and emphasizes the most important threats to users of SNSs as well as illustrates the fundamental factors behind these threats. It also presents policy and technical recommendations to improve privacy and security without compromising the benefits of information sharing through SNSs. Key words:},
	journal = {International Journal of Computer Science and Network Security},
	author = {Hasib, Abdullah Al},
	year = {2009},
	pages = {288--293},
}

@article{rathore_social_2017,
	title = {Social network security: {Issues}, challenges, threats, and solutions},
	volume = {421},
	issn = {0020-0255},
	shorttitle = {Social network security},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517309106},
	doi = {10.1016/j.ins.2017.08.063},
	abstract = {Social networks are very popular in today's world. Millions of people use various forms of social networks as they allow individuals to connect with friends and family, and share private information. However, issues related to maintaining the privacy and security of a user's information can occur, especially when the user's uploaded content is multimedia, such as photos, videos, and audios. Uploaded multimedia content carries information that can be transmitted virally and almost instantaneously within a social networking site and beyond. In this paper, we present a comprehensive survey of different security and privacy threats that target every user of social networking sites. In addition, we separately focus on various threats that arise due to the sharing of multimedia content within a social networking site. We also discuss current state-of- the-art defense solutions that can protect social network users from these threats. We then present future direction and discuss some easy-to-apply response techniques to achieve the goal of a trustworthy and secure social network ecosystem.},
	language = {en},
	urldate = {2022-04-23},
	journal = {Information Sciences},
	author = {Rathore, Shailendra and Sharma, Pradip Kumar and Loia, Vincenzo and Jeong, Young-Sik and Park, Jong Hyuk},
	month = dec,
	year = {2017},
	keywords = {Multimedia data, Security and privacy, Security threats, Social network service},
	pages = {43--69},
}

@article{fire_online_2014,
	title = {Online {Social} {Networks}: {Threats} and {Solutions}},
	volume = {16},
	issn = {1553-877X},
	shorttitle = {Online {Social} {Networks}},
	doi = {10.1109/COMST.2014.2321628},
	abstract = {Many online social network (OSN) users are unaware of the numerous security risks that exist in these networks, including privacy violations, identity theft, and sexual harassment, just to name a few. According to recent studies, OSN users readily expose personal and private details about themselves, such as relationship status, date of birth, school name, email address, phone number, and even home address. This information, if put into the wrong hands, can be used to harm users both in the virtual world and in the real world. These risks become even more severe when the users are children. In this paper, we present a thorough review of the different security and privacy risks, which threaten the well-being of OSN users in general, and children in particular. In addition, we present an overview of existing solutions that can provide better protection, security, and privacy for OSN users. We also offer simple-to-implement recommendations for OSN users, which can improve their security and privacy when using these platforms. Furthermore, we suggest future research directions.},
	number = {4},
	journal = {IEEE Communications Surveys Tutorials},
	author = {Fire, Michael and Goldschmidt, Roy and Elovici, Yuval},
	year = {2014},
	note = {Conference Name: IEEE Communications Surveys Tutorials},
	keywords = {Computer security, Facebook, Online services, Online social networks, Privacy, Social network services, Twitter, online social network security solutions, online social network security threats, security and privacy},
	pages = {2019--2036},
}

@article{wang_systematic_2019,
	title = {Systematic {Literature} {Review} on the {Spread} of {Health}-related {Misinformation} on {Social} {Media}},
	volume = {240},
	issn = {0277-9536},
	url = {https://www.sciencedirect.com/science/article/pii/S0277953619305465},
	doi = {10.1016/j.socscimed.2019.112552},
	abstract = {Contemporary commentators describe the current period as “an era of fake news” in which misinformation, generated intentionally or unintentionally, spreads rapidly. Although affecting all areas of life, it poses particular problems in the health arena, where it can delay or prevent effective care, in some cases threatening the lives of individuals. While examples of the rapid spread of misinformation date back to the earliest days of scientific medicine, the internet, by allowing instantaneous communication and powerful amplification has brought about a quantum change. In democracies where ideas compete in the marketplace for attention, accurate scientific information, which may be difficult to comprehend and even dull, is easily crowded out by sensationalized news. In order to uncover the current evidence and better understand the mechanism of misinformation spread, we report a systematic review of the nature and potential drivers of health-related misinformation. We searched PubMed, Cochrane, Web of Science, Scopus and Google databases to identify relevant methodological and empirical articles published between 2012 and 2018. A total of 57 articles were included for full-text analysis. Overall, we observe an increasing trend in published articles on health-related misinformation and the role of social media in its propagation. The most extensively studied topics involving misinformation relate to vaccination, Ebola and Zika Virus, although others, such as nutrition, cancer, fluoridation of water and smoking also featured. Studies adopted theoretical frameworks from psychology and network science, while co-citation analysis revealed potential for greater collaboration across fields. Most studies employed content analysis, social network analysis or experiments, drawing on disparate disciplinary paradigms. Future research should examine susceptibility of different sociodemographic groups to misinformation and understand the role of belief systems on the intention to spread misinformation. Further interdisciplinary research is also warranted to identify effective and tailored interventions to counter the spread of health-related misinformation online.},
	language = {en},
	urldate = {2022-04-22},
	journal = {Social Science \& Medicine},
	author = {Wang, Yuxi and McKee, Martin and Torbica, Aleksandra and Stuckler, David},
	month = nov,
	year = {2019},
	keywords = {Fake news, Health, Misinformation, Social media},
	pages = {112552},
}

@book{reddy2004investigation,
	title = {Investigation of aeronautical and engineering component failures},
	publisher = {CRC Press},
	author = {Reddy, A Venugopal},
	year = {2004},
}

@book{makhlouf2015handbook,
	title = {Handbook of materials failure analysis with case studies from the chemicals, concrete and power industries},
	publisher = {Butterworth-Heinemann},
	author = {Makhlouf, Abdel Salam Hamdy and Aliofkhazraei, Mahmood},
	year = {2015},
}

@book{kolb2014experiential,
	title = {Experiential learning: {Experience} as the source of learning and development},
	publisher = {FT press},
	author = {Kolb, David A},
	year = {2014},
}

@inproceedings{davis_experience_2022,
	title = {Experience {Paper}: {A} {First} {Offering} of {Software} {Engineering}},
	abstract = {This paper describes our first offering of a project-based software engineering course for undergraduate seniors. The course was given to 72 undergraduates, mostly seniors majoring in computer engineering. Our project taught the full engineering cycle with a narrative based on supporting the re-use of software. In two parts spanning 13 weeks, successful teams deployed a web service. We identify lessons learned and opportunities for improvement.},
	language = {en},
	booktitle = {Proceedings of {The} {First} {International} {Workshop} on {Designing} and {Running} {Project}-{Based} {Courses} in {Software} {Engineering} {Education} ({ICSE}-{DREE})},
	author = {Davis, James C and Amusuo, Paschal and Bushagour, Joseph R},
	year = {2022},
	pages = {5},
}

@inproceedings{vasudeva_varma_case_2005,
	address = {Melbourne, Victoria, Australia},
	title = {Case studies: the potential teaching instruments for software engineering education},
	isbn = {978-0-7695-2472-6},
	shorttitle = {Case studies},
	url = {https://ieeexplore.ieee.org/document/1579146/},
	doi = {10.1109/QSIC.2005.18},
	abstract = {The current approaches to the Software Engineering Education fall short to fulfill the industry demand for quality software engineering. A constant need to create and imbibe more effective learning environments is growing in order to manage this demand. This paper discusses the learning disabilities possessed by both the conventional and the non-conventional approaches for teaching Software Engineering. We propose that case studies can be used as effective teaching mediums and a case study centric learning environment can address these learning disabilities. A case study approach can help the students to gain and retain realistic exposure to concepts of Software Engineering as they are applied in the real world, and the students of today can be groomed as excellent professionals who have experienced the intricacies and complexities of the real world as well as tried their hands to manage these complexities.},
	language = {en},
	urldate = {2022-04-22},
	booktitle = {Fifth {International} {Conference} on {Quality} {Software} ({QSIC}'05)},
	publisher = {IEEE},
	author = {{Vasudeva Varma} and {Kirti Garg}},
	year = {2005},
	pages = {279--284},
}

@inproceedings{garg_study_2007,
	address = {Dublin, Ireland},
	title = {A {Study} of the {Effectiveness} of {Case} {Study} {Approach} in {Software} {Engineering} {Education}},
	isbn = {978-0-7695-2893-9},
	url = {http://ieeexplore.ieee.org/document/4271619/},
	doi = {10.1109/CSEET.2007.8},
	abstract = {Software Engineering (SE) educators have been advocating the use of non-conventional approaches for SE education since long. In this context, we conducted action-research to compare the effectiveness of a case study approach with conventional lecture based approach.},
	language = {en},
	urldate = {2022-04-22},
	booktitle = {20th {Conference} on {Software} {Engineering} {Education} \& {Training} ({CSEET}'07)},
	publisher = {IEEE},
	author = {Garg, Kirti and Varma, Vasudeva},
	month = jul,
	year = {2007},
	note = {ISSN: 1093-0175},
	pages = {309--316},
}

@inproceedings{wohlin_achieving_1999,
	address = {New Orleans, LA, USA},
	title = {Achieving industrial relevance in software engineering education},
	isbn = {978-0-7695-0131-4},
	url = {http://ieeexplore.ieee.org/document/755175/},
	doi = {10.1109/CSEE.1999.755175},
	abstract = {This paper presents a collection of experiences related to success factors in graduate and postgraduate education. The experiences are mostly concerned with how to make the education relevant from an industrial viewpoint. This is emphasized as a key issue in software engineering education and research, since the main objective is to give the students a good basis for largescale software development in an industrial environment. The presentation is divided into experiences at the graduate and postgraduate levels respectively. For each level a number of strategies to achieve industrial relevance are presented. These strategies have been successful, but it is concluded that more can be done regarding industrial collaboration in the planning and conduction of experiments and case studies. Another interesting strategy for the future is a special postgraduate programme for people employed in industry.},
	language = {en},
	urldate = {2022-04-22},
	booktitle = {Proceedings 12th {Conference} on {Software} {Engineering} {Education} and {Training} ({Cat}. {No}.{PR00131})},
	publisher = {IEEE},
	author = {Wohlin, C. and Regnell, B.},
	year = {1999},
	pages = {16--25},
}

@book{schon1987educating,
	title = {Educating the reflective practitioner: {Toward} a new design for teaching and learning in the professions.},
	publisher = {Jossey-Bass},
	author = {Schön, Donald A},
	year = {1987},
}

@article{newport_when_2020,
	title = {When technology goes awry},
	volume = {63},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3391975},
	doi = {10.1145/3391975},
	abstract = {On engineers' obligation to tame their creations.},
	number = {5},
	urldate = {2022-04-22},
	journal = {Communications of the ACM},
	author = {Newport, Cal},
	month = apr,
	year = {2020},
	pages = {49--52},
}

@article{prince2004does,
	title = {Does active learning work? {A} review of the research},
	volume = {93},
	number = {3},
	journal = {Journal of engineering education},
	author = {Prince, Michael},
	year = {2004},
	note = {Publisher: Wiley Online Library},
	pages = {223--231},
}

@article{prince2006inductive,
	title = {Inductive teaching and learning methods: {Definitions}, comparisons, and research bases},
	volume = {95},
	number = {2},
	journal = {Journal of engineering education},
	author = {Prince, Michael J and Felder, Richard M},
	year = {2006},
	note = {Publisher: Wiley Online Library},
	pages = {123--138},
}

@misc{noauthor_fall_nodate,
	title = {Fall 2021 {CoE} {Teaching} {Awards} {Announced}},
	url = {https://engineering.purdue.edu/IE/news/2022/fall-2021-coe-teaching-awards},
}

@article{mellegard2018contrasting,
	title = {Contrasting big bang with continuous integration through defect reports},
	volume = {37},
	number = {3},
	journal = {IEEE Software},
	author = {Mellegard, Niklas and Burden, Hakan and Levin, Daniel and Lind, Kenneth and Magazinius, Ana},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {14--20},
}

@article{sadagheyani_investigating_2020,
	title = {Investigating the role of social media on mental health},
	volume = {25},
	issn = {2042-8308},
	url = {https://doi.org/10.1108/MHSI-06-2020-0039},
	doi = {10.1108/MHSI-06-2020-0039},
	abstract = {Purpose Today with the internet expansion, social media has also been identified as a factor in evolutions. Social media is the title used to refer to the set of sites and tools that have been born and developed in the space created by modern media such as communication networks, the internet and mobile phones. The effects of emerging phenomena, such as social media on human health, especially mental health, are important. As the effects of social media on users mental health is unclear, and the evidence in this field is contradictory, this study aims to determine the role of social media on mental health. Design/methodology/approach The current study was a review conducted in 2020. According to keywords, an extensive search was conducted on Web of Science, PubMed, Scopus, Google Scholar, Magiran and SID databases. In total, 501 articles were obtained. The articles were screened in three stages. Finally, out of 501 evaluated articles, 50 cases were carefully assessed and included in the study. Findings The findings showed that social media has negative and positive effects on mental health. Negative effects included anxiety, depression, loneliness, poor sleep quality, poor mental health indicators, thoughts of self-harm and suicide, increased levels of psychological distress, cyber bullying, body image dissatisfaction, fear of missing out and decreased life satisfaction. Positive effects included accessing other people’s health experiences and expert health information, managing depression, emotional support and community building, expanding and strengthening offline networks and interactions, self-expression and self-identity, establish and maintain relationships. Originality/value The impact of social media on mental health can be considered as a double-edged sword. The important thing is to be able to reduce the negative effects of social media on mental health and turn it into an opportunity by implementing appropriate strategies and actions and to increase and strengthen the positive effects.},
	number = {1},
	urldate = {2022-04-20},
	journal = {Mental Health and Social Inclusion},
	author = {Sadagheyani, Hassan Ebrahimpour and Tatari, Farin},
	month = jan,
	year = {2020},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Health, Mental health, Mental well-being, Social media},
	pages = {41--51},
}

@inproceedings{chandramouli_emerging_2011,
	title = {Emerging social media threats: {Technology} and policy perspectives},
	shorttitle = {Emerging social media threats},
	abstract = {Traditional cyber threats or attacks have targeted information and communication infrastructure that usually result in economic loss. Typically, launching these attacks requires an advanced skill level. Governments around the world have a good understanding of these threats and therefore have put in place many policies to deal with them. The rapid growth of social media is giving rise to new types of threats that spill over from the cyber world to real-life. These threats profoundly alter the psychological, social and cultural dynamics of vulnerable social media users. Also, it is becoming increasingly easy even for an average user to exploit social media for malicious purposes. Organizations and governments are finding it difficult to accurately detect, identify, predict, and prevent the malicious exploitation of social media. Quantifying the socio-psychological effect of social media vulnerabilities is another major challenge. Due to these reasons there is a lack of policies to deal with this issue. In this paper, we discuss several challenges in this emerging area, from technologies to policies.},
	booktitle = {2011 {Second} {Worldwide} {Cybersecurity} {Summit} ({WCS})},
	author = {Chandramouli, R.},
	month = jun,
	year = {2011},
	keywords = {Electronic mail, Facebook, Internet, Media, Monitoring, Pragmatics, Psychology},
	pages = {1--4},
}

@article{whiting_why_2013,
	title = {Why people use social media: a uses and gratifications approach},
	volume = {16},
	issn = {1352-2752},
	shorttitle = {Why people use social media},
	url = {https://doi.org/10.1108/QMR-06-2013-0041},
	doi = {10.1108/QMR-06-2013-0041},
	abstract = {Purpose – This paper seeks to demonstrate the importance of uses and gratifications theory to social media. By applying uses and gratifications theory, this paper will explore and discuss the uses and gratifications that consumer receive from using social media. This paper seeks to provide a better and more comprehensive understanding of why consumers use social media. Design/methodology/approach – Exploratory study was conducted. 25 in‐depth interviews were conducted with individuals who use social media. Findings – This study identified ten uses and gratifications for using social media. The ten uses and gratifications are: social interaction, information seeking, pass time, entertainment, relaxation, communicatory utility, convenience utility, expression of opinion, information sharing, and surveillance/knowledge about others. Research limitations/implications – Limitations are small sample size. Research implications are that uses and gratifications theory has specific relevance to social media and should be given more prominence. Uses and gratifications theory helps explain the many and varied reasons why consumers use social media. Practical implications – This paper helps organizations to understand why consumers use social media and what gratifications they receive from social media. Originality/value – This paper makes the contribution that uses and gratifications theory has specific relevance and should be given more prominence within the area of social media. This paper also provides a rich and vivid understanding of why consumers use social media.},
	number = {4},
	urldate = {2022-04-20},
	journal = {Qualitative Market Research: An International Journal},
	author = {Whiting, Anita and Williams, David},
	month = jan,
	year = {2013},
	note = {Publisher: Emerald Group Publishing Limited},
	keywords = {Consumer generated media, Exploratory study, In‐depth interviews, Qualitative study, Social media, Uses and gratifications theory, Uses of social media, Web 2.0},
	pages = {362--369},
}

@book{miller_how_2016,
	title = {How the {World} {Changed} {Social} {Media}},
	isbn = {978-1-910634-49-3},
	url = {https://library.oapen.org/handle/20.500.12657/32834},
	abstract = {How the World Changed Social Media is the first book in Why We Post, a book series that investigates the findings of nine anthropologists who each spent 15 months living in communities across the world. This book offers a comparative analysis summarising the results of the research and exploring the impact of social media on politics and gender, education and commerce. What is the result of the increased emphasis on visual communication? Are we becoming more individual or more social? Why is public social media so conservative? Why does equality online fail to shift inequality offline? How did memes become the moral police of the internet? Supported by an introduction to the project’s academic framework and theoretical terms that help to account for the findings, the book argues that the only way to appreciate and understand something as intimate and ubiquitous as social media is to be immersed in the lives of the people who post. Only then can we discover how people all around the world have already transformed social media in such unexpected ways and assess the consequences.},
	language = {English},
	urldate = {2022-04-20},
	publisher = {UCL Press},
	author = {Miller, Daniel and Sinanan, Jolynna and Wang, Xinyuan and McDonald, Tom and Haynes, Nell and Costa, Elisabetta and Spyer, Juliano and Venkatraman, Shriram and Nicolescu, Razvan},
	year = {2016},
	doi = {10.14324/111.9781910634493},
	note = {Accepted: 2016-12-31 23:55:55},
	keywords = {Anthropology, China, Facebook, Field research, bic Book Industry Communication::J Society \& social sciences, bic Book Industry Communication::J Society \& social sciences::JH Sociology \& anthropology::JHM Anthropology::JHMC Social \& cultural anthropology, ethnography, memes, social media, society},
}

@book{reason_human_1990,
	title = {Human {Error}},
	isbn = {978-0-521-31419-0},
	abstract = {James Reason has produced a major theoretical integration of several previously isolated literatures in his new book Human Error. Much of the theoretical structure is new and original. Particularly important is the identification of cognitive processes common to a wide variety of error types. Modern technology has now reached a point where improved safety can only be achieved on the basis of a better understanding of human error mechanisms. In its treatment of major accidents, the book spans the disciplinary gulf between psychological theory and those concerned with maintaining the reliability of hazardous technologies. As such, it is essential reading not only for cognitive scientists and human factors specialists, but also for reliability engineers and risk managers. No existing book speaks with so much clarity to both the theorists and the practitioners of human reliability.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Reason, James},
	month = oct,
	year = {1990},
	note = {Google-Books-ID: WJL8NZc8lZ8C},
	keywords = {Psychology / Cognitive Psychology \& Cognition},
}

@article{rolland_benefit_2013,
	title = {The {Benefit} of {Social} {Media}: {Bulletin} {Board} {Focus} {Groups} as a {Tool} for {Co}-creation},
	volume = {55},
	issn = {1470-7853},
	shorttitle = {The {Benefit} of {Social} {Media}},
	url = {https://doi.org/10.2501/IJMR-2013-068},
	doi = {10.2501/IJMR-2013-068},
	abstract = {Bulletin board methodology emerged at the end of the 1990s and is becoming the most frequently used qualitative study technique. This interactive approach groups a community of participants in a private or public online forum for a duration that varies from several days to several months. Discoveries, exchanges of view, personal opinions and group reactions are all part of the power and interest of the internet in this era of social media. This article presents the principles of bulletin board development, and specifics to aid understanding of this tool within social networks and to help organisations adapt to a paradigm shift in marketing in which consumer-respondents are co-creators of meaning and knowledge.},
	language = {en},
	number = {6},
	urldate = {2022-04-18},
	journal = {International Journal of Market Research},
	author = {Rolland, Sylvie E. and Parmentier, Guy},
	month = nov,
	year = {2013},
	note = {Publisher: SAGE Publications},
	pages = {809--827},
}

@inproceedings{yue_influence_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Influence of {Content} {Layout} and {Motivation} on {Users}' {Herd} {Behavior} in {Social} {Discovery}},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858497},
	doi = {10.1145/2858036.2858497},
	abstract = {Social product discovery is an emerging paradigm that enables users to seek information and inspiration from peer-contributed contents. Researchers have observed herd behaviors in social discovery, i.e., basing beliefs and decisions on what similarly situated others have done. In this paper, we explore the effects of content layout and motivation on users' herd behaviors in social discovery. We conduct an eye-tracking study with 120 participants to compare goal- and action-oriented users' behaviors on a grid versus waterfall style social discovery site. The results show that users have a higher tendency to herd on a grid-style website, more so for goal-oriented users.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yue, Yanzhen and Ma, Xiaojuan and Jiang, Zhenhui},
	month = may,
	year = {2016},
	keywords = {grid, herd, layout, motivation, social discovery, waterfall},
	pages = {5715--5719},
}

@inproceedings{ashktorab_designing_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Designing {Cyberbullying} {Mitigation} and {Prevention} {Solutions} through {Participatory} {Design} {With} {Teenagers}},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858548},
	doi = {10.1145/2858036.2858548},
	abstract = {While social media platforms enable individuals to easily communicate and share experiences, they have also emerged as a tool for cyberbullying. Teenagers represent an especially vulnerable population for negative emotional responses to cyberbullying. At the same time, attempts to mitigate or prevent cyberbullying from occurring in these networked spaces have largely failed because of the complexity and nuance with which young people bully others online. To address challenges related to designing for cyberbullying intervention and mitigation, we detail findings from participatory design work with two groups of high school students in spring 2015. Over the course of five design sessions spanning five weeks, participants shared their experiences with cyberbullying and iteratively designed potential solutions. We provide an in-depth discussion of the range of cyberbullying mitigation solutions participants designed. We focus on challenges participants' identified in designing for cyberbullying support and prevention and present a set of five potential cyberbullying mitigation solutions based on the results of the design sessions.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ashktorab, Zahra and Vitak, Jessica},
	month = may,
	year = {2016},
	keywords = {adolescents, cyberbullying, participatory design, social media},
	pages = {3895--3905},
}

@inproceedings{ma_anonymity_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Anonymity, {Intimacy} and {Self}-{Disclosure} in {Social} {Media}},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858414},
	doi = {10.1145/2858036.2858414},
	abstract = {Self-disclosure is rewarding and provides significant benefits for individuals, but it also involves risks, especially in social media settings. We conducted an online experiment to study the relationship between content intimacy and willingness to self-disclose in social media, and how identification (real name vs. anonymous) and audience type (social ties vs. people nearby) moderate that relationship. Content intimacy is known to regulate self-disclosure in face-to-face communication: people self-disclose less as content intimacy increases. We show that such regulation persists in online social media settings. Further, although anonymity and an audience of social ties are both known to increase self-disclosure, it is unclear whether they (1) increase self-disclosure baseline for content of all intimacy levels, or (2) weaken intimacy's regulation effect, making people more willing to disclose intimate content. We show that intimacy always regulates self-disclosure, regardless of settings. We also show that anonymity mainly increases self-disclosure baseline and (sometimes) weakens the regulation. On the other hand, an audience of social ties increases the baseline but strengthens the regulation. Finally, we demonstrate that anonymity has a more salient effect on content of negative valence.The results are critical to understanding the dynamics and opportunities of self-disclosure in social media services that vary levels of identification and types of audience.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Xiao and Hancock, Jeff and Naaman, Mor},
	month = may,
	year = {2016},
	keywords = {anonymity, intimacy, self-disclosure, social media, valence},
	pages = {3857--3869},
}

@inproceedings{de_choudhury_discovering_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Discovering {Shifts} to {Suicidal} {Ideation} from {Mental} {Health} {Content} in {Social} {Media}},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858207},
	doi = {10.1145/2858036.2858207},
	abstract = {History of mental illness is a major factor behind suicide risk and ideation. However research efforts toward characterizing and forecasting this risk is limited due to the paucity of information regarding suicide ideation, exacerbated by the stigma of mental illness. This paper fills gaps in the literature by developing a statistical methodology to infer which individuals could undergo transitions from mental health discourse to suicidal ideation. We utilize semi-anonymous support communities on Reddit as unobtrusive data sources to infer the likelihood of these shifts. We develop language and interactional measures for this purpose, as well as a propensity score matching based statistical approach. Our approach allows us to derive distinct markers of shifts to suicidal ideation. These markers can be modeled in a prediction framework to identify individuals likely to engage in suicidal ideation in the future. We discuss societal and ethical implications of this research.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {De Choudhury, Munmun and Kiciman, Emre and Dredze, Mark and Coppersmith, Glen and Kumar, Mrinal},
	month = may,
	year = {2016},
	keywords = {mental health, reddit, social media, suicidal ideation},
	pages = {2098--2110},
}

@inproceedings{such_photo_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Photo {Privacy} {Conflicts} in {Social} {Media}: {A} {Large}-scale {Empirical} {Study}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {Photo {Privacy} {Conflicts} in {Social} {Media}},
	url = {https://doi.org/10.1145/3025453.3025668},
	doi = {10.1145/3025453.3025668},
	abstract = {Items in social media such as photos may be co-owned by multiple users, i.e., the sharing decisions of the ones who upload them have the potential to harm the privacy of the others. Previous works uncovered coping strategies by co-owners to manage their privacy, but mainly focused on general practices and experiences. We establish an empirical base for the prevalence, context and severity of privacy conflicts over co-owned photos. To this aim, a parallel survey of pre-screened 496 uploaders and 537 co-owners collected occurrences and type of conflicts over co-owned photos, and any actions taken towards resolving them. We uncover nuances and complexities not known before, including co-ownership types, and divergences in the assessment of photo audiences. We also find that an all-or-nothing approach seems to dominate conflict resolution, even when parties actually interact and talk about the conflict. Finally, we derive key insights for designing systems to mitigate these divergences and facilitate consensus.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Such, Jose M. and Porter, Joel and Preibusch, Sören and Joinson, Adam},
	month = may,
	year = {2017},
	keywords = {co-ownership, conflicts, online social networks, photo sharing, privacy, social media},
	pages = {3821--3832},
}

@inproceedings{usmani_characterizing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Characterizing {Social} {Insider} {Attacks} on {Facebook}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025901},
	doi = {10.1145/3025453.3025901},
	abstract = {Facebook accounts are secured against unauthorized access through passwords and device-level security. Those defenses, however, may not be sufficient to prevent social insider attacks, where attackers know their victims, and gain access to a victim's account by interacting directly with their device. To characterize these attacks, we ran two MTurk studies. In the first (n = 1,308), using the list experiment method, we estimated that 24\% of participants had perpetrated social insider attacks and that 21\% had been victims (and knew about it). In the second study (n = 45), participants wrote stories detailing personal experiences with such attacks. Using thematic analysis, we typified attacks around five motivations (fun, curiosity, jealousy, animosity, and utility), and explored dimensions associated with each type. Our combined findings indicate that social insider attacks are common, often have serious emotional consequences, and have no simple mitigation.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Usmani, Wali Ahmed and Marques, Diogo and Beschastnikh, Ivan and Beznosov, Konstantin and Guerreiro, Tiago and Carriço, Luís},
	month = may,
	year = {2017},
	keywords = {facebook, insider attack, privacy, usable security},
	pages = {3810--3820},
}

@inproceedings{devito_algorithms_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {"{Algorithms} ruin everything": \#{RIPTwitter}, {Folk} {Theories}, and {Resistance} to {Algorithmic} {Change} in {Social} {Media}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {"{Algorithms} ruin everything"},
	url = {https://doi.org/10.1145/3025453.3025659},
	doi = {10.1145/3025453.3025659},
	abstract = {As algorithmically-driven content curation has become an increasingly common feature of social media platforms, user resistance to algorithmic change has become more frequent and visible. These incidents of user backlash point to larger issues such as inaccurate understandings of how algorithmic systems work as well as mismatches between designer and user intent. Using a content analysis of 102,827 tweets from \#RIPTwitter, a recent hashtag-based backlash to rumors about introducing algorithmic curation to Twitter's timeline, this study addresses the nature of user resistance in the form of the complaints being expressed, folk theories of the algorithmic system espoused by users, and how these folk theories potentially frame user reactions. We find that resistance to algorithmic change largely revolves around expectation violation, with folk theories acting as frames for reactions such that more detailed folk theories are expressed through more specific reactions to algorithmic change.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {DeVito, Michael A. and Gergle, Darren and Birnholtz, Jeremy},
	month = may,
	year = {2017},
	keywords = {algorithm awareness, algorithmic curation, algorithms, expectation violation, folk theories, machine classification, social media, technology continuance, user resistance},
	pages = {3163--3174},
}

@inproceedings{deloatch_i_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {I {Need} {Your} {Encouragement}! {Requesting} {Supportive} {Comments} on {Social} {Media} {Reduces} {Test} {Anxiety}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025709},
	doi = {10.1145/3025453.3025709},
	abstract = {Many students underperform on exams due to experiencing high test anxiety. We report on a study comparing a novel intervention of seeking support from one's social network to the more common approaches of expressive writing and studying task-relevant materials for simulated open-ended test questions. We measured in-the-moment (state) anxiety before and after each intervention, and correctness of the solutions. We also surveyed students to learn about their perceptions of the interventions. Our results showed that social support decreased the anxiety of high test-anxious students by 21\% with the reduction in anxiety correlating with the number of messages received. Social support also allowed high test-anxious students to score at the level of low test-anxious students. Expressive writing showed a similar effect, but increased the anxiety of low test-anxious students by 61\%. Studying task materials had no effect on anxiety and high test-anxious students performed worse than low test-anxious students. Despite benefiting from social support, we found that students were uncomfortable soliciting support from their online social network. Realizing the benefits of this approach may therefore require different formulations of social support in practice.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Deloatch, Robert and Bailey, Brian P. and Kirlik, Alex and Zilles, Craig},
	month = may,
	year = {2017},
	keywords = {anxiety, expressive writing, programming, social support},
	pages = {736--747},
}

@inproceedings{flintham_falling_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Falling for {Fake} {News}: {Investigating} the {Consumption} of {News} via {Social} {Media}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Falling for {Fake} {News}},
	url = {https://doi.org/10.1145/3173574.3173950},
	doi = {10.1145/3173574.3173950},
	abstract = {In the so called 'post-truth' era, characterized by a loss of public trust in various institutions, and the rise of 'fake news' disseminated via the internet and social media, individuals may face uncertainty about the veracity of information available, whether it be satire or malicious hoax. We investigate attitudes to news delivered by social media, and subsequent verification strategies applied, or not applied, by individuals. A survey reveals that two thirds of respondents regularly consumed news via Facebook, and that one third had at some point come across fake news that they initially believed to be true. An analysis task involving news presented via Facebook reveals a diverse range of judgement forming strategies, with participants relying on personal judgements as to plausibility and scepticism around sources and journalistic style. This reflects a shift away from traditional methods of accessing the news, and highlights the difficulties in combating the spread of fake news.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Flintham, Martin and Karner, Christian and Bachour, Khaled and Creswick, Helen and Gupta, Neha and Moran, Stuart},
	month = apr,
	year = {2018},
	keywords = {facebook, fake news, post-truth, social media, trust, verification},
	pages = {1--10},
}

@inproceedings{alvarado_towards_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Towards {Algorithmic} {Experience}: {Initial} {Efforts} for {Social} {Media} {Contexts}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Towards {Algorithmic} {Experience}},
	url = {https://doi.org/10.1145/3173574.3173860},
	doi = {10.1145/3173574.3173860},
	abstract = {Algorithms influence most of our daily activities, decisions, and they guide our behaviors. It has been argued that algorithms even have a direct impact on democratic societies. Human - Computer Interaction research needs to develop analytical tools for describing the interaction with, and experience of algorithms. Based on user participatory workshops focused on scrutinizing Facebook's newsfeed, an algorithm-influenced social media, we propose the concept of Algorithmic Experience (AX) as an analytic framing for making the interaction with and experience of algorithms explicit. Connecting it to design, we articulate five functional categories of AX that are particularly important to cater for in social media: profiling transparency and management, algorithmic awareness and control, and selective algorithmic memory.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Alvarado, Oscar and Waern, Annika},
	month = apr,
	year = {2018},
	keywords = {algorithmic experience, algorithms, research through design, social media, user-centered design},
	pages = {1--12},
}

@inproceedings{lottridge_lets_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Let's {Hate} {Together}: {How} {People} {Share} {News} in {Messaging}, {Social}, and {Public} {Networks}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Let's {Hate} {Together}},
	url = {https://doi.org/10.1145/3173574.3173634},
	doi = {10.1145/3173574.3173634},
	abstract = {There are currently a wide variety of ways to share news with others: from sharing in a personal message, to sharing on a social network, to publicly posting. Through a survey with over one thousand people and an artifact analysis of 262 shared articles, we examine differences in motivations and frequency of sharing news on public, social and private platforms. We find that public sharing is more focused on spreading an ideology, while private sharing in messaging is dominated by stories inspired by the recipient's interests or context. The survey revealed three main groups of news sharing practices: those who shared to all channels (public, social, private), those who didn't share at all, and those who shared to private and social. The groups differed in their attitudes toward online discussion; those that shared the most were neutral and those that didn't share had negative attitudes about discussion online. We discuss sharing practices and implications for social systems that support sharing news.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lottridge, Danielle and Bentley, Frank R.},
	month = apr,
	year = {2018},
	keywords = {diffusion, messaging, news, sharing, social media},
	pages = {1--13},
}

@inproceedings{difranzo_social_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Social {Media} {TestDrive}: {Real}-{World} {Social} {Media} {Education} for the {Next} {Generation}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Social {Media} {TestDrive}},
	url = {https://doi.org/10.1145/3290605.3300533},
	doi = {10.1145/3290605.3300533},
	abstract = {Social media sites are where life happens for many of today's young people, so it is important to teach them to use these sites safely and effectively. Many youth receive classroom education on digital literacy topics, but have few chances to build actual skills. Social Media TestDrive, an interactive social media simulation, fills a gap in digital literacy education by combining experiential learning in a realistic and safe social media environment with educator-facilitated classroom lessons. The tool was piloted with 12 educators and over 200 students, and formative evaluation data suggest that TestDrive achieved high levels of engagement with both groups. Students reported the modules enhanced their understanding of digital citizenship issues, and educators noted that students were engaging in meaningful classroom conversations. Finally, we discuss the importance of involving multiple stakeholder groups (e.g., researchers, youth, educators, curriculum developers) in designing educational technology.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {DiFranzo, Dominic and Choi, Yoon Hyung and Purington, Amanda and Taft, Jessie G. and Whitlock, Janis and Bazarova, Natalya N.},
	month = may,
	year = {2019},
	keywords = {digital citizen, education, social media, youth development},
	pages = {1--11},
}

@inproceedings{ma_when_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {When {Do} {People} {Trust} {Their} {Social} {Groups}?},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300297},
	doi = {10.1145/3290605.3300297},
	abstract = {Trust facilitates cooperation and supports positive outcomes in social groups, including member satisfaction, information sharing, and task performance. Extensive prior research has examined individuals' general propensity to trust, as well as the factors that contribute to their trust in specific groups. Here, we build on past work to present a comprehensive framework for predicting trust in groups. By surveying 6,383 Facebook Groups users about their trust attitudes and examining aggregated behavioral and demographic data for these individuals, we show that (1) an individual's propensity to trust is associated with how they trust their groups, (2) smaller, closed, older, more exclusive, or more homogeneous groups are trusted more, and (3) a group's overall friendship-network structure and an individual's position within that structure can also predict trust. Last, we demonstrate how group trust predicts outcomes at both individual and group level such as the formation of new friendship ties.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Xiao and Cheng, Justin and Iyer, Shankar and Naaman, Mor},
	month = may,
	year = {2019},
	keywords = {communities, facebook, groups, trust},
	pages = {1--12},
}

@incollection{eschler_emergent_2020,
	address = {New York, NY, USA},
	title = {Emergent {Self}-{Regulation} {Practices} in {Technology} and {Social} {Media} {Use} of {Individuals} {Living} with {Depression}},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376773},
	abstract = {Much human-computer interaction work related to depression focuses on the population level (e.g., studying social media hashtags related to depression) or evaluates prototypes for digital interventions to manage depression. However, little is known about how people living with depression perceive and manage technology use, such as time spent on social media per day. For this study, we interviewed 30 individuals living with depression to explore their technology and social media use. We find that these individuals demonstrated emergent practices related to self-regulation, such as learning to monitor and adjust technology use to improve their emotional, cognitive, and behavioral health. Our findings add a human-centered viewpoint to the relationship between living with depression and technology and social media use. We present design implications of these findings for better empowering individuals with depression to encourage their natural inclinations to self-regulate technology and social media use.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Eschler, Jordan and Burgess, Eleanor R. and Reddy, Madhu and Mohr, David C.},
	month = apr,
	year = {2020},
	keywords = {depression, qualitative, self-regulation, social media},
	pages = {1--13},
}

@incollection{trieu_private_2020,
	address = {New York, NY, USA},
	title = {Private {Responses} for {Public} {Sharing}: {Understanding} {Self}-{Presentation} and {Relational} {Maintenance} via {Stories} in {Social} {Media}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Private {Responses} for {Public} {Sharing}},
	url = {https://doi.org/10.1145/3313831.3376549},
	abstract = {With nearly two billion users, social media Stories-an ephemeral format of sharing-are increasingly popular and projected to overtake sharing via public feeds. Sharing via Stories differs from Feeds sharing by removing the visible feedback (e.g. "likes" and "comments") which has come to characterize social media. Given the salience of responses visibility to self-presentation and relational maintenance in social media literature, we conducted semi-structured interviews (N = 22) to explore how people understand these processes when using Stories. We find that users have lower expectations for responses with Stories and experience lower pressure for self-presentation. This fosters more frequent sharing and a sense of daily connectedness, which strong ties can find valuable. Finally, the act of viewing takes on new significance of signaling attention when made known to the sharer. Our findings point to the importance of effort and attention in understanding responses on social media.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Trieu, Penny and Baym, Nancy K.},
	month = apr,
	year = {2020},
	keywords = {relational maintenance, self-presentation, social media, stories},
	pages = {1--13},
}

@incollection{burke_social_2020,
	address = {New York, NY, USA},
	title = {Social {Comparison} and {Facebook}: {Feedback}, {Positivity}, and {Opportunities} for {Comparison}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Social {Comparison} and {Facebook}},
	url = {https://doi.org/10.1145/3313831.3376482},
	abstract = {People compare themselves to one another both offline and online. The specific online activities that worsen social comparison are partly understood, though much existing research relies on people recalling their own online activities post hoc and is situated in only a few countries. To better understand social comparison worldwide and the range of associated behaviors on social media, a survey of 38,000 people from 18 countries was paired with logged activity on Facebook for the prior month. People who reported more frequent social comparison spent more time on Facebook, had more friends, and saw proportionally more social content on the site. They also saw greater amounts of feedback on friends' posts and proportionally more positivity. There was no evidence that social comparison happened more with acquaintances than close friends. One in five respondents recalled recently seeing a post that made them feel worse about themselves but reported conflicting views: half wished they hadn't seen the post, while a third felt very happy for the poster. Design opportunities are discussed, including hiding feedback counts, filters for topics and people, and supporting meaningful interactions, so that when comparisons do occur, people are less affected by them.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Burke, Moira and Cheng, Justin and de Gant, Bethany},
	month = apr,
	year = {2020},
	keywords = {envy, facebook, social comparison, social media, well-being},
	pages = {1--13},
}

@inproceedings{charmaraman_prototyping_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Prototyping for {Social} {Wellbeing} with {Early} {Social} {Media} {Users}: {Belonging}, {Experimentation}, and {Self}-{Care}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Prototyping for {Social} {Wellbeing} with {Early} {Social} {Media} {Users}},
	url = {https://doi.org/10.1145/3411764.3445332},
	doi = {10.1145/3411764.3445332},
	abstract = {Many 10-14 year olds are at the early stages of using social media, habits they develop on popular platforms can have lasting effects on their socio-emotional wellbeing. We led a remote innovation workshop with 23 middle schoolers on digital wellbeing, identity exploration, and computational concepts related to social computing. This workshop was a unique opportunity to reflect on emergent habits, discuss them with peers, and imagine oneself as an ICT innovator. Resulting themes related to participants’ social wellbeing online included a) sense of belonging to communities of interest, friends, and family, b) self-care and social support strategies involving managing risks, control, and empathy, and c) experimentation while building self-confidence and bravely exploring audience reactions. Participants iteratively designed and tested a sandbox social network website, resulting in Social Sketch. Reflecting on our study, we describe the process for conceptualizing Social Sketch, and challenges in social media innovation with teenagers.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Charmaraman, Linda and Grevet Delcourt, Catherine},
	month = may,
	year = {2021},
	keywords = {ICT, adolescence, cooperative inquiry, prototyping, social media, teenagers, wellbeing, wellness},
	pages = {1--15},
}

@inproceedings{tian_deeptest_2018,
	address = {Gothenburg Sweden},
	title = {{DeepTest}: automated testing of deep-neural-network-driven autonomous cars},
	isbn = {978-1-4503-5638-1},
	shorttitle = {{DeepTest}},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180220},
	doi = {10.1145/3180155.3180220},
	abstract = {Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.},
	language = {en},
	urldate = {2022-04-13},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
	month = may,
	year = {2018},
	pages = {303--314},
}

@inproceedings{tang_empirical_2021,
	address = {Madrid, ES},
	title = {An {Empirical} {Study} of {Refactorings} and {Technical} {Debt} in {Machine} {Learning} {Systems}},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9401990/},
	doi = {10.1109/ICSE43902.2021.00033},
	abstract = {Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today’s data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semanticspreserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major crosscutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.},
	language = {en},
	urldate = {2022-04-13},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita},
	month = may,
	year = {2021},
	pages = {238--250},
}

@misc{haidt_why_2022,
	title = {Why the {Past} 10 {Years} of {American} {Life} {Have} {Been} {Uniquely} {Stupid}},
	url = {https://www.theatlantic.com/magazine/archive/2022/05/social-media-democracy-trust-babel/629369/},
	abstract = {It’s not just a phase.},
	language = {en},
	urldate = {2022-04-12},
	journal = {The Atlantic},
	author = {Haidt, Jonathan},
	month = apr,
	year = {2022},
	note = {Section: Ideas},
}

@article{ud_din_privacy_2018,
	title = {Privacy and {Security} {Issues} in {Online} {Social} {Networks}},
	volume = {10},
	doi = {10.3390/fi10120114},
	abstract = {The advent of online social networks (OSN) has transformed a common passive reader into a content contributor. It has allowed users to share information and exchange opinions, and also express themselves in online virtual communities to interact with other users of similar interests. However, OSN have turned the social sphere of users into the commercial sphere. This should create a privacy and security issue for OSN users. OSN service providers collect the private and sensitive data of their customers that can be misused by data collectors, third parties, or by unauthorized users. In this paper, common security and privacy issues are explained along with recommendations to OSN users to protect themselves from these issues whenever they use social media.},
	journal = {Future Internet},
	author = {Ud Din, Ikram and Islam, Naveed and Rodrigues, Joel and Guizani, Mohsen},
	month = nov,
	year = {2018},
}

@article{wendell_reflective_2017,
	title = {Reflective {Decision}‐{Making} in {Elementary} {Students}' {Engineering} {Design}},
	volume = {106},
	issn = {1069-4730, 2168-9830},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jee.20173},
	doi = {10.1002/jee.20173},
	abstract = {Purpose This qualitative study sought to propose and operationalize a definition of reflective decision-making among elementary students. We investigated how urban elementary students enact reflective decision making in a formal engineering design curriculum.
Method We used naturalistic inquiry methodology and video recorded seven Engineering is Elementary design challenges in four classrooms. Students worked in small teams, and we focused on their planning and redesign phases. Maximum variation sampling, constant comparative analysis, and microethnographic accounts demonstrated the diversity of resources students utilized in their decision making.
Results In student discourse, we found evidence for six reflective decision-making elements: articulating multiple solutions, evaluating pros and cons, intentionally selecting a solution, retelling the performance of a solution, analyzing a solution according to evidence, and purposefully choosing improvements. The discourse patterns used to enact these elements both supported and interfered with students’ achievement of design goals.
Conclusions Our results suggest that during engineering design tasks, young learners working in small teams can respond productively to opportunities to engage in sophisticated discourse. However, further work is needed on tools and strategies that support reflective decision-making by all students during engineering design in elementary school.},
	language = {en},
	number = {3},
	urldate = {2022-04-06},
	journal = {Journal of Engineering Education},
	author = {Wendell, Kristen Bethke and Wright, Christopher G. and Paugh, Patricia},
	month = jul,
	year = {2017},
	pages = {356--397},
}

@inproceedings{niedermaier_correct_2020,
	title = {Correct and {Control} {Complex} {IoT} {Systems}: {Evaluation} of a {Classification} for {System} {Anomalies}},
	shorttitle = {Correct and {Control} {Complex} {IoT} {Systems}},
	doi = {10.1109/QRS51102.2020.00050},
	abstract = {In practice there are deficiencies in precise interteam communications about system anomalies to perform troubleshooting and postmortem analysis along different teams operating complex IoT systems. We evaluate the quality in use of an adaptation of IEEE Std. 1044-2009 with the objective to differentiate the handling of fault detection and fault reaction from handling of defect and its options for defect correction. We extended the scope of IEEE Std. 1044-2009 from anomalies related to software only to anomalies related to complex IoT systems. To evaluate the quality in use of our classification a study was conducted at Robert Bosch GmbH. We applied our adaptation to a postmortem analysis of an IoT solution and evaluated the quality in use by conducting interviews with three stakeholders. Our adaptation was effectively applied and interteam communications as well as iterative and inductive learning for product improvement were enhanced.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Niedermaier, Sina and Heisse, Stefan and Wagner, Stefan},
	month = dec,
	year = {2020},
	keywords = {Fault detection, IEEE Standards, Interviews, Security, Software quality, Software reliability, Stakeholders, anomaly, classification, complex system, defect, failure, fault},
	pages = {321--328},
}

@article{liblit_building_nodate,
	title = {Building a {Better} {Backtrace}: {Techniques} for {Postmortem} {Program} {Analysis}},
	abstract = {After a program has crashed, it can be diﬃcult to reconstruct why the failure occurred, or what actions led to the error. We propose a family of analysis techniques that use the evidence left behind by a failed program to build a time line of its possible actions from launch through termination. Our design can operate with zero run time instrumentation, or can ﬂexibly incorporate a wide variety of artifacts such as stack traces and event logs for increased precision. Eﬃcient demanddriven algorithms are provided, and the approach is well suited for incorporation into interactive debugging support tools.},
	language = {en},
	author = {Liblit, Ben and Aiken, Alex},
	pages = {13},
}

@article{yang_symbian_nodate,
	title = {Symbian {OS} system event log and postmortem software fault analysis},
	abstract = {This thesis introduces a postmortem software failure analysis system named MobileCrash. The system is to catch Symbian OS panics and exceptions, to collect related information and to transmit crash logs to a central database for analysis. The basics of software failure analysis and similar systems on other operation systems are presented in the thesis. After revealing the system design of the MobileCrash, the system event log as a supplementary of the MobileCrash is introduced. The thesis also introduces the core of crash analysis together with real case studies in Symbian OS. Software developers can benefit from this thesis by learning about software failure analysis on Symbian OS. Project managers can get basic information on software failure analysis from this thesis to better control projects based on Symbian OS.},
	language = {en},
	author = {Yang, Zhigang},
	pages = {62},
}

@inproceedings{jetley_using_2006,
	title = {Using {Abstraction}-driven {Slicing} for {Postmortem} {Analysis} of {Software}},
	doi = {10.1109/ICPC.2006.50},
	abstract = {Post-mortem analysis - the process of tracing software failure to source code - is an important means for maintenance engineers and regulatory reviewers for establishing the cause of an error. Historically, static slicing techniques have been used for aiding post-mortem fault analysis. However, the slices obtained in this manner can often be too large and may not give a clear understanding of the code when dealing with complex reactive systems. We propose using model abstraction in conjunction with slicing to ameliorate the problem of understanding large slices. Combining slicing with abstraction provides the analyst with an integrated cognition model, leading to a better understanding of the code, and consequently more efficient error analysis. We formalize this concept through the notion of abstraction-driven slicing, and use it to develop CAdS, an automated tool to aid postmortem error detection in C programs using abstraction-driven static slicing. We list our experiences with CAdS and illustrate how it can be used to reduce effort involved in the postmortem analysis process},
	booktitle = {14th {IEEE} {International} {Conference} on {Program} {Comprehension} ({ICPC}'06)},
	author = {Jetley, R. and Zhang, Yi and Iyer, S.P.},
	month = jun,
	year = {2006},
	note = {ISSN: 1092-8138},
	keywords = {Application software, Cognition, Computer errors, Computer science, Error analysis, Failure analysis, Food manufacturing, Forensics, Instruments, Software maintenance},
	pages = {107--116},
}

@article{ohmann_lightweight_2017,
	title = {Lightweight control-flow instrumentation and postmortem analysis in support of debugging},
	volume = {24},
	issn = {0928-8910, 1573-7535},
	url = {http://link.springer.com/10.1007/s10515-016-0190-1},
	doi = {10.1007/s10515-016-0190-1},
	abstract = {Debugging is difﬁcult and costly. As a programmer looks for a bug, it would be helpful to see a complete trace of events leading to the point of failure. Unfortunately, full tracing is simply too slow to use after deployment, and may even be impractical during testing. We aid post-deployment debugging by giving programmers additional information about program activity shortly before failure. We use latent information in post-failure memory dumps, augmented by low-overhead, tunable runtime tracing. Our results with a realistically-tuned tracing scheme show low enough overhead (0–5 \%) to be used in production runs. We demonstrate several potential uses of this enhanced information, including a novel postmortem static slice restriction technique and a reduced view of potentially-executed code. Experimental evaluation shows our approach to be very effective. For example, our analyses shrink stacksensitive interprocedural static slices by 53–78 \% in larger applications.},
	language = {en},
	number = {4},
	urldate = {2022-04-05},
	journal = {Automated Software Engineering},
	author = {Ohmann, Peter and Liblit, Ben},
	month = dec,
	year = {2017},
	pages = {865--904},
}

@article{xu_pomp_nodate,
	title = {{POMP}: {Postmortem} {Program} {Analysis} with {Hardware}-{Enhanced} {Post}-{Crash} {Artifacts}},
	abstract = {While a core dump carries a large amount of information, it barely serves as informative debugging aids in locating software faults because it carries information that indicates only a partial chronology of how program reached a crash site. Recently, this situation has been signiﬁcantly improved. With the emergence of hardwareassisted processor tracing, software developers and security analysts can trace program execution and integrate them into a core dump. In comparison with an ordinary core dump, the new post-crash artifact provides software developers and security analysts with more clues as to a program crash. To use it for failure diagnosis, however, it still requires strenuous manual efforts.},
	language = {en},
	author = {Xu, Jun and Mu, Dongliang and Xing, Xinyu and Liu, Peng and Chen, Ping and Mao, Bing},
	pages = {17},
}

@article{manevich_pse_nodate,
	title = {{PSE}: {Explaining} {Program} {Failures} via {Postmortem} {Static} {Analysis}},
	abstract = {In this paper, we describe PSE (Postmortem Symbolic Evaluation), a static analysis algorithm that can be used by programmers to diagnose software failures. The algorithm requires minimal information about a failure, namely its kind (e.g. NULL dereference), and its location in the program’s source code. It produces a set of execution traces along which the program can be driven to the given failure.},
	language = {en},
	author = {Manevich, Roman and Sridharan, Manu and Adams, Stephen and Das, Manuvir and Yang, Zhe},
	pages = {10},
}

@inproceedings{bala_challenges_2015,
	title = {Challenges and {Outcomes} of {Enterprise} {Social} {Media} {Implementation}: {Insights} from {Cummins}, {Inc}.},
	shorttitle = {Challenges and {Outcomes} of {Enterprise} {Social} {Media} {Implementation}},
	doi = {10.1109/HICSS.2015.222},
	abstract = {Enterprise social media (ESM) are web-based platforms that improve communication and collaboration in organizations. Although the practitioner literature and industry reports have suggested the potential value of ESM for organizations, there has been limited research that focuses on the implementation process and outcomes of ESM. We conducted a mixed methods case study of a large-scale ESM implementation in a Fortune 500 manufacturing company, Cummins, Inc., and found several major challenges that Cummins faced during the implementation, such as the lack of interest and use by employees, the lack of fit with existing organizational and individual processes, inconsistent performance of the ESM platform, and unfavorable business conditions that affected organization-wide programs and projects. While employees were initially enthusiastic about the ESM, they were not using the platform as much as anticipated after the implementation. We offer a set of lessons from our in-depth case study that organizations implementing ESM should find beneficial.},
	booktitle = {2015 48th {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Bala, Hillol and Massey, Anne P. and Rajanayakam, John and Hsieh, Christine J.},
	month = jan,
	year = {2015},
	note = {ISSN: 1530-1605},
	keywords = {Blogs, Collaboration, Companies, Encyclopedias, Media},
	pages = {1839--1848},
}

@misc{auxier_social_2021,
	title = {Social {Media} {Use} in 2021},
	url = {https://www.pewresearch.org/internet/2021/04/07/social-media-use-in-2021/},
	abstract = {A majority of Americans say they use YouTube and Facebook, while use of Instagram, Snapchat and TikTok is especially common among adults under 30.},
	language = {en},
	urldate = {2022-02-03},
	journal = {Pew Research Center - Internet},
	author = {Auxier, Brooke and Anderson, Monica},
	month = apr,
	year = {2021},
}

@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {0035-9246},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {http://www.jstor.org/stable/2346101},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	urldate = {2022-03-26},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {289--300},
}

@article{noauthor_facebook_2021,
	chapter = {Tech},
	title = {The {Facebook} {Files}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/the-facebook-files-11631713039},
	abstract = {Facebook knows, in acute detail, that its platforms are riddled with flaws but hasn’t fixed them. That’s a key finding of a Journal series that launched this week, based on an array of internal company documents. Read all the stories here.},
	language = {en-US},
	urldate = {2022-03-26},
	journal = {Wall Street Journal},
	month = oct,
	year = {2021},
	keywords = {FB, Facebook, GRAPHICS, Mark Zuckerberg, Media/Entertainment, Online Service Providers, SYND, Social Media Platforms/Tools, Technology, WSJ-PRO-WSJ.com, entertainment, graphics, media, online service providers, social media platforms, technology, tools},
}

@misc{mastodon_mastodonmastodon_2022,
	title = {mastodon/mastodon},
	copyright = {AGPL-3.0},
	url = {https://github.com/mastodon/mastodon},
	abstract = {Your self-hosted, globally interconnected microblogging community},
	urldate = {2022-03-26},
	publisher = {Mastodon},
	author = {{mastodon}},
	month = mar,
	year = {2022},
	note = {original-date: 2016-02-22T15:01:25Z},
	keywords = {activity-stream, activitypub, docker, mastodon, microblog, social-network, webfinger},
}

@misc{diaspora_diasporadiaspora_2022,
	title = {diaspora/diaspora},
	url = {https://github.com/diaspora/diaspora},
	abstract = {A privacy-aware, distributed, open source social network.},
	urldate = {2022-03-26},
	publisher = {diaspora},
	author = {{diaspora}},
	month = mar,
	year = {2022},
	note = {original-date: 2010-09-15T05:20:04Z},
	keywords = {decentralized, distributed, federated, hacktoberfest, rails, ruby, social-network},
}

@article{reason_understanding_1995,
	title = {Understanding adverse events: human factors.},
	volume = {4},
	issn = {2044-5415, 2044-5423},
	shorttitle = {Understanding adverse events},
	url = {https://qualitysafety.bmj.com/content/4/2/80},
	doi = {10.1136/qshc.4.2.80},
	abstract = {(1) Human rather than technical failures now represent the greatest threat to complex and potentially hazardous systems. This includes healthcare systems. (2) Managing the human risks will never be 100\% effective. Human fallibility can be moderated, but it cannot be eliminated. (3) Different error types have different underlying mechanisms, occur in different parts of the organisation, and require different methods of risk management. The basic distinctions are between: Slips, lapses, trips, and fumbles (execution failures) and mistakes (planning or problem solving failures). Mistakes are divided into rule based mistakes and knowledge based mistakes. Errors (information-handling problems) and violations (motivational problems) Active versus latent failures. Active failures are committed by those in direct contact with the patient, latent failures arise in organisational and managerial spheres and their adverse effects may take a long time to become evident. (4) Safety significant errors occur at all levels of the system, not just at the sharp end. Decisions made in the upper echelons of the organisation create the conditions in the workplace that subsequently promote individual errors and violations. Latent failures are present long before an accident and are hence prime candidates for principled risk management. (5) Measures that involve sanctions and exhortations (that is, moralistic measures directed to those at the sharp end) have only very limited effectiveness, especially so in the case of highly trained professionals. (6) Human factors problems are a product of a chain of causes in which the individual psychological factors (that is, momentary inattention, forgetting, etc) are the last and least manageable links. Attentional "capture" (preoccupation or distraction) is a necessary condition for the commission of slips and lapses. Yet, its occurrence is almost impossible to predict or control effectively. The same is true of the factors associated with forgetting. States of mind contributing to error are thus extremely difficult to manage; they can happen to the best of people at any time. (7) People do not act in isolation. Their behaviour is shaped by circumstances. The same is true for errors and violations. The likelihood of an unsafe act being committed is heavily influenced by the nature of the task and by the local workplace conditions. These, in turn, are the product of "upstream" organisational factors. Great gains in safety can ve achieved through relatively small modifications of equipment and workplaces. (8) Automation and increasing advanced equipment do not cure human factors problems, they merely relocate them. In contrast, training people to work effectively in teams costs little, but has achieved significant enhancements of human performance in aviation. (9) Effective risk management depends critically on a confidential and preferable anonymous incident monitoring system that records the individual, task, situational, and organisational factors associated with incidents and near misses. (10) Effective risk management means the simultaneous and targeted deployment of limited remedial resources at different levels of the system: the individual or team, the task, the situation, and the organisation as a whole.},
	language = {en},
	number = {2},
	urldate = {2022-03-26},
	journal = {BMJ Quality \& Safety},
	author = {Reason, J.},
	month = jun,
	year = {1995},
	note = {Publisher: BMJ Publishing Group Ltd
Section: Research Article},
	pages = {80--89},
}

@article{jin_bugredux_nodate,
	title = {{BugRedux}: {Reproducing} field failures for in-house debugging},
	abstract = {A recent survey conducted among developers of the Apache, Eclipse, and Mozilla projects showed that the ability to recreate ﬁeld failures is considered of fundamental importance when investigating bug reports. Unfortunately, the information typically contained in a bug report, such as memory dumps or call stacks, is usually insufﬁcient for recreating the problem. Even more advanced approaches for gathering ﬁeld data and help in-house debugging tend to collect either too little information, and be ineffective, or too much information, and be inefﬁcient. To address these issues, we present BUGREDUX, a novel general approach for in-house debugging of ﬁeld failures. BUGREDUX aims to synthesize, using execution data collected in the ﬁeld, executions that mimic the observed ﬁeld failures. We deﬁne several instances of BUGREDUX that collect different types of execution data and perform, through an empirical study, a cost-beneﬁt analysis of the approach and its variations. In the study, we apply BUGREDUX to 16 failures of 14 real-world programs. Our results are promising in that they show that it is possible to synthesize in-house executions that reproduce failures observed in the ﬁeld using a suitable set of execution data.},
	language = {en},
	author = {Jin, Wei and Orso, Alessandro},
	pages = {11},
}

@inproceedings{jin2012bugredux,
	title = {Bugredux: {Reproducing} field failures for in-house debugging},
	booktitle = {2012 34th international conference on software engineering ({ICSE})},
	author = {Jin, Wei and Orso, Alessandro},
	year = {2012},
	note = {tex.organization: IEEE},
	pages = {474--484},
}

@inproceedings{DBLP:conf/icse/BellSK13,
	title = {Chronicler: lightweight recording to reproduce field failures},
	url = {https://doi.org/10.1109/ICSE.2013.6606582},
	doi = {10.1109/ICSE.2013.6606582},
	booktitle = {35th international conference on software engineering, {ICSE} '13, san francisco, {CA}, {USA}, may 18-26, 2013},
	publisher = {IEEE Computer Society},
	author = {Bell, Jonathan and Sarda, Nikhil and Kaiser, Gail E.},
	editor = {Notkin, David and Cheng, Betty H. C. and Pohl, Klaus},
	year = {2013},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/icse/BellSK13.bib
tex.timestamp: Wed, 16 Oct 2019 14:14:49 +0200},
	pages = {362--371},
}

@inproceedings{tomassi_bugswarm_2019,
	address = {Montreal, QC, Canada},
	title = {{BugSwarm}: {Mining} and {Continuously} {Growing} a {Dataset} of {Reproducible} {Failures} and {Fixes}},
	isbn = {978-1-72810-869-8},
	shorttitle = {{BugSwarm}},
	url = {https://ieeexplore.ieee.org/document/8812141/},
	doi = {10.1109/ICSE.2019.00048},
	abstract = {Fault-detection, localization, and repair methods are vital to software quality; but it is difﬁcult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and ﬁxes are vital to good experimental evaluation of approaches to software quality, but they are difﬁcult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like TRAVIS-CI, which are widely used, fully conﬁgurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BUGSWARM, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BUGSWARM toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually.},
	language = {en},
	urldate = {2022-03-25},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Tomassi, David A. and Dmeiri, Naji and Wang, Yichen and Bhowmick, Antara and Liu, Yen-Chuan and Devanbu, Premkumar T. and Vasilescu, Bogdan and Rubio-Gonzalez, Cindy},
	month = may,
	year = {2019},
	pages = {339--349},
}

@inproceedings{xiao_tracking_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Tracking data structures for postmortem analysis ({NIER} track)},
	isbn = {978-1-4503-0445-0},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985938},
	doi = {10.1145/1985793.1985938},
	abstract = {Analyzing the runtime behaviors of the data structures is important because they usually relate to the obscured program performance and understanding issues. The runtime evolution history of data structures creates the possibility of building a lightweight and non-checkpointing based solution for the backward analysis for validating and mining both the temporal and stationary properties of the data structure. We design and implement TAEDS, a framework that focuses on gathering the data evolution history of a program at the runtime and provides a virtual machine for programmers to examine the behavior of data structures back in time. We show that our approach facilitates many programming tasks such as diagnosing memory problems and improving the design of the data structures themselves.},
	language = {en},
	urldate = {2022-03-25},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Xiao, Xiao and Zhou, Jinguo and Zhang, Charles},
	month = may,
	year = {2011},
	pages = {896--899},
}

@inproceedings{streit_why_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Why software quality improvement fails (and how to succeed nevertheless)},
	isbn = {978-1-4503-0445-0},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985895},
	doi = {10.1145/1985793.1985895},
	abstract = {Quality improvement is the key to enormous cost reduction in the IT business. However, improvement projects often fail in practice. In many cases, stakeholders fearing, e.g., a loss of power or not recognizing the beneﬁts inhibit the improvement. Systematic change management and an economic perspective help to overcome these issues, but are little known and seldom applied.},
	language = {en},
	urldate = {2022-03-25},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Streit, Jonathan and Pizka, Markus},
	month = may,
	year = {2011},
	pages = {726--735},
}

@article{falessi_achieving_2014,
	title = {Achieving and {Maintaining} {CMMI} {Maturity} {Level} 5 in a {Small} {Organization}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/6728932/},
	doi = {10.1109/MS.2014.17},
	language = {en},
	number = {5},
	urldate = {2022-03-25},
	journal = {IEEE Software},
	author = {Falessi, Davide and Shaw, Michele and Mullen, Kathleen},
	month = sep,
	year = {2014},
	pages = {80--86},
}

@article{pino_software_2008,
	title = {Software process improvement in small and medium software enterprises: a systematic review},
	volume = {16},
	issn = {0963-9314, 1573-1367},
	shorttitle = {Software process improvement in small and medium software enterprises},
	url = {http://link.springer.com/10.1007/s11219-007-9038-z},
	doi = {10.1007/s11219-007-9038-z},
	abstract = {Small and medium enterprises are a very important cog in the gears of the world economy. The software industry in most countries is composed of an industrial scheme that is made up mainly of small and medium software enterprises—SMEs. To strengthen these types of organizations, efﬁcient Software Engineering practices are needed—practices which have been adapted to their size and type of business. Over the last two decades, the Software Engineering community has expressed special interest in software process improvement (SPI) in an effort to increase software product quality, as well as the productivity of software development. However, there is a widespread tendency to make a point of stressing that the success of SPI is only possible for large companies. In this article, a systematic review of published case studies on the SPI efforts carried out in SMEs is presented. Its objective is to analyse the existing approaches towards SPI which focus on SMEs and which report a case study carried out in industry. A further objective is that of discussing the signiﬁcant issues related to this area of knowledge, and to provide an up-to-date state of the art, from which innovative research activities can be thought of and planned.},
	language = {en},
	number = {2},
	urldate = {2022-03-25},
	journal = {Software Quality Journal},
	author = {Pino, Francisco J. and García, Félix and Piattini, Mario},
	month = jun,
	year = {2008},
	pages = {237--261},
}

@article{satariano_eu_2022,
	chapter = {Technology},
	title = {E.{U}. {Takes} {Aim} at {Big} {Tech}’s {Power} {With} {Landmark} {Digital} {Act}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2022/03/24/technology/eu-regulation-apple-meta-google.html},
	abstract = {The European Union was expected to finalize the Digital Markets Act, the most sweeping legislation to regulate tech since a European privacy law was passed in 2018.},
	language = {en-US},
	urldate = {2022-03-24},
	journal = {The New York Times},
	author = {Satariano, Adam},
	month = mar,
	year = {2022},
	keywords = {Alphabet Inc, Amazon.com Inc, Android (Operating System), Antitrust Laws and Competition Issues, Apple Inc, Computers and the Internet, Data-Mining and Database Marketing, E-Commerce, European Union, Facebook Inc, General Data Protection Regulation (GDPR), Google Inc, Instagram Inc, Instant Messaging, Law and Legislation, Meta Platforms Inc, Microsoft Corp, Mobile Applications, Mobile Commerce and Payments, Online Advertising, Regulation and Deregulation of Industry, Social Media},
}

@article{lehtinen_what_nodate,
	title = {What are {Problem} {Causes} of {Software} {Projects}? {Data} of {Root} {Cause} {Analysis} at {Four} {Software} {Companies}},
	abstract = {Root cause analysis (RCA) is a structured investigation of a problem to detect the causes that need to be prevented. We applied ARCA, an RCA method, to target problems of four medium-sized software companies and collected 648 causes of software engineering problems. Thereafter, we applied grounded theory to the causes to study their types and related process areas. We detected 14 types of causes in 6 process areas. Our results indicate that development work and software testing are the most common process areas, whereas lack of instructions and experiences, insufficient work practices, low quality task output, task difficulty, and challenging existing product are the most common types of the causes. As the types of causes are evenly distributed between the cases, we hypothesize that the distributions could be generalizable. Finally, we found that only 2.5\% of the causes are related to software development tools that are widely investigated in software engineering research.},
	language = {en},
	author = {Lehtinen, Timo O A and Mantyla, Mika V},
	pages = {4},
}

@inproceedings{lehtinen2011problem,
	title = {What are problem causes of software projects? {Data} of root cause analysis at four software companies},
	booktitle = {2011 international symposium on empirical software engineering and measurement},
	author = {Lehtinen, Timo OA and Mantyla, Mika V},
	year = {2011},
	note = {tex.organization: IEEE},
	pages = {388--391},
}

@inproceedings{lehtinen2012perceived,
	title = {Perceived feasibility of using root cause analysis in post project reviews: an empirical investigation'},
	booktitle = {{ESEIW}/{International} doctoral symposium on empirical software engineering ({IDoESE}’2012), lund, september, 2012},
	author = {Lehtinen, Timo OA},
	year = {2012},
	note = {tex.organization: IEEE/ESEM},
	pages = {1--8},
}

@article{myllyaho2004review,
	title = {A review of small and large post-mortem analysis methods},
	journal = {Proceedings of the ICSSEA, Paris},
	author = {Myllyaho, Mauri and Salo, Outi and Kääriäinen, Jukka and Hyysalo, Jarkko and Koskela, Juha},
	year = {2004},
	pages = {1--8},
}

@article{hoda_toward_nodate,
	title = {Toward {Learning} {Teams}},
	language = {en},
	author = {Hoda, Rashina and Babb, Jeffry and Norbjerg, Jacob},
	pages = {4},
}

@inproceedings{babb2013barriers,
	title = {Barriers to learning in agile software development projects},
	booktitle = {International conference on agile software development},
	author = {Babb, Jeffry S and Hoda, Rashina and Nørbjerg, Jacob},
	year = {2013},
	note = {tex.organization: Springer},
	pages = {1--15},
}

@article{hoda2013toward,
	title = {Toward learning teams},
	volume = {30},
	number = {4},
	journal = {IEEE software},
	author = {Hoda, Rashina and Babb, Jeffry and Nørbjerg, Jacob},
	year = {2013},
	note = {Publisher: IEEE},
	pages = {95--98},
}

@inproceedings{hazzan_reflective_nodate,
	title = {The {Reflective} {Practitioner} {Perspective} in {Software} {Engineering}},
	language = {en},
	booktitle = {{CHI} 2004 {One} {Day} {Workshop} - {Designing} for {Reflective} {Practitioners}},
	author = {Hazzan, Orit and Tomayko, Jim},
	pages = {4},
}

@article{hamill_common_2009,
	title = {Common {Trends} in {Software} {Fault} and {Failure} {Data}},
	volume = {35},
	abstract = {The benefits of the analysis of software faults and failures have been widely recognized. However, detailed studies based on empirical data are rare. In this paper, we analyze the fault and failure data from two large, real-world case studies. Specifically, we explore: 1) the localization of faults that lead to individual software failures and 2) the distribution of different types of software faults. Our results show that individual failures are often caused by multiple faults spread throughout the system. This observation is important since it does not support several heuristics and assumptions used in the past. In addition, it clearly indicates that finding and fixing faults that lead to such software failures in large, complex systems are often difficult and challenging tasks despite the advances in software development. Our results also show that requirement faults, coding faults, and data problems are the three most common types of software faults. Furthermore, these results show that contrary to the popular belief, a significant percentage of failures are linked to late life cycle activities. Another important aspect of our work is that we conduct intra- and interproject comparisons, as well as comparisons with the findings from related studies. The consistency of several main trends across software systems in this paper and several related research efforts suggests that these trends are likely to be intrinsic characteristics of software faults and failures rather than project specific.},
	language = {en},
	number = {4},
	journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
	author = {Hamill, Maggie and Goseva-Popstojanova, Katerina},
	year = {2009},
	pages = {13},
}

@article{hamill2009common,
	title = {Common trends in software fault and failure data},
	volume = {35},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Hamill, Maggie and Goseva-Popstojanova, Katerina},
	year = {2009},
	note = {Publisher: IEEE},
	pages = {484--496},
}

@inproceedings{kloos2011risk,
	title = {Risk-based testing of safety-critical embedded systems driven by fault tree analysis},
	booktitle = {2011 {IEEE} fourth international conference on software testing, verification and validation workshops},
	author = {Kloos, Johannes and Hussain, Tanvir and Eschbach, Robert},
	year = {2011},
	note = {tex.organization: IEEE},
	pages = {26--33},
}

@misc{felderer2014taxonomy,
	title = {A taxonomy of risk-based testing},
	publisher = {Springer},
	author = {Felderer, Michael and Schieferdecker, Ina},
	year = {2014},
	note = {Number: 5
Pages: 559–568
Volume: 16},
}

@article{felderer2014multiple,
	title = {A multiple case study on risk-based testing in industry},
	volume = {16},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Felderer, Michael and Ramler, Rudolf},
	year = {2014},
	note = {Publisher: Springer},
	pages = {609--625},
}

@article{freimut_industrial_2005,
	title = {An industrial case study of implementing and validating defect classification for process improvement and quality management},
	abstract = {Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements.},
	language = {en},
	author = {Freimut, B and Denger, C and Ketterer, M},
	year = {2005},
	pages = {10},
}

@inproceedings{felderer2013using,
	title = {Using defect taxonomies to improve the maturity of the system test process: results from an industrial case study},
	booktitle = {International conference on software quality},
	author = {Felderer, Michael and Beer, Armin},
	year = {2013},
	note = {tex.organization: Springer},
	pages = {125--146},
}

@inproceedings{kelly2001case,
	title = {A case study in the use of defect classification in inspections},
	booktitle = {Proceedings of the 2001 conference of the {Centre} for {Advanced} {Studies} on {Collaborative} research},
	author = {Kelly, Diane and Shepard, Terry},
	year = {2001},
	pages = {7},
}

@article{lutz2004empirical,
	title = {Empirical analysis of safety-critical anomalies during operations},
	volume = {30},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Lutz, Robyn R and Mikulski, Inés Carmen},
	year = {2004},
	note = {Publisher: IEEE},
	pages = {172--180},
}

@inproceedings{freimut2005industrial,
	title = {An industrial case study of implementing and validating defect classification for process improvement and quality management},
	booktitle = {11th {IEEE} international software metrics symposium ({METRICS}'05)},
	author = {Freimut, Bernd and Denger, Christian and Ketterer, Markus},
	year = {2005},
	note = {tex.organization: IEEE},
	pages = {10--pp},
}

@article{falessi_failure_nodate,
	title = {On {Failure} {Classification}: {The} {Impact} of "{Getting} {It} {Wrong}"},
	abstract = {Bug classification is a well-established practice which supports important activities such as enhancing verification and validation (V\&V) efficiency and effectiveness. The state of the practice is manual and hence classification errors occur. This paper investigates the sensitivity of the value of bug classification (specifically, failure type classification) to its error rate; i.e., the degree to which misclassified historic bugs decrease the V\&V effectiveness (i.e., the ability to find bugs of a failure type of interest). Results from the analysis of an industrial database of more than 3,000 bugs show that the impact of classification error rate on V\&V effectiveness significantly varies with failure type. Specifically, there are failure types for which a 5\% classification error can decrease the ability to find them by 66\%. Conversely, there are failure types for which the V\&V effectiveness is robust to very high error rates. These results show the utility of future research aimed at: 1) providing better tool support for decreasing human errors in classifying the failure type of bugs, 2) providing more robust approaches for the selection of V\&V techniques, and 3) including robustness as an important criterion when evaluating technologies.},
	language = {en},
	author = {Falessi, Davide and Kidwell, Bill and Hayes, Jane Huffman and Shull, Forrest},
	pages = {4},
}

@inproceedings{el1998repeatability,
	title = {The repeatability of code defect classifications},
	booktitle = {Proceedings ninth international symposium on software reliability engineering (cat. {No}. {98TB100257})},
	author = {El Emam, Khaled and Wieczorek, Isabella},
	year = {1998},
	note = {tex.organization: IEEE},
	pages = {322--333},
}

@inproceedings{falessi2014failure,
	title = {On failure classification: {The} impact of" getting it wrong"},
	booktitle = {Companion proceedings of the 36th international conference on software engineering},
	author = {Falessi, Davide and Kidwell, Bill and Huffman Hayes, Jane and Shull, Forrest},
	year = {2014},
	pages = {512--515},
}

@article{hazzan_reective_2002,
	title = {The reﬂective practitioner perspective in software engineering education q},
	abstract = {This paper focuses on the application of the reﬂective practitioner (RP) perspective to the profession of software engineering (SE). The RP perspective guides professional people to rethink their professional creations during and after the accomplishment of the creation process. Analysis of the ﬁeld of SE supports the adoption of the RP perspective to SE in general and to SE education in particular. The RP perspective emphasizes the studio––the basic training method in architecture schools––as the educational environment for design studies. In such studios students develop projects with a close guidance of a tutor. Analysis of the kind of tasks that architecture students are working on and a comparison of these tasks to the problems that SE students are facing, suggest that the studio may be an appropriate teaching method in SE as well. The paper presents the main ideas of the RP perspective and examines its ﬁtness to SE in general and to SE education in particular. The discussion is based on analysis of the RP perspective and of the SE profession, visits to architecture studios, and conversations with tutors in architecture studios and with computing science practitioners.},
	language = {en},
	journal = {Journal of Systems and Software (JSS)},
	author = {Hazzan, Orit},
	year = {2002},
	pages = {11},
}

@incollection{goos_reflective_2003,
	address = {Berlin, Heidelberg},
	title = {The {Reflective} {Practitioner} {Perspective} in {eXtreme} {Programming}},
	volume = {2753},
	isbn = {978-3-540-40662-4 978-3-540-45122-8},
	url = {http://link.springer.com/10.1007/978-3-540-45122-8_7},
	abstract = {This paper examines ways by which a reflective mode of thinking may improve eXtreme Programming (XP) practices. Specifically, the paper describes the reflective practitioner perspective and suggests specific ways in which such an approach may be interwoven within XP practices.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Extreme {Programming} and {Agile} {Methods} - {XP}/{Agile} {Universe} 2003},
	publisher = {Springer Berlin Heidelberg},
	author = {Hazzan, Orit and Tomayko, Jim},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Maurer, Frank and Wells, Don},
	year = {2003},
	doi = {10.1007/978-3-540-45122-8_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {51--61},
}

@article{hazzan_reective_2002-1,
	title = {The reﬂective practitioner perspective in software engineering education q},
	abstract = {This paper focuses on the application of the reﬂective practitioner (RP) perspective to the profession of software engineering (SE). The RP perspective guides professional people to rethink their professional creations during and after the accomplishment of the creation process. Analysis of the ﬁeld of SE supports the adoption of the RP perspective to SE in general and to SE education in particular. The RP perspective emphasizes the studio––the basic training method in architecture schools––as the educational environment for design studies. In such studios students develop projects with a close guidance of a tutor. Analysis of the kind of tasks that architecture students are working on and a comparison of these tasks to the problems that SE students are facing, suggest that the studio may be an appropriate teaching method in SE as well. The paper presents the main ideas of the RP perspective and examines its ﬁtness to SE in general and to SE education in particular. The discussion is based on analysis of the RP perspective and of the SE profession, visits to architecture studios, and conversations with tutors in architecture studios and with computing science practitioners.},
	language = {en},
	author = {Hazzan, Orit},
	year = {2002},
	pages = {11},
}

@article{raelin2001public,
	title = {Public reflection as the basis of learning},
	volume = {32},
	number = {1},
	journal = {Management learning},
	author = {Raelin, Joseph A},
	year = {2001},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {11--30},
}

@inproceedings{moore_lessons_2000,
	address = {Limerick, Ireland},
	title = {Lessons learned from teaching reflective software engineering using the {Leap} toolkit},
	isbn = {978-1-58113-206-9},
	url = {http://portal.acm.org/citation.cfm?doid=337180.337508},
	doi = {10.1145/337180.337508},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Proceedings of the 22nd international conference on {Software} engineering  - {ICSE} '00},
	publisher = {ACM Press},
	author = {Moore, Carlton A.},
	year = {2000},
	pages = {672--675},
}

@article{babb_embedding_2014,
	title = {Embedding {Reflection} and {Learning} into {Agile} {Software} {Development}},
	volume = {31},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/6785921/},
	doi = {10.1109/MS.2014.54},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Software},
	author = {Babb, Jeffry and Hoda, Rashina and Norbjerg, Jacob},
	month = jul,
	year = {2014},
	pages = {51--57},
}

@article{mitchell_assessment_nodate,
	title = {An {Assessment} {Strategy} to {Determine} {Learning} {Outcomes} in a {Software} {Engineering} {Problem}-based {Learning} {Course}},
	language = {en},
	author = {Mitchell, George G and Delaney, James Declan},
	pages = {9},
}

@inproceedings{upchurch_reflective_1999,
	address = {San Juan, Puerto Rico},
	title = {Reflective essays in software engineering},
	volume = {3},
	isbn = {978-0-7803-5643-6},
	url = {http://ieeexplore.ieee.org/document/840324/},
	doi = {10.1109/FIE.1999.840324},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {{FIE}'99 {Frontiers} in {Education}. 29th {Annual} {Frontiers} in {Education} {Conference}. {Designing} the {Future} of {Science} and {Engineering} {Education}. {Conference} {Proceedings} ({IEEE} {Cat}. {No}.{99CH37011}},
	publisher = {Stripes Publishing L.L.C},
	author = {Upchurch, R.L. and Sims-Knight, J.E.},
	year = {1999},
	pages = {13A6/13--13A6/19},
}

@article{parsons_coderetreats_2014,
	title = {Coderetreats: {Reflective} {Practice} and the {Game} of {Life}},
	volume = {31},
	issn = {0740-7459},
	shorttitle = {Coderetreats},
	url = {http://ieeexplore.ieee.org/document/6756713/},
	doi = {10.1109/MS.2014.25},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Software},
	author = {Parsons, David and Mathrani, Anuradha and Susnjak, Teo and Leist, Arno},
	month = jul,
	year = {2014},
	pages = {58--64},
}

@incollection{oconnor_building_2009,
	address = {Berlin, Heidelberg},
	title = {Building an {Observatory} of {Course}-of-{Action} in {Software} {Engineering}: {Towards} a {Link} between {ISO}/{IEC} {Software} {Engineering} {Standards} and a {Reflective} {Practice}},
	volume = {42},
	isbn = {978-3-642-04132-7 978-3-642-04133-4},
	shorttitle = {Building an {Observatory} of {Course}-of-{Action} in {Software} {Engineering}},
	url = {http://link.springer.com/10.1007/978-3-642-04133-4_16},
	abstract = {As a help to compete in an evolving market, small software companies may use an observatory of their course-of-action. The course of action considers the observable aspect of the actor’s activity. Its analysis provides a description of actors’ activity and it can express recommendations concerning both the individual situations and the collective situation. The observatory is an articulated set of data collecting methods supported with semantic wikis and a dedicated application. A case study, based on the activity of a team of 6 young software engineers, depicts some aspects of the building and the filling of the course-of-action observatory. As primary results of this work, we may think that observing and analyzing software engineer’s activity help to reveal his/her theory-in-use – what governs engineers’ behavior and tends to be tacit structures – That may help engineers to establish links between “Project Processes-in-use” and a simplified Process Reference Model and contribute to reduce the fit between a project-in-action and espoused SE standards.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Software {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bru, François-Xavier and Frappin, Gaëlle and Legrand, Ludovic and Merrer, Estéban and Piteau, Sylvain and Salou, Guillaume and Saliou, Philippe and Ribaud, Vincent},
	editor = {O’Connor, Rory V. and Baddoo, Nathan and Cuadrago Gallego, Juan and Rejas Muslera, Ricardo and Smolander, Kari and Messnarz, Richard},
	year = {2009},
	doi = {10.1007/978-3-642-04133-4_16},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {185--200},
}

@inproceedings{dors_reflective_2020,
	address = {Germany},
	title = {Reflective {Practice} in {Software} {Development} {Studios}: {Findings} from an {Ethnographic} {Study}},
	isbn = {978-1-72816-807-4},
	shorttitle = {Reflective {Practice} in {Software} {Development} {Studios}},
	url = {https://ieeexplore.ieee.org/document/9206217/},
	doi = {10.1109/CSEET49119.2020.9206217},
	abstract = {Over the last two decades, software educators have adopted new approaches, techniques, and tools for practical learning. Previous research has found that studio-based learning, which has been in use by some design and architecture courses, is suitable for learning the practical aspects of software engineering. The studies available recognize the presence of reflective practice in software development studios; however, they do not show empirical evidence of its contribution to learning. The goal of this study is to understand the use of reflective practice in software development studios and its contributions to the practical learning of software engineering. The qualitative data were collected using an ethnographic method through participant observation and from written students' self-reflections, which were analyzed using Cycle Coding Method supported by Atlas.ti Qualitative Data Analysis tool. The findings suggest that reflective practice promotes the emergence of new ideas while contributing to the development of skills that are valuable to software engineering professional practice. This research presents a qualitative look at how these are developed in the context of a particular software development studio.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {2020 {IEEE} 32nd {Conference} on {Software} {Engineering} {Education} and {Training} ({CSEE}\&{T})},
	publisher = {IEEE},
	author = {Dors, Tania Mara and Van Amstel, Frederick M. C. and Binder, Fabio and Reinehr, Sheila and Malucelli, Andreia},
	month = nov,
	year = {2020},
	pages = {1--10},
}

@article{ras_experience_2007,
	title = {Experience {Management} {Wikis} for {Reflective} {Practice} in {Software} {Capstone} {Projects}},
	volume = {50},
	issn = {0018-9359, 1557-9638},
	url = {https://ieeexplore.ieee.org/document/4358726/},
	doi = {10.1109/TE.2007.904580},
	abstract = {Software engineering curriculum guidelines state that students should practice methods, techniques, and tools. A capstone project is one possibility to address this aim. A capstone project helps the students to increase their problem solving competencies, improve their social skills (e.g., communication skills), and gather practical experience. A crux of such projects is that students perform “reﬂective” practice in order to learn from their experiences. The authors believe that experience gathering and reuse are effective techniques to stimulate reﬂective activities. An adapted free- and open-source Wiki-based system called software organization platform (SOP) is used to support students in managing their observations and experiences. The system can be used for experience exchange within the team and for experience reuse in forthcoming projects. The results of a case study show that standard Wiki functions improve communication and information sharing by means of explicit observation and experience documentation. A total of 183 documented observations and experiences at the end of the project provide a measure for the amount of reﬂection students have had during the capstone project. Still, the advantages of using Wikis will decrease when no technical adaptations of the Wiki to the learning objectives and to the software engineering tasks are made. Limitations of the case study, future evaluation steps, and planned developments of SOP will be provided in this paper.},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Transactions on Education},
	author = {Ras, Eric and Carbon, Ralf and Decker, BjÖrn and Rech, JÖrg},
	month = nov,
	year = {2007},
	pages = {312--320},
}

@article{bull_supporting_2014,
	title = {Supporting {Reflective} {Practice} in {Software} {Engineering} {Education} through a {Studio}-{Based} {Approach}},
	volume = {31},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/6774769/},
	doi = {10.1109/MS.2014.52},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Software},
	author = {Bull, Christopher N. and Whittle, Jon},
	month = jul,
	year = {2014},
	pages = {44--50},
}

@inproceedings{prior_reflection_2016,
	address = {Canberra Australia},
	title = {Reflection is hard: teaching and learning reflective practice in a software studio},
	isbn = {978-1-4503-4042-7},
	shorttitle = {Reflection is hard},
	url = {https://dl.acm.org/doi/10.1145/2843043.2843346},
	doi = {10.1145/2843043.2843346},
	abstract = {We have observed that it is a non-trivial exercise for undergraduate students to learn how to reflect. Reflective practice is now recognised as important for software developers and has become a key part of software studios in universities, but there is limited empirical investigation into how best to teach and learn reflection. In the literature on reflection in software studios, there are many papers that claim that reflection in the studio is mandatory. However, there is inadequate guidance about teaching early stage students to reflect in that literature. The essence of the work presented in this paper is a beginning to the consideration of how the teaching of software development can best be combined with teaching reflective practice for early stage software development students.. We started on a research programme to understand how to encourage students to learn to reflect. As we were unsure about teaching reflection, and we wished to change our teaching as we progressively understood better what to do, we chose action research as the most suitable approach. Within the action research cycles we used ethnography to understand what was happening with the students when they attempted to reflect. This paper reports on the first 4 semesters of research.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Proceedings of the {Australasian} {Computer} {Science} {Week} {Multiconference}},
	publisher = {ACM},
	author = {Prior, Julia and Ferguson, Samuel and Leaney, John},
	month = feb,
	year = {2016},
	pages = {1--8},
}

@article{dyba_reflective_2014,
	title = {The {Reflective} {Software} {Engineer}: {Reflective} {Practice}},
	volume = {31},
	issn = {0740-7459},
	shorttitle = {The {Reflective} {Software} {Engineer}},
	url = {http://ieeexplore.ieee.org/document/6834681/},
	doi = {10.1109/MS.2014.97},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Software},
	author = {Dyba, Tore and Maiden, Neil and Glass, Robert},
	month = jul,
	year = {2014},
	pages = {32--36},
}

@article{oconnor_foundations_2007,
	title = {Foundations in {Nursing} and {Health} {Care}: {Beginning} {Reflective} {Practice}},
	volume = {85},
	issn = {00012092},
	shorttitle = {Foundations in {Nursing} and {Health} {Care}},
	url = {http://doi.wiley.com/10.1016/S0001-2092(07)60057-X},
	doi = {10.1016/S0001-2092(07)60057-X},
	language = {en},
	number = {2},
	urldate = {2022-03-23},
	journal = {AORN Journal},
	author = {O'Connor, Ellen},
	month = feb,
	year = {2007},
	pages = {429},
}

@article{amulya_what_nodate,
	title = {What is {Reflective} {Practice}?},
	language = {en},
	author = {Amulya, Joy},
	pages = {4},
}

@article{finlay_reflecting_nodate,
	title = {Reflecting on ‘{Reflective} practice’},
	language = {en},
	author = {Finlay, Linda},
	pages = {28},
}

@article{van_manen_epistemology_1995,
	title = {On the {Epistemology} of {Reflective} {Practice}},
	volume = {1},
	issn = {1354-0602, 1470-1278},
	url = {https://www.tandfonline.com/doi/full/10.1080/1354060950010104},
	doi = {10.1080/1354060950010104},
	abstract = {Schb'n (1987) has suggested that professional education undervalues practical knowledge and grants privileged status to intellectual scientific and rational knowledge forms that may only be marginally relevant to practical acting. This is not just an issue of sociology of knowledge. The literature of teaching and teacher education has shown that professional practices of educating cannot be properly understood unless we are willing to conceive of practical knowledge and reflective practice quite differently. It is for this reason that I would like to raise some questions about the meaning and place of practical reflection in teaching and about the relation between knowledge and action in teaching, the kind of teaching that is educational or pedagogical.},
	language = {en},
	number = {1},
	urldate = {2022-03-23},
	journal = {Teachers and Teaching},
	author = {van Manen, Max},
	month = mar,
	year = {1995},
	pages = {33--50},
}

@article{mamede_structure_2004,
	title = {The structure of reflective practice in medicine},
	volume = {38},
	issn = {0308-0110, 1365-2923},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1365-2929.2004.01917.x},
	doi = {10.1111/j.1365-2929.2004.01917.x},
	abstract = {BACKGROUND The capability to reﬂect consciously upon one’s professional practice is generally considered important for the development of expertise and, hence, for education. However, to our knowledge no empirical research has been conducted to date into the nature of reﬂective practice in medicine.
PURPOSE To study the structure of reﬂective practice in medicine.
METHODS A questionnaire based on the literature was developed and administered to a group of primary care doctors. The data were subjected to conﬁrmatory factor analysis using structural equations modelling.
RESULTS A 5-factor model of reﬂective practice emerged. It consisted of the following factors: deliberate induction; deliberate deduction; testing and synthesising; openness for reﬂection, and meta-reasoning. The model ﬁtted the data sufﬁciently.
CONCLUSION A multidimensional structure of reﬂective practice in medicine was brought to light by the study. Its components in terms of reasoning processes, behaviours and attitudes were identiﬁed and measured among doctors. Once conceptualised and measured, reﬂective practice can be studied to gain a better understanding of its relation to expertise development in medicine. In addition, training students to apply reﬂective practices may become a goal in medical education.},
	language = {en},
	number = {12},
	urldate = {2022-03-23},
	journal = {Medical Education},
	author = {Mamede, Silvia and Schmidt, Henk G},
	month = dec,
	year = {2004},
	pages = {1302--1308},
}

@incollection{baumeister_reflection_2017,
	address = {Cham},
	title = {Reflection in {Agile} {Retrospectives}},
	volume = {283},
	isbn = {978-3-319-57632-9 978-3-319-57633-6},
	url = {http://link.springer.com/10.1007/978-3-319-57633-6_1},
	abstract = {A retrospective is a standard agile meeting practice designed for agile software teams to reflect and tune their process. Despite its integral importance, we know little about what aspects are focused upon during retrospectives and how reflection occurs in this practice. We conducted Case Study research involving data collected from interviews of sixteen software practitioners from four agile teams and observations of their retrospective meetings. We found that the important aspects focused on during the retrospective meeting include obstacles, feelings, previous action points, background reasons, areas of improvement, different ideas and perspectives and generating a plan. Reflection occurs when the agile teams embody these aspects within three levels of reflection: reporting and responding, relating and reasoning, and reconstructing. Critically, we show that agile teams may not achieve all levels of reflection simply by performing retrospective meetings. One of the key contributions of our work is to present a reflection framework for agile retrospective meetings that explains and embeds three levels of reflection within the five steps of a standard agile retrospective. Agile teams can use this framework to achieve better focus and higher levels of reflection in their retrospective meetings.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	publisher = {Springer International Publishing},
	author = {Andriyani, Yanti and Hoda, Rashina and Amor, Robert},
	editor = {Baumeister, Hubert and Lichter, Horst and Riebisch, Matthias},
	year = {2017},
	doi = {10.1007/978-3-319-57633-6_1},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {3--19},
}

@inproceedings{li_redoshunter_2021,
	title = {{ReDoSHunter}: {A} {Combined} {Static} and {Dynamic} {Approach} for {Regular} {Expression} {DoS} {Detection}},
	abstract = {Regular expression Denial of Service (ReDoS) is a class of algorithmic complexity attacks using the regular expressions (regexes) that cause the typical backtracking-based matching algorithms to run super-linear time. Due to the wide adoption of regexes in computation, ReDoS poses a pervasive and serious security threat. Early detection of ReDoSvulnerable regexes in software is thus vital. Existing detection approaches mainly fall into two categories: static and dynamic analysis. However, they all suffer from either poor precision or poor recall in the detection of vulnerable regexes. The problem of accurately detecting vulnerable regexes at high precision and high recall remains unsolved. Furthermore, we observed that many ReDoS-vulnerable regex contain more than one vulnerability in reality. Another problem with existing approaches is that they are incapable of detecting multiple vulnerabilities in one regex. To address these two problems, we propose ReDoSHunter, a ReDoS-vulnerable regex detection framework that can effectively pinpoint the multiple vulnerabilities in a vulnerable regex, and generate examples of attack-triggering strings. ReDoSHunter is driven by ﬁve vulnerability patterns derived from massive vulnerable regexes. Besides pinpointing vulnerabilities, ReDoSHunter can assess the degree (i.e., exponential or polynomial) of the vulnerabilities detected. Our experiment results show that ReDoSHunter achieves 100\% precision and 100\% recall in the detection of ReDoS-vulnerable regexes in three large-scale datasets with 37,651 regexes. It signiﬁcantly outperforms seven state-of-the-art techniques. ReDoSHunter uncovered 28 new ReDoS-vulnerabilities in 26 well-maintained popular projects, resulting in 26 assigned CVEs and 2 ﬁxes.},
	language = {en},
	booktitle = {{IEEE} {Security} \& {Privacy}},
	author = {Li, Yeting and Chen, Zixuan and Cao, Jialun and Xu, Zhiwu and Peng, Qiancheng and Chen, Haiming and Chen, Liyuan and Cheung, Shing-Chi},
	year = {2021},
	pages = {19},
}

@misc{smith_social_2007,
	title = {Social {Software} {Building} {Blocks}},
	url = {https://web.archive.org/web/20171123070545/http://nform.com/ideas/social-software-building-blocks},
	urldate = {2022-02-15},
	author = {Smith, Gene},
	month = apr,
	year = {2007},
}

@article{khan_social_2014,
	title = {Social {Media} {Risks} and {Benefits}: {A} {Public} {Sector} {Perspective}},
	volume = {32},
	issn = {0894-4393},
	shorttitle = {Social {Media} {Risks} and {Benefits}},
	url = {https://doi.org/10.1177/0894439314524701},
	doi = {10.1177/0894439314524701},
	abstract = {Social media are becoming an important intermediary for interaction between governments, governments and citizens, and governmental agencies and businesses. This is due to the unique characteristics of social media: openness, participation, and sharing. However, despite rapid adoption, a growing concern and skepticism regarding the use of social media exists in the public sector. The purpose of this study is to investigate empirically the risks and benefits of social media use by public agencies. For this purpose, a research model was developed and tested in a survey of 289 government sector employees from six South Korean government research institutes. We found that both risks (i.e., social risk, time, psychological risks, and privacy concern) and benefits (i.e., social connectivity, social involvement, information attainment, and entertainment) significantly affect public sector employees’ satisfaction with and intention to use social media. However, the effect of the benefits on users’ satisfaction was stronger than the risks. The results of the study have important implications for researchers and policy makers.},
	language = {en},
	number = {5},
	urldate = {2022-02-27},
	journal = {Social Science Computer Review},
	author = {Khan, Gohar Feroz and Swar, Bobby and Lee, Sang Kon},
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	keywords = {government research institutes (GRIs), public sector, risks and benefits, social media, social network service (SNS)},
	pages = {606--627},
}

@inproceedings{bauer_post_2013,
	address = {New York, NY, USA},
	series = {{WPES} '13},
	title = {The post anachronism: the temporal dimension of facebook privacy},
	isbn = {978-1-4503-2485-4},
	shorttitle = {The post anachronism},
	url = {https://doi.org/10.1145/2517840.2517859},
	doi = {10.1145/2517840.2517859},
	abstract = {This paper reports on two studies that investigate empirically how privacy preferences about the audience and emphasis of Facebook posts change over time. In a 63-participant longitudinal study, participants gave their audience and emphasis preferences for up to ten of their Facebook posts in the week they were posted, again one week later, and again one month later. In a 234-participant retrospective study, participants expressed their preferences about posts made in the past week, as well as one year prior. We found that participants did not want content to fade away wholesale with age; the audience participants wanted to be able to access posts remained relatively constant over time. However, participants did want a handful of posts to become more private over time, as well as others to become more visible. Participants' predictions about how their preferences would change correlated poorly with their actual changes in preferences over time, casting doubt on ideas for setting an expiration date for content. Although older posts were seen as less relevant and had often been forgotten, participants found value in these posts for reminiscence. Surprisingly, we observed few concerns about privacy or self-presentation for older posts. We discuss our findings' implications for retrospective privacy mechanisms.},
	urldate = {2022-03-21},
	booktitle = {Proceedings of the 12th {ACM} workshop on {Workshop} on privacy in the electronic society},
	publisher = {Association for Computing Machinery},
	author = {Bauer, Lujo and Cranor, Lorrie Faith and Komanduri, Saranga and Mazurek, Michelle L. and Reiter, Michael K. and Sleeper, Manya and Ur, Blase},
	month = nov,
	year = {2013},
	keywords = {access control, facebook, privacy, sns, temporality, time, users},
	pages = {1--12},
}

@article{redmiles_i_2019,
	title = {“{I} {Just} {Want} to {Feel} {Safe}”: {A} {Diary} {Study} of {Safety} {Perceptions} on {Social} {Media}},
	volume = {13},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	shorttitle = {“{I} {Just} {Want} to {Feel} {Safe}”},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/3356},
	abstract = {Social media can increase social capital, provide entertainment, and enable meaningful discourse. However, threats to safety experienced on social media platforms can inhibt users’ ability to gain these benefits. Threats to safety—whether real or perceived—detract from the pleasure people get out of their online interactions and damage the quality of online social spaces. While prior work has individually explored specific threats to safety – privacy, security, harassment – in this work we more broadly capture and characterize the full breadth of day-to-day experiences that influence users’ overall perceptions of safety on social media. We explore these perceptions through a three-week diary study (n=39). We contribute a novel, multidimensional taxonomy of how social media users define ’safety’, centered around security, privacy, and community. We conclude with a discussion of how safety perceptions can be used as a metric for social media quality, and detail the potential for enhancing safety perception through communityenhancing affordances and algorithmic transparency.},
	language = {en},
	urldate = {2022-03-21},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Redmiles, Elissa M. and Bodford, Jessica and Blackwell, Lindsay},
	month = jul,
	year = {2019},
	pages = {405--416},
}

@inproceedings{kim_rvfuzzer_2019,
	title = {\{{RVFuzzer}\}: {Finding} {Input} {Validation} {Bugs} in {Robotic} {Vehicles} through \{{Control}-{Guided}\} {Testing}},
	isbn = {978-1-939133-06-9},
	shorttitle = {\{{RVFuzzer}\}},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/kim},
	language = {en},
	urldate = {2022-03-21},
	author = {Kim, Taegyu and Kim, Chung Hwan and Rhee, Junghwan and Fei, Fan and Tu, Zhan and Walkup, Gregory and Zhang, Xiangyu and Deng, Xinyan and Xu, Dongyan},
	year = {2019},
	pages = {425--442},
}

@article{karhumaa_bluetooth_nodate,
	title = {{BLUETOOTH} {LOW} {ENERGY} {LINK} {LAYER} {INJECTION}},
	abstract = {Bluetooth Low Energy is a very widely used short-range wireless technology. During the last few years many high visibility Bluetooth related vulnerabilities have been discovered. A signiﬁcant amount of them have had an impact on implementations of the lowest protocol layers of Bluetooth in ﬁrmware running on separate embedded System on Chip dedicated for wireless communication. Bluetooth LE Link Layer implementations have not yet been under systematic fuzzing by vendors as there has been no mature way to inject fuzzed Link Layer packets over the air to the target device.},
	language = {en},
	author = {Karhumaa, Matias},
	pages = {36},
}

@inproceedings{aranda_secret_2009,
	title = {The secret life of bugs: {Going} past the errors and omissions in software repositories},
	shorttitle = {The secret life of bugs},
	doi = {10.1109/ICSE.2009.5070530},
	abstract = {Every bug has a story behind it. The people that discover and resolve it need to coordinate, to get information from documents, tools, or other people, and to navigate through issues of accountability, ownership, and organizational structure. This paper reports on a field study of coordination activities around bug fixing that used a combination of case study research and a survey of software professionals. Results show that the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through automation of electronic repositories, and that such automation provides incomplete and often erroneous accounts of coordination. The paper uses rich bug histories and survey results to identify common bug fixing coordination patterns and to provide implications for tool designers and researchers of coordination in software development.},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	author = {Aranda, Jorge and Venolia, Gina},
	month = may,
	year = {2009},
	note = {ISSN: 1558-1225},
	keywords = {Automation, Computer bugs, Data mining, History, Navigation, Productivity, Programming, Software debugging, Software development management, Spatial databases},
	pages = {298--308},
}

@misc{hopkins_how_2017,
	title = {How to {Define} {Social} {Media} – {An} {Academic} {Summary}},
	url = {http://julianhopkins.com/how-to-define-social-media-an-academic-summary/},
	language = {en-GB},
	urldate = {2022-02-25},
	author = {Hopkins, Julian},
	month = oct,
	year = {2017},
}

@article{gurses_two_2013,
	title = {Two tales of privacy in online social networks},
	volume = {11},
	issn = {1558-4046},
	doi = {10.1109/MSP.2013.47},
	abstract = {Privacy is one of the friction points that emerge when communications are mediated in online social networks (OSNs). Different communities of computer science researchers have framed the OSN privacy problem as one of surveillance, institutional privacy, or social privacy. In tackling these problems, researchers have also treated them as if they were independent. In this article, the authors argue that the different privacy problems are entangled and that OSN privacy research would benefit from a more holistic approach.},
	number = {3},
	journal = {IEEE Security Privacy},
	author = {Gürses, Seda and Diaz, Claudia},
	month = may,
	year = {2013},
	note = {Conference Name: IEEE Security Privacy},
	keywords = {Computer science, Data privacy, Media, Privacy, Security, Social networking (online), Surveillance, human-computer interaction, online social networks, privacy-enhancing technologies, social privacy, surveillance},
	pages = {29--37},
}

@inproceedings{guo_mining_2012,
	title = {Mining {Privacy} {Settings} to {Find} {Optimal} {Privacy}-{Utility} {Tradeoffs} for {Social} {Network} {Services}},
	doi = {10.1109/SocialCom-PASSAT.2012.22},
	abstract = {Privacy has been a big concern for users of social network services (SNS). On recent criticism about privacy protection, most SNS now provide fine privacy controls, allowing users to set visibility levels for almost every profile item. However, this also creates a number of difficulties for users. First, SNS providers often set most items by default to the highest visibility to improve the utility of social network, which may conflict with users' intention. It is often formidable for a user to fine-tune tens of privacy settings towards the user desired settings. Second, tuning privacy settings involves an intricate tradeoff between privacy and utility. When you turn off the visibility of one item to protect your privacy, the social utility of that item is turned off as well. It is challenging for users to make a tradeoff between privacy and utility for each privacy setting. We propose a framework for users to conveniently tune the privacy settings towards the user desired privacy level and social utilities. It mines the privacy settings of a large number of users in a SNS, e.g., Facebook, to generate latent trait models for the level of privacy concern and the level of utility preference. A tradeoff algorithm is developed for helping users find the optimal privacy settings for a specified level of privacy concern and a personalized utility preference. We crawl a large number of Facebook accounts and derive the privacy settings with a novel method. These privacy setting data are used to validate and showcase the proposed approach.},
	booktitle = {2012 {International} {Conference} on {Privacy}, {Security}, {Risk} and {Trust} and 2012 {International} {Confernece} on {Social} {Computing}},
	author = {Guo, Shumin and Chen, Keke},
	month = sep,
	year = {2012},
	keywords = {Data Mining, Data models, Data privacy, Electronic mail, Facebook, Privacy, Social Network Services, Training data, Utility},
	pages = {656--665},
}

@inproceedings{alexander_privacy_2018,
	title = {Privacy {Shield} securing privacy in social networks},
	doi = {10.1109/ICCIC.2018.8782322},
	abstract = {Privacy is so important for social networks. Social networks are uncertain in terms of privacy. Now a days, privacy breach is a major problem. Data privacy will be lost when it is distributed in a social network. This is because of multi distributing the data without taking control of data. To deal with these problems, Securing privacy in Social Networks has been proposed based on privacy control over data. An efficient system named, Privacy Shield is designed to preserve privacy of data which can be managed by shared users in social networks. A third party server called Eco Server is used to do this. User privileges are administered by controlling the contents shared in social networks by the eco server. Base64 algorithm is used for data encryption.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Computational} {Intelligence} and {Computing} {Research} ({ICCIC})},
	author = {Alexander, Elizabeth and Vanathi, R. and Dhikhi, T and Sowmiya, M.},
	month = dec,
	year = {2018},
	note = {ISSN: 2473-943X},
	keywords = {Data privacy, Feeds, MD5, Organizations, Privacy, Security, Servers, Social networking (online), data privacy, post sharing, social networks, social profiling},
	pages = {1--4},
}

@inproceedings{namara_potential_2018,
	title = {The {Potential} for {User}-{Tailored} {Privacy} on {Facebook}},
	doi = {10.1109/PAC.2018.00010},
	abstract = {Research shows that Facebook users differ extensively in their use of various privacy features, and that they generally find it difficult to translate their desired privacy preferences into concrete interface actions. Our work explores the use of User-Tailored Privacy (UTP) to adapt Facebook's privacy features to the user's personal preferences. We developed adaptive versions of 19 Facebook privacy features, and for each feature we test three adaptation methods (Automation, Highlight and Suggestion) that can be used to implement the adaptive behavior. In a "think-aloud" semistructured interview study (N=18), we show participants paper prototypes of our adaptive privacy features and ask participants to judge the presented adaptive capabilities and the three adaptation methods that implement them. Our findings provide insights into the viability of User-Tailored Privacy. Specifically, we find that the optimal adaptation method depends on the users' familiarity with the privacy feature and how they use them, and their judgment of the awkwardness and irreversibility of the implemented privacy functionality. We conclude with design recommendations for the implementation of User-Tailored Privacy on Facebook and other social network platforms.},
	booktitle = {2018 {IEEE} {Symposium} on {Privacy}-{Aware} {Computing} ({PAC})},
	author = {Namara, Moses and Sloan, Henry and Jaiswal, Priyanka and Knijnenburg, Bart P.},
	month = sep,
	year = {2018},
	keywords = {Automation, Data privacy, Decision making, Facebook, Privacy, Task analysis, privacy, privacy on social media, social media, user-tailored privacy},
	pages = {31--42},
}

@inproceedings{smith_big_2012,
	title = {Big data privacy issues in public social media},
	doi = {10.1109/DEST.2012.6227909},
	abstract = {Big Data is a new label given to a diverse field of data intensive informatics in which the datasets are so large that they become hard to work with effectively. The term has been mainly used in two contexts, firstly as a technological challenge when dealing with dataintensive domains such as high energy physics, astronomy or internet search, and secondly as a sociological problem when data about us is collected and mined by companies such as Facebook, Google, mobile phone companies, retail chains and governments. In this paper we look at this second issue from a new perspective, namely how can the user gain awareness of the personally relevant part Big Data that is publicly available in the social web. The amount of user-generated media uploaded to the web is expanding rapidly and it is beyond the capabilities of any human to sift through it all to see which media impacts our privacy. Based on an analysis of social media in Flickr, Locr, Facebook and Google+, we discuss privacy implications and potential of the emerging trend of geo-tagged social media. We then present a concept with which users can stay informed about which parts of the social Big Data deluge is relevant to them.},
	booktitle = {2012 6th {IEEE} {International} {Conference} on {Digital} {Ecosystems} and {Technologies} ({DEST})},
	author = {Smith, Matthew and Szongott, Christian and Henne, Benjamin and von Voigt, Gabriele},
	month = jun,
	year = {2012},
	note = {ISSN: 2150-4946},
	keywords = {Data handling, Data storage systems, Facebook, Global Positioning System, Information management, Media, Privacy},
	pages = {1--6},
}

@inproceedings{bojanova_classifying_2021,
	title = {Classifying {Memory} {Bugs} {Using} {Bugs} {Framework} {Approach}},
	doi = {10.1109/COMPSAC51774.2021.00159},
	abstract = {In this work, we present an orthogonal classification of memory corruption bugs, allowing precise structured descriptions of related software vulnerabilities. The Common Weakness Enumeration (CWE) is a well-known and used list of software weaknesses. However, it’s exhaustive list approach is prone to gaps and overlaps in coverage. Instead, we utilize the Bugs Framework (BF) approach to define language-independent classes that cover all possible kinds of memory corruption bugs. Each class is a taxonomic category of a weakness type, defined by sets of operations, cause→consequence relations, and attributes. A BF description of a bug or a weakness is an instance of a taxonomic BF class, with one operation, one cause, one consequence, and their attributes. Any memory vulnerability then can be described as a chain of such instances and their consequence–cause transitions. We showcase that BF is a classification system that extends the CWE, providing a structured way to precisely describe real world vulnerabilities. It allows clear communication about software bugs and weaknesses and can help identify exploit mitigation techniques.},
	booktitle = {2021 {IEEE} 45th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Bojanova, Irena and Eduardo Galhardo, Carlos},
	month = jul,
	year = {2021},
	note = {ISSN: 0730-3157},
	keywords = {Bug classification, Computer bugs, Conferences, Resource management, Software, Taxonomy, Tools, bug taxonomy, memory corruption, software vulnerability, software weakness},
	pages = {1157--1164},
}

@misc{biden_remarks_2022,
	title = {Remarks of {President} {Joe} {Biden} – {State} of the {Union} {Address} {As} {Prepared} for {Delivery}},
	url = {https://www.whitehouse.gov/briefing-room/speeches-remarks/2022/03/01/remarks-of-president-joe-biden-state-of-the-union-address-as-delivered/},
	abstract = {United States Capitol Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the},
	language = {en-US},
	urldate = {2022-03-03},
	journal = {The White House},
	author = {Biden, Joe},
	month = mar,
	year = {2022},
}

@article{carr_social_2015,
	title = {Social {Media}: {Defining}, {Developing}, and {Divining}},
	volume = {23},
	issn = {1545-6870},
	shorttitle = {Social {Media}},
	url = {https://doi.org/10.1080/15456870.2015.972282},
	doi = {10.1080/15456870.2015.972282},
	abstract = {What is a social medium, and how may one moderate, isolate, and influence communicative processes within? Although scholars assume an inherent understanding of social media based on extant technology, there is no commonly accepted definition of what social media are, both functionally and theoretically, within communication studies. Given this lack of understanding, cogent theorizing regarding the uses and effects of social media has been limited. This work first draws on extant definitions of social media and subcategories (e.g., social network sites) from public relations, information technology, and management scholarship, as well as the popular press, to develop a definition of social media precise enough to embody these technologies yet robust enough to remain applicable in 2035. It then broadly explores emerging developments in the features, uses, and users of social media for which future theories will need to account. Finally, it divines and prioritizes challenges that may not yet be apparent to theorizing communication processes with and in mercurial social media. We address how social media may uniquely isolate and test communicative principles to advance our understanding of human–human and human–computer interaction. In all, this article provides a common framework to ground and facilitate future communication scholarship and beyond.},
	number = {1},
	urldate = {2022-03-08},
	journal = {Atlantic Journal of Communication},
	author = {Carr, Caleb T. and Hayes, Rebecca A.},
	month = jan,
	year = {2015},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15456870.2015.972282},
	pages = {46--65},
}

@inproceedings{yuan_simple_2014,
	title = {Simple {Testing} {Can} {Prevent} {Most} {Critical} {Failures}: {An} {Analysis} of {Production} {Failures} in {Distributed} \{{Data}-{Intensive}\} {Systems}},
	isbn = {978-1-931971-16-4},
	shorttitle = {Simple {Testing} {Can} {Prevent} {Most} {Critical} {Failures}},
	url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/yuan},
	language = {en},
	urldate = {2022-03-07},
	author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U. and Stumm, Michael},
	year = {2014},
	pages = {249--265},
}

@inproceedings{wang_exploratory_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	isbn = {978-1-4503-8562-6},
	url = {https://doi.org/10.1145/3468264.3468559},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	urldate = {2022-03-07},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	month = aug,
	year = {2021},
	keywords = {Unmanned aerial vehicles, empirical study, software bugs},
	pages = {20--31},
}

@inproceedings{makhshari_iot_2021,
	title = {{IoT} {Bugs} and {Development} {Challenges}},
	doi = {10.1109/ICSE43902.2021.00051},
	abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We collected 5,565 bug reports from 91 representative IoT project repositories and categorized a random sample of 323 based on the observed failures, root causes, and the locations of the faulty components. In addition, we conducted nine interviews with IoT experts to uncover more details about IoT bugs and to gain insight into IoT developers' challenges. Lastly, we surveyed 194 IoT developers to validate our findings and gain further insights. We propose the first bug taxonomy for IoT systems based on our results. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Makhshari, Amir and Mesbah, Ali},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Computer bugs, Correlation, Empirical Study, Faces, Internet of Things, Mining Software Repositories, Software Engineering, Systematics, Taxonomy, Tools},
	pages = {460--472},
}

@inproceedings{seaman_defect_2008,
	address = {New York, NY, USA},
	series = {{ESEM} '08},
	title = {Defect categorization: making use of a decade of widely varying historical data},
	isbn = {978-1-59593-971-5},
	shorttitle = {Defect categorization},
	url = {https://doi.org/10.1145/1414004.1414030},
	doi = {10.1145/1414004.1414030},
	abstract = {This paper describes our experience in aggregating a number of historical datasets containing inspection defect data using different categorization schemes. Our goal was to make use of the historical data by creating models to guide future development projects. We describe our approach to reconciling the different choices used in the historical datasets to categorize defects, and the challenges we faced. We also present a set of recommendations for others involved in classifying defects.},
	urldate = {2022-03-07},
	booktitle = {Proceedings of the {Second} {ACM}-{IEEE} international symposium on {Empirical} software engineering and measurement},
	publisher = {Association for Computing Machinery},
	author = {Seaman, Carolyn B. and Shull, Forrest and Regardie, Myrna and Elbert, Denis and Feldmann, Raimund L. and Guo, Yuepu and Godfrey, Sally},
	month = oct,
	year = {2008},
	keywords = {defect categories, defects, historical data},
	pages = {149--157},
}

@article{hafiz_game_2016,
	title = {Game of detections: how are security vulnerabilities discovered in the wild?},
	volume = {21},
	issn = {1573-7616},
	shorttitle = {Game of detections},
	url = {https://doi.org/10.1007/s10664-015-9403-7},
	doi = {10.1007/s10664-015-9403-7},
	abstract = {There is little or no information available on what actually happens when a software vulnerability is detected. We performed an empirical study on reporters of the three most prominent security vulnerabilities: buffer overflow, SQL injection, and cross site scripting vulnerabilities. The goal was to understand the methods and tools used during the discovery and whether the community of developers exploring one security vulnerability differs—in their approach—from another community of developers exploring a different vulnerability. The reporters were featured in the SecurityFocus repository for twelve month periods for each vulnerability. We collected 127 responses. We found that the communities differ based on the security vulnerability they target; but within a specific community, reporters follow similar approaches. We also found a serious problem in the vulnerability reporting process that is common for all communities. Most reporters, especially the experienced ones, favor full-disclosure and do not collaborate with the vendors of vulnerable software. They think that the public disclosure, sometimes supported by a detailed exploit, will put pressure on vendors to fix the vulnerabilities. But, in practice, the vulnerabilities not reported to vendors are less likely to be fixed. Ours is the first study on vulnerability repositories that targets the reporters of the most common security vulnerabilities, thus concentrating on the people involved in the process; previous works have overlooked this rich information source. The results are valuable for beginners exploring how to detect and report security vulnerabilities and for tool vendors and researchers exploring how to automate and fix the process.},
	language = {en},
	number = {5},
	urldate = {2022-03-07},
	journal = {Empirical Software Engineering},
	author = {Hafiz, Munawar and Fang, Ming},
	month = oct,
	year = {2016},
	pages = {1920--1959},
}

@inproceedings{perumal_rule-based_2016,
	title = {Rule-based conflict resolution framework for {Internet} of {Things} device management in smart home environment},
	doi = {10.1109/GCCE.2016.7800444},
	abstract = {Recent developments in Internet of Things (IoT) tremendously have introduced several heterogeneous systems and devices that characterize a smart home. Generally, these heterogeneous systems are dissimilar and accomplish various services and functionalities. Due to the gradual changes of managing resources in smart home, more heterogeneous systems are being introduced from time to time depending on the consumer requirement. As such, more dependencies are created among heterogeneous systems, and this could lead towards conflict occurrences among them. Conflicts could occur in smart home when two or more events generated by heterogeneous systems need to be triggered at an instance of time. In this paper, we present a rule-based conflict resolution framework using scheduling algorithm for managing heterogeneous systems in smart home environment. Events are captured and processed by the framework which performs corresponding conflict resolution on the heterogeneous systems. The developed framework was implemented with several heterogeneous systems to validate their effectiveness in solving conflict occurrences. The framework was ascertained to be consistent in smart home environment.},
	booktitle = {2016 {IEEE} 5th {Global} {Conference} on {Consumer} {Electronics}},
	author = {Perumal, Thinagaran and Sulaiman, Md Nasir and Datta, Soumya Kanti and Ramachandran, Thinaharan and Leong, Chui Yew},
	month = oct,
	year = {2016},
	keywords = {Conflict resolution, ECA, Smart home environment},
	pages = {1--2},
}

@inproceedings{han_ghostnet_2020,
	address = {Seattle, WA, USA},
	title = {{GhostNet}: {More} {Features} {From} {Cheap} {Operations}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{GhostNet}},
	url = {https://ieeexplore.ieee.org/document/9157333/},
	doi = {10.1109/CVPR42600.2020.00165},
	abstract = {Deploying convolutional neural networks (CNNs) on embedded devices is difﬁcult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7\% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC2012 classiﬁcation dataset. Code is available at https: //github.com/huawei-noah/ghostnet.},
	language = {en},
	urldate = {2022-03-04},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
	month = jun,
	year = {2020},
	pages = {1577--1586},
}

@article{davidsen_longitudinal_2010,
	title = {A longitudinal study of development and maintenance},
	volume = {52},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584910000431},
	doi = {10.1016/j.infsof.2010.03.003},
	abstract = {Objective: The paper presents ﬁnding relative to the distribution of work between maintenance and development tasks, comparing to the results reported earlier to assess the stability of important metrics within the area.
Method: This paper presents the main results of a survey-investigation performed in 2008 in 67 Norwegian organizations comparing the distribution of work to results from similar investigations performed in Norway in 1993, 1998, and 2003. Some comparisons to similar investigations performed in USA before this is also provided.
Results: The amount of application portfolio upkeep (work made to keep up the functional coverage of the application system portfolio of the organization, including the development of replacement systems), is at the same level as reported in 1998 and 2003. The level of application maintenance is also on the same level as the similar investigations conducted in 2003 and 1998. There was a signiﬁcant increase in both maintenance and application portfolio upkeep from 1993 to 1998, which could partly be attributed to be the extra maintenance and replacement-oriented work necessary to deal with the ‘‘year 2000 problem”, but this seemed to be reversed in 2003 and 2008. As for the 2003 investigation, the slow ITmarket in general seemed to have inﬂuenced the results negatively seen from the point of view of application systems support efﬁciency in organization. No similar explanation can be used for the 2008 numbers.
Conclusion: Based on the last surveys it seems than a stable level of work distribution both on maintenance and application portfolio upkeep have been reached, although the underlying development technologies are still undergoing large changes. This is contrary to others claiming that the amount of maintenance is still increasing.},
	language = {en},
	number = {7},
	urldate = {2022-03-04},
	journal = {Information and Software Technology},
	author = {Davidsen, Magne Kristoffer and Krogstie, John},
	month = jul,
	year = {2010},
	pages = {707--719},
}

@article{li_muafl_2022,
	title = {\${\textbackslash}mu\${AFL}: {Non}-intrusive {Feedback}-driven {Fuzzing} for {Microcontroller} {Firmware}},
	shorttitle = {\${\textbackslash}mu\${AFL}},
	url = {http://arxiv.org/abs/2202.03013},
	doi = {10.1145/3510003.3510208},
	abstract = {Fuzzing is one of the most effective approaches to finding software flaws. However, applying it to microcontroller firmware incurs many challenges. For example, rehosting-based solutions cannot accurately model peripheral behaviors and thus cannot be used to fuzz the corresponding driver code. In this work, we present \${\textbackslash}mu\$AFL, a hardware-in-the-loop approach to fuzzing microcontroller firmware. It leverages debugging tools in existing embedded system development to construct an AFL-compatible fuzzing framework. Specifically, we use the debug dongle to bridge the fuzzing environment on the PC and the target firmware on the microcontroller device. To collect code coverage information without costly code instrumentation, \${\textbackslash}mu\$AFL relies on the ARM ETM hardware debugging feature, which transparently collects the instruction trace and streams the results to the PC. However, the raw ETM data is obscure and needs enormous computing resources to recover the actual instruction flow. We therefore propose an alternative representation of code coverage, which retains the same path sensitivity as the original AFL algorithm, but can directly work on the raw ETM data without matching them with disassembled instructions. To further reduce the workload, we use the DWT hardware feature to selectively collect runtime information of interest. We evaluated \${\textbackslash}mu\$AFL on two real evaluation boards from two major vendors: NXP and STMicroelectronics. With our prototype, we discovered ten zero-day bugs in the driver code shipped with the SDK of STMicroelectronics and three zero-day bugs in the SDK of NXP. Eight CVEs have been allocated for them. Considering the wide adoption of vendor SDKs in real products, our results are alarming.},
	urldate = {2022-03-04},
	journal = {arXiv:2202.03013 [cs]},
	author = {Li, Wenqiang and Shi, Jiameng and Li, Fengjun and Lin, Jingqiang and Wang, Wei and Guan, Le},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.03013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@inproceedings{peng_usbfuzz_2020,
	title = {\{{USBFuzz}\}: {A} {Framework} for {Fuzzing} \{{USB}\} {Drivers} by {Device} {Emulation}},
	isbn = {978-1-939133-17-5},
	shorttitle = {\{{USBFuzz}\}},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/peng},
	language = {en},
	urldate = {2022-03-04},
	author = {Peng, Hui and Payer, Mathias},
	year = {2020},
	pages = {2559--2575},
}

@article{li_muafl_2022-1,
	title = {\${\textbackslash}mu\${AFL}: {Non}-intrusive {Feedback}-driven {Fuzzing} for {Microcontroller} {Firmware}},
	shorttitle = {\${\textbackslash}mu\${AFL}},
	url = {http://arxiv.org/abs/2202.03013},
	doi = {10.1145/3510003.3510208},
	abstract = {Fuzzing is one of the most effective approaches to finding software flaws. However, applying it to microcontroller firmware incurs many challenges. For example, rehosting-based solutions cannot accurately model peripheral behaviors and thus cannot be used to fuzz the corresponding driver code. In this work, we present \${\textbackslash}mu\$AFL, a hardware-in-the-loop approach to fuzzing microcontroller firmware. It leverages debugging tools in existing embedded system development to construct an AFL-compatible fuzzing framework. Specifically, we use the debug dongle to bridge the fuzzing environment on the PC and the target firmware on the microcontroller device. To collect code coverage information without costly code instrumentation, \${\textbackslash}mu\$AFL relies on the ARM ETM hardware debugging feature, which transparently collects the instruction trace and streams the results to the PC. However, the raw ETM data is obscure and needs enormous computing resources to recover the actual instruction flow. We therefore propose an alternative representation of code coverage, which retains the same path sensitivity as the original AFL algorithm, but can directly work on the raw ETM data without matching them with disassembled instructions. To further reduce the workload, we use the DWT hardware feature to selectively collect runtime information of interest. We evaluated \${\textbackslash}mu\$AFL on two real evaluation boards from two major vendors: NXP and STMicroelectronics. With our prototype, we discovered ten zero-day bugs in the driver code shipped with the SDK of STMicroelectronics and three zero-day bugs in the SDK of NXP. Eight CVEs have been allocated for them. Considering the wide adoption of vendor SDKs in real products, our results are alarming.},
	urldate = {2022-03-04},
	journal = {arXiv:2202.03013 [cs]},
	author = {Li, Wenqiang and Shi, Jiameng and Li, Fengjun and Lin, Jingqiang and Wang, Wei and Guan, Le},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.03013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{li_muafl_2022-2,
	title = {\${\textbackslash}mu\${AFL}: {Non}-intrusive {Feedback}-driven {Fuzzing} for {Microcontroller} {Firmware}},
	shorttitle = {\${\textbackslash}mu\${AFL}},
	url = {http://arxiv.org/abs/2202.03013},
	doi = {10.1145/3510003.3510208},
	abstract = {Fuzzing is one of the most effective approaches to finding software flaws. However, applying it to microcontroller firmware incurs many challenges. For example, rehosting-based solutions cannot accurately model peripheral behaviors and thus cannot be used to fuzz the corresponding driver code. In this work, we present \${\textbackslash}mu\$AFL, a hardware-in-the-loop approach to fuzzing microcontroller firmware. It leverages debugging tools in existing embedded system development to construct an AFL-compatible fuzzing framework. Specifically, we use the debug dongle to bridge the fuzzing environment on the PC and the target firmware on the microcontroller device. To collect code coverage information without costly code instrumentation, \${\textbackslash}mu\$AFL relies on the ARM ETM hardware debugging feature, which transparently collects the instruction trace and streams the results to the PC. However, the raw ETM data is obscure and needs enormous computing resources to recover the actual instruction flow. We therefore propose an alternative representation of code coverage, which retains the same path sensitivity as the original AFL algorithm, but can directly work on the raw ETM data without matching them with disassembled instructions. To further reduce the workload, we use the DWT hardware feature to selectively collect runtime information of interest. We evaluated \${\textbackslash}mu\$AFL on two real evaluation boards from two major vendors: NXP and STMicroelectronics. With our prototype, we discovered ten zero-day bugs in the driver code shipped with the SDK of STMicroelectronics and three zero-day bugs in the SDK of NXP. Eight CVEs have been allocated for them. Considering the wide adoption of vendor SDKs in real products, our results are alarming.},
	urldate = {2022-03-04},
	journal = {arXiv:2202.03013 [cs]},
	author = {Li, Wenqiang and Shi, Jiameng and Li, Fengjun and Lin, Jingqiang and Wang, Wei and Guan, Le},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.03013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{adams_people_2021,
	title = {People systematically overlook subtractive changes},
	volume = {592},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03380-y},
	doi = {10.1038/s41586-021-03380-y},
	abstract = {Improving objects, ideas or situations—whether a designer seeks to advance technology, a writer seeks to strengthen an argument or a manager seeks to encourage desired behaviour—requires a mental search for possible changes1–3. We investigated whether people are as likely to consider changes that subtract components from an object, idea or situation as they are to consider changes that add new components. People typically consider a limited number of promising ideas in order to manage the cognitive burden of searching through all possible ideas, but this can lead them to accept adequate solutions without considering potentially superior alternatives4–10. Here we show that people systematically default to searching for additive transformations, and consequently overlook subtractive transformations. Across eight experiments, participants were less likely to identify advantageous subtractive changes when the task did not (versus did) cue them to consider subtraction, when they had only one opportunity (versus several) to recognize the shortcomings of an additive search strategy or when they were under a higher (versus lower) cognitive load. Defaulting to searches for additive changes may be one reason that people struggle to mitigate overburdened schedules11, institutional red tape12 and damaging effects on the planet13,14.},
	language = {en},
	number = {7853},
	urldate = {2022-03-03},
	journal = {Nature},
	author = {Adams, Gabrielle S. and Converse, Benjamin A. and Hales, Andrew H. and Klotz, Leidy E.},
	month = apr,
	year = {2021},
	note = {Number: 7853
Publisher: Nature Publishing Group},
	keywords = {Decision making, Human behaviour},
	pages = {258--261},
}

@inproceedings{kalliamvakou_promises_2014,
	address = {New York, NY, USA},
	series = {{MSR} 2014},
	title = {The promises and perils of mining {GitHub}},
	isbn = {978-1-4503-2863-0},
	url = {https://doi.org/10.1145/2597073.2597074},
	doi = {10.1145/2597073.2597074},
	abstract = {With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40\% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub.},
	urldate = {2022-03-03},
	booktitle = {Proceedings of the 11th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
	month = may,
	year = {2014},
	keywords = {Mining software repositories, bias, code reviews, git, github},
	pages = {92--101},
}

@article{case_emerging_2011,
	title = {Emerging {Research} {Methodologies} in {Engineering} {Education} {Research}},
	volume = {100},
	issn = {10694730},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.2168-9830.2011.tb00008.x},
	doi = {10.1002/j.2168-9830.2011.tb00008.x},
	abstract = {BACKGROUND Methodology refers to the theoretical arguments that researchers use in order to justify their research methods and design. There is an extensive range of well established methodologies in the educational research literature of which a growing subset is beginning to be used in engineering education research.
PURPOSE A more explicit engagement with methodologies, particularly those that are only emerging in engineering education research, is important so that engineering education researchers can broaden the set of research questions they are able to address. SCOPE/METHOD Seven methodologies are outlined and for each an exemplar paper is analyzed in order to demonstrate the methodology in operation and to highlight its particular contribution. The methodologies are: Case Study, Grounded Theory, Ethnography, Action Research, Phenomenography, Discourse Analysis, and Narrative Analysis. It is noted that many of the exemplar papers use some of these methodologies in combination.
CONCLUSIONS The exemplar papers show that collectively these methodologies might allow the research community to be able to better address questions around key engineering education challenges, such as students' responses to innovative pedagogies, diversity issues in engineering, and the changing requirements for engineering graduates in the twenty-first century.},
	language = {en},
	number = {1},
	urldate = {2022-03-01},
	journal = {Journal of Engineering Education},
	author = {Case, Jennifer M. and Light, Gregory},
	month = jan,
	year = {2011},
	pages = {186--210},
}

@article{delatte_failure_2010,
	title = {Failure literacy in structural engineering},
	volume = {32},
	issn = {01410296},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014102960900399X},
	doi = {10.1016/j.engstruct.2009.12.015},
	abstract = {The history of the development of practice in many engineering disciplines is, in large part, the story of failures, both imminent and actual, and of the changes to designs, standards and procedures made as the result of timely interventions or forensic analyses. All engineers, and more particularly structural engineers, should be failure literate. Failure literacy means knowing about the critical historical failure cases that have shaped the profession: not merely the surface technical details, but the environment, the communications difficulties and the procedural issues. In the US, an intensive effort has been under way for nearly a decade to promote failure literacy in engineering education and practice. A number of educational resources have been developed to make it easier for engineering students and practicing engineers to learn from failures.},
	language = {en},
	number = {7},
	urldate = {2022-03-01},
	journal = {Engineering Structures},
	author = {Delatte, Norbert},
	month = jul,
	year = {2010},
	pages = {1952--1954},
}

@book{delatte2008beyond,
	title = {Beyond failure: {Forensic} case studies for civil engineers},
	publisher = {American Society of Civil Engineers},
	author = {Delatte Jr, Norbert J},
	year = {2008},
	note = {tex.organization: American Society of Civil Engineers},
}

@article{ubani_study_2013,
	title = {A study of failure and abandonment of public sector-driven civil engineering projects in {Nigeria}: {An} empirical review},
	volume = {4},
	issn = {2153649X},
	shorttitle = {A study of failure and abandonment of public sector-driven civil engineering projects in {Nigeria}},
	url = {http://www.scihub.org/AJSIR/PDF/2013/1/AJSIR-4-1-75-82.pdf},
	doi = {10.5251/ajsir.2013.4.1.75.82},
	abstract = {Incessant failure and abandonment of public sector projects are still posing serious concern and challenges to the society and other stakeholders in civil engineering and construction industries. The study identifies and examines the salient factors and the warning signals responsible for failure and abandonment of public sector driven civil engineering projects with a view of directing efforts towards forestalling the problems. Opinion survey was adopted with area and judgmental sampling procedures. Primary data based on the identified factors, was captured with the instrument of questionnaire from professionals in civil engineering projects operating in the South East geopolitical zone of Nigeria. The analytical tools used in the study were severity index, spearman’s rank correlation coefficient, relative agreement factors and Kendall’s coefficient of concordance (W.). The rankings of different professionals were significantly correlated. The result of percentage relative agreement factors indicates that the most salient factors causing failure and abandonment of public sector driven civil engineering projects in order of significance are frequent changes in government and political power, unreliable mode of financing and payment of completed work, and project contract sum indirectly used to compensate political big-wigs etc. Wtest further substantiates the results by indicating significant degree of concordance in opinion of experts. The study therefore concludes that politically- induced corruption, undefined and non compliance to the agreed mode of financing and payment of completed work are the bane of project success. It is therefore a matter of legislation and policy formulation which should be instituted to avert failure and abandonment of public sector-driven civil engineering projects.},
	language = {en},
	number = {1},
	urldate = {2022-03-01},
	journal = {American Journal of Scientific and Industrial Research},
	author = {Ubani, E. and Ononuju, C.},
	month = feb,
	year = {2013},
	pages = {75--82},
}

@article{johnson_forensic_2002,
	title = {Forensic software engineering: are software failures symptomatic of systemic problems?},
	volume = {40},
	issn = {09257535},
	shorttitle = {Forensic software engineering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925753501000868},
	doi = {10.1016/S0925-7535(01)00086-8},
	abstract = {There is a growing realization that existing accident investigation techniques fail to meet the challenges created by incidents that involve software failures. Existing software development techniques cannot easily be used to provide retrospective information about the complex and systemic causes of major accidents. This paper, therefore, argues that we must develop speciﬁc techniques to support forensic software engineering. It is important that these techniques should look beyond ‘programmer error’ as a primary cause of software failure. They must enable investigators to identify the systemic problems that are created by inadequate investment, by poor management leadership and by the breakdown in communication between development teams. This argument builds on previous work by Leveson and by Reason. They have focused on the importance of a systemic approach to the development of safety-critical applications. Relatively little attention has been paid to a systemic analysis of their failure. Later sections of this paper analyze the potential problems that can arise when a systemic approach is extended from systems development to accident investigation. \# 2002 Elsevier Science Ltd. All rights reserved.},
	language = {en},
	number = {9},
	urldate = {2022-03-01},
	journal = {Safety Science},
	author = {Johnson, Chris},
	month = dec,
	year = {2002},
	pages = {835--847},
}

@inproceedings{pasquale_towards_2018,
	address = {Gothenburg Sweden},
	title = {Towards forensic-ready software systems},
	isbn = {978-1-4503-5662-6},
	url = {https://dl.acm.org/doi/10.1145/3183399.3183426},
	doi = {10.1145/3183399.3183426},
	abstract = {As software becomes more ubiquitous, and the risk of cyber-crimes increases, ensuring that software systems are forensic-ready (i.e., capable of supporting potential digital investigations) is critical. However, little or no attention has been given to how well-suited existing software engineering methodologies and practices are for the systematic development of such systems. In this paper, we consider the meaning of forensic readiness of software, define forensic readiness requirements, and highlight some of the open software engineering challenges in the face of forensic readiness. We use a real software system developed to investigate online sharing of child abuse media to illustrate the presented concepts.},
	language = {en},
	urldate = {2022-03-01},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results}},
	publisher = {ACM},
	author = {Pasquale, Liliana and Alrajeh, Dalal and Peersman, Claudia and Tun, Thein and Nuseibeh, Bashar and Rashid, Awais},
	month = may,
	year = {2018},
	pages = {9--12},
}

@inproceedings{grispos_are_2017,
	address = {Brighton, United Kingdom},
	title = {Are you ready? {Towards} the engineering of forensic-ready systems},
	isbn = {978-1-5090-5476-3},
	shorttitle = {Are you ready?},
	url = {http://ieeexplore.ieee.org/document/7956555/},
	doi = {10.1109/RCIS.2017.7956555},
	abstract = {As security incidents continue to impact organisations, there is a growing demand for systems to be ‘forensicready’ – to maximise the potential use of evidence whilst minimising the costs of an investigation. Researchers have supported organisational forensic readiness efforts by proposing the use of policies and processes, aligning systems with forensics objectives and training employees. However, recent work has also proposed an alternative strategy for implementing forensic readiness called forensic-by-design. This is an approach that involves integrating requirements for forensics into relevant phases of the systems development lifecycle with the aim of engineering forensic-ready systems. While this alternative forensic readiness strategy has been discussed in the literature, no previous research has examined the extent to which organisations actually use this approach for implementing forensic readiness. Hence, we investigate the extent to which organisations consider requirements for forensics during systems development. We ﬁrst assessed existing research to identify the various perspectives of implementing forensic readiness, and then undertook an online survey to investigate the consideration of requirements for forensics during systems development lifecycles. Our ﬁndings provide an initial assessment of the extent to which requirements for forensics are considered within organisations. We then use our ﬁndings, coupled with the literature, to identify a number of research challenges regarding the engineering of forensic-ready systems.},
	language = {en},
	urldate = {2022-03-01},
	booktitle = {2017 11th {International} {Conference} on {Research} {Challenges} in {Information} {Science} ({RCIS})},
	publisher = {IEEE},
	author = {Grispos, George and Garcia-Galan, Jesus and Pasquale, Liliana and Nuseibeh, Bashar},
	month = may,
	year = {2017},
	pages = {328--333},
}

@inproceedings{jan2015transformer,
	title = {Transformer failures, causes \& impact},
	booktitle = {International conference data mining, civil and mechanical engineering},
	author = {Jan, Shayan Tariq and Afzal, Raheel and Khan, Akif Zia},
	year = {2015},
	pages = {49--52},
}

@article{barber_quality_2000,
	title = {Quality failure costs in civil engineering projects},
	volume = {17},
	issn = {0265-671X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/02656710010298544/full/html},
	doi = {10.1108/02656710010298544},
	abstract = {A methodology was developed to measure cost of quality failures in two major road projects, largely based upon a work-shadowing method. Shows how the initial data were collected and categorised into definable groups and how the costs were estimated for each of these categories. The findings suggest that, if the projects examined are typical, the cost of failures may be a significant percentage of total costs, and that conventional means of identifying them may not be reliable. Moreover, the costs will not be easy to eradicate without widespread changes in attitudes and norms of behaviour within the industry and improved managerial co-ordination of activities throughout the supply chain.},
	language = {en},
	number = {4/5},
	urldate = {2022-03-01},
	journal = {International Journal of Quality \& Reliability Management},
	author = {Barber, Patrick and Graves, Andrew and Hall, Mark and Sheath, Darryl and Tomkins, Cyril},
	month = jun,
	year = {2000},
	pages = {479--492},
}

@article{abdulrahman_capturing_1993,
	title = {Capturing the {Cost} of {Quality} {Failures} in {Civil} {Engineering}},
	volume = {10},
	issn = {0265-671X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/02656719310037956/full/html},
	doi = {10.1108/02656719310037956},
	abstract = {A description of failure events during construction illustrates the urgent need to emphasize the management of quality in civil engineering projects. During the construction of a civil engineering project, cost control techniques are used to monitor cost trends and to detect cost deviations in order to control project cost. However, this technique does not reveal the cause of any failure. The nature and collection of failure costs have been part of quality costing. Hypothetical illustrations show how failure costs can be extracted during construction using a matrix. Quality cost information can be used to supplement cost control techniques for cost control purposes and in identifying weaknesses within a system.},
	language = {en},
	number = {3},
	urldate = {2022-03-01},
	journal = {International Journal of Quality \& Reliability Management},
	author = {Abdul‐Rahman, Hamzah},
	month = mar,
	year = {1993},
}

@article{delatte_forensics_2002,
	title = {Forensics and {Case} {Studies} in {Civil} {Engineering} {Education}: {State} of the {Art}},
	volume = {16},
	issn = {0887-3828, 1943-5509},
	shorttitle = {Forensics and {Case} {Studies} in {Civil} {Engineering} {Education}},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%290887-3828%282002%2916%3A3%2898%29},
	doi = {10.1061/(ASCE)0887-3828(2002)16:3(98)},
	abstract = {This paper reviews the state of the art in the use of forensic engineering and failure case studies in civil engineering education. The study of engineering failures can offer students valuable insights into associated technical, ethical, and professional issues. Lessons learned from failures have substantially affected civil engineering practice. For the student, study of these cases can help place design and analysis procedures into historical context and reinforce the necessity of lifelong learning. Three approaches for bringing forensics and failure case studies into the civil engineering curriculum are discussed in this paper. These are stand-alone forensic engineering or failure case study courses, capstone design projects, and integration of case studies into the curriculum. Some of the cases have been developed and used in courses at the United States Military Academy and the Univ. of Alabama at Birmingham, as well as at other institutions. Finally, the writers have tried to assemble many of the known sources of material, including books, technical papers, and magazine articles, videos, Web sites, prepared PowerPoint presentations, and television programs.},
	language = {en},
	number = {3},
	urldate = {2022-03-01},
	journal = {Journal of Performance of Constructed Facilities},
	author = {Delatte, Norbert J. and Rens, Kevin L.},
	month = aug,
	year = {2002},
	pages = {98--109},
}

@article{pietroforte_civil_nodate,
	title = {Civil {Engineering} {Education} through {Case} {Studies} of {Failures}},
	abstract = {Teaching through failure case studies complements and integrates the traditional civil engineering topics that are introduced with the lecture-based method. This paper describes the development of a course on failures at Worcester Polytechnic Institute with particular reference to the implementation of case study instruction and structuring of the course. The topic of failures is used to assess the decision-making process that underpins the life cycle of a facility, and the variables that affect the performance of this process. The primary objective of the case study method is to enhance learning through students' active participation in case presentation and discussion. The method is effective in blending real world situations and applications of theoretical and scientific principles.},
	language = {en},
	author = {Pietroforte, Roberto and Member, Associate},
	pages = {5},
}

@inproceedings{delatte_using_2000,
	address = {San Juan, Puerto Rico, United States},
	title = {Using {Failure} {Case} {Studies} in {Civil} {Engineering} {Education}},
	isbn = {978-0-7844-0482-9},
	url = {http://ascelibrary.org/doi/10.1061/40482%28280%2946},
	doi = {10.1061/40482(280)46},
	abstract = {The study of engineering failures can offer students valuable insights into the associated technical, ethical, and professional issues. In many cases, lessons learned from failures have substantially affected civil engineering practice. For the student, study of these cases can help place design and analysis procedures into historical context and reinforce the necessity of lifelong learning. Of course, it would be impossible to add an undergraduate course on failures and lessons learned to an already overcrowded curriculum. A more practical solution is to integrate failure case studies into existing undergraduate courses. In this paper a master plan for the use of failure case studies in an undergraduate civil engineering curriculum is discussed. First, for a selected number of courses, several relevant principles are identified. Examples include drawing correct and complete free body diagrams (statics), compression member buckling (mechanics of materials, structures), shear strength of concrete beams (reinforced concrete design), and connection detailing and behavior (structural steel design). For each of these principles, failure case studies have been identified. They are discussed in some detail within this paper, and resources and references for further development of the case studies are indicated. Some of the cases have been developed and used in courses at the United States Military Academy and the University of Alabama at Birmingham.},
	language = {en},
	urldate = {2022-03-01},
	booktitle = {Forensic {Engineering} (2000)},
	publisher = {American Society of Civil Engineers},
	author = {Delatte, Norbert J.},
	month = apr,
	year = {2000},
	pages = {430--440},
}

@article{zhang_experimental_2004,
	title = {Experimental study of steam turbine control valves},
	volume = {218},
	issn = {0954-4062, 2041-2983},
	url = {http://journals.sagepub.com/doi/10.1243/095440604323052283},
	doi = {10.1243/095440604323052283},
	abstract = {Because of the converging–diverging con guration of the valve passage, venturi valves have been widely used in large turbines to regulate inlet ow as turbine governing valves for about half a century. F rom the 1960s, a number of valve failure incidents have been reported. Improvement to current designs was strongly demanded but, owing to the complicated nature of the uid–structure interaction mechanisms, the basic mechanism causing valve failure is still far from being fully understood. Experimental investigations on a half-scale valve were performed here. The study con rmed that asymmetric unstable ow is the root cause of valve problems, such as noise, vibration and failure.},
	language = {en},
	number = {5},
	urldate = {2022-03-01},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
	author = {Zhang, D and Engeda, A and Hardin, J R and Aungier, R H},
	month = may,
	year = {2004},
	pages = {493--507},
}

@inproceedings{janzen_influence_2006,
	title = {On the {Influence} of {Test}-{Driven} {Development} on {Software} {Design}},
	doi = {10.1109/CSEET.2006.25},
	abstract = {Test-driven development (TDD) is an agile software development strategy that addresses both design and testing. This paper describes a controlled experiment that examines the effects of TDD on internal software design quality. The experiment was conducted with undergraduate students in a software engineering course. Students in three groups completed semester-long programming projects using either an iterative test-first (TDD), iterative test-last, or linear test-last approach. Results from this study indicate that TDD can be an effective software design approach improving both code-centric aspects such as object decomposition, test coverage, and external quality, and developer-centric aspects including productivity and confidence. In addition, iterative development approaches that include automated testing demonstrated benefits over a more traditional linear approach with manual tests. This study demonstrates the viability of teaching TDD with minimal effort in the context of a relatively traditional development approach. Potential dangers with TDD are identified regarding programmer motivation and discipline. Pedagogical implications and instructional techniques which may foster TDD adoption will also be referenced},
	booktitle = {19th {Conference} on {Software} {Engineering} {Education} {Training} ({CSEET}'06)},
	author = {Janzen, D.S. and Saiedian, H.},
	month = apr,
	year = {2006},
	note = {ISSN: 2377-570X},
	keywords = {Automatic testing, Iterative methods, Linear programming, Productivity, Programming profession, Software design, Software engineering, Software testing, System testing, Writing},
	pages = {141--148},
}

@inproceedings{falessi_value-based_2008,
	title = {Value-{Based} {Design} {Decision} {Rationale} {Documentation}: {Principles} and {Empirical} {Feasibility} {Study}},
	shorttitle = {Value-{Based} {Design} {Decision} {Rationale} {Documentation}},
	doi = {10.1109/WICSA.2008.8},
	abstract = {The explicit documentation of the rationale of design decisions is a practice generally encouraged, but rarely implemented in industry because of a variety of inhibitors. Methods proposed in the past for design decisions rationale documentation (DDRD) aimed to maximize benefits for the DDRD consumer by imposing on the producer of DDRD the burden to document all the potentially useful information. We propose here a compromise which consists in tailoring DDRD, based on its intended use or purpose. In our view, the adoption of a tailored DDRD, consisting only of the required set of information, would mitigate the effects of DDRD inhibitors. The aim of this paper is twofold: i) to discuss the application of value-based software engineering principles to DDRD, ii) to describe a controlled experiment to empirically analyze the feasibility of the proposed method. Results show that the level of utility related to the same category of DDRD information significantly changes depending on its purpose; such result is novel and it demonstrates the feasibility of the proposed value-based DDRD.},
	booktitle = {Seventh {Working} {IEEE}/{IFIP} {Conference} on {Software} {Architecture} ({WICSA} 2008)},
	author = {Falessi, Davide and Cantone, Giovanni and Kruchten, Philippe},
	month = feb,
	year = {2008},
	keywords = {Application software, Computer architecture, Computer industry, Costs, Design decision rationale documentation, Documentation, Inhibitors, Software architecture, Software design, Software engineering, Software testing, empirical software engineering, value based software engineering},
	pages = {189--198},
}

@article{glass_project_2002,
	title = {Project retrospectives, and why they never happen},
	volume = {19},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/1032872/},
	doi = {10.1109/MS.2002.1032872},
	language = {en},
	number = {5},
	urldate = {2022-02-28},
	journal = {IEEE Software},
	author = {Glass, R.L.},
	month = sep,
	year = {2002},
	pages = {112--111},
}

@misc{noauthor_admin_nodate,
	title = {Admin {Option} to {Disable} {Trending} {Feature} · {Issue} \#7702 · mastodon/mastodon},
	url = {https://github.com/mastodon/mastodon/issues/7702},
	abstract = {Instance admins should be able to easily disable the trending hashtag feature. [x ] I searched or browsed the repo’s other issues to ensure this is not a duplicate.},
	language = {en},
	urldate = {2022-02-27},
	journal = {GitHub},
}

@article{twenge_underestimating_2020,
	title = {Underestimating digital media harm},
	volume = {4},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-0839-4},
	doi = {10.1038/s41562-020-0839-4},
	language = {en},
	number = {4},
	urldate = {2022-02-27},
	journal = {Nature Human Behaviour},
	author = {Twenge, Jean M. and Haidt, Jonathan and Joiner, Thomas E. and Campbell, W. Keith},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Sociology},
	pages = {346--348},
}

@article{kietzmann_social_2011,
	series = {{SPECIAL} {ISSUE}: {SOCIAL} {MEDIA}},
	title = {Social media? {Get} serious! {Understanding} the functional building blocks of social media},
	volume = {54},
	issn = {0007-6813},
	shorttitle = {Social media?},
	url = {https://www.sciencedirect.com/science/article/pii/S0007681311000061},
	doi = {10.1016/j.bushor.2011.01.005},
	abstract = {Traditionally, consumers used the Internet to simply expend content: they read it, they watched it, and they used it to buy products and services. Increasingly, however, consumers are utilizing platforms—such as content sharing sites, blogs, social networking, and wikis—to create, modify, share, and discuss Internet content. This represents the social media phenomenon, which can now significantly impact a firm's reputation, sales, and even survival. Yet, many executives eschew or ignore this form of media because they don’t understand what it is, the various forms it can take, and how to engage with it and learn. In response, we present a framework that defines social media by using seven functional building blocks: identity, conversations, sharing, presence, relationships, reputation, and groups. As different social media activities are defined by the extent to which they focus on some or all of these blocks, we explain the implications that each block can have for how firms should engage with social media. To conclude, we present a number of recommendations regarding how firms should develop strategies for monitoring, understanding, and responding to different social media activities.},
	language = {en},
	number = {3},
	urldate = {2022-02-27},
	journal = {Business Horizons},
	author = {Kietzmann, Jan H. and Hermkens, Kristopher and McCarthy, Ian P. and Silvestre, Bruno S.},
	month = may,
	year = {2011},
	keywords = {Facebook, LinkedIn, Social media, Social networks, Twitter, User-generated content, Web 2.0, YouTube},
	pages = {241--251},
}

@article{ralph_empirical_2021,
	title = {Empirical {Standards} for {Software} {Engineering} {Research}},
	url = {http://arxiv.org/abs/2010.03525},
	abstract = {Empirical Standards are natural-language models of a scientific community's expectations for a specific kind of study (e.g. a questionnaire survey). The ACM SIGSOFT Paper and Peer Review Quality Initiative generated empirical standards for research methods commonly used in software engineering. These living documents, which should be continuously revised to reflect evolving consensus around research best practices, will improve research quality and make peer review more effective, reliable, transparent and fair.},
	urldate = {2022-02-27},
	journal = {arXiv:2010.03525 [cs]},
	author = {Ralph, Paul and Ali, Nauman bin and Baltes, Sebastian and Bianculli, Domenico and Diaz, Jessica and Dittrich, Yvonne and Ernst, Neil and Felderer, Michael and Feldt, Robert and Filieri, Antonio and de França, Breno Bernard Nicolau and Furia, Carlo Alberto and Gay, Greg and Gold, Nicolas and Graziotin, Daniel and He, Pinjia and Hoda, Rashina and Juristo, Natalia and Kitchenham, Barbara and Lenarduzzi, Valentina and Martínez, Jorge and Melegati, Jorge and Mendez, Daniel and Menzies, Tim and Molleri, Jefferson and Pfahl, Dietmar and Robbes, Romain and Russo, Daniel and Saarimäki, Nyyti and Sarro, Federica and Taibi, Davide and Siegmund, Janet and Spinellis, Diomidis and Staron, Miroslaw and Stol, Klaas and Storey, Margaret-Anne and Taibi, Davide and Tamburri, Damian and Torchiano, Marco and Treude, Christoph and Turhan, Burak and Wang, Xiaofeng and Vegas, Sira},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.03525},
	keywords = {Computer Science - General Literature, Computer Science - Software Engineering},
}

@article{johnson_mixed_2004,
	title = {Mixed {Methods} {Research}: {A} {Research} {Paradigm} {Whose} {Time} {Has} {Come}},
	volume = {33},
	issn = {0013-189X, 1935-102X},
	shorttitle = {Mixed {Methods} {Research}},
	url = {http://journals.sagepub.com/doi/10.3102/0013189X033007014},
	doi = {10.3102/0013189X033007014},
	abstract = {The purposes of this article are to position mixed methods research ( mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm “wars” and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research ( mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it.},
	language = {en},
	number = {7},
	urldate = {2022-02-27},
	journal = {Educational Researcher},
	author = {Johnson, R. Burke and Onwuegbuzie, Anthony J.},
	month = oct,
	year = {2004},
	pages = {14--26},
}

@book{margetts_political_2015,
	title = {Political {Turbulence}: {How} {Social} {Media} {Shape} {Collective} {Action}},
	isbn = {978-1-4008-7355-5},
	shorttitle = {Political {Turbulence}},
	abstract = {How social media is giving rise to a chaotic new form of politicsAs people spend increasing proportions of their daily lives using social media, such as Twitter and Facebook, they are being invited to support myriad political causes by sharing, liking, endorsing, or downloading. Chain reactions caused by these tiny acts of participation form a growing part of collective action today, from neighborhood campaigns to global political movements. Political Turbulence reveals that, in fact, most attempts at collective action online do not succeed, but some give rise to huge mobilizations—even revolutions.Drawing on large-scale data generated from the Internet and real-world events, this book shows how mobilizations that succeed are unpredictable, unstable, and often unsustainable. To better understand this unruly new force in the political world, the authors use experiments that test how social media influence citizens deciding whether or not to participate. They show how different personality types react to social influences and identify which types of people are willing to participate at an early stage in a mobilization when there are few supporters or signals of viability. The authors argue that pluralism is the model of democracy that is emerging in the social media age—not the ordered, organized vision of early pluralists, but a chaotic, turbulent form of politics.This book demonstrates how data science and experimentation with social data can provide a methodological toolkit for understanding, shaping, and perhaps even predicting the outcomes of this democratic turbulence.},
	language = {en},
	publisher = {Princeton University Press},
	author = {Margetts, Helen and John, Peter and Hale, Scott and Yasseri, Taha},
	month = nov,
	year = {2015},
	note = {Google-Books-ID: tfuxCQAAQBAJ},
	keywords = {Political Science / General, Political Science / Political Ideologies / Democracy, Political Science / Political Process / Political Advocacy, Social Science / Media Studies},
}

@article{miller_empirical_1990,
	title = {An empirical study of the reliability of {UNIX} utilities},
	volume = {33},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/96267.96279},
	doi = {10.1145/96267.96279},
	abstract = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
	language = {en},
	number = {12},
	urldate = {2022-02-25},
	journal = {Communications of the ACM},
	author = {Miller, Barton P. and Fredriksen, Louis and So, Bryan},
	month = dec,
	year = {1990},
	pages = {32--44},
}

@article{sen_cute_nodate,
	title = {{CUTE}: {A} {Concolic} {Unit} {Testing} {Engine} for {C}},
	abstract = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more speciﬁcally, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an eﬃcient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.},
	language = {en},
	author = {Sen, Koushik and Marinov, Darko and Agha, Gul},
	pages = {10},
}

@article{godefroid_dart_nodate,
	title = {{DART}: {Directed} {Automated} {Random} {Testing}},
	abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles – there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
	language = {en},
	author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
	pages = {11},
}

@article{de_moura_satisfiability_2011,
	title = {Satisfiability modulo theories: introduction and applications},
	volume = {54},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Satisfiability modulo theories},
	url = {https://dl.acm.org/doi/10.1145/1995376.1995394},
	doi = {10.1145/1995376.1995394},
	abstract = {Checking the satisfiability of logical formulas, SMT solvers scale orders of magnitude beyond custom ad hoc solvers.},
	language = {en},
	number = {9},
	urldate = {2022-02-25},
	journal = {Communications of the ACM},
	author = {De Moura, Leonardo and Bjørner, Nikolaj},
	month = sep,
	year = {2011},
	pages = {69--77},
}

@inproceedings{visser2004test,
	title = {Test input generation with java {PathFinder}},
	booktitle = {Proceedings of the 2004 {ACM} {SIGSOFT} international symposium on {Software} testing and analysis},
	author = {Visser, Willem and Pǎsǎreanu, Corina S and Khurshid, Sarfraz},
	year = {2004},
	pages = {97--107},
}

@article{cadar2013symbolic,
	title = {Symbolic execution for software testing: three decades later},
	volume = {56},
	number = {2},
	journal = {Communications of the ACM},
	author = {Cadar, Cristian and Sen, Koushik},
	year = {2013},
	note = {Publisher: ACM New York, NY, USA},
	pages = {82--90},
}

@article{king_symbolic_1976,
	title = {Symbolic execution and program testing},
	volume = {19},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/360248.360252},
	doi = {10.1145/360248.360252},
	abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may he symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described, it interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
	language = {en},
	number = {7},
	urldate = {2022-02-25},
	journal = {Communications of the ACM},
	author = {King, James C.},
	month = jul,
	year = {1976},
	pages = {385--394},
}

@inproceedings{cadar_symbolic_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Symbolic execution for software testing in practice: preliminary assessment},
	isbn = {978-1-4503-0445-0},
	shorttitle = {Symbolic execution for software testing in practice},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985995},
	doi = {10.1145/1985793.1985995},
	abstract = {We present results for the “Impact Project Focus Area” on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry.},
	language = {en},
	urldate = {2022-02-25},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Cadar, Cristian and Godefroid, Patrice and Khurshid, Sarfraz and Păsăreanu, Corina S. and Sen, Koushik and Tillmann, Nikolai and Visser, Willem},
	month = may,
	year = {2011},
	pages = {1066--1071},
}

@article{misra_how_2016,
	title = {How {Socially} {Aware} {Are} {Social} {Media} {Privacy} {Controls}?},
	volume = {49},
	issn = {1558-0814},
	doi = {10.1109/MC.2016.83},
	abstract = {Social media sites are key mediators of online communication. Yet the privacy controls for these sites are not fully socially aware, even when privacy management is known to be fundamental to successful social relationships.},
	number = {3},
	journal = {Computer},
	author = {Misra, Gaurav and Such, Jose M.},
	month = mar,
	year = {2016},
	note = {Conference Name: Computer},
	keywords = {Access control, Context awareness, Facebook, Google, Media, Privacy, Social Computing, Social network services, privacy, privacy controls, security, social media, social media privacy controls},
	pages = {96--99},
}

@book{beck2000extreme,
	title = {Extreme programming explained: embrace change},
	publisher = {addison-wesley professional},
	author = {Beck, Kent},
	year = {2000},
}

@article{fagan1977inspecting,
	title = {Inspecting software design and code},
	volume = {23},
	number = {10},
	journal = {Datamation},
	author = {Fagan, Michael E},
	year = {1977},
	note = {Publisher: CAHNERS-DENVER PUBLISHING CO 8773 S RIDGELINE BLVD, HIGHLANDS RANCH, CO …},
	pages = {133},
}

@article{fagan1999design,
	title = {Design and code inspections to reduce errors in program development},
	volume = {38},
	number = {2.3},
	journal = {IBM Systems Journal},
	author = {Fagan, Michael E},
	year = {1999},
	note = {Publisher: IBM},
	pages = {258--287},
}

@article{knight_improved_1993,
	title = {An improved inspection technique},
	volume = {36},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/163359.163366},
	doi = {10.1145/163359.163366},
	language = {en},
	number = {11},
	urldate = {2022-02-25},
	journal = {Communications of the ACM},
	author = {Knight, John C. and Myers, E. Ann},
	month = nov,
	year = {1993},
	pages = {51--61},
}

@book{gilb1993software,
	title = {Software inspections},
	publisher = {Addison-Wesley Reading, Masachusetts},
	author = {Gilb, Tom and Graham, Dorothy},
	year = {1993},
}

@article{shen_rtkaller_2021,
	title = {Rtkaller: {State}-aware {Task} {Generation} for {RTOS} {Fuzzing}},
	volume = {20},
	issn = {1539-9087},
	shorttitle = {Rtkaller},
	url = {http://doi.org/10.1145/3477014},
	doi = {10.1145/3477014},
	abstract = {A real-time operating system (RTOS) is an operating system designed to meet certain real-time requirements. It is widely used in embedded applications, and its correctness is safety-critical. However, the validation of RTOS is challenging due to its complex real-time features and large code base. In this paper, we propose Rtkaller, a state-aware kernel fuzzer for the vulnerability detection in RTOS. First, Rtkaller implements an automatic task initialization to transform the syscall sequences into initial tasks with more real-time information. Then, a coverage-guided task mutation is designed to generate those tasks that explore more in-depth real-time related code for parallel execution. Moreover, Rtkaller realizes a task modification to correct those tasks that may hang during fuzzing. We evaluated it on recent versions of rt-Linux, which is one of the most widely used RTOS. Compared to the state-of-the-art kernel fuzzers Syzkaller and Moonshine, Rtkaller achieves the same code coverage at the speed of 1.7X and 1.6X, gains an increase of 26.1\% and 22.0\% branch coverage within 24 hours respectively. More importantly, Rtkaller has confirmed 28 previously unknown vulnerabilities that are missed by other fuzzers.},
	number = {5s},
	urldate = {2022-02-25},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Shen, Yuheng and Sun, Hao and Jiang, Yu and Shi, Heyuan and Yang, Yixiao and Chang, Wanli},
	month = sep,
	year = {2021},
	keywords = {Fuzz testing, RTOS, task generation, vulnerability detection},
	pages = {83:1--83:22},
}

@inproceedings{fioraldi_use_2021,
	title = {The {Use} of {Likely} {Invariants} as {Feedback} for {Fuzzers}},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/fioraldi},
	language = {en},
	urldate = {2022-02-25},
	author = {Fioraldi, Andrea and D'Elia, Daniele Cono and Balzarotti, Davide},
	year = {2021},
	pages = {2829--2846},
}

@article{chen_metamorphic_nodate,
	title = {Metamorphic {Testing}: {A} {New} {Approach} for {Generating} {Next} {Test} {Cases}},
	abstract = {In software testing, a set of test cases is constructed according to some prede ned selection criteria. The software is then examined against these test cases. Three interesting observations have been made on the current artifacts of software testing. Firstly, an error-revealing test case is considered useful while a successful test case which does not reveal software errors is usually not further investigated. Whether these successful test cases still contain useful information for revealing software errors has not been properly studied. Secondly, no matter how extensive the testing has been conducted in the development phase, errors may still exist in the software 5 . These errors, if left undetected, may eventually cause damage to the production system. The study of techniques for uncovering software errors in the production phase is seldom addressed in the literature. Thirdly, as indicated by Weyuker in 6 , the availability of test oracles is pragmatically unattainable in most situations. However, the availability of test oracles is generally assumed in conventional software testing techniques. In this paper, we propose a novel test case selection technique that derives new test cases from the successful ones. The selection aims at revealing software errors that are possibly left undetected in successful test cases which may be generated using some existing strategies. As such, the proposed technique augments the e ectiveness of existing test selection strategies. The yThis project was partially supported by a grant from the Australian Research Council and the Hong Kong Research Grant Council.},
	language = {en},
	author = {Chen, T Y and {Cheung} and {Yiu}},
	pages = {11},
}

@article{waldman_privacy_2016,
	title = {Privacy, {Sharing}, and {Trust}: {The} {Facebook} {Study}},
	volume = {67},
	issn = {0008-7262},
	shorttitle = {Privacy, {Sharing}, and {Trust}},
	url = {https://scholarlycommons.law.case.edu/caselrev/vol67/iss1/10},
	number = {1},
	journal = {Case Western Reserve Law Review},
	author = {Waldman, Ari},
	month = jan,
	year = {2016},
	pages = {193},
}

@book{wohlin_experimentation_2012,
	address = {Berlin, Heidelberg},
	title = {Experimentation in {Software} {Engineering}},
	isbn = {978-3-642-29043-5 978-3-642-29044-2},
	url = {http://link.springer.com/10.1007/978-3-642-29044-2},
	language = {en},
	urldate = {2022-02-22},
	publisher = {Springer Berlin Heidelberg},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders},
	year = {2012},
	doi = {10.1007/978-3-642-29044-2},
}

@article{lange_effects_2006,
	title = {Effects of {Defects} in {UML} {Models} – {An} {Experimental} {Investigation}},
	abstract = {The Uniﬁed Modeling Language (UML) is the de facto standard for designing and architecting software systems. UML oﬀers a large number of diagram types that can be used with varying degree of rigour. As a result UML models may contain consistency defects. Previous research has shown that industrial UML models that are used as basis for implementation and maintenance contain large numbers of defects. This study investigates to what extent implementers detect defects and to what extent defects cause diﬀerent interpretations by diﬀerent readers. We performed two controlled experiments with a large group of students (111) and a group of industrial practitioners (48). The experiment’s results show that defects often remain undetected and cause misinterpretations. We present a classiﬁcation of defect types based on a ranking of detection rate and risk for misinterpretation. Additionally we observed eﬀects of using domain knowledge to compensate defects. The results are generalizable to industrial UML users and can be used for improving quality assurance techniques for UML-based development.},
	language = {en},
	author = {Lange, Christian F J and Lange, C F J and Chaudron, Michel R V and Chaudron, M R V},
	year = {2006},
	pages = {10},
}

@inproceedings{hayhurst2001practical,
	title = {A practical approach to modified condition/decision coverage},
	volume = {1},
	booktitle = {20th {DASC}. 20th digital avionics systems conference (cat. {No}. {01CH37219})},
	author = {Hayhurst, Kelly J and Veerhusen, Dan S},
	year = {2001},
	note = {tex.organization: IEEE},
	pages = {1B2--1},
}

@inproceedings{mockus_test_2009,
	address = {Lake Buena Vista, FL, USA},
	title = {Test coverage and post-verification defects: {A} multiple case study},
	isbn = {978-1-4244-4842-5},
	shorttitle = {Test coverage and post-verification defects},
	url = {http://ieeexplore.ieee.org/document/5315981/},
	doi = {10.1109/ESEM.2009.5315981},
	abstract = {Test coverage is a promising measure of test effectiveness and development organizations are interested in costeffective levels of coverage that provide sufﬁcient fault removal with contained testing effort. We have conducted a multiple-case study on two dissimilar industrial software projects to investigate if test coverage reﬂects test effectiveness and to ﬁnd the relationship between test effort and the level of test coverage. We ﬁnd that in both projects the increase in test coverage is associated with decrease in ﬁeld reported problems when adjusted for the number of prerelease changes. A qualitative investigation revealed several potential explanations, including code complexity, developer experience, the type of functionality, and remote development teams. All these factors were related to the level of coverage and quality, with coverage having an effect even after these adjustments. We also ﬁnd that the test effort increases exponentially with test coverage, but the reduction in ﬁeld problems increases linearly with test coverage. This suggests that for most projects the optimal levels of coverage are likely to be well short of 100\%.},
	language = {en},
	urldate = {2022-02-18},
	booktitle = {2009 3rd {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE},
	author = {Mockus, Audris and Nagappan, Nachiappan and Dinh-Trong, Trung T.},
	month = oct,
	year = {2009},
	pages = {291--301},
}

@inproceedings{piwowarski_coverage_1993,
	address = {Baltimore, MD, USA},
	title = {Coverage measurement experience during function test},
	isbn = {978-0-8186-3700-1},
	url = {http://ieeexplore.ieee.org/document/346035/},
	doi = {10.1109/ICSE.1993.346035},
	abstract = {This paper discusses the issues of test coverage measurement in industry and justijies the benefits of the measurement using a framework developed by the authors. Experience with the measurement is formalized and packaged so that other researchers in industry can share and reuse it. In the paper, function test of large-scale system software is defined and analyzed. Based on the discussions of function test, a framework for analyzing the function test error removal process is developed. An experience-based error removal model and a cost model are proven to be useful tools for justifving tesf coverage measurement during function test. Data obtained from a real project is analyzed using theframework for validation.},
	language = {en},
	urldate = {2022-02-18},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE Comput. Soc. Press},
	author = {Piwowarski, P. and Ohba, M. and Caruso, J.},
	year = {1993},
	pages = {287--301},
}

@inproceedings{ivankovic_code_2019,
	address = {Tallinn Estonia},
	title = {Code coverage at {Google}},
	isbn = {978-1-4503-5572-8},
	url = {https://dl.acm.org/doi/10.1145/3338906.3340459},
	doi = {10.1145/3338906.3340459},
	abstract = {Code coverage is a measure of the degree to which a test suite exercises a software system. Although coverage is well established in software engineering research, deployment in industry is often inhibited by the perceived usefulness and the computational costs of analyzing coverage at scale. At Google, coverage information is computed for one billion lines of code daily, for seven programming languages. A key aspect of making coverage information actionable is to apply it at the level of changesets and code review.},
	language = {en},
	urldate = {2022-02-18},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ivanković, Marko and Petrović, Goran and Just, René and Fraser, Gordon},
	month = aug,
	year = {2019},
	pages = {955--963},
}

@article{rasmussen_skills_1983,
	title = {Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models},
	volume = {SMC-13},
	issn = {0018-9472, 2168-2909},
	url = {http://ieeexplore.ieee.org/document/6313160/},
	doi = {10.1109/TSMC.1983.6313160},
	abstract = {The introduction of information technology based on digital computers for the design of man-machine interface systems has led to a requirement for consistent models of human performance in routine task environments and during unfamiliar task conditions. A discussion is pre­ sented of the requirement for different types of models for representing performance at the skill-, rule-, and knowledge-based levels, together with a review of the different ways in which information is perceived at these different levels in terms of signals, signs, and symbols. Particular attention is paid to the different possible ways of representing system properties which underlie knowledge-based performance and which can be char­ acterized at several levels of abstraction—from the representation of physical form, through functional representation, to representation in terms of intention or purpose. Furthermore, the role of qualitative and quantita­ tive models in the design and evaluation of interface systems is mentioned, and the need to consider such distinctions carefully is discussed.},
	language = {en},
	number = {3},
	urldate = {2022-02-16},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Rasmussen, Jens},
	month = may,
	year = {1983},
	pages = {257--266},
}

@article{humphrey_why_2005,
	title = {Why {Big} {Software} {Projects} {Fail}: {The} 12 {Key} {Questions}},
	language = {en},
	author = {Humphrey, Watts S},
	year = {2005},
	pages = {5},
}

@inproceedings{leesatapornwongsa_taxdc_2016,
	address = {Atlanta Georgia USA},
	title = {{TaxDC}: {A} {Taxonomy} of {Non}-{Deterministic} {Concurrency} {Bugs} in {Datacenter} {Distributed} {Systems}},
	isbn = {978-1-4503-4091-5},
	shorttitle = {{TaxDC}},
	url = {https://dl.acm.org/doi/10.1145/2872362.2872374},
	doi = {10.1145/2872362.2872374},
	abstract = {We present TaxDC, the largest and most comprehensive taxonomy of non-deterministic concurrency bugs in distributed systems. We study 104 distributed concurrency (DC) bugs from four widely-deployed cloud-scale datacenter distributed systems, Cassandra, Hadoop MapReduce, HBase and ZooKeeper. We study DC-bug characteristics along several axes of analysis such as the triggering timing condition and input preconditions, error and failure symptoms, and ﬁx strategies, collectively stored as 2,083 classiﬁcation labels in TaxDC database. We discuss how our study can open up many new research directions in combating DC bugs.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Leesatapornwongsa, Tanakorn and Lukman, Jeffrey F. and Lu, Shan and Gunawi, Haryadi S.},
	month = mar,
	year = {2016},
	pages = {517--530},
}

@book{mizuno_management_2020,
	title = {Management for {Quality} {Improvement}: {The} 7 {New} {QC} {Tools}},
	isbn = {978-1-00-016208-0},
	shorttitle = {Management for {Quality} {Improvement}},
	abstract = {With continuous improvement (kaizen) and Total Quality Control (TQC) becoming increasingly important to world class companies, there's an urgent need to build quality into every management decision. The tools presented in this book allow you to do just that. They represent the most important advance in quality deployment and project management in recent years.Unlike the seven traditional QC tools, which measure quality problems that already exist and are used by quality circles, these seven new QC tools make it possible for managers to plan wide-ranging and detailed TQC objectives throughout the entire organization. These tools, some borrowed from other disciplines and others developed specifically for quality management, include the relations diagram, the KJ method (affinity diagram), the systematic diagram, the matrix diagram, matrix data analysis, the process decision program chart (PDPC), and the arrow diagram. Together they will help you to: Expand the scope of quality efforts company-wide. Set up and manage the systems necessary to resolve major quality problems. Anticipate potential quality problems and actually eliminate defects before they happen.Never before available in English, Management for Quality Improvement is absolutely essential reading if you are in any area of project management, quality assurance, MIS, or TQC.},
	language = {en},
	publisher = {CRC Press},
	author = {Mizuno, Sigeru},
	month = aug,
	year = {2020},
	note = {Google-Books-ID: aQ\_2DwAAQBAJ},
	keywords = {Business \& Economics / Management, Business \& Economics / Quality Control},
}

@misc{look_what_2021,
	title = {What is {Trust} \& {Safety}?},
	url = {https://sylvialook.medium.com/intro-to-trust-safety-in-tech-part-1-c142521f1806},
	abstract = {In recent years, there has been a spike in job openings for Trust \& Safety (T\&S) roles. Tech companies like TikTok, Airbnb, Twitter…},
	language = {en},
	urldate = {2022-02-02},
	journal = {Medium},
	author = {Look, Sylvia},
	month = oct,
	year = {2021},
}

@article{nakajo_case_1991,
	title = {A case history analysis of software error cause-effect relationships},
	volume = {17},
	issn = {1939-3520},
	doi = {10.1109/32.83917},
	abstract = {Approximately 700 errors in four commercial measuring-control software products were analyzed, and the cause-effect relationships of errors occurring during software development were identified. The analysis method used defined appropriate observation points along the path leading from cause to effect of a software error and gathered the corresponding data by analyzing each error using fault tree analysis. Each observation point's data were categorized, and the relationships between two adjoining points were summarized using a cross-indexing table. Four major error-occurrence mechanisms were identified; two are related to hardware and software interface specification misunderstandings, while the other two are related to system and module function misunderstandings. The effects of structured analysis and structured design methods on software errors were evaluated.{\textless}{\textgreater}},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Nakajo, T. and Kume, H.},
	month = aug,
	year = {1991},
	keywords = {Cause effect analysis, Computer aided software engineering, Computer errors, Data analysis, Error analysis, History, Humans, Marine vehicles, Programming, US Department of Transportation},
	pages = {830--838},
}

@inproceedings{thung_empirical_2012,
	title = {An {Empirical} {Study} of {Bugs} in {Machine} {Learning} {Systems}},
	doi = {10.1109/ISSRE.2012.22},
	abstract = {Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important. A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6\% of the bugs belong to the algorithm/method category, 15.6\% of the bugs belong to the non-functional category, and 13\% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research.},
	booktitle = {2012 {IEEE} 23rd {International} {Symposium} on {Software} {Reliability} {Engineering}},
	author = {Thung, Ferdian and Wang, Shaowei and Lo, David and Jiang, Lingxiao},
	month = nov,
	year = {2012},
	note = {ISSN: 2332-6549},
	keywords = {Software reliability},
	pages = {271--280},
}

@misc{wikimedia_foundation_trust_2022,
	title = {Trust and {Safety}/{Case} {Review} {Committee} - {Meta}},
	url = {https://meta.wikimedia.org/wiki/Trust_and_Safety/Case_Review_Committee},
	language = {en},
	urldate = {2022-02-07},
	journal = {Wikimedia},
	author = {{Wikimedia Foundation}},
	month = feb,
	year = {2022},
}

@misc{twitter_about_nodate,
	title = {About {Twitter} {\textbar} {Trust} and {Safety} {Council}},
	url = {https://about.twitter.com/en/our-priorities/healthy-conversations/trust-and-safety-council.html},
	abstract = {Twitter's policy and enforcement actions are informed by public interest organizations that protect individuals and their civil liberties.},
	language = {en},
	urldate = {2022-02-07},
	author = {{Twitter}},
}

@misc{macgillivray_databite_2020,
	title = {Databite {No}. 134: {Origins} of {Trust} and {Safety} with {Alexander} {Macgillivray} and {Nicole} {Wong}},
	shorttitle = {Origins of {Trust} and {Safety}},
	url = {https://datasociety.net/library/origins-of-trust-and-safety/},
	abstract = {Alexander Macgillivray and Nicole Wong discuss the role of Trust and Safety, and make suggestions as regulation, policy, and public awareness evolve.},
	language = {en-US},
	urldate = {2022-02-03},
	journal = {Data \& Society},
	author = {Macgillivray, Alexander and Wong, Nicole},
	month = jul,
	year = {2020},
}

@inproceedings{wang_comprehensive_2017,
	address = {Urbana-Champaign, IL, USA},
	series = {{ASE} 2017},
	title = {A comprehensive study on real world concurrency bugs in {Node}.js},
	isbn = {978-1-5386-2684-9},
	abstract = {Node.js becomes increasingly popular in building server-side JavaScript applications. It adopts an event-driven model, which supports asynchronous I/O and non-deterministic event processing. This asynchrony and non-determinism can introduce intricate concurrency bugs, and leads to unpredictable behaviors. An in-depth understanding of real world concurrency bugs in Node.js applications will significantly promote effective techniques in bug detection, testing and fixing for Node.js. In this paper, we present NodeCB, a comprehensive study on real world concurrency bugs in Node.js applications. Specifically, we have carefully studied 57 real bug cases from open-source Node.js applications, and have analyzed their bug characteristics, e.g., bug patterns and root causes, bug impacts, bug manifestation, and fix strategies. Through this study, we obtain several interesting findings, which may open up many new research directions in combating concurrency bugs in Node.js. For example, one finding is that two thirds of the bugs are caused by atomicity violation. However, due to lack of locks and transaction mechanism, Node.js cannot easily express and guarantee the atomic intention.},
	urldate = {2022-01-11},
	booktitle = {Proceedings of the 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Wang, Jie and Dou, Wensheng and Gao, Yu and Gao, Chushu and Qin, Feng and Yin, Kang and Wei, Jun},
	month = oct,
	year = {2017},
	keywords = {JavaScript, Node.js, concurrency bug, empirical study, event-driven},
	pages = {520--531},
}

@article{amedie_impact_2015,
	title = {The {Impact} of {Social} {Media} on {Society}},
	url = {https://scholarcommons.scu.edu/engl_176/2},
	journal = {Pop Culture Intersections},
	author = {Amedie, Jacob},
	month = sep,
	year = {2015},
}

@misc{noauthor_fig_nodate,
	title = {Fig. 2. {Reference} {Architecture} of an {Online} {Social} {Network} {Platform}},
	url = {https://www.researchgate.net/figure/Reference-Architecture-of-an-Online-Social-Network-Platform_fig1_227226547},
	abstract = {Download scientific diagram {\textbar} Reference Architecture of an Online Social Network Platform   from publication: Online Social Networks: Status and Trends {\textbar} The rapid proliferation of Online Social Network (OSN) sites has made a profound impact on the WWW, which tends to reshape its structure, design, and utility. Industry experts believe that OSNs create a potentially transformational change in consumer behavior and will bring a... {\textbar} Online Social Networks, Consumer Satisfaction and Consumer Behavior {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2022-02-16},
	journal = {ResearchGate},
}

@article{kruchten_41_1995,
	title = {The 4+1 {View} {Model} of architecture},
	volume = {12},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/469759/},
	doi = {10.1109/52.469759},
	language = {en},
	number = {6},
	urldate = {2021-08-20},
	journal = {IEEE Software},
	author = {Kruchten, P.B.},
	month = nov,
	year = {1995},
	pages = {42--50},
}

@article{clinch_learning_2018,
	title = {Learning from {Our} {Mistakes}: {Identifying} {Opportunities} for {Technology} {Intervention} against {Everyday} {Cognitive} {Failure}},
	volume = {17},
	issn = {1536-1268},
	shorttitle = {Learning from {Our} {Mistakes}},
	url = {https://ieeexplore.ieee.org/document/8383664/},
	doi = {10.1109/MPRV.2018.022511240},
	language = {en},
	number = {2},
	urldate = {2022-02-16},
	journal = {IEEE Pervasive Computing},
	author = {Clinch, Sarah and Mascolo, Cecilia},
	month = apr,
	year = {2018},
	pages = {22--33},
}

@inproceedings{hara_data-driven_2018,
	address = {Montreal QC Canada},
	title = {A {Data}-{Driven} {Analysis} of {Workers}' {Earnings} on {Amazon} {Mechanical} {Turk}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174023},
	doi = {10.1145/3173574.3174023},
	language = {en},
	urldate = {2022-02-16},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hara, Kotaro and Adams, Abigail and Milland, Kristy and Savage, Saiph and Callison-Burch, Chris and Bigham, Jeffrey P.},
	month = apr,
	year = {2018},
	pages = {1--14},
}

@incollection{lee_case_2014,
	address = {Heidelberg},
	title = {A {Case} {Study} in {Defect} {Measurement} and {Root} {Cause} {Analysis} in a {Turkish} {Software} {Organization}},
	volume = {496},
	isbn = {978-3-319-00947-6 978-3-319-00948-3},
	url = {http://link.springer.com/10.1007/978-3-319-00948-3_4},
	abstract = {In software projects, ﬁnal products aim to meet customer needs and concurrently to have the least number of defects. Defect identiﬁcation and removal processes offer valuable insights regarding all stages of software development. Therefore, defects are recorded during the software development process with the intentions of not only ﬁxing them before the product is delivered to the customer, but also accumulating data that can be researched upon. That data can later be used for software process improvement. One of the techniques for analyzing defects is the root cause analysis (RCA). A case study is conducted in one of the leading, medium sized software companies of Turkey by utilizing the RCA method. The collected defect data has been analyzed with Pareto charts and the root causes for outstanding defect categories have been identiﬁed with the use of ﬁshbone diagrams and expert grading, demonstrating that these techniques can be effectively used in RCA. The main root causes of the investigated defect items have been identiﬁed as lack of knowledge and extenuation of the undertaken task, and corrective actions have been proposed to upper management. The case study is formulated in a way to provide a basis for software development organizations that aim to conduct defect analysis and obtain meaningful results. All stages of the research and the case study are explained in detail and the efforts spent are given.},
	language = {en},
	urldate = {2022-02-16},
	booktitle = {Software {Engineering} {Research}, {Management} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Atagoren, Cagla and Chouseinoglou, Oumout},
	editor = {Lee, Roger},
	year = {2014},
	doi = {10.1007/978-3-319-00948-3_4},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {55--72},
}

@inproceedings{lehtinen_what_2011,
	address = {Banff, AB, Canada},
	title = {What are {Problem} {Causes} of {Software} {Projects}? {Data} of {Root} {Cause} {Analysis} at {Four} {Software} {Companies}},
	isbn = {978-1-4577-2203-5 978-0-7695-4604-9},
	shorttitle = {What are {Problem} {Causes} of {Software} {Projects}?},
	url = {http://ieeexplore.ieee.org/document/6092595/},
	doi = {10.1109/ESEM.2011.55},
	abstract = {Root cause analysis (RCA) is a structured investigation of a problem to detect the causes that need to be prevented. We applied ARCA, an RCA method, to target problems of four medium-sized software companies and collected 648 causes of software engineering problems. Thereafter, we applied grounded theory to the causes to study their types and related process areas. We detected 14 types of causes in 6 process areas. Our results indicate that development work and software testing are the most common process areas, whereas lack of instructions and experiences, insufficient work practices, low quality task output, task difficulty, and challenging existing product are the most common types of the causes. As the types of causes are evenly distributed between the cases, we hypothesize that the distributions could be generalizable. Finally, we found that only 2.5\% of the causes are related to software development tools that are widely investigated in software engineering research.},
	language = {en},
	urldate = {2022-02-16},
	booktitle = {2011 {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE},
	author = {Lehtinen, Timo O.A. and Mantyla, Mika V.},
	month = sep,
	year = {2011},
	pages = {388--391},
}

@techreport{obar_social_2015,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Social {Media} {Definition} and the {Governance} {Challenge} - {An} {Introduction} to the {Special} {Issue}},
	url = {https://papers.ssrn.com/abstract=2663153},
	abstract = {This introduction to a special issue of "Telecommunications Policy" entitled "The Governance of Social Media" begins with a definition of social media that informs all contributions in the special issue. A section describing the challenges associated with the governance of social media is presented next, followed by an overview of the various articles included in the special issue.},
	language = {en},
	number = {ID 2663153},
	urldate = {2022-02-15},
	institution = {Social Science Research Network},
	author = {Obar, Jonathan A. and Wildman, Steven S.},
	month = jul,
	year = {2015},
	doi = {10.2139/ssrn.2663153},
	keywords = {communication policy, information policy, internet governance, privacy, social media, social network, surveillance, telecommunication policy},
}

@inproceedings{pereira_social_2010,
	title = {Social software building blocks: {Revisiting} the honeycomb framework},
	shorttitle = {Social software building blocks},
	doi = {10.1109/i-Society16502.2010.6018707},
	abstract = {The possibility of developing more interactive and innovative applications allowed users interact with each other and have a primary role as producers of content — these systems are called social software. This paper examines the definition of the concept of social software with its design process and structure. We introduce the social software honeycomb, a framework built to help in understanding social software. Based on an analysis of an inclusive social network we revisit this framework discussing its elements, suggesting its expansion and how it can be used in the design and study of social software. We argue that the framework must be extended and theoretically grounded in order to address several points imposed by the “social”.},
	booktitle = {2010 {International} {Conference} on {Information} {Society}},
	author = {Pereira, Roberto and Baranauskas, M. Cecilia C. and da Silva, Sergio Roberto P.},
	month = jun,
	year = {2010},
	keywords = {Collaboration, Context, Encyclopedias, Internet, Social network services, Software, Videos},
	pages = {253--258},
}

@article{lehtinen_perceived_nodate,
	title = {Perceived {Feasibility} of {Using} {Root} {Cause} {Analysis} in {Post} {Project} {Reviews}: an {Empirical} {Investigation}},
	abstract = {Root cause analysis (RCA) is a structured investigation of the problem to identify which underlying causes need to be fixed. In software engineering (SE), the scientific work on RCA is rather scarce. Feasibility of RCA to identify SE problem causes is not widely studied, the RCA methods are not compared with one another, and there are only a few studies on the required effort to conduct RCA. Additionally, studying how the participating people experience RCA is totally set aside.},
	language = {en},
	author = {Lehtinen, Timo O A},
	pages = {8},
}

@article{lehtinen_perceived_2014,
	title = {Perceived causes of software project failures – {An} analysis of their relationships},
	volume = {56},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584914000263},
	doi = {10.1016/j.infsof.2014.01.015},
	abstract = {Objective: The aim of this study is to conduct in-depth analysis of software project failures in four software product companies in order to understand the causes of failures and their relationships. For each failure, we want to understand which causes, so called bridge causes, interconnect different process areas, and which causes were perceived as the most promising targets for process improvement.
Method: The causes of failures were detected by conducting root cause analysis. For each cause, we classified its type, process area, and interconnectedness to other causes. We quantitatively analyzed which type, process area, and interconnectedness categories (bridge, local) were common among the causes selected as the most feasible targets for process improvement activities. Finally, we qualitatively analyzed the bridge causes in order to find common denominators for the causal relationships interconnecting the process areas.
Results: For each failure, our method identified causal relationships diagrams including 130 to 185 causes each. All four cases were unique, albeit some similarities occurred. On average, 50\% of the causes were bridge causes. Lack of cooperation, weak task backlog, and lack of software testing resources were common bridge causes. Bridge causes, and causes related to tasks, people, and methods were common among the causes perceived as the most feasible targets for process improvement. The causes related to the project environment were frequent, but seldom perceived as feasible targets for process improvement.
Conclusion: Prevention of a software project failure requires a case-specific analysis and controlling causes outside the process area where the failure surfaces. This calls for collaboration between the individuals and managers responsible for different process areas.},
	language = {en},
	number = {6},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Lehtinen, Timo O.A. and Mäntylä, Mika V. and Vanhanen, Jari and Itkonen, Juha and Lassenius, Casper},
	month = jun,
	year = {2014},
	pages = {623--643},
}

@article{politowski_learning_2018,
	title = {Learning from the past: {A} process recommendation system for video game projects using postmortems experiences},
	volume = {100},
	issn = {09505849},
	shorttitle = {Learning from the past},
	url = {http://arxiv.org/abs/2009.02445},
	doi = {10.1016/j.infsof.2018.04.003},
	abstract = {Context: The video game industry is a billion dollar industry that faces problems in the way games are developed. One method to address these problems is using developer aid tools, such as Recommendation Systems. These tools assist developers by generating recommendations to help them perform their tasks.},
	language = {en},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Politowski, Cristiano and Fontoura, Lisandra M. and Petrillo, Fabio and Guéhéneuc, Yann-Gaël},
	month = aug,
	year = {2018},
	note = {arXiv: 2009.02445},
	keywords = {Computer Science - Software Engineering},
	pages = {103--118},
}

@article{lehtinen_recurring_2017,
	title = {Recurring opinions or productive improvements—what agile teams actually discuss in retrospectives},
	volume = {22},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-016-9464-2},
	doi = {10.1007/s10664-016-9464-2},
	abstract = {Team-level retrospectives are widely used in agile and lean software development, yet little is known about what is actually discussed during retrospectives or their outcomes. In this paper, we synthesise the outcomes of sprint retrospectives in a large, distributed, agile software development organisation. This longitudinal case study analyses data from 37 teamlevel retrospectives for almost 3 years. We report the outcomes of the retrospectives, their perceived importance for process improvement and relatVed action proposals. Most discussions were related to topics close to and controllable by the team. However, the discussions might suffer from participant bias, and in cases where they are not supported by hard evidence, they might not reflect reality, but rather the sometimes strong opinions of the participants. Some discussions were related to topics that could not be resolved at the team level due to their complexity. Certain topics recurred over a long period of time, either reflecting issues that can and have been solved previously, but that recur naturally as development proceeds, or reflecting waste since they cannot be resolved or improved on by the team due to a lack of controllability or their complexity. For example, the discussion on estimation accuracy did not reflect the true situation and improving the estimates was complicated. On the other hand, discussions on the high number of known bugs recurred despite effective improvements as development proceeded.},
	language = {en},
	number = {5},
	urldate = {2022-02-15},
	journal = {Empirical Software Engineering},
	author = {Lehtinen, Timo O. A. and Itkonen, Juha and Lassenius, Casper},
	month = oct,
	year = {2017},
	pages = {2409--2452},
}

@article{lehtinen_tool_2014,
	title = {A tool supporting root cause analysis for synchronous retrospectives in distributed software teams},
	volume = {56},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584914000159},
	doi = {10.1016/j.infsof.2014.01.004},
	abstract = {Objective: This paper presents a real-time cloud-based software tool (ARCA-tool) we developed to support RCA in distributed teams and its initial empirical evaluation. The feasibility of using RCA with distributed teams is also evaluated.
Method: We compared our tool with 35 existing RCA software tools. We conducted field studies of four distributed agile software teams at two international software product companies. The teams conducted RCA collaboratively in synchronous retrospective meetings by using the tool we developed. We collected the data using observations, interviews and questionnaires.
Results: Comparison revealed that none of the existing 35 tools matched all the features of our ARCA-tool. The team members found ARCA-tool to be an essential part of their distributed retrospectives. They considered the software as efficient and very easy to learn and use. Additionally, the team members perceived RCA to be a vital part of the retrospectives. In contrast to the prior retrospective practices of the teams, the introduced RCA method was evaluated as efficient and easy to use.
Conclusion: RCA is a useful practice in synchronous distributed retrospectives. However, it requires software tool support for enabling real-time view and co-creation of a cause-effect diagram. ARCA-tool supports synchronous RCA, and includes support for logging problems and causes, problem prioritization, cause-effect diagramming, and logging of process improvement proposals. It enables conducting RCA in distributed retrospectives.},
	language = {en},
	number = {4},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Lehtinen, Timo O.A. and Virtanen, Risto and Viljanen, Juha O. and Mäntylä, Mika V. and Lassenius, Casper},
	month = apr,
	year = {2014},
	pages = {408--437},
}

@article{lehtinen_diagrams_2015,
	title = {Diagrams or structural lists in software project retrospectives – {An} experimental comparison},
	volume = {103},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121215000151},
	doi = {10.1016/j.jss.2015.01.020},
	abstract = {Root cause analysis (RCA) is a recommended practice in retrospectives and cause–effect diagram (CED) is a commonly recommended technique for RCA. Our objective is to evaluate whether CED improves the outcome and perceived utility of RCA. We conducted a controlled experiment with 11 student software project teams by using a single factor paired design resulting in a total of 22 experimental units. Two visualization techniques of underlying causes were compared: CED and a structural list of causes. We used the output of RCA, questionnaires, and group interviews to compare the two techniques. In our results, CED increased the total number of detected causes. CED also increased the links between causes, thus, suggesting more structured analysis of problems. Furthermore, the participants perceived that CED improved organizing and outlining the detected causes. The implication of our results is that using CED in the RCA of retrospectives is recommended, yet, not mandatory as the groups also performed well with the structural list. In addition to increased number of detected causes, CED is visually more attractive and preferred by retrospective participants, even though it is somewhat harder to read and requires speciﬁc software tools.},
	language = {en},
	urldate = {2022-02-15},
	journal = {Journal of Systems and Software},
	author = {Lehtinen, Timo O.A. and Mäntylä, Mika V. and Itkonen, Juha and Vanhanen, Jari},
	month = may,
	year = {2015},
	pages = {17--35},
}

@article{charette_its_2017,
	title = {{IT}'s {Fatal} {Amnesia}},
	volume = {50},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/7842842/},
	doi = {10.1109/MC.2017.32},
	language = {en},
	number = {2},
	urldate = {2022-02-15},
	journal = {Computer},
	author = {Charette, Robert N.},
	month = feb,
	year = {2017},
	pages = {86--91},
}

@article{cerpa_why_2009,
	title = {Why did your project fail?},
	volume = {52},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1610252.1610286},
	doi = {10.1145/1610252.1610286},
	abstract = {Introduction
            
            
              We have been developing software since the 1960s but still have not learned enough to ensure that our software development projects are successful. Boehm
              2
              suggested that realistic schedule and budgets together with a continuing steam of requirements changes are high risk factors. The Standish Group in 1994 noted that approximately 31\% of corporate software development projects were cancelled before completion and 53\% were challenged and cost 180\% above their original estimate.
              13
              Glass discussed 16 project disasters.
              5
              He found that the failed projects he reviewed were mostly huge and that the failure factors were not just management factors but also included technical factors. Linberg in 1999 found that 20\% of software projects failed, and that 46\% experienced cost and schedule overruns or significantly reduced functionality.
              8
              Later, Glass revisited failed projects and found that poor estimation was high on his list of failure factors.
              6
            
            
              In 2007 the Standish Group reported that 35\% of software projects started in 2006 were successful compared with only 16\% in the corresponding 1994 report; however, the 2007 CHAOS report still identifies 46\% (53\% in 1994) of software projects as challenged (having cost or time overruns or not fully meeting user's requirements) and 19\% (31\% in 1994) as outright failures.
              12
              The validity of the Standish Group findings has been questioned as not consistent with cost overrun results of other surveys.
              7
              Jørgensen and Moløkken-Østvold suggested that there are serious problems with the way the Standish Group conducted their research and that the findings were biased toward reports of failure because a random sample of top IT executives was asked to share failure stories when mailed confidential surveys.
            
            
              Recently Charette
              4
              commented that "billions of dollars are wasted each year on failed software projects" and that "we have a dismal history of projects that have gone awry."
              4
              Charette suggests that from 5\%-15\% of projects will be abandoned before or shortly after delivery as hopelessly inadequate.
              4
              Other recent studies, suggest various failure rates for software development projects up to 85\%.
              7
              Stories of software failure capture public attention and in general there is a perception that software quality is not improving but getting worse.
            
            
              Developing software systems is an expensive, and often a difficult process. Although there are many guidelines for successful software development,
              9,11
              few project post-mortems are conducted, and little understanding is gained from the results of past projects. The project manager (PM) and the development team must deal with many pressures from project stakeholders (for example, upper level management, marketing, accounting, customers, and users) during the software development process. These pressures impact both the cost and the quality of the software produced. There are generally more than one or two reasons for a software project to fail, and it usually is a combination of technical, project management and business decisions. Many software project failure factors have been described in the literature.
              1-13
              However, most organizations do not see preventing failure as an urgent matter. It is not understood why this attitude persists.
              4
            
            
              Because most of the literature is based on a handful of failed project case studies, in our research we analyze 70 failed software projects to determine those practices that affected project outcome. We are interested in providing, from the perspective of software practitioners, quantitative evidence targeting those aspects of the development process that contribute to project failure. Essentially, we are interested in updating the results of prior studies and testing the validity of previously reported anecdotal evidence. To date, no one has taken a set of project data and teased it out to identify, for a whole group of such projects, the most common failure factors. We are interested in everyday projects that are not high profile enough to be reported in the literature. Our work builds on that previously reported by Boehm,
              2
              Charette,
              4
              Glass,
              5,6
              Jørgensen and Moløkken,
              7
              Linberg,
              8
              and the Standish Group,
              12,13
              among others.},
	language = {en},
	number = {12},
	urldate = {2022-02-15},
	journal = {Communications of the ACM},
	author = {Cerpa, Narciso and Verner, June M.},
	month = dec,
	year = {2009},
	pages = {130--134},
}

@article{bjornson_improving_2009,
	title = {Improving the effectiveness of root cause analysis in post mortem analysis: {A} controlled experiment},
	volume = {51},
	issn = {09505849},
	shorttitle = {Improving the effectiveness of root cause analysis in post mortem analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095058490800030X},
	doi = {10.1016/j.infsof.2008.02.003},
	abstract = {Retrospective analysis is a way to share knowledge following the completion of a project or major milestone. However, in the busy workday of a software project, there is rarely time for such reviews and there is a need for eﬀective methods that will yield good results quickly without the need for external consultants or experts. Building on an existing method for retrospective analysis and theories of group involvement, we propose improvements to the root cause analysis phase of a lightweight retrospective analysis method known as post mortem analysis (PMA). In particular, to facilitate brainstorming during the root cause analysis phase of the PMA, we propose certain processual changes to facilitate more active individual participation and the use of less rigidly structured diagrams. We conducted a controlled experiment to compare this new variation of the method with the existing one, and conclude that in our setting of small software teams with no access to an experienced facilitator, the new variation is more eﬀective when it comes to identifying possible root causes of problems and successes. The modiﬁed method also produced more speciﬁc starting points for improving the software development process.},
	language = {en},
	number = {1},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Bjørnson, Finn Olav and Wang, Alf Inge and Arisholm, Erik},
	month = jan,
	year = {2009},
	pages = {150--161},
}

@article{bjornson_knowledge_2008,
	title = {Knowledge management in software engineering: {A} systematic review of studied concepts, findings and research methods used},
	volume = {50},
	issn = {09505849},
	shorttitle = {Knowledge management in software engineering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584908000487},
	doi = {10.1016/j.infsof.2008.03.006},
	abstract = {Software engineering is knowledge-intensive work, and how to manage software engineering knowledge has received much attention. This systematic review identifies empirical studies of knowledge management initiatives in software engineering, and discusses the concepts studied, the major findings, and the research methods used. Seven hundred and sixty-two articles were identified, of which 68 were studies in an industry context. Of these, 29 were empirical studies and 39 reports of lessons learned. More than half of the empirical studies were case studies.},
	language = {en},
	number = {11},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Bjørnson, Finn Olav and Dingsøyr, Torgeir},
	month = oct,
	year = {2008},
	pages = {1055--1068},
}

@article{collier1996defined,
	title = {A defined process for project post mortem review},
	volume = {13},
	number = {4},
	journal = {IEEE software},
	author = {Collier, Bonnie and DeMarco, Tom and Fearey, Peter},
	year = {1996},
	note = {Publisher: IEEE},
	pages = {65--72},
}

@article{raelin2001public,
	title = {Public reflection as the basis of learning},
	volume = {32},
	number = {1},
	journal = {Management learning},
	author = {Raelin, Joseph A},
	year = {2001},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {11--30},
}

@inproceedings{basili1993experience,
	title = {The experience factory and its relationship to other improvement paradigms},
	booktitle = {European software engineering conference},
	author = {Basili, Victor R},
	year = {1993},
	note = {tex.organization: Springer},
	pages = {68--83},
}

@article{caldiera1994experience,
	title = {The experience factory},
	volume = {1},
	journal = {Encyclopedia of Software Eng.: Vol},
	author = {Basili, Victor and Caldiera, Gianluigi and Rombach, H Dieter},
	year = {1994},
	pages = {469--476},
}

@incollection{goos_augmenting_2001,
	address = {Berlin, Heidelberg},
	title = {Augmenting {Experience} {Reports} with {Lightweight} {Postmortem} {Reviews}},
	volume = {2188},
	isbn = {978-3-540-42571-7 978-3-540-44813-6},
	url = {http://link.springer.com/10.1007/3-540-44813-6_17},
	abstract = {Many small and medium-sized companies that develop software experience the same problems repeatedly, and have few systems in place to learn from their own mistakes as well as their own successes. Here, we propose a lightweight method to collect experience from completed software projects, and compare the results of this method to more widely applied experience reports. We find that the new method captures more information about core processes related to software development in contrast to experience reports that focus more on management processes.},
	language = {en},
	urldate = {2022-02-15},
	booktitle = {Product {Focused} {Software} {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dingsøyr, Torgeir and Moe, Nils Brede and Nytrø, Ø;ystein},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Bomarius, Frank and Komi-Sirviö, Seija},
	year = {2001},
	doi = {10.1007/3-540-44813-6_17},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {167--181},
}

@article{petrillo_what_2009,
	title = {What went wrong? {A} survey of problems in game development},
	volume = {7},
	issn = {1544-3574, 1544-3574},
	shorttitle = {What went wrong?},
	url = {https://dl.acm.org/doi/10.1145/1486508.1486521},
	doi = {10.1145/1486508.1486521},
	abstract = {Despite its growth and profitability, many reports about game projects show that their production is not a simple task, but one beset by common problems and still distant from having a healthy and synergetic work process. The goal of this article is to survey the problems in the development process of electronic games, which are mainly collected from game postmortems, by exploring their similarities and differences to well-known problems in traditional information systems.},
	language = {en},
	number = {1},
	urldate = {2022-02-15},
	journal = {Computers in Entertainment},
	author = {Petrillo, Fábio and Pimenta, Marcelo and Trindade, Francisco and Dietrich, Carlos},
	month = feb,
	year = {2009},
	pages = {1--22},
}

@article{birk_postmortem_2002,
	title = {Postmortem: never leave a project without it},
	volume = {19},
	issn = {0740-7459},
	shorttitle = {Postmortem},
	url = {http://ieeexplore.ieee.org/document/1003452/},
	doi = {10.1109/MS.2002.1003452},
	language = {en},
	number = {3},
	urldate = {2022-02-15},
	journal = {IEEE Software},
	author = {Birk, A. and Dingsoyr, T. and Stalhane, T.},
	month = may,
	year = {2002},
	pages = {43--45},
}

@incollection{abrahamsson_organizational_2007,
	address = {Berlin, Heidelberg},
	title = {Organizational {Learning} {Through} {Project} {Postmortem} {Reviews} – {An} {Explorative} {Case} {Study}},
	volume = {4764},
	isbn = {978-3-540-74765-9 978-3-540-75381-0},
	url = {http://link.springer.com/10.1007/978-3-540-75381-0_13},
	abstract = {A central issue in knowledge management and software process improvement is to learn from experience. In software engineering, most experience is gathered in projects, which makes project experience a prime source for learning. Many companies conduct postmortem reviews, but we have found few companies that analyze the outcome of several reviews to facilitate learning on an organizational level. This paper reports an explorative study of what we can learn from analyzing postmortem review reports of twelve projects in a medium-size software company.},
	language = {en},
	urldate = {2022-02-15},
	booktitle = {Software {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dingsøyr, Torgeir and Moe, Nils Brede and Schalken, Joost and Stålhane, Tor},
	editor = {Abrahamsson, Pekka and Baddoo, Nathan and Margaria, Tiziana and Messnarz, Richard},
	year = {2007},
	doi = {10.1007/978-3-540-75381-0_13},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {136--147},
}

@article{desouza_experiences_2005,
	title = {Experiences with conducting project postmortems: reports versus stories},
	volume = {10},
	issn = {1077-4866, 1099-1670},
	shorttitle = {Experiences with conducting project postmortems},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spip.224},
	doi = {10.1002/spip.224},
	language = {en},
	number = {2},
	urldate = {2022-02-15},
	journal = {Software Process: Improvement and Practice},
	author = {Desouza, Kevin C. and Dingsøyr, Torgeir and Awazu, Yukika},
	month = apr,
	year = {2005},
	pages = {203--215},
}

@incollection{misra_technical_2019,
	address = {Cham},
	title = {Technical and {Managerial} {Difficulties} in {Postmortem} {Analysis} in {Software} {Projects}},
	volume = {11623},
	isbn = {978-3-030-24307-4 978-3-030-24308-1},
	url = {http://link.springer.com/10.1007/978-3-030-24308-1_5},
	abstract = {Software is successfully applied in a wide variety of areas. However, software projects have suﬀered from poor reputation by repeatedly bursting deadlines, costs or failing to fully meet user requirements. Postmortem Analysis is an activity to analyze what happened in projects in search of understanding the failures occurred and the achieved successes. Despite bringing interesting data for improving future projects, Postmortem Analysis is often neglected in organizations. This article seeks to identify and analyze the technical and managerial diﬃculties that exist in its accomplishment through bibliographical research. As a result, it is possible to conclude that the main diﬃculties for realizing postmortem activities are the shortage of time, lack of management support, conﬂicts between stakeholders, diﬃculty in extracting and collecting data, lack of agreement regarding evaluation criteria, lack of standards for achievement, and lack of useful or eﬃcient historical data.},
	language = {en},
	urldate = {2022-02-15},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2019},
	publisher = {Springer International Publishing},
	author = {Vieira, Felipe J. R. and Oliveira, Manoela R. and do Nascimento, Rogério P. C. and Soares, Michel S.},
	editor = {Misra, Sanjay and Gervasi, Osvaldo and Murgante, Beniamino and Stankova, Elena and Korkhov, Vladimir and Torre, Carmelo and Rocha, Ana Maria A.C. and Taniar, David and Apduhan, Bernady O. and Tarantino, Eufemia},
	year = {2019},
	doi = {10.1007/978-3-030-24308-1_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {59--69},
}

@article{lauritsen2007empirical,
	title = {An empirical study of introducing the {Failure} {Mode} and {Effect} {Analysis} technique},
	journal = {Eur SPI’2007},
	author = {Lauritsen, Torgrim and Stålhane, Tor},
	year = {2007},
}

@misc{myhrer_student_nodate,
	title = {Student experiment using {Preliminary} {Hazard} {Analysis}},
	abstract = {Abstract. Cost-effective development of business critical system requires that we identify potential risks in the early stages of development. The safety analysis method Preliminary Hazard Analysis (PHA) is one such method used for identifying risks in the requirement phase when developing safety critical systems. In this paper we report on the result from an experiment that assessed two ways of using PHA: requirements and scenarios. The experiment showed no significant difference in the number of hazards found for the two approaches. However, scenarios turned out to get results that involved more parts of the system, while requirements got results that set more focus on fewer parts of the system.},
	author = {Myhrer, Per Trygve and Stålhane, Tor},
}

@article{dalcher_learning_2002,
	title = {Learning from {Failures}},
	volume = {7},
	issn = {1077-4866, 1099-1670},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spip.156},
	doi = {10.1002/spip.156},
	language = {en},
	number = {2},
	urldate = {2022-02-14},
	journal = {Software Process: Improvement and Practice},
	author = {Dalcher, Darren and Tully, Colin},
	month = jun,
	year = {2002},
	pages = {71--89},
}

@article{dalcher_understanding_nodate,
	title = {Understanding {Stories} of {Information} {Systems} {Failures}},
	abstract = {Information systems development failures are prevalent in many domains and countries. The aim of this paper is to explore some of the issues related to the study of such phenomena. Failure situations are not set-up in advance as the subject of studies. Analysing causes and relationship retrospectively depends on the ability to obtain rich and subjective contextual information that can be shed a light on the circumstances that precipitate failures. The paper makes the case for the use of case history and ante-narrative methods for understanding such scenarios.},
	language = {en},
	author = {Dalcher, Darren and Park, Trent and Road, Bramley},
	pages = {16},
}

@article{dalcher_learning_2003,
	title = {Learning from {Information} {Systems} failures by using narrative and ante- narrative methods},
	language = {en},
	author = {Dalcher, Darren},
	year = {2003},
	pages = {6},
}

@incollection{ruhe_rethinking_2014,
	address = {Berlin, Heidelberg},
	title = {Rethinking {Success} in {Software} {Projects}: {Looking} {Beyond} the {Failure} {Factors}},
	isbn = {978-3-642-55034-8 978-3-642-55035-5},
	shorttitle = {Rethinking {Success} in {Software} {Projects}},
	url = {http://link.springer.com/10.1007/978-3-642-55035-5_2},
	abstract = {The notions of success and failure in software projects are confusing. Failure is often considered in the context of the iron triangle as the inability to meet time, cost, and performance constraints. While there is a consensus around the prevalence of project failure, new projects seem destined to repeat past mistakes. This chapter tries to advance the discussion by offering a new perspective for reasoning about the meaning of success and the different types of software project failures.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Software {Project} {Management} in a {Changing} {World}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dalcher, Darren},
	editor = {Ruhe, Günther and Wohlin, Claes},
	year = {2014},
	doi = {10.1007/978-3-642-55035-5_2},
	pages = {27--49},
}

@article{pankratz_eliminating_nodate,
	title = {{ELIMINATING} {FAILURE} {BY} {LEARNING} {FROM} {IT} – {SYSTEMATIC} {REVIEW} {OF} {IS} {PROJECT} {FAILURE}},
	abstract = {Researchers analyzing project success and failure emphasize the prevailing challenge of successfully completing information system (IS) projects. We conduct an extensive systematic literature review of factors that contributed to failure of real-life IS projects. Our resulting overview entails 54 failure factors, which we grouped in 10 categories applying data-driven qualitative content analysis. We extend our holistic overview by linking the factors to specific project failure dimensions and integrating a stakeholder perspective to account for failure responsibility. Our analysis yields widely acknowledged failure factors like insufficient stakeholder involvement as well as less common factors like history of prior successes. Researchers gain insights into project failure factors along with responsible stakeholders and affected failure dimensions, and can use our overview to identify factors or areas of concern to guide future research. Our overview provides a pillar for IS practitioners to learn from others and to eliminate failure by avoiding past mistakes.},
	language = {en},
	author = {Pankratz, Oleg and Basten, Dirk},
	pages = {20},
}

@inproceedings{boicu_learning_2007,
	address = {Cincinnati, OH, USA},
	title = {Learning complex problem solving expertise from failures},
	isbn = {978-0-7695-3069-7},
	url = {http://ieeexplore.ieee.org/document/4457218/},
	doi = {10.1109/ICMLA.2007.42},
	abstract = {Our research addresses the issue of developing knowledge-based agents that capture and use the problem solving knowledge of subject matter experts from diverse application domains. This paper emphasizes the use of negative examples in agent learning by presenting several strategies for capturing expert’s knowledge when the agent fails to correctly solve a problem. These strategies have been implemented into the Disciple learning agent shell and used in complex application domains such as intelligence analysis, center of gravity determination, and emergency response planning.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Sixth {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA} 2007)},
	publisher = {IEEE},
	author = {Boicu, Cristina and Tecuci, Gheorghe and Boicu, Mihai},
	month = dec,
	year = {2007},
	pages = {118--123},
}

@article{blume_decentralized_2009,
	title = {Decentralized {Organizational} {Learning}: {An} {Experimental} {Investigation}},
	volume = {99},
	issn = {0002-8282},
	shorttitle = {Decentralized {Organizational} {Learning}},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.99.4.1178},
	doi = {10.1257/aer.99.4.1178},
	abstract = {We experimentally study decentralized organizational learning. Our objective is to understand how learning members of an organization cope with the confounding effects of the simultaneous learning of others. We test the predictions of a stylized, rational agent model of organizational learning that provides sharp predictions as to how learning members of an organization might cope with the simultaneous learning of others as a function of fundamental variables, e.g., firm size and the discount factor. While the problem of learning while others are learning is quite difficult, we find support for the comparative static predictions of the model's unique symmetric equilibrium. (JEL C72, D23, D83)},
	language = {en},
	number = {4},
	urldate = {2022-02-14},
	journal = {American Economic Review},
	author = {Blume, Andreas and Duffy, John and Franco, April M},
	month = aug,
	year = {2009},
	pages = {1178--1205},
}

@article{blume_decentralized_2007,
	title = {Decentralized learning from failure},
	volume = {133},
	issn = {00220531},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022053106000238},
	doi = {10.1016/j.jet.2006.01.005},
	abstract = {We study decentralized learning in organizations. Decentralization is captured through Crawford and Haller’s [Learning how to cooperate: optimal play in repeated coordination games, Econometrica 58 (1990) 571–595] attainability constraints on strategies. We analyze a repeated game with imperfectly observable actions. A ﬁxed subset of action proﬁles are successes and all others are failures. The location of successes is unknown. The game is played until either there is a success or the time horizon is reached. We partially characterize optimal attainable strategies in the inﬁnite horizon game by showing that after any ﬁxed time, agents will occasionally randomize while at the same time mixing probabilities cannot be uniformly bounded away from zero.},
	language = {en},
	number = {1},
	urldate = {2022-02-14},
	journal = {Journal of Economic Theory},
	author = {Blume, Andreas and Franco, April Mitchell},
	month = mar,
	year = {2007},
	pages = {504--523},
}

@inproceedings{lincke_comparing_2008,
	address = {New York, NY, USA},
	series = {{ISSTA} '08},
	title = {Comparing software metrics tools},
	isbn = {978-1-60558-050-0},
	url = {https://doi.org/10.1145/1390630.1390648},
	doi = {10.1145/1390630.1390648},
	abstract = {This paper shows that existing software metric tools interpret and implement the definitions of object-oriented software metrics differently. This delivers tool-dependent metrics results and has even implications on the results of analyses based on these metrics results. In short, the metrics-based assessment of a software system and measures taken to improve its design differ considerably from tool to tool. To support our case, we conducted an experiment with a number of commercial and free metrics tools. We calculated metrics values using the same set of standard metrics for three software systems of different sizes. Measurements show that, for the same software system and metrics, the metrics values are tool depended. We also defined a (simple) software quality model for "maintainability" based on the metrics selected. It defines a ranking of the classes that are most critical wrt. maintainability. Measurements show that even the ranking of classes in a software system is metrics tool dependent.},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 2008 international symposium on {Software} testing and analysis},
	publisher = {Association for Computing Machinery},
	author = {Lincke, Rüdiger and Lundberg, Jonas and Löwe, Welf},
	month = jul,
	year = {2008},
	keywords = {comparing tools, software quality metrics},
	pages = {131--142},
}

@inproceedings{bacchelli_expectations_2013,
	address = {San Francisco, CA, USA},
	title = {Expectations, outcomes, and challenges of modern code review},
	isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
	url = {http://ieeexplore.ieee.org/document/6606617/},
	doi = {10.1109/ICSE.2013.6606617},
	abstract = {Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more “lightweight” than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classiﬁed hundreds of review comments across diverse teams at Microsoft. Our study reveals that while ﬁnding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional beneﬁts such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we ﬁnd that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Bacchelli, Alberto and Bird, Christian},
	month = may,
	year = {2013},
	pages = {712--721},
}

@inproceedings{bohme_fuzzing_2020,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2020},
	title = {Fuzzing: on the exponential cost of vulnerability discovery},
	isbn = {978-1-4503-7043-1},
	shorttitle = {Fuzzing},
	url = {https://doi.org/10.1145/3368089.3409729},
	doi = {10.1145/3368089.3409729},
	abstract = {We present counterintuitive results for the scalability of fuzzing. Given the same non-deterministic fuzzer, finding the same bugs linearly faster requires linearly more machines. For instance, with twice the machines, we can find all known bugs in half the time. Yet, finding linearly more bugs in the same time requires exponentially more machines. For instance, for every new bug we want to find in 24 hours, we might need twice more machines. Similarly for coverage. With exponentially more machines, we can cover the same code exponentially faster, but uncovered code only linearly faster. In other words, re-discovering the same vulnerabilities is cheap but finding new vulnerabilities is expensive. This holds even under the simplifying assumption of no parallelization overhead. We derive these observations from over four CPU years worth of fuzzing campaigns involving almost three hundred open source programs, two state-of-the-art greybox fuzzers, four measures of code coverage, and two measures of vulnerability discovery. We provide a probabilistic analysis and conduct simulation experiments to explain this phenomenon.},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Böhme, Marcel and Falk, Brandon},
	month = nov,
	year = {2020},
	keywords = {efficiency, fuzzing, scalability, software testing},
	pages = {713--724},
}

@article{kim_vicarious_2007,
	title = {Vicarious {Learning} from the {Failures} and {Near}-{Failures} of {Others}: {Evidence} from the {U}.{S}. {Commercial} {Banking} {Industry}},
	volume = {50},
	issn = {0001-4273, 1948-0989},
	shorttitle = {Vicarious {Learning} from the {Failures} and {Near}-{Failures} of {Others}},
	url = {http://journals.aom.org/doi/10.5465/amj.2007.25529755},
	doi = {10.5465/amj.2007.25529755},
	language = {en},
	number = {3},
	urldate = {2022-02-14},
	journal = {Academy of Management Journal},
	author = {Kim, Ji-Yub (Jay) and Miner, Anne S.},
	month = jun,
	year = {2007},
	pages = {687--714},
}

@inproceedings{sandoval_alcocer_learning_2016,
	address = {Delft The Netherlands},
	title = {Learning from {Source} {Code} {History} to {Identify} {Performance} {Failures}},
	isbn = {978-1-4503-4080-9},
	url = {https://dl.acm.org/doi/10.1145/2851553.2851571},
	doi = {10.1145/2851553.2851571},
	abstract = {Source code changes may inadvertently introduce performance regressions. Benchmarking each software version is traditionally employed to identify performance regressions. Although e↵ective, this exhaustive approach is hard to carry out in practice. This paper contrasts source code changes against performance variations. By analyzing 1,288 software versions from 17 open source projects, we identiﬁed 10 source code changes leading to a performance variation (improvement or regression). We have produced a cost model to infer whether a software commit introduces a performance variation by analyzing the source code and sampling the execution of a few versions. By proﬁling the execution of only 17\% of the versions, our model is able to identify 83\% of the performance regressions greater than 5\% and 100\% of the regressions greater than 50\%.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 7th {ACM}/{SPEC} on {International} {Conference} on {Performance} {Engineering}},
	publisher = {ACM},
	author = {Sandoval Alcocer, Juan Pablo and Bergel, Alexandre and Valente, Marco Tulio},
	month = mar,
	year = {2016},
	pages = {37--48},
}

@article{argote_persistence_2022,
	title = {The {Persistence} and {Transfer} of {Learning} in {Industrial} {Settings}},
	language = {en},
	author = {Argote, Linda and Beckman, Sara L and Epple, Dennis},
	year = {2022},
	pages = {16},
}

@article{faraj_coordinating_2000,
	title = {Coordinating {Expertise} in {Software} {Development} {Teams}},
	volume = {46},
	issn = {0025-1909, 1526-5501},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.46.12.1554.12072},
	doi = {10.1287/mnsc.46.12.1554.12072},
	language = {en},
	number = {12},
	urldate = {2022-02-14},
	journal = {Management Science},
	author = {Faraj, Samer and Sproull, Lee},
	month = dec,
	year = {2000},
	pages = {1554--1568},
}

@inproceedings{pflugler_dual-sided_2016,
	address = {Alexandria Virginia USA},
	title = {The {Dual}-sided {Effect} of {Project} {Failure} on {IT} {Professionals}},
	isbn = {978-1-4503-4203-2},
	url = {https://dl.acm.org/doi/10.1145/2890602.2890610},
	doi = {10.1145/2890602.2890610},
	abstract = {The effects of project failure on IT professionals have not received much attention in IT research. A failed project evokes negative emotions and therefore could trigger turnover, which has negative influences from the perspective of IT human resource management. However, the failure of IT projects could also have positive influences as professionals might learn from the failed project. This paper focuses on analyzing this dual-sided effect of project failure on IT professionals. We develop hypotheses that will be tested with a large data set from an IT service provider in future research. We expect to contribute to theory by analyzing whether project failure triggers turnover and by analyzing whether IT professionals learn from failed projects and perform better in the future.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 2016 {ACM} {SIGMIS} {Conference} on {Computers} and {People} {Research}},
	publisher = {ACM},
	author = {Pflügler, Christoph and Wiesche, Manuel and Krcmar, Helmut},
	month = jun,
	year = {2016},
	pages = {33--37},
}

@article{wastell_learning_1999,
	title = {Learning {Dysfunctions} in {Information} {Systems} {Development}: {Overcoming} the {Social} {Defenses} with {Transitional} {Objects}},
	volume = {23},
	issn = {02767783},
	shorttitle = {Learning {Dysfunctions} in {Information} {Systems} {Development}},
	url = {https://www.jstor.org/stable/249490?origin=crossref},
	doi = {10.2307/249490},
	language = {en},
	number = {4},
	urldate = {2022-02-14},
	journal = {MIS Quarterly},
	author = {Wastell, David G.},
	month = dec,
	year = {1999},
	pages = {581},
}

@article{pflugler_i_nodate,
	title = {“{Do} {I} {Want} to {Have} {Losers} {In} {My} {Team}?” – {A} {Quantitative} {Study} of {Learning} from {IT} {Project} {Failure}},
	abstract = {This paper is motivated by a lack of research on the learning from failed IT projects of IT professionals. It remains unclear whether they learn from failed projects and conduct more successful projects in the future. We investigate this research gap with a large quantitative dataset from a German IT service provider. We find that IT professionals learn from failed projects and can leverage this knowledge in the future. Therefore, they should not be seen as “losers”, but as a valuable human resource. Our research contributes to the limited research of learning from failure in IT literature. We show that results that have been obtained in other domains are transferable to the IT domain. Our research is limited by the circumstance, that our dataset comes from only one IT company. This is the first paper that analyzes learning from failure of IT professionals and their performance in future projects.},
	language = {en},
	author = {Pflügler, Christoph and Jäschke, Tamara and Mälzer, Thorsten and Wiesche, Manuel and Krcmar, Helmut},
	pages = {10},
}

@article{hollan_distributed_2000,
	title = {Distributed cognition: toward a new foundation for human-computer interaction research},
	volume = {7},
	issn = {1073-0516, 1557-7325},
	shorttitle = {Distributed cognition},
	url = {https://dl.acm.org/doi/10.1145/353485.353487},
	doi = {10.1145/353485.353487},
	abstract = {We are quickly passing through the historical moment when people work in front of a single computer, dominated by a small CRT and focused on tasks involving only local information. Networked computers are becoming ubiquitous and are playing increasingly significant roles in our lives and in the basic infrastructures of science, business, and social interaction. For human-computer interaction to advance in the new millennium we need to better understand the emerging dynamic of interaction in which the focus task is no longer confined to the desktop but reaches into a complex networked world of information and computer-mediated interactions. We think the theory of distributed cognition has a special role to play in understanding interactions between people and technologies, for its focus has always been on whole environments: what we really do in them and how we coordinate our activity in them. Distributed cognition provides a radical reorientation of how to think about designing and supporting human-computer interaction. As a theory it is specifically tailored to understanding interactions among people and technologies. In this article we propose distributed cognition as a new foundation for human-computer interaction, sketch an integrated research framework, and use selections from our earlier work to suggest how this framework can provide new opportunities in the design of digital work materials.},
	language = {en},
	number = {2},
	urldate = {2022-02-14},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Hollan, James and Hutchins, Edwin and Kirsh, David},
	month = jun,
	year = {2000},
	pages = {174--196},
}

@article{madsen_no_2018,
	title = {No {Firm} {Is} an {Island}: {The} {Role} of {Population}-{Level} {Actors} in {Organizational} {Learning} from {Failure}},
	volume = {29},
	issn = {1047-7039, 1526-5455},
	shorttitle = {No {Firm} {Is} an {Island}},
	url = {http://pubsonline.informs.org/doi/10.1287/orsc.2017.1199},
	doi = {10.1287/orsc.2017.1199},
	abstract = {When a serious failure occurs within a population of organizations, members of individual organizations in the population attempt to learn vicariously from the event so that future failures may be avoided. This organization-level vicarious learning process has been extensively studied in the organizational learning literature. However, following a serious failure in one organization, a parallel process also plays out at the population level as population-level actors draw lessons from the failure and exert inﬂuence over organizations in the population in the interest of preventing future failures. Such population-level processes may exert powerful inﬂuences on organization-level learning, but have only begun to be explored in the literature. This paper begins to ﬁll this gap by theorizing and studying the role of population-level actors in organizational learning from failure within and across organizational populations. It examines these issues in a global sample of large airlines operating between 1981 and 2011. The ﬁndings indicate that population-level forces are a major driver of improvement and learning in members of organizational populations—speciﬁcally, that the monitoring strength and activity of population-level actors inﬂuence the rates of organizational learning from failure within their populations.},
	language = {en},
	number = {4},
	urldate = {2022-02-14},
	journal = {Organization Science},
	author = {Madsen, Peter M. and Desai, Vinit},
	month = aug,
	year = {2018},
	pages = {739--753},
}

@article{clinch_learning_2018,
	title = {Learning from {Our} {Mistakes}: {Identifying} {Opportunities} for {Technology} {Intervention} against {Everyday} {Cognitive} {Failure}},
	volume = {17},
	issn = {1536-1268},
	shorttitle = {Learning from {Our} {Mistakes}},
	url = {https://ieeexplore.ieee.org/document/8383664/},
	doi = {10.1109/MPRV.2018.022511240},
	language = {en},
	number = {2},
	urldate = {2022-02-14},
	journal = {IEEE Pervasive Computing},
	author = {Clinch, Sarah and Mascolo, Cecilia},
	month = apr,
	year = {2018},
	pages = {22--33},
}

@inproceedings{borge_patterns_2012,
	address = {Seattle, Washington, USA},
	title = {Patterns of team processes and breakdowns in information analysis tasks},
	isbn = {978-1-4503-1086-4},
	url = {http://dl.acm.org/citation.cfm?doid=2145204.2145369},
	doi = {10.1145/2145204.2145369},
	abstract = {In this paper we present findings from a laboratory study of teams of three, collaborating to complete a complex information sharing, synthesis, decision-making task. We use interaction analysis, communication analysis, and task analysis methods to identify the primary activities teams engaged in as they solved a complex information dependant decision-making task. These activities serve as the foundation to present findings related to common team problems and patterns of interaction associated with team performance. We found differences between high and low performing teams related to verbal equity and how they shared and synthesized information.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the {ACM} 2012 conference on {Computer} {Supported} {Cooperative} {Work} - {CSCW} '12},
	publisher = {ACM Press},
	author = {Borge, Marcela and Ganoe, Craig H. and Shih, Shin-I and Carroll, John M.},
	year = {2012},
	pages = {1105},
}

@inproceedings{bohme_directed_2017,
	address = {New York, NY, USA},
	series = {{CCS} '17},
	title = {Directed {Greybox} {Fuzzing}},
	isbn = {978-1-4503-4946-8},
	url = {http://doi.org/10.1145/3133956.3134020},
	doi = {10.1145/3133956.3134020},
	abstract = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into Google's continuous fuzzing platform OSS-Fuzz. Due to its directedness, AFLGo could find 39 bugs in several well-fuzzed, security-critical projects like LibXML2. 17 CVEs were assigned.},
	urldate = {2022-02-13},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Böhme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
	month = oct,
	year = {2017},
	keywords = {coverage-based greybox fuzzing, crash reproduction, directed testing, patch testing, reachability, verifying true positives},
	pages = {2329--2344},
}

@inproceedings{liu_large-scale_2020,
	title = {A {Large}-{Scale} {Empirical} {Study} on {Vulnerability} {Distribution} within {Projects} and the {Lessons} {Learned}},
	abstract = {The number of vulnerabilities increases rapidly in recent years, due to advances in vulnerability discovery solutions. It enables a thorough analysis on the vulnerability distribution and provides support for correlation analysis and prediction of vulnerabilities. Previous research either focuses on analyzing bugs rather than vulnerabilities, or only studies general vulnerability distribution among projects rather than the distribution within each project. In this paper, we collected a large vulnerability dataset, consisting of all known vulnerabilities associated with five representative open source projects, by utilizing automated crawlers and spending months of manual efforts. We then analyzed the vulnerability distribution within each project over four dimensions, including files, functions, vulnerability types and responsible developers. Based on the results analysis, we presented 12 practical insights on the distribution of vulnerabilities. Finally, we applied such insights on several vulnerability discovery solutions (including static analysis and dynamic fuzzing), and helped them find 10 zero-day vulnerabilities in target projects, showing that our insights are useful.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Liu, Bingchang and Meng, Guozhu and Zou, Wei and Gong, Qi and Li, Feng and Lin, Min and Sun, Dandan and Huo, Wei and Zhang, Chao},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	keywords = {Crawlers, Empirical Study, Fuzzing, Manuals, Security, Semantics, Software engineering, Static analysis, Vulnerability Distribution},
	pages = {1547--1559},
}

@article{perry_empirical_nodate,
	title = {An {Empirical} {Study} of {Software} {Interface} {Faults} — {An} {Update}},
	abstract = {In [Perry 85] we reported work in progress on the analysis of software interface faults in a large software system. In this update of that work, we complete the analysis of the interface faults for this large system and extend our results. In our original analysis, we determined that there were 15 categories of interface faults in the prima facie interface fault set of errors reports. We add one new category as a result of our completed analysis: hardware/software interface problems. We summarize our original ﬁndings for the set of errors determined by our operational deﬁnition of interface faults, present our new ﬁndings for the single-ﬁle interface faults, and then present the characterization of interface faults with respect to the total error population.},
	language = {en},
	author = {Perry, Dewayne E and Evangelist, W Michael},
	pages = {22},
}

@inproceedings{ye_fuzzer_2017,
	title = {A {Fuzzer} {Based} on a {Fine}-{Grained} {Deeper} {Strategy}},
	doi = {10.1109/ICISCE.2017.15},
	abstract = {Software nowadays suffers much danger from vulnerabilities, threatening much valuable thing, e.g. the security of private information. Fuzzing is a successful tool in bug-detecting without resorting to much prior knowledge, and has actually discovered many bugs. However, traditional fuzzing has a common drawback that it is limited in a superficial level, and nearly cannot drill deep into the program. This paper proposes DO-Fuzzer (Depth-Oriented-Fuzzer) based on an evolutionary skeleton, employing a fine-grained deep strategy to guide the fuzzer deep into the program. Experiment shows that the deep strategy can augment the fuzzer to detect program-path and consequently program-bugs.},
	booktitle = {2017 4th {International} {Conference} on {Information} {Science} and {Control} {Engineering} ({ICISCE})},
	author = {Ye, Jiaxi and Feng, Chao and Tang, Chaojing},
	month = jul,
	year = {2017},
	keywords = {Computer bugs, Deep strategy, Evolution, Fuzz, Markov processes, Security, Skeleton, Tools},
	pages = {24--28},
}

@article{kampenes_systematic_2007,
	title = {A systematic review of effect size in software engineering experiments},
	volume = {49},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584907000195},
	doi = {10.1016/j.infsof.2007.02.015},
	language = {en},
	number = {11-12},
	urldate = {2022-02-10},
	journal = {Information and Software Technology},
	author = {Kampenes, Vigdis By and Dybå, Tore and Hannay, Jo E. and Sjøberg, Dag I.K.},
	month = nov,
	year = {2007},
	pages = {1073--1086},
}

@inproceedings{yskout_security_2015,
	address = {Florence, Italy},
	title = {Do {Security} {Patterns} {Really} {Help} {Designers}?},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194582/},
	doi = {10.1109/ICSE.2015.49},
	abstract = {Security patterns are well-known solutions to security-speciﬁc problems. They are often claimed to beneﬁt designers without much security expertise. We have performed an empirical study to investigate whether the usage of security patterns by such an audience leads to a more secure design, or to an increased productivity of the designers. Our study involved 32 teams of master students enrolled in a course on software architecture, working on the design of a realisticallysized banking system. Irrespective of whether the teams were using security patterns, we have not been able to detect a difference between the two treatment groups. However, the teams prefer to work with the support of security patterns.},
	language = {en},
	urldate = {2022-02-09},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Yskout, Koen and Scandariato, Riccardo and Joosen, Wouter},
	month = may,
	year = {2015},
	pages = {292--302},
}

@article{prechelt_two_2002,
	title = {Two controlled experiments assessing the usefulness of design pattern documentation in program maintenance},
	volume = {28},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/1010061/},
	doi = {10.1109/TSE.2002.1010061},
	abstract = {ÐUsing design patterns is claimed to improve programmer productivity and software quality. Such improvements may manifest both at construction time (in faster and better program design) and at maintenance time (in faster and more accurate program comprehension). This paper focuses on the maintenance context and reports on experimental tests of the following question: Does it help the maintainer if the design patterns in the program code are documented explicitly (using source code comments) compared to a well-commented program without explicit reference to design patterns? Subjects performed maintenance tasks on two programs ranging from 360 to 560 LOC including comments. Both programs contained design patterns. The controlled variable was whether the use of design patterns was documented explicitly or not. The experiments thus tested whether pattern comment lines (PCL) help during maintenance if patterns are relevant and sufficient program comments are already present. It turns out that this question is a challenge for the experimental methodology: A setup leading to relevant results is quite difficult to find. We discuss these issues in detail and suggest a general approach to such situations. The experiment was performed with Java by 74 German graduate students and then repeated with C++ by 22 American undergraduate students. A conservative analysis of the results supports the hypothesis that pattern-relevant maintenance tasks were completed faster or with fewer errors if redundant design pattern information was provided. Redundant means that the information carried in pattern comments is also available in different form in other comments. The contribution of this article is twofold: It provides the first controlled experiment results on design pattern usage and it presents a solution approach to an important class of experiment design problems for experiments regarding documentation.},
	language = {en},
	number = {6},
	urldate = {2022-02-09},
	journal = {IEEE Transactions on Software Engineering},
	author = {Prechelt, L. and Unger-Lamprecht, B. and Philippsen, M. and Tichy, W.F.},
	month = jun,
	year = {2002},
	pages = {595--606},
}

@article{prechelt_controlled_2001,
	title = {A controlled experiment in maintenance: comparing design patterns to simpler solutions},
	volume = {27},
	issn = {00985589},
	shorttitle = {A controlled experiment in maintenance},
	url = {http://ieeexplore.ieee.org/document/988711/},
	doi = {10.1109/32.988711},
	abstract = {ÐSoftware design patterns package proven solutions to recurring design problems in a form that simplifies reuse. We are seeking empirical evidence whether using design patterns is beneficial. In particular, one may prefer using a design pattern even if the actual design problem is simpler than that solved by the pattern, i.e., if not all of the functionality offered by the pattern is actually required. Our experiment investigates software maintenance scenarios that employ various design patterns and compares designs with patterns to simpler alternatives. The subjects were professional software engineers. In most of our nine maintenance tasks, we found positive effects from using a design pattern: Either its inherent additional flexibility was achieved without requiring more maintenance time or maintenance time was reduced compared to the simpler alternative. In a few cases, we found negative effects: The alternative solution was less error-prone or required less maintenance time. Although most of these effects were expected, a few were surprising: A negative effect occurs although a certain application of the Observer pattern appears to be well justified and a positive effect occurs despite superfluous flexibility (and, hence, complexity) introduced by a certain application of the Decorator pattern. Overall, we conclude that, unless there is a clear reason to prefer the simpler solution, it is probably wise to choose the flexibility provided by the design pattern because unexpected new requirements often appear. We identify several questions for future empirical research.},
	language = {en},
	number = {12},
	urldate = {2022-02-09},
	journal = {IEEE Transactions on Software Engineering},
	author = {Prechelt, L. and Unger, B. and Tichy, W.F. and Brossler, P. and Votta, L.G.},
	month = dec,
	year = {2001},
	pages = {1134--1144},
}

@article{yuan_simple_nodate,
	title = {Simple {Testing} {Can} {Prevent} {Most} {Critical} {Failures}},
	abstract = {Large, production quality distributed systems still fail periodically, and do so sometimes catastrophically, where most or all users experience an outage or data loss. We present the result of a comprehensive study investigating 198 randomly selected, user-reported failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults eventually evolve into a user-visible failure. We found that from a testing point of view, almost all failures require only 3 or fewer nodes to reproduce, which is good news considering that these services typically run on a very large number of nodes. However, multiple inputs are needed to trigger the failures with the order between them being important. Finally, we found the error logs of these systems typically contain sufﬁcient data on both the errors and the input events that triggered the failure, enabling the diagnose and the reproduction of the production failures.},
	language = {en},
	author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U and Stumm, Michael},
	pages = {18},
}

@article{medeiros_vulnerable_2020,
	title = {Vulnerable {Code} {Detection} {Using} {Software} {Metrics} and {Machine} {Learning}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3041181},
	abstract = {Software metrics are widely-used indicators of software quality and several studies have shown that such metrics can be used to estimate the presence of vulnerabilities in the code. In this paper, we present a comprehensive experiment to study how effective software metrics can be to distinguish the vulnerable code units from the non-vulnerable ones. To this end, we use several machine learning algorithms (Random Forest, Extreme Boosting, Decision Tree, SVM Linear, and SVM Radial) to extract vulnerability-related knowledge from software metrics collected from the source code of several representative software projects developed in C/C++ (Mozilla Firefox, Linux Kernel, Apache HTTPd, Xen, and Glibc). We consider different combinations of software metrics and diverse application scenarios with different security concerns (e.g., highly critical or non-critical systems). This experiment contributes to understanding whether software metrics can effectively be used to distinguish vulnerable code units in different application scenarios, and how can machine learning algorithms help in this regard. The main observation is that using machine learning algorithms on top of software metrics helps to indicate vulnerable code units with a relatively high level of confidence for security-critical software systems (where the focus is on detecting the maximum number of vulnerabilities, even if false positives are reported), but they are not helpful for low-critical or non-critical systems due to the high number of false positives (that bring an additional development cost frequently not affordable).},
	journal = {IEEE Access},
	author = {Medeiros, Nádia and Ivaki, Naghmeh and Costa, Pedro and Vieira, Marco},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Application scenarios, Machine learning algorithms, Measurement, Security, Software, Software metrics, Support vector machines, Tools, machine learning, security vulnerabilities, software metrics, software security},
	pages = {219174--219198},
}

@inproceedings{alves_software_2016,
	title = {Software {Metrics} and {Security} {Vulnerabilities}: {Dataset} and {Exploratory} {Study}},
	shorttitle = {Software {Metrics} and {Security} {Vulnerabilities}},
	doi = {10.1109/EDCC.2016.34},
	abstract = {Code with certain characteristics is more prone to have security vulnerabilities. In fact, studies show that code not following best practices is harder to verify and maintain, and consequently is more probable to have vulnerabilities left unnoticed or inadvertently introduced. In this experience report, we study whether software metrics can reflect such characteristics, thus having some correlation with the existence of vulnerabilities. The analysis is based on 2875 security patches, used to build a dataset with metrics and vulnerabilities for all the functions, classes and files of 5750 versions of five widely used projects that are exposed to attacks: Linux Kernel, Mozilla, Xen Hypervisor, httpd and glibc. We calculated software metrics from their sources and used correlation algorithm and statistical tests on these metrics in order to identify relations between them and the existing vulnerabilities. Results show that software metrics are able to discriminate vulnerable and non vulnerable functions, but it is not possible to find strong correlations between these metrics and the number of vulnerabilities existing in the analyzed functions. Finally, the results indicate that vulnerable functions are probable to have other vulnerabilities in the future.},
	booktitle = {2016 12th {European} {Dependable} {Computing} {Conference} ({EDCC})},
	author = {Alves, Henrique and Fonseca, Baldoino and Antunes, Nuno},
	month = sep,
	year = {2016},
	keywords = {Computer bugs, Correlation, Kernel, Security, Software Metrics, Software Security, Software metrics, Vulnerabilities},
	pages = {37--44},
}

@inproceedings{medeiros_software_2017,
	title = {Software {Metrics} as {Indicators} of {Security} {Vulnerabilities}},
	doi = {10.1109/ISSRE.2017.11},
	abstract = {Detecting software security vulnerabilities and distinguishing vulnerable from non-vulnerable code is anything but simple. Most of the time, vulnerabilities remain undisclosed until they are exposed, for instance, by an attack during the software operational phase. Software metrics are widely-used indicators of software quality, but the question is whether they can be used to distinguish vulnerable software units from the non-vulnerable ones during development. In this paper, we perform an exploratory study on software metrics, their interdependency, and their relation with security vulnerabilities. We aim at understanding: i) the correlation between software architectural characteristics, represented in the form of software metrics, and the number of vulnerabilities; and ii) which are the most informative and discriminative metrics that allow identifying vulnerable units of code. To achieve these goals, we use, respectively, correlation coefficients and heuristic search techniques. Our analysis is carried out on a dataset that includes software metrics and reported security vulnerabilities, exposed by security attacks, for all functions, classes, and files of five widely used projects. Results show: i) a strong correlation between several project-level metrics and the number of vulnerabilities, ii) the possibility of using a group of metrics, at both file and function levels, to distinguish vulnerable and non-vulnerable code with a high level of accuracy.},
	booktitle = {2017 {IEEE} 28th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Medeiros, Nádia and Ivaki, Naghmeh and Costa, Pedro and Vieira, Marco},
	month = oct,
	year = {2017},
	note = {ISSN: 2332-6549},
	keywords = {Complexity theory, Correlation, Feature Selection, Security, Security Vulnerabilities, Software Metrics, Software metrics, Software systems},
	pages = {216--227},
}

@inproceedings{medeiros_software_2017-1,
	title = {Software {Metrics} as {Indicators} of {Security} {Vulnerabilities}},
	doi = {10.1109/ISSRE.2017.11},
	abstract = {Detecting software security vulnerabilities and distinguishing vulnerable from non-vulnerable code is anything but simple. Most of the time, vulnerabilities remain undisclosed until they are exposed, for instance, by an attack during the software operational phase. Software metrics are widely-used indicators of software quality, but the question is whether they can be used to distinguish vulnerable software units from the non-vulnerable ones during development. In this paper, we perform an exploratory study on software metrics, their interdependency, and their relation with security vulnerabilities. We aim at understanding: i) the correlation between software architectural characteristics, represented in the form of software metrics, and the number of vulnerabilities; and ii) which are the most informative and discriminative metrics that allow identifying vulnerable units of code. To achieve these goals, we use, respectively, correlation coefficients and heuristic search techniques. Our analysis is carried out on a dataset that includes software metrics and reported security vulnerabilities, exposed by security attacks, for all functions, classes, and files of five widely used projects. Results show: i) a strong correlation between several project-level metrics and the number of vulnerabilities, ii) the possibility of using a group of metrics, at both file and function levels, to distinguish vulnerable and non-vulnerable code with a high level of accuracy.},
	booktitle = {2017 {IEEE} 28th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Medeiros, Nádia and Ivaki, Naghmeh and Costa, Pedro and Vieira, Marco},
	month = oct,
	year = {2017},
	note = {ISSN: 2332-6549},
	keywords = {Complexity theory, Correlation, Feature Selection, Security, Security Vulnerabilities, Software Metrics, Software metrics, Software systems},
	pages = {216--227},
}

@article{laptev_optimizing_nodate,
	title = {Optimizing {Regular} {Expression} {Clustering} for {Massive} {Pattern} {Search}},
	abstract = {Optimizing the search for a large sets of patterns, due to their prevalence, is becoming critical in a variety of applications including ﬁnancial services, healthcare monitoring, RFIDbased inventory management, publish subscribe and network intrusion detection systems (NIDS). For example in NIDS, to identify particular packets in a packet stream, modern network devices need to perform deep packet inspection at high rates given a limited memory and a large number of signatures. It is often proposed to group together regular expressions (REs) denoting similar patterns into a DFA or an NFA to decrease the memory usage and increase the average throughput. In this paper, we propose a framework comprised of three novel distance metrics used to cluster REs and an execution technique that uses the information aggregated by the distance metrics to optimize the number of memory reads during execution, thereby increasing the overall throughput. Our distance metrics are unique in that each metric is specialized for a particular type of patterns. The ﬁrst two metrics optimize for compactness of REs and specialize in regular expressions that have long, contiguous similarity segments. Our third metric optimizes for redundancy and specializes in REs that have random segments of similar. We show how compression, prefetching and memory access optimizations are made possible when the notion of redundancy is leveraged. We validate our approach by implementing it in a modern NIDS Snort, which matches potentially thousands of patterns against an incoming network stream. We observe a ﬁvefold speed improvement over the unmodiﬁed version of Snort.},
	language = {en},
	author = {Laptev, Nikolay and Mousavi, Hamid and Shkapsky, Alexander and Zaniolo, Carlo},
	pages = {13},
}

@article{wang_school_2009,
	title = {School {Bullying} {Among} {Adolescents} in the {United} {States}: {Physical}, {Verbal}, {Relational}, and {Cyber}},
	volume = {45},
	issn = {1054-139X},
	url = {https://www.sciencedirect.com/science/article/pii/S1054139X09001384},
	doi = {https://doi.org/10.1016/j.jadohealth.2009.03.021},
	abstract = {Purpose Four forms of school bullying behaviors among US adolescents and their association with sociodemographic characteristics, parental support, and friends were examined. Methods Data were obtained from the Health Behavior in School-Aged Children (HBSC) 2005 Survey, a nationally representative sample of grades 6–10 (N = 7,182). The revised Olweus Bully/Victim Questionnaire was used to measure physical, verbal, and relational forms of bullying. Two items were added using the same format to measure cyber bullying. For each form, four categories were created: bully, victim, bully-victim, and not involved. Multinomial logistic regressions were applied, with sociodemographic variables, parental support, and number of friends as predictors. Results Prevalence rates of having bullied others or having been bullied at school for at least once in the last 2 months were 20.8\% physically, 53.6\% verbally, 51.4\% socially, or 13.6\% electronically. Boys were more involved in physical or verbal bullying, whereas girls were more involved in relational bullying. Boys were more likely to be cyber bullies, whereas girls were more likely to be cyber victims. African-American adolescents were involved in more bullying (physical, verbal, or cyber) but less victimization (verbal or relational). Higher parental support was associated with less involvement across all forms and classifications of bullying. Having more friends was associated with more bullying and less victimization for physical, verbal, and relational forms but was not associated with cyber bullying. Conclusions Parental support may protect adolescents from all four forms of bullying. Friends associate differentially with traditional and cyber bullying. Results indicate that cyber bullying is a distinct nature from that of traditional bullying.},
	number = {4},
	journal = {Journal of Adolescent Health},
	author = {Wang, Jing and Iannotti, Ronald J. and Nansel, Tonja R.},
	year = {2009},
	keywords = {Cyber bullying, Parental support, Peers, Relational bullying, School bullying, Sociodemographic characteristics},
	pages = {368--375},
}

@article{dalcher_stories_2004,
	title = {Stories and {Histories}: {Case} {Study} {Research} (and {Beyond}) in {Information} {Systems} {Failures}},
	abstract = {Information systems development failures are prevalent in many domains and countries. The aim of this chapter is to explore some of the issues related to the study of such phenomena. Failure situations are not set-up in advance as the subject of studies. Analysing causes and relationships retrospectively depends on the ability to obtain rich and subjective contextual information that can be utilised for shedding a light on the circumstances that precipitate failures. This chapter makes the case for the use of case history and ante-narrative methods for understanding such rich and complex scenarios.},
	language = {en},
	author = {Dalcher, Darren},
	year = {2004},
	pages = {18},
}

@inproceedings{claessen_quickcheck_2000,
	address = {New York, NY, USA},
	series = {{ICFP} '00},
	title = {{QuickCheck}: a lightweight tool for random testing of {Haskell} programs},
	isbn = {978-1-58113-202-1},
	shorttitle = {{QuickCheck}},
	url = {http://doi.org/10.1145/351240.351266},
	doi = {10.1145/351240.351266},
	abstract = {Quick Check is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are described as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffices to obtain good coverage of the definition under test.},
	urldate = {2022-02-02},
	booktitle = {Proceedings of the fifth {ACM} {SIGPLAN} international conference on {Functional} programming},
	publisher = {Association for Computing Machinery},
	author = {Claessen, Koen and Hughes, John},
	month = sep,
	year = {2000},
	pages = {268--279},
}

@article{klees_evaluating_2018,
	title = {Evaluating {Fuzz} {Testing}},
	url = {http://arxiv.org/abs/1808.09700},
	abstract = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
	language = {en},
	urldate = {2022-02-03},
	journal = {arXiv:1808.09700 [cs]},
	author = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
	month = oct,
	year = {2018},
	note = {arXiv: 1808.09700},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{guler_antifuzz_2019,
	title = {\{{AntiFuzz}\}: {Impeding} {Fuzzing} {Audits} of {Binary} {Executables}},
	isbn = {978-1-939133-06-9},
	shorttitle = {\{{AntiFuzz}\}},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/guler},
	language = {en},
	urldate = {2022-02-03},
	author = {Güler, Emre and Aschermann, Cornelius and Abbasi, Ali and Holz, Thorsten},
	year = {2019},
	pages = {1931--1947},
}

@inproceedings{jung_fuzzification_2019,
	title = {Fuzzification: \{{Anti}-{Fuzzing}\} {Techniques}},
	isbn = {978-1-939133-06-9},
	shorttitle = {Fuzzification},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/jung#},
	language = {en},
	urldate = {2022-02-03},
	author = {Jung, Jinho and Hu, Hong and Solodukhin, David and Pagan, Daniel and Lee, Kyu Hyung and Kim, Taesoo},
	year = {2019},
	pages = {1913--1930},
}

@book{allen_sage_2017,
	address = {2455 Teller Road, Thousand Oaks California 91320},
	title = {The {SAGE} {Encyclopedia} of {Communication} {Research} {Methods}},
	isbn = {978-1-4833-8143-5 978-1-4833-8141-1},
	url = {https://sk.sagepub.com/reference/the-sage-encyclopedia-of-communication-research-methods/},
	language = {en},
	urldate = {2022-02-03},
	publisher = {SAGE Publications, Inc},
	author = {Allen, Mike},
	year = {2017},
	doi = {10.4135/9781483381411},
}

@inproceedings{goodman_deepstate_2018,
	address = {San Diego, CA},
	title = {{DeepState}: {Symbolic} {Unit} {Testing} for {C} and {C}++},
	isbn = {978-1-891562-50-1},
	shorttitle = {{DeepState}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/07/bar2018_9_Goodman_paper.pdf},
	doi = {10.14722/bar.2018.23009},
	abstract = {Unit testing is a popular software development methodology that can help developers detect functional regressions, explore boundary conditions, and document expected behavior. However, writing comprehensive unit tests is challenging and time-consuming, and developers seldom explore the obscure (and bug-hiding) corners of software behavior without assistance. DeepState is a tool that provides a Google Test-like API to give C and C++ developers push-button access to symbolic execution engines, such as Manticore and angr, and fuzzers, such as Dr. Fuzz. Rather than learning multiple complex tools, users learn one interface for deﬁning a test harness, and can use various methods to automatically generate tests for software. In addition to providing a familiar interface to binary analysis and fuzzing for parameterized unit testing, DeepState also provides constructs that aid in the construction of API-sequence tests, where the tool chooses the functions or methods to call, allowing for even more diverse and powerful tests. By serving as a front-end to multiple tools, DeepState additionally provides a way to apply (novel) high-level strategies to test generation, and to compare effectiveness and efﬁciency of testing back-ends, including binary analysis tools.},
	language = {en},
	urldate = {2022-02-03},
	booktitle = {Proceedings 2018 {Workshop} on {Binary} {Analysis} {Research}},
	publisher = {Internet Society},
	author = {Goodman, Peter and Groce, Alex},
	year = {2018},
}

@article{gorwa_algorithmic_2020,
	title = {Algorithmic content moderation: {Technical} and political challenges in the automation of platform governance},
	volume = {7},
	issn = {2053-9517},
	shorttitle = {Algorithmic content moderation},
	url = {https://doi.org/10.1177/2053951719897945},
	doi = {10.1177/2053951719897945},
	abstract = {As government pressure on major technology companies builds, both firms and legislators are searching for technical solutions to difficult platform governance puzzles such as hate speech and misinformation. Automated hash-matching and predictive machine learning tools – what we define here as algorithmic moderation systems – are increasingly being deployed to conduct content moderation at scale by major platforms for user-generated content such as Facebook, YouTube and Twitter. This article provides an accessible technical primer on how algorithmic moderation works; examines some of the existing automated tools used by major platforms to handle copyright infringement, terrorism and toxic speech; and identifies key political and ethical issues for these systems as the reliance on them grows. Recent events suggest that algorithmic moderation has become necessary to manage growing public expectations for increased platform responsibility, safety and security on the global stage; however, as we demonstrate, these systems remain opaque, unaccountable and poorly understood. Despite the potential promise of algorithms or ‘AI’, we show that even ‘well optimized’ moderation systems could exacerbate, rather than relieve, many existing problems with content policy as enacted by platforms for three main reasons: automated moderation threatens to (a) further increase opacity, making a famously non-transparent set of practices even more difficult to understand or audit, (b) further complicate outstanding issues of fairness and justice in large-scale sociotechnical systems and (c) re-obscure the fundamentally political nature of speech decisions being executed at scale.},
	language = {en},
	number = {1},
	urldate = {2022-01-14},
	journal = {Big Data \& Society},
	author = {Gorwa, Robert and Binns, Reuben and Katzenbach, Christian},
	month = jan,
	year = {2020},
	keywords = {Platform governance, algorithms, artificial intelligence, content moderation, copyright, toxic speech},
	pages = {2053951719897945},
}

@inproceedings{long_experience_2008,
	address = {Leipzig, Germany},
	title = {Experience applying the {SPIN} model checker to an industrial telecommunications system},
	isbn = {978-1-60558-079-1},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368187},
	doi = {10.1145/1368088.1368187},
	abstract = {Model checking has for years been advertised as a way of ensuring the correctness of complex software systems. However, there exist surprisingly few critical studies of the application of model checking to industrial-scale software systems by people other than the model checker’s own authors. In this paper we report our experience in applying the Spin model checker to the validation of the failover protocols of a commercial telecommunications system. While we conclude that model checking is not yet ready for such applications, we ﬁnd that current research in the model checking community is working to address the diﬃculties we encountered.},
	language = {en},
	urldate = {2022-02-02},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering  - {ICSE} '08},
	publisher = {ACM Press},
	author = {Long, Barry and Dingel, Juergen and Graham, T.C. Nicholas},
	year = {2008},
	pages = {693},
}

@inproceedings{chen_savior_2020,
	title = {{SAVIOR}: {Towards} {Bug}-{Driven} {Hybrid} {Testing}},
	shorttitle = {{SAVIOR}},
	doi = {10.1109/SP40000.2020.00002},
	abstract = {Hybrid testing combines fuzz testing and concolic execution. It leverages fuzz testing to test easy-to-reach code regions and uses concolic execution to explore code blocks guarded by complex branch conditions. As a result, hybrid testing is able to reach deeper into program state space than fuzz testing or concolic execution alone. Recently, hybrid testing has seen significant advancement. However, its code coverage-centric design is inefficient in vulnerability detection. First, it blindly selects seeds for concolic execution and aims to explore new code continuously. However, as statistics show, a large portion of the explored code is often bug-free. Therefore, giving equal attention to every part of the code during hybrid testing is a non-optimal strategy. It slows down the detection of real vulnerabilities by over 43\%. Second, classic hybrid testing quickly moves on after reaching a chunk of code, rather than examining the hidden defects inside. It may frequently miss subtle vulnerabilities despite that it has already explored the vulnerable code paths.We propose SAVIOR, a new hybrid testing framework pioneering a bug-driven principle. Unlike the existing hybrid testing tools, SAVIOR prioritizes the concolic execution of the seeds that are likely to uncover more vulnerabilities. Moreover, SAVIOR verifies all vulnerable program locations along the executing program path. By modeling faulty situations using SMT constraints, SAVIOR reasons the feasibility of vulnerabilities and generates concrete test cases as proofs. Our evaluation shows that the bug-driven approach outperforms mainstream automated testing techniques, including state-of-the-art hybrid testing systems driven by code coverage. On average, SAVIOR detects vulnerabilities 43.4\% faster than DRILLER and 44.3\% faster than QSYM, leading to the discovery of 88 and 76 more unique bugs, respectively. According to the evaluation on 11 well fuzzed benchmark programs, within the first 24 hours, SAVIOR triggers 481 UBSAN violations, among which 243 are real bugs.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Chen, Yaohui and Li, Peng and Xu, Jun and Guo, Shengjian and Zhou, Rundong and Zhang, Yulong and Wei, Tao and Lu, Long},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Benchmark testing, Computer bugs, Fuzzing, Security, Software, Tools},
	pages = {1580--1596},
}

@inproceedings{chen_savior_2020-1,
	title = {{SAVIOR}: {Towards} {Bug}-{Driven} {Hybrid} {Testing}},
	shorttitle = {{SAVIOR}},
	doi = {10.1109/SP40000.2020.00002},
	abstract = {Hybrid testing combines fuzz testing and concolic execution. It leverages fuzz testing to test easy-to-reach code regions and uses concolic execution to explore code blocks guarded by complex branch conditions. As a result, hybrid testing is able to reach deeper into program state space than fuzz testing or concolic execution alone. Recently, hybrid testing has seen significant advancement. However, its code coverage-centric design is inefficient in vulnerability detection. First, it blindly selects seeds for concolic execution and aims to explore new code continuously. However, as statistics show, a large portion of the explored code is often bug-free. Therefore, giving equal attention to every part of the code during hybrid testing is a non-optimal strategy. It slows down the detection of real vulnerabilities by over 43\%. Second, classic hybrid testing quickly moves on after reaching a chunk of code, rather than examining the hidden defects inside. It may frequently miss subtle vulnerabilities despite that it has already explored the vulnerable code paths.We propose SAVIOR, a new hybrid testing framework pioneering a bug-driven principle. Unlike the existing hybrid testing tools, SAVIOR prioritizes the concolic execution of the seeds that are likely to uncover more vulnerabilities. Moreover, SAVIOR verifies all vulnerable program locations along the executing program path. By modeling faulty situations using SMT constraints, SAVIOR reasons the feasibility of vulnerabilities and generates concrete test cases as proofs. Our evaluation shows that the bug-driven approach outperforms mainstream automated testing techniques, including state-of-the-art hybrid testing systems driven by code coverage. On average, SAVIOR detects vulnerabilities 43.4\% faster than DRILLER and 44.3\% faster than QSYM, leading to the discovery of 88 and 76 more unique bugs, respectively. According to the evaluation on 11 well fuzzed benchmark programs, within the first 24 hours, SAVIOR triggers 481 UBSAN violations, among which 243 are real bugs.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Chen, Yaohui and Li, Peng and Xu, Jun and Guo, Shengjian and Zhou, Rundong and Zhang, Yulong and Wei, Tao and Lu, Long},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Benchmark testing, Computer bugs, Fuzzing, Security, Software, Tools},
	pages = {1580--1596},
}

@inproceedings{poncin_process_2011,
	title = {Process {Mining} {Software} {Repositories}},
	doi = {10.1109/CSMR.2011.5},
	abstract = {Software developers’ activities are in general recorded in software repositories such as version control systems, bug trackers and mail archives. While abundant information is usually present in such repositories, successful information extraction is often challenged by the necessity to simultaneously analyze different repositories and to combine the information obtained. We propose to apply process mining techniques, originally developed for business process analysis, to address this challenge. However, in order for process mining to become applicable, different software repositories should be combined, and “related” software development events should be matched: e.g., mails sent about a file, modifications of the file and bug reports that can be traced back to it. The combination and matching of events has been implemented in FRASR (Framework for Analyzing Software Repositories), augmenting the process mining framework ProM. FRASR has been successfully applied in a series of case studies addressing such aspects of the development process as roles of different developers and the way bug reports are handled.},
	booktitle = {2011 15th {European} {Conference} on {Software} {Maintenance} and {Reengineering}},
	author = {Poncin, Wouter and Serebrenik, Alexander and Brand, Mark van den},
	month = mar,
	year = {2011},
	note = {ISSN: 1534-5351},
	keywords = {Computer bugs, Control systems, Data mining, PROM, Postal services, Process control, Software, process mining, software repositories},
	pages = {5--14},
}

@inproceedings{latoza_maintaining_2006,
	address = {Shanghai China},
	title = {Maintaining mental models: a study of developer work habits},
	isbn = {978-1-59593-375-1},
	shorttitle = {Maintaining mental models},
	url = {https://dl.acm.org/doi/10.1145/1134285.1134355},
	doi = {10.1145/1134285.1134355},
	abstract = {To understand developers’ typical tools, activities, and practices and their satisfaction with each, we conducted two surveys and eleven interviews. We found that many problems arose because developers were forced to invest great effort recovering implicit knowledge by exploring code and interrupting teammates and this knowledge was only saved in their memory. Contrary to expectations that email and IM prevent expensive task switches caused by face-to-face interruptions, we found that face-to-face communication enjoys many advantages. Contrary to expectations that documentation makes understanding design rationale easy, we found that current design documents are inadequate. Contrary to expectations that code duplication involves the copy and paste of code snippets, developers reported several types of duplication. We use data to characterize these and other problems and draw implications for the design of tools for their solution.},
	language = {en},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 28th international conference on {Software} engineering},
	publisher = {ACM},
	author = {LaToza, Thomas D. and Venolia, Gina and DeLine, Robert},
	month = may,
	year = {2006},
	pages = {492--501},
}

@article{dalcher2002forensic,
	title = {Forensic software engineering and stories of failures},
	journal = {Investigation and Reporting of Incidents and Accidents (IRIA 2002)},
	author = {Dalcher, Darren},
	year = {2002},
}

@inproceedings{sjoberg_conducting_2002,
	address = {Nara, Japan},
	title = {Conducting realistic experiments in software engineering},
	isbn = {9780769517964},
	url = {http://ieeexplore.ieee.org/document/1166921/},
	doi = {10.1109/ISESE.2002.1166921},
	urldate = {2022-01-31},
	booktitle = {Proceedings {International} {Symposium} on {Empirical} {Software} {Engineering}},
	publisher = {IEEE Comput. Soc},
	author = {Sjoberg, D.I.K. and Anda, B. and Arisholm, E. and Dyba, T. and Jorgensen, M. and Karahasanovic, A. and Koren, E.F. and Vokac, M.},
	year = {2002},
	pages = {17--26},
}

@inproceedings{fonseca_describing_2017,
	title = {Describing {What} {Experimental} {Software} {Engineering} {Experts} {Do} {When} {They} {Design} {Their} {Experiments} - {A} {Qualitative} {Study}},
	doi = {10.1109/ESEM.2017.63},
	abstract = {Background: Although there has been a significant amount of research focused on designing and conducting controlled experiments, few studies report how experienced experimental software engineering researchers actually design and conduct their studies. Aims: This study aimed to offer a practical perspective from their viewpoint regarding controlled experiment planning. Method: We collected data through semi-structured interviews from 11 researchers, and we used qualitative analysis methods from the grounded theory approach to analyze them. Result: Although the complete study presents four research questions, in this paper, we answer the first one. As a result, we present a preliminary result about what these experts actually do when they design experiments. Conclusions: This work contributes to a better understanding of the practical performance of experimental software engineering.},
	booktitle = {2017 {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Fonseca, Liliane Sheyla da Silva and Seaman, Carolyn Budinger and Soares, Sergio Castelo Branco},
	month = nov,
	year = {2017},
	keywords = {Data analysis, Encoding, Experiments, Guidelines, Human Subjects, Interviews, Planning, Qualitative Study, Software, Software Engineering, Software engineering},
	pages = {211--216},
}

@article{sjoeberg_survey_2005,
	title = {A survey of controlled experiments in software engineering},
	volume = {31},
	issn = {1939-3520},
	doi = {10.1109/TSE.2005.97},
	abstract = {The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Sjoeberg, D.I.K. and Hannay, J.E. and Hansen, O. and Kampenes, V.B. and Karahasanovic, A. and Liborg, N.-K. and Rekdal, A.C.},
	month = sep,
	year = {2005},
	keywords = {Application software, Computer Society, Computer industry, Conference proceedings, Index Terms- Controlled experiments, Programming, Recruitment, Software engineering, Software maintenance, Software metrics, Software systems, empirical software engineering., research methodology, survey},
	pages = {733--753},
}

@article{ko_practical_2015,
	title = {A practical guide to controlled experiments of software engineering tools with human participants},
	volume = {20},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-013-9279-3},
	doi = {10.1007/s10664-013-9279-3},
	language = {en},
	number = {1},
	urldate = {2022-01-31},
	journal = {Empirical Software Engineering},
	author = {Ko, Andrew J. and LaToza, Thomas D. and Burnett, Margaret M.},
	month = feb,
	year = {2015},
	pages = {110--141},
}

@article{litzinger_engineering_2011,
	title = {Engineering {Education} and the {Development} of {Expertise}},
	volume = {100},
	issn = {10694730},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.2168-9830.2011.tb00006.x},
	doi = {10.1002/j.2168-9830.2011.tb00006.x},
	abstract = {BACKGROUND Although engineering education has evolved in ways that improve the readiness of graduates to meet the challenges of the twenty-first century, national and international organizations continue to call for change. Future changes in engineering education should be guided by research on expertise and the learning processes that support its development.
PURPOSE The goals of this paper are: to relate key findings from studies of the development of expertise to engineering education, to summarize instructional practices that are consistent with these findings, to provide examples of learning experiences that are consistent with these instructional practices, and finally, to identify challenges to implementing such learning experiences in engineering programs. SCOPE/METHOD The research synthesized for this article includes that on the development of expertise, students’ approaches to learning, students’ responses to instructional practices, and the role of motivation in learning. In addition, literature on the dominant teaching and learning practices in engineering education is used to frame some of the challenges to implementing alternative approaches to learning.
CONCLUSION Current understanding of expertise, and the learning processes that develop it, indicates that engineering education should encompass a set of learning experiences that allow students to construct deep conceptual knowledge, to develop the ability to apply key technical and professional skills fluently, and to engage in a number of authentic engineering projects. Engineering curricula and teaching methods are often not well aligned with these goals. Curriculum-level instructional design processes should be used to design and implement changes that will improve alignment.},
	language = {en},
	number = {1},
	urldate = {2022-01-29},
	journal = {Journal of Engineering Education},
	author = {Litzinger, Thomas and Lattuca, Lisa R. and Hadgraft, Roger and Newstetter, Wendy},
	month = jan,
	year = {2011},
	pages = {123--150},
}

@article{vessey_expertise_1985,
	title = {Expertise in debugging computer programs: {A} process analysis},
	volume = {23},
	issn = {00207373},
	shorttitle = {Expertise in debugging computer programs},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020737385800547},
	doi = {10.1016/S0020-7373(85)80054-7},
	language = {en},
	number = {5},
	urldate = {2022-01-29},
	journal = {International Journal of Man-Machine Studies},
	author = {Vessey, Iris},
	month = nov,
	year = {1985},
	pages = {459--494},
}

@book{noauthor_how_2000,
	address = {Washington, D.C.},
	title = {How {People} {Learn}: {Brain}, {Mind}, {Experience}, and {School}: {Expanded} {Edition}},
	isbn = {978-0-309-07036-2},
	shorttitle = {How {People} {Learn}},
	url = {http://www.nap.edu/catalog/9853},
	language = {en},
	urldate = {2022-01-29},
	publisher = {National Academies Press},
	month = aug,
	year = {2000},
	doi = {10.17226/9853},
	note = {Pages: 9853},
}

@article{spohrer_novice_1986,
	title = {Novice mistakes: are the folk wisdoms correct?},
	volume = {29},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Novice mistakes},
	url = {https://dl.acm.org/doi/10.1145/6138.6145},
	doi = {10.1145/6138.6145},
	abstract = {An evaluation of two folk wisdoms serves to elucidate the underlying or "deep-structure" reasons for novice errors.},
	language = {en},
	number = {7},
	urldate = {2022-01-29},
	journal = {Communications of the ACM},
	author = {Spohrer, James C. and Soloway, Elliot},
	month = jul,
	year = {1986},
	pages = {624--632},
}

@article{wheatley_learning_2010,
	title = {Learning from failure [{Manufacturing} {Safety}]},
	volume = {5},
	issn = {1750-9637},
	doi = {10.1049/et.2010.1314},
	abstract = {First, let's briefly review the facts. Since 2002, when Toyota introduced an electronic throttle control system, there have been a spate of what are termed 'unintended acceleration' incidents involving Toyota vehicles. Several investigations by the US National Highway Traffic Safety Administration duly followed, and there were a couple of small recalls. Then, in August 2009, came the harrowing deaths of California Highway Patrol officer Mark Saylor and three members of his family. In a recorded emergency call, the cautious and experienced officer can be heard saying that the accelerator was stuck, and that the brakes weren't working. As he approached an intersection, his last words were: "Hold on and pray" Finally prompted to acknowledge a problem, Toyota subsequently recalled 2.3 million vehicles, blaming an American supplier for faulty workmanship. But the supplier fought back, pointing out that they had only become a supplier in 2005 three years after reports of unintended acceleration first surfaced. In all, one American Firm of safety consultants has Identified 2,262 instances of the problem, leading to 819 crashes and 26 deaths. The world's media previously fond of extolling Toyota's Just-in-Time productions systems and quality-conscious virtues now rounded on the company, which quickly found its reputation in tatters. Public apologies soon followed from Toyota president Akio Toyoda, the grandson of the company's founder, as did still further recalls. Yet at the time of writing, no firm cause has conclusively been identified, and Toyota's reputation remains troubled.},
	number = {13},
	journal = {Engineering Technology},
	author = {Wheatley, Malcolm},
	month = sep,
	year = {2010},
	note = {Conference Name: Engineering Technology},
	pages = {56--58},
}

@incollection{shanks_implementing_2003,
	edition = {1},
	title = {Implementing {Enterprise} {Resource} {Planning} {Systems}: {The} {Role} of {Learning} from {Failure}},
	isbn = {978-0-521-81902-2 978-0-511-81507-2},
	shorttitle = {Implementing {Enterprise} {Resource} {Planning} {Systems}},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511815072A021/type/book_part},
	abstract = {ERP implementations remain problematic despite the fact that many of the issues are by now quite well known. In this paper, we take a different perspective from the critical success factors and risks approaches that are common in the information systems discipline to explain why ERP implementations fail. Speci®cally, we adapt Sitkin's theory of intelligent failure to ERP implementations resulting in a theory that we call ``learning from failure.'' We then examine from the viewpoint of this theory the details of two SAP R/3 implementations, one of which failed while the other succeeded. Although it is impossible to state, unequivocally, that the implementation that failed did so because it did not use the approach that was derived from the theory, the analysis reveals that the company that followed many of the tenets of the theory succeeded while the other did not.},
	language = {en},
	urldate = {2022-01-28},
	booktitle = {Second-{Wave} {Enterprise} {Resource} {Planning} {Systems}},
	publisher = {Cambridge University Press},
	author = {Scott, Judy E. and Vessey, Iris},
	editor = {Shanks, Graeme and Seddon, Peter B. and Willcocks, Leslie P.},
	month = sep,
	year = {2003},
	doi = {10.1017/CBO9780511815072.011},
	pages = {241--274},
}

@article{schrenker_learning_2007,
	title = {Learning from {Failure}: {The} {Teachings} of {Petroski}},
	volume = {41},
	issn = {0899-8205, 1943-5967},
	shorttitle = {Learning from {Failure}},
	url = {http://www.aami-bit.org/doi/abs/10.2345/0899-8205%282007%2941%5B395%3ALFFTTO%5D2.0.CO%3B2},
	doi = {10.2345/0899-8205(2007)41[395:LFFTTO]2.0.CO;2},
	language = {en},
	number = {5},
	urldate = {2022-01-27},
	journal = {Biomedical Instrumentation \& Technology},
	author = {Schrenker, Richard},
	month = sep,
	year = {2007},
	pages = {395--398},
}

@article{fenton_quantitative_2000,
	title = {Quantitative analysis of faults and failures in a complex software system},
	volume = {26},
	issn = {1939-3520},
	doi = {10.1109/32.879815},
	abstract = {The authors describe a number of results from a quantitative study of faults and failures in two releases of a major commercial software system. They tested a range of basic software engineering hypotheses relating to: the Pareto principle of distribution of faults and failures; the use of early fault data to predict later fault and failure data; metrics for fault prediction; and benchmarking fault data. For example, we found strong evidence that a small number of modules contain most of the faults discovered in prerelease testing and that a very small number of modules contain most of the faults discovered in operation. We found no evidence to support previous claims relating module size to fault density nor did we find evidence that popular complexity metrics are good predictors of either fault-prone or failure-prone modules. We confirmed that the number of faults discovered in prerelease testing is an order of magnitude greater than the number discovered in 12 months of operational use. The most important result was strong evidence of a counter-intuitive relationship between pre- and postrelease faults; those modules which are the most fault-prone prerelease are among the least fault-prone postrelease, while conversely, the modules which are most fault-prone postrelease are among the least fault-prone prerelease. This observation has serious ramifications for the commonly used fault density measure. Our results provide data-points in building up an empirical picture of the software development process.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fenton, N.E. and Ohlsson, N.},
	month = aug,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Benchmark testing, Computer industry, Density measurement, Failure analysis, Phase measurement, Programming, Software engineering, Software metrics, Software systems, Software testing},
	pages = {797--814},
}

@inproceedings{9519496,
	title = {Runtime recovery of web applications under zero-day {ReDoS} attacks},
	doi = {10.1109/SP40001.2021.00077},
	booktitle = {2021 {IEEE} symposium on security and privacy ({SP})},
	author = {Bai, Zhihao and Wang, Ke and Zhu, Hang and Cao, Yinzhi and Jin, Xin},
	year = {2021},
	pages = {1575--1588},
}

@article{buschmann_learning_2010,
	title = {Learning from {Failure}, {Part} {III}: {On} {Hammers} and {Nails}, and {Falling} in {Love} with {Technology} and {Design}},
	volume = {27},
	issn = {1937-4194},
	shorttitle = {Learning from {Failure}, {Part} {III}},
	doi = {10.1109/MS.2010.47},
	abstract = {Architects are directly affected when software architecture failure and mistakes occur. Architectural mistakes are caused by the perspectives architects have on the design and realization technologies they use for a project. Yet, taking the right perspective is actually pretty difficult as it can differ from project to project. " What's the simplest solution that could possibly work?" is the question architects must ask. Design tactics is a methodology architects can use in making and challenging their concrete design decisions. Only architectures with solutions that are easy to develop, maintain, deploy, configure, and use intuitively can result in affordable systems with high user acceptance.},
	number = {2},
	journal = {IEEE Software},
	author = {Buschmann, Frank},
	month = mar,
	year = {2010},
	note = {Conference Name: IEEE Software},
	keywords = {Concrete, Nails, Software architecture, architecture, design quality, design simplicity, design tactics, software},
	pages = {49--51},
}

@article{buschmann_learning_2010-1,
	title = {Learning from {Failure}, {Part} 2: {Featuritis}, {Performitis}, and {Other} {Diseases}},
	volume = {27},
	issn = {1937-4194},
	shorttitle = {Learning from {Failure}, {Part} 2},
	doi = {10.1109/MS.2010.14},
	abstract = {In the first part of this article, the author analyzed some common software architecture mistakes. In this article, the author discussed and explored the three mistakes that most architects know all too well. The author and his architect colleague Klaus Marquardt named these mistakes as if they were diseases: featuritis, flexibilitis, and performitis.},
	number = {1},
	journal = {IEEE Software},
	author = {Buschmann, Frank},
	month = jan,
	year = {2010},
	note = {Conference Name: IEEE Software},
	keywords = {Diseases, Software architecture, architecture quality, feature coverage, flexibility, performance, walking skeletons},
	pages = {10--11},
}

@article{buschmann_learning_2009,
	title = {Learning from {Failure}, {Part} 1: {Scoping} and {Requirements} {Woes}},
	volume = {26},
	issn = {1937-4194},
	shorttitle = {Learning from {Failure}, {Part} 1},
	doi = {10.1109/MS.2009.179},
	abstract = {The paper is an editorial on software architecture. Software projects fail for the same reasons. The mistakes that can lead software projects to trouble before concrete architecture elaboration even begins include missing, wrong, or creeping system scope; and vague, unnecessary, or extreme nonfunctional requirements. These mistakes aren't the prime responsibility of architects, but architects are directly affected if they occur because without an appropriate system scope and correspondingly appropriate requirements, they can't define sustainable architectures. A system's scope defines its responsibilities, but also its boundaries. Failing to define a precise system scope can result in architectures that support the wrong functionality, too much functionality, too many functionality variations, too few functions, or poor quality. Architects should pay special attention to nonfunctional requirements that too often include vague or unnecessarily extreme specifications. Without precision, architects must guess which nonfunctional qualities are actually needed, and if they must guess, they'll likely guess wrong. An agile, incremental approach to software development define an initial system scope and set of requirements in a reasonable time and adjust this big picture step-wise until it has enough focus, substance, and clarity. Then, architects get concrete guidance for their work and can act rather than react when designing the system's architecture. Only then do architects receive a safety network that allows them to identify and correct design flaws in their own area of responsibility.},
	number = {6},
	journal = {IEEE Software},
	author = {Buschmann, Frank},
	month = nov,
	year = {2009},
	note = {Conference Name: IEEE Software},
	keywords = {Computer architecture, Concrete, Programming, Safety, Software architecture, functionality, nonfunctional requirements, requirements engineering, software architect, software architecture, software engineering, system scope},
	pages = {68--69},
}

@article{baldoni_survey_2018,
	title = {A {Survey} of {Symbolic} {Execution} {Techniques}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3182657},
	doi = {10.1145/3182657},
	abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program’s authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the past four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience.},
	number = {3},
	urldate = {2022-01-27},
	journal = {ACM Computing Surveys},
	author = {Baldoni, Roberto and Coppa, Emilio and D’elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
	month = may,
	year = {2018},
	keywords = {Symbolic execution, concolic execution, software testing, static analysis},
	pages = {50:1--50:39},
}

@inproceedings{yun_qsym_2018,
	title = {\{{QSYM}\} : {A} {Practical} {Concolic} {Execution} {Engine} {Tailored} for {Hybrid} {Fuzzing}},
	isbn = {978-1-939133-04-5},
	shorttitle = {\{{QSYM}\}},
	url = {https://www.usenix.org/conference/usenixsecurity18/presentation/yun},
	language = {en},
	urldate = {2022-01-27},
	author = {Yun, Insu and Lee, Sangho and Xu, Meng and Jang, Yeongjin and Kim, Taesoo},
	year = {2018},
	pages = {745--761},
}

@inproceedings{stephens_driller_2016,
	address = {San Diego, CA},
	title = {Driller: {Augmenting} {Fuzzing} {Through} {Selective} {Symbolic} {Execution}},
	isbn = {978-1-891562-41-9},
	shorttitle = {Driller},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2017/09/driller-augmenting-fuzzing-through-selective-symbolic-execution.pdf},
	doi = {10.14722/ndss.2016.23368},
	abstract = {Memory corruption vulnerabilities are an everpresent risk in software, which attackers can exploit to obtain unauthorized access to conﬁdential information. As products with access to sensitive data are becoming more prevalent, the number of potentially exploitable systems is also increasing, resulting in a greater need for automated software vetting tools. DARPA recently funded a competition, with millions of dollars in prize money, to further research focusing on automated vulnerability ﬁnding and patching, showing the importance of research in this area. Current techniques for ﬁnding potential bugs include static, dynamic, and concolic analysis systems, which each having their own advantages and disadvantages. A common limitation of systems designed to create inputs which trigger vulnerabilities is that they only ﬁnd shallow bugs and struggle to exercise deeper paths in executables.},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Proceedings 2016 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Stephens, Nick and Grosen, John and Salls, Christopher and Dutcher, Andrew and Wang, Ruoyu and Corbetta, Jacopo and Shoshitaishvili, Yan and Kruegel, Christopher and Vigna, Giovanni},
	year = {2016},
}

@misc{noauthor_under-constrained_nodate,
	title = {Under-{Constrained} {Symbolic} {Execution}: {Correctness} {Checking} for {Real} {Code} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/ramos},
	urldate = {2022-01-27},
}

@article{hofmann_requirements_2001,
	title = {Requirements engineering as a success factor in software projects},
	volume = {18},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/936219/},
	doi = {10.1109/MS.2001.936219},
	language = {en},
	number = {4},
	urldate = {2022-01-26},
	journal = {IEEE Software},
	author = {Hofmann, H.F. and Lehner, F.},
	month = jul,
	year = {2001},
	pages = {58--66},
}

@article{iqbal_requirements_2020,
	title = {Requirements engineering issues causing software development outsourcing failure},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0229785},
	doi = {10.1371/journal.pone.0229785},
	abstract = {Software development outsourcing is becoming more and more famous because of the advantages like cost abatement, process enhancement, and coping with the scarcity of needed resources. Studies confirm that unfortunately a large proportion of the software development outsourcing projects fails to realize anticipated benefits. Investigations into the failures of such projects divulge that in several cases software development outsourcing projects are failed because of the issues that are associated with requirements engineering process. The objective of this study is the identification and the ranking of the commonly occurring issues of the requirements engineering process in the case of software development outsourcing. For this purpose, contemporary literature has been assessed rigorously, issues faced by practitioners have been identified and three questionnaire surveys have been organized by involving experienced software development outsourcing practitioners. The Delphi technique, cut-off value method and 50\% rule have also been employed. The study explores 150 issues (129 issues from literature and 21 from industry) of requirements engineering process for software development outsourcing, groups the 150 issues into 7 identified categories and then extricates 43 customarily or commonly arising issues from the 150 issues. Founded on ‘frequency of occurrence’ the 43 customarily arising issues have been ranked with respect to respective categories (category-wise ranking) and with respect to all the categories (overall ranking). Categories of the customarily arising issues have also been ranked. The issues’ identification and ranking contribute to design proactive software project management plan for dealing with software development outsourcing failures and attaining conjectured benefits of the software development outsourcing.},
	language = {en},
	number = {4},
	urldate = {2022-01-26},
	journal = {PLOS ONE},
	author = {Iqbal, Javed and Ahmad, Rodina B. and Khan, Muzafar and {Fazal-e-Amin} and Alyahya, Sultan and Nizam Nasir, Mohd Hairul and Akhunzada, Adnan and Shoaib, Muhammad},
	editor = {Xin, Baogui},
	month = apr,
	year = {2020},
	keywords = {outsourcing},
	pages = {e0229785},
}

@article{peslak_review_2007,
	title = {A {Review} of the {Impact} of {Acm} {Code} of {Conduct} on {Information} {Technology} {Moral} {Judgment} and {Intent}},
	volume = {47},
	copyright = {Copyright International Association for Computer Information Systems Spring 2007},
	issn = {08874417},
	url = {https://www.proquest.com/docview/232575060/abstract/8B06FB3924724E2EPQ/1},
	abstract = {One of the most widely recognized code of ethics in information technology (IT) is the ACM (Association for Computing Machinery) Code of Ethics. Adopted in 1992, the code covers many of the key ethical areas that are encountered in information technology practice. But this code has been lightly studied in the literature, including its recognition and its acceptance. Likewise, its effectiveness in influencing moral intent has not previously been established. This manuscript reviews selected key statements from the ACM Code of Ethics to determine the level of agreement with these statements. The surveyed group includes IT students, faculty, and staff. In general, agreement on all issues is found, though varying in degree. Next, the study analyzes the relationship between ACM code agreement and ethical intent. The relationships in nearly all cases are positive and significant. Finally, it examines the influence this code has among participants in a hypothetical hostile work situation. Specifically, it examines moral intent when a supervisor recommends an action directly contrary to the code. The difference between actions in a hostile situation versus no supervisor influence is found to be significant in some cases. [PUBLICATION ABSTRACT]},
	language = {English},
	number = {3},
	urldate = {2022-01-26},
	journal = {The Journal of Computer Information Systems},
	author = {Peslak, Alan R.},
	year = {2007},
	note = {Num Pages: 10
Place: Stillwater, United Kingdom
Publisher: Taylor \& Francis Ltd.},
	keywords = {Agreements, Behavior, Codes, Computers, Decision making, Education, Education--Computer Applications, Ethics, Impact analysis, Influence, Information technology, Judgment, Professional ethics, Professionals, Studies},
	pages = {1--10},
}

@article{tan_bug_2014,
	title = {Bug characteristics in open source software},
	volume = {19},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-013-9258-8},
	doi = {10.1007/s10664-013-9258-8},
	language = {en},
	number = {6},
	urldate = {2022-01-20},
	journal = {Empirical Software Engineering},
	author = {Tan, Lin and Liu, Chen and Li, Zhenmin and Wang, Xuanhui and Zhou, Yuanyuan and Zhai, Chengxiang},
	month = dec,
	year = {2014},
	pages = {1665--1705},
}

@inproceedings{lloyd_iec_2009,
	address = {London, UK},
	title = {{IEC} 61508 and {IEC} 61511 assessments - some lessons learned},
	isbn = {978-1-84919-195-1},
	url = {https://digital-library.theiet.org/content/conferences/10.1049/cp.2009.1540},
	doi = {10.1049/cp.2009.1540},
	abstract = {In recent years we have conducted about 25 assessments using IEC 61508 or IEC 61511, working mainly to Safety Integrity Level (SIL) 2, but on some occasions to SIL 3. In this paper we present some of the lessons we have learned and offer advice to those seeking certification for components, systems or generic process capability. We cover the three main parts of the IEC 61508 standard: Functional Safety (FS) Management; Hardware; Software. More recently, our work has included software products whose assessment has entailed building complex arguments for their compliance. This has led us to use argument structuring techniques that we comment on at the end of this paper.},
	language = {en},
	urldate = {2022-01-19},
	booktitle = {4th {IET} {International} {Conference} on {Systems} {Safety} 2009. {Incorporating} the {SaRS} {Annual} {Conference}},
	publisher = {IET},
	author = {Lloyd, M.H. and Reeve, P.J.},
	year = {2009},
	pages = {2A1--2A1},
}

@article{noauthor_ecfr_nodate,
	title = {{eCFR} :: 21 {CFR} {Part} 820 -- {Quality} {System} {Regulation}},
	language = {en},
	journal = {CFR Part},
	pages = {14},
}

@inproceedings{chen_towards_2016,
	title = {Towards {Automated} {Dynamic} {Analysis} for {Linux}-based {Embedded} {Firmware}},
	doi = {10.14722/ndss.2016.23415},
	author = {Chen, Daming and Egele, Manuel and Woo, Maverick and Brumley, David},
	month = jan,
	year = {2016},
}

@article{yin_firmhunter_2021,
	title = {{FirmHunter}: {State}-{Aware} and {Introspection}-{Driven} {Grey}-{Box} {Fuzzing} towards {IoT} {Firmware}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {{FirmHunter}},
	url = {https://www.mdpi.com/2076-3417/11/19/9094},
	doi = {10.3390/app11199094},
	abstract = {IoT devices are exponentially increasing in all aspects of our lives. Via the web interfaces of IoT devices, attackers can control IoT devices by exploiting their vulnerabilities. In order to guarantee IoT security, testing these IoT devices to detect vulnerabilities is very important. In this work, we present FirmHunter, an automated state-aware and introspection-driven grey-box fuzzer towards Linux-based firmware images on the basis of emulation. It employs a message-state queue to overcome the dependency problem in test cases. Furthermore, it implements a scheduler collecting execution information from system introspection to drive fuzzing towards more interesting test cases, which speeds up vulnerability discovery. We evaluate FirmHunter by emulating and fuzzing eight firmware images including seven routers and one IP camera with a state-of-the-art IoT fuzzer FirmFuzz and a web application scanner ZAP. Our evaluation results show that (1) the message-state queue enables FirmHunter to parse the dependencies in test cases and find real-world vulnerabilities that other fuzzers cannot detect; (2) our scheduler accelerates the discovery of vulnerabilities by an average of 42\%; and (3) FirmHunter is able to find unknown vulnerabilities.},
	language = {en},
	number = {19},
	urldate = {2022-01-18},
	journal = {Applied Sciences},
	author = {Yin, Qidi and Zhou, Xu and Zhang, Hangwei},
	month = jan,
	year = {2021},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {IoT, firmware, fuzzing, vulnerability},
	pages = {9094},
}

@inproceedings{srivastava_firmfuzz_2019,
	address = {New York, NY, USA},
	series = {{IoT} {S}\&amp;{P}'19},
	title = {{FirmFuzz}: {Automated} {IoT} {Firmware} {Introspection} and {Analysis}},
	isbn = {978-1-4503-6838-4},
	shorttitle = {{FirmFuzz}},
	url = {https://doi.org/10.1145/3338507.3358616},
	doi = {10.1145/3338507.3358616},
	abstract = {While the number of IoT devices grows at an exhilarating pace their security remains stagnant. Imposing secure coding standards across all vendors is infeasible. Testing individual devices allows an analyst to evaluate their security post deployment. Any discovered vulnerabilities can then be disclosed to the vendors in order to assist them in securing their products. The search for vulnerabilities should ideally be automated for efficiency and furthermore be device-independent for scalability. We present FirmFuzz, an automated device-independent emulation and dynamic analysis framework for Linux-based firmware images. It employs a greybox-based generational fuzzing approach coupled with static analysis and system introspection to provide targeted and deterministic bug discovery within a firmware image. We evaluate FirmFuzz by emulating and dynamically analyzing 32 images (from 27 unique devices) with a network accessible from the host performing the emulation. During testing, FirmFuzz discovered seven previously undisclosed vulnerabilities across six different devices: two IP cameras and four routers. So far, 4 CVE's have been assigned.},
	urldate = {2022-01-18},
	booktitle = {Proceedings of the 2nd {International} {ACM} {Workshop} on {Security} and {Privacy} for the {Internet}-of-{Things}},
	publisher = {Association for Computing Machinery},
	author = {Srivastava, Prashast and Peng, Hui and Li, Jiahao and Okhravi, Hamed and Shrobe, Howard and Payer, Mathias},
	month = nov,
	year = {2019},
	keywords = {dynamic analysis, firmware testing, fuzzing, iot, vulnerability analysis},
	pages = {15--21},
}

@article{haque_well_2021,
	title = {Well {Begun} is {Half} {Done}: {An} {Empirical} {Study} of {Exploitability} \& {Impact} of {Base}-{Image} {Vulnerabilities}},
	shorttitle = {Well {Begun} is {Half} {Done}},
	url = {http://arxiv.org/abs/2112.12597},
	abstract = {Container technology, (e.g., Docker) is being widely adopted for deploying software infrastructures or applications in the form of container images. Security vulnerabilities in the container images are a primary concern for developing containerized software. Exploitation of the vulnerabilities could result in disastrous impact, such as loss of confidentiality, integrity, and availability of containerized software. Understanding the exploitability and impact characteristics of vulnerabilities can help in securing the configuration of containerized software. However, there is a lack of research aimed at empirically identifying and understanding the exploitability and impact of vulnerabilities in container images. We carried out an empirical study to investigate the exploitability and impact of security vulnerabilities in base-images and their prevalence in open-source containerized software. We considered base-images since container images are built from base-images that provide all the core functionalities to build and operate containerized software. We discovered and characterized the exploitability and impact of security vulnerabilities in 261 base-images, which are the origin of 4,681 actively maintained official container images in the largest container registry, i.e., Docker Hub. To characterize the prevalence of vulnerable base-images in real-world projects, we analysed 64,579 containerized software from GitHub. Our analysis of a set of \$1,983\$ unique base-image security vulnerabilities revealed 13 novel findings. These findings are expected to help developers to understand the potential security problems related to base-images and encourage them to investigate base-images from security perspective before developing their applications.},
	urldate = {2022-01-18},
	journal = {arXiv:2112.12597 [cs]},
	author = {Haque, Mubin Ul and Babar, M. Ali},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.12597},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{zhang_general_2020,
	title = {A {General} {Framework} to {Understand} {Vulnerabilities} in {Information} {Systems}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3006361},
	abstract = {Firms and organizations are increasingly facing security issues related to vulnerabilities in their information systems. Firms, especially small and medium-sized enterprises, usually have very limited security resources and thus have difficulty understanding vulnerabilities and fixing them accordingly. This study aims to build a general framework that can help firms understand the characteristics of vulnerabilities in information systems: for instance, what category a specific vulnerability belongs to, what potential risks it poses, and what the key clues are to addressing it. To this end, we collect data on real vulnerabilities that have emerged in firms' information systems from a popular vulnerability report platform. Features are extracted at four different levels, namely, the word, phrase, topic, and record levels. The experimental results show that the general framework helps characterize the modes and patterns of various types of vulnerabilities. This study contributes to the security literature by providing a deeper understanding of the characteristics of vulnerabilities and their related suggested solutions. Firms can apply this framework to ensure information security.},
	journal = {IEEE Access},
	author = {Zhang, Xiong and Xie, Haoran and Yang, Hao and Shao, Hongkai and Zhu, Minghao},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Analytical models, Classification, Databases, Information security, Software, Testing, information security, risk-level prediction, topic analysis, vulnerability},
	pages = {121858--121873},
}

@inproceedings{mesa_understanding_2018,
	address = {New York, NY, USA},
	series = {{SPLC} '18},
	title = {Understanding vulnerabilities in plugin-based web systems: an exploratory study of wordpress},
	isbn = {978-1-4503-6464-5},
	shorttitle = {Understanding vulnerabilities in plugin-based web systems},
	url = {http://doi.org/10.1145/3233027.3233042},
	doi = {10.1145/3233027.3233042},
	abstract = {A common software product line strategy involves plugin-based web systems that support simple and quick incorporation of custom behaviors. As a result, they have been widely adopted to create web-based applications. Indeed, the popularity of ecosystems that support plugin-based development (e.g., WordPress) is largely due to the number of customization options available as community-contributed plugins. However, plugin-related vulnerabilities tend to be recurrent, exploitable and hard to be detected and may lead to severe consequences for the customized product. Hence, there is a need to further understand such vulnerabilities to enable preventing relevant security threats. Therefore, we conducted an exploratory study to characterize vulnerabilities caused by plugins in web-based systems. To this end, we went over WordPress vulnerability bulletins cataloged by the National Vulnerability Database as well as associated patches maintained by the WordPress plugins repository. We identified the main types of vulnerabilities caused by plugins as well as their impact and the size of the patch to fix the vulnerability. Moreover, we identified the most common security-related topics discussed among WordPress developers. We observed that, while plugin-related vulnerabilities may have severe consequences and might remain unnoticed for years before being fixed, they can commonly be mitigated with small and localized changes to the source code. The characterization helps to provide an understanding on how typical plugin-based vulnerabilities manifest themselves in practice. Such information can be helpful to steer future research on plugin-based vulnerability detection and prevention.},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 22nd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} 1},
	publisher = {Association for Computing Machinery},
	author = {Mesa, Oslien and Vieira, Reginaldo and Viana, Marx and Durelli, Vinicius H. S. and Cirilo, Elder and Kalinowski, Marcos and Lucena, Carlos},
	month = sep,
	year = {2018},
	keywords = {exploratory study, plugin-based web systems, security, software product lines},
	pages = {149--159},
}

@inproceedings{li_large-scale_2017,
	address = {New York, NY, USA},
	series = {{CCS} '17},
	title = {A {Large}-{Scale} {Empirical} {Study} of {Security} {Patches}},
	isbn = {978-1-4503-4946-8},
	url = {http://doi.org/10.1145/3133956.3134072},
	doi = {10.1145/3133956.3134072},
	abstract = {Given how the "patching treadmill" plays a central role for enabling sites to counter emergent security concerns, it behooves the security community to understand the patch development process and characteristics of the resulting fixes. Illumination of the nature of security patch development can inform us of shortcomings in existing remediation processes and provide insights for improving current practices. In this work we conduct a large-scale empirical study of security patches, investigating more than 4,000 bug fixes for over 3,000 vulnerabilities that affected a diverse set of 682 open-source software projects. For our analysis we draw upon the National Vulnerability Database, information scraped from relevant external references, affected software repositories, and their associated security fixes. Leveraging this diverse set of information, we conduct an analysis of various aspects of the patch development life cycle, including investigation into the duration of impact a vulnerability has on a code base, the timeliness of patch development, and the degree to which developers produce safe and reliable fixes. We then characterize the nature of security fixes in comparison to other non-security bug fixes, exploring the complexity of different types of patches and their impact on code bases. Among our findings we identify that: security patches have a lower footprint in code bases than non-security bug patches; a third of all security issues were introduced more than 3 years prior to remediation; attackers who monitor open-source repositories can often get a jump of weeks to months on targeting not-yet-patched systems prior to any public disclosure and patch distribution; nearly 5\% of security fixes negatively impacted the associated software; and 7\% failed to completely remedy the security hole they targeted.},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Li, Frank and Paxson, Vern},
	month = oct,
	year = {2017},
	keywords = {empirical study, patch complexity, security patches, vulnerabilities},
	pages = {2201--2215},
}

@inproceedings{elia_analysis_2017,
	title = {An {Analysis} of {OpenStack} {Vulnerabilities}},
	doi = {10.1109/EDCC.2017.29},
	abstract = {Cloud management frameworks provide an effective way to deploy and manage the hardware, storage and network resources for supporting critical cloud infrastructures. OpenStack is used in the context of business critical systems and frequently deals with highly sensitive resources, where a security breach may result in severe damage, including information theft or financial losses. Despite this, there is little information on how much security is a concern during design and implementation of OpenStack components. This work analyses 5 years of security reports on OpenStack and the corresponding patches, with the goal of characterizing the most frequent vulnerabilities, how they can be exploited, and their root causes. The goal is to identify vulnerability trends, characterize frequent threats, and shed some light on the overall security of OpenStack. Special focus is placed on the framework component for virtualization management (Nova), by also analyzing the code of the available patches. Overall results show a preponderance of vulnerabilities that may be exploited to cause DoS and expose sensitive information. Also, 2/3 of the total number of vulnerabilities can be exploited by insider attacks, urging administrators to focus protection efforts on them. Finally, many bugs remain undetected for long periods when most of them are easy to avoid or detect and correct.},
	booktitle = {2017 13th {European} {Dependable} {Computing} {Conference} ({EDCC})},
	author = {Elia, Ivano Alessandro and Antunes, Nuno and Laranjeiro, Nuno and Vieira, Marco},
	month = sep,
	year = {2017},
	keywords = {Authentication, Cloud, Cloud computing, Computer bugs, Hardware, Market research, OpenStack, Security, Vulnerabilities},
	pages = {129--134},
}

@inproceedings{perez-botero_characterizing_2013,
	address = {New York, NY, USA},
	series = {Cloud {Computing} '13},
	title = {Characterizing hypervisor vulnerabilities in cloud computing servers},
	isbn = {978-1-4503-2067-2},
	url = {http://doi.org/10.1145/2484402.2484406},
	doi = {10.1145/2484402.2484406},
	abstract = {The rise of the Cloud Computing paradigm has led to security concerns, taking into account that resources are shared and mediated by a Hypervisor which may be targeted by rogue guest VMs and remote attackers. In order to better define the threats to which a cloud server's Hypervisor is exposed, we conducted a thorough analysis of the codebase of two popular open-source Hypervisors, Xen and KVM, followed by an extensive study of the vulnerability reports associated with them. Based on our findings, we propose a characterization of Hypervisor Vulnerabilities comprised of three dimensions: the trigger source (i.e. where the attacker is located), the attack vector (i.e. the Hypervisor functionality that enables the security breach), and the attack target (i.e. the runtime domain that is compromised). This can be used to understand potential paths different attacks can take, and which vulnerabilities enable them. Moreover, most common paths can be discovered to learn where the defenses should be focused, or conversely, least common paths can be used to find yet-unexplored ways attackers may use to get into the system.},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 2013 international workshop on {Security} in cloud computing},
	publisher = {Association for Computing Machinery},
	author = {Perez-Botero, Diego and Szefer, Jakub and Lee, Ruby B.},
	month = may,
	year = {2013},
	keywords = {attack vectors, hypervisor vulnerabilities, secure cloud computing, virtualization, vulnerability categorization},
	pages = {3--10},
}

@inproceedings{cerdeira_sok_2020,
	title = {{SoK}: {Understanding} the {Prevailing} {Security} {Vulnerabilities} in {TrustZone}-assisted {TEE} {Systems}},
	shorttitle = {{SoK}},
	doi = {10.1109/SP40000.2020.00061},
	abstract = {Hundreds of millions of mobile devices worldwide rely on Trusted Execution Environments (TEEs) built with Arm TrustZone for the protection of security-critical applications (e.g., DRM) and operating system (OS) components (e.g., Android keystore). TEEs are often assumed to be highly secure; however, over the past years, TEEs have been successfully attacked multiple times, with highly damaging impact across various platforms. Unfortunately, these attacks have been possible by the presence of security flaws in TEE systems. In this paper, we aim to understand which types of vulnerabilities and limitations affect existing TrustZone-assisted TEE systems, what are the main challenges to build them correctly, and what contributions can be borrowed from the research community to overcome them. To this end, we present a security analysis of popular TrustZone-assisted TEE systems (targeting Cortex-A processors) developed by Qualcomm, Trustonic, Huawei, Nvidia, and Linaro. By studying publicly documented exploits and vulnerabilities as well as by reverse engineering the TEE firmware, we identified several critical vulnerabilities across existing systems which makes it legitimate to raise reasonable concerns about the security of commercial TEE implementations.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Cerdeira, David and Santos, Nuno and Fonseca, Pedro and Pinto, Sandro},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Androids, Arm, Computer bugs, Hardware, Humanoid robots, Kernel, Linux, Security, Security Vulnerabilities, TEE, TrustZone},
	pages = {1416--1432},
}

@misc{inc_threatscope_2020,
	title = {{ThreatSCOPE}: {Addressing} software vulnerability in embedded systems},
	shorttitle = {{ThreatSCOPE}},
	url = {https://researchoutreach.org/articles/threatscope-addressing-software-vulnerability-embedded-systems/},
	abstract = {developed ThreatSCOPE, a system assurance tool that analyses and mitigates software vulnerabilities and cyber threats in embedded systems.},
	language = {en-GB},
	urldate = {2022-01-17},
	journal = {Research Outreach},
	author = {Inc, Contact Details Address: BlueRISC and Street, 400 Amity and Amherst, Suites 0-1-3-4},
	month = dec,
	year = {2020},
}

@inproceedings{ferrante_parallel_2012,
	address = {Berlin, Heidelberg},
	series = {{SAFECOMP}'12},
	title = {Parallel {NuSMV}: a {NuSMV} extension for the verification of complex embedded systems},
	isbn = {978-3-642-33674-4},
	shorttitle = {Parallel {NuSMV}},
	url = {https://doi.org/10.1007/978-3-642-33675-1_38},
	doi = {10.1007/978-3-642-33675-1_38},
	abstract = {In this paper we present Parallel NuSMV, a tool based on the NuSMV model checker that integrates the ManySAT parallel SAT solver. The PNuSMV is part of the FormalSpecs Verifier framework for the formal verification of Simulink/Stateflow models. The experiments we performed show that the use of a parallel SAT solver allows for an average speedup of an order of magnitude or more on industry-level size models. The main contributions of the papers are (1) the description of the PNuSMV model checker (2) the description of the verification time speedup w.r.t. the NuSMV tool for the verification of industrial-sized embedded systems and (3) the integration of the tool in the FormalSpecs Verifier framework for the verification of Simulink/Stateflow models with the application to a cruise control case study.},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 2012 international conference on {Computer} {Safety}, {Reliability}, and {Security}},
	publisher = {Springer-Verlag},
	author = {Ferrante, Orlando and Benvenuti, Luca and Mangeruca, Leonardo and Sofronis, Christos and Ferrari, Alberto},
	month = sep,
	year = {2012},
	keywords = {contract-based design, embedded systems, formal verification, model checking},
	pages = {409--416},
}

@inproceedings{andrade_test_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Test {Case} {Generation} of {Embedded} {Real}-{Time} {Systems} with {Interruptions} for {FreeRTOS}},
	isbn = {978-3-642-10452-7},
	doi = {10.1007/978-3-642-10452-7_5},
	abstract = {This paper discusses issues raised in the construction of test models and automatic generation of test cases for embedded real-time systems with interruptions that can run on the FreeRTOS operating system. The focus is on the use of symbolic transition systems (STSs) as the formalism from which test cases are generated by using the STG tool. The solution presented considers a test case execution model for real-time systems with interruptions that can be based on the integrated use of FreeRTOS components. A case study is presented to illustrate all steps from the construction of the test model to test case generation.},
	language = {en},
	booktitle = {Formal {Methods}: {Foundations} and {Applications}},
	publisher = {Springer},
	author = {Andrade, Wilkerson L. and Machado, Patrícia D. L. and Alves, Everton L. G. and Almeida, Diego R.},
	editor = {Oliveira, Marcel Vinícius Medeiros and Woodcock, Jim},
	year = {2009},
	keywords = {Alarm System, Handler Task, Interruption Behaviour, Sequence Diagram, Test Case Generation},
	pages = {54--69},
}

@misc{noauthor_create_nodate,
	title = {Create a short {URL} - {Google} {Help}},
	url = {https://support.google.com/faqs/answer/190768?hl=en},
	urldate = {2022-01-17},
}

@book{augustine2022enterprise,
	title = {Enterprise digital transformation: {Technology}, tools, and use cases},
	isbn = {978-1-00-054053-6},
	url = {https://books.google.com/books?id=2tpXEAAAQBAJ},
	publisher = {CRC Press},
	author = {Augustine, P. and Raj, P. and Munirathinam, S.},
	year = {2022},
}

@article{mullainathan_media_2002,
	title = {Media bias},
	abstract = {There are two different types of media bias. One bias, which we refer to as ideology, reflects a news outlet's desire to affect reader opinions in a particular direction. The second bias, which we refer to as spin, reflects the outlet's attempt to simply create a memorable story. We examine competition among media outlets in the presence of these biases. Whereas competition can eliminate the effect of ideological bias, it actually exaggerates the incentive to spin stories.},
	language = {en},
	author = {Mullainathan, Sendhil and Shleifer, Andrei},
	year = {2002},
	pages = {27},
}

@article{gao_em-fuzz_2020,
	title = {{EM}-{Fuzz}: {Augmented} {Firmware} {Fuzzing} via {Memory} {Checking}},
	volume = {39},
	issn = {1937-4151},
	shorttitle = {{EM}-{Fuzz}},
	doi = {10.1109/TCAD.2020.3013046},
	abstract = {Embedded systems are increasingly interconnected in the emerging application scenarios. Many of these applications are safety critical, making it a high priority to ensure that the systems are free from malicious attacks. This work aims to detect vulnerabilities, that could be exploited by adversaries to compromise functional correctness, in the embedded firmware, which is challenging especially due to the absence of source code. In particular, we propose EM-Fuzz, a firmware vulnerability detection technique that tightly integrates fuzzing with real-time memory checking. Based on the memory instrumentation, the firmware fuzzing can not only be guided by the traditional branch coverage to generate high-quality seeds to explore hard-to-reach regions but also by the recorded memory sensitive operations to continuously exercise sensitive regions which are prone to being attacked. More importantly, the instrumentation integrates real-time memory checkers to expose memory vulnerabilities, which is not well-supported by existing fuzzers without source code. The experiments on several real-world embedded firmware such as OpenSSL demonstrate that EM-Fuzz significantly improves the performance of state-of-the-art fuzzing tools, such as AFL and AFLFast, with the coverage improvements of 93.98\% and 46.89\%, respectively. Furthermore, EM-Fuzz exposes a total of 23 vulnerabilities, with an average of about 7-h per vulnerability. AFL and AFLFast together find 10 vulnerabilities, costing about 13 h and 10-h per vulnerability on average, respectively. Out of these 23 vulnerabilities, 16 are previously unknown and have been reported to the upstream product vendors, 7 of which have been assigned with unique CVE identifiers in the U.S. National Vulnerability Database.},
	number = {11},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Gao, Jian and Xu, Yiwen and Jiang, Yu and Liu, Zhe and Chang, Wanli and Jiao, Xun and Sun, Jiaguang},
	month = nov,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {Design automation, Embedded firmware, Fuzzing, Instruments, Integrated circuits, Memory management, Micromechanical devices, Tools, guided fuzzing, memory checking, vulnerability},
	pages = {3420--3432},
}

@inproceedings{makhshari_iot_2021,
	title = {{IoT} {Development} {In} {The} {Wild}: {Bug} {Taxonomy} and {Developer} {Challenges}},
	shorttitle = {{IoT} {Development} {In} {The} {Wild}},
	doi = {10.1109/ICSE-Companion52605.2021.00103},
	abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Makhshari, Amir and Mesbah, Ali},
	month = may,
	year = {2021},
	note = {ISSN: 2574-1926},
	keywords = {Computer bugs, Correlation, Empirical-Study, Faces, Internet-of-Things, Mining-Software-Repositories, Software engineering, Software-Engineering, Systematics, Taxonomy, Tools},
	pages = {225--226},
}

@inproceedings{fontana_evaluating_2021,
	title = {Evaluating the {Architectural} {Debt} of {IoT} {Projects}},
	doi = {10.1109/SERP4IoT52556.2021.00011},
	abstract = {We observed a great and increasing interest in the last few years towards the evaluation of technical debt of software projects, in particular in the direction of code and architectural debt evaluation. This kind of analysis has not yet been performed for IoT projects. Hence, in this paper we start this exploration through the analysis of four Open Source IoT projects. We focus our attention on architectural debt and we exploit a tool, called Arcan, developed for architectural smell detection and for the computation of an architectural debt index. The results show that also IoT projects are subjected to architectural debt, and in particular to the presence of Cyclic Dependency and Unstable Dependency smells. However, there is evidence that the continuous refactoring of the code helps in avoiding the increase of debt, hence also developers of IoT projects should schedule periodical clean-ups of their code.},
	booktitle = {2021 {IEEE}/{ACM} 3rd {International} {Workshop} on {Software} {Engineering} {Research} and {Practices} for the {IoT} ({SERP4IoT})},
	author = {Fontana, Francesca Arcelli and Pigazzini, Ilaria},
	month = jun,
	year = {2021},
	keywords = {Architectural Debt, Architectural Debt Index, Architectural Smells, Conferences, Indexes, IoT Projects, Schedules, Software, Software engineering, Tools},
	pages = {27--31},
}

@misc{baker_wired_2020,
	title = {Wired {Magazine}},
	url = {https://ischoolwikis.sjsu.edu/lispublications/wiki/civilian-publications/wired-magazine/},
	language = {en-US},
	urldate = {2022-01-12},
	journal = {Library \& Information Science Publications Wiki},
	author = {Baker, Bethany},
	year = {2020},
}

@misc{noauthor_credibility_2021,
	title = {Credibility of major news organizations in the {U}.{S}. 2021},
	url = {https://www.statista.com/statistics/239784/credibility-of-major-news-organizations-in-the-us/},
	abstract = {According to a survey held among adults in the United States in May 2021, ABC was considered to be the most credible news source in the country, with 58 percent of respondents believing the organization to be very or somewhat credible.},
	language = {en},
	urldate = {2022-01-12},
	journal = {Statista},
	year = {2021},
}

@book{storey_safety_1996,
	address = {USA},
	title = {Safety {Critical} {Computer} {Systems}},
	isbn = {978-0-201-42787-5},
	abstract = {From the Publisher: Increasingly, microcomputers are being used in applications where their correct operation is vital to ensure the safety of the public and the environment: from anti-lock braking systems in automobiles, to fly-by-wire aircraft, to shut-down systems at nuclear power plants. It is, therefore, vital that engineers are aware of the safety implications of the systems they develop. This book is an introduction to the field of safety-critical computer systems, and is written for any engineer who uses microcomputers within real-time embedded systems. It assumes no prior knowledge of safety, or of any specific computer hardware or programming language. This book covers all phases of the life of a safety-critical system from its conception and specification, through to its certification, installation, service and decommissioning; provides information on how to assess the safety implications of projects, and determine the measures necessary to develop systems to meet safety needs; gives a thorough grounding in the techniques available to investigate the safety aspects of computer-based systems and the methods that may be used to enhance their dependability; and uses case studies and worked examples from a wide range of industrial sectors including the nuclear, aircraft, automotive and consumer products industries. This text is intended for both engineering and computer science students, and for practising engineers within computer-related industries. The approach taken is equally suited to engineers who consider computers from a hardware, software or systems viewpoint.},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Storey, Neil R.},
	year = {1996},
	keywords = {IoTFailurePaper},
}

@article{holm_empirical_2012,
	title = {Empirical {Analysis} of {System}-{Level} {Vulnerability} {Metrics} through {Actual} {Attacks}},
	volume = {9},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2012.66},
	abstract = {The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Holm, Hannes and Ekstedt, Mathias and Andersson, Dennis},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Authorization, Computational modeling, Computer crime, Mathematical model, Network security, Network-level security and protection, Risk management, Telecommunication network management, network management, phreaking), risk management, unauthorized access (hacking},
	pages = {825--837},
}

@inproceedings{clements_halucinator_2020,
	title = {\{{HALucinator}\}: {Firmware} {Re}-hosting {Through} {Abstraction} {Layer} {Emulation}},
	isbn = {978-1-939133-17-5},
	shorttitle = {\{{HALucinator}\}},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/clements},
	language = {en},
	urldate = {2022-01-12},
	author = {Clements, Abraham A. and Gustafson, Eric and Scharnowski, Tobias and Grosen, Paul and Fritz, David and Kruegel, Christopher and Vigna, Giovanni and Bagchi, Saurabh and Payer, Mathias},
	year = {2020},
	pages = {1201--1218},
}

@article{holm_empirical_2012-1,
	title = {Empirical {Analysis} of {System}-{Level} {Vulnerability} {Metrics} through {Actual} {Attacks}},
	volume = {9},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2012.66},
	abstract = {The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Holm, Hannes and Ekstedt, Mathias and Andersson, Dennis},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Authorization, Computational modeling, Computer crime, Mathematical model, Network security, Network-level security and protection, Risk management, Telecommunication network management, network management, phreaking), risk management, unauthorized access (hacking},
	pages = {825--837},
}

@inproceedings{bilge_before_2012,
	address = {New York, NY, USA},
	series = {{CCS} '12},
	title = {Before we knew it: an empirical study of zero-day attacks in the real world},
	isbn = {978-1-4503-1651-4},
	shorttitle = {Before we knew it},
	url = {http://doi.org/10.1145/2382196.2382284},
	doi = {10.1145/2382196.2382284},
	abstract = {Little is known about the duration and prevalence of zero-day attacks, which exploit vulnerabilities that have not been disclosed publicly. Knowledge of new vulnerabilities gives cyber criminals a free pass to attack any target of their choosing, while remaining undetected. Unfortunately, these serious threats are difficult to analyze, because, in general, data is not available until after an attack is discovered. Moreover, zero-day attacks are rare events that are unlikely to be observed in honeypots or in lab experiments. In this paper, we describe a method for automatically identifying zero-day attacks from field-gathered data that records when benign and malicious binaries are downloaded on 11 million real hosts around the world. Searching this data set for malicious files that exploit known vulnerabilities indicates which files appeared on the Internet before the corresponding vulnerabilities were disclosed. We identify 18 vulnerabilities exploited before disclosure, of which 11 were not previously known to have been employed in zero-day attacks. We also find that a typical zero-day attack lasts 312 days on average and that, after vulnerabilities are disclosed publicly, the volume of attacks exploiting them increases by up to 5 orders of magnitude.},
	urldate = {2022-01-10},
	booktitle = {Proceedings of the 2012 {ACM} conference on {Computer} and communications security},
	publisher = {Association for Computing Machinery},
	author = {Bilge, Leyla and Dumitraş, Tudor},
	month = oct,
	year = {2012},
	keywords = {full disclosure, vulnerabilities, zero-day attacks},
	pages = {833--844},
}

@inproceedings{parnas_inspection_1994,
	title = {Inspection of {Safety}-{Critical} {Software} {Using} {Program}-{Function} {Tables}},
	abstract = {Software whose failure could cause serious damage or loss of life must be carefully inspected before it enters service. To be conﬁdent that we have considered all cases and possible event sequences, we must follow a systematic procedure based on a sound mathematical model. This paper describes our experience with such a procedure.},
	language = {en},
	booktitle = {{IFIP}},
	author = {Parnas, David Lorge},
	year = {1994},
	pages = {8},
}

@misc{noauthor_software_nodate,
	title = {Software {Engineering} {Security} {Practices} for {ML} on {IoT} - v2},
	url = {https://www.overleaf.com/project/61ae63b3af2cec826f472fdf},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2022-01-09},
}

@article{johnman_predicting_2018,
	title = {Predicting {FTSE} 100 returns and volatility using sentiment analysis},
	volume = {58},
	issn = {1467-629X},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/acfi.12373},
	doi = {10.1111/acfi.12373},
	abstract = {We investigate the statistical and economic effect of positive and negative sentiment on daily excess returns and volatility in the FTSE 100 index, using business news articles published by the Guardian Media Group between 01/01/2000 and 01/06/2016. The analysis indicates that while business news sentiment derived from articles aimed at retail traders does not influence excess returns in the FTSE 100 index, it does affect volatility, with negative sentiment increasing volatility and positive sentiment reducing it. Further, an ETF-based trading strategy based on these findings is found to outperform the naïve buy-and-hold approach.},
	language = {en},
	number = {S1},
	urldate = {2022-01-08},
	journal = {Accounting \& Finance},
	author = {Johnman, Mark and Vanstone, Bruce James and Gepp, Adrian},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/acfi.12373},
	keywords = {FTSE 100, IoTFailurePaper, News, Sentiment analysis, Text mining, Trading strategy},
	pages = {253--274},
}

@book{NAP11923,
	address = {Washington, DC},
	title = {Software for dependable systems: {Sufficient} evidence?},
	isbn = {978-0-309-10394-7},
	url = {https://www.nap.edu/catalog/11923/software-for-dependable-systems-sufficient-evidence},
	abstract = {The focus of Software for Dependable Systems is a set of fundamental principles that underlie software system dependability and that suggest a different approach to the development and assessment of dependable software., it is difficult to assess the dependability of software. The field of software engineering suffers from a pervasive lack of evidence about the incidence and severity of software failures; about the dependability of existing software systems; about the efficacy of existing and proposed development methods; about the benefits of certification schemes; and so on. There are many anecdotal reports, which2̆014although often useful for indicating areas of concern or highlighting promising avenues of research2̆014do little to establish a sound and complete basis for making policy decisions regarding dependability. The committee regards claims of extraordinary dependability that are sometimes made on this basis for the most critical of systems as unsubstantiated, and perhaps irresponsible. This difficulty regarding the lack of evidence for system dependability leads to two conclusions: (1) that better evidence is needed, so that approaches aimed at improving the dependability of software can be objectively assessed, and (2) that, for now, the pursuit of dependability in software systems should focus on the construction and evaluation of evidence.committee also recognized the importance of adopting the practices that are already known and used by the best developers; this report gives a sample of such practices. Some of these (such as systematic configuration management and automated regression testing) are relatively easy to adopt; others (such as constructing hazard analyses and threat models, exploiting formal notations when appropriate, and applying static analysis to code) will require new training for many developers. However valuable, though, these practices are in themselves no silver bullet, and new techniques and methods will be required in order to build future software systems to the level of dependability that will be required.},
	publisher = {The National Academies Press},
	author = {Council, National Research},
	editor = {Jackson, Daniel and Thomas, Martyn and Millett, Lynette I.},
	year = {2007},
	doi = {10.17226/11923},
	keywords = {IoTFailurePaper},
}

@techreport{noauthor_baseline_2017,
	type = {Report/{Study}},
	title = {Baseline {Security} {Recommendations} for {IoT}},
	url = {https://www.enisa.europa.eu/publications/baseline-security-recommendations-for-iot},
	abstract = {The study which is titled ‘Baseline Security Recommendations for Internet of Things in the context of critical information infrastructures’, aims to set the scene for IoT security in Europe. It serves as a reference point in this field  and as a foundation for relevant forthcoming initiatives and developments.},
	language = {en},
	urldate = {2022-01-07},
	institution = {ENISA},
	year = {2017},
	keywords = {IoTFailurePaper},
}

@techreport{noauthor_internet_2021,
	title = {Internet of {Things} ({IoT}) security best practices},
	url = {https://docs.microsoft.com/en-us/azure/iot-fundamentals/iot-security-best-practices},
	abstract = {Best practices for securing your IoT data and infrastructure},
	language = {en-us},
	urldate = {2022-01-07},
	institution = {Microsoft},
	year = {2021},
	keywords = {IoTFailurePaper},
}

@techreport{jtc_1sc_27_cybersecurity_2021,
	title = {Cybersecurity - {IoT} security and privacy - {Guidelines}},
	url = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/04/43/44373.html},
	language = {en},
	number = {DIS 27400},
	urldate = {2022-01-07},
	institution = {ISO/IEC},
	author = {{JTC 1/SC 27}},
	year = {2021},
	keywords = {IoTFailurePaper},
}

@book{noauthor_internet_2018,
	address = {Geneva, Switzerland},
	edition = {Edition 1.0},
	series = {International standard / {International} {Electrotechnical} {Commission}},
	title = {Internet of {Things} ({IoT}) - {Reference} guide},
	isbn = {978-2-8322-5972-6},
	language = {en},
	number = {30141},
	publisher = {IEC Central Office},
	year = {2018},
}

@book{hobbs2019embedded,
	title = {Embedded software development for safety-critical systems},
	publisher = {CRC Press},
	author = {Hobbs, Chris},
	year = {2019},
}

@article{zhou_reviewing_2021,
	title = {Reviewing {IoT} {Security} via {Logic} {Bugs} in {IoT} {Platforms} and {Systems}},
	volume = {8},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2021.3059457},
	abstract = {In recent years, Internet-of-Things (IoT) platforms and systems have been rapidly emerging. Although IoT is a new technology, new does not mean simpler (than existing networked systems). Contrarily, the complexity (of IoT platforms and systems) is actually being increased in terms of the interactions between the physical world and cyberspace. The increased complexity indeed results in new vulnerabilities. This article seeks to provide a review of the recently discovered logic bugs that are specific to IoT platforms and systems and discuss the lessons we learned from these bugs. In particular, 20 logic bugs and one weakness falling into seven categories of vulnerabilities are reviewed in this survey.},
	number = {14},
	journal = {IEEE Internet of Things Journal},
	author = {Zhou, Wei and Cao, Chen and Huo, Dongdong and Cheng, Kai and Zhang, Lan and Guan, Le and Liu, Tao and Jia, Yan and Zheng, Yaowen and Zhang, Yuqing and Sun, Limin and Wang, Yazhe and Liu, Peng},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Automation, Cloud computing, Computer bugs, Internet of Things, Internet of Things (IoT), IoTFailurePaper, Mobile applications, Monitoring, Security, logic bugs, privacy, security},
	pages = {11621--11639},
}

@article{hatton_language_2007,
	title = {Language subsetting in an industrial context: {A} comparison of {MISRA} {C} 1998 and {MISRA} {C} 2004},
	volume = {49},
	issn = {09505849},
	shorttitle = {Language subsetting in an industrial context},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584906000991},
	doi = {10.1016/j.infsof.2006.07.004},
	abstract = {The MISRA C standard [7] ﬁrst appeared in 1998 with the objective of providing a set of guidelines to restrict features in the ISO C language of known undeﬁned or otherwise dangerous behaviour. The standard was assembled by representatives of a number of companies in the automobile sector in response to the rapidly growing use of C in electronic embedded systems in automobiles. The standard attempts to build on the earlier work of [6], [3] and others. Due to various perceived deﬁciencies, notably considerable ambiguity in the rule deﬁnitions, a revision was planned and eventually appeared in 2004. This paper measures how well the two standards compare on the same population of software and also determines how well the 2004 version achieved its stated goals. Given its increasing inﬂuence, the results raise important concerns.},
	language = {en},
	number = {5},
	urldate = {2022-01-06},
	journal = {Information and Software Technology},
	author = {Hatton, Les},
	month = may,
	year = {2007},
	pages = {475--482},
}

@article{hatton_safer_2004,
	title = {Safer language subsets: an overview and a case history, {MISRA} {C}},
	volume = {46},
	issn = {09505849},
	shorttitle = {Safer language subsets},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584903002076},
	doi = {10.1016/j.infsof.2003.09.016},
	language = {en},
	number = {7},
	urldate = {2022-01-06},
	journal = {Information and Software Technology},
	author = {Hatton, Les},
	month = jun,
	year = {2004},
	pages = {465--472},
}

@techreport{basalaj_correlation_2006,
	address = {Programming Research Ltd.},
	title = {Correlation between coding standards compliance and software quality},
	abstract = {Software Quality has different meaning to different people. The ISO 9126 standard was developed to introduce clarity and establish a framework for quality to be measured. This paper aims to explore how Internal Quality characteristics of a software system (source code) can be measured effectively. Instead of relying on traditional software metrics, which are shown to be a poor predictor of underlying software quality, we advocate measuring compliance to a coding standard. We show qualitative and quantitative evidence of how adoption of a coding standard helps organizations in improving the quality of their C/C++ software.},
	urldate = {2022-01-06},
	institution = {IEE},
	author = {Basalaj, W. and {van den Beuken, Frank}},
	year = {2006},
}

@inproceedings{boogerd_assessing_2008,
	address = {Beijing, China},
	title = {Assessing the value of coding standards: {An} empirical study},
	isbn = {978-1-4244-2613-3},
	shorttitle = {Assessing the value of coding standards},
	url = {http://ieeexplore.ieee.org/document/4658076/},
	doi = {10.1109/ICSM.2008.4658076},
	abstract = {In spite of the widespread use of coding standards and tools enforcing their rules, there is little empirical evidence supporting the intuition that they prevent the introduction of faults in software. Not only can compliance with a set of rules having little impact on the number of faults be considered wasted effort, but it can actually result in an increase in faults, as any modiﬁcation has a non-zero probability of introducing a fault or triggering a previously concealed one. Therefore, it is important to build a body of empirical knowledge, helping us understand which rules are worthwhile enforcing, and which ones should be ignored in the context of fault reduction. In this paper, we describe two approaches to quantify the relation between rule violations and actual faults, and present empirical data on this relation for the MISRA C 2004 standard on an industrial case study.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {2008 {IEEE} {International} {Conference} on {Software} {Maintenance}},
	publisher = {IEEE},
	author = {Boogerd, Cathal and Moonen, Leon},
	month = sep,
	year = {2008},
	pages = {277--286},
}

@inproceedings{bai_exploring_2019,
	title = {Exploring {Tools} and {Strategies} {Used} {During} {Regular} {Expression} {Composition} {Tasks}},
	doi = {10.1109/ICPC.2019.00039},
	abstract = {Regular expressions are frequently found in programming projects. Studies have found that developers can accurately determine whether a string matches a regular expression. However, we still do not know the challenges associated with composing regular expressions. We conduct an exploratory case study to reveal the tools and strategies developers use during regular expression composition. In this study, 29 students are tasked with composing regular expressions that pass unit tests illustrating the intended behavior. The tasks are in Java and the Eclipse IDE was set up with JUnit tests. Participants had one hour to work and could use any Eclipse tools, web search, or web-based tools they desired. Screen-capture software recorded all interactions with browsers and the IDE. We analyzed the videos quantitatively by transcribing logs and extracting personas. Our results show that participants were 30\% successful (28 of 94 attempts) at achieving a 100\% pass rate on the unit tests. When participants used tools frequently, as in the case of the novice tester and the knowledgeable tester personas, or when they guess at a solution prior to searching, they are more likely to pass all the unit tests. We also found that compile errors often arise when participants searched for a result and copy/pasted the regular expression from another language into their Java files. These results point to future research into making regular expression composition easier for programmers, such as integrating visualization into the IDE to reduce context switching or providing language migration support when reusing regular expressions written in another language to reduce compile errors.},
	booktitle = {2019 {IEEE}/{ACM} 27th {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Bai, Gina R. and Clee, Brian and Shrestha, Nischal and Chapman, Carl and Wright, Cimone and Stolee, Kathryn T.},
	month = may,
	year = {2019},
	note = {ISSN: 2643-7171},
	keywords = {Exploratory study, personas, problem solving strategies, regular expressions},
	pages = {197--208},
}

@book{ieeeacm_international_conference_on_automated_software_engineering_2019_2019,
	title = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}: 10-15 {November} 2019, {San} {Diego}, {California}.},
	shorttitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	url = {https://ieeexplore.ieee.org/servlet/opac?punumber=8949433},
	language = {English},
	urldate = {2022-01-06},
	author = {IEEE/ACM International Conference on Automated Software Engineering, IEEE Computer Society and {Technical Council on Software Engineering} and {ACM Sigsoft} and {ACM SIGAI} and {IEEE Computer Society} and {Institute of Electrical and Electronics Engineers}},
	year = {2019},
	note = {OCLC: 1151050482},
}

@inproceedings{davis_why_2019,
	address = {Tallinn Estonia},
	title = {Why aren’t regular expressions a lingua franca? an empirical study on the re-use and portability of regular expressions},
	isbn = {978-1-4503-5572-8},
	shorttitle = {Why aren’t regular expressions a lingua franca?},
	url = {https://dl.acm.org/doi/10.1145/3338906.3338909},
	doi = {10.1145/3338906.3338909},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Davis, James C. and Michael IV, Louis G. and Coghlan, Christy A. and Servant, Francisco and Lee, Dongyoon},
	month = aug,
	year = {2019},
	pages = {443--454},
}

@book{ko_amy_j_cooperative_2021,
	title = {Cooperative {Software} {Development}},
	url = {https://faculty.washington.edu/ajko/books/cooperative-software-development/print},
	language = {en},
	author = {{Ko, Amy J.}},
	year = {2021},
}

@inproceedings{yin_how_2011,
	address = {Szeged, Hungary},
	title = {How do fixes become bugs?},
	isbn = {978-1-4503-0443-6},
	url = {http://dl.acm.org/citation.cfm?doid=2025113.2025121},
	doi = {10.1145/2025113.2025121},
	abstract = {Software bugs aﬀect system reliability. When a bug is exposed in the ﬁeld, developers need to ﬁx them. Unfortunately, the bug-ﬁxing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors’ reputation.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th {European} conference on {Foundations} of software engineering - {SIGSOFT}/{FSE} '11},
	publisher = {ACM Press},
	author = {Yin, Zuoning and Yuan, Ding and Zhou, Yuanyuan and Pasupathy, Shankar and Bairavasundaram, Lakshmi},
	year = {2011},
	pages = {26},
}

@article{eisenstadt_my_1997,
	title = {My hairiest bug war stories},
	volume = {40},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/248448.248456},
	doi = {10.1145/248448.248456},
	language = {en},
	number = {4},
	urldate = {2022-01-06},
	journal = {Communications of the ACM},
	author = {Eisenstadt, Marc},
	month = apr,
	year = {1997},
	pages = {30--37},
}

@inproceedings{ko_debugging_2008,
	address = {Leipzig, Germany},
	title = {Debugging reinvented: asking and answering why and why not questions about program behavior},
	isbn = {978-1-60558-079-1},
	shorttitle = {Debugging reinvented},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368130},
	doi = {10.1145/1368088.1368130},
	abstract = {When software developers want to understand the reason for a program’s behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of why did and why didn’t questions derived from the program’s code and execution. The tool then finds one or more possible explanations for the output in question, using a combination of static and dynamic slicing, precise call graphs, and new algorithms for determining potential sources of values and explanations for why a line of code was not reached. Evaluations of the tool on one task showed that novice programmers with the Whyline were twice as fast as expert programmers without it. The tool has the potential to simplify debugging in many software development contexts.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering  - {ICSE} '08},
	publisher = {ACM Press},
	author = {Ko, Andrew J. and Myers, Brad A.},
	year = {2008},
	pages = {301},
}

@article{zimmermann_what_2010,
	title = {What {Makes} a {Good} {Bug} {Report}?},
	volume = {36},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/5487527/},
	doi = {10.1109/TSE.2010.63},
	abstract = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.},
	language = {en},
	number = {5},
	urldate = {2022-01-06},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zimmermann, Thomas and Premraj, Rahul and Bettenburg, Nicolas and Just, Sascha and Schroter, Adrian and Weiss, Cathrin},
	month = sep,
	year = {2010},
	pages = {618--643},
}

@inproceedings{beller_dichotomy_2018,
	address = {Gothenburg Sweden},
	title = {On the dichotomy of debugging behavior among programmers},
	isbn = {978-1-4503-5638-1},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180175},
	doi = {10.1145/3180155.3180175},
	abstract = {Debugging is an inevitable activity in most software projects, often difﬁcult and more time-consuming than expected, giving it the nickname the “dirty little secret of computer science.” Surprisingly, we have little knowledge on how software engineers debug software problems in the real world, whether they use dedicated debugging tools, and how knowledgeable they are about debugging. This study aims to shed light on these aspects by following a mixed-methods research approach. We conduct an online survey capturing how 176 developers reﬂect on debugging. We augment this subjective survey data with objective observations on how 458 developers use the debugger included in their integrated development environments (IDEs) by instrumenting the popular ECLIPSE and INTELLIJ IDEs with the purpose-built plugin WATCHDOG 2.0. To clarify the insights and discrepancies observed in the previous steps, we followed up by conducting interviews with debugging experts and regular debugging users. Our results indicate that IDE-provided debuggers are not used as often as expected, as “printf debugging” remains a feasible choice for many programmers. Furthermore, both knowledge and use of advanced debugging features are low. These results call to strengthen hands-on debugging experience in computer science curricula and have already reﬁned the implementation of modern IDE debuggers.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Beller, Moritz and Spruit, Niels and Spinellis, Diomidis and Zaidman, Andy},
	month = may,
	year = {2018},
	pages = {572--583},
}

@article{van1997software,
	title = {Software release management},
	volume = {22},
	number = {6},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Van Der Hoek, Andre and Hall, Richard S and Heimbigner, Dennis and Wolf, Alexander L},
	year = {1997},
	note = {Publisher: ACM New York, NY, USA},
	pages = {159--175},
}

@article{chen_continuous_2015,
	title = {Continuous {Delivery}: {Huge} {Benefits}, but {Challenges} {Too}},
	volume = {32},
	issn = {0740-7459},
	shorttitle = {Continuous {Delivery}},
	url = {http://ieeexplore.ieee.org/document/7006384/},
	doi = {10.1109/MS.2015.27},
	language = {en},
	number = {2},
	urldate = {2022-01-06},
	journal = {IEEE Software},
	author = {Chen, Lianping},
	month = mar,
	year = {2015},
	pages = {50--54},
}

@inproceedings{hilton_usage_2016,
	address = {Singapore Singapore},
	title = {Usage, costs, and benefits of continuous integration in open-source projects},
	isbn = {978-1-4503-3845-5},
	url = {https://dl.acm.org/doi/10.1145/2970276.2970358},
	doi = {10.1145/2970276.2970358},
	abstract = {Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and beneﬁts associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 opensource projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and beneﬁts of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as ﬁnding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 31st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Hilton, Michael and Tunnell, Timothy and Huang, Kai and Marinov, Darko and Dig, Danny},
	month = aug,
	year = {2016},
	pages = {426--437},
}

@article{glerum_debugging_nodate,
	title = {Debugging in the (very) large: ten years of implementation and experience},
	abstract = {Windows Error Reporting (WER) is a distributed system that automates the processing of error reports coming from an installed base of a billion machines. WER has collected billions of error reports in ten years of operation. It collects error data automatically and classifies errors into buckets, which are used to prioritize developer effort and report fixes to users. WER uses a progressive approach to data collection, which minimizes overhead for most reports yet allows developers to collect detailed information when needed. WER takes advantage of its scale to use error statistics as a tool in debugging; this allows developers to isolate bugs that could not be found at smaller scale. WER has been designed for large scale: one pair of database servers can record all the errors that occur on all Windows computers worldwide.},
	language = {en},
	author = {Glerum, Kirk and Kinshumann, Kinshuman and Greenberg, Steve and Aul, Gabriel and Orgovan, Vince and Nichols, Greg and Grant, David and Loihle, Gretchen and Hunt, Galen},
	pages = {14},
}

@inproceedings{brindescu_how_2014,
	address = {Hyderabad India},
	title = {How do centralized and distributed version control systems impact software changes?},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568322},
	doi = {10.1145/2568225.2568322},
	abstract = {Distributed Version Control Systems (DVCS) have seen an increase in popularity relative to traditional Centralized Version Control Systems (CVCS). Yet we know little on whether developers are beneﬁtting from the extra power of DVCS. Without such knowledge, researchers, developers, tool builders, and team managers are in the danger of making wrong assumptions.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Brindescu, Caius and Codoban, Mihai and Shmarkatiuk, Sergii and Dig, Danny},
	month = may,
	year = {2014},
	pages = {322--333},
}

@inproceedings{van_lamsweerde_requirements_2008,
	address = {Atlanta, Georgia},
	title = {Requirements engineering: from craft to discipline},
	isbn = {978-1-59593-995-1},
	shorttitle = {Requirements engineering},
	url = {http://portal.acm.org/citation.cfm?doid=1453101.1453133},
	doi = {10.1145/1453101.1453133},
	abstract = {Getting the right software requirements under the right environment assumptions is a critical precondition for developing the right software. This task is intrinsically difficult. We need to produce a complete, adequate, consistent, and well-structured set of measurable requirements and assumptions from incomplete, imprecise, and sparse material originating from multiple, often conflicting sources. The system we need to consider comprises software and environment components including people and devices.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of software engineering - {SIGSOFT} '08/{FSE}-16},
	publisher = {ACM Press},
	author = {van Lamsweerde, Axel},
	year = {2008},
	pages = {238},
}

@article{mader_developers_2015,
	title = {Do developers benefit from requirements traceability when evolving and maintaining a software system?},
	volume = {20},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-014-9314-z},
	doi = {10.1007/s10664-014-9314-z},
	abstract = {Software traceability is a required component of many software development processes. Advocates of requirements traceability cite advantages like easier program comprehension and support for software maintenance (i.e., software change). However, despite its growing popularity, there exists no published evaluation about the usefulness of requirements traceability. It is important, if not crucial, to investigate whether the use of requirements traceability can significantly support development tasks to eventually justify its costs. We thus conducted a controlled experiment with 71 subjects re-performing real maintenance tasks on two third-party development projects: half of the tasks with and the other half without traceability. Subjects sketched their task solutions on paper to focus on the their ability to solving the problems rather than their programming skills. Our findings show that subjects with traceability performed on average 24 \% faster on a given task and created on average 50 \% more correct solutions—suggesting that traceability not only saves effort but can profoundly improve software maintenance quality.},
	language = {en},
	number = {2},
	urldate = {2022-01-06},
	journal = {Empirical Software Engineering},
	author = {Mäder, Patrick and Egyed, Alexander},
	month = apr,
	year = {2015},
	pages = {413--441},
}

@inproceedings{kim_field_2012,
	address = {Cary, North Carolina},
	title = {A field study of refactoring challenges and benefits},
	isbn = {978-1-4503-1614-9},
	url = {http://dl.acm.org/citation.cfm?doid=2393596.2393655},
	doi = {10.1145/2393596.2393655},
	abstract = {It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring beneﬁts or investigate developers’ perception towards these beneﬁts. This paper presents a ﬁeld study of refactoring beneﬁts and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey ﬁnds that the refactoring deﬁnition in practice is not conﬁned to a rigorous deﬁnition of semanticspreserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized eﬀort on refactoring Windows. The quantitative analysis of Windows 7 version history ﬁnds that the binary modules refactored by this team experienced signiﬁcant reduction in the number of inter-module dependencies and post-release defects, indicating a visible beneﬁt of refactoring.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering} - {FSE} '12},
	publisher = {ACM Press},
	author = {Kim, Miryung and Zimmermann, Thomas and Nagappan, Nachiappan},
	year = {2012},
	pages = {1},
}

@inproceedings{mohanani_requirements_2014,
	address = {Hyderabad India},
	title = {Requirements fixation},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568235},
	doi = {10.1145/2568225.2568235},
	abstract = {There is a broad consensus that understanding system desiderata (requirements) and design creativity are both important for software engineering success. However, little research has addressed the relationship between design creativity and the way requirements are framed or presented. This paper therefore aims to investigate the possibility that the way desiderata are framed or presented can affect design creativity. Forty two participants took part in a randomized control trial where one group received desiderata framed as “requirements” while the other received desiderata framed as “ideas”. Participants produced design concepts which were judged for originality. Participants who received requirements framing produced significantly less original designs than participants who received ideas framing (MannWhitney U=116.5, p=0.004). We conclude that framing desiderata as “requirements” may cause requirements fixation where designers’ preoccupation with satisfying explicit requirements inhibits their creativity.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Mohanani, Rahul and Ralph, Paul and Shreeve, Ben},
	month = may,
	year = {2014},
	pages = {895--906},
}

@inproceedings{kim_field_2012-1,
	address = {Cary, North Carolina},
	title = {A field study of refactoring challenges and benefits},
	isbn = {978-1-4503-1614-9},
	url = {http://dl.acm.org/citation.cfm?doid=2393596.2393655},
	doi = {10.1145/2393596.2393655},
	abstract = {It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring beneﬁts or investigate developers’ perception towards these beneﬁts. This paper presents a ﬁeld study of refactoring beneﬁts and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey ﬁnds that the refactoring deﬁnition in practice is not conﬁned to a rigorous deﬁnition of semanticspreserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized eﬀort on refactoring Windows. The quantitative analysis of Windows 7 version history ﬁnds that the binary modules refactored by this team experienced signiﬁcant reduction in the number of inter-module dependencies and post-release defects, indicating a visible beneﬁt of refactoring.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering} - {FSE} '12},
	publisher = {ACM Press},
	author = {Kim, Miryung and Zimmermann, Thomas and Nagappan, Nachiappan},
	year = {2012},
	pages = {1},
}

@inproceedings{khadka_how_2014,
	address = {Hyderabad India},
	title = {How do professionals perceive legacy systems and software modernization?},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568318},
	doi = {10.1145/2568225.2568318},
	abstract = {Existing research in legacy system modernization has traditionally focused on technical challenges, and takes the standpoint that legacy systems are obsolete, yet crucial for an organization’s operation. Nonetheless, it remains unclear whether practitioners in the industry also share this perception. This paper describes the outcome of an exploratory study in which 26 industrial practitioners were interviewed on what makes a software system a legacy system, what the main drivers are that lead to the modernization of such systems, and what challenges are faced during the modernization process. The ﬁndings of the interviews have been validated by means of a survey with 198 respondents. The results show that practitioners value their legacy systems highly, the challenges they face are not just technical, but also include business and organizational aspects.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Khadka, Ravi and Batlajery, Belfrit V. and Saeidi, Amir M. and Jansen, Slinger and Hage, Jurriaan},
	month = may,
	year = {2014},
	pages = {36--47},
}

@inproceedings{ernst_measure_2015,
	address = {Bergamo Italy},
	title = {Measure it? {Manage} it? {Ignore} it? software practitioners and technical debt},
	isbn = {978-1-4503-3675-8},
	shorttitle = {Measure it?},
	url = {https://dl.acm.org/doi/10.1145/2786805.2786848},
	doi = {10.1145/2786805.2786848},
	abstract = {The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ernst, Neil A. and Bellomo, Stephany and Ozkaya, Ipek and Nord, Robert L. and Gorton, Ian},
	month = aug,
	year = {2015},
	pages = {50--60},
}

@inproceedings{walker_crosscutting_2012,
	address = {Cary, North Carolina},
	title = {Do crosscutting concerns cause modularity problems?},
	isbn = {978-1-4503-1614-9},
	url = {http://dl.acm.org/citation.cfm?doid=2393596.2393654},
	doi = {10.1145/2393596.2393654},
	abstract = {It has been claimed that crosscutting concerns are pervasive and problematic, leading to diﬃculties in program comprehension, evolution, and long-term design degradation. To consider whether this theory bears out, we examine the patch history of the Mozilla project over a period of a decade to consider whether crosscutting concerns exist therein and whether we can see evidence of problems arising from them. Mozilla is an interesting case, due to its longevity; size; polylingual nature; and use of a patch review process, which maintains strong connections between issue reports and the patches that are intended to address each. We perform several statistical analyses of the over 200,000 patches submitted to address over 90,000 issues reported in this time period. We ﬁnd that 90\% of patches show little or no evidence of scattering, that the scattering of a patch tends to decrease slightly upon review on average, and that the system shows at worst a slow increase of average scattering over time.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering} - {FSE} '12},
	publisher = {ACM Press},
	author = {Walker, Robert J. and Rawal, Shreya and Sillito, Jonathan},
	year = {2012},
	pages = {1},
}

@inproceedings{murphy-hill_how_2009,
	address = {Vancouver, BC, Canada},
	title = {How we refactor, and how we know it},
	isbn = {978-1-4244-3453-4},
	url = {http://ieeexplore.ieee.org/document/5070529/},
	doi = {10.1109/ICSE.2009.5070529},
	abstract = {Much of what we know about how programmers refactor in the wild is based on studies that examine just a few software projects. Researchers have rarely taken the time to replicate these studies in other contexts or to examine the assumptions on which they are based. To help put refactoring research on a sound scientiﬁc basis, we draw conclusions using four data sets spanning more than 13 000 developers, 240 000 tool-assisted refactorings, 2500 developer hours, and 3400 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. For example, we ﬁnd that programmers frequently do not indicate refactoring activity in commit logs, which contradicts assumptions made by several previous researchers. In contrast, we were able to conﬁrm the assumption that programmers do frequently intersperse refactoring with other program changes. By conﬁrming assumptions and replicating studies made by other researchers, we can have greater conﬁdence that those researchers’ conclusions are generalizable.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Murphy-Hill, Emerson and Parnin, Chris and Black, Andrew P.},
	year = {2009},
	pages = {287--297},
}

@inproceedings{petre_uml_2013,
	address = {San Francisco, CA, USA},
	title = {{UML} in practice},
	isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
	url = {http://ieeexplore.ieee.org/document/6606618/},
	doi = {10.1109/ICSE.2013.6606618},
	abstract = {UML has been described by some as “the lingua franca of software engineering”. Evidence from industry does not necessarily support such endorsements. How exactly is UML being used in industry – if it is? This paper presents a corpus of interviews with 50 professional software engineers in 50 companies and identifies 5 patterns of UML use.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Petre, Marian},
	month = may,
	year = {2013},
	pages = {722--731},
}

@misc{malte_ubl_design_2020,
	title = {Design {Docs} at {Google}},
	url = {https://www.industrialempathy.com/posts/design-docs-at-google/},
	language = {en},
	urldate = {2022-01-06},
	author = {{Malte Ubl}},
	year = {2020},
}

@inproceedings{beck_industrial_1996,
	address = {Berlin, Germany},
	title = {Industrial experience with design patterns},
	isbn = {978-0-8186-7247-7},
	url = {http://ieeexplore.ieee.org/document/493406/},
	doi = {10.1109/ICSE.1996.493406},
	abstract = {A design pattern is a particular prose form of recording design information such that designs which have worked well in the past can be applied again in similar situations in the future. The availability of a collection of design patterns can help both the experienced and the novice designer recognize situations in which design reuse could or should occur.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of {IEEE} 18th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Beck, K. and Crocker, R. and Meszaros, G. and Coplien, J.O. and Dominick, L. and Paulisch, F. and Vlissides, J.},
	year = {1996},
	pages = {103--114},
}

@article{holm_empirical_2012,
	title = {Empirical {Analysis} of {System}-{Level} {Vulnerability} {Metrics} through {Actual} {Attacks}},
	volume = {9},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2012.66},
	abstract = {The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Holm, Hannes and Ekstedt, Mathias and Andersson, Dennis},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Authorization, Computational modeling, Computer crime, Mathematical model, Network security, Network-level security and protection, Risk management, Telecommunication network management, network management, phreaking), risk management, unauthorized access (hacking},
	pages = {825--837},
}

@article{saltzer_protection_1975,
	title = {The protection of information in computer systems},
	volume = {63},
	issn = {1558-2256},
	doi = {10.1109/PROC.1975.9939},
	abstract = {This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures-whether hardware or software-that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysts of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading.},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Saltzer, J.H. and Schroeder, M.D.},
	month = sep,
	year = {1975},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Access control, Authorization, Computer architecture, Data security, Information security, Modems, Permission, Protection, Terminology},
	pages = {1278--1308},
}

@article{holm_empirical_2012-1,
	title = {Empirical {Analysis} of {System}-{Level} {Vulnerability} {Metrics} through {Actual} {Attacks}},
	volume = {9},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2012.66},
	abstract = {The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Holm, Hannes and Ekstedt, Mathias and Andersson, Dennis},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Authorization, Computational modeling, Computer crime, Mathematical model, Network security, Network-level security and protection, Risk management, Telecommunication network management, network management, phreaking), risk management, unauthorized access (hacking},
	pages = {825--837},
}

@inproceedings{papp_embedded_2015,
	title = {Embedded systems security: {Threats}, vulnerabilities, and attack taxonomy},
	shorttitle = {Embedded systems security},
	doi = {10.1109/PST.2015.7232966},
	abstract = {Embedded systems are the driving force for technological development in many domains such as automotive, healthcare, and industrial control in the emerging post-PC era. As more and more computational and networked devices are integrated into all aspects of our lives in a pervasive and “invisible” way, security becomes critical for the dependability of all smart or intelligent systems built upon these embedded systems. In this paper, we conduct a systematic review of the existing threats and vulnerabilities in embedded systems based on public available data. Moreover, based on the information, we derive an attack taxonomy for embedded systems. We envision that the findings in this paper provide a valuable insight of the threat landscape facing embedded systems. The knowledge can be used for a better understanding and the identification of security risks in system analysis and design.},
	booktitle = {2015 13th {Annual} {Conference} on {Privacy}, {Security} and {Trust} ({PST})},
	author = {Papp, Dorottya and Ma, Zhendong and Buttyan, Levente},
	month = jul,
	year = {2015},
	keywords = {Authentication, Cryptography, Embedded systems, Protocols, Taxonomy},
	pages = {145--152},
}

@inproceedings{papp_embedded_2015-1,
	title = {Embedded systems security: {Threats}, vulnerabilities, and attack taxonomy},
	shorttitle = {Embedded systems security},
	doi = {10.1109/PST.2015.7232966},
	abstract = {Embedded systems are the driving force for technological development in many domains such as automotive, healthcare, and industrial control in the emerging post-PC era. As more and more computational and networked devices are integrated into all aspects of our lives in a pervasive and “invisible” way, security becomes critical for the dependability of all smart or intelligent systems built upon these embedded systems. In this paper, we conduct a systematic review of the existing threats and vulnerabilities in embedded systems based on public available data. Moreover, based on the information, we derive an attack taxonomy for embedded systems. We envision that the findings in this paper provide a valuable insight of the threat landscape facing embedded systems. The knowledge can be used for a better understanding and the identification of security risks in system analysis and design.},
	booktitle = {2015 13th {Annual} {Conference} on {Privacy}, {Security} and {Trust} ({PST})},
	author = {Papp, Dorottya and Ma, Zhendong and Buttyan, Levente},
	month = jul,
	year = {2015},
	keywords = {Authentication, Cryptography, Embedded systems, Protocols, Taxonomy},
	pages = {145--152},
}

@article{barrett_water_nodate,
	title = {Water {Supply} {Hacks} {Are} a {Serious} {Threat}—and {Only} {Getting} {Worse}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/threat-to-water-supply-is-real-and-only-getting-worse/},
	abstract = {An ex-employee allegedly tampered with a Kansas water system. It was too easy, and it's happening too often.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Barrett, Brian},
	note = {Section: tags},
	keywords = {critical infrastructure, cybersecurity, hacks, vulnerabilities},
}

@article{newman_decades-old_nodate,
	title = {Decades-{Old} {Code} {Is} {Putting} {Millions} of {Critical} {Devices} at {Risk}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/urgent-11-ipnet-vulnerable-devices/},
	abstract = {Nearly two decades ago, a company called Interpeak created a network protocol that became an industry standard. It also had severe bugs that are only now coming to light.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Newman, Lily Hay},
	note = {Section: tags},
	keywords = {bugs, critical infrastructure, iot, vulnerabilities},
}

@article{stewart_teslas_2018,
	title = {Tesla's {Self}-{Driving} {Autopilot} {Involved} in {Another} {Deadly} {Crash}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/tesla-autopilot-self-driving-crash-california/},
	abstract = {The automaker says its semi-autonomous system was engaged when a Model X SUV hit a freeway barrier last week in California, killing the driver.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Stewart, Jack},
	year = {2018},
	note = {Section: tags},
	keywords = {elon musk, self-driving cars, tesla},
}

@misc{noauthor_hackers_2015,
	title = {Hackers {Cut} a {Corvette}'s {Brakes} {Via} a {Common} {Car} {Gadget} {\textbar} {WIRED}},
	url = {http://archive.fo/z1qCi},
	abstract = {The free dongles that insurance companies ask customers to plug into their dashes could expose your car to hackers.},
	urldate = {2022-01-05},
	journal = {archive.fo},
	month = aug,
	year = {2015},
}

@article{boudette_us_2021,
	chapter = {Business},
	title = {U.{S}. {Will} {Investigate} {Tesla}’s {Autopilot} {System} {Over} {Crashes} {With} {Emergency} {Vehicles}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2021/08/16/business/tesla-autopilot-nhtsa.html},
	abstract = {It will be the broadest look yet at Tesla’s assisted-driving technology. The National Highway Traffic Safety Administration has the authority to force a recall or require new safety features.},
	language = {en-US},
	urldate = {2022-01-04},
	journal = {The New York Times},
	author = {Boudette, Neal E. and Chokshi, Niraj},
	month = aug,
	year = {2021},
	keywords = {Automobile Safety Features and Defects, Deaths (Fatalities), Driver Distraction and Fatigue, Driverless and Semiautonomous Vehicles, Electric and Hybrid Vehicles, Musk, Elon, National Highway Traffic Safety Administration, Tesla Motors Inc, Traffic Accidents and Safety},
}

@inproceedings{li_have_2006,
	address = {New York, NY, USA},
	series = {{ASID} '06},
	title = {Have things changed now? an empirical study of bug characteristics in modern open source software},
	isbn = {978-1-59593-576-2},
	shorttitle = {Have things changed now?},
	url = {https://doi.org/10.1145/1181309.1181314},
	doi = {10.1145/1181309.1181314},
	abstract = {Software errors are a major cause for system failures. To effectively design tools and support for detecting and recovering from software failures requires a deep understanding of bug characteristics. Recently, software and its development process have significantly changed in many ways, including more help from bug detection tools, shift towards multi-threading architecture, the open-source development paradigm and increasing concerns about security and user-friendly interface. Therefore, results from previous studies may not be applicable to present software. Furthermore, many new aspects such as security, concurrency and open-source-related characteristics have not well studied. Additionally, previous studies were based on a small number of bugs, which may lead to non-representative results.To investigate the impacts of the new factors on software errors, we analyze bug characteristics by first sampling hundreds of real world bugs in two large, representative open-source projects. To validate the representativeness of our results, we use natural language text classification techniques and automatically analyze around 29, 000 bugs from the Bugzilla databases of the software.Our study has discovered several new interesting characteristics: (1) memory-related bugs have decreased because quite a few effective detection tools became available recently; (2) surprisingly, some simple memory-related bugs such as NULL pointer dereferences that should have been detected by existing tools in development are still a major component, which indicates that the tools have not been used with their full capacity; (3) semantic bugs are the dominant root causes, as they are application specific and difficult to fix, which suggests that more efforts should be put into detecting and fixing them; (4) security bugs are increasing, and the majority of them cause severe impacts.},
	urldate = {2022-01-04},
	booktitle = {Proceedings of the 1st workshop on {Architectural} and system support for improving software dependability},
	publisher = {Association for Computing Machinery},
	author = {Li, Zhenmin and Tan, Lin and Wang, Xuanhui and Lu, Shan and Zhou, Yuanyuan and Zhai, Chengxiang},
	month = oct,
	year = {2006},
	keywords = {bug characteristics, bug detection, empirical study, open source, security},
	pages = {25--33},
}

@inproceedings{jimenez_empirical_2016,
	title = {An {Empirical} {Analysis} of {Vulnerabilities} in {OpenSSL} and the {Linux} {Kernel}},
	doi = {10.1109/APSEC.2016.025},
	abstract = {Vulnerabilities are one of the main concerns faced by practitioners when working with security critical applications. Unfortunately, developers and security teams, even experienced ones, fail to identify many of them with severe consequences. Vulnerabilities are hard to discover since they appear in various forms, caused by many different issues and their identification requires an attacker's mindset. In this paper, we aim at increasing the understanding of vulnerabilities by investigating their characteristics on two major open-source software systems, i.e., the Linux kernel and OpenSSL. In particular, we seek to analyse and build a profile for vulnerable code, which can ultimately help researchers in building automated approaches like vulnerability prediction models. Thus, we examine the location, criticality and category of vulnerable code along with its relation with software metrics. To do so, we collect more than 2,200 vulnerable files accounting for 863 vulnerabilities and compute more than 35 software metrics. Our results indicate that while 9 Common Weakness Enumeration (CWE) types of vulnerabilities are prevalent, only 3 of them are critical in OpenSSL and 2 of them in the Linux kernel. They also indicate that different types of vulnerabilities have different characteristics, i.e., metric profiles, and that vulnerabilities of the same type have different profiles in the two projects we examined. We also found that the file structure of the projects can provide useful information related to the vulnerabilities. Overall, our results demonstrate the need for making project specific approaches that focus on specific types of vulnerabilities.},
	booktitle = {2016 23rd {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Jimenez, Matthieu and Papadakis, Mike and Traon, Yves Le},
	month = dec,
	year = {2016},
	note = {ISSN: 1530-1362},
	keywords = {Common Vulnerability Exposures, Kernel, Linux, Predictive models, Security, Software Metrics, Software Security, Software metrics, Vulnerabilities},
	pages = {105--112},
}

@article{tan_bug_2014,
	title = {Bug characteristics in open source software},
	volume = {19},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-013-9258-8},
	doi = {10.1007/s10664-013-9258-8},
	abstract = {To design effective tools for detecting and recovering from software failures requires a deep understanding of software bug characteristics. We study software bug characteristics by sampling 2,060 real world bugs in three large, representative open-source projects—the Linux kernel, Mozilla, and Apache. We manually study these bugs in three dimensions—root causes, impacts, and components. We further study the correlation between categories in different dimensions, and the trend of different types of bugs. The findings include: (1) semantic bugs are the dominant root cause. As software evolves, semantic bugs increase, while memory-related bugs decrease, calling for more research effort to address semantic bugs; (2) the Linux kernel operating system (OS) has more concurrency bugs than its non-OS counterparts, suggesting more effort into detecting concurrency bugs in operating system code; and (3) reported security bugs are increasing, and the majority of them are caused by semantic bugs, suggesting more support to help developers diagnose and fix security bugs, especially semantic security bugs. In addition, to reduce the manual effort in building bug benchmarks for evaluating bug detection and diagnosis tools, we use machine learning techniques to classify 109,014 bugs automatically.},
	language = {en},
	number = {6},
	urldate = {2022-01-04},
	journal = {Empirical Software Engineering},
	author = {Tan, Lin and Liu, Chen and Li, Zhenmin and Wang, Xuanhui and Zhou, Yuanyuan and Zhai, Chengxiang},
	month = dec,
	year = {2014},
	pages = {1665--1705},
}

@inproceedings{chen_data-driven_2003,
	title = {A {Data}-{Driven} {Finite} {State} {Machine} {Model} for {Analyzing} {Security} {Vulnerabilities}.},
	booktitle = {{DSN}},
	publisher = {Citeseer},
	author = {Chen, Shuo and Kalbarczyk, Zbigniew and Xu, Jun and Iyer, Ravishankar K.},
	year = {2003},
	pages = {605--614},
}

@inproceedings{turns_integrating_2014,
	title = {Integrating {Reflection} into {Engineering} {Education}},
	url = {http://peer.asee.org/20668},
	doi = {10.18260/1-2--20668},
	language = {en},
	urldate = {2021-12-14},
	booktitle = {2014 {ASEE} {Annual} {Conference} \& {Exposition} {Proceedings}},
	author = {Turns, Jennifer and Sattler, Brook and Yasuhara, Ken and Borgford-Parnell, Jim and Atman, Cynthia},
	year = {2014},
}

@article{chen_security_2006,
	title = {Security {Vulnerabilities}: {From} {Analysis} to {Detection} and {Masking} {Techniques}},
	volume = {94},
	issn = {1558-2256},
	shorttitle = {Security {Vulnerabilities}},
	doi = {10.1109/JPROC.2005.862473},
	abstract = {This paper presents a study that uses extensive analysis of real security vulnerabilities to drive the development of: 1) runtime techniques for detection/masking of security attacks and 2) formal source code analysis methods to enable identification and removal of potential security vulnerabilities. A finite-state machine (FSM) approach is employed to decompose programs into multiple elementary activities, making it possible to extract simple predicates to be ensured for security. The FSM analysis pinpoints common characteristics among a broad range of security vulnerabilities: predictable memory layout, unprotected control data, and pointer taintedness. We propose memory layout randomization and control data randomization to mask the vulnerabilities at runtime. We also propose a static analysis approach to detect potential security vulnerabilities using the notion of pointer taintedness.},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Chen, S. and Xu, J. and Kalbarczyk, Z. and Iyer, K.},
	month = feb,
	year = {2006},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Buffer overflow, Computer science, Computer security, Data analysis, Data mining, Data security, Databases, Gain measurement, Protection, Runtime, randomization, security attack, vulnerability},
	pages = {407--418},
}

@article{oconnor_weekend_2019,
	chapter = {Well},
	title = {In {Weekend} {Outage}, {Diabetes} {Monitors} {Fail} to {Send} {Crucial} {Alerts}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2019/12/02/well/live/Dexcom-G6-diabetes-monitor-outage.html},
	abstract = {Parents who use the Dexcom G6 depend on alarms on their phones if their children’s blood sugar levels are dangerous. They say the outage put them at risk.},
	language = {en-US},
	urldate = {2022-01-04},
	journal = {The New York Times},
	author = {O’Connor, Anahad},
	month = dec,
	year = {2019},
	keywords = {Children and Childhood, Computer Network Outages, DexCom Inc, Diabetes, Mobile Applications, Parenting, Smartphones, Wearable Computing},
}

@article{saad2018pendekatan,
	title = {Named entity recognition approach for malay crime news retrieval},
	volume = {18},
	doi = {https://doi.org/10.17576/gema-2018-1804-14},
	number = {4},
	journal = {GEMA Online® Journal of Language Studies},
	author = {Saad, Saidah and Mansor, Mohamed Kamil},
	year = {2018},
	keywords = {IoTFailurePaper},
}

@article{foufi_mining_2019,
	title = {Mining of {Textual} {Health} {Information} from {Reddit}: {Analysis} of {Chronic} {Diseases} {With} {Extracted} {Entities} and {Their} {Relations}},
	volume = {21},
	issn = {1438-8871},
	shorttitle = {Mining of {Textual} {Health} {Information} from {Reddit}},
	url = {http://www.jmir.org/2019/6/e12876/},
	doi = {10.2196/12876},
	abstract = {Background: Social media platforms constitute a rich data source for natural language processing tasks such as named entity recognition, relation extraction, and sentiment analysis. In particular, social media platforms about health provide a different insight into patient’s experiences with diseases and treatment than those found in the scientific literature.
Objective: This paper aimed to report a study of entities related to chronic diseases and their relation in user-generated text posts. The major focus of our research is the study of biomedical entities found in health social media platforms and their relations and the way people suffering from chronic diseases express themselves.
Methods: We collected a corpus of 17,624 text posts from disease-specific subreddits of the social news and discussion website Reddit. For entity and relation extraction from this corpus, we employed the PKDE4J tool developed by Song et al (2015). PKDE4J is a text mining system that integrates dictionary-based entity extraction and rule-based relation extraction in a highly flexible and extensible framework.
Results: Using PKDE4J, we extracted 2 types of entities and relations: biomedical entities and relations and subject-predicate-object entity relations. In total, 82,138 entities and 30,341 relation pairs were extracted from the Reddit dataset. The most highly mentioned entities were those related to oncological disease (2884 occurrences of cancer) and asthma (2180 occurrences). The relation pair anatomy-disease was the most frequent (5550 occurrences), the highest frequent entities in this pair being cancer and lymph. The manual validation of the extracted entities showed a very good performance of the system at the entity extraction task (3682/5151, 71.48\% extracted entities were correctly labeled).
Conclusions: This study showed that people are eager to share their personal experience with chronic diseases on social media platforms despite possible privacy and security issues. The results reported in this paper are promising and demonstrate the need for more in-depth studies on the way patients with chronic diseases express themselves on social media platforms.},
	language = {en},
	number = {6},
	urldate = {2022-01-04},
	journal = {Journal of Medical Internet Research},
	author = {Foufi, Vasiliki and Timakum, Tatsawan and Gaudet-Blavignac, Christophe and Lovis, Christian and Song, Min},
	month = jun,
	year = {2019},
	keywords = {IoTFailurePaper},
	pages = {e12876},
}

@article{drury_survey_2019,
	title = {A survey of the applications of text mining for agriculture},
	volume = {163},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169919302960},
	doi = {10.1016/j.compag.2019.104864},
	abstract = {Agricultural researchers, in common with other domains, have recently began to have access to large collections of agricultural texts such as scientific papers and news stories. These texts can be analysed with text mining techniques to resolve agricultural problems or extract knowledge. Despite the potential of these techniques, text mining is a relatively underused technique in the agricultural domain. Therefore, this survey is intended to provide a current state of the art survey of the application of text mining techniques to agricultural problems.},
	language = {en},
	urldate = {2022-01-04},
	journal = {Computers and Electronics in Agriculture},
	author = {Drury, Brett and Roche, Mathieu},
	month = aug,
	year = {2019},
	keywords = {Agriculture, Information extraction, Information retrieval, IoTFailurePaper, Sentiment analysis, Survey, Text mining},
	pages = {104864},
}

@misc{noauthor_as_nodate,
	title = {As {U}.{S}. {Investigates} {Fatal} {Tesla} {Crash}, {Company} {Defends} {Autopilot} {System} - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2016/07/13/business/tesla-autopilot-fatal-crash-investigation.html},
	urldate = {2022-01-03},
}

@misc{noauthor_inside_nodate,
	title = {Inside a {Fatal} {Tesla} {Autopilot} {Accident}: ‘{It} {Happened} {So} {Fast}’ - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/08/17/business/tesla-autopilot-accident.html},
	urldate = {2022-01-03},
}

@misc{noauthor_boeing_nodate,
	title = {Boeing {Starliner} {Flight}’s {Flaws} {Show} ‘{Fundamental} {Problem},’ {NASA} {Says} - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2020/02/07/science/boeing-starliner-nasa.html},
	urldate = {2022-01-03},
}

@article{bilton_nest_2016,
	chapter = {Style},
	title = {Nest {Thermostat} {Glitch} {Leaves} {Users} in the {Cold}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2016/01/14/fashion/nest-thermostat-glitch-battery-dies-software-freeze.html},
	abstract = {As more “smart” devices enter our lives, small software bugs can cause big headaches.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Bilton, Nick},
	month = jan,
	year = {2016},
	keywords = {Bilton, Nick, Defective Products, Home Automation and Smart Homes, Nest Labs Inc, Thermostats},
}

@misc{noauthor_mt_nodate,
	title = {The {M}.{T}.{A}. {Is} {Breached} by {Hackers} as {Cyberattacks} {Surge} - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/06/02/nyregion/mta-cyber-attack.html},
	urldate = {2022-01-03},
}

@article{sanger_pipeline_2021,
	chapter = {U.S.},
	title = {Pipeline {Attack} {Yields} {Urgent} {Lessons} {About} {U}.{S}. {Cybersecurity}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2021/05/14/us/politics/pipeline-hack.html},
	abstract = {The hack underscored how vulnerable government and industry are to even basic assaults on computer networks.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Sanger, David E. and Perlroth, Nicole},
	month = may,
	year = {2021},
	keywords = {Biden, Joseph R Jr, Colonial Pipeline Co, Cyberwarfare and Defense, DarkSide (Hacking Group), Gordon, Susan M, Pipelines, United States Cyber Command, United States Politics and Government},
}

@article{perlroth_cyberattacks_2018,
	chapter = {U.S.},
	title = {Cyberattacks {Put} {Russian} {Fingers} on the {Switch} at {Power} {Plants}, {U}.{S}. {Says}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2018/03/15/us/politics/russia-cyberattacks.html},
	abstract = {In the last year, Russian hackers have gone from infiltrating business networks of energy, water and nuclear plants to worming their way into control rooms.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Perlroth, Nicole and Sanger, David E.},
	month = mar,
	year = {2018},
	keywords = {Cyberwarfare and Defense, Electric Light and Power, Nuclear Energy, Russia, Russian Interference in 2016 US Elections and Ties to Trump Associates, United States, United States International Relations, United States Politics and Government, Water},
}

@article{robles_dangerous_2021,
	chapter = {U.S.},
	title = {‘{Dangerous} {Stuff}’: {Hackers} {Tried} to {Poison} {Water} {Supply} of {Florida} {Town}},
	issn = {0362-4331},
	shorttitle = {‘{Dangerous} {Stuff}’},
	url = {https://www.nytimes.com/2021/02/08/us/oldsmar-florida-water-supply-hack.html},
	abstract = {For years, cybersecurity experts have warned of attacks on small municipal systems. In Oldsmar, Fla., the levels of lye were changed and could have sickened residents.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Robles, Frances and Perlroth, Nicole},
	month = feb,
	year = {2021},
	keywords = {Cyberwarfare and Defense, Florida, Gualtieri, Bob, Tampa (Fla), Water},
}

@article{bowles_thermostats_2018,
	chapter = {Technology},
	title = {Thermostats, {Locks} and {Lights}: {Digital} {Tools} of {Domestic} {Abuse}},
	issn = {0362-4331},
	shorttitle = {Thermostats, {Locks} and {Lights}},
	url = {https://www.nytimes.com/2018/06/23/technology/smart-home-devices-domestic-abuse.html},
	abstract = {Internet-connected home devices that are marketed as the newest conveniences are also being used to harass, monitor and control.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Bowles, Nellie},
	month = jun,
	year = {2018},
	keywords = {Domestic Violence, Home Appliances, Home Automation and Smart Homes, Mobile Applications, Women and Girls},
}

@misc{noauthor_tesla_nodate,
	title = {Tesla {Autopilot} {Faces} {U}.{S}. {Inquiry} {After} {Series} of {Crashes} - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/08/16/business/tesla-autopilot-nhtsa.html},
	urldate = {2022-01-03},
}

@article{caron_more_2020,
	chapter = {Parenting},
	title = {‘{More} {Anxiety} {Than} {Relief}’: {Baby} {Monitors} {That} {Track} {Vital} {Signs} {Are} {Raising} {Questions}},
	issn = {0362-4331},
	shorttitle = {‘{More} {Anxiety} {Than} {Relief}’},
	url = {https://www.nytimes.com/2020/04/17/parenting/owlet-baby-monitor.html},
	abstract = {After a popular app stopped receiving medical data, some families wondered how reliable monitoring is.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Caron, Christina},
	month = apr,
	year = {2020},
	keywords = {Babies and Infants, Children and Childhood, Company Reports, Cribs (Baby Beds), Infant Mortality, Mobile Applications, Parenting, Sleep, Sudden Infant Death Syndrome},
}

@article{sharkey_identification_2016,
	title = {Identification and {Classification} of {Restoration} {Interdependencies} in the {Wake} of {Hurricane} {Sandy}},
	volume = {22},
	issn = {1076-0342, 1943-555X},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29IS.1943-555X.0000262},
	doi = {10.1061/(ASCE)IS.1943-555X.0000262},
	abstract = {This paper introduces the new concept of restoration interdependencies that exist among infrastructures during their restoration efforts after an extreme event. Restoration interdependencies occur whenever a restoration task in one infrastructure is impacted by a restoration task, or lack thereof, in another infrastructure. This work identifies examples of observed restoration interdependencies during the restoration efforts after Hurricane Sandy as reported by major newspapers in the affected areas. A classification scheme for the observed restoration interdependencies is provided that includes five distinct classes: traditional precedence, effectiveness precedence, options precedence, time-sensitive options, and competition for resources. This work provides an overview of these different classes by providing the frequency they were observed, the infrastructures involved with the restoration interdependency, and a discussion of their potential impact on interdependent infrastructure restoration. The analysis is important because it provides a new understanding of how the restoration efforts of infrastructures are linked across systems and motivates the need for potential information-sharing in interdependent infrastructure restoration. DOI: 10.1061/(ASCE)IS.1943-555X.0000262. © 2015 American Society of Civil Engineers.},
	language = {en},
	number = {1},
	urldate = {2022-01-03},
	journal = {Journal of Infrastructure Systems},
	author = {Sharkey, Thomas C. and Nurre, Sarah G. and Nguyen, Huy and Chow, Joe H. and Mitchell, John E. and Wallace, William A.},
	month = mar,
	year = {2016},
	keywords = {IoTFailurePaper},
	pages = {04015007},
}

@article{zhou_delineating_2020,
	title = {Delineating {Infrastructure} {Failure} {Interdependencies} and {Associated} {Stakeholders} through {News} {Mining}: {The} {Case} of {Hong} {Kong}’s {Water} {Pipe} {Bursts}},
	volume = {36},
	issn = {0742-597X, 1943-5479},
	shorttitle = {Delineating {Infrastructure} {Failure} {Interdependencies} and {Associated} {Stakeholders} through {News} {Mining}},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29ME.1943-5479.0000821},
	doi = {10.1061/(ASCE)ME.1943-5479.0000821},
	abstract = {The failure of one infrastructure system could trigger cascading impacts on other interdependent infrastructures. In order to improve the management of diverse infrastructure systems, decision makers should be mindful of infrastructure failure interdependencies (IFIs) and associated stakeholders when the failure of a particular infrastructure occurs. Currently, approaches to identify IFIs and associated stakeholders rely heavily on expert knowledge or limited historical records. To complement the shortage of empirical evidence, a synthetic approach that exploits media news is proposed to delineate the patterns of IFIs and stakeholders associated with the initial infrastructure failure. The integrated approach collects and cleanses the corpus from news articles, prepares the domain knowledge components, recognizes the affected infrastructure and stakeholder entities, verifies the information captured, applies association rule learning to discover IFI chains, and adopts a network analysis to depict associated stakeholders. Incidents of bursting water pipes in Hong Kong are used as a case study to demonstrate the proposed approach with 2,828 news articles being collected and analyzed. Altogether, 18 one-order or second-order IFI rules are identified. Besides, 25 associated stakeholders are delineated from the news, and they are divided into three tiers according to their degree centralities. The findings provide insightful information to policymakers for helping to respond to the cascading effects among infrastructures and coordinate a wide spectrum of stakeholders who might be embroiled. DOI: 10.1061/(ASCE)ME.1943-5479.0000821. © 2020 American Society of Civil Engineers.},
	language = {en},
	number = {5},
	urldate = {2022-01-02},
	journal = {Journal of Management in Engineering},
	author = {Zhou, Shenghua and Ng, S. Thomas and Yang, Yifan and Xu, J. Frank},
	month = sep,
	year = {2020},
	keywords = {IoTFailurePaper},
	pages = {04020060},
}

@inproceedings{jimenez_empirical_2016-1,
	title = {An {Empirical} {Analysis} of {Vulnerabilities} in {OpenSSL} and the {Linux} {Kernel}},
	doi = {10.1109/APSEC.2016.025},
	abstract = {Vulnerabilities are one of the main concerns faced by practitioners when working with security critical applications. Unfortunately, developers and security teams, even experienced ones, fail to identify many of them with severe consequences. Vulnerabilities are hard to discover since they appear in various forms, caused by many different issues and their identification requires an attacker's mindset. In this paper, we aim at increasing the understanding of vulnerabilities by investigating their characteristics on two major open-source software systems, i.e., the Linux kernel and OpenSSL. In particular, we seek to analyse and build a profile for vulnerable code, which can ultimately help researchers in building automated approaches like vulnerability prediction models. Thus, we examine the location, criticality and category of vulnerable code along with its relation with software metrics. To do so, we collect more than 2,200 vulnerable files accounting for 863 vulnerabilities and compute more than 35 software metrics. Our results indicate that while 9 Common Weakness Enumeration (CWE) types of vulnerabilities are prevalent, only 3 of them are critical in OpenSSL and 2 of them in the Linux kernel. They also indicate that different types of vulnerabilities have different characteristics, i.e., metric profiles, and that vulnerabilities of the same type have different profiles in the two projects we examined. We also found that the file structure of the projects can provide useful information related to the vulnerabilities. Overall, our results demonstrate the need for making project specific approaches that focus on specific types of vulnerabilities.},
	booktitle = {2016 23rd {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Jimenez, Matthieu and Papadakis, Mike and Traon, Yves Le},
	month = dec,
	year = {2016},
	note = {ISSN: 1530-1362},
	keywords = {Common Vulnerability Exposures, Kernel, Linux, Predictive models, Security, Software Metrics, Software Security, Software metrics, Vulnerabilities},
	pages = {105--112},
}

@article{avizienis_basic_2004,
	title = {Basic concepts and taxonomy of dependable and secure computing},
	volume = {1},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2004.2},
	abstract = {This paper gives the main definitions relating to dependability, a generic concept including a special case of such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.},
	number = {1},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Avizienis, A. and Laprie, J.-C. and Randell, B. and Landwehr, C.},
	month = jan,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Availability, Books, Communication system security, Fault tolerance, Index Terms- Dependability, Maintenance, Safety, Standardization, Taxonomy, Uncertainty, attacks, errors, failures, fault forecasting., fault removal, fault tolerance, faults, security, trust, vulnerabilities},
	pages = {11--33},
}

@article{aslam_use_nodate,
	title = {Use of {A} {Taxonomy} of {Security} {Faults}},
	language = {en},
	author = {Aslam, Taimur and Krsul, Ivan and Spafford, Eugene H},
	pages = {12},
}

@article{landwehr_taxonomy_1994,
	title = {A taxonomy of computer program security flaws},
	volume = {26},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/185403.185412},
	doi = {10.1145/185403.185412},
	abstract = {An organized record of actual flaws can be useful to computer system designers, programmers, analysts, administrators, and users. This survey provides a taxonomy for computer program security flaws, with an Appendix that documents 50 actual security flaws. These flaws have all been described previously in the open literature, but in widely separated places. For those new to the field of computer security, they provide a good introduction to the characteristics of security flaws and how they can arise. Because these flaws were not randomly selected from a valid statistical sample of such flaws, we make no strong claims concerning the likely distribution of actual security flaws within the taxonomy. However, this method of organizing security flaw data can help those who have custody of more representative samples to organize them and to focus their efforts to remove and, eventually, to prevent the introduction of security flaws.},
	number = {3},
	urldate = {2021-12-31},
	journal = {ACM Computing Surveys},
	author = {Landwehr, Carl E. and Bull, Alan R. and McDermott, John P. and Choi, William S.},
	month = sep,
	year = {1994},
	keywords = {error/defect classification, security flaw, taxonomy},
	pages = {211--254},
}

@article{aslam_use_nodate-1,
	title = {Use of {A} {Taxonomy} of {Security} {Faults}},
	language = {en},
	author = {Aslam, Taimur and Krsul, Ivan and Spafford, Eugene H},
	pages = {12},
}

@book{leveson1995safeware,
	address = {New York, NY, USA},
	title = {Safeware: {System} safety and computers},
	isbn = {0-201-11972-2},
	publisher = {ACM},
	author = {Leveson, Nancy G.},
	year = {1995},
	keywords = {IoTFailurePaper},
}

@incollection{gervasi_pathology_2021,
	address = {Cham},
	title = {The {Pathology} of {Failures} in {IoT} {Systems}},
	volume = {12957},
	isbn = {978-3-030-87012-6 978-3-030-87013-3},
	url = {https://link.springer.com/10.1007/978-3-030-87013-3_33},
	abstract = {The presence of faults is inevitable in the Internet of Things (IoT) systems. Dependability in these systems is challenging due to the increasing level of dynamicity, heterogeneity, and complexity. IoT connects anything, anytime, and everywhere, introducing a complex relationship of interdependence, generating an increase in the susceptibility of the propagation of failures. The purpose of this study is to propose a pathology of failure in IoT Systems, exploring and characterizing faults, errors, failures, and their eﬀects. This study investigates and classiﬁes the source of faults, deﬁnes a taxonomy of the types of faults prone to happen, and deﬁnes the failure propagation model. As a result, the pathology establishes a common reference for fault, errors, and failures to be used by researchers and practitioners to improve tools for fault detection, fault diagnosis, fault tolerance, and fault handling in IoT Systems. This paper also proposes a failure propagation model for IoT systems that identify diﬀerent combinations, paths, and fault-failure propagation eﬀects.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2021},
	publisher = {Springer International Publishing},
	author = {Melo, Mário and Aquino, Gibeon},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Garau, Chiara and Blečić, Ivan and Taniar, David and Apduhan, Bernady O. and Rocha, Ana Maria A. C. and Tarantino, Eufemia and Torre, Carmelo Maria},
	year = {2021},
	doi = {10.1007/978-3-030-87013-3_33},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {IoTFailurePaper},
	pages = {437--452},
}

@inproceedings{makhshari_iot_2021,
	address = {Madrid, ES},
	title = {{IoT} {Bugs} and {Development} {Challenges}},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9402092/},
	doi = {10.1109/ICSE43902.2021.00051},
	abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners’ point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We collected 5,565 bug reports from 91 representative IoT project repositories and categorized a random sample of 323 based on the observed failures, root causes, and the locations of the faulty components. In addition, we conducted nine interviews with IoT experts to uncover more details about IoT bugs and to gain insight into IoT developers’ challenges. Lastly, we surveyed 194 IoT developers to validate our findings and gain further insights. We propose the first bug taxonomy for IoT systems based on our results. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
	language = {en},
	urldate = {2021-07-13},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Makhshari, Amir and Mesbah, Ali},
	year = {2021},
	keywords = {IoTFailurePaper},
	pages = {460--472},
}

@inproceedings{luo_internet_2020,
	title = {An {Internet} of {Things} ({loT}) {Perspective} of {Understanding} the {Boeing} 737 {MAX} {Crash}},
	doi = {10.1109/PHM-Shanghai49105.2020.9280967},
	abstract = {Recently, two serious airplane crashes involving Ethiopian Airlines and Lion Airlines have greatly impacted the aviation industry, and sparked worldwide discussions as well as investigations from government agencies to original equipment manufacturers. This paper first briefly describes the two airplane crashes and presents a summary report. Then the possible causes of the crashes are analyzed from the design of the stall protection system and other aspects, including airworthiness certification, aircraft management and support. Especially, the crashes are analyzed from the Internet of Things (IoT) perspective and system reliability recommendations are presented. Lastly, discussions concerning the balance and integration between IoT systems and human beings are presented based on the multiple investigation outcomes.},
	booktitle = {2020 {Global} {Reliability} and {Prognostics} and {Health} {Management} ({PHM}-{Shanghai})},
	author = {Luo, Pan and Li, Meiyan and Li, Zhaojun Steven},
	month = oct,
	year = {2020},
	keywords = {Accidents, Airplanes, Boeing 737 MAX, Government, Industries, Internet of Things, IoT, IoTFailurePaper, Prognostics and health management, Reliability, crash, reliability},
	pages = {1--8},
}

@article{bagchi_new_2020,
	title = {New {Frontiers} in {IoT}: {Networking}, {Systems}, {Reliability}, and {Security} {Challenges}},
	volume = {7},
	issn = {2327-4662, 2372-2541},
	shorttitle = {New {Frontiers} in {IoT}},
	url = {https://ieeexplore.ieee.org/document/9136673/},
	doi = {10.1109/JIOT.2020.3007690},
	abstract = {The ﬁeld of IoT has blossomed and is positively inﬂuencing many application domains. In this article, we bring out the unique challenges this ﬁeld poses to research in computer systems and networking. The unique challenges arise from the unique characteristics of IoT systems such as the diversity of application domains where they are used and the increasingly demanding protocols they are being called upon to run (such as video and LIDAR processing) on constrained resources (on-node and network). We show how these open challenges can beneﬁt from foundations laid in other areas, such as ﬁfth-generation network cellular protocols, machine learning model reduction, and device–edge–cloud ofﬂoading. We then discuss the unique challenges for reliability, security, and privacy posed by IoT systems due to their salient characteristics which include heterogeneity of devices and protocols, dependence on the physical environment, and the close coupling with humans. We again show how open research challenges beneﬁt from the reliability, security, and privacy advancements in other areas. We conclude by providing a vision for a desirable end state for IoT systems.},
	language = {en},
	number = {12},
	urldate = {2021-09-21},
	journal = {IEEE Internet of Things Journal},
	author = {Bagchi, Saurabh and Abdelzaher, Tarek F. and Govindan, Ramesh and Shenoy, Prashant and Atrey, Akanksha and Ghosh, Pradipta and Xu, Ran},
	year = {2020},
	keywords = {IoTFailurePaper},
	pages = {11330--11346},
}

@article{eschelbeck_laws_2005,
	title = {The {Laws} of {Vulnerabilities}: {Which} security vulnerabilities really matter?},
	volume = {10},
	issn = {13634127},
	shorttitle = {The {Laws} of {Vulnerabilities}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1363412705000646},
	doi = {10.1016/j.istr.2005.09.005},
	abstract = {New security vulnerabilities are discovered on a daily basis. With each new announcement, the same questions arise. How signiﬁcant is this vulnerability? How prevalent? How easy is it to exploit? Due to a lack of global vulnerability data, answers are hard to ﬁnd and risk rating is even more difﬁcult. The Laws of Vulnerabilities are the conclusions of analyzing statistical vulnerability information over a three-year period. Those vulnerabilities have been identiﬁed in the real world across hundreds of thousands of systems and networks. These data are not identiﬁable to individual users or systems. However, it provides signiﬁcant statistical data for research and analysis, which enabled us to deﬁne and publish the Laws of Vulnerabilities (http://www.qualys.com/research/rnd/vulnlaws/).},
	language = {en},
	number = {4},
	urldate = {2021-12-28},
	journal = {Information Security Technical Report},
	author = {Eschelbeck, Gerhard},
	month = jan,
	year = {2005},
	pages = {213--219},
}

@inproceedings{cai_understanding_2019,
	address = {New York, NY, USA},
	series = {{APSys} '19},
	title = {Understanding {Security} {Vulnerabilities} in {File} {Systems}},
	isbn = {978-1-4503-6893-3},
	url = {https://doi.org/10.1145/3343737.3343753},
	doi = {10.1145/3343737.3343753},
	abstract = {File systems have been developed for decades with the security-critical foundation provided by operating systems. However, they are still vulnerable to malware attacks and software defects. In this paper, we undertake the first attempt to systematically understand the security vulnerabilities in various file systems. We conduct an empirical study of 157 real cases reported in Common Vulnerabilities and Exposures (CVE). We characterize the file system vulnerabilities in different dimensions that include the common vulnerabilities leveraged by adversaries to initiate their attacks, their exploitation procedures, root causes, consequences, and mitigation approaches. We believe the insights derived from this study have broad implications related to the further enhancement of the security aspect of file systems, and the associated vulnerability detection tools.},
	urldate = {2021-12-27},
	booktitle = {Proceedings of the 10th {ACM} {SIGOPS} {Asia}-{Pacific} {Workshop} on {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cai, Miao and Huang, Hao and Huang, Jian},
	month = aug,
	year = {2019},
	pages = {8--15},
}

@article{noghabi_emerging_2020,
	title = {The {Emerging} {Landscape} of {Edge} {Computing}},
	volume = {23},
	issn = {2375-0529, 2375-0537},
	url = {https://dl.acm.org/doi/10.1145/3400713.3400717},
	doi = {10.1145/3400713.3400717},
	abstract = {On examining current edge computing deployments (edge-site deployments) we find significantly different characteristics than the original vision of edge computing (cyber foraging). In current deployments, edge clusters are deployed by a single entity (not multi-tenant) with enterprise applications (rather than consumer applications) driving the deployments. Table 2 expands on these points.},
	language = {en},
	number = {4},
	urldate = {2021-12-20},
	journal = {GetMobile: Mobile Computing and Communications},
	author = {Noghabi, Shadi A. and Cox, Landon and Agarwal, Sharad and Ananthanarayanan, Ganesh},
	month = may,
	year = {2020},
	pages = {11--20},
}

@article{parker_build_2020,
	title = {Build {It}, {Break} {It}, {Fix} {It}: {Contesting} {Secure} {Development}},
	volume = {23},
	issn = {2471-2566, 2471-2574},
	shorttitle = {Build {It}, {Break} {It}, {Fix} {It}},
	url = {https://dl.acm.org/doi/10.1145/3383773},
	doi = {10.1145/3383773},
	abstract = {Typical security contests focus on breaking or mitigating the impact of buggy systems. We present the Build-it, Break-it, Fix-it (BIBIFI) contest, which aims to assess the ability to securely build software, not just break it. In BIBIFI, teams build specified software with the goal of maximizing correctness, performance, and security. The latter is tested when teams attempt to break other teams’ submissions. Winners are chosen from among the best builders and the best breakers. BIBIFI was designed to be open-ended—teams can use any language, tool, process, and so on, that they like. As such, contest outcomes shed light on factors that correlate with successfully building secure software and breaking insecure software. We ran three contests involving a total of 156 teams and three different programming problems. Quantitative analysis from these contests found that the most efficient build-it submissions used C/C++, but submissions coded in a statically type safe language were 11× less likely to have a security flaw than C/C++ submissions. Break-it teams that were also successful build-it teams were significantly better at finding security bugs.},
	language = {en},
	number = {2},
	urldate = {2021-12-15},
	journal = {ACM Transactions on Privacy and Security},
	author = {Parker, James and Hicks, Michael and Ruef, Andrew and Mazurek, Michelle L. and Levin, Dave and Votipka, Daniel and Mardziel, Piotr and Fulton, Kelsey R.},
	year = {2020},
	keywords = {gamification},
	pages = {1--36},
}

@inproceedings{hornbrook_intercultural_2021,
	title = {An {Intercultural} {Engineering} {Module} for {Software} {Engineers}},
	abstract = {The world is continuing to intertwine more and more because of technology, and, as a result, it is becoming more important to develop software products that are culturally inclusive and acceptable. It is important to educate future software engineers about intercultural engineering, both in terms of requirements and in terms of engineering collaboration. In this poster we discuss a learning module for a software engineering course on Intercultural Engineering. We describe three case studies for use in assignments or in-class discussion.},
	language = {en},
	booktitle = {24th {Annual} {Colloquium} on {International} {Engineering} {Education} ({ACIEE})},
	author = {Hornbrook, Nicole and Davis, James C},
	year = {2021},
}

@techreport{jones2006economics,
	title = {The economics of software maintenance in the twenty first century},
	author = {Jones, Capers},
	year = {2006},
}

@article{reason2006revisiting,
	title = {Revisiting the {Swiss} cheese model of accidents},
	volume = {27},
	number = {4},
	journal = {Journal of Clinical Engineering},
	author = {Reason, J and Hollnagel, E and Paries, J},
	year = {2006},
	pages = {110--115},
}

@article{larouzee_good_2020,
	title = {Good and bad reasons: {The} {Swiss} cheese model and its critics},
	volume = {126},
	issn = {09257535},
	shorttitle = {Good and bad reasons},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925753520300576},
	doi = {10.1016/j.ssci.2020.104660},
	abstract = {This article provides a historical and critical account of James Reason’s contribution to safety research with a focus on the Swiss cheese model (SCM), its developments and its critics. This article shows that the SCM is a product of specific historical circumstances, has been developed over a ten years period following several steps, and has benefited of the direct influence of John Wreathall. Reason took part in intense intellectual debates and publications in the 1980s during which many ideas circulated among researchers, featuring authors as influent as Donald Norman, Jens Rasmussen, Charles Perrow or Barry Turner. The 1980s and 1990s were highly productive from a safety research point of view (e.g. human error, incubation models, high reliability organisation, safety culture) and Reason has considerably influenced it with a rich production of models, based on both research and industrial projects. Historical perspectives offer interesting insights because they can question research, the conditions of its production, its relevance and, sometimes, its success, as for the SCM. But, because of this success, critics have vividly argued about some of the SCM limitations, including its simplistic vision of accidents and its degree of generality. Against these positions, the article develops a ‘critique of the criticism’, and the article concludes that the SCM remains a relevant model because of its systemic foundations and its sustained use in high-risk industries; despite of course, the need to keep imagining alternatives based on the mix of collective empirical, practical and graphical research which was in the SCM background.},
	language = {en},
	urldate = {2021-12-15},
	journal = {Safety Science},
	author = {Larouzee, Justin and Le Coze, Jean-Christophe},
	month = jun,
	year = {2020},
	pages = {104660},
}

@article{jones2009motivating,
	title = {Motivating students to engage in learning: the {MUSIC} model of academic motivation.},
	volume = {21},
	number = {2},
	journal = {International Journal of Teaching and Learning in Higher Education},
	author = {Jones, Brett D},
	year = {2009},
	note = {Publisher: ERIC},
	pages = {272--285},
}

@misc{loukides_thinking_2021,
	title = {Thinking {About} {Glue} – {O}’{Reilly}},
	url = {https://www.oreilly.com/radar/thinking-about-glue/},
	urldate = {2021-12-15},
	author = {Loukides, Mike},
	year = {2021},
}

@inproceedings{baltes_sketches_2014,
	address = {Hong Kong China},
	title = {Sketches and diagrams in practice},
	isbn = {978-1-4503-3056-5},
	url = {https://dl.acm.org/doi/10.1145/2635868.2635891},
	doi = {10.1145/2635868.2635891},
	abstract = {Sketches and diagrams play an important role in the daily work of software developers. In this paper, we investigate the use of sketches and diagrams in software engineering practice. To this end, we used both quantitative and qualitative methods. We present the results of an exploratory study in three companies and an online survey with 394 participants. Our participants included software developers, software architects, project managers, consultants, as well as researchers. They worked in diﬀerent countries and on projects from a wide range of application areas. Most questions in the survey were related to the last sketch or diagram that the participants had created. Contrary to our expectations and previous work, the majority of sketches and diagrams contained at least some UML elements. However, most of them were informal. The most common purposes for creating sketches and diagrams were designing, explaining, and understanding, but analyzing requirements was also named often. More than half of the sketches and diagrams were created on analog media like paper or whiteboards and have been revised after creation. Most of them were used for more than a week and were archived. We found that the majority of participants related their sketches to methods, classes, or packages, but not to source code artifacts with a lower level of abstraction.},
	language = {en},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Baltes, Sebastian and Diehl, Stephan},
	year = {2014},
	pages = {530--541},
}

@inproceedings{shaffer_impact_2021,
	address = {Virtual Event USA},
	title = {The {Impact} of {Programming} {Project} {Milestones} on {Procrastination}, {Project} {Outcomes}, and {Course} {Outcomes}: {A} {Quasi}-{Experimental} {Study} in a {Third}-{Year} {Data} {Structures} {Course}},
	isbn = {978-1-4503-8062-1},
	shorttitle = {The {Impact} of {Programming} {Project} {Milestones} on {Procrastination}, {Project} {Outcomes}, and {Course} {Outcomes}},
	url = {https://dl.acm.org/doi/10.1145/3408877.3432356},
	doi = {10.1145/3408877.3432356},
	abstract = {When faced with a large and complex project for the first time, students face numerous self-regulatory challenges that they may be ill-equipped to overcome. These challenges can result in degraded project outcomes, as commonly observed in programming-intensive mid-level CS courses. We have previously found that success in these situations is associated with a disciplined personal software process. Procrastination is a prominent failure of self-regulation that can occur for a number of reasons, e.g., low expectancy of success, low perceived value of the task at hand, or decision-paralysis regarding how to begin when faced with a large task. It is pervasive, but may be addressed through targeted interventions. We draw on theory related to goal theory and problem-solving in engineering education to evaluate the value of explicit project milestones at curbing procrastination and its negative impacts on relatively long-running software projects. We conduct a quasi-experiment in which we study differences in project and course outcomes between students in a treatment (with milestones) and control group (without milestones). We found that students in the treatment group were more likely to finish their projects on time, produced projects with higher correctness, and finished the course with generally better outcomes. Within the treatment group, we found that students who completed more milestones saw better outcomes than those who completed fewer milestones. We found no differences in withdrawal or failure rates between the treatment and control groups. An end-of-term survey indicated that student perceptions of the milestones were overwhelmingly positive.},
	language = {en},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the 52nd {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Shaffer, Clifford A. and Kazerouni, Ayaan M.},
	year = {2021},
	pages = {907--913},
}

@inproceedings{shamshiri_automatically_2015,
	title = {Do {Automatically} {Generated} {Unit} {Tests} {Find} {Real} {Faults}? {An} {Empirical} {Study} of {Effectiveness} and {Challenges} ({T})},
	shorttitle = {Do {Automatically} {Generated} {Unit} {Tests} {Find} {Real} {Faults}?},
	doi = {10.1109/ASE.2015.86},
	abstract = {Rather than tediously writing unit tests manually, tools can be used to generate them automatically - sometimes even resulting in higher code coverage than manual testing. But how good are these tests at actually finding faults? To answer this question, we applied three state-of-the-art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. Although the automatically generated test suites detected 55.7\% of the faults overall, only 19.9\% of all the individual test suites detected a fault. By studying the effectiveness and problems of the individual tools and the tests they generate, we derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. These insights include 1) improving the obtained code coverage so that faulty statements are executed in the first instance, 2) improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, and 3) improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time.},
	booktitle = {2015 30th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Shamshiri, Sina and Just, René and Rojas, José Miguel and Fraser, Gordon and McMinn, Phil and Arcuri, Andrea},
	month = nov,
	year = {2015},
	keywords = {Generators, Java, Manuals, Software, Testing, Writing, automated test generation, empirical study, regression testing, test effectiveness, unit testing},
	pages = {201--211},
}

@inproceedings{chen_learning-guided_2019,
	title = {Learning-{Guided} {Network} {Fuzzing} for {Testing} {Cyber}-{Physical} {System} {Defences}},
	doi = {10.1109/ASE.2019.00093},
	abstract = {The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds-a water purification plant and a water distribution system-finding attacks that drive them into 27 different unsafe states involving water flow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, finding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Chen, Yuqi and Poskitt, Christopher M. and Sun, Jun and Adepu, Sridhar and Zhang, Fan},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Actuators, Benchmark testing, Fuzzing, Machine learning, Monitoring, Predictive models, benchmark generation, cyber-physical systems, fuzzing, machine learning, metaheuristic optimisation, testing},
	pages = {962--973},
}

@article{hellendoorn_are_2019,
	title = {Are {My} {Invariants} {Valid}? {A} {Learning} {Approach}},
	shorttitle = {Are {My} {Invariants} {Valid}?},
	url = {http://arxiv.org/abs/1903.06089},
	abstract = {Ensuring that a program operates correctly is a difficult task in large, complex systems. Enshrining invariants -- desired properties of correct execution -- in code or comments can support maintainability and help sustain correctness. Tools that can automatically infer and recommend invariants can thus be very beneficial. However, current invariant-suggesting tools, such as Daikon, suffer from high rates of false positives, in part because they only leverage traced program values from available test cases, rather than directly exploiting knowledge of the source code per se. We propose a machine-learning approach to judging the validity of invariants, specifically of method pre- and post-conditions, based directly on a method's source code. We introduce a new, scalable approach to creating labeled invariants: using programs with large test-suites, we generate Daikon invariants using traces from subsets of these test-suites, and then label these as valid/invalid by cross-validating them with held-out tests. This process induces a large set of labels that provide a form of noisy supervision, which is then used to train a deep neural model, based on gated graph neural networks. Our model learns to map the lexical, syntactic, and semantic structure of a given method's body into a probability that a candidate pre- or post-condition on that method's body is correct and is able to accurately label invariants based on the noisy signal, even in cross-project settings. Most importantly, it performs well on a hand-curated dataset of invariants.},
	urldate = {2021-12-12},
	journal = {arXiv:1903.06089 [cs]},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Polozov, Oleksandr and Marron, Mark},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.06089},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{almaghairbe_machine_2020,
	address = {New York, NY, USA},
	series = {{ICEMIS}'20},
	title = {Machine {Learning} {Techniques} for {Automated} {Software} {Fault} {Detection} via {Dynamic} {Execution} {Data}: {Empirical} {Evaluation} {Study}},
	isbn = {978-1-4503-7736-2},
	shorttitle = {Machine {Learning} {Techniques} for {Automated} {Software} {Fault} {Detection} via {Dynamic} {Execution} {Data}},
	url = {https://doi.org/10.1145/3410352.3410747},
	doi = {10.1145/3410352.3410747},
	abstract = {The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.},
	urldate = {2021-12-12},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Engineering} \& {MIS} 2020},
	publisher = {Association for Computing Machinery},
	author = {Almaghairbe, Rafig and Roper, Marc and Almabruk, Tahani},
	month = sep,
	year = {2020},
	keywords = {Automated Testing Oracles, Empirical Study, Machine Learning Techniques, Specification Mining},
	pages = {1--12},
}

@article{wang_detecting_2017,
	title = {Detecting {Bugs} of {Concurrent} {Programs} {With} {Program} {Invariants}},
	volume = {66},
	issn = {1558-1721},
	doi = {10.1109/TR.2017.2681107},
	abstract = {Concurrency bug detection is a time-consuming activity in the debugging process for concurrent programs. Existing techniques mainly focus on detecting data race bugs with pattern analysis; however, the number of interleaving patterns could be huge, only the most suspicious write-read pattern is given, and an oracle is needed, which is not available in the operational phase. This paper proposes a program-invariant-based technique to detect a class of concurrent program bugs. By unit testing of the components of a concurrent program, we obtain a set of program invariants, which can be used as an oracle to obtain “bad” invariants when the program is online. By using the function call graph of the components and applying a reduction technique to the invariants, we find the candidates of suspicious functions and rank them. From the interactions among components, we analyze the causes to the concurrency bugs. Experimental results show that our proposed technique is effective in concurrency bug detection.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Wang, Rong and Ding, Zuohua and Gui, Ning and Liu, Yang},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Reliability},
	keywords = {Bug detection, Computer bugs, Concurrent computing, Message systems, Pattern analysis, Programming, System recovery, Testing, concurrent program, function call graph, program invariants, suspicious rate},
	pages = {425--439},
}

@inproceedings{almaghairbe_empirical_2019,
	title = {An {Empirical} {Comparison} of {Two} {Different} {Strategies} to {Automated} {Fault} {Detection}: {Machine} {Learning} {Versus} {Dynamic} {Analysis}},
	shorttitle = {An {Empirical} {Comparison} of {Two} {Different} {Strategies} to {Automated} {Fault} {Detection}},
	doi = {10.1109/ISSREW.2019.00099},
	abstract = {Software testing is an established method to ensure software quality and reliability, but it is an expensive process. In recent years, the automation of test case generation has received significant attention as a way to reduce costs. However, the oracle problem (a mechanism for determine the (in) correctness of an executed test case) is still major problem which has been largely ignored. Recent work has shown that building a test oracle using the principles of anomaly detection techniques (mainly semisupervised/ unsupervised learning models based on dynamic execution data consisting of an amalgamation of input/output pairs and execution traces) is able to demonstrate a reasonable level of success in automatically detect passing and failing execution [1], [2]. In this paper, we present a comparison study between our machine-learning based approaches and an existing techniques from the specification mining domain (the data invariant detector Daikon [3]). The two approaches are evaluated on a range of midsized systems and compared in terms of their fault detection ability. The results show that in most cases semi-supervised learning techniques perform far better as an automated test classifier than Daikon. However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Almaghairbe, Rafig and Roper, Marc},
	month = oct,
	year = {2019},
	keywords = {Automated Test Oracles, Empirical Study, Machine Learning Techniques, Software Testing},
	pages = {378--385},
}

@article{bodei_measuring_2019,
	series = {Selected papers of {ICTCS} 2016 ({The} {Italian} {Conference} on {Theoretical} {Computer} {Science} ({ICTCS})},
	title = {Measuring security in {IoT} communications},
	volume = {764},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397518307205},
	doi = {10.1016/j.tcs.2018.12.002},
	abstract = {More smart objects and more applications on the Internet of Things (IoT) mean more security challenges. In IoT security is crucial but difficult to obtain. On the one hand the usual trade-off between highly secure and usable systems is more impelling than ever; on the other hand security is considered a feature that has a cost often unaffordable. Therefore, IoT designers not only need tools to assess possible risks and to study countermeasures, but also methodologies to estimate their costs. Here, we present a methodology, based on the process calculus IoT-LySa, to infer quantitative measures on evolution of systems. The derived quantitative evaluation is exploited to establish the cost of the possible security countermeasures, in terms of time and energy.},
	language = {en},
	urldate = {2021-12-09},
	journal = {Theoretical Computer Science},
	author = {Bodei, Chiara and Chessa, Stefano and Galletta, Letterio},
	month = apr,
	year = {2019},
	keywords = {Cost evaluation, Internet of Things, Security},
	pages = {100--124},
}

@inproceedings{toussaint_machine_2020,
	title = {Machine {Learning} {Systems} in the {IoT}: {Trustworthiness} {Trade}-offs for {Edge} {Intelligence}},
	shorttitle = {Machine {Learning} {Systems} in the {IoT}},
	doi = {10.1109/CogMI50398.2020.00030},
	abstract = {Machine learning systems (MLSys) are emerging in the Internet of Things (IoT) to provision edge intelligence, which is paving our way towards the vision of ubiquitous intelligence. However, despite the maturity of machine learning systems and the IoT, we are facing severe challenges when integrating MLSys and IoT in practical context. For instance, many machine learning systems have been developed for large-scale production (e.g., cloud environments), but IoT introduces additional demands due to heterogeneous and resource-constrained devices and decentralized operation environment. To shed light on this convergence of MLSys and IoT, this paper analyzes the tradeoffs by covering the latest developments (up to 2020) on scaling and distributing ML across cloud, edge, and IoT devices. We position machine learning systems as a component of the IoT, and edge intelligence as a socio-technical system. On the challenges of designing trustworthy edge intelligence, we advocate a holistic design approach that takes multi-stakeholder concerns, design requirements and trade-offs into consideration, and highlight the future research opportunities in edge intelligence.},
	booktitle = {2020 {IEEE} {Second} {International} {Conference} on {Cognitive} {Machine} {Intelligence} ({CogMI})},
	author = {Toussaint, Wiebke and Ding, Aaron Yi},
	month = oct,
	year = {2020},
	keywords = {Adaptation models, Computational modeling, Data models, Internet of Things, Machine learning, Predictive models, Training, edge intelligence, machine learning systems, smart services, trade-offs, trustworthiness},
	pages = {177--184},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Learning} {Systems} in the {IoT}: {Trustworthiness} {Trade}-offs for {Edge} {Intelligence}},
	shorttitle = {Machine {Learning} {Systems} in the {IoT}},
	url = {https://ieeexplore-ieee-org.ezproxy.lib.purdue.edu/document/9319287/},
	abstract = {Machine learning systems (MLSys) are emerging in the Internet of Things (IoT) to provision edge intelligence, which is paving our way towards the vision of ubiquitous intelligence. However, despite the maturity of machine learning systems and the IoT, we are facing severe challenges when integrating MLSys and IoT in practical context. For instance, many machine learning systems have been developed for large-scale production (e.g., cloud environments), but IoT introduces additional demands due to heterogeneous and resource-constrained devices and decentralized operation environment. To shed light on this convergence of MLSys and IoT, this paper analyzes the tradeoffs by covering the latest developments (up to 2020) on scaling and distributing ML across cloud, edge, and IoT devices. We position machine learning systems as a component of the IoT, and edge intelligence as a socio-technical system. On the challenges of designing trustworthy edge intelligence, we advocate a holistic design approach that takes multi-stakeholder concerns, design requirements and trade-offs into consideration, and highlight the future research opportunities in edge intelligence.},
	language = {en-US},
	urldate = {2021-12-09},
}

@book{heldal_functional_1992,
	address = {London},
	series = {Workshops in {Computing}},
	title = {Functional {Programming}, {Glasgow} 1991: {Proceedings} of the 1991 {Glasgow} {Workshop} on {Functional} {Programming}, {Portree}, {Isle} of {Skye}, 12–14 {August} 1991},
	isbn = {978-3-540-19760-7 978-1-4471-3196-0},
	shorttitle = {Functional {Programming}, {Glasgow} 1991},
	url = {http://link.springer.com/10.1007/978-1-4471-3196-0},
	language = {en},
	urldate = {2021-12-09},
	publisher = {Springer London},
	editor = {Heldal, Rogardt and Holst, Carsten Kehler and Wadler, Philip and van Rijsbergen, C. J.},
	year = {1992},
	doi = {10.1007/978-1-4471-3196-0},
}

@inproceedings{nagappan_diversity_2013,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2013},
	title = {Diversity in software engineering research},
	isbn = {978-1-4503-2237-9},
	url = {http://doi.org/10.1145/2491411.2491415},
	doi = {10.1145/2491411.2491415},
	abstract = {One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space).},
	urldate = {2021-12-07},
	booktitle = {Proceedings of the 2013 9th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Nagappan, Meiyappan and Zimmermann, Thomas and Bird, Christian},
	month = aug,
	year = {2013},
	keywords = {Coverage, Diversity, Representativeness, Sampling},
	pages = {466--476},
}

@book{mullaney2021your,
	title = {Your computer is on fire},
	publisher = {MIT Press},
	author = {Mullaney, Thomas S and Peters, Benjamin and Hicks, Mar and Philip, Kavita},
	year = {2021},
}

@article{sutcliffe2018activity,
	title = {Activity in social media and intimacy in social relationships},
	volume = {85},
	journal = {Computers in human behavior},
	author = {Sutcliffe, Alistair G and Binder, Jens F and Dunbar, Robin IM},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {227--235},
}

@article{spaiser2017communication,
	title = {Communication power struggles on social media: {A} case study of the 2011–12 {Russian} protests},
	volume = {14},
	number = {2},
	journal = {Journal of Information Technology \& Politics},
	author = {Spaiser, Viktoria and Chadefaux, Thomas and Donnay, Karsten and Russmann, Fabian and Helbing, Dirk},
	year = {2017},
	note = {Publisher: Taylor \& Francis},
	pages = {132--153},
}

@article{pineiro2017influence,
	title = {Influence of social media over the stock market},
	volume = {34},
	number = {1},
	journal = {Psychology \& Marketing},
	author = {Piñeiro-Chousa, Juan and Vizcaíno-González, Marcos and Pérez-Pico, Ada María},
	year = {2017},
	note = {Publisher: Wiley Online Library},
	pages = {101--108},
}

@inproceedings{arcuri_automated_2014,
	address = {New York, NY, USA},
	series = {{ASE} '14},
	title = {Automated unit test generation for classes with environment dependencies},
	isbn = {978-1-4503-3013-8},
	url = {https://doi.org/10.1145/2642937.2642986},
	doi = {10.1145/2642937.2642986},
	abstract = {Automated test generation for object-oriented software typically consists of producing sequences of calls aiming at high code coverage. In practice, the success of this process may be inhibited when classes interact with their environment, such as the file system, network, user-interactions, etc. This leads to two major problems: First, code that depends on the environment can sometimes not be fully covered simply by generating sequences of calls to a class under test, for example when execution of a branch depends on the contents of a file. Second, even if code that is environment-dependent can be covered, the resulting tests may be unstable, i.e., they would pass when first generated, but then may fail when executed in a different environment. For example, tests on classes that make use of the system time may have failing assertions if the tests are executed at a different time than when they were generated. In this paper, we apply bytecode instrumentation to automatically separate code from its environmental dependencies, and extend the EvoSuite Java test generation tool such that it can explicitly set the state of the environment as part of the sequences of calls it generates. Using a prototype implementation, which handles a wide range of environmental interactions such as the file system, console inputs and many non-deterministic functions of the Java virtual machine (JVM), we performed experiments on 100 Java projects randomly selected from SourceForge (the SF100 corpus). The results show significantly improved code coverage - in some cases even in the order of +80\%/+90\%. Furthermore, our techniques reduce the number of unstable tests by more than 50\%.},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} international conference on {Automated} software engineering},
	publisher = {Association for Computing Machinery},
	author = {Arcuri, Andrea and Fraser, Gordon and Galeotti, Juan Pablo},
	month = sep,
	year = {2014},
	keywords = {automated test generation, environment, unit testing},
	pages = {79--90},
}

@inproceedings{arcuri_automated_2014-1,
	address = {New York, NY, USA},
	series = {{ASE} '14},
	title = {Automated unit test generation for classes with environment dependencies},
	isbn = {978-1-4503-3013-8},
	url = {https://doi.org/10.1145/2642937.2642986},
	doi = {10.1145/2642937.2642986},
	abstract = {Automated test generation for object-oriented software typically consists of producing sequences of calls aiming at high code coverage. In practice, the success of this process may be inhibited when classes interact with their environment, such as the file system, network, user-interactions, etc. This leads to two major problems: First, code that depends on the environment can sometimes not be fully covered simply by generating sequences of calls to a class under test, for example when execution of a branch depends on the contents of a file. Second, even if code that is environment-dependent can be covered, the resulting tests may be unstable, i.e., they would pass when first generated, but then may fail when executed in a different environment. For example, tests on classes that make use of the system time may have failing assertions if the tests are executed at a different time than when they were generated. In this paper, we apply bytecode instrumentation to automatically separate code from its environmental dependencies, and extend the EvoSuite Java test generation tool such that it can explicitly set the state of the environment as part of the sequences of calls it generates. Using a prototype implementation, which handles a wide range of environmental interactions such as the file system, console inputs and many non-deterministic functions of the Java virtual machine (JVM), we performed experiments on 100 Java projects randomly selected from SourceForge (the SF100 corpus). The results show significantly improved code coverage - in some cases even in the order of +80\%/+90\%. Furthermore, our techniques reduce the number of unstable tests by more than 50\%.},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} international conference on {Automated} software engineering},
	publisher = {Association for Computing Machinery},
	author = {Arcuri, Andrea and Fraser, Gordon and Galeotti, Juan Pablo},
	month = sep,
	year = {2014},
	keywords = {automated test generation, environment, unit testing},
	pages = {79--90},
}

@inproceedings{galler_automatically_2010,
	address = {New York, NY, USA},
	series = {{AST} '10},
	title = {Automatically extracting mock object behavior from {Design} by {Contract}\&\#x2122; specification for test data generation},
	isbn = {978-1-60558-970-1},
	url = {https://doi.org/10.1145/1808266.1808273},
	doi = {10.1145/1808266.1808273},
	abstract = {Test data generation is an important task in the process of automated unit test generation. Random and heuristic approaches are well known for test input data generation. Unfortunately, in the presence of complex pre-conditions especially in the case of non-primitive data types those approaches often fail. A promising technique for generating an object that exactly satisfies a given pre-condition is mocking, i.e., replacing the concrete implementation with an implementation only considering the necessary behavior for a specific test case. In this paper we follow this technique and present an approach for automatically deriving the behavior of mock objects from given Design by Contract™ specifications. The generated mock objects behave according to the Design by Contract™ specification of the original class. Furthermore, we make sure that the observed behavior of the mock object satisfies the pre-condition of the method under test. We evaluate the approach using the Java implementations of 20 common Design Patterns and a stack based calculator. Our approach clearly outperforms pure random data generation in terms of line coverage.},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 5th {Workshop} on {Automation} of {Software} {Test}},
	publisher = {Association for Computing Machinery},
	author = {Galler, Stefan J. and Maller, Andreas and Wotawa, Franz},
	month = may,
	year = {2010},
	keywords = {Design-by-Contract, automated unit testing, mock object, test data generation},
	pages = {43--50},
}

@inproceedings{marri_empirical_2009,
	title = {An empirical study of testing file-system-dependent software with mock objects},
	doi = {10.1109/IWAST.2009.5069054},
	abstract = {Unit testing is a technique of testing a single unit of a program in isolation. The testability of the unit under test can be reduced when the unit interacts with its environment. The construction of high-covering unit tests and their execution require appropriate interactions with the environment such as a file system or database. To help set up the required environment, developers can use mock objects to simulate the behavior of the environment. In this paper, we present an empirical study to analyze the use of mock objects to test file-system-dependent software. We use a mock object of the FileSystem API provided with the Pex automatic testing tool in our study. We share our insights gained on the benefits of using mock objects in unit testing and discuss the faced challenges.},
	booktitle = {2009 {ICSE} {Workshop} on {Automation} of {Software} {Test}},
	author = {Marri, Madhuri R. and Xie, Tao and Tillmann, Nikolai and de Halleux, Jonathan and Schulte, Wolfram},
	month = may,
	year = {2009},
	keywords = {Automatic testing, Computer science, Databases, File systems, Logic, Software testing, System testing},
	pages = {149--153},
}

@inproceedings{gafford_synthesis-based_2020,
	address = {New York, NY, USA},
	series = {{ASE} '20},
	title = {Synthesis-based resolution of feature interactions in cyber-physical systems},
	isbn = {978-1-4503-6768-4},
	url = {https://doi.org/10.1145/3324884.3416630},
	doi = {10.1145/3324884.3416630},
	abstract = {The feature interaction problem arises when two or more independent features interact with each other in an undesirable manner. Feature interactions remain a challenging and important problem in emerging domains of cyber-physical systems (CPS), such as intelligent vehicles, unmanned aerial vehicles (UAVs) and the Internet of Things (IoT), where the outcome of an unexpected interaction may result in a safety failure. Existing approaches to resolving feature interactions rely on priority lists or fixed strategies, but may not be effective in scenarios where none of the competing feature actions are satisfactory with respect to system requirements. This paper proposes a novel synthesis-based approach to resolution, where a conflict among features is resolved by synthesizing an action that best satisfies the specification of desirable system behaviors in the given environmental context. Unlike existing resolution methods, our approach is capable of producing a desirable system outcome even when none of the conflicting actions are satisfactory. The effectiveness of the proposed approach is demonstrated using a case study involving interactions among safety-critical features in an autonomous drone.},
	urldate = {2021-12-04},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Gafford, Benjamin and Dürschmid, Tobias and Moreno, Gabriel A. and Kang, Eunsuk},
	month = dec,
	year = {2020},
	pages = {1090--1102},
}

@article{spiekermann_inside_2019,
	title = {Inside the {Organization}: {Why} {Privacy} and {Security} {Engineering} {Is} a {Challenge} for {Engineers}},
	volume = {107},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Inside the {Organization}},
	url = {https://ieeexplore.ieee.org/document/8466102/},
	doi = {10.1109/JPROC.2018.2866769},
	language = {en},
	number = {3},
	urldate = {2021-12-01},
	journal = {Proceedings of the IEEE},
	author = {Spiekermann, Sarah and Korunovska, Jana and Langheinrich, Marc},
	month = mar,
	year = {2019},
	pages = {600--615},
}

@inproceedings{benbenisty_privacy_2021,
	title = {Privacy as first-class requirements in software development: {A} socio-technical approach},
	abstract = {Privacy requirements have become increasingly important as information about us is continuously accumulated and digitally stored. However, despite the many proposed methodologies and tools to address these requirements, privacy engineering is often underperformed in most domains of the software industry. Two of the major reasons underlying this under-performance are (1) the low expertise and understanding of privacy by the two main actors in requirements engineering: users and analysts, and (2) the fact that software developers often do not perceive privacy requirements as a priority for their companies, thus neglecting to meet these requirements even when they do have the required knowledge, skills, and supporting tools to do so. To address these two problems, we propose to integrate knowledge from software engineering and organizational psychology in an iterative, customizable, socio-technical environment. Such environment has the potential to support the design of systems by providing technical tools for eliciting, modeling, and designing privacy aspects, thus addressing the knowledge gap of both data subjects and analysts, and social mechanisms for achieving a supportive and sustainable organizational privacy climate within a company, thus reorienting the organizational attention and engagement toward addressing privacy requirements.},
	language = {en},
	booktitle = {Automated {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results}},
	author = {Benbenisty, Yizhaq and Hadar, Irit and Luria, Gil and Spoletini, Paola},
	year = {2021},
	pages = {5},
}

@misc{noauthor_gdpr_nodate,
	title = {{GDPR} {Enforcement} {Tracker} - list of {GDPR} fines},
	url = {https://www.enforcementtracker.com},
	abstract = {List and overview of fines and penalties under the EU General Data Protection Regulation (GDPR, DSGVO)},
	urldate = {2021-12-01},
}

@misc{noauthor_top_nodate,
	title = {Top 10 {Open} {Source} {Social} {Network} {Development} {Platforms}},
	url = {https://topdigital.agency/ top-10-open-source-social-network-development-platforms/},
	urldate = {2021-01-01},
}

@misc{noauthor_top_nodate-1,
	title = {The {Top} 1,539 {Social} {Network} {Open} {Source} {Projects} on {Github}},
	url = {https://awesomeopensource.com/projects/social-network},
}

@misc{noauthor_top_nodate-2,
	title = {The {Top} 1,539 {Social} {Network} {Open} {Source} {Projects} on {Github}},
	url = {https://awesomeopensource.com/projects/social-network},
	urldate = {2021-12-01},
}

@misc{noauthor_9_nodate,
	title = {9 {Decentralized}, {Open} {Source} {Alternative} {Social} {Media} {Platforms}},
	url = {https://itsfoss.com/mainstream-social-media-alternaives/},
	abstract = {Tired of Big Tech prying on your data and invading your privacy? Here are some open source, decentralized alternate social platforms.},
	language = {en\_US},
	urldate = {2021-12-01},
	journal = {https://itsfoss.com/},
	note = {Section: List},
}

@misc{noauthor_most_nodate,
	title = {Most used social media 2021},
	url = {https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/},
	abstract = {Facebook, YouTube, and WhatsApp are the most popular social networks worldwide, each with more than two billion active users.},
	language = {en},
	urldate = {2021-12-01},
	journal = {Statista},
}

@article{zulli_rethinking_2020,
	title = {Rethinking the “social” in “social media”: {Insights} into topology, abstraction, and scale on the {Mastodon} social network},
	volume = {22},
	issn = {1461-4448, 1461-7315},
	shorttitle = {Rethinking the “social” in “social media”},
	url = {http://journals.sagepub.com/doi/10.1177/1461444820912533},
	doi = {10.1177/1461444820912533},
	abstract = {Online interactions are often understood through the corporate social media (CSM) model where social interactions are determined through layers of abstraction and centralization that eliminate users from decision-making processes. This study demonstrates how alternative social media (ASM)—namely Mastodon—restructure the relationship between the technical structure of social media and the social interactions that follow, offering a particular type of sociality distinct from CSM. Drawing from a variety of qualitative data, this analysis finds that (1) the decentralized structure of Mastodon enables community autonomy, (2) Mastodon’s open-source protocol allows the internal and technical development of the site to become a social enterprise in and of itself, and (3) Mastodon’s horizontal structure shifts the site’s scaling focus from sheer number of users to quality engagement and niche communities. To this end, Mastodon helps us rethink “the social” in social media in terms of topology, abstraction, and scale.},
	language = {en},
	number = {7},
	urldate = {2021-12-01},
	journal = {New Media \& Society},
	author = {Zulli, Diana and Liu, Miao and Gehl, Robert},
	month = jul,
	year = {2020},
	pages = {1188--1205},
}

@incollection{burgess_alternative_2018,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Alternative {Social} {Media}: {From} {Critique} to {Code}},
	isbn = {978-1-4129-6229-2 978-1-4739-8406-6},
	shorttitle = {Alternative {Social} {Media}},
	url = {https://sk.sagepub.com/reference/the-sage-handbook-of-social-media/i2724.xml},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {The {SAGE} {Handbook} of {Social} {Media}},
	publisher = {SAGE Publications Ltd},
	author = {Gehl, Robert W.},
	collaborator = {Burgess, Jean and Marwick, Alice and Poell, Thomas},
	year = {2018},
	doi = {10.4135/9781473984066.n19},
	pages = {330--350},
}

@inproceedings{towne_your_2013,
	address = {San Antonio, Texas, USA},
	title = {Your process is showing: controversy management and perceived quality in wikipedia},
	isbn = {978-1-4503-1331-5},
	shorttitle = {Your process is showing},
	url = {http://dl.acm.org/citation.cfm?doid=2441776.2441896},
	doi = {10.1145/2441776.2441896},
	abstract = {Large-scale collaboration systems often separate their content from the deliberation around how that content was produced. Surfacing this deliberation may engender trust in the content generation process if the deliberation process appears fair, well-reasoned, and thorough. Alternatively, it could encourage doubts about content quality, especially if the process appears messy or biased. In this paper we report the results of an experiment where we found that surfacing deliberation generally led to decreases in perceptions of quality for the article under consideration, especially – but not only – if the discussion revealed conflict. The effect size depends on the type of editors’ interactions. Finally, this decrease in actual article quality rating was accompanied by self-reported improved perceptions of the article and Wikipedia overall.},
	language = {en},
	urldate = {2021-11-30},
	booktitle = {Proceedings of the 2013 conference on {Computer} supported cooperative work - {CSCW} '13},
	publisher = {ACM Press},
	author = {Towne, W. Ben and Kittur, Aniket and Kinnaird, Peter and Herbsleb, James},
	year = {2013},
	pages = {1059},
}

@inproceedings{muller-birn_work--rule_2013,
	address = {Munich, Germany},
	title = {Work-to-rule: the emergence of algorithmic governance in {Wikipedia}},
	isbn = {978-1-4503-2104-4},
	shorttitle = {Work-to-rule},
	url = {http://dl.acm.org/citation.cfm?doid=2482991.2482999},
	doi = {10.1145/2482991.2482999},
	abstract = {Research has shown the importance of a functioning governance system for the success of peer production communities. It particularly highlights the role of human coordination and communication within the governance regime. In this article, we extend this line of research by differentiating two categories of governance mechanisms. The ﬁrst category is based primarily on communication, in which social norms emerge that are often formalized by written rules and guidelines. The second category refers to the technical infrastructure that enables users to access artifacts, and that allows the community to communicate and coordinate their collective actions to create those artifacts. We collected qualitative and quantitative data from Wikipedia in order to show how a community’s consensus gradually converts social mechanisms into algorithmic mechanisms. In detail, we analyze algorithmic governance mechanisms in two embedded cases: the software extension “ﬂagged revisions” and the bot “xqbot”. Our insights point towards a growing relevance of algorithmic governance in the realm of governing large-scale peer production communities. This extends previous research, in which algorithmic governance is almost absent. Further research is needed to unfold, understand, and also modify existing interdependencies between social and algorithmic governance mechanisms.},
	language = {en},
	urldate = {2021-11-30},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Communities} and {Technologies} - {C}\&{T} '13},
	publisher = {ACM Press},
	author = {Müller-Birn, Claudia and Dobusch, Leonhard and Herbsleb, James D.},
	year = {2013},
	pages = {80--89},
}

@inproceedings{tsay_social_2013,
	address = {San Francisco, CA, USA},
	title = {Social media in transparent work environments},
	isbn = {978-1-4673-6290-0},
	url = {http://ieeexplore.ieee.org/document/6614733/},
	doi = {10.1109/CHASE.2013.6614733},
	abstract = {Social media is being integrated into work environments making them more transparent. When the work environment is transparent, it has the potential to allow projects to transmit information about work artifacts and events quickly through a large network. Using signaling theory, we propose a theory that users interpret this information and then make workrelated decisions about attention and effort allocation in a principled manner. In the open source context of voluntary participation, broadcast activity information act as signals that allow developers to make highly informed choices about where to expend their attention and effort and with whom to collaborate. We propose four potential signals from literature and interviews with developers in our research setting and discuss the implications for social media in software development environments.},
	language = {en},
	urldate = {2021-11-30},
	booktitle = {2013 6th {International} {Workshop} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering} ({CHASE})},
	publisher = {IEEE},
	author = {Tsay, Jason and Dabbish, Laura and Herbsleb, James D.},
	month = may,
	year = {2013},
	pages = {65--72},
}

@article{shaikh_learning-based_2021,
	title = {A {Learning}-{Based} {Fault} {Localization} {Approach} {Using} {Subset} of {Likely} and {Dynamic} {Invariants}},
	volume = {31},
	doi = {10.32604/iasc.2022.021163},
	abstract = {Fault localization is one of the main tasks of software debugging. Developers spend a lot of time, cost, and effort to locate the faults correctly manually. For reducing this effort, many automatic fault localization techniques have been proposed, which inputs test suites and outputs a sorted list of faulty entities of the program. For further enhancement in this area, we developed a system called SILearning, which is based on invariant analysis. It learns from some existing fixed bugs to locate faulty methods in the program. It combines machine-learned ranking, program invariant differences, and spectrum-based fault localization (SBFL). Using the execution of test cases and code coverage analysis, it obtains each method's invariant differences and the suspiciousness value based on the program spectral location and uses them as features for ranking the faulty methods. The experimental analysis of SILearning has been performed on the dataset of real fault which is extracted from the database Defects4J. The tools used in this research are Daikon and cobertura for detection of the invariants and code coverage, respectively. The results show that SILearning performs better when combined features are utilized and can successfully locate the faulty methods on average for 76.1, 90.4, 108.2, 123, and 143.5 at the top positions of 1, 2, 3, 4, and 5.},
	journal = {Intelligent Automation and Soft Computing},
	author = {Shaikh, Asadullah and Rizwan, Syed and Alghamdi, Abdullah and Islam, Noman and Elmagzoub, M and Syed, Darakhshan},
	month = oct,
	year = {2021},
}

@article{hellendoorn_are_2019,
	title = {Are {My} {Invariants} {Valid}? {A} {Learning} {Approach}},
	shorttitle = {Are {My} {Invariants} {Valid}?},
	url = {http://arxiv.org/abs/1903.06089},
	abstract = {Ensuring that a program operates correctly is a difficult task in large, complex systems. Enshrining invariants -- desired properties of correct execution -- in code or comments can support maintainability and help sustain correctness. Tools that can automatically infer and recommend invariants can thus be very beneficial. However, current invariant-suggesting tools, such as Daikon, suffer from high rates of false positives, in part because they only leverage traced program values from available test cases, rather than directly exploiting knowledge of the source code per se. We propose a machine-learning approach to judging the validity of invariants, specifically of method pre- and post-conditions, based directly on a method's source code. We introduce a new, scalable approach to creating labeled invariants: using programs with large test-suites, we generate Daikon invariants using traces from subsets of these test-suites, and then label these as valid/invalid by cross-validating them with held-out tests. This process induces a large set of labels that provide a form of noisy supervision, which is then used to train a deep neural model, based on gated graph neural networks. Our model learns to map the lexical, syntactic, and semantic structure of a given method's body into a probability that a candidate pre- or post-condition on that method's body is correct and is able to accurately label invariants based on the noisy signal, even in cross-project settings. Most importantly, it performs well on a hand-curated dataset of invariants.},
	urldate = {2021-11-30},
	journal = {arXiv:1903.06089 [cs]},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Polozov, Oleksandr and Marron, Mark},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.06089},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{almaghairbe_machine_2020,
	address = {New York, NY, USA},
	series = {{ICEMIS}'20},
	title = {Machine {Learning} {Techniques} for {Automated} {Software} {Fault} {Detection} via {Dynamic} {Execution} {Data}: {Empirical} {Evaluation} {Study}},
	isbn = {978-1-4503-7736-2},
	shorttitle = {Machine {Learning} {Techniques} for {Automated} {Software} {Fault} {Detection} via {Dynamic} {Execution} {Data}},
	url = {https://doi.org/10.1145/3410352.3410747},
	doi = {10.1145/3410352.3410747},
	abstract = {The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.},
	urldate = {2021-11-30},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Engineering} \& {MIS} 2020},
	publisher = {Association for Computing Machinery},
	author = {Almaghairbe, Rafig and Roper, Marc and Almabruk, Tahani},
	month = sep,
	year = {2020},
	keywords = {Automated Testing Oracles, Empirical Study, Machine Learning Techniques, Specification Mining},
	pages = {1--12},
}

@article{wang_detecting_2017,
	title = {Detecting {Bugs} of {Concurrent} {Programs} {With} {Program} {Invariants}},
	volume = {66},
	issn = {1558-1721},
	doi = {10.1109/TR.2017.2681107},
	abstract = {Concurrency bug detection is a time-consuming activity in the debugging process for concurrent programs. Existing techniques mainly focus on detecting data race bugs with pattern analysis; however, the number of interleaving patterns could be huge, only the most suspicious write-read pattern is given, and an oracle is needed, which is not available in the operational phase. This paper proposes a program-invariant-based technique to detect a class of concurrent program bugs. By unit testing of the components of a concurrent program, we obtain a set of program invariants, which can be used as an oracle to obtain “bad” invariants when the program is online. By using the function call graph of the components and applying a reduction technique to the invariants, we find the candidates of suspicious functions and rank them. From the interactions among components, we analyze the causes to the concurrency bugs. Experimental results show that our proposed technique is effective in concurrency bug detection.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Wang, Rong and Ding, Zuohua and Gui, Ning and Liu, Yang},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Reliability},
	keywords = {Bug detection, Computer bugs, Concurrent computing, Message systems, Pattern analysis, Programming, System recovery, Testing, concurrent program, function call graph, program invariants, suspicious rate},
	pages = {425--439},
}

@inproceedings{almaghairbe_empirical_2019,
	title = {An {Empirical} {Comparison} of {Two} {Different} {Strategies} to {Automated} {Fault} {Detection}: {Machine} {Learning} {Versus} {Dynamic} {Analysis}},
	shorttitle = {An {Empirical} {Comparison} of {Two} {Different} {Strategies} to {Automated} {Fault} {Detection}},
	doi = {10.1109/ISSREW.2019.00099},
	abstract = {Software testing is an established method to ensure software quality and reliability, but it is an expensive process. In recent years, the automation of test case generation has received significant attention as a way to reduce costs. However, the oracle problem (a mechanism for determine the (in) correctness of an executed test case) is still major problem which has been largely ignored. Recent work has shown that building a test oracle using the principles of anomaly detection techniques (mainly semisupervised/ unsupervised learning models based on dynamic execution data consisting of an amalgamation of input/output pairs and execution traces) is able to demonstrate a reasonable level of success in automatically detect passing and failing execution [1], [2]. In this paper, we present a comparison study between our machine-learning based approaches and an existing techniques from the specification mining domain (the data invariant detector Daikon [3]). The two approaches are evaluated on a range of midsized systems and compared in terms of their fault detection ability. The results show that in most cases semi-supervised learning techniques perform far better as an automated test classifier than Daikon. However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Almaghairbe, Rafig and Roper, Marc},
	month = oct,
	year = {2019},
	keywords = {Automated Test Oracles, Empirical Study, Machine Learning Techniques, Software Testing},
	pages = {378--385},
}

@inproceedings{zhang_firmware_2020,
	address = {New York, NY, USA},
	series = {Internetware'20},
	title = {Firmware {Fuzzing}: {The} {State} of the {Art}},
	isbn = {978-1-4503-8819-1},
	shorttitle = {Firmware {Fuzzing}},
	url = {https://doi.org/10.1145/3457913.3457934},
	doi = {10.1145/3457913.3457934},
	abstract = {Background: Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited. Due to the limited resources of IoT devices, it is impractical to deploy sophisticated run-time protection techniques. Under an insecure network environment, when a firmware is exploited, it may lead to denial of service, information disclosure, elevation of privilege, or even life-threatening. Therefore, firmware vulnerability detection by fuzzing has become the key to ensure the security of IoT devices, and has also become a hot topic in academic and industrial research. With the rapid growth of the existing IoT devices, the size and complexity of firmware, the variety of firmware types, and the firmware defects, existing IoT firmware fuzzing methods face challenges. Objective: This paper summarizes the typical types of IoT firmware fuzzing methods, analyzes the contribution of these works, and summarizes the shortcomings of existing fuzzing methods. Method: We design several research questions, extract keywords from the research questions, then use the keywords to search for related literature. Result: We divide the existing firmware fuzzing work into real-device-based fuzzing and simulation-based fuzzing according to the firmware execution environment, and simulation-based fuzzing is the mainstream in the future; we found that the main types of vulnerabilities targeted by existing fuzzing methods are memory corruption vulnerabilities; firmware fuzzing faces more difficulties than ordinary software fuzzing. Conclusion: Through the analysis of the advantages and disadvantages of different methods, this review provides guidance for further improving the performance of fuzzing techniques, and proposes several recommendations from the findings of this review.},
	urldate = {2021-11-30},
	booktitle = {12th {Asia}-{Pacific} {Symposium} on {Internetware}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Chi and Wang, Yu and Wang, Linzhang},
	month = nov,
	year = {2020},
	keywords = {Internet of Things, firmware, fuzzing, literature review},
	pages = {110--115},
}

@inproceedings{aliabadi_artinali_2017,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2017},
	title = {{ARTINALI}: dynamic invariant detection for cyber-physical system security},
	isbn = {978-1-4503-5105-8},
	shorttitle = {{ARTINALI}},
	url = {https://doi.org/10.1145/3106237.3106282},
	doi = {10.1145/3106237.3106282},
	abstract = {Cyber-Physical Systems (CPSes) are being widely deployed in security critical scenarios such as smart homes and medical devices. Unfortunately, the connectedness of these systems and their relative lack of security measures makes them ripe targets for attacks. Specification-based Intrusion Detection Systems (IDS) have been shown to be effective for securing CPSs. Unfortunately, deriving invariants for capturing the specifications of CPS systems is a tedious and error-prone process. Therefore, it is important to dynamically monitor the CPS system to learn its common behaviors and formulate invariants for detecting security attacks. Existing techniques for invariant mining only incorporate data and events, but not time. However, time is central to most CPS systems, and hence incorporating time in addition to data and events, is essential for achieving low false positives and false negatives. This paper proposes ARTINALI, which mines dynamic system properties by incorporating time as a first-class property of the system. We build ARTINALI-based Intrusion Detection Systems (IDSes) for two CPSes, namely smart meters and smart medical devices, and measure their efficacy. We find that the ARTINALI-based IDSes significantly reduce the ratio of false positives and false negatives by 16 to 48\% (average 30.75\%) and 89 to 95\% (average 93.4\%) respectively over other dynamic invariant detection tools.},
	urldate = {2021-11-30},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Aliabadi, Maryam Raiyat and Kamath, Amita Ajith and Gascon-Samson, Julien and Pattabiraman, Karthik},
	month = aug,
	year = {2017},
	keywords = {CPS, Cyber Physical System, Multi-dimensional model, Security, Software Engineering},
	pages = {349--361},
}

@inproceedings{chen_learning-guided_2019,
	title = {Learning-{Guided} {Network} {Fuzzing} for {Testing} {Cyber}-{Physical} {System} {Defences}},
	doi = {10.1109/ASE.2019.00093},
	abstract = {The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds-a water purification plant and a water distribution system-finding attacks that drive them into 27 different unsafe states involving water flow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, finding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Chen, Yuqi and Poskitt, Christopher M. and Sun, Jun and Adepu, Sridhar and Zhang, Fan},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Actuators, Benchmark testing, Fuzzing, Machine learning, Monitoring, Predictive models, benchmark generation, cyber-physical systems, fuzzing, machine learning, metaheuristic optimisation, testing},
	pages = {962--973},
}

@inproceedings{viticchie_remotely_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Remotely {Assessing} {Integrity} of {Software} {Applications} by {Monitoring} {Invariants}: {Present} {Limitations} and {Future} {Directions}},
	isbn = {978-3-319-76687-4},
	shorttitle = {Remotely {Assessing} {Integrity} of {Software} {Applications} by {Monitoring} {Invariants}},
	doi = {10.1007/978-3-319-76687-4_5},
	abstract = {Invariants monitoring is a software attestation technique that aims at proving the integrity of a running application by checking likely invariants, which are predicates built on variables’ values. Being very promising in literature, we developed a software protection that remotely checks invariants. However, we faced a series of issues and limitations. This paper, after presenting an extensive background on invariants and their use, reports, analyses, and categorizes the identified limitations. Our work suggests that, even if it is still promising, further studies are needed to decree if invariants monitoring could be practically used as a remote protection of software applications.},
	language = {en},
	booktitle = {Risks and {Security} of {Internet} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Viticchié, Alessio and Basile, Cataldo and Lioy, Antonio},
	editor = {Cuppens, Nora and Cuppens, Frédéric and Lanet, Jean-Louis and Legay, Axel and Garcia-Alfaro, Joaquin},
	year = {2018},
	pages = {66--82},
}

@inproceedings{zhang_firmware_2020-1,
	address = {New York, NY, USA},
	series = {Internetware'20},
	title = {Firmware {Fuzzing}: {The} {State} of the {Art}},
	isbn = {978-1-4503-8819-1},
	shorttitle = {Firmware {Fuzzing}},
	url = {https://doi.org/10.1145/3457913.3457934},
	doi = {10.1145/3457913.3457934},
	abstract = {Background: Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited. Due to the limited resources of IoT devices, it is impractical to deploy sophisticated run-time protection techniques. Under an insecure network environment, when a firmware is exploited, it may lead to denial of service, information disclosure, elevation of privilege, or even life-threatening. Therefore, firmware vulnerability detection by fuzzing has become the key to ensure the security of IoT devices, and has also become a hot topic in academic and industrial research. With the rapid growth of the existing IoT devices, the size and complexity of firmware, the variety of firmware types, and the firmware defects, existing IoT firmware fuzzing methods face challenges. Objective: This paper summarizes the typical types of IoT firmware fuzzing methods, analyzes the contribution of these works, and summarizes the shortcomings of existing fuzzing methods. Method: We design several research questions, extract keywords from the research questions, then use the keywords to search for related literature. Result: We divide the existing firmware fuzzing work into real-device-based fuzzing and simulation-based fuzzing according to the firmware execution environment, and simulation-based fuzzing is the mainstream in the future; we found that the main types of vulnerabilities targeted by existing fuzzing methods are memory corruption vulnerabilities; firmware fuzzing faces more difficulties than ordinary software fuzzing. Conclusion: Through the analysis of the advantages and disadvantages of different methods, this review provides guidance for further improving the performance of fuzzing techniques, and proposes several recommendations from the findings of this review.},
	urldate = {2021-11-30},
	booktitle = {12th {Asia}-{Pacific} {Symposium} on {Internetware}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Chi and Wang, Yu and Wang, Linzhang},
	month = nov,
	year = {2020},
	keywords = {Internet of Things, firmware, fuzzing, literature review},
	pages = {110--115},
}

@inproceedings{alimi_analysis_2014,
	title = {Analysis of embedded applications by evolutionary fuzzing},
	doi = {10.1109/HPCSim.2014.6903734},
	abstract = {In this paper, we propose to use fuzzing techniques to discover vulnerabilities in programs hosted into smart cards used for telecommunications or banking purposes (SIM cards, credit cards, secure element into NFC mobile devices...). Those programs - called applets - usually host sensitive applications and manipulate sensitive data. A flaw by design or by implementation in one of those applet could have disastrous consequences. The proposed approach uses a genetic algorithm to optimize the vulnerabilities search. We illustrate the benefit of the proposed method on a MasterCard M/Chip applet through experimental results.},
	booktitle = {2014 {International} {Conference} on {High} {Performance} {Computing} {Simulation} ({HPCS})},
	author = {Alimi, V. and Vernois, S. and Rosenberger, C.},
	month = jul,
	year = {2014},
	keywords = {Credit cards, Genetic algorithms, Libraries, Protocols, Smart cards, Sociology, Statistics},
	pages = {551--557},
}

@article{eceiza_fuzzing_2021,
	title = {Fuzzing the {Internet} of {Things}: {A} {Review} on the {Techniques} and {Challenges} for {Efficient} {Vulnerability} {Discovery} in {Embedded} {Systems}},
	volume = {8},
	issn = {2327-4662},
	shorttitle = {Fuzzing the {Internet} of {Things}},
	doi = {10.1109/JIOT.2021.3056179},
	abstract = {With a growing number of embedded devices that create, transform, and send data autonomously at its core, the Internet of Things (IoT) is a reality in different sectors, such as manufacturing, healthcare, or transportation. With this expansion, the IoT is becoming more present in critical environments, where security is paramount. Infamous attacks, such as Mirai, have shown the insecurity of the devices that power the IoT, as well as the potential of such large-scale attacks. Therefore, it is important to secure these embedded systems that form the backbone of the IoT. However, the particular nature of these devices and their resource constraints mean that the most cost-effective manner of securing these devices is to secure them before they are deployed, by minimizing the number of vulnerabilities they ship. To this end, fuzzing has proved itself as a valuable technique for automated vulnerability finding, where specially crafted inputs are fed to programs in order to trigger vulnerabilities and crash the system. In this survey, we link the world of embedded IoT devices and fuzzing. For this end, we list the particularities of the embedded world as far as security is concerned, we perform a literature review on fuzzing techniques and proposals, studying their applicability to embedded IoT devices and, finally, we present future research directions by pointing out the gaps identified in the review.},
	number = {13},
	journal = {IEEE Internet of Things Journal},
	author = {Eceiza, Maialen and Flores, Jose Luis and Iturbe, Mikel},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Embedded system, Embedded systems, Fuzzing, Hardware, Internet of Things, Security, Software, Task analysis, fuzzing, internet of Things (IoT), software testing, vulnerabilities},
	pages = {10390--10411},
}

@inproceedings{fioraldi_use_2021,
	title = {The {Use} of {Likely} {Invariants} as {Feedback} for {Fuzzers}},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/fioraldi},
	language = {en},
	urldate = {2021-11-29},
	author = {Fioraldi, Andrea and D'Elia, Daniele Cono and Balzarotti, Davide},
	year = {2021},
	pages = {2829--2846},
}

@inproceedings{katz_detecting_2020,
	title = {Detecting {Execution} {Anomalies} {As} an {Oracle} for {Autonomy} {Software} {Robustness}},
	doi = {10.1109/ICRA40945.2020.9197060},
	abstract = {We propose a method for detecting execution anomalies in robotics and autonomy software. The algorithm uses system monitoring techniques to obtain profiles of executions. It uses a clustering algorithm to create clusters of those executions, representing nominal execution. A distance metric determines whether additional execution profiles belong to the existing clusters or should be considered anomalies. The method is suitable for identifying faults in robotics and autonomy systems. We evaluate the technique in simulation on two robotics systems, one of which is a real-world industrial system. We find that our technique works well to detect possibly unsafe behavior in autonomous systems.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Katz, Deborah S. and Hutchison, Casidhe and Zizyte, Milda and Goues, Claire Le},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Clustering algorithms, Instruments, Safety, Service robots, Testing, Tools},
	pages = {9366--9373},
}

@misc{isoiec_jtc_1sc_41_isoiec_2018,
	title = {{ISO}/{IEC} 30141:2018 {Internet} of {Things} ({IoT}) — {Reference} {Architecture}},
	url = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/06/56/65695.html},
	language = {en},
	urldate = {2021-11-29},
	journal = {ISO},
	author = {{ISO/IEC JTC 1/SC 41}},
	month = aug,
	year = {2018},
}

@article{rymaszewska_iot_2017,
	series = {Service {Implementation} in {Manufacturing} {Firms}: {Strategy}, {Economics} and {Practice}},
	title = {{IoT} powered servitization of manufacturing – an exploratory case study},
	volume = {192},
	issn = {0925-5273},
	url = {https://www.sciencedirect.com/science/article/pii/S0925527317300531},
	doi = {10.1016/j.ijpe.2017.02.016},
	abstract = {More than ever companies are challenged to rethink their offerings while simultaneously being provided with a unique opportunity for creating or recreating their product-service systems. This paper seeks to address how servitisation can utilise the third wave of Internet development, referred to as the Internet of Things (IoT), which may unlock the potential for innovative product-service systems on an unprecedented scale. By providing an analysis of this technological breakthrough and the literature on servitisation, these concepts are combined to address the question of how organisations offering product-service systems can reap the benefits that the IoT. An analysis of three successful IoT implementation cases in manufacturing companies, representing different industry sectors such as metal processing, power generation and distribution, is provided. The results of the empirical research presented in the paper provide an insight into different ways of creating value in servitisation. The paper also proposes a framework that is aimed at proving a better understanding of how companies can create value, and add it to their servitisation processes with, the data obtained by the IoT based solutions. From the value chain perspective, IoT aided servitisation enables organisations to extend their value chains in order better serve their customers which, in turn, might result in increased profitability. The article proposes further research avenues, and offers valuable insight for practitioners.},
	language = {en},
	urldate = {2021-11-28},
	journal = {International Journal of Production Economics},
	author = {Rymaszewska, Anna and Helo, Petri and Gunasekaran, Angappa},
	month = oct,
	year = {2017},
	keywords = {Creation, Internet of Things, IoT, Manufacturing, Servitization, Value},
	pages = {92--105},
}

@inproceedings{salva_using_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Model} {Learning} for the {Generation} of {Mock} {Components}},
	isbn = {978-3-030-64881-7},
	doi = {10.1007/978-3-030-64881-7_1},
	abstract = {Mocking objects is a common technique that substitutes parts of a program to simplify the test case development, to increase test coverage or to speed up performance. Today, mocks are almost exclusively used with object oriented programs. But mocks could offer the same benefits with communicating systems to make them more reliable. This paper proposes a model-based approach to help developers generate mocks for this kind of system, i.e. systems made up of components interacting with each other by data networks and whose communications can be monitored. The approach combines model learning to infer models from event logs, quality metric measurements to help chose the components that may be replaced by mocks, and mock generation and execution algorithms to reduce the mock development time. The approach has been implemented as a tool chain with which we performed experimentations to evaluate its benefits in terms of usability and efficiency.},
	language = {en},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Salva, Sébastien and Blot, Elliott},
	editor = {Casola, Valentina and De Benedictis, Alessandra and Rak, Massimiliano},
	year = {2020},
	keywords = {Communicating systems, Mock, Model learning, Quality metrics},
	pages = {3--19},
}

@inproceedings{marri_empirical_2009-1,
	title = {An empirical study of testing file-system-dependent software with mock objects},
	doi = {10.1109/IWAST.2009.5069054},
	abstract = {Unit testing is a technique of testing a single unit of a program in isolation. The testability of the unit under test can be reduced when the unit interacts with its environment. The construction of high-covering unit tests and their execution require appropriate interactions with the environment such as a file system or database. To help set up the required environment, developers can use mock objects to simulate the behavior of the environment. In this paper, we present an empirical study to analyze the use of mock objects to test file-system-dependent software. We use a mock object of the FileSystem API provided with the Pex automatic testing tool in our study. We share our insights gained on the benefits of using mock objects in unit testing and discuss the faced challenges.},
	booktitle = {2009 {ICSE} {Workshop} on {Automation} of {Software} {Test}},
	author = {Marri, Madhuri R. and Xie, Tao and Tillmann, Nikolai and de Halleux, Jonathan and Schulte, Wolfram},
	month = may,
	year = {2009},
	keywords = {Automatic testing, Computer science, Databases, File systems, Logic, Software testing, System testing},
	pages = {149--153},
}

@inproceedings{galler_automatically_2010-1,
	address = {New York, NY, USA},
	series = {{AST} '10},
	title = {Automatically extracting mock object behavior from {Design} by {Contract}\&\#x2122; specification for test data generation},
	isbn = {978-1-60558-970-1},
	url = {https://doi.org/10.1145/1808266.1808273},
	doi = {10.1145/1808266.1808273},
	abstract = {Test data generation is an important task in the process of automated unit test generation. Random and heuristic approaches are well known for test input data generation. Unfortunately, in the presence of complex pre-conditions especially in the case of non-primitive data types those approaches often fail. A promising technique for generating an object that exactly satisfies a given pre-condition is mocking, i.e., replacing the concrete implementation with an implementation only considering the necessary behavior for a specific test case. In this paper we follow this technique and present an approach for automatically deriving the behavior of mock objects from given Design by Contract™ specifications. The generated mock objects behave according to the Design by Contract™ specification of the original class. Furthermore, we make sure that the observed behavior of the mock object satisfies the pre-condition of the method under test. We evaluate the approach using the Java implementations of 20 common Design Patterns and a stack based calculator. Our approach clearly outperforms pure random data generation in terms of line coverage.},
	urldate = {2021-11-26},
	booktitle = {Proceedings of the 5th {Workshop} on {Automation} of {Software} {Test}},
	publisher = {Association for Computing Machinery},
	author = {Galler, Stefan J. and Maller, Andreas and Wotawa, Franz},
	month = may,
	year = {2010},
	keywords = {Design-by-Contract, automated unit testing, mock object, test data generation},
	pages = {43--50},
}

@book{runeson_case_2012,
	edition = {1st},
	title = {Case {Study} {Research} in {Software} {Engineering}: {Guidelines} and {Examples}},
	isbn = {978-1-118-10435-4},
	shorttitle = {Case {Study} {Research} in {Software} {Engineering}},
	abstract = {Based on their own experiences of in-depth case studies of software projects in international corporations, in this bookthe authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering. This is the first software engineering specific book on thecase study research method.},
	publisher = {Wiley Publishing},
	author = {Runeson, Per and Host, Martin and Rainer, Austen and Regnell, Bjorn},
	year = {2012},
}

@inproceedings{salva_using_2020-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Model} {Learning} for the {Generation} of {Mock} {Components}},
	isbn = {978-3-030-64881-7},
	doi = {10.1007/978-3-030-64881-7_1},
	abstract = {Mocking objects is a common technique that substitutes parts of a program to simplify the test case development, to increase test coverage or to speed up performance. Today, mocks are almost exclusively used with object oriented programs. But mocks could offer the same benefits with communicating systems to make them more reliable. This paper proposes a model-based approach to help developers generate mocks for this kind of system, i.e. systems made up of components interacting with each other by data networks and whose communications can be monitored. The approach combines model learning to infer models from event logs, quality metric measurements to help chose the components that may be replaced by mocks, and mock generation and execution algorithms to reduce the mock development time. The approach has been implemented as a tool chain with which we performed experimentations to evaluate its benefits in terms of usability and efficiency.},
	language = {en},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Salva, Sébastien and Blot, Elliott},
	editor = {Casola, Valentina and De Benedictis, Alessandra and Rak, Massimiliano},
	year = {2020},
	keywords = {Communicating systems, Mock, Model learning, Quality metrics},
	pages = {3--19},
}

@inproceedings{tillmann_mock-object_2006,
	title = {Mock-object generation with behavior},
	doi = {10.1109/ASE.2006.51},
	abstract = {Unit testing is a popular way to guide software development and testing. Each unit test should target a single feature, but in practice it is difficult to test features in isolation. Mock objects are a well-known technique to substitute parts of a program which are irrelevant for a particular unit test. Today mock objects are usually written manually supported by tools that generate method stubs or distill behavior from existing programs. We have developed a prototype tool based on symbolic execution of .NET code that generates mock objects including their behavior by analyzing all uses of the mock object in a given unit test. It is not required that an actual implementation of the mocked behavior exists. We are working towards an integration of our tool into Visual Studio Team System},
	booktitle = {21st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE}'06)},
	author = {Tillmann, Nikolai and Schulte, Wolfram},
	month = sep,
	year = {2006},
	note = {ISSN: 1938-4300},
	keywords = {Concrete, Debugging, Instruments, Programming, Prototypes, Software engineering, Software testing},
	pages = {365--368},
}

@article{ernst_daikon_2007,
	series = {Special issue on {Experimental} {Software} and {Toolkits}},
	title = {The {Daikon} system for dynamic detection of likely invariants},
	volume = {69},
	issn = {0167-6423},
	url = {https://www.sciencedirect.com/science/article/pii/S016764230700161X},
	doi = {10.1016/j.scico.2007.01.015},
	abstract = {Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x=a), non-zero (x≠0), being in a range (a≤x≤b), linear relationships (y=ax+b), ordering (x≤y), functions from a library (x=fn(y)), containment (x∈y), sortedness (xissorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C++, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikon’s output has been used for generating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csail.mit.edu/daikon/.},
	language = {en},
	number = {1},
	urldate = {2021-11-23},
	journal = {Science of Computer Programming},
	author = {Ernst, Michael D. and Perkins, Jeff H. and Guo, Philip J. and McCamant, Stephen and Pacheco, Carlos and Tschantz, Matthew S. and Xiao, Chen},
	month = dec,
	year = {2007},
	keywords = {Daikon, Dynamic analysis, Dynamic invariant detection, Inductive logic programming, Inference, Invariant, Likely invariant, Program understanding, Specification, Specification mining},
	pages = {35--45},
}

@book{marwick2013status,
	title = {Status update},
	publisher = {Yale University Press},
	author = {Marwick, Alice E},
	year = {2013},
}

@article{winner_artifacts_1980,
	title = {Do {Artifacts} {Have} {Politics}?},
	language = {en},
	journal = {Daedalus},
	author = {Winner, Langdon},
	year = {1980},
	pages = {17},
}

@article{jonassen2014engineers,
	title = {Engineers as problem solvers},
	journal = {Cambridge handbook of engineering education research},
	author = {Jonassen, David H},
	year = {2014},
	note = {Publisher: Cambridge University Press New York},
	pages = {103--118},
}

@article{downey2014normative,
	title = {The normative contents of engineering formation: {Engineering} studies},
	journal = {The Cambridge Handbook of Engineering Education Research},
	author = {Downey, GL},
	year = {2014},
	note = {Publisher: Cambridge University Press},
	pages = {693--711},
}

@misc{noauthor_case_nodate,
	title = {Case {Studies} - {Trust} \& {Safety} {Foundation} {Project}},
	url = {https://www.tsf.foundation/case-studies},
	urldate = {2021-11-22},
}

@inproceedings{winkler2021partial,
	title = {A partial replication of “{DeepBugs}: {A} learning approach to name-based bug detection”},
	booktitle = {Proceedings of the 29th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering ({ESEC}/{FSE}'21 artifact track)},
	author = {Winkler, Jordan Matthew and Agarwal, Abhimanyu and Tung, Caleb and Ugalde, Dario Rios and Jung, Young Jin and Davis, James C.},
	year = {2021},
}

@inproceedings{fu2019edgewise,
	title = {Edgewise: a better stream processing engine for the edge},
	booktitle = {2019 {USENIX} annual technical conference ({USENIX} {ATC})},
	author = {Fu, Xinwei and Ghaffar, Talha and Davis, James C and Lee, Dongyoon},
	year = {2019},
	pages = {929--946},
}

@inproceedings{davis2017case,
	title = {The case of the poisoned event handler: {Weaknesses} in the {Node}.js event-driven architecture},
	booktitle = {Proceedings of the 10th european workshop on systems security ({EuroSec})},
	author = {Davis, James and Kildow, Gregor and Lee, Dongyoon},
	year = {2017},
	pages = {1--6},
}

@article{kazerouni2021fast,
	title = {Fast and accurate incremental feedback for students’ software tests using selective mutation analysis},
	volume = {175},
	journal = {Journal of Systems and Software (JSS)},
	author = {Kazerouni, Ayaan M and Davis, James C and Basak, Arinjoy and Shaffer, Clifford A and Servant, Francisco and Edwards, Stephen H},
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {110905},
}

@inproceedings{rupprecht2020improving,
	title = {Improving reproducibility of data science pipelines through transparent provenance capture},
	volume = {13},
	booktitle = {Proceedings of the {VLDB} endowment ({VLDB})},
	author = {Rupprecht, Lukas and Davis, James C and Arnold, Constantine and Gur, Yaniv and Bhagwat, Deepavali},
	year = {2020},
	note = {Number: 12
tex.organization: VLDB Endowment},
	pages = {3354--3368},
}

@inproceedings{rupprecht2019ursprung,
	title = {Ursprung: {Provenance} for large-scale analytics environments},
	booktitle = {Proceedings of the 2019 international conference on management of data ({SIGMOD}-{Demo})},
	author = {Rupprecht, Lukas and Davis, James C and Arnold, Constantine and Lubbock, Alexander and Tyson, Darren and Bhagwat, Deepavali},
	year = {2019},
	pages = {1989--1992},
}

@inproceedings{wittern2019empirical,
	title = {An empirical study of {GraphQL} schemas},
	booktitle = {International conference on service-oriented computing ({ICSOC})},
	author = {Wittern, Erik and Cha, Alan and Davis, James C and Baudart, Guillaume and Mandel, Louis},
	year = {2019},
	note = {tex.organization: Springer, Cham},
	pages = {3--19},
}

@inproceedings{davis2019rethinking,
	title = {Rethinking regex engines to address {ReDoS}},
	booktitle = {Proceedings of the 2019 27th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering ({ESEC}/{FSE} student research competition)},
	author = {Davis, James C},
	year = {2019},
	pages = {1256--1258},
}

@inproceedings{davis2019testing,
	title = {Testing regex generalizability and its implications: {A} large-scale many-language measurement study},
	booktitle = {2019 34th {IEEE}/{ACM} international conference on automated software engineering ({ASE})},
	author = {Davis, James C and Moyer, Daniel and Kazerouni, Ayaan M and Lee, Dongyoon},
	year = {2019},
	note = {tex.organization: IEEE},
	pages = {427--439},
}

@inproceedings{davis2019aren,
	title = {Why aren’t regular expressions a lingua franca? an empirical study on the re-use and portability of regular expressions},
	booktitle = {Proceedings of the 2019 27th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering ({ESEC}/{FSE})},
	author = {Davis, James C and Michael IV, Louis G and Coghlan, Christy A and Servant, Francisco and Lee, Dongyoon},
	year = {2019},
	pages = {443--454},
}

@misc{davis2020file,
	title = {File metadata verification in a distributed file system},
	publisher = {Google Patents},
	author = {Davis, James C and Davis, Willard A},
	year = {2020},
}

@misc{davis2021injection,
	title = {Injection of simulated hardware failure(s) in a file system for establishing file system tolerance-to-storage-failure(s)},
	author = {Davis, James C and Davis, Willard A},
	year = {2021},
}

@inproceedings{davis2017node,
	title = {Node.fz: {Fuzzing} the server-side event-driven architecture},
	booktitle = {Proceedings of the twelfth european conference on computer systems ({EuroSys})},
	author = {Davis, James and Thekumparampil, Arun and Lee, Dongyoon},
	year = {2017},
	pages = {145--160},
}

@inproceedings{cha2020principled,
	title = {A principled approach to {GraphQL} query cost analysis},
	booktitle = {Proceedings of the 28th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering},
	author = {Cha, Alan and Wittern, Erik and Baudart, Guillaume and Davis, James C and Mandel, Louis and Laredo, Jim A},
	year = {2020},
	pages = {257--268},
}

@misc{davis2018detection,
	title = {Detection of file corruption in a distributed file system},
	publisher = {Google Patents},
	author = {Davis, James C and Davis, Willard A and Knop, Felipe},
	year = {2018},
}

@phdthesis{davis2020impact,
	title = {On the impact and defeat of regular expression denial of service},
	school = {Virginia Tech},
	author = {Davis, James Collins},
	year = {2020},
}

@inproceedings{davis2021using,
	title = {Using selective memoization to defeat regular expression denial of service ({ReDoS})},
	booktitle = {2021 {IEEE} symposium on security and privacy ({SP}), los alamitos, {CA}, {USA}},
	author = {Davis, James C and Servant, Francisco and Lee, Dongyoon},
	year = {2021},
	pages = {543--559},
}

@article{davisplausible,
	title = {Plausible plausible deniability with an epistemological gap},
	author = {Davis, James C},
}

@misc{davis2021performing,
	title = {Performing hierarchical provenance collection},
	publisher = {Google Patents},
	author = {Davis, James Collins and Rupprecht, Lukas and Bhagwat, Deepavali and Arnold, Constantine and Sawdon, Wayne},
	year = {2021},
}

@article{herbold2020large,
	title = {Large-scale manual validation of bug fixing commits: {A} fine-grained analysis of tangling},
	journal = {arXiv preprint arXiv:2011.06244},
	author = {Herbold, Steffen and Trautsch, Alexander and Ledel, Benjamin and Aghamohammadi, Alireza and Ghaleb, Taher Ahmed and Chahal, Kuljit Kaur and Bossenmaier, Tim and Nagaria, Bhaveet and Makedonski, Philip and Ahmadabadi, Matin Nili and {others}},
	year = {2020},
}

@inproceedings{davis2018sense,
	title = {A sense of time for javascript and {Node}.js: {First}-class timeouts as a cure for event handler poisoning},
	booktitle = {27th {USENIX} security symposium ({USENIX} security)},
	author = {Davis, James C and Williamson, Eric R and Lee, Dongyoon},
	year = {2018},
	pages = {343--359},
}

@misc{davis2018testing,
	title = {Testing of lock managers in computing environments},
	publisher = {Google Patents},
	author = {Davis, Willard A and Davis, James C},
	year = {2018},
}

@misc{noauthor_yolo_2021,
	title = {{YOLO} {Object} {Detectors}, {You} {Only} {Look} {Once}},
	url = {https://github.com/tensorflow/models/tree/master/official/vision/beta/projects/yolo},
	abstract = {Models and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.},
	urldate = {2021-11-22},
	journal = {GitHub},
	year = {2021},
}


@inproceedings{goel2021low,
	title = {Low-power multi-camera object re-identification using hierarchical neural networks},
	volume = {2021},
	booktitle = {{ACM}/{IEEE} international symposium on low power electronics and design ({ISLPED})},
	author = {Goel, Abhinav and Tung, Caleb and Hu, Xiao and Wang, Haobo and Davis, James C and Thiruvathukal, George K and Lu, Yung-Hsiang},
	year = {2021},
}

@inproceedings{goel2022efficient,
	title = {Efficient computer vision on edge devices with pipeline-parallel hierarchical neural networks},
	booktitle = {Asia and south pacific design automation conference ({ASP}-{DAC})},
	author = {Goel, Abhinav and Tung, Caleb and Hu, Xiao and Thiruvathukal, George and Davis, James C and Lu, Yung-Hsiang},
	year = {2022},
}

@misc{noauthor_add_2017,
	title = {Add moderation tool to edit content warning on a post · {Issue} \#1307 · mastodon/mastodon},
	url = {https://github.com/mastodon/mastodon/issues/1307},
	abstract = {Rather than asking users to delete posts, we should really add a mechanism for being able to edit the content warning of a post in the moderation tools, both through reports and also with a clickab...},
	language = {en},
	urldate = {2021-11-21},
	journal = {GitHub},
	year = {2017},
}

@misc{noauthor_disallow_2018,
	title = {Disallow access to private posts older than the follow relationship · {Issue} \#8283 · mastodon/mastodon},
	url = {https://github.com/mastodon/mastodon/issues/8283},
	abstract = {It appears people have different expectations as to whether posts older than a follow relationship should be accessible to new followers. I\&\#39;m honestly not sure what the best behavior should be....},
	language = {en},
	urldate = {2021-11-21},
	journal = {GitHub},
	year = {2018},
}

@misc{noauthor_mastodon_nodate,
	title = {Mastodon {Homepage}},
	url = {https://joinmastodon.org},
	urldate = {2021-11-21},
}

@misc{tyler_cave_what_2019,
	title = {What is {Mastodon} and why is everyone talking about it?},
	url = {https://www.androidauthority.com/what-is-mastodon-1052151/},
	urldate = {2021-11-21},
	author = {{Tyler Cave}},
	year = {2019},
}

@misc{valens_ana_mastodon_2021,
	title = {Mastodon is crumbling—and many blame its creator},
	url = {https://www.dailydot.com/debug/mastodon-fediverse-eugen-rochko/},
	urldate = {2021-11-21},
	author = {{Valens, Ana}},
	year = {2021},
}

@misc{noauthor_mastodon_2019,
	title = {Mastodon is crumbling—and many blame its creator},
	url = {https://www.dailydot.com/debug/mastodon-fediverse-eugen-rochko/},
	abstract = {It was hailed as a progressive alternative to Twitter. But marginalized, queer users are being alienated by well-off tech bros.},
	language = {en-US},
	urldate = {2021-11-21},
	journal = {The Daily Dot},
	month = jan,
	year = {2019},
}

@article{manes_art_2021,
	title = {The {Art}, {Science}, and {Engineering} of {Fuzzing}: {A} {Survey}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {The {Art}, {Science}, and {Engineering} of {Fuzzing}},
	doi = {10.1109/TSE.2019.2946563},
	abstract = {Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.},
	number = {11},
	journal = {IEEE Transactions on Software Engineering},
	author = {Manès, Valentin J.M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Computer bugs, Fuzzing, Security, Software security, Terminology, automated software testing, fuzz testing, fuzzing},
	pages = {2312--2331},
}

@article{sharp_ethnographic_2004,
	title = {An {Ethnographic} {Study} of {XP} {Practice}},
	volume = {9},
	issn = {1382-3256},
	url = {http://link.springer.com/10.1023/B:EMSE.0000039884.79385.54},
	doi = {10.1023/B:EMSE.0000039884.79385.54},
	abstract = {Agile methods are a response to more rigorous and traditional approaches to software development which are perceived to have failed both customers and software development practitioners. eXtreme Programming (XP) is an example agile method and we report on an ethnographic study of XP practice carried out in a small company developing web-based intelligent advertisements. We identify ﬁve characterizing themes within XP practice and summarize these ﬁndings in terms of XP culture.},
	language = {en},
	number = {4},
	urldate = {2021-11-18},
	journal = {Empirical Software Engineering},
	author = {Sharp, Helen and Robinson, Hugh},
	month = dec,
	year = {2004},
	pages = {353--375},
}

@inproceedings{mclaughlin_regulator_2022,
	title = {Regulator: {Dynamic} {Analysis} to {Detect} {ReDoS}},
	abstract = {Regular expressions (regexps) are a convenient way for programmers to express complex string searching logic. Several popular programming languages expose an interface to a regexp matching subsystem, either by language-level primitives or through standard libraries. The implementations behind these matching systems vary greatly in their capabilities and running-time characteristics. In particular, backtracking matchers may exhibit worst-case running-time that is either linear, polynomial, or exponential in the length of the string being searched. Such super-linear worst-case regexps expose applications to Regular Expression Denial-of-Service (ReDoS) when inputs can be controlled by an adversarial attacker. In this work, we investigate the impact of ReDoS in backtracking engines, a popular type of engine used by most programming languages. We evaluate several existing tools against a dataset of broadly collected regexps, and ﬁnd that despite extensive theoretical work in this ﬁeld, none are able to achieve both high precision and high recall. To address this gap in existing work, we develop REGULATOR, a novel dynamic, fuzzer-based analysis system for identifying regexps vulnerable to ReDoS. We implement this system by directly instrumenting a popular backtracking regexp engine, which increases the scope of supported regexp syntax and features over prior work. Finally, we evaluate this system against three common regexp datasets, and demonstrate a seven-fold increase in true positives discovered when comparing against existing tools.},
	language = {en},
	booktitle = {{USENIX} {Security}},
	author = {McLaughlin, Robert and Pagani, Fabio and Spahn, Noah and Kruegel, Christopher and Vigna, Giovanni},
	year = {2022},
	pages = {17},
}

@article{saltzer_protection_1975,
	title = {The protection of information in computer systems},
	volume = {63},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1451869/},
	doi = {10.1109/PROC.1975.9939},
	language = {en},
	number = {9},
	urldate = {2021-11-14},
	journal = {Proceedings of the IEEE},
	author = {Saltzer, J.H. and Schroeder, M.D.},
	year = {1975},
	pages = {1278--1308},
}

@incollection{us_department_of_defense_department_1985,
	address = {London},
	title = {Department of {Defense} {Trusted} {Computer} {System} {Evaluation} {Criteria}},
	isbn = {978-0-333-53947-7 978-1-349-12020-8},
	url = {http://link.springer.com/10.1007/978-1-349-12020-8_1},
	language = {en},
	urldate = {2021-11-14},
	booktitle = {The ‘{Orange} {Book}’ {Series}},
	publisher = {Palgrave Macmillan UK},
	author = {{US Department of Defense}},
	editor = {{US Department of Defense}},
	year = {1985},
	doi = {10.1007/978-1-349-12020-8_1},
	pages = {1--129},
}

@inproceedings{rigby_convergent_2013,
	address = {Saint Petersburg, Russia},
	title = {Convergent contemporary software peer review practices},
	isbn = {978-1-4503-2237-9},
	url = {http://dl.acm.org/citation.cfm?doid=2491411.2491444},
	doi = {10.1145/2491411.2491444},
	abstract = {Software peer review is practiced on a diverse set of software projects that have drastically diﬀerent settings, cultures, incentive systems, and time pressures. In an eﬀort to characterize and understand these diﬀerences we examine two Google-led projects, Android and Chromium OS, three Microsoft projects, Bing, Oﬃce, and MS SQL, and projects internal to AMD. We contrast our ﬁndings with data taken from traditional software inspection conducted on a Lucent project and from open source software peer review on six projects, including Apache, Linux, and KDE. Our measures of interest include the review interval, the number of developers involved in review, and proxy measures for the number of defects found during review. We ﬁnd that despite diﬀerences among projects, many of the characteristics of the review process have independently converged to similar values which we think indicate general principles of code review practice. We also introduce a measure of the degree to which knowledge is shared during review. This is an aspect of review practice that has traditionally only had experiential support. Our knowledge sharing measure shows that conducting peer review increases the number of distinct ﬁles a developer knows about by 66\% to 150\% depending on the project. This paper is one of the ﬁrst studies of contemporary review in software ﬁrms and the most diverse study of peer review to date.},
	language = {en},
	urldate = {2021-11-09},
	booktitle = {Proceedings of the 2013 9th {Joint} {Meeting} on {Foundations} of {Software} {Engineering} - {ESEC}/{FSE} 2013},
	publisher = {ACM Press},
	author = {Rigby, Peter C. and Bird, Christian},
	year = {2013},
	pages = {202},
}

@inproceedings{czerwonka_code_2015,
	address = {Florence, Italy},
	title = {Code {Reviews} {Do} {Not} {Find} {Bugs}. {How} the {Current} {Code} {Review} {Best} {Practice} {Slows} {Us} {Down}},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7202946/},
	doi = {10.1109/ICSE.2015.131},
	abstract = {Because of its many uses and benefits, code reviews are a standard part of the modern software engineering workflow. Since they require involvement of people, code reviewing is often the longest part of the code integration activities. Using experience gained at Microsoft and with support of data, we posit (1) that code reviews often do not find functionality issues that should block a code submission; (2) that effective code reviews should be performed by people with specific set of skills; and (3) that the social aspect of code reviews cannot be ignored. We find that we need to be more sophisticated with our guidelines for the code review workflow. We show how our findings from code reviewing practice influence our code review tools at Microsoft. Finally, we assert that, due to its costs, code reviewing practice is a topic deserving to be better understood, systematized and applied to software engineering workflow with more precision than the best practice currently prescribes.},
	language = {en},
	urldate = {2021-11-09},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Czerwonka, Jacek and Greiler, Michaela and Tilford, Jack},
	month = may,
	year = {2015},
	pages = {27--28},
}

@article{alhazmi2021m,
	title = {I’m all ears! {Listening} to software developers on putting {GDPR} principles into software development practice},
	journal = {Personal and Ubiquitous Computing},
	author = {Alhazmi, Abdulrahman and Arachchilage, Nalin Asanka Gamagedara},
	year = {2021},
	note = {Publisher: Springer},
	pages = {1--14},
}

@article{alhazmi_im_2021,
	title = {I’m all ears! {Listening} to software developers on putting {GDPR} principles into software development practice},
	volume = {25},
	issn = {1617-4909, 1617-4917},
	url = {https://link.springer.com/10.1007/s00779-021-01544-1},
	doi = {10.1007/s00779-021-01544-1},
	abstract = {Previous research has been carried out to identify the impediments that prevent developers from incorporating privacy protocols into software applications. No research has been carried out to find out why developers are not able to develop systems that preserve privacy while specifically considering the General Data Protection Regulation principles (GDPR principles). Consequently, this paper aims to examine the issues, which prevent developers from creating applications, which consider and include GDPR principles into their software systems. From our research findings, we identified the lack of familiarity with GDPR principles by developers as one of the obstacles that prevent GDPR onboarding. Those who were familiar with the principles did not have the requisite knowledge about the principles including their techniques. Developers focused on functional than on privacy requirements. Unavailability of resourceful online tools and lack of support from institutions and clients were also identified as issues inimical to the onboarding of GDPR principles.},
	language = {en},
	number = {5},
	urldate = {2021-11-09},
	journal = {Personal and Ubiquitous Computing},
	author = {Alhazmi, Abdulrahman and Arachchilage, Nalin Asanka Gamagedara},
	month = oct,
	year = {2021},
	pages = {879--892},
}

@inproceedings{ma_detection_2016,
	title = {Detection of {Runtime} {Conflicts} among {Services} in {Smart} {Cities}},
	doi = {10.1109/SMARTCOMP.2016.7501688},
	abstract = {The populations of large cities around the world are growing rapidly. Cities are beginning to address this problem by implementing significant sensing and actuation infrastructure and building services on this infrastructure. However, as the density of sensing and actuation increases and as the complexities of services grow there is an increasing potential for conflicts across Smart City services. These conflicts can cause unsafe situations and disrupt the benefits that the services were originally intended to provide. Although some of the conflicts can be detected and avoided during designing the services, many can still occur unpredictably during runtime. This paper carefully defines and enumerates the main issues regarding the detection and resolution of runtime conflicts in smart cities. In particular, it focuses on conflicts that arise across services. This issue is becoming more and more important as Smart City designs attempt to integrate services from different domains (transportation, energy, public safety, emergency, medical, and many others). Research challenges are identified and then addressed that deal with uncertainty, dynamism, real-time, mobility and spatio-temporal availability, duration and scale of effect, efficiency, and ownership. A watchdog architecture is also described that oversees the services operating in a Smart City. This watchdog solution detects and resolves conflicts, it learns and adapts, and it provides additional inputs to decision making aspects of services. Using data from a Smart City dataset, an emulated set of services and activities using those services are created to perform a conflict analysis. A second analysis hypothesizes 41 future services across 5 domains. Both of these evaluations demonstrate the high probability of conflicts in smart cities of the future.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Ma, M. and Preum, S. Masud and Tarneberg, W. and Ahmed, M. and Ruiters, M. and Stankovic, J.},
	month = may,
	year = {2016},
	keywords = {Roads, Safety, Smart cities, Uncertainty, Vehicles},
	pages = {1--10},
}

@inproceedings{yuan_deresolver_2021,
	address = {New York, NY, USA},
	series = {{ICCPS} '21},
	title = {{DeResolver}: a decentralized negotiation and conflict resolution framework for smart city services},
	isbn = {978-1-4503-8353-0},
	shorttitle = {{DeResolver}},
	url = {https://doi.org/10.1145/3450267.3450538},
	doi = {10.1145/3450267.3450538},
	abstract = {As various smart services are increasingly deployed in modern cities, many unexpected conflicts arise due to various physical world couplings. Existing solutions for conflict resolution often rely on centralized control to enforce predetermined and fixed priorities of different services, which is challenging due to the inconsistent and private objectives of the services. Also, the centralized solutions miss opportunities to more effectively resolve conflicts according to their spatiotemporal locality of the conflicts. To address this issue, we design a decentralized negotiation and conflict resolution framework named DeResolver, which allows services to resolve conflicts by communicating and negotiating with each other to reach a Pareto-optimal agreement autonomously and efficiently. Our design features a two-level semi-supervised learning-based algorithm to predict acceptable proposals and their rankings of each opponent through the negotiation. Our design is evaluated with a smart city case study of three services: intelligent traffic light control, pedestrian service, and environmental control. In this case study, a data-driven evaluation is conducted using a large data set consisting of the GPS locations of 246 surveillance cameras and an automatic traffic monitoring system with more than 3 million records per day to extract real-world vehicle routes. The evaluation results show that our solution achieves much more balanced results, i.e., only increasing the average waiting time of vehicles, the measurement metric of intelligent traffic light control service, by 6.8\% while reducing the weighted sum of air pollutant emission, measured for environment control service, by 12.1\%, and the pedestrian waiting time, the measurement metric of pedestrian service, by 33.1\%, compared to priority-based solution.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the {ACM}/{IEEE} 12th {International} {Conference} on {Cyber}-{Physical} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yukun and Ma, Meiyi and Han, Songyang and Zhang, Desheng and Miao, Fei and Stankovic, John and Lin, Shan},
	month = may,
	year = {2021},
	keywords = {conflicts across services, decentralized resolution, multiple services negotiation, smart services},
	pages = {98--109},
}

@inproceedings{ma_cityguard_2017,
	address = {New York, NY, USA},
	series = {{IoTDI} '17},
	title = {{CityGuard}: {A} {Watchdog} for {Safety}-{Aware} {Conflict} {Detection} in {Smart} {Cities}},
	isbn = {978-1-4503-4966-6},
	shorttitle = {{CityGuard}},
	url = {https://doi.org/10.1145/3054977.3054989},
	doi = {10.1145/3054977.3054989},
	abstract = {Nowadays, increasing number of smart services are being developed and deployed in cities around the world. IoT platforms have emerged to integrate smart city services and city resources, and thus improve city performance in the domains of transportation, emergency, environment, public safety, etc. Despite the increasing intelligence of smart services and the sophistication of platforms, the safety issues in smart cities are not addressed adequately, especially the safety issues arising from the integration of smart services. Therefore, CityGuard, a safety-aware watchdog architecture is developed. To the best of our knowledge, it is the first architecture that detects and resolves conflicts among actions of different services considering both safety and performance requirements. Prior to developing CityGuard, safety and performance requirements and a spectrum of conflicts are specified. Sophisticated models are used to analyze secondary effects, and detect device and environmental conflicts. A simulation based on New York City is used for the evaluation. The results show that CityGuard (i) identifies unsafe actions and thus helps to prevent the city from safety hazards, (ii) detects and resolves two major types of conflicts, i.e., device and environmental conflicts, and (iii) improves the overall city performance.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
	month = apr,
	year = {2017},
	keywords = {City Safety, City Simulation, Conflict Detection, Smart City},
	pages = {259--270},
}

@techreport{mccall_safetap_2021,
	type = {report},
	title = {{SafeTAP}: {An} {Efficient} {Incremental} {Analyzer} for {Trigger}-{Action} {Programs}},
	shorttitle = {{SafeTAP}},
	url = {https://kilthub.cmu.edu/articles/report/SafeTAP_An_Efficient_Incremental_Analyzer_for_Trigger-Action_Programs/14792271/1},
	abstract = {Home automation rules that allow users to connect smart home devices using trigger-action programs (TAP) can interact in subtle and unexpected ways. Determining whether these rules are free of undesirable behavior is challenging; so researchers have developed tools to analyze rules and assist users. However, it is unclear whether users need such tools, and what help they need from such tools. To answer this question, we performed a user study where half of the participants were given our custom analysis tool SafeTAP and the other half were not. We found that users are not good at finding issues in their TAP rules, despite perceiving such tasks as easy. The user study also indicates that users would like to check their rules every time they make rule changes. Therefore, we designed a novel incremental symbolic model checking (SMC) algorithm, which extends the basic SMC algorithm of SafeTAP. SafeTAPΔ only performs analysis caused by the addition or removal of rules and reports only new violations that have not already been reported to the user. We evaluate the performance of SafeTAPΔ and show that incremental checking on average improves the performance by 6X when adding new rules.},
	language = {en},
	urldate = {2021-11-05},
	institution = {Carnegie Mellon University},
	author = {McCall, McKenna and Shezan, Faysal Hossain and Bichhawat, Abhishek and Cobb, Camille and Jia, Limin and Tian, Yuan and Grace, Cooper and Yang, Mitchell},
	month = jun,
	year = {2021},
	doi = {10.1184/R1/14792271.v1},
}

@inproceedings{alhanahnah_scalable_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {Scalable analysis of interaction threats in {IoT} systems},
	isbn = {978-1-4503-8008-9},
	url = {https://doi.org/10.1145/3395363.3397347},
	doi = {10.1145/3395363.3397347},
	abstract = {The ubiquity of Internet of Things (IoT) and our growing reliance on IoT apps are leaving us more vulnerable to safety and security threats than ever before. Many of these threats are manifested at the interaction level, where undesired or malicious coordinations between apps and physical devices can lead to intricate safety and security issues. This paper presents IoTCOM, an approach to automatically discover such hidden and unsafe interaction threats in a compositional and scalable fashion. It is backed with auto-mated program analysis and formally rigorous violation detection engines. IoTCOM relies on program analysis to automatically infer the relevant app’s behavior. Leveraging a novel strategy to trim the extracted app’s behavior prior to translating them to analyzable formal specifications,IoTCOM mitigates the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IoTCOM’s ability to effectively detect a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown, and to significantly outperform the existing techniques in terms of scalability.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Alhanahnah, Mohannad and Stevens, Clay and Bagheri, Hamid},
	month = jul,
	year = {2020},
	keywords = {Formal Verification, Interaction Threats, IoT Safety},
	pages = {272--285},
}

@inproceedings{chi_cross-app_2020,
	title = {Cross-{App} {Interference} {Threats} in {Smart} {Homes}: {Categorization}, {Detection} and {Handling}},
	shorttitle = {Cross-{App} {Interference} {Threats} in {Smart} {Homes}},
	doi = {10.1109/DSN48063.2020.00056},
	abstract = {Internet of Thing platforms prosper home automation applications (apps). Prior research concerns intra-app security. Our work reveals that automation apps, even secured individually, still cause a family of threats when they interplay, termed as Cross-App Interference (CAI) threats. We systematically categorize such threats and encode them using satisfiability modulo theories (SMT). We present HomeGuard, a system for detecting and handling CAI threats in real deployments. A symbolic executor is built to extract rule semantics, and instrumentation is utilized to capture configuration during app installation. Rules and configuration are checked against SMT models, the solutions of which indicate the existence of corresponding CAI threats. We further combine app functionalities, device attributes and CAI types to label the risk level of CAI instances. In our evaluation, HomeGuard discovers 663 CAI instances from 146 SmartThings market apps, imposing minor latency upon app installation and no runtime overhead.},
	booktitle = {2020 50th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	author = {Chi, Haotian and Zeng, Qiang and Du, Xiaojiang and Yu, Jiaping},
	month = jun,
	year = {2020},
	note = {ISSN: 1530-0889},
	keywords = {Actuators, Automation, Safety, Semantics, Smart homes, Temperature sensors, n/a},
	pages = {411--423},
}

@article{gold_ethics_2022,
	title = {Ethics in the mining of software repositories},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-021-10057-7},
	doi = {10.1007/s10664-021-10057-7},
	abstract = {Research in Mining Software Repositories (MSR) is research involving human subjects, as the repositories usually contain data about developers’ and users’ interactions with the repositories and with each other. The ethics issues raised by such research therefore need to be considered before beginning. This paper presents a discussion of ethics issues that can arise in MSR research, using the mining challenges from the years 2006 to 2021 as a case study to identify the kinds of data used. On the basis of contemporary research ethics frameworks we discuss ethics challenges that may be encountered in creating and using repositories and associated datasets. We also report some results from a small community survey of approaches to ethics in MSR research. In addition, we present four case studies illustrating typical ethics issues one encounters in projects and how ethics considerations can shape projects before they commence. Based on our experience, we present some guidelines and practices that can help in considering potential ethics issues and reducing risks.},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {Empirical Software Engineering},
	author = {Gold, Nicolas E. and Krinke, Jens},
	month = jan,
	year = {2022},
	pages = {17},
}

@article{kommeren_philips_2007,
	title = {Philips experiences in global distributed software development},
	volume = {12},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-007-9047-3},
	doi = {10.1007/s10664-007-9047-3},
	abstract = {Global software development is increasingly common. Main expected benefits are improvements in time-to-market efficiency and access to greater—and less costly—resources. A number of problems are still to be solved before the full potential of global development can be obtained. This paper describes the experience of over 10 years of global distributed development at Philips, derived from about 200 projects. We discuss the experience and lessons learnt from multi-site development. Main lessons learned are that explicit agreements and ways of working should be defined for the following areas needing the most attention; team coordination and communication, requirements and architectures, integration, and configuration management. In addition, we discuss the experience gained from subcontracting software development to suppliers. Main lesson learned from subcontracting software development is the need for explicit attention and ways of working with respect to selection of suppliers, specification of the work to be subcontracted and establishment and content of the contract.},
	language = {en},
	number = {6},
	urldate = {2021-11-02},
	journal = {Empirical Software Engineering},
	author = {Kommeren, Rob and Parviainen, Päivi},
	month = oct,
	year = {2007},
	pages = {647--660},
}

@inproceedings{leesatapornwongsa_taxdc_2016,
	address = {Atlanta Georgia USA},
	title = {{TaxDC}: {A} {Taxonomy} of {Non}-{Deterministic} {Concurrency} {Bugs} in {Datacenter} {Distributed} {Systems}},
	isbn = {978-1-4503-4091-5},
	shorttitle = {{TaxDC}},
	url = {https://dl.acm.org/doi/10.1145/2872362.2872374},
	doi = {10.1145/2872362.2872374},
	abstract = {We present TaxDC, the largest and most comprehensive taxonomy of non-deterministic concurrency bugs in distributed systems. We study 104 distributed concurrency (DC) bugs from four widely-deployed cloud-scale datacenter distributed systems, Cassandra, Hadoop MapReduce, HBase and ZooKeeper. We study DC-bug characteristics along several axes of analysis such as the triggering timing condition and input preconditions, error and failure symptoms, and ﬁx strategies, collectively stored as 2,083 classiﬁcation labels in TaxDC database. We discuss how our study can open up many new research directions in combating DC bugs.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Leesatapornwongsa, Tanakorn and Lukman, Jeffrey F. and Lu, Shan and Gunawi, Haryadi S.},
	month = mar,
	year = {2016},
	pages = {517--530},
}

@article{sun_conflict_2015,
	title = {Conflict {Detection} {Scheme} {Based} on {Formal} {Rule} {Model} for {Smart} {Building} {Systems}},
	volume = {45},
	issn = {2168-2305},
	doi = {10.1109/THMS.2014.2364613},
	abstract = {Smart building systems can provide flexible and configurational sensing and controlling operations according to users' requirements. As the number and the complexity of service rules customized by users have significantly increased, there is an increasing danger of conflict during the interaction process between users and the system. To address this issue, we propose a new rule conflict detection scheme tailored for the smart building system. First, we present a formal rule model UTEA based on User, Triggers, Environment entities, and Actuators. This model can handle not only controlled devices with discrete status but also real-valued environmental data such as temperature and humidity. In addition, this model takes multiple users with different authorities into account. Second, we define 11 rule relations and further classify conflicts into five categories. Third, we implement a rule storage system for detecting conflicts and design a conflict detection algorithm, which can detect the conflict between two rules as well as cycle conflict/multicross contradiction among multiple rules. We evaluated our scheme in a real smart building system with more than 30 000 service rules. The experiment results show that our scheme improves the performance in terms of error/missed-detection rates and running time.},
	number = {2},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Sun, Yan and Wang, Xukai and Luo, Hong and Li, Xiangyang},
	month = apr,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Human-Machine Systems},
	keywords = {Actuators, Conflict detection, Humidity, Intelligent sensors, Smart buildings, Sun, Temperature sensors, rule model, service, smart building system},
	pages = {215--227},
}

@inproceedings{ma_cityguard_2017-1,
	title = {{CityGuard}: {A} {Watchdog} for {Safety}-{Aware} {Conflict} {Detection} in {Smart} {Cities}},
	shorttitle = {{CityGuard}},
	abstract = {Nowadays, increasing number of smart services are being developed and deployed in cities around the world. IoT platforms have emerged to integrate smart city services and city resources, and thus improve city performance in the domains of transportation, emergency, environment, public safety, etc. Despite the increasing intelligence of smart services and the sophistication of platforms, the safety issues in smart cities are not addressed adequately, especially the safety issues arising from the integration of smart services. Therefore, CityGuard, a safety-aware watchdog architecture is developed. To the best of our knowledge, it is the first architecture that detects and resolves conflicts among actions of different services considering both safety and performance requirements. Prior to developing CityGuard, safety and performance requirements and a spectrum of conflicts are specified. Sophisticated models are used to analyze secondary effects, and detect device and environmental conflicts. A simulation based on New York City is used for the evaluation. The results show that CityGuard (i) identifies unsafe actions and thus helps to prevent the city from safety hazards, (ii) detects and resolves two major types of conflicts, i.e., device and environmental conflicts, and (iii) improves the overall city performance.},
	booktitle = {2017 {IEEE}/{ACM} {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation} ({IoTDI})},
	author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
	month = apr,
	year = {2017},
	keywords = {City Safety, City Simulation, Conflict Detection, Context, Emergency services, Roads, Smart City, Smart cities},
	pages = {259--270},
}

@article{nosco_industrial_nodate,
	title = {The {Industrial} {Age} of {Hacking}},
	abstract = {There is a cognitive bias in the hacker community to select a piece of software and invest signiﬁcant human resources into ﬁnding bugs in that software without any prior indication of success. We label this strategy depth-ﬁrst search and propose an alternative: breadth-ﬁrst search. In breadthﬁrst search, humans perform minimal work to enable automated analysis on a range of targets before committing additional time and effort to research any particular one. We present a repeatable human study that leverages teams of varying skill while using automation to the greatest extent possible. Our goal is a process that is effective at ﬁnding bugs; has a clear plan for the growth, coaching, and efﬁcient use of team members; and supports measurable, incremental progress. We derive an assembly-line process that improves on what was once intricate, manual work. Our work provides evidence that the breadth-ﬁrst approach increases the effectiveness of teams.},
	language = {en},
	author = {Nosco, Tim and Ziegler, Jared and Clark, Zechariah},
	pages = {19},
}

@inproceedings{ma_detection_2016-1,
	title = {Detection of {Runtime} {Conflicts} among {Services} in {Smart} {Cities}},
	doi = {10.1109/SMARTCOMP.2016.7501688},
	abstract = {The populations of large cities around the world are growing rapidly. Cities are beginning to address this problem by implementing significant sensing and actuation infrastructure and building services on this infrastructure. However, as the density of sensing and actuation increases and as the complexities of services grow there is an increasing potential for conflicts across Smart City services. These conflicts can cause unsafe situations and disrupt the benefits that the services were originally intended to provide. Although some of the conflicts can be detected and avoided during designing the services, many can still occur unpredictably during runtime. This paper carefully defines and enumerates the main issues regarding the detection and resolution of runtime conflicts in smart cities. In particular, it focuses on conflicts that arise across services. This issue is becoming more and more important as Smart City designs attempt to integrate services from different domains (transportation, energy, public safety, emergency, medical, and many others). Research challenges are identified and then addressed that deal with uncertainty, dynamism, real-time, mobility and spatio-temporal availability, duration and scale of effect, efficiency, and ownership. A watchdog architecture is also described that oversees the services operating in a Smart City. This watchdog solution detects and resolves conflicts, it learns and adapts, and it provides additional inputs to decision making aspects of services. Using data from a Smart City dataset, an emulated set of services and activities using those services are created to perform a conflict analysis. A second analysis hypothesizes 41 future services across 5 domains. Both of these evaluations demonstrate the high probability of conflicts in smart cities of the future.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Ma, M. and Preum, S. Masud and Tarneberg, W. and Ahmed, M. and Ruiters, M. and Stankovic, J.},
	month = may,
	year = {2016},
	keywords = {Roads, Safety, Smart cities, Uncertainty, Vehicles},
	pages = {1--10},
}

@article{kozlov_assessing_2008,
	title = {Assessing maintainability change over multiple software releases},
	volume = {20},
	issn = {1532060X, 15320618},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/smr.361},
	doi = {10.1002/smr.361},
	abstract = {The focus of the paper is to reveal the relationships between software maintainability and other internal software quality attributes. The source code characteristics of ﬁve Java-based open-source software products are analyzed using the software measurement tool SoftCalc. The relationships between maintainability and internal quality attributes are identiﬁed based on the Pearson product moment correlation analysis. Our results show negative correlations between maintainability and some well-known internal software quality attributes, as well as the ones between maintainability and complexity metrics. Particularly, according to our results, the Number of Data Variables Declared and the Decisional Complexity McClure Metric have the strongest correlations with maintainability. The results of our study, that is to say, knowledge about the relationships between internal software quality attributes and maintainability, can be used as a basis for improvement of software maintainability at earlier stages of the software development process. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2021-10-26},
	journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	author = {Kozlov, Denis and Koskinen, Jussi and Sakkinen, Markku and Markkula, Jouni},
	month = jan,
	year = {2008},
	pages = {31--58},
}

@inproceedings{ahmed_empirical_2015,
	address = {Beijing, China},
	title = {An {Empirical} {Study} of {Design} {Degradation}: {How} {Software} {Projects} {Get} {Worse} over {Time}},
	isbn = {978-1-4673-7899-4},
	shorttitle = {An {Empirical} {Study} of {Design} {Degradation}},
	url = {http://ieeexplore.ieee.org/document/7321186/},
	doi = {10.1109/ESEM.2015.7321186},
	abstract = {Method: We conducted an empirical study on the presence and evolution of code smells, used as an indicator of design degradation in 220 open source projects.
Results: The best approach to maintain the quality of a project is to spend time reducing both software defects (bugs) and design issues (refactoring). We found that design issues are frequently ignored in favor of fixing defects. We also found that design issues have a higher chance of being fixed in the early stages of a project, and that efforts to correct these stall as projects mature and the code base grows, leading to a build-up of problems.
Conclusions: From studying a large set of open source projects, our research suggests that while core contributors tend to fix design issues more often than non-core contributors, there is no difference once the relative quantity of commits is accounted for. We also show that design issues tend to build up over time.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {2015 {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {IEEE},
	author = {Ahmed, Iftekhar and Mannan, Umme Ayda and Gopinath, Rahul and Jensen, Carlos},
	month = oct,
	year = {2015},
	pages = {1--10},
}

@inproceedings{codabux_managing_2013,
	address = {San Francisco, CA, USA},
	title = {Managing technical debt: {An} industrial case study},
	isbn = {978-1-4673-6443-0},
	shorttitle = {Managing technical debt},
	url = {https://ieeexplore.ieee.org/document/6608672},
	doi = {10.1109/MTD.2013.6608672},
	abstract = {Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management’s high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20\% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {2013 4th {International} {Workshop} on {Managing} {Technical} {Debt} ({MTD})},
	publisher = {IEEE},
	author = {Codabux, Zadia and Williams, Byron},
	month = may,
	year = {2013},
	pages = {8--15},
}

@inproceedings{besker_how_2019,
	address = {Kallithea-Chalkidiki, Greece},
	title = {How {Regulations} of {Safety}-{Critical} {Software} {Affect} {Technical} {Debt}},
	isbn = {978-1-72813-421-5},
	url = {https://ieeexplore.ieee.org/document/8906517/},
	doi = {10.1109/SEAA.2019.00020},
	abstract = {In recent years in the software industry, the use of safety-critical software is increasing at a rapid rate. However, little is known about the relationship between safety-critical regulations and the management of technical debt. The research is based on interviews with 19 practitioners working in different safety-critical domains implementing software according to different safety regulation standards. The results are three-fold. First, the result shows that performing technical debt refactoring tasks in safety-critical software requires several additional activities and costs, compared to non-safety-critical software. This study has also identified several negative effects due to the impact of these regulatory requirements. Second, the results show that the safety-critical regulations strengthen the implementation of both source code and architecture and thereby initially limit the introduction of technical debt. However, at the same time, the regulations also force the software companies to perform later suboptimal work-around solutions that are counterproductive in achieving a high-quality software since the regulations constrain the possibility of performing optimal TD refactoring activities. Third, the result shows that technical debt refactoring decisions are heavily weighed on the costs associated with the application’s recertification process and that these decisions seldom include the benefits of the refactoring activities in a structured way.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {2019 45th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	publisher = {IEEE},
	author = {Besker, Terese and Martini, Antonio and Bosch, Jan},
	month = aug,
	year = {2019},
	pages = {74--81},
}

@article{paliotta_paying_nodate,
	title = {Paying off technical debt in safety-critical automotive software},
	language = {en},
	author = {Paliotta, John},
	pages = {2},
}

@article{cunningham1992wycash,
	title = {The {WyCash} portfolio management system},
	volume = {4},
	number = {2},
	journal = {ACM SIGPLAN OOPS Messenger},
	author = {Cunningham, Ward},
	year = {1992},
	note = {Publisher: ACM New York, NY, USA},
	pages = {29--30},
}

@misc{mcconnell2008managing,
	title = {Managing technical debt},
	author = {McConnell, Steve},
	year = {2008},
}

@inproceedings{ernst_measure_2015,
	address = {Bergamo Italy},
	title = {Measure it? {Manage} it? {Ignore} it? software practitioners and technical debt},
	isbn = {978-1-4503-3675-8},
	shorttitle = {Measure it?},
	url = {https://dl.acm.org/doi/10.1145/2786805.2786848},
	doi = {10.1145/2786805.2786848},
	abstract = {The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ernst, Neil A. and Bellomo, Stephany and Ozkaya, Ipek and Nord, Robert L. and Gorton, Ian},
	month = aug,
	year = {2015},
	pages = {50--60},
}

@article{lim_balancing_2012,
	title = {A {Balancing} {Act}: {What} {Software} {Practitioners} {Have} to {Say} about {Technical} {Debt}},
	volume = {29},
	issn = {0740-7459},
	shorttitle = {A {Balancing} {Act}},
	url = {http://ieeexplore.ieee.org/document/6280547/},
	doi = {10.1109/MS.2012.130},
	language = {en},
	number = {6},
	urldate = {2021-10-26},
	journal = {IEEE Software},
	author = {Lim, Erin and Taksande, Nitin and Seaman, Carolyn},
	month = nov,
	year = {2012},
	pages = {22--27},
}

@article{hofstede1984hofstede,
	title = {Hofstede's culture dimensions: {An} independent validation using {Rokeach}'s value survey},
	volume = {15},
	number = {4},
	journal = {Journal of cross-cultural psychology},
	author = {Hofstede, Geert and Bond, Michael H},
	year = {1984},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {417--433},
}

@article{kruchten_technical_2012,
	title = {Technical {Debt}: {From} {Metaphor} to {Theory} and {Practice}},
	volume = {29},
	issn = {0740-7459},
	shorttitle = {Technical {Debt}},
	url = {http://ieeexplore.ieee.org/document/6336722/},
	doi = {10.1109/MS.2012.167},
	language = {en},
	number = {6},
	urldate = {2021-10-25},
	journal = {IEEE Software},
	author = {Kruchten, Philippe and Nord, Robert L. and Ozkaya, Ipek},
	month = nov,
	year = {2012},
	pages = {18--21},
}

@misc{tulsa_replication_2018,
	title = {Replication {Data} for: {Cybersecurity} {Research} {Datasets}: {Taxonomy} and {Empirical} {Analysis}},
	shorttitle = {Replication {Data} for},
	url = {https://dataverse.harvard.edu/citation?persistentId=doi:10.7910/DVN/4EPUIA},
	abstract = {We inspect 965 cybersecurity research papers published between 2012 and 2016 in order to understand better how datasets are used, produced and shared. We construct a taxonomy of the types of data created and shared, informed and validated by the examined papers. We then analyze the gathered data on datasets. Three quarters of existing datasets used as input to research are publicly available, but less than one ﬁfth of datasets created by researchers are publicly shared. Using a series of linear regressions, we demonstrate that those researchers who do make public the datasets they create are rewarded with more citations to the associated papers. Hence, we conclude that an under-appreciated incentive exists for researchers to share their created datasets with the broader research community.},
	language = {en},
	urldate = {2021-10-25},
	publisher = {Harvard Dataverse},
	author = {Tulsa), Tyler (The University Of, Moore},
	year = {2018},
	doi = {10.7910/DVN/4EPUIA},
	note = {Type: dataset},
}

@article{leesatapornwongsa_transactuations_2020,
	title = {Transactuations: {Where} {Transactions} {Meet} the {Physical} {World}},
	volume = {36},
	issn = {0734-2071, 1557-7333},
	shorttitle = {Transactuations},
	url = {https://dl.acm.org/doi/10.1145/3380907},
	doi = {10.1145/3380907},
	abstract = {A large class of IoT applications read sensors, execute application logic, and actuate actuators. However, the lack of highlevel programming abstractions compromises correctness especially in presence of failures and unwanted interleaving between applications. A key problem arises when operations on IoT devices or the application itself fails, which leads to inconsistencies between the physical state and application state, breaking application semantics and causing undesired consequences. Transactions are a well-established abstraction for correctness, but assume properties that are absent in an IoT context. In this paper, we study one such environment, smart home, and establish inconsistencies manifesting out of failures. We propose an abstraction called transactuation that empowers developers to build reliable applications. Our runtime, Relacs, implements the abstraction atop a real smarthome platform. We evaluate programmability, performance, and effectiveness of transactuations to demonstrate its potential as a powerful abstraction and execution model.},
	language = {en},
	number = {4},
	urldate = {2021-10-22},
	journal = {ACM Transactions on Computer Systems},
	author = {Leesatapornwongsa, Tanakorn and Sengupta, Aritra and Ardekani, Masoud Saeida and Petri, Gustavo and Stuardo, Cesar A.},
	month = jun,
	year = {2020},
	pages = {1--31},
}

@article{celik_sensitive_nodate,
	title = {Sensitive {Information} {Tracking} in {Commodity} {IoT}},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital connectivity has had profound effects on society–smart homes, personal monitoring devices, enhanced manufacturing and other IoT applications have changed the way we live, play, and work. Yet extant IoT platforms provide few means of evaluating the use (and potential avenues for misuse) of sensitive information. Thus, consumers and organizations have little information to assess the security and privacy risks these devices present. In this paper, we present SAINT, a static taint analysis tool for IoT applications. SAINT operates in three phases; (a) translation of platform-speciﬁc IoT source code into an intermediate representation (IR), (b) identifying sensitive sources and sinks, and (c) performing static analysis to identify sensitive data ﬂows. We evaluate SAINT on 230 SmartThings market apps and ﬁnd 138 (60\%) include sensitive data ﬂows. In addition, we demonstrate SAINT on IOTBENCH, a novel open-source test suite containing 19 apps with 27 unique data leaks. Through this effort, we introduce a rigorously grounded framework for evaluating the use of sensitive information in IoT apps—and therein provide developers, markets, and consumers a means of identifying potential threats to security and privacy.},
	language = {en},
	author = {Celik, Z Berkay and Babun, Leonardo and Sikder, Amit K and Aksu, Hidayet and Tan, Gang and McDaniel, Patrick and Uluagac, A Selcuk},
	pages = {19},
}

@inproceedings{li_what_2015,
	address = {Florence, Italy},
	title = {What {Makes} a {Great} {Software} {Engineer}?},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194618/},
	doi = {10.1109/ICSE.2015.335},
	abstract = {Good software engineers are essential to the creation of good software. However, most of what we know about softwareengineering expertise are vague stereotypes, such as ‘excellent communicators’ and ‘great teammates’. The lack of specificity in our understanding hinders researchers from reasoning about them, employers from identifying them, and young engineers from becoming them. Our understanding also lacks breadth: what are all the distinguishing attributes of great engineers (technical expertise and beyond)? We took a first step in addressing these gaps by interviewing 59 experienced engineers across 13 divisions at Microsoft, uncovering 53 attributes of great engineers. We explain the attributes and examine how the most salient of these impact projects and teams. We discuss implications of this knowledge on research and the hiring and training of engineers.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Li, Paul Luo and Ko, Andrew J. and Zhu, Jiamin},
	month = may,
	year = {2015},
	pages = {700--710},
}

@inproceedings{baltes_towards_2018,
	address = {Lake Buena Vista FL USA},
	title = {Towards a theory of software development expertise},
	isbn = {978-1-4503-5573-5},
	url = {https://dl.acm.org/doi/10.1145/3236024.3236061},
	doi = {10.1145/3236024.3236061},
	abstract = {Software development includes diverse tasks such as implementing new features, analyzing requirements, and fixing bugs. Being an expert in those tasks requires a certain set of skills, knowledge, and experience. Several studies investigated individual aspects of software development expertise, but what is missing is a comprehensive theory. We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance. Our theory currently focuses on programming, but already provides valuable insights for researchers, developers, and employers. The theory describes important properties of software development expertise and which factors foster or hinder its formation, including how developers’ performance may decline over time. Moreover, our quantitative results show that developers’ expertise self-assessments are context-dependent and that experience is not necessarily related to expertise.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Baltes, Sebastian and Diehl, Stephan},
	month = oct,
	year = {2018},
	keywords = {Exemplar},
	pages = {187--200},
}

@article{li_what_2020,
	title = {What distinguishes great software engineers?},
	volume = {25},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-019-09773-y},
	doi = {10.1007/s10664-019-09773-y},
	abstract = {Great software engineers are essential to the creation of great software. However, today, we lack an understanding of what distinguishes great engineers from ordinary ones. We address this knowledge gap by conducting one of the largest mixed-method studies of experienced engineers to date. We surveyed 1,926 expert engineers, including senior engineers, architects, and technical fellows, asking them to judge the importance of a comprehensive set of 54 attributes of great engineers. We then conducted 77 email interviews to interpret our findings and to understand the influence of contextual factors on the ratings. After synthesizing the findings, we believe that the top five distinguishing characteristics of great engineers are writing good code, adjusting behaviors to account for future value and costs, practicing informed decision-making, avoiding making others’ jobs harder, and learning continuously. We relate the findings to prior work, and discuss implications for researchers, practitioners, and educators.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Empirical Software Engineering},
	author = {Li, Paul Luo and Ko, Amy J. and Begel, Andrew},
	month = jan,
	year = {2020},
	pages = {322--352},
}

@article{capretz_personality_2003,
	title = {Personality types in software engineering},
	volume = {58},
	issn = {10715819},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581902001374},
	doi = {10.1016/S1071-5819(02)00137-4},
	abstract = {Software engineering is forecast to be among the fastest growing employment ﬁeld in the next decades. The purpose of this investigation is two-fold: Firstly, empirical studies on the personality types of software professionals are reviewed. Secondly, this work provides an upto-date personality proﬁle of software engineers according to the Myers–Briggs Type Indicator.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {International Journal of Human-Computer Studies},
	author = {Capretz, Luiz Fernando},
	month = feb,
	year = {2003},
	pages = {207--214},
}

@article{chaki_adaptive_2021,
	title = {Adaptive {Priority}-based {Conflict} {Resolution} of {IoT} {Services}},
	url = {http://arxiv.org/abs/2107.08348},
	abstract = {We propose a novel conflict resolution framework for IoT services in multi-resident smart homes. An adaptive priority model is developed considering the residents' contextual factors (e.g., age, illness, impairment). The proposed priority model is designed using the concept of the analytic hierarchy process. A set of experiments on real-world datasets are conducted to show the efficiency of the proposed approach.},
	urldate = {2021-10-21},
	journal = {arXiv:2107.08348 [cs]},
	author = {Chaki, Dipankar and Bouguettaya, Athman},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.08348},
	keywords = {Computer Science - Computers and Society, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{chaki_dynamic_2021,
	title = {Dynamic {Conflict} {Resolution} of {IoT} {Services} in {Smart} {Homes}},
	url = {http://arxiv.org/abs/2110.07083},
	abstract = {We propose a novel conflict resolution framework for IoT services in multi-resident smart homes. The proposed framework employs a preference extraction model based on a temporal proximity strategy. We design a preference aggregation model using a matrix factorization-based approach (i.e., singular value decomposition). The concepts of current resident item matrix and ideal resident item matrix are introduced as key criteria to cater to the conflict resolution framework. Finally, a set of experiments on real-world datasets are conducted to show the effectiveness of the proposed approach.},
	urldate = {2021-10-21},
	journal = {arXiv:2110.07083 [cs]},
	author = {Chaki, Dipankar and Bouguettaya, Athman},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.07083},
	keywords = {Computer Science - Computers and Society, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{ananthanarayanan_keeping_2019,
	address = {Dresden Germany},
	title = {Keeping {Master} {Green} at {Scale}},
	isbn = {978-1-4503-6281-8},
	url = {https://dl.acm.org/doi/10.1145/3302424.3303970},
	doi = {10.1145/3302424.3303970},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Proceedings of the {Fourteenth} {EuroSys} {Conference} 2019},
	publisher = {ACM},
	author = {Ananthanarayanan, Sundaram and Ardekani, Masoud Saeida and Haenikel, Denis and Varadarajan, Balaji and Soriano, Simon and Patel, Dhaval and Adl-Tabatabai, Ali-Reza},
	month = mar,
	year = {2019},
	pages = {1--15},
}

@inproceedings{sadowski_modern_2018,
	address = {Gothenburg Sweden},
	title = {Modern code review: a case study at google},
	isbn = {978-1-4503-5659-6},
	shorttitle = {Modern code review},
	url = {https://dl.acm.org/doi/10.1145/3183519.3183525},
	doi = {10.1145/3183519.3183525},
	abstract = {Employing lightweight, tool-based code review of code changes (aka modern code review) has become the norm for a wide variety of open-source and industrial systems. In this paper, we make an exploratory investigation of modern code review at Google. Google introduced code review early on and evolved it over the years; our study sheds light on why Google introduced this practice and analyzes its current status, after the process has been reﬁned through decades of code changes and millions of code reviews. By means of 12 interviews, a survey with 44 respondents, and the analysis of review logs for 9 million reviewed changes, we investigate motivations behind code review at Google, current practices, and developers’ satisfaction and challenges.},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Sadowski, Caitlin and Söderberg, Emma and Church, Luke and Sipko, Michal and Bacchelli, Alberto},
	month = may,
	year = {2018},
	pages = {181--190},
}

@article{potvin_why_2016,
	title = {Why {Google} stores billions of lines of code in a single repository},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2854146},
	doi = {10.1145/2854146},
	abstract = {Google's monolithic repository provides a common source of truth for tens of thousands of developers around the world.},
	language = {en},
	number = {7},
	urldate = {2021-10-21},
	journal = {Communications of the ACM},
	author = {Potvin, Rachel and Levenberg, Josh},
	month = jun,
	year = {2016},
	pages = {78--87},
}

@article{capretz_making_2010,
	title = {Making {Sense} of {Software} {Development} and {Personality} {Types}},
	volume = {12},
	issn = {1520-9202},
	url = {http://ieeexplore.ieee.org/document/5403172/},
	doi = {10.1109/MITP.2010.33},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {IT Professional},
	author = {Capretz, L.F. and Ahmed, F.},
	month = jan,
	year = {2010},
	keywords = {Exemplar},
	pages = {6--13},
}

@article{peck_where_1980,
	title = {"{Where} has all the judgment gone?" {The} fifth {Laurits} {Bjerrum} memorial lecture},
	volume = {17},
	issn = {0008-3674, 1208-6010},
	shorttitle = {"{Where} has all the judgment gone?},
	url = {http://www.nrcresearchpress.com/doi/10.1139/t80-065},
	doi = {10.1139/t80-065},
	abstract = {This lecture aims at significantly reducing the probability of failure of earth dams by consciously fostering the application of engineering judgment in design and application.},
	language = {en},
	number = {4},
	urldate = {2021-10-20},
	journal = {Canadian Geotechnical Journal},
	author = {Peck, Ralph B.},
	month = nov,
	year = {1980},
	pages = {584--590},
}

@article{petroski_failure_1993,
	title = {Failure as {Source} of {Engineering} {Judgment}: {Case} of {John} {Roebling}},
	volume = {7},
	issn = {0887-3828, 1943-5509},
	shorttitle = {Failure as {Source} of {Engineering} {Judgment}},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%290887-3828%281993%297%3A1%2846%29},
	doi = {10.1061/(ASCE)0887-3828(1993)7:1(46)},
	abstract = {The proper use of the concepts and realities of failure is essential for successful design practice, which involves proper engineering judgment. Among the most valuable sources of good design judgment are case studies of how great engineers designed against failure. John Roebling is among the engineers whose works provide excellent models of good judgment and the explicit use of the knowledge of failures in designing successfulstructures. Roebling's use of failure concepts and case studies to avoid failure in his own designs provides a paradigm for good engineering practice generally. Although the analytical state of the art has certainly advanced since Roebling's time, the basic ideas of good engineering practice are no different now than they were in the 19th century. It therefore follows that a study of the methods of model engineers like Roebling can help develop judgment in modern engineers and thereby reduce the occurrence of failures in modern designs.},
	language = {en},
	number = {1},
	urldate = {2021-10-20},
	journal = {Journal of Performance of Constructed Facilities},
	author = {Petroski, Henry},
	month = feb,
	year = {1993},
	pages = {46--58},
}

@article{holt_designers_1997,
	title = {The designer's judgement},
	volume = {18},
	issn = {0142694X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0142694X96000130},
	doi = {10.1016/S0142-694X(96)00013-0},
	abstract = {Judgement is ubiquitous in engineering work but the demand for the exercise of good judgement is particularly evident in engineering design. Every advance, or change of direction, in the design process is the result of the designer's judgement. But the notion o f judgement is somewhat elusive. What is it? How can we know whether a judgement is good or bad? Can judgement be taught and if so when and how? This paper attempts to answer at least in part these questions. It draws on the work of Sir Geoffrey Vickers, seeking to adapt and relate his ideas on judgement to engineering design. It constructs aframework for design in which judgements are made in three knowledge domains; the formative, the commercial and the instrumental. It then considers the efficacy of contemporary academe and industry in developing the capacity for judgement in the neophyte engineer. It concludes with a broad strategy for teaching design in engineering courses. 0 19917Published by Elsevier Science Ltd.},
	language = {en},
	number = {1},
	urldate = {2021-10-20},
	journal = {Design Studies},
	author = {Holt, J.E.},
	month = jan,
	year = {1997},
	pages = {113--123},
}

@article{davis_plea_2012,
	title = {A {Plea} for {Judgment}},
	volume = {18},
	issn = {1353-3452, 1471-5546},
	url = {http://link.springer.com/10.1007/s11948-011-9254-6},
	doi = {10.1007/s11948-011-9254-6},
	abstract = {Judgment is central to engineering, medicine, the sciences and many other practical activities. For example, one who otherwise knows what engineers know but lacks ‘‘engineering judgment’’ may be an expert of sorts, a handy resource much like a reference book or database, but cannot be a competent engineer. Though often overlooked or at least passed over in silence, the central place of judgment in engineering, the sciences, and the like should be obvious once pointed out. It is important here because it helps to explain where ethics ﬁts into these disciplines. There is no good engineering, no good science, and so on without good judgment and no good judgment in these disciplines without ethics. Doing even a minimally decent job of teaching one of these disciplines necessarily includes teaching its ethics; teaching the ethics is teaching the discipline (or at least a large part of it).},
	language = {en},
	number = {4},
	urldate = {2021-10-20},
	journal = {Science and Engineering Ethics},
	author = {Davis, Michael},
	month = dec,
	year = {2012},
	pages = {789--808},
}

@techreport{abet2020abet,
	title = {{ABET} criteria for accrediting engineering programs},
	author = {{ABET Engineering Accreditation Commission}},
	year = {2020},
}

@article{weedon_role_2019,
	title = {The {Role} of {Rhetoric} in {Engineering} {Judgment}},
	volume = {62},
	issn = {0361-1434, 1558-1500},
	url = {https://ieeexplore.ieee.org/document/8667081/},
	doi = {10.1109/TPC.2019.2900824},
	abstract = {Introduction: ABET has approved changes to the EAC’s Criterion 3 that will take effect for the 2019-2020 accreditation cycle. Among many changes and rearrangements is the introduction of the term “engineering judgment” as one of the competencies that students must develop to prepare for professional engineering. Literature review: However, engineering judgment is not deﬁned in the criterion, and although it is a ubiquitous concept in the philosophy of engineering and engineering education, little empirical investigation has been undertaken into the practice of engineering judgment. And there is even less conceptual or empirical investigation into communication’s role in the practice of engineering judgment. Research questions: 1. What does engineering judgment look like in practice? 2. How does the sociotechnical situation affect engineering judgment? 3. What role does rhetoric have, not only in communicating judgments, but in forming them as well? 4. How can teachers and practitioners in engineering and technical communication use these ﬁndings to facilitate better judgment in the classroom and at work? Methods: Using videotape and ﬁeldnotes, the author examines the two sequences of decision-making from a student engineering design project. An ethnomethodologically inspired framework is used to exhibit the phenomenal details of “doing” engineering judgment. Discussion/conclusion: Data reveal that engineering judgment may be fruitfully understood by educators as not just a cognitive and individual ability to apply technical knowledge, but instead a capacity of participants to rhetorically establish common cause to interrogate and reﬂect on the relations between technical data and situations.},
	language = {en},
	number = {2},
	urldate = {2021-10-20},
	journal = {IEEE Transactions on Professional Communication},
	author = {Weedon, Scott},
	month = jun,
	year = {2019},
	pages = {165--177},
}

@article{reitz2001engineering,
	title = {What is engineering judgment?},
	volume = {73},
	number = {9},
	journal = {Machine Design},
	author = {Reitz, V and Graham, R and Wescott, T},
	year = {2001},
	note = {Publisher: Penton Publishing},
	pages = {174--174},
}

@article{herkert_managements_1991,
	title = {Management's hat trick: {Misuse} of "engineering judgment" in the challenger incident},
	volume = {10},
	issn = {0167-4544, 1573-0697},
	shorttitle = {Management's hat trick},
	url = {http://link.springer.com/10.1007/BF00382881},
	doi = {10.1007/BF00382881},
	language = {en},
	number = {8},
	urldate = {2021-10-20},
	journal = {Journal of Business Ethics},
	author = {Herkert, Joseph R.},
	month = aug,
	year = {1991},
	pages = {617--620},
}

@incollection{kirkman2005impact,
	title = {The impact of cultural value diversity on multicultural team performance},
	booktitle = {Managing multinational teams: {Global} perspectives},
	publisher = {Emerald Group Publishing Limited},
	author = {Kirkman, Bradley L and Shapiro, Debra L},
	year = {2005},
}

@phdthesis{spring_human_2019,
	title = {Human decision-making in computer security incident response},
	language = {en},
	school = {University College London},
	author = {Spring, Jonathan Michael},
	year = {2019},
}

@inproceedings{garcia_comprehensive_2020,
	title = {A {Comprehensive} {Study} of {Autonomous} {Vehicle} {Bugs}},
	abstract = {Self-driving cars, or Autonomous Vehicles (AVs), are increasingly becoming an integral part of our daily life. About 50 corporations are actively working on AVs, including large companies such as Google, Ford, and Intel. Some AVs are already operating on public roads, with at least one unfortunate fatality recently on record. As a result, understanding bugs in AVs is critical for ensuring their security, safety, robustness, and correctness. While previous studies have focused on a variety of domains (e.g., numerical software; machine learning; and error-handling, concurrency, and performance bugs) to investigate bug characteristics, AVs have not been studied in a similar manner. Recently, two software systems for AVs, Baidu Apollo and Autoware, have emerged as frontrunners in the open-source community and have been used by large companies and governments (e.g., Lincoln, Volvo, Ford, Intel, Hitachi, LG, and the US Department of Transportation). From these two leading AV software systems, this paper describes our investigation of 16,851 commits and 499 AV bugs and introduces our classification of those bugs into 13 root causes, 20 bug symptoms, and 18 categories of software components those bugs often affect. We identify 16 major findings from our study and draw broader lessons from them to guide the research community towards future directions in software bug detection, localization, and repair.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Garcia, Joshua and Feng, Yang and Shen, Junjie and Almanee, Sumaya and Xia, Yuan and Chen, Qi Alfred},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	keywords = {Autonomous vehicles, Companies, Computer bugs, Maintenance engineering, Open source software, Software engineering, Software systems, autonomous vehicles, bugs, defects, empirical software engineering},
	pages = {385--396},
}

@article{maznevski_cultural_2002,
	title = {Cultural {Dimensions} at the {Individual} {Level} of {Analysis}: {The} {Cultural} {Orientations} {Framework}},
	volume = {2},
	issn = {1470-5958, 1741-2838},
	shorttitle = {Cultural {Dimensions} at the {Individual} {Level} of {Analysis}},
	url = {http://journals.sagepub.com/doi/10.1177/147059580223001},
	doi = {10.1177/147059580223001},
	abstract = {This article describes a theoretically-grounded framework of cultural dimensions conceptualized and operationalized at the individual level of analysis, based on the work of anthropologists Kluckhohn and Strodtbeck. We present empirical data gathered from five countries – Canada, Mexico, the Netherlands, Taiwan, and the United States – to assess the validity of the framework. We then use the results to explore how the cultural orientations framework can add insight and new perspectives to critical questions in cross cultural management research.},
	language = {en},
	number = {3},
	urldate = {2021-10-14},
	journal = {International Journal of Cross Cultural Management},
	author = {Maznevski, Martha L. and Gomez, Carolina B. and DiStefano, Joseph J. and Noorderhaven, Niels G. and Wu, Pei-Chuan},
	month = dec,
	year = {2002},
	pages = {275--295},
}

@article{goldberg_alternative_nodate,
	title = {An {Alternative} "{Description} of {Personality}": {The} {Big}-{Five} {Factor} {Structure}},
	language = {en},
	author = {Goldberg, Lewis R},
	pages = {14},
}

@article{lee_internet_2015,
	title = {The {Internet} of {Things} ({IoT}): {Applications}, investments, and challenges for enterprises},
	volume = {58},
	issn = {00076813},
	shorttitle = {The {Internet} of {Things} ({IoT})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0007681315000373},
	doi = {10.1016/j.bushor.2015.03.008},
	abstract = {The Internet of Things (IoT), also called the Internet of Everything or the Industrial Internet, is a new technology paradigm envisioned as a global network of machines and devices capable of interacting with each other. The IoT is recognized as one of the most important areas of future technology and is gaining vast attention from a wide range of industries. This article presents ﬁve IoT technologies that are essential in the deployment of successful IoT-based products and services and discusses three IoT categories for enterprise applications used to enhance customer value. In addition, it examines the net present value method and the real option approach widely used in the justiﬁcation of technology projects and illustrates how the real option approach can be applied for IoT investment. Finally, this article discusses ﬁve technical and managerial challenges.},
	language = {en},
	number = {4},
	urldate = {2021-07-14},
	journal = {Business Horizons},
	author = {Lee, In and Lee, Kyoochun},
	year = {2015},
}

@techreport{stouffer_guide_2015,
	title = {Guide to {Industrial} {Control} {Systems} ({ICS}) {Security}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-82r2.pdf},
	abstract = {This document provides guidance on how to secure Industrial Control Systems (ICS), including Supervisory Control and Data Acquisition (SCADA) systems, Distributed Control Systems (DCS), and other control system configurations such as Programmable Logic Controllers (PLC), while addressing their unique performance, reliability, and safety requirements. The document provides an overview of ICS and typical system topologies, identifies typical threats and vulnerabilities to these systems, and provides recommended security countermeasures to mitigate the associated risks.},
	language = {en},
	number = {NIST SP 800-82r2},
	urldate = {2021-09-29},
	institution = {National Institute of Standards and Technology},
	author = {Stouffer, Keith and Pillitteri, Victoria and Lightman, Suzanne and Abrams, Marshall and Hahn, Adam},
	year = {2015},
	doi = {10.6028/NIST.SP.800-82r2},
}

@article{freedman_why_2010,
	title = {Why {Scientific} {Studies} {Are} {So} {Often} {Wrong}: {The} {Streetlight} {Effect}},
	url = {https://www.discovermagazine.com/the-sciences/why-scientific-studies-are-so-often-wrong-the-streetlight-effect},
	urldate = {2021-10-04},
	journal = {Discover Magazine},
	author = {Freedman, David},
	year = {2010},
}

@article{long_oct_2009,
	title = {Oct. 26, 1992: {Software} {Glitch} {Cripples} {Ambulance} {Service}},
	issn = {1059-1028},
	url = {https://www.wired.com/2009/10/1026london-ambulance-computer-meltdown/},
	abstract = {Computerized systems can help, or hurt. In the case of the London Ambulance Service, it definitely hurt.},
	language = {en-US},
	urldate = {2021-10-05},
	journal = {Wired},
	author = {Long, Tony},
	year = {2009},
	note = {Section: tags},
	keywords = {20th century, britain, computers, health, london, tech gone bad},
}

@inproceedings{wang_exploratory_2021,
	address = {Athens Greece},
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468559},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	publisher = {ACM},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	year = {2021},
}

@inproceedings{celik2018soteria,
	title = {Soteria: {Automated} iot safety and security analysis},
	booktitle = {{USENIX} {Annual} {Technical} {Conference} ({ATC}\vphantom{\{}\}},
	author = {Celik, Z Berkay and McDaniel, Patrick and Tan, Gang},
	year = {2018},
}

@inproceedings{glomsrud_structured_2019,
	title = {A {Structured} {STPA} {Safety} and {Security} {Co}-analysis {Framework} for {Autonomous} {Ships}},
	isbn = {978-981-11-2724-3},
	url = {http://rpsonline.com.sg/proceedings/9789811127243/html/0105.xml},
	doi = {10.3850/978-981-11-2724-3_0105-cd},
	abstract = {Research and development of autonomous ships is progressing rapidly and involves all relevant stakeholders’ contributions. DNV GL as a classiﬁcation society is obligated to develop new maritime rules and guidelines aiming to ensure safe, secure and efﬁcient future maritime operations. We need to understand new challenges and assess new risks arising from the development and operations of such ships. Autonomous ships are complex safety-critical cyber-physical systems (CPS) of which safety and security are two crucial properties. It is imperative to identity inter-dependencies between safety and security for thoroughly assessing and managing potential risks. Therefore, we need a systematic framework with concrete execution steps to perform a combined safety and security co-analysis. The System Theoretic Process Analysis (STPA) is primarily developed for safety analysis of systems which can be depicted using well deﬁned control system designs. However, the STPA needs to be improved to properly analyze autonomous ships which typically integrate emerging technologies and deﬁne novel functionalities. This paper explores the feasibility of applying an STPA safety and security co-analysis on novel CPS. We identify a gap existing between the ﬁrst two steps in basic STPA, i.e., it is not straightforward to model the control structure(s) (in Step 2) from the system engineering foundation established in Step 1, with limited prior knowledge. Our ﬁrst contribution is to bridge this gap by extending the analysis of Step 1. More speciﬁcally, functional requirements derived from safety (or security) constraints are used to facilitate the modeling of control structures. The STPA safety and security co-analysis has been studied to assess the risks of CPS collectively. However, to the best of our knowledge, not too much work focusing on how to tightly incorporate conventional security analysis techniques into the STPA analysis process has been performed. Our second contribution is to improve the current STPA-Sec by appropriately integrating widely adopted security analysis methods into the analysis process. A comprehensive list of vulnerabilities and threats can be identiﬁed through the improved analysis process and speciﬁc security requirements are derived accordingly.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {European {Safety} and {Reliability} {Conference} ({ESREL})},
	author = {Glomsrud, Jon Arne},
	year = {2019},
}

@article{laplante_software_2017,
	title = {Software {Engineering} of {Safety}-{Critical} {Systems}: {Themes} {From} {Practitioners}},
	volume = {66},
	issn = {0018-9529, 1558-1721},
	shorttitle = {Software {Engineering} of {Safety}-{Critical} {Systems}},
	url = {http://ieeexplore.ieee.org/document/8006260/},
	doi = {10.1109/TR.2017.2731953},
	abstract = {This study addresses two important questions related to engineering of safety-critical software and software-intensive systems. The ﬁrst question is: which software and softwareintensive systems should be considered safety critical? The second question is: what processes, design practices, and tools have practitioners been using for building these systems? We answer these questions through an analysis of unstructured interviews with experienced engineers who self-describe as working on safety-critical systems. Then, a thematic analysis of these responses was conducted. The results of this study are intended to provide guidance to those building safety-critical systems and have implications on state engineering licensure boards, in the determination of legal liability, and in risk assessment for policymakers, corporate governors, and insurance executives.},
	language = {en},
	number = {3},
	urldate = {2021-08-26},
	journal = {IEEE Transactions on Reliability},
	author = {Laplante, Phillip A. and DeFranco, Joanna F.},
	year = {2017},
}

@article{munaiahCuratingGitHubEngineered2017,
	title = {Curating {GitHub} for engineered software projects},
	volume = {22},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-017-9512-6},
	doi = {10.1007/s10664-017-9512-6},
	language = {en},
	number = {6},
	urldate = {2021-04-01},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Munaiah, Nuthan and Kroh, Steven and Cabrey, Craig and Nagappan, Meiyappan},
	year = {2017},
}

@article{leveson_role_2004,
	title = {Role of {Software} in {Spacecraft} {Accidents}},
	volume = {41},
	issn = {0022-4650, 1533-6794},
	url = {https://arc.aiaa.org/doi/10.2514/1.11950},
	doi = {10.2514/1.11950},
	language = {en},
	number = {4},
	urldate = {2021-10-06},
	journal = {Journal of Spacecraft and Rockets},
	author = {Leveson, Nancy G.},
	year = {2004},
}

@inproceedings{gladisch_experience_2019,
	title = {Experience {Paper}: {Search}-{Based} {Testing} in {Automated} {Driving} {Control} {Applications}},
	shorttitle = {Experience {Paper}},
	doi = {10.1109/ASE.2019.00013},
	abstract = {Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Gladisch, Christoph and Heinz, Thomas and Heinzemann, Christian and Oehlerking, Jens and von Vietinghoff, Anne and Pfitzer, Tim},
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Measurement, Monitoring, Optimization, Software, Test pattern generators, Tools, automated driving, automated test generation, experience paper, search-based testing},
}

@article{fenton_strategy_1998,
	title = {A strategy for improving safety related software engineering standards},
	volume = {24},
	issn = {00985589},
	url = {http://ieeexplore.ieee.org/document/730547/},
	doi = {10.1109/32.730547},
	abstract = {There are many standards which are relevant for building safety or mission critical software systems. An effective standard is one that should help developers, assessors, and users of such systems. For developers the standard should help them build the system cost-effectively, and it should be clear what is required in order to conform to the standard. For assessors it should be possible to determine, objectively, compliance to the standard. Users and society at large should have some assurance that a system developed to the standard has quantified risks and benefits. Unfortunately, the existing standards do not adequately fulfill any of these varied requirements. We explain why standards are the way they are and then provide a strategy for improving them. Our approach is to evaluate standards on a number of key criteria that enable us to interpret the standard, identify its scope, and check the ease with which it can be applied and checked. We also need to demonstrate that the use of a standard is likely either to deliver reliable and safe systems at an acceptable cost or help predict reliability and safety accurately. Throughout the paper we examine, by example, a specific standard for safety critical systems (namely IEC 1508) and show how it can be improved by applying our strategy.},
	language = {en},
	number = {11},
	urldate = {2021-10-11},
	journal = {IEEE Transactions on Software Engineering (TSE)},
	author = {Fenton, N.E. and Neil, M.},
	year = {1998},
	pages = {1002--1013},
}

@inproceedings{emami-naeini_privacy_2017,
	title = {Privacy {Expectations} and {Preferences} in an {IoT} {World}},
	abstract = {With the rapid deployment of Internet of Things (IoT) technologies and the variety of ways in which IoT-connected sensors collect and use personal data, there is a need for transparency, control, and new tools to ensure that individual privacy requirements are met. To develop these tools, it is important to better understand how people feel about the privacy implications of IoT and the situations in which they prefer to be notiﬁed about data collection. We report on a 1,007-participant vignette study focusing on privacy expectations and preferences as they pertain to a set of 380 IoT data collection and use scenarios. Participants were presented with 14 scenarios that varied across eight categorical factors, including the type of data collected (e.g. location, biometrics, temperature), how the data is used (e.g., whether it is shared, and for what purpose), and other attributes such as the data retention period. Our ﬁndings show that privacy preferences are diverse and context dependent; participants were more comfortable with data being collected in public settings rather than in private places, and are more likely to consent to data being collected for uses they ﬁnd beneﬁcial. They are less comfortable with the collection of biometrics (e.g. ﬁngerprints) than environmental data (e.g. room temperature, physical presence). We also ﬁnd that participants are more likely to want to be notiﬁed about data practices that they are uncomfortable with. Finally, our study suggests that after observing individual decisions in just three data-collection scenarios, it is possible to predict their preferences for the remaining scenarios, with our model achieving an average accuracy of up to 86\%.},
	language = {en},
	booktitle = {Symposium on {Usable} {Privacy} and {Security} ({SOUPS})},
	author = {Emami-Naeini, Pardis and Bhagavatula, Sruti and Habib, Hana and Degeling, Martin and Bauer, Lujo and Cranor, Lorrie Faith and Sadeh, Norman},
	year = {2017},
}

@techreport{faganFoundationalCybersecurityActivities2020,
	address = {Gaithersburg, MD},
	title = {Foundational cybersecurity activities for {IoT} device manufacturers},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8259.pdf},
	abstract = {Internet of Things (IoT) devices often lack device cybersecurity capabilities their customers—organizations and individuals—can use to help mitigate their cybersecurity risks. Manufacturers can help their customers by improving how securable the IoT devices they make are by providing necessary cybersecurity functionality and by providing customers with the cybersecurity-related information they need. This publication describes recommended activities related to cybersecurity that manufacturers should consider performing before their IoT devices are sold to customers. These foundational cybersecurity activities can help manufacturers lessen the cybersecurity-related efforts needed by customers, which in turn can reduce the prevalence and severity of IoT device compromises and the attacks performed using compromised devices.},
	language = {en},
	number = {NIST IR 8259},
	urldate = {2021-03-29},
	institution = {National Institute of Standards and Technology},
	author = {Fagan, Michael and Megas, Katerina N and Scarfone, Karen and Smith, Matthew},
	year = {2020},
	doi = {10.6028/NIST.IR.8259},
	pages = {NIST IR 8259},
}

@inproceedings{ding_gfxdoctor_2017,
	title = {{GfxDoctor}: {A} {Holistic} {Graphics} {Energy} {Profiler} for {Mobile} {Devices}},
	isbn = {978-1-4503-4938-3},
	shorttitle = {{GfxDoctor}},
	url = {https://dl.acm.org/doi/10.1145/3064176.3064206},
	doi = {10.1145/3064176.3064206},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Twelfth {European} {Conference} on {Computer} {Systems} ({EuroSys})},
	publisher = {ACM},
	author = {Ding, Ning and Hu, Y. Charlie},
	year = {2017},
}

@inproceedings{charpenay2016introducing,
	title = {Introducing thing descriptions and interactions: {An} ontology for the web of things.},
	booktitle = {Joint {Proceedings} of the 3rd {Stream} {Reasoning} ({SR} 2016) and the 1st {Semantic} {Web} {Technologies} for the {Internet} of {Things} ({SWIT}) workshops at {ISWC}},
	author = {Charpenay, Victor and Käbisch, Sebastian and Kosch, Harald},
	year = {2016},
}

@inproceedings{bagheri_synthesis_2020,
	address = {Seoul South Korea},
	title = {Synthesis of assurance cases for software certification},
	isbn = {978-1-4503-7126-1},
	url = {https://dl.acm.org/doi/10.1145/3377816.3381728},
	doi = {10.1145/3377816.3381728},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results} ({ICSE}-{NIER})},
	publisher = {ACM},
	author = {Bagheri, Hamid and Kang, Eunsuk and Mansoor, Niloofar},
	year = {2020},
}

@inproceedings{alhanahnah_scalable_2020,
	title = {Scalable analysis of interaction threats in {IoT} systems},
	isbn = {978-1-4503-8008-9},
	url = {https://dl.acm.org/doi/10.1145/3395363.3397347},
	doi = {10.1145/3395363.3397347},
	abstract = {The ubiquity of Internet of Things (IoT) and our growing reliance on IoT apps are leaving us more vulnerable to safety and security threats than ever before. Many of these threats are manifested at the interaction level, where undesired or malicious coordinations between apps and physical devices can lead to intricate safety and security issues. This paper presents IotCom, an approach to automatically discover such hidden and unsafe interaction threats in a compositional and scalable fashion. It is backed with automated program analysis and formally rigorous violation detection engines. IotCom relies on program analysis to automatically infer the relevant app’s behavior. Leveraging a novel strategy to trim the extracted app’s behavior prior to translating them to analyzable formal specifications, IotCom mitigates the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IotCom’s ability to effectively detect a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown, and to significantly outperform the existing techniques in terms of scalability.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {{ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis} ({ISSTA})},
	publisher = {ACM},
	author = {Alhanahnah, Mohannad and Stevens, Clay and Bagheri, Hamid},
	year = {2020},
}

@inproceedings{bird2009promises,
	title = {The promises and perils of mining git},
	booktitle = {{IEEE} {International} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Bird, Christian and Rigby, Peter C and Barr, Earl T and Hamilton, David J and German, Daniel M and Devanbu, Prem},
	year = {2009},
}

@article{carreras_guzman_integrated_2021,
	title = {An integrated safety and security analysis for cyber-physical harm scenarios},
	volume = {144},
	issn = {0925-7535},
	url = {https://www.sciencedirect.com/science/article/pii/S0925753521003015},
	doi = {10.1016/j.ssci.2021.105458},
	abstract = {Increasing digitalization and autonomous solutions in physical systems promise to enhance their performance, cost-efficiency and reliability. However, the integration of novel information technologies with safety-related systems also brings new vulnerabilities and risks that challenge the traditional field of safety analysis. Particularly, cyber security threats are becoming key factors in complex accident scenarios in cyber-physical systems (CPSs), where unintentional errors and design flaws overlap with cyber security vulnerabilities that could lead to harm to humans and assets. This overlap between safety and security analysis is still a loosely defined domain without established theories and methods, leading to complications during the risk analysis of CPSs. In this paper, we first describe how the domain of safety science increasingly overlaps with security analysis. Subsequently, based on this overlapping, we illustrate and complement an integrated method for the identification of harm scenarios in CPSs. This method, coined Uncontrolled Flows of Information and Energy (UFoI-E), offers a distinct theoretical foundation rooted in accident causation models and a framework to design diagrammatic representations of CPSs during the analysis. After summarizing these features of the UFoI-E method, we present our original contribution to the method, which is a new practical toolkit for risk identification composed of an ontology of harm scenarios and a database of checklists built from lessons learned analysis and expert knowledge. Finally, we demonstrate an application of the method in an illustrative case and show representative fields for future work.},
	language = {en},
	urldate = {2021-09-21},
	journal = {Safety Science},
	author = {Carreras Guzman, Nelson H. and Kozine, Igor and Lundteigen, Mary Ann},
	year = {2021},
	keywords = {Autonomous Systems, Bowtie Method, Cyber-Physical Harm Analysis for Safety and Security (CyPHASS), Cyber-Physical Systems (CPSs)},
}

@phdthesis{islam_towards_2020,
	type = {{PhD} {Thesis}},
	title = {Towards understanding the challenges faced by machine learning software developers and enabling automated solutions},
	language = {en},
	school = {Iowa State University},
	author = {Islam, Johirul},
	year = {2020},
}

@article{laplante_licensing_2014,
	title = {Licensing professional software engineers: seize the opportunity},
	volume = {57},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Licensing professional software engineers},
	url = {https://dl.acm.org/doi/10.1145/2618111},
	doi = {10.1145/2618111},
	abstract = {Professional organizations should be in the forefront of the ongoing discussion about licensing professional software engineers.},
	language = {en},
	number = {7},
	urldate = {2021-10-13},
	journal = {Communications of the ACM},
	author = {Laplante, Phillip A.},
	month = jul,
	year = {2014},
	pages = {38--40},
}

@article{house2013presidential,
	title = {Presidential policy directive/{PPD} 21–{Critical} infrastructure security and resilience},
	journal = {Washington, DC},
	author = {House, White},
	year = {2013},
}

@misc{dahmen-lhuissier_etsi_nodate,
	title = {{ETSI} - {Consumer} {IoT} security},
	url = {https://www.etsi.org/technologies/consumer-iot-security},
	abstract = {Consumer IoT security},
	language = {en-gb},
	urldate = {2021-10-12},
	journal = {ETSI},
	author = {Dahmen-Lhuissier, Sabine},
}

@misc{noauthor_isoiec_nodate,
	title = {{ISO}/{IEC} 30147:2021 {\textbar} {IEC} {Webstore}},
	url = {https://webstore.iec.ch/publication/62644},
	urldate = {2021-10-12},
}

@misc{noauthor_iec_nodate,
	title = {{IEC} 61508-1:2010 {\textbar} {IEC} {Webstore} {\textbar} functional safety, smart city},
	url = {https://webstore.iec.ch/publication/5515#additionalinfo},
	urldate = {2021-10-12},
}

@misc{noauthor_general_nodate,
	title = {General {Data} {Protection} {Regulation} ({GDPR}) – {Official} {Legal} {Text}},
	url = {https://gdpr-info.eu/},
	abstract = {General Data Protection Regulation (EU GDPR) – The official PDF of the Regulation (EU) 2016/679, its recitals \& key issues as a neatly arranged website.},
	language = {en-US},
	urldate = {2021-10-12},
	journal = {General Data Protection Regulation (GDPR)},
}

@misc{noauthor_family_2021,
	type = {Guides},
	title = {Family {Educational} {Rights} and {Privacy} {Act} ({FERPA})},
	url = {https://www2.ed.gov/policy/gen/guid/fpco/ferpa/index.html},
	abstract = {Family Educational Rights and Privacy Act (FERPA) Home Page.},
	language = {en},
	urldate = {2021-10-12},
	month = aug,
	year = {2021},
	note = {Publisher: US Department of Education (ED)},
}

@misc{rights_ocr_health_2021,
	type = {Text},
	title = {Health {Information} {Privacy}},
	url = {https://www.hhs.gov/hipaa/index.html},
	abstract = {Health Information Privacy},
	language = {en},
	urldate = {2021-10-12},
	journal = {HHS.gov},
	author = {Rights (OCR), Office for Civil},
	month = jun,
	year = {2021},
	note = {Last Modified: 2021-08-16T16:08:51-0400},
}

@book{hobbs2019embedded,
	title = {Embedded software development for safety-critical systems},
	publisher = {CRC Press},
	author = {Hobbs, Chris},
	year = {2019},
}

@book{sommerville2015software,
	title = {Software engineering},
	volume = {137035152},
	publisher = {Pearson Education},
	author = {Sommerville, Ian},
	year = {2015},
}

@inproceedings{atzori_blockchain-based_2017,
	title = {Blockchain-{Based} {Architectures} for the {Internet} of {Things}: {A} {Survey}},
	shorttitle = {Blockchain-{Based} {Architectures} for the {Internet} of {Things}},
	doi = {10.2139/SSRN.2846810},
	abstract = {This research explores the main features of three blockchain-based platforms for the Internet of Things, as recently emerged in academia as well as in industry. If properly engineered, the blockchain technology offers a disruptive solution to the problem of security and privacy in the Internet of Things environment, providing a new computational layer where data can be safely processed and analyzed, remaining private. The blockchain can also enable micro-payment functionality between digitally-enhanced devices, through ultra-light cryptocurrencies and smart contracts. The implementation of such features is expected to ensure a more efficient allocation of resources at global level, however it may also lead to undesirable consequences – such as a hyper-tokenization of society and a potentially dystopian concentration of power on big global platforms. Therefore, overall benefits and drawbacks of the blockchain deployment must necessarily take account of specific contexts of use, finding a balance between need for innovation, economic development and social sustainability.},
	author = {Atzori, M.},
	year = {2017},
}

@inproceedings{teslya_blockchain-based_2017,
	title = {Blockchain-based platform architecture for industrial {IoT}},
	doi = {10.23919/FRUCT.2017.8250199},
	abstract = {The development of robotics, the Internet of Things concept, big data processing techniques, automation, and distributed digital ledgers leads to the fourth industrial revolution. One of the main issues of new industry is interaction between the "smart factory" components both internally and with other factories based on the Internet of Things. This interaction should provide trust between the participants of the Internet of Things; control over the distribution of resources (such as maintenance time, energy, etc.) and finished products. The paper describes one of the possible ways of integrating Internet of Things and blockchain technologies to solve these issues. For this purpose, an architecture has been developed that combines Smart-M3 information sharing platform and blockchain platform. One of the main features of the proposed architecture is the use of smart contracts for processing and storing information related to the interaction between smart space components.},
	booktitle = {2017 21st {Conference} of {Open} {Innovations} {Association} ({FRUCT})},
	author = {Teslya, Nikolay and Ryabchikov, Igor},
	month = nov,
	year = {2017},
	keywords = {Contracts, Fault tolerance, Fault tolerant systems, Internet of Things, Production facilities, Service robots},
	pages = {321--329},
}

@article{reyna_blockchain_2018,
	title = {On blockchain and its integration with {IoT}. {Challenges} and opportunities},
	volume = {88},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329205},
	doi = {10.1016/j.future.2018.05.046},
	abstract = {In the Internet of Things (IoT) vision, conventional devices become smart and autonomous. This vision is turning into a reality thanks to advances in technology, but there are still challenges to address, particularly in the security domain e.g., data reliability. Taking into account the predicted evolution of the IoT in the coming years, it is necessary to provide confidence in this huge incoming information source. Blockchain has emerged as a key technology that will transform the way in which we share information. Building trust in distributed environments without the need for authorities is a technological advance that has the potential to change many industries, the IoT among them. Disruptive technologies such as big data and cloud computing have been leveraged by IoT to overcome its limitations since its conception, and we think blockchain will be one of the next ones. This paper focuses on this relationship, investigates challenges in blockchain IoT applications, and surveys the most relevant work in order to analyze how blockchain could potentially improve the IoT.},
	language = {en},
	urldate = {2021-10-11},
	journal = {Future Generation Computer Systems},
	author = {Reyna, Ana and Martín, Cristian and Chen, Jaime and Soler, Enrique and Díaz, Manuel},
	month = nov,
	year = {2018},
	keywords = {Blockchain, Internet of Things, Smart contract, Trust},
	pages = {173--190},
}

@article{astarita_review_2020,
	title = {A {Review} of {Blockchain}-{Based} {Systems} in {Transportation}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2078-2489/11/1/21},
	doi = {10.3390/info11010021},
	abstract = {This paper presents a literature review about the application of blockchain-based systems in transportation. The main aim was to identify, through the implementation of a multi-step methodology: current research-trends, main gaps in the literature, and possible future challenges. First, a bibliometric analysis was carried out to obtain a broad overview of the topic of interest. Subsequently, the most influential contributions were analysed in depth, with reference to the following two areas: supply chain and logistics; road traffic management and smart cities. The most important result is that the blockchain technology is still in an early stage, but appears extremely promising, given its possible applications within multiple fields, such as food track and trace, regulatory compliance, smart vehicles\&rsquo; security, and supply-demand matching. Much effort is still necessary for reaching the maturation stage because several models have been theorized in recent years, but very few have been implemented within real contexts. Moreover, the link blockchain-sustainability was explored, showing that this technology could be the trigger for limiting food waste, reducing exhaust gas emissions, favouring correct urban development, and, in general, improving quality of life.},
	language = {en},
	number = {1},
	urldate = {2021-10-11},
	journal = {Information},
	author = {Astarita, Vittorio and Giofrè, Vincenzo Pasquale and Mirabelli, Giovanni and Solina, Vittorio},
	month = jan,
	year = {2020},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {blockchain, literature review, logistics, supply chain, transportation},
	pages = {21},
}

@inproceedings{shafagh_towards_2017,
	address = {New York, NY, USA},
	series = {{CCSW} '17},
	title = {Towards {Blockchain}-based {Auditable} {Storage} and {Sharing} of {IoT} {Data}},
	isbn = {978-1-4503-5204-8},
	url = {https://doi.org/10.1145/3140649.3140656},
	doi = {10.1145/3140649.3140656},
	abstract = {Today the cloud plays a central role in storing, processing, and distributing data. Despite contributing to the rapid development of IoT applications, the current IoT cloud-centric architecture has led into a myriad of isolated data silos that hinders the full potential of holistic data-driven analytics within the IoT. In this paper, we present a blockchain-based design for the IoT that brings a distributed access control and data management. We depart from the current trust model that delegates access control of our data to a centralized trusted authority and instead empower the users with data ownership. Our design is tailored for IoT data streams and enables secure data sharing. We enable a secure and resilient access control management, by utilizing the blockchain as an auditable and distributed access control layer to the storage layer. We facilitate the storage of time-series IoT data at the edge of the network via a locality-aware decentralized storage system that is managed with the blockchain technology. Our system is agnostic of the physical storage nodes and supports as well utilization of cloud storage resources as storage nodes.},
	urldate = {2021-10-10},
	booktitle = {Proceedings of the 2017 on {Cloud} {Computing} {Security} {Workshop}},
	publisher = {Association for Computing Machinery},
	author = {Shafagh, Hossein and Burkhalter, Lukas and Hithnawi, Anwar and Duquennoy, Simon},
	month = nov,
	year = {2017},
	keywords = {access control, blockchain, cloud, edge, iot, security, time-series},
	pages = {45--50},
}

@article{sun_blockchain-based_2016,
	title = {Blockchain-based sharing services: {What} blockchain technology can contribute to smart cities},
	volume = {2},
	issn = {2199-4730},
	shorttitle = {Blockchain-based sharing services},
	url = {https://doi.org/10.1186/s40854-016-0040-y},
	doi = {10.1186/s40854-016-0040-y},
	abstract = {The notion of smart city has grown popular over the past few years. It embraces several dimensions depending on the meaning of the word “smart” and benefits from innovative applications of new kinds of information and communications technology to support communal sharing.},
	number = {1},
	urldate = {2021-10-11},
	journal = {Financial Innovation},
	author = {Sun, Jianjun and Yan, Jiaqi and Zhang, Kem Z. K.},
	month = dec,
	year = {2016},
	keywords = {Blockchain, Internet of Things (IoT), Sharing economy, Smart city, Smart contract},
	pages = {26},
}

@inproceedings{samaniego_blockchain_2016,
	title = {Blockchain as a {Service} for {IoT}},
	doi = {10.1109/iThings-GreenCom-CPSCom-SmartData.2016.102},
	abstract = {A blockchain is a distributed and decentralized ledger that contains connected blocks of transactions. Unlike other ledger approaches, blockchain guarantees tamper proof storage of approved transactions. Due to its distributed and decentralized organization, blockchain is beeing used within IoT e.g. to manage device configuration, store sensor data and enable micro-payments. This paper presents the idea of using blockchain as a service for IoT and evaluates the performance of a cloud and edge hosted blockchain implementation.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Internet} of {Things} ({iThings}) and {IEEE} {Green} {Computing} and {Communications} ({GreenCom}) and {IEEE} {Cyber}, {Physical} and {Social} {Computing} ({CPSCom}) and {IEEE} {Smart} {Data} ({SmartData})},
	author = {Samaniego, Mayra and Jamsrandorj, Uurtsaikh and Deters, Ralph},
	month = dec,
	year = {2016},
	keywords = {Bandwidth, Blockchain as a service, Cloud computing, Computer science, Delays, Distributed databases, Internet of Things, IoT, Online banking, edge},
	pages = {433--436},
}

@incollection{lieberman_end-user_2006,
	address = {Dordrecht},
	series = {Human-{Computer} {Interaction} {Series}},
	title = {End-{User} {Development}: {An} {Emerging} {Paradigm}},
	isbn = {978-1-4020-5386-3},
	shorttitle = {End-{User} {Development}},
	url = {https://doi.org/10.1007/1-4020-5386-X_1},
	abstract = {We think that over the next few years, the goal of interactive systems and services will evolve from just making systems easy to use (even though that goal has not yet been completely achieved) to making systems that are easy to develop by end users. By now, most people have become familiar with the basic functionality and interfaces of computers, but they are not able to manage any programming language. Therefore, they cannot develop new applications or modify current ones according to their needs.In order to address such challenges it is necessary a new paradigm, based on a multidisciplinary approach involving several types of expertise, such as software engineering, human-computer interaction, CSCW, which are now rather fragmented and with little interaction. The resulting methods and tools can provide results useful across many application domains, such as ERP, multi-device services (accessible through both mobile and stationary devices), and professional applications.Key words. tailorability, end user programming, flexibility, usability},
	language = {en},
	urldate = {2021-10-11},
	booktitle = {End {User} {Development}},
	publisher = {Springer Netherlands},
	author = {Lieberman, Henry and Paternò, Fabio and Klann, Markus and Wulf, Volker},
	editor = {Lieberman, Henry and Paternò, Fabio and Wulf, Volker},
	year = {2006},
	doi = {10.1007/1-4020-5386-X_1},
	keywords = {Agile Software Development, Computer Support Cooperative Work, Human Centric Computing, Software Cost Estimation, Software Professional},
	pages = {1--8},
}

@article{ko_state_2011,
	title = {The state of the art in end-user software engineering},
	volume = {43},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/1922649.1922658},
	doi = {10.1145/1922649.1922658},
	abstract = {Most programs today are written not by professional software developers, but by people with expertise in other domains working towards goals for which they need computational support. For example, a teacher might write a grading spreadsheet to save time grading, or an interaction designer might use an interface builder to test some user interface design ideas. Although these end-user programmers may not have the same goals as professional developers, they do face many of the same software engineering challenges, including understanding their requirements, as well as making decisions about design, reuse, integration, testing, and debugging. This article summarizes and classifies research on these activities, defining the area of End-User Software Engineering (EUSE) and related terminology. The article then discusses empirical research about end-user software engineering activities and the technologies designed to support them. The article also addresses several crosscutting issues in the design of EUSE tools, including the roles of risk, reward, and domain complexity, and self-efficacy in the design of EUSE tools and the potential of educating users about software engineering principles.},
	number = {3},
	urldate = {2021-10-11},
	journal = {ACM Computing Surveys},
	author = {Ko, Amy J. and Abraham, Robin and Beckwith, Laura and Blackwell, Alan and Burnett, Margaret and Erwig, Martin and Scaffidi, Chris and Lawrance, Joseph and Lieberman, Henry and Myers, Brad and Rosson, Mary Beth and Rothermel, Gregg and Shaw, Mary and Wiedenbeck, Susan},
	month = apr,
	year = {2011},
	keywords = {End-user software engineering, end-user development, end-user programming, human-computer interaction, visual programming},
	pages = {21:1--21:44},
}

@misc{IETF2014COAP,
	title = {{RFC} 7252: {The} constrained application protocol ({CoAP})},
	url = {https://datatracker.ietf.org/doc/html/rfc7252},
	author = {{Shelby} and {Z.} and Hartke, K. and Bormann, C.},
	month = jun,
	year = {2014},
}

@book{white2011making,
	title = {Making embedded systems: {Design} patterns for great software},
	publisher = {O'Reilly Media, Inc.},
	author = {White, Elecia},
	year = {2011},
}

@inproceedings{tomlein_chariot_2017,
	address = {New York, NY, USA},
	series = {{IoT} '17},
	title = {{CharIoT}: an end-user programming environment for the {IoT}},
	isbn = {978-1-4503-5318-2},
	shorttitle = {{CharIoT}},
	url = {https://doi.org/10.1145/3131542.3140261},
	doi = {10.1145/3131542.3140261},
	abstract = {Despite the breadth of related work, enabling end-users of varying technical ability to leverage sensor data to control their Internet of Things (IoT)-enabled installations remains a challenge. This work proposes a unified interface that provides three building blocks to support the end-user configuration of IoT environments: capturing higher-level events in the installation through virtual sensors, construction of automation rules with a visual overview of the current configuration and support for sharing configuration between end-users using a recommendation mechanism.},
	urldate = {2021-10-09},
	booktitle = {Proceedings of the {Seventh} {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Tomlein, Matúš and Boovaraghavan, Sudershan and Agarwal, Yuvraj and Dey, Anind K.},
	month = oct,
	year = {2017},
	keywords = {end-user programming, visual programming},
	pages = {1--2},
}

@article{lyytinen2002ubiquitous,
	title = {Ubiquitous computing},
	volume = {45},
	number = {12},
	journal = {Communications of the ACM},
	author = {Lyytinen, Kalle and Yoo, Youngjin},
	year = {2002},
	note = {Publisher: Citeseer},
	pages = {63--96},
}

@book{mitchell1996city,
	title = {City of bits: space, place, and the infobahn},
	publisher = {MIT press},
	author = {Mitchell, William John},
	year = {1996},
}

@book{Brooks1978MythicalManMonth,
	address = {USA},
	edition = {1st},
	title = {The mythical man-month: {Essays} on software engineering},
	isbn = {0-201-00650-2},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Brooks, Frederick P.},
	year = {1978},
}

@inproceedings{zhang_iot_2014,
	title = {{IoT} {Security}: {Ongoing} {Challenges} and {Research} {Opportunities}},
	shorttitle = {{IoT} {Security}},
	doi = {10.1109/SOCA.2014.58},
	abstract = {The Internet of Things (IoT) opens opportunities for wearable devices, home appliances, and software to share and communicate information on the Internet. Given that the shared data contains a large amount of private information, preserving information security on the shared data is an important issue that cannot be neglected. In this paper, we begin with general information security background of IoT and continue on with information security related challenges that IoT will encountered. Finally, we will also point out research directions that could be the future work for the solutions to the security challenges that IoT encounters.},
	booktitle = {2014 {IEEE} 7th {International} {Conference} on {Service}-{Oriented} {Computing} and {Applications}},
	author = {Zhang, Zhi-Kai and Cho, Michael Cheng Yi and Wang, Chia-Wei and Hsu, Chia-Wei and Chen, Chong-Kuan and Shieh, Shiuhpyng},
	month = nov,
	year = {2014},
	note = {ISSN: 2163-2871},
	keywords = {Androids, Cryptography, Humanoid robots, Internet of Things, Malware, Software, authenticity, identification, information security, malware, naming},
	pages = {230--234},
}

@inproceedings{berger_using_2018,
	address = {New York, NY, USA},
	series = {{SEsCPS} '18},
	title = {On using blockchains for safety-critical systems},
	isbn = {978-1-4503-5728-9},
	url = {https://doi.org/10.1145/3196478.3196480},
	doi = {10.1145/3196478.3196480},
	abstract = {Today's industries in various domains are becoming more and more driven by software as innovator. They range from web applications powering our increasingly digitalized daily lives to deeply embedded systems driving complex and safety-critical cyber-physical systems (CPS) as in, for example, self-driving vehicles. Companies need to continuously rejuvenate their product portfolio for adopting new ideas to remain competitive. A recent idea that is permeating from its original application domain of financial use cases are blockchains, where researchers and companies try to apply key ideas behind them to other domains.},
	urldate = {2021-10-08},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Software} {Engineering} for {Smart} {Cyber}-{Physical} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Berger, Christian and Penzenstadler, Birgit and Drögehorn, Olaf},
	month = may,
	year = {2018},
	pages = {30--36},
}

@article{kaku_using_2016,
	series = {The 11th {International} {Conference} on {Future} {Networks} and {Communications} ({FNC} 2016) / {The} 13th {International} {Conference} on {Mobile} {Systems} and {Pervasive} {Computing} ({MobiSPC} 2016) / {Affiliated} {Workshops}},
	title = {Using {Provenance} and {CoAP} to track {Requests}/{Responses} in {IoT}},
	volume = {94},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S187705091631763X},
	doi = {10.1016/j.procs.2016.08.023},
	abstract = {Until recently, not much attention has been drawn to the need to provide documentary evidence for ensuring reliability, transparency and, most importantly, tracing the source of requests/responses in the Internet of Things. The knowledge of provenance is considered as a key component in establishing the above-mentioned issues. Most research, to a large extent, focus on requesting data, which is based on user inference and decision making, by utilising provenance information. However, little or nothing has been done regarding requests and responses and, most importantly, from the machine perspective. Consequently, this paper proposes a light-weight prototype system for tracing the source of requests/responses using provenance information over CoAP in the Internet of Things. We also provide performance evaluation of the prototypic system using metrics such as response time (ms) and throughput (KB/s). Finally, findings from our experiment are presented and discussed.},
	language = {en},
	urldate = {2021-10-08},
	journal = {Procedia Computer Science},
	author = {Kaku, Emmanuel and Lomotey, Richard . K. and Deters, Ralph},
	month = jan,
	year = {2016},
	keywords = {CoAP, IoT, Meta-data, Provenance, REST, URI},
	pages = {144--151},
}

@misc{noauthor_opentelemetry_nodate,
	title = {{OpenTelemetry} {\textbar} {Core} {Concepts}},
	url = {https://opentelemetry.lightstep.com/core-concepts/context-propagation/},
	abstract = {OpenTelemetry makes robust, portable telemetry a built-in feature of cloud-native software, providing a single set of APIs, libraries, agents, and collector services to capture distributed traces and metrics from your application.},
	language = {en},
	urldate = {2021-10-08},
}

@article{incki_novel_2018,
	title = {A {Novel} {Runtime} {Verification} {Solution} for {IoT} {Systems}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2813887},
	abstract = {Internet of Things (IoT) systems promise a seamless connected world with machines integrating their services without human intervention. It's highly probable that the entities participating in such autonomous machine to machine interactions are to be provided by different manufactures. Thus, integrating such heterogeneous devices from many providers complicates design and verification of IoT systems at an unprecedented scale. In this paper, we propose a novel runtime verification approach for IoT systems. The contributions of our proposed solution include: exploiting the interactions in message sequence charts (MSC) to specify message exchanges of constrained application protocol-based IoT systems in terms of events, a novel event calculus for formally describing IoT system constraints specified by means of MSCs, and an event processing algebra that uses complex-event processing techniques for detecting failures in the system by monitoring the runtime event occurrences with respect to the system constraints defined by event calculus. We further demonstrate the viability of proposed solution with case studies.},
	journal = {IEEE Access},
	author = {Incki, Koray and Ari, Ismail},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Calculus, Internet of Things, Mediation, Monitoring, Protocols, Runtime, Servers, Testing, complex-event processing, event calculus, message sequence charts, runtime verification},
	pages = {13501--13512},
}

@inproceedings{asadollah_runtime_2018,
	title = {A {Runtime} {Verification} {Tool} for {Detecting} {Concurrency} {Bugs} in {FreeRTOS} {Embedded} {Software}},
	doi = {10.1109/ISPDC2018.2018.00032},
	abstract = {This article presents a runtime verification tool for embedded software executing under the open source real-time operating system FreeRTOS. The tool detects and diagnoses concurrency bugs such as deadlock, starvation, and suspension based-locking. The tool finds concurrency bugs at runtime without debugging and tracing the source code. The tool uses the Tracealyzer tool for logging relevant events. Analysing the logs, our tool can detect the concurrency bugs by applying algorithms for diagnosing each concurrency bug type individually. In this paper, we present the implementation of the tool, as well as its functional architecture, together with illustration of its use. The tool can be used during program testing to gain interesting information about embedded software executions. We present initial results of running the tool on some classical bug examples running on an AVR 32-bit board SAM4S.},
	booktitle = {2018 17th {International} {Symposium} on {Parallel} and {Distributed} {Computing} ({ISPDC})},
	author = {Asadollah, Sara Abbaspour and Sundmark, Daniel and Eldh, Sigrid and Hansson, Hans},
	month = jun,
	year = {2018},
	keywords = {Bug Detector, Computer bugs, Concurrency Bugs, Concurrent computing, Embedded Software, Embedded software, FreeRTOS, Monitoring, Runtime, Runtime Verification Tool, Task analysis, Tools},
	pages = {172--179},
}

@inproceedings{incki_runtime_2017,
	title = {Runtime verification of {IoT} systems using {Complex} {Event} {Processing}},
	doi = {10.1109/ICNSC.2017.8000163},
	abstract = {Internet of Things (IoT) is a new computing paradigm that is proliferated by wide adoption of application level protocols such as MQTT and CoAP, each of which defines different styles of sequential interaction of events. Even though there is a considerable effort in the literature for verification of such complex and distributed systems, a practical solution for IoT systems that supports runtime system verification is still missing. In this paper, we present a runtime monitoring approach for IoT systems that exploits event relations expressed in terms of sequential interaction messaging model of Constrained Application Protocol (CoAP). We propose the use of Complex-Event Processing (CEP) to detect failures at runtime by exploiting complex event patterns defined via predetermined event algebra. We further present a simple case scenario to demonstrate the applicability of the approach on Wireless Token Ring Protocol execution.},
	booktitle = {2017 {IEEE} 14th {International} {Conference} on {Networking}, {Sensing} and {Control} ({ICNSC})},
	author = {İnçki, Koray and Arı, İsmail and Sözer, Hasan},
	month = may,
	year = {2017},
	keywords = {CoAP, Correlation, Internet of things, Tools, complex-event processing, event algebra, runtime monitoring, verification},
	pages = {625--630},
}

@techreport{qiu_fault_2003,
	title = {Fault {Detection}, {Isolation}, and {Diagnosis} in {Multihop} {Wireless} {Networks}},
	abstract = {Network management in multihop wireless networks is key to efficient and reliable network operation. In this paper, we focus on a part of the general network management problem, namely fault detection, isolation, and diagnosis. We propose a system that employs online trace-driven simulation as a diagnostic tool for detecting faults and performing root cause analysis. We apply our system to diagnose faults such as packet dropping, link congestion, MAC misbehavior, and external noise, and show that it yields reasonably accurate results. In addition, we show that our system can be used to evaluate alternative network and node configurations to improve performance of on-going long-lived flows. Moreover, our technique is general enough to be applied in other wireless and wireline network management system.},
	author = {Qiu, Lili and Bahl, Paramvir and Rao, Ananth and Zhou, Lidong},
	year = {2003},
}

@article{ayers_traceback_nodate,
	title = {{TraceBack}: {First} {Fault} {Diagnosis} by {Reconstruction} of {Distributed} {Control} {Flow}},
	abstract = {Faults that occur in production systems are the most important faults to fix, but most production systems lack the debugging facilities present in development environments. TraceBack provides debugging information for production systems by providing execution history data about program problems (such as crashes, hangs, and exceptions). TraceBack supports features commonly found in production environments such as multiple threads, dynamically loaded modules, multiple source languages (e.g., Java applications running with JNI modules written in C++), and distributed execution across multiple computers. TraceBack supports first fault diagnosis—discovering what went wrong the first time a fault is encountered. The user can see how the program reached the fault state without having to re-run the computation; in effect enabling a limited form of a debugger in production code.},
	language = {en},
	author = {Ayers, Andrew and Schooler, Richard and Metcalf, Chris and Agarwal, Anant and Rhee, Junghwan and Witchel, Emmett},
	pages = {12},
}

@article{conger_hundreds_2021,
	chapter = {Technology},
	title = {Hundreds of {Google} {Employees} {Unionize}, {Culminating} {Years} of {Activism}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2021/01/04/technology/google-employees-union.html},
	abstract = {The creation of the union, a rarity in Silicon Valley, follows years of increasing outspokenness by Google workers. Executives have struggled to handle the change.},
	language = {en-US},
	urldate = {2021-10-07},
	journal = {The New York Times},
	author = {Conger, Kate},
	month = jan,
	year = {2021},
	keywords = {Alphabet Inc, Alphabet Workers Union, Artificial Intelligence, Collective Bargaining, Communications Workers of America, Computers and the Internet, Corporate Social Responsibility, Engineering and Engineers, Google Inc, Labor and Jobs, Organized Labor, Pichai, Sundar},
}

@article{kruchten_licensing_2008,
	title = {Licensing software engineers?},
	volume = {25},
	number = {6},
	journal = {IEEE Software},
	author = {Kruchten, Philippe},
	year = {2008},
	note = {Publisher: IEEE},
	pages = {35--37},
}

@incollection{bartocci_introduction_2018,
	address = {Cham},
	title = {Introduction to {Runtime} {Verification}},
	volume = {10457},
	isbn = {978-3-319-75631-8 978-3-319-75632-5},
	url = {http://link.springer.com/10.1007/978-3-319-75632-5_1},
	abstract = {The aim of this chapter is to act as a primer for those wanting to learn about Runtime Veriﬁcation (RV). We start by providing an overview of the main speciﬁcation languages used for RV. We then introduce the standard terminology necessary to describe the monitoring problem, covering the pragmatic issues of monitoring and instrumentation, and discussing extensively the monitorability problem.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Lectures on {Runtime} {Verification}},
	publisher = {Springer International Publishing},
	author = {Bartocci, Ezio and Falcone, Yliès and Francalanza, Adrian and Reger, Giles},
	editor = {Bartocci, Ezio and Falcone, Yliès},
	year = {2018},
	doi = {10.1007/978-3-319-75632-5_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--33},
}

@inproceedings{reiss_iot_2019,
	title = {{IoT} {End} {User} {Programming} {Models}},
	doi = {10.1109/SERP4IoT.2019.00008},
	abstract = {The advent of smart devices and sensors (the Internet of Things or IoT) will create increasing demands for the automation of devices based on sensor, time, and other inputs. This is essentially a programming task with all the problems and difficulties that programming entails, for example, modularity, feature interaction, debugging, and understanding. Moreover, much of the programming for smart devices is going to be done not by professional programmers but by end users, often end users without any programming experience or computational literacy. Our research is aimed at exploring the programming space and the associated issues using a case study of a smart sign that can be controlled using a variety of sensors. We have developed a general system for programming smart devices and, in this paper, explore a variety of different user interfaces for programming this system for our smart sign.},
	booktitle = {2019 {IEEE}/{ACM} 1st {International} {Workshop} on {Software} {Engineering} {Research} {Practices} for the {Internet} of {Things} ({SERP4IoT})},
	author = {Reiss, Steven P.},
	month = may,
	year = {2019},
	keywords = {Debugging, Internet of Things, Internet of thingws, Programming profession, Sensors, Smart devices, Software, debugging, end-user programming, program understanding},
	pages = {1--8},
}

@inproceedings{heimdahl_safety_2007,
	address = {Minneapolis, MN, USA},
	title = {Safety and {Software} {Intensive} {Systems}: {Challenges} {Old} and {New}},
	isbn = {978-0-7695-2829-8},
	shorttitle = {Safety and {Software} {Intensive} {Systems}},
	url = {http://ieeexplore.ieee.org/document/4221617/},
	doi = {10.1109/FOSE.2007.18},
	abstract = {There is an increased use of software in safety-critical systems; a trend that is likely to continue in the future. Although traditional system safety techniques are applicable to software intensive systems, there are new challenges emerging. In this report we will address four issues we believe will pose challenges in the future.},
	language = {en},
	urldate = {2021-10-06},
	booktitle = {Future of {Software} {Engineering} ({FOSE} '07)},
	publisher = {IEEE},
	author = {Heimdahl, Mats P.E.},
	month = may,
	year = {2007},
	pages = {137--152},
}

@article{leveson1993investigation,
	title = {An investigation of the {Therac}-25 accidents},
	volume = {26},
	number = {7},
	journal = {Computer},
	author = {Leveson, Nancy G and Turner, Clark S},
	year = {1993},
	note = {Publisher: IEEE},
	pages = {18--41},
}

@incollection{romanovsky_devils_2019,
	address = {Cham},
	title = {Devil’s in the {Detail}: {Through}-{Life} {Safety} and {Security} {Co}-assurance {Using} {SSAF}},
	volume = {11698},
	isbn = {978-3-030-26600-4 978-3-030-26601-1},
	shorttitle = {Devil’s in the {Detail}},
	url = {http://link.springer.com/10.1007/978-3-030-26601-1_21},
	abstract = {Regulatory bodies, industry and academia present a plethora of approaches for risk analysis and engineering for safety and security. However, few standards and approaches discuss the management of both safety and security risks. Fewer yet provide detail on how the two attributes interact within a given system. In this paper, the SafetySecurity Assurance Framework (SSAF) is presented as a candidate solution to many of the extant challenges of attribute co-assurance. It is a holistic approach, based on the concept of independent co-assurance, that considers both the technical risk impact and the socio-technical impact on assurance. The Framework’s Technical Risk Model (TRM) is applied and evaluated against a case study of an insulin pump. It is argued that SSAF TRM is not only a plausible and practical approach, but also more eﬀective for co-assurance than many existing approaches alone.},
	language = {en},
	urldate = {2021-10-05},
	booktitle = {Computer {Safety}, {Reliability}, and {Security}},
	publisher = {Springer International Publishing},
	author = {Johnson, Nikita and Kelly, Tim},
	editor = {Romanovsky, Alexander and Troubitsyna, Elena and Bitsch, Friedemann},
	year = {2019},
	doi = {10.1007/978-3-030-26601-1_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {299--314},
}

@inproceedings{asadollah_runtime_2018,
	title = {A {Runtime} {Verification} {Tool} for {Detecting} {Concurrency} {Bugs} in {FreeRTOS} {Embedded} {Software}},
	doi = {10.1109/ISPDC2018.2018.00032},
	abstract = {This article presents a runtime verification tool for embedded software executing under the open source real-time operating system FreeRTOS. The tool detects and diagnoses concurrency bugs such as deadlock, starvation, and suspension based-locking. The tool finds concurrency bugs at runtime without debugging and tracing the source code. The tool uses the Tracealyzer tool for logging relevant events. Analysing the logs, our tool can detect the concurrency bugs by applying algorithms for diagnosing each concurrency bug type individually. In this paper, we present the implementation of the tool, as well as its functional architecture, together with illustration of its use. The tool can be used during program testing to gain interesting information about embedded software executions. We present initial results of running the tool on some classical bug examples running on an AVR 32-bit board SAM4S.},
	booktitle = {2018 17th {International} {Symposium} on {Parallel} and {Distributed} {Computing} ({ISPDC})},
	author = {Asadollah, Sara Abbaspour and Sundmark, Daniel and Eldh, Sigrid and Hansson, Hans},
	month = jun,
	year = {2018},
	keywords = {Bug Detector, Computer bugs, Concurrency Bugs, Concurrent computing, Embedded Software, Embedded software, FreeRTOS, Monitoring, Runtime, Runtime Verification Tool, Task analysis, Tools},
	pages = {172--179},
}

@inproceedings{kraft_trace_2010,
	title = {Trace {Recording} for {Embedded} {Systems}: {Lessons} {Learned} from {Five} {Industrial} {Projects}},
	isbn = {978-3-642-16611-2},
	shorttitle = {Trace {Recording} for {Embedded} {Systems}},
	doi = {10.1007/978-3-642-16612-9_24},
	abstract = {This paper presents experiences from five industry collaboration projects performed between 2004 – 2009 where solutions for
embedded systems trace recording have been developed and evaluated; in four cases for specific industrial systems and in the
last case as a generic solution for a commercial real-time operating system, in collaboration with the RTOS company. The experiences
includes technical solutions regarding efficient instrumentation and logging, technology transfer issues and evaluation results
regarding CPU and RAM overhead. A brief overview of the Tracealyzer tool is also presented, a result of the first project (2004) which still is used by ABB Robotics and now in commercialization.},
	author = {Kraft, Johan and Wall, Anders and Kienle, Holger},
	month = nov,
	year = {2010},
	pages = {315--329},
}

@inproceedings{emt_department________________________bonn-rhein-sieg_university_of_applied_sciences_sankt-augustin________________________germany_use_2020,
	title = {{THE} {USE} {OF} {PERCEPIO} {TRACEALYZER} {FOR} {THE} {DEVELOPMENT} {OF} {FREERTOS}-{BASED} {APPLICATIONS}},
	url = {https://mcfpga.nure.ua/conf/2020-mcfpga/10-35598-mcfpga-2020-008},
	doi = {10.35598/mcfpga.2020.008},
	abstract = {This paper discusses some problems of development and testing of FreeRTOS-based application. The use of Tracealyzer software tool is proposed to make this process more convenient. The benefits of such usage have been shown on typical issues that can be met in development and debug phase.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {{MC}\&{FPGA}-2020},
	author = {{EMT Department
                        Bonn-Rhein-Sieg University of Applied Sciences Sankt-Augustin,
                        Germany} and Khomenko, Maksym and Velihorskyi, Oleksandr and {Biomedical
                        radioelectronic apparatus and system department Chernihiv National
                        University of Technology Chernihiv, Ukraine}},
	year = {2020},
	pages = {26--29},
}

@inproceedings{yang_medley_2019,
	address = {Davis CA USA},
	title = {Medley: {A} {Novel} {Distributed} {Failure} {Detector} for {IoT} {Networks}},
	isbn = {978-1-4503-7009-7},
	shorttitle = {Medley},
	url = {https://dl.acm.org/doi/10.1145/3361525.3361556},
	doi = {10.1145/3361525.3361556},
	abstract = {Efficient and correct operation of an IoT network requires the presence of a failure detector and membership protocol amongst the IoT nodes. This paper presents a new failure detector for IoT settings where nodes are connected via a wireless ad-hoc network. This failure detector, which we name Medley, is fully decentralized, allows IoT nodes to maintain a local membership list of other alive nodes, detects failures quickly (and updates the membership list), and incurs low communication overhead in the underlying ad-hoc network. In order to minimize detection time and communication, we adapt a failure detector originally proposed for datacenters (SWIM), for the IoT environment. In Medley each node picks a medley of ping targets in a randomized and skewed manner, preferring nearer nodes. Via analysis and NS-3 simulation we show the right mix of pinging probabilities that simultaneously optimize detection time and communication traffic. We have also implemented Medley for Raspberry Pis, and present deployment results.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 20th {International} {Middleware} {Conference}},
	publisher = {ACM},
	author = {Yang, Rui and Zhu, Shichu and Li, Yifei and Gupta, Indranil},
	month = dec,
	year = {2019},
	pages = {319--331},
}

@misc{kingatua_iot_2019,
	title = {{IoT} {System} {Tests} :: {Checking} for {Failure}},
	shorttitle = {{IoT} {System} {Tests}},
	url = {https://medium.com/supplyframe-hardware/iot-system-tests-checking-for-failure-c146d2ebb8ef},
	abstract = {Internet of Things sensors use a diverse range of technologies to collect information, communicate with each other, and transmit data to…},
	language = {en},
	urldate = {2021-10-04},
	journal = {Supplyframe},
	author = {Kingatua, Amos},
	month = may,
	year = {2019},
}

@inproceedings{huang_software_2014,
	title = {Software {Failure} {Detection} {Using} {Pattern}'s {Position} {Distribution}},
	doi = {10.1109/ICMTMA.2014.145},
	abstract = {Pattern-based software failure detection is an important topic of research in recent years. In this method, a set of patterns from program execution traces are extracted, and represented as features, while their occurrence frequencies are treated as the corresponding feature values. But this conventional method has its limitation due to neglect the pattern's position information, which is important for the classification of program traces. Patterns occurs in the different positions of the trace are likely to represent different meanings. In this paper, we present a novel approach for using pattern's position distribution as features to detect software failure. In this method, we divide sequence into several sections and then compute pattern's distribution in each section, the distribution of all patterns are then used as features to train a classifier. This method outperforms conventional frequency based method due to effectively identify software failures occur through mis-used software patterns. The comparative experiments in both artificial and real datasets show the effectiveness of this method.},
	booktitle = {2014 {Sixth} {International} {Conference} on {Measuring} {Technology} and {Mechatronics} {Automation}},
	author = {Huang, He and Chen, Ziniu and Chen, Peng and Sun, Yan and Xie, Qianlong and Ma, Shuai and Chen, Hongjun},
	month = jan,
	year = {2014},
	note = {ISSN: 2157-1481},
	keywords = {Automation, Classification, Feature, Mechatronics, Pattern, Position Distribution, Software Failure Detection},
	pages = {593--597},
}

@article{hangal_tracking_nodate,
	title = {Tracking {Down} {Software} {Bugs} {Using} {Automatic} {Anomaly} {Detection}},
	abstract = {This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program’s results.},
	language = {en},
	author = {Hangal, Sudheendra and Lam, Monica S},
	pages = {11},
}

@misc{noauthor_toward_nodate,
	title = {Toward automatic detection of software failures {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/707619},
	urldate = {2021-10-04},
}

@article{islam_towards_nodate,
	title = {Towards understanding the challenges faced by machine learning software developers and enabling automated solutions},
	language = {en},
	author = {Islam, Johirul},
	pages = {162},
}

@inproceedings{vahabzadeh_empirical_2015,
	address = {Bremen, Germany},
	title = {An empirical study of bugs in test code},
	isbn = {978-1-4673-7532-0},
	url = {http://ieeexplore.ieee.org/document/7332456/},
	doi = {10.1109/ICSM.2015.7332456},
	abstract = {Testing aims at detecting (regression) bugs in production code. However, testing code is just as likely to contain bugs as the code it tests. Buggy test cases can silently miss bugs in the production code or loudly ring false alarms when the production code is correct. We present the ﬁrst empirical study of bugs in test code to characterize their prevalence and root cause categories. We mine the bug repositories and version control systems of 211 Apache Software Foundation (ASF) projects and ﬁnd 5,556 test-related bug reports. We (1) compare properties of test bugs with production bugs, such as active time and ﬁxing effort needed, and (2) qualitatively study 443 randomly sampled test bug reports in detail and categorize them based on their impact and root causes. Our results show that (1) around half of all the projects had bugs in their test code; (2) the majority of test bugs are false alarms, i.e., test fails while the production code is correct, while a minority of these bugs result in silent horrors, i.e., test passes while the production code is incorrect; (3) incorrect and missing assertions are the dominant root cause of silent horror bugs; (4) semantic (25\%), ﬂaky (21\%), environmentrelated (18\%) bugs are the dominant root cause categories of false alarms; (5) the majority of false alarm bugs happen in the exercise portion of the tests, and (6) developers contribute more actively to ﬁxing test bugs and test bugs are ﬁxed sooner compared to production bugs. In addition, we evaluate whether existing bug detection tools can detect bugs in test code.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Vahabzadeh, Arash and Fard, Amin Milani and Mesbah, Ali},
	month = sep,
	year = {2015},
	pages = {101--110},
}

@inproceedings{islam_repairing_2020,
	address = {Seoul South Korea},
	title = {Repairing deep neural networks: fix patterns and challenges},
	isbn = {978-1-4503-7121-6},
	shorttitle = {Repairing deep neural networks},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380378},
	doi = {10.1145/3377811.3380378},
	abstract = {Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
	month = jun,
	year = {2020},
	pages = {1135--1146},
}

@inproceedings{islam_comprehensive_2019,
	address = {Tallinn Estonia},
	title = {A comprehensive study on deep learning bug characteristics},
	isbn = {978-1-4503-5572-8},
	url = {https://dl.acm.org/doi/10.1145/3338906.3338955},
	doi = {10.1145/3338906.3338955},
	abstract = {Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48\% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43\% of the times. We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh},
	month = aug,
	year = {2019},
	pages = {510--520},
}

@inproceedings{zhang_empirical_2018,
	address = {Amsterdam Netherlands},
	title = {An empirical study on {TensorFlow} program bugs},
	isbn = {978-1-4503-5699-2},
	url = {https://dl.acm.org/doi/10.1145/3213846.3213866},
	doi = {10.1145/3213846.3213866},
	abstract = {Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research e orts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To ll this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These ndings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 27th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu},
	month = jul,
	year = {2018},
	pages = {129--140},
}

@article{humbatova_taxonomy_2019,
	title = {Taxonomy of {Real} {Faults} in {Deep} {Learning} {Systems}},
	url = {http://arxiv.org/abs/1910.11015},
	abstract = {The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Over ow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our nal taxonomy was validated with a survey involving an additional set of 21 developers, con rming that almost all fault categories (13/15) were experienced by at least 50\% of the survey participants.},
	language = {en},
	urldate = {2021-10-04},
	journal = {arXiv:1910.11015 [cs]},
	author = {Humbatova, Nargiz and Jahangirova, Gunel and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.11015},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{saha_empirical_2014,
	address = {Antwerp, Belgium},
	title = {An empirical study of long lived bugs},
	isbn = {978-1-4799-3752-3},
	url = {http://ieeexplore.ieee.org/document/6747164/},
	doi = {10.1109/CSMR-WCRE.2014.6747164},
	abstract = {Bug ﬁxing is a crucial part of software development and maintenance. A large number of bugs often indicate poor software quality since buggy behavior not only causes failures that may be costly but also has a detrimental effect on the user’s overall experience with the software product. The impact of long lived bugs can be even more critical since experiencing the same bug version after version can be particularly frustrating for user. While there are many studies that investigate factors affecting bug ﬁxing time for entire bug repositories, to the best of our knowledge, none of these studies investigates the extent and reasons of long lived bugs. In this paper, we analyzed long lived bugs from ﬁve different perspectives: their proportion, severity, assignment, reasons, as well as the nature of ﬁxes. Our study on four open-source projects shows that there are a considerable number of long lived bugs in each system and over 90\% of them adversely affect the user’s experience. The reasons of these long lived bugs are diverse including long assignment time, not understanding their importance in advance etc. However, many bug-ﬁxes were delayed without any speciﬁc reasons. Our analysis of bug ﬁxing changes further shows that many long lived bugs can be ﬁxed quickly through careful prioritization. We believe our results will help both developers and researchers to better understand factors behind delays, improve the overall bug ﬁxing process, and investigate analytical approaches for prioritizing bugs based on bug severity as well as expected bug ﬁxing effort. Index Terms—Bug tracking system, bug triaging, bug survival time I. INTRODUCTION Software development and maintenance is a complex process. Although developers and testers try their best to make their software error free, in practice software ships with bugs. The number of bugs in software is a signiﬁcant indicator of software quality since bugs can adversely affect users experience directly. Therefore, developers are generally very active in ﬁnding and removing bugs.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2014 {Software} {Evolution} {Week} - {IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering} ({CSMR}-{WCRE})},
	publisher = {IEEE},
	author = {Saha, Ripon K. and Khurshid, Sarfraz and Perry, Dewayne E.},
	month = feb,
	year = {2014},
	pages = {144--153},
}

@inproceedings{wan_bug_2017,
	address = {Buenos Aires, Argentina},
	title = {Bug {Characteristics} in {Blockchain} {Systems}: {A} {Large}-{Scale} {Empirical} {Study}},
	isbn = {978-1-5386-1544-7},
	shorttitle = {Bug {Characteristics} in {Blockchain} {Systems}},
	url = {http://ieeexplore.ieee.org/document/7962390/},
	doi = {10.1109/MSR.2017.59},
	abstract = {Bugs severely hurt blockchain system dependability. A thorough understanding of blockchain bug characteristics is required to design effective tools for preventing, detecting and mitigating bugs. We perform an empirical study on bug characteristics in eight representative open source blockchain systems. First, we manually examine 1,108 bug reports to understand the nature of the reported bugs. Second, we leverage card sorting to label the bug reports, and obtain ten bug categories in blockchain systems. We further investigate the frequency distribution of bug categories across projects and programming languages. Finally, we study the relationship between bug categories and bug ﬁxing time. The ﬁndings include: (1) semantic bugs are the dominant runtime bug category; (2) frequency distributions of bug types show similar trends across different projects and programming languages; (3) security bugs take the longest median time to be ﬁxed; (4) 35.71\% performance bugs are ﬁxed in more than one year; performance bugs take the longest average time to be ﬁxed.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Wan, Zhiyuan and Lo, David and Xia, Xin and Cai, Liang},
	month = may,
	year = {2017},
	pages = {413--424},
}

@inproceedings{zhang_empirical_2020,
	address = {Seoul South Korea},
	title = {An empirical study on program failures of deep learning jobs},
	isbn = {978-1-4503-7121-6},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380362},
	doi = {10.1145/3377811.3380362},
	abstract = {Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zhang, Ru and Xiao, Wencong and Zhang, Hongyu and Liu, Yu and Lin, Haoxiang and Yang, Mao},
	month = jun,
	year = {2020},
	pages = {1159--1170},
}

@inproceedings{thung_empirical_2012,
	address = {Dallas, TX, USA},
	title = {An {Empirical} {Study} of {Bugs} in {Machine} {Learning} {Systems}},
	isbn = {978-1-4673-4638-2 978-0-7695-4888-3},
	url = {http://ieeexplore.ieee.org/document/6405375/},
	doi = {10.1109/ISSRE.2012.22},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2012 {IEEE} 23rd {International} {Symposium} on {Software} {Reliability} {Engineering}},
	publisher = {IEEE},
	author = {Thung, Ferdian and Wang, Shaowei and Lo, David and Jiang, Lingxiao},
	month = nov,
	year = {2012},
	pages = {271--280},
}

@inproceedings{eghbali_no_2020,
	address = {Virtual Event Australia},
	title = {No strings attached: an empirical study of string-related software bugs},
	isbn = {978-1-4503-6768-4},
	shorttitle = {No strings attached},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416576},
	doi = {10.1145/3324884.3416576},
	abstract = {Strings play many roles in programming because they often contain complex and semantically rich information. For example, programmers use strings to filter inputs via regular expression matching, to express the names of program elements accessed through some form of reflection, to embed code written in another formal language, and to assemble textual output produced by a program. The omnipresence of strings leads to a wide range of mistakes that developers may make, yet little is currently known about these mistakes. The lack of knowledge about string-related bugs leads to developers repeating the same mistakes again and again, and to poor support for finding and fixing such bugs. This paper presents the first empirical study of the root causes, consequences, and other properties of string-related bugs. We systematically study 204 string-related bugs in a diverse set of projects written in JavaScript, a language where strings play a particularly important role. Our findings include (i) that many string-related mistakes are caused by a recurring set of root cause patterns, such as incorrect string literals and regular expressions, (ii) that string-related bugs have a diverse set of consequences, including incorrect output or silent omission of expected behavior, (iii) that fixing string-related bugs often requires changing just a single line, with many of the required repair ingredients available in the surrounding code, (iv) that stringrelated bugs occur across all parts of applications, including the core components, and (v) that almost none of these bugs are detected by existing static analyzers. Our findings not only show the importance and prevalence of string-related bugs, but they help developers to avoid common mistakes and tool builders to tackle the challenge of finding and fixing string-related bugs.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Eghbali, Aryaz and Pradel, Michael},
	month = dec,
	year = {2020},
	pages = {956--967},
}

@article{miller_relevance_2020,
	title = {The {Relevance} of {Classic} {Fuzz} {Testing}: {Have} {We} {Solved} {This} {One}?},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {The {Relevance} of {Classic} {Fuzz} {Testing}},
	url = {http://arxiv.org/abs/2008.06537},
	doi = {10.1109/TSE.2020.3047766},
	abstract = {As fuzz testing has passed its 30th anniversary, and in the face of the incredible progress in fuzz testing techniques and tools, the question arises if the classic, basic fuzz technique is still useful and applicable? In that tradition, we have updated the basic fuzz tools and testing scripts and applied them to a large collection of Unix utilities on Linux, FreeBSD, and MacOS. As before, our failure criteria was whether the program crashed or hung. We found that 9 crash or hang out of 74 utilities on Linux, 15 out of 78 utilities on FreeBSD, and 12 out of 76 utilities on MacOS. A total of 24 different utilities failed across the three platforms. We note that these failure rates are somewhat higher than our in previous 1995, 2000, and 2006 studies of the reliability of command line utilities. In the basic fuzz tradition, we debugged each failed utility and categorized the causes the failures. Classic categories of failures, such as pointer and array errors and not checking return codes, were still broadly present in the current results. In addition, we found a couple of new categories of failures appearing. We present examples of these failures to illustrate the programming practices that allowed them to happen.},
	language = {en},
	urldate = {2021-10-04},
	journal = {IEEE Transactions on Software Engineering},
	author = {Miller, Barton P. and Zhang, Mengxiao and Heymann, Elisa R.},
	year = {2020},
	note = {arXiv: 2008.06537},
	keywords = {Computer Science - Software Engineering},
	pages = {1--1},
}

@inproceedings{menghi_trace-checking_2021,
	address = {Madrid, ES},
	title = {Trace-{Checking} {CPS} {Properties}: {Bridging} the {Cyber}-{Physical} {Gap}},
	isbn = {978-1-66540-296-5},
	shorttitle = {Trace-{Checking} {CPS} {Properties}},
	url = {https://ieeexplore.ieee.org/document/9402030/},
	doi = {10.1109/ICSE43902.2021.00082},
	abstract = {Cyber-physical systems combine software and physical components. Speciﬁcation-driven trace-checking tools for CPS usually provide users with a speciﬁcation language to express the requirements of interest, and an automatic procedure to check whether these requirements hold on the execution traces of a CPS. Although there exist several speciﬁcation languages for CPS, they are often not sufﬁciently expressive to allow the speciﬁcation of complex CPS properties related to the software and the physical components and their interactions.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Menghi, Claudio and Vigano, Enrico and Bianculli, Domenico and Briand, Lionel C.},
	month = may,
	year = {2021},
	pages = {847--859},
}

@inproceedings{dorri_blockchain_2017,
	title = {Blockchain for {IoT} security and privacy: {The} case study of a smart home},
	shorttitle = {Blockchain for {IoT} security and privacy},
	doi = {10.1109/PERCOMW.2017.7917634},
	abstract = {Internet of Things (IoT) security and privacy remain a major challenge, mainly due to the massive scale and distributed nature of IoT networks. Blockchain-based approaches provide decentralized security and privacy, yet they involve significant energy, delay, and computational overhead that is not suitable for most resource-constrained IoT devices. In our previous work, we presented a lightweight instantiation of a BC particularly geared for use in IoT by eliminating the Proof of Work (POW) and the concept of coins. Our approach was exemplified in a smart home setting and consists of three main tiers namely: cloud storage, overlay, and smart home. In this paper we delve deeper and outline the various core components and functions of the smart home tier. Each smart home is equipped with an always online, high resource device, known as “miner” that is responsible for handling all communication within and external to the home. The miner also preserves a private and secure BC, used for controlling and auditing communications. We show that our proposed BC-based smart home framework is secure by thoroughly analysing its security with respect to the fundamental security goals of confidentiality, integrity, and availability. Finally, we present simulation results to highlight that the overheads (in terms of traffic, processing time and energy consumption) introduced by our approach are insignificant relative to its security and privacy gains.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} ({PerCom} {Workshops})},
	author = {Dorri, Ali and Kanhere, Salil S. and Jurdak, Raja and Gauravaram, Praveen},
	month = mar,
	year = {2017},
	keywords = {Cloud computing, Conferences, Internet of Things, Online banking, Privacy, Security, Smart homes},
	pages = {618--623},
}

@article{satija_blockene_nodate,
	title = {Blockene: {A} {High}-throughput {Blockchain} {Over} {Mobile} {Devices}},
	abstract = {We introduce Blockene, a blockchain that reduces resource usage at member nodes by orders of magnitude, requiring only a smartphone to participate in block validation and consensus. Despite being lightweight, Blockene provides a high throughput of transactions and scales to a large number of participants. Blockene consumes negligible battery and data in smartphones, enabling millions of users to participate in the blockchain without incentives, to secure transactions with their collective honesty. Blockene achieves these properties with a novel split-trust design based on delegating storage and gossip to untrusted nodes.},
	language = {en},
	author = {Satija, Sambhav and Mehra, Apurv and Singanamalla, Sudheesh and Grover, Karan and Sivathanu, Muthian and Chandran, Nishanth and Gupta, Divya and Lokam, Satya},
	pages = {17},
}

@article{hangal_tracking_nodate-1,
	title = {Tracking {Down} {Software} {Bugs} {Using} {Automatic} {Anomaly} {Detection}},
	abstract = {This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program’s results.},
	language = {en},
	author = {Hangal, Sudheendra and Lam, Monica S},
	pages = {11},
}

@misc{noauthor_osmotic_nodate,
	title = {Osmotic {Flow}: {Osmotic} {Computing} + {IoT} {Workflow} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/7912282},
	urldate = {2021-10-03},
}

@article{pflanzner_taxonomy_2018,
	title = {A {Taxonomy} and {Survey} of {IoT} {Cloud} {Applications}},
	volume = {3},
	doi = {10.4108/eai.6-4-2018.154391},
	abstract = {Internet of Things (IoT) systems are realized by dynamic global network infrastructure with self-configuring capabilities, in which things can interact and communicate in the environment through the Internet by exchanging sensor data, and react autonomously to events generally without direct human intervention. Such systems can be utilized in many application areas, thus they may have very dierent properties. There is a growing number of cloud providers oering IoT-specific services, since cloud computing has the potential to satisfy IoT needs such as standardizing the custom data structures of the devices, processing and visualization tasks. IoT application developers do not only have to decide which cloud provider to use, but they also have to choose which combination of protocols and data structures best fits their application. As a result, it is necessary to know what properties these systems have and to learn to what extent cloud providers support IoT capabilities. In this paper, we address these issues and investigate 23 IoT cloud use cases and perform a detailed classification of them in a survey, and introduce a taxonomy of IoT application properties based on this survey. We also compare current cloud providers supporting IoT capabilities and gather requirements for IoT device simulation to support further research on IoT application development.},
	journal = {EAI Endorsed Transactions on Internet of Things},
	author = {Pflanzner, Tamas and Kertész, Attila},
	month = apr,
	year = {2018},
	pages = {154391},
}

@article{hamdan_edge-computing_2020,
	title = {Edge-{Computing} {Architectures} for {Internet} of {Things} {Applications}: {A} {Survey}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Edge-{Computing} {Architectures} for {Internet} of {Things} {Applications}},
	url = {https://www.mdpi.com/1424-8220/20/22/6441},
	doi = {10.3390/s20226441},
	abstract = {The rapid growth of the Internet of Things (IoT) applications and their interference with our daily life tasks have led to a large number of IoT devices and enormous sizes of IoT-generated data. The resources of IoT devices are limited; therefore, the processing and storing IoT data in these devices are inefficient. Traditional cloud-computing resources are used to partially handle some of the IoT resource-limitation issues; however, using the resources in cloud centers leads to other issues, such as latency in time-critical IoT applications. Therefore, edge-cloud-computing technology has recently evolved. This technology allows for data processing and storage at the edge of the network. This paper studies, in-depth, edge-computing architectures for IoT (ECAs-IoT), and then classifies them according to different factors such as data placement, orchestration services, security, and big data. Besides, the paper studies each architecture in depth and compares them according to various features. Additionally, ECAs-IoT is mapped according to two existing IoT layered models, which helps in identifying the capabilities, features, and gaps of every architecture. Moreover, the paper presents the most important limitations of existing ECAs-IoT and recommends solutions to them. Furthermore, this survey details the IoT applications in the edge-computing domain. Lastly, the paper recommends four different scenarios for using ECAs-IoT by IoT applications.},
	language = {en},
	number = {22},
	urldate = {2021-10-03},
	journal = {Sensors},
	author = {Hamdan, Salam and Ayyash, Moussa and Almajali, Sufyan},
	month = jan,
	year = {2020},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Internet of Things, cloud computing, edge computing},
	pages = {6441},
}

@article{shi_edge_2016,
	title = {Edge {Computing}: {Vision} and {Challenges}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Edge {Computing}},
	doi = {10.1109/JIOT.2016.2579198},
	abstract = {The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
	number = {5},
	journal = {IEEE Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	month = oct,
	year = {2016},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Bandwidth, Cloud computing, Data privacy, Edge computing, Internet of Things (IoT), Internet of things, Mobile handsets, Smart homes, Time factors, smart home and city},
	pages = {637--646},
}

@inproceedings{villari_towards_2018,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Towards {Osmotic} {Computing}: {Looking} at {Basic} {Principles} and {Technologies}},
	isbn = {978-3-319-61566-0},
	shorttitle = {Towards {Osmotic} {Computing}},
	doi = {10.1007/978-3-319-61566-0_86},
	abstract = {Osmotic Computing is becoming the new paradigm in the area of Computing. This paper shows how it can represents the glue of recent topics including Cloud, Edge and Fog Computing, and Internet of Things (IoT). Osmotic Computing introduces elements allowing to treat computation, networking, storage, data transfer and management among Cloud and IoT devices in Edge computing layers in a more harmonized fashion. In particular, we discuss how it can enable an abstraction of services that could bring into a new Software Defined of Everything era.},
	language = {en},
	booktitle = {Complex, {Intelligent}, and {Software} {Intensive} {Systems}},
	publisher = {Springer International Publishing},
	author = {Villari, Massimo and Celesti, Antonio and Fazio, Maria},
	editor = {Barolli, Leonard and Terzo, Olivier},
	year = {2018},
	keywords = {Edge Computing, Future Internet Services, Interrupt Latency, Microservices (MS), Software Defined},
	pages = {906--915},
}

@article{villari_osmotic_2016,
	title = {Osmotic {Computing}: {A} {New} {Paradigm} for {Edge}/{Cloud} {Integration}},
	volume = {3},
	issn = {2325-6095},
	shorttitle = {Osmotic {Computing}},
	doi = {10.1109/MCC.2016.124},
	abstract = {Osmotic computing is a new paradigm to support the efficient execution of Internet of Things (IoT) services and applications at the network edge. This paradigm is founded on the need for a holistic distributed system abstraction enabling the deployment of lightweight microservices on resource-constrained IoT platforms at the network edge, coupled with more complex microservices running on large-scale datacenters. This paradigm is driven by the significant increase in resource capacity/capability at the network edge, along with support for data transfer protocols that enable such resources to interact more seamlessly with datacenter-based services. This installment of "Blue Skies" discusses osmotic computing features, challenges, and future directions.},
	number = {6},
	journal = {IEEE Cloud Computing},
	author = {Villari, Massimo and Fazio, Maria and Dustdar, Schahram and Rana, Omer and Ranjan, Rajiv},
	month = nov,
	year = {2016},
	note = {Conference Name: IEEE Cloud Computing},
	keywords = {Adaptation models, Cloud computing, Computational modeling, Containers, Edge computing, Internet of Things, Quality of service, cloud computing, edge cloud integration, edge computing},
	pages = {76--83},
}

@inproceedings{buzachis_towards_2018,
	title = {Towards {Osmotic} {Computing}: {Analyzing} {Overlay} {Network} {Solutions} to {Optimize} the {Deployment} of {Container}-{Based} {Microservices} in {Fog}, {Edge} and {IoT} {Environments}},
	shorttitle = {Towards {Osmotic} {Computing}},
	doi = {10.1109/CFEC.2018.8358729},
	abstract = {In recent years, the rapid growth of new Cloud technologies acted as an enabling factor for the adoption of microservices based architecture that leverages container virtualization in order to build modular and robust systems. As the number of containers running on hosts increases, it becomes essential to have tools to manage them in a simple, straightforward manner and with a high level of abstraction. Osmotic Computing is an emerging research field that studies the migration, deployment and optimization of microservices from the Cloud to Fog, Edge, and Internet of Things (IoT) environments. However, in order to achieve Osmotic Computing environments, connectivity issues have to be addressed. This paper investigates these connectivity issues leveraging different network overlays. In particular, we analyze the performance of four network overlays that are OVN, Calico, Weave, and Flannel. Our results give a concrete overview in terms of overhead and performances for each proposed overlay solution, helping us to understand which the best overlay solution is. Specifically, we deployed CoAP and FTP microservices which helped us to carry out these benchmarks and collect the results in terms of transfer times.},
	booktitle = {2018 {IEEE} 2nd {International} {Conference} on {Fog} and {Edge} {Computing} ({ICFEC})},
	author = {Buzachis, Alina and Galletta, Antonino and Carnevale, Lorenzo and Celesti, Antonio and Fazio, Maria and Villari, Massimo},
	month = may,
	year = {2018},
	keywords = {Cloud computing, Computer architecture, Containers, Ecosystems, Edge computing, Internet of Things},
	pages = {1--10},
}

@article{fernandez_enabling_2019,
	title = {Enabling the {Orchestration} of {IoT} {Slices} through {Edge} and {Cloud} {Microservice} {Platforms}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/19/13/2980},
	doi = {10.3390/s19132980},
	abstract = {This article addresses one of the main challenges related to the practical deployment of Internet of Things (IoT) solutions: the coordinated operation of entities at different infrastructures to support the automated orchestration of end-to-end Internet of Things services. This idea is referred to as \&ldquo;Internet of Things slicing\&rdquo; and is based on the network slicing concept already defined for the Fifth Generation (5G) of mobile networks. In this context, we present the architectural design of a slice orchestrator addressing the aforementioned challenge, based on well-known standard technologies and protocols. The proposed solution is able to integrate existing technologies, like cloud computing, with other more recent technologies like edge computing and network slicing. In addition, a functional prototype of the proposed orchestrator has been implemented, using open-source software and microservice platforms. As a first step to prove the practical feasibility of our solution, the implementation of the orchestrator considers cloud and edge domains. The validation results obtained from the prototype prove the feasibility of the solution from a functional perspective, verifying its capacity to deploy Internet of Things related functions even on resource constrained platforms. This approach enables new application models where these Internet of Things related functions can be onboarded on small unmanned aerial vehicles, offering a flexible and cost-effective solution to deploy these functions at the network edge. In addition, this proposal can also be used on commercial cloud platforms, like the Google Compute Engine, showing that it can take advantage of the benefits of edge and cloud computing respectively.},
	language = {en},
	number = {13},
	urldate = {2021-10-03},
	journal = {Sensors},
	author = {Fernandez, Juan-Manuel and Vidal, Ivan and Valera, Francisco},
	month = jan,
	year = {2019},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Internet of Things (IoT), IoT slice, Small Unmanned Aerial Vehicle (SUAV), edge computing, microservice, network slicing, orchestration},
	pages = {2980},
}

@inproceedings{kaur_osmotic_2020,
	title = {Osmotic {Computing} and {Related} {Challenges}: {A} {Survey}},
	shorttitle = {Osmotic {Computing} and {Related} {Challenges}},
	doi = {10.1109/PDGC50313.2020.9315757},
	abstract = {Internet of Things (IoT) is associated with a worldwide network of interconnecting devices which are further connected to the internet, thus considerably increasing the number, range and type of devices. These devices provide anywhere and anytime connection to anyone. The IoT devices produce a large volume of data which necessitates data management. Osmotic Computing (OC), a new paradigm is driven by an increase in the resource capability or capacity at the network edge. The process of OC represents how to migrate services across the data centre to the network edge. Thus, it implies the dynamic management of macroservices and microservices across edge and cloud data centres. The goal of this work is to identify key findings of OC.},
	booktitle = {2020 {Sixth} {International} {Conference} on {Parallel}, {Distributed} and {Grid} {Computing} ({PDGC})},
	author = {Kaur, Akashdeep and Kumar, Rajesh and Saxena, Sharad},
	month = nov,
	year = {2020},
	note = {ISSN: 2573-3079},
	keywords = {Cloud computing, Computer architecture, Data centers, Edge computing, Internet of Things, IoT, MicroELements, Microservices, Osmosis, Osmotic Computing, Service Migration, Solvents},
	pages = {378--383},
}

@inproceedings{kaur_osmotic_2020-1,
	title = {Osmotic {Computing} and {Related} {Challenges}: {A} {Survey}},
	shorttitle = {Osmotic {Computing} and {Related} {Challenges}},
	doi = {10.1109/PDGC50313.2020.9315757},
	abstract = {Internet of Things (IoT) is associated with a worldwide network of interconnecting devices which are further connected to the internet, thus considerably increasing the number, range and type of devices. These devices provide anywhere and anytime connection to anyone. The IoT devices produce a large volume of data which necessitates data management. Osmotic Computing (OC), a new paradigm is driven by an increase in the resource capability or capacity at the network edge. The process of OC represents how to migrate services across the data centre to the network edge. Thus, it implies the dynamic management of macroservices and microservices across edge and cloud data centres. The goal of this work is to identify key findings of OC.},
	booktitle = {2020 {Sixth} {International} {Conference} on {Parallel}, {Distributed} and {Grid} {Computing} ({PDGC})},
	author = {Kaur, Akashdeep and Kumar, Rajesh and Saxena, Sharad},
	month = nov,
	year = {2020},
	note = {ISSN: 2573-3079},
	keywords = {Cloud computing, Computer architecture, Data centers, Edge computing, Internet of Things, IoT, MicroELements, Microservices, Osmosis, Osmotic Computing, Service Migration, Solvents},
	pages = {378--383},
}

@inproceedings{buzachis_towards_2019,
	title = {Towards {Osmotic} {Computing}: a {Blue}-{Green} {Strategy} for the {Fast} {Re}-{Deployment} of {Microservices}},
	shorttitle = {Towards {Osmotic} {Computing}},
	doi = {10.1109/ISCC47284.2019.8969621},
	abstract = {The rapid development of Cloud, Edge, Fog Computing and Internet of Things (IoT) technologies has played a key role in the Industry 4.0 evolution. In this context, the Osmotic Computing paradigm, theorized in 2016 as integration between a centralized Cloud layer and Edge and/or IoT layers, has further emphasized the Industry 4.0 objectives including productivity and Quality of Services (QoS). This emerging paradigm proposes a new elastic management model of microservices, where deployment and migration strategies are strongly related to the underlaying infrastructure requirements (i.e., load balancing, reliability, availability, and so on) and applications (i.e., anomalies detection, awareness of the context, proximity, QoS, and so on). Specifically, knowing that an Osmotic application must have a failover behavior (highly horizontally/vertically scalable, 24 hours 24 available, fault-tolerant and secure), this paper highlights the Osmotic ecosystem platform focusing on the implementation of a blue-green mechanism for the fast re-deployment of microservices, exploiting emerging technologies, such as Docker, Kubernetes, Agento and MongoDB. Experiments shows the time required to arrange, deploy and destroy microservices.},
	booktitle = {2019 {IEEE} {Symposium} on {Computers} and {Communications} ({ISCC})},
	author = {Buzachis, Alina and Galletta, Antonino and Celesti, Antonio and Carnevale, Lorenzo and Villari, Massimo},
	month = jun,
	year = {2019},
	note = {ISSN: 2642-7389},
	keywords = {Cloud Computing, Cloud computing, Computer architecture, Containers, Edge Computing, Internet of Things, IoT, Measurement, Microservice., Monitoring, Orchestration, Osmotic Computing, Quality of service},
	pages = {1--6},
}

@inproceedings{carnevale_cloud_2018,
	title = {From the {Cloud} to {Edge} and {IoT}: a {Smart} {Orchestration} {Architecture} for {Enabling} {Osmotic} {Computing}},
	shorttitle = {From the {Cloud} to {Edge} and {IoT}},
	doi = {10.1109/WAINA.2018.00122},
	abstract = {The latest technological and conceptual developments have destroyed the centralized Cloud Computing model, moving Cloud services in emerging ICT infrastructures such as Edge, Fog and Internet of Things (IoT) that are closer to end users. Specifically, current Cloud computing programming models and resource orchestration techniques are challenged by the recent evolution of the IoT phenomenon because smart devices are becoming more and more pervasive, powerful and inexpensive. Therefore, services need to be place near such devices. In this regard, the Osmotic Computing aims to provide a new computing paradigm based on the deployment and migration strategies related to the infrastructures and applications requirements across Cloud, Edge, Fog an IoT layers. In this scientific paper, we investigate the Smart Orchestration of a new software abstraction called MicroELement (MEL), that encapsulates resources, services and data necessary to run IoT applications. Several use cases are presented for describing the Artificial Intelligence processes that enables the MELs deployment.},
	booktitle = {2018 32nd {International} {Conference} on {Advanced} {Information} {Networking} and {Applications} {Workshops} ({WAINA})},
	author = {Carnevale, Lorenzo and Celesti, Antonio and Galletta, Antonino and Dustdar, Schahram and Villari, Massimo},
	month = may,
	year = {2018},
	keywords = {Artificial Intelligence, Cloud Computing, Cloud computing, Computational modeling, Computer architecture, Containers, Edge Computing, Elasticity, Internet of Things, IoT, Orchestration, Osmotic Computing, Quality of service},
	pages = {419--424},
}

@inproceedings{zhang_10_2020,
	title = {The 10 {Research} {Topics} in the {Internet} of {Things}},
	doi = {10.1109/CIC50333.2020.00015},
	abstract = {Since the term first coined in 1999 by Kevin Ashton, the Internet of Things (IoT) has gained significant momentum as a technology to connect physical objects to the Internet and to facilitate machine-to-human and machine-to-machine communications. Over the past two decades, IoT has been an active area of research and development endeavors by many technical and commercial communities. Yet, IoT technology is still not mature and many issues need to be addressed. In this paper, we identify 10 key research topics and discuss the research problems and opportunities within these topics.},
	booktitle = {2020 {IEEE} 6th {International} {Conference} on {Collaboration} and {Internet} {Computing} ({CIC})},
	author = {Zhang, Wei Emma and Sheng, Quan Z. and Mahmood, Adnan and Tran, Dai Hoang and Zaib, Munazza and Hamad, Salma Abdalla and Aljubairy, Abdulwahab and Alhazmi, Ahoud Abdulrahmn F. and Sagar, Subhash and Ma, Congbo},
	month = dec,
	year = {2020},
	keywords = {Conversational IoT, Energy Harvesting, Energy harvesting, Indexes, Internet of Things, IoT Service Discovery, Linked data, Recommendation, Search, Search problems, Semantics, Sensors, Summarization},
	pages = {34--43},
}

@article{schallehn_beyond_nodate,
	title = {Beyond {Proofs} of {Concept}: {Scaling} the {Industrial} {IoT}},
	language = {en},
	author = {Schallehn, Michael and Schorling, Christopher and Bowen, Peter and Straehle, Oliver},
	pages = {12},
}

@article{chandran_turning_nodate,
	title = {Turning {Contradictions} into {Innovations} or: {How} {We} {Learned} to {Stop} {Whining} and {Improve} {Security} {Operations}},
	abstract = {Eﬀorts to improve the eﬃciency of security operation centers (SOCs) have emphasized building tools for analysts or understanding the human and organizational factors involved. The importance of viewing the viability of a solution from multiple perspectives has been largely ignored. Multiple perspectives arise because of inherent conﬂicts among the objectives a SOC has to meet and diﬀerences between the goals of the parties involved. During the 3.5 years that we have used anthropological ﬁeldwork methods to study SOCs, we discovered that successful SOC innovations must resolve these conﬂicts to be eﬀective in improving operational eﬃciency. This discovery was guided by Activity Theory (AT), which provided a framework for analyzing our ﬁeldwork data. We use the version of AT proposed by Engestr¨om to model SOC operations. Template analysis, a qualitative data analysis technique, guided by AT validated the existence of contradictions in SOCs. The same technique was used to elicit from the data concrete contradictions and how they were resolved. Our analysis provide evidence of the importance of conﬂict resolution as a prerequisite for operations improvement. AT enabled us to understand why some of our innovations worked in the SOCs we studied (and why others failed). AT helps us see a potentially successful and repeatable mechanism for introducing new technologies to future SOCs. Understanding and supporting all of the spoken and unspoken requirements of SOC analysts and managers appears to be the only way to get new technologies accepted and used in SOCs.},
	language = {en},
	author = {Chandran, Sathya and McHugh, John and Ou, Xinming},
	pages = {16},
}

@book{storey_safety_1996,
	address = {USA},
	title = {Safety critical computer systems},
	isbn = {0-201-42787-7},
	abstract = {From the Publisher:Increasingly, microcomputers are being used in applications where their correct operation is vital to ensure the safety of the public and the environment: from anti-lock braking systems in automobiles, to fly-by-wire aircraft, to shut-down systems at nuclear power plants. It is, therefore, vital that engineers are aware of the safety implications of the systems they develop. This book is an introduction to the field of safety-critical computer systems, and is written for any engineer who uses microcomputers within real-time embedded systems. It assumes no prior knowledge of safety, or of any specific computer hardware or programming language. This book covers all phases of the life of a safety-critical system from its conception and specification, through to its certification, installation, service and decommissioning; provides information on how to assess the safety implications of projects, and determine the measures necessary to develop systems to meet safety needs; gives a thorough grounding in the techniques available to investigate the safety aspects of computer-based systems and the methods that may be used to enhance their dependability; and uses case studies and worked examples from a wide range of industrial sectors including the nuclear, aircraft, automotive and consumer products industries. This text is intended for both engineering and computer science students, and for practising engineers within computer-related industries. The approach taken is equally suited to engineers who consider computers from a hardware, software or systems viewpoint.},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Storey, Neil R.},
	year = {1996},
	note = {storey1996safety},
}

@inproceedings{yagemann_feasibility_2020,
	address = {Austin USA},
	title = {On the {Feasibility} of {Automating} {Stock} {Market} {Manipulation}},
	isbn = {978-1-4503-8858-0},
	url = {https://dl.acm.org/doi/10.1145/3427228.3427241},
	doi = {10.1145/3427228.3427241},
	abstract = {This work presents the first findings on the feasibility of using botnets to automate stock market manipulation. Our analysis incorporates data gathered from SEC case files, security surveys of online brokerages, and dark web marketplace data. We address several technical challenges, including how to adapt existing techniques for automation, the cost of hijacking brokerage accounts, avoiding detection, and more. We consolidate our findings into a working proof-of-concept, man-in-the-browser malware, Bot2Stock, capable of controlling victim email and brokerage accounts to commit fraud. We evaluate our bots and protocol using agent-based market simulations, where we find that a 1.5\% ratio of bots to benign traders yields a 2.8\% return on investment (ROI) per attack. Given the short duration of each attack ({\textless} 1 minute), achieving this ratio is trivial, requiring only 4 bots to target stocks like IBM. 1,000 bots, cumulatively gathered over 1 year, can turn \$100,000 into \$1,022,000, placing Bot2Stock on par with existing botnet scams.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {Annual {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Yagemann, Carter and Chung, Simon P. and Uzun, Erkam and Ragam, Sai and Saltaformaggio, Brendan and Lee, Wenke},
	month = dec,
	year = {2020},
	pages = {277--290},
}

@inproceedings{karami_awakening_2021,
	address = {Virtual},
	title = {Awakening the {Web}'s {Sleeper} {Agents}: {Misusing} {Service} {Workers} for {Privacy} {Leakage}},
	isbn = {978-1-891562-66-2},
	shorttitle = {Awakening the {Web}'s {Sleeper} {Agents}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/ndss2021_1C-2_23104_paper.pdf},
	doi = {10.14722/ndss.2021.23104},
	abstract = {Service workers are a powerful technology supported by all major modern browsers that can improve users’ browsing experience by offering capabilities similar to those of native applications. While they are gaining signiﬁcant traction in the developer community, they have not received much scrutiny from security researchers. In this paper, we explore the capabilities and inner workings of service workers and conduct the ﬁrst comprehensive large-scale study of their API use in the wild. Subsequently, we show how attackers can exploit the strategic placement of service workers for history-snifﬁng in most major browsers, including Chrome and Firefox. We demonstrate two novel history-snifﬁng attacks that exploit the lack of appropriate isolation in these browsers, including a nondestructive cache-based version. Next, we present a series of use cases that illustrate how our techniques enable privacy-invasive attacks that can infer sensitive application-level information, such as a user’s social graph. We have disclosed our techniques to all vulnerable vendors, prompting the Chromium team to explore a redesign of their site isolation mechanisms for defending against our attacks. We also propose a countermeasure that can be incorporated by websites to protect their users, and develop a tool that streamlines its deployment, thus facilitating adoption at a large scale. Overall, our work presents a cautionary tale on the severe risks of browsers deploying new features without an in-depth evaluation of their security and privacy implications.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {Proceedings 2021 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Karami, Soroush and Ilia, Panagiotis and Polakis, Jason},
	year = {2021},
}

@inproceedings{lopez-morales_honeyplc_2020,
	address = {Virtual Event USA},
	title = {{HoneyPLC}: {A} {Next}-{Generation} {Honeypot} for {Industrial} {Control} {Systems}},
	isbn = {978-1-4503-7089-9},
	shorttitle = {{HoneyPLC}},
	url = {https://dl.acm.org/doi/10.1145/3372297.3423356},
	doi = {10.1145/3372297.3423356},
	abstract = {Industrial Control Systems (ICS) provide management and control capabilities for mission-critical utilities such as the nuclear, power, water, and transportation grids. Within ICS, Programmable Logic Controllers (PLCs) play a key role as they serve as a convenient bridge between the cyber and the physical worlds, e.g., controlling centrifuge machines in nuclear power plants. The critical roles that ICS and PLCs play have made them the target of sophisticated cyberattacks that are designed to disrupt their operation, which creates both social unrest and financial losses. In this context, honeypots have been shown to be highly valuable tools for collecting real data, e.g., malware payload, to better understand the many different methods and strategies that attackers use.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {López-Morales, Efrén and Rubio-Medrano, Carlos and Doupé, Adam and Shoshitaishvili, Yan and Wang, Ruoyu and Bao, Tiffany and Ahn, Gail-Joon},
	month = oct,
	year = {2020},
	pages = {279--291},
}

@inproceedings{giese_amazon_2021,
	address = {Abu Dhabi United Arab Emirates},
	title = {Amazon echo dot or the reverberating secrets of {IoT} devices},
	isbn = {978-1-4503-8349-3},
	url = {https://dl.acm.org/doi/10.1145/3448300.3467820},
	doi = {10.1145/3448300.3467820},
	abstract = {Smart speakers, such as the Amazon Echo Dot, are very popular and routinely trusted with private and sensitive information. Yet, little is known about their security and potential attack vectors. We develop and synthesize a set of IoT forensics techniques, apply them to reverse engineer the hardware and software of the Amazon Echo Dot, and demonstrate its lacking protections of private user data. An adversary with physical access to such devices (e.g., purchasing a used one) can retrieve sensitive information such as Wi-Fi credentials, the physical location of (previous) owners, and cyber-physical devices (e.g., cameras, door locks). We show that such information, including all previous passwords and tokens, remains on the flash memory, even after a factory reset. This is due to the wear-leveling algorithms of the flash memory and lack of encryption. We identify and discuss the design flaws in the storage of sensitive information and the process of de-provisioning used devices. We demonstrate the practical feasibility of such attacks on 86 used devices purchased on eBay and flea markets. Finally, we propose secure design alternatives and mitigation techniques.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {Proceedings of the 14th {ACM} {Conference} on {Security} and {Privacy} in {Wireless} and {Mobile} {Networks}},
	publisher = {ACM},
	author = {Giese, Dennis and Noubir, Guevara},
	month = jun,
	year = {2021},
	pages = {13--24},
}

@techreport{noauthor_why_2020,
	title = {Why {IoT} {Projects} {Fail}},
	institution = {Beecham Research},
	year = {2020},
}

@inproceedings{pan_homecloud_2016,
	title = {{HomeCloud}: {An} edge cloud framework and testbed for new application delivery},
	shorttitle = {{HomeCloud}},
	doi = {10.1109/ICT.2016.7500391},
	abstract = {Conventional centralized cloud computing is a success for benefits such as on-demand, elasticity, and high colocation of data and computation. However, the paradigm shift towards “Internet of things” (IoT) will pose some unavoidable challenges: (1) massive data volume impossible for centralized datacenters to handle; (2) high latency between edge “things” and centralized datacenters; (3) monopoly, inhibition of innovations, and non-portable applications due to the proprietary application delivery in centralized cloud. The emergence of edge cloud gives hope to address these challenges. In this paper, we propose a new framework called “HomeCloud” focusing on an open and efficient new application delivery in edge cloud integrating two complementary technologies: Network Function Virtualization (NFV) and Software-Defined Networking (SDN). We also present a preliminary proof-of-concept testbed demonstrating the whole process of delivering a simple multi-party chatting application in the edge cloud. In the future, the HomeCloud framework can be further extended to support other use cases that demand portability, cost-efficiency, scalability, flexibility, and manageability. To the best of our knowledge, this framework is the first effort aiming at facilitating new application delivery in such a new edge cloud context.},
	booktitle = {2016 23rd {International} {Conference} on {Telecommunications} ({ICT})},
	author = {Pan, Jianli and Ma, Lin and Ravindran, Ravishankar and TalebiFard, Peyman},
	month = may,
	year = {2016},
	keywords = {Cloud Computing, Cloud computing, Edge Cloud, HomeCloud, Internet of Things, Internet of things, Monitoring, Monopoly, Network Function Virtualization (NFV), Servers, Software-Defined Networking (SDN)},
	pages = {1--6},
}

@article{nagappan_realizing_2008,
	title = {Realizing quality improvement through test driven development: results and experiences of four industrial teams},
	volume = {13},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Realizing quality improvement through test driven development},
	url = {http://link.springer.com/10.1007/s10664-008-9062-z},
	doi = {10.1007/s10664-008-9062-z},
	abstract = {Test-driven development (TDD) is a software development practice that has been used sporadically for decades. With this practice, a software engineer cycles minute-by-minute between writing failing unit tests and writing implementation code to pass those tests. Testdriven development has recently re-emerged as a critical enabling practice of agile software development methodologies. However, little empirical evidence supports or refutes the utility of this practice in an industrial context. Case studies were conducted with three development teams at Microsoft and one at IBM that have adopted TDD. The results of the case studies indicate that the pre-release defect density of the four products decreased between 40\% and 90\% relative to similar projects that did not use the TDD practice. Subjectively, the teams experienced a 15–35\% increase in initial development time after adopting TDD.},
	language = {en},
	number = {3},
	urldate = {2021-09-29},
	journal = {Empirical Software Engineering},
	author = {Nagappan, Nachiappan and Maximilien, E. Michael and Bhat, Thirumalesh and Williams, Laurie},
	month = jun,
	year = {2008},
	pages = {289--302},
}

@article{pan_future_2018,
	title = {Future {Edge} {Cloud} and {Edge} {Computing} for {Internet} of {Things} {Applications}},
	volume = {5},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2017.2767608},
	abstract = {The Internet is evolving rapidly toward the future Internet of Things (IoT) which will potentially connect billions or even trillions of edge devices which could generate huge amount of data at a very high speed and some of the applications may require very low latency. The traditional cloud infrastructure will run into a series of difficulties due to centralized computation, storage, and networking in a small number of datacenters, and due to the relative long distance between the edge devices and the remote datacenters. To tackle this challenge, edge cloud and edge computing seem to be a promising possibility which provides resources closer to the resource-poor edge IoT devices and potentially can nurture a new IoT innovation ecosystem. Such prospect is enabled by a series of emerging technologies, including network function virtualization and software defined networking. In this survey paper, we investigate the key rationale, the state-of-the-art efforts, the key enabling technologies and research topics, and typical IoT applications benefiting from edge cloud. We aim to draw an overall picture of both ongoing research efforts and future possible research directions through comprehensive discussions.},
	number = {1},
	journal = {IEEE Internet of Things Journal},
	author = {Pan, Jianli and McElhannon, James},
	month = feb,
	year = {2018},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Cloud computing, Computational modeling, Edge cloud, Edge computing, HomeCloud, Internet of Things, Internet of Things (IoT), Network function virtualization, Software defined networking, edge computing, network function virtualization (NFV), software defined networking (SDN), survey},
	pages = {439--449},
}

@techreport{griffor_framework_2017,
	address = {Gaithersburg, MD},
	title = {Framework for cyber-physical systems: volume 1, overview},
	shorttitle = {Framework for cyber-physical systems},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-201.pdf},
	language = {en},
	number = {NIST SP 1500-201},
	urldate = {2021-09-29},
	institution = {National Institute of Standards and Technology},
	author = {Griffor, Edward R and Greer, Chris and Wollman, David A and Burns, Martin J},
	month = jun,
	year = {2017},
	doi = {10.6028/NIST.SP.1500-201},
	pages = {NIST SP 1500--201},
}

@article{leveson_software_1986,
	title = {Software safety: why, what, and how},
	volume = {18},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Software safety},
	url = {https://dl.acm.org/doi/10.1145/7474.7528},
	doi = {10.1145/7474.7528},
	abstract = {Software safety issues become important when computers are used to control real-time, safety-critical processes. This survey attempts to explain why there is a problem, what the problem is, and what is known about how to solve it. Since this is a relatively new software research area, emphasis is placed on delineating the outstanding issues and research topics.},
	language = {en},
	number = {2},
	urldate = {2021-09-29},
	journal = {ACM Computing Surveys},
	author = {Leveson, Nancy G.},
	month = jun,
	year = {1986},
	pages = {125--163},
}

@article{neumann_computer-related_1986,
	title = {Some {Computer}-{Related} {Disasters} and {Other} {Egregious} {Horrors}},
	volume = {1},
	issn = {0885-8985},
	url = {http://ieeexplore.ieee.org/document/5004967/},
	doi = {10.1109/MAES.1986.5004967},
	language = {en},
	number = {10},
	urldate = {2021-09-29},
	journal = {IEEE Aerospace and Electronic Systems Magazine},
	author = {Neumann, Peter G.},
	month = oct,
	year = {1986},
	pages = {18--19},
}

@incollection{grilo_computer_2018,
	address = {Cham},
	series = {Materials {Forming}, {Machining} and {Tribology}},
	title = {Computer {Vision} in {Industrial} {Automation} and {Mobile} {Robots}},
	isbn = {978-3-319-78488-5},
	url = {https://doi.org/10.1007/978-3-319-78488-5_8},
	abstract = {Computer vision is presently a very relevant and important tool in both industrial manufacturing and mobile robots. As human vision is the most relevant sense to feed the brain with environmental information for decision making, computer vision is nowadays becoming the main artificial sensor in the domains of industrial quality assurance and trajectory control of mobile robots.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Introduction to {Mechanical} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Grilo, Frederico and Figueiredo, Joao},
	editor = {Davim, J. Paulo},
	year = {2018},
	doi = {10.1007/978-3-319-78488-5_8},
	keywords = {Computer vision, Industrial automation, Mobile robots},
	pages = {241--266},
}

@techreport{uddin_security_2021,
	title = {Security and {Machine} {Learning} {Adoption} in {IoT}: {A} {Preliminary} {Study} of {IoT} {Developer} {Discussions}},
	shorttitle = {Security and {Machine} {Learning} {Adoption} in {IoT}},
	url = {http://arxiv.org/abs/2104.00634},
	abstract = {Internet of Things (IoT) is deﬁned as the connection between places and physical objects (i.e., things) over the internet/network via smart computing devices. IoT is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As such, it is crucial to ensure IoT devices follow strict security requirements. At the same time, the prevalence of IoT devices offers developers a chance to design and develop Machine Learning (ML)-based intelligent software systems using their IoT devices. However, given the diversity of IoT devices, IoT developers may ﬁnd it challenging to introduce appropriate security and ML techniques into their devices. Traditionally, we learn about the IoT ecosystem/problems by conducting surveys of IoT developers/practitioners. Another way to learn is by analyzing IoT developer discussions in popular online developer forums like Stack Overﬂow (SO). However, we are aware of no such studies that focused on IoT developers’ security and ML-related discussions in SO. This paper offers the results of preliminary study of IoT developer discussions in SO. First, we collect around 53K IoT posts (questions + accepted answers) from SO. Second, we tokenize each post into sentences. Third, we automatically identify sentences containing security and MLrelated discussions. We ﬁnd around 12\% of sentences contain security discussions, while around 0.12\% sentences contain MLrelated discussions. There is no overlap between security and ML-related discussions, i.e., IoT developers discussing security requirements did not discuss ML requirements and vice versa. We ﬁnd that IoT developers discussing security issues frequently inquired about how the shared data can be stored, shared, and transferred securely across IoT devices and users. We also ﬁnd that IoT developers are interested to adopt deep neural network-based ML models into their IoT devices, but they ﬁnd it challenging to accommodate those into their resource-constrained IoT devices. Our ﬁndings offer implications for IoT vendors and researchers to develop and design novel techniques for improved security and ML adoption into IoT devices.},
	language = {en},
	urldate = {2021-09-29},
	author = {Uddin, Gias},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.00634},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@inproceedings{carreras_guzman_combined_2019,
	title = {Combined {Safety} and {Security} {Risk} {Analysis} using the {Ufoi}-{E} {Method}: {A} {Case} {Study} of an {Autonomous} {Surface} {Vessel}},
	isbn = {978-981-11-2724-3},
	shorttitle = {Combined {Safety} and {Security} {Risk} {Analysis} using the {Ufoi}-{E} {Method}},
	url = {http://rpsonline.com.sg/proceedings/9789811127243/html/0208.xml},
	doi = {10.3850/978-981-11-2724-3_0208-cd},
	abstract = {Many standards consider safety and security risk analysis as separate fields, specifying the system specific safety or security issues and methods to analyze them. Having these separated fields of safety and security standards complicates the risk analysis of cyber-physical systems (CPSs), where safety and security issues coexist within the integrated layers of the system. Even though several integrated safety and security analysis methods exist in the literature, they are not tailored to assess the complex and tight interactions among the CPS layers and the system’s surrounding environments. Therefore, this paper describes a method to conduct a combined safety and security risk analysis in CPSs for safety verification. Namely, we propose the Uncontrolled Flows of Information and Energy (UFoI-E) method, introducing novel diagrammatic representations to consider the dependencies within a CPS and its surrounding environments. As a case study, this paper describes a risk analysis of the collision avoidance function of an autonomous surface vessel, proving the convenience of examining the safety of autonomous vessels as safe and secure CPSs. The results of this paper may be input to new revisions and initiatives on new standards combining safety and security analysis.},
	language = {en},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 29th {European} {Safety} and {Reliability} {Conference} ({ESREL})},
	publisher = {Research Publishing Services},
	author = {Carreras Guzman, Nelson H. and Kufoalor, D. Kwame Minde and Kozine, Igor and Lundteigen, Mary Ann},
	year = {2019},
	keywords = {Exemplar, formal methods},
	pages = {4099--4106},
}

@article{sisinni_industrial_2018,
	title = {Industrial {Internet} of {Things}: {Challenges}, {Opportunities}, and {Directions}},
	volume = {14},
	issn = {1551-3203, 1941-0050},
	shorttitle = {Industrial {Internet} of {Things}},
	url = {https://ieeexplore.ieee.org/document/8401919/},
	doi = {10.1109/TII.2018.2852491},
	abstract = {Internet of Things (IoT) is an emerging domain that promises ubiquitous connection to the Internet, turning common objects into connected devices. The IoT paradigm is changing the way people interact with things around them. It paves the way for creating pervasively connected infrastructures to support innovative services and promises better ﬂexibility and efﬁciency. Such advantages are attractive not only for consumer applications, but also for the industrial domain. Over the last few years, we have been witnessing the IoT paradigm making its way into the industry marketplace with purposely designed solutions. In this paper, we clarify the concepts of IoT, Industrial IoT, and Industry 4.0. We highlight the opportunities brought in by this paradigm shift as well as the challenges for its realization. In particular, we focus on the challenges associated with the need of energy efﬁciency, real-time performance, coexistence, interoperability, and security and privacy. We also provide a systematic overview of the state-of-the-art research efforts and potential research directions to solve Industrial IoT challenges.},
	language = {en},
	number = {11},
	urldate = {2021-09-29},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Sisinni, Emiliano and Saifullah, Abusayeed and Han, Song and Jennehag, Ulf and Gidlund, Mikael},
	month = nov,
	year = {2018},
	pages = {4724--4734},
}

@article{gebremichael_security_2020,
	title = {Security and {Privacy} in the {Industrial} {Internet} of {Things}: {Current} {Standards} and {Future} {Challenges}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Security and {Privacy} in the {Industrial} {Internet} of {Things}},
	doi = {10.1109/ACCESS.2020.3016937},
	abstract = {The Internet of Things (IoT) is rapidly becoming an integral component of the industrial market in areas such as automation and analytics, giving rise to what is termed as the Industrial IoT (IIoT). The IIoT promises innovative business models in various industrial domains by providing ubiquitous connectivity, efficient data analytics tools, and better decision support systems for a better market competitiveness. However, IIoT deployments are vulnerable to a variety of security threats at various levels of the connectivity and communications infrastructure. The complex nature of the IIoT infrastructure means that availability, confidentiality and integrity are difficult to guarantee, leading to a potential distrust in the network operations and concerns of loss of critical infrastructure, compromised safety of network end-users and privacy breaches on sensitive information. This work attempts to look at the requirements currently specified for a secure IIoT ecosystem in industry standards, such as Industrial Internet Consortium (IIC) and OpenFog Consortium, and to what extent current IIoT connectivity protocols and platforms hold up to the standards with regard to security and privacy. The paper also discusses possible future research directions to enhance the security, privacy and safety of the IIoT.},
	journal = {IEEE Access},
	author = {Gebremichael, Teklay and Ledwaba, Lehlogonolo P. I. and Eldefrawy, Mohamed H. and Hancke, Gerhard P. and Pereira, Nuno and Gidlund, Mikael and Akerberg, Johan},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Cryptography, IIoT, Industrial Internet of Things, Internet of Things, Peer-to-peer computing, Privacy, Protocols, Standards, industrial networks, security and privacy},
	pages = {152351--152366},
}

@article{gupta_scalability_nodate,
	title = {Scalability in {Internet} of {Things}: {Features}, {Techniques} and {Research} {Challenges}},
	abstract = {Internet of things (IOT) is one of the most rapidly emerging fields in today’s scenario. IOT is basically a network of objects that are connected to each other through the internet and are also capable of transferring and modifying data with the help of embedded sensors. IOT is becoming the next phase of internet which has the ability to collect, analyse and spread data that can be turned into information and knowledge. With the growing idea of IOT we face a major challenge of “scalability in IOT”. Scalability is the ability of a device to adapt to the changes in the environment and meet the changing needs in the future. It is essential feature of any system which has the capability to handle the growing amount of work. It is a desirable attribute of a system or a network whose lack can cause a poor system performance and the necessity of reengineering of the whole system. In this paper we present the definition, features to be considered for scalability, techniques to achieve scalability, types of scalability and research and challenges.},
	language = {en},
	author = {Gupta, Anisha and Christie, Rivana and Manjula, R},
	pages = {12},
}

@article{antikainen_improving_2021,
	title = {Improving {Service} {Scalability} in {IoT} {Platform} {Business}},
	abstract = {The presented productized service models are one step that the case companies can take to improve their service scalability, but the models are not a solution to all scalability problems. However, similar models could be used in other companies that provide their service offerings through an IoT platform to improve their service scalability as well.},
	language = {en},
	author = {Antikainen, Eino},
	year = {2021},
	pages = {82},
}

@article{hill_what_1990,
	title = {What is scalability?},
	volume = {18},
	issn = {0163-5964},
	url = {https://doi.org/10.1145/121973.121975},
	doi = {10.1145/121973.121975},
	abstract = {Scalability is a frequently-claimed attribute of multiprocessor systems. While the basic notion is intuitive, scalability has no generally-accepted definition. For this reason, current use of the term adds more to marketing potential than technical insight.In this paper, I first examine formal definitions of scalability, but I fail to lind a useful, rigorous definition of it. I then question whether scalability is useful and conclude by challenging the technical community to either (1) rigorously define scalability or (2) stop using it to describe systems.},
	number = {4},
	urldate = {2021-09-29},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Hill, Mark D.},
	month = dec,
	year = {1990},
	keywords = {multiprocessor, parallel random access machine (PRAM), scalability and speedup},
	pages = {18--21},
}

@article{lee_tackling_2021,
	title = {Tackling {IoT} {Scalability} with {5G} {NFV}-{Enabled} {Network} {Slicing}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	url = {http://www.scirp.org/Journal/Paperabs.aspx?paperid=110420},
	doi = {10.4236/ait.2021.113009},
	abstract = {With emerging large volume and diverse heterogeneity of Internet of Things (IoT) applications, the one-size-fits-all design of the current 4G networks is no longer adequate to serve various types of IoT applications. Consequently, the concepts of network slicing enabled by Network Function Virtualization (NFV) have been proposed in the upcoming 5G networks. 5G network slicing allows IoT applications of different QoS requirements to be served by different virtual networks. Moreover, these network slices are equipped with scalability that allows them to grow or shrink their instances of Virtual Network Functions (VNFs) when needed. However, all current research only focuses on scalability on a single network slice, which is the scalability at the VNF level only. Such a design will eventually reach the capacity limit of a single slice under stressful incoming traffic, and cause the breakdown of an IoT system. Therefore, we propose a new IoT scalability architecture in this research to provide scalability at the NS level and design a testbed to implement the proposed architecture in order to verify its effectiveness. For evaluation, three systems are compared for their throughput, response time, and CPU utilization under three different types of IoT traffic, including the single slice scaling system, the multiple slices scaling system and the hybrid scaling system where both single slicing and multiple slicing can be simultaneously applied. Due to the balanced tradeoff between slice scalability and resource availability, the hybrid scaling system turns out to perform the best in terms of throughput and response time with medium CPU utilization.},
	language = {en},
	number = {3},
	urldate = {2021-09-29},
	journal = {Advances in Internet of Things},
	author = {Lee, Pei-Hsuan and Lin, Fuchun Joseph},
	month = jul,
	year = {2021},
	note = {Number: 3
Publisher: Scientific Research Publishing},
	pages = {123--139},
}

@techreport{moro_visconti_digital_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Digital {Scalability} and {Growth} {Options} of {Intangible} {Assets}},
	url = {https://papers.ssrn.com/abstract=3533865},
	abstract = {Scalability indicates the ability of a process, network, or system to handle a growing amount of work. Scalability fosters economic marginality, especially in intangible-driven businesses where variable costs are typically negligible. Massive volumes may offset low margins, producing economic gains. Digitalization is defined as the concept of “going paperless”, the technical process of transforming analog information or physical products into digital form. Digital scalability operates in a web context, where networked agents interact to generate co-created value. Economic and financial margins that represent a primary parameter for valuation are boosted by cost savings and scalable increases of expected revenues. Digitalized intangibles synergistically interact through networked platforms that reshape traditional supply chains.},
	language = {en},
	number = {ID 3533865},
	urldate = {2021-09-29},
	institution = {Social Science Research Network},
	author = {Moro Visconti, Roberto},
	month = feb,
	year = {2019},
	doi = {10.2139/ssrn.3533865},
	keywords = {Blitzscaling, Break-even Point, CAPEX, Digital Supply Chain, EBIT, EBITDA, Externalities, Fixed Costs, Geolocalization, Learning Curves, Liquidity, Networks, OPEX, On-demand Economy, Outsourcing, Real Options, Tracking, Variable Costs, eCommerce, eProcurement},
}

@article{romero-gazquez_in4wood_2021,
	title = {{IN4WOOD}: {A} {Successful} {European} {Training} {Action} of {Industry} 4.0 for {Academia} and {Business}},
	issn = {1557-9638},
	shorttitle = {{IN4WOOD}},
	doi = {10.1109/TE.2021.3111696},
	abstract = {The Industry 4.0 (I4.0) aims to develop a framework where the new technologies interoperate with each other and with employees, creating a smart and efficient environment. Although there are many public and private initiatives focused on boosting the deployment of I4.0 in all sectors worldwide, the adoption is slower than expected. One of the main reasons is the lack of training in those technologies involved in I4.0, the so-called key-enabling technologies (KET). In this article, the current status of I4.0 adoption from the industry, employees, and training point of view is analyzed. The lack of I4.0 competences in the curricula of vocational education training (VET) and higher education (HE) is also highlighted. Finally, the European innovative training action IN4WOOD is presented as a successful open and free training tool developed to offer students, employees, and managers an easy way to learn, use, and deploy KET of I4.0. Although the main target users of the training action are those in the furniture and woodworking sector, it has been designed to be useful also for users in other business sectors. The training tool is composed of more than 300 video learning pills, practical use cases, gamification, and evaluation test for measuring the level of knowledge acquired. The training tool has been tested in a pilot launched in four European countries. The results from the pilot prove that the IN4WOOD training helps to fill the skill gaps identified in the current VET/HE students and improves the competitiveness of employees, managers, and enterprises.},
	journal = {IEEE Transactions on Education},
	author = {Romero-Gázquez, Jose Luis and Cañavate-Cruzado, Gregorio and Bueno-Delgado, María-Victoria},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Education},
	keywords = {Business, Companies, Europe, Higher education (HE), IN4WOOD, Industries, Production facilities, Tools, Training, industry 4.0 (I4.0), key-enabling technologies (KET), vocational education training (VET).},
	pages = {1--10},
}

@article{verma_auto-scaling_2021,
	title = {Auto-scaling techniques for {IoT}-based cloud applications: a review},
	volume = {24},
	issn = {1573-7543},
	shorttitle = {Auto-scaling techniques for {IoT}-based cloud applications},
	url = {https://doi.org/10.1007/s10586-021-03265-9},
	doi = {10.1007/s10586-021-03265-9},
	abstract = {Cloud and IoT applications have inquiring effects that can strongly influence today’s ever-growing internet life along with necessity to resolve numerous challenges for each application such as scalability, security, privacy, and reliability. During the deployment of IoT-based Cloud applications, the demand for Cloud tenants is dynamic that makes challenging to maintain scalability of the system. Developing an effective scaling technique is not merely a big concern, but how to achieve autonomic scaling results using future load prediction and migration policies is also a crucial phase. Also, to evaluate such auto-scaling strategy, certain Quality of Service (QoS) metrics must be recognized, explored and leveraged to enhance the performance of the system. Therefore, in this paper, a survey of existing auto-scaling, load prediction and VM migration techniques for IoT-based Cloud applications has been carried out along with the evaluation of various QoS parameters. Further, the future trends have also been discussed for performing auto-scaling in a Cloud environment.},
	language = {en},
	number = {3},
	urldate = {2021-09-29},
	journal = {Cluster Computing},
	author = {Verma, Shveta and Bala, Anju},
	month = sep,
	year = {2021},
	pages = {2425--2459},
}

@article{lou_investigation_2021,
	title = {An investigation into the state-of-the-practice autonomous driving testing},
	url = {http://arxiv.org/abs/2106.12233},
	abstract = {Autonomous driving shows great potential to reform modern transportation and its safety is attracting much attention from public. Autonomous driving systems generally include deep neural networks (DNNs) for gaining better performance (e.g., accuracy on object detection and trajectory prediction). However, compared with traditional software systems, this new paradigm (i.e., program + DNNs) makes software testing more difficult. Recently, software engineering community spent significant effort in developing new testing methods for autonomous driving systems. However, it is not clear that what extent those testing methods have addressed the needs of industrial practitioners of autonomous driving. To fill this gap, in this paper, we present the first comprehensive study to identify the current practices and needs of testing autonomous driving systems in industry. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. Through thematic analysis of interview and questionnaire data, we identified five urgent needs of testing autonomous driving systems from industry. We further analyzed the limitations of existing testing methods to address those needs and proposed several future directions for software testing researchers.},
	urldate = {2021-09-29},
	journal = {arXiv:2106.12233 [cs]},
	author = {Lou, Guannan and Deng, Yao and Zheng, Xi and Zhang, Tianyi and Zhang, Mengshi},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.12233},
	keywords = {Computer Science - Software Engineering},
}

@incollection{rathee_introduction_2020,
	address = {Singapore},
	series = {Studies in {Big} {Data}},
	title = {Introduction to {Blockchain} and {IoT}},
	isbn = {9789811387753},
	url = {https://doi.org/10.1007/978-981-13-8775-3_1},
	abstract = {The blockchain is emerging rapidly as a current area of research these days. The blockchain is a technology used to run bitcoin. It is distributed database maintaining a list of record growing continuously called blocks in order to ensure the security of those blocks from revision and tampering. Every block is connected to other blocks by maintaining the hash of the previous block in the chain. This chapter discusses the technical aspects of blockchain and IoT. The IoT is merely not a concept these days. It is the necessity of time in everyday life. The “smartphone” is the most familiar application of IoT in the day-to-day life. The application of IoT is not limited to smart homes. It is ranging from industrial and commercial sectors to agriculture, public safety, and the health sector. The IoT can also be considered as “Internet of Everything (IoE)” because of a wide range of real-life applications of IoT.},
	language = {en},
	urldate = {2021-09-28},
	booktitle = {Advanced {Applications} of {Blockchain} {Technology}},
	publisher = {Springer},
	author = {Rathee, Priyanka},
	editor = {Kim, Shiho and Deka, Ganesh Chandra},
	year = {2020},
	doi = {10.1007/978-981-13-8775-3_1},
	keywords = {Bitcoin, Blockchain, IoT},
	pages = {1--14},
}

@article{uviase_iot_2018,
	title = {{IoT} {Architectural} {Framework}: {Connection} and {Integration} {Framework} for {IoT} {Systems}},
	volume = {264},
	issn = {2075-2180},
	shorttitle = {{IoT} {Architectural} {Framework}},
	url = {http://arxiv.org/abs/1803.04780},
	doi = {10.4204/EPTCS.264.1},
	abstract = {The proliferation of the Internet of Things (IoT) has since seen a growing interest in architectural design and adaptive frameworks to promote the connection between heterogeneous IoT devices and IoT systems. The most widely favoured software architecture in IoT is the Service Oriented Architecture (SOA), which aims to provide a loosely coupled systems to leverage the use and reuse of IoT services at the middle-ware layer, to minimise system integration problems. However, despite the flexibility offered by SOA, the challenges of integrating, scaling and ensuring resilience in IoT systems persist. One of the key causes of poor integration in IoT systems is the lack of an intelligent, connection-aware framework to support interaction in IoT systems. This paper reviews existing architectural frameworks for integrating IoT devices and identifies the key areas that require further research improvements. The paper concludes by proposing a possible solution based on microservice. The proposed IoT integration framework benefits from an intelligent API layer that employs an external service assembler, service auditor, service monitor and service router component to coordinate service publishing, subscription, decoupling and service combination within the architecture.},
	urldate = {2021-09-28},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Uviase, Onoriode and Kotonya, Gerald},
	month = feb,
	year = {2018},
	note = {arXiv: 1803.04780},
	keywords = {Computer Science - Computers and Society, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering, Internet of Things},
	pages = {1--17},
}

@article{badiger_violet_2018,
	title = {{VIoLET}: {A} {Large}-scale {Virtual} {Environment} for {Internet} of {Things}},
	volume = {11014},
	shorttitle = {{VIoLET}},
	url = {http://arxiv.org/abs/1806.06032},
	doi = {10.1007/978-3-319-96983-1_22},
	abstract = {IoT deployments have been growing manifold, encompassing sensors, networks, edge, fog and cloud resources. Despite the intense interest from researchers and practitioners, most do not have access to large-scale IoT testbeds for validation. Simulation environments that allow analytical modeling are a poor substitute for evaluating software platforms or application workloads in realistic computing environments. Here, we propose VIoLET, a virtual environment for deﬁning and launching large-scale IoT deployments within cloud VMs. It oﬀers a declarative model to specify container-based compute resources that match the performance of the native edge, fog and cloud devices using Docker. These can be interconnected by complex topologies on which private/public networks, and bandwidth and latency rules are enforced. Users can conﬁgure synthetic sensors for data generation on these devices as well. We validate VIoLET for deployments with {\textgreater} 400 devices and {\textgreater} 1500 device-cores, and show that the virtual IoT environment closely matches the expected compute and network performance at modest costs. This ﬁlls an important gap between IoT simulators and real deployments.},
	language = {en},
	urldate = {2021-09-28},
	journal = {arXiv:1806.06032 [cs]},
	author = {Badiger, Shreyas and Baheti, Shrey and Simmhan, Yogesh},
	year = {2018},
	note = {arXiv: 1806.06032},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {309--324},
}

@inproceedings{wang_iot_2015,
	title = {An {IoT} {Application} for {Fault} {Diagnosis} and {Prediction}},
	doi = {10.1109/DSDIS.2015.97},
	abstract = {Internet of Things (IoT) has become an important topic in both industry and academia for the recent years as it offers great potentials in numerous real world applications. This paper considers the problem of fault diagnosis and prediction from IoT data collected in the process industry. We propose a solution by making use of IoT enabling technologies offered by SAP. The proposed solution first discovers the causal relationship of the physical devices by analyzing only the device sensor data without the knowledge of the physical manufacturing system. While faults of certain devices can be detected by monitoring the healthy index of these devices in real-time, possible faults of other devices can be predicted based on the causal relationship discovered in the previous step. Such prediction capability enables new breeds of predictive maintenance applications where appropriate actions can be recommended to operators of the manufacturing system in a timely manner. The viability of the proposed solution is confirmed by a real world application of IoT conducted with an industry partner.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Data} {Science} and {Data} {Intensive} {Systems}},
	author = {Wang, Chen and Vo, Hoang Tam and Ni, Peng},
	month = dec,
	year = {2015},
	keywords = {Algorithm design and analysis, Databases, Fault diagnosis, Industries, IoT, Monitoring, Prediction algorithms, Predictive maintenance, Predictive models, Real-time systems},
	pages = {726--731},
}

@article{zhang_blockchain-based_2021,
	title = {Blockchain-{Based} {Federated} {Learning} for {Device} {Failure} {Detection} in {Industrial} {IoT}},
	volume = {8},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2020.3032544},
	abstract = {Device failure detection is one of most essential problems in Industrial Internet of Things (IIoT). However, in conventional IIoT device failure detection, client devices need to upload raw data to the central server for model training, which might lead to disclosure of sensitive business data. Therefore, in this article, to ensure client data privacy, we propose a blockchain-based federated learning approach for device failure detection in IIoT. First, we present a platform architecture of blockchain-based federated learning systems for failure detection in IIoT, which enables verifiable integrity of client data. In the architecture, each client periodically creates a Merkle tree in which each leaf node represents a client data record, and stores the tree root on a blockchain. Furthermore, to address the data heterogeneity issue in IIoT failure detection, we propose a novel centroid distance weighted federated averaging (CDW\_FedAvg) algorithm taking into account the distance between positive class and negative class of each client data set. In addition, to motivate clients to participate in federated learning, a smart contact-based incentive mechanism is designed depending on the size and the centroid distance of client data used in local model training. A prototype of the proposed architecture is implemented with our industry partner, and evaluated in terms of feasibility, accuracy, and performance. The results show that the approach is feasible, and has satisfactory accuracy and performance.},
	number = {7},
	journal = {IEEE Internet of Things Journal},
	author = {Zhang, Weishan and Lu, Qinghua and Yu, Qiuyu and Li, Zhaotong and Liu, Yue and Lo, Sin Kit and Chen, Shiping and Xu, Xiwei and Zhu, Liming},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {AI, Blockchain, Collaborative work, Computational modeling, Data models, IoT, Servers, Training, blockchain, edge computing, failure detection, federated learning, machine learning},
	pages = {5926--5937},
}

@article{banerjee_hardware-assisted_2020,
	title = {A {Hardware}-assisted {Heartbeat} {Mechanism} for {Fault} {Identification} in {Large}-scale {IoT} {Systems}},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2020.3009212},
	abstract = {With increased inter-connectivity among disparate devices, such as Internet-of-Things (IoT) devices, including those deployed in a nation's critical infrastructure, there is a need to ensure that any failure in the deployed devices can be detected. The capability to automatically detect device failures is particularly crucial in a large-scale, complex IoT system, since it can be very time-consuming and challenging to investigate a large number of geographically-dispersed devices that are also of different makes and types. In this paper, we present a faulty-device identification technique that is designed to achieve lightweight processor-level architectural support. Specifically, a hardware-based monitoring agent is incorporated within a processor and connected to a separate monitoring program when an examination is required. By analyzing information collected by the agent, the monitoring program determines whether the device being monitored is functioning. Findings from our detailed evaluation demonstrate that the proposed approach can detect around 90\% of the failures with minimal hardware overhead of approximately 5k gates. This area overhead is reasonable and amounts to 7.69\% of the ARM Cortex-M4 a lightweight IoT-class processor that has a total area (excluding optional caches and scratch-pad memory) of 65k gates.},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Banerjee, Mandrita and Borges, Carlo and Choo, Kim-Kwang Raymond and Lee, Junghee and Nicopoulos, Chrysostomos},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Biomedical monitoring, Circuit faults, Computer security, Control-flow integrity, Heart beat, Internet-of-Things (IoT), Logic gates, Monitoring, Self testing, Task analysis},
	pages = {1--1},
}

@inproceedings{lohokare_iot_2017,
	title = {An {IoT} ecosystem for the implementation of scalable wireless home automation systems at smart city level},
	doi = {10.1109/TENCON.2017.8228095},
	abstract = {Today home automation is quite common in developed nations. Remote access of appliances and devices in a home is provided via the internet. The challenge lies in making the system smart and deployable in a scalable manner across an entire city in a developing nation. This paper proposes a framework for making home automation achievable across a smart city in a scalable and data efficient manner. We have proposed an end to end home automation suite with modular implementation using open source components. The backend implemented supports multiple IoT protocols and the system is horizontally scalable. The `Rules engine' implemented analyzes the data real time thus triggering various actions pre-decided by users. The system involves energy monitoring for enabling judicial energy consumption in a home. Unlike smart-meters, which give data about total energy consumption per house, the proposed solution is an innovative way to get fairly accurate analysis of per device consumption. A single system can be deployed and can handle data coming from homes for an entire city. The implemented system includes web-dashboard and android application for monitoring the consumption. The scalable architecture used ensures that there will be no problem handling data traffic as number of users increase. Experiments on the prototype system showed low latency for real time analytics even with large simultaneous data streams. The unique features of this system are its modular \& dynamic nature, scalability and real time analytics for quick decision making.},
	booktitle = {{TENCON} 2017 - 2017 {IEEE} {Region} 10 {Conference}},
	author = {Lohokare, Jay and Dani, Reshul and Rajurkar, Ajit and Apte, Ameya},
	month = nov,
	year = {2017},
	note = {ISSN: 2159-3450},
	keywords = {Cloud computing, Home automation, Internet of Things, IoT framework, IoT protocols, Protocols, Real-time systems, Scalability, Servers, Sparks, apache kafka, cloud computing, emqttd, home automation, mqtt, real time analytics, rules engine, smart home, spark streaming, wireless networks},
	pages = {1503--1508},
}

@inproceedings{theodoridis_developing_2013,
	title = {Developing an {IoT} {Smart} {City} framework},
	doi = {10.1109/IISA.2013.6623710},
	abstract = {In this paper, we discuss key findings, technological challenges and socioeconomic opportunities in Smart City era. Most of the conclusions were gathered during SmartSantander project, an EU project that is developing a city-scale testbed for IoT and Future Internet experimentation, providing an integrated framework for implementing Smart City services.},
	booktitle = {{IISA} 2013},
	author = {Theodoridis, Evangelos and Mylonas, Georgios and Chatzigiannakis, Ioannis},
	month = jul,
	year = {2013},
	keywords = {Cities and towns, Future Internet, Internet, IoT, Monitoring, Semantics, Sensors, Servers, Smart City, Smart phones, Socioeconomic Impact},
	pages = {1--6},
}

@inproceedings{velasquez_fast-data_2018,
	title = {Fast-data architecture proposal to alert people in emergency},
	doi = {10.1109/CCWC.2018.8301721},
	abstract = {This paper states a brief overview of technologies related to Smart Cities and Big Data ecosystems in order to develop and present an architecture proposal for deploying services using the paradigm of Fast Data. The main goal of this architecture, is to present a set of tools and how it could be integrated for providing fast data services focus on Resilient Smart Cities. Finally, proposals for analysis the response times, delays, incidents, and future works are presented.},
	booktitle = {2018 {IEEE} 8th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {Velásquez, Washington and Munoz-Arcentales, Andres and Salvachúa, Joaquin},
	month = jan,
	year = {2018},
	keywords = {Architecture, Big Data, Emergency, Emergency services, Fast Data, IoT, Proposals, Real-time systems, Smart City, Smart cities, Sparks, Tools},
	pages = {165--168},
}

@inproceedings{kyriazis_sustainable_2013,
	title = {Sustainable smart city {IoT} applications: {Heat} and electricity management amp; {Eco}-conscious cruise control for public transportation},
	shorttitle = {Sustainable smart city {IoT} applications},
	doi = {10.1109/WoWMoM.2013.6583500},
	abstract = {In a world of multi-stakeholder information and assets provision on top of millions of real-time interacting and communicating things, systems based on Internet of Things (IoT) technologies aim at exploiting these assets in a resilient and sustainable way allowing them to reach their full potential. In this paper we present two innovative smart city IoT applications: the first one refers to heat and energy management, and aims at utilizing different resources (such as heat and electricity meters) in order to optimize use of energy in commercial and residential areas. The second application refers to cruise control for public transportation, and aims at utilizing different resources (such as environmental and traffic sensors) in order to provide driving recommendations that aim at eco efficiency. We also highlight the IoT challenges as well as potential enabling technologies that will allow for the realization of the proposed applications.},
	booktitle = {2013 {IEEE} 14th {International} {Symposium} on "{A} {World} of {Wireless}, {Mobile} and {Multimedia} {Networks}" ({WoWMoM})},
	author = {Kyriazis, Dimosthenis and Varvarigou, Theodora and White, Daniel and Rossi, Andrea and Cooper, Joshua},
	month = jun,
	year = {2013},
	keywords = {Cities and towns, Electricity, Internet, Real-time systems, Resistance heating, Sensors, Transportation},
	pages = {1--5},
}

@inproceedings{mezghani_autonomic_2020,
	title = {Autonomic {Coordination} of {IoT} {Device} {Management} {Platforms}},
	doi = {10.1109/WETICE49692.2020.00015},
	abstract = {With the adoption of the Internet of Things (IoT), devices become integrated in our daily lives. These devices are more and more interdependent, either by guaranteeing connectivity, or by providing data for complex services. In an ever-evolving IoT context, continuously managing these connected objects is crucial, to handle vulnerabilities, ensure their well-functioning, and add new services. These features fall under the scope of Device Management (DM). The distribution and scalability of DM platforms become all the more exacerbated as device profiles and deployment use cases get more heterogeneous. Thus, multiple actors are involved in IoT services, with their respective, isolated, DM platforms. However, DM operations, such as reboots and firmware updates, have a direct impact on the state of devices, which requires awareness across DM platforms. In this paper, we address conflicts linked to connectivity dependencies, which may arise during the simultaneous execution of DM operations on interdependent devices. Hence, we propose a coordinator that collects DM operation intentions from isolated DM platforms and detects the relevant dependencies, so as to plan and orchestrate the execution of these operations. An evaluation of this middleware is presented, with perspectives for industrial applications.},
	booktitle = {2020 {IEEE} 29th {International} {Conference} on {Enabling} {Technologies}: {Infrastructure} for {Collaborative} {Enterprises} ({WETICE})},
	author = {Mezghani, Emna and Berlemont, Samuel and Douet, Marc},
	month = sep,
	year = {2020},
	note = {ISSN: 2641-8169},
	keywords = {Autonomic Computing, Complexity theory, Coordination, Device Management, Internet of Things, Middleware, Planning, Scalability, Task analysis, Topology},
	pages = {30--35},
}

@article{douglass_agile_nodate,
	title = {Agile analysis practices for safety-critical software development},
	abstract = {Because of their discipline and efﬁciency, agile development practices should be applied to the development of safety-critical software. Bruce Douglass, author of the IBM Rational Harmony for Embedded RealTime Development process, explains the key analysis practices for the development of safety-critical systems and how they can be realized in an agile way.},
	language = {en},
	author = {Douglass, Bruce},
	pages = {13},
}

@article{sun_edgeiot_2016,
	title = {{EdgeIoT}: {Mobile} {Edge} {Computing} for the {Internet} of {Things}},
	volume = {54},
	issn = {1558-1896},
	shorttitle = {{EdgeIoT}},
	doi = {10.1109/MCOM.2016.1600492CM},
	abstract = {In order to overcome the scalability problem of the traditional Internet of Things architecture (i.e., data streams generated from distributed IoT devices are transmitted to the remote cloud via the Internet for further analysis), this article proposes a novel approach to mobile edge computing for the IoT architecture, edgeIoT, to handle the data streams at the mobile edge. Specifically, each BS is connected to a fog node, which provides computing resources locally. On the top of the fog nodes, the SDN-based cellular core is designed to facilitate packet forwarding among fog nodes. Meanwhile, we propose a hierarchical fog computing architecture in each fog node to provide flexible IoT services while maintaining user privacy: each user's IoT devices are associated with a proxy VM (located in a fog node), which collects, classifies, and analyzes the devices' raw data streams, converts them into metadata, and transmits the metadata to the corresponding application VMs (which are owned by IoT service providers). Each application VM receives the corresponding metadata from different proxy VMs and provides its service to users. In addition, a novel proxy VM migration scheme is proposed to minimize the traffic in the SDNbased core.},
	number = {12},
	journal = {IEEE Communications Magazine},
	author = {Sun, Xiang and Ansari, Nirwan},
	month = dec,
	year = {2016},
	note = {Conference Name: IEEE Communications Magazine},
	keywords = {Edge computing, Internet of Things, Mobile computing},
	pages = {22--29},
}

@article{pan_edgechain_2019,
	title = {{EdgeChain}: {An} {Edge}-{IoT} {Framework} and {Prototype} {Based} on {Blockchain} and {Smart} {Contracts}},
	volume = {6},
	issn = {2327-4662},
	shorttitle = {{EdgeChain}},
	doi = {10.1109/JIOT.2018.2878154},
	abstract = {The emerging Internet of Things (IoT) is facing significant scalability and security challenges. On one hand, IoT devices are “weak” and need external assistance. Edge computing provides a promising direction addressing the deficiency of centralized cloud computing in scaling massive number of devices. On the other hand, IoT devices are also relatively “vulnerable” facing malicious hackers due to resource constraints. The emerging blockchain and smart contracts technologies bring a series of new security features for IoT and edge computing. In this paper, to address the challenges, we design and prototype an edge-IoT framework named “EdgeChain” based on blockchain and smart contracts. The core idea is to integrate a permissioned blockchain and the internal currency or “coin” system to link the edge cloud resource pool with each IoT device' account and resource usage, and hence behavior of the IoT devices. EdgeChain uses a credit-based resource management system to control how much resource IoT devices can obtain from edge servers, based on predefined rules on priority, application types, and past behaviors. Smart contracts are used to enforce the rules and policies to regulate the IoT device behavior in a nondeniable and automated manner. All the IoT activities and transactions are recorded into blockchain for secure data logging and auditing. We implement an EdgeChain prototype and conduct extensive experiments to evaluate the ideas. The results show that while gaining the security benefits of blockchain and smart contracts, the cost of integrating them into EdgeChain is within a reasonable and acceptable range.},
	number = {3},
	journal = {IEEE Internet of Things Journal},
	author = {Pan, Jianli and Wang, Jianyu and Hester, Austin and Alqerm, Ismail and Liu, Yuanni and Zhao, Ying},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Blockchain, Cloud computing, Edge computing, EdgeChain, Internet of Things, Internet of Things (IoT), Servers, edge computing, fog computing, scalability, security, smart contracts},
	pages = {4719--4732},
}

@article{cui_blockchain_2019,
	title = {Blockchain in {IoT}: {Current} {Trends}, {Challenges}, and {Future} {Roadmap}},
	volume = {3},
	issn = {2509-3436},
	shorttitle = {Blockchain in {IoT}},
	url = {https://doi.org/10.1007/s41635-019-00079-5},
	doi = {10.1007/s41635-019-00079-5},
	abstract = {The Internet of Things (IoT) is one of the most promising technologies in the era of information technology. IoT enables ubiquitous data collections and network communications to bring significant and indispensable convenience and intelligence both to daily life and industrial operations. However, IoT is still confronting a number of challenges and manifesting a series of issues that need to be addressed urgently. Counterfeit hardware, software faults, security issues during communication, system management difficulties, and data privacy issues are significant issues for current IoT infrastructure. Meanwhile, blockchain, as an emerging information technology, has attracted huge public interest and has shown significant promise because of its decentralization, transparency, and security. The features of blockchain seem to be an ideal match for IoT, and by applying blockchain to an IoT environment, some of the aforementioned weaknesses can be addressed. This paper’s purpose is to introduce the use of blockchain in IoT applications. We present various challenges facing an IoT system and summarize the benefits of adopting blockchain into IoT infrastructure. We primarily focus on illustrating the blockchain applications in IoT with refined capabilities and enhanced security. To shed light on blockchain in IoT research, we also discuss limitations and future directions.},
	language = {en},
	number = {4},
	urldate = {2021-09-23},
	journal = {Journal of Hardware and Systems Security},
	author = {Cui, Pinchen and Guin, Ujjwal and Skjellum, Anthony and Umphress, David},
	month = dec,
	year = {2019},
	pages = {338--364},
}

@incollection{miraz_blockchain_2020,
	address = {Singapore},
	series = {Studies in {Big} {Data}},
	title = {Blockchain of {Things} ({BCoT}): {The} {Fusion} of {Blockchain} and {IoT} {Technologies}},
	isbn = {9789811387753},
	shorttitle = {Blockchain of {Things} ({BCoT})},
	url = {https://doi.org/10.1007/978-981-13-8775-3_7},
	abstract = {Blockchain, as well as Internet of Things (IoT), is considered as two major disruptive emerging technologies. However, both of them suffer from innate technological limitations to some extent. IoT requires strengthening its security features while Blockchain inherently possesses them due to its extensive use of cryptographic mechanisms and Blockchain––in an inverted manner–– needs contributions from the distributed nodes for its P2P (Peer-to-Peer) consensus model while IoT rudimentarily embodies them within its architecture. This chapter, therefore, acutely dissects the viability, along with prospective challenges, of incorporating Blockchain with IoT technologies––inducing the notion of Blockchain of Things (BCoT)––as well as the benefits such consolidation can offer.},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {Advanced {Applications} of {Blockchain} {Technology}},
	publisher = {Springer},
	author = {Miraz, Mahdi H.},
	editor = {Kim, Shiho and Deka, Ganesh Chandra},
	year = {2020},
	doi = {10.1007/978-981-13-8775-3_7},
	keywords = {Blockchain, Blockchain of Things (BCoT), Internet of things (IoT), Security, Wireless Sensor Network (WSN)},
	pages = {141--159},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917634&casa_token=qR5HPcERvs0AAAAA:_9hENFOlx7g69XskpxQ9FW5EtBxl8eEyOotnwXf1obtiSk_GxdPwiI9PA4LZhPxt1wO5vdtuVw&tag=1},
	urldate = {2021-09-23},
}

@misc{noauthor_ieee_nodate-1,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917634&casa_token=qR5HPcERvs0AAAAA:_9hENFOlx7g69XskpxQ9FW5EtBxl8eEyOotnwXf1obtiSk_GxdPwiI9PA4LZhPxt1wO5vdtuVw&tag=1},
	urldate = {2021-09-23},
}

@inproceedings{dorri_towards_2017,
	title = {Towards an {Optimized} {BlockChain} for {IoT}},
	abstract = {There has been increasing interest in adopting BlockChain (BC), that underpins the crypto-currency Bitcoin, in Internet of Things (IoT) for security and privacy. However, BCs are computationally expensive and involve high bandwidth overhead and delays, which are not suitable for most IoT devices. This paper proposes a lightweight BC-based architecture for IoT that virtually eliminates the overheads of classic BC, while maintaining most of its security and privacy benefits. IoT devices benefit from a private immutable ledger, that acts similar to BC but is managed centrally, to optimize energy consumption. High resource devices create an overlay network to implement a publicly accessible distributed BC that ensures end-to-end security and privacy. The proposed architecture uses distributed trust to reduce the block validation processing time. We explore our approach in a smart home setting as a representative case study for broader IoT applications. Qualitative evaluation of the architecture under common threat models highlights its effectiveness in providing security and privacy for IoT applications. Simulations demonstrate that our method decreases packet and processing overhead significantly compared to the BC implementation used in Bitcoin.},
	booktitle = {2017 {IEEE}/{ACM} {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation} ({IoTDI})},
	author = {Dorri, Ali and Kanhere, Salil S. and Jurdak, Raja},
	month = apr,
	year = {2017},
	keywords = {BlockChain, Computer architecture, Data privacy, Internet of Things, Online banking, Peer-to-peer computing, Privacy, Security, Smart homes},
	pages = {173--178},
}

@article{panarello_blockchain_2018,
	title = {Blockchain and {IoT} {Integration}: {A} {Systematic} {Survey}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Blockchain and {IoT} {Integration}},
	url = {https://www.mdpi.com/1424-8220/18/8/2575},
	doi = {10.3390/s18082575},
	abstract = {The Internet of Things (IoT) refers to the interconnection of smart devices to collect data and make intelligent decisions. However, a lack of intrinsic security measures makes IoT vulnerable to privacy and security threats. With its \&ldquo;security by design,\&rdquo; Blockchain (BC) can help in addressing major security requirements in IoT. BC capabilities like immutability, transparency, auditability, data encryption and operational resilience can help solve most architectural shortcomings of IoT. This article presents a comprehensive survey on BC and IoT integration. The objective of this paper is to analyze the current research trends on the usage of BC-related approaches and technologies in an IoT context. This paper presents the following novelties, with respect to related work: (i) it covers different application domains, organizing the available literature according to this categorization, (ii) it introduces two usage patterns, i.e., device manipulation and data management (open marketplace solution), and (iii) it reports on the development level of some of the presented solutions. We also analyze the main challenges faced by the research community in the smooth integration of BC and IoT, and point out the main open issues and future research directions. Last but not least, we also present a survey about novel uses of BC in the machine economy.},
	language = {en},
	number = {8},
	urldate = {2021-09-23},
	journal = {Sensors},
	author = {Panarello, Alfonso and Tapas, Nachiket and Merlino, Giovanni and Longo, Francesco and Puliafito, Antonio},
	month = aug,
	year = {2018},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Internet of Things, IoT, blockchain, machine economy, survey},
	pages = {2575},
}

@article{novo_blockchain_2018,
	title = {Blockchain {Meets} {IoT}: {An} {Architecture} for {Scalable} {Access} {Management} in {IoT}},
	volume = {5},
	issn = {2327-4662},
	shorttitle = {Blockchain {Meets} {IoT}},
	doi = {10.1109/JIOT.2018.2812239},
	abstract = {The Internet of Things (IoT) is stepping out of its infancy into full maturity and establishing itself as a part of the future Internet. One of the technical challenges of having billions of devices deployed worldwide is the ability to manage them. Although access management technologies exist in IoT, they are based on centralized models which introduce a new variety of technical limitations to manage them globally. In this paper, we propose a new architecture for arbitrating roles and permissions in IoT. The new architecture is a fully distributed access control system for IoT based on blockchain technology. The architecture is backed by a proof of concept implementation and evaluated in realistic IoT scenarios. The results show that the blockchain technology could be used as access management technology in specific scalable IoT scenarios.},
	number = {2},
	journal = {IEEE Internet of Things Journal},
	author = {Novo, Oscar},
	month = apr,
	year = {2018},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Access control, Bitcoin, Contracts, Internet of Things, Internet of Things (IoT), Peer-to-peer computing, blockchain, smart contracts},
	pages = {1184--1195},
}

@incollection{bondavalli_security_2014,
	address = {Cham},
	title = {Security {Application} of {Failure} {Mode} and {Effect} {Analysis} ({FMEA})},
	volume = {8666},
	isbn = {978-3-319-10505-5 978-3-319-10506-2},
	url = {http://link.springer.com/10.1007/978-3-319-10506-2_21},
	abstract = {Increasingly complex systems lead to an interweaving of security, safety, availability and reliability concerns. Most dependability analysis techniques do not include security aspects. In order to include security, a holistic risk model for systems is needed. In our novel approach, the basic failure cause, failure mode and failure effect model known from FMEA is used as a template for a vulnerability cause-effect chain, and an FMEA analysis technique extended with security is presented. This represents a unified model for safety and security cause-effect analysis. As an example the technique is then applied to a distributed industrial measurement system.},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {Computer {Safety}, {Reliability}, and {Security}},
	publisher = {Springer International Publishing},
	author = {Schmittner, Christoph and Gruber, Thomas and Puschner, Peter and Schoitsch, Erwin},
	editor = {Bondavalli, Andrea and Di Giandomenico, Felicita},
	year = {2014},
	doi = {10.1007/978-3-319-10506-2_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {310--325},
}

@article{noauthor_procedures_nodate,
	title = {Procedures for {Performing} a {Failure} {Mode}, {Effects} and {Criticality} {Analysis}},
	language = {en},
	pages = {80},
}

@incollection{van_der_aalst_combined_2012,
	address = {Berlin, Heidelberg},
	title = {A {Combined} {Process} for {Elicitation} and {Analysis} of {Safety} and {Security} {Requirements}},
	volume = {113},
	isbn = {978-3-642-31071-3 978-3-642-31072-0},
	url = {http://link.springer.com/10.1007/978-3-642-31072-0_24},
	abstract = {The aim of safety and security assessments are very similar since they both consider harm during system development. However, they apply different means for it and are performed in separated processes. As security and safety areas are merging in new systems that are critical, and more openly interconnected, there is a need to relate the different processes during the development. A combined assessment process could save resources compared to separated safety and security assessments, as well as support the understanding of mutual constraints and the resolution of conflicts between the two areas. We suggest a combined method covering the harm identification and analysis part of the assessment process using UML-based models. The process is applied on a case from the Air Traffic Management domain. Experts’ opinions about the results have also been collected for feedback.},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {Enterprise, {Business}-{Process} and {Information} {Systems} {Modeling}},
	publisher = {Springer Berlin Heidelberg},
	author = {Raspotnig, Christian and Karpati, Peter and Katta, Vikash},
	editor = {van der Aalst, Wil and Mylopoulos, John and Rosemann, Michael and Shaw, Michael J. and Szyperski, Clemens and Bider, Ilia and Halpin, Terry and Krogstie, John and Nurcan, Selmin and Proper, Erik and Schmidt, Rainer and Soffer, Pnina and Wrycza, Stanisław},
	year = {2012},
	doi = {10.1007/978-3-642-31072-0_24},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {347--361},
}

@inproceedings{wilson_trust_2017,
	address = {Niagara Falls New York USA},
	title = {Trust but {Verify}: {Auditing} the {Secure} {Internet} of {Things}},
	isbn = {978-1-4503-4928-4},
	shorttitle = {Trust but {Verify}},
	url = {https://dl.acm.org/doi/10.1145/3081333.3081342},
	doi = {10.1145/3081333.3081342},
	abstract = {Internet-of-Things devices often collect and transmit sensitive information like camera footage, health monitoring data, or whether someone is home. These devices protect data in transit with end-to-end encryption, typically using TLS connections between devices and associated cloud services.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the 15th {Annual} {International} {Conference} on {Mobile} {Systems}, {Applications}, and {Services}},
	publisher = {ACM},
	author = {Wilson, Judson and Wahby, Riad S. and Corrigan-Gibbs, Henry and Boneh, Dan and Levis, Philip and Winstein, Keith},
	month = jun,
	year = {2017},
	pages = {464--474},
}

@inproceedings{wang_fear_2018,
	address = {San Diego, CA},
	title = {Fear and {Logging} in the {Internet} of {Things}},
	isbn = {978-1-891562-49-5},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-2_Wang_paper.pdf},
	doi = {10.14722/ndss.2018.23282},
	abstract = {As the Internet of Things (IoT) continues to proliferate, diagnosing incorrect behavior within increasinglyautomated homes becomes considerably more difﬁcult. Devices and apps may be chained together in long sequences of triggeraction rules to the point that from an observable symptom (e.g., an unlocked door) it may be impossible to identify the distantly removed root cause (e.g., a malicious app). This is because, at present, IoT audit logs are siloed on individual devices, and hence cannot be used to reconstruct the causal relationships of complex workﬂows. In this work, we present ProvThings, a platform-centric approach to centralized auditing in the Internet of Things. ProvThings performs efﬁcient automated instrumentation of IoT apps and device APIs in order to generate data provenance that provides a holistic explanation of system activities, including malicious behaviors. We prototype ProvThings for the Samsung SmartThings platform, and benchmark the efﬁcacy of our approach against a corpus of 26 IoT attacks. Through the introduction of a selective code instrumentation optimization, we demonstrate in evaluation that ProvThings imposes just 5\% overhead on physical IoT devices while enabling real time querying of system behaviors, and further consider how ProvThings can be leveraged to meet the needs of a variety of stakeholders in the IoT ecosystem.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Wang, Qi and Hassan, Wajih Ul and Bates, Adam and Gunter, Carl},
	year = {2018},
}

@inproceedings{nguyen_iotsan_2018,
	address = {New York, NY, USA},
	series = {{CoNEXT} '18},
	title = {{IotSan}: fortifying the safety of {IoT} systems},
	isbn = {978-1-4503-6080-7},
	shorttitle = {{IotSan}},
	url = {https://doi.org/10.1145/3281411.3281440},
	doi = {10.1145/3281411.3281440},
	abstract = {Today's IoT systems include event-driven smart applications (apps) that interact with sensors and actuators. A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states. Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. In this paper, we design IotSan, a novel practical system that uses model checking as a building block to reveal "interaction-level" flaws by identifying events that can lead the system to unsafe states. In building IotSan, we design novel techniques tailored to IoT systems, to alleviate the state explosion associated with model checking. IotSan also automatically translates IoT apps into a format amenable to model checking. Finally, to understand the root cause of a detected vulnerability, we design an attribution mechanism to identify problematic and potentially malicious apps. We evaluate IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities. We also evaluate IotSan with malicious SmartThings apps from a previous effort. IotSan detects the potential safety violations and also effectively attributes these apps as malicious.},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 14th {International} {Conference} on emerging {Networking} {EXperiments} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Dang Tu and Song, Chengyu and Qian, Zhiyun and Krishnamurthy, Srikanth V. and Colbert, Edward J. M. and McDaniel, Patrick},
	month = dec,
	year = {2018},
	pages = {191--203},
}

@inproceedings{ding_safety_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {On the {Safety} of {IoT} {Device} {Physical} {Interaction} {Control}},
	isbn = {978-1-4503-5693-0},
	url = {https://doi.org/10.1145/3243734.3243865},
	doi = {10.1145/3243734.3243865},
	abstract = {Emerging Internet of Things (IoT) platforms provide increased functionality to enable human interaction with the physical world in an autonomous manner. The physical interaction features of IoT platforms allow IoT devices to make an impact on the physical environment. However, such features also bring new safety challenges, where attackers can leverage stealthy physical interactions to launch attacks against IoT systems. In this paper, we propose a framework called IoTMon that discovers any possible physical interactions and generates all potential interaction chains across applications in the IoT environment. IoTMon also includes an assessment of the safety risk of each discovered inter-app interaction chain based on its physical influence. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of IoTMon and present a comprehensive system evaluation on the Samsung SmartThings platform. We study 185 official SmartThings applications and find they can form 162 hidden inter-app interaction chains through physical surroundings. In particular, our experiment reveals that 37 interaction chains are highly risky and could be potentially exploited to impact the safety of the IoT{\textasciitilde}environment.},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Wenbo and Hu, Hongxin},
	month = oct,
	year = {2018},
	keywords = {internet of things, physical interaction control, safety},
	pages = {832--846},
}

@inproceedings{trimananda_understanding_2020,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2020},
	title = {Understanding and automatically detecting conflicting interactions between smart home {IoT} applications},
	isbn = {978-1-4503-7043-1},
	url = {https://doi.org/10.1145/3368089.3409682},
	doi = {10.1145/3368089.3409682},
	abstract = {Smart home devices provide the convenience of remotely control-ling and automating home appliances. The most advanced smart home environments allow developers to write apps to make smart home devices work together to accomplish tasks, e.g., home security and energy conservation. A smart home app typically implements narrow functionality and thus to fully implement desired functionality homeowners may need to install multiple apps. These different apps can conflict with each other and these conflicts can result in undesired actions such as locking the door during a fire. In this paper, we study conflicts between apps on Samsung SmartThings, the most popular platform for developing and deploying smart home IoT devices. By collecting and studying 198 official and 69 third-party apps, we found significant app conflicts in 3 categories: (1) close to 60\% of app pairs that access the same device, (2) more than 90\% of app pairs with physical interactions, and (3) around 11\% of app pairs that access the same global variable. Our results suggest that the problem of conflicts between smart home apps is serious and can create potential safety risks. We then developed a conflict detection tool that uses model checking to automatically detect up to 96\% of the conflicts.},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Trimananda, Rahmadi and Aqajari, Seyed Amir Hossein and Chuang, Jason and Demsky, Brian and Xu, Guoqing Harry and Lu, Shan},
	month = nov,
	year = {2020},
	keywords = {concurrency, model checking, program analysis, smart home apps},
	pages = {1215--1227},
}

@inproceedings{gu_iotgaze_2020,
	title = {{IoTGaze}: {IoT} {Security} {Enforcement} via {Wireless} {Context} {Analysis}},
	shorttitle = {{IoTGaze}},
	doi = {10.1109/INFOCOM41043.2020.9155459},
	abstract = {Internet of Things (IoT) has become the most promising technology for service automation, monitoring, and interconnection, etc. However, the security and privacy issues caused by IoT arouse concerns. Recent research focuses on addressing security issues by looking inside platform and apps. In this work, we creatively change the angle to consider security problems from a wireless context perspective. We propose a novel framework called IoTGaze, which can discover potential anomalies and vulnerabilities in the IoT system via wireless traffic analysis. By sniffing the encrypted wireless traffic, IoTGaze can automatically identify the sequential interaction of events between apps and devices. We discover the temporal event dependencies and generate the Wireless Context for the IoT system. Meanwhile, we extract the IoT Context, which reflects user's expectation, from IoT apps' descriptions and user interfaces. If the wireless context does not match the expected IoT context, IoTGaze reports an anomaly. Furthermore, IoTGaze can discover the vulnerabilities caused by the inter-app interaction via hidden channels, such as temperature and illuminance. We provide a proof-of-concept implementation and evaluation of our framework on the Samsung SmartThings platform. The evaluation shows that IoTGaze can effectively discover anomalies and vulnerabilities, thereby greatly enhancing the security of IoT systems.},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Gu, Tianbo and Fang, Zheng and Abhishek, Allaukik and Fu, Hao and Hu, Pengfei and Mohapatra, Prasant},
	month = jul,
	year = {2020},
	note = {ISSN: 2641-9874},
	keywords = {Anomaly Detection, Communication system security, Feature extraction, Internet of Things, IoT Security, Natural Language Processing, Privacy, Security, Surveillance, Wireless Context, Wireless communication, Wireless sensor networks},
	pages = {884--893},
}

@article{sulaman_comparison_2019,
	title = {Comparison of the {FMEA} and {STPA} safety analysis methods–a case study},
	volume = {27},
	issn = {1573-1367},
	url = {https://doi.org/10.1007/s11219-017-9396-0},
	doi = {10.1007/s11219-017-9396-0},
	abstract = {As our society becomes more and more dependent on IT systems, failures of these systems can harm more and more people and organizations. Diligently performing risk and hazard analysis helps to minimize the potential harm of IT system failures on the society and increases the probability of their undisturbed operation. Risk and hazard analysis is an important activity for the development and operation of critical software intensive systems, but the increased complexity and size puts additional requirements on the effectiveness of risk and hazard analysis methods. This paper presents a qualitative comparison of two hazard analysis methods, failure mode and effect analysis (FMEA) and system theoretic process analysis (STPA), using case study research methodology. Both methods have been applied on the same forward collision avoidance system to compare the effectiveness of the methods and to investigate what are the main differences between them. Furthermore, this study also evaluates the analysis process of both methods by using a qualitative criteria derived from the technology acceptance model (TAM). The results of the FMEA analysis were compared to the results of the STPA analysis, which were presented in a previous study. Both analyses were conducted on the same forward collision avoidance system. The comparison shows that FMEA and STPA deliver similar analysis results.},
	language = {en},
	number = {1},
	urldate = {2021-09-21},
	journal = {Software Quality Journal},
	author = {Sulaman, Sardar Muhammad and Beer, Armin and Felderer, Michael and Höst, Martin},
	month = mar,
	year = {2019},
	pages = {349--387},
}

@inproceedings{schmittner_case_2015,
	address = {New York, NY, USA},
	series = {{CPSS} '15},
	title = {A {Case} {Study} of {FMVEA} and {CHASSIS} as {Safety} and {Security} {Co}-{Analysis} {Method} for {Automotive} {Cyber}-physical {Systems}},
	isbn = {978-1-4503-3448-8},
	url = {http://doi.org/10.1145/2732198.2732204},
	doi = {10.1145/2732198.2732204},
	abstract = {The increasing integration of computational components and physical systems creates cyber-physical system, which provide new capabilities and possibilities for humans to control and interact with physical machines. However, the correlation of events in cyberspace and physical world also poses new safety and security challenges. This calls for holistic approaches to safety and security analysis for the identification of safety failures and security threats and a better understanding of their interplay. This paper presents the application of two promising methods, i.e. Failure Mode, Vulnerabilities and Effects Analysis (FMVEA) and Combined Harm Assessment of Safety and Security for Information Systems (CHASSIS), to a case study of safety and security co-analysis of cyber-physical systems in the automotive domain. We present the comparison, discuss their applicabilities, and identify future research needs.},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 1st {ACM} {Workshop} on {Cyber}-{Physical} {System} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Schmittner, Christoph and Ma, Zhendong and Schoitsch, Erwin and Gruber, Thomas},
	month = apr,
	year = {2015},
	keywords = {automotive, cyber-physical system, safety and security co-analysis, systems engineering},
	pages = {69--80},
}

@article{kriaa_comparing_2013,
	title = {Comparing {Two} {Approaches} to {Safety} and {Security} {Modelling}: {BDMP} {Technique} and {CHASSIS} {Method}},
	abstract = {In the context of risk analysis for critical systems, it is recognized an emerging need for combining safety and security aspects. Safety critical systems have traditionally been isolated, but with the development of digital industrial control systems they become interconnected and thereby more exposed to cyber security threats. In this paper we focus on combining safety and security aspects during the development of such systems by comparing two of few existing approaches that allow modelling of both aspects. The two methods – Combined Harm Assessment of Safety and Security for Information Systems (CHASSIS) and Boolean logic Driven Markov Processes (BDMP) - are based on extensions of two different approaches, UML and fault/attack trees respectively, and provide a risk assessment framework for critical systems, whose failure or malfunction can have a large negative impact on humans, the environment or economic assets. A system transporting a polluting substance has been modelled with both approaches as a basis for the comparison. It mainly emphasizes the complementarities of both approaches and their possible combination. The results suggest that such a combination of the two approaches is viable and promising.},
	language = {en},
	author = {Kriaa, Siwar and Raspotnig, Christian and Bouissou, Marc and Piètre-Cambacedes, Ludovic and Halgand, Yoran and Katta, Vikash},
	year = {2013},
	pages = {18},
}

@misc{kriaa_comparing_2013,
	title = {Comparing {Two} {Approaches} to {Safety} and {Security} {Modelling} : {BDMP} {Technique} and {CHASSIS} {Method}},
	shorttitle = {Comparing {Two} {Approaches} to {Safety} and {Security} {Modelling}},
	url = {https://www.semanticscholar.org/paper/Comparing-Two-Approaches-to-Safety-and-Security-%3A-Kriaa-Raspotnig/d356c60b7a901c5575b7d8bc96349e432c7c5314},
	abstract = {In the context of risk analysis for critical systems, it is recognized an emerging need for combining safety and security aspects. Safety critical systems have traditionally been isolated, but with the development of digital industrial control systems they become interconnected and thereby more exposed to cyber security threats. In this paper we focus on combining safety and security aspects during the development of such systems by comparing two of few existing approaches that allow modelling of both aspects. The two methods – Combined Harm Assessment of Safety and Security for Information Systems (CHASSIS) and Boolean logic Driven Markov Processes (BDMP) are based on extensions of two different approaches, UML and fault/attack trees respectively, and provide a risk assessment framework for critical systems, whose failure or malfunction can have a large negative impact on humans, the environment or economic assets. A system transporting a polluting substance has been modelled with both approaches as a basis for the comparison. It mainly emphasizes the complementarities of both approaches and their possible combination. The results suggest that such a combination of the two approaches is viable and promising.},
	language = {en},
	urldate = {2021-09-21},
	author = {Kriaa, Siwar and Raspotnig, Christian and Bouissou, M. and Piètre-Cambacédès, L. and Peter and Karpati and Halgand, Yoran and Katta, Vikash},
	year = {2013},
}

@inproceedings{young_systems_2013,
	address = {New Orleans Louisiana USA},
	title = {Systems thinking for safety and security},
	isbn = {978-1-4503-2015-3},
	url = {https://dl.acm.org/doi/10.1145/2523649.2530277},
	doi = {10.1145/2523649.2530277},
	abstract = {The fundamental challenge facing security professionals is preventing losses, be they operational, financial or mission losses. As a result, one could argue that security professionals share this challenge with safety professionals. Despite their shared challenge, there is little evidence that recent advances that enable one community to better prevent losses have been shared with the other for possible implementation. Limitations in current safety approaches have led researchers and practitioners to develop new models and techniques. These techniques could potentially benefit the field of security. This paper describes a new systems thinking approach to safety that may be suitable for meeting the challenge of securing complex systems against cyber disruptions. SystemsTheoretic Process Analysis for Security (STPA-Sec) augments traditional security approaches by introducing a top-down analysis process designed to help a multidisciplinary team consisting of security, operations, and domain experts identify and constrain the system from entering vulnerable states that lead to losses. This new framework shifts the focus of the security analysis away from threats as the proximate cause of losses and focuses instead on the broader system structure that allowed the system to enter a vulnerable system state that the threat exploits to produce the disruption leading to the loss.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {Proceedings of the 29th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Young, William and Leveson, Nancy},
	month = dec,
	year = {2013},
	pages = {1--8},
}

@article{kavallieratos_cybersecurity_2020,
	title = {Cybersecurity and {Safety} {Co}-{Engineering} of {Cyberphysical} {Systems}—{A} {Comprehensive} {Survey}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1999-5903/12/4/65},
	doi = {10.3390/fi12040065},
	abstract = {Safeguarding both safety and cybersecurity is paramount to the smooth and trustworthy operation of contemporary cyber physical systems, many of which support critical functions and services. As safety and security have been known to be interdependent, they need to be jointly considered in such systems. As a result, various approaches have been proposed to address safety and cybersecurity co-engineering in cyber physical systems. This paper provides a comprehensive survey of safety and cybersecurity co-engineering methods, and discusses relevant open issues and research challenges. Despite the extent of the existing literature, several aspects of the subject still remain to be fully addressed.},
	language = {en},
	number = {4},
	urldate = {2021-09-20},
	journal = {Future Internet},
	author = {Kavallieratos, Georgios and Katsikas, Sokratis and Gkioulos, Vasileios},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {co-engineering, cyber physical systems, cybersecurity, safety},
	pages = {65},
}

@inproceedings{sabaliauskaite_integrating_2017,
	title = {Integrating {Six}-{Step} {Model} with {Information} {Flow} {Diagrams} for {Comprehensive} {Analysis} of {Cyber}-{Physical} {System} {Safety} and {Security}},
	doi = {10.1109/HASE.2017.25},
	abstract = {An approach for integrating Six-Step Model (SSM) with Information Flow Diagrams (IFDs) is proposed. SSM is a model for Cyber-Physical System (CPS) safety and security analysis, which incorporates six hierarchies of CPS, namely, functions, structure, failures, safety countermeasures, cyber-attacks, and security countermeasures. Relationship matrices are used in SSM to identify inter-relationships between these hierarchies and determine the effect of failures and cyber-attacks on CPSs. Although SSM is a useful tool for CPS safety and security modeling, it lacks guidance for identifying failures and attacks, and selecting adequate set of safety and security countermeasures. To address this issue, an approach for integrating SSM with IFDs is proposed and explained using the water treatment system example.},
	booktitle = {2017 {IEEE} 18th {International} {Symposium} on {High} {Assurance} {Systems} {Engineering} ({HASE})},
	author = {Sabaliauskaite, Giedre and Adepu, Sridhar},
	month = jan,
	year = {2017},
	note = {ISSN: 1530-2059},
	keywords = {3-Step Model, Analytical models, Communication channels, Computational modeling, GTST-MLD, ISA-99, Information Flow Diagram, Safety, Security, Sensors, Six-Step Model, cyber-attacks, cyber-physical system, failures, safety, security},
	pages = {41--48},
}

@article{carreras_guzman_comparative_2021,
	title = {A {Comparative} {Study} of {STPA}-{Extension} and the {UFoI}-{E} {Method} for {Safety} and {Security} {Co}-analysis},
	volume = {211},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832021001745},
	doi = {10.1016/j.ress.2021.107633},
	abstract = {Emerging challenges in cyber-physical systems (CPSs) have been encouraging the development of safety and security co-analysis methods. These methods aim at mitigating the new risks associated with the convergence of safety-related systemic flaws and security-related cyber-attacks that have led to major losses in CPSs. Although several studies have reviewed existing safety and security co-analysis methods, only a few empirical studies have attempted to compare their strengths and limitations to guide risk analysis in practice. This paper bridges the gap between two novel safety and security co-analysis methods and their practical implementations. Namely, this paper compares a novel extension of the System-Theoretic Process Analysis (STPA-Extension) and the Uncontrolled Flows of Information and Energy (UFoI-E) method through a common case study. In our case study, the CPS under analysis is a conceptual autonomous ship. We conducted our comparative study as two independent teams to guarantee that the implementation of one method did not influence the other method. Furthermore, we developed a comparative framework that evaluates the relative completeness and the effort required in each analysis. Finally, we propose a tailored combination of these methods, exploiting their unique strengths to achieve more complete and cost-effective risk analysis results.},
	language = {en},
	urldate = {2021-09-20},
	journal = {Reliability Engineering \& System Safety},
	author = {Carreras Guzman, Nelson H. and Zhang, Jin and Xie, Jing and Glomsrud, Jon Arne},
	month = jul,
	year = {2021},
	keywords = {Safety and security, comparative study, cyber-physical systems (CPSs), autonomous ship, risk identification},
	pages = {107633},
}

@inproceedings{kaneko_stamp_2020,
	title = {{STAMP} {S} amp;{S}: {Safety} amp; {Security} {Scenario} for {Specification} and {Standard} in the society of {AI}/{IoT}},
	shorttitle = {{STAMP} {S} amp;{S}},
	doi = {10.1109/QRS-C51114.2020.00037},
	abstract = {Systems, including AI/IoT, have complex relationships. It is necessary to analyze risks from various perspectives to build a system that can be used safely and securely throughout society, including people and organizations. In order to model a complex system hierarchically, we propose to model the control structure diagram of STAMP into five layers according to the life cycle of software and system requirements. Based on the STAMP S \& S five-layer model, the necessity of performing safety and security analysis in each layer and performing scenario analysis to generate specifications and standards ensuring safety and security is presented.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Kaneko, Tomoko and Yoshioka, Nobukazu and Sasaki, Ryoichi},
	month = dec,
	year = {2020},
	keywords = {Analytical models, Artificial intelligence, Complex systems, STAMP, Safety, Scenario, Security, Specification, Standard, Standards, Task analysis},
	pages = {168--175},
}

@book{internationale_elektrotechnische_kommission_industrial-process_2019,
	address = {Geneva},
	edition = {Edition 1.0, 2019-05},
	series = {{IEC} technical report},
	title = {Industrial-process measurement, control and automation: framework for functional safety and security},
	isbn = {978-2-8322-6925-1},
	shorttitle = {Industrial-process measurement, control and automation},
	language = {en},
	number = {63069},
	publisher = {IEC Central Office},
	editor = {Internationale Elektrotechnische Kommission},
	year = {2019},
}

@inproceedings{wang2019looking,
	title = {Looking from the mirror: {Evaluating} iot device security through mobile companion apps},
	booktitle = {28th \{{USENIX}\} security symposium (\{{USENIX}\} security 19)},
	author = {Wang, Xueqiang and Sun, Yuqiong and Nanda, Susanta and Wang, XiaoFeng},
	year = {2019},
	pages = {1151--1167},
}

@inproceedings{chen_iotfuzzer_2018,
	address = {San Diego, CA},
	title = {{IoTFuzzer}: {Discovering} {Memory} {Corruptions} in {IoT} {Through} {App}-based {Fuzzing}},
	isbn = {978-1-891562-49-5},
	shorttitle = {{IoTFuzzer}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-1_Chen_paper.pdf},
	doi = {10.14722/ndss.2018.23159},
	abstract = {With more IoT devices entering the consumer market, it becomes imperative to detect their security vulnerabilities before an attacker does. Existing binary analysis based approaches only work on ﬁrmware, which is less accessible except for those equipped with special tools for extracting the code from the device. To address this challenge in IoT security analysis, we present in this paper a novel automatic fuzzing framework, called IOTFUZZER, which aims at ﬁnding memory corruption vulnerabilities in IoT devices without access to their ﬁrmware images. The key idea is based upon the observation that most IoT devices are controlled through their ofﬁcial mobile apps, and such an app often contains rich information about the protocol it uses to communicate with its device. Therefore, by identifying and reusing program-speciﬁc logic (e.g., encryption) to mutate the test case (particularly message ﬁelds), we are able to effectively probe IoT targets without relying on any knowledge about its protocol speciﬁcations. In our research, we implemented IOTFUZZER and evaluated 17 real-world IoT devices running on different protocols, and our approach successfully identiﬁed 15 memory corruption vulnerabilities (including 8 previously unknown ones).},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Chen, Jiongyi and Diao, Wenrui and Zhao, Qingchuan and Zuo, Chaoshun and Lin, Zhiqiang and Wang, XiaoFeng and Lau, Wing Cheong and Sun, Menghan and Yang, Ronghai and Zhang, Kehuan},
	year = {2018},
}

@incollection{abrahamsson_use_2015,
	address = {Cham},
	title = {On the {Use} of {Safety} {Certification} {Practices} in {Autonomous} {Field} {Robot} {Software} {Development}: {A} {Systematic} {Mapping} {Study}},
	volume = {9459},
	isbn = {978-3-319-26843-9 978-3-319-26844-6},
	shorttitle = {On the {Use} of {Safety} {Certification} {Practices} in {Autonomous} {Field} {Robot} {Software} {Development}},
	url = {http://link.springer.com/10.1007/978-3-319-26844-6_25},
	abstract = {Robotics has recently seen an increasing development, and the areas addressed within robotics has extended into domains we consider safety-critical, fostering the development of standards that facilitate the development of safe robots. Safety standards describe concepts to maintain desired reactions or performance in malfunctioning systems, and inﬂuence industry regarding software development and project management. However, academia seemingly did not reach the same degree of utilisation of standards. This paper presents the ﬁndings from a systematic mapping study in which we study the state-of-the-art in developing software for safety-critical software for autonomous ﬁeld robots. The purpose of the study is to identify practices used for the development of autonomous ﬁeld robots and how these practices relate to available safety standards. Our ﬁndings from reviewing 49 papers show that standards, if at all, are barely used. The majority of the papers propose various solutions to achieve safety, and about half of the papers refer to non-standardised approaches that mainly address the methodical rather than the development level. The present study thus shows an emerging ﬁeld still on the quest for suitable approaches to develop safety-critical software, awaiting appropriate standards for this support.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer International Publishing},
	author = {Ingibergsson, Johann Thor Mogensen and Schultz, Ulrik Pagh and Kuhrmann, Marco},
	editor = {Abrahamsson, Pekka and Corral, Luis and Oivo, Markku and Russo, Barbara},
	year = {2015},
	doi = {10.1007/978-3-319-26844-6_25},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {335--352},
}

@article{ebeid_survey_2018,
	title = {A survey of {Open}-{Source} {UAV} flight controllers and flight simulators},
	volume = {61},
	issn = {01419331},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0141933118300930},
	doi = {10.1016/j.micpro.2018.05.002},
	abstract = {The current disruptive innovation in civilian drone (UAV) applications has led to an increased need for research and development in UAV technology. The key challenges currently being addressed are related to UAV platform properties such as functionality, reliability, fault tolerance, and endurance, which are all tightly linked to the UAV ﬂight controller hardware and software. The lack of standardization of ﬂight controller architectures and the use of proprietary closed-source ﬂight controllers on many UAV platforms, however, complicates this work: solutions developed for one ﬂight controller may be diﬃcult to port to another without substantial extra development and testing. Using open-source ﬂight controllers mitigates some of these challenges and enables other researchers to validate and build upon existing research.},
	language = {en},
	urldate = {2021-09-20},
	journal = {Microprocessors and Microsystems},
	author = {Ebeid, Emad and Skriver, Martin and Terkildsen, Kristian Husum and Jensen, Kjeld and Schultz, Ulrik Pagh},
	month = sep,
	year = {2018},
	keywords = {UAVs},
	pages = {11--20},
}

@article{albonico_mining_2021,
	title = {Mining {Energy}-{Related} {Practices} in {Robotics} {Software}},
	url = {http://arxiv.org/abs/2103.13762},
	abstract = {Robots are becoming more and more commonplace in many industry settings. This successful adoption can be partly attributed to (1) their increasingly affordable cost and (2) the possibility of developing intelligent, software-driven robots. Unfortunately, robotics software consumes signiﬁcant amounts of energy. Moreover, robots are often battery-driven, meaning that even a small energy improvement can help reduce its energy footprint and increase its autonomy and user experience.},
	language = {en},
	urldate = {2021-09-20},
	journal = {arXiv:2103.13762 [cs]},
	author = {Albonico, Michel and Malavolta, Ivano and Pinto, Gustavo and Guzman, Emitza and Chinnappan, Katerina and Lago, Patricia},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.13762},
	keywords = {Computer Science - Robotics, Computer Science - Software Engineering},
}

@inproceedings{fischer-nielsen_forgotten_2020,
	address = {Seoul South Korea},
	title = {The forgotten case of the dependency bugs: on the example of the robot operating system},
	isbn = {978-1-4503-7123-0},
	shorttitle = {The forgotten case of the dependency bugs},
	url = {https://dl.acm.org/doi/10.1145/3377813.3381364},
	doi = {10.1145/3377813.3381364},
	abstract = {A dependency bug is a software fault that manifests itself when accessing an unavailable asset. Dependency bugs are pervasive and we all hate them. This paper presents a case study of dependency bugs in the Robot Operating System (ROS), applying mixed methods: a qualitative investigation of 78 dependency bug reports, a quantitative analysis of 1354 ROS bug reports against 19553 reports in the top 30 GitHub projects, and a design of three dependency linters evaluated on 406 ROS packages.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Fischer-Nielsen, Anders and Fu, Zhoulai and Su, Ting and Wąsowski, Andrzej},
	month = jun,
	year = {2020},
	pages = {21--30},
}

@inproceedings{lee_cyber_2008,
	address = {Orlando, FL, USA},
	title = {Cyber {Physical} {Systems}: {Design} {Challenges}},
	isbn = {978-0-7695-3132-8},
	shorttitle = {Cyber {Physical} {Systems}},
	url = {http://ieeexplore.ieee.org/document/4519604/},
	doi = {10.1109/ISORC.2008.25},
	abstract = {Cyber-Physical Systems (CPS) are integrations of computation and physical processes. Embedded computers and networks monitor and control the physical processes, usually with feedback loops where physical processes affect computations and vice versa. The economic and societal potential of such systems is vastly greater than what has been realized, and major investments are being made worldwide to develop the technology. There are considerable challenges, particularly because the physical components of such systems introduce safety and reliability requirements qualitatively different from those in generalpurpose computing. Moreover, physical components are qualitatively different from object-oriented software components. Standard abstractions based on method calls and threads do not work. This paper examines the challenges in designing such systems, and in particular raises the question of whether today’s computing and networking technologies provide an adequate foundation for CPS. It concludes that it will not be sufﬁcient to improve design processes, raise the level of abstraction, or verify (formally or otherwise) designs that are built on today’s abstractions. To realize the full potential of CPS, we will have to rebuild computing and networking abstractions. These abstractions will have to embrace physical dynamics and computation in a uniﬁed way.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {2008 11th {IEEE} {International} {Symposium} on {Object} and {Component}-{Oriented} {Real}-{Time} {Distributed} {Computing} ({ISORC})},
	publisher = {IEEE},
	author = {Lee, Edward A.},
	month = may,
	year = {2008},
	pages = {363--369},
}

@inproceedings{reddy_cloud-based_2014,
	address = {Maui, HI, USA},
	title = {Cloud-{Based} {Cyber} {Physical} {Systems}: {Design} {Challenges} and {Security} {Needs}},
	isbn = {978-1-4799-7394-1},
	shorttitle = {Cloud-{Based} {Cyber} {Physical} {Systems}},
	url = {http://ieeexplore.ieee.org/document/7051787/},
	doi = {10.1109/MSN.2014.50},
	abstract = {The cyber-physical systems are the combination of computational elements and physical entities that can interact with humans through many modalities. The security includes the malicious attempts by adversary that disrupts or fails the functions of physical systems and affects infrastructure, businesses, and routine human life. The research in cyberphysical systems is in its initial stage. Therefore, first we discussed the status of security in cloud cyber-physical systems. Second, we introduced the challenges ahead to the design and development of the future engineering systems with new security capabilities. Third, we presented the security requirements in Hadoop distributed file systems. Since trustbased packet transfer in sensor networks is one of the important security issue infrastructure security, we presented a trust-based approach using Sporas formula and presented the simulations to trust of a successive node before transferring the packets. Finally, the paper presents the future research on cyber-physical systems in the cloud environment.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {2014 10th {International} {Conference} on {Mobile} {Ad}-hoc and {Sensor} {Networks}},
	publisher = {IEEE},
	author = {Reddy, Yenumula B.},
	month = dec,
	year = {2014},
	pages = {315--322},
}

@inproceedings{fitzgerald_cyber-physical_2015,
	address = {Florence, Italy},
	title = {Cyber-{Physical} {Systems} {Design}: {Formal} {Foundations}, {Methods} and {Integrated} {Tool} {Chains}},
	isbn = {978-1-4673-7043-1},
	shorttitle = {Cyber-{Physical} {Systems} {Design}},
	url = {http://ieeexplore.ieee.org/document/7166696/},
	doi = {10.1109/FormaliSE.2015.14},
	abstract = {The engineering of dependable cyber-physical systems (CPSs) is inherently collaborative, demanding cooperation between diverse disciplines. A goal of current research is the development of integrated tool chains for model-based CPS design that support co-modelling, analysis, co-simulation, testing and implementation. We discuss the role of formal methods in addressing three key aspects of this goal: providing reasoning support for semantically heterogeneous models, managing the complexity and scale of design space exploration, and supporting traceability and provenance in the CPS design set. We brieﬂy outline an approach to the development of such a tool chain based on existing tools and discuss ongoing challenges and open research questions in this area.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {2015 {IEEE}/{ACM} 3rd {FME} {Workshop} on {Formal} {Methods} in {Software} {Engineering}},
	publisher = {IEEE},
	author = {Fitzgerald, John and Gamble, Carl and Larsen, Peter Gorm and Pierce, Kenneth and Woodcock, Jim},
	month = may,
	year = {2015},
	pages = {40--46},
}

@inproceedings{ding_safety_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {On the {Safety} of {IoT} {Device} {Physical} {Interaction} {Control}},
	isbn = {978-1-4503-5693-0},
	url = {https://doi.org/10.1145/3243734.3243865},
	doi = {10.1145/3243734.3243865},
	abstract = {Emerging Internet of Things (IoT) platforms provide increased functionality to enable human interaction with the physical world in an autonomous manner. The physical interaction features of IoT platforms allow IoT devices to make an impact on the physical environment. However, such features also bring new safety challenges, where attackers can leverage stealthy physical interactions to launch attacks against IoT systems. In this paper, we propose a framework called IoTMon that discovers any possible physical interactions and generates all potential interaction chains across applications in the IoT environment. IoTMon also includes an assessment of the safety risk of each discovered inter-app interaction chain based on its physical influence. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of IoTMon and present a comprehensive system evaluation on the Samsung SmartThings platform. We study 185 official SmartThings applications and find they can form 162 hidden inter-app interaction chains through physical surroundings. In particular, our experiment reveals that 37 interaction chains are highly risky and could be potentially exploited to impact the safety of the IoT{\textasciitilde}environment.},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Wenbo and Hu, Hongxin},
	month = oct,
	year = {2018},
	keywords = {internet of things, physical interaction control, safety},
	pages = {832--846},
}

@article{subramanian_quantitative_2016,
	title = {Quantitative {Assessment} of {Safety} and {Security} of {System} {Architectures} for {Cyberphysical} {Systems} {Using} the {NFR} {Approach}},
	volume = {10},
	issn = {1937-9234},
	doi = {10.1109/JSYST.2013.2294628},
	abstract = {Cyberphysical systems (CPSs) are an integral part of modern societies since most critical infrastructures are controlled by these systems. CPSs incorporate computer-based and network-based technologies for the monitoring and control of physical processes. Two critically important properties of CPSs are safety and security. It is widely accepted that properties such as safety and security should be considered at the system design phase itself, particularly at the architectural level wherein such properties are embedded in the final system. However, safety and security are interrelated, and there seems to be a lack of techniques that consider both of them together. The nonfunctional requirement (NFR) approach is a technique that allows the simultaneous evaluation of both safety and security at the architectural level. In this paper, we apply the NFR approach to quantitatively evaluate the safety and security properties of an example CPS, i.e., an oil pipeline control system. We conclude that the NFR approach provides practical results that can be used by designers and developers to create safe and secure CPSs.},
	number = {2},
	journal = {IEEE Systems Journal},
	author = {Subramanian, Nary and Zalewski, Janusz},
	month = jun,
	year = {2016},
	note = {Conference Name: IEEE Systems Journal},
	keywords = {Communication system security, Cyberphysical systems (CPSs), Monitoring, Pipelines, Safety, Security, Wireless communication, Wireless sensor networks, nonfunctional requirement (NFR) approach, safety, security, system architecture assessment},
	pages = {397--409},
}

@article{wang_hyperscan_nodate,
	title = {Hyperscan: {A} {Fast} {Multi}-pattern {Regex} {Matcher} for {Modern} {CPUs}},
	abstract = {Regular expression matching serves as a key functionality of modern network security applications. Unfortunately, it often becomes the performance bottleneck as it involves compute-intensive scan of every byte of packet payload. With trends towards increasing network bandwidth and a large ruleset of complex patterns, the performance requirement gets ever more demanding. In this paper, we present Hyperscan, a high performance regular expression matcher for commodity server machines. Hyperscan employs two core techniques for efficient pattern matching. First, it exploits graph decomposition that translates regular expression matching into a series of string and finite automata matching. Unlike existing solutions, string matching becomes a part of regular expression matching, eliminating duplicate operations. Decomposed regular expression components also increase the chance of fast DFA matching as they tend to be smaller than the original pattern. Second, Hyperscan accelerates both string and finite automata matching using SIMD operations, which brings substantial throughput improvement. Our evaluation shows that Hyperscan improves the performance of Snort by a factor of 8.7 for a real traffic trace.},
	language = {en},
	author = {Wang, Xiang and Hong, Yang and Chang, Harry and Langdale, Geoff and Hu, Jiayu},
	pages = {19},
}

@article{biro_safe_2021,
	title = {Safe and secure cyber-physical systems},
	volume = {33},
	issn = {2047-7481},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2340},
	doi = {10.1002/smr.2340},
	abstract = {Cyber-Physical Systems (CPSs) differ from traditional Information Technology (IT) systems in such a way that they interact with the physical environment, i.e., they can monitor and manipulate real objects and processes. For this special issue, the authors of the best papers of IWCFS 2019 were invited to submit extended versions of their workshop papers. Additionally, we received eight submissions from around the globe as a result of an open call. After thorough and stringent reviews, we selected six articles that provide relevant contributions to the field of safety and security for CPSs.},
	language = {en},
	number = {9},
	urldate = {2021-09-16},
	journal = {Journal of Software: Evolution and Process},
	author = {Biró, Miklós and Mashkoor, Atif and Sametinger, Johannes},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2340},
	keywords = {cyber-physical system, safety, security},
	pages = {e2340},
}

@article{ponsard_goal-driven_2021,
	title = {A goal-driven approach for the joint deployment of safety and security standards for operators of essential services},
	volume = {33},
	issn = {2047-7481},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2338},
	doi = {10.1002/smr.2338},
	abstract = {Designing safety-critical software in domains ensuring essential services like transportation, energy, or health requires high assurance techniques and compliance with domain specific standards. As a result of the global interconnectivity and the evolution toward cyber-physical systems, the increasing exposure to cyber threats calls for the adoption of cyber security standards and frameworks. Although safety and security have different cultures, both fields share similar concepts and tools and are worth being investigated together. This paper provides the background to understand emerging co-engineering approaches. It advocates for the use of a model-based approach to provide a sound risk-oriented process and to capture rationales interconnecting top-level standards/directives to concrete safety/security measures. We show the benefits of adopting goal-oriented analysis that can be transposed later to domain-specific frameworks. Both qualitative and quantitative reasoning aspects are analyzed and discussed, especially to support trade-off analysis. Our work is driven by a representative case study in drinking water utility in the scope of the NIS regulation for operator of essential services.},
	language = {en},
	number = {9},
	urldate = {2021-09-16},
	journal = {Journal of Software: Evolution and Process},
	author = {Ponsard, Christophe and Grandclaudon, Jeremy and Massonet, Philippe},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2338},
	keywords = {NIS directive, co-engineering, cyber security, risk management, safety analysis, standards},
	pages = {e2338},
}

@article{lyu_safety_2019,
	title = {Safety and security risk assessment in cyber-physical systems},
	volume = {4},
	issn = {2398-3396},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-cps.2018.5068},
	doi = {10.1049/iet-cps.2018.5068},
	abstract = {The term cyber physical systems (CPS) refers to a new generation of systems with integrated computational and physical capabilities through computation, communication, and control. In the past decades, related techniques for CPS have been well studied and developed, and are widely applied in the fields such as industrial automation, smart transportation, aerospace, environment monitoring, and smart grids. However, with the expansion of CPS complexity and the enhancement of the system openness, most of CPS become not only safety-critical but also security-critical since deeply involving both physical objects and computer networks. In the last decade, it is no longer rare to see safety incidents and security attacks happening in industries. Safety and security issues are increasingly converging on CPS, leading to new situations in which these two closely interdependent issues should now be considered together, rather than separately or in sequence. This paper reviews the existing approaches of risk assessment and management from the perspective of safety, security, and their integration. The comparisons of these approaches are summarised with their pros and cons before the technical gaps between the demand and the current situation of safety and security issues in CPS are identified.},
	language = {en},
	number = {3},
	urldate = {2021-09-16},
	journal = {IET Cyber-Physical Systems: Theory \& Applications},
	author = {Lyu, Xiaorong and Ding, Yulong and Yang, Shuang-Hua},
	year = {2019},
	note = {\_eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-cps.2018.5068},
	keywords = {CPS complexity, computer networks, cyber-physical systems, integrated computational capabilities, physical capabilities, physical objects, risk management, safety incidents, safety-critical system, security attacks, security of data, security risk assessment, security-critical system, system openness},
	pages = {221--232},
}

@incollection{skavhaug_goal-oriented_2016,
	address = {Cham},
	title = {Goal-{Oriented} {Co}-{Engineering} of {Security} and {Safety} {Requirements} in {Cyber}-{Physical} {Systems}},
	volume = {9923},
	isbn = {978-3-319-45479-5 978-3-319-45480-1},
	url = {http://link.springer.com/10.1007/978-3-319-45480-1_27},
	abstract = {Many safety critical systems are integrating more and more software based systems and are becoming increasingly connected. Such Cyber-Physical Systems require high assurance both on safety and security but also on how such properties aﬀect each other. This covers not only design time aspects but also the run-time: as cyber-security threats evolve constantly, it is necessary to consider how to perform updates of the software without breaking any safety properties. This paper proposes a method to co-engineer them based on sound techniques taken from goal-oriented requirements engineering. The approach is illustrated on a case study from the automotive domain. The case study illustrates the challenges to safety and security co-engineering created by the trend of growing connectivity and the evolution towards more autonomous vehicles in the transportation domain.},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {Computer {Safety}, {Reliability}, and {Security}},
	publisher = {Springer International Publishing},
	author = {Ponsard, Christophe and Dallons, Gautier and Massonet, Philippe},
	editor = {Skavhaug, Amund and Guiochet, Jérémie and Schoitsch, Erwin and Bitsch, Friedemann},
	year = {2016},
	doi = {10.1007/978-3-319-45480-1_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {334--345},
}

@article{vasisht_farmbeats_nodate,
	title = {{FarmBeats}: {An} {IoT} {Platform} for {Data}-{Driven} {Agriculture}},
	abstract = {Data-driven techniques help boost agricultural productivity by increasing yields, reducing losses and cutting down input costs. However, these techniques have seen sparse adoption owing to high costs of manual data collection and limited connectivity solutions. In this paper, we present FarmBeats, an end-to-end IoT platform for agriculture that enables seamless data collection from various sensors, cameras and drones. FarmBeats’s system design that explicitly accounts for weather-related power and Internet outages has enabled six month long deployments in two US farms.},
	language = {en},
	author = {Vasisht, Deepak and Kapetanovic, Zerina and Won, Jong-ho and Jin, Xinxin and Chandra, Ranveer and Kapoor, Ashish and Sinha, Sudipta N and Sudarshan, Madhusudhan and Stratman, Sean},
	pages = {17},
}

@article{celik_soteria_nodate,
	title = {{SOTERIA}: {Automated} {IoT} {Safety} and {Security} {Analysis}},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital systems have changed the way we live, play and work. Yet existing IoT platforms cannot evaluate whether an IoT app or environment is safe, secure, and operates correctly. In this paper, we present SOTERIA, a static analysis system for validating whether an IoT app or IoT environment (collection of apps working in concert) adheres to identiﬁed safety, security, and functional properties. SOTERIA operates in three phases; (a) translation of platform-speciﬁc IoT source code into an intermediate representation (IR), (b) extracting a state model from the IR, (c) applying model checking to verify desired properties. We evaluate SOTERIA on 65 SmartThings market apps through 35 properties and ﬁnd nine (14\%) individual apps violate ten (29\%) properties. Further, our study of combined app environments uncovered eleven property violations not exhibited in the isolated apps. Lastly, we demonstrate SOTERIA on MALIOT, a novel open-source test suite containing 17 apps with 20 unique violations.},
	language = {en},
	author = {Celik, Z Berkay and McDaniel, Patrick and Tan, Gang},
	pages = {13},
}

@inproceedings{celik_iotguard_2019,
	address = {San Diego, CA},
	title = {{IoTGuard}: {Dynamic} {Enforcement} of {Security} and {Safety} {Policy} in {Commodity} {IoT}},
	isbn = {978-1-891562-55-6},
	shorttitle = {{IoTGuard}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_07A-1_Celik_paper.pdf},
	doi = {10.14722/ndss.2019.23326},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital connectivity has changed the way we live, play, and work. To date, the traditional approach to securing IoT has treated devices individually. However, in practice, it has been recently shown that the interactions among devices are often the real cause of safety and security violations. In this paper, we present IOTGUARD, a dynamic, policy-based enforcement system for IoT, which protects users from unsafe and insecure device states by monitoring the behavior of IoT and triggeraction platform apps. IOTGUARD operates in three phases: (a) implementation of a code instrumentor that adds extra logic to an app’s source code to collect app’s information at runtime, (b) storing the apps’ information in a dynamic model that represents the runtime execution behavior of apps, and (c) identifying IoT safety and security policies, and enforcing relevant policies on the dynamic model of individual apps or sets of interacting apps. We demonstrate IOTGUARD on 20 ﬂawed apps and ﬁnd that IOTGUARD correctly enforces 12 of the 12 policy violations. In addition, we evaluate IOTGUARD on 35 SmartThings IoT and 30 IFTTT trigger-action platform market apps executed in a simulated smart home. IOTGUARD enforces 11 unique policies and blocks 16 states in six (17.1\%) SmartThings and ﬁve (16.6\%) IFTTT apps. IOTGUARD imposes only 17.3\% runtime overhead on an app and 19.8\% for ﬁve interacting apps. Through this effort, we introduce a rigorously grounded system for enforcing correct operation of IoT devices through systematically identiﬁed IoT policies, demonstrating the effectiveness and value of monitoring IoT apps with tools such as IOTGUARD.},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Celik, Z. Berkay and Tan, Gang and McDaniel, Patrick},
	year = {2019},
}

@inproceedings{sokolowski_automating_2021,
	address = {Athens Greece},
	title = {Automating serverless deployments for {DevOps} organizations},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468575},
	doi = {10.1145/3468264.3468575},
	abstract = {DevOps unifies software development and operations in cross-functional teams to improve software delivery and operations (SDO) performance. Ideally, cross-functional DevOps teams independently deploy their services, but the correct operation of a service often demands other services, requiring coordination to ensure the correct deployment order. This issue is currently solved either with a central deployment or manual out-of-band communication across teams, e.g., via phone, chat, or email. Unfortunately, both contradict the independence of teams, hindering SDO performanceÐthe reason why DevOps is adopted in the first place. In this work, we conduct a study on 73 IT professionals, showing that, in practice, they resort to manual coordination for correct deployments even if they expect better SDO performance with fully automated approaches. To address this issue, we propose µs ([mju:z] łmusež ), a novel IaC system automating deployment coordination in a fully decentralized fashion, still retaining compatibility with the DevOps practiceÐin contrast to today’s solutions. We implement µs, demonstrate that it effectively enables automated coordination, introduces negligible definition overhead, has no performance overhead, and is broadly applicable, as shown by the migration of 64 third-party IaC projects.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Sokolowski, Daniel and Weisenburger, Pascal and Salvaneschi, Guido},
	month = aug,
	year = {2021},
	pages = {57--69},
}

@article{melzner_case_2013,
	title = {A case study on automated safety compliance checking to assist fall protection design and planning in building information models},
	volume = {31},
	issn = {0144-6193, 1466-433X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01446193.2013.780662},
	doi = {10.1080/01446193.2013.780662},
	abstract = {Worldwide occupational safety statistics show that the construction industry in many countries experiences one of the highest accident rates of all industry sectors. Falls remain a major concern as they contribute to very serious injuries or even fatalities on construction projects around the world. Since the standards and rules for protective safety equipment vary by country, the growing numbers of internationally operating companies are in need of tools that allow ubiquitous understanding and planning of safety regardless of the country where they operate. The problem is examined using a customizable automatic safety rule-checking platform for building information models. The applied rule-based checking algorithms are designed to be add-ons to existing building information modelling (BIM) software and can check models for safety hazards early in the design and planning process. Once hazards have been identiﬁed preventative safety equipment can be designed, estimated, and included in the construction schedule before construction starts. A case study implements the safety rule-checking platform on a high-rise building project. Fall protection regulations from both the USA and Germany are applied to the developed rule-checking platform. Visualization of the safety information further explains the differences in the results once country-speciﬁc safety-regulative standards are applied on the same building information model. The case study also indicates that the role of BIM in safety design and planning can effectively assist the traditional safety decision-making process for fall protection equipment.},
	language = {en},
	number = {6},
	urldate = {2021-09-15},
	journal = {Construction Management and Economics},
	author = {Melzner, Jürgen and Zhang, Sijie and Teizer, Jochen and Bargstädt, Hans-Joachim},
	month = jun,
	year = {2013},
	pages = {661--674},
}

@article{fang_framework_2016,
	title = {A framework for real-time pro-active safety assistance for mobile crane lifting operations},
	volume = {72},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580516301807},
	doi = {10.1016/j.autcon.2016.08.025},
	abstract = {Despite many safety considerations addressed in lift pre-planning, the ability to provide real-time safety assistance to crane operators and to mitigate human errors during the lifting operation is missing. This research developed a framework for real-time pro-active safety assistance for mobile crane lifting operations. First, crane poses are reconstructed in real-time based on the critical motions of crane parts captured by a sensor system. Second, as-is lift site conditions are automatically modeled and updated based on point cloud data. Lastly, the risk of colliding the crane parts and lifted load into nearby obstructions is pro-actively analyzed and warnings are provided to the operator through a graphical user interface. A prototype system was developed based on the framework and deployed on a mobile crane. Field test results indicate that the system can accurately reconstruct crane motion in real-time and provide pro-active warnings that allow the operator to make timely decisions to mitigate the risk.},
	language = {en},
	urldate = {2021-09-15},
	journal = {Automation in Construction},
	author = {Fang, Yihai and Cho, Yong K. and Chen, Jingdao},
	month = dec,
	year = {2016},
	pages = {367--379},
}

@inproceedings{qu_experimental_2020,
	title = {An {Experimental} {Study} on {Microservices} based {Edge} {Computing} {Platforms}},
	doi = {10.1109/INFOCOMWKSHPS50562.2020.9163068},
	abstract = {The rapid technological advances in the Internet of Things (IoT) allows the blueprint of Smart Cities to become feasible by integrating heterogeneous cloud/fog/edge computing paradigms to collaboratively provide variant smart services in our cities and communities. Thanks to attractive features like fine granularity and loose coupling, the microservices architecture has been proposed to provide scalable and extensible services in large scale distributed IoT systems. Recent studies have evaluated and analyzed the performance interference between microservices based on scenarios on the cloud computing environment. However, they are not holistic for IoT applications given the restriction of the edge device like computation consumption and network capacity. This paper investigates multiple microservice deployment policies on edge computing platform. The microservices are developed as docker containers, and comprehensive experimental results demonstrate the performance and interference of microservices running on benchmark scenarios.},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications} {Workshops} ({INFOCOM} {WKSHPS})},
	author = {Qu, Qian and Xu, Ronghua and Nikouei, Seyed Yahya and Chen, Yu},
	month = jul,
	year = {2020},
	keywords = {Benchmark testing, Cloud computing, Computer architecture, Container, Containers, Edge Computing, Edge computing, Internet of Things (IoT), Microservices Architecture, Performance evaluation, Service-oriented architecture},
	pages = {836--841},
}

@inproceedings{oconnor_exploring_2016,
	title = {Exploring the {Impact} of {Situational} {Context} — {A} {Case} {Study} of a {Software} {Development} {Process} for a {Microservices} {Architecture}},
	doi = {10.1109/ICSSP.2016.009},
	abstract = {Over the decades, a variety of software development processes have been proposed, each with their own advantages and disadvantages. It is however widely accepted that there is no single process that is perfectly suited to all settings, thus a software process should be molded to the needs of its situational context. In previous work, we have consolidated a substantial body of related research into an initial reference framework of the situational factors affecting the software development process. Practitioners can consult this framework in order to profile their context, a step necessary for effective software process decision making. In this paper, we report on the findings from a case study involving process discovery in a small but successful and growing software development firm. In this organization, which has a focus on continuous software evolution and delivery, we also applied the situational factors reference framework, finding that context is a complex and key informant for software process decisions. Studies of this type highlight the role of situational context in software process definition and evolution, and they raise awareness not just of the importance of situational context, but also of the complexity surrounding software process contexts, a complexity which may not be fully appreciated in all software development settings.},
	booktitle = {2016 {IEEE}/{ACM} {International} {Conference} on {Software} and {System} {Processes} ({ICSSP})},
	author = {O’Connor, Rory and Elger, Peter and Clarke, Paul M.},
	month = may,
	year = {2016},
	keywords = {Agile, Companies, Computer architecture, Containers, Context, Lean, Personnel, Process Selection, Software, Software Development Context, Software Development Process},
	pages = {6--10},
}

@inproceedings{santana_microservices_2018,
	title = {Microservices: {A} {Mapping} {Study} for {Internet} of {Things} {Solutions}},
	shorttitle = {Microservices},
	doi = {10.1109/NCA.2018.8548331},
	abstract = {The Internet of Things (IoT) involves the connectivity of a number of different physical and virtual devices, allowing the development of new services and applications. These intelligent objects, along with their tasks, constitute domain-specific applications (vertical markets), while ubiquitous and analytical services form domain-independent services (horizontal markets). The development of these applications and services in these markets brings challenges such as deployment, scalability, integration, interoperability, mobility and performance. Recent researches indicates that Microservices have been adopted in several domains such as IoT. This paper provides an overview of the current state of the art (studies selected by May 2018) regarding the adoption of Microservices in the development of IoT applications by means of the mapping study methodology. In this context, we present the eighteen selected studies, the existing contributions and the future research perspectives.},
	booktitle = {2018 {IEEE} 17th {International} {Symposium} on {Network} {Computing} and {Applications} ({NCA})},
	author = {Santana, Cleber and Alencar, Brenno and Prazeres, Cássio},
	month = nov,
	year = {2018},
	keywords = {Architecture, Cloud Computing, Cloud computing, Computer architecture, Fog Computing, Internet of Things, Microservices, Reactive Microservices, Scalability, Service-oriented architecture, Systematics, systematic mapping study},
	pages = {1--4},
}

@inproceedings{krivic_microservices_2017,
	address = {Cham},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Microservices as {Agents} in {IoT} {Systems}},
	isbn = {978-3-319-59394-4},
	doi = {10.1007/978-3-319-59394-4_3},
	abstract = {Developing robust monolith systems has achieved its limitations, since the implementation of changes in today’s large, complex, and fast evolving systems would be too slow and inefficient. As a response to these problems, microservice architecture emerged, and quickly became a widely used solution. Such modular architecture is appropriate for distributed environment of Internet of Things (IoT) solutions. In this paper we present a solution for service management on Machine-to-Machine (M2M) devices within IoT system by using collaborative microservices. Collaboration of distributed modules highly reminds of multi-agent systems where autonomous agents also cooperate to provide services to the end-user. Because of these similarities we consider microservices as modern agents that could improve systems in distributed environments, such as IoT.},
	language = {en},
	booktitle = {Agent and {Multi}-{Agent} {Systems}: {Technology} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Krivic, Petar and Skocir, Pavle and Kusek, Mario and Jezic, Gordan},
	editor = {Jezic, Gordan and Kusek, Mario and Chen-Burger, Yun-Heh Jessica and Howlett, Robert J. and Jain, Lakhmi C.},
	year = {2017},
	keywords = {Agents, IoT, M2M, Microservices, Service management},
	pages = {22--31},
}

@inproceedings{abdollahi_vayghan_deploying_2018,
	title = {Deploying {Microservice} {Based} {Applications} with {Kubernetes}: {Experiments} and {Lessons} {Learned}},
	shorttitle = {Deploying {Microservice} {Based} {Applications} with {Kubernetes}},
	doi = {10.1109/CLOUD.2018.00148},
	abstract = {Microservices represent a new architectural style where small and loosely coupled modules can be developed and deployed independently to compose an application. This architectural style brings various benefits such as maintainability and flexibility in scaling and aims at decreasing downtime in case of failure or upgrade. One of the enablers is Kubernetes, an open source platform that provides mechanisms for deploying, maintaining, and scaling containerized applications across a cluster of hosts. Moreover, Kubernetes enables healing through failure recovery actions to improve the availability of applications. As our ultimate goal is to devise architectures to enable high availability (HA) with Kubernetes for microservice based applications, in this paper we examine the availability achievable through Kubernetes under its default configuration. We have conducted a set of experiments which show that the service outage can be significantly higher than expected.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Abdollahi Vayghan, Leila and Saied, Mohamed Aymen and Toeroe, Maria and Khendek, Ferhat},
	month = jul,
	year = {2018},
	note = {ISSN: 2159-6190},
	keywords = {Cloud computing, Computer crashes, Containers, IP networks, Maintenance engineering, Measurement, Microservices, Containers, Orchestration, Docker, Kubernetes, Failure, Availability, Streaming media},
	pages = {970--973},
}

@article{yaguache_enabling_nodate,
	title = {Enabling {Edge} {Computing} {Using} {Container} {Orchestration} and {Software} {Deﬁned} {Networking}},
	abstract = {With software-deﬁned wide-area networks (SD-WAN) being increasingly adopted, and Kubernetes becoming the de-facto container orchestration tool, the opportunities for deploying edge-computing applications running over a SD-WAN scenario are vast. In this context, a service discovery function will help developing a dynamic infrastructure where clients are able to seek and ﬁnd particular services. Service discovery also enables a self-healing network capable of detecting the unavailable services. Most of the research in the service discovery ﬁeld focuses in the discovery of cloud-based services over software-deﬁned networks (SDN). A lack of research in containerized service discovery over SD-WAN is evident. In this thesis, an in-house service discovery solution that works alongside a container orchestrator for allowing an improved traﬃc handling and better user experience through containerized service discovery and service requests redirection is developed. First, a proof-ofconcept SD-WAN topology was implemented alongside a Kubernetes cluster and the in-house service discovery solution. Next, the implementation’s performance is tested based on the time required for discovering whether a service has been created, updated or removed. Finally, improvements in node distance computation, local breakout support and the usage of data plane programmability are discussed.},
	language = {en},
	author = {Yaguache, Felipe Andres Rodriguez},
	pages = {63},
}

@article{kurbatov_design_nodate,
	title = {Design and implementation of secure communication between microservices},
	language = {en},
	author = {Kurbatov, Aleksandr},
	pages = {50},
}

@article{truong_devops_2020,
	title = {{DevOps} {Contract} for {Assuring} {Execution} of {IoT} {Microservices} in the {Edge}},
	volume = {9},
	issn = {2542-6605},
	url = {https://www.sciencedirect.com/science/article/pii/S2542660519301726},
	doi = {10.1016/j.iot.2019.100150},
	abstract = {The increasing availability of edge and IoT infrastructure-as-a-service allows us to develop lightweight IoT components and deploy them into edge/IoT infrastructures, enabling edge analytics and controls. This paper introduces the development of service contracts for IoT microservices from DevOps perspectives. We analyze stakeholders and present our methods to support stakeholders to program IoT service contracts. We address the diversity of service contracts by using common languages for IoT data and programming. We integrate the development and operation lifecycle of IoT contracts with IoT software components and with supporting DevOps services. To illustrate our approach, we use a real-world Base Transceiver Station maintenance application with Raspberry Pi, Java, JavaScript, JSON and other microservices.},
	language = {en},
	urldate = {2021-09-15},
	journal = {Internet of Things},
	author = {Truong, Hong-Linh and Klein, Peter},
	month = mar,
	year = {2020},
	keywords = {Edge computing, Execution management, IoT, Service contract},
	pages = {100150},
}

@article{alam_orchestration_2018,
	title = {Orchestration of {Microservices} for {IoT} {Using} {Docker} and {Edge} {Computing}},
	volume = {56},
	issn = {1558-1896},
	doi = {10.1109/MCOM.2018.1701233},
	abstract = {The world of connected devices has led to the rise of the Internet of Things paradigm, where applications rely on multiple devices, gathering and sharing data across highly heterogeneous networks. The variety of possible mechanisms, protocols, and hardware has become a hindrance in the development of architectures capable of addressing the most common IoT use cases, while abstracting services from the underlying communication subsystem. Moreover, the world is moving toward new strict requirements in terms of timeliness and low latency in combination with ultra-high availability and reliability. Thus, future IoT architectures will also have to support the requirements of these cyber-physical applications. In this regard, edge computing has been presented as one of the most promising solutions, relying on the cooperation of nodes by moving services directly to end devices and caching information locally. Therefore, in this article, we propose a modular and scalable architecture based on lightweight virtualization. The provided modularity, combined with the orchestration supplied by Docker, simplifies management and enables distributed deployments, creating a highly dynamic system. Moreover, characteristics such as fault tolerance and system availability are achieved by distributing the application logic across different layers, where failures of devices and micro-services can be masked by this natively redundant architecture, with minimal impact on the overall system performance. Experimental results have validated the implementation of the proposed architecture for on-demand services deployment across different architecture layers.},
	number = {9},
	journal = {IEEE Communications Magazine},
	author = {Alam, Muhammad and Rufino, Joao and Ferreira, Joaquim and Ahmed, Syed Hassan and Shah, Nadir and Chen, Yuanfang},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Communications Magazine},
	keywords = {Cloud computing, Computer architecture, Edge computing, Fault tolerance, Logic gates, Topology, Virtualization},
	pages = {118--123},
}

@article{martinez_uav_2020,
	title = {{UAV} {Integration} in {Current} {Construction} {Safety} {Planning} and {Monitoring} {Processes}: {Case} {Study} of a {High}-{Rise} {Building} {Construction} {Project} in {Chile}},
	volume = {36},
	issn = {0742-597X, 1943-5479},
	shorttitle = {{UAV} {Integration} in {Current} {Construction} {Safety} {Planning} and {Monitoring} {Processes}},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29ME.1943-5479.0000761},
	doi = {10.1061/(ASCE)ME.1943-5479.0000761},
	abstract = {The majority of construction fatalities and accidents in Chile occur in high-rise building construction projects that also suffer from an insufficient number of safety managers on-site. Unmanned aerial vehicles (UAVs) and their generated aerial visual contents have the potential to help the limited number of safety managers in such projects to quickly and properly inspect the inaccessible, hard-to-reach, or unsafe locations on the site and to enhance their safety assessment of those projects. In this study, a case study approach was adopted to investigate how UAV technology and their generated aerial visual contents might affect the current approach of conducting safety planning and monitoring in a high-rise building construction site in Chile with a limited number of safety managers. The case study involved three steps: (1) understanding the current safety planning and monitoring process in a high-rise construction project, (2) investigating how UAVrelated tasks and generated visual contents could be integrated into the current process, and (3) assessment of how such UAV integration might affect the current safety planning and monitoring process. The outcome of the case study provided a detailed overview of the new steps required to integrate UAVs in the current safety planning and monitoring process and concluded that adoption of UAV technology had enhanced identification and assessment of hazards in the high-rise project. Hazards associated with unsafe acts and conditions at height (e.g., missing guardrails or safety nets around unprotected edges or openings and loose or unsecured material at height) were the most common types of hazards identified using the UAV in this case study. Safety managers in the project also rated aerial videos captured by the UAV as the most useful type of data for their safety planning and monitoring tasks. The UAV also significantly reduced the amount of time required for safety managers to conduct their site visit walkthroughs on the project site. DOI: 10.1061/(ASCE)ME.1943-5479.0000761. © 2020 American Society of Civil Engineers.},
	language = {en},
	number = {3},
	urldate = {2021-09-15},
	journal = {Journal of Management in Engineering},
	author = {Martinez, Jhonattan G. and Gheisari, Masoud and Alarcón, Luis F.},
	month = may,
	year = {2020},
	pages = {05020005},
}

@book{creswell2017research,
	title = {Research design: {Qualitative}, quantitative, and mixed methods approaches},
	publisher = {Sage publications},
	author = {Creswell, John W and Creswell, J David},
	year = {2017},
}

@techreport{FTCBCP2018internet,
	title = {The internet of things and consumer product hazards},
	institution = {Federal Trade Commission’s Bureau of Consumer Protection},
	author = {BCP Staff},
	year = {2018},
}

@article{sundmaeker2010vision,
	title = {Vision and challenges for realising the {Internet} of {Things}},
	volume = {3},
	number = {3},
	journal = {Cluster of European research projects on the internet of things, European Commision},
	author = {Sundmaeker, Harald and Guillemin, Patrick and Friess, Peter and Woelfflé, Sylvie and {others}},
	year = {2010},
	note = {Publisher: Citeseer},
	pages = {34--36},
}

@article{shakhatreh_unmanned_2019,
	title = {Unmanned {Aerial} {Vehicles} ({UAVs}): {A} {Survey} on {Civil} {Applications} and {Key} {Research} {Challenges}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Unmanned {Aerial} {Vehicles} ({UAVs})},
	doi = {10.1109/ACCESS.2019.2909530},
	abstract = {The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than \$45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.},
	journal = {IEEE Access},
	author = {Shakhatreh, Hazim and Sawalmeh, Ahmad H. and Al-Fuqaha, Ala and Dou, Zuochao and Almaita, Eyad and Khalil, Issa and Othman, Noor Shamsiah and Khreishah, Abdallah and Guizani, Mohsen},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Civil infrastructure inspection, Communication system security, Market research, Security, Surveillance, UAVs, Unmanned aerial vehicles, Wireless communication, Wireless sensor networks, delivery of goods, precision agriculture, real-time monitoring, remote sensing, search and rescue, security and surveillance, wireless coverage},
	pages = {48572--48634},
}

@article{baker_model-driven_nodate,
	title = {Model-{Driven} {Engineering} in a {Large} {Industrial} {Context} — {Motorola} {Case} {Study}},
	abstract = {In an ongoing eﬀort to reduce development costs in spite of increasing system complexity, Motorola has been a long-time adopter of Model-Driven Engineering (MDE) practices. The foundation of this approach is the creation of rigorous models throughout the development process, thereby enabling the introduction of automation. In this paper we present our experiences within Motorola in deploying a top-down approach to MDE for more than 15 years. We describe some of the key competencies that have been developed and the impact of MDE within the organization. Next we present some of the main issues encountered during MDE deployment, together with some possible resolutions.},
	language = {en},
	author = {Baker, Paul and Loh, Shiou and Weil, Frank},
	pages = {16},
}

@article{whittle_state_2014,
	title = {The {State} of {Practice} in {Model}-{Driven} {Engineering}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/6507223/},
	doi = {10.1109/MS.2013.65},
	language = {en},
	number = {3},
	urldate = {2021-09-14},
	journal = {IEEE Software},
	author = {Whittle, Jon and Hutchinson, John and Rouncefield, Mark},
	month = may,
	year = {2014},
	pages = {79--85},
}

@article{hutchinson_model-driven_nodate,
	title = {Model-driven engineering practices in industry},
	abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some ‘lessons learned’, in particular the importance of complex organizational, managerial and social factors – as opposed to simple technical factors – in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
	language = {en},
	author = {Hutchinson, John and Rouncefield, Mark and Whittle, Jon},
	pages = {10},
}

@inproceedings{hutchinson_model-driven_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Model-driven engineering practices in industry},
	isbn = {978-1-4503-0445-0},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985882},
	doi = {10.1145/1985793.1985882},
	abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some ‘lessons learned’, in particular the importance of complex organizational, managerial and social factors – as opposed to simple technical factors – in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
	language = {en},
	urldate = {2021-09-14},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Hutchinson, John and Rouncefield, Mark and Whittle, Jon},
	month = may,
	year = {2011},
	pages = {633--642},
}

@article{mashkoor_evaluating_2018,
	title = {Evaluating the suitability of state-based formal methods for industrial deployment},
	volume = {48},
	issn = {1097-024X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2634},
	doi = {10.1002/spe.2634},
	abstract = {After a number of success stories in safety-critical domains, we are starting to witness applications of formal methods in contemporary systems and software engineering. However, one thing that is still missing is the evaluation criteria that help software practitioners choose the right formal method for the problem at hand. In this paper, we present the criteria for evaluating and comparing different formal methods. The criteria were chosen through a literature review, discussions with experts from academia and practitioners from industry, and decade-long personal experience with the application of formal methods in industrial and academic projects. The criteria were then evaluated on several model-oriented state-based formal methods. Our research shows that besides technical grounds (eg, modeling capabilities and supported development phases), formal methods should also be evaluated from social and industrial perspectives. We also found out that it is not possible to generate a matrix that renders the selection of the right formal method an automatic process. However, we can generate several pointers, which make this selection process a lot less cumbersome.},
	language = {en},
	number = {12},
	urldate = {2021-09-14},
	journal = {Software: Practice and Experience},
	author = {Mashkoor, Atif and Kossak, Felix and Egyed, Alexander},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2634},
	keywords = {evaluation criteria, formal methods},
	pages = {2350--2379},
}

@article{newcombe_how_2015,
	title = {How {Amazon} web services uses formal methods},
	volume = {58},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2699417},
	doi = {10.1145/2699417},
	abstract = {Engineers use TLA+ to prevent serious but subtle bugs from reaching production.},
	language = {en},
	number = {4},
	urldate = {2021-09-14},
	journal = {Communications of the ACM},
	author = {Newcombe, Chris and Rath, Tim and Zhang, Fan and Munteanu, Bogdan and Brooker, Marc and Deardeuff, Michael},
	month = mar,
	year = {2015},
	pages = {66--73},
}

@inproceedings{ponsard_assessment_2018,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Assessment of {Emerging} {Standards} for {Safety} and {Security} {Co}-{Design} on a {Railway} {Case} {Study}},
	isbn = {978-3-030-02852-7},
	doi = {10.1007/978-3-030-02852-7_12},
	abstract = {Design for safety-critical software intended for domains like transportation or medical systems is known to be difficult but is required to give a sufficient level of assurance that the system will not harm or kill people. To add to the difficulty, systems have now become highly connected and are turning into cyber-physical systems. This results in the need to address intentional cyber security threats on top of risks related to unintentional software defects. Different approaches are being defined to co-engineer both software security and safety in a consistent way. This paper aims at providing a deeper understanding of those approaches and the evolution of related standards by analysing them using a sound goal-oriented framework that can model both kind of properties and also reason on them in a risk-oriented way. In the process interesting co-design patterns are also identified and discussed. The approach is driven by a real world open specification from the railways.},
	language = {en},
	booktitle = {New {Trends} in {Model} and {Data} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Ponsard, Christophe and Grandclaudon, Jeremy and Massonet, Philippe and Touzani, Mounir},
	editor = {Abdelwahed, El Hassan and Bellatreche, Ladjel and Benslimane, Djamal and Golfarelli, Matteo and Jean, Stéphane and Mery, Dominique and Nakamatsu, Kazumi and Ordonez, Carlos},
	year = {2018},
	keywords = {Co-design, Cyber security, Goals, Rolls Royce-Fall21, Safety, Standards, Threats},
	pages = {130--145},
}

@article{suo_merging_2018,
	title = {Merging safety and cybersecurity analysis in product design},
	volume = {12},
	issn = {1751-9578},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-its.2018.5323},
	doi = {10.1049/iet-its.2018.5323},
	abstract = {When developing cyber-physical systems such as automated vehicles, safety and cybersecurity analyses are often conducted separately. However, unlike in the IT world, safety hazards and cybersecurity threats converge in cyber-physical systems; a malicious party can exploit cyber-threats to create extremely hazardous situations, whether in autonomous vehicles or nuclear plants. The authors propose a framework for integrated system-level analyses for functional safety and cyber security. They present a generic model named Threat Identification and Refinement for Cyber-Physical Systems (TIRCPS) extending Microsoft's six classes of threat modelling including Spoofing, Tampering, Repudiation, Information Disclosure, Denial-of-Service and Elevation Privilege. TIRCPS introduces three benefits of developing complex systems: first, it allows the refinement of abstract threats into specific ones as physical design information becomes available. Second, the approach provides support for constructing attack trees with traceability from high-level goals and hazardous events (HEs) to threats. Third, TIRCPS formalises the definition of threats such that intelligent tools can be built to automatically detect most of a system's vulnerable components requiring protection. They present a case study on an automated-driving system to illustrate the proposed approach. The analysis results of a hierarchical attack tree with cyber-threats traceable to high-level HEs are used to design mitigation solutions.},
	language = {en},
	number = {9},
	urldate = {2021-09-13},
	journal = {IET Intelligent Transport Systems},
	author = {Suo, Dajiang and Siegel, Joshua E. and Sarma, Sanjay E.},
	year = {2018},
	note = {\_eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-its.2018.5323},
	keywords = {Microsoft, Rolls Royce-Fall21, TIRCPS, abstract threats refinement, automated vehicles, automated-driving system, autonomous vehicles, cyber-physical systems, cyber-threats, cybersecurity analysis, denial-of-service, elevation privilege, extremely hazardous situations, functional safety analysis, hazardous events, hazards, hierarchical attack tree, information disclosure, intelligent tools, malicious party, nuclear plants, physical design information, product design, repudiation, security of data, spoofing, system-level analysis, tampering, threat identification-and-refinement-for-cyber-physical systems, threat modelling},
	pages = {1103--1109},
}

@techreport{faganIoTDeviceCybersecurity2020,
	address = {Gaithersburg, MD},
	title = {{IoT} device cybersecurity capability core baseline},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8259A.pdf},
	abstract = {Device cybersecurity capabilities are cybersecurity features or functions that computing devices provide through their own technical means (i.e., device hardware and software). This publication defines an Internet of Things (IoT) device cybersecurity capability core baseline, which is a set of device capabilities generally needed to support common cybersecurity controls that protect an organization’s devices as well as device data, systems, and ecosystems. The purpose of this publication is to provide organizations a starting point to use in identifying the device cybersecurity capabilities for new IoT devices they will manufacture, integrate, or acquire. This publication can be used in conjunction with NISTIR 8259, Foundational Cybersecurity Activities for IoT Device Manufacturers.},
	language = {en},
	number = {NIST IR 8259A},
	urldate = {2021-03-29},
	institution = {National Institute of Standards and Technology},
	author = {Fagan, Michael and Megas, Katerina N and Scarfone, Karen and Smith, Matthew},
	month = may,
	year = {2020},
	doi = {10.6028/NIST.IR.8259a},
	pages = {NIST IR 8259A},
}

@techreport{boecklConsiderationsManagingInternet2019,
	address = {Gaithersburg, MD},
	title = {Considerations for managing {Internet} of {Things} ({IoT}) cybersecurity and privacy risks},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8228.pdf},
	abstract = {The Internet of Things (IoT) is a rapidly evolving and expanding collection of diverse technologies that interact with the physical world. Many organizations are not necessarily aware of the large number of IoT devices they are already using and how IoT devices may affect cybersecurity and privacy risks differently than conventional information technology (IT) devices do. The purpose of this publication is to help federal agencies and other organizations better understand and manage the cybersecurity and privacy risks associated with their individual IoT devices throughout the devices’ lifecycles. This publication is the introductory document providing the foundation for a planned series of publications on more specific aspects of this topic.},
	language = {en},
	number = {NIST IR 8228},
	urldate = {2021-03-29},
	institution = {National Institute of Standards and Technology},
	author = {Boeckl, Katie and Fagan, Michael and Fisher, William and Lefkovitz, Naomi and Megas, Katerina N and Nadeau, Ellen and O'Rourke, Danna Gabel and Piccarreta, Ben and Scarfone, Karen},
	month = jun,
	year = {2019},
	doi = {10.6028/NIST.IR.8228},
	pages = {NIST IR 8228},
}

@book{paulitsch_non-functional_2008,
	title = {Non-functional {Avionics} {Requirements}},
	volume = {17},
	abstract = {Embedded systems in aerospace become more and more integrated in order to reduce weight, volume/size, and power of hardware for more fuel-effi ciency. Such integration tendencies change architectural approaches of system ar chi tec tures, which subsequently change non-functional requirements for plat forms. This paper provides some insight into state-of-the-practice of non-func tional requirements for developing ultra-critical embedded systems in the aero space industry, including recent changes and trends. In particular, formal requi re ment capture and formal analysis of non-functional requirements of avionic systems - including hard-real time, fault-tolerance, reliability, and per for mance - are exemplified by means of recent developments in SAL and HiLiTE.},
	author = {Paulitsch, Michael and Ruess, Harald and Sorea, Maria},
	month = oct,
	year = {2008},
	doi = {10.1007/978-3-540-88479-8_26},
	note = {Journal Abbreviation: Communications in Computer and Information Science
Pages: 384
Publication Title: Communications in Computer and Information Science},
}

@incollection{rayadurgam_improving_2016,
	address = {Cham},
	title = {Improving an {Industrial} {Test} {Generation} {Tool} {Using} {SMT} {Solver}},
	volume = {9690},
	isbn = {978-3-319-40647-3 978-3-319-40648-0},
	url = {http://link.springer.com/10.1007/978-3-319-40648-0_8},
	abstract = {We present an SMT solving based test generation approach for MATLAB Simulink designs, implemented in the HiLiTE tool developed by Honeywell for veriﬁcation of avionic systems. The test requirements for a Simulink model are represented by a set of behavioral equivalence classes for each block in the model, in terms of its input(s) and output. A unique feature of our approach is that the equivalence class deﬁnitions, as well as the upstream subgraph of a block under test, are translated as constraints into SMT expressions. An SMT solver is called at the back-end of HiLiTE to ﬁnd a satisﬁable solution that is further augmented into an end-to-end test case at the model level.},
	language = {en},
	urldate = {2021-09-13},
	booktitle = {{NASA} {Formal} {Methods}},
	publisher = {Springer International Publishing},
	author = {Ren, Hao and Bhatt, Devesh and Hvozdovic, Jan},
	editor = {Rayadurgam, Sanjai and Tkachuk, Oksana},
	year = {2016},
	doi = {10.1007/978-3-319-40648-0_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {100--106},
}

@article{bhatt_effective_nodate,
	title = {Effective {Verification} of {Flight} {Critical} {Software} {Systems}: {Issues} and {Approaches}},
	language = {en},
	author = {Bhatt, Devesh and Schloegel, Kirk},
	pages = {4},
}

@misc{barnat_aerospace_nodate,
	title = {Aerospace {Advanced} {Technology} {Europe}},
	abstract = {Abstract. Embedded systems have become an inevitable part of control systems in many industrial domains including avionics. The nature of this domain traditionally requires the highest possible degree of system availability and integrity. While embedded systems have become extremely complex and they have been continuously replacing legacy mechanical components, the amount of defects of hardware and software has to be kept to absolute minimum to avoid casualties and material damages. Despite the above-mentioned facts, significant improvements are still required in the validation and verification processes accompanying embedded systems development. In this paper we report on integration of a parallel, explicit-state LTL model checker (DIVINE) and a tool for requirements-based verification of aerospace system components (HiLiTE, a tool implemented and used by Honeywell). HiLiTE and the proposed partial toolchain use MATLAB Simulink/Stateflow as the primary design language. The work has been conducted within the Artemis project industrial Framework for Embedded Systems Tools (iFEST). 1},
	author = {Barnat, J. and Beran, J. and Brim, L. and Kratochvíla, T. and Ročkai, P.},
}

@incollection{newell_mechanisms_1981,
	address = {Hillsdale, NJ},
	title = {Mechanisms of {Skill} {Acquisition} and the {Law} of {Practice}},
	booktitle = {Cognitive {Skills} and {Their} {Acquisition}},
	publisher = {Lawrence Erlbaum Associates, Inc.},
	author = {Newell, A. and Rosenbloom, P. S.},
	editor = {Anderson, J. R.},
	year = {1981},
	note = {Section: 1},
	pages = {1--51},
}

@article{samuel_studies_1959,
	title = {Some {Studies} in {Machine} {Learning} {Using} the {Game} of {Checkers}},
	volume = {3},
	number = {3},
	journal = {IBM Journal of Research and Development},
	author = {Samuel, A. L.},
	year = {1959},
	pages = {211--229},
}

@misc{author_suppressed_2021,
	title = {Suppressed for {Anonymity}},
	author = {Author, N. N.},
	year = {2021},
}

@book{michalski_machine_1983,
	address = {Palo Alto, CA},
	title = {Machine {Learning}: {An} {Artificial} {Intelligence} {Approach}, {Vol}. {I}},
	publisher = {Tioga},
	editor = {Michalski, R. S. and Carbonell, J. G. and Mitchell, T. M.},
	year = {1983},
}

@phdthesis{kearns_computational_1989,
	type = {{PhD} {Thesis}},
	title = {Computational {Complexity} of {Machine} {Learning}},
	school = {Department of Computer Science, Harvard University},
	author = {Kearns, M. J.},
	year = {1989},
}

@inproceedings{langley_crafting_2000,
	address = {Stanford, CA},
	title = {Crafting {Papers} on {Machine} {Learning}},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Machine} {Learning} ({ICML} 2000)},
	publisher = {Morgan Kaufmann},
	author = {Langley, P.},
	editor = {Langley, Pat},
	year = {2000},
	pages = {1207--1216},
}

@book{duda_pattern_2000,
	edition = {2nd},
	title = {Pattern {Classification}},
	publisher = {John Wiley and Sons},
	author = {Duda, R. O. and Hart, P. E. and Stork, D. G.},
	year = {2000},
}

@techreport{mitchell_need_1980,
	address = {New Brunswick, MA},
	title = {The {Need} for {Biases} in {Learning} {Generalizations}},
	institution = {Computer Science Department, Rutgers University},
	author = {Mitchell, T. M.},
	year = {1980},
}

@article{noauthor_exploiting_2022,
	title = {Exploiting {Input} {Sanitization} for {Regex} {Denial} of {Service}},
	language = {en},
	year = {2022},
	pages = {12},
}

@article{leitner_mixed-method_2019,
	title = {A mixed-method empirical study of {Function}-as-a-{Service} software development in industrial practice},
	volume = {149},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121218302735},
	doi = {10.1016/j.jss.2018.12.013},
	abstract = {Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of “serverless” computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the “glue” that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers.},
	language = {en},
	urldate = {2021-09-11},
	journal = {Journal of Systems and Software},
	author = {Leitner, Philipp and Wittern, Erik and Spillner, Josef and Hummer, Waldemar},
	month = mar,
	year = {2019},
	keywords = {Cloud computing, Empirical research, Function-as-a-Service, Serverless},
	pages = {340--359},
}

@inproceedings{datta_valve_2020,
	address = {Taipei Taiwan},
	title = {Valve: {Securing} {Function} {Workflows} on {Serverless} {Computing} {Platforms}},
	isbn = {978-1-4503-7023-3},
	shorttitle = {Valve},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380173},
	doi = {10.1145/3366423.3380173},
	abstract = {Serverless Computing has quickly emerged as a dominant cloud computing paradigm, allowing developers to rapidly prototype eventdriven applications using a composition of small functions that each perform a single logical task. However, many such application workflows are based in part on publicly-available functions developed by third-parties, creating the potential for functions to behave in unexpected, or even malicious, ways. At present, developers are not in total control of where and how their data is flowing, creating significant security and privacy risks in growth markets that have embraced serverless (e.g., IoT).},
	language = {en},
	urldate = {2021-09-11},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Datta, Pubali and Kumar, Prabuddha and Morris, Tristan and Grace, Michael and Rahmati, Amir and Bates, Adam},
	month = apr,
	year = {2020},
	pages = {939--950},
}

@book{esposito_pervasive_2019,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Pervasive {Systems}, {Algorithms} and {Networks}: 16th {International} {Symposium}, {I}-{SPAN} 2019, {Naples}, {Italy}, {September} 16-20, 2019, {Proceedings}},
	volume = {1080},
	isbn = {978-3-030-30142-2 978-3-030-30143-9},
	shorttitle = {Pervasive {Systems}, {Algorithms} and {Networks}},
	url = {http://link.springer.com/10.1007/978-3-030-30143-9},
	language = {en},
	urldate = {2021-09-09},
	publisher = {Springer International Publishing},
	editor = {Esposito, Christian and Hong, Jiman and Choo, Kim-Kwang Raymond},
	year = {2019},
	doi = {10.1007/978-3-030-30143-9},
}

@book{esposito_pervasive_2019-1,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Pervasive {Systems}, {Algorithms} and {Networks}: 16th {International} {Symposium}, {I}-{SPAN} 2019, {Naples}, {Italy}, {September} 16-20, 2019, {Proceedings}},
	volume = {1080},
	isbn = {978-3-030-30142-2 978-3-030-30143-9},
	shorttitle = {Pervasive {Systems}, {Algorithms} and {Networks}},
	url = {http://link.springer.com/10.1007/978-3-030-30143-9},
	language = {en},
	urldate = {2021-09-09},
	publisher = {Springer International Publishing},
	editor = {Esposito, Christian and Hong, Jiman and Choo, Kim-Kwang Raymond},
	year = {2019},
	doi = {10.1007/978-3-030-30143-9},
}

@incollection{fathoni_performance_2019,
	title = {Performance {Comparison} of {Lightweight} {Kubernetes} in {Edge} {Devices}},
	isbn = {978-3-030-30142-2},
	abstract = {Traditional cloud computing has the challenge to serve many clients with many services. Spreading the services to across of edge server will reduce the load of traditional cloud computing. Kubernetes is one of the platforms used for cloud management. Kubernetes helps to deploy, and scaling the application. Nowadays, a lot of communities build a lightweight Kubernetes than suitable for edge device such as Raspberry Pi. This paper Investigate the performance of Kubernetes lightweight that installed in the Raspberry Pi.},
	author = {Fathoni, Halim and Yang, Chao-Tung and Chang, Chih-Hung and Huang, Chin-Yin},
	month = nov,
	year = {2019},
	doi = {10.1007/978-3-030-30143-9_25},
	pages = {304--309},
}

@book{esposito_pervasive_2019-2,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Pervasive {Systems}, {Algorithms} and {Networks}: 16th {International} {Symposium}, {I}-{SPAN} 2019, {Naples}, {Italy}, {September} 16-20, 2019, {Proceedings}},
	volume = {1080},
	isbn = {978-3-030-30142-2 978-3-030-30143-9},
	shorttitle = {Pervasive {Systems}, {Algorithms} and {Networks}},
	url = {http://link.springer.com/10.1007/978-3-030-30143-9},
	language = {en},
	urldate = {2021-09-09},
	publisher = {Springer International Publishing},
	editor = {Esposito, Christian and Hong, Jiman and Choo, Kim-Kwang Raymond},
	year = {2019},
	doi = {10.1007/978-3-030-30143-9},
}

@article{alam_orchestration_2018-1,
	title = {Orchestration of {Microservices} for {IoT} {Using} {Docker} and {Edge} {Computing}},
	volume = {56},
	issn = {1558-1896},
	doi = {10.1109/MCOM.2018.1701233},
	abstract = {The world of connected devices has led to the rise of the Internet of Things paradigm, where applications rely on multiple devices, gathering and sharing data across highly heterogeneous networks. The variety of possible mechanisms, protocols, and hardware has become a hindrance in the development of architectures capable of addressing the most common IoT use cases, while abstracting services from the underlying communication subsystem. Moreover, the world is moving toward new strict requirements in terms of timeliness and low latency in combination with ultra-high availability and reliability. Thus, future IoT architectures will also have to support the requirements of these cyber-physical applications. In this regard, edge computing has been presented as one of the most promising solutions, relying on the cooperation of nodes by moving services directly to end devices and caching information locally. Therefore, in this article, we propose a modular and scalable architecture based on lightweight virtualization. The provided modularity, combined with the orchestration supplied by Docker, simplifies management and enables distributed deployments, creating a highly dynamic system. Moreover, characteristics such as fault tolerance and system availability are achieved by distributing the application logic across different layers, where failures of devices and micro-services can be masked by this natively redundant architecture, with minimal impact on the overall system performance. Experimental results have validated the implementation of the proposed architecture for on-demand services deployment across different architecture layers.},
	number = {9},
	journal = {IEEE Communications Magazine},
	author = {Alam, Muhammad and Rufino, Joao and Ferreira, Joaquim and Ahmed, Syed Hassan and Shah, Nadir and Chen, Yuanfang},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Communications Magazine},
	keywords = {Cloud computing, Computer architecture, Edge computing, Fault tolerance, Logic gates, Topology, Virtualization},
	pages = {118--123},
}

@inproceedings{bandeira_we_2019,
	title = {We {Need} to {Talk} {About} {Microservices}: an {Analysis} from the {Discussions} on {StackOverflow}},
	shorttitle = {We {Need} to {Talk} {About} {Microservices}},
	doi = {10.1109/MSR.2019.00051},
	abstract = {Microservices are a new and rapidly growing architectural model aimed at developing highly scalable software solutions based on independently deployable and evolvable components. Due to its novelty, microservice-related discussions are increasing in Q\&A websites, such as StackOverflow (SO). In order to understand what is being discussed by the microservice community, this work has applied mining techniques and topic modelling to a manually-curated dataset of 1,043 microservice-related posts from StackOverflow. As a result, we found that 13.68\% of microservice technical posts on SO discuss a single technology: Netflix Eureka. Moreover, buzzwords in the microservice ecosystem, e.g., blue/green deployment, were not identified as relevant subjects of discussion on SO. Finally, we show how a high discussion rate on SO may not reflect the popularity of a certain subject within the microservice community.},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Bandeira, Alan and Medeiros, Carlos Alberto and Paixao, Matheus and Maia, Paulo Henrique},
	month = may,
	year = {2019},
	note = {ISSN: 2574-3864},
	keywords = {Authentication, Biological system modeling, Cloud computing, Computer architecture, Fault tolerance, Manuals, Microservices, StackOverflow, Topic Modelling, Tools},
	pages = {255--259},
}

@inproceedings{qu_experimental_2020-1,
	title = {An {Experimental} {Study} on {Microservices} based {Edge} {Computing} {Platforms}},
	doi = {10.1109/INFOCOMWKSHPS50562.2020.9163068},
	abstract = {The rapid technological advances in the Internet of Things (IoT) allows the blueprint of Smart Cities to become feasible by integrating heterogeneous cloud/fog/edge computing paradigms to collaboratively provide variant smart services in our cities and communities. Thanks to attractive features like fine granularity and loose coupling, the microservices architecture has been proposed to provide scalable and extensible services in large scale distributed IoT systems. Recent studies have evaluated and analyzed the performance interference between microservices based on scenarios on the cloud computing environment. However, they are not holistic for IoT applications given the restriction of the edge device like computation consumption and network capacity. This paper investigates multiple microservice deployment policies on edge computing platform. The microservices are developed as docker containers, and comprehensive experimental results demonstrate the performance and interference of microservices running on benchmark scenarios.},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications} {Workshops} ({INFOCOM} {WKSHPS})},
	author = {Qu, Qian and Xu, Ronghua and Nikouei, Seyed Yahya and Chen, Yu},
	month = jul,
	year = {2020},
	keywords = {Benchmark testing, Cloud computing, Computer architecture, Container, Containers, Edge Computing, Edge computing, Internet of Things (IoT), Microservices Architecture, Performance evaluation, Service-oriented architecture},
	pages = {836--841},
}

@article{li_understanding_2021,
	title = {Understanding and addressing quality attributes of microservices architecture: {A} {Systematic} literature review},
	volume = {131},
	issn = {0950-5849},
	shorttitle = {Understanding and addressing quality attributes of microservices architecture},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584920301993},
	doi = {10.1016/j.infsof.2020.106449},
	abstract = {Context: As a rapidly adopted architectural style in software engineering, Microservices Architecture (MSA) advocates implementing small-scale and independently distributed services, rather than binding all functions into one monolith. Although many initiatives have contributed to the quality improvement of microservices-based systems, there is still a lack of a systematic understanding of the Quality Attributes (QAs) associated with MSA. Objective: This study aims to investigate the evidence-based state-of-the-art of QAs of microservices-based systems. Method: We carried out a Systematic Literature Review (SLR) to identify and synthesize the relevant studies that report evidence related to QAs of MSA. Results: Based on the data extracted from the 72 selected primary studies, we portray an overview of the six identified QAs most concerned in MSA, scalability, performance, availability, monitorability, security, and testability. We identify 19 tactics that architecturally address the critical QAs in MSA, including two tactics for scalability, four for performance, four for availability, four for monitorability, three for security, and two for testability. Conclusion: This SLR concludes that for MSA-based systems: 1) Although scalability is the commonly acknowledged benefit of MSA, it is still an indispensable concern among the identified QAs, especially when trading-off with other QAs, e.g., performance. Apart from the six identified QAs in this study, other QAs for MSA like maintainability need more attention for effective improvement and evaluation in the future. 3) Practitioners need to carefully make the decision of migrating to MSA based on the return on investment, since this architectural style additionally cause some pains in practice.},
	language = {en},
	urldate = {2021-09-09},
	journal = {Information and Software Technology},
	author = {Li, Shanshan and Zhang, He and Jia, Zijia and Zhong, Chenxing and Zhang, Cheng and Shan, Zhihao and Shen, Jinfeng and Babar, Muhammad Ali},
	month = mar,
	year = {2021},
	keywords = {Microservices, Monolith, Quality attributes, Systematic literature review},
	pages = {106449},
}

@article{mendonca_developing_2021,
	title = {Developing {Self}-{Adaptive} {Microservice} {Systems}: {Challenges} and {Directions}},
	volume = {38},
	issn = {1937-4194},
	shorttitle = {Developing {Self}-{Adaptive} {Microservice} {Systems}},
	doi = {10.1109/MS.2019.2955937},
	abstract = {A self-adaptive system can dynamically monitor and adapt its behavior to preserve and enhance its quality attributes under uncertain operating conditions. This article identifies key challenges for the development of microservice applications as self-adaptive systems, using a cloud-based intelligent video-surveillance application as a motivating example. It also suggests potential new directions for addressing most of the identified challenges by leveraging existing microservice practices and technologies.},
	number = {2},
	journal = {IEEE Software},
	author = {Mendonca, Nabor C. and Jamshidi, Pooyan and Garlan, David and Pahl, Claus},
	month = mar,
	year = {2021},
	note = {Conference Name: IEEE Software},
	keywords = {Computer architecture, Containers, DevOps, Face recognition, Pipelines, Self-adaptive systems, Streaming media, Video surveillance, continuous delivery, microservices},
	pages = {70--79},
}

@article{zhou_fault_2021,
	title = {Fault {Analysis} and {Debugging} of {Microservice} {Systems}: {Industrial} {Survey}, {Benchmark} {System}, and {Empirical} {Study}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {Fault {Analysis} and {Debugging} of {Microservice} {Systems}},
	doi = {10.1109/TSE.2018.2887384},
	abstract = {The complexity and dynamism of microservice systems pose unique challenges to a variety of software engineering tasks such as fault analysis and debugging. In spite of the prevalence and importance of microservices in industry, there is limited research on the fault analysis and debugging of microservice systems. To fill this gap, we conduct an industrial survey to learn typical faults of microservice systems, current practice of debugging, and the challenges faced by developers in practice. We then develop a medium-size benchmark microservice system (being the largest and most complex open source microservice system within our knowledge) and replicate 22 industrial fault cases on it. Based on the benchmark system and the replicated fault cases, we conduct an empirical study to investigate the effectiveness of existing industrial debugging practices and whether they can be further improved by introducing the state-of-the-art tracing and visualization techniques for distributed systems. The results show that the current industrial practices of microservice debugging can be improved by employing proper tracing and visualization techniques and strategies. Our findings also suggest that there is a strong need for more intelligent trace analysis and visualization, e.g., by combining trace visualization and improved fault localization, and employing data-driven and learning-based recommendation for guided visual exploration and comparison of traces.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Ji, Chao and Li, Wenhai and Ding, Dan},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Benchmark testing, Companies, Computer architecture, Debugging, Industries, Microservices, Runtime, Visualization, debugging, fault localization, tracing, visualization},
	pages = {243--260},
}

@article{zhou_fault_2021-1,
	title = {Fault {Analysis} and {Debugging} of {Microservice} {Systems}: {Industrial} {Survey}, {Benchmark} {System}, and {Empirical} {Study}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {Fault {Analysis} and {Debugging} of {Microservice} {Systems}},
	doi = {10.1109/TSE.2018.2887384},
	abstract = {The complexity and dynamism of microservice systems pose unique challenges to a variety of software engineering tasks such as fault analysis and debugging. In spite of the prevalence and importance of microservices in industry, there is limited research on the fault analysis and debugging of microservice systems. To fill this gap, we conduct an industrial survey to learn typical faults of microservice systems, current practice of debugging, and the challenges faced by developers in practice. We then develop a medium-size benchmark microservice system (being the largest and most complex open source microservice system within our knowledge) and replicate 22 industrial fault cases on it. Based on the benchmark system and the replicated fault cases, we conduct an empirical study to investigate the effectiveness of existing industrial debugging practices and whether they can be further improved by introducing the state-of-the-art tracing and visualization techniques for distributed systems. The results show that the current industrial practices of microservice debugging can be improved by employing proper tracing and visualization techniques and strategies. Our findings also suggest that there is a strong need for more intelligent trace analysis and visualization, e.g., by combining trace visualization and improved fault localization, and employing data-driven and learning-based recommendation for guided visual exploration and comparison of traces.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Ji, Chao and Li, Wenhai and Ding, Dan},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Benchmark testing, Companies, Computer architecture, Debugging, Industries, Microservices, Runtime, Visualization, debugging, fault localization, tracing, visualization},
	pages = {243--260},
}

@article{janusz_zalewski_trends_2019,
	title = {Trends and challenges in the aviation systems safety and cybersecurity},
	volume = {23},
	issn = {14286394},
	url = {https://doi.org/10.17466/tq2019/23.2/a},
	doi = {10.17466/tq2019/23.2/a},
	abstract = {Aviation systems are an essential component of every nation’s critical infrastructure. Considering millions of passengers flying per year and busy airports, the safe and secure flight and traffic operation is of primary importance to the proper functioning of the society. This paper discusses fundamental problems of providing critical systems safety and cybersecurity in the aviation infrastructure including both airborne and ground systems such as avionics, navigation, air traffic control and management, as well as unmanned systems. It reviews the major challenges and current trends in providing viable solutions. Both industrial practices and research approaches are mentioned, including established methodologies and standards, as well as new developments in certification.},
	language = {en},
	number = {2},
	urldate = {2021-09-08},
	journal = {TASK Quarterly},
	author = {{Janusz Zalewski} and {Andrew Kornecki}},
	year = {2019},
	pages = {159--175},
}

@inproceedings{valdes_aviation_2017,
	address = {Rome, Italy},
	title = {{AVIATION} 4.0: {MORE} {SAFETY} {THROUGH} {AUTOMATION} {AND} {DIGITIZATION}},
	shorttitle = {{AVIATION} 4.0},
	url = {http://library.witpress.com/viewpaper.asp?pcode=SAFE17-021-1},
	doi = {10.2495/SAFE170211},
	abstract = {The whole world is talking about Industry 4.0 or the fourth industrial revolution. That is the current trend of higher levels of automation, digitalization and data exchange in manufacturing technologies. It includes cyber-physical systems, the Internet of Things and cloud computing among others technological assets. With more than 5000 sensors, which generate up to 10 GB of data per second, new modern aircraft engines are a clearly exponent of what digitalization and the internet of aircraft things could furnish, as part of the upcoming Industry 4.0 revolution, in the aviation industry. Called Aviation 4.0, this new era has the potential to help to improve all key performance areas of air transport. Particularly, in an industry where safety levels are so high, and the margins for improvement are extremely tight, this upcoming era might imply a paradigm shift opportunity in safety improvement. In an attempt to define Aviation 4.0 this paper discusses the stages of aviation development from basic VFR flight rules at the Aviation 1.0, up to Aviation 4.0 stage where cyber-physical systems will be designed to assist humans physically tireless, unkind or hazardous work and to take decisions and to complete tasks autonomously. The paper illustrates current real cases of application of Aviation 4.0 concept to increase aviation safety and introduces future possible applications while outlines how they might significantly increase safety levels in aviation.},
	language = {en},
	urldate = {2021-09-08},
	author = {Valdes, Rosa Arnaldo and Comendador, Victor Fernando Gómez},
	month = jun,
	year = {2017},
	pages = {225--236},
}

@inproceedings{pate_amelia_2018,
	title = {{AMELIA}: {An} application of the {Internet} of {Things} for aviation safety},
	shorttitle = {{AMELIA}},
	doi = {10.1109/CCNC.2018.8319163},
	abstract = {This paper presents AMELIA: Aircraft Monitoring and Electronically Linked Instantaneous Analytics as an application of the Internet of Things (IoT) for aviation safety - a safety-critical use-case - from an edge computing perspective. AMELIA is a multi-layered edge computing system that automatically detects aircraft emergencies, and only transmits relevant data and information to enable quicker and more efficient response to emergencies. We describe a prototype of AMELIA to illustrate, explore, and motivate the potentials of the IoT for aviation safety, and lay a foundation for the design of diverse high-impact edge computing systems on the IoT.},
	booktitle = {2018 15th {IEEE} {Annual} {Consumer} {Communications} {Networking} {Conference} ({CCNC})},
	author = {Pate, Jeremiah and Adegbija, Tosiron},
	month = jan,
	year = {2018},
	note = {ISSN: 2331-9860},
	keywords = {Aerospace electronics, Aircraft, Edge computing, Internet of Things, IoT prototype, Monitoring, Prototypes, Safety, Sensors, automatic control, aviation safety, edge computing},
	pages = {1--6},
}

@article{hamzehloui_study_2019,
	title = {A {Study} on the {Most} {Prominent} {Areas} of {Research} in {Microservices}},
	volume = {9},
	doi = {10.18178/ijmlc.2019.9.2.793},
	abstract = {Microservices have recently gained a lot of attention in the software industry. Their modularity and smaller size offer flexibility advantageous to both development and operational teams. However, the bigger picture is still lacking despite numerous researches on microservices. There are few aspects of microservices that have never been discussed in depth despite being acknowledged repeatedly. The current research is the continuation of our previous paper, "A systematic mapping on microservices". In the named paper we have identified the focus areas of microservices' researches. Along with our previous findings we have spotted several crucial key points that require further discussions. These includes: definition of microservices, their sizes and boundaries. We have also explored the relationship of microservices with SOA and DDD. These are the two terms that are frequently associated with microservices. Finally, we have discussed DevOps, cloud and virtualization as three of the most essential factors in microservices ecosystem. We attempted to clarify the role of each of these factors. Based on our findings, there is still no standardized definition for microservices to-date. In absence of clear guidelines, SOA and DDD concepts are widely being used to develop microservices. DevOps practices together with the cloud environment are playing an important role in facilitating the implementation of microservices. We have also identified containerization as an effective method to overcome the hardware limitation besides speeding up the delivery process. © 2019, International Association of Computer Science and Information Technology.},
	journal = {International Journal of Machine Learning and Computing},
	author = {Hamzehloui, Mohammad and Sahibuddin, Shamsul and Ashabi, Ardavan},
	month = apr,
	year = {2019},
	pages = {242--247},
}

@article{bowen_safety-critical_1993,
	title = {Safety-{Critical} {Systems}, {Formal} {Methods} and {Standards}},
	volume = {8},
	abstract = {Standards concerned with the development of safety-critical systems, and the software in such systems in particular, abound today as the software crisis increasingly affects the world of embedded computer-based systems. The use of formal methods is often advocated as a way of increasing confidence in such systems. This paper examines the industrial use of these techniques, the recommendations concerning formal methods in a number of current and draft standards, and comments on the applicability and problems of using formal methods for the development of safety-critical systems of an industrial scale. Some possible future directions are suggested.},
	journal = {Software Engineering Journal},
	author = {Bowen, Jonathan P. and Stavridou, Victoria},
	year = {1993},
	pages = {189--209},
}

@inproceedings{campeanu_mapping_2018,
	title = {A mapping study on microservice architectures of {Internet} of {Things} and cloud computing solutions},
	doi = {10.1109/MECO.2018.8406008},
	abstract = {Internet of Things is a fairly new paradigm adopted by the industry, which offers the connectivity, via wireless systems, of all the devices that surround us. One of the challenges of IoT relates to the required resources to store and compute the huge amount of data resulted from devices' connections. Cloud computing is a solution to the IoT challenges; it provides on-demand resources in an easy-to-access manner. Another trend in the enterprise world is the usage of microservice architectures. Being a newly developed paradigm, and although its principles are defined, it is difficult to have a vision of the existing microservice-based research solutions. This paper, through the mapping study methodology, provides an overview of the current state-of-the-art and -practice regarding the usage of microservice architectures by IoT and cloud computing solutions. More specifically, we synthesize the data from 364 selected studies and describe the research types, number of publications and their main venues.},
	booktitle = {2018 7th {Mediterranean} {Conference} on {Embedded} {Computing} ({MECO})},
	author = {Campeanu, Gabriel},
	month = jun,
	year = {2018},
	keywords = {Cloud computing, Computer architecture, Embedded computing, Internet of Things, IoT, Libraries, Market research, SLR, Systematics, cloud computing, microservice, systematic mapping study},
	pages = {1--4},
}

@misc{garavel_experiment_1993,
	title = {An {Experiment} with the {LOTOS} {Formal} {Description} {Technique} on the {Flight} {Warning} {Computer} of {Airbus} 330/340 {Aircrafts}},
	abstract = {This paper presents the main results of a two-year study concerning the introduction of  formal methods in the life cycle of avionics software. This study was done in the framework  of the EUREKA European project AIMS (Aerospace Intelligent Management and development  environment for embedded Systems).  The ISO language Lotos was used to describe a significant subset of the Flight Warning  Computer of Airbus 330/340 aircrafts, which is a typical representative of Embedded Computer  Systems. Six Lotos descriptions were developed, (using both the abstract data types and the  process algebra features of Lotos) which are probably among the largest algebraic specifications  written today. The Caesar/Ald' ebaran toolset for Lotos was used to support the description  and analysis process. The Lotos descriptions were automatically translated into executable  prototypes, and then validated by means of simulation and testing. The paper presents the  techniques used and the results obtained. It e...},
	author = {Garavel, Hubert and Hautbois, René-Pierre},
	year = {1993},
}

@misc{noauthor_-178c_2021,
	title = {{DO}-{178C}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=DO-178C&oldid=1032602731},
	abstract = {DO-178C, Software Considerations in Airborne Systems and Equipment Certification is the primary document by which the certification authorities such as FAA, EASA and Transport Canada approve all commercial software-based aerospace systems. The document is published by RTCA, Incorporated, in a joint effort with EUROCAE, and replaces DO-178B. The new document is called DO-178C/ED-12C and was completed in November 2011 and approved by the RTCA in December 2011.  It became available for sale and use in January 2012.The FAA approved AC 20-115C  on 19 Jul 2013, making DO-178C a recognized "acceptable means, but not the only means, for showing compliance with the applicable airworthiness regulations for the software aspects of airborne systems and equipment certification."},
	language = {en},
	urldate = {2021-09-08},
	journal = {Wikipedia},
	month = jul,
	year = {2021},
	note = {Page Version ID: 1032602731},
}

@misc{noauthor_-178b_2021,
	title = {{DO}-{178B}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=DO-178B&oldid=1042198834},
	abstract = {DO-178B, Software Considerations in Airborne Systems and Equipment Certification is a guideline dealing with the safety of safety-critical software used in certain airborne systems. It was jointly developed by the safety-critical working group RTCA SC-167 of the Radio Technical Commission for Aeronautics (RTCA) and WG-12 of the European Organisation for Civil Aviation Equipment (EUROCAE). RTCA  published the document as RTCA/DO-178B, while EUROCAE published the document as ED-12B. Although technically a guideline, it was a de facto standard for developing avionics software systems until it was replaced in 2012 by DO-178C. 
The Federal Aviation Administration (FAA) applies DO-178B as the document it uses for guidance to determine if the software will perform reliably in an airborne environment, when specified by the Technical Standard Order (TSO) for which certification is sought. In the United States, the introduction of TSOs into the airworthiness certification process, and by extension DO-178B, is explicitly established in  Title 14: Aeronautics and Space of the Code of Federal Regulations (CFR), also known as the Federal Aviation Regulations, Part 21, Subpart O.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Wikipedia},
	month = sep,
	year = {2021},
	note = {Page Version ID: 1042198834},
}

@article{macher_bridging_2014,
	title = {Bridging {Automotive} {Systems}, {Safety} and {Software} {Engineering} with a {Seamless} {Toolchain}},
	abstract = {Multi-core technologies strongly support functional integration, e.g. integration of different applications on the same control unit. However, these applications require different safety concepts with different levels of criticality; and providing consistency of the safety concept during the entire product lifecycle is a tedious task. The aim of this paper is to enhance a modeldriven systems and safety engineering framework for multi-core systems, enabling the seamless description of the system, from requirements at the system level down to software component implementation.},
	language = {en},
	author = {Macher, Georg and Armengaud, Eric and Kreiner, Christian},
	year = {2014},
	pages = {9},
}

@article{lutz_software_2000,
	title = {Software {Engineering} for {Safety}: {A} {Roadmap}},
	abstract = {This report describes the current state of software engineering for safety and proposes some directions for needed work that appears to be achievable in the near future.},
	language = {en},
	author = {Lutz, Robyn},
	year = {2000},
	pages = {11},
}

@incollection{yasuura_security_2017,
	address = {Cham},
	title = {Security and {Privacy} in {IoT} {Era}},
	isbn = {978-3-319-55344-3 978-3-319-55345-0},
	url = {http://link.springer.com/10.1007/978-3-319-55345-0_14},
	abstract = {Trends in miniaturization have resulted in an explosion of small, low power devices with network connectivity. Welcome to the era of Internet of Things (IoT), wearable devices, and automated home and industrial systems. These devices are loaded with sensors, collect information from their surroundings, process it, and relay it to remote locations for further analysis. Pervasive and seeminly harmless, this new breed of devices raise security and privacy concerns. In this chapter, we evaluate the security of these devices from an industry point of view, concentrating on the design ﬂow, and catalogue the types of vulnerabilities we have found. We also present an in-depth evaluation of the Google Nest Thermostat, the Nike+ Fuelband SE Fitness Tracker, the Haier SmartCare home automation system, and the Itron Centron CL200 electric meter. We study and present an analysis of the effects of these compromised devices in an every day setting. We then ﬁnish by discussing design ﬂow enhancements, with security mechanisms that can be efﬁciently added into a device in a comparative way.},
	language = {en},
	urldate = {2021-09-07},
	booktitle = {Smart {Sensors} at the {IoT} {Frontier}},
	publisher = {Springer International Publishing},
	author = {Arias, Orlando and Ly, Kelvin and Jin, Yier},
	editor = {Yasuura, Hiroto and Kyung, Chong-Min and Liu, Yongpan and Lin, Youn-Long},
	year = {2017},
	doi = {10.1007/978-3-319-55345-0_14},
	pages = {351--378},
}

@article{gan_leveraging_2019,
	title = {Leveraging {Deep} {Learning} to {Improve} the {Performance} {Predictability} of {Cloud} {Microservices}},
	url = {http://arxiv.org/abs/1905.00968},
	abstract = {Performance unpredictability is a major roadblock towards cloud adoption, and has performance, cost, and revenue ramifications. Predictable performance is even more critical as cloud services transition from monolithic designs to microservices. Detecting QoS violations after they occur in systems with microservices results in long recovery times, as hotspots propagate and amplify across dependent services. We present Seer, an online cloud performance debugging system that leverages deep learning and the massive amount of tracing data cloud systems collect to learn spatial and temporal patterns that translate to QoS violations. Seer combines lightweight distributed RPC-level tracing, with detailed low-level hardware monitoring to signal an upcoming QoS violation, and diagnose the source of unpredictable performance. Once an imminent QoS violation is detected, Seer notifies the cluster manager to take action to avoid performance degradation altogether. We evaluate Seer both in local clusters, and in large-scale deployments of end-to-end applications built with microservices with hundreds of users. We show that Seer correctly anticipates QoS violations 91\% of the time, and avoids the QoS violation to begin with in 84\% of cases. Finally, we show that Seer can identify application-level design bugs, and provide insights on how to better architect microservices to achieve predictable performance.},
	urldate = {2021-09-07},
	journal = {arXiv:1905.00968 [cs]},
	author = {Gan, Yu and Zhang, Yanqi and Hu, Kelvin and Cheng, Dailun and He, Yuan and Pancholi, Meghna and Delimitrou, Christina},
	month = may,
	year = {2019},
	note = {arXiv: 1905.00968
version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@misc{noauthor_open-source_2019,
	title = {An {Open}-{Source} {Benchmark} {Suite} for {Cloud} and {IoT} {Microservices}},
	url = {https://deepai.org/publication/an-open-source-benchmark-suite-for-cloud-and-iot-microservices},
	abstract = {05/27/19 - Cloud services have recently started undergoing a major shift from monolithic
applications, to graphs of hundreds of loosely-coupl...},
	urldate = {2021-09-07},
	journal = {DeepAI},
	month = may,
	year = {2019},
}

@inproceedings{gan_open-source_2019,
	address = {Providence RI USA},
	title = {An {Open}-{Source} {Benchmark} {Suite} for {Microservices} and {Their} {Hardware}-{Software} {Implications} for {Cloud} \& {Edge} {Systems}},
	isbn = {978-1-4503-6240-5},
	url = {https://dl.acm.org/doi/10.1145/3297858.3304013},
	doi = {10.1145/3297858.3304013},
	language = {en},
	urldate = {2021-09-07},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and He, Yuan and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and Lin, Rick and Liu, Zhongling and Padilla, Jake and Delimitrou, Christina},
	month = apr,
	year = {2019},
	pages = {3--18},
}

@inproceedings{butler_virtual_2004,
	address = {New Orleans, LA, USA},
	title = {Virtual fences for controlling cows},
	isbn = {978-0-7803-8232-9},
	url = {http://ieeexplore.ieee.org/document/1302415/},
	doi = {10.1109/ROBOT.2004.1302415},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation}, 2004. {Proceedings}. {ICRA} '04. 2004},
	publisher = {IEEE},
	author = {Butler, Z. and Corke, P. and Peterson, R. and Rus, D.},
	year = {2004},
	pages = {4429--4436 Vol.5},
}

@incollection{ammari_decade_2014,
	address = {Berlin, Heidelberg},
	title = {A {Decade} of {Wireless} {Sensing} {Applications}: {Survey} and {Taxonomy}},
	isbn = {978-3-642-40008-7 978-3-642-40009-4},
	shorttitle = {A {Decade} of {Wireless} {Sensing} {Applications}},
	url = {http://link.springer.com/10.1007/978-3-642-40009-4_2},
	abstract = {The popularity of low-power wireless sensors increased signiﬁcantly in the last decade, triggering a golden era for wireless sensor network research and development. During the early years of the twenty-ﬁrst century, wireless sensor network applications have evolved from small demonstrations with a lifetime of only a few hours to complete systems made up of hundreds of tiny wireless nodes deployed in a wide variety of settings, ranging from harsh and remote environments to residential buildings and clinical units. This survey gives an overview of the most relevant applications of wireless sensor network applications deployed during the last ten years, and classiﬁes them using a novel taxonomy that aims to help identifying relevant programming constructs and run-time services. With more than 60 applications reviewed, ranging from military and civilian surveillance to tracking systems, from environmental and structural monitoring to home and building automation, from agriculture and industrial settings to health care, this survey will serve as a reference to guide researchers and system designers.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {The {Art} of {Wireless} {Sensor} {Networks}},
	publisher = {Springer Berlin Heidelberg},
	author = {Oppermann, Felix Jonathan and Boano, Carlo Alberto and Römer, Kay},
	editor = {Ammari, Habib M.},
	year = {2014},
	doi = {10.1007/978-3-642-40009-4_2},
	note = {Series Title: Signals and Communication Technology},
	pages = {11--50},
}

@article{ning_cyberentity_2013,
	title = {Cyberentity {Security} in the {Internet} of {Things}},
	volume = {46},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/6475947/},
	doi = {10.1109/MC.2013.74},
	language = {en},
	number = {4},
	urldate = {2021-09-06},
	journal = {Computer},
	author = {Ning, Huansheng and Liu, Hong and Yang, Laurence T.},
	month = apr,
	year = {2013},
	pages = {46--53},
}

@inproceedings{zhou_cloudthings_2013,
	address = {Whistler, BC, Canada},
	title = {{CloudThings}: {A} common architecture for integrating the {Internet} of {Things} with {Cloud} {Computing}},
	isbn = {978-1-4673-6085-2 978-1-4673-6084-5 978-1-4673-6083-8},
	shorttitle = {{CloudThings}},
	url = {http://ieeexplore.ieee.org/document/6581037/},
	doi = {10.1109/CSCWD.2013.6581037},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Proceedings of the 2013 {IEEE} 17th {International} {Conference} on {Computer} {Supported} {Cooperative} {Work} in {Design} ({CSCWD})},
	publisher = {IEEE},
	author = {Zhou, Jiehan and Leppanen, Teemu and Harjula, Erkki and Ylianttila, Mika and Ojala, Timo and Yu, Chen and Jin, Hai},
	month = jun,
	year = {2013},
	pages = {651--657},
}

@inproceedings{buckl_services_2009,
	address = {Bradford, United Kingdom},
	title = {Services to the {Field}: {An} {Approach} for {Resource} {Constrained} {Sensor}/{Actor} {Networks}},
	isbn = {978-1-4244-3999-7},
	shorttitle = {Services to the {Field}},
	url = {http://ieeexplore.ieee.org/document/5136693/},
	doi = {10.1109/WAINA.2009.20},
	abstract = {More and more devices become network enabled and are integrated within one large, distributed system. The serviceoriented paradigm is the main concept to implement this approach and to cope with the heterogeneity of the underlying network. However, resource constraints imposed by the underlying hardware, such as 8-Bit micro controllers, require efﬁcient protocols and often prohibit the use of technologies known from the Web service domain, the major implementation of the service-oriented paradigm. Nevertheless, a quick and seamless information ﬂow between embedded devices and Web services is an important requirement for many application scenarios, e.g., real-time aware production management or the Internet of Things. Within this paper, we present an approach that allows to proﬁt from the beneﬁts of traditional SOA implementations, such as Web service interfaces and an IP compatible addressing scheme, and on the other hand can be implemented on resource constraint devices. The main idea is to use a data-centric processing paradigm at the device level and a gateway that mediates between the Web service and the embedded device world.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {2009 {International} {Conference} on {Advanced} {Information} {Networking} and {Applications} {Workshops}},
	publisher = {IEEE},
	author = {Buckl, Christian and Sommer, Stephan and Scholz, Andreas and Knoll, Alois and Kemper, Alfons and Heuer, Jörg and Schmitt, Anton},
	month = may,
	year = {2009},
	pages = {476--481},
}

@article{gershenfeld2004internet,
	title = {The internet of things},
	volume = {291},
	number = {4},
	journal = {Scientific American},
	author = {Gershenfeld, Neil and Krikorian, Raffi and Cohen, Danny},
	year = {2004},
	note = {Publisher: JSTOR},
	pages = {76--81},
}

@article{akyildiz_wireless_2002,
	title = {Wireless sensor networks: a survey},
	abstract = {This paper describes the concept of sensor networks which has been made viable by the convergence of microelectro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors inﬂuencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are also discussed. Ó 2002 Published by Elsevier Science B.V.},
	language = {en},
	journal = {Computer Networks},
	author = {Akyildiz, I F and Su, W and Sankarasubramaniam, Y and Cayirci, E},
	year = {2002},
	pages = {30},
}

@incollection{floerkemeier_sensor_2008,
	address = {Berlin, Heidelberg},
	title = {Sensor {Applications} in the {Supply} {Chain}: {The} {Example} of {Quality}-{Based} {Issuing} of {Perishables}},
	volume = {4952},
	isbn = {978-3-540-78730-3 978-3-540-78731-0},
	shorttitle = {Sensor {Applications} in the {Supply} {Chain}},
	url = {http://link.springer.com/10.1007/978-3-540-78731-0_9},
	abstract = {Miniaturization and price decline are increasingly allowing for the use of RFID tags and sensors in inter-organizational supply chain applications. This contribution aims at investigating the potential of sensor-based issuing policies on product quality in the perishables supply chain. We develop a simple simulation model that allows us to study the quality of perishable goods at a retailer under different issuing policies at the distributor. Our results show that policies that rely on automatically collected expiry dates and product quality bear the potential to improve the quality of items in stores with regard to mean quality and standard deviation.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {The {Internet} of {Things}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dada, Ali and Thiesse, Frédéric},
	editor = {Floerkemeier, Christian and Langheinrich, Marc and Fleisch, Elgar and Mattern, Friedemann and Sarma, Sanjay E.},
	year = {2008},
	doi = {10.1007/978-3-540-78731-0_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {140--154},
}

@article{babun_real-time_2019,
	title = {Real-time {Analysis} of {Privacy}-(un)aware {IoT} {Applications}},
	url = {http://arxiv.org/abs/1911.10461},
	abstract = {Users trust IoT apps to control and automate their smart devices. These apps necessarily have access to sensitive data to implement their functionality. However, users lack visibility into how their sensitive data is used (or leaked), and they often blindly trust the app developers. In this paper, we present IOTWATCH, a novel dynamic analysis tool that uncovers the privacy risks of IoT apps in real-time. We designed and built IOTWATCH based on an IoT privacy survey that considers the privacy needs of IoT users. IOTWATCH provides users with a simple interface to specify their privacy preferences with an IoT app. Then, in runtime, it analyzes both the data that is sent out of the IoT app and its recipients using Natural Language Processing (NLP) techniques. Moreover, IOTWATCH informs the users with its ﬁndings to make them aware of the privacy risks with the IoT app. We implemented IOTWATCH on real IoT applications. Speciﬁcally, we analyzed 540 IoT apps to train the NLP model and evaluate its effectiveness. IOTWATCH successfully classiﬁes IoT app data sent to external parties to correct privacy labels with an average accuracy of 94.25\%, and ﬂags IoT apps that leak privacy data to unauthorized parties. Finally, IOTWATCH yields minimal overhead to an IoT app’s execution, on average 105 ms additional latency.},
	language = {en},
	urldate = {2021-09-06},
	journal = {arXiv:1911.10461 [cs]},
	author = {Babun, Leonardo and Celik, Z. Berkay and McDaniel, Patrick and Uluagac, A. Selcuk},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.10461},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{rondon_lightningstrike_2021,
	address = {Abu Dhabi United Arab Emirates},
	title = {{LightningStrike}: (in)secure practices of {E}-{IoT} systems in the wild},
	isbn = {978-1-4503-8349-3},
	shorttitle = {{LightningStrike}},
	url = {https://dl.acm.org/doi/10.1145/3448300.3467830},
	doi = {10.1145/3448300.3467830},
	abstract = {The widespread adoption of specialty smart ecosystems has changed the everyday lives of users. As a part of smart ecosystems, Enterprise Internet of Things (E-IoT) allows users to integrate and control more complex installations in comparison to off-the-shelf IoT systems. With E-IoT, users have a complete control of audio, video, scheduled events, lightning fixtures, shades, door access, and relays via available user interfaces. As such, these systems see widespread use in government or smart private offices, schools, smart buildings, professional conference rooms, hotels, smart homes, yachts, and similar professional settings. However, even with their widespread use, the security of many E-IoT systems has not been researched in the literature. Further, many E-IoT systems utilize proprietary communication protocols that rely mostly on security through obscurity, which has perhaps led many users to mistakenly assume that these systems are secure. To address this open research problem and determine if E-IoT systems are vulnerable, we focus on one of the core E-IoT components, E-IoT communication buses. Communication buses are used by E-IoT proprietary protocols to connect multiple E-IoT devices (e.g., keypads and touchscreens) and trigger pre-configured events upon user actions. In this study, we introduce LightningStrike, the implementation of four proof-ofconcept attacks that demonstrate several weaknesses in E-IoT proprietary communication protocols through communication buses. With LightningStrike, we show that it is feasible for an attacker to compromise E-IoT systems using E-IoT communication buses. We demonstrate that popular E-IoT proprietary communication protocols are susceptible to Denial-of-Service, eavesdropping, impersonation, and replay attacks. As E-IoT systems control physical access, safety components, and emergency equipment, an attacker with a low level of knowledge and effort can easily exploit E-IoT vulnerabilities to impact the security and safety of users, smart systems, and smart buildings worldwide.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Proceedings of the 14th {ACM} {Conference} on {Security} and {Privacy} in {Wireless} and {Mobile} {Networks}},
	publisher = {ACM},
	author = {Rondon, Luis Puche and Babun, Leonardo and Aris, Ahmet and Akkaya, Kemal and Uluagac, A. Selcuk},
	month = jun,
	year = {2021},
	pages = {106--116},
}

@inproceedings{rondon_poisonivy_2020,
	address = {Virtual Event Japan},
	title = {{PoisonIvy}: ({In})secure {Practices} of {Enterprise} {IoT} {Systems} in {Smart} {Buildings}},
	isbn = {978-1-4503-8061-4},
	shorttitle = {{PoisonIvy}},
	url = {https://dl.acm.org/doi/10.1145/3408308.3427606},
	doi = {10.1145/3408308.3427606},
	abstract = {The rise of IoT devices has led to the proliferation of smart buildings, offices, and homes worldwide. Although commodity IoT devices are employed by ordinary end-users, complex environments such as smart buildings, government, or private smart offices, conference rooms, or hospitality require customized and highly reliable solutions. Those systems called Enterprise Internet of Things (EIoT) connect such environments to the Internet and are professionally managed solutions usually offered by dedicated vendors (e.g., Control4, Crestron, Lutron, etc.). As EIoT systems require specialized training, software, and equipment to deploy, many of these systems are closed-source and proprietary in nature. This has led to very little research investigating the security of EIoT systems and their components. In effect, EIoT systems in smart settings such as smart buildings present an unprecedented and unexplored threat vector for an attacker. In this work, we explore EIoT system vulnerabilities and insecure development practices. Specifically, focus on the usage of drivers as an attack mechanism, and introduce PoisonIvy, a number of novel attacks that demonstrate how it is possible for an attacker to easily attack and command EIoT system controllers using malicious drivers. Specifically, we show how drivers used to integrate third-party services and devices to EIoT systems can be trivially misused in a systematic fashion. To demonstrate the capabilities of attackers, we implement and evaluate PoisonIvy using a testbed of real EIoT devices in a smart building setting. We show that an attacker can easily perform DoS attacks, gain remote control, and maliciously abuse system resources (e.g., bitcoin mining) of EIoT systems. Further, we discuss the (in)securities in drivers and possible countermeasures. To the best of our knowledge, this is the first work to analyze the (in)securities of EIoT deployment practices and demonstrate the associated vulnerabilities in this ecosystem. With this work, we raise awareness on the (in)secure development practices used for EIoT systems, the consequences of which can largely impact the security, privacy, safety, reliability, and performance of millions, if not billions, of EIoT systems worldwide.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Proceedings of the 7th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {ACM},
	author = {Rondon, Luis Puche and Babun, Leonardo and Aris, Ahmet and Akkaya, Kemal and Uluagac, A. Selcuk},
	month = nov,
	year = {2020},
	pages = {130--139},
}

@inproceedings{sikder_aegis_2019,
	address = {San Juan Puerto Rico USA},
	title = {Aegis: a context-aware security framework for smart home systems},
	isbn = {978-1-4503-7628-0},
	shorttitle = {Aegis},
	url = {https://dl.acm.org/doi/10.1145/3359789.3359840},
	doi = {10.1145/3359789.3359840},
	abstract = {Our everyday lives are expanding fast with the introduction of new Smart Home Systems (SHSs). Today, a myriad of SHS devices and applications are widely available to users and have already started to re-define our modern lives. Smart home users utilize the apps to control and automate such devices. Users can develop their own apps or easily download and install them from vendor-specific app markets. App-based SHSs offer many tangible benefits to our lives, but also unfold diverse security risks. Several attacks have already been reported for SHSs. However, current security solutions consider smart home devices and apps individually to detect malicious actions rather than the context of the SHS as a whole. The existing mechanisms cannot capture user activities and sensor-device-user interactions in a holistic fashion. To address these issues, in this paper, we introduce Aegis, a novel context-aware security framework to detect malicious behavior in a SHS. Specifically, Aegis observes the states of the connected smart home entities (sensors and devices) for different user activities and usage patterns in a SHS and builds a contextual model to differentiate between malicious and benign behavior. We evaluated the efficacy and performance of Aegis in multiple smart home settings (i.e., single bedroom, double bedroom, duplex) with real-life users performing day-to-day activities and real SHS devices. We also measured the performance of Aegis against five different malicious behaviors. Our detailed evaluation shows that Aegis can detect malicious behavior in SHS with high accuracy (over 95\%) and secure the SHS regardless of the smart home layout, device configuration, installed apps, and enforced user policies. Finally, Aegis achieves minimum overhead in detecting malicious behavior in SHS, ensuring easy deployability in real-life smart environments.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Proceedings of the 35th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Sikder, Amit Kumar and Babun, Leonardo and Aksu, Hidayet and Uluagac, A. Selcuk},
	month = dec,
	year = {2019},
	pages = {28--41},
}

@article{celik_verifying_2019,
	title = {Verifying {Internet} of {Things} {Safety} and {Security} in {Physical} {Spaces}},
	volume = {17},
	issn = {1540-7993, 1558-4046},
	url = {https://ieeexplore.ieee.org/document/8733994/},
	doi = {10.1109/MSEC.2019.2911511},
	language = {en},
	number = {5},
	urldate = {2021-09-06},
	journal = {IEEE Security \& Privacy},
	author = {Celik, Z. Berkay and McDaniel, Patrick and Tan, Gang and Babun, Leonardo and Uluagac, A. Selcuk},
	month = sep,
	year = {2019},
	pages = {30--37},
}

@inproceedings{datta_valve_2020,
	address = {Taipei Taiwan},
	title = {Valve: {Securing} {Function} {Workflows} on {Serverless} {Computing} {Platforms}},
	isbn = {978-1-4503-7023-3},
	shorttitle = {Valve},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380173},
	doi = {10.1145/3366423.3380173},
	abstract = {Serverless Computing has quickly emerged as a dominant cloud computing paradigm, allowing developers to rapidly prototype eventdriven applications using a composition of small functions that each perform a single logical task. However, many such application workflows are based in part on publicly-available functions developed by third-parties, creating the potential for functions to behave in unexpected, or even malicious, ways. At present, developers are not in total control of where and how their data is flowing, creating significant security and privacy risks in growth markets that have embraced serverless (e.g., IoT).},
	language = {en},
	urldate = {2021-09-02},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Datta, Pubali and Kumar, Prabuddha and Morris, Tristan and Grace, Michael and Rahmati, Amir and Bates, Adam},
	month = apr,
	year = {2020},
	pages = {939--950},
}

@misc{noauthor_mixed-method_nodate,
	title = {A mixed-method empirical study of {Function}-as-a-{Service} software development in industrial practice {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0164121218302735?token=296A8F69C4D6DDED6E7076AFB02E6863649FA17AEDAE2F3163F9885605C7C7426783763C70C137E7EE5B59380E4BE1D5&originRegion=us-east-1&originCreation=20210902140209},
	language = {en},
	urldate = {2021-09-02},
	doi = {10.1016/j.jss.2018.12.013},
}

@article{thibaud_internet_2018,
	title = {Internet of {Things} ({IoT}) in high-risk {Environment}, {Health} and {Safety} ({EHS}) industries: {A} comprehensive review},
	volume = {108},
	issn = {0167-9236},
	shorttitle = {Internet of {Things} ({IoT}) in high-risk {Environment}, {Health} and {Safety} ({EHS}) industries},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923618300344},
	doi = {10.1016/j.dss.2018.02.005},
	abstract = {The rise of ubiquitous systems is sustained by the development and progressive adoption of the Internet of Things (IoT) devices and their enabling technologies. IoT has been shown to have significant potential in high-risk Environment, Health, and Safety (EHS) industries. In these industries, human lives are at stake and IoT-based applications are primed to offer safe, reliable, and efficient solutions due to their ability to operate at a fine granular level and provide rich low-level information. We review existing published research on IoT-based applications in high-risk EHS industries with specific emphasis on healthcare industry, food supply chain (FSC), mining and energy industries (oil \& gas and nuclear), intelligent transportation (e.g., connected vehicles), and building \& infrastructure management for emergency response operations until 2016. We also highlight IoT-related challenges and proposed solutions in high risk EHS industries. We then conclude by presenting research challenges and expected trends for IoT in these industries.},
	language = {en},
	urldate = {2021-09-02},
	journal = {Decision Support Systems},
	author = {Thibaud, Montbel and Chi, Huihui and Zhou, Wei and Piramuthu, Selwyn},
	month = apr,
	year = {2018},
	keywords = {Connected vehicles, Environment Health and Safety (EHS), Food supply chain, Healthcare, Internet of Things (IoT), Smart city},
	pages = {79--95},
}

@article{conway1968committees,
	title = {How do committees invent},
	volume = {14},
	number = {4},
	journal = {Datamation},
	author = {Conway, Melvin E},
	year = {1968},
	pages = {28--31},
}

@article{sreekanti_fault-tolerance_2020,
	title = {A {Fault}-{Tolerance} {Shim} for {Serverless} {Computing}},
	url = {http://arxiv.org/abs/2003.06007},
	abstract = {Serverless computing has grown in popularity in recent years, with an increasing number of applications being built on Functions-as-a-Service (FaaS) platforms. By default, FaaS platforms support retry-based fault tolerance, but this is insufficient for programs that modify shared state, as they can unwittingly persist partial sets of updates in case of failures. To address this challenge, we would like atomic visibility of the updates made by a FaaS application. In this paper, we present AFT, an atomic fault tolerance shim for serverless applications. AFT interposes between a commodity FaaS platform and storage engine and ensures atomic visibility of updates by enforcing the read atomic isolation guarantee. AFT supports new protocols to guarantee read atomic isolation in the serverless setting. We demonstrate that aft introduces minimal overhead relative to existing storage engines and scales smoothly to thousands of requests per second, while preventing a significant number of consistency anomalies.},
	urldate = {2021-09-02},
	journal = {arXiv:2003.06007 [cs]},
	author = {Sreekanti, Vikram and Wu, Chenggang and Chhatrapati, Saurav and Gonzalez, Joseph E. and Hellerstein, Joseph M. and Faleiro, Jose M.},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.06007},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{marquez_actual_2018,
	title = {Actual {Use} of {Architectural} {Patterns} in {Microservices}-{Based} {Open} {Source} {Projects}},
	doi = {10.1109/APSEC.2018.00017},
	abstract = {Microservice-based systems instantiate an architectural style that conceives of systems as sets of modular, customer-centric, independent, and scalable services. These systems express a similar essential structural organization and seems appropriate to design them using architectural patterns because these combine an understanding of the system domain and good practices. Code repository platforms provide the developer community with ideas and examples about microservice systems, but since they are in early adoption, there is still no clear notion of which actual microservice systems incarnate architectural patterns (if any), reducing the use of frameworks and the achievement of quality attributes. This paper extends a previous study on architectural patterns for microservices in academic and industry sources. We explored which architectural patterns for microservices are used in actual microservice-based open source systems, by subjecting thirty well-known open source projects to a comprehensive multi-criteria code and design review. We found that (1) open source projects use only a few architectural patterns broadly; (2) most projects use the same few frameworks; (3) there are very few microservice architectural patterns as such; and (4) what most projects use (what was previously called) are SOA patterns. This study shows that microservice systems builders do use architectural patterns, but only a few of them. It remains to be determined whether additional patterns would be productively used to build microservice systems, or the few ones currently used are the only ones actually necessary.},
	booktitle = {2018 25th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Márquez, Gastón and Astudillo, Hernán},
	month = dec,
	year = {2018},
	note = {ISSN: 2640-0715},
	keywords = {Architectural patterns, Companies, Computer architecture, Correlation, Framework, Industries, Market research, Microservices, Open source projects, Quality attributes, Scalability, Service-oriented architecture},
	pages = {31--40},
}

@inproceedings{zhang_microservice_2019,
	title = {Microservice {Architecture} in {Reality}: {An} {Industrial} {Inquiry}},
	shorttitle = {Microservice {Architecture} in {Reality}},
	doi = {10.1109/ICSA.2019.00014},
	abstract = {Background: Seeking an appropriate architecture for a software design is always a challenge in recent decades. Although microservices as a lightweight architecture style is claimed that can improve the current practices with several characteristics, many practices are based upon the different circumstances and reflect the variant effects. An empirical inquiry brings us a systematic insight into the industrial practices on microservices. Objective: This study is to investigate the gap between the ideal visions and real industrial practices on microservices and what benefits we can gain from the industrial experiences. Method: We carried out a series of industrial interviews with thirteen different types of companies. The collected data were then codified according to the defined qualitative methods. Results: We characterized the gaps between the typical characteristics accepted in the community and the industrial practices of microservices. Furthermore, the compromise between benefits and sufferings of microservices around these nine dimensions were also investigated. Conclusion: We confirmed the benefits of the microservices that can be obtained from practice as well as their possible pains that need to be addressed with extra expense from experiences. Besides, some outlined pains, e.g., organizational transformation, decomposition, distributed monitoring, and bug localization, may inspire researchers to conduct the further research.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Software} {Architecture} ({ICSA})},
	author = {Zhang, He and Li, Shanshan and Jia, Zijia and Zhong, Chenxing and Zhang, Cheng},
	month = mar,
	year = {2019},
	keywords = {Companies, Computer architecture, Industries, Instruments, Interviews, Pain, Software, empirical study, interview, microservices, pains},
	pages = {51--60},
}

@inproceedings{haselbock_expert_2018,
	title = {An {Expert} {Interview} {Study} on {Areas} of {Microservice} {Design}},
	doi = {10.1109/SOCA.2018.00028},
	abstract = {Microservices are single-responsibility units that are implemented in various technologies by independent, cross-cutting teams. A shift to a microservice architecture therefore touches many different areas, including system design, organizational structures, and runtime infrastructure. To investigate the importance of different areas of microservice design, we interviewed 10 microservice domain experts to find out which design areas are relevant for microservices, how important they are, and why they are important. This paper presents the resulting microservice design areas, assessments of their importance, and rationales for the provided assessments.},
	booktitle = {2018 {IEEE} 11th {Conference} on {Service}-{Oriented} {Computing} and {Applications} ({SOCA})},
	author = {Haselböck, Stefan and Weinreich, Rainer and Buchgeher, Georg},
	month = nov,
	year = {2018},
	note = {ISSN: 2163-2871},
	keywords = {Cloud computing, Companies, Computer architecture, Encoding, Interviews, Monitoring, Software engineering, expert interview study, microservice design areas, microservices},
	pages = {137--144},
}

@inproceedings{liu_microservices_2020,
	title = {Microservices: architecture, container, and challenges},
	shorttitle = {Microservices},
	doi = {10.1109/QRS-C51114.2020.00107},
	abstract = {Microservices are emerging as a new computing paradigm which is a suitable complementation of cloud computing. Microservices will decompose traditional monolithic applications into a set of fine-grained services, which can be independently developed, tested, and deployed. However, there are many challenges of microservices. This paper provides a comprehensive overview of microservices. More specifically, firstly, we systematically compare traditional monolithic architecture, service-oriented architecture (SOA), and microservices architecture. Secondly, we give an overview of the container technology. Finally, we outline the technical challenges of microservices, such as performance, debugging and data consistency.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Liu, Guozhi and Huang, Bi and Liang, Zhihong and Qin, Minmin and Zhou, Hua and Li, Zhang},
	month = dec,
	year = {2020},
	keywords = {Computer architecture, Containers, Debugging, Security, Service-oriented architecture, Software quality, Software reliability, container, debugging, microservices, monolithic architecture, performance, service-oriented architecture},
	pages = {629--635},
}

@article{cerny_contextual_2018,
	title = {Contextual understanding of microservice architecture: current and future directions},
	volume = {17},
	issn = {1559-6915},
	shorttitle = {Contextual understanding of microservice architecture},
	url = {https://doi.org/10.1145/3183628.3183631},
	doi = {10.1145/3183628.3183631},
	abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key differences between these two approaches and their features, we can design a more effective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the differences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.},
	number = {4},
	urldate = {2021-09-01},
	journal = {ACM SIGAPP Applied Computing Review},
	author = {Cerny, Tomas and Donahoo, Michael J. and Trnka, Michal},
	month = jan,
	year = {2018},
	keywords = {SOA, architectures, microservices, self-contained systems, survey, systematic mapping study},
	pages = {29--45},
}

@inproceedings{cerny_disambiguation_2017,
	address = {Krakow Poland},
	title = {Disambiguation and {Comparison} of {SOA}, {Microservices} and {Self}-{Contained} {Systems}},
	isbn = {978-1-4503-5027-3},
	url = {https://dl.acm.org/doi/10.1145/3129676.3129682},
	doi = {10.1145/3129676.3129682},
	abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key diﬀerences between these two approaches and their features, we can design a more eﬀective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the diﬀerences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the {International} {Conference} on {Research} in {Adaptive} and {Convergent} {Systems}},
	publisher = {ACM},
	author = {Cerny, Tomas and Donahoo, Michael J. and Pechanec, Jiri},
	month = sep,
	year = {2017},
	pages = {228--235},
}

@inproceedings{cerny_disambiguation_2017-1,
	address = {Krakow Poland},
	title = {Disambiguation and {Comparison} of {SOA}, {Microservices} and {Self}-{Contained} {Systems}},
	isbn = {978-1-4503-5027-3},
	url = {https://dl.acm.org/doi/10.1145/3129676.3129682},
	doi = {10.1145/3129676.3129682},
	abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key diﬀerences between these two approaches and their features, we can design a more eﬀective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the diﬀerences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the {International} {Conference} on {Research} in {Adaptive} and {Convergent} {Systems}},
	publisher = {ACM},
	author = {Cerny, Tomas and Donahoo, Michael J. and Pechanec, Jiri},
	month = sep,
	year = {2017},
	pages = {228--235},
}

@inproceedings{munaf_microservices_2019,
	title = {Microservices {Architecture}: {Challenges} and {Proposed} {Conceptual} {Design}},
	shorttitle = {Microservices {Architecture}},
	doi = {10.1109/COMTECH.2019.8737831},
	abstract = {Microservices Architecture has evolved in the recent past and has gained significant popularity offering various benefits as compared to existing architectures addressing various serious concerns of the recent era in Software Engineering. This paper will first briefly introduce the microservices architecture and its evolution being still in inception phase. After enlisting its offered benefits, its envisaged implementation challenges will be addressed including various options available for coping up with those challenges by using empirical and conceptual method. Based on the challenges and available options, a conceptual design of microservices architecture including major components and their role will be proposed. This paper addresses the new comers for understanding of microservices architecture, researchers and the practitioners for verification of evolved conceptual design and future prospects.},
	booktitle = {2019 {International} {Conference} on {Communication} {Technologies} ({ComTech})},
	author = {Munaf, Raja Mubashir and Ahmed, Jawwad and Khakwani, Faraz and Rana, Tauseef},
	month = mar,
	year = {2019},
	keywords = {Computer architecture, Databases, Fine grained SOA, Microservices, Microservices Architecture, SOA, Scalability, Service Oriented Architecture, Service-oriented architecture, Software Architecture, Software systems, Task analysis},
	pages = {82--87},
}

@inproceedings{liu_revealer_2021,
	address = {San Francisco, CA, USA},
	title = {Revealer: {Detecting} and {Exploiting} {Regular} {Expression} {Denial}-of-{Service} {Vulnerabilities}},
	isbn = {978-1-72818-934-5},
	shorttitle = {Revealer},
	url = {https://ieeexplore.ieee.org/document/9519406/},
	doi = {10.1109/SP40001.2021.00062},
	abstract = {Regular expression Denial-of-Service (ReDoS) is a class of algorithmic complexity attacks. Attackers can craft particular strings to trigger the worst-case super-linear matching time of some vulnerable regular expressions (regex) with extended features that are commonly supported by popular programming languages. ReDoS attacks can severely degrade the performance of web applications, which extensively employ regexes in their server-side logic. Nevertheless, the characteristics of vulnerable regexes with extended features remain understudied, making it difficult to mitigate or even detect such vulnerabilities.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Liu, Yinxi and Zhang, Mingxue and Meng, Wei},
	month = may,
	year = {2021},
	pages = {1468--1484},
}

@inproceedings{alshuqayran_systematic_2016,
	title = {A {Systematic} {Mapping} {Study} in {Microservice} {Architecture}},
	doi = {10.1109/SOCA.2016.15},
	abstract = {The accelerating progress of network speed, reliability and security creates an increasing demand to move software and services from being stored and processed locally on users' machines to being managed by third parties that are accessible through the network. This has created the need to develop new software development methods and software architectural styles that meet these new demands. One such example in software architectural design is the recent emergence of the microservices architecture to address the maintenance and scalability demands of online service providers. As microservice architecture is a new research area, the need for a systematic mapping study is crucial in order to summarise the progress so far and identify the gaps and requirements for future studies. In this paper we present a systematic mapping study of microservices architectures and their implementation. Our study focuses on identifying architectural challenges, the architectural diagrams/views and quality attributes related to microsevice systems.},
	booktitle = {2016 {IEEE} 9th {International} {Conference} on {Service}-{Oriented} {Computing} and {Applications} ({SOCA})},
	author = {Alshuqayran, Nuha and Ali, Nour and Evans, Roger},
	month = nov,
	year = {2016},
	keywords = {Business, Computer architecture, Conferences, Security, Service-oriented architecture, Systematics},
	pages = {44--51},
}

@inproceedings{pereira-vale_security_2019,
	title = {Security {Mechanisms} {Used} in {Microservices}-{Based} {Systems}: {A} {Systematic} {Mapping}},
	shorttitle = {Security {Mechanisms} {Used} in {Microservices}-{Based} {Systems}},
	doi = {10.1109/CLEI47609.2019.235060},
	abstract = {Microservices is an architectural style that conceives systems as a modular, costumer, independent and scalable suite of services; it offers several advantages but its growing popularity has given rise to security challenges. Building secure systems is greatly helped by deploying existing security mechanisms, but current literature does not guide developers about which mechanisms are actually used by developers of microservices-based systems. This article describes the design and results of a systematic mapping study to identify the security mechanisms used in microservices-based systems described in the literature. The study yielded 321 articles, of which 26 are primary studies. Key findings are that (i) the studies mention 18 security mechanisms; (ii) the most mentioned security mechanisms are authentication, authorization and credentials; and (iii) almost 2/3 of security mechanisms focus on stopping or mitigating attacks, but none on recovering from them. Additionally, it emerges that experiments and case studies are the most used empirical strategies in microservices security research. The clear identification of most-used security solutions will facilitate the reuse of existing architectural knowledge to address security problems in microservices-based systems.},
	booktitle = {2019 {XLV} {Latin} {American} {Computing} {Conference} ({CLEI})},
	author = {Pereira-Vale, Anelis and Márquez, Gastón and Astudillo, Hernán and Fernandez, Eduardo B.},
	month = sep,
	year = {2019},
	keywords = {Authentication, Cloud computing, Computer architecture, Conferences, Software, Systematics, microservices-based systems, secure software development, security mechanisms, systematic mapping},
	pages = {01--10},
}

@article{de_toledo_identifying_2021,
	title = {Identifying architectural technical debt, principal, and interest in microservices: {A} multiple-case study},
	volume = {177},
	issn = {0164-1212},
	shorttitle = {Identifying architectural technical debt, principal, and interest in microservices},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121221000650},
	doi = {10.1016/j.jss.2021.110968},
	abstract = {Background:
Using a microservices architecture is a popular strategy for software organizations to deliver value to their customers fast and continuously. However, scientific knowledge on how to manage architectural debt in microservices is scarce.
Objectives:
In the context of microservices applications, this paper aims to identify architectural technical debts (ATDs), their costs, and their most common solutions.
Method:
We conducted an exploratory multiple case study by conducting 25 interviews with practitioners working with microservices in seven large companies.
Results:
We found 16 ATD issues, their negative impact (interest), and common solutions to repay each debt together with the related costs (principal). Two examples of critical ATD issues found were the use of shared databases that, if not properly planned, leads to potential breaks on services every time the database schema changes and bad API designs, which leads to coupling among teams. We identified ATDs occurring in different domains and stages of development and created a map of the relationships among those debts.
Conclusion:
The findings may guide organizations in developing microservices systems that better manage and avoid architectural debts.},
	language = {en},
	urldate = {2021-09-01},
	journal = {Journal of Systems and Software},
	author = {de Toledo, Saulo S. and Martini, Antonio and Sjøberg, Dag I. K.},
	month = jul,
	year = {2021},
	keywords = {Cost of software, Cross-company study, Qualitative analysis, Software maintainability, Software quality},
	pages = {110968},
}

@inproceedings{soares_de_toledo_architectural_2019,
	title = {Architectural {Technical} {Debt} in {Microservices}: {A} {Case} {Study} in a {Large} {Company}},
	shorttitle = {Architectural {Technical} {Debt} in {Microservices}},
	doi = {10.1109/TechDebt.2019.00026},
	abstract = {Introduction: Software companies aim to achieve continuous delivery to constantly provide value to their customers. A popular strategy is to use microservices architecture. However, such an architecture is also subject to debt, which hinders the continuous delivery process and thus negatively affects the software released to the customers. Objectives: The aim of this study is to identify issues, solutions and risks related to Architecture Technical Debt in microservices. Method: We conducted an exploratory case study of a real life project with about 1000 services in a large, international company. Through qualitative analysis of documents and interviews, we investigated Architecture Technical Debt in the communication layer of a system with microservices architecture. Results: Our main contributions are a list of Architecture Technical Debt issues specific for the communication layer in a system with microservices architecture, as well as their associated negative impact (interest), a solution to repay the debt and the its cost (principal). Among the found Architecture Technical Debt issues were the existence of business logic in the communication layer and a high amount of point-to-point connections between services. The studied solution consists of the implementation of different canonical models specific to different domains, the removal of business logic from the communication layer, and migration from services to use the communication layer correctly. We also contributed with a list of possible risks that can affect the payment of the debt, as lack of funding and inadequate prioritization. Conclusion: We found issues, solutions and possible risks that are specific for microservices architectures not yet encountered in the current literature. Our results may be useful for practitioners that want to avoid or repay Technical Debt in their microservices architecture.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Technical} {Debt} ({TechDebt})},
	author = {Soares de Toledo, Saulo and Martini, Antonio and Przybyszewska, Agata and Sjøberg, Dag I.K.},
	month = may,
	year = {2019},
	keywords = {Companies, Computer architecture, Data models, Informatics, Software, Technical Debt, Architecture, Microservices, Case Study},
	pages = {78--87},
}

@inproceedings{wen_empirical_2021,
	address = {Athens Greece},
	title = {An empirical study on challenges of application development in serverless computing},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468558},
	doi = {10.1145/3468264.3468558},
	abstract = {Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers’ perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q\&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, ∗Jinfeng Wen and Zhenpeng Chen made equal contributions to this work.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wen, Jinfeng and Chen, Zhenpeng and Liu, Yi and Lou, Yiling and Ma, Yun and Huang, Gang and Jin, Xin and Liu, Xuanzhe},
	month = aug,
	year = {2021},
	keywords = {Empirical study, Serverless computing},
	pages = {416--428},
}

@inproceedings{wen_empirical_2021-1,
	address = {Athens Greece},
	title = {An empirical study on challenges of application development in serverless computing},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468558},
	doi = {10.1145/3468264.3468558},
	abstract = {Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers’ perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q\&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, ∗Jinfeng Wen and Zhenpeng Chen made equal contributions to this work.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wen, Jinfeng and Chen, Zhenpeng and Liu, Yi and Lou, Yiling and Ma, Yun and Huang, Gang and Jin, Xin and Liu, Xuanzhe},
	month = aug,
	year = {2021},
	pages = {416--428},
}

@misc{bidlingmaier_additional_2021,
	title = {An additional non-backtracking {RegExp} engine},
	url = {https://v8.dev/blog/non-backtracking-regexp},
	author = {Bidlingmaier, Martin},
	year = {2021},
}

@article{zalewski_iot_2019,
	title = {{IoT} {Safety}: {State} of the {Art}},
	volume = {21},
	issn = {1941-045X},
	shorttitle = {{IoT} {Safety}},
	doi = {10.1109/MITP.2018.2883858},
	abstract = {Reports on safety issues relating to the Internet of Things (IoT). Users, developers, managers, and other stakeholders are concerned that the heterogeneity and complexity of this technology, essentially composed of systems of systems, may open the door to security breaches on an unprecedented scale. There is, however, another important system property, which is not very often brought up as an imminent concern, but is equally important. This is device safety, the violation of which may cause severe harm to the environment in which the device operates.},
	number = {1},
	journal = {IT Professional},
	author = {Zalewski, Janusz},
	month = jan,
	year = {2019},
	note = {Conference Name: IT Professional},
	keywords = {Computer security, Internet of Things, Safety, Security, Smart devices},
	pages = {16--20},
}

@inproceedings{van2021memoized,
	title = {Memoized regular expressions},
	booktitle = {International conference on implementation and application of automata},
	author = {van der Merwe, Brink and Mouton, Jacobie and van Litsenborgh, Steyn and Berglund, Martin},
	year = {2021},
	note = {tex.organization: Springer},
	pages = {39--52},
}

@article{ralph_illusion_2013,
	title = {The illusion of requirements in software development},
	volume = {18},
	issn = {0947-3602, 1432-010X},
	url = {http://link.springer.com/10.1007/s00766-012-0161-4},
	doi = {10.1007/s00766-012-0161-4},
	abstract = {This viewpoint explores the possibility that many software development projects may have no useful requirements. Speciﬁcally, for problems (e.g., knowledge worker burnout) with two completely different solutions (e.g., better tool support or hire more employees), an analyst may state a goal (e.g., decrease work hours) but more speciﬁc desiderata are contingent on the chosen solution. Furthermore, without fully exploring the design space, the designer cannot be sure whether there exists another approach, which would achieve the goal without any commonality with known approaches. In these situations of sparse requirements, analysts may misrepresent design decisions as requirements, creating an illusion of requirements in software development.},
	language = {en},
	number = {3},
	urldate = {2021-08-31},
	journal = {Requirements Engineering},
	author = {Ralph, Paul},
	month = sep,
	year = {2013},
	pages = {293--296},
}

@inproceedings{ralph_is_2015,
	address = {Florence, Italy},
	title = {Is {Requirements} {Engineering} {Inherently} {Counterproductive}?},
	isbn = {978-1-4673-7100-1},
	url = {http://ieeexplore.ieee.org/document/7184708/},
	doi = {10.1109/TwinPeaks.2015.12},
	abstract = {This paper explores the possibility that requirements engineering is, in principle, detrimental to software project success. Requirements engineering is conceptually divided into two distinct processes: sensemaking (learning about the project context) and problem structuring (specifying problems, goals, requirements, constraints, etc.). An interdisciplinary literature review revealed substantial evidence that while sensemaking improves design performance, problem structuring reduces design performance. Future research should therefore investigate decoupling the sensemaking aspects of requirements engineering from the problem structuring aspects.},
	language = {en},
	urldate = {2021-08-31},
	booktitle = {2015 {IEEE}/{ACM} 5th {International} {Workshop} on the {Twin} {Peaks} of {Requirements} and {Architecture}},
	publisher = {IEEE},
	author = {Ralph, Paul and Mohanani, Rahul},
	month = may,
	year = {2015},
	pages = {20--23},
}

@article{madni_leveraging_2019,
	title = {Leveraging {Digital} {Twin} {Technology} in {Model}-{Based} {Systems} {Engineering}},
	volume = {7},
	issn = {2079-8954},
	url = {http://www.mdpi.com/2079-8954/7/1/7},
	doi = {10.3390/systems7010007},
	abstract = {Digital twin, a concept introduced in 2002, is becoming increasingly relevant to systems engineering and, more speciﬁcally, to model-based system engineering (MBSE). A digital twin, like a virtual prototype, is a dynamic digital representation of a physical system. However, unlike a virtual prototype, a digital twin is a virtual instance of a physical system (twin) that is continually updated with the latter’s performance, maintenance, and health status data throughout the physical system’s life cycle. This paper presents an overall vision and rationale for incorporating digital twin technology into MBSE. The paper discusses the beneﬁts of integrating digital twins with system simulation and Internet of Things (IoT) in support of MBSE and provides speciﬁc examples of the use and beneﬁts of digital twin technology in different industries. It concludes with a recommendation to make digital twin technology an integral part of MBSE methodology and experimentation testbeds.},
	language = {en},
	number = {1},
	urldate = {2021-08-30},
	journal = {Systems},
	author = {Madni, Azad and Madni, Carla and Lucero, Scott},
	month = jan,
	year = {2019},
	pages = {7},
}

@article{lamport_state_1978,
	title = {State the problem before describing the solution},
	volume = {3},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/1010734.1010737},
	doi = {10.1145/1010734.1010737},
	language = {en},
	number = {1},
	urldate = {2021-08-30},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Lamport, Leslie},
	month = jan,
	year = {1978},
	pages = {26--26},
}

@article{davison2004principles,
	title = {Principles of canonical action research},
	volume = {14},
	number = {1},
	journal = {Information systems journal},
	author = {Davison, Robert and Martinsons, Maris G and Kock, Ned},
	year = {2004},
	note = {Publisher: Wiley Online Library},
	pages = {65--86},
}

@article{lutters2007revealing,
	title = {Revealing actual documentation usage in software maintenance through war stories},
	volume = {49},
	number = {6},
	journal = {Information and Software Technology},
	author = {Lutters, Wayne G and Seaman, Carolyn B},
	year = {2007},
	note = {Publisher: Elsevier},
	pages = {576--587},
}

@article{rekdal_academic_2014,
	title = {Academic urban legends},
	volume = {44},
	issn = {0306-3127, 1460-3659},
	url = {http://journals.sagepub.com/doi/10.1177/0306312714535679},
	doi = {10.1177/0306312714535679},
	abstract = {Many of the messages presented in respectable scientific publications are, in fact, based on various forms of rumors. Some of these rumors appear so frequently, and in such complex, colorful, and entertaining ways that we can think of them as academic urban legends. The explanation for this phenomenon is usually that authors have lazily, sloppily, or fraudulently employed sources, and peer reviewers and editors have not discovered these weaknesses in the manuscripts during evaluation. To illustrate this phenomenon, I draw upon a remarkable case in which a decimal point error appears to have misled millions into believing that spinach is a good nutritional source of iron. Through this example, I demonstrate how an academic urban legend can be conceived and born, and can continue to grow and reproduce within academia and beyond.},
	language = {en},
	number = {4},
	urldate = {2021-07-20},
	journal = {Social Studies of Science},
	author = {Rekdal, Ole Bjørn},
	month = aug,
	year = {2014},
	keywords = {Plagiarism, Training PhD students},
	pages = {638--654},
}

@inproceedings{tondel2020using,
	title = {Using situational and narrative analysis for investigating the messiness of software security},
	booktitle = {Proceedings of the 14th {ACM}/{IEEE} international symposium on empirical software engineering and measurement ({ESEM})},
	author = {Tøndel, Inger Anne and Cruzes, Daniela Soares and Jaatun, Martin Gilje},
	year = {2020},
	pages = {1--6},
}

@article{baltes_sampling_2021,
	title = {Sampling in {Software} {Engineering} {Research}: {A} {Critical} {Review} and {Guidelines}},
	shorttitle = {Sampling in {Software} {Engineering} {Research}},
	url = {http://arxiv.org/abs/2002.07764},
	abstract = {Representative sampling appears rare in empirical software engineering research. Not all studies need representative samples, but a general lack of representative sampling undermines a scientiﬁc ﬁeld. This article therefore reports a critical review of the state of sampling in recent, high-quality software engineering research. The key ﬁndings are: (1) random sampling is rare; (2) sophisticated sampling strategies are very rare; (3) sampling, representativeness and randomness often appear misunderstood. These ﬁndings suggest that software engineering research has a generalizability crisis. To address these problems, this paper synthesizes existing knowledge of sampling into a succinct primer and proposes extensive guidelines for improving the conduct, presentation and evaluation of sampling in software engineering research. It is further recommended that while researchers should strive for more representative samples, disparaging non-probability sampling is generally capricious and particularly misguided for predominately qualitative research.},
	language = {en},
	urldate = {2021-07-30},
	journal = {arXiv:2002.07764 [cs]},
	author = {Baltes, Sebastian and Ralph, Paul},
	month = jul,
	year = {2021},
	note = {arXiv: 2002.07764},
	keywords = {Computer Science - Software Engineering},
}

@article{kitchenhamPrinciplesSurveyResearch,
	title = {Principles of {Survey} {Research} {Part} 2: {Designing} a {Survey}},
	volume = {27},
	language = {en},
	number = {1},
	journal = {ACM SIGSOFT},
	author = {Kitchenham, Barbara A and Pfleeger, Shad Lawrence},
	pages = {3},
}

@article{pfleegerPrinciplesSurveyResearch,
	title = {Principles of {Survey} {Research} {Part} 1: {Turning} {Lemons} into {Lemonade}},
	volume = {26},
	language = {en},
	number = {6},
	journal = {ACM SIGSOFT},
	author = {Pfleeger, Shad Lawrence and Kitchenham, Barbara A},
	pages = {3},
}

@incollection{easterbrookSelectingEmpiricalMethods2008,
	address = {London},
	title = {Selecting {Empirical} {Methods} for {Software} {Engineering} {Research}},
	isbn = {978-1-84800-043-8 978-1-84800-044-5},
	url = {http://link.springer.com/10.1007/978-1-84800-044-5_11},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	language = {en},
	urldate = {2021-04-06},
	booktitle = {Guide to {Advanced} {Empirical} {Software} {Engineering}},
	publisher = {Springer London},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	year = {2008},
	doi = {10.1007/978-1-84800-044-5_11},
	pages = {285--311},
}

@article{dittrichEthicsSocialHoneypots,
	title = {The ethics of social honeypots},
	language = {en},
	journal = {Research Ethics},
	author = {Dittrich, David},
	pages = {19},
}

@article{kwasnik_role_nodate,
	title = {The {Role} of {Classification} in {Knowledge} {Representation} and {Discovery}'},
	abstract = {THELINK BETWEEN CLASSIFICATION AND KNOWLEDGE is explored. Classification schemes have properties that enable the representation of entities and relationships in structures that reflect knowledge of the domain being classified. The strengths and limitations of four classificatory approaches are described in terms of their ability to reflect, discover, and create new knowledge. These approaches are hierarchies, trees, paradigms, and faceted analysis. Examples are provided of the way in which knowledge and the classification process affect each other.},
	language = {en},
	author = {Kwasnik, Barbarah},
	pages = {26},
}

@article{usman_taxonomies_2017,
	title = {Taxonomies in software engineering: {A} {Systematic} mapping study and a revised taxonomy development method},
	volume = {85},
	issn = {09505849},
	shorttitle = {Taxonomies in software engineering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584917300472},
	doi = {10.1016/j.infsof.2017.01.006},
	abstract = {Objective: The objective of this paper is to characterize the state-of-the-art research on SE taxonomies.
Method: A systematic mapping study was conducted, based on 270 primary studies.
Results: An increasing number of SE taxonomies have been published since 2000 in a broad range of venues, including the top SE journals and conferences. The majority of taxonomies can be grouped into the following SWEBOK knowledge areas: construction (19.55\%), design (19.55\%), requirements (15.50\%) and maintenance (11.81\%). Illustration (45.76\%) is the most frequently used approach for taxonomy validation. Hierarchy (53.14\%) and faceted analysis (39.48\%) are the most frequently used classiﬁcation structures. Most taxonomies rely on qualitative procedures to classify subject matter instances, but in most cases (86.53\%) these procedures are not described in suﬃcient detail. The majority of the taxonomies (97\%) target unique subject matters and many taxonomy-papers are cited frequently. Most SE taxonomies are designed in an ad-hoc way. To address this issue, we have revised an existing method for developing taxonomies in a more systematic way.
Conclusion: There is a strong interest in taxonomies in SE, but few taxonomies are extended or revised. Taxonomy design decisions regarding the used classiﬁcation structures, procedures and descriptive bases are usually not well described and motivated.},
	language = {en},
	urldate = {2021-07-14},
	journal = {Information and Software Technology},
	author = {Usman, Muhammad and Britto, Ricardo and Börstler, Jürgen and Mendes, Emilia},
	month = may,
	year = {2017},
	pages = {43--59},
}

@article{DBLP:journals/corr/abs-2010-03525,
	title = {{ACM} {SIGSOFT} empirical standards},
	volume = {abs/2010.03525},
	url = {https://arxiv.org/abs/2010.03525},
	journal = {CoRR},
	author = {Ralph, Paul and Baltes, Sebastian and Bianculli, Domenico and Dittrich, Yvonne and Felderer, Michael and Feldt, Robert and Filieri, Antonio and Furia, Carlo Alberto and Graziotin, Daniel and He, Pinjia and Hoda, Rashina and Juristo, Natalia and Kitchenham, Barbara A. and Robbes, Romain and Méndez, Daniel and Molleri, Jefferson and Spinellis, Diomidis and Staron, Miroslaw and Stol, Klaas-Jan and Tamburri, Damian A. and Torchiano, Marco and Treude, Christoph and Turhan, Burak and Vegas, Sira},
	year = {2020},
	note = {arXiv: 2010.03525
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/corr/abs-2010-03525.bib
tex.timestamp: Mon, 18 Jan 2021 08:56:31 +0100},
}

@article{wuFeasibilityStealthilyIntroducing,
	title = {On the {Feasibility} of {Stealthily} {Introducing} {Vulnerabilities} in {Open}-{Source} {Software} via {Hypocrite} {Commits}},
	abstract = {Open source software (OSS) has thrived since the forming of Open Source Initiative in 1998. A prominent example is the Linux kernel, which has been used by numerous major software vendors and empowering billions of devices. The higher availability and lower costs of OSS boost its adoption, while its openness and flexibility enable quicker innovation. More importantly, the OSS development approach is believed to produce more reliable and higher-quality software since it typically has thousands of independent programmers testing and fixing bugs of the software collaboratively.},
	language = {en},
	author = {Wu, Qiushi and Lu, Kangjie},
	pages = {16},
}

@article{gatesExpandingParticipationUndergraduate1999,
	title = {Expanding {Participation} in {Undergraduate} {Research} {Using} the {Affinity} {Group} {Model}*},
	volume = {88},
	issn = {10694730},
	url = {http://doi.wiley.com/10.1002/j.2168-9830.1999.tb00467.x},
	doi = {10.1002/j.2168-9830.1999.tb00467.x},
	abstract = {The benefits of working in a research group are clear: students develop domain expertise, gain an understanding and appreciation of the research process and its practice, and acquire team, communication, problem-solving, and higher-level thinking skills. Students with this experience are better equipped to make informed judgements about technical matters and to communicate and work in teams to solve complex problems. Clearly, this type of research experience must be made available to a broader population. This paper discusses how the Systems and Software Engineering Affinity Research Group model provides a socialization mechanism and infrastructure that supports the development and management of large research groups that engage undergraduate and graduate students, who have a wide range of skill levels and experiences, in research and projects. This non-hierarchical model, which is based on the cooperative paradigm, integrates students into small research groups and an encompassing large research group, and uses structured activities to develop their research, technical, communication, and group skills.},
	language = {en},
	number = {4},
	urldate = {2021-05-04},
	journal = {Journal of Engineering Education},
	author = {Gates, Ann Q. and Teller, Patricia J. and Bernat, Andrew and Delgado, Nelly and Della-Piana, Connie Kubo},
	month = oct,
	year = {1999},
	pages = {409--414},
}

@phdthesis{emami2020informing,
	title = {Informing privacy and security decision making in an {IoT} world},
	school = {Carnegie Mellon University},
	author = {Emami-Naeini, Pardis},
	year = {2020},
}

@article{leveson_software_1991,
	title = {Software safety in embedded computer systems},
	volume = {34},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/102792.102799},
	doi = {10.1145/102792.102799},
	language = {en},
	number = {2},
	urldate = {2021-08-26},
	journal = {Communications of the ACM},
	author = {Leveson, Nancy G.},
	month = feb,
	year = {1991},
	pages = {34--46},
}

@article{nguyen_cyber-physical_2018,
	title = {Cyber-{Physical} {Specification} {Mismatches}},
	volume = {2},
	issn = {2378-962X, 2378-9638},
	url = {https://dl.acm.org/doi/10.1145/3170500},
	doi = {10.1145/3170500},
	abstract = {Embedded systems use increasingly complex software and are evolving into cyber-physical systems (CPS) with sophisticated interaction and coupling between physical and computational processes. Many CPS operate in safety-critical environments and have stringent certification, reliability, and correctness requirements. These systems undergo changes throughout their lifetimes, where either the software or physical hardware is updated in subsequent design iterations. One source of failure in safety-critical CPS is when there are unstated assumptions in either the physical or cyber parts of the system, and new components do not match those assumptions. In this work, we present an automated method toward identifying unstated assumptions in CPS. Dynamic specifications in the form of candidate invariants of both the software and physical components are identified using dynamic analysis (executing and/or simulating the system implementation or model thereof). A prototype tool called Hynger (for HYbrid iNvariant GEneratoR) was developed that instruments Simulink/Stateflow (SLSF) model diagrams to generate traces in the input format compatible with the Daikon invariant inference tool, which has been extensively applied to software systems. Hynger, in conjunction with Daikon, is able to detect candidate invariants of several CPS case studies. We use the running example of a DC-to-DC power converter and demonstrate that Hynger can detect a specification mismatch where a tolerance assumed by the software is violated due to a plant change. Another case study of an automotive control system is also introduced to illustrate the power of Hynger and Daikon in automatically identifying cyber-physical specification mismatches.},
	language = {en},
	number = {4},
	urldate = {2021-07-14},
	journal = {ACM Transactions on Cyber-Physical Systems},
	author = {Nguyen, Luan V. and Hoque, Khaza Anuarul and Bak, Stanley and Drager, Steven and Johnson, Taylor T.},
	month = sep,
	year = {2018},
	pages = {1--26},
}

@inproceedings{celik_iotguard_2019,
	address = {San Diego, CA},
	title = {{IoTGuard}: {Dynamic} {Enforcement} of {Security} and {Safety} {Policy} in {Commodity} {IoT}},
	isbn = {978-1-891562-55-6},
	shorttitle = {{IoTGuard}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_07A-1_Celik_paper.pdf},
	doi = {10.14722/ndss.2019.23326},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital connectivity has changed the way we live, play, and work. To date, the traditional approach to securing IoT has treated devices individually. However, in practice, it has been recently shown that the interactions among devices are often the real cause of safety and security violations. In this paper, we present IOTGUARD, a dynamic, policy-based enforcement system for IoT, which protects users from unsafe and insecure device states by monitoring the behavior of IoT and triggeraction platform apps. IOTGUARD operates in three phases: (a) implementation of a code instrumentor that adds extra logic to an app’s source code to collect app’s information at runtime, (b) storing the apps’ information in a dynamic model that represents the runtime execution behavior of apps, and (c) identifying IoT safety and security policies, and enforcing relevant policies on the dynamic model of individual apps or sets of interacting apps. We demonstrate IOTGUARD on 20 ﬂawed apps and ﬁnd that IOTGUARD correctly enforces 12 of the 12 policy violations. In addition, we evaluate IOTGUARD on 35 SmartThings IoT and 30 IFTTT trigger-action platform market apps executed in a simulated smart home. IOTGUARD enforces 11 unique policies and blocks 16 states in six (17.1\%) SmartThings and ﬁve (16.6\%) IFTTT apps. IOTGUARD imposes only 17.3\% runtime overhead on an app and 19.8\% for ﬁve interacting apps. Through this effort, we introduce a rigorously grounded system for enforcing correct operation of IoT devices through systematically identiﬁed IoT policies, demonstrating the effectiveness and value of monitoring IoT apps with tools such as IOTGUARD.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Celik, Z. Berkay and Tan, Gang and McDaniel, Patrick},
	year = {2019},
}

@article{wolf_safety_2018,
	title = {Safety and {Security} in {Cyber}-{Physical} {Systems} and {Internet}-of-{Things} {Systems}},
	volume = {106},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/8232537/},
	doi = {10.1109/JPROC.2017.2781198},
	abstract = {Safety and security have traditionally been distinct problems in engineering and computer science. The introduction of computing elements to create cyber-physical systems (CPSs) has opened up a vast new range of potential problems that do not always show up on the radar of traditional engineers. Security, in contrast, is traditionally viewed as a data or communications security problem to be handled by computer scientists and/ or computer engineers. Advances in CPSs and the Internetof-Things (IoT) requires us to take a unified view of safety and security. This paper defines a safety/security threat model for CPSs and IoT systems and surveys emerging techniques which improve the safety and security of CPSs and IoT systems.},
	language = {en},
	number = {1},
	urldate = {2021-08-26},
	journal = {Proceedings of the IEEE},
	author = {Wolf, Marilyn and Serpanos, Dimitrios},
	month = jan,
	year = {2018},
	pages = {9--20},
}

@inproceedings{lee_development_2017,
	address = {Sapporo, Japan},
	title = {Development of an {IoT}-based bridge safety monitoring system},
	isbn = {978-1-5090-4897-7},
	url = {http://ieeexplore.ieee.org/document/7988352/},
	doi = {10.1109/ICASI.2017.7988352},
	abstract = {In this study, an IoT-based bridge safety monitoring system is developed using the ZigBee technology. This system is composed of: (1) monitoring devices installed in the bridge environment; (2) communication devices connecting the bridge monitoring devices and the cloud-based server; (3) a dynamic database that stores bridge condition data; and (4) a cloud-based server that calculates and analyzes data transmitted from the monitoring devices. This system can monitor and analyze in real time the conditions of a bridge and its environment, including the waters levels nearby, pipelines, air and other safety conditions. The detected data and images are transmitted to the server and database for users to have realtime monitoring of the bridge conditions via mobile telecommunication devices.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {2017 {International} {Conference} on {Applied} {System} {Innovation} ({ICASI})},
	publisher = {IEEE},
	author = {Lee, Jin-Lian and Tyan, Yaw-Yauan and Wen, Ming-Hui and Wu, Yun-Wu},
	month = may,
	year = {2017},
	pages = {84--86},
}

@inproceedings{singh_systematic_1999,
	address = {Takamatsu, Japan},
	title = {A systematic approach to software safety},
	isbn = {978-0-7695-0509-1},
	url = {http://ieeexplore.ieee.org/document/809632/},
	doi = {10.1109/APSEC.1999.809632},
	abstract = {This paper briefly presents a modified software quality framework that could be employed to address critical software specifications, primarily of safety, within the context of total quality and with differing interests of stakeholders in mind. The modified framework is derived from the McCall software quality-factors framework to accommodate safety.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings {Sixth} {Asia} {Pacific} {Software} {Engineering} {Conference} ({ASPEC}'99) ({Cat}. {No}.{PR00509})},
	publisher = {IEEE Comput. Soc},
	author = {Singh, R.},
	year = {1999},
	pages = {420--423},
}

@inproceedings{conmy_assuring_2014,
	address = {Miami Beach, FL, USA},
	title = {Assuring {Safety} for {Component} {Based} {Software} {Engineering}},
	isbn = {978-1-4799-3466-9 978-1-4799-3465-2},
	url = {http://ieeexplore.ieee.org/document/6754596/},
	doi = {10.1109/HASE.2014.25},
	abstract = {Developing Safety-Critical Systems (SCS) is an expensive activity largely due to the cost of testing both components and the systems produced by integrating them. In more mainstream system design, Model-Based Development (MBD) and ComponentBased Software Engineering (CBSE) are seen as complementary activities that can reduce these costs, however their use is not yet well supported in the safety critical domain, as safety is an emergent property. The contributions of this paper are to describe some of the challenges of using these approaches in SCS, and then argue how through appropriate safety argument patterns the challenges can be addressed.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {2014 {IEEE} 15th {International} {Symposium} on {High}-{Assurance} {Systems} {Engineering}},
	publisher = {IEEE},
	author = {Conmy, Philippa and Bate, Iain},
	month = jan,
	year = {2014},
	pages = {121--128},
}

@article{rowland1995professional,
	title = {Professional competence in safety-related software engineering},
	volume = {10},
	number = {2},
	journal = {Software Engineering Journal},
	author = {Rowland, JJ and Rowland, D},
	year = {1995},
	note = {Publisher: IET},
	pages = {43--48},
}

@article{noauthor_internet_nodate,
	title = {Internet of {Things} ({IoT}) in high-risk {Environment}, {Health} and {Safety} ({EHS}) industries: {A} comprehensive review {\textbar} {Elsevier} {Enhanced} {Reader}},
	language = {en},
	journal = {Internet of Things},
	pages = {18},
}

@article{taivalsaari_roadmap_2017,
	title = {A {Roadmap} to the {Programmable} {World}: {Software} {Challenges} in the {IoT} {Era}},
	volume = {34},
	issn = {0740-7459, 1937-4194},
	shorttitle = {A {Roadmap} to the {Programmable} {World}},
	url = {https://ieeexplore.ieee.org/document/7819416/},
	doi = {10.1109/MS.2017.26},
	language = {en},
	number = {1},
	urldate = {2021-07-14},
	journal = {IEEE Software},
	author = {Taivalsaari, Antero and Mikkonen, Tommi},
	month = jan,
	year = {2017},
	pages = {72--80},
}

@article{aly_is_2019,
	title = {Is {Fragmentation} a {Threat} to the {Success} of the {Internet} of {Things}?},
	volume = {6},
	issn = {2327-4662, 2372-2541},
	url = {https://ieeexplore.ieee.org/document/8424819/},
	doi = {10.1109/JIOT.2018.2863180},
	language = {en},
	number = {1},
	urldate = {2021-07-30},
	journal = {IEEE Internet of Things Journal},
	author = {Aly, Mohab and Khomh, Foutse and Gueheneuc, Yann-Gael and Washizaki, Hironori and Yacout, Soumaya},
	month = feb,
	year = {2019},
	pages = {472--487},
}

@inproceedings{heimdahl_safety_2007,
	address = {Minneapolis, MN, USA},
	title = {Safety and {Software} {Intensive} {Systems}: {Challenges} {Old} and {New}},
	isbn = {978-0-7695-2829-8},
	shorttitle = {Safety and {Software} {Intensive} {Systems}},
	url = {http://ieeexplore.ieee.org/document/4221617/},
	doi = {10.1109/FOSE.2007.18},
	abstract = {There is an increased use of software in safety-critical systems; a trend that is likely to continue in the future. Although traditional system safety techniques are applicable to software intensive systems, there are new challenges emerging. In this report we will address four issues we believe will pose challenges in the future.},
	language = {en},
	urldate = {2021-08-30},
	booktitle = {Future of {Software} {Engineering} ({FOSE} '07)},
	publisher = {IEEE},
	author = {Heimdahl, Mats P.E.},
	month = may,
	year = {2007},
	pages = {137--152},
}

@inproceedings{loupos2019cognition,
	title = {Cognition enabled {IoT} platform for industrial {IoT} safety, security and {Privacy}—{The} {CHARIOT} project},
	booktitle = {2019 {IEEE} 24th international workshop on computer aided modeling and design of communication links and networks ({CAMAD})},
	author = {Loupos, Konstantinos and Caglayan, Bora and Papageorgiou, Alexandros and Starynkevitch, Basile and Vedrine, Franck and Skoufis, Christos and Christofi, Stelios and Karakostas, Bill and Mygiakis, Antonis and Theofilis, George and {others}},
	year = {2019},
	note = {tex.organization: IEEE},
	pages = {1--4},
}

@article{kanan2018iot,
	title = {An {IoT}-based autonomous system for workers' safety in construction sites with real-time alarming, monitoring, and positioning strategies},
	volume = {88},
	journal = {Automation in Construction},
	author = {Kanan, Riad and Elhassan, Obaidallah and Bensalem, Rofaida},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {73--86},
}

@article{witjaksono_iot_2018,
	title = {{IOT} for {Agriculture}: {Food} {Quality} and {Safety}},
	volume = {343},
	issn = {1757-8981, 1757-899X},
	shorttitle = {{IOT} for {Agriculture}},
	url = {https://iopscience.iop.org/article/10.1088/1757-899X/343/1/012023},
	doi = {10.1088/1757-899X/343/1/012023},
	abstract = {Food is the main energy source for the living beings; as such food quality and safety have been in the highest demand throughout the human history. Internet of things (IOT) is a technology with a vision to connect anything at anytime and anywhere. Utilizing IOT in the food supply chain (FSC) is believed to enhance the quality of life by tracing and tracking the food conditions and live-sharing the obtained data with the consumers or the FSC supervisors. Currently, full application of IOT in the FSC is still in the developing stage and there is a big gap for improvements. The purpose of this paper is to explore the possibility of applying IOT for agriculture to trace and track food quality and safety. Mobile application for food freshness investigation was successfully developed and the results showed that consumer mobile camera could be used to test the freshness of food. By applying the IOT technology this information could be shared with all the consumers and also the supervisors.},
	language = {en},
	urldate = {2021-08-26},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Witjaksono, Gunawan and Saeed Rabih, Almur Abdelkreem and Yahya, Noorhana bt and Alva, Sagir},
	month = mar,
	year = {2018},
	pages = {012023},
}

@inproceedings{vishal_iot-driven_2017,
	address = {Mysuru},
	title = {{IoT}-driven road safety system},
	isbn = {978-1-5386-1205-7 978-1-5386-2361-9},
	url = {http://ieeexplore.ieee.org/document/8284624/},
	doi = {10.1109/ICEECCOT.2017.8284624},
	abstract = {Roads are integral part of human civilization. They are the nervous system of any country; hence these are being laid on hill sides and narrow ridges which is a major hazard to human life. As roads play a crucial role in our daily routine these can be modelled in a smart manner to serve us with enhanced capabilities. The architecture of IoT is comprised of an ability to make things more coherent and effective. This paper synchronizes the concept of IoT with roads to make them smart. The paper talks about using the IoT technologies, with the onset of smart cities, to reduce the risk of run off road collisions. As every vehicle is IoT enabled and connected to the internet, we have an effective technique to guide emergency service vehicles through the road within least time. This IoT system is a combination of simple cost-effective antenna technology and internet platforms which works with complete automation. These abilities will make the system to serve us with better accuracy and delicacy.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {2017 {International} {Conference} on {Electrical}, {Electronics}, {Communication}, {Computer}, and {Optimization} {Techniques} ({ICEECCOT})},
	publisher = {IEEE},
	author = {Vishal, Dasari and Afaque, H. Saliq and Bhardawaj, Harsh and Ramesh, T. K.},
	month = dec,
	year = {2017},
	pages = {1--5},
}

@article{chatterjee_safety_2020,
	title = {The safety of {IoT}-enabled system in smart cities of {India}: do ethics matter?},
	volume = {36},
	issn = {2514-9369, 2514-9369},
	shorttitle = {The safety of {IoT}-enabled system in smart cities of {India}},
	url = {https://www.emerald.com/insight/content/doi/10.1108/IJOES-05-2019-0085/full/html},
	doi = {10.1108/IJOES-05-2019-0085},
	abstract = {Purpose – The purpose of this study is to analyze the impact of ethics and technology towards safety of internet of things (IoT)-enabled system in smart cities of India (SCI). Design/methodology/approach – The determinants that would impact on securing IoT-enabled system in SCI have been identiﬁed by the studies of literature. Some hypothesis has been formulated. A conceptual model has been developed. Hypotheses and conceptual models have been tested by a statistical approach through survey works considering the feedbacks of 331 usable respondents. The results have been discussed followed by explaining the implications of this study. A comprehensive conclusion has been provided at the end.},
	language = {en},
	number = {4},
	urldate = {2021-08-26},
	journal = {International Journal of Ethics and Systems},
	author = {Chatterjee, Sheshadri},
	month = sep,
	year = {2020},
	pages = {601--618},
}

@inproceedings{wu_design_2019,
	address = {Limerick, Ireland},
	title = {Design and {Implementation} of a {Wearable} {Sensor} {Network} {System} for {IoT}-{Connected} {Safety} and {Health} {Applications}},
	isbn = {978-1-5386-4980-0},
	url = {https://ieeexplore.ieee.org/document/8767280/},
	doi = {10.1109/WF-IoT.2019.8767280},
	abstract = {This paper presents a wearable sensor network system for Internet of Things (IoT) connected safety and health applications. Safety and health of workers are important for industrial workplace; therefore, an IoT network system which can monitor both environmental and physiological can greatly improve the safety in the workplace. The proposed network system incorporates multiple wearable sensors to monitor environmental and physiological parameters. The wearable sensors on different subjects can communicate with each other and transmit the data to a gateway via a LoRa network which forms a heterogeneous IoT platform with Bluetooth-based medical signal sensing network. Once harmful environments are detected and, the sensor node will provide an effective notiﬁcation and warning mechanism for the users. A smart IoT gateway is implemented to provide data processing, local web server and cloud connection. After the gateway receives the data from wearable sensors, it will forward the data to an IoT cloud for further data storage, processing and visualization.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {2019 {IEEE} 5th {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	publisher = {IEEE},
	author = {Wu, Fan and Wu, Taiyang and Yuce, Mehmet Rasit},
	month = apr,
	year = {2019},
	pages = {87--90},
}

@article{chung_iot-based_2020,
	title = {{IoT}-based application for construction site safety monitoring},
	issn = {1562-3599, 2331-2327},
	url = {https://www.tandfonline.com/doi/full/10.1080/15623599.2020.1847405},
	doi = {10.1080/15623599.2020.1847405},
	abstract = {Hong Kong construction safety has witnessed substantial improvement in the last three decades, however, accidents still occur frequently as more than 4,000 accidents are reported in the year 2017. Against this background, this research, firstly, aims to investigate the effectiveness of safety training for construction personnel in Hong Kong. A questionnaire is designed to explore the efficacy and weaknesses of mandatory basic safety training. The results indicate the inadequate knowledge of the concept of personal protective equipment as the main weakness of the workers. Secondly, to overcome the training weakness, an Internet-of-Things (IoT) based innovative safety model is designed to provide real-time monitoring of construction site personnel and environment. The proposed model not only identifies realtime personnel safety problems, i.e., near misses, to reduce the accident rates but also stores the digital data to improve future training and system itself. The proposed model in this research provides a costeffective solution for optimal construction safety to the stakeholders. A cost comparison analysis suggests that the IoT system can provide 1) 78\% cost-savings with respect to the traditional manual system and 2) 65\% cost-savings with respect to the traditional sensor system.},
	language = {en},
	urldate = {2021-08-26},
	journal = {International Journal of Construction Management},
	author = {Chung, William Wong Shiu and Tariq, Salman and Mohandes, Saeed Reza and Zayed, Tarek},
	month = nov,
	year = {2020},
	pages = {1--17},
}

@inproceedings{ding_safety_2018,
	address = {Toronto Canada},
	title = {On the {Safety} of {IoT} {Device} {Physical} {Interaction} {Control}},
	isbn = {978-1-4503-5693-0},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243865},
	doi = {10.1145/3243734.3243865},
	abstract = {Emerging Internet of Things (IoT) platforms provide increased functionality to enable human interaction with the physical world in an autonomous manner. The physical interaction features of IoT platforms allow IoT devices to make an impact on the physical environment. However, such features also bring new safety challenges, where attackers can leverage stealthy physical interactions to launch attacks against IoT systems. In this paper, we propose a framework called IoTMon that discovers any possible physical interactions and generates all potential interaction chains across applications in the IoT environment. IoTMon also includes an assessment of the safety risk of each discovered inter-app interaction chain based on its physical influence. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of IoTMon and present a comprehensive system evaluation on the Samsung SmartThings platform. We study 185 official SmartThings applications and find they can form 162 hidden inter-app interaction chains through physical surroundings. In particular, our experiment reveals that 37 interaction chains are highly risky and could be potentially exploited to impact the safety of the IoT environment.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Ding, Wenbo and Hu, Hongxin},
	month = oct,
	year = {2018},
	pages = {832--846},
}

@inproceedings{nguyen_iotsan_2018,
	address = {Heraklion Greece},
	title = {{IotSan}: fortifying the safety of {IoT} systems},
	isbn = {978-1-4503-6080-7},
	shorttitle = {{IotSan}},
	url = {https://dl.acm.org/doi/10.1145/3281411.3281440},
	doi = {10.1145/3281411.3281440},
	abstract = {Today’s IoT systems include event-driven smart applications (apps) that interact with sensors and actuators. A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states. Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. In this paper, we design IotSan, a novel practical system that uses model checking as a building block to reveal “interaction-level” flaws by identifying events that can lead the system to unsafe states. In building IotSan, we design novel techniques tailored to IoT systems, to alleviate the state explosion associated with model checking. IotSan also automatically translates IoT apps into a format amenable to model checking. Finally, to understand the root cause of a detected vulnerability, we design an attribution mechanism to identify problematic and potentially malicious apps. We evaluate IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities. We also evaluate IotSan with malicious SmartThings apps from a previous effort. IotSan detects the potential safety violations and also effectively attributes these apps as malicious.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the 14th {International} {Conference} on emerging {Networking} {EXperiments} and {Technologies}},
	publisher = {ACM},
	author = {Nguyen, Dang Tu and Song, Chengyu and Qian, Zhiyun and Krishnamurthy, Srikanth V. and Colbert, Edward J. M. and McDaniel, Patrick},
	month = dec,
	year = {2018},
	pages = {191--203},
}

@article{medikonda_framework_2009,
	title = {A framework for software safety in safety-critical systems},
	volume = {34},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/1507195.1507207},
	doi = {10.1145/1507195.1507207},
	abstract = {Software for safety-critical systems must deal with the hazards identified by safety analysis in order to make the system safe, riskfree, and fail-safe. Because human lives may be lost and tremendous economic costs may result if the software fails, the development of high-integrity software adopts practices that impose greater rigor on the software development processes. Software safety is a composite of many factors. Existing software quality models like McCall’s and Boehm’s and ISO 9126 are inadequate in addressing the software safety issues of real time safety-critical embedded systems. At present there does not exist any standard framework that comprehensively addresses the factors, criteria and metrics (FCM) approach of the quality models in respect of software safety. The safety of a software component must be considered within the context of both the overall system of which it is a component and the environment in which this system operates. It is not useful to investigate the safety of a software component in isolation. This paper proposes a new framework for software safety based on the McCall’s software quality model that specifically identifies the criteria corresponding to software safety in safety critical applications. The criteria in the proposed software safety framework pertains to system hazard analysis, completeness of requirements, identification of softwarerelated safety-critical requirements, safety-constraints based design, run-time issues management, and software safety-critical testing. This framework is then applied to a prototype safetycritical system viz. a software–based Railroad Crossing Control System (RCCS) to validate its utility.},
	language = {en},
	number = {2},
	urldate = {2021-08-26},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Medikonda, Ben Swarup and Panchumarthy, Seetha Ramaiah},
	month = feb,
	year = {2009},
	pages = {1--9},
}

@article{penzenstadler_safety_2014,
	title = {Safety, {Security}, {Now} {Sustainability}: {The} {Nonfunctional} {Requirement} for the 21st {Century}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Safety, {Security}, {Now} {Sustainability}},
	url = {https://ieeexplore.ieee.org/document/6728940/},
	doi = {10.1109/MS.2014.22},
	language = {en},
	number = {3},
	urldate = {2021-08-26},
	journal = {IEEE Software},
	author = {Penzenstadler, Birgit and Raturi, Ankita and Richardson, Debra and Tomlinson, Bill},
	month = may,
	year = {2014},
	pages = {40--47},
}

@article{bhansali_software_2005,
	title = {Software safety: current status and future direction},
	volume = {30},
	issn = {0163-5948},
	shorttitle = {Software safety},
	url = {https://dl.acm.org/doi/10.1145/1039174.1039193},
	doi = {10.1145/1039174.1039193},
	abstract = {This paper describes the current status of software safety in terms of research and existing standards. It highlights the differences between various standards set up by government agencies to accomplish the same safety objectives. For example, European standards tend to place more emphasis on static analysis whereas American standards prefer dynamic testing to verify the software. An optimal verification approach is still a debatable issue in the software safety community. As for future direction, the author believes that the key to making safer and cheaper software is to have better requirements validation that ensure that the requirements are correct and complete before the design and coding phases begin.},
	language = {en},
	number = {1},
	urldate = {2021-08-26},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Bhansali, P. V.},
	month = jan,
	year = {2005},
	pages = {3},
}

@inproceedings{beltrame_engineering_2018,
	address = {Gothenburg Sweden},
	title = {Engineering safety in swarm robotics},
	isbn = {978-1-4503-5760-9},
	url = {https://dl.acm.org/doi/10.1145/3196558.3196565},
	doi = {10.1145/3196558.3196565},
	abstract = {Robotics, artificial intelligence, and the Internet-of-Things are driving current research and development for the technology sector. Robotic and multi-robot systems are becoming pervasive and more and more lives rely on their proper functioning in transportation, medical systems, personal robotics, and manufacturing. Assuring the security and safety of these systems is of primary importance to guarantee the real-world applicability of current research, and we argue that it should be an integral part of system design. Current software standards for safety and security for critical systems (e.g. industrial and aerospace) are not directly applicable to the large distributed systems that are envisioned for the near future. In this paper, we propose to address safety and security of swarm robotics systems at the programming language level. We propose to extend the Buzz multi-robot scripting language with constructs and code analysis that allow the verification of safety and security during development. We believe that detecting and correcting issues with what are inherently emergent systems—i.e. where collective behavior might not be immediately apparent from a single robot’s code—during development would allow for a more effective advancement of swarm robotics.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Robotics} {Software} {Engineering}},
	publisher = {ACM},
	author = {Beltrame, Giovanni and Merlo, Ettore and Panerati, Jacopo and Pinciroli, Carlo},
	month = may,
	year = {2018},
	pages = {36--39},
}

@article{macher_filling_2015,
	title = {Filling the gap between automotive systems, safety, and software engineering},
	volume = {132},
	issn = {0932-383X, 1613-7620},
	url = {http://link.springer.com/10.1007/s00502-015-0301-x},
	doi = {10.1007/s00502-015-0301-x},
	language = {en},
	number = {3},
	urldate = {2021-08-26},
	journal = {e \& i Elektrotechnik und Informationstechnik},
	author = {Macher, Georg and Stolz, Michael and Armengaud, Eric and Kreiner, Christian},
	month = jun,
	year = {2015},
	pages = {142--148},
}

@misc{firesmith_engineering_2007,
	title = {Engineering {Safety}- and {Security}-{Related} {Requirements} for {Software}- {Intensive} {Systems}},
	author = {Firesmith, Donald},
	year = {2007},
}

@inproceedings{bak_sandboxing_2011,
	address = {Chicago, IL, USA},
	title = {Sandboxing {Controllers} for {Cyber}-{Physical} {Systems}},
	isbn = {978-1-61284-640-8},
	url = {http://ieeexplore.ieee.org/document/5945416/},
	doi = {10.1109/ICCPS.2011.25},
	abstract = {Cyber-physical systems bridge the gap between cyber components, typically written in software, and the physical world. Software written with traditional development practices, however, likely contains bugs or unintended interactions among components, which can result in uncontrolled and possibly disastrous physical-world interactions. Complete veriﬁcation of cyber-physical systems, however, is often impractical due to outsourced development of software, cost, software created without formal models, or excessively large or complex models where the veriﬁcation process becomes intractable.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2011 {IEEE}/{ACM} {Second} {International} {Conference} on {Cyber}-{Physical} {Systems}},
	publisher = {IEEE},
	author = {Bak, Stanley and Manamcheri, Karthik and Mitra, Sayan and Caccamo, Marco},
	month = apr,
	year = {2011},
	pages = {3--12},
}

@inproceedings{johnson_safety_2010,
	address = {Manchester, UK},
	title = {Safety arguments for next generation, location aware computing},
	isbn = {978-1-84919-303-0},
	url = {https://digital-library.theiet.org/content/conferences/10.1049/cp.2010.0813},
	doi = {10.1049/cp.2010.0813},
	abstract = {A range of common software components are gradually being integrated into the infrastructures that support safety-critical systems. These include network management tools, operating systems-especially Linux, Voice Over IP (VOIP) communications technologies, Satellite Based Augmentation Systems for navigation/timing data etc. The increasing use of these common components creates concerns that bugs might affect multiple systems across many different safety-related industries. It also raises significant security concerns. Malware has been detected in power distribution, healthcare, military and transportation infrastructures. Most previous attacks do not seem to have deliberately targeted critical applications. However, there is no room for complacency in the face of increasing vulnerability to cyber attacks on safetyrelated systems. This paper illustrates the threat to Air Traffic Management infrastructures and goes on to present a roadmap to increase our resilience to future CyberSafety attacks. Some components of this proposal are familiar concepts from Security Management Systems (SecMS); including a focus on incident reporting and the need for improved risk assessment tools. Other components of the roadmap focus on structural and organizational problems that have limited the effectiveness of existing SecMS; in particular there is a need to raise awareness amongst regulators and senior management who often lack the technical and engineering background to understand the nature of the threats to safety-critical software.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {5th {IET} {International} {Conference} on {System} {Safety} 2010},
	publisher = {IET},
	author = {Johnson, C.W. and Holloway, C.M.},
	year = {2010},
	pages = {1C1--1C1},
}

@article{firesmith_engineering_2004,
	title = {Engineering {Safety} {Requirements}, {Safety} {Constraints}, and {Safety}-{Critical} {Requirements}.},
	volume = {3},
	issn = {1660-1769},
	url = {http://www.jot.fm/contents/issue_2004_03/column3.html},
	doi = {10.5381/jot.2004.3.3.c3},
	abstract = {As software-intensive systems become more pervasive, more and more safety-critical systems are being developed. In this column, I will use the concept of a quality model to define safety as a quality factor. Thus, safety (like security and survivability) is a kind of defensibility, which is a kind of dependability, which is a kind of quality. Next, I discuss the structure of quality requirements and show how safety requirements can be engineered based on safety’s numerous quality subfactors. Then, I define and discuss safety constraints (i.e., mandated safeguards) and safety-critical requirements (i.e., functional, data, and interface requirements that can cause accidents if not implemented correctly). Finally, I pose a set of questions regarding the engineering of these three kinds of safety-related requirements for future research and experience to answer.},
	language = {en},
	number = {3},
	urldate = {2021-08-26},
	journal = {The Journal of Object Technology},
	author = {Firesmith, Donald},
	year = {2004},
	pages = {27},
}

@techreport{firesmith_common_2003,
	address = {Fort Belvoir, VA},
	title = {Common {Concepts} {Underlying} {Safety} {Security} and {Survivability} {Engineering}:},
	shorttitle = {Common {Concepts} {Underlying} {Safety} {Security} and {Survivability} {Engineering}},
	url = {http://www.dtic.mil/docs/citations/ADA421683},
	language = {en},
	urldate = {2021-08-26},
	institution = {Defense Technical Information Center},
	author = {Firesmith, Donald G.},
	month = dec,
	year = {2003},
	doi = {10.21236/ADA421683},
}

@inproceedings{firesmith_engineering_2005,
	address = {St. Louis, MO, USA},
	title = {Engineering safety-related requirements for software-intensive systems},
	url = {http://ieeexplore.ieee.org/document/1553680/},
	doi = {10.1109/ICSE.2005.1553680},
	abstract = {Many software-intensive systems have significant safety ramifications and need to have their associated safety-related requirements properly engineered. However, there is little effective interaction and collaboration between the requirements and safety teams on most projects. This tutorial is intended to improve such collaboration by providing clear definitions of the different kinds of safety-related requirements, examples of such requirements, and a generic process for producing them.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings. 27th {International} {Conference} on {Software} {Engineering}, 2005. {ICSE} 2005.},
	publisher = {IEEe},
	author = {Firesmith, D.G.},
	year = {2005},
	pages = {720--721},
}

@article{bak_safety_2015,
	title = {Safety and {Progress} for {Distributed} {Cyber}-{Physical} {Systems} with {Unreliable} {Communication}},
	volume = {14},
	issn = {1539-9087, 1558-3465},
	url = {https://dl.acm.org/doi/10.1145/2739046},
	doi = {10.1145/2739046},
	abstract = {Cyber-physical systems (CPSs) may interact and manipulate objects in the physical world, and therefore formal guarantees about their behavior are strongly desired. Static-time proofs of safety invariants, however, may be intractable for systems with distributed physical-world interactions. This is further complicated when realistic communication models are considered, for which there may not be bounds on message delays, or even when considering that messages will eventually reach their destination.
            In this work, we address the challenge of proving safety and progress in distributed CPSs communicating over an unreliable communication layer. We show that for this type of communication model, system safety is closely related to the results of a hybrid system’s reachability computation, which can be computed at runtime. However, since computing reachability at runtime may be computationally intensive, we provide an approach that moves significant parts of the computation to design time. This approach is demonstrated with a case study of a simulation of multiple vehicles moving within a shared environment.},
	language = {en},
	number = {4},
	urldate = {2021-07-14},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Bak, Stanley and Huang, Zhenqi and Abad, Fardin Abdi Taghi and Caccamo, Marco},
	month = dec,
	year = {2015},
	pages = {1--22},
}

@inproceedings{liang_understanding_2016,
	address = {Shanghai, China},
	title = {Understanding and detecting performance and security bugs in {IOT} {OSes}},
	isbn = {978-1-5090-2239-7},
	url = {http://ieeexplore.ieee.org/document/7515933/},
	doi = {10.1109/SNPD.2016.7515933},
	abstract = {Operating system (OS) plays an important role in the efficiency and security of service in Internet of Things (IOT). Considering the limited storage resources and power utilization mechanism in IOT OS, we explore the performance bugs and security bugs of three open source IOT OS (Contiki, TinyOS and RIOT OS), including the features of bugs cause, bugs mitigation, bugs detection rules and bugs fixing in them. We present Rulede, a tool built on LLVM compiler framework to find bugs. Experimental results show that Rulede can effectively detect performance bugs and security bugs in target IOT OSes.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2016 17th {IEEE}/{ACIS} {International} {Conference} on {Software} {Engineering}, {Artificial} {Intelligence}, {Networking} and {Parallel}/{Distributed} {Computing} ({SNPD})},
	publisher = {IEEE},
	author = {Liang, Hongliang and Zhao, Qian and Wang, Yuying and Liu, Haifeng},
	month = may,
	year = {2016},
	pages = {413--418},
}

@phdthesis{bak2013verifiable,
	title = {Verifiable {COTS}-based cyber-physical systems},
	school = {University of Illinois at Urbana-Champaign},
	author = {Bak, Stanley Zbigniew},
	year = {2013},
}

@inproceedings{bak_verifying_2016,
	address = {Pittsburgh Pennsylvania},
	title = {Verifying cyber-physical systems by combining software model checking with hybrid systems reachability},
	isbn = {978-1-4503-4485-2},
	url = {https://dl.acm.org/doi/10.1145/2968478.2968490},
	doi = {10.1145/2968478.2968490},
	abstract = {Cyber-physical systems (CPS) span the communication, computation and control domains. Creating a single, complete, and detailed model of a CPS is not only diﬃcult, but, in terms of veriﬁcation, probably not useful; current veriﬁcation algorithms are likely intractable for such allencompassing models. However, speciﬁc CPS domains have specialized formal reasoning methods that can successfully analyze certain aspects of the integrated system. To prove overall system correctness, however, care must be taken to ensure the interfaces of the proofs are consistent and leave no gaps, which can be diﬃcult since they may use diﬀerent model types and describe diﬀerent aspects of the CPS.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Embedded} {Software}},
	publisher = {ACM},
	author = {Bak, Stanley and Chaki, Sagar},
	month = oct,
	year = {2016},
	pages = {1--10},
}

@inproceedings{looga_mammoth_2012,
	address = {Hangzhou, China},
	title = {{MAMMOTH}: {A} massive-scale emulation platform for {Internet} of {Things}},
	isbn = {978-1-4673-1857-0 978-1-4673-1855-6},
	shorttitle = {{MAMMOTH}},
	url = {http://ieeexplore.ieee.org/document/6664581/},
	doi = {10.1109/CCIS.2012.6664581},
	abstract = {Internet of Things (IoT) is increasingly used in a plethora of fields to enable radically new ways for various purposes, ranging from monitoring the environment to enhancing the wellbeing of human life. With the ever-increasing size of such networks, it is fundamental to understand the issues that come with scaling on different networking layers. A cost-efficient approach to examine large-scale networks is to use simulators or emulators to test the infrastructure and its ability to support the desired applications. In this paper, we investigate and compare the currently available simulation/emulation software. We found out that the current solutions are mostly appropriate for small- and medium-scale emulation, however they are not suitable for large-scale testing that reaches millions of node running concurrently. We then propose a large-scale IoT emulator called MAMMotH and present a brief overview of its design. Finally we discuss some of the current issues and future directions, e.g. radio link simulation.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2012 {IEEE} 2nd {International} {Conference} on {Cloud} {Computing} and {Intelligence} {Systems}},
	publisher = {IEEE},
	author = {Looga, Vilen and Ou, Zhonghong and Deng, Yang and Yla-Jaaski, Antti},
	month = oct,
	year = {2012},
	pages = {1235--1239},
}

@inproceedings{voas_testing_2018,
	address = {Bamberg},
	title = {Testing {IoT} {Systems}},
	isbn = {978-1-5386-5207-7},
	url = {https://ieeexplore.ieee.org/document/8359148/},
	doi = {10.1109/SOSE.2018.00015},
	abstract = {This article presents challenges and solutions to testing systems based on the underlying products and services commonly referred to as the Internet of ‘things’ (IoT).},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2018 {IEEE} {Symposium} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	publisher = {IEEE},
	author = {Voas, Jeff and Kuhn, Rick and Laplante, Phil},
	month = mar,
	year = {2018},
	pages = {48--52},
}

@article{gutierrez-madronal_iotteg_2018,
	title = {{IoT}–{TEG}: {Test} event generator system},
	volume = {137},
	issn = {01641212},
	shorttitle = {{IoT}–{TEG}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121217301280},
	doi = {10.1016/j.jss.2017.06.037},
	language = {en},
	urldate = {2021-07-14},
	journal = {Journal of Systems and Software},
	author = {Gutiérrez-Madroñal, L. and Medina-Bulo, I. and Domínguez-Jiménez, J.J.},
	month = mar,
	year = {2018},
	pages = {784--803},
}

@inproceedings{johnson_cyber-physical_2015,
	address = {Seattle Washington},
	title = {Cyber-physical specification mismatch identification with dynamic analysis},
	isbn = {978-1-4503-3455-6},
	url = {https://dl.acm.org/doi/10.1145/2735960.2735979},
	doi = {10.1145/2735960.2735979},
	abstract = {Embedded systems use increasingly complex software and are evolving into cyber-physical systems (CPS) with sophisticated interaction and coupling between physical and computational processes. Many CPS operate in safety-critical environments and have stringent certiﬁcation, reliability, and correctness requirements. These systems undergo changes throughout their lifetimes, where either the software or physical hardware is updated in subsequent design iterations. One source of failure in safety-critical CPS is when there are unstated assumptions in either the physical or cyber parts of the system, and new components do not match those assumptions. In this work, we present an automated method towards identifying unstated assumptions in CPS. Dynamic speciﬁcations in the form of candidate invariants of both the software and physical components are identiﬁed using dynamic analysis (executing and/or simulating the system implementation or model thereof). A prototype tool called Hynger (for HYbrid iNvariant GEneratoR) was developed that instruments Simulink/Stateﬂow (SLSF) model diagrams to generate traces in the input format compatible with the Daikon invariant inference tool, which has been extensively applied to software systems. Hynger, in conjunction with Daikon, is able to detect candidate invariants of several CPS case studies. We use the running example of a DC-to-DC power converter, and demonstrate that Hynger can detect a speciﬁcation mismatch where a tolerance assumed by the software is violated due to a plant change.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the {ACM}/{IEEE} {Sixth} {International} {Conference} on {Cyber}-{Physical} {Systems}},
	publisher = {ACM},
	author = {Johnson, Taylor T. and Bak, Stanley and Drager, Steven},
	month = apr,
	year = {2015},
	pages = {208--217},
}

@incollection{margaria_model-based_2016,
	address = {Cham},
	title = {Model-{Based} {Testing} as a {Service} for {IoT} {Platforms}},
	volume = {9953},
	isbn = {978-3-319-47168-6 978-3-319-47169-3},
	url = {http://link.springer.com/10.1007/978-3-319-47169-3_55},
	abstract = {The Internet of Things (IoT) has increased its footprint becoming globally a ’must have’ for today’s most innovative companies. Applications extend to multitude of domains, such as smart cities, healthcare, logistics, manufacturing, etc. Gartner Group estimates an increase up to 21 billion connected things by 2020. To manage ’things’ heterogeneity and data streams over large scale and secured deployments, IoT and data platforms are becoming a central part of the IoT. To respond to this fast growing demand we see more and more platforms being developed, requiring systematic testing. Combining Model-Based Testing (MBT) technique and a service-oriented solution, we present Model-Based Testing As A Service (MBTAAS) for testing data and IoT platforms. In this paper, we present a ﬁrst step towards MBTAAS for data and IoT Platforms, with experimentation on FIWARE, one of the EU most emerging IoT enabled platforms.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}: {Discussion}, {Dissemination}, {Applications}},
	publisher = {Springer International Publishing},
	author = {Ahmad, Abbas and Bouquet, Fabrice and Fourneret, Elizabeta and Le Gall, Franck and Legeard, Bruno},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2016},
	doi = {10.1007/978-3-319-47169-3_55},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {727--742},
}

@article{mccormickSmartDevicePush2021,
	title = {Smart {Device} {Push} {Brings} {IT}, {R}\&{D} {Teams} {Together}},
	abstract = {Information Technology (IT) staffers at consumer packaged goods companies increasingly are working with research and development (R\&D) teams to develop more connected devices. To develop Colgate's new smart toothbrush, the R\&D team worked on the brush head and sensors, while the IT team developed the underlying application, which uses machine learning to analyze data collected by the toothbrush to make recommendations for brushing better. Colgate's Mike Crowe said, "It is really about maximizing the use of the combined skills." The IT and R\&D teams at Proctor \& Gamble (P\&G) also have collaborated on a smart toothbrush, a personalized skin-care analyzer, and a facial-hair style assistant. McKinsey’s Ed Roth said teaming IT and R\&D is not without its challenges. “It’s not a natural pairing,” Roth said, because each unit is staffed by people from a different work culture.},
	journal = {The Wall Street Journal},
	author = {McCormick, John},
	month = mar,
	year = {2021},
}

@article{atzori_internet_2010,
	title = {The {Internet} of {Things}: {A} survey},
	volume = {54},
	issn = {13891286},
	shorttitle = {The {Internet} of {Things}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128610001568},
	doi = {10.1016/j.comnet.2010.05.010},
	abstract = {This paper addresses the Internet of Things. Main enabling factor of this promising paradigm is the integration of several technologies and communications solutions. Identiﬁcation and tracking technologies, wired and wireless sensor and actuator networks, enhanced communication protocols (shared with the Next Generation Internet), and distributed intelligence for smart objects are just the most relevant. As one can easily imagine, any serious contribution to the advance of the Internet of Things must necessarily be the result of synergetic activities conducted in different ﬁelds of knowledge, such as telecommunications, informatics, electronics and social science. In such a complex scenario, this survey is directed to those who want to approach this complex discipline and contribute to its development. Different visions of this Internet of Things paradigm are reported and enabling technologies reviewed. What emerges is that still major issues shall be faced by the research community. The most relevant among them are addressed in details.},
	language = {en},
	number = {15},
	urldate = {2021-07-14},
	journal = {Computer Networks},
	author = {Atzori, Luigi and Iera, Antonio and Morabito, Giacomo},
	month = oct,
	year = {2010},
	pages = {2787--2805},
}

@article{roman_features_2013,
	title = {On the features and challenges of security and privacy in distributed internet of things},
	volume = {57},
	issn = {13891286},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128613000054},
	doi = {10.1016/j.comnet.2012.12.018},
	abstract = {In the Internet of Things, services can be provisioned using centralized architectures, where central entities acquire, process, and provide information. Alternatively, distributed architectures, where entities at the edge of the network exchange information and collaborate with each other in a dynamic way, can also be used. In order to understand the applicability and viability of this distributed approach, it is necessary to know its advantages and disadvantages – not only in terms of features but also in terms of security and privacy challenges. The purpose of this paper is to show that the distributed approach has various challenges that need to be solved, but also various interesting properties and strengths.},
	language = {en},
	number = {10},
	urldate = {2021-07-14},
	journal = {Computer Networks},
	author = {Roman, Rodrigo and Zhou, Jianying and Lopez, Javier},
	month = jul,
	year = {2013},
	pages = {2266--2279},
}

@article{colakovic_internet_2018,
	title = {Internet of {Things} ({IoT}): {A} review of enabling technologies, challenges, and open research issues},
	volume = {144},
	issn = {13891286},
	shorttitle = {Internet of {Things} ({IoT})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128618305243},
	doi = {10.1016/j.comnet.2018.07.017},
	language = {en},
	urldate = {2021-07-14},
	journal = {Computer Networks},
	author = {Čolaković, Alem and Hadžialić, Mesud},
	month = oct,
	year = {2018},
	pages = {17--39},
}

@article{swamy_empirical_2020,
	title = {An {Empirical} {Study} on {System} {Level} {Aspects} of {Internet} of {Things} ({IoT})},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9218916/},
	doi = {10.1109/ACCESS.2020.3029847},
	abstract = {Internet of Things (IoT) is an integration of the Sensor, Embedded, Computing, and Communication technologies. The purpose of the IoT is to provide seamless services to anything, anytime at any place. IoT technologies play a crucial role everywhere, which brings the fourth revolution of disruptive technologies after the internet and Information and Communication Technology (ICT). The Research \& Development community has predicted that the impact of IoT will be more than the internet and ICT on society, which improves the well-being of society and industries. Addressing the predominant system-level design aspects like energy efﬁciency, robustness, scalability, interoperability, and security issues result in the use of a potential IoT system. This paper presents the current state of art of the functional pillars of IoT and its emerging applications to motivate academicians and researches to develop real-time, energyefﬁcient, scalable, reliable, and secure IoT applications. This paper summarizes the architecture of IoT, with the contemporary status of IoT architectures. Highlights of the IoT system-level issues to develop more advanced real-time IoT applications have been discussed. Millions of devices exchange information using different communication standards, and interoperability between them is a signiﬁcant issue. This paper provides the current status of the communication standards and application layer protocols used in IoT with the detailed analysis. The computing paradigms like Cloud, Cloudlet, Fog, and Edge computing facilitate IoT with various services like data ofﬂoading, resource and device management, etc. In this paper, an exhaustive analysis of Edge Computing in IoT with different edge computing architectures and existing status are deliberated. The widespread adoption of IoT in society has resulted in privacy and security issues. This paper emphasizes on analyzing the security challenges, privacy and security threats, conventional mitigation techniques, and further scope for IoT security. The features like fewer memory footprints, scheduling, real-time task execution, fewer interrupt, and thread switching latency of Real-Time Operating Systems (RTOS) enables the development of time critical IoT applications. Also, this review offers the analysis of the RTOS’s suitable for IoT with the current status and networking stack. Finally, open research issues in IoT system development are discussed.},
	language = {en},
	urldate = {2021-07-30},
	journal = {IEEE Access},
	author = {Swamy, S. Narasimha and Kota, Solomon Raju},
	year = {2020},
	pages = {188082--188134},
}

@article{zou_smart_2019,
	title = {Smart {Contract} {Development}: {Challenges} and {Opportunities}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {Smart {Contract} {Development}},
	url = {https://ieeexplore.ieee.org/document/8847638/},
	doi = {10.1109/TSE.2019.2942301},
	abstract = {Smart contract, a term which was originally coined to refer to the automation of legal contracts in general, has recently seen much interest due to the advent of blockchain technology. Recently, the term is popularly used to refer to low-level code scripts running on a blockchain platform. Our study focuses exclusively on this subset of smart contracts. Such smart contracts have increasingly been gaining ground, ﬁnding numerous important applications (e.g., crowdfunding) in the real world. Despite the increasing popularity, smart contract development still remains somewhat a mystery to many developers largely due to its special design and applications. Are there any differences between smart contract development and traditional software development? What kind of challenges are faced by developers during smart contract development? Questions like these are important but have not been explored by researchers yet. In this paper, we performed an exploratory study to understand the current state and potential challenges developers are facing in developing smart contracts on blockchains, with a focus on Ethereum (the most popular public blockchain platform for smart contracts). Toward this end, we conducted this study in two phases. In the ﬁrst phase, we conducted semi-structured interviews with 20 developers from GitHub and industry professionals who are working on smart contracts. In the second phase, we performed a survey on 232 practitioners to validate the ﬁndings from the interviews. Our interview and survey results revealed several major challenges developers are facing during smart contract development: (1) there is no effective way to guarantee the security of smart contract code; (2) existing tools for development are still very basic; (3) the programming languages and the virtual machines still have a number of limitations; (4) performance problems are hard to handle under resource constrained running environment; and (5) online resources (including advanced/updated documents and community support) are still limited. Our study suggests several directions that researchers and practitioners can work on to help improve developers’ experience on developing high-quality smart contracts.},
	language = {en},
	urldate = {2021-07-14},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zou, Weiqin and Lo, David and Kochhar, Pavneet Singh and Le, Xuan-Bach D. and Xia, Xin and Feng, Yang and Chen, Zhenyu and Xu, Baowen},
	year = {2019},
	pages = {1--1},
}

@article{risteska_stojkoska_review_2017,
	title = {A review of {Internet} of {Things} for smart home: {Challenges} and solutions},
	volume = {140},
	issn = {09596526},
	shorttitle = {A review of {Internet} of {Things} for smart home},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095965261631589X},
	doi = {10.1016/j.jclepro.2016.10.006},
	abstract = {Although Internet of Things (IoT) brings signiﬁcant advantages over traditional communication technologies for smart grid and smart home applications, these implementations are still very rare. Relying on a comprehensive literature review, this paper aims to contribute towards narrowing the gap between the existing state-of-the-art smart home applications and the prospect of their integration into an IoT enabled environment. We propose a holistic framework which incorporates different components from IoT architectures/frameworks proposed in the literature, in order to efﬁciently integrate smart home objects in a cloud-centric IoT based solution. We identify a smart home management model for the proposed framework and the main tasks that should be performed at each level. We additionally discuss practical design challenges with emphasis on data processing, as well as smart home communication protocols and their interoperability. We believe that the holistic framework ascertained in this paper can be used as a solid base for the future developers of Internet of Things based smart home solutions.},
	language = {en},
	urldate = {2021-07-14},
	journal = {Journal of Cleaner Production},
	author = {Risteska Stojkoska, Biljana L. and Trivodaliev, Kire V.},
	month = jan,
	year = {2017},
	keywords = {home automation},
	pages = {1454--1464},
}

@article{javed_internet_2018,
	title = {Internet of {Things} ({IoT}) {Operating} {Systems} {Support}, {Networking} {Technologies}, {Applications}, and {Challenges}: {A} {Comparative} {Review}},
	volume = {20},
	issn = {1553-877X, 2373-745X},
	shorttitle = {Internet of {Things} ({IoT}) {Operating} {Systems} {Support}, {Networking} {Technologies}, {Applications}, and {Challenges}},
	url = {https://ieeexplore.ieee.org/document/8320780/},
	doi = {10.1109/COMST.2018.2817685},
	abstract = {The Internet of Things (IoT) has become a reality. As the IoT is now becoming a far more common ﬁeld, the demand for IoT technologies to manage the communication of devices with the rest of the world has increased. The IoT is connecting various individual devices called things and wireless sensor networks is also playing an important role. A thing can be deﬁned as an embedded device based on a micro controller that can transmit and receive information. These devices are extremely low in power, memory, and resources. Therefore, the research community has recognized the importance of IoT device operating systems (OSs). An adequate OS with a kernel, networking, real-time capability, and more can make these devices ﬂexible. This review provides a detailed comparison of the OSs designed for IoT devices on the basis of their architecture, scheduling methods, networking technologies, programming models, power and memory management methods, together with other features required for IoT applications. In addition, various applications, challenges, and case studies in the ﬁeld of IoT research is discussed.},
	language = {en},
	number = {3},
	urldate = {2021-07-14},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Javed, Farhana and Afzal, Muhamamd Khalil and Sharif, Muhammad and Kim, Byung-Seo},
	year = {2018},
	pages = {2062--2100},
}

@article{chen_internet--things_2018,
	title = {Internet-of-{Things} {Security} and {Vulnerabilities}: {Taxonomy}, {Challenges}, and {Practice}},
	volume = {2},
	issn = {2509-3428, 2509-3436},
	shorttitle = {Internet-of-{Things} {Security} and {Vulnerabilities}},
	url = {http://link.springer.com/10.1007/s41635-017-0029-7},
	doi = {10.1007/s41635-017-0029-7},
	abstract = {Recent years have seen rapid development and deployment of Internet-of-Things (IoT) applications in a diversity of application domains. This has resulted in creation of new applications (e.g., vehicle networking, smart grid, and wearables) as well as advancement, consolidation, and transformation of various traditional domains (e.g., medical and automotive). One upshot of this scale and diversity of applications is the emergence of new and critical threats to security and privacy: it is getting increasingly easier for an adversary to break into an application, make it unusable, or steal sensitive information and data. This paper provides a summary of IoT security attacks and develops a taxonomy and classification based on the application domain and underlying system architecture. We also discuss some key characteristics of IoT that make it difficult to develop robust security architectures for IoT applications.},
	language = {en},
	number = {2},
	urldate = {2021-07-14},
	journal = {Journal of Hardware and Systems Security},
	author = {Chen, Kejun and Zhang, Shuai and Li, Zhikun and Zhang, Yi and Deng, Qingxu and Ray, Sandip and Jin, Yier},
	month = jun,
	year = {2018},
	pages = {97--110},
}

@article{bornholt_programming_nodate,
	title = {Programming the {Internet} of {Uncertain} {\textless}{T}{\textgreater}hings},
	abstract = {The transformation from desktops and servers to devices and cloud services—the Internet of things (IoT)—is well underway. A key problem facing IoT applications is their increasing reliance on estimated data from diverse sources, such as sensors, machine learning, and human computing. Current programming abstractions treat these estimates as if they were precise, creating buggy applications. Existing approaches to mitigate noise in estimates either add naive ad-hoc ﬁlters or construct sophisticated statistical models. They are either too fragile or too complex for ordinary developers to use. IoT developers need abstractions that help them (1) reason about estimates, because they are noisy and inherently inaccurate; (2) trade accuracy for energy efﬁciency on battery limited devices; and (3) compose data from disparate sources on devices and in the cloud.},
	language = {en},
	author = {Bornholt, James and Meng, Na and Mytkowicz, Todd and McKinley, Kathryn S},
	pages = {7},
}

@article{morin_model-based_2017,
	title = {Model-{Based} {Software} {Engineering} to {Tame} the {IoT} {Jungle}},
	volume = {34},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/7819419/},
	doi = {10.1109/MS.2017.11},
	language = {en},
	number = {1},
	urldate = {2021-07-14},
	journal = {IEEE Software},
	author = {Morin, Brice and Harrand, Nicolas and Fleurey, Franck},
	month = jan,
	year = {2017},
	pages = {30--36},
}

@inproceedings{goel2021low,
	title = {Low-power multi-camera object re-identification using hierarchical neural networks},
	booktitle = {{ACM}/{IEEE} {International} {Symposium} on {Low} {Power} {Electronics} and {Design} ({ISLPED})},
	author = {Goel, Abhinav and Tung, Caleb and Hu, Xiao and Wang, Haobo and Davis, James C and Thiruvathukal, George K and Lu, Yung-Hsiang},
	year = {2021},
}

@phdthesis{xiangInterpretedFormalismSystem2016,
	title = {Interpreted {Formalism}: {Towards} {System} {Assurance} and the {Real}-{World} {Semantics} of {Software}},
	copyright = {Attribution 4.0 International (CC BY)},
	shorttitle = {Interpreted {Formalism}},
	url = {https://libraetd.lib.virginia.edu/public_view/g732d899x},
	abstract = {Software systems, especially cyber-physical systems, sense and influence real-world entities under the control of software logic in order to realize desired real-world behaviors. Such software systems are based upon three essential components: (1) a computing platform, (2) a set of physical entities with which the computing platform interacts, and (3) the relationship between the first two components. These three components seem familiar, and the third component seems trivial. In fact, the third component, the relationship, is crucial, because it defines how logical values read and produced by the computing platform will be affected by and will affect the various physical entities.
Formally, the relationship between real-world entities and a computer system’s logic is the interpretation of the logic. Software logic is necessarily formal, but, in practice, interpretations are usually documented informally and incompletely, and programmers treat elements in software logic as if they were the real-world entities themselves. As a result, faults are introduced into systems due to unrecognized discrepancies, and executions end up violating constraints inherited from the real world. The results are software and system failures and adverse downstream consequences.
This dissertation argues that, to mitigate such risks, software engineers should produce not just traditional software, but a new engineering structure, the interpreted formalism. The structure combines software logic with an explicitly documented interpretation. Among other things, an interpretation documents differences that arise inevitably between real-world values and corresponding logic values. An interpreted formalism provides centralized documentation of a system’s software and its intended relationship to the real world in an analyzable form, facilitating fault detection.
An implementation of the interpretation, real-world type, is introduced. For a specific software system, an interpretation is composed of a set of real-world types, and an interpreted formalism is implemented as a real-world type system combined with a software system.
The pragmatics of the interpreted formalism concept are illustrated by conducting case studies on open-source software systems of different sizes. The interpreted formalism is evaluated from several viewpoints: (a) overall feasibility, (b) error detection capability, (c) effort level required, and (d) scalability. The results of the case studies suggest that (1) the interpreted formalism concept can be used on modern software systems of different sizes, (2) the technology is capable of detecting real errors that violate real-world constraints, and (3) the effort required from engineers for developing and using the interpreted formalism can be reduced greatly by an automated synthesis framework developed as part of this research.},
	language = {en},
	urldate = {2021-05-24},
	school = {University of Virginia},
	author = {Xiang, Jian},
	collaborator = {Knight, John},
	month = dec,
	year = {2016},
	doi = {10.18130/V3DS4G},
	keywords = {Software Assurance, Software Dependability, Static Analysis, Type System},
}

@inproceedings{chauhan2016development,
	title = {A development framework for programming cyber-physical systems},
	booktitle = {2016 {IEEE}/{ACM} 2nd international workshop on software engineering for smart cyber-physical systems ({SEsCPS})},
	author = {Chauhan, Saurabh and Patel, Pankesh and Delicato, Flávia C and Chaudhary, Sanjay},
	year = {2016},
	note = {tex.organization: IEEE},
	pages = {47--53},
}

@inproceedings{corno_easing_2018,
	address = {Gothenburg Sweden},
	title = {Easing {IoT} development for novice programmers through code recipes},
	isbn = {978-1-4503-5660-2},
	url = {https://dl.acm.org/doi/10.1145/3183377.3183385},
	doi = {10.1145/3183377.3183385},
	abstract = {The co-existence of various kinds of devices, protocols, architectures, and programming languages make Internet of Things (IoT) systems complex to develop, even for experienced programmers. Perforce, Software Engineering challenges are even more difficult to address by novice programmers. Previous research focused on identifying the most challenging issues that novice programmers experience when developing IoT systems. The results suggested that the integration of heterogeneous software components is one of the most painful issues, mainly due to the lack of documentation understandable by inexperienced developers, from both conceptual and technical perspectives. In fact, novice programmers devote a significant effort looking for documentation and code samples willing to understand them conceptually, or in the worst case, at least to make them work. Driven by the research question: “How do the lessons learned by IoT novice programmers can be captured, so they become an asset for other novice developers?”, in this paper, we introduce Code Recipes. They consist of summarized and welldefined documentation modules, independent from programming languages or run-time environments, by which non-expert programmers can smoothly become familiar with source code, written by other developers that faced similar issues. Through a use case, we show how Code Recipes are a feasible mechanism to support novice IoT programmers in building their IoT systems.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} {Education} and {Training}},
	publisher = {ACM},
	author = {Corno, Fulvio and De Russis, Luigi and Sáenz, Juan Pablo},
	month = may,
	year = {2018},
	pages = {13--16},
}

@inproceedings{corno_towards_2019,
	address = {Glasgow Scotland Uk},
	title = {Towards {Computational} {Notebooks} for {IoT} {Development}},
	isbn = {978-1-4503-5971-9},
	url = {https://dl.acm.org/doi/10.1145/3290607.3312963},
	doi = {10.1145/3290607.3312963},
	abstract = {Internet of Things systems are complex to develop. They are required to exhibit various features and run across several environments. Software developers have to deal with this heterogeneity both when configuring the development and execution environments and when writing the code. Meanwhile, computational notebooks have been gaining prominence due to their capability to consolidate text, executable code, and visualizations in a single document. Although they are mainly used in the field of data science, the characteristics of such notebooks could make them suitable to support the development of IoT systems as well. This work proposes an IoT-tailored literate computing approach in the form of a computational notebook. We present a use case of a typical IoT system involving several interconnected components and describe the implementation of a computational notebook as a tool to support its development. Finally, we point out the opportunities and limitations of this approach.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Corno, Fulvio and De Russis, Luigi and Sáenz, Juan Pablo},
	month = may,
	year = {2019},
	pages = {1--6},
}

@article{alamalhodaei_security_2021,
	title = {Security flaws found in popular {EV} chargers},
	url = {https://techcrunch.com/2021/08/03/security-flaws-found-in-popular-ev-chargers/amp/?guccounter=1},
	journal = {TechCrunch},
	author = {Alamalhodaei, Aria},
	month = aug,
	year = {2021},
}

@inproceedings{emami-naeiniExploringHowPrivacy2019,
	address = {Glasgow Scotland Uk},
	title = {Exploring {How} {Privacy} and {Security} {Factor} into {IoT} {Device} {Purchase} {Behavior}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300764},
	doi = {10.1145/3290605.3300764},
	abstract = {Despite growing concerns about security and privacy of Internet of Things (IoT) devices, consumers generally do not have access to security and privacy information when purchasing these devices. We interviewed 24 participants about IoT devices they purchased. While most had not considered privacy and security prior to purchase, they reported becoming concerned later due to media reports, opinions shared by friends, or observing unexpected device behavior. Those who sought privacy and security information before purchase, reported that it was difficult or impossible to find. We asked interviewees to rank factors they would consider when purchasing IoT devices; after features and price, privacy and security were ranked among the most important. Finally, we showed interviewees our prototype privacy and security label. Almost all found it to be accessible and useful, encouraging them to incorporate privacy and security in their IoT purchase decisions.},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Emami-Naeini, Pardis and Dixon, Henry and Agarwal, Yuvraj and Cranor, Lorrie Faith},
	month = may,
	year = {2019},
	pages = {1--12},
}

@inproceedings{emami-naeiniAskExpertsWhat2020,
	address = {San Francisco, CA, USA},
	title = {Ask the {Experts}: {What} {Should} {Be} on an {IoT} {Privacy} and {Security} {Label}?},
	isbn = {978-1-72813-497-0},
	shorttitle = {Ask the {Experts}},
	url = {https://ieeexplore.ieee.org/document/9152770/},
	doi = {10.1109/SP40000.2020.00043},
	abstract = {Information about the privacy and security of Internet of Things (IoT) devices is not readily available to consumers who want to consider it before making purchase decisions. While legislators have proposed adding succinct, consumer accessible, labels, they do not provide guidance on the content of these labels. In this paper, we report on the results of a series of interviews and surveys with privacy and security experts, as well as consumers, where we explore and test the design space of the content to include on an IoT privacy and security label. We conduct an expert elicitation study by following a three-round Delphi process with 22 privacy and security experts to identify the factors that experts believed are important for consumers when comparing the privacy and security of IoT devices to inform their purchase decisions. Based on how critical experts believed each factor is in conveying risk to consumers, we distributed these factors across two layers—a primary layer to display on the product package itself or prominently on a website, and a secondary layer available online through a web link or a QR code. We report on the experts’ rationale and arguments used to support their choice of factors. Moreover, to study how consumers would perceive the privacy and security information speciﬁed by experts, we conducted a series of semi-structured interviews with 15 participants, who had purchased at least one IoT device (smart home device or wearable). Based on the results of our expert elicitation and consumer studies, we propose a prototype privacy and security label to help consumers make more informed IoTrelated purchase decisions. Index Terms—Internet of Things (IoT), Privacy and Security, Label, Expert Elicitation, Delphi.},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Emami-Naeini, Pardis and Agarwal, Yuvraj and Faith Cranor, Lorrie and Hibshi, Hanan},
	month = may,
	year = {2020},
	pages = {447--464},
}

@techreport{joint_task_force_transformation_initiative_guide_2012,
	address = {Gaithersburg, MD},
	title = {Guide for conducting risk assessments},
	url = {https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-30r1.pdf},
	language = {en},
	number = {NIST SP 800-30r1},
	urldate = {2021-06-08},
	institution = {National Institute of Standards and Technology},
	author = {{Joint Task Force Transformation Initiative}},
	year = {2012},
	doi = {10.6028/NIST.SP.800-30r1},
	note = {Edition: 0},
	pages = {NIST SP 800--30r1},
}

@article{fan_formal_nodate,
	title = {Formal methods for safe autonomy: data-driven verification, synthesis, and applications},
	language = {en},
	author = {Fan, Chuchu},
	pages = {182},
}

@article{uddin_empirical_nodate,
	title = {An {Empirical} {Study} of {IoT} {Topics} in {IoT} {Developer} {Discussions} on {Stack} {Overﬂow}},
	language = {en},
	author = {Uddin, Gias and Sabir, Fatima and Alam, Omar and Khomh, Foutse},
	pages = {47},
}

@article{ahmad_empirical_2018,
	title = {An {Empirical} {Study} of {Investigating} {Mobile} {Applications} {Development} {Challenges}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8326707/},
	doi = {10.1109/ACCESS.2018.2818724},
	abstract = {Context: mobile application development is rapidly evolving with substantial economic and scientiﬁc interest. One of the primary reasons for mobile application development failure is the increasing number of mobile platforms; some organizations endorse mobile application development before understanding the associated development challenges of each target platform. Objective: the objective of this paper is to identify the challenges of native, web, and hybrid mobile applications, which can undermine the successful development of such applications. Method: we adopted a two-phase research approach: at ﬁrst, the challenges were identiﬁed via a systematic literature review (SLR); and then, the identiﬁed challenges were validated through conducting interviews with practitioners. Results: through both research approaches, we identiﬁed nine challenges vital to the success of mobile application development and four additional challenges from interviews not reported in the literature. A comparison of the challenges (native, web, and hybrid) identiﬁed in SLR indicates that there are slightly more differences than similarities between the challenges. On the other hand, the challenges (native, web, and hybrid) identiﬁed in interviews indicates that there are more similarities than differences between the challenges. Our results show a weak negative correlation between the ranks obtained from the SLR and the interviews ([rs(9) = −.034], p = 0.932). The results obtained from our t-test (i.e., t = 0.868, p = 0.402 {\textgreater} 0.05) depicts that there is no signiﬁcant difference between the ﬁndings of SLR and interviews. Conclusions: mobile application development organizations should try to address the identiﬁed challenges when developing mobile applications (native, web, or hybrid) to increase the probability of mobile application success.},
	language = {en},
	urldate = {2021-07-14},
	journal = {IEEE Access},
	author = {Ahmad, Arshad and Li, Kan and Feng, Chong and Asim, Syed Mohammad and Yousif, Abdallah and Ge, Shi},
	year = {2018},
	pages = {17711--17728},
}

@book{iotify_testing_2019,
	title = {Testing the {Internet} of {Things}},
	publisher = {IoTIfy},
	author = {IoTify},
	year = {2019},
}

@techreport{voas_networks_2016,
	address = {Gaithersburg, MD},
	title = {Networks of 'things':},
	shorttitle = {Networks of 'things'},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-183.pdf},
	abstract = {System primitives allow formalisms, reasoning, simulations, and reliability and security risktradeoffs to be formulated and argued. In this work, five core primitives belonging to most distributed systems are presented. These primitives apply well to systems with large amounts of data, scalability concerns, heterogeneity concerns, temporal concerns, and elements of unknown pedigree with possible nefarious intent. These primitives are the basic building blocks for a Network of ‘Things’ (NoT), including the Internet of Things (IoT). This document offers an underlying and foundational understanding of IoT based on the realization that IoT involves sensing, computing, communication, and actuation. The material presented here is generic to all distributed systems that employ IoT technologies (i.e., ‘things’ and networks). The expected audience is computer scientists, IT managers, networking specialists, and networking and cloud computing software engineers. To our knowledge, the ideas and the manner in which IoT is presented here is unique.},
	language = {en},
	number = {NIST SP 800-183},
	urldate = {2021-07-14},
	institution = {National Institute of Standards and Technology},
	author = {Voas, Jeffrey M},
	year = {2016},
	doi = {10.6028/NIST.SP.800-183},
	note = {Edition: 0},
	pages = {NIST SP 800--183},
}

@article{corno_how_2020,
	title = {How is {Open} {Source} {Software} {Development} {Different} in {Popular} {IoT} {Projects}?},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8986632/},
	doi = {10.1109/ACCESS.2020.2972364},
	abstract = {From the software point of view, the development of IoT applications differs from other kinds of applications due to the speciﬁc features that the former exhibit. In this paper, we investigate how developers contribute to IoT applications in the Open Source Software (OSS) context, to gain a deeper understanding of how their work differs from that of non-IoT applications. To that end, we conducted a quantitative analysis of a broad set of the 60 most popular publicly available IoT and non-IoT projects on GitHub. By comparing how developers contribute to these projects, our analysis provides insight into the purpose and characteristics of the code, the behavior of the contributors, and the maturity of the IoT software development ecosystem. Results reveal signiﬁcant differences between IoT and non-IoT application development, in terms of how applications are realized, in the diversity of developers’ specializations, and in how code is reused. This work provides evidence about some Open Source IoT software development peculiarities to be considered by future research efforts aimed at better satisfying software engineering needs in the IoT scenario.},
	language = {en},
	urldate = {2021-08-19},
	journal = {IEEE Access},
	author = {Corno, Fulvio and De Russis, Luigi and Saenz, Juan Pablo},
	year = {2020},
	pages = {28337--28348},
}

@inproceedings{hnat_hitchhikers_2011,
	address = {Seattle, Washington},
	title = {The hitchhiker's guide to successful residential sensing deployments},
	isbn = {978-1-4503-0718-5},
	url = {http://dl.acm.org/citation.cfm?doid=2070942.2070966},
	doi = {10.1145/2070942.2070966},
	abstract = {Homes are rich with information about people’s energy consumption, medical health, and personal or family functions. In this paper, we present our experiences deploying large-scale residential sensing systems in over 20 homes. Deploying small-scale systems in homes can be deceptively easy, but in our deployments we encountered a phase transition in which deployment effort increases dramatically as residential deployments scale up in terms of 1) the number of nodes, 2) the length of time, and 3) the number of houses. In this paper, we distill our experiences down to a set of guidelines and design principles to help future deployments avoid the potential pitfalls of large-scale sensing in homes.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the 9th {ACM} {Conference} on {Embedded} {Networked} {Sensor} {Systems} - {SenSys} '11},
	publisher = {ACM Press},
	author = {Hnat, Timothy W. and Srinivasan, Vijay and Lu, Jiakang and Sookoor, Tamim I. and Dawson, Raymond and Stankovic, John and Whitehouse, Kamin},
	year = {2011},
	keywords = {home automation},
	pages = {232},
}

@inproceedings{liangUnderstandingBoundingFunctions2021,
	address = {Madrid, Spain},
	title = {Understanding {Bounding} {Functions} in {Safety}-{Critical} {UAV} {Software}},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9401993/},
	doi = {10.1109/ICSE43902.2021.00119},
	abstract = {Unmanned Aerial Vehicles (UAVs) are an emerging computation platform known for their safety-critical need. In this paper, we conduct an empirical study on a widely used open-source UAV software framework, Paparazzi, with the goal of understanding the safety-critical concerns of UAV software from a bottom-up developer-in-the-field perspective. We set our focus on the use of Bounding Functions (BFs), the runtime checks injected by Paparazzi developers on the range of variables. Through an in-depth analysis on BFs in the Paparazzi autopilot software, we found a large number of them (109 instances) are used to bound safety-critical variables essential to the cyberphysical nature of the UAV, such as its thrust, its speed, and its sensor values. The novel contributions of this study are two fold. First, we take a static approach to classify all BF instances, presenting a novel datatype-based 5-category taxonomy with finegrained insight on the role of BFs in ensuring the safety of UAV systems. Second, we dynamically evaluate the impact of the BF uses through a differential approach, establishing the UAV behavioral difference with and without BFs. The two-pronged static and dynamic approach together illuminates a rarely studied design space of safety-critical UAV software systems.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Liang, Xiaozhou and Burns, John Henry and Sanchez, Joseph and Dantu, Karthik and Ziarek, Lukasz and Liu, Yu David},
	month = may,
	year = {2021},
	pages = {1311--1322},
}

@incollection{lee_embedded_2002,
	title = {Embedded {Software}},
	volume = {56},
	isbn = {978-0-12-012156-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065245802800043},
	abstract = {The science of computation has systematically abstracted away the physical world. Embedded software systems, however, engage the physical world. Time, concurrency, liveness, robustness, continuums, reactivity, and resource management must be remarried to computation. Prevailing abstractions of computational systems leave out these "nonfunctional" aspects. This chapter explains why embedded software is not just software on small computers, and why it therefore needs fundamentally new views of computation. It suggests component architectures based on a principle called "actor-oriented design," where actors interact according to a model of computation, and describes some models of computation that are suitable for embedded software. It then suggests that actors can define interfaces that declare dynamic aspects that are essential to embedded software, such as temporal properties. These interfaces can be structured in a "system-level-type system" that supports the sort of design-time- and run-time-type checking that conventional software benefits from.},
	language = {en},
	urldate = {2021-07-13},
	booktitle = {Advances in {Computers}},
	publisher = {Elsevier},
	author = {Lee, Edward A.},
	year = {2002},
	doi = {10.1016/S0065-2458(02)80004-3},
	pages = {55--95},
}

@article{schneier_iot_2017,
	title = {{IoT} {Security}: {What}’s {Plan} {B}?},
	volume = {15},
	issn = {1540-7993},
	shorttitle = {{IoT} {Security}},
	url = {http://ieeexplore.ieee.org/document/8055681/},
	doi = {10.1109/MSP.2017.3681066},
	language = {en},
	number = {5},
	urldate = {2021-08-26},
	journal = {IEEE Security \& Privacy},
	author = {Schneier, Bruce},
	year = {2017},
	pages = {96--96},
}

@article{corno_challenges_2019,
	title = {On the challenges novice programmers experience in developing {IoT} systems: {A} {Survey}},
	volume = {157},
	issn = {01641212},
	shorttitle = {On the challenges novice programmers experience in developing {IoT} systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121219301566},
	doi = {10.1016/j.jss.2019.07.101},
	language = {en},
	urldate = {2021-07-14},
	journal = {Journal of Systems and Software},
	author = {Corno, Fulvio and De Russis, Luigi and Sáenz, Juan Pablo},
	month = nov,
	year = {2019},
	pages = {110389},
}

@article{chen_application_2017,
	title = {Application of {Fault} {Tree} {Analysis} and {Fuzzy} {Neural} {Networks} to {Fault} {Diagnosis} in the {Internet} of {Things} ({IoT}) for {Aquaculture}},
	volume = {17},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/17/1/153},
	doi = {10.3390/s17010153},
	abstract = {In the Internet of Things (IoT) equipment used for aquaculture is often deployed in outdoor ponds located in remote areas. Faults occur frequently in these tough environments and the staff generally lack professional knowledge and pay a low degree of attention in these areas. Once faults happen, expert personnel must carry out maintenance outdoors. Therefore, this study presents an intelligent method for fault diagnosis based on fault tree analysis and a fuzzy neural network. In the proposed method, ﬁrst, the fault tree presents a logic structure of fault symptoms and faults. Second, rules extracted from the fault trees avoid duplicate and redundancy. Third, the fuzzy neural network is applied to train the relationship mapping between fault symptoms and faults. In the aquaculture IoT, one fault can cause various fault symptoms, and one symptom can be caused by a variety of faults. Four fault relationships are obtained. Results show that one symptom-to-one fault, two symptoms-to-two faults, and two symptoms-to-one fault relationships can be rapidly diagnosed with high precision, while one symptom-to-two faults patterns perform not so well, but are still worth researching. This model implements diagnosis for most kinds of faults in the aquaculture IoT.},
	language = {en},
	number = {12},
	urldate = {2021-07-14},
	journal = {Sensors},
	author = {Chen, Yingyi and Zhen, Zhumi and Yu, Huihui and Xu, Jing},
	month = jan,
	year = {2017},
	pages = {153},
}

@inproceedings{zhou_empirical_2015,
	address = {Florence, Italy},
	title = {An {Empirical} {Study} on {Quality} {Issues} of {Production} {Big} {Data} {Platform}},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7202945/},
	doi = {10.1109/ICSE.2015.130},
	abstract = {Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. To date, there is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-deﬁned escalation process (i.e., incident management process), which helps customers report service quality issues on 24/7 basis. This paper investigates the common symptom, causes and mitigation of service quality issues in Big Data platform. We conduct a comprehensive empirical study on 210 real service quality issues of ProductA. Our major ﬁndings include (1) 21.0\% of escalations are caused by hardware faults; (2) 36.2\% are caused by system side defects; (3) 37.2\% are due to customer side faults. We also studied the general diagnosis process and the commonly adopted mitigation solutions. Our study results provide valuable guidance on improving existing development and maintenance practice of production Big Data platform, and motivate tool support. Index Terms- empirical study, Big Data computing, quality issues, escalations, fault tolerance.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Zhou, Hucheng and Lou, Jian-Guang and Zhang, Hongyu and Lin, Haibo and Lin, Haoxiang and Qin, Tingting},
	month = may,
	year = {2015},
	pages = {17--26},
}

@inproceedings{goyal_mind_2016,
	address = {Pisa Italy},
	title = {Mind the tracker you wear: a security analysis of wearable health trackers},
	isbn = {978-1-4503-3739-7},
	shorttitle = {Mind the tracker you wear},
	url = {https://dl.acm.org/doi/10.1145/2851613.2851685},
	doi = {10.1145/2851613.2851685},
	abstract = {Wearable tracking devices have gained widespread usage and popularity because of the valuable services they oﬀer, monitoring human’s health parameters and, in general, assisting persons to take a better care of themselves. Nevertheless, the security risks associated with such devices can represent a concern among consumers, because of the sensitive information these devices deal with, like sleeping patterns, eating habits, heart rate and so on. In this paper, we analyse the key security and privacy features of two entry level health trackers from leading vendors (Jawbone and Fitbit), exploring possible attack vectors and vulnerabilities at several system levels. The results of the analysis show how these devices are vulnerable to several attacks (perpetrated with consumer-level devices equipped with just bluetooth and Wi-Fi) that can compromise users’ data privacy and security, and eventually call the tracker vendors to raise the stakes against such attacks.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the 31st {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Goyal, Rohit and Dragoni, Nicola and Spognardi, Angelo},
	month = apr,
	year = {2016},
	keywords = {consumer electronics},
	pages = {131--136},
}

@inproceedings{ronen_iot_2017,
	address = {San Jose, CA, USA},
	title = {{IoT} {Goes} {Nuclear}: {Creating} a {ZigBee} {Chain} {Reaction}},
	isbn = {978-1-5090-5533-3},
	shorttitle = {{IoT} {Goes} {Nuclear}},
	url = {http://ieeexplore.ieee.org/document/7958578/},
	doi = {10.1109/SP.2017.14},
	abstract = {Within the next few years, billions of IoT devices will densely populate our cities. In this paper we describe a new type of threat in which adjacent IoT devices will infect each other with a worm that will rapidly spread over large areas, provided that the density of compatible IoT devices exceeds a certain critical mass. In particular, we developed and veriﬁed such an infection using the popular Philips Hue smart lamps as a platform. The worm spreads by jumping directly from one lamp to its neighbors, using only their built-in ZigBee wireless connectivity and their physical proximity. The attack can start by plugging in a single infected bulb anywhere in the city, and then catastrophically spread everywhere within minutes. It enables the attacker to turn all the city lights on or off, to permanently brick them, or to exploit them in a massive DDOS attack. To demonstrate the risks involved, we use results from percolation theory to estimate the critical mass of installed devices for a typical city such as Paris whose area is about 105 square kilometers: The chain reaction will ﬁzzle if there are fewer than about 15,000 randomly located smart lamps in the whole city, but will spread everywhere when the number exceeds this critical mass (which had almost certainly been surpassed already).},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Ronen, Eyal and Shamir, Adi and Weingarten, Achi-Or and OFlynn, Colin},
	month = may,
	year = {2017},
	pages = {195--212},
}

@article{suler_online_nodate,
	title = {The {Online} {Disinhibition} {Effect}},
	abstract = {While online, some people self-disclose or act out more frequently or intensely than they would in person. This article explores six factors that interact with each other in creating this online disinhibition effect: dissociative anonymity, invisibility, asynchronicity, solipsistic introjection, dissociative imagination, and minimization of authority. Personality variables also will influence the extent of this disinhibition. Rather than thinking of disinhibition as the revealing of an underlying “true self,” we can conceptualize it as a shift to a constellation within self-structure, involving clusters of affect and cognition that differ from the in-person constellation.},
	language = {en},
	author = {Suler, John},
	pages = {83},
}

@misc{yliluoma_perl-compatible_nodate,
	title = {Perl-compatible regular expression optimizer},
	url = {https://bisqwit.iki.fi/source/regexopt.html},
	author = {Yliluoma, Joel},
}

@incollection{ilieNFAReductions2004,
	address = {Berlin, Heidelberg},
	title = {On {NFA} {Reductions}},
	volume = {3113},
	isbn = {978-3-540-22393-1 978-3-540-27812-2},
	url = {http://link.springer.com/10.1007/978-3-540-27812-2_11},
	abstract = {We give faster algorithms for two methods of reducing the number of states in nondeterministic ﬁnite automata. The ﬁrst uses equivalences and the second uses preorders. We develop restricted reduction algorithms that operate on position automata while preserving some of its properties. We show empirically that these reductions are effective in largely reducing the memory requirements of regular expression search algorithms, and compare the eﬀectiveness of diﬀerent reductions.},
	language = {en},
	urldate = {2021-05-07},
	booktitle = {Theory {Is} {Forever}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ilie, Lucian and Navarro, Gonzalo and Yu, Sheng},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Karhumäki, Juhani and Maurer, Hermann and Păun, Gheorghe and Rozenberg, Grzegorz},
	year = {2004},
	doi = {10.1007/978-3-540-27812-2_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {112--124},
}

@article{champarnaudNFAReductionAlgorithms2004,
	title = {{NFA} reduction algorithms by means of regular inequalities},
	volume = {327},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397504004803},
	doi = {10.1016/j.tcs.2004.02.048},
	abstract = {We present different techniques for reducing the number of states and transitions in nondeterministic automata. These techniques are based on the two preorders over the set of states, related to the inclusion of left and right languages. Since their exact computation is NP-hard, we focus on polynomial approximations which enable a reduction of the NFA all the same. Our main algorithm relies on a ﬁrst approximation, which can be easily implemented by means of matrix products with an O(mn3) time complexity, and optimized to an O(mn) time complexity, where m is the number of transitions and n is the number of states. This ﬁrst algorithm appears to be more efﬁcient than the known techniques based on equivalence relations as described by Lucian Ilie and Sheng Yu. Afterwards, we brieﬂy describe some more accurate approximations and the exact (but exponential) calculation of these preorders by means of determinization.},
	language = {en},
	number = {3},
	urldate = {2021-05-07},
	journal = {Theoretical Computer Science},
	author = {Champarnaud, J.-M. and Coulon, F.},
	month = nov,
	year = {2004},
	pages = {241--253},
}

@article{schmerge_regis_nodate,
	title = {{ReGiS}: {Regular} {Expression} {Simpliﬁcation} via {Rewrite}-{Guided} {Synthesis}},
	abstract = {Expression simpliﬁcation is an important task necessary in a variety of domains, e.g., compilers, digital logic design, etc. Syntax-guided synthesis (SyGuS) with a cost function can be used for this purpose, but ordered enumeration through a large space of candidate expressions can be expensive. Equality saturation is an alternative approach which allows efﬁcient construction and maintenance of expression equivalence classes generated by rewrite rules, but the procedure may not reach saturation, meaning global minimality cannot be conﬁrmed. We present a new approach called rewrite-guided synthesis (ReGiS), in which a unique interplay between SyGuS and equality saturation-based rewriting helps to overcome these problems, resulting in an efﬁcient, scalable framework for expression simpliﬁcation. We demonstrate the ﬂexibility and practicality of our approach by applying ReGiS to regular expression denial of service (ReDoS) attack prevention. Many real-world regular expression matching engines are vulnerable to these complexity-based attacks, and while much research has focused on detecting vulnerable regular expressions, we allow developers to go further, by automatically transforming their regular expressions to remove vulnerabilities.},
	language = {en},
	author = {Schmerge, Jordan and Claver, Miles and Garner, Jackson and Vossen, Jake and McClurg, Jedidiah},
	pages = {13},
}

@article{gramlichMinimizingNfaRegular2007,
	title = {Minimizing nfa's and regular expressions},
	volume = {73},
	issn = {00220000},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022000006001735},
	doi = {10.1016/j.jcss.2006.11.002},
	abstract = {We show inapproximability results concerning minimization of nondeterministic ﬁnite automata (nfa’s) as well as of regular expressions relative to given nfa’s, regular expressions or deterministic ﬁnite automata (dfa’s). We show that it is impossible to efﬁciently minimize a given nfa or regular expression with n states, transitions, respectively symbols within the factor o(n), unless P = PSPACE. For the unary case, we show that for any δ {\textgreater} 0 it is impossible to efﬁciently construct an approximately minimal nfa or regular expression within the factor n1−δ, unless P = NP.},
	language = {en},
	number = {6},
	urldate = {2021-05-07},
	journal = {Journal of Computer and System Sciences},
	author = {Gramlich, Gregor and Schnitger, Georg},
	month = sep,
	year = {2007},
	pages = {908--923},
}

@article{arnoldNoteMinimalNondeterministic,
	title = {A note about minimal non-deterministic automata},
	abstract = {With every rational language we associate a canonical non-deterministic automaton, that subsumes all possible “minimal” automata recognizing this language.},
	language = {en},
	author = {Arnold, A and Dicky, A and Nivat, M},
	pages = {5},
}

@book{carrez1970minimalization,
	title = {On the minimalization of non-deterministic automaton},
	publisher = {Laboratoire de calcul de l'Université des Sciences et techniques de Lille},
	author = {Carrez, Christian},
	year = {1970},
}

@inproceedings{goldEthicalMiningCase2020,
	address = {Seoul Republic of Korea},
	title = {Ethical {Mining}: {A} {Case} {Study} on {MSR} {Mining} {Challenges}},
	isbn = {978-1-4503-7517-7},
	shorttitle = {Ethical {Mining}},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387462},
	doi = {10.1145/3379597.3387462},
	abstract = {Research in Mining Software Repositories (MSR) is research involving human subjects, as the repositories usually contain data about developers’ interactions with the repositories. Therefore, any research in the area needs to consider the ethics implications of the intended activity before starting. This paper presents a discussion of the ethics implications of MSR research, using the mining challenges from the years 2010 to 2019 as a case study to identify the kinds of data used. It highlights problems that one may encounter in creating such datasets, and discusses ethics challenges that may be encountered when using existing datasets, based on a contemporary research ethics framework. We suggest that the MSR community should increase awareness of ethics issues by openly discussing ethics considerations in published articles.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Gold, Nicolas E. and Krinke, Jens},
	month = jun,
	year = {2020},
	pages = {265--276},
}

@misc{TracyChouLaunches,
	title = {Tracy {Chou} launches {Block} {Party} to combat online harassment and abuse {\textbar} {TechCrunch}},
	url = {https://techcrunch.com/2021/01/15/block-party-launch/},
	urldate = {2021-04-07},
}

@article{fitzgerald2003legal,
	title = {Legal issues relating to free and open source software},
	journal = {Legal Issues Relationg to Free and Open Source Software},
	author = {Fitzgerald, Brian and Bassett, Graham},
	year = {2003},
	note = {Publisher: Queensland University of Technology},
	pages = {11--36},
}

@inproceedings{novielli2014towards,
	title = {Towards discovering the role of emotions in stack overflow},
	booktitle = {Proceedings of the 6th international workshop on social software engineering},
	author = {Novielli, Nicole and Calefato, Fabio and Lanubile, Filippo},
	year = {2014},
	pages = {33--36},
}

@article{blei2003latent,
	title = {Latent dirichlet allocation},
	volume = {3},
	journal = {the Journal of machine Learning research},
	author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	year = {2003},
	note = {Publisher: JMLR. org},
	pages = {993--1022},
}

@inproceedings{masmoudi2017event,
	title = {From {Event} to {Evidence}: {An} {Approach} for multi-tenant cloud services’ accountability},
	booktitle = {2017 {IEEE} 31st international conference on advanced information networking and applications ({AINA})},
	author = {Masmoudi, Fatma and Sellami, Mohamed and Loulou, Monia and Kacem, Ahmed Hadj},
	year = {2017},
	note = {tex.organization: IEEE},
	pages = {1082--1089},
}

@article{lessig2000code,
	title = {Code is law},
	volume = {1},
	number = {2000},
	journal = {Harvard magazine},
	author = {Lessig, Lawrence},
	year = {2000},
}

@article{moghaddam2006coding,
	title = {Coding issues in grounded theory},
	volume = {16},
	number = {1},
	journal = {Issues in educational research},
	author = {Moghaddam, Alireza},
	year = {2006},
	pages = {52--66},
}

@inproceedings{masmoudi2018optimal,
	title = {Optimal evidence collection for accountability in the cloud},
	booktitle = {2018 {IEEE} 15th international conference on e-{Business} engineering ({ICEBE})},
	author = {Masmoudi, Fatma and Sellami, Mohamed and Loulou, Monia and Kacem, Ahmed Hadj},
	year = {2018},
	note = {tex.organization: IEEE},
	pages = {78--85},
}

@article{liGDPRComplianceContext2020,
	title = {{GDPR} {Compliance} in the {Context} of {Continuous} {Integration}},
	url = {http://arxiv.org/abs/2002.06830},
	abstract = {The enactment of the General Data Protection Regulation (GDPR) in 2018 forced any organization that collects and/or processes EU-based personal data to comply with stringent privacy regulations. Software organizations have struggled to achieve GDPR compliance both before and after the GDPR deadline. While some studies have relied on surveys or interviews to ﬁnd general implications of the GDPR, there is a lack of in-depth studies that investigate compliance practices and compliance challenges of software organizations. In particular, there is no information on small and medium enterprises (SMEs), which represent the majority of organizations in the EU, nor on organizations that practice continuous integration. Using design science methodology, we conducted an in-depth study over the span of 20 months regarding GDPR compliance practices and challenges in collaboration with a small, startup organization. We ﬁrst identiﬁed our collaborator’s business problems and then iteratively developed two artifacts to address those problems: a set of operationalized GDPR principles, and an automated GDPR tool that tests those GDPR-derived privacy requirements. This design science approach resulted in four implications for research and for practice. For example, our research reveals that GDPR regulations can be partially operationalized and tested through automated means, which improves compliance practices, but more research is needed to create more efﬁcient and effective means to disseminate and manage GDPR knowledge among software developers.},
	language = {en},
	urldate = {2021-03-26},
	journal = {arXiv:2002.06830 [cs]},
	author = {Li, Ze Shi and Werner, Colin and Ernst, Neil and Damian, Daniela},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06830},
	keywords = {Computer Science - Software Engineering},
}

@article{liPrivacyStreamsEnablingTransparency2017,
	title = {{PrivacyStreams}: {Enabling} {Transparency} in {Personal} {Data} {Processing} for {Mobile} {Apps}},
	volume = {1},
	issn = {2474-9567},
	shorttitle = {{PrivacyStreams}},
	url = {https://dl.acm.org/doi/10.1145/3130941},
	doi = {10.1145/3130941},
	language = {en},
	number = {3},
	urldate = {2021-03-26},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Li, Yuanchun and Chen, Fanglin and Li, Toby Jia-Jun and Guo, Yao and Huang, Gang and Fredrikson, Matthew and Agarwal, Yuvraj and Hong, Jason I.},
	month = sep,
	year = {2017},
	pages = {1--26},
}

@article{hadarPrivacyDesignersSoftware2018a,
	title = {Privacy by designers: software developers’ privacy mindset},
	volume = {23},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Privacy by designers},
	url = {http://link.springer.com/10.1007/s10664-017-9517-1},
	doi = {10.1007/s10664-017-9517-1},
	abstract = {Privacy by design (PbD) is a policy measure that guides software developers to apply inherent solutions to achieve better privacy protection. For PbD to be a viable option, it is important to understand developers’ perceptions, interpretation and practices as to informational privacy (or data protection). To this end, we conducted in-depth interviews with 27 developers from different domains, who practice software design. Grounded analysis of the data revealed an interplay between several different forces affecting the way in which developers handle privacy concerns. Borrowing the schema of Social Cognitive Theory (SCT), we classified and analyzed the cognitive, organizational and behavioral factors that play a role in developers’ privacy decision making. Our findings indicate that developers use the vocabulary of data security to approach privacy challenges, and that this vocabulary limits their perceptions of privacy mainly to third-party threats coming from outside of the organization; that organizational privacy climate is a powerful means for organizations to guide developers toward particular practices of privacy; and that software architectural patterns frame privacy solutions that are used throughout the development process, possibly explaining developers’ preference of policy-based solutions to architectural solutions. Further, we show, through the use of the SCT schema for framing the findings of this study, how a theoretical model of the factors that influence developers’ privacy practices can be conceptualized and used as a guide for future research toward effective implementation of PbD.},
	language = {en},
	number = {1},
	urldate = {2021-03-26},
	journal = {Empirical Software Engineering},
	author = {Hadar, Irit and Hasson, Tomer and Ayalon, Oshrat and Toch, Eran and Birnhack, Michael and Sherman, Sofia and Balissa, Arod},
	month = feb,
	year = {2018},
	pages = {259--289},
}

@article{spiekermannEngineeringPrivacy2009,
	title = {Engineering {Privacy}},
	volume = {35},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/4657365/},
	doi = {10.1109/TSE.2008.88},
	abstract = {In this paper, we integrate insights from diverse islands of research on electronic privacy to offer a holistic view of privacy engineering and a systematic structure for the discipline’s topics. First, we discuss privacy requirements grounded in both historic and contemporary perspectives on privacy. We use a three-layer model of user privacy concerns to relate them to system operations (data transfer, storage, and processing) and examine their effects on user behavior. In the second part of this paper, we develop guidelines for building privacy-friendly systems. We distinguish two approaches: “privacy-by-policy” and “privacy-by-architecture.” The privacy-bypolicy approach focuses on the implementation of the notice and choice principles of fair information practices, while the privacy-byarchitecture approach minimizes the collection of identifiable personal data and emphasizes anonymization and client-side data storage and processing. We discuss both approaches with a view to their technical overlaps and boundaries as well as to economic feasibility. This paper aims to introduce engineers and computer scientists to the privacy research domain and provide concrete guidance on how to design privacy-friendly systems.},
	language = {en},
	number = {1},
	urldate = {2021-03-25},
	journal = {IEEE Transactions on Software Engineering},
	author = {Spiekermann, S. and Cranor, L.F.},
	month = jan,
	year = {2009},
	pages = {67--82},
}

@inproceedings{piccioniEmpiricalStudyAPI2013,
	address = {Baltimore, Maryland},
	title = {An {Empirical} {Study} of {API} {Usability}},
	isbn = {978-0-7695-5056-5},
	url = {http://ieeexplore.ieee.org/document/6681333/},
	doi = {10.1109/ESEM.2013.14},
	abstract = {Modern software development extensively involves reusing library components accessed through their Application Programming Interfaces (APIs). Usability is therefore a fundamental goal of API design, but rigorous empirical studies of API usability are still relatively uncommon. In this paper, we present the design of an API usability study which combines interview questions based on the cognitive dimensions framework, with systematic observations of programmer behavior while solving programming tasks based on “tokens”. We also discuss the implementation of the study to assess the usability of a persistence library API (offering functionalities such as storing objects into relational databases). The study involved 25 programmers (including students, researchers, and professionals), and provided additional evidence to some critical features evidenced by related studies, such as the difﬁculty of ﬁnding good names for API features and of discovering relations between API types. It also discovered new issues relevant to API design, such as the impact of ﬂexibility, and conﬁrmed the crucial importance of accurate documentation for usability.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {2013 {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE},
	author = {Piccioni, Marco and Furia, Carlo A. and Meyer, Bertrand},
	month = oct,
	year = {2013},
	pages = {5--14},
}

@article{schellerAutomatedMeasurementAPI2015,
	title = {Automated measurement of {API} usability: {The} {API} {Concepts} {Framework}},
	volume = {61},
	issn = {09505849},
	shorttitle = {Automated measurement of {API} usability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584915000178},
	doi = {10.1016/j.infsof.2015.01.009},
	abstract = {Objective: To make API usability measurement easier, an automated and objective measurement method would be needed. This article proposes such a method. Since it would be impossible to ﬁnd and integrate all possible factors that inﬂuence API usability in one step, the main goal is to prove the feasibility of the introduced approach, and to deﬁne an extensible framework so that additional factors can easily be deﬁned and added later.
Method: A literature review is conducted to ﬁnd potential factors inﬂuencing API usability. From these factors, a selected few are investigated more closely with usability studies. The statistically evaluated results from these studies are used to deﬁne speciﬁc elements of the introduced framework. Further, the inﬂuence of the user as a critical factor for the framework’s feasibility is evaluated.
Results: The API Concepts Framework is deﬁned, with an extensible structure based on concepts that represent the user’s actions, measurable properties that deﬁne what inﬂuences the usability of these concepts, and learning effects that represent the inﬂuence of the user’s experience. A comparison of values calculated by the framework with user studies shows promising results.
Conclusion: It is concluded that the introduced approach is feasible and provides useful results for evaluating API usability. The extensible framework easily allows to add new concepts and measurable properties in the future.},
	language = {en},
	urldate = {2021-03-24},
	journal = {Information and Software Technology},
	author = {Scheller, Thomas and Kühn, Eva},
	month = may,
	year = {2015},
	pages = {145--162},
}

@article{merigouxCatalaMovingFuture,
	title = {Catala: {Moving} {Towards} the {Future} of {Legal} {Expert} {Systems}},
	abstract = {Software called legal expert systems are used around the world by private and public organizations to compute taxes. A bug in such programs can lead to tax miscalculations and heavy legal and democratic consequences. Yet, increasing evidence suggests that some legal expert systems may not meet satisfying criteria to be in compliance with the law. Moreover, they are difﬁcult to adapt to the continuous ﬂow of new legislation just by using traditional software development process. To prevent further software decay and reconcile these systems with the growing demand for algorithmic transparency, we argue that there is a need for a new development process for legal expert systems. As such, we present a solution built by lawyers and computer scientists : a new programming language coupled with a pair programming development process.},
	language = {en},
	author = {Merigoux, Denis and Huttner, Liane},
	pages = {11},
}

@inproceedings{matthewsWhenTrustedBlack2020,
	address = {New York NY USA},
	title = {When {Trusted} {Black} {Boxes} {Don}'t {Agree}: {Incentivizing} {Iterative} {Improvement} and {Accountability} in {Critical} {Software} {Systems}},
	isbn = {978-1-4503-7110-0},
	shorttitle = {When {Trusted} {Black} {Boxes} {Don}'t {Agree}},
	url = {https://dl.acm.org/doi/10.1145/3375627.3375807},
	doi = {10.1145/3375627.3375807},
	abstract = {Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Matthews, Jeanna Neefe and Northup, Graham and Grasso, Isabella and Lorenz, Stephen and Babaeianjelodar, Marzieh and Bashaw, Hunter and Mondal, Sumona and Matthews, Abigail and Njie, Mariama and Goldthwaite, Jessica},
	month = feb,
	year = {2020},
	pages = {102--108},
}

@incollection{irvineShortPaperIntegrating2020,
	address = {Cham},
	title = {Short {Paper}: {Integrating} the {Data} {Protection} {Impact} {Assessment} into the {Software} {Development} {Lifecycle}},
	volume = {12484},
	isbn = {978-3-030-66171-7 978-3-030-66172-4},
	shorttitle = {Short {Paper}},
	url = {http://link.springer.com/10.1007/978-3-030-66172-4_13},
	abstract = {Recent years have seen many privacy violations that have cost both the users of software systems and the businesses that run them in a variety of ways. One potential cause of these violations may be the ad hoc nature of the implementation of privacy measures within software systems, which may stem from the poor representation of privacy within many Software Development LifeCycle (SDLC) processes. We propose to give privacy a higher priority within the SDLC through the creation of a confederated Privacy-Aware SDLC (PASDLC) which incorporates the Data Protection Impact Assessment (DPIA) lifecycle. The PASDLC brings stakeholders of the software system closer together through the implementation of multiple interception points, whilst prompting the stakeholders to consider privacy within the software system. We consider many challenges to the creation of the PASDLC, including potential communication issues from confederating the processes of a SDLC and the eﬀective measurement of privacy as an attribute of a software system.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Data {Privacy} {Management}, {Cryptocurrencies} and {Blockchain} {Technology}},
	publisher = {Springer International Publishing},
	author = {Irvine, Christopher and Balasubramaniam, Dharini and Henderson, Tristan},
	editor = {Garcia-Alfaro, Joaquin and Navarro-Arribas, Guillermo and Herrera-Joancomarti, Jordi},
	year = {2020},
	doi = {10.1007/978-3-030-66172-4_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {219--228},
}

@inproceedings{shethUsThemStudy2014,
	address = {Hyderabad India},
	title = {Us and them: a study of privacy requirements across north america, asia, and europe},
	isbn = {978-1-4503-2756-5},
	shorttitle = {Us and them},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568244},
	doi = {10.1145/2568225.2568244},
	abstract = {Data privacy when using online systems like Facebook and Amazon has become an increasingly popular topic in the last few years. However, only a little is known about how users and developers perceive privacy and which concrete measures would mitigate their privacy concerns. To investigate privacy requirements, we conducted an online survey with closed and open questions and collected 408 valid responses. Our results show that users often reduce privacy to security, with data sharing and data breaches being their biggest concerns. Users are more concerned about the content of their documents and their personal data such as location than about their interaction data. Unlike users, developers clearly prefer technical measures like data anonymization and think that privacy laws and policies are less eﬀective. We also observed interesting diﬀerences between people from diﬀerent geographies. For example, people from Europe are more concerned about data breaches than people from North America. People from Asia/Paciﬁc and Europe believe that content and metadata are more critical for privacy than people from North America. Our results contribute to developing a user-driven privacy framework that is based on empirical evidence in addition to the legal, technical, and commercial perspectives.},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Sheth, Swapneel and Kaiser, Gail and Maalej, Walid},
	month = may,
	year = {2014},
	pages = {859--870},
}

@article{merigouxCatalaProgrammingLanguage2021,
	title = {Catala: {A} {Programming} {Language} for the {Law}},
	shorttitle = {Catala},
	url = {http://arxiv.org/abs/2103.03198},
	abstract = {Law at large underpins modern society, codifying and governing many aspects of citizens' daily lives. Oftentimes, law is subject to interpretation, debate and challenges throughout various courts and jurisdictions. But in some other areas, law leaves little room for interpretation, and essentially aims to rigorously describe a computation, a decision procedure or, simply said, an algorithm. Unfortunately, prose remains a woefully inadequate tool for the job. The lack of formalism leaves room for ambiguities; the structure of legal statutes, with many paragraphs and sub-sections spread across multiple pages, makes it hard to compute the intended outcome of the algorithm underlying a given text; and, as with any other piece of poorly-specified critical software, the use of informal language leaves corner cases unaddressed. We introduce Catala, a new programming language that we specifically designed to allow a straightforward and systematic translation of statutory law into an executable implementation. Catala aims to bring together lawyers and programmers through a shared medium, which together they can understand, edit and evolve, bridging a gap that often results in dramatically incorrect implementations of the law. We have implemented a compiler for Catala, and have proven the correctness of its core compilation steps using the F* proof assistant. We evaluate Catala on several legal texts that are algorithms in disguise, notably section 121 of the US federal income tax and the byzantine French family benefits; in doing so, we uncover a bug in the official implementation. We observe as a consequence of the formalization process that using Catala enables rich interactions between lawyers and programmers, leading to a greater understanding of the original legislative intent, while producing a correct-by-construction executable specification reusable by the greater software ecosystem.},
	language = {en},
	urldate = {2021-03-24},
	journal = {arXiv:2103.03198 [cs]},
	author = {Merigoux, Denis and Chataing, Nicolas and Protzenko, Jonathan},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.03198},
	keywords = {Computer Science - Programming Languages},
}

@article{senarathWillTheyUse2019,
	title = {Will {They} {Use} {It} or {Not}? {Investigating} {Software} {Developers}’ {Intention} to {Follow} {Privacy} {Engineering} {Methodologies}},
	volume = {22},
	issn = {2471-2566, 2471-2574},
	shorttitle = {Will {They} {Use} {It} or {Not}?},
	url = {https://dl.acm.org/doi/10.1145/3364224},
	doi = {10.1145/3364224},
	abstract = {With the increasing concerns over privacy in software systems, there is a growing enthusiasm to develop methods to support the development of privacy aware software systems. Inadequate privacy in software system designs could result in users losing their sensitive data, such as health information and financial information, which may cause financial and reputation loss. Privacy Engineering Methodologies (PEMs) are introduced into the software development processes with the goal of guiding software developers to embed privacy into the systems they design. However, for PEMs to be successful it is imperative that software developers have a positive intention to use PEMs. Otherwise, developers may attempt to bypass the privacy methodologies or use them partially and hence develop software systems that may not protect user privacy appropriately. To investigate the factors that affect software developers’ behavioural intention to follow PEMs, in this article, we conducted a study with 149 software developers. Findings of the study show that the usefulness of the PEM to the developers’ existing work to be the strongest determinant that affects software developers’ intention to follow PEMs. Moreover, the compatibility of the PEM with their way of work and how the PEM demonstrates its results when used were also found to be significant. These findings provide important insights in understanding the behaviour of software developers and how they perceive PEMs. The findings could be used to assist organisations and researchers to deploy PEMs and design PEMs that are positively accepted by software developers.},
	language = {en},
	number = {4},
	urldate = {2021-03-24},
	journal = {ACM Transactions on Privacy and Security},
	author = {Senarath, Awanthika and Grobler, Marthie and Arachchilage, Nalin Asanka Gamagedara},
	month = dec,
	year = {2019},
	pages = {1--30},
}

@article{shapiroPrivacyDesignMoving2010,
	title = {Privacy by design: moving from art to practice},
	volume = {53},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Privacy by design},
	url = {https://dl.acm.org/doi/10.1145/1743546.1743559},
	doi = {10.1145/1743546.1743559},
	abstract = {Designing privacy into systems at the beginning of the development process necessitates the effective translation of privacy principles, models, and mechanisms into system requirements.},
	language = {en},
	number = {6},
	urldate = {2021-03-24},
	journal = {Communications of the ACM},
	author = {Shapiro, Stuart S.},
	month = jun,
	year = {2010},
	pages = {27--29},
}

@article{gursesEngineeringPrivacyDesign,
	title = {Engineering {Privacy} by {Design}},
	abstract = {The design and implementation of privacy requirements in systems is a diﬃcult problem and requires the translation of complex social, legal and ethical concerns into systems requirements. The concept of “privacy by design” has been proposed to serve as a guideline on how to address these concerns.},
	language = {en},
	author = {Gurses, Seda and Troncoso, Carmela and Diaz, Claudia},
	pages = {25},
}

@article{diazDiazClaudiaCOSIC,
	title = {Diaz, {Claudia} ‡ {COSIC}/{iMinds}, {Dept}. of {Electrical} {Engineering}, {KU} {Leuven}},
	language = {en},
	author = {Diaz, Claudia},
	pages = {21},
}

@inproceedings{kneuperTranslatingDataProtection2020,
	address = {Valletta, Malta},
	title = {Translating {Data} {Protection} into {Software} {Requirements}:},
	isbn = {978-989-758-399-5},
	shorttitle = {Translating {Data} {Protection} into {Software} {Requirements}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0008873902570264},
	doi = {10.5220/0008873902570264},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Information} {Systems} {Security} and {Privacy}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Kneuper, Ralf},
	year = {2020},
	pages = {257--264},
}

@article{haeberlenPeerReviewPracticalAccountability,
	title = {{PeerReview}: {Practical} {Accountability} for {Distributed} {Systems}},
	abstract = {We describe PeerReview, a system that provides accountability in distributed systems. PeerReview ensures that Byzantine faults whose eﬀects are observed by a correct node are eventually detected and irrefutably linked to a faulty node. At the same time, PeerReview ensures that a correct node can always defend itself against false accusations. These guarantees are particularly important for systems that span multiple administrative domains, which may not trust each other.},
	language = {en},
	author = {Haeberlen, Andreas and Kouznetsov, Petr and Druschel, Peter},
	pages = {14},
}

@article{smithInformationPrivacyMeasuring1996,
	title = {Information {Privacy}: {Measuring} {Individuals}' {Concerns} about {Organizational} {Practices}},
	volume = {20},
	issn = {02767783},
	shorttitle = {Information {Privacy}},
	url = {https://www.jstor.org/stable/249477?origin=crossref},
	doi = {10.2307/249477},
	language = {en},
	number = {2},
	urldate = {2021-03-24},
	journal = {MIS Quarterly},
	author = {Smith, H. Jeff and Milberg, Sandra J. and Burke, Sandra J.},
	month = jun,
	year = {1996},
	pages = {167},
}

@article{kostovaPrivacyEngineeringMeets2020,
	title = {Privacy {Engineering} {Meets} {Software} {Engineering}. {On} the {Challenges} of {Engineering} {Privacy} {ByDesign}},
	url = {http://arxiv.org/abs/2007.08613},
	abstract = {Current day software development relies heavily on the use of service architectures and on agile iterative development methods to design, implement, and deploy systems. These practices result in systems made up of multiple services that introduce new data ﬂows and evolving designs that escape the control of a single designer. Academic privacy engineering literature typically abstracts away such conditions of software production in order to achieve generalizable results. Yet, through a systematic study of the literature, we show that proposed solutions inevitably make assumptions about software architectures, development methods and scope of designer control that are misaligned with current practices. These misalignments are likely to pose an obstacle to operationalizing privacy engineering solutions in the wild. Speciﬁcally, we identify important limitations in the approaches that researchers take to design and evaluate privacy enhancing technologies which ripple to proposals for privacy engineering methodologies. Based on our analysis, we delineate research and actions needed to re-align research with practice, changes that serve a precondition for the operationalization of academic privacy results in common software engineering practices.},
	language = {en},
	urldate = {2021-03-24},
	journal = {arXiv:2007.08613 [cs]},
	author = {Kostova, Blagovesta and Gürses, Seda and Troncoso, Carmela},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08613},
	keywords = {Computer Science - Software Engineering},
}

@article{spiekermannChallengesPrivacyDesign2012,
	title = {The challenges of privacy by design},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2209249.2209263},
	doi = {10.1145/2209249.2209263},
	abstract = {Heralded by regulators, Privacy by Design holds the promise to solve the digital world's privacy problems. But there are immense challenges, including management commitment and step-by-step methods to integrate privacy into systems.},
	language = {en},
	number = {7},
	urldate = {2021-03-24},
	journal = {Communications of the ACM},
	author = {Spiekermann, Sarah},
	month = jul,
	year = {2012},
	pages = {38--40},
}

@inproceedings{veseliEngineeringPrivacyDesign2019,
	address = {Limassol Cyprus},
	title = {Engineering privacy by design: lessons from the design and implementation of an identity wallet platform},
	isbn = {978-1-4503-5933-7},
	shorttitle = {Engineering privacy by design},
	url = {https://dl.acm.org/doi/10.1145/3297280.3297429},
	doi = {10.1145/3297280.3297429},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Veseli, Fatbardh and Olvera, Jetzabel Serna and Pulls, Tobias and Rannenberg, Kai},
	month = apr,
	year = {2019},
	pages = {1475--1483},
}

@inproceedings{katellSituatedInterventionsAlgorithmic2020,
	address = {Barcelona Spain},
	title = {Toward situated interventions for algorithmic equity: lessons from the field},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Toward situated interventions for algorithmic equity},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372874},
	doi = {10.1145/3351095.3372874},
	abstract = {Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is “scalable” beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.},
	language = {en},
	urldate = {2021-03-23},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Katell, Michael and Young, Meg and Dailey, Dharma and Herman, Bernease and Guetler, Vivian and Tam, Aaron and Bintz, Corinne and Raz, Daniella and Krafft, P. M.},
	month = jan,
	year = {2020},
	pages = {45--55},
}

@inproceedings{coleskyCriticalAnalysisPrivacy2016,
	address = {San Jose, CA},
	title = {A {Critical} {Analysis} of {Privacy} {Design} {Strategies}},
	isbn = {978-1-5090-3690-5},
	url = {http://ieeexplore.ieee.org/document/7527750/},
	doi = {10.1109/SPW.2016.23},
	abstract = {The upcoming General Data Protection Regulation is quickly becoming of great concern to organizations which process personal data of European citizens. It is however nontrivial to translate these legal requirements into privacy friendly designs. One recently proposed approach to make ‘privacy by design’ more practical is privacy design strategies. This paper improves the strategy definitions and suggests an additional level of abstraction between strategies and privacy patterns: ‘tactics’. We have identified a collection of such tactics based on an extensive literature review, in particular a catalogue of surveyed privacy patterns. We explore the relationships between the concepts we introduce and similar concepts used in software engineering. This paper helps bridge the gap between data protection requirements set out in law, and system development practice.},
	language = {en},
	urldate = {2021-03-23},
	booktitle = {2016 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	publisher = {IEEE},
	author = {Colesky, Michael and Hoepman, Jaap-Henk and Hillen, Christiaan},
	month = may,
	year = {2016},
	pages = {33--40},
}

@inproceedings{mailewadissanayakaReviewMongoDBSingularity2017,
	address = {Austin Texas USA},
	title = {A {Review} of {MongoDB} and {Singularity} {Container} {Security} in regards to {HIPAA} {Regulations}},
	isbn = {978-1-4503-5195-9},
	url = {https://dl.acm.org/doi/10.1145/3147234.3148133},
	doi = {10.1145/3147234.3148133},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Companion {Proceedings} of the10th {International} {Conference} on {Utility} and {Cloud} {Computing}},
	publisher = {ACM},
	author = {Mailewa Dissanayaka, Akalanka and Shetty, Roshan Ramprasad and Kothari, Samip and Mengel, Susan and Gittner, Lisa and Vadapalli, Ravi},
	month = dec,
	year = {2017},
	pages = {91--97},
}

@inproceedings{khaitzinPrivacyEnforcementLarge2018,
	address = {Haifa Israel},
	title = {Privacy {Enforcement} at a {Large} {Scale} for {GDPR} {Compliance}},
	isbn = {978-1-4503-5849-1},
	url = {https://dl.acm.org/doi/10.1145/3211890.3211913},
	doi = {10.1145/3211890.3211913},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 11th {ACM} {International} {Systems} and {Storage} {Conference}},
	publisher = {ACM},
	author = {Khaitzin, Ety and Shlomo, Roee and Anderson, Maya},
	month = jun,
	year = {2018},
	pages = {124--124},
}

@inproceedings{murmannUsableTransparencyEnhancing2018,
	address = {Barcelona Spain},
	title = {Usable transparency for enhancing privacy in mobile health apps},
	isbn = {978-1-4503-5941-2},
	url = {https://dl.acm.org/doi/10.1145/3236112.3236184},
	doi = {10.1145/3236112.3236184},
	abstract = {We report on our research on usable transparency in the context of mobile health (mhealth) tracking. Usable transparency refers to the usability of transparency-enhancing tools (TETs), which seek to aid users of online data services in improving their privacy. Focusing on ﬁtness tracking scenarios, our research addresses the conceptual and technical demands of such tools in terms of usability.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Human}-{Computer} {Interaction} with {Mobile} {Devices} and {Services} {Adjunct}},
	publisher = {ACM},
	author = {Murmann, Patrick},
	month = sep,
	year = {2018},
	pages = {440--442},
}

@inproceedings{koscinaEnablingTrustHealthcare2019,
	address = {Thessaloniki, Greece},
	title = {Enabling trust in healthcare data exchange with a federated blockchain-based architecture},
	isbn = {978-1-4503-6988-6},
	url = {http://dl.acm.org/citation.cfm?doid=3358695.3360897},
	doi = {10.1145/3358695.3360897},
	abstract = {We propose a new healthcare data exchange platform for research centers, hospitals and healthcare institutions. Our model is based on a federated blockchain network that interconnect the healthcare institutions and orchestrate the data life cycle from the data publication to the data consumption. The blockchain is responsabible to keep the traceability of the whole process and we use a specially designed smart contract to control the data sharing process. Moreover, we provide the means to enforce GDPR and thus achieve a GDPR compliant model.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {{IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence} on   - {WI} '19 {Companion}},
	publisher = {ACM Press},
	author = {Koscina, Mirko and Manset, David and Negri, Claudia and Perez, Octavio},
	year = {2019},
	pages = {231--237},
}

@inproceedings{povseItAllFun2018,
	address = {Ljubljana Slovenia},
	title = {It's all fun and games, and some legalese: data protection implications for increasing cyber-skills of employees through games},
	isbn = {978-1-4503-6515-4},
	shorttitle = {It's all fun and games, and some legalese},
	url = {https://dl.acm.org/doi/10.1145/3277570.3277580},
	doi = {10.1145/3277570.3277580},
	abstract = {In order to combat cyber-attacks, an organisation can decide to train its employees. Improving cyber-skills of employees through educational games means their personal data will be processed and therefore it falls under the scope of the General Data Protection Regulation (GDPR). The goal of this paper is to address challenges that organisations are likely to face in practice, such as invalidity of employees’ consent and over-intrusive monitoring. It argues that in order to approach training lawfully, organisations should (1) choose their external trainer with due diligence, (2) carry out a data protection impact assessment, and under certain circumstances (3) appoint a data protection officer.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {Central} {European} {Cybersecurity} {Conference} 2018},
	publisher = {ACM},
	author = {Povse, Danaja Fabcic},
	month = nov,
	year = {2018},
	pages = {1--5},
}

@inproceedings{sanchez-rolaCanOptOut2019,
	address = {Auckland New Zealand},
	title = {Can {I} {Opt} {Out} {Yet}?: {GDPR} and the {Global} {Illusion} of {Cookie} {Control}},
	isbn = {978-1-4503-6752-3},
	shorttitle = {Can {I} {Opt} {Out} {Yet}?},
	url = {https://dl.acm.org/doi/10.1145/3321705.3329806},
	doi = {10.1145/3321705.3329806},
	abstract = {The European Union’s (EU) General Data Protection Regulation (GDPR), in effect since May 2018, enforces strict limitations on handling users’ personal data, hence impacting their activity tracking on the Web. In this study, we perform an evaluation of the tracking performed in 2,000 high-traffic websites, hosted both inside and outside of the EU.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2019 {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Sanchez-Rola, Iskander and Dell'Amico, Matteo and Kotzias, Platon and Balzarotti, Davide and Bilge, Leyla and Vervier, Pierre-Antoine and Santos, Igor},
	month = jul,
	year = {2019},
	pages = {340--351},
}

@article{kotsiosAnalysisConsequencesGeneral2019,
	title = {An {Analysis} of the {Consequences} of the {General} {Data} {Protection} {Regulation} on {Social} {Network} {Research}},
	volume = {2},
	issn = {2469-7818, 2469-7826},
	url = {https://dl.acm.org/doi/10.1145/3365524},
	doi = {10.1145/3365524},
	abstract = {This article examines the principles outlined in the General Data Protection Regulation in the context of social network data. We provide both a practical guide to General Data Protection Regulation--compliant social network data processing, covering aspects such as data collection, consent, anonymization, and data analysis, and a broader discussion of the problems emerging when the general principles on which the regulation is based are instantiated for this research area.},
	language = {en},
	number = {3},
	urldate = {2021-03-19},
	journal = {ACM Transactions on Social Computing},
	author = {Kotsios, Andreas and Magnani, Matteo and Vega, Davide and Rossi, Luca and Shklovski, Irina},
	month = dec,
	year = {2019},
	pages = {1--22},
}

@inproceedings{zarourSoftwareSecuritySpecifications2020,
	address = {Trondheim Norway},
	title = {Software {Security} {Specifications} and {Design}: {How} {Software} {Engineers} and {Practitioners} {Are} {Mixing} {Things} up},
	isbn = {978-1-4503-7731-7},
	shorttitle = {Software {Security} {Specifications} and {Design}},
	url = {https://dl.acm.org/doi/10.1145/3383219.3383284},
	doi = {10.1145/3383219.3383284},
	abstract = {Huge numbers of worldwide-deployed software suffer from poor quality and possess vulnerabilities with serious impact. Meanwhile, people are using such software to save and manage their valuable information including their monetary data. This has increased the hackers’ appetite to attack software. Henceforth, researchers and practitioners are convinced that software security is not an added value or a gold-plating need. Consequently, security requirements specification and implementation become vital during the software development process. Unfortunately, researchers and practitioners are doing so in a rush. This has made them mix concepts and practices up in a way that can terribly make the problem of delivering software overdue more chronic which will result in a security and technical debt. This research represents a corrective study that sheds light on what has been achieved in analyzing and designing secure software and what are the problems committed and how to handle them.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {ACM},
	author = {Zarour, Mohammad and Alenezi, Mamdouh and Alsarayrah, Khalid},
	month = apr,
	year = {2020},
	pages = {451--456},
}

@article{shastriUnderstandingBenchmarkingImpact2020,
	title = {Understanding and benchmarking the impact of {GDPR} on database systems},
	volume = {13},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3384345.3384354},
	doi = {10.14778/3384345.3384354},
	abstract = {The General Data Protection Regulation (GDPR) provides new rights and protections to European people concerning their personal data. We analyze GDPR from a systems perspective, translating its legal articles into a set of capabilities and characteristics that compliant systems must support. Our analysis reveals the phenomenon of metadata explosion, wherein large quantities of metadata needs to be stored along with the personal data to satisfy the GDPR requirements. Our analysis also helps us identify new workloads that must be supported under GDPR. We design and implement an open-source benchmark called GDPRbench that consists of workloads and metrics needed to understand and assess personal-data processing database systems. To gauge the readiness of modern database systems for GDPR, we follow best practices and developer recommendations to modify Redis, PostgreSQL, and a commercial database system to be GDPR compliant. Our experiments demonstrate that the resulting GDPR-compliant systems achieve poor performance on GPDR workloads, and that performance scales poorly as the volume of personal data increases. We discuss the real-world implications of these .ndings, and identify research challenges towards making GDPR-compliance efficient in production environments. We release all of our so.ware artifacts and datasets at h.p://www:gdprbench:org},
	language = {en},
	number = {7},
	urldate = {2021-03-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Shastri, Supreeth and Banakar, Vinay and Wasserman, Melissa and Kumar, Arun and Chidambaram, Vijay},
	month = mar,
	year = {2020},
	pages = {1064--1077},
}

@inproceedings{krogerHowAppVendors2020,
	address = {Virtual Event Ireland},
	title = {How do app vendors respond to subject access requests? {A} longitudinal privacy study on {iOS} and {Android} {Apps}},
	isbn = {978-1-4503-8833-7},
	shorttitle = {How do app vendors respond to subject access requests?},
	url = {https://dl.acm.org/doi/10.1145/3407023.3407057},
	doi = {10.1145/3407023.3407057},
	abstract = {EU data protection laws grant consumers the right to access the personal data that companies hold about them. In a first-of-itskind longitudinal study, we examine how service providers have complied with subject access requests over four years. In three iterations between 2015 and 2019, we sent subject access requests to vendors of 225 mobile apps popular in Germany. Throughout the iterations, 19 to 26 \% of the vendors were unreachable or did not reply at all. Our subject access requests were fulfilled in 15 to 53 \% of the cases, with an unexpected decline between the GDPR enforcement date and the end of our study. The remaining responses exhibit a long list of shortcomings, including severe violations of information security and data protection principles. Some responses even contained deceptive and misleading statements (7 to 13 \%). Further, 9 \% of the apps were discontinued and 27 \% of the user accounts vanished during our study, mostly without proper notification about the consequences for our personal data. While we observe improvements for selected aspects over time, the results indicate that subject access request handling will be unsatisfactory as long as vendors accept such requests via email and process them manually.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {ACM},
	author = {Kröger, Jacob Leon and Lindemann, Jens and Herrmann, Dominik},
	month = aug,
	year = {2020},
	pages = {1--10},
}

@inproceedings{soeCircumventionDesignDark2020,
	address = {Tallinn Estonia},
	title = {Circumvention by design - dark patterns in cookie consent for online news outlets},
	isbn = {978-1-4503-7579-5},
	url = {https://dl.acm.org/doi/10.1145/3419249.3420132},
	doi = {10.1145/3419249.3420132},
	abstract = {To ensure that users of online services understand what data are collected and how they are used in algorithmic decision-making, the European Union’s General Data Protection Regulation (GDPR) specifies informed consent as a minimal requirement. For online news outlets consent is commonly elicited through interface design elements in the form of a pop-up. We have manually analyzed 300 data collection consent notices from news outlets that are built to ensure compliance with GDPR. The analysis uncovered a variety of strategies or dark patterns that circumvent the intent of GDPR by design. We further study the presence and variety of these dark patterns in these “cookie consents” and use our observations to specify the concept of dark pattern in the context of consent elicitation.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 11th {Nordic} {Conference} on {Human}-{Computer} {Interaction}: {Shaping} {Experiences}, {Shaping} {Society}},
	publisher = {ACM},
	author = {Soe, Than Htut and Nordberg, Oda Elise and Guribye, Frode and Slavkovik, Marija},
	month = oct,
	year = {2020},
	pages = {1--12},
}

@inproceedings{vanbinsbergenEFLINTDomainspecificLanguage2020,
	address = {Virtual USA},
	title = {{eFLINT}: a domain-specific language for executable norm specifications},
	isbn = {978-1-4503-8174-1},
	shorttitle = {{eFLINT}},
	url = {https://dl.acm.org/doi/10.1145/3425898.3426958},
	doi = {10.1145/3425898.3426958},
	abstract = {Software systems that share potentially sensitive data are subjected to laws, regulations, policies and/or contracts. The monitoring, control and enforcement processes applied to these systems are currently to a large extent manual, which we rather automate by embedding the processes as dedicated and adaptable software services in order to improve efficiency and effectiveness. This approach requires such regulatory services to be closely aligned with a formal description of the relevant norms. This paper presents eflint, a domain-specific language developed for formalizing norms. The theoretical foundations of the language are found in transition systems and in Hohfeld’s framework of legal fundamental conceptions. The language can be used to formalize norms from a large variety of sources. The resulting specifications are executable and support several forms of reasoning such as automatic case assessment, manual exploration and simulation. Moreover, the specifications can be used to develop regulatory services for several types of monitoring, control and enforcement. The language is evaluated through a case study formalizing articles 6(1)(a) and 16 of the General Data Protection Regulation (GDPR). A prototype implementation of eflint is discussed and is available online.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {ACM},
	author = {van Binsbergen, L. Thomas and Liu, Lu-Chi and van Doesburg, Robert and van Engers, Tom},
	month = nov,
	year = {2020},
	pages = {124--136},
}

@article{huynhAddressingRegulatoryRequirements2021,
	title = {Addressing {Regulatory} {Requirements} on {Explanations} for {Automated} {Decisions} with {Provenance}—{A} {Case} {Study}},
	volume = {2},
	issn = {2691-199X, 2639-0175},
	url = {https://dl.acm.org/doi/10.1145/3436897},
	doi = {10.1145/3436897},
	abstract = {AI-based automated decisions are increasingly used as part of new services being deployed to the general public. This approach to building services presents significant potential benefits, such as the reduced speed of execution, increased accuracy, lower cost, and ability to adapt to a wide variety of situations. However, equally significant concerns have been raised and are now well documented such as concerns about privacy, fairness, bias, and ethics. On the consumer side, more often than not, the users of those services are provided with no or inadequate explanations for decisions that may impact their lives. In this article, we report the experience of developing a socio-technical approach to constructing explanations for such decisions from their audit trails, or provenance, in an automated manner. The work has been carried out in collaboration with the UK Information Commissioner’s Office. In particular, we have implemented an automated Loan Decision scenario, instrumented its decision pipeline to record provenance, categorized relevant explanations according to their audience and their regulatory purposes, built an explanation-generation prototype, and deployed the whole system in an online demonstrator.},
	language = {en},
	number = {2},
	urldate = {2021-03-19},
	journal = {Digital Government: Research and Practice},
	author = {Huynh, Trung Dong and Tsakalakis, Niko and Helal, Ayah and Stalla-Bourdillon, Sophie and Moreau, Luc},
	month = mar,
	year = {2021},
	pages = {1--14},
}

@inproceedings{zimmeckStandardizingImplementingNot2020,
	address = {Virtual Event USA},
	title = {Standardizing and {Implementing} {Do} {Not} {Sell}},
	isbn = {978-1-4503-8086-7},
	url = {https://dl.acm.org/doi/10.1145/3411497.3420224},
	doi = {10.1145/3411497.3420224},
	abstract = {The California Consumer Privacy Act gives consumers the right to request that businesses do not sell their personal information. “Selling” is defined broadly and covers, among others, making personal information available to ad networks on websites via third party cookies. We began standardizing and implementing Do Not Sell technologies with the goal of integrating Do Not Sell directly into browser settings. Based on OptMeowt, our proof of concept Do Not Sell browser extension, we conduct experiments on the design, implementation, and current state of Do Not Sell. OptMeowt automatically places Do Not Sell cookies on visited sites and sends Do Not Sell headers per our draft standard. We believe that standardizing Do Not Sell provides an important building block for evolving the web towards increased privacy protections.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 19th {Workshop} on {Privacy} in the {Electronic} {Society}},
	publisher = {ACM},
	author = {Zimmeck, Sebastian and Alicki, Kuba},
	month = nov,
	year = {2020},
	pages = {15--20},
}

@inproceedings{kasem-madaniPrivacyPreservingWarningManagement2020,
	address = {Rennes France},
	title = {Privacy-{Preserving} {Warning} {Management} for an {Identity} {Leakage} {Warning} {Network}},
	isbn = {978-1-4503-7599-3},
	url = {https://dl.acm.org/doi/10.1145/3424954.3424955},
	doi = {10.1145/3424954.3424955},
	abstract = {Identity leakage is the public disclosure of user accounts that were stolen from an online service provider, e.g. email adresses and passwords. Identity leakage is an emerging threat to the security of user accounts because the number of online identities grows notably faster than the amount of used email adresses and passwords.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {European} {Interdisciplinary} {Cybersecurity} {Conference}},
	publisher = {ACM},
	author = {Kasem-Madani, Saffija and Malderle, Timo and Boes, Felix and Meier, Michael},
	month = nov,
	year = {2020},
	pages = {1--6},
}

@inproceedings{marquezExploringSecurityIssues2019,
	address = {Montreal, QC, Canada},
	title = {Exploring {Security} {Issues} in {Telehealth} {Systems}},
	isbn = {978-1-72812-251-9},
	url = {https://ieeexplore.ieee.org/document/8823899/},
	doi = {10.1109/SEH.2019.00019},
	abstract = {Telehealth systems (TS’s) provide remote healthbased services to improve the quality of service of patient treatment. Most healthcare professionals have access to standard telecommunications technology (such as Wireless Body Area Network (WBAN), biosensors, remote medical robots, and others) to offer remote care of elderly and physically less able patients as well as remote surgeries, treatments, and diagnoses. In order to ensure the functionality of TS’s, several systemic properties must be satisﬁed, including security. Although there are studies that discuss different security approaches in TS’s, it is difﬁcult to have a clear view of existing security issues and solutions for these systems. In this article, a systematic mapping study was performed to detect, organize and characterize security issues in TS’s. We identiﬁed 41 studies which were classiﬁed according to their research strategy, target problem, security issue addressed, and proposals. Results reveal that (i) 4 security issues were identiﬁed; (ii) 3 strategies were distinguished to handle security issues; (iii) patient and wireless medical data are the most affected medical supplies. Security in TS’s reveals diverse challenges that concern Software Engineering. Areas such as requirements, software architecture, and security patterns play an important role in order to handle security issues.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {2019 {IEEE}/{ACM} 1st {International} {Workshop} on {Software} {Engineering} for {Healthcare} ({SEH})},
	publisher = {IEEE},
	author = {Marquez, Gaston and Astudillo, Hernan and Taramasco, Carla},
	month = may,
	year = {2019},
	pages = {65--72},
}

@article{ModelingInformationSeeking1996,
	title = {Modeling the {Information} {Seeking} of {Professionals}: {A} {General} {Model} {Derived} from {Research} on {Engineers}, {Health} {Care} {Professionals}, and {Lawyers}},
	volume = {66},
	issn = {0024-2519, 1549-652X},
	shorttitle = {Modeling the {Information} {Seeking} of {Professionals}},
	url = {https://www.journals.uchicago.edu/doi/10.1086/602864},
	doi = {10.1086/602864},
	language = {en},
	number = {2},
	urldate = {2021-03-31},
	journal = {The Library Quarterly},
	month = apr,
	year = {1996},
	pages = {161--193},
}

@article{friedmanLawLawyersLegal,
	title = {Law, {Lawyers}, and {Legal} {Practice} in {Silicon} {Valley}: {A} {Preliminary} {Report}},
	language = {en},
	author = {Friedman, Lawrence M and Gordon, Robert W and Pirie, Sophie and Whatley, Edwin},
	pages = {15},
}

@article{duboisHowLawyersEngineer2020,
	title = {How do {Lawyers} {Engineer} and {Develop} {LegalTech} {Projects}? {A} {Story} of {Opportunities}, {Platforms}, {Creative} {Rationalities}, and {Strategies}},
	volume = {2},
	issn = {2652-4074},
	shorttitle = {How do {Lawyers} {Engineer} and {Develop} {LegalTech} {Projects}?},
	url = {https://lthj.qut.edu.au/article/view/1558},
	doi = {10.5204/lthj.v3i1.1558},
	abstract = {Over the last 15 years, the working context of lawyers has undergone many changes. Evolving in an increasingly competitive, deregulated, and globalized market, they are subject to higher tax pressure while being exposed to unbridled technological innovation. Indeed, a growing number of entrepreneurs are using digital solutions to provide online legal services that are supposed to be faster and cheaper. If many of them are nonlawyer legal entrepreneurs, many lawyers are also engineering innovative projects and launching their own start-up companies, known as “LegalTech” or “LawTech.” However, few studies—or none to our limited knowledge—provide an empirically grounded analysis of such projects, leaving some questions unanswered. Who are these entrepreneurial lawyers? How and why do they engineer and develop LegalTech projects? How do they challenge the legal profession? To answer these questions, this article draws on a qualitative study of three contrasted start-ups Belgian lawyers have recently developed. The research methodology combines gray and scientific literature reviews, webdocument (hereafter “manifestos”) analysis, and semi-directive interviews led with the start-up’s founders (n = 5), the Bar Association’s representatives (n = 3), and some members of the main Belgian LegalTech network (n = 4).},
	language = {en},
	number = {2},
	urldate = {2021-03-31},
	journal = {Law, Technology and Humans},
	author = {Dubois, Christophe},
	month = dec,
	year = {2020},
}

@article{bobkowskaEfficientCollaborationLawyers,
	title = {On efficient collaboration between lawyers and software engineers when transforming legal regulations to law-related requirements},
	abstract = {In order to develop information systems which comply with law, cooperation between lawyers and software engineers is necessary. The problem is how to transform legal regulations to law-related systems requirements efficiently. In this paper, we present both lawyer's perspective and software engineer's perspective on this problem and then we attempt to capture a common information space. The lawyer's perspective delivers a method for identifying and analyzing relevant laws and legal regulations. The software engineer's perspective discovers the specifics of dealing with law-related requirements with the use of requirements engineering as a frame of reference. The common information space includes a process of transforming legal regulations to legal requirements and a description of knowledge which must be shared in order to facilitate efficient collaboration.},
	language = {en},
	author = {Bobkowska, Anna and Kowalska, Magdalena},
	pages = {5},
}

@article{ajzen2011theory,
	title = {The theory of planned behaviour: {Reactions} and reflections},
	author = {Ajzen, Icek},
	year = {2011},
	note = {Publisher: Taylor \& Francis},
}

@article{johnson2004mixed,
	title = {Mixed methods research: {A} research paradigm whose time has come},
	volume = {33},
	number = {7},
	journal = {Educational researcher},
	author = {Johnson, R Burke and Onwuegbuzie, Anthony J},
	year = {2004},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {14--26},
}

@article{jonassenEverydayProblemSolving2006,
	title = {Everyday {Problem} {Solving} in {Engineering}:: {Lessons} tor {Engineering} {Educators}},
	language = {en},
	author = {Jonassen, David},
	year = {2006},
	pages = {14},
}

@article{jonassenDesignTheoryProblem2000,
	title = {Toward a design theory of problem solving},
	volume = {48},
	issn = {1042-1629, 1556-6501},
	url = {http://link.springer.com/10.1007/BF02300500},
	doi = {10.1007/BF02300500},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {Educational Technology Research and Development},
	author = {Jonassen, David H.},
	month = dec,
	year = {2000},
	pages = {63--85},
}

@article{richards1983conversational,
	title = {Conversational analysis},
	journal = {Language and communication},
	author = {Richards, Jack C and Schmidt, Richard W},
	year = {1983},
	note = {Publisher: Longman New York},
	pages = {117--154},
}

@article{trevelyanPublishedResearchEngineering2007,
	title = {Published {Research} on {Engineering} {Work}},
	volume = {133},
	issn = {1052-3928, 1943-5541},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%291052-3928%282007%29133%3A4%28300%29},
	doi = {10.1061/(ASCE)1052-3928(2007)133:4(300)},
	abstract = {Engineering literature mostly focuses on the “objects” and techniques that interest engineers: there are only a few accounts of the people that do engineering work and what that work actually involves. The relevant literature contributes a combination of personal opinion, anecdotal reports, and empirical evidence from quantitative and a few qualitative surveys. Most of the empirical research provides only indirect and limited evidence on engineering work as it is practiced because the survey questions seem to be based on preconceived notions of engineering work. Almost all the papers in the literature that refer to engineering practice aim to provide evidence for changing engineering education or evidence to support a particular set of engineering competencies. From these narrow objectives we can learn a little about engineering work. More empirical evidence is needed before we can work towards a comprehensive understanding, let alone improvement of engineering working methods and practices.},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {Journal of Professional Issues in Engineering Education and Practice},
	author = {Trevelyan, James and Tilli, Sabbia},
	month = oct,
	year = {2007},
	pages = {300--307},
}

@article{trevelyanTechnicalCoordinationEngineering2007,
	title = {Technical {Coordination} in {Engineering} {Practice}},
	abstract = {An empirical ethnographic survey of engineers using interviews andfieldohservations in Australia provides evidence that coordinating technical work of other people by gaining their willing cooperation is a major aspect of engineering practice. Technical coordination in the context of this study means working with and influencing other people so they conscientiously perform necessary work to a mutually agreed schedule. While coordination seems to be non-technical, analysis provides evidence supporting the critical importance of technical expertise. Coordination usually involves one-on-one relationships with superiors, clients, peers, subordinates, and outsiders. Coordinating the work of other people seems to be importantfromthe start of an engineering career. Engineering education only provides limited informal coordination skill development and current accreditation criteria may not reflect this aspect of engineering. This paper suggests ways in which students can leam coordination, and describes some ofthe author's experiences in applying this research.},
	language = {en},
	author = {Trevelyan, James},
	year = {2007},
	pages = {15},
}

@article{johriSociomaterialBricolageCreation2011,
	title = {Sociomaterial bricolage: {The} creation of location-spanning work practices by global software developers},
	volume = {53},
	issn = {09505849},
	shorttitle = {Sociomaterial bricolage},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584911000437},
	doi = {10.1016/j.infsof.2011.01.014},
	abstract = {Objective: This study draws on practice-based studies of work to examine successful work practices of global software developers. The primary aim of this study was to understand how workers develop practices that allow them to function effectively across geographically dispersed locations.
Method: An ethnographically-informed ﬁeld study was conducted with data collection at two international locations of a ﬁrm. Interview, observation and archival data were collected. A total of 42 interviews and 3 weeks of observations were conducted.
Results: Teams spread across different locations around the world developed work practices through sociomaterial bricolage. Two facets of technology use were necessary for the creation of these practices: multiplicity of media and relational personalization at dyadic and team levels. New practices were triggered by the need to achieve a work-life balance, which was disturbed by global development. Reﬂecting on my role as a researcher, I underscore the importance of understanding researchers’ own frames of reference and using research practices that mirror informants’ work practices.
Conclusion: Software developers on global teams face unique challenges which necessitate a shift in their work practices. Successful teams are able to create practices that span locations while still being tied to location based practices. Inventive use of material and social resources is central to the creation of these practices.},
	language = {en},
	number = {9},
	urldate = {2021-03-26},
	journal = {Information and Software Technology},
	author = {Johri, Aditya},
	month = sep,
	year = {2011},
	pages = {955--968},
}

@inproceedings{jesiekWorkProgressNovel2020,
	address = {Virtual On line},
	title = {Work in {Progress}: {Novel} {Ethnographic} {Approaches} for {Investigating} {Engineering} {Practice}},
	shorttitle = {Work in {Progress}},
	url = {http://peer.asee.org/35672},
	doi = {10.18260/1-2--35672},
	language = {en},
	urldate = {2021-03-26},
	booktitle = {2020 {ASEE} {Virtual} {Annual} {Conference} {Content} {Access} {Proceedings}},
	publisher = {ASEE Conferences},
	author = {Jesiek, Brent and Johri, Aditya and Brozina, Cory and Korte, Russell},
	month = jun,
	year = {2020},
	pages = {35672},
}

@article{jesiekTypologySociotechnicalEngineering,
	title = {Toward a typology of the sociotechnical in engineering practice},
	abstract = {There remains a surprising lack of empirical research on the day-today work experiences of engineers and other technical professionals. Nonetheless, a growing body of scholarship in engineering studies and allied fields has revealed that engineering practice can be viewed as a kind of sociotechnical performance, with many job tasks requiring technical expertise as well as extensive social interactions and negotiations. The goal of this paper is to offer preliminary insights toward developing a typology of the sociotechnical in engineering practice. Analyzing interviews with early career engineers, we present our findings organized around three themes: 1) learning in engineering practice, 2) engineers as sociotechnical gatekeepers, and 3) sociotechnical interactions with boundary objects. These findings have implications for engineering education by demonstrating the need for learning activities that mirror the engineering workplace.},
	language = {en},
	author = {Jesiek, Brent K and Buswell, Natascha T and Mazzurco, Andrea and Zephirin, Tasha},
	pages = {10},
}

@article{almeidaEngineeringCommunicationIndustry2020,
	title = {Engineering communication in industry and cross-generational challenges: an exploratory study},
	issn = {0304-3797, 1469-5898},
	shorttitle = {Engineering communication in industry and cross-generational challenges},
	url = {https://www.tandfonline.com/doi/full/10.1080/03043797.2020.1737646},
	doi = {10.1080/03043797.2020.1737646},
	abstract = {Engineering communication is a highly desirable competence in industry. However, to become eﬃcient communicators, engineers must consider cross-generational styles in the process. In this exploratory case study, engineering leaders from four industrial segments (High-Tech, Automotive, Aerospace, and Manufacturing) who have had a history of communicating throughout diﬀerent contexts and projects were studied. Interviews and memos were primary data sources used in this study. Key ﬁndings included the need for younger generations of engineers to communicate thoughtfully and with humility. Also, older engineers expressed the need to adjust their communication styles with younger engineers by adapting their communication to include varying modalities (e.g., in person, in writing, with technology). The ﬁndings from this exploratory study can help engineering educators consider how to teach communication skills to students while increasing their awareness for cross-generational interactions upon graduation.},
	language = {en},
	urldate = {2021-03-26},
	journal = {European Journal of Engineering Education},
	author = {Almeida, Lilian Maria de Souza and Becker, Kurt Henry and Villanueva, Idalis},
	month = mar,
	year = {2020},
	pages = {1--13},
}

@article{bielefeldtWorkingEngineersSatisfaction2019,
	title = {Working engineers’ satisfaction with helping people and society through their jobs},
	volume = {44},
	issn = {0304-3797, 1469-5898},
	url = {https://www.tandfonline.com/doi/full/10.1080/03043797.2018.1476468},
	doi = {10.1080/03043797.2018.1476468},
	abstract = {This research explored the extent that working engineers were satisfied with their ability to help or serve people and/or society through their jobs. Over 450 engineering graduates responded to an online survey, including alumni recently transitioning to the workforce from 16 U.S. institutions and professional volunteers with Engineers Without BordersU.S.A. Only 18\% of the respondents currently working in engineering jobs had some level of dissatisfaction with helping others through their job; this differed by job sector and discipline but not gender or between recent alumni and service-active engineers. Forty per cent cited dissatisfaction with service aspects of their work as a contributing factor for leaving an engineering job. A few seemed to have left engineering careers due to dissatisfaction with their ability to help others. The results point to the importance of aligning personal goals for helping people/ society with engineering careers; employers facilitating these connections may reap benefits in employee retention.},
	language = {en},
	number = {6},
	urldate = {2021-03-26},
	journal = {European Journal of Engineering Education},
	author = {Bielefeldt, Angela R. and Canney, Nathan E.},
	month = nov,
	year = {2019},
	pages = {939--953},
}

@article{cranorInformingCaliforniaPrivacy2021,
	title = {Informing {California} privacy regulations with evidence from research},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3447253},
	doi = {10.1145/3447253},
	abstract = {Designing and testing 'Do Not Sell My Personal Information' icons.},
	language = {en},
	number = {3},
	urldate = {2021-03-19},
	journal = {Communications of the ACM},
	author = {Cranor, Lorrie Faith},
	month = mar,
	year = {2021},
	pages = {29--32},
}

@inproceedings{jesiekClosingPracticeGap2017,
	address = {Indianapolis, IN},
	title = {Closing the practice gap: {Studying} boundary spanning in engineering practice to inform educational practice},
	isbn = {978-1-5090-5920-1},
	shorttitle = {Closing the practice gap},
	url = {http://ieeexplore.ieee.org/document/8190503/},
	doi = {10.1109/FIE.2017.8190503},
	abstract = {How to adequately prepare engineering graduates for careers in industry has long been a concern for academics, policymakers, and employers. However, the realities of engineering practice remain somewhat mysterious to many students and instructors, often leaving engineering graduates underprepared for the workplace, and employers dissatisfied with new employees. In this work, we seek to better understand the job expectations and realities of working life as experienced by early career engineers. In this paper, we more specifically focus on three engineering practice scenarios drawn from three interviews to demonstrate the complex, sociotechnical nature of engineering work in large corporations. Our analysis of workplace boundaries, boundary spanning activities, and personal attributes revealed an inextricably linked set of concepts that we present in a new conceptual framework. We conclude with a discussion of implications for both how we understand engineering practice and approach the teaching of engineering. It is expected that this paper can help students, faculty, and industry affiliates reflect on the realities of engineering work.},
	language = {en},
	urldate = {2021-03-26},
	booktitle = {2017 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	publisher = {IEEE},
	author = {Jesiek, Brent K. and Trellinger, Natascha and Nittala, Swetha},
	month = oct,
	year = {2017},
	pages = {1--9},
}

@inproceedings{jesiekBecomingBoundarySpanning2016,
	address = {New Orleans, Louisiana},
	title = {Becoming {Boundary} {Spanning} {Engineers}: {Research} {Methods} and {Preliminary} {Findings}},
	shorttitle = {Becoming {Boundary} {Spanning} {Engineers}},
	url = {http://peer.asee.org/26370},
	doi = {10.18260/p.26370},
	abstract = {A growing body of evidence suggests that practicing engineers are increasingly expected to act as boundary spanners who can participate in and manage diverse local and global teams, translate competing stakeholder demands into effective design solutions, and leverage expert knowledge from multiple fields and specialties. The larger project represented by this paper responds to this reality by proposing boundary spanning as a core meta-attribute for engineering students and early career professionals. This paper more specifically offers a detailed description of the study design for a major phase of this research project that involves conducting in-depth, semi-structured interviews about boundary spanning experiences with more than two dozen early career engineers in the manufacturing, construction, and electronics industries. To keep the scope of the present account more manageable, this paper provides a preliminary window onto our findings. We utilize a single case approach with a focus on three themes emerging from our analysis of two interviews with one research subject. Namely, we discuss transitions from school to internships to full-time job, the social aspects of engineering practice, and the emotional and psychological dimensions of professional work. One leading objective for this paper is to explore the utility of investigating the realities of engineering work through the lens of an overarching meta-attribute such as boundary spanning. We also propose that our findings provide valuable glimpses of engineering practice that might benefit students who are studying or considering studying engineering. This paper may additionally appeal to educators and researchers who are interested in qualitative methods and/or empirical studies of professional practice.},
	language = {en},
	urldate = {2021-03-26},
	booktitle = {2016 {ASEE} {Annual} {Conference} \& {Exposition} {Proceedings}},
	publisher = {ASEE Conferences},
	author = {Jesiek, Brent and Trellinger, Natascha and Mazzurco, Andrea},
	month = jun,
	year = {2016},
	pages = {26370},
}

@article{baytiyehIdentifyingChallengingFactors2012,
	title = {Identifying the challenging factors in the transition from colleges of engineering to employment},
	volume = {37},
	issn = {0304-3797, 1469-5898},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03043797.2011.644761},
	doi = {10.1080/03043797.2011.644761},
	language = {en},
	number = {1},
	urldate = {2021-03-26},
	journal = {European Journal of Engineering Education},
	author = {Baytiyeh, Hoda and Naja, Mohamad},
	month = mar,
	year = {2012},
	pages = {3--14},
}

@inproceedings{aydinWhenGDPRMeets2020,
	address = {Merkez Turkey},
	title = {When {GDPR} {Meets} {CRAs} ({Credit} {Reference} {Agencies}): {Looking} through the {Lens} of {Twitter}},
	isbn = {978-1-4503-8751-4},
	shorttitle = {When {GDPR} {Meets} {CRAs} ({Credit} {Reference} {Agencies})},
	url = {https://dl.acm.org/doi/10.1145/3433174.3433586},
	doi = {10.1145/3433174.3433586},
	abstract = {Collecting information about consumers and businesses from various sources, Credit reference agencies (CRAs) help many organizations such as financial institutions to assess creditworthiness of applicants and customers of their services. CRAs’ business model depends on processing a high volume of personal data including highly sensitive ones, which must be processed within the relevant legal frameworks in different countries they operate their business, e.g., the European Union’s new GDPR (General Data Protection Regulation). This paper reports a data-driven analysis of CRA- and GDPR-related discussions on Twitter. Our analysis covers the three largest multi-national CRAs: Equifax, Experian and TransUnion and we also looked at the UK’s data protection authority, ICO, and two UK-based privacy-advocating NGOs, Privacy International and Open Rights Group (ORG). We have analyzed public tweets of their official Twitter accounts and other public tweets talking about them. Our analysis revealed a very surprising lack of awareness of CRA- and GDPR-related data privacy issues within the general public and an astonishing lack of active communications of CRAs to the general public on relevant GDPR-related privacy issues: out of 39,549 collected tweets we identified only 153 relevant tweets (0.387\%). This small number of tweets are dominated by mentions of security issues (\%73.2), especially data breaches affecting CRAs, not data subject rights or privacy issues directly. Other tweets are mainly about complaints regarding inaccurate data in credit files and questions about how to exercise right to rectification, just two of many data subject rights defined in the GDPR.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {13th {International} {Conference} on {Security} of {Information} and {Networks}},
	publisher = {ACM},
	author = {Aydin, Kubra and Saglam, Rahime Belen and Li, Shujun and Bulbul, Abdullah},
	month = nov,
	year = {2020},
	pages = {1--8},
}

@article{baratiGDPRComplianceVerification2020,
	title = {{GDPR} {Compliance} {Verification} in {Internet} of {Things}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9127459/},
	doi = {10.1109/ACCESS.2020.3005509},
	abstract = {Data privacy in Internet of Things (IoT) applications remains a major concern of regulation bodies. The introduction of the European General Data Protection Regulation (GDPR) enables users to control how their data is accessed and processed, requiring consent from users before any data manipulation is carried out on their (personal) data by smart devices or cloud-hosted services. Blockchains provide the beneﬁts of a distributed and immutable ledger recording digital transactions across a global network of peer nodes. Blockchain support for tracking of operations carried out by an IoT-based system provides greater conﬁdence to a user that the IoT device is not infringing user privacy (as the Blockchain can be audited to verify which operation was carried out, by which actor). A formal model (following the privacy-bydesign approach) is proposed for supporting GDPR compliance checking for smart devices. The privacy requirements of such applications are related to GDPR obligations of device (and software systems) operators (such as user consent, data protection, right to forget etc). Three smart contracts are proposed as a practical solution to support automated veriﬁcation of operations carried out by devices on user data, in accordance with GDPR rules. We evaluate the performance and scalability costs of our approach using a Blockchain test network.},
	language = {en},
	urldate = {2021-03-24},
	journal = {IEEE Access},
	author = {Barati, Masoud and Rana, Omer and Petri, Ioan and Theodorakopoulos, George},
	year = {2020},
	pages = {119697--119709},
}

@article{shastriGDPRAntipatterns2021,
	title = {{GDPR} anti-patterns},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3378061},
	doi = {10.1145/3378061},
	abstract = {How design and operation of modern cloud-scale systems conflict with GDPR.},
	language = {en},
	number = {2},
	urldate = {2021-03-19},
	journal = {Communications of the ACM},
	author = {Shastri, Supreeth and Wasserman, Melissa and Chidambaram, Vijay},
	month = jan,
	year = {2021},
	pages = {59--65},
}

@article{pappachanSieveMiddlewareApproach2020,
	title = {Sieve: a middleware approach to scalable access control for database management systems},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {Sieve},
	url = {https://dl.acm.org/doi/10.14778/3407790.3407835},
	doi = {10.14778/3407790.3407835},
	abstract = {Current approaches for enforcing Fine Grained Access Control (FGAC) in DBMS do not scale to scenarios when the number of access control policies are in the order of thousands. This paper identiﬁes such a use case in the context of emerging smart spaces wherein systems may be required by legislation, such as Europe’s GDPR and California’s CCPA, to empower users to specify who may have access to their data and for what purposes. We present Sieve, a layered approach of implementing FGAC in existing DBMSs, that exploits a variety of their features (e.g., UDFs, index usage hints, query explain) to scale to a large number of policies. Given a query, Sieve exploits its context to ﬁlter the policies that need to be checked. It also generates guarded expressions that save on evaluation cost by grouping policies and exploit database indices to cut on read cost. Our experimental results demonstrate that existing DBMSs can utilize Sieve to signiﬁcantly reduce query-time policy evaluation cost. Using Sieve DBMSs can support real-time access control in applications such as emerging smart environments.},
	language = {en},
	number = {12},
	urldate = {2021-03-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Pappachan, Primal and Yus, Roberto and Mehrotra, Sharad and Freytag, Johann-Christoph},
	month = aug,
	year = {2020},
	pages = {2424--2437},
}

@inproceedings{jungPrivacyOracleSystem2008,
	address = {Alexandria, Virginia, USA},
	title = {Privacy oracle: a system for finding application leaks with black box differential testing},
	isbn = {978-1-59593-810-7},
	shorttitle = {Privacy oracle},
	url = {http://portal.acm.org/citation.cfm?doid=1455770.1455806},
	doi = {10.1145/1455770.1455806},
	abstract = {We describe the design and implementation of Privacy Oracle, a system that reports on application leaks of user information via the network trafﬁc that they send. Privacy Oracle treats each application as a black box, without access to either its internal structure or communication protocols. This means that it can be used over a broad range of applications and information leaks (i.e., not only Web trafﬁc content or credit card numbers). To accomplish this, we develop a differential testing technique in which perturbations in the application inputs are mapped to perturbations in the application outputs to discover likely leaks; we leverage alignment algorithms from computational biology to ﬁnd high quality mappings between different byte-sequences efﬁciently. Privacy Oracle includes this technique and a virtual machine-based testing system. To evaluate it, we tested 26 popular applications, including system and ﬁle utilities, media players, and IM clients. We found that Privacy Oracle discovered many small and previously undisclosed information leaks. In several cases, these are leaks of directly identifying information that are regularly sent in the clear (without endto-end encryption) and which could make users vulnerable to tracking by third parties or providers.},
	language = {en},
	urldate = {2021-04-04},
	booktitle = {Proceedings of the 15th {ACM} conference on {Computer} and communications security - {CCS} '08},
	publisher = {ACM Press},
	author = {Jung, Jaeyeon and Sheth, Anmol and Greenstein, Ben and Wetherall, David and Maganis, Gabriel and Kohno, Tadayoshi},
	year = {2008},
	pages = {279},
}

@misc{directorTop10Open2019,
	title = {Top 10 {Open} {Source} {Social} {Network} {Development} {Platforms} - {TDA}},
	url = {https://topdigital.agency/top-10-open-source-social-network-development-platforms/},
	abstract = {World Web Technology, a social network website development company, shares its top 10 open-source development platforms.},
	language = {en-US},
	urldate = {2021-04-06},
	journal = {Top Digital Agency},
	author = {Director, Sanjay Ghinaiya},
	month = sep,
	year = {2019},
}

@inproceedings{lodgeIoTAppDevelopment2018,
	address = {Singapore Singapore},
	title = {{IoT} {App} {Development}: {Supporting} {Data} {Protection} by {Design} and {Default}},
	isbn = {978-1-4503-5966-5},
	shorttitle = {{IoT} {App} {Development}},
	url = {https://dl.acm.org/doi/10.1145/3267305.3274151},
	doi = {10.1145/3267305.3274151},
	abstract = {In the domestic IoT domain, data is often collected by physical sensors and actuators embedded in the household and used to provide contextually relevant services to end users. Given that this data is often personal, the EU’s General Data Protection Regulation can implicate IoT app developers, requiring them to adhere to "data protection by design and default" to ensure safeguards that protect a data subject’s rights. Yet the simple-to-use task-oriented development environments that are commonly used to build domestic IoT apps provide little support for developers to engage with data protection measures. In this paper we present an overview of an IoT development environment that has been designed to help developers engage with data protection at app design time. We describe a data tracking feature, which makes all personal ﬂows in an app explicit at development time and which provides the foundation for an additonal set of data protection measures, including personal data disclosure risk assessments, transparency of processing and runtime inspection.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2018 {ACM} {International} {Joint} {Conference} and 2018 {International} {Symposium} on {Pervasive} and {Ubiquitous} {Computing} and {Wearable} {Computers}},
	publisher = {ACM},
	author = {Lodge, Tom and Crabtree, Andy and Brown, Anthony},
	month = oct,
	year = {2018},
	pages = {901--910},
}

@inproceedings{ghayyurDesigningPrivacyPreserving2020,
	address = {Virtual Event Japan},
	title = {Designing privacy preserving data sharing middleware for internet of things},
	isbn = {978-1-4503-8136-9},
	url = {https://dl.acm.org/doi/10.1145/3419016.3431484},
	doi = {10.1145/3419016.3431484},
	abstract = {The rise of low-cost Internet of Things (IoT) sensing and communication capabilities has given rise to a range of new smart services that rely on heterogeneous data from devices embedded in our everyday lives. The provision of such IoT services relies on environmental or user data from other data controllers (e.g. network provider, water agency, building management). Recent privacy regulations such as the European General Data Protection Requirement (GDPR) and California Consumer Privacy Act (CCPA) have made it mandatory for data controllers to perform enhanced processing of the shared data with appropriate privacy-preserving mechanisms before release to service providers. To facilitate this, we propose PEIoT, a system for orchestrating privacy-enhanced data flows that (a) provides users (data subjects) with capabilities to opt-in/opt-out in the data that is shared with the service providers and (b) enable data controllers to invoke a range of Privacy Enhancing Technologies (PETs) such as anonymization, randomization, and perturbation to transform data streams into their privacy preserving counterparts. PE-IoT is based on a new model for privacy compliant data sharing and we describe the design and architecture of the PE-IoT system based on this model.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {Third} {Workshop} on {Data}: {Acquisition} {To} {Analysis}},
	publisher = {ACM},
	author = {Ghayyur, Sameera and Pappachan, Primal and Wang, Guoxi and Mehrotra, Sharad and Venkatasubramanian, Nalini},
	month = nov,
	year = {2020},
	pages = {1--6},
}

@inproceedings{arztFlowDroidPreciseContext2014,
	address = {Edinburgh United Kingdom},
	title = {{FlowDroid}: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for {Android} apps},
	isbn = {978-1-4503-2784-8},
	shorttitle = {{FlowDroid}},
	url = {https://dl.acm.org/doi/10.1145/2594291.2594299},
	doi = {10.1145/2594291.2594299},
	abstract = {Today’s smartphones are a ubiquitous source of private and conﬁdential data. At the same time, smartphone users are plagued by carelessly programmed apps that leak important data by accident, and by malicious apps that exploit their given privileges to copy such data intentionally. While existing static taint-analysis approaches have the potential of detecting such data leaks ahead of time, all approaches for Android use a number of coarse-grain approximations that can yield high numbers of missed leaks and false alarms. In this work we thus present FLOWDROID, a novel and highly precise static taint analysis for Android applications. A precise model of Android’s lifecycle allows the analysis to properly handle callbacks invoked by the Android framework, while context, ﬂow, ﬁeld and object-sensitivity allows the analysis to reduce the number of false alarms. Novel on-demand algorithms help FLOWDROID maintain high efﬁciency and precision at the same time.},
	language = {en},
	urldate = {2021-04-04},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Arzt, Steven and Rasthofer, Siegfried and Fritz, Christian and Bodden, Eric and Bartel, Alexandre and Klein, Jacques and Le Traon, Yves and Octeau, Damien and McDaniel, Patrick},
	month = jun,
	year = {2014},
	pages = {259--269},
}

@inproceedings{floreaGlobalViewHard2019,
	address = {Montreal, QC, Canada},
	title = {A {Global} {View} on the {Hard} {Skills} and {Testing} {Tools} in {Software} {Testing}},
	isbn = {978-1-5386-9196-0},
	url = {https://ieeexplore.ieee.org/document/8807575/},
	doi = {10.1109/ICGSE.2019.00035},
	abstract = {Developing software with high quality is challenging in distributed software development. The purpose of the current study is to investigate the testing skills and tools required in the ever-changing world of global software engineering, according to industrial needs. We analysed 500 job ads from 33 countries. The results show that a quarter of the testers and a fifth of developers are asked to work in distributed projects. The testers are asked to be highly skilled in a variety of test activities and tools, while the testing-skills demand for developers is low and somewhat vague. The profile of testers has a strong technical component in addition to the managerial one. Our findings show that employers need most that testers are competent in automated testing. Furthermore, the industry does not cover all aspects of testing with the demand for testers and developers. Surprisingly, neither role is asked to test the implementation of the general data protection requirements. Our study bridges the industrial needs and the practitioners’ skill development process. Therefore, software testers can use our study as a reference point to enhance their skills. Employers should use our results to check their testing-skill coverage within the development teams. Tertiary education providers are encouraged to use our findings, to update the curriculum in the software development area.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {2019 {ACM}/{IEEE} 14th {International} {Conference} on {Global} {Software} {Engineering} ({ICGSE})},
	publisher = {IEEE},
	author = {Florea, Raluca and Stray, Viktoria},
	month = may,
	year = {2019},
	pages = {143--151},
}

@misc{prakashDecentralizedOpenSource,
	title = {9 {Decentralized}, {Open} {Source} {Alternative} {Social} {Media} {Platforms}},
	url = {https://itsfoss.com/mainstream-social-media-alternaives/},
	abstract = {Tired of Big Tech prying on your data and invading your privacy? Here are some open source, decentralized alternate social platforms.},
	language = {en\_US},
	urldate = {2021-04-06},
	journal = {https://itsfoss.com/},
	author = {Prakash, Abhishek},
}

@article{kalloniatis2008addressing,
	title = {Addressing privacy requirements in system design: the {PriS} method},
	volume = {13},
	number = {3},
	journal = {Requirements Engineering},
	author = {Kalloniatis, Christos and Kavakli, Evangelia and Gritzalis, Stefanos},
	year = {2008},
	note = {Publisher: Springer},
	pages = {241--255},
}

@article{deng2011privacy,
	title = {A privacy threat analysis framework: supporting the elicitation and fulfillment of privacy requirements},
	volume = {16},
	number = {1},
	journal = {Requirements Engineering},
	author = {Deng, Mina and Wuyts, Kim and Scandariato, Riccardo and Preneel, Bart and Joosen, Wouter},
	year = {2011},
	note = {Publisher: Springer},
	pages = {3--32},
}

@inproceedings{pereraIntegratingHumanValues2019,
	address = {Jeju Island, Korea (South)},
	title = {Towards {Integrating} {Human} {Values} into {Software}: {Mapping} {Principles} and {Rights} of {GDPR} to {Values}},
	isbn = {978-1-72813-912-8},
	shorttitle = {Towards {Integrating} {Human} {Values} into {Software}},
	url = {https://ieeexplore.ieee.org/document/8920447/},
	doi = {10.1109/RE.2019.00053},
	abstract = {Software has become an integral part of human life. This gives rise to the need of developing software that respects human values such as transparency, fairness and privacy. Software that compromises on human values (e.g. privacy) can affect people’s reputation and impinges on their ability to function in society with the usual freedom and autonomy. Integrating human values into software is, however, a challenging task due to its imprecise and subjective nature. Enforcing regulations is one way to make software development considerate of the desired standards and values. The European Union’s General Data Protection Regulation (GDPR) on software is one such effort to protect EU citizens’ data and personal information. GDPR prescribes data protection principles and data subject rights mainly to protect user privacy. Looking beyond privacy, we studied GDPR to identify the extent to which it covers human values. We mapped GDPR’s data protection principles and data subject rights to a widely accepted human values structure adopted from social sciences. Our results show that GDPR addresses not only privacy but also several other human values including power, security and universalism. Moreover, fairness and transparency stand out as the most value-conscious principles prescribed in GDPR.},
	language = {en},
	urldate = {2021-03-30},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	publisher = {IEEE},
	author = {Perera, Harsha and Hussain, Waqar and Mougouei, Davoud and Shams, Rifat Ara and Nurwidyantoro, Arif and Whittle, Jon},
	month = sep,
	year = {2019},
	pages = {404--409},
}

@inproceedings{chopraSocialMachinesSocial2016,
	address = {Montréal Québec Canada},
	title = {From {Social} {Machines} to {Social} {Protocols}: {Software} {Engineering} {Foundations} for {Sociotechnical} {Systems}},
	isbn = {978-1-4503-4143-1},
	shorttitle = {From {Social} {Machines} to {Social} {Protocols}},
	url = {https://dl.acm.org/doi/10.1145/2872427.2883018},
	doi = {10.1145/2872427.2883018},
	abstract = {The overarching vision of social machines is to facilitate social processes by having computers provide administrative support. We conceive of a social machine as a sociotechnical system (STS): a software-supported system in which autonomous principals such as humans and organizations interact to exchange information and services. Existing approaches for social machines emphasize the technical aspects and inadequately support the meanings of social processes, leaving them informally realized in human interactions. We posit that a fundamental rethinking is needed to incorporate accountability, essential for addressing the openness of the Web and the autonomy of its principals.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 25th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Chopra, Amit K. and Singh, Munindar P.},
	month = apr,
	year = {2016},
	pages = {903--914},
}

@inproceedings{hutchinsonAccountabilityMachineLearning2021,
	address = {Virtual Event Canada},
	title = {Towards {Accountability} for {Machine} {Learning} {Datasets}: {Practices} from {Software} {Engineering} and {Infrastructure}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {Towards {Accountability} for {Machine} {Learning} {Datasets}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445918},
	doi = {10.1145/3442188.3445918},
	abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decisionmaking, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
	month = mar,
	year = {2021},
	pages = {560--575},
}

@misc{Top155Social,
	title = {The {Top} 155 {Social} {Network} {Open} {Source} {Projects}},
	url = {https://awesomeopensource.com/projects/social-network},
	urldate = {2021-04-06},
}

@inproceedings{hjerppeGeneralDataProtection2019,
	address = {Jeju Island, Korea (South)},
	title = {The {General} {Data} {Protection} {Regulation}: {Requirements}, {Architectures}, and {Constraints}},
	isbn = {978-1-72813-912-8},
	shorttitle = {The {General} {Data} {Protection} {Regulation}},
	url = {https://ieeexplore.ieee.org/document/8920529/},
	doi = {10.1109/RE.2019.00036},
	abstract = {The General Data Protection Regulation (GDPR) in the European Union is the most famous recently enacted privacy regulation. Despite of the regulation’s legal, political, and technological ramiﬁcations, relatively little research has been carried out for better understanding the GDPR’s practical implications for requirements engineering and software architectures. Building on a grounded theory approach with close ties to the Finnish software industry, this paper contributes to the sealing of this gap in previous research. Three questions are asked and answered in the context of software development organizations. First, the paper elaborates nine practical constraints under which many small and medium-sized enterprises (SMEs) often operate when implementing solutions that address the new regulatory demands. Second, the paper elicits nine regulatory requirements from the GDPR for software architectures. Third, the paper presents an implementation for a software architecture that complies both with the requirements elicited and the constraints elaborated.},
	language = {en},
	urldate = {2021-03-31},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	publisher = {IEEE},
	author = {Hjerppe, Kalle and Ruohonen, Jukka and Leppanen, Ville},
	month = sep,
	year = {2019},
	pages = {265--275},
}

@article{blixDataProtectionDesign,
	title = {Data {Protection} by {Design} in {Systems} {Development}},
	abstract = {Data protection by design is a principle to systems development meaning that the protection of personal data is built into the systems design from the start. For many jurisdictions, this principle is becoming a legal requirement. Using a research approach based on design science, a framework is constructed helping systems developers achieve privacy by design in a systematic manner. The framework articulate how the business requirements can be captured, assessed, and implemented in the systems development. Examples of how the data protection principles can be concretely implemented is also presented.},
	language = {en},
	author = {Blix, Fredrik and Elshekeil, Salah Addin and Laoyookhong, Saran},
	pages = {6},
}

@inproceedings{boehm1976quantitative,
	title = {Quantitative evaluation of software quality},
	booktitle = {Proceedings of the 2nd international conference on {Software} engineering},
	author = {Boehm, Barry W and Brown, John R and Lipow, Mlity},
	year = {1976},
	pages = {592--605},
}

@incollection{alshammariPrincipledApproachEngineering2017,
	address = {Cham},
	title = {Towards a {Principled} {Approach} for {Engineering} {Privacy} by {Design}},
	volume = {10518},
	isbn = {978-3-319-67279-3 978-3-319-67280-9},
	url = {http://link.springer.com/10.1007/978-3-319-67280-9_9},
	abstract = {Privacy by Design has emerged as a proactive approach for embedding privacy into the early stages of the design of information and communication technologies, but it is no ‘silver bullet’. Challenges involved in engineering Privacy by Design include a lack of holistic and systematic methodologies that address the complexity and variability of privacy issues and support the translation of its principles into engineering activities. A consequence is that its principles are given at a high level of abstraction without accompanying tools and guidelines to address these challenges. We analyse three privacy requirements engineering methods from which we derive a set of criteria that aid in identifying data-processing activities that may lead to privacy violations and harms and also aid in specifying appropriate design decisions. We also present principles for engineering Privacy by Design that can be developed upon these criteria. Based on these, we outline some preliminary thoughts on the form of a principled framework that addresses the plurality and contextuality of privacy issues and supports the translation of the principles of Privacy by Design into engineering activities.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Privacy {Technologies} and {Policy}},
	publisher = {Springer International Publishing},
	author = {Alshammari, Majed and Simpson, Andrew},
	editor = {Schweighofer, Erich and Leitold, Herbert and Mitrakas, Andreas and Rannenberg, Kai},
	year = {2017},
	doi = {10.1007/978-3-319-67280-9_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {161--177},
}

@inproceedings{hafizCollectionPrivacyDesign2006,
	address = {Portland, Oregon},
	title = {A collection of privacy design patterns},
	isbn = {978-1-60558-372-3},
	url = {http://portal.acm.org/citation.cfm?doid=1415472.1415481},
	doi = {10.1145/1415472.1415481},
	abstract = {The growth in computing power has enabled the storage and analysis of large volumes of data. Monitoring the Internet access proﬁles of millions of users has become feasible and also economically lucrative. The interesting thing here is that it is not only the crooks who are interested in privacy intrusion, but government agencies also have vested interest in proﬁling the population mass. This paper describes 4 design patterns that can aide the decision making process for the designers of privacy protecting systems. These design patterns are applicable to the design of anonymity systems for various types of online communication, online data sharing, location monitoring, voting and electronic cash management.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 2006 conference on {Pattern} languages of programs - {PLoP} '06},
	publisher = {ACM Press},
	author = {Hafiz, Munawar},
	year = {2006},
	pages = {1},
}

@article{gerberHowStopEngineers2009,
	title = {How to {Stop} {Engineers} from {Becoming} “{Bush} {Lawyers}”: {The} {Art} of {Teaching} {Law} to {Engineering} and {Construction} {Students}},
	volume = {1},
	issn = {1943-4162, 1943-4170},
	shorttitle = {How to {Stop} {Engineers} from {Becoming} “{Bush} {Lawyers}”},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%291943-4162%282009%291%3A4%28179%29},
	doi = {10.1061/(ASCE)1943-4162(2009)1:4(179)},
	abstract = {Law forms a core part of most engineering and construction programs. The way that law subjects are taught varies dramatically, and too often focuses on trying to teach students complex aspects of the law, such as contract, tort, and trade practices. This paper suggests that the aim of including law subjects in construction and engineering degrees needs to be clearly understood as this determines the content of the law subject. It is argued that the reason for including a law subject should be not to teach students the law, but rather to train them to recognize when legal issues arise in their work, and how to respond to such issues. With this aim in mind, a model curriculum is proposed and insight given into how to most effectively implement such a course.},
	language = {en},
	number = {4},
	urldate = {2021-03-31},
	journal = {Journal of Legal Affairs and Dispute Resolution in Engineering and Construction},
	author = {Gerber, Paula},
	month = nov,
	year = {2009},
	pages = {179--188},
}

@inproceedings{ayalonHowDevelopersMake2017,
	address = {Portland Oregon USA},
	title = {How {Developers} {Make} {Design} {Decisions} about {Users}' {Privacy}: {The} {Place} of {Professional} {Communities} and {Organizational} {Climate}},
	isbn = {978-1-4503-4688-7},
	shorttitle = {How {Developers} {Make} {Design} {Decisions} about {Users}' {Privacy}},
	url = {https://dl.acm.org/doi/10.1145/3022198.3026326},
	doi = {10.1145/3022198.3026326},
	abstract = {New technologies continuously challenge people's information privacy, while privacy protection practices, such as Privacy-by-Design, did not become widespread engineering practices. The difficulty of designing privacy-preserving information systems highlights the need for studying developers’ privacy decision-making processes, as the developers’ communities are well coordinated and they are the ones who can promote or hinder privacy in the systems they design. We conducted an online survey (n=101), to understand the effectors on developers’ professional privacy attitudes and practices. We investigate whether developer’s privacy decision-making is shaped by several factors, including organizational (organizational privacy climate, business and legal contexts), professional (privacy knowledge) and personal (personal perceived privacy).},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {Companion of the 2017 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher = {ACM},
	author = {Ayalon, Oshrat and Toch, Eran and Hadar, Irit and Birnhack, Michael},
	month = feb,
	year = {2017},
	pages = {135--138},
}

@inproceedings{ahmadianPrivacyenhancedSystemDesign2019,
	address = {Limassol Cyprus},
	title = {Privacy-enhanced system design modeling based on privacy features},
	isbn = {978-1-4503-5933-7},
	url = {https://dl.acm.org/doi/10.1145/3297280.3297431},
	doi = {10.1145/3297280.3297431},
	abstract = {To ensure that their stakeholders’ privacy concerns are addressed systematically from the early development phases, organizations can perform a privacy enhancement of the system design. Such a privacy enhancement needs to account for three crucial types of input: First, risks to the rights of natural persons. Second, potential interrelations and dependencies among the privacy controls. Third, potential trade-offs regarding the costs of the controls. Despite numerous existing privacy enhancing technologies and catalogs of privacy controls, there has been no systematic methodology to support privacy enhancement based on these types of input.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Ahmadian, Amir Shayan and Strüber, Daniel and Jürjens, Jan},
	month = apr,
	year = {2019},
	pages = {1492--1499},
}

@article{rivaSoKEngineeringPrivacyaware2020,
	title = {{SoK}: {Engineering} privacy-aware high-tech systems},
	abstract = {The processing of personal data is becoming a key business factor, especially for high-tech system industries such as automotive and healthcare service providers. To protect such data, the European Union (EU) has introduced the General Data Protection Regulation (GDPR), with the aim to standardize and strengthen data protection policies across EU countries. The GDPR defines stringent requirements on the collection and processing of personal data and imposes severe fines and penalties on data controllers and processors for non-compliance. Although the GDPR is enforce since 2018, many public and private organizations are still struggling to fully comply with the regulation. A main reason for this is the lack of usable methodologies that can support developers in designing of GDPRcomplaint high-tech systems. This paper examines the growing literature on methodologies for the design of privacy-aware systems, and identifies the main challenges to be addressed in order to facilitate developers in the design of such systems. In particular, we investigate to what extent existing methodologies (i) cover GDPR and privacy-by-design principles, (ii) address different levels of system design concerns, and (iii) have demonstrated their suitability for the purpose. Our literature study shows that the domain landscape appears to be heterogeneous and disconnected, as existing methodologies often focus only on subsets of the GDPR principles and/or on specific angles of system design. Based on our findings, we provide recommendations on the definition of comprehensive methodologies tailored to designing GDPR-compliant high-tech systems.},
	language = {en},
	author = {Riva, Giovanni Maria and Vasenev, Alexandr and Zannone, Nicola},
	year = {2020},
	pages = {10},
}

@inproceedings{chen2019reliable,
	title = {How reliable is the crowdsourced knowledge of security implementation?},
	booktitle = {2019 {IEEE}/{ACM} 41st international conference on software engineering ({ICSE})},
	author = {Chen, Mengsu and Fischer, Felix and Meng, Na and Wang, Xiaoyin and Grossklags, Jens},
	year = {2019},
	note = {tex.organization: IEEE},
	pages = {536--547},
}

@article{balebakoImprovingAppPrivacy2014,
	title = {Improving {App} {Privacy}: {Nudging} {App} {Developers} to {Protect} {User} {Privacy}},
	volume = {12},
	issn = {1540-7993},
	shorttitle = {Improving {App} {Privacy}},
	url = {http://ieeexplore.ieee.org/document/6876252/},
	doi = {10.1109/MSP.2014.70},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {IEEE Security \& Privacy},
	author = {Balebako, Rebecca and Cranor, Lorrie},
	month = jul,
	year = {2014},
	pages = {55--58},
}

@article{szekely201310,
	title = {10 what do {IT} professionals think about surveillance?},
	volume = {16},
	journal = {Internet and surveillance: The challenges of Web 2.0 and social media},
	author = {Szekely, Ivan},
	year = {2013},
	note = {Publisher: Routledge},
	pages = {198},
}

@inproceedings{tahaeiUnderstandingPrivacyRelatedQuestions2020,
	address = {Honolulu HI USA},
	title = {Understanding {Privacy}-{Related} {Questions} on {Stack} {Overflow}},
	isbn = {978-1-4503-6708-0},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376768},
	doi = {10.1145/3313831.3376768},
	abstract = {We analyse Stack Overﬂow (SO) to understand challenges and confusions developers face while dealing with privacy-related topics. We apply topic modelling techniques to 1,733 privacyrelated questions to identify topics and then qualitatively analyse a random sample of 315 privacy-related questions. Identiﬁed topics include privacy policies, privacy concerns, access control, and version changes. Results show that developers do ask SO for support on privacy-related issues. We also ﬁnd that platforms such as Apple and Google are deﬁning privacy requirements for developers by specifying what “sensitive” information is and what types of information developers need to communicate to users (e.g. privacy policies). We also examine the accepted answers in our sample and ﬁnd that 28\% of them link to ofﬁcial documentation and more than half are answered by SO users without references to any external resources.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Tahaei, Mohammad and Vaniea, Kami and Saphra, Naomi},
	month = apr,
	year = {2020},
	pages = {1--14},
}

@inproceedings{senarathWhyDevelopersCannot2018,
	address = {Christchurch New Zealand},
	title = {Why developers cannot embed privacy into software systems?: {An} empirical investigation},
	isbn = {978-1-4503-6403-4},
	shorttitle = {Why developers cannot embed privacy into software systems?},
	url = {https://dl.acm.org/doi/10.1145/3210459.3210484},
	doi = {10.1145/3210459.3210484},
	abstract = {Pervasive use of software applications continue to challenge user privacy when users interact with software systems. Even though privacy practices such as Privacy by Design (PbD), have clear instructions for software developers to embed privacy into software designs, those practices are yet to become a common practice among software developers. The difficulty of developing privacy preserving software systems highlights the importance of investigating software developers and the problems they face when they are asked to embed privacy into application designs. Software developers are the community who can put practices such as PbD into action. Therefore identifying the problems they face when embedding privacy into software applications and providing solutions to those problems are important to enable the development of privacy preserving software systems. This study investigates 36 software developers in a software design task with instructions to embed privacy in order to identify the problems they face. We derive recommendation guidelines to address the problems to enable the development of privacy preserving software systems.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering} 2018},
	publisher = {ACM},
	author = {Senarath, Awanthika and Arachchilage, Nalin A. G.},
	month = jun,
	year = {2018},
	pages = {211--216},
}

@article{bednarEngineeringPrivacyDesign2019,
	title = {Engineering {Privacy} by {Design}: {Are} engineers ready to live up to the challenge?},
	volume = {35},
	issn = {0197-2243, 1087-6537},
	shorttitle = {Engineering {Privacy} by {Design}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01972243.2019.1583296},
	doi = {10.1080/01972243.2019.1583296},
	abstract = {Organizations struggle to comply with legal requirements as well as customers’ calls for better data protection. On the implementation level, incorporation of privacy protections in products and services depends on the commitment of the engineers who design them. We interviewed six senior engineers, who work for globally leading IT corporations and research institutions, to investigate their motivation and ability to comply with privacy regulations. Our findings point to a lack of perceived responsibility, control, autonomy, and frustrations with interactions with the legal world. While we increasingly call on engineers to go beyond functional requirements and be responsive to human values in our increasingly technological society, we may be facing the dilemma of asking engineers to live up to a challenge they are currently not ready to embrace.},
	language = {en},
	number = {3},
	urldate = {2021-03-24},
	journal = {The Information Society},
	author = {Bednar, Kathrin and Spiekermann, Sarah and Langheinrich, Marc},
	month = may,
	year = {2019},
	pages = {122--142},
}

@inproceedings{urbanMeasuringImpactGDPR2020,
	address = {Taipei Taiwan},
	title = {Measuring the {Impact} of the {GDPR} on {Data} {Sharing} in {Ad} {Networks}},
	isbn = {978-1-4503-6750-9},
	url = {https://dl.acm.org/doi/10.1145/3320269.3372194},
	doi = {10.1145/3320269.3372194},
	abstract = {The European General Data Protection Regulation (GDPR), which went into effect in May 2018, brought new rules for the processing of personal data that affect many business models, including online advertising. The regulation’s definition of personal data applies to every company that collects data from European Internet users. This includes tracking services that, until then, argued that they were collecting anonymous information and data protection requirements would not apply to their businesses.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 15th {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Urban, Tobias and Tatang, Dennis and Degeling, Martin and Holz, Thorsten and Pohlmann, Norbert},
	month = oct,
	year = {2020},
	pages = {222--235},
}

@inproceedings{van2019data,
	title = {Data, data, everywhere: quantifying software developers’ privacy attitudes},
	booktitle = {Int. {Workshop} on socio-technical aspects in security},
	author = {van der Linden, Dirk and Hadar, Irit and Edwards, Matthew and Rashid, Awais},
	year = {2019},
}

@article{tahaeiPrivacyChampionsSoftware2021,
	title = {Privacy {Champions} in {Software} {Teams}: {Understanding} {Their} {Motivations}, {Strategies}, and {Challenges}},
	abstract = {Software development teams are responsible for making and implementing software design decisions that directly impact end-user privacy, a challenging task to do well. Privacy Champions—people who strongly care about advocating privacy—play a useful role in supporting privacy-respecting development cultures. To understand their motivations, challenges, and strategies for protecting end-user privacy, we conducted 12 interviews with Privacy Champions in software development teams. We find that common barriers to implementing privacy in software design include: negative privacy culture, internal prioritisation tensions, limited tool support, unclear evaluation metrics, and technical complexity. To promote privacy, Privacy Champions regularly use informal discussions, management support, communication among stakeholders, and documentation and guidelines. They perceive code reviews and practical training as more instructive than general privacy awareness and on-boarding training. Our study is a first step towards understanding how Privacy Champions work to improve their organisation’s privacy approaches and improve the privacy of enduser products.},
	language = {en},
	author = {Tahaei, Mohammad and Frik, Alisa and Vaniea, Kami},
	year = {2021},
	pages = {16},
}

@inproceedings{razaghpanahAppsTrackersPrivacy2018,
	address = {San Diego, CA},
	title = {Apps, {Trackers}, {Privacy}, and {Regulators}: {A} {Global} {Study} of the {Mobile} {Tracking} {Ecosystem}},
	isbn = {978-1-891562-49-5},
	shorttitle = {Apps, {Trackers}, {Privacy}, and {Regulators}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05B-3_Razaghpanah_paper.pdf},
	doi = {10.14722/ndss.2018.23353},
	abstract = {Third-party services form an integral part of the mobile ecosystem: they ease application development and enable features such as analytics, social network integration, and app monetization through ads. However, aided by the general opacity of mobile systems, such services are also largely invisible to users. This has negative consequences for user privacy as third-party services can potentially track users without their consent, even across multiple applications. Using real-world mobile trafﬁc data gathered by the Lumen Privacy Monitor (Lumen), a privacyenhancing app with the ability to analyze network trafﬁc on mobile devices in user space, we present insights into the mobile advertising and tracking ecosystem and its stakeholders. In this study, we develop automated methods to detect third-party advertising and tracking services at the trafﬁc level. Using this technique we identify 2,121 such services, of which 233 were previously unknown to other popular advertising and tracking blacklists. We then uncover the business relationships between the providers of these services and characterize them by their prevalence in the mobile and Web ecosystem. Our analysis of the privacy policies of the largest advertising and tracking service providers shows that sharing harvested data with subsidiaries and third-party afﬁliates is the norm. Finally, we seek to identify the services likely to be most impacted by privacy regulations such as the European General Data Protection Regulation (GDPR) and ePrivacy directives.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Razaghpanah, Abbas and Nithyanand, Rishab and Vallina-Rodriguez, Narseo and Sundaresan, Srikanth and Allman, Mark and Kreibich, Christian and Gill, Phillipa},
	year = {2018},
}

@inproceedings{krebsTellMeWhat2019,
	address = {Glasgow Scotland Uk},
	title = {Tell {Me} {What} {You} {Know}: {GDPR} {Implications} on {Designing} {Transparency} and {Accountability} for {News} {Recommender} {Systems}},
	isbn = {978-1-4503-5971-9},
	shorttitle = {Tell {Me} {What} {You} {Know}},
	url = {https://dl.acm.org/doi/10.1145/3290607.3312808},
	doi = {10.1145/3290607.3312808},
	abstract = {The GDPR has a significant impact on the way users interact with technologies, especially the everyday platforms used to personalize news and related forms of information. This paper presents the initial results from a study whose primary objective is to empirically test those platforms’ level of compliance with the so-called ‘right to explanation’. Four research topics considered as gaps in existing legal and HCI scholarship originated from the project’s initial phase, namely (1) GDPR compliance through user-centered design; (2) the inclusion of values in the system; (3) design considerations regarding interaction strategies, algorithmic experience, transparency, and explanations; and (4) technical challenges. The second phase is currently ongoing and allows us to make some observations regarding the registration process and the privacy policies of three categories of news actors: first-party content providers, news aggregators and social media platforms.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Krebs, Luciana Monteiro and Alvarado Rodriguez, Oscar Luis and Dewitte, Pierre and Ausloos, Jef and Geerts, David and Naudts, Laurens and Verbert, Katrien},
	month = may,
	year = {2019},
	pages = {1--6},
}

@article{liHowDevelopersTalk2021,
	title = {How {Developers} {Talk} {About} {Personal} {Data} and {What} {It} {Means} for {User} {Privacy}: {A} {Case} {Study} of a {Developer} {Forum} on {Reddit}},
	volume = {4},
	issn = {2573-0142},
	shorttitle = {How {Developers} {Talk} {About} {Personal} {Data} and {What} {It} {Means} for {User} {Privacy}},
	url = {https://dl.acm.org/doi/10.1145/3432919},
	doi = {10.1145/3432919},
	abstract = {While online developer forums are major resources of knowledge for application developers, their roles in promoting better privacy practices remain underexplored. In this paper, we conducted a qualitative analysis of a sample of 207 threads (4772 unique posts) mentioning different forms of personal data from the /r/androiddev forum on Reddit. We started with bottom-up open coding on the sampled posts to develop a typology of discussions about personal data use and conducted follow-up analyses to understand what types of posts elicited in-depth discussions on privacy issues or mentioned risky data practices. Our results show that Android developers rarely discussed privacy concerns when talking about a specific app design or implementation problem, but often had active discussions around privacy when stimulated by certain external events representing new privacy-enhancing restrictions from the Android operating system, app store policies, or privacy laws. Developers often felt these restrictions could cause considerable cost yet fail to generate any compelling benefit for themselves. Given these results, we present a set of suggestions for Android OS and the app store to design more effective methods to enhance privacy, and for developer forums(e.g., /r/androiddev) to encourage more in-depth privacy discussions and nudge developers to think more about privacy.},
	language = {en},
	number = {CSCW3},
	urldate = {2021-03-19},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Li, Tianshi and Louie, Elizabeth and Dabbish, Laura and Hong, Jason I.},
	month = jan,
	year = {2021},
	pages = {1--28},
}

@article{stewartEmpiricalExaminationConcern2002,
	title = {An {Empirical} {Examination} of the {Concern} for {Information} {Privacy} {Instrument}},
	volume = {13},
	issn = {1047-7047, 1526-5536},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/isre.13.1.36.97},
	doi = {10.1287/isre.13.1.36.97},
	language = {en},
	number = {1},
	urldate = {2021-03-24},
	journal = {Information Systems Research},
	author = {Stewart, Kathy A. and Segars, Albert H.},
	month = mar,
	year = {2002},
	pages = {36--49},
}

@inproceedings{utzInformedConsentStudying2019,
	address = {London United Kingdom},
	title = {({Un})informed {Consent}: {Studying} {GDPR} {Consent} {Notices} in the {Field}},
	isbn = {978-1-4503-6747-9},
	shorttitle = {({Un})informed {Consent}},
	url = {https://dl.acm.org/doi/10.1145/3319535.3354212},
	doi = {10.1145/3319535.3354212},
	abstract = {Since the adoption of the General Data Protection Regulation (GDPR) in May 2018 more than 60 \% of popular websites in Europe display cookie consent notices to their visitors. This has quickly led to users becoming fatigued with privacy notifications and contributed to the rise of both browser extensions that block these banners and demands for a solution that bundles consent across multiple websites or in the browser. In this work, we identify common properties of the graphical user interface of consent notices and conduct three experiments with more than 80,000 unique users on a German website to investigate the influence of notice position, type of choice, and content framing on consent. We find that users are more likely to interact with a notice shown in the lower (left) part of the screen. Given a binary choice, more users are willing to accept tracking compared to mechanisms that require them to allow cookie use for each category or company individually. We also show that the widespread practice of nudging has a large effect on the choices users make. Our experiments show that seemingly small implementation decisions can substantially impact whether and how people interact with consent notices. Our findings demonstrate the importance for regulation to not just require consent, but also provide clear requirements or guidance for how this consent has to be obtained in order to ensure that users can make free and informed choices.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Utz, Christine and Degeling, Martin and Fahl, Sascha and Schaub, Florian and Holz, Thorsten},
	month = nov,
	year = {2019},
	pages = {973--990},
}

@article{zaeemEffectGDPRPrivacy2021,
	title = {The {Effect} of the {GDPR} on {Privacy} {Policies}: {Recent} {Progress} and {Future} {Promise}},
	volume = {12},
	issn = {2158-656X, 2158-6578},
	shorttitle = {The {Effect} of the {GDPR} on {Privacy} {Policies}},
	url = {https://dl.acm.org/doi/10.1145/3389685},
	doi = {10.1145/3389685},
	abstract = {The General Data Protection Regulation (GDPR) is considered by some to be the most important change in data privacy regulation in 20 years. Effective May 2018, the European Union GDPR privacy law applies to any organization that collects and processes the personal information of EU citizens within or outside the EU. In this work, we seek to quantify the progress the GDPR has made in improving privacy policies around the globe. We leverage our data mining tool, PrivacyCheck, to automatically compare three corpora (totaling 550) of privacy policies, pre- and post-GDPR. In addition, to evaluate the current level of compliance with the GDPR around the globe, we manually studied the policies within two corpora (450 policies). We find that the GDPR has made progress in protecting user data, but more progress is necessary—particularly in the area of giving users the right to edit and delete their information—to entirely fulfill the GDPR’s promise. We also observe that the GDPR encourages sharing user data with law enforcement, and as a result, many policies have facilitated such sharing after the GDPR. Finally, we see that when there is non-compliance with the GDPR, it is often in the form of failing to explicitly indicate compliance, which in turn speaks to an organization’s lack of transparency and disclosure regarding their processing and protection of personal information. If Personally Identifiable Information (PII) is the “currency of the Internet,” these findings mark continued alarm regarding an individual’s agency to protect and secure their PII assets.},
	language = {en},
	number = {1},
	urldate = {2021-03-19},
	journal = {ACM Transactions on Management Information Systems},
	author = {Zaeem, Razieh Nokhbeh and Barber, K. Suzanne},
	month = mar,
	year = {2021},
	pages = {1--20},
}

@article{strobel2013empathy,
	title = {Empathy and care within engineering: qualitative perspectives from engineering faculty and practicing engineers},
	volume = {5},
	number = {2},
	journal = {Engineering Studies},
	author = {Strobel, Johannes and Hess, Justin and Pan, Rui and Wachter Morris, Carrie A},
	year = {2013},
	note = {Publisher: Taylor \& Francis},
	pages = {137--159},
}

@inproceedings{levy2018importance,
	title = {The importance of empathy for analyzing privacy requirements},
	booktitle = {2018 {IEEE} 5th international workshop on evolving security \& privacy requirements engineering ({ESPRE})},
	author = {Levy, Meira and Hadar, Irit},
	year = {2018},
	note = {tex.organization: IEEE},
	pages = {9--13},
}

@incollection{ajzen1985intentions,
	title = {From intentions to actions: {A} theory of planned behavior},
	booktitle = {Action control},
	publisher = {Springer},
	author = {Ajzen, Icek},
	year = {1985},
	pages = {11--39},
}

@inproceedings{tesfayPrivacyGuideImplementationEU2018,
	address = {Tempe AZ USA},
	title = {{PrivacyGuide}: {Towards} an {Implementation} of the {EU} {GDPR} on {Internet} {Privacy} {Policy} {Evaluation}},
	isbn = {978-1-4503-5634-3},
	shorttitle = {{PrivacyGuide}},
	url = {https://dl.acm.org/doi/10.1145/3180445.3180447},
	doi = {10.1145/3180445.3180447},
	abstract = {Nowadays Internet services have dramatically changed the way people interact with each other and many of our daily activities are supported by those services. Statistical indicators show that more than half of the world’s population uses the Internet generating about 2.5 quintillion bytes of data on daily basis. While such a huge amount of data is useful in a number of fields, such as in medical and transportation systems, it also poses unprecedented threats for user’s privacy. This is aggravated by the excessive data collection and user profiling activities of service providers. Yet, regulation require service providers to inform users about their data collection and processing practices. The de facto way of informing users about these practices is through the use of privacy policies. Unfortunately, privacy policies suffer from bad readability and other complexities which make them unusable for the intended purpose. To address this issue, we introduce PrivacyGuide, a privacy policy summarization tool inspired by the European Union (EU) General Data Protection Regulation (GDPR) and based on machine learning and natural language processing techniques. Our results show that PrivacyGuide is able to classify privacy policy content into eleven privacy aspects with a weighted average accuracy of 74\% and further shed light on the associated risk level with an accuracy of 90\%.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {Fourth} {ACM} {International} {Workshop} on {Security} and {Privacy} {Analytics}},
	publisher = {ACM},
	author = {Tesfay, Welderufael B. and Hofmann, Peter and Nakamura, Toru and Kiyomoto, Shinsaku and Serna, Jetzabel},
	month = mar,
	year = {2018},
	pages = {15--21},
}

@inproceedings{nokhbehzaeemPrivacyCheckV2Tool2020,
	address = {Virtual Event Ireland},
	title = {{PrivacyCheck} v2: {A} {Tool} that {Recaps} {Privacy} {Policies} for {You}},
	isbn = {978-1-4503-6859-9},
	shorttitle = {{PrivacyCheck} v2},
	url = {https://dl.acm.org/doi/10.1145/3340531.3417469},
	doi = {10.1145/3340531.3417469},
	abstract = {Despite the efforts to regulate privacy policies to protect user privacy, these policies remain lengthy and hard to comprehend. Powered by machine learning, our publicly available browser extension, PrivacyCheck v2, automatically summarizes any privacy policy by answering 20 questions based upon User Control and the General Data Protection Regulation. Furthermore, PrivacyCheck v2 incorporates a competitor analysis tool that highlights the top competitors with the best privacy policies in the same market sector. PrivacyCheck v2 enhances the users’ understanding of privacy policies and empowers them to make informed decisions when it comes to selecting services with better privacy policies.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Nokhbeh Zaeem, Razieh and Anya, Safa and Issa, Alex and Nimergood, Jake and Rogers, Isabelle and Shah, Vinay and Srivastava, Ayush and Barber, K. Suzanne},
	month = oct,
	year = {2020},
	pages = {3441--3444},
}

@incollection{voigtPracticalImplementationRequirements2017,
	title = {Practical implementation of the requirements under the {GDPR}},
	booktitle = {The {EU} {General} {Data} {Protection} {Regulation} ({GDPR})},
	publisher = {Springer},
	author = {Voigt, Paul and von dem Bussche, Axel},
	year = {2017},
	pages = {245--249},
}

@inproceedings{sirurAreWeThere2018,
	address = {Toronto Canada},
	title = {Are {We} {There} {Yet}?: {Understanding} the {Challenges} {Faced} in {Complying} with the {General} {Data} {Protection} {Regulation} ({GDPR})},
	isbn = {978-1-4503-5988-7},
	shorttitle = {Are {We} {There} {Yet}?},
	url = {https://dl.acm.org/doi/10.1145/3267357.3267368},
	doi = {10.1145/3267357.3267368},
	abstract = {The EU General Data Protection Regulation (GDPR), enforced from 25th May 2018, aims to reform how organisations view and control the personal data of private EU citizens. The scope of GDPR is somewhat unprecedented: it regulates every aspect of personal data handling, includes hefty potential penalties for non-compliance, and can prosecute any company in the world that processes EU citizens’ data. In this paper, we look behind the scenes to investigate the real challenges faced by organisations in engaging with the GDPR. This considers issues in working with the regulation, the implementation process, and how compliance is verified. Our research approach relies on literature but, more importantly, draws on detailed interviews with several organisations. Key findings include the fact that large organisations generally found GDPR compliance to be reasonable and doable. The same was found for small-to-medium organisations (SMEs/SMBs) that were highly security-oriented. SMEs with less focus on data protection struggled to make what they felt was a satisfactory attempt at compliance. The main issues faced in their compliance attempts emerged from: the sheer breadth of the regulation; questions around how to enact the qualitative recommendations of the regulation; and the need to map out the entirety of their complex data networks.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Multimedia} {Privacy} and {Security}},
	publisher = {ACM},
	author = {Sirur, Sean and Nurse, Jason R.C. and Webb, Helena},
	month = jan,
	year = {2018},
	pages = {88--95},
}

@inproceedings{bartoliniGDPRBusinessProcesses2019,
	address = {Las Palmas de Gran Canaria, Spain},
	title = {{GDPR} and business processes: an effective solution},
	isbn = {978-1-4503-6085-2},
	shorttitle = {{GDPR} and business processes},
	url = {http://dl.acm.org/citation.cfm?doid=3309772.3309779},
	doi = {10.1145/3309772.3309779},
	abstract = {In the European Union, the recent update to data protection laws by virtue of the General Data Protection Regulation (GDPR) significantly changed the landscape of the processing of personal data. Consequently, adequate solutions to ensure that the controller and processor properly understand and meet the data protection requirements are needed. In enterprise reality it is quite common to use Business Process (BP) models to manage the different business activities. Hence the idea of integrating privacy concepts into BP models so as to leverage them to the role of GDPR recommenders. To this end, suggestions and recommendations about data management pursuant to GDPR provisions have been added to specific tasks of the BP, to improve both the process management and personnel learning and training. Feasibility of the proposed idea, implemented into an Eclipse plugin, has been provided through a realistic example.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Applications} of {Intelligent} {Systems}  - {APPIS} '19},
	publisher = {ACM Press},
	author = {Bartolini, Cesare and Calabró, Antonello and Marchetti, Eda},
	year = {2019},
	pages = {1--5},
}

@inproceedings{capodieciBusinessProcessAwareness2019,
	address = {Cairo Egypt},
	title = {Business process awareness to support {GDPR} compliance},
	isbn = {978-1-4503-6292-4},
	url = {https://dl.acm.org/doi/10.1145/3361570.3361573},
	doi = {10.1145/3361570.3361573},
	abstract = {This paper proposes a model-driven approach based on business process awareness to support compliance with the European General Data Protection Regulation (GDPR, EU 2016/679). GDPR concerns the processing of personal data and the free movement of such data, and its main purpose is to safeguard the human dignity and fundamental rights of the data subject. To achieve this goal, it is necessary to identify the motivation for data management, to define who has access to data, and to determine with high precision how, when and how many times the organisation should store and manage these data. GDPR requires the self-assessment of digital risk on the basis of an impact assessment analysis. The adoption of GDPR by an organisation raises the main question of how to audit the organisation’s adherence. Starting from BPMN, which can allow businesses to better understand their internal business procedures, we propose an approach that helps to identify the most important key point(s) for GDPR compliance. To analyse the potential applicability of our thesis, we describe a vacation request scenario to which we apply the proposed approach.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Information} {Systems} and {Technologies}},
	publisher = {ACM},
	author = {Capodieci, Antonio and Mainetti, Luca},
	month = mar,
	year = {2019},
	pages = {1--6},
}

@inproceedings{manginiEmpiricalStudyImpact2020,
	address = {Virtual Event Ireland},
	title = {An empirical study on the impact of {GDPR} and right to be forgotten - organisations and users perspective},
	isbn = {978-1-4503-8833-7},
	url = {https://dl.acm.org/doi/10.1145/3407023.3407080},
	doi = {10.1145/3407023.3407080},
	abstract = {The General Data Protection Regulation (GDPR) is a prescriptive legislation in the European Union (EU) for privacy and data protection that applies to every organisation within the EU and any organisation outside the EU if they offer goods or services to EU citizens. The enforcement of GDPR created a big challenge for organisations which were required to create new professional figures, system, policies, procedures and standards, budget for new investments, and to set up a project plan or catalogue specific to the GDPR. This paper focuses on the GDPR ‘right to be forgotten’ and the specific implementation challenges it poses. The research study used two surveys to collect data from both organisations and users. The results show that while organisations are struggling with GDPR and right to be forgotten, there are also positive aspects about its implementation that translate into improved data privacy. The findings related to the users show that they are in general happy with the legislation.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {ACM},
	author = {Mangini, Vincenzo and Tal, Irina and Moldovan, Arghir-Nicolae},
	month = aug,
	year = {2020},
	pages = {1--9},
}

@article{chungCONANCOunterNArratives2019,
	title = {{CONAN} -- {COunter} {NArratives} through {Nichesourcing}: a {Multilingual} {Dataset} of {Responses} to {Fight} {Online} {Hate} {Speech}},
	shorttitle = {{CONAN} -- {COunter} {NArratives} through {Nichesourcing}},
	url = {http://arxiv.org/abs/1910.03270},
	doi = {10.18653/v1/P19-1271},
	abstract = {Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms, dealing with hatred online is still a tough problem. Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). In this paper, we describe the creation of the ﬁrst large-scale, multilingual, expert-based dataset of hate speech/counternarrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. Finally, we provide initial experiments to assess the quality of our data.},
	language = {en},
	urldate = {2021-04-02},
	journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	author = {Chung, Y. L. and Kuzmenko, E. and Tekiroglu, S. S. and Guerini, M.},
	year = {2019},
	note = {arXiv: 1910.03270},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	pages = {2819--2829},
}

@article{vidgenDirectionsAbusiveLanguage2020,
	title = {Directions in {Abusive} {Language} {Training} {Data}: {Garbage} {In}, {Garbage} {Out}},
	volume = {15},
	issn = {1932-6203},
	shorttitle = {Directions in {Abusive} {Language} {Training} {Data}},
	url = {http://arxiv.org/abs/2004.01670},
	doi = {10.1371/journal.pone.0243300},
	abstract = {Data-driven analysis and detection of abusive online content covers many different tasks, phenomena, contexts, and methodologies. This paper systematically reviews abusive language dataset creation and content in conjunction with an open website for cataloguing abusive language data. This collection of knowledge leads to a synthesis providing evidence-based recommendations for practitioners working with this complex and highly diverse data.},
	language = {en},
	number = {12},
	urldate = {2021-04-02},
	journal = {PLOS ONE},
	author = {Vidgen, Bertie and Derczynski, Leon},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.01670},
	keywords = {Computer Science - Computation and Language},
	pages = {e0243300},
}

@inproceedings{rezvanQualityTypeawareAnnotated2018,
	address = {Amsterdam Netherlands},
	title = {A {Quality} {Type}-aware {Annotated} {Corpus} and {Lexicon} for {Harassment} {Research}},
	isbn = {978-1-4503-5563-6},
	url = {https://dl.acm.org/doi/10.1145/3201064.3201103},
	doi = {10.1145/3201064.3201103},
	abstract = {A quality annotated corpus is essential to research. Despite the recent focus of the Web science community on cyberbullying research, the community lacks standard benchmarks. This paper provides both a quality annotated corpus and an offensive words lexicon capturing different types of harassment content: (i) sexual, (ii) racial, (iii) appearance-related, (iv) intellectual, and (v) political1. We first crawled data from Twitter using this content-tailored offensive lexicon. As mere presence of an offensive word is not a reliable indicator of harassment, human judges annotated tweets for the presence of harassment. Our corpus consists of 25,000 annotated tweets for the five types of harassment content and is available on the Git repository2.},
	language = {en},
	urldate = {2021-04-02},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Web} {Science}},
	publisher = {ACM},
	author = {Rezvan, Mohammadreza and Shekarpour, Saeedeh and Balasuriya, Lakshika and Thirunarayan, Krishnaprasad and Shalin, Valerie L. and Sheth, Amit},
	month = may,
	year = {2018},
	pages = {33--36},
}

@article{18956GoogleLLC2021,
	title = {18-956 {Google} {LLC} v. {Oracle} {America}, {Inc}. (04/05/2021)},
	language = {en},
	year = {2021},
	pages = {62},
}

@article{jurgensJustComprehensiveStrategy2019,
	title = {A {Just} and {Comprehensive} {Strategy} for {Using} {NLP} to {Address} {Online} {Abuse}},
	url = {http://arxiv.org/abs/1906.01738},
	abstract = {Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow deﬁnition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.},
	language = {en},
	urldate = {2021-04-02},
	journal = {arXiv:1906.01738 [cs]},
	author = {Jurgens, David and Chandrasekharan, Eshwar and Hemphill, Libby},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01738},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Social and Information Networks},
}

@article{polettoResourcesBenchmarkCorpora2020,
	title = {Resources and benchmark corpora for hate speech detection: a systematic review},
	issn = {1574-020X, 1574-0218},
	shorttitle = {Resources and benchmark corpora for hate speech detection},
	url = {http://link.springer.com/10.1007/s10579-020-09502-8},
	doi = {10.1007/s10579-020-09502-8},
	abstract = {Hate Speech in social media is a complex phenomenon, whose detection has recently gained signiﬁcant traction in the Natural Language Processing community, as attested by several recent review works. Annotated corpora and benchmarks are key resources, considering the vast number of supervised approaches that have been proposed. Lexica play an important role as well for the development of hate speech detection systems. In this review, we systematically analyze the resources made available by the community at large, including their development methodology, topical focus, language coverage, and other factors. The results of our analysis highlight a heterogeneous, growing landscape, marked by several issues and venues for improvement.},
	language = {en},
	urldate = {2021-04-02},
	journal = {Language Resources and Evaluation},
	author = {Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
	month = sep,
	year = {2020},
}

@article{dadvarCyberbullyingDetectionSocial,
	title = {Cyberbullying {Detection} in {Social} {Networks} {Using} {Deep} {Learning} {Based} {Models}; {A} {Reproducibility} {Study}},
	abstract = {Cyberbullying is a disturbing online misbehaviour with troubling consequences. It appears in different forms, and in most of the social networks, it is in textual format. Automatic detection of such incidents requires intelligent systems. Most of the existing studies have approached this problem with conventional machine learning models and the majority of the developed models in these studies are adaptable to a single social network at a time. In recent studies, deep learning based models have found their way in the detection of cyberbullying incidents, claiming that they can overcome the limitations of the conventional models, and improve the detection performance. In this paper, we investigate the findings of a recent literature in this regard. We successfully reproduced the findings of this literature and validated their findings using the same datasets, namely Wikipedia, Twitter, and Formspring, used by the authors. Then we expanded our work by applying the developed methods on a new YouTube dataset ({\textasciitilde}54k posts by {\textasciitilde}4k users) and investigated the performance of the models in new social media platforms. We also transferred and evaluated the performance of the models trained on one platform to another platform. Our findings show that the deep learning based models outperform the machine learning models previously applied to the same YouTube dataset. We believe that the deep learning based models can also benefit from integrating other sources of information and looking into the impact of profile information of the users in social networks.},
	language = {en},
	author = {Dadvar, Maral and Eckert, Kai},
	pages = {13},
}

@article{emmeryCurrentLimitationsCyberbullying2019,
	title = {Current {Limitations} in {Cyberbullying} {Detection}: on {Evaluation} {Criteria}, {Reproducibility}, and {Data} {Scarcity}},
	shorttitle = {Current {Limitations} in {Cyberbullying} {Detection}},
	url = {http://arxiv.org/abs/1910.11922},
	abstract = {The detection of online cyberbullying has seen an increase in societal importance, popularity in research, and available open data. Nevertheless, while computational power and affordability of resources continue to increase, the access restrictions on high-quality data limit the applicability of state-of-the-art techniques. Consequently, much of the recent research uses small, heterogeneous datasets, without a thorough evaluation of applicability. In this paper, we further illustrate these issues, as we (i) evaluate many publicly available resources for this task and demonstrate difﬁculties with data collection. These predominantly yield small datasets that fail to capture the required complex social dynamics and impede direct comparison of progress. We (ii) conduct an extensive set of experiments that indicate a general lack of cross-domain generalization of classiﬁers trained on these sources, and openly provide this framework to replicate and extend our evaluation criteria. Finally, we (iii) present an effective crowdsourcing method: simulating real-life bullying scenarios in a lab setting generates plausible data that can be effectively used to enrich real data. This largely circumvents the restrictions on data that can be collected, and increases classiﬁer performance. We believe these contributions can aid in improving the empirical practices of future research in the ﬁeld.},
	language = {en},
	urldate = {2021-04-01},
	journal = {arXiv:1910.11922 [cs]},
	author = {Emmery, Chris and Verhoeven, Ben and De Pauw, Guy and Jacobs, Gilles and Van Hee, Cynthia and Lefever, Els and Desmet, Bart and Hoste, Véronique and Daelemans, Walter},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.11922},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Social and Information Networks},
}

@article{qianBenchmarkDatasetLearning2019,
	title = {A {Benchmark} {Dataset} for {Learning} to {Intervene} in {Online} {Hate} {Speech}},
	url = {http://arxiv.org/abs/1909.04251},
	abstract = {Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets1 collected from Gab2 and Reddit3. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk4 Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.},
	language = {en},
	urldate = {2021-04-01},
	journal = {arXiv:1909.04251 [cs]},
	author = {Qian, Jing and Bethke, Anna and Liu, Yinyin and Belding, Elizabeth and Wang, William Yang},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.04251},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@inproceedings{fehnunsvagEffectsUserFeatures2018,
	address = {Brussels, Belgium},
	title = {The {Effects} of {User} {Features} on {Twitter} {Hate} {Speech} {Detection}},
	url = {http://aclweb.org/anthology/W18-5110},
	doi = {10.18653/v1/W18-5110},
	abstract = {The paper investigates the potential effects user features have on hate speech classiﬁcation. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classiﬁer based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of classiﬁcation performance. While the incorporation of user features resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Proceedings of the 2nd {Workshop} on {Abusive} {Language} {Online} ({ALW2})},
	publisher = {Association for Computational Linguistics},
	author = {Fehn Unsvåg, Elise and Gambäck, Björn},
	year = {2018},
	pages = {75--85},
}

@inproceedings{kennedyTechnologySolutionsCombat2017,
	address = {Vancouver, BC, Canada},
	title = {Technology {Solutions} to {Combat} {Online} {Harassment}},
	url = {http://aclweb.org/anthology/W17-3011},
	doi = {10.18653/v1/W17-3011},
	abstract = {This work is part of a new initiative to use machine learning to identify online harassment in social media and comment streams. Online harassment goes underreported due to the reliance on humans to identify and report harassment, reporting that is further slowed by requirements to ﬁll out forms providing context. In addition, the time for moderators to respond and apply human judgment can take days, but response times in terms of minutes are needed in the online context. Though some of the major social media companies have been doing proprietary work in automating the detection of harassment, there are few tools available for use by the public. In addition, the amount of labeled online harassment data and availability of cross platform online harassment datasets is limited. We present the methodology used to create a harassment dataset and classiﬁer and the dataset used to help the system learn what harassment looks like.},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Proceedings of the {First} {Workshop} on {Abusive} {Language} {Online}},
	publisher = {Association for Computational Linguistics},
	author = {Kennedy, George and McCollough, Andrew and Dixon, Edward and Bastidas, Alexei and Ryan, John and Loo, Chris and Sahay, Saurav},
	year = {2017},
	pages = {73--77},
}

@article{fountaLargeScaleCrowdsourcing,
	title = {Large {Scale} {Crowdsourcing} and {Characterization} of {Twitter} {Abusive} {Behavior}},
	abstract = {In recent years online social networks have suffered an increase in sexism, racism, and other types of aggressive and cyberbullying behavior, often manifesting itself through offensive, abusive, or hateful language. Past scientiﬁc work focused on studying these forms of abusive activity in popular online social networks, such as Facebook and Twitter. Building on such work, we present an eight month study of the various forms of abusive behavior on Twitter, in a holistic fashion. Departing from past work, we examine a wide variety of labeling schemes, which cover different forms of abusive behavior. We propose an incremental and iterative methodology that leverages the power of crowdsourcing to annotate a large collection of tweets with a set of abuse-related labels. By applying our methodology and performing statistical analysis for label merging or elimination, we identify a reduced but robust set of labels to characterize abuse-related tweets. Finally, we offer a characterization of our annotated dataset of 80 thousand tweets, which we make publicly available for further scientiﬁc exploration.},
	language = {en},
	author = {Founta, Antigoni-Maria and Djouvas, Constantinos and Chatzakou, Despoina and Leontiadis, Ilias and Blackburn, Jeremy and Stringhini, Gianluca and Vakali, Athena and Sirivianos, Michael and Kourtellis, Nicolas},
	pages = {10},
}

@inproceedings{golbeckLargeLabeledCorpus2017,
	address = {Troy New York USA},
	title = {A {Large} {Labeled} {Corpus} for {Online} {Harassment} {Research}},
	isbn = {978-1-4503-4896-6},
	url = {https://dl.acm.org/doi/10.1145/3091478.3091509},
	doi = {10.1145/3091478.3091509},
	abstract = {A fundamental part of conducting cross-disciplinary web science research is having useful, high-quality datasets that provide value to studies across disciplines. In this paper, we introduce a large, handcoded corpus of online harassment data. A team of researchers collaboratively developed a codebook using grounded theory and labeled 35,000 tweets. Our resulting dataset has roughly 15\% positive harassment examples and 85\% negative examples. This data is useful for training machine learning models, identifying textual and linguistic features of online harassment, and for studying the nature of harassing comments and the culture of trolling.},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Proceedings of the 2017 {ACM} on {Web} {Science} {Conference}},
	publisher = {ACM},
	author = {Golbeck, Jennifer and Ashktorab, Zahra and Banjo, Rashad O. and Berlinger, Alexandra and Bhagwan, Siddharth and Buntain, Cody and Cheakalos, Paul and Geller, Alicia A. and Gergory, Quint and Gnanasekaran, Rajesh Kumar and Gunasekaran, Raja Rajan and Hoffman, Kelly M. and Hottle, Jenny and Jienjitlert, Vichita and Khare, Shivika and Lau, Ryan and Martindale, Marianna J. and Naik, Shalmali and Nixon, Heather L. and Ramachandran, Piyush and Rogers, Kristine M. and Rogers, Lisa and Sarin, Meghna Sardana and Shahane, Gaurav and Thanki, Jayanee and Vengataraman, Priyanka and Wan, Zijian and Wu, Derek Michael},
	month = jun,
	year = {2017},
	pages = {229--233},
}

@article{leeComparativeStudiesDetecting2018,
	title = {Comparative {Studies} of {Detecting} {Abusive} {Language} on {Twitter}},
	url = {http://arxiv.org/abs/1808.10245},
	abstract = {The context-dependent nature of online aggression makes annotating large collections of data extremely difﬁcult. Previously studied datasets in abusive language detection have been insufﬁcient in size to efﬁciently train deep learning models. Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released. However, this dataset has not been comprehensively studied to its potential. In this paper, we conduct the ﬁrst comparative study of various learning models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for improvements. Experimental results show that bidirectional GRU networks trained on word-level features, with Latent Topic Clustering modules, is the most accurate model scoring 0.805 F1.},
	language = {en},
	urldate = {2021-04-01},
	journal = {arXiv:1808.10245 [cs]},
	author = {Lee, Younghun and Yoon, Seunghyun and Jung, Kyomin},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.10245},
	keywords = {Computer Science - Computation and Language},
}

@article{kielaHatefulMemesChallenge2020,
	title = {The {Hateful} {Memes} {Challenge}: {Detecting} {Hate} {Speech} in {Multimodal} {Memes}},
	shorttitle = {The {Hateful} {Memes} {Challenge}},
	url = {http://arxiv.org/abs/2005.04790},
	abstract = {This work proposes a new challenge set for multimodal classiﬁcation, focusing on detecting hate speech in multimodal memes. It is constructed such that unimodal models struggle and only multimodal models can succeed: difﬁcult examples (“benign confounders”) are added to the dataset to make it hard to rely on unimodal signals. The task requires subtle reasoning, yet is straightforward to evaluate as a binary classiﬁcation problem. We provide baseline performance numbers for unimodal models, as well as for multimodal models with various degrees of sophistication. We ﬁnd that state-of-the-art methods perform poorly compared to humans (64.73\% vs. 84.7\% accuracy), illustrating the difﬁculty of the task and highlighting the challenge that this important problem poses to the community.},
	language = {en},
	urldate = {2021-04-01},
	journal = {arXiv:2005.04790 [cs]},
	author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.04790},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{benghabrit2015abstract,
	title = {Abstract accountability language: {Translation}, compliance and application},
	booktitle = {2015 asia-pacific software engineering conference ({APSEC})},
	author = {Benghabrit, Walid and Grall, Hervé and Royer, Jean-Claude and Sellami, Mohamed},
	year = {2015},
	note = {tex.organization: IEEE},
	pages = {214--221},
}

@inproceedings{baratiDevelopingGDPRCompliant2019,
	address = {Auckland New Zealand},
	title = {Developing {GDPR} {Compliant} {User} {Data} {Policies} for {Internet} of {Things}},
	isbn = {978-1-4503-6894-0},
	url = {https://dl.acm.org/doi/10.1145/3344341.3368812},
	doi = {10.1145/3344341.3368812},
	abstract = {With recent adoption of Internet of Things (IoT) technologies and their use in industry, user data privacy concerns remain a major preoccupation of regulation bodies. The European General Data Protection Regulation (GDPR) enables users to control their data and be informed about any devices involved in collecting and processing this data. The overall objective is to enable individuals to have full rights and control over their data assets and to be able to transfer their data without any unmitigated risk. Blockchains provide the benefits of a distributed ledger that can securely manage digital transactions – where the centralisation of data is eliminated. Blockchains have recently entered as an enabling technology into the IoT market, and used in a variety of different application areas. Blockchains enable the implementation of a more trusted system capable of processing operations between IoT services and sources of data. In smart buildings, for example, Blockchains support the formation of smart contracts as a means to give transactional capabilities to IoT devices, allowing users to keep data ownership and privacy using an immutable dataset. We describe how Blockchain technology can be used to develop an audit trail of data generated in IoT devices, enabling GDPR rules to be verified on such a trail. We describe how to translate a set of such rules into smart contracts to protect personal data in a transparent and automatic way.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 12th {IEEE}/{ACM} {International} {Conference} on {Utility} and {Cloud} {Computing}},
	publisher = {ACM},
	author = {Barati, Masoud and Petri, Ioan and Rana, Omer F.},
	month = dec,
	year = {2019},
	pages = {133--141},
}

@article{bastidasHarassmentDetectionBenchmark,
	title = {Harassment detection: a benchmark on the \#{HackHarassment} dataset},
	language = {en},
	author = {Bastidas, Alexei and Dixon, Edward and Loo, Chris and Ryan, John},
	pages = {3},
}

@article{rivest1978method,
	title = {A method for obtaining digital signatures and public-key cryptosystems},
	volume = {21},
	number = {2},
	journal = {Communications of the ACM},
	author = {Rivest, Ronald L and Shamir, Adi and Adleman, Leonard},
	year = {1978},
	note = {Publisher: ACM New York, NY, USA},
	pages = {120--126},
}

@inproceedings{dwork2008differential,
	title = {Differential privacy: {A} survey of results},
	booktitle = {International conference on theory and applications of models of computation},
	author = {Dwork, Cynthia},
	year = {2008},
	note = {tex.organization: Springer},
	pages = {1--19},
}

@inproceedings{fanEmpiricalEvaluationGDPR2020,
	address = {Coimbra, Portugal},
	title = {An {Empirical} {Evaluation} of {GDPR} {Compliance} {Violations} in {Android} {mHealth} {Apps}},
	isbn = {978-1-72819-870-5},
	url = {https://ieeexplore.ieee.org/document/9251060/},
	doi = {10.1109/ISSRE5003.2020.00032},
	abstract = {The purpose of the General Data Protection Regulation (GDPR) is to provide improved privacy protection. If an app controls personal data from users, it needs to be compliant with GDPR. However, GDPR lists general rules rather than exact step-by-step guidelines about how to develop an app that fulﬁlls the requirements. Therefore, there may exist GDPR compliance violations in existing apps, which would pose severe privacy threats to app users. In this paper, we take mobile health applications (mHealth apps) as a peephole to examine the status quo of GDPR compliance in Android apps. We ﬁrst propose an automated system, named HPDROID, to bridge the semantic gap between the general rules of GDPR and the app implementations by identifying the data practices declared in the app privacy policy and the data relevant behaviors in the app code. Then, based on HPDROID, we detect three kinds of GDPR compliance violations, including the incompleteness of privacy policy, the inconsistency of data collections, and the insecurity of data transmission. We perform an empirical evaluation of 796 mHealth apps. The results reveal that 189 (23.7\%) of them do not provide complete privacy policies. Moreover, 59 apps collect sensitive data through different measures, but 46 (77.9\%) of them contain at least one inconsistent collection behavior. Even worse, among the 59 apps, only 8 apps try to ensure the transmission security of collected data. However, all of them contain at least one encryption or SSL misuse. Our work exposes severe privacy issues to raise awareness of privacy protection for app users and developers.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {2020 {IEEE} 31st {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Fan, Ming and Yu, Le and Chen, Sen and Zhou, Hao and Luo, Xiapu and Li, Shuyue and Liu, Yang and Liu, Jun and Liu, Ting},
	month = oct,
	year = {2020},
	pages = {253--264},
}

@inproceedings{macgahanProvableEnforcementHIPAAcompliant2017,
	address = {Indianapolis Indiana USA},
	title = {Provable enforcement of {HIPAA}-compliant release of medical records using the history aware programming language},
	isbn = {978-1-4503-4702-0},
	url = {https://dl.acm.org/doi/10.1145/3078861.3084176},
	doi = {10.1145/3078861.3084176},
	abstract = {Dependence on reliable information systems to safeguard personally identi able information implies a need for privacy policies which guide the release and management of such information, whose mismanaged disclosure can be damaging to both the subject and the organization that releases it. Enforcing such policies requires a ention to detail and care, and thus any aid that a compiler can render may be of value. We present a demonstration of compiler enforcement of privacy policy by implementation of the History Aware Programming Language (HAPL) framework. is framework allows expression of arbitrary HAPL code for actors in an actor system to be used to back a web application. is code is then checked for compliance with privacy policies described in assume-guarantee form before being assembled into a functioning application. e framework is demonstrated by implementing ve use cases based on scenarios described in the Health Insurance Portability and Accountability Act (HIPAA), and the performance of the framework is tested.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 22nd {ACM} on {Symposium} on {Access} {Control} {Models} and {Technologies}},
	publisher = {ACM},
	author = {MacGahan, Thomas and Johnson, Claiborne and Rodriguez, Armando and von Ronne, Jeffery and Niu, Jianwei},
	month = jun,
	year = {2017},
	pages = {191--198},
}

@inproceedings{benthallSituatedInformationFlow2019,
	address = {Nashville, Tennessee},
	title = {Situated information flow theory},
	isbn = {978-1-4503-7147-6},
	url = {http://dl.acm.org/citation.cfm?doid=3314058.3314066},
	doi = {10.1145/3314058.3314066},
	abstract = {A key component of recent privacy rules is restriction on the flows of personal information or data based on information categories. This tendency conflicts with the fact that data’s meaning is not stable but depends on how it was formed and with what other information it is combined. These properties of information challenge naive intuitions that information ‘flows’ like a fluid, such as water or oil. Rather, we build on on Dretske, Pearl, and Nissenbaum to develop situated information flow theory (SIFT): a view of information flows as causal flows with nomic associations due to a larger context of causal relations. The semantics of situated information flow are precise within the statistical framework of Bayesian networks. We argue this understanding of information flow has three policy implications. (1) Restrictions on data transfers are more precise and enforceable than restrictions on information flow. (2) Information ‘categories’ or meanings must be defined relative to a particular class of observers and take into account their reasonable background information. (3) The semantics of data are ambiguous when there is uncertainty about causal structure, and this structure is learned from data aggregation. Hence, the information asymmetry between data aggregators and individual data subjects are one reason why data processors are ‘opaque’ and difficult to regulate.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 6th {Annual} {Symposium} on {Hot} {Topics} in the {Science} of {Security} - {HotSoS} '19},
	publisher = {ACM Press},
	author = {Benthall, Sebastian},
	year = {2019},
	pages = {1--10},
}

@article{akanfeDesignInclusiveFinancial2021,
	title = {Design of an {Inclusive} {Financial} {Privacy} {Index} ({INF}-{PIE}): {A} {Financial} {Privacy} and {Digital} {Financial} {Inclusion} {Perspective}},
	volume = {12},
	issn = {2158-656X, 2158-6578},
	shorttitle = {Design of an {Inclusive} {Financial} {Privacy} {Index} ({INF}-{PIE})},
	url = {https://dl.acm.org/doi/10.1145/3403949},
	doi = {10.1145/3403949},
	abstract = {Financial privacy is an important part of an individual's privacy, but efforts to enhance financial privacy have often not been given enough prominence by some countries when advancing financial inclusion. This impedes under-served communities from utilizing financial services. This article adopts a design science approach to create an INclusive Financial Privacy IndEx (INF-PIE) from the two perspectives of financial privacy and digital financial inclusion to help ensure financial services for a wide range of populations. This article first examines the privacy policies of Mobile Wallet and Remittance (MWR) apps (a digital financial solution), uses an analytics approach for extracting semi-structured information components; and based on text categorization and topic modeling, creates privacy policy compliance scores. In particular, it analyses the privacy policies using natural language processing techniques such as Term Frequency-Inverse Document Frequency (tf-idf) and Latent Dirichlet Allocation (LDA). This article then develops a digital financial inclusion score through a multivariate analysis of indexes extracted from the global findex dataset using Principal Component Analysis (PCA). Finally, the INF-PIE framework is established to analyze various countries and assess their financial privacy and digital financial inclusion practices. This framework can show how countries’ relative data privacy compliance and digital financial inclusion practices underscore their inclusive financial privacy.},
	language = {en},
	number = {1},
	urldate = {2021-03-19},
	journal = {ACM Transactions on Management Information Systems},
	author = {Akanfe, Oluwafemi and Valecha, Rohit and Rao, H. Raghav},
	month = mar,
	year = {2021},
	pages = {1--21},
}

@article{murrillModesConstitutionalInterpretation,
	title = {Modes of {Constitutional} {Interpretation}},
	abstract = {When exercising its power to review the constitutionality of governmental action, the Supreme Court has relied on certain “methods” or “modes” of interpretation—that is, ways of figuring out a particular meaning of a provision within the Constitution. This report broadly describes the most common modes of constitutional interpretation; discusses examples of Supreme Court decisions that demonstrate the application of these methods; and provides a general overview of the various arguments in support of, and in opposition to, the use of such methods of constitutional interpretation.},
	language = {en},
	author = {Murrill, Brandon J},
	pages = {28},
}

@inproceedings{deyoungExperiencesLogicalSpecification2010,
	address = {Chicago, Illinois, USA},
	title = {Experiences in the logical specification of the {HIPAA} and {GLBA} privacy laws},
	isbn = {978-1-4503-0096-4},
	url = {http://portal.acm.org/citation.cfm?doid=1866919.1866930},
	doi = {10.1145/1866919.1866930},
	abstract = {Despite the wide array of frameworks proposed for the formal speciﬁcation and analysis of privacy laws, there has been comparatively little work on expressing large fragments of actual privacy laws in these frameworks. We attempt to bridge this gap by giving complete logical formalizations of the transmission-related portions of the Health Insurance Portability and Accountability Act (HIPAA) and the Gramm-Leach-Bliley Act (GLBA). To this end, we develop the PrivacyLFP logic, whose features include support for disclosure purposes, real-time constructs, and self-reference via ﬁxed points. To illustrate these features and demonstrate PrivacyLFP’s utility, we present formalizations of a collection of clauses from these laws. Due to their size, our full formalizations of HIPAA and GLBA appear in a companion technical report. We discuss ambiguities in the laws that our formalizations revealed and sketch preliminary ideas for computer-assisted enforcement of such privacy policies.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 9th annual {ACM} workshop on {Privacy} in the electronic society - {WPES} '10},
	publisher = {ACM Press},
	author = {DeYoung, Henry and Garg, Deepak and Jia, Limin and Kaynar, Dilsun and Datta, Anupam},
	year = {2010},
	pages = {73},
}

@inproceedings{bennettGLOBALPrivacyProtection2018,
	address = {Singapore Singapore},
	title = {{GLOBAL} {Privacy} {Protection}: {Adequate} {Laws}, {Accountable} {Organizations} and/or {Data} {Localization}?},
	isbn = {978-1-4503-5966-5},
	shorttitle = {{GLOBAL} {Privacy} {Protection}},
	url = {https://dl.acm.org/doi/10.1145/3267305.3274149},
	doi = {10.1145/3267305.3274149},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2018 {ACM} {International} {Joint} {Conference} and 2018 {International} {Symposium} on {Pervasive} and {Ubiquitous} {Computing} and {Wearable} {Computers}},
	publisher = {ACM},
	author = {Bennett, Colin and Oduro-Marfo, Smith},
	month = oct,
	year = {2018},
	pages = {880--890},
}

@inproceedings{kangAlgorithmicAccountabilityPublic2020,
	address = {Barcelona Spain},
	title = {Algorithmic accountability in public administration: the {GDPR} paradox},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Algorithmic accountability in public administration},
	url = {https://dl.acm.org/doi/10.1145/3351095.3373153},
	doi = {10.1145/3351095.3373153},
	abstract = {The EU General Data Protection Regulation (“GDPR”) is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Kang, Sunny Seon},
	month = jan,
	year = {2020},
	pages = {32--32},
}

@inproceedings{biegaOperationalizingLegalPrinciple2020,
	address = {Virtual Event China},
	title = {Operationalizing the {Legal} {Principle} of {Data} {Minimization} for {Personalization}},
	isbn = {978-1-4503-8016-4},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401034},
	doi = {10.1145/3397271.3401034},
	abstract = {Article 5(1)(c) of the European Union’s General Data Protection Regulation (GDPR) requires that "personal data shall be [...] adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’)". To date, the legal and computational definitions of ‘purpose limitation’ and ‘data minimization’ remain largely unclear. In particular, the interpretation of these principles is an open issue for information access systems that optimize for user experience through personalization and do not strictly require personal data collection for the delivery of basic service. In this paper, we identify a lack of a homogeneous interpretation of the data minimization principle and explore two operational definitions applicable in the context of personalization. The focus of our empirical study in the domain of recommender systems is on providing foundational insights about the (i) feasibility of different data minimization definitions, (ii) robustness of different recommendation algorithms to minimization, and (iii) performance of different minimization strategies. We find that the performance decrease incurred by data minimization might not be substantial, but that it might disparately impact different users—a finding which has implications for the viability of different formal minimization definitions. Overall, our analysis uncovers the complexities of the data minimization problem in the context of personalization and maps the remaining computational and regulatory challenges.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Biega, Asia J. and Potash, Peter and Daumé, Hal and Diaz, Fernando and Finck, Michèle},
	month = jul,
	year = {2020},
	pages = {399--408},
}

@article{kaminskiRecentRenaissancePrivacy2020,
	title = {A recent renaissance in privacy law},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3411049},
	doi = {10.1145/3411049},
	abstract = {Considering the recent increased attention to privacy law issues amid the typically slow pace of legal change.},
	language = {en},
	number = {9},
	urldate = {2021-03-19},
	journal = {Communications of the ACM},
	author = {Kaminski, Margot},
	month = aug,
	year = {2020},
	pages = {24--27},
}

@inproceedings{anishEnhancedAccountabilityComplying2019,
	address = {Montreal, QC, Canada},
	title = {Towards {Enhanced} {Accountability} in {Complying} with {Healthcare} {Regulations}},
	isbn = {978-1-72812-251-9},
	url = {https://ieeexplore.ieee.org/document/8823886/},
	doi = {10.1109/SEH.2019.00012},
	abstract = {The healthcare ecosystem is highly complex. It is composed of multiple stakeholders with intersecting interests. The healthcare regulations such as Health Insurance Portability and Accountability Act, much like the systems they protect, are complex and often difficult to interpret. Regulations contain obligations that must be fulfilled by healthcare systems that form the backbone of modern healthcare. In this paper, we present a model for extracting and deconstructing obligations. The Obligation Model allows for capturing the essence of obligations in terms of their attributes such as Responsible entity, Trigger, Action, Deadline and Reference. We augment the extracted obligations with auxiliary information present in regulation documents and provide an ownership based rendering of the deconstructed obligation in an HTML format. This helps in establishing an explicit ownership of obligations and contributes towards enhancing accountability of stakeholders towards fulfilling the obligations. The rendering will be useful for building compliant healthcare systems by making the legal text more comprehensible for system designers.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {2019 {IEEE}/{ACM} 1st {International} {Workshop} on {Software} {Engineering} for {Healthcare} ({SEH})},
	publisher = {IEEE},
	author = {Anish, Preethu Rose and Joshi, Vivek and Sainani, Abhishek and Ghaisas, Smita},
	month = may,
	year = {2019},
	pages = {25--28},
}

@article{greengardWeighingImpactGDPR2018,
	title = {Weighing the impact of {GDPR}},
	volume = {61},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3276744},
	doi = {10.1145/3276744},
	abstract = {The EU data regulation will affect computer, Internet, and technology usage within and outside the EU; how it will play out remains to be seen.},
	language = {en},
	number = {11},
	urldate = {2021-03-19},
	journal = {Communications of the ACM},
	author = {Greengard, Samuel},
	month = oct,
	year = {2018},
	pages = {16--18},
}

@article{SigmaProblemSearch2021,
	title = {The 2 {Sigma} {Problem}: {The} {Search} for {Methods} of {Group} {Instruction} as {Effective} as {One}-to-{One} {Tutoring}},
	language = {en},
	year = {2021},
	pages = {14},
}

@article{lockwood2006someone,
	title = {“{Someone} like me can be successful”: {Do} college students need same-gender role models?},
	volume = {30},
	number = {1},
	journal = {Psychology of women quarterly},
	author = {Lockwood, Penelope},
	year = {2006},
	note = {Publisher: SAGE Publications Sage CA: Los Angeles, CA},
	pages = {36--46},
}

@article{dimitrov_intercultural_nodate,
	title = {Intercultural {Teaching} {Competence} in the {Disciplines}: {Teaching} {Strategies} for {Intercultural} {Learning}},
	abstract = {As universities continue to internationalize their curricula and recruit a growing number of international students, instructors facilitate learning in increasingly diverse classrooms. This chapter explores the application of Intercultural Teaching Competence (ITC) by faculty members across the disciplines at a large Canadian research university. Based on focus group interviews with instructors in eighteen disciplines, it provides varied and concrete examples of how instructors mobilize intercultural teaching competence to navigate diverse classrooms, promote perspective-taking and global learning goals among students, practice culturally relevant teaching, and validate different ways of knowing and communicating among students through assessment practices. Placing disciplines at the centre of the discussion in this way elucidates the extent to which ITC may be adapted to fit the contours of the academic field and allows readers to explore best practices for facilitating the development of intercultural competence among students in their disciplines. Finally, the implications of disciplinary differences in ITC are discussed for faculty development and curriculum support.},
	language = {en},
	author = {Dimitrov, Nanda and Haque, Aisha},
	pages = {23},
}

@article{gondra_intercultural_2018,
	title = {Intercultural {Knowledge} {Development} {During} {Short}-{Term} {Study} {Abroad} in the {Basque} {Country}: {A} {Cultural} and {Linguistic} {Minority} {Context}},
	volume = {30},
	issn = {2380-8144, 1085-4568},
	shorttitle = {Intercultural {Knowledge} {Development} {During} {Short}-{Term} {Study} {Abroad} in the {Basque} {Country}},
	url = {http://frontiersjournal.org/index.php/Frontiers/article/view/427},
	doi = {10.36366/frontiers.v30i3.427},
	abstract = {This investigation examined intercultural knowledge development in a short-term study abroad program in a cultural and linguistic minority context (Basque Country, Spain). A pre- and postprogram quantitative and qualitative design was used with 26 participants. The quantitative, surveybased results demonstrated an increase in intercultural knowledge over the study’s five-week duration. Qualitative analysis of interview data indicated that students’ intercultural knowledge aligned with Lussier’s (2007) description of knowledge about “small c” culture and included knowledge of social groups—a distinct finding from prior research. Students’ knowledge changed over the period abroad, indicating adjustment to City Life and Time and Schedule norms. Additionally, knowledge growth was greatest with respect to subthemes strongly linked to the minority context (e.g. Basque ethnicity, culture, language). This study offers detailed information about intercultural knowledge development during short-term study abroad experiences and demonstrates that minority context programs encourage development of intercultural knowledge about “small c” culture and social groups.},
	language = {en},
	number = {3},
	urldate = {2021-08-16},
	journal = {Frontiers: The Interdisciplinary Journal of Study Abroad},
	author = {Gondra, Ager and Czerwionka, Lori},
	month = nov,
	year = {2018},
	pages = {119--146},
}

@article{lehto_intercultural_2014,
	title = {Intercultural {Interactions} {Outside} the {Classroom}: {Narratives} on a {US} {Campus}},
	volume = {55},
	issn = {1543-3382},
	shorttitle = {Intercultural {Interactions} {Outside} the {Classroom}},
	url = {http://muse.jhu.edu/content/crossref/journals/journal_of_college_student_development/v055/55.8.lehto.html},
	doi = {10.1353/csd.2014.0083},
	language = {en},
	number = {8},
	urldate = {2021-08-16},
	journal = {Journal of College Student Development},
	author = {Lehto, Xinran Y. and Cai, Liping A. and Fu, Xiaoxiao and Chen, Yi},
	year = {2014},
	pages = {837--853},
}

@article{young-jones_autonomy-supportive_2021,
	title = {Autonomy-supportive language in the syllabus: supporting students from the first day},
	volume = {26},
	issn = {1356-2517, 1470-1294},
	shorttitle = {Autonomy-supportive language in the syllabus},
	url = {https://www.tandfonline.com/doi/full/10.1080/13562517.2019.1661375},
	doi = {10.1080/13562517.2019.1661375},
	abstract = {An autonomy supportive classroom enhances the learning climate and improves academic motivation. Alternatively, a controlling classroom environment constricts the learning climate and hinders academic motivation. The current study evaluated whether autonomy supportive or controlling language presented in a class syllabus inﬂuenced students’ perceptions of a college course. Students were randomly assigned to read a syllabus written with either autonomy supportive or controlling language. After reading, participants rated their perceptions of the learning climate, intrinsic motivation, satisfaction of their basic psychological needs, and intentions to take the class. Analyses revealed that students who viewed the autonomy-supportive syllabus had a more overall positive perception of the course compared to students who read the controlling syllabus. The ﬁndings suggest language within a course syllabus can inﬂuence student’s early perceptions of and intentions toward taking a course, but not their reported level of intrinsic motivation. Implications for instructors and future directions are discussed.},
	language = {en},
	number = {4},
	urldate = {2021-08-10},
	journal = {Teaching in Higher Education},
	author = {Young-Jones, Adena and Levesque, Chantal and Fursa, Sophie and McCain, Jason},
	month = may,
	year = {2021},
	pages = {541--556},
}

@article{cushner_developing_2015,
	title = {Developing intercultural competence through overseas student teaching: checking our assumptions},
	volume = {26},
	issn = {1467-5986, 1469-8439},
	shorttitle = {Developing intercultural competence through overseas student teaching},
	url = {http://www.tandfonline.com/doi/full/10.1080/14675986.2015.1040326},
	doi = {10.1080/14675986.2015.1040326},
	language = {en},
	number = {3},
	urldate = {2021-08-16},
	journal = {Intercultural Education},
	author = {Cushner, Kenneth and Chang, Shu-Ching},
	month = may,
	year = {2015},
	pages = {165--178},
}

@inproceedings{raduDatasetNonFunctionalBugs2019,
	address = {Montreal, QC, Canada},
	title = {A {Dataset} of {Non}-{Functional} {Bugs}},
	isbn = {978-1-72813-412-3},
	url = {https://ieeexplore.ieee.org/document/8816810/},
	doi = {10.1109/MSR.2019.00066},
	abstract = {While several researchers have published bug data sets in the past, there has been less focus on bugs related to nonfunctional requirements. Non-functional requirements describe the quality attributes of a program. In this work, we introduce NFBugs, a data set of 133 non-functional bug ﬁxes collected from 65 open-source projects written in Java and Python. NFBugs can be used to support code recommender systems focusing on nonfunctional properties.},
	language = {en},
	urldate = {2021-03-29},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Radu, Aida and Nadi, Sarah},
	month = may,
	year = {2019},
	pages = {399--403},
}

@article{brooks_no_1986,
	title = {No {Silver} {Bullet} – {Essence} and {Accident} in {Software} {Engineering}},
	language = {en},
	author = {Brooks, Frederick},
	year = {1986},
	pages = {16},
}

@article{penzenstadlerSafetySecurityNow2014,
	title = {Safety, {Security}, {Now} {Sustainability}: {The} {Nonfunctional} {Requirement} for the 21st {Century}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Safety, {Security}, {Now} {Sustainability}},
	url = {https://ieeexplore.ieee.org/document/6728940/},
	doi = {10.1109/MS.2014.22},
	language = {en},
	number = {3},
	urldate = {2021-03-24},
	journal = {IEEE Software},
	author = {Penzenstadler, Birgit and Raturi, Ankita and Richardson, Debra and Tomlinson, Bill},
	month = may,
	year = {2014},
	pages = {40--47},
}

@inproceedings{veras_errors_2010,
	address = {San Jose, CA, USA},
	title = {Errors on {Space} {Software} {Requirements}: {A} {Field} {Study} and {Application} {Scenarios}},
	isbn = {978-1-4244-9056-1},
	shorttitle = {Errors on {Space} {Software} {Requirements}},
	url = {http://ieeexplore.ieee.org/document/5635117/},
	doi = {10.1109/ISSRE.2010.30},
	abstract = {This paper presents a field study on real errors found in space software requirements documents. The goal is to understand and characterize the most frequent types of requirement problems in this critical application domain. To classify the software requirement errors analyzed we initially used a well-known existing taxonomy that was later extended in order to allow a more thorough analysis. The results of the study show a high rate of requirement errors (9.5 errors per each 100 requirements), which is surprising if we consider that the focus of the work is critical embedded software. Besides the characterization of the most frequent types of errors, the paper also proposes a set of operators that define how to inject realistic errors in requirement documents. This may be used in several scenarios, including: evaluating and training reviewers, estimating the number of requirement errors in real specifications, defining checklists for quick requirement verification, and defining benchmarks for requirements specifications.},
	language = {en},
	urldate = {2021-06-28},
	booktitle = {2010 {IEEE} 21st {International} {Symposium} on {Software} {Reliability} {Engineering}},
	publisher = {IEEE},
	author = {Veras, Paulo C. and Villani, Emilia and Ambrosio, Ana Maria and Silva, Nuno and Vieira, Marco and Madeira, Henrique},
	month = nov,
	year = {2010},
	pages = {61--70},
}

@inproceedings{lutz_analyzing_1992,
	address = {San Diego, CA, USA},
	title = {Analyzing software requirements errors in safety-critical, embedded systems},
	isbn = {978-0-8186-3120-7},
	url = {http://ieeexplore.ieee.org/document/324825/},
	doi = {10.1109/ISRE.1993.324825},
	abstract = {This paper analyzes the root causes of safety-related software errors in safety-critical, embedded systems. The results show that software errors identified as potentially hazardous t o the s y s t e m tend t o be produced by different error mechanisms than non-safetyrelated software errors. Safety-related software errors are shown to arise most commonly from (1) discrepancies between the documented requirements specifications and the requirements needed for correct functioning of the s y s t e m and (2) misunderstandings of the soflware's interface with the rest of ihe system. The paper uses these results t o identify methods by which requirements errors can be prevented. The goal is t o reduce safety-related software errors and io enhance the safety of complex, embedded systems.},
	language = {en},
	urldate = {2021-06-28},
	booktitle = {[1993] {Proceedings} of the {IEEE} {International} {Symposium} on {Requirements} {Engineering}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Lutz, R.R.},
	year = {1992},
	pages = {126--133},
}

@article{biAccessibilitySoftwarePractice2021,
	title = {Accessibility in {Software} {Practice}: {A} {Practitioner}'s {Perspective}},
	shorttitle = {Accessibility in {Software} {Practice}},
	url = {http://arxiv.org/abs/2103.08778},
	abstract = {Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this paper, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitioners' viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct v.s. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We also propose some remedies to resolve the gaps and to highlight key future research directions.},
	language = {en},
	urldate = {2021-03-29},
	journal = {arXiv:2103.08778 [cs]},
	author = {Bi, Tingting and Xia, Xin and Lo, David and Grundy, John and Zimmermann, Thomas and Ford, Denae},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.08778},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{bird_does_2009,
	address = {Vancouver, BC, Canada},
	title = {Does distributed development affect software quality? {An} empirical case study of {Windows} {Vista}},
	isbn = {978-1-4244-3453-4},
	shorttitle = {Does distributed development affect software quality?},
	url = {http://ieeexplore.ieee.org/document/5070550/},
	doi = {10.1109/ICSE.2009.5070550},
	abstract = {It is widely believed that distributed software development is riskier and more challenging than collocated development. Prior literature on distributed development in software engineering and other ﬁelds discuss various challenges, including cultural barriers, expertise transfer difﬁculties, and communication and coordination overhead. We evaluate this conventional belief by examining the overall development of Windows Vista and comparing the postrelease failures of components that were developed in a distributed fashion with those that were developed by collocated teams. We found a negligible difference in failures. This difference becomes even less signiﬁcant when controlling for the number of developers working on a binary. We also examine component characteristics such as code churn, complexity, dependency information, and test code coverage and ﬁnd very little difference between distributed and collocated components to investigate if less complex components are more distributed. Further, we examine the software process and phenomena that occurred during the Vista development cycle and present ways in which the development process utilized may be insensitive to geography by mitigating the difﬁculties introduced in prior work in this area.},
	language = {en},
	urldate = {2021-06-21},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Bird, Christian and Nagappan, Nachiappan and Devanbu, Premkumar and Gall, Harald and Murphy, Brendan},
	year = {2009},
	pages = {518--528},
}

@inproceedings{mcnamara2018does,
	title = {Does {ACM}’s code of ethics change ethical decision making in software development?},
	booktitle = {Proceedings of the 2018 26th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering},
	author = {McNamara, Andrew and Smith, Justin and Murphy-Hill, Emerson},
	year = {2018},
	pages = {729--733},
}

@inproceedings{parkSystematicTestingReactive2015,
	address = {Florence, Italy},
	title = {Systematic {Testing} of {Reactive} {Software} with {Non}-{Deterministic} {Events}: {A} {Case} {Study} on {LG} {Electric} {Oven}},
	isbn = {978-1-4799-1934-5},
	shorttitle = {Systematic {Testing} of {Reactive} {Software} with {Non}-{Deterministic} {Events}},
	url = {http://ieeexplore.ieee.org/document/7202947/},
	doi = {10.1109/ICSE.2015.132},
	abstract = {Most home appliance devices such as electric ovens are reactive systems which repeat receiving a user input/event through an event handler, updating their internal state based on the input, and generating outputs. A challenge to test a reactive program is to check if the program correctly reacts to various non-deterministic sequence of events because an unexpected sequence of events may make the system fail due to the race conditions between the main loop and asynchronous event handlers. Thus, it is important to systematically generate/test various sequences of events by controlling the order of events and relative timing of event occurrences with respect to the main loop execution. In this paper, we report our industrial experience to solve the aforementioned problem by developing a systematic event generation framework based on concolic testing technique. We have applied the framework to a LG electric oven and detected several critical bugs including one that makes the oven ignore user inputs due to the illegal state transition.},
	language = {en},
	urldate = {2021-04-05},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Park, Yongbae and Hong, Shin and Kim, Moonzoo and Lee, Dongju and Cho, Junhee},
	month = may,
	year = {2015},
	keywords = {IoT},
	pages = {29--38},
}

@article{chapman_what_2017,
	title = {What can agile methods bring to high-integrity software development?},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3133233},
	doi = {10.1145/3133233},
	abstract = {Considering the issues and opportunities raised by Agile practices in the development of high-integrity software.},
	language = {en},
	number = {10},
	urldate = {2021-06-25},
	journal = {Communications of the ACM},
	author = {Chapman, Roderick and White, Neil and Woodcock, Jim},
	month = sep,
	year = {2017},
	pages = {38--41},
}

@inproceedings{muenchWhatYouCorrupt2018,
	address = {San Diego, CA},
	title = {What {You} {Corrupt} {Is} {Not} {What} {You} {Crash}: {Challenges} in {Fuzzing} {Embedded} {Devices}},
	isbn = {978-1-891562-49-5},
	shorttitle = {What {You} {Corrupt} {Is} {Not} {What} {You} {Crash}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-4_Muench_paper.pdf},
	doi = {10.14722/ndss.2018.23166},
	abstract = {As networked embedded systems are becoming more ubiquitous, their security is becoming critical to our daily life. While manual or automated large scale analysis of those systems regularly uncover new vulnerabilities, the way those systems are analyzed follows often the same approaches used on desktop systems. More speciﬁcally, traditional testing approaches relies on observable crashes of a program, and binary instrumentation techniques are used to improve the detection of those faulty states.},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Muench, Marius and Stijohann, Jan and Kargl, Frank and Francillon, Aurelien and Balzarotti, Davide},
	year = {2018},
	keywords = {IoT},
}

@article{eismann_serverless_2021,
	title = {Serverless {Applications}: {Why}, {When}, and {How}?},
	volume = {38},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Serverless {Applications}},
	url = {https://ieeexplore.ieee.org/document/9190031/},
	doi = {10.1109/MS.2020.3023302},
	language = {en},
	number = {1},
	urldate = {2021-06-02},
	journal = {IEEE Software},
	author = {Eismann, Simon and Scheuner, Joel and van Eyk, Erwin and Schwinger, Maximilian and Grohmann, Johannes and Herbst, Nikolas and Abad, Cristina L. and Iosup, Alexandru},
	month = jan,
	year = {2021},
	pages = {32--39},
}

@article{ferrari_formal_2021,
	title = {Formal {Methods} in {Railways}: a {Systematic} {Mapping} {Study}},
	shorttitle = {Formal {Methods} in {Railways}},
	url = {http://arxiv.org/abs/2107.05413},
	abstract = {Formal methods are mathematically-based techniques for the rigorous development of software-intensive systems. The railway signaling domain is a field in which formal methods have traditionally been applied, with several success stories. This article reports on a mapping study that surveys the landscape of research on applications of formal methods to the development of railway systems. Our main results are as follows: (i) we identify a total of 328 primary studies relevant to our scope published between 1989 and 2020, of which 44\% published during the last 5 years and 24\% involving industry; (ii) the majority of studies are evaluated through Examples (41\%) and Experience Reports (38\%), while full-fledged Case Studies are limited (1.5\%); (iii) Model checking is the most commonly adopted technique (47\%), followed by simulation (27\%) and theorem proving (19.5\%); (iv) the dominant languages are UML (18\%) and B (15\%), while frequently used tools are ProB (9\%), NuSMV (8\%) and UPPAAL (7\%); however, a diverse landscape of languages and tools is employed; (v) the majority of systems are interlocking products (40\%), followed by models of high-level control logic (27\%); (vi) most of the studies focus on the Architecture (66\%) and Detailed Design (45\%) development phases. Based on these findings, we highlight current research gaps and expected actions. In particular, the need to focus on more empirically sound research methods, such as Case Studies and Controlled Experiments, and to lower the degree of abstraction, by applying formal methods and tools to development phases that are closer to software development. Our study contributes with an empirically based perspective on the future of research and practice in formal methods applications for railways.},
	language = {en},
	urldate = {2021-07-15},
	journal = {arXiv:2107.05413 [cs]},
	author = {Ferrari, Alessio and ter Beek, Maurice H.},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.05413},
	keywords = {Computer Science - Software Engineering, Exemplar},
}

@inproceedings{koziolekOpenPnPPlugandProduceArchitecture2019,
	address = {Montreal, QC, Canada},
	title = {{OpenPnP}: {A} {Plug}-and-{Produce} {Architecture} for the {Industrial} {Internet} of {Things}},
	isbn = {978-1-72811-760-7},
	shorttitle = {{OpenPnP}},
	url = {https://ieeexplore.ieee.org/document/8804425/},
	doi = {10.1109/ICSE-SEIP.2019.00022},
	abstract = {Industrial control systems are complex, softwareintensive systems that manage mission-critical production processes. Commissioning such systems requires installing, conﬁguring, and integrating thousands of sensors, actuators, and controllers and is still a largely manual and costly process. Therefore, practitioners and researchers have been working on “plug and produce” approaches that automate commissioning for more than 15 years, but have often focused on network discovery and proprietary technologies. We introduce the vendor-neutral OpenPnP reference architecture, which can largely automate the conﬁguration and integration tasks for commissioning. Using an example implementation, we demonstrate that OpenPnP can reduce the conﬁguration and integration effort up to 90 percent and scales up to tens of thousands of communicated signals per second for large Industrial Internet-of-Things (IIoT) systems. OpenPnP can serve as a template for practitioners implementing IIoT applications throughout the automation industry and streamline commissioning processes in many thousands of control system installations.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Koziolek, Heiko and Burger, Andreas and Platenius-Mohr, Marie and Ruckert, Julius and Stomberg, Gosta},
	month = may,
	year = {2019},
	pages = {131--140},
}

@article{harris2008good,
	title = {The good engineer: {Giving} virtue its due in engineering ethics},
	volume = {14},
	number = {2},
	journal = {Science and Engineering Ethics},
	author = {Harris, Charles E},
	year = {2008},
	note = {Publisher: Springer},
	pages = {153--164},
}

@phdthesis{brown1998authenticity,
	title = {Authenticity in learning, teaching and assessment: {Perspectives} from a course in professional ethics for software engineers},
	school = {University of Sheffield, Division of Education},
	author = {Brown, Guy Jason},
	year = {1998},
}

@article{gotterbarn1997software,
	title = {Software engineering code of ethics},
	volume = {40},
	number = {11},
	journal = {Communications of the ACM},
	author = {Gotterbarn, Don and Miller, Keith and Rogerson, Simon},
	year = {1997},
	note = {Publisher: ACM New York, NY, USA},
	pages = {110--118},
}

@article{wanHowDoesMachine2020,
	title = {How does {Machine} {Learning} {Change} {Software} {Development} {Practices}?},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/8812912/},
	doi = {10.1109/TSE.2019.2937083},
	abstract = {Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit signiﬁcant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers signiﬁcant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our ﬁndings, we highlight future research directions and provide recommendations for practitioners.},
	language = {en},
	urldate = {2021-05-11},
	journal = {IEEE Transactions on Software Engineering},
	author = {Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.},
	year = {2020},
	pages = {1--1},
}

@inproceedings{li_evolving_2021,
	address = {Madrid, Spain},
	title = {Evolving {Software} to be {ML}-{Driven} {Utilizing} {Real}-{World} {A}/{B} {Testing}: {Experiences}, {Insights}, {Challenges}},
	isbn = {978-1-66543-869-8},
	shorttitle = {Evolving {Software} to be {ML}-{Driven} {Utilizing} {Real}-{World} {A}/{B} {Testing}},
	url = {https://ieeexplore.ieee.org/document/9402122/},
	doi = {10.1109/ICSE-SEIP52600.2021.00026},
	abstract = {ML-driven software is heralded as the next major advancement in software engineering; existing software today can benefit from being evolved to be ML-driven. In this paper, we contribute practical knowledge about evolving software to be ML-driven, utilizing real-world A/B testing. We draw on experiences evolving two software features from the Windows operating system to be ML-driven, with more than ten realworld A/B tests on millions of PCs over more than two years. We discuss practical reasons for using A/B testing to engineer ML-driven software, insights for success, as well as on-going realworld challenges. This knowledge may help practitioners, as well as help direct future research and innovations.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Li, Paul Luo and Chai, Xiaoyu and Campbell, Frederick and Liao, Jilong and Abburu, Neeraja and Kang, Minsuk and Niculescu, Irina and Brake, Greg and Patil, Siddharth and Dooley, James and Paddock, Brandon},
	month = may,
	year = {2021},
	pages = {170--179},
}

@article{liskovPROGRAMMINGABSTRACTDATA,
	title = {{PROGRAMMING} {WITH} {ABSTRACT} {DATA} {TYPES}},
	abstract = {The motivation behind the work in very-high-level languages is to ease the programming task by providing the programmer with a language containing primitives or abstractions suitable to his problem area. The programmer is then able to spend his effort in the right place; he concentrates on solving his problem, and the resulting program will be more reliable as a result. Clearly, this is a worthwhile goal.},
	language = {en},
	author = {Liskov, Barbara and Zilles, Stephen},
	pages = {10},
}

@article{gulwaniProgramSynthesis2017,
	title = {Program {Synthesis}},
	volume = {4},
	issn = {2325-1107, 2325-1131},
	url = {http://www.nowpublishers.com/article/Details/PGL-010},
	doi = {10.1561/2500000010},
	language = {en},
	number = {1-2},
	urldate = {2021-04-26},
	journal = {Foundations and Trends® in Programming Languages},
	author = {Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
	year = {2017},
	pages = {1--119},
}

@inproceedings{zhangEndtoEndAutomaticCloud2019,
	address = {Amsterdam Netherlands},
	title = {An {End}-to-{End} {Automatic} {Cloud} {Database} {Tuning} {System} {Using} {Deep} {Reinforcement} {Learning}},
	isbn = {978-1-4503-5643-5},
	url = {https://dl.acm.org/doi/10.1145/3299869.3300085},
	doi = {10.1145/3299869.3300085},
	language = {en},
	urldate = {2021-04-18},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Zhang, Ji and Liu, Yu and Zhou, Ke and Li, Guoliang and Xiao, Zhili and Cheng, Bin and Xing, Jiashu and Wang, Yangtao and Cheng, Tianheng and Liu, Li and Ran, Minwei and Li, Zekang},
	month = jun,
	year = {2019},
	pages = {415--432},
}

@inproceedings{molinaTrainingBinaryClassifiers2019,
	address = {Montreal, QC, Canada},
	title = {Training {Binary} {Classifiers} as {Data} {Structure} {Invariants}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8811951/},
	doi = {10.1109/ICSE.2019.00084},
	abstract = {We present a technique to distinguish valid from invalid data structure objects. The technique is based on building an artiﬁcial neural network, more precisely a binary classiﬁer, and training it to identify valid and invalid instances of a data structure. The obtained classiﬁer can then be used in place of the data structure’s invariant, in order to attempt to identify (in)correct behaviors in programs manipulating the structure. In order to produce the valid objects to train the network, an assumed-correct set of object building routines is randomly executed. Invalid instances are produced by generating values for object ﬁelds that “break” the collected valid values, i.e., that assign values to object ﬁelds that have not been observed as feasible in the assumed-correct executions that led to the collected valid instances. We experimentally assess this approach, over a benchmark of data structures. We show that this learning technique produces classiﬁers that achieve signiﬁcantly better accuracy in classifying valid/invalid objects compared to a technique for dynamic invariant detection, and leads to improved bug ﬁnding.},
	language = {en},
	urldate = {2021-04-18},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Molina, Facundo and Degiovanni, Renzo and Ponzio, Pablo and Regis, German and Aguirre, Nazareno and Frias, Marcelo},
	month = may,
	year = {2019},
	pages = {759--770},
}

@article{fraser_no_2008,
	title = {No {Silver} {Bullet}: {Software} {Engineering} {Reloaded}},
	volume = {25},
	issn = {0740-7459},
	shorttitle = {No {Silver} {Bullet}},
	url = {http://ieeexplore.ieee.org/document/4420077/},
	doi = {10.1109/MS.2008.14},
	language = {en},
	number = {1},
	urldate = {2021-07-07},
	journal = {IEEE Software},
	author = {Fraser, Steven and Mancl, Dennis},
	month = jan,
	year = {2008},
	pages = {91--94},
}

@article{chapman_correctness_nodate,
	title = {Correctness by {Construction}: {A} {Manifesto} for {High} {Integrity} {Software}},
	abstract = {High integrity software systems are often so large that conventional development processes cannot get anywhere near achieving tolerable defect rates. This paper presents Correctness by Construction (CbyC)—an approach that has delivered very low defect rate software costeffectively. We describe the main principles of CbyC and the results achieved to date. We also touch on some of the barriers that we have encountered in trying to field CbyC within our own and other organisations.},
	language = {en},
	author = {Chapman, Roderick},
	pages = {4},
}

@article{luqi_formal_1997,
	title = {Formal methods: promises and problems},
	volume = {14},
	issn = {07407459},
	shorttitle = {Formal methods},
	url = {http://ieeexplore.ieee.org/document/566430/},
	doi = {10.1109/52.566430},
	language = {en},
	number = {1},
	urldate = {2021-06-25},
	journal = {IEEE Software},
	author = {{Luqi} and Goguen, J.A.},
	month = feb,
	year = {1997},
	pages = {73--85},
}

@inproceedings{allamanisMiningIdiomsSource2014,
	address = {Hong Kong, China},
	title = {Mining idioms from source code},
	isbn = {978-1-4503-3056-5},
	url = {http://dl.acm.org/citation.cfm?doid=2635868.2635901},
	doi = {10.1145/2635868.2635901},
	language = {en},
	urldate = {2021-04-18},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering} - {FSE} 2014},
	publisher = {ACM Press},
	author = {Allamanis, Miltiadis and Sutton, Charles},
	year = {2014},
	pages = {472--483},
}

@inproceedings{olney2016part,
	title = {Part of speech tagging {Java} method names},
	booktitle = {2016 {IEEE} international conference on software maintenance and evolution ({ICSME})},
	author = {Olney, Wyatt and Hill, Emily and Thurber, Chris and Lemma, Bezalem},
	year = {2016},
	note = {tex.organization: IEEE},
	pages = {483--487},
}

@article{parnas_really_2010,
	title = {Really rethinking 'formal methods'},
	volume = {43},
	number = {1},
	journal = {Computer},
	author = {Parnas, David Lorge},
	year = {2010},
	note = {Publisher: Institute of Electrical and Electronics Engineers, Inc., 3 Park Avenue, 17 …},
	pages = {28--34},
}

@article{abrial2009faultless,
	title = {Faultless systems: {Yes} we can!},
	volume = {42},
	number = {9},
	journal = {Computer},
	author = {Abrial, Jean-Raymond},
	year = {2009},
	note = {Publisher: IEEE},
	pages = {30--36},
}

@incollection{sommerville_formal_nodate,
	title = {Formal {Specification}},
	url = {https://iansommerville.com/software-engineering-book/downloads/},
	urldate = {2021-06-25},
	booktitle = {Software {Engineering}},
	author = {Sommerville, Ian},
}

@article{white_formal_2017,
	title = {Formal verification: will the seedling ever flower?},
	volume = {375},
	issn = {1364-503X, 1471-2962},
	shorttitle = {Formal verification},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0402},
	doi = {10.1098/rsta.2015.0402},
	abstract = {In one sense, formal specification and verification have been highly successful: techniques have been developed in pioneering academic research, transferred to software companies through training and partnerships, and successfully deployed in systems with national significance. Altran UK has been in the vanguard of this movement. This paper summarizes some of our key deployments of formal techniques over the past 20 years, including both security- and safety-critical systems. The impact of formal techniques, however, remains within an industrial niche, and while government and suppliers across industry search for solutions to the problems of poor-quality software, the wider software industry remains resistant to adoption of this proven solution. We conclude by reflecting on some of the challenges we face as a community in ensuring that formal techniques achieve their true potential impact on society.
            This article is part of the themed issue ‘Verified trustworthy software systems’.},
	language = {en},
	number = {2104},
	urldate = {2021-06-25},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {White, Neil and Matthews, Stuart and Chapman, Roderick},
	month = oct,
	year = {2017},
	pages = {20150402},
}

@article{woodcock_formal_nodate,
	title = {Formal methods: {Practice} and experience},
	volume = {41},
	language = {en},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Woodcock, Jim and Larsen, Peter Gorm and Bicarregui, Juan and Fitzgerald, John},
	pages = {36},
}

@article{wing_specifiers_1990,
	title = {A specifier's introduction to formal methods},
	volume = {23},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/58215/},
	doi = {10.1109/2.58215},
	language = {en},
	number = {9},
	urldate = {2021-06-28},
	journal = {Computer},
	author = {Wing, J.M.},
	month = sep,
	year = {1990},
	pages = {8--22},
}

@article{avizienis_basic_2004,
	title = {Basic concepts and taxonomy of dependable and secure computing},
	volume = {1},
	abstract = {This paper gives the main definitions relating to dependability, a generic concept including as special case such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.},
	language = {en},
	number = {1},
	journal = {IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING},
	author = {Avizienis, A},
	year = {2004},
	pages = {23},
}

@article{jhala_software_nodate,
	title = {Software model checking},
	volume = {41},
	language = {en},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Jhala, Ranjit and Majumdar, Rupak},
	pages = {54},
}

@article{holzmann_mars_2014,
	title = {Mars code},
	volume = {57},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2560217.2560218},
	doi = {10.1145/2560217.2560218},
	abstract = {Redundant software (and hardware) ensured Curiosity reached its destination and functioned as its designers intended.},
	language = {en},
	number = {2},
	urldate = {2021-06-25},
	journal = {Communications of the ACM},
	author = {Holzmann, Gerard J.},
	month = feb,
	year = {2014},
	pages = {64--73},
}

@inproceedings{fonseca_empirical_2017,
	address = {Belgrade Serbia},
	title = {An {Empirical} {Study} on the {Correctness} of {Formally} {Verified} {Distributed} {Systems}},
	isbn = {978-1-4503-4938-3},
	url = {https://dl.acm.org/doi/10.1145/3064176.3064183},
	doi = {10.1145/3064176.3064183},
	abstract = {Recent advances in formal veriﬁcation techniques enabled the implementation of distributed systems with machinechecked proofs. While results are encouraging, the importance of distributed systems warrants a large scale evaluation of the results and veriﬁcation practices.},
	language = {en},
	urldate = {2021-06-25},
	booktitle = {Proceedings of the {Twelfth} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Fonseca, Pedro and Zhang, Kaiyuan and Wang, Xi and Krishnamurthy, Arvind},
	month = apr,
	year = {2017},
	pages = {328--343},
}

@article{hoare_axiomatic_1969,
	title = {An axiomatic basis for computer programming},
	volume = {12},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/363235.363259},
	doi = {10.1145/363235.363259},
	abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and practical, may follow from a pursuance of these topics.},
	language = {en},
	number = {10},
	urldate = {2021-07-12},
	journal = {Communications of the ACM},
	author = {Hoare, C. A. R.},
	month = oct,
	year = {1969},
	pages = {576--580},
}

@article{chlipala2017formal,
	title = {Formal reasoning about programs},
	journal = {url: http://adam. chlipala. net/frap},
	author = {Chlipala, Adam},
	year = {2017},
}

@book{fowler2010domain,
	title = {Domain-specific languages},
	publisher = {Pearson Education},
	author = {Fowler, Martin},
	year = {2010},
}

@article{burrows_chubby_nodate,
	title = {The {Chubby} lock service for loosely-coupled distributed systems},
	abstract = {We describe our experiences with the Chubby lock service, which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. Chubby provides an interface much like a distributed ﬁle system with advisory locks, but the design emphasis is on availability and reliability, as opposed to high performance. Many instances of the service have been used for over a year, with several of them each handling a few tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modiﬁed to accommodate the differences.},
	language = {en},
	author = {Burrows, Mike},
	pages = {16},
}

@article{swartz_airport_nodate,
	title = {Airport 95: {Automated} {Baggage} {System}?},
	volume = {21},
	abstract = {The Denver International Airport automated baggage system was a major news story spanning the years 1994-95. Reconstruction of the events of the project management of this system serves as an example of project summary reporting, which is stipulated in every project management methodology, but which is seldom or never done. The author provides sufficient detail to enable simulation of the design approach alternatives. If other projects are reported in the same format, it will be possible to compare projects on a design phase and/or event-by-event basis. The author recommends establishment and maintenance of a knowledge base of specific causes for failed software development projects. Background On 28 February, 1995 the opening of the Denver International Airport represented the first new major airport since Dallas/Fort Worth in 1975. The DIA automated baggage system was a major news story of 1994/1995. In the Open Channel column of the February, 1995 issue of the IEEE Computer magazine, I advocated further investigation into the problems associated with this project.},
	language = {en},
	number = {2},
	journal = {ACM SIGSOFT},
	author = {Swartz, A John},
	pages = {5},
}

@inproceedings{tian2015comparative,
	title = {A comparative study on the effectiveness of part-of-speech tagging techniques on bug reports},
	booktitle = {2015 {IEEE} 22nd international conference on software analysis, evolution, and reengineering ({SANER})},
	author = {Tian, Yuan and Lo, David},
	year = {2015},
	note = {tex.organization: IEEE},
	pages = {570--574},
}

@inproceedings{chitchyanSustainabilityDesignRequirements2016,
	address = {Austin Texas},
	title = {Sustainability design in requirements engineering: state of practice},
	isbn = {978-1-4503-4205-6},
	shorttitle = {Sustainability design in requirements engineering},
	url = {https://dl.acm.org/doi/10.1145/2889160.2889217},
	doi = {10.1145/2889160.2889217},
	abstract = {Sustainability is now a major concern in society, but there is little understanding of how it is perceived by software engineering professionals and how sustainability design can become an embedded part of software engineering process. This paper presents the results of a qualitative study exploring requirements engineering practitioners’ perceptions and attitudes towards sustainability. It identiﬁes obstacles and mitigation strategies regarding the application of sustainability design principles in daily work life. The results of this study reveal several factors that can prevent sustainability design from becoming a ﬁrst class citizen in software engineering: software practitioners tend to have a narrow understanding of the concept of sustainability; organizations show limited awareness of its potential opportunities and beneﬁts; and the norms in the discipline are not conducive to sustainable outcomes. These ﬁndings suggest the need for focused eﬀorts in sustainability education, but also a need to rethink professional norms and practices.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering} {Companion}},
	publisher = {ACM},
	author = {Chitchyan, Ruzanna and Becker, Christoph and Betz, Stefanie and Duboc, Leticia and Penzenstadler, Birgit and Seyff, Norbert and Venters, Colin C.},
	month = may,
	year = {2016},
	pages = {533--542},
}

@inproceedings{laignerMonolithicBigData2020,
	address = {Portoroz, Slovenia},
	title = {From a {Monolithic} {Big} {Data} {System} to a {Microservices} {Event}-{Driven} {Architecture}},
	isbn = {978-1-72819-532-2},
	url = {https://ieeexplore.ieee.org/document/9226286/},
	doi = {10.1109/SEAA51224.2020.00045},
	abstract = {Context] Data-intensive systems, a.k.a. big data systems (BDS), are software systems that handle a large volume of data in the presence of performance quality attributes, such as scalability and availability. Before the advent of big data management systems (e.g. Cassandra) and frameworks (e.g. Spark), organizations had to cope with large data volumes with customtailored solutions. In particular, a decade ago, Tecgraf/PUCRio developed a system to monitor truck ﬂeet in real-time and proactively detect events from the positioning data received. Over the years, the system evolved into a complex and large obsolescent code base involving a costly maintenance process. [Goal] We report our experience on replacing a legacy BDS with a microservice-based event-driven system. [Method] We applied action research, investigating the reasons that motivate the adoption of a microservice-based event-driven architecture, intervening to deﬁne the new architecture, and documenting the challenges and lessons learned. [Results] We perceived that the resulting architecture enabled easier maintenance and faultisolation. However, the myriad of technologies and the complex data ﬂow were perceived as drawbacks. Based on the challenges faced, we highlight opportunities to improve the design of big data reactive systems. [Conclusions] We believe that our experience provides helpful takeaways for practitioners modernizing systems with data-intensive requirements.},
	language = {en},
	urldate = {2021-05-14},
	booktitle = {2020 46th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	publisher = {IEEE},
	author = {Laigner, Rodrigo and Kalinowski, Marcos and Diniz, Pedro and Barros, Leonardo and Cassino, Carlos and Lemos, Melissa and Arruda, Darlan and Lifschitz, Sergio and Zhou, Yongluan},
	month = aug,
	year = {2020},
	pages = {213--220},
}

@inproceedings{offutt_fault_2001,
	address = {Hong Kong, China},
	title = {A fault model for subtype inheritance and polymorphism},
	isbn = {978-0-7695-1306-5},
	url = {http://ieeexplore.ieee.org/document/989461/},
	doi = {10.1109/ISSRE.2001.989461},
	abstract = {Although program faults are widely studied, there are many aspects of faults that we still do not understand, particularly about 00 software. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a f u l l understanding of the characteristics of faults is crucial to several research areas. The power that inheritance and polymorphism brings to the expressiveness of programming languages also brings a number of new anomalies and fuult types. This paper presents a model f o r the appearance and realization of 00 faults and defines and discusses specijic categories of inheritance and polymorphic faults. The model and categories can be used to support empirical investigations of object-oriented testing techniques, to inspire further research into objectoriented testing and analysis, and to help improve design and development of object-oriented software.},
	language = {en},
	urldate = {2021-07-13},
	booktitle = {Proceedings 12th {International} {Symposium} on {Software} {Reliability} {Engineering}},
	publisher = {IEEE Comput. Soc},
	author = {Offutt, J. and Alexander, R. and Wu, Y. and Xiao, Q. and Hutchinson, C.},
	year = {2001},
	pages = {84--93},
}

@article{lagoFramingSustainabilityProperty2015,
	title = {Framing sustainability as a property of software quality},
	volume = {58},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2714560},
	doi = {10.1145/2714560},
	abstract = {This framework addresses the environmental dimension of software performance, as applied here by a paper mill and a car-sharing service.},
	language = {en},
	number = {10},
	urldate = {2021-03-24},
	journal = {Communications of the ACM},
	author = {Lago, Patricia and Koçak, Sedef Akinli and Crnkovic, Ivica and Penzenstadler, Birgit},
	month = sep,
	year = {2015},
	pages = {70--78},
}

@inproceedings{leveson1992high,
	title = {High-pressure steam engines and computer software},
	booktitle = {Proceedings of the 14th international conference on {Software} engineering},
	author = {Leveson, Nancy G},
	year = {1992},
	pages = {2--14},
}

@article{david_l_parnas_evaluation_1990,
	title = {Evaluation of safety-critical software},
	volume = {33},
	language = {en},
	number = {6},
	journal = {Communications of the ACM},
	author = {{David L Parnas} and van Schouwen, A. John and Kwan, Shu Po},
	year = {1990},
	pages = {13},
}

@article{hatton_n-version_1997,
	title = {N-version design versus one good version},
	volume = {14},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/636672/},
	doi = {10.1109/52.636672},
	language = {en},
	number = {6},
	urldate = {2021-06-25},
	journal = {IEEE Software},
	author = {Hatton, L.},
	month = dec,
	year = {1997},
	pages = {71--76},
}

@article{randell_turing_2000,
	title = {Turing {Memorial} {Lecture} {Facing} {Up} to {Faults}},
	volume = {43},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/43.2.95},
	doi = {10.1093/comjnl/43.2.95},
	language = {en},
	number = {2},
	urldate = {2021-06-25},
	journal = {The Computer Journal},
	author = {Randell, B.},
	month = feb,
	year = {2000},
	pages = {95--106},
}

@article{knight_experimental_1986,
	title = {An experimental evaluation of the assumption of independence in multiversion programming},
	volume = {SE-12},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/6312924/},
	doi = {10.1109/TSE.1986.6312924},
	language = {en},
	number = {1},
	urldate = {2021-06-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Knight, John C. and Leveson, Nancy G.},
	month = jan,
	year = {1986},
	pages = {96--109},
}

@article{knight_consistent_nodate,
	title = {The {Consistent} {Comparison} {Problem} in {N}-{Version} {Software}},
	abstract = {We have identified a difficulty in the implementation of N-version programming. The problem, which we call the Consistent Comparison Problem, arises for applications in which decisions are based on the results of comparisons of finite-precision numbers. We show that when versions make comparisons involving the results of finite-precision calculations, it is impossible to guarantee the consistency of their results. It is therefore possible that correct versions may arrive at completely different outputs for an application that does not apparently have multiple correct solutions. If this problem is not dealt with explicitly, an N-version system may be unable to reach a consensus even when none of its component versions fails.},
	language = {en},
	author = {Knight, John C and Nancy, A N D},
	pages = {5},
}

@article{chaum_security_1985,
	title = {Security without identification: transaction systems to make big brother obsolete},
	volume = {28},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Security without identification},
	url = {https://dl.acm.org/doi/10.1145/4372.4373},
	doi = {10.1145/4372.4373},
	abstract = {The large-scale automated transaction systems of the near future can be designed to protect the privacy and maintain the security of both individuals and organizations.},
	language = {en},
	number = {10},
	urldate = {2021-06-04},
	journal = {Communications of the ACM},
	author = {Chaum, David},
	month = oct,
	year = {1985},
	pages = {1030--1044},
}

@book{sanger2019perfect,
	title = {The perfect weapon: {War}, sabotage, and fear in the cyber age},
	publisher = {Broadway Books},
	author = {Sanger, David E},
	year = {2019},
}

@inproceedings{vanderlindenSchrodingerSecurityOpening2020,
	address = {Seoul South Korea},
	title = {Schrödinger's security: opening the box on app developers' security rationale},
	isbn = {978-1-4503-7121-6},
	shorttitle = {Schrödinger's security},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380394},
	doi = {10.1145/3377811.3380394},
	abstract = {Research has established the wide variety of security failures in mobile apps, their consequences, and how app developers introduce or exacerbate them. What is not well known is why developers do so—what is the rationale underpinning the decisions they make which eventually strengthen or weaken app security? This is all the more complicated in modern app development’s increasingly diverse demographic: growing numbers of independent, solo, or small team developers who do not have the organizational structures and support that larger software development houses enjoy.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {van der Linden, Dirk and Anthonysamy, Pauline and Nuseibeh, Bashar and Tun, Thein Than and Petre, Marian and Levine, Mark and Towse, John and Rashid, Awais},
	month = jun,
	year = {2020},
	pages = {149--160},
}

@article{gasibaSecureCodingEducation2021,
	title = {Is {Secure} {Coding} {Education} in the {Industry} {Needed}? {An} {Investigation} {Through} a {Large} {Scale} {Survey}},
	shorttitle = {Is {Secure} {Coding} {Education} in the {Industry} {Needed}?},
	url = {http://arxiv.org/abs/2102.05343},
	abstract = {The Department of Homeland Security in the United States estimates that 90\% of software vulnerabilities can be traced back to defects in design and software coding. The ﬁnancial impact of these vulnerabilities has been shown to exceed 380 million USD in industrial control systems alone. Since software developers write software, they also introduce these vulnerabilities into the source code. However, secure coding guidelines exist to prevent software developers from writing vulnerable code. This study focuses on the human factor, the software developer, and secure coding, in particular secure coding guidelines. We want to understand the software developers’ awareness and compliance to secure coding guidelines and why, if at all, they aren’t compliant or aware. We base our results on a large-scale survey on secure coding guidelines, with more than 190 industrial software developers. Our work’s main contribution motivates the need to educate industrial software developers on secure coding guidelines, and it gives a list of ﬁfteen actionable items to be used by practitioners in the industry. We also make our raw data openly available for further research.},
	language = {en},
	urldate = {2021-05-20},
	journal = {arXiv:2102.05343 [cs]},
	author = {Gasiba, Tiago Espinha and Lechner, Ulrike and Pinto-Albuquerque, Maria and Mendez, Daniel},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05343},
	keywords = {Computer Science - Software Engineering},
}

@article{landwehrTaxonomyComputerProgram1994,
	title = {A taxonomy of computer program security flaws},
	volume = {26},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/185403.185412},
	doi = {10.1145/185403.185412},
	abstract = {An organized record of actual flaws can be useful to computer system designers, programmers, analysts, administrators, and users. This survey provides a taxonomy for computer program security flaws, with an Appendix that documents 50 actual security flaws. These flaws have all been described previously in the open literature, but in widely separated places. For those new to the field of computer security, they provide a good introduction to the characteristics of security flaws and how they can arise. Because these flaws were not randomly selected from a valid statistical sample of such flaws, we make no strong claims concerning the likely distribution of actual security flaws within the taxonomy. However, this method of organizing security flaw data can help those who have custody of more representative samples to organize them and to focus their efforts to remove and, eventually, to prevent the introduction of security flaws.},
	language = {en},
	number = {3},
	urldate = {2021-04-30},
	journal = {ACM Computing Surveys},
	author = {Landwehr, Carl E. and Bull, Alan R. and McDermott, John P. and Choi, William S.},
	month = sep,
	year = {1994},
	pages = {211--254},
}

@incollection{howard2005measuring,
	title = {Measuring relative attack surfaces},
	booktitle = {Computer security in the 21st century},
	publisher = {Springer},
	author = {Howard, Michael and Pincus, Jon and Wing, Jeannette M},
	year = {2005},
	pages = {109--137},
}

@article{shin_evaluating_2011,
	title = {Evaluating {Complexity}, {Code} {Churn}, and {Developer} {Activity} {Metrics} as {Indicators} of {Software} {Vulnerabilities}},
	volume = {37},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/5560680/},
	doi = {10.1109/TSE.2010.81},
	abstract = {Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.},
	language = {en},
	number = {6},
	urldate = {2021-06-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Shin, Yonghee and Meneely, Andrew and Williams, Laurie and Osborne, Jason A.},
	month = nov,
	year = {2011},
	pages = {772--787},
}

@inproceedings{assalThinkSecureBeginning2019,
	address = {Glasgow Scotland Uk},
	title = {Think secure from the beginning: {A} {Survey} with {Software} {Developers}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {\textit{'{Think} secure from the beginning'}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300519},
	doi = {10.1145/3290605.3300519},
	abstract = {Vulnerabilities persist despite existing software security initiatives and best practices. This paper focuses on the human factors of software security, including human behaviour and motivation. We conducted an online survey to explore the interplay between developers and software security processes, e.g., we looked into how developers influence and are influenced by these processes. Our data included responses from 123 software developers currently employed in North America who work on various types of software applications. Whereas developers are often held responsible for security vulnerabilities, our analysis shows that the real issues frequently stem from a lack of organizational or process support to handle security throughout development tasks. Our participants are self-motivated towards software security, and the majority did not dismiss it but identified obstacles to achieving secure code. Our work highlights the need to look beyond the individual, and take a holistic approach to investigate organizational issues influencing software security.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Assal, Hala and Chiasson, Sonia},
	month = may,
	year = {2019},
	pages = {1--13},
}

@inproceedings{thomasSecurityApplicationDevelopment2018,
	address = {Montreal QC Canada},
	title = {Security {During} {Application} {Development}: an {Application} {Security} {Expert} {Perspective}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Security {During} {Application} {Development}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173836},
	doi = {10.1145/3173574.3173836},
	abstract = {Many of the security problems that people face today, such as security breaches and data theft, are caused by security vulnerabilities in application source code. Thus, there is a need to understand and improve the experiences of those who can prevent such vulnerabilities in the ﬁrst place - software developers as well as application security experts. Several studies have examined developers’ perceptions and behaviors regarding security vulnerabilities, demonstrating the challenges they face in performing secure programming and utilizing tools for vulnerability detection. We expand upon this work by focusing on those primarily responsible for application security - security auditors. In an interview study of 32 application security experts, we examine their views on application security processes, their workﬂows, and their interactions with developers in order to further inform the design of tools and processes to improve application security.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Thomas, Tyler W. and Tabassum, Madiha and Chu, Bill and Lipford, Heather},
	month = apr,
	year = {2018},
	pages = {1--12},
}

@inproceedings{lopezHopefullyWeAre2019,
	address = {Montreal, QC, Canada},
	title = {"{Hopefully} {We} {Are} {Mostly} {Secure}": {Views} on {Secure} {Code} in {Professional} {Practice}},
	isbn = {978-1-72812-239-7},
	shorttitle = {"{Hopefully} {We} {Are} {Mostly} {Secure}"},
	url = {https://ieeexplore.ieee.org/document/8816991/},
	doi = {10.1109/CHASE.2019.00023},
	abstract = {Security of software systems is of general concern, yet breaches caused by common vulnerabilities still occur. Software developers are routinely called upon to ”do more” to address this situation. However there has been little focus on the developers’ point of view, and understanding how security features in their day-to-day activities. This paper reports preliminary ﬁndings of semi-structured interviews taken during an ethnographic study of professional software developers in one organization who are not security experts. The overall study aims to understand how security features in day-to-day practice, while analysis of the interview data asks whether developers are responsible for security. The study reveals that awareness around security matters is raised through several paths including processes, standards, practices and company training and that a focus on security is driven by contextual factors. Security is taken care of with policies and through safeguards, and is handled differently depending on whether a team is developing new features, and hence ”looking forward”, or working with existing code and hence ”looking back”. Developers take and share responsibility for security in the code, but suggest that their responsibility has limits, and relies on collective practice.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {2019 {IEEE}/{ACM} 12th {International} {Workshop} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering} ({CHASE})},
	publisher = {IEEE},
	author = {Lopez, Tamara and Sharp, Helen and Tun, Thein and Bandara, Arosha and Levine, Mark and Nuseibeh, Bashar},
	month = may,
	year = {2019},
	pages = {61--68},
}

@article{tiefenauSecurityAvailabilityMultiple,
	title = {Security, {Availability}, and {Multiple} {Information} {Sources}:  {Exploring} {Update} {Behavior} of {System} {Administrators}},
	abstract = {Experts agree that keeping systems up to date is a powerful security measure. Previous work found that users sometimes explicitly refrain from performing timely updates, e.g., due to bad experiences which has a negative impact on end-user security. Another important user group has been investigated less extensively: system administrators, who are responsible for keeping complex and heterogeneous system landscapes available and secure.},
	language = {en},
	author = {Tiefenau, Christian and Häring, Maximilian and Krombholz, Katharina},
	pages = {21},
}

@article{dalelaMixedmethodStudySecurity2021,
	title = {A {Mixed}-method {Study} on {Security} and {Privacy} {Practices} in {Danish} {Companies}},
	url = {http://arxiv.org/abs/2104.04030},
	abstract = {Increased levels of digitalization in society expose companies to new security threats, requiring them to establish adequate security and privacy measures. Additionally, the presence of exogenous forces like new regulations, e.g., GDPR and the global COVID-19 pandemic, pose new challenges for companies that should preserve an adequate level of security while having to adapt to change. In this paper, we investigate such challenges through a two-phase study in companies located in Denmark—a country characterized by a high level of digitalization and trust—focusing on software development and techrelated companies. Our results show a number of issues, most notably i) a misalignment between software developers and management when it comes to the implementation of security and privacy measures, ii) difﬁculties in adapting company practices in light of implementing GDPR compliance, and iii) different views on the need to adapt security measures to cope with the COVID-19 pandemic.},
	language = {en},
	urldate = {2021-05-20},
	journal = {arXiv:2104.04030 [cs]},
	author = {Dalela, Asmita and Giallorenzo, Saverio and Kulyk, Oksana and Mauro, Jacopo and Paja, Elda},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.04030},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{pan_easy_2019,
	address = {Montreal, QC, Canada},
	title = {Easy {Modelling} and {Verification} of {Unpredictable} and {Preemptive} {Interrupt}-{Driven} {Systems}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8812085/},
	doi = {10.1109/ICSE.2019.00037},
	abstract = {The widespread real-time and embedded systems are mostly interrupt-driven because their heavy interaction with the environment is often initiated by interrupts. With the interrupt arrival being unpredictable and the interrupt handling being preemptive, a large number of possible system behaviours are generated, which makes the correctness assurance of such systems difﬁcult and costly. Model checking is considered to be one of the effective methods for exhausting behavioural state space for correctness. However, existing modelling approaches for interrupt-driven systems are based on either calculus or automata theory, and have a steep learning curve. To address this problem, we propose a new modelling language called interrupt sequence diagram (ISD). By extending the popular UML sequence diagram notations, the ISD supports the modelling of interrupts’ essential features visually and concisely. We also propose an automatabased semantics for ISD, based on which ISD can be transformed to a subset of hybrid automata so as to leverage the abundant off-the-shelf checkers. Experiments on examples from both realworld and existing literature were conducted, and the results demonstrate our approach’s usability and effectiveness.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Pan, Minxue and Chen, Shouyu and Pei, Yu and Zhang, Tian and Li, Xuandong},
	month = may,
	year = {2019},
	pages = {212--222},
}

@article{choiNTFUZZEnablingTypeAware,
	title = {{NTFUZZ}: {Enabling} {Type}-{Aware} {Kernel} {Fuzzing} on {Windows} with {Static} {Binary} {Analysis}},
	abstract = {Although it is common practice for kernel fuzzers to leverage type information of system calls, current Windows kernel fuzzers do not follow the practice as most system calls are private and largely undocumented. In this paper, we present a practical static binary analyzer that automatically infers system call types on Windows at scale. We incorporate our analyzer to NTFUZZ, a type-aware Windows kernel fuzzing framework. To our knowledge, this is the ﬁrst practical fuzzing system that utilizes scalable binary analysis on a COTS OS. With NTFUZZ, we found 11 previously unknown kernel bugs, and earned \$25,000 through the bug bounty program offered by Microsoft. All these results conﬁrm the practicality of our system as a kernel fuzzer.},
	language = {en},
	author = {Choi, Jaeseung and Kim, Kangsu and Lee, Daejin and Cha, Sang Kil},
	pages = {17},
}

@article{rediniDIANEIdentifyingFuzzing,
	title = {{DIANE}: {Identifying} {Fuzzing} {Triggers} in {Apps} to {Generate} {Under}-constrained {Inputs} for {IoT} {Devices}},
	abstract = {Internet of Things (IoT) devices have rooted themselves in the everyday life of billions of people. Thus, researchers have applied automated bug finding techniques to improve their overall security. However, due to the difficulties in extracting and emulating custom firmware, black-box fuzzing is often the only viable analysis option. Unfortunately, this solution mostly produces invalid inputs, which are quickly discarded by the targeted IoT device and do not penetrate its code. Another proposed approach is to leverage the companion app (i.e., the mobile app typically used to control an IoT device) to generate well-structured fuzzing inputs. Unfortunately, the existing solutions produce fuzzing inputs that are constrained by app-side validation code, thus significantly limiting the range of discovered vulnerabilities.},
	language = {en},
	author = {Redini, Nilo and Continella, Andrea and Das, Dipanjan and Pasquale, Giulio De and Spahn, Noah and Machiry, Aravind and Bianchi, Antonio and Kruegel, Christopher and Vigna, Giovanni},
	keywords = {IoT},
	pages = {17},
}

@article{tsipenyukSevenPerniciousKingdoms2005,
	title = {Seven {Pernicious} {Kingdoms}: {A} {Taxonomy} of {Software} {Security} {Errors}},
	volume = {3},
	issn = {1540-7993},
	shorttitle = {Seven {Pernicious} {Kingdoms}},
	url = {http://ieeexplore.ieee.org/document/1556543/},
	doi = {10.1109/MSP.2005.159},
	abstract = {We want to help developers and security practitioners understand common types of coding errors that lead to vulnerabilities. By organizing these errors into a simple taxonomy, we can teach developers to recognize categories of problems that lead to vulnerabilities and identify existing errors as they build software.},
	language = {en},
	number = {6},
	urldate = {2021-04-30},
	journal = {IEEE Security and Privacy Magazine},
	author = {Tsipenyuk, K. and Chess, B. and McGraw, G.},
	month = nov,
	year = {2005},
	pages = {81--84},
}

@inproceedings{acarDevelopersNeedSupport2017,
	address = {Cambridge, MA, USA},
	title = {Developers {Need} {Support}, {Too}: {A} {Survey} of {Security} {Advice} for {Software} {Developers}},
	isbn = {978-1-5386-3467-7},
	shorttitle = {Developers {Need} {Support}, {Too}},
	url = {http://ieeexplore.ieee.org/document/8077802/},
	doi = {10.1109/SecDev.2017.17},
	abstract = {Increasingly developers are becoming aware of the importance of software security, as frequent high-proﬁle security incidents emphasize the need for secure code. Faced with this new problem, most developers will use their normal approach: web search. But are the resulting web resources useful and effective at promoting security in practice? Recent research has identiﬁed security problems arising from Q\&A resources that help with speciﬁc secure-programming problems, but the web also contains many general resources that discuss security and secure programming more broadly, and to our knowledge few if any of these have been empirically evaluated. The continuing prevalence of security bugs suggests that this guidance ecosystem is not currently working well enough: either effective guidance is not available, or it is not reaching the developers who need it. This paper takes a ﬁrst step toward understanding and improving this guidance ecosystem by identifying and analyzing 19 general advice resources. The results identify important gaps in the current ecosystem and provide a basis for future work evaluating existing resources and developing new ones to ﬁll these gaps.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {2017 {IEEE} {Cybersecurity} {Development} ({SecDev})},
	publisher = {IEEE},
	author = {Acar, Yasemin and Stransky, Christian and Wermke, Dominik and Weir, Charles and Mazurek, Michelle L. and Fahl, Sascha},
	month = sep,
	year = {2017},
	pages = {22--26},
}

@inproceedings{chaPrincipledApproachGraphQL2020,
	address = {Virtual Event USA},
	title = {A principled approach to {GraphQL} query cost analysis},
	isbn = {978-1-4503-7043-1},
	url = {https://dl.acm.org/doi/10.1145/3368089.3409670},
	doi = {10.1145/3368089.3409670},
	abstract = {The landscape of web APIs is evolving to meet new client requirements and to facilitate how providers fulfill them. A recent web API model is GraphQL, which is both a query language and a runtime. Using GraphQL, client queries express the data they want to retrieve or mutate, and servers respond with exactly those data or changes. GraphQL’s expressiveness is risky for service providers because clients can succinctly request stupendous amounts of data, and responding to overly complex queries can be costly or disrupt service availability. Recent empirical work has shown that many service providers are at risk. Using traditional API management methods is not sufficient, and practitioners lack principled means of estimating and measuring the cost of the GraphQL queries they receive.},
	language = {en},
	urldate = {2021-04-02},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Cha, Alan and Wittern, Erik and Baudart, Guillaume and Davis, James C. and Mandel, Louis and Laredo, Jim A.},
	month = nov,
	year = {2020},
	pages = {257--268},
}

@article{yangInterviewStudyHow,
	title = {An interview study of how developers use execution logs in embedded software engineering},
	abstract = {Execution logs capture the run-time behavior of software systems. To assist developers in their maintenance tasks, many studies have proposed tools to analyze execution information from logs. However, it is as yet unknown how industry developers analyze logs in embedded software engineering.},
	language = {en},
	author = {Yang, Nan and Schiffelers, Ramon and Lukkien, Johan and Cuijpers, Pieter and Serebrenik, Alexander},
	keywords = {IoT},
	pages = {10},
}

@article{chenLearningGuidedNetworkFuzzing,
	title = {Learning-{Guided} {Network} {Fuzzing} for {Testing} {Cyber}-{Physical} {System} {Defences}},
	abstract = {The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically ﬁnding ‘test suites’ of CPS network attacks, without requiring any knowledge of the system’s control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efﬁcacy of smart fuzzing by implementing it for two real-world CPS testbeds—a water puriﬁcation plant and a water distribution system—ﬁnding attacks that drive them into 27 different unsafe states involving water ﬂow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, ﬁnding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.},
	language = {en},
	author = {Chen, Yuqi and Poskitt, Christopher M and Sun, Jun and Adepu, Sridhar and Zhang, Fan},
	keywords = {IoT},
	pages = {12},
}

@techreport{alspaugh1992software,
	title = {Software requirements for the a-7e aircraft.},
	institution = {NAVAL RESEARCH LAB WASHINGTON DC},
	author = {Alspaugh, Thomas A and Faulk, Stuart R and Britton, Kathryn H and Parker, R Alan and Parnas, David L},
	year = {1992},
}

@article{zave_reference_2000,
	title = {A reference model for requirements and specifications},
	volume = {17},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/896248/},
	doi = {10.1109/52.896248},
	language = {en},
	number = {3},
	urldate = {2021-06-23},
	journal = {IEEE Software},
	author = {Zave, P. and Jackson, M. and Gunter, E.L. and Gunter, C.A.},
	month = jun,
	year = {2000},
	pages = {37--43},
}

@inproceedings{osterweil1997software,
	title = {Software processes are software too, revisited: an invited talk on the most influential paper of {ICSE} 9},
	booktitle = {Proceedings of the 19th international conference on {Software} engineering},
	author = {Osterweil, Leon J},
	year = {1997},
	pages = {540--548},
}

@article{mckeeman_differential_1998,
	title = {Differential {Testing} for {Software}},
	volume = {10},
	language = {en},
	number = {1},
	author = {McKeeman, William M},
	year = {1998},
	pages = {8},
}

@inproceedings{pirelliRequirementsElicitationService2019,
	address = {Jeju Island, Korea (South)},
	title = {Requirements {Elicitation} with a {Service} {Canvas} for {Packaged} {Enterprise} {Systems}},
	isbn = {978-1-72813-912-8},
	url = {https://ieeexplore.ieee.org/document/8920569/},
	doi = {10.1109/RE.2019.00043},
	abstract = {We present a technique for eliciting requirements based on the use of a service canvas and the results of its application in the early phase of a customer relationship management integration project. The project was a collaboration between a research group and two industry partners. We describe (1) our service canvas, (2) how we designed a set of workshops to elicit the requirements, (3) the support tools used for running the workshops, and (4) the resulting canvas, listing the customer relationship management requirements, that was the basis for the project proposal. We explain how, as participant observers, we conducted the project and how we collected and analyzed the data. We describe what worked well and the lessons we learned. We outline some practical problems that remain unsolved.},
	language = {en},
	urldate = {2021-03-22},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	publisher = {IEEE},
	author = {Pirelli, Blagovesta and Etzlinger, Lucien and Derrier, David and Regev, Gil and Wegmann, Alain},
	month = sep,
	year = {2019},
	pages = {340--350},
}


@inproceedings{appleton_streamed_1998,
	title = {Streamed {Lines}: {Branching} {Patterns} for {Parallel} {Software} {Development}},
	abstract = {Most software version control systems provide mechanisms for branching into multiple lines of development and merging source code from one development line into another. However, the techniques, policies and guidelines for using these mechanisms are often misapplied or not fully understood. This is unfortunate, since the use or misuse of branching and merging can make or break a parallel software development project. Streamed Lines is a pattern language for organizing related lines of development into appropriately diverging and converging streams of source code changes.},
	language = {en},
	author = {Appleton, Brad and Berczuk, Stephen P and Cabrera, Ralph and Orenstein, Robert},
	year = {1998},
	pages = {67},
}

@incollection{osterweilSoftwareProcessesAre2011,
	address = {Berlin, Heidelberg},
	title = {Software {Processes} are {Software} {Too}},
	isbn = {978-3-642-19822-9 978-3-642-19823-6},
	url = {http://link.springer.com/10.1007/978-3-642-19823-6_17},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {Engineering of {Software}},
	publisher = {Springer Berlin Heidelberg},
	author = {Osterweil, Leon},
	editor = {Tarr, Peri L. and Wolf, Alexander L.},
	year = {2011},
	doi = {10.1007/978-3-642-19823-6_17},
	pages = {323--344},
}

@inproceedings{tsantalis2009identification,
	title = {Identification of extract method refactoring opportunities},
	booktitle = {2009 13th european conference on software maintenance and reengineering},
	author = {Tsantalis, Nikolaos and Chatzigeorgiou, Alexander},
	year = {2009},
	note = {tex.organization: IEEE},
	pages = {119--128},
}

@article{tsantalis2011identification,
	title = {Identification of extract method refactoring opportunities for the decomposition of methods},
	volume = {84},
	number = {10},
	journal = {Journal of Systems and Software},
	author = {Tsantalis, Nikolaos and Chatzigeorgiou, Alexander},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {1757--1782},
}

@incollection{garlanINTRODUCTIONSOFTWAREARCHITECTURE1993,
	title = {An {INTRODUCTION} {TO} {SOFTWARE} {ARCHITECTURE}},
	volume = {2},
	isbn = {978-981-02-1594-1 978-981-279-803-9},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9789812798039_0001},
	abstract = {This work was funded in part by the Department of Defense Advanced Research Project Agency under grant MDA972-92-J-1002, by National Science Foundation Grants CCR-9109469 and CCR-9112880, and by a grant from Siemens Corporate Research. It was also funded in part by the Carnegie Mellon University School of Computer Science and Software Engineering Institute (which is sponsored by the U.S. Department of Defense). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government, the Department of Defense, the National Science Foundation, Siemens Corporation, or Carnegie Mellon University.},
	language = {en},
	urldate = {2021-04-21},
	booktitle = {Series on {Software} {Engineering} and {Knowledge} {Engineering}},
	publisher = {WORLD SCIENTIFIC},
	author = {Garlan, David and Shaw, Mary},
	collaborator = {Ambriola, Vincenzo and Tortora, Genoveffa},
	month = dec,
	year = {1993},
	doi = {10.1142/9789812798039_0001},
	pages = {1--39},
}

@article{martinDesignPrinciplesDesign2000,
	title = {Design {Principles} and {Design} {Patterns}},
	language = {en},
	author = {Martin, Robert C},
	year = {2000},
	pages = {34},
}

@article{adamsPeopleSystematicallyOverlook,
	title = {People systematically overlook subtractive changes},
	language = {en},
	author = {Adams, Gabrielle S},
	pages = {17},
}

@article{kruchten_41_1995,
	title = {The 4+1 {View} {Model} of architecture},
	volume = {12},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/469759/},
	doi = {10.1109/52.469759},
	language = {en},
	number = {6},
	urldate = {2021-08-20},
	journal = {IEEE Software},
	author = {Kruchten, P.B.},
	month = nov,
	year = {1995},
	pages = {42--50},
}

@article{shneiderman_experimental_1977,
	title = {Experimental investigations of the utility of detailed flowcharts in programming},
	volume = {20},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/359605.359610},
	doi = {10.1145/359605.359610},
	abstract = {This paper describes previous research on flowcharts and a series of controlled experiments to test the utility of detailed flowcharts as an aid to program composition, comprehension, debugging, and modification. No statistically significant difference between flowchart and nonflowchart groups has been shown, thereby calling into question the utility of detailed flowcharting. A program of further research is suggested.},
	language = {en},
	number = {6},
	urldate = {2021-06-21},
	journal = {Communications of the ACM},
	author = {Shneiderman, Ben and Mayer, Richard and McKay, Don and Heller, Peter},
	month = jun,
	year = {1977},
	pages = {373--381},
}

@article{lampsonHintsComputerSystemDesign,
	title = {Hints for {ComputerSystemDesign}},
	abstract = {Experience with the design and implementation of a number of computer systems, and study of many other systems, has led to some general hints for system design which are described here. They are illustrated by a number of examples, ranging from hardware such as the Alto and the Dorado to applications programs such as Bravo and Star.},
	language = {en},
	journal = {SOSP},
	author = {Lampson, Butler W},
	pages = {16},
}

@book{fowler2018refactoring,
	title = {Refactoring: improving the design of existing code},
	publisher = {Addison-Wesley Professional},
	author = {Fowler, Martin},
	year = {2018},
}

@inproceedings{tufanoWhenWhyYour2015,
	address = {Florence, Italy},
	title = {When and {Why} {Your} {Code} {Starts} to {Smell} {Bad}},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194592/},
	doi = {10.1109/ICSE.2015.59},
	abstract = {In past and recent years, the issues related to managing technical debt received signiﬁcant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To ﬁll this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smellintroducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., those identiﬁed as smellintroducing). Our ﬁndings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Oliveto, Rocco and Di Penta, Massimiliano and De Lucia, Andrea and Poshyvanyk, Denys},
	month = may,
	year = {2015},
	pages = {403--414},
}

@article{krueger1992software,
	title = {Software reuse},
	volume = {24},
	number = {2},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Krueger, Charles W},
	year = {1992},
	note = {Publisher: ACM New York, NY, USA},
	pages = {131--183},
}

@article{frakes_software_2005,
	title = {Software reuse research: status and future},
	volume = {31},
	issn = {0098-5589},
	shorttitle = {Software reuse research},
	url = {http://ieeexplore.ieee.org/document/1492369/},
	doi = {10.1109/TSE.2005.85},
	abstract = {This paper briefly summarizes software reuse research, discusses major research contributions and unsolved problems, provides pointers to key publications, and introduces four papers selected from The Eighth International Conference on Software Reuse (ICSR8).},
	language = {en},
	number = {7},
	urldate = {2021-07-13},
	journal = {IEEE Transactions on Software Engineering},
	author = {Frakes, W.B. and {Kyo Kang}},
	month = jul,
	year = {2005},
	pages = {529--536},
}

@article{morisio_success_2002,
	title = {Success and failure factors in software reuse},
	volume = {28},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/995420/},
	doi = {10.1109/TSE.2002.995420},
	abstract = {ÐThis paper aims at identifying some of the key factors in adopting or running a company-wide software reuse program. Key factors are derived from empirical evidence of reuse practices, as emerged from a survey of projects for the introduction of reuse in European companies: 24 such projects performed from 1994 to 1997 were analyzed using structured interviews. The projects were undertaken in both large and small companies, working in a variety of business domains, and using both object-oriented and procedural development approaches. Most of them produce software with high commonality between applications, and have at least reasonably mature processes. Despite that apparent potential for success, around one-third of the projects failed. Three main causes of failure were not introducing reuse-specific processes, not modifying nonreuse processes, and not considering human factors. The root cause was a lack of commitment by top management, or nonawareness of the importance of those factors, often coupled with the belief that using the object-oriented approach or setting up a repository seamlessly is all that is necessary to achieve success in reuse. Conversely, successes were achieved when, given a potential for reuse because of commonality among applications, management committed to introducing reuse processes, modifying nonreuse processes, and addressing human factors. While addressing those three issues turned out to be essential, the lower-level details of how to address them varied greatly: for instance, companies produced large-grained or small-grained reusable assets, did or did not perform domain analysis, did or did not use dedicated reuse groups, used specific tools for the repository or no tools. As far as these choices are concerned, the key point seems to be the sustainability of the approach and its suitability to the context of the company.},
	language = {en},
	number = {4},
	urldate = {2021-07-13},
	journal = {IEEE Transactions on Software Engineering},
	author = {Morisio, M. and Ezran, M. and Tully, C.},
	month = apr,
	year = {2002},
	pages = {340--357},
}

@incollection{varnell-sarjeant_comparing_2015,
	title = {Comparing {Reuse} {Strategies} in {Different} {Development} {Environments}},
	volume = {97},
	isbn = {978-0-12-802133-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065245814000035},
	language = {en},
	urldate = {2021-06-29},
	booktitle = {Advances in {Computers}},
	publisher = {Elsevier},
	author = {Varnell-Sarjeant, Julia and Amschler Andrews, Anneliese},
	year = {2015},
	doi = {10.1016/bs.adcom.2014.10.002},
	pages = {1--47},
}

@article{parnas_modular_1985,
	title = {The {Modular} {Structure} of {Complex} {Systems}},
	volume = {SE-11},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/1702002/},
	doi = {10.1109/TSE.1985.232209},
	language = {en},
	number = {3},
	urldate = {2021-07-12},
	journal = {IEEE Transactions on Software Engineering},
	author = {Parnas, D.L. and Clements, P.C. and Weiss, D.M.},
	month = mar,
	year = {1985},
	pages = {259--266},
}

@inproceedings{hayhurst_considering_2003,
	address = {Indianapolis, IN, USA},
	title = {Considering object oriented technology in aviation applications},
	isbn = {978-0-7803-7844-5},
	url = {http://ieeexplore.ieee.org/document/5731073/},
	doi = {10.1109/DASC.2003.1245823},
	abstract = {Few developers of commercial aviation software products are using object-oriented technology (OOT), despite its popularity in some other industries. Safety concerns about using OOT in critical applications, uncertainty about how to comply with regulatory requirements, and basic conservatism within the aviation community have been factors behind this caution.},
	language = {en},
	urldate = {2021-07-13},
	booktitle = {22nd {Digital} {Avionics} {Systems} {Conference} {Proceedings} ({Cat} {No} {03CH37449}) {DASC}-03},
	publisher = {IEEE},
	author = {{Hayhurst} and {Holloway}},
	year = {2003},
	pages = {3.B.1--3.1},
}

@article{weisertPseudoObjectorientedProgramming2002,
	title = {Pseudo object-oriented programming considered harmful},
	volume = {37},
	issn = {0362-1340, 1558-1160},
	url = {https://dl.acm.org/doi/10.1145/510857.510865},
	doi = {10.1145/510857.510865},
	language = {en},
	number = {4},
	urldate = {2021-04-30},
	journal = {ACM SIGPLAN Notices},
	author = {Weisert, Conrad},
	month = apr,
	year = {2002},
	pages = {31--31},
}

@article{mohagheghi_quality_2007,
	title = {Quality, productivity and economic benefits of software reuse: a review of industrial studies},
	volume = {12},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Quality, productivity and economic benefits of software reuse},
	url = {http://link.springer.com/10.1007/s10664-007-9040-x},
	doi = {10.1007/s10664-007-9040-x},
	abstract = {Systematic software reuse is proposed to increase productivity and software quality and lead to economic benefits. Reports of successful software reuse programs in industry have been published. However, there has been little effort to organize the evidence systematically and appraise it. This review aims to assess the effects of software reuse in industrial contexts. Journals and major conferences between 1994 and 2005 were searched to find observational studies and experiments conducted in industry, returning eleven papers of observational type. Systematic software reuse is significantly related to lower problem (defect, fault or error) density in five studies and to decreased effort spent on correcting problems in three studies. The review found evidence for significant gains in apparent productivity in three studies. Other significant benefits of software reuse were reported in single studies or the results were inconsistent. Evidence from industry is sparse and combining results was done by vote-counting. Researchers should pay more attention to using comparable metrics, performing longitudinal studies, and explaining the results and impact on industry. For industry, evaluating reuse of COTS or OSS components, integrating reuse activities in software processes, better data collection and evaluating return on investment are major challenges.},
	language = {en},
	number = {5},
	urldate = {2021-07-13},
	journal = {Empirical Software Engineering},
	author = {Mohagheghi, Parastoo and Conradi, Reidar},
	month = sep,
	year = {2007},
	pages = {471--516},
}

@inproceedings{leveson2004making,
	title = {Making embedded software reuse practical and safe},
	booktitle = {Proceedings of the 12th acm sigsoft twelfth international symposium on foundations of software engineering},
	author = {Leveson, Nancy G and Weiss, Kathryn Anne},
	year = {2004},
	pages = {171--178},
}

@article{parnasCriteriaBeUsed1972,
	title = {On the criteria to be used in decomposing systems into modules},
	volume = {15},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/361598.361623},
	doi = {10.1145/361598.361623},
	abstract = {This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a "modularization" is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.},
	language = {en},
	number = {12},
	urldate = {2021-04-19},
	journal = {Communications of the ACM},
	author = {Parnas, D. L.},
	month = dec,
	year = {1972},
	pages = {1053--1058},
}

@article{xingPseudoObjectorientedProgramming2003,
	title = {On pseudo object-oriented programming considered harmful},
	volume = {46},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/944217.944246},
	doi = {10.1145/944217.944246},
	abstract = {Reconsidering and disentangling some fundamental relationships and issues.},
	language = {en},
	number = {10},
	urldate = {2021-04-30},
	journal = {Communications of the ACM},
	author = {Xing, Cong-cong and Belkhouche, Boumediene},
	month = oct,
	year = {2003},
	pages = {115--117},
}

@article{leitner_mixed-method_2019,
	title = {A mixed-method empirical study of {Function}-as-a-{Service} software development in industrial practice},
	volume = {149},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121218302735},
	doi = {10.1016/j.jss.2018.12.013},
	language = {en},
	urldate = {2021-06-02},
	journal = {Journal of Systems and Software},
	author = {Leitner, Philipp and Wittern, Erik and Spillner, Josef and Hummer, Waldemar},
	month = mar,
	year = {2019},
	pages = {340--359},
}

@inproceedings{vanDeursen2017TeachingSWArch,
	title = {A collaborative approach to teaching software architecture},
	doi = {10.1145/3017680.3017737},
	booktitle = {Proceedings of 48th {ACM} technical symposium on computer science education ({SIGCSE})},
	author = {van Deursen, Arie and Aniche, Maurício and Aué, Joop and Slag, Rogier and de Jong, Michael and Nederlof, Alex and Bouwers, Eric},
	year = {2017},
}

@inproceedings{paul_why_2021,
	address = {Madrid, Spain},
	title = {Why {Security} {Defects} {Go} {Unnoticed} during {Code} {Reviews}? {A} {Case}-{Control} {Study} of the {Chromium} {OS} {Project}},
	isbn = {978-1-66540-296-5},
	shorttitle = {Why {Security} {Defects} {Go} {Unnoticed} during {Code} {Reviews}?},
	url = {https://ieeexplore.ieee.org/document/9402130/},
	doi = {10.1109/ICSE43902.2021.00124},
	abstract = {Peer code review has been found to be effective in identifying security vulnerabilities. However, despite practicing mandatory code reviews, many Open Source Software (OSS) projects still encounter a large number of post-release security vulnerabilities, as some security defects escape those. Therefore, a project manager may wonder if there was any weakness or inconsistency during a code review that missed a security vulnerability. Answers to this question may help a manager pinpointing areas of concern and taking measures to improve the effectiveness of his/her project’s code reviews in identifying security defects. Therefore, this study aims to identify the factors that differentiate code reviews that successfully identified security defects from those that missed such defects.},
	language = {en},
	urldate = {2021-05-27},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Paul, Rajshakhar and Turzo, Asif Kamal and Bosu, Amiangshu},
	month = may,
	year = {2021},
	pages = {1373--1385},
}

@article{noauthor_challenges_nodate,
	title = {Challenges and success factors for large-scale agile transformations: {A} systematic literature review {\textbar} {Elsevier} {Enhanced} {Reader}},
	language = {en},
	pages = {23},
}

@article{mccracken_life_1982,
	title = {Life cycle concept considered harmful},
	volume = {7},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/1005937.1005943},
	doi = {10.1145/1005937.1005943},
	language = {en},
	number = {2},
	urldate = {2021-08-26},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {McCracken, Daniel D. and Jackson, Michael A.},
	month = apr,
	year = {1982},
	pages = {29--32},
}

@article{boehm_spiral_1988,
	title = {A spiral model of software development and enhancement},
	volume = {21},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/59/},
	doi = {10.1109/2.59},
	language = {en},
	number = {5},
	urldate = {2021-08-26},
	journal = {Computer},
	author = {Boehm, B. W.},
	month = may,
	year = {1988},
	pages = {61--72},
}

@incollection{douglass2013agile,
	title = {Agile development for embedded systems},
	booktitle = {Software engineering for embedded systems},
	publisher = {Elsevier},
	author = {Douglass, Bruce},
	year = {2013},
	pages = {731--766},
}

@book{kenschwaberScrumGuide2020,
	title = {The {Scrum} {Guide}},
	url = {https://scrumguides.org},
	author = {Ken Schwaber, Jeff Sutherland},
	year = {2020},
}

@article{herbsleb_empirical_2003,
	title = {An empirical study of speed and communication in globally distributed software development},
	volume = {29},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/1205177/},
	doi = {10.1109/TSE.2003.1205177},
	abstract = {Global software development is rapidly becoming the norm for technology companies. Previous qualitative research suggests that distributed development may increase development cycle time for individual work items (modification requests). We use both data from the source code change management system and survey data to model the extent of delay in a distributed software development organization and explore several possible mechanisms for this delay. One key finding is that distributed work items appear to take about two and one-half times as long to complete as similar items where all the work is colocated. The data strongly suggest a mechanism for the delay, i.e., that distributed work items involve more people than comparable same-site work items, and the number of people involved is strongly related to the calendar time to complete a work item. We replicate the analysis of change data in a different organization with a different product and different sites and confirm our main findings. We also report survey results showing differences between same-site and distributed social networks, testing several hypotheses about characteristics of distributed social networks that may be related to delay. We discuss implications of our findings for practices and collaboration technology that have the potential for dramatically speeding distributed software development.},
	language = {en},
	number = {6},
	urldate = {2021-06-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Herbsleb, J.D. and Mockus, A.},
	month = jun,
	year = {2003},
	pages = {481--494},
}

@inproceedings{herbsleb_global_2005,
	address = {St. Louis, MO, USA},
	title = {Global software development at siemens: experience from nine projects},
	shorttitle = {Global software development at siemens},
	url = {http://ieeexplore.ieee.org/document/1553598/},
	doi = {10.1109/ICSE.2005.1553598},
	abstract = {We report on the experiences of Siemens Corporation in nine globally-distributed software development projects. These projects represent a range of collaboration models, from co-development to outsourcing of components to outsourcing the software for an entire project. We report experience and lessons in issues of project management, division of labor, ongoing coordination of technical work, and communication. We include lessons learned, and conclude the paper with suggestions about important open research issues in this area.},
	language = {en},
	urldate = {2021-06-21},
	booktitle = {Proceedings. 27th {International} {Conference} on {Software} {Engineering}, 2005. {ICSE} 2005.},
	publisher = {IEEe},
	author = {Herbsleb, J.D. and Paulish, D.J. and Bass, M.},
	year = {2005},
	pages = {524--533},
}

@inproceedings{holmstrom_global_2006,
	address = {Florianopolis, Brazil},
	title = {Global {Software} {Development} {Challenges}: {A} {Case} {Study} on {Temporal}, {Geographical} and {Socio}-{Cultural} {Distance}},
	isbn = {978-0-7695-2663-8},
	shorttitle = {Global {Software} {Development} {Challenges}},
	url = {http://ieeexplore.ieee.org/document/4031736/},
	doi = {10.1109/ICGSE.2006.261210},
	abstract = {Global software development (GSD) is a phenomenon that is receiving considerable interest from companies all over the world. In GSD, stakeholders from different national and organizational cultures are involved in developing software and the many benefits include access to a large labour pool, cost advantage and round-the-clock development. However, GSD is technologically and organizationally complex and presents a variety of challenges to be managed by the software development team. In particular, temporal, geographical and socio-cultural distances impose problems not experienced in traditional systems development. In this paper, we present findings from a case study in which we explore the particular challenges associated with managing GSD. Our study also reveals some of the solutions that are used to deal with these challenges. We do so by empirical investigation at three US based GSD companies operating in Ireland. Based on qualitative interviews we present challenges related to temporal, geographical and socio-cultural distance.},
	language = {en},
	urldate = {2021-06-21},
	booktitle = {2006 {IEEE} {International} {Conference} on {Global} {Software} {Engineering} ({ICGSE}'06)},
	publisher = {IEEE},
	author = {Holmstrom, Helena and Conchuir, Eoin and Agerfalk, Par and Fitzgerald, Brian},
	month = oct,
	year = {2006},
	pages = {3--11},
}

@article{russoAgileSuccessModel,
	title = {The {Agile} {Success} {Model}: {A} {Mixed} {Methods} {Study} of a {Large}-{Scale} {Agile} {Transformation}},
	volume = {37},
	abstract = {Organizations are increasingly adopting Agile frameworks for their internal software development. Cost reduction, rapid deployment, requirements and mental model alignment are typical reasons for an Agile transformation. This paper presents an in-depth field study of a large-scale Agile transformation in a missioncritical environment, where stakeholders’ commitment was a critical success factor. The goal of such a transformation was to implement mission-oriented features, reducing costs and time to operate in critical scenarios. The project lasted several years and involved over 40 professionals. We report how a hierarchical and plan-driven organization exploited Agile methods to develop a Command \& Control (C2) system. Accordingly, we first abstract our experience, inducing a success model of general use for other comparable organizations by performing a post-mortem study. The goal of the inductive research process was to identify critical success factors and their relations. Finally, we validated and generalized our model through Partial Least Squares Structural Equation Modelling, surveying 200 software engineers involved in similar projects. We conclude the paper with data-driven recommendations concerning the management of Agile projects. CCS Concepts: • Social and professional topics → Project and people management; Software management; Sustainability; • Software and its engineering → Agile software development.},
	language = {en},
	number = {4},
	author = {Russo, Daniel},
	pages = {45},
}

@inproceedings{mitchellComparisonSoftwareCost2009,
	address = {Lake Buena Vista, FL, USA},
	title = {A comparison of software cost, duration, and quality for waterfall vs. iterative and incremental development: {A} systematic review},
	isbn = {978-1-4244-4842-5},
	shorttitle = {A comparison of software cost, duration, and quality for waterfall vs. iterative and incremental development},
	url = {http://ieeexplore.ieee.org/document/5314228/},
	doi = {10.1109/ESEM.2009.5314228},
	abstract = {The objective of this study is to present a body of evidence that will assist software project managers to make informed choices about software development approaches for their projects. In particular, two broadly defined competing approaches, the traditional “waterfall” approach and iterative and incremental development (IID), are compared with regards to development cost and duration, and resulting product quality. The method used for this comparison is a systematic literature review. The small set of studies we located did not demonstrate any identifiable cost, duration, or quality trends, although there was some evidence suggesting the superiority of IID (in particular XP). The results of this review indicate that further empirical studies, both quantitative and qualitative, on this topic need to be undertaken. In order to effectively compare study results, the research community needs to reach a consensus on a set of comparable parameters that best assess cost, duration, and quality.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {2009 3rd {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE},
	author = {Mitchell, Susan M. and Seaman, Carolyn B.},
	month = oct,
	year = {2009},
	pages = {511--515},
}

@article{beckEmbracingChangeExtreme1999,
	title = {Embracing change with extreme programming},
	volume = {32},
	issn = {00189162},
	url = {http://ieeexplore.ieee.org/document/796139/},
	doi = {10.1109/2.796139},
	language = {en},
	number = {10},
	urldate = {2021-04-04},
	journal = {Computer},
	author = {Beck, K.},
	month = oct,
	year = {1999},
	pages = {70--77},
}

@article{battin_leveraging_2001,
	title = {Leveraging resources in global software development},
	volume = {18},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/914750/},
	doi = {10.1109/52.914750},
	language = {en},
	number = {2},
	urldate = {2021-06-21},
	journal = {IEEE Software},
	author = {Battin, R.D. and Crocker, R. and Kreidler, J. and Subramanian, K.},
	month = apr,
	year = {2001},
	pages = {70--77},
}

@article{herbsleb_architectures_1999,
	title = {Architectures, coordination, and distance: {Conway}'s law and beyond},
	volume = {16},
	issn = {07407459},
	shorttitle = {Architectures, coordination, and distance},
	url = {http://ieeexplore.ieee.org/document/795103/},
	doi = {10.1109/52.795103},
	language = {en},
	number = {5},
	urldate = {2021-06-21},
	journal = {IEEE Software},
	author = {Herbsleb, J.D. and Grinter, R.E.},
	month = oct,
	year = {1999},
	pages = {63--70},
}

@article{herbsleb_formulation_nodate,
	title = {Formulation and {Preliminary} {Test} of an {Empirical} {Theory} of {Coordination} in {Software} {Engineering}},
	abstract = {Motivated by evidence that coordination and dependencies among engineering decisions in a software project are key to better understanding and better methods of software creation, we set out to create empirically testable theory to characterize and make predictions about coordination of engineering decisions. We demonstrate that our theory is capable of expressing some of the main ideas about coordination in software engineering, such as Conway’s law and the effects of information hiding in modular design. We then used software project data to create measures and test two hypotheses derived from our theory. Our results provide preliminary support for our formulations.},
	language = {en},
	author = {Herbsleb, James D and Mockus, Audris},
	pages = {10},
}

@inproceedings{elovitz1979experiment,
	title = {An experiment in software engineering: {The} architecture research facility as a case study},
	booktitle = {Proceedings of the 4th international conference on {Software} engineering},
	author = {Elovitz, Honey S},
	year = {1979},
	pages = {145--152},
}

@inproceedings{begel2008pair,
	title = {Pair programming: what's in it for me?},
	booktitle = {Proceedings of the {Second} {ACM}-{IEEE} international symposium on {Empirical} software engineering and measurement},
	author = {Begel, Andrew and Nagappan, Nachiappan},
	year = {2008},
	pages = {120--128},
}

@inproceedings{ford2016paradise,
	title = {Paradise unplugged: {Identifying} barriers for female participation on stack overflow},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} international symposium on foundations of software engineering},
	author = {Ford, Denae and Smith, Justin and Guo, Philip J and Parnin, Chris},
	year = {2016},
	pages = {846--857},
}

@inproceedings{mao2013pricing,
	title = {Pricing crowdsourcing-based software development tasks},
	booktitle = {2013 35th international conference on software engineering ({ICSE})},
	author = {Mao, Ke and Yang, Ye and Li, Mingshu and Harman, Mark},
	year = {2013},
	note = {tex.organization: IEEE},
	pages = {1205--1208},
}

@article{valett_summary_1989,
	title = {A summary of software measurement experiences in the software engineering laboratory},
	volume = {9},
	number = {2},
	journal = {Journal of Systems and Software},
	author = {Valett, Jon D and McGarry, Frank E},
	year = {1989},
	note = {Publisher: Elsevier},
	pages = {137--148},
}

@inproceedings{hulkko2005multiple,
	title = {A multiple case study on the impact of pair programming on product quality},
	booktitle = {Proceedings of the 27th international conference on {Software} engineering},
	author = {Hulkko, Hanna and Abrahamsson, Pekka},
	year = {2005},
	pages = {495--504},
}

@article{menzies_software_2013,
	title = {Software {Analytics}: {So} {What}?},
	volume = {30},
	issn = {0740-7459},
	shorttitle = {Software {Analytics}},
	url = {http://ieeexplore.ieee.org/document/6547619/},
	doi = {10.1109/MS.2013.86},
	language = {en},
	number = {4},
	urldate = {2021-06-21},
	journal = {IEEE Software},
	author = {Menzies, Tim and Zimmermann, Thomas},
	month = jul,
	year = {2013},
	pages = {31--37},
}

@article{boehm_spiral_1988-1,
	title = {A spiral model of software development and enhancement},
	volume = {21},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/59/},
	doi = {10.1109/2.59},
	language = {en},
	number = {5},
	urldate = {2021-08-20},
	journal = {Computer},
	author = {Boehm, B. W.},
	month = may,
	year = {1988},
	pages = {61--72},
}

@inproceedings{royce1970managing,
	title = {Managing the development of large software systems: concepts and techniques},
	booktitle = {Proceedings of {IEEE} {WESCON}},
	author = {Royce, Winston W},
	year = {1970},
	pages = {328--338},
}

@book{yourdon2004death,
	title = {Death march},
	publisher = {Prentice Hall Professional},
	author = {Yourdon, Edward},
	year = {2004},
}

@inproceedings{boehm_view_2006,
	address = {Shanghai China},
	title = {A view of 20th and 21st century software engineering},
	isbn = {978-1-59593-375-1},
	url = {https://dl.acm.org/doi/10.1145/1134285.1134288},
	doi = {10.1145/1134285.1134288},
	abstract = {George Santayana's statement, "Those who cannot remember the past are condemned to repeat it," is only half true. The past also includes successful histories. If you haven't been made aware of them, you're often condemned not to repeat their successes.},
	language = {en},
	urldate = {2021-07-15},
	booktitle = {Proceedings of the 28th international conference on {Software} engineering},
	publisher = {ACM},
	author = {Boehm, Barry},
	month = may,
	year = {2006},
	pages = {12--29},
}

@inproceedings{perry_empirical_2000,
	title = {Empirical {Studies} of {Software} {Engineering}: {A} {Roadmap}},
	abstract = {In this article we summarize the strengths and weaknesses of empirical research in software engineering. We argue that in order to improve the current situation we must create better studies and draw more credible interpretations from them. We finally present a roadmap for this improvement, which includes a general structure for software empirical studies and concrete steps for achieving these goals: designing better studies, collecting data more effectively, and involving others in our empirical enterprises.},
	language = {en},
	booktitle = {Proceedings of the conference on {The} future of {Software} engineering},
	author = {Perry, Dewayne and Porter, Adam and Votta, Lawrence},
	year = {2000},
	pages = {11},
}

@book{demarco2013peopleware,
	title = {Peopleware: productive projects and teams},
	publisher = {Addison-Wesley},
	author = {DeMarco, Tom and Lister, Tim},
	year = {2013},
}

@misc{harman_how_1998,
	title = {How long is this going to take?},
	url = {http://www0.cs.ucl.ac.uk/staff/M.Harman/exe11.html},
	author = {Harman, Mark},
	year = {1998},
}

@article{boochHistorySoftwareEngineering2018,
	title = {The {History} of {Software} {Engineering}},
	volume = {35},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/8474489/},
	doi = {10.1109/MS.2018.3571234},
	language = {en},
	number = {5},
	urldate = {2021-05-16},
	journal = {IEEE Software},
	author = {Booch, Grady},
	month = sep,
	year = {2018},
	pages = {108--114},
}

@article{knight_should_2002,
	title = {Should software engineers be licensed?},
	volume = {45},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/581571.581601},
	doi = {10.1145/581571.581601},
	language = {en},
	number = {11},
	urldate = {2021-06-29},
	journal = {Communications of the ACM},
	author = {Knight, John C. and Leveson, Nancy G.},
	month = nov,
	year = {2002},
	pages = {87--90},
}

@article{white_acms_2002,
	title = {{ACM}'s position on the licensing of software engineers},
	volume = {45},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/581571.581602},
	doi = {10.1145/581571.581602},
	language = {en},
	number = {11},
	urldate = {2021-06-29},
	journal = {Communications of the ACM},
	author = {White, John and Simons, Barbara},
	month = nov,
	year = {2002},
	pages = {91},
}

@inproceedings{gotterbarn1997professionalization,
	title = {The professionalization of software engineering and its significance for ethics education},
	volume = {1},
	booktitle = {Proceedings frontiers in education 1997 27th annual conference. {Teaching} and learning in an era of change},
	author = {Gotterbarn, Donald},
	year = {1997},
	note = {tex.organization: IEEE},
	pages = {484--489},
}

@article{mok_review_2010,
	title = {A {Review} of the {Professionalization} of the {Software} {Industry}: {Has} it {Made} {Software} {Engineering} a {Real} {Profession}?},
	volume = {16},
	abstract = {Every industry strives to be called a “profession”, and software engineering is no exception. This paper attempts to define “profession” from three different perspectives and provides a chronological narration of the professionalization efforts of major IT bodies such as the IEEE Computer Society, Association of Computing Machinery and British Computer Society to promote software engineering from “occupation” to “profession”. The outcome of this professionalization process is then examined against the three vastly different definitions of “profession” to qualitatively gauge the success of the professionalization process.},
	language = {en},
	number = {1},
	journal = {International Journal of Information Technology},
	author = {Mok, Heng Ngee},
	year = {2010},
	pages = {16},
}

@article{landwehr_software_2017,
	title = {Software {Systems} {Engineering} programmes a capability approach},
	volume = {125},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121216302576},
	doi = {10.1016/j.jss.2016.12.016},
	language = {en},
	urldate = {2021-07-12},
	journal = {Journal of Systems and Software},
	author = {Landwehr, Carl and Ludewig, Jochen and Meersman, Robert and Parnas, David Lorge and Shoval, Peretz and Wand, Yair and Weiss, David and Weyuker, Elaine},
	month = mar,
	year = {2017},
	pages = {354--364},
}

@article{parnas_software_1999,
	title = {Software engineering programs are not computer science programs},
	volume = {16},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/805469/},
	doi = {10.1109/52.805469},
	language = {en},
	number = {6},
	urldate = {2021-06-29},
	journal = {IEEE Software},
	author = {Parnas, D.L.},
	month = dec,
	year = {1999},
	pages = {19--30},
}

@article{mcconnell1999software,
	title = {Software engineering principles},
	volume = {16},
	number = {2},
	journal = {IEEE software},
	author = {McConnell, Steve},
	year = {1999},
	note = {Publisher: IEEE Computer Society},
	pages = {6},
}

@article{feitelsonHowDevelopersChoose2020,
	title = {How {Developers} {Choose} {Names}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9018121/},
	doi = {10.1109/TSE.2020.2976920},
	abstract = {The names of variables and functions serve as implicit documentation and are instrumental for program comprehension. But choosing good meaningful names is hard. We perform a sequence of experiments in which a total of 334 subjects are required to choose names in given programming scenarios. The ﬁrst experiment shows that the probability that two developers would select the same name is low: in the 47 instances in our experiments the median probability was only 6.9\%. At the same time, given that a speciﬁc name is chosen, it is usually understood by the majority of developers. Analysis of the names given in the experiment suggests a model where naming is a (not necessarily cognizant or serial) three-step process: (1) selecting the concepts to include in the name, (2) choosing the words to represent each concept, and (3) constructing a name using these words. A followup experiment, using the same experimental setup, then checked whether using this model explicitly can improve the quality of names. The results were that names selected by subjects using the model were judged by two independent judges to be superior to names chosen in the original experiment by a ratio of two-to-one. Using the model appears to encourage the use of more concepts and longer names.},
	language = {en},
	urldate = {2021-05-25},
	journal = {IEEE Transactions on Software Engineering},
	author = {Feitelson, Dror and Mizrahi, Ayelet and Noy, Nofar and Ben Shabat, Aviad and Eliyahu, Or and Sheffer, Roy},
	year = {2020},
	pages = {1--1},
}

@article{allamanisSurveyMachineLearning2018,
	title = {A {Survey} of {Machine} {Learning} for {Big} {Code} and {Naturalness}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3212695},
	doi = {10.1145/3212695},
	abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
	language = {en},
	number = {4},
	urldate = {2021-04-18},
	journal = {ACM Computing Surveys},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	month = sep,
	year = {2018},
	pages = {1--37},
}

@article{institute1991ansi,
	title = {{ANSI}/{IEEE} standard glossary of software engineering terminology},
	author = {Electrical, Institute of and Engineers, Electronics},
	year = {1991},
}

@article{abran2004software,
	title = {Software engineering body of knowledge},
	journal = {IEEE Computer Society, Angela Burgess},
	author = {Abran, Alain and Moore, James W and Bourque, Pierre and Dupuis, Robert and Tripp, L},
	year = {2004},
}

@inproceedings{wainakhIdBenchEvaluatingSemantic2021,
	address = {Madrid, Spain},
	title = {{IdBench}: {Evaluating} {Semantic} {Representations} of {Identifier} {Names} in {Source} {Code}},
	isbn = {978-1-66540-296-5},
	shorttitle = {{IdBench}},
	url = {https://ieeexplore.ieee.org/document/9401986/},
	doi = {10.1109/ICSE43902.2021.00059},
	abstract = {Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of namebased analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., le n and s i z e , are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents IdBench, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use IdBench to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Wainakh, Yaza and Rauf, Moiz and Pradel, Michael},
	month = may,
	year = {2021},
	pages = {562--573},
}

@article{ramseyLiterateProgrammingSimplified1994,
	title = {Literate programming simplified},
	volume = {11},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/311070/},
	doi = {10.1109/52.311070},
	language = {en},
	number = {5},
	urldate = {2021-04-26},
	journal = {IEEE Software},
	author = {Ramsey, N.},
	month = sep,
	year = {1994},
	pages = {97--105},
}
@misc{Intro2TFMG,
author = {Jaeyoun Kim, Jing Li},
title = {Introducing the Model Garden for TensorFlow 2},
url = {https://blog.tensorflow.org/2020/03/introducing-model-garden-for-tensorflow-2.html},
year={2020}
}

@misc{PytorchHub,
author = {Pytorch},
title = {PyTorch Hub},
url = {https://pytorch.org/hub/},
year={2021}
}

@misc{npm,
author = {NPM},
title = {npm},
url = {https://www.npmjs.com/},
year={2022}
}

@misc{maven,
author = {Maven},
title = {Apache Maven Project},
url = {https://maven.apache.org/
},
year={2022}
}

@misc{Lower2015CVIndustry,
author = {David Lowe},
title = {The Computer Vision Industry},
url = {https://www.cs.ubc.ca/~lowe/vision.html},
year={2015}
}

@article{jordan2015ML,
  title={Machine learning: Trends, perspectives, and prospects},
  author={Jordan, Michael I and Mitchell, Tom M},
  journal={Science},
  year={2015},
}

@article{das2017MLsurvey,
  title={A survey on machine learning: concept, algorithms and applications},
  author={Das, Kajaree and Behera, Rabi Narayan},
  journal={International Journal of Innovative Research in Computer and Communication Engineering},
  year={2017}
}

@article{fitzgerald2006OSStransformation,
  title={The transformation of open source software},
  author={Fitzgerald, Brian},
  journal={MIS quarterly},
  year={2006},
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  year={2019}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv},
  year={2016}
}

@article{willemink2020preparing,
  title={Preparing medical imaging data for machine learning},
  author={Willemink, Martin J and Koszek, Wojciech A and Hardell, Cailin and Wu, Jie and Fleischmann, Dominik and Harvey, Hugh and Folio, Les R and Summers, Ronald M and Rubin, Daniel L and Lungren, Matthew P},
  journal={Radiology},
  year={2020}
}

@article{ralph2020empirical,
  title={Empirical standards for software engineering research},
  author={Ralph, Paul and Ali, Nauman bin and Baltes, Sebastian and Bianculli, Domenico and Diaz, Jessica and Dittrich, Yvonne and Ernst, Neil and Felderer, Michael and Feldt, Robert and Filieri, Antonio and others},
  journal={arXiv},
  year={2020}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{khan2018CNN4CV,
  title={A guide to convolutional neural networks for computer vision},
  author={Khan, Salman and Rahmani, Hossein and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
  journal={Synthesis Lectures on Computer Vision},
  year={2018},
}

@book{thiruvathukal2022low,
  title={Low-power Computer Vision: Improve the Efficiency of Artificial Intelligence},
  author={Thiruvathukal, George K and Lu, Yung-Hsiang and Kim, Jaeyoun and Chen, Yiran and Chen, Bo},
  year={2022},
}

@inproceedings{schelter2017automatically,
  title={Automatically tracking metadata and provenance of machine learning experiments},
  author={Schelter, Sebastian and Boese, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
  booktitle={Machine Learning Systems Workshop at NIPS},
  year={2017}
}

@inproceedings{vartak2016modeldb,
  title={ModelDB: a system for machine learning model management},
  author={Vartak, Manasi and Subramanyam, Harihar and Lee, Wei-En and Viswanathan, Srinidhi and Husnoo, Saadiyah and Madden, Samuel and Zaharia, Matei},
  booktitle={the Workshop on Human-In-the-Loop Data Analytics},
  year={2016}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={IEEE conference on computer vision and pattern recognition (CVPR)},
  year={2009},
}

@InProceedings{sweinml,
author="Lwakatare, Lucy Ellen
and Raj, Aiswarya
and Bosch, Jan
and Olsson, Helena Holmstr{\"o}m
and Crnkovic, Ivica",
editor="Kruchten, Philippe
and Fraser, Steven
and Coallier, Fran{\c{c}}ois",
booktitle="Agile Processes in Software Engineering and Extreme Programming",
year="2019",
}


@INPROCEEDINGS{8804457,  
author={Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},  
booktitle={International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},   
title={Software Engineering for Machine Learning: A Case Study},   
year={2019}}

@article{reproducescience,
author = {Matthew Hutson},
title = {Artificial intelligence faces reproducibility crisis},
journal = {Science},
year = {2018},
}


@article{flopsbench,
author = {Blott, Michaela and Halder, Lisa and Leeser, Miriam and Doyle, Linda},
title = {QuTiBench: Benchmarking Neural Networks on Heterogeneous Hardware},
year = {2019},
journal = {Journal on Emerging Technologies in Computing Systems (JETC)},
}


@InProceedings{10.1007/978-3-319-46493-0_38,
author={He, Kaiming
and Zhang, Xiangyu
and Ren, Shaoqing
and Sun, Jian},
editor={Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
title={Identity Mappings in Deep Residual Networks},
booktitle={European conference on computer vision (ECCV)},
year={2016},
}

@article{DBLP:journals/corr/Krizhevsky14,
  author    = {Alex Krizhevsky},
  title     = {One weird trick for parallelizing convolutional neural networks},
  journal={arXiv},
  year = {2014},
  url = {https://arxiv.org/abs/1404.5997}
}

@inproceedings{10.5555/2999134.2999257,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
booktitle = {International Conference on Neural Information Processing Systems (NeurIPS)},
}

@INPROCEEDINGS{7780459,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
}

@article{han,
title = {Pre-trained models: Past, present and future},
journal = {AI Open},
year = {2021},
author = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
}

@ARTICLE{Pan,
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Transfer Learning}, 
  year={2010},
  }

@INPROCEEDINGS{liuhpc,  
author={Liu, Tong and Alibhai, Shakeel and Wang, Jinzhen and Liu, Qing and He, Xubin and Wu, Chentao},  
booktitle={International Conference on Networking, Architecture and Storage (NAS)},   
title={Exploring Transfer Learning to Reduce Training Overhead of HPC Data in Machine Learning},   
year={2019}}

@misc{banna2021experience,
      title={An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors}, 
      author={Vishnu Banna and Akhil Chinnakotla and Zhengxin Yan and Anirudh Vegesana and Naveen Vivek and Kruthi Krishnappa and Wenxin Jiang and Yung-Hsiang Lu and George K. Thiruvathukal and James C. Davis},
      year={2021},
      journal={arXiv},
}

@inproceedings{torchvision,
author = {Marcel, S\'{e}bastien and Rodriguez, Yann},
title = {Torchvision the Machine-Vision Package of Torch},
booktitle={ACM international conference on Multimedia},
year = {2010},
}

@book{thiruvathukal2022lpcv,
  title={Low-power Computer Vision: Improve the Efficiency of Artificial Intelligence},
  author={Thiruvathukal, George K and Lu, Yung-Hsiang and Kim, Jaeyoun and Chen, Yiran and Chen, Bo},
  year={2022},
}

@misc{tensorflowmodelgarden2020,
  author = {Hongkun Yu and Chen Chen and Xianzhi Du and Yeqing Li and
            Abdullah Rashwan and Le Hou and Pengchong Jin and Fan Yang and
            Frederick Liu and Jaeyoun Kim and Jing Li},
  title = {{TensorFlow Model Garden}},
  howpublished = {\url{https://github.com/tensorflow/models}},
  year = {2020},
}

# GitHub Repository Citations
# =====================================

@software{tensorflowrepo,
  author = {Google Brain Team},
  title = {TensorFlow},
  year  = {2022},
  url   = {https://github.com/tensorflow/tensorflow},
}


@software{HuggingFaceUserPage,
  author = {{Hugging Face}},
  title = {Hugging Face Users},
  year  = {2022},
  url   = {https://huggingface.co/users
},
}

@software{onnxopcounter,
  author = {blacklong28},
  title = {onnx-opcounter},
  year  = {2022},
  url   = {https://github.com/blacklong28/onnx-opcounter},
}

@software{fvcore,
  author = {Facebook AI Research Team},
  title = {fvcore},
  year  = {2022},
  url   = {https://github.com/facebookresearch/fvcore},
  organization = {Facebook Research}
}

@misc{tfhub,
  author = {TensorFlow team},
  title = {TensorFlow Hub},
  year  = {2022},
  url   = {https://www.tensorflow.org/hub},
}

@misc{TensorFlowHubIntroduction,
  author = {Josh Gordon},
  title = {{Introducing TensorFlow Hub: A Library for Reusable Machine Learning Modules in TensorFlow}},
  year = {2018},
}


@misc{npmsAttribute,
  author = {{André} Cruz and {André} Duarte},
  title = {TensorFlow Hub},
  year  = {2022},
  url   = {https://npms.io/about},
}

@misc{GitHUbBadge,
  author = {GitHub},
  title = {Adding a workflow status badge},
  year  = {2022},
  url   = {https://docs.github.com/en/actions/monitoring-and-troubleshooting-workflows/adding-a-workflow-status-badge},
}

@misc{ClamAV,
author = {Cisco},
title = {{ClamAV}},
url = {https://www.clamav.net/},
year={2022}
}


@misc{HFAutoEvaluator,
	title = {Announcing {Evaluation} on the {Hub}},
	url = {https://huggingface.co/blog/eval-on-the-hub},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-12-19},
	author = {Tunstall, Lewis and Thakur, Abhishek and Thrush, Tristan and Luccioni, Sasha and Werra, Leandro von and Rajani, Nazneen and Piktus, Ola and Sanseviero, Omar and Kiela, Douwe},
}

@misc{GPT4Chan,
  author = {Jon Fingas},
  title = {AI trained on 4chan's most hateful board is just as toxic as you'd expect},
  year  = {2022},
  url   = {https://www.engadget.com/ai-bot-4chan-hate-machine-162550734.html},
}

@misc{HFDoc,
  author = {Hugging Face},
  title = {Hugging Face Documentations},
  year  = {2022},
  url   = {https://huggingface.co/docs},
}


@misc{Databricks2022ModelRegistry,
  author = {Databricks},
  title = {MLflow Model Registry},
  year  = {2022},
  url   = {https://databricks.com/product/mlflow-model-registry
},
}

@phdthesis{tschacher2016typosquatting,
  title={Typosquatting in programming language package managers},
  author={Tschacher, Nikolai Philipp},
  year={2016},
  school={Universit{\"a}t Hamburg, Fachbereich Informatik}
}

@misc{Neptune2022MLModelRegistry,
  author = {Stephen Oladele},
  title = {ML Model Registry: What It Is, Why It Matters, How to Implement It},
  year  = {2022},
  url   = {https://neptune.ai/blog/ml-model-registry},
}

@software{netron,
  author = {Lutz Roeder},
  title = {netron},
  year  = {2022},
  url   = {https://netron.app/},
}

@software{tf2onnx,
  author = {ONNX},
  title = {tf2onnx},
  year  = {2022},
  url   = {https://github.com/onnx/tensorflow-onnx},
}

@software{pytorch,
  author = {PyTorch},
  title = {pytorch},
  year  = {2022},
  url   = {https://github.com/pytorch/pytorch},
}

@software{KerasApplication,
  author = {Keras},
  title = {Keras Applications},
  year  = {2022},
  url   = {https://keras.io/api/applications/},
}

@misc{pypi,
author = {PyPI},
title = {Python Package Index},
url = {https://pypi.org},
year={2022}
}

@misc{ONNXModelZoo,
  author = {ONNX},
  title = {ONNX Model Zoo},
  year  = {2022},
  url   = {https://github.com/onnx/models},
}

@misc{NVIDIANGC,
author = {NVIDIA},
title = {{NVIDIA NGC}: {AI} {Development} {Catalog}},
url = {https://catalog.ngc.nvidia.com/},
year={2022}
}

# =====================================

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{10.1145/3382494.3410681,
author = {Serban, Alex and van der Blom, Koen and Hoos, Holger and Visser, Joost},
title = {Adoption and Effects of Software Engineering Best Practices in Machine Learning},
year = {2020},
booktitle = {International Symposium on Empirical Software Engineering and Measurement (ESEM)},
}

@inproceedings{papernot2017practical,
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
title = {Practical Black-Box Attacks against Machine Learning},
year = {2017},
booktitle = {Asia Conference on Computer and Communications Security (ASIACCS)},
}

@inproceedings{kurita2020weight,
    title = "Weight Poisoning Attacks on Pretrained Models",
    author = "Kurita, Keita  and
      Michel, Paul  and
      Neubig, Graham",
    booktitle = "Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{pham,
author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
title = {Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance},
year = {2020},
booktitle = {International Conference on Automated Software Engineering (ASE)},
}

@INPROCEEDINGS{park,
  author={Shams, Shayan and Platania, Richard and Lee, Kisung and Park, Seung-Jong},
  booktitle={International Conference on Distributed Computing Systems (ICDCS)}, 
  title={Evaluation of Deep Learning Frameworks Over Different HPC Architectures}, 
  year={2017},
}

@INPROCEEDINGS{liu,
  author={Liu, Ling and Wu, Yanzhao and Wei, Wenqi and Cao, Wenqi and Sahin, Semih and Zhang, Qi},
  booktitle={2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)}, 
  title={Benchmarking Deep Learning Frameworks: Design Considerations, Metrics and Beyond}, 
  year={2018},
}

@article{whatsinastar,
author = {Borges, Hudson and Valente, Marco},
year = {2018},
title = {What’s in a GitHub Star? Understanding Repository Starring Practices in a Social Coding Platform},
journal = {Journal of Systems and Software},
}

@article{DBLP:journals/corr/HowardZCKWWAA17,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017},
  url       = {http://arxiv.org/abs/1704.04861},
}

# THE ABOVE CITATIONS ARE GOOD AND HAVE BEEN REVISED

@article{cowley_framework_2022,
        title = {A framework for rigorous evaluation of human performance in human and machine learning comparison studies},
        number = {1},
        journal = {Scientific Reports},
        author = {Cowley, Hannah P. and Natter, Mandy and Gray-Roncal, Karla and Rhodes, Rebecca E. and Johnson, Erik C. and Drenkow, Nathan and Shead, Timothy M. and Chance, Frances S. and Wester, Brock and Gray-Roncal, William},
        year = {2022},
}


@article{liu_comparison_2019,
        title = {A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis},
        journal={The lancet digital health},
        author = {Liu, Xiaoxuan and Faes, Livia and Kale, Aditya U and Wagner, Siegfried K and Fu, Dun Jack and Bruynseels, Alice and Mahendiran, Thushika and Moraes, Gabriella and Shamdas, Mohith and Kern, Christoph and Ledsam, Joseph R and Schmid, Martin K and Balaskas, Konstantinos and Topol, Eric J and Bachmann, Lucas M and Keane, Pearse A and Denniston, Alastair K},
        date = {2019},
}

@inproceedings{kovalev,
author = {Kovalev, Vassili and Kalinovsky, Alexander and Kovalev, Sergey},
year = {2016},
month = {10},
pages = {},
title = {Deep Learning with Theano, Torch, Caffe, TensorFlow, and Deeplearning4J: Which One Is the Best in Speed and Accuracy?}
}

@inproceedings{hinton,
author = {Geoffrey E Hinton and Simon Osindero and Yee-Whye Teh},
year = {2006},
title = {A fast learning algorithm for deep belief nets},
booktitle = {Neural computation}
}

@inproceedings{kurakin,
author = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio},
year = {2016},
title = {Adversarial machine learning at scale},
journal = {arXiv},
url = {https://arxiv.org/abs/1611.01236}
}

@INPROCEEDINGS{phamdeviate,
  author={Pham, Hung Viet and Kim, Mijung and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
  booktitle={International Conference on Automated Software Engineering (ASE)}, 
  title={DEVIATE: A Deep Learning Variance Testing Framework}, 
  year={2021}}

@Inbook{Thrun1998,
author="Thrun, Sebastian
and Pratt, Lorien",
title="Learning to Learn: Introduction and Overview",
bookTitle="Learning to Learn",
year="1998",
}



@misc{goodfellow2015explaining,
      title={Explaining and Harnessing Adversarial Examples}, 
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}




@techreport{gu2019badnets,
  doi = {10.48550/ARXIV.1708.06733},
  url = {https://arxiv.org/abs/1708.06733},
  author = {Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  keywords = {Cryptography and Security (cs.CR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  institution = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{wangrisk,
title={Poster: Risks of Transferring Knowledge from Deep Models},
author={Bolun Wang and Yuanshun Yao and Bimal Viswanath and Haitao Zheng and Ben Y. Zhao},
year={2018}
}

@misc{you2021ranking,
      title={Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs}, 
      author={Kaichao You and Yong Liu and Jianmin Wang and Michael I. Jordan and Mingsheng Long},
      year={2021},
      eprint={2110.10545},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Conf. on Empirical Methods in Natural Language Processing",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
}



@misc{vgg16,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      month={04},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{resnetv2101,
      title={Identity Mappings in Deep Residual Networks}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2016},
      month={07},
      eprint={1603.05027},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{mobilenetv2,
      title={MobileNetV2: Inverted Residuals and Linear Bottlenecks}, 
      author={Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
      year={2019},
      eprint={1801.04381},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{inceptionv3,
      title={Rethinking the Inception Architecture for Computer Vision}, 
      author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
      year={2015},
      eprint={1512.00567},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{liang_pruning_2021,
title = {Pruning and quantization for deep neural network acceleration: A survey},
journal = {Neurocomputing},
volume = {461},
pages = {370-403},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.045},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221010894},
author = {Tailin Liang and John Glossner and Lei Wang and Shaobo Shi and Xiaotong Zhang},
keywords = {Convolutional neural network, Neural network acceleration, Neural network quantization, Neural network pruning, Low-bit mathematics},
abstract = {Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.}
}

@article{raymond1999cathedral,
  title={The cathedral and the bazaar},
  author={Raymond, Eric},
  journal={Knowledge, Technology \& Policy},
  volume={12},
  number={3},
  pages={23--49},
  year={1999},
  publisher={Springer}
}

@inproceedings{abdalkareem2017developers,
author = {Abdalkareem, Rabe and Nourry, Olivier and Wehaibi, Sultan and Mujahid, Suhaib and Shihab, Emad},
title = {Why Do Developers Use Trivial Packages? An Empirical Case Study on Npm},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106267},
doi = {10.1145/3106237.3106267},
abstract = {Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call `trivial packages'. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages. Therefore, in this paper, we mine more than 230,000 npm packages and 38,000 JavaScript applications in order to study the prevalence of trivial packages. We found that trivial packages are common and are increasing in popularity, making up 16.8\% of the studied npm packages. We performed a survey with 88 Node.js developers who use trivial packages to understand the reasons and drawbacks of their use. Our survey revealed that trivial packages are used because they are perceived to be well implemented and tested pieces of code. However, developers are concerned about maintaining and the risks of breakages due to the extra dependencies trivial packages introduce. To objectively verify the survey results, we empirically validate the most cited reason and drawback and find that, contrary to developers' beliefs, only 45.2\% of trivial packages even have tests. However, trivial packages appear to be `deployment tested' and to have similar test, usage and community interest as non-trivial packages. On the other hand, we found that 11.5\% of the studied trivial packages have more than 20 dependencies. Hence, developers should be careful about which trivial packages they decide to use.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {385–395},
numpages = {11},
keywords = {Empirical Studies, Node.js, Code Reuse, JavaScript},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@misc{virustotal,
	title = {Virus Total},
	url = {https://www.virustotal.com},
	urldate = {2022-09-01},
	file = {VirusTotal - Home:C\:\\Users\\nicho\\Zotero\\storage\\FPAF6RKI\\upload.html:text/html},
}

@inproceedings{commonvoice:2020,
  author = {Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G.},
  title = {Common Voice: A Massively-Multilingual Speech Corpus},
  booktitle = {Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)},
  pages = {4211--4215},
  year = 2020
}


@misc{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	shorttitle = {wav2vec 2.0},
	url = {http://arxiv.org/abs/2006.11477},
	doi = {10.48550/arXiv.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2022-09-02},
	publisher = {arXiv},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv:2006.11477 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\nicho\\Zotero\\storage\\9YSK534A\\Baevski et al. - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learn.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nicho\\Zotero\\storage\\D935XKB5\\2006.html:text/html},
}

@book{saldana2011fundamentals,
  title={Fundamentals of qualitative research},
  author={Saldana, Johnny},
  year={2011},
  publisher={Oxford University Press}
}


@misc{registries_nine_2020-2,
	title = {Nine {Best} {Practices} for {Research} {Software} {Registries} and {Repositories}: {A} {Concise} {Guide}},
	shorttitle = {Nine {Best} {Practices} for {Research} {Software} {Registries} and {Repositories}},
	url = {http://arxiv.org/abs/2012.13117},
	abstract = {Scientific software registries and repositories serve various roles in their respective disciplines. These resources improve software discoverability and research transparency, provide information for software citations, and foster preservation of computational methods that might otherwise be lost over time, thereby supporting research reproducibility and replicability. However, developing these resources takes effort, and few guidelines are available to help prospective creators of registries and repositories. To address this need, we present a set of nine best practices that can help managers define the scope, practices, and rules that govern individual registries and repositories. These best practices were distilled from the experiences of the creators of existing resources, convened by a Task Force of the FORCE11 Software Citation Implementation Working Group during the years 2019-2020. We believe that putting in place specific policies such as those presented here will help scientific software registries and repositories better serve their users and their disciplines.},
	language = {en},
	urldate = {2022-09-02},
	publisher = {arXiv},
	author = {Monteil, Alain et al.},
	month = dec,
	year = {2020},
	note = {arXiv:2012.13117 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Digital Libraries},
	file = {Registries et al. - 2020 - Nine Best Practices for Research Software Registri.pdf:C\:\\Users\\nicho\\Zotero\\storage\\JMWN79V3\\Registries et al. - 2020 - Nine Best Practices for Research Software Registri.pdf:application/pdf},
}

@techreport{JRC119336,
	number = {KJ-NA-30040-EN-N},
	issn = {1831-9424 (online)},
	year = {2020},
	author = {Hamon, Ronan and Junklewitz, Henrik and Sanchez Martin, Jose Ignacio},
	isbn = {978-92-76-14660-5},
    journal = {European Commission: Joint Research Centre},
	publisher = {Publications Office of the European Union},
	title = {Robustness and Explainability of Artificial Intelligence},
	url = {http://dx.doi.org/10.2760/57493},
	doi = {10.2760/57493}
}


@article{liu_comparison_2019-2,
	title = {A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis},
	volume = {1},
	issn = {25897500},
	shorttitle = {A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589750019301232},
	doi = {10.1016/S2589-7500(19)30123-2},
	abstract = {Background Deep learning offers considerable promise for medical diagnostics. We aimed to evaluate the diagnostic accuracy of deep learning algorithms versus health-care professionals in classifying diseases using medical imaging.},
	language = {en},
	number = {6},
	urldate = {2022-09-02},
	journal = {The Lancet Digital Health},
	author = {Liu, Xiaoxuan and Faes, Livia and Kale, Aditya U and Wagner, Siegfried K and Fu, Dun Jack and Bruynseels, Alice and Mahendiran, Thushika and Moraes, Gabriella and Shamdas, Mohith and Kern, Christoph and Ledsam, Joseph R and Schmid, Martin K and Balaskas, Konstantinos and Topol, Eric J and Bachmann, Lucas M and Keane, Pearse A and Denniston, Alastair K},
	month = oct,
	year = {2019},
	pages = {e271--e297},
	file = {Liu et al. - 2019 - A comparison of deep learning performance against .pdf:C\:\\Users\\nicho\\Zotero\\storage\\AGGKJ25N\\Liu et al. - 2019 - A comparison of deep learning performance against .pdf:application/pdf},
}


@inproceedings{allamanis_convolutional_2016,
	title = {A {Convolutional} {Attention} {Network} for {Extreme} {Summarization} of {Source} {Code}},
	url = {https://proceedings.mlr.press/v48/allamanis16.html},
	abstract = {Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.},
	language = {en},
	urldate = {2022-12-13},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {2091--2100},
	file = {Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\R63PSMCD\\Allamanis et al. - 2016 - A Convolutional Attention Network for Extreme Summ.pdf:application/pdf},
}

@inproceedings{gousios_exploratory_2014,
	address = {New York, NY, USA},
	series = {{ICSE} 2014},
	title = {An exploratory study of the pull-based software development model},
	isbn = {978-1-4503-2756-5},
	url = {https://doi.org/10.1145/2568225.2568260},
	doi = {10.1145/2568225.2568260},
	abstract = {The advent of distributed version control systems has led to the development of a new paradigm for distributed software development; instead of pushing changes to a central repository, developers pull them from other repositories and merge them locally. Various code hosting sites, notably Github, have tapped on the opportunity to facilitate pull-based development by offering workflow support tools, such as code reviewing systems and integrated issue trackers. In this work, we explore how pull-based software development works, first on the GHTorrent corpus and then on a carefully selected sample of 291 projects. We find that the pull request model offers fast turnaround, increased opportunities for community engagement and decreased time to incorporate contributions. We show that a relatively small number of factors affect both the decision to merge a pull request and the time to process it. We also examine the reasons for pull request rejection and find that technical ones are only a small minority.},
	urldate = {2022-12-13},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Gousios, Georgios and Pinzger, Martin and Deursen, Arie van},
	month = may,
	year = {2014},
	keywords = {distributed software development, empirical software engineering, pull request, Pull-based development},
	pages = {345--355},
	file = {Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\BL77TULR\\Gousios et al. - 2014 - An exploratory study of the pull-based software de.pdf:application/pdf},
}

@inproceedings{sarker_socio-technical_2019,
	title = {Socio-{Technical} {Work}-{Rate} {Increase} {Associates} {With} {Changes} in {Work} {Patterns} in {Online} {Projects}},
	doi = {10.1109/ICSE.2019.00099},
	abstract = {Software developers work on a variety of tasks ranging from the technical, e.g., writing code, to the social, e.g., participating in issue resolution discussions. The amount of work developers perform per week (their work-rate) also varies and depends on project needs and developer schedules. Prior work has shown that while moderate levels of increased technical work and multitasking lead to higher productivity, beyond a certain threshold, they can lead to lowered performance. Here, we study how increases in the short-term work-rate along both the technical and social dimensions are associated with changes in developers' work patterns, in particular communication sentiment, technical productivity, and social productivity. We surveyed active and prolific developers on GitHub to understand the causes and impacts of increased work-rates. Guided by the responses, we developed regression models to study how communication and committing patterns change with increased work-rates and fit those models to large-scale data gathered from traces left by thousands of GitHub developers. From our survey and models, we find that most developers do experience work-rate-increase-related changes in behavior. Most notably, our models show that there is a sizable effect when developers comment much more than their average: the negative sentiment in their comments increases, suggesting an increased level of stress. Our models also show that committing patterns do not change with increased commenting, and vice versa, suggesting that technical and social activities tend not to be multitasked.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Sarker, Farhana and Vasilescu, Bogdan and Blincoe, Kelly and Filkov, Vladimir},
	month = may,
	year = {2019},
	note = {ISSN: 1558-1225},
	keywords = {collaboration, comments, commits, Data models, discussions, empirical analysis, focus switching, GitHub, issues, multitasking, Multitasking, online projects, open source software, Productivity, pull requests, recommendations, regression modeling, repositories, sentiment, social activities, social coding, socio technical, Software, software developers, software engineering, Stress, survey, Switches, Task analysis, team members, teams, technical activities, work patterns, work rate increase, work related stress},
	pages = {936--947},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\7KFVS8PQ\\Sarker et al. - 2019 - Socio-Technical Work-Rate Increase Associates With.pdf:application/pdf},
}

@article{hejderup_can_2022,
	title = {Can we trust tests to automate dependency updates? {A} case study of {Java} {Projects}},
	volume = {183},
	issn = {0164-1212},
	shorttitle = {Can we trust tests to automate dependency updates?},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121221001941},
	doi = {10.1016/j.jss.2021.111097},
	abstract = {Developers are increasingly using services such as Dependabot to automate dependency updates. However, recent research has shown that developers perceive such services as unreliable, as they heavily rely on test coverage to detect conflicts in updates. To understand the prevalence of tests exercising dependencies, we calculate the test coverage of direct and indirect uses of dependencies in 521 well-tested Java projects. We find that tests only cover 58\% of direct and 21\% of transitive dependency calls. By creating 1,122,420 artificial updates with simple faults covering all dependency usages in 262 projects, we measure the effectiveness of test suites in detecting semantic faults in dependencies; we find that tests can only detect 47\% of direct and 35\% of indirect artificial faults on average. To increase reliability, we investigate the use of change impact analysis as a means of reducing false negatives; on average, our tool can uncover 74\% of injected faults in direct dependencies and 64\% for transitive dependencies, nearly two times more than test suites. We then apply our tool in 22 real-world dependency updates, where it identifies three semantically conflicting cases and three cases of unused dependencies that tests were unable to detect. Our findings indicate that the combination of static and dynamic analysis should be a requirement for future dependency updating systems.},
	language = {en},
	urldate = {2022-12-13},
	journal = {Journal of Systems and Software},
	author = {Hejderup, Joseph and Gousios, Georgios},
	month = jan,
	year = {2022},
	keywords = {Dependency management, Library updates, Package management, Semantic versioning, Software migration},
	pages = {111097},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\BSEAG8YX\\Hejderup and Gousios - 2022 - Can we trust tests to automate dependency updates.pdf:application/pdf},
}

@article{you_ranking_nodate,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain under-exploited —practitioners usually pick one PTM from the provided model hub by popularity and then ﬁne-tune the PTM to solve the target task. This na¨ıve but common practice poses two obstacles to full exploitation of pre-trained model hubs: ﬁrst, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively ﬁne-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-ﬁtting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks before ﬁne-tuning. (2) The best ranked PTM can either be ﬁne-tuned and deployed if we have no preference for the model’s architecture or the target PTM can be tuned by the top K ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as B-Tuning, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	language = {en},
	author = {You, Kaichao},
	file = {You - Ranking and Tuning Pre-trained Models A New Parad.pdf:C\:\\Users\\nicho\\Zotero\\storage\\QGHJNRWW\\You - Ranking and Tuning Pre-trained Models A New Parad.pdf:application/pdf},
}

@book{ritchie_qualitative_2013,
	title = {Qualitative {Research} {Practice}: {A} {Guide} for {Social} {Science} {Students} and {Researchers}},
	isbn = {978-1-4462-9620-2},
	shorttitle = {Qualitative {Research} {Practice}},
	abstract = {Why use qualitative methods? What kinds of questions can qualitative methods help you answer? How do you actually do rigorous and reflective qualitative research in the real world? Written by a team of leading researchers associated with NatCen Social Research (the National Centre for Social Research) this textbook leads students and researchers through the entire process of qualitative research from beginning to end - moving through design, sampling, data collection, analysis and reporting. In this fully revised second edition you will find: A practical account of how to carry out qualitative research which recognises a range of current approaches and applications A brand new chapter on ethics A brand new chapter on observational research Updated advice on using software when analysing your qualitative data New case studies which illustrate issues you may encounter and how problems have been tackled by other researchers. This book is an ideal guide for students, practitioners and researchers faced with the challenges of doing qualitative research in both applied and academic settings in messy real-life contexts.},
	language = {en},
	publisher = {SAGE},
	author = {Ritchie, Jane and Lewis, Jane and Lewis, Professor of Social Policy Jane and Nicholls, Carol McNaughton and Ormston, Rachel},
	month = nov,
	year = {2013},
	note = {Google-Books-ID: EQSIAwAAQBAJ},
	keywords = {Reference / Research},
}

@inproceedings{paule_vulnerabilities_2019,
	title = {Vulnerabilities in {Continuous} {Delivery} {Pipelines}? {A} {Case} {Study}},
	shorttitle = {Vulnerabilities in {Continuous} {Delivery} {Pipelines}?},
	doi = {10.1109/ICSA-C.2019.00026},
	abstract = {More and more companies are in the process of adopting modern continuous software development practices and approaches like continuous integration (CI), continuous delivery (CD), or DevOps. These approaches can support companies in order to increase the development speed, the frequency of product increments, and the time to market. To be able to get these advantages, especially the tooling and infrastructure need to be reliable and secure. In case CI/CD is compromised or even unavailable, all mentioned advantages are at stake. Potentially, this could also even hinder the forthcoming of the software development. Therefore, our goal was to identify which vulnerabilities are present in industry CD pipelines and how they can be detected. In this paper, we present our results of an industry case study which includes a qualitative survey of agile project teams regarding the awareness of security in CI/CD, the analysis and abstraction of two CD pipelines, and a threat analysis based on the deducted CD pipeline to identify vulnerabilities. In this case study, we found that the team members that work with the CD pipeline in different roles do not have a strong security background but are aware of security attributes in general. Furthermore, two CD pipelines from industry projects were analyzed using the STRIDE threat analysis approach. In total, we identified 22 vulnerabilities that have been confirmed by the project teams.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Software} {Architecture} {Companion} ({ICSA}-{C})},
	author = {Paule, Christina and Düllmann, Thomas F. and Van Hoorn, André},
	month = mar,
	year = {2019},
	keywords = {Companies, Industries, Pipelines, Security, Servers, Software, Tools},
	pages = {102--108},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\TCDTZI5S\\Paule et al. - 2019 - Vulnerabilities in Continuous Delivery Pipelines .pdf:application/pdf},
}

@inproceedings{ihita_security_2021,
	title = {Security {Analysis} of {Large} {Scale} {IoT} {Network} for {Pollution} {Monitoring} in {Urban} {India}},
	doi = {10.1109/WF-IoT51360.2021.9595688},
	abstract = {The surge in the development and adoption of Internet of Things (IoT)-enabled smart city technologies has brought with it a diverse set of critical security challenges. In this paper, protocol and network security threats pertaining to a large-scale IoT-enabled pollution monitoring sensor network, AirIoT, deployed in and around an educational campus in the Indian city of Hyderabad, have been explored. Using the STRIDE methodology, the paper assesses various threat vectors for the deployment. As solutions, the paper proposes an approach for end-to-end encryption, protocol and dashboard security, and a proof of concept deauthentication detector. This baseline threat analysis and risk assessment can provide a foundation for securing Wi-Fi and mobile network-based large-scale IoT deployments.},
	booktitle = {2021 {IEEE} 7th {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	author = {Ihita, G.V. and Viswanadh, K.S. and Sudhansh, Y. and Chaudhari, S. and Gaur, S.},
	month = jun,
	year = {2021},
	keywords = {Internet of Things, IoT security, Large-scale deployment, Network security, Pollution, Pollution monitoring, Protocols, Risk management, Surges, Threat analysis, Threat assessment},
	pages = {283--288},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\nicho\\Zotero\\storage\\RM9JY8KH\\9595688.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\VS8AYAY2\\Ihita et al. - 2021 - Security Analysis of Large Scale IoT Network for P.pdf:application/pdf},
}

@inproceedings{wittern_look_2016,
	address = {New York, NY, USA},
	series = {{MSR} '16},
	title = {A look at the dynamics of the {JavaScript} package ecosystem},
	isbn = {978-1-4503-4186-8},
	url = {https://doi.org/10.1145/2901739.2901743},
	doi = {10.1145/2901739.2901743},
	abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node. js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem's growth and activity, into conflicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
	urldate = {2022-12-13},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
	month = may,
	year = {2016},
	keywords = {JavaScript, Node.js, software ecosystem analysis},
	pages = {351--361},
	file = {Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\NWKTZ3EH\\Wittern et al. - 2016 - A look at the dynamics of the JavaScript package e.pdf:application/pdf},
}

@inproceedings{decan_impact_2018,
	address = {New York, NY, USA},
	series = {{MSR} '18},
	title = {On the impact of security vulnerabilities in the npm package dependency network},
	isbn = {978-1-4503-5716-6},
	url = {https://doi.org/10.1145/3196398.3196401},
	doi = {10.1145/3196398.3196401},
	abstract = {Security vulnerabilities are among the most pressing problems in open source software package libraries. It may take a long time to discover and fix vulnerabilities in packages. In addition, vulnerabilities may propagate to dependent packages, making them vulnerable too. This paper presents an empirical study of nearly 400 security reports over a 6-year period in the npm dependency network containing over 610k JavaScript packages. Taking into account the severity of vulnerabilities, we analyse how and when these vulnerabilities are discovered and fixed, and to which extent they affect other packages in the packaging ecosystem in presence of dependency constraints. We report our findings and provide guidelines for package maintainers and tool developers to improve the process of dealing with security issues.},
	urldate = {2022-12-13},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Decan, Alexandre and Mens, Tom and Constantinou, Eleni},
	month = may,
	year = {2018},
	keywords = {dependency network, security vulnerability, semantic versioning, software ecosystem, software repository mining},
	pages = {181--191},
	file = {Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\6FLZZTDC\\Decan et al. - 2018 - On the impact of security vulnerabilities in the n.pdf:application/pdf},
}

@inproceedings{davis_why_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Why aren’t regular expressions a lingua franca? an empirical study on the re-use and portability of regular expressions},
	isbn = {978-1-4503-5572-8},
	shorttitle = {Why aren’t regular expressions a lingua franca?},
	url = {https://doi.org/10.1145/3338906.3338909},
	doi = {10.1145/3338906.3338909},
	abstract = {This paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics? In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language.We experimentally evaluated the riskiness of this practice using a novel regex corpus — 537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences. We report that developers’ belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15\% exhibit semantic differences across languages and 10\% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules.},
	urldate = {2022-12-13},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Davis, James C. and Michael IV, Louis G. and Coghlan, Christy A. and Servant, Francisco and Lee, Dongyoon},
	month = aug,
	year = {2019},
	keywords = {developer perceptions, empirical software engineering, mining software repositories, portability, re-use, ReDoS, Regular expressions},
	pages = {443--454},
	file = {Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\CSTC82E2\\Davis et al. - 2019 - Why aren’t regular expressions a lingua franca an.pdf:application/pdf},
}

@inproceedings{davis_testing_2019,
	title = {Testing {Regex} {Generalizability} {And} {Its} {Implications}: {A} {Large}-{Scale} {Many}-{Language} {Measurement} {Study}},
	shorttitle = {Testing {Regex} {Generalizability} {And} {Its} {Implications}},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Davis, James C and Moyer, Daniel and Kazerouni, Ayaan M and Lee, Dongyoon},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Automata, Computer languages, Data driven design, Empirical software engineering, Engines, Measurement, Methods, Regular expressions, Software, Static analysis, Tools},
	pages = {427--439},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\UT7GUGCL\\Davis et al. - 2019 - Testing Regex Generalizability And Its Implication.pdf:application/pdf},
}

@inproceedings{thomeczek_security_2015,
	address = {Paris, France},
	title = {Security {Analysis} of {Linux} {Kernel} {Features} for {Embedded} {Software} {Systems} in {Vehicles}},
	url = {https://hal.archives-ouvertes.fr/hal-01193025},
	abstract = {This paper describes different safety and security mechanisms implemented in the Linux kernel to prevent and protect against accidental or malicious misbehaviour in user applications. First, we present a generic system model for user applications with different levels of criticality and deterministic behaviour. From this, a theoretical model for failure modes and attack scenarios on the stability of the operating system and concurrently executed user applications is derived. Then, technologies in the Linux kernel to counter the identified failure modes and attack scenarios are examined and compared. Current work in progress is to implement requirement-based tests for these security measures and assess their effectiveness, efficiency and limits.},
	urldate = {2022-12-13},
	booktitle = {{CARS} 2015 - {Critical} {Automotive} applications: {Robustness} \& {Safety}},
	author = {Thomeczek, Ludwig},
	month = sep,
	year = {2015},
	keywords = {embedded, Linux, operating systems, safety, security, vehicle},
}

@inproceedings{danielis_iso-compliant_2020,
	title = {An {ISO}-{Compliant} {Test} {Procedure} for {Technical} {Risk} {Analyses} of {IoT} {Systems} {Based} on {STRIDE}},
	doi = {10.1109/COMPSAC48688.2020.0-203},
	abstract = {The rising number of IoT systems deployed in production increases the risk of becoming a victim of costly attacks. The complex architecture of such systems increases their attack surface making the systems more vulnerable by attacks that can even paralyze production. For 2020, it is estimated that more than 30 billion IoT devices are in use worldwide; of which more than 5.8 billion in the industrial sector. It is therefore imperative today to have a threat modeling tool that examines all system components and assigns them a risk of being exposed. This paper proposes a novel ISO-compliant test procedure for the technical risk analysis of IoT systems since state-of-the-art methods are either not ISO-compliant or are not suitable for technical risk analyses. It is based on the generic threat model Microsoft STRIDE, which has been developed to consider and evaluate all system components. The test procedure is able to identify vulnerabilities and assigns a risk to them. It complies with ISO/IEC 27001 and implies ISO/IEC 27005 and ISO 31000. The threats are stored in a threat database and linked to countermeasures. The designed tool implementing the novel test procedure automatically proposes countermeasures for certain attacks to the user. The tool uses a learning database in which new insights can be entered during each risk analysis so that the tool is improved with each use. As proof of concept, an IoT system is analyzed for its risks and the high achievable accuracy of the proposed risk assessment is shown.},
	booktitle = {2020 {IEEE} 44th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Danielis, Peter and Beckmann, Moritz and Skodzik, Jan},
	month = jul,
	year = {2020},
	note = {ISSN: 0730-3157},
	keywords = {Companies, IEC Standards, ISO Standards, risk assessment, ISO compliance, STRIDE, IoT, compliance management, Risk management, Security, Tools},
	pages = {499--504},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\MB89E6YR\\Danielis et al. - 2020 - An ISO-Compliant Test Procedure for Technical Risk.pdf:application/pdf},
}

@inproceedings{khan_stride-based_2017,
	title = {{STRIDE}-based threat modeling for cyber-physical systems},
	doi = {10.1109/ISGTEurope.2017.8260283},
	abstract = {Critical infrastructures and industrial control systems are complex Cyber-Physical Systems (CPS). To ensure reliable operations of such systems, comprehensive threat modeling during system design and validation is of paramount significance. Previous works in literature mostly focus on safety, risks and hazards in CPS but lack effective threat modeling necessary to eliminate cyber vulnerabilities. Further, impact of cyber attacks on physical processes is not fully understood. This paper presents a comprehensive threat modeling framework for CPS using STRIDE, a systematic approach for ensuring system security at the component level. This paper first devises a feasible and effective methodology for applying STRIDE and then demonstrates it against a real synchrophasor-based synchronous islanding testbed in the laboratory. It investigates (i) what threat types could emerge in each system component based on the security properties lacking, and (ii) how a vulnerability in a system component risks the entire system security. The paper identifies that STRIDE is a light-weight and effective threat modeling methodology for CPS that simplifies the task for security analysts to identify vulnerabilities and plan appropriate component level security measures at the system design stage.},
	booktitle = {2017 {IEEE} {PES} {Innovative} {Smart} {Grid} {Technologies} {Conference} {Europe} ({ISGT}-{Europe})},
	author = {Khan, Rafiullah and McLaughlin, Kieran and Laverty, David and Sezer, Sakir},
	month = sep,
	year = {2017},
	keywords = {Circuit breakers, Cyber physical systems, cyber security, Microgrids, Phasor measurement units, Safety, smart grid, STRIDE, Synchronization, synchrophasors, threat modeling},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\nicho\\Zotero\\storage\\WB9FHL5F\\Khan et al. - 2017 - STRIDE-based threat modeling for cyber-physical sy.pdf:application/pdf},
}

@misc{kexugit_uncover_nodate,
	title = {Uncover {Security} {Design} {Flaws} {Using} {The} {STRIDE} {Approach}},
	url = {https://learn.microsoft.com/en-us/archive/msdn-magazine/2006/november/uncover-security-design-flaws-using-the-stride-approach},
	language = {en-us},
	urldate = {2022-12-13},
	author = {kexugit},
}

@misc{OWASPThreatModelingProcess,
	title = {Threat {Modeling} {Process} {\textbar} {OWASP} {Foundation}},
	url = {https://owasp.org/www-community/Threat_Modeling_Process},
	abstract = {Threat Modeling Process on the main website for The OWASP Foundation. OWASP is a nonprofit foundation that works to improve the security of software.},
	language = {en},
	urldate = {2022-12-13},
}

@incollection{ritchie2002qualitative,
  title={Qualitative data analysis for applied policy research},
  author={Ritchie, Jane and Spencer, Liz},
  booktitle={Analyzing qualitative data},
  pages={187--208},
  year={2002},
  publisher={Routledge}
}

@book{ritchie2013qualitative,
  title={Qualitative research practice: A guide for social science students and researchers},
  author={Ritchie, Jane and Lewis, Jane and Nicholls, Carol McNaughton and Ormston, Rachel and others},
  year={2013},
  publisher={sage}
}

@article{Srivastava2008FrameworkAnalysis,
author = {Srivastava, Aashish and Thomson, Stanley},
year = {2009},
title = {Framework Analysis: A Qualitative Methodology for Applied Policy Research},
volume = {4},
journal = {Journal of Administration and Governance (JOAAG)}
}


@INPROCEEDINGS{Franch2002Quality,  
author={Franch, X. and Carvallo, J.P.},  
booktitle={Proceedings IEEE Joint International Conference on Requirements Engineering (RE)},   
title={A quality-model-based approach for describing and evaluating software packages},   
year={2002},  
volume={},  
number={},  
pages={104-111},  
doi={10.1109/ICRE.2002.1048512}
}

@article{MUJAHID2022111588,
title = {What are the characteristics of highly-selected packages? A case study on the npm ecosystem},
journal = {Journal of Systems and Software},
pages = {111588},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111588},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002643},
author = {Suhaib Mujahid and Rabe Abdalkareem and Emad Shihab},
keywords = {highly-selected packages, Package quality, Software ecosystem, npm},
abstract = {With the popularity of software ecosystems, the number of open source components (known as packages) has grown rapidly. Identifying high-quality and well-maintained packages from a large pool of packages to depend on is a basic and important problem, as it is beneficial for various applications, such as package recommendation and package search. However, no systematic and comprehensive work focuses on addressing this problem except in online discussions or informal literature and interviews. To fill this gap, in this paper, we conducted a mixed qualitative and quantitative analysis to understand how developers identify and select relevant open source packages. In particular, we started by surveying 118 JavaScript developers from the npm ecosystem to qualitatively understand the factors that make a package to be highly-selected within the npm ecosystem. The survey results showed that JavaScript developers believe that highly-selected packages are well-documented, receive a high number of stars on GitHub, have a large number of downloads, and do not suffer from vulnerabilities. Then, we conducted an experiment to quantitatively validate the developers’ perception of the factors that make a highly-selected package. In this analysis, we collected and mined historical data from 2,527 packages divided into highly-selected and not highly-selected packages. For each package in the dataset, we collected quantitative data to present the factors studied in the developers’ survey. Next, we used regression analysis to quantitatively investigate which of the studied factors are the most important. Our regression analysis complements our survey results about highly-selected packages. In particular, the results showed that highly-selected packages tend to be correlated by the number of downloads, stars, and how large the package’s readme file is.}
}

@inproceedings{2022JiangEmpirical,
author = {Jiang, Wenxin and Synovic, Nicholas and Sethi, Rohan and Indarapu, Aryan and Hyatt, Matt and Schorlemmer, Taylor R. and Thiruvathukal, George K. and Davis, James C.},
title = {An Empirical Study of Artifacts and Security Risks in the Pre-Trained Model Supply Chain},
year = {2022},
booktitle = {ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
keywords = {software supply chain, deep neural networks, model hubs, software reuse, empirical software engineering, machine learning},
}

@misc{TFHub,
author = {TensorFlow},
title = {TensorFlow Hub},
url = {https://www.tensorflow.org/hub},
year={2022}
}

@misc{PapersWithCode,
author = {{Meta AI Research}},
title = {Papers With Code},
url = {https://paperswithcode.com/},
year={2023}
}

@misc{AIMET,
author = {Qualcomm Innovation Center},
title = {AI Model Efficiency Toolkit (AIMET)},
url = {https://quic.github.io/aimet-pages/index.html},
year={2023}
}

@misc{PINTOModelZoo,
author = {PINTO0309},
title = {PINTO Model Zoo},
url = {https://github.com/PINTO0309/PINTO_model_zoo},
year={2023}
}

@misc{JetsonZoo,
author = {eLinux},
title = {Jetson Zoo},
url = {https://elinux.org/Jetson_Zoo},
year={2022}
}

@misc{HuggingFaceWeb1,
author = {{{Hugging Face}}},
title = {Hugging Face – The AI community building the future.},
url = {https://huggingface.co/},
year={2021}
}

@inproceedings{chard2015globus,
  title={Globus data publication as a service: Lowering barriers to reproducible science},
  author={Chard, Kyle and Pruyne, Jim and Blaiszik, Ben and {others}},
  booktitle={IEEE International Conference on e-Science},
  year={2015},
}

@inproceedings{anbalagan2009predicting,
  title={On predicting the time taken to correct bug reports in open source projects},
  author={Anbalagan, Prasanth and Vouk, Mladen},
  booktitle={International Conf. on Software Maintenance},
  year={2009},
}

@inproceedings{Jiang2022PTMReuse,
author = {Jiang, Wenxin and Synovic, Nicholas and Hyatt, Matt and Schorlemmer, Taylor R. and Sethi, Rohan and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
title = {An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry},
year = {2023},
booktitle = {ICSE},
location = {Melbourne, Australia},
}

@article{RASHEED2022106043,
title = {Explainable, trustworthy, and ethical machine learning for healthcare: A survey},
journal = {Computers in Biology and Medicine},
volume = {149},
pages = {106043},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.106043},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522007569},
author = {Khansa Rasheed and Adnan Qayyum and Mohammed Ghaly and Ala Al-Fuqaha and Adeel Razi and Junaid Qadir},
keywords = {Explainable machine learning, Interpretable machine learning, Trustworthiness, Healthcare},
abstract = {With the advent of machine learning (ML) and deep learning (DL) empowered applications for critical applications like healthcare, the questions about liability, trust, and interpretability of their outputs are raising. The black-box nature of various DL models is a roadblock to clinical utilization. Therefore, to gain the trust of clinicians and patients, we need to provide explanations about the decisions of models. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provided a comprehensive review of explainable and interpretable ML techniques for various healthcare applications. Along with highlighting security, safety, and robustness challenges that hinder the trustworthiness of ML, we also discussed the ethical issues arising because of the use of ML/DL for healthcare. We also describe how explainable and trustworthy ML can resolve all these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.}
}


@misc{ChatGPT,
    author = {OpenAI},
    title = {Introducing ChatGPT},
    year = {2022},
    howpublished = {\url{https://openai.com/blog/chatgpt}},
}

