{
    "arxiv_id": "2303.08934",
    "paper_title": "PTMTorrent: A Dataset for Mining Open-source Pre-trained Model Packages",
    "authors": [
        "Wenxin Jiang",
        "Nicholas Synovic",
        "Purvish Jajal",
        "Taylor R. Schorlemmer",
        "Arav Tewari",
        "Bhavesh Pareek",
        "George K. Thiruvathukal",
        "James C. Davis"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.SE"
    ],
    "abstract": "Due to the cost of developing and training deep learning models from scratch, machine learning engineers have begun to reuse pre-trained models (PTMs) and fine-tune them for downstream tasks. PTM registries known as \"model hubs\" support engineers in distributing and reusing deep learning models. PTM packages include pre-trained weights, documentation, model architectures, datasets, and metadata. Mining the information in PTM packages will enable the discovery of engineering phenomena and tools to support software engineers. However, accessing this information is difficult - there are many PTM registries, and both the registries and the individual packages may have rate limiting for accessing the data. We present an open-source dataset, PTMTorrent, to facilitate the evaluation and understanding of PTM packages. This paper describes the creation, structure, usage, and limitations of the dataset. The dataset includes a snapshot of 5 model hubs and a total of 15,913 PTM packages. These packages are represented in a uniform data schema for cross-hub mining. We describe prior uses of this data and suggest research opportunities for mining using our dataset. The PTMTorrent dataset (v1) is available at: https://app.globus.org/file-manager?origin_id=55e17a6e-9d8f-11ed-a2a2-8383522b48d9&origin_path=%2F~%2F. Our dataset generation tools are available on GitHub: https://doi.org/10.5281/zenodo.7570357.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08934v1"
    ],
    "publication_venue": "5 pages, 2 figures, Accepted to MSR'23"
}