

\section{Appendix}

\subsection{Preliminaries on Quantitative Susceptibility Maps (QSM) and Rim+ Lesions}

QSM is an MRI imaging technique that can measure the underlying tissue apparent magnetic susceptibility \cite{stuber2016iron,de2010quantitative}, quantifying specific biomarkers such as iron that are independent of imaging parameters and field strength \cite{deh2015reproducibility}.
The forward model of generating magnetic field from susceptibility maps with additive noise is a spatial convolutional process and can be described as the following:
\begin{equation}
    b = \chi * d + n,
    \label{eq:qsm}
\end{equation}
where $b$ is the magnetic field, $\chi$ is the tissue susceptibility, $d$ is the dipole convolution kernel, and $n$ is the additive measurement noise.
Given $b$ and $d$, QSM recovers $\chi$ from solving the ill-posed dipole inversion problem \cite{wang2015quantitative} in Eq.~\eqref{eq:qsm}.
Rim + lesions are characterized by a paramagnetic rim with iron deposited at the edge of the lesion.
QSM is sensitive to such magnetic susceptibility changes and provides consistent measurements of the susceptibility value of the rim across patients and scanners, which is beneficial for a machine learning model such as deep neural network to learn patterns of rim+ versus rim- lesions.

Recent studies have shown that patients with the presence of rim+ lesions are associated with a more severe disease course \cite{Marcille2022,absinta2019association} and there are growing interests in using these lesions as an imaging biomarker.
However, recent a few methods \cite{zhang2022qsmrim,barquero2020rimnet,lou2021fully} developed for rim+ lesion identification are not satisfactory, partially because CNNs with deeper structures or transformer based neural networks \cite{dosovitskiy2020image,liu2021swin} injected with fewer inductive biases demand very large datasets or feature reuse \cite{matsoukas2022makes}, and a method with domain-specific priors describing rims is lacking.
% more powerful only when trained through massive data sets.
% In rim+ lesion identification, former methods \cite{zhang2022qsmrim,barquero2020rimnet,lou2021fully} generalize poorly due to the high data imbalance and insufficient amount of data.
We first brief a set of methods that integrate priors with value accumulation, and then describe recent neural networks using other priors.

\subsection{Related Works}

There are quite a few classic methods involving a process of accumulating feature values, and examples include not limited to histogram of gradients \cite{dalal2005histograms}, local binary pattern \cite{guo2010completed} and polar transformation \cite{esteves2018polar}. 
Hough transform (HT) \cite{osti4746348} and its subsequent variants or improvements are most commonly seen methods that take advantages of the value accumulation process.
% HT is originally devised for straight line detection \cite{osti4746348}, and then generalized for arbitrary shape detection \cite{ballard1981generalizing,duda1972use}.
% HT can efficiently accumulate evidence from local image features for all possible target shapes and is sensitive to the presence of only part of a shape.
The key idea is the accumulation process that maps local image features to an application-specific accumulator space such as line parameterization using polar coordinates \cite{lin2020deep,zhao2021deep}.
We will brief methods using accumulator space voting for obtaining peak responses, and then describe methods utilizes accumulator space priors with convolutions, followed by more general networks with prior information.
% and leave the methods with the accumulation by prediction in the supplementary materials.
 
\subsubsection{Accumulation by Prediction}
Leibe \emph{et al.} \cite{leibe2006interleaving} proposes a probabilistic Hough voting model for object detection, and the parameters of which are further optimized by a max-margin constraint \cite{maji2009object} for better performance. 
LV-Metric \cite{codella2008left} uses HT to segment left ventricle in cine MRI images.
Deep Voting \cite{xie2015deep} generates votes via neural networks for nucleus localization in microscopy images.
Hough-CNN \cite{milletari2017hough} utilizes Hough voting to improve the performance of MRI and ultrasound image segmentation.
The state-of-the-art performance for object detection in 3D point clouds has also been achieved by network-predicted votes \cite{qi2019deep,qi2020imvotenet}.
Memory U-Net \cite{zhang2021memory} generates Hough votes using CNNs for lesion instance segmentation.
This line of works mainly uses classical or learning models to generate votes for object detection or segmentation, leaving much room for exploiting the global prior information from the accumulator space.

\subsubsection{Accumulator Space Convolution}

Local convolution filters applied to the accumulator space leads to aggregation of structural features such as lines \cite{lin2020deep,zhao2021deep} and circles in the feature map space, which is beneficial for integrating priors into networks.
Unlike attention based methods \cite{wang2018non,zhang2021efficient}, this accumulator space convolution captures long-range information explicitly by direct prior parameterization.
% the following works further utilizes the global prior information injected by the accumulator space and apply convolution in the new space to learn structured information such as lines \cite{lin2020deep,zhao2021deep}, or circles in our work.
Lin \emph{et al.} \cite{lin2020deep} uses HT as a global prior for line parameterization to segment straight lines, and Zhao \emph{et al.} \cite{zhao2021deep} incorporates accumulator space into the loss function for improved semantic line detection.
Interestingly, semantic correspondence detection for both 3D point clouds \cite{lee2021deep} and 2D images \cite{min2019hyperpixel} have also been improved by performing convolutions in the accumulator space.
Zhao \emph{et al.} \cite{zhao20223d} combines Manhattan world assumption and latent features from CNNs using HT for 3D room layout estimation.
Our work generalizes the key step, i.e. the value accumulation along a given direction, from pioneer works, and makes possible incorporating a wide range of priors from classical image processing methods into neural networks.

\subsubsection{Network with Priors}

Training deep networks usually demands very large datasets \cite{deng2009imagenet,lin2014microsoft}, which is difficult and even impossible for resource-limited clinical applications. 
% The priors encoding domain-specific knowledge can reduce the amount of data needed for network training or improve the task performance, providing flexibility to resource-limited clinical applications.
The priors encoding domain-specific knowledge has shed light and provides flexibility to resource- and data-limited clinical applications.
The distance transformation mapping \cite{ma2020distance} and spatial information encoding \cite{liu2018intriguing} have been proven successful in developing a variety of edge-aware loss functions \cite{kervadec2019boundary,9434085,karimi2019reducing}, network layers with anatomical coordinates \cite{zhang2021all} as the prior information, and spatially covariant network weight generation \cite{zhang2023spatially}, improving the performance of medical image segmentation.
Ill-posed medical image reconstruction problems rely heavily on carefully designed regularization priors and inserting the prior knowledge of a physical equation \cite{o2017introduction} describing the inverse problem has been proven very effective.
Typical works include but not limited to using deep unfolding network \cite{hershey2014deep} to approximate the forward physical model \cite{8434321,hammernik2018learning}, inserting ADMM solver \cite{boyd2011distributed} into the network training phase for compressed sensing MRI \cite{NIPS2016_1679091c}, and tuning network weights with fidelity-imposed loss function \cite{zhang2020fidelity,jiang2020neural}, acceleration of MRI acquisition \cite{zhang2023laro}, reconstruction of brain quantitative susceptibility for oxygen extraction fraction mapping \cite{cho2022qq} and myelin water fraction mapping \cite{kim2022subsecond}.
In addition, there are other works injecting physics knowledge into learning models to improve performance for systems such as lithography \cite{zhang2016enabling,zhang2017bilinear,jiang2019fast}, thermal conductance cooling \cite{hu2022thermal}, and massive machine-type communications \cite{teng2020accumulated}.     
Polar or log polar features have also been widely used in various applications including but not limited to modulation classification \cite{teng2020accumulated}, rotation- and scale-invariant polar transformer network \cite{esteves2018polar}, general object detection \cite{xie2020polarmask,xu2019explicit,park2022eigencontours}, correspondence matching \cite{ebel2019beyond}, and cell detection \cite{schmidt2018cell} and segmentation \cite{stringer2021cellpose}.
Our work DeDA, a simple operation symmetric to the grid sampling, makes it simple and fast to integrate customized priors such as rim parametrization into neural networks.
% Typical works include but not limited to using deep unfolding network \cite{hershey2014deep} to approximate the forward physical model \cite{8434321,hammernik2018learning}, inserting ADMM solver \cite{boyd2011distributed} into the network training phase for compressed sensing MRI \cite{NIPS2016_1679091c}, tuning network weights with fidelity loss \cite{zhang2020bayesian,zhang2020fidelity,zhang2021probabilistic,zhang2021temporal}, accelerating MRI acquisition \cite{zhang2022laro}, and reconstructing brain quantitative susceptibility for oxygen extraction fraction mapping \cite{cho2022qq} and myelin water fraction mapping \cite{kim2022subsecond}. 
% Polar or log polar features have also been widely used in various applications including but not limited modulation classification \cite{teng2020accumulated}, rotation- and scale-invariant polar transformer network \cite{esteves2018polar}, general object detection \cite{xie2020polarmask,xu2019explicit,park2022eigencontours}, correspondence matching \cite{ebel2019beyond}, and cell detection \cite{schmidt2018cell} and segmentation \cite{stringer2021cellpose}.

\subsection{Experiments and Results}

\subsubsection{More Details on the Network Training}

We applied a stratified five-fold cross-validation procedure to train and validate the performance of our methods and the other methods. 
The stratified procedure was performed to balance the number of rim+ lesions in each of the five folds. 
We first grouped subjects into four groups, where the first group contained subjects with no rim + lesion, the second subjects with 1–3 rim + lesions, the third subjects with 4–6 rim + lesions, and the fourth subjects with more than 6 rim + lesions. 
The data was then randomly split into the five folds within each of these groups. 
All experiments were conducted within this stratified five-fold cross validation setting.

Our network was implemented in version 3.7 of Python with version 1.9.0 of PyTorch library \cite{paszke2019pytorch} on a computer equipped with two Nvidia Titan XP GPUs. 
Particularly, the DeDA operation in Eq.~\ref{eq:deda} was implemented in C++ with version 11.1 of CUDA. 
The Adam algorithm \cite{kingma2014adam} with an initial learning rate of $1e-4$ and a multi-step learning rate scheduler with rate halving at 50\%, 70\%, and 90\% of the total epochs, were used for training. 
A mini-batch size of 32 was used for training, and training was stopped after 50 epochs. 
We used three random seeds to train three models for each fold and the final prediction result was determined by ensembing of the logits from three models, followed by a Sigmoid function to produce probability. 
A sensitivity analysis was performed and found out that three random seeds were well in terms of the computational cost versus the performance gain as a balance of all performance metrics.

Data augmentation was performed to enrich the training dataset and improve model generalizability.
Specifically, random flipping, random affine transformation and random Gaussian blurring were used to augment our data.
For augmentation in the training set, lesions were moved to align their center of mass to the geometric center of the image patch. 
Flipping was performed on an orthogonal direction randomly chosen from the axial, coronal, or sagittal direction. 
Affine transformations were performed with a random scale ranging from 0.95 to 1.05 and a random rotation degree between -5 and 5°. 
The final transformed patch was obtained after a trilinear interpolation. 
The blurring was performed using a random-sized Gaussian filter where the kernel radius was determined by $4\sigma + 0.5$. 
The voxel size of our QSM image was $0.75 \times 0.75 \times 3$, thus for the coronal and sagittal direction, we randomly sampled $\sigma \sim \mathcal{N}(0.1,0.95)$, and for the axial direction we randomly sampled $\sigma \sim \mathcal{N}(0.03,0.3)$ for optimal performance.

\subsubsection{More Results forVariants of ResNet and Transformer}

\begin{table}[!t]

\caption{ Performance comparison for transformer networks and deeper convolutional networks.
}

\label{tab:trans}
%\vskip 0.15in
\begin{center}
\resizebox{1\columnwidth}{!}{
%\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccccccccc}
\hline
\hline
Backbone	&Pre-Conv	&Pre-Trained	&accu	&f1	&sens	&spec	&ppv	&ROC AUC	&pROC AUC	&PR AUC	\\
\hline											
NesT-Base	&	&	&0.870	&0.265	&0.554	&0.884	&0.174	&0.836	&0.246	&0.194	\\
NesT-Base	&\checkmark	&	&0.963	&0.560	&0.554	&0.981	&0.566	&0.923	&0.619	&0.518	\\
NesT-Base	&\checkmark	&\checkmark	&0.933	&0.446	&0.638	&0.946	&0.342	&0.896	&0.519	&0.406	\\
NesT-Base	&	&\checkmark	&0.869	&0.261	&0.542	&0.884	&0.172	&0.841	&0.253	&0.198	\\
NesT-Small	&	&	&0.912	&0.350	&0.559	&0.927	&0.255	&0.859	&0.359	&0.289	\\
NesT-Small	&	&	&0.959	&0.547	&0.576	&0.976	&0.520	&0.925	&0.617	&0.513	\\
NesT-Small	&	&\checkmark	&0.891	&0.316	&0.593	&0.904	&0.215	&0.843	&0.325	&0.266	\\
NesT-Small	&	&\checkmark	&0.863	&0.251	&0.542	&0.877	&0.164	&0.861	&0.248	&0.187	\\
NesT-Tiny	&	&	&0.862	&0.256	&0.559	&0.875	&0.166	&0.828	&0.237	&0.190	\\
NesT-Tiny	&\checkmark	&	&0.965	&0.565	&0.537	&0.984	&0.597	&0.920	&0.611	&0.511	\\
NesT-Tiny	&\checkmark	&\checkmark	&0.946	&0.447	&0.514	&0.965	&0.396	&0.886	&0.494	&0.391	\\
NesT-Tiny	&	&\checkmark	&0.860	&0.256	&0.565	&0.873	&0.165	&0.827	&0.239	&0.167	\\
\hline											
ViT-Base	&	&	&0.896	&0.295	&0.514	&0.913	&0.207	&0.797	&0.342	&0.264	\\
ViT-Large	&	&	&0.959	&0.545	&0.576	&0.976	&0.518	&0.915	&0.598	&0.497	\\
ViT-Huge	&	&	&0.955	&0.517	&0.565	&0.972	&0.476	&0.909	&0.575	&0.461	\\
\hline											
ResNet-18	&	&	&0.969	&0.641	&0.650	&0.983	&0.632	&0.941	&0.712	&0.612	\\
ResNet-34	&	&	&0.966	&0.634	&0.684	&0.979	&0.590	&0.945	&0.695	&0.587	\\
ResNet-50	&	&	&0.966	&0.621	&0.661	&0.979	&0.585	&0.940	&0.700	&0.584	\\
ResNet-101	&	&	&0.970	&0.639	&0.644	&0.983	&0.633	&0.944	&0.704	&0.597	\\




\hline
\hline
\end{tabular}
}
\end{center}

\end{table}

We included more results on variants of residual and transformer networks.
Two popular transformer networks ViT \cite{dosovitskiyimage} and Swin \cite{liu2021swin} were used to demonstrate the performance.
Particularly we found out that the original Swin transformer performed poorly on the rim dataset, and thus we adopted Nested Transformer (NesT) \cite{zhang2022nested} that was adapted and improved from Swin.
In addition, we further conducted an ablation study on whether to adopt a pre-convolutional (pre-conv) layer (comprises with a $3\times 3 \times 3$ convolution, batch normalization and ReLU activation) or load pre-trained weights for NesT.

As we can see from Table~\ref{tab:trans}, NesT-* with pre-conv layer outperformed their counterparts without such layer, and loading pre-trained weights for all NesT based networks were not useful for the rim+ lesion identification task. 
This can be attributed to the fact that network weights pre-trained on natural images can not be adapted trivially to the new modality adopted in this task.
Considering the difficulty of getting such labeled datasets, it is almost impossible to have domain-specific pre-trained network for this clinical application.
However, interestingly, the best performing model among NesT-* was achieved by NesT-Tiny, which is consistent with results from residual networks \cite{he2016deep} where ResNet-18 achieved the best overall performance (in terms of $F_1$, pROC AUC and PR AUC).