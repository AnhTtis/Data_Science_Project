
\section{Experiments and Results}

For fair and consistent comparison, the dataset applied in the previous work \cite{zhang2022qsmrim} was asked for and used to demonstrate the performance of the proposed DeDA-based rim parameterization DA-TR.
A total of 172 subjects were included in the dataset, and 177 lesions were identified as rim+ lesions and 3986 lesions were identified as rim- lesions, please refer to \cite{zhang2022qsmrim} for more details about the image acquisition and pre-processing.

\subsection{Comparator Methods and Implementation Details}

\textbf{Comparator Methods}:
Three methods have been developed so far for rim+ lesion identification, of which APRL \cite{lou2021fully} and RimNet \cite{barquero2020rimnet} are on phase imaging and QSMRim-Net \cite{zhang2022qsmrim} is on QSM.
In comparison with these methods, we use QSM along with T2-FLAIR images as the network inputs for RimNet and QSMRimNet, and use the QSM image to extract first-order radiomic features for APRL.
Furthermore, we also applied residual networks (ResNet) \cite{he2016deep}, vision transformer (ViT)\cite{dosovitskiy2020image}, Swin transformer \cite{liu2021swin}, and Nested transformer \cite{zhang2022nested} as backbone architecture for our application, and determined that ResNet with 18 convolution layers works the best.
Transformer-based networks with fewer inductive biases rely heavily on the use of a large training dataset or depends strongly on the feature reuse \cite{matsoukas2022makes}, as a result, these networks as well as CNNs with deeper structures are prone to overfit small datasets.
Therefore, integrating proper priors into networks is crucial for rim+ lesion identification.

\textbf{Implementation Details}:
A stratified five-fold cross-validation procedure was applied to train and validate the performance, and all experiments including ablation study were carried out within this setting.
Each lesion was cropped into patches with a fixed size of $32\times32\times8$ voxels.
Random flipping, random affine transformation and random Gaussian blurring were used to augment our data.
% Our network was implemented in version 3.7 of Python with version 1.9.0 of PyTorch library \cite{paszke2019pytorch} on a computer equipped with two Nvidia Titan XP GPUs. 
% Particularly, the DeDA operation in Eq.~\ref{eq:deda} was implemented in C++ with CUDA 11.1.
More details of the training procedure can be found out in the appendix.

\begin{figure}[!t]
	\centering
        \vspace{-1ex}
	\subfloat[Pearson's Corr]{\includegraphics[width=.26\columnwidth]{./subject-wise.pdf}
    \label{fig:subject-wise}}
	\subfloat[pROC Curves]{\includegraphics[width=.37\columnwidth]{./pROC.pdf}
    \label{fig:pROC}} 
	\subfloat[PR Curves]{\includegraphics[width=.37\columnwidth]{./PR.pdf}
    \label{fig:PR}}
    \vspace{-1ex}
	\caption{
    	The predicted count of rim + lesions from DA-TR-Net versus the expert human count is shown in (a), where points in the plot have been jittered for better visualization. 
            The pROC and PR curves for the proposed and other comparator methods are shown in (b) and (c), where AUC denotes the area under the curve.
	\label{fig:curves}
        }
        \vspace{-2ex}
\end{figure}

\begin{table}[!b]

\caption{ Results of the proposed and other methods using a stratified five-fold cross-validation scheme.
The best performing metric is bolded.
}
\vspace{-1ex}
\label{tab:overall}
%\vskip 0.15in
\begin{center}
\resizebox{1.\columnwidth}{!}{
%\setlength{\tabcolsep}{4pt}
\begin{tabular}{ lcccccccccc}
\hline
\hline
Method	&Accuracy	&$F_1$ 	&Sensitivity	&Specificity	&Precision	&ROC AUC	&pROC AUC &PR AUC & $\rho$ (95\%CI) & MSE \\
\hline
APRL \cite{lou2021fully}	&0.954	&0.538	&0.627	&0.969	&0.470	&0.940	&0.644	&0.507	&0.68 (0.59,0.75)	&3.16	\\
RimNet \cite{barquero2020rimnet}	&0.970	&0.650	&0.655	&0.984	&0.644	&0.950	&0.737	&0.659	&0.75 (0.67,0.81)	&2.41	\\
QSMRimNet \cite{zhang2022qsmrim}	&0.977	&0.711	&0.667	&0.991	&0.761	&0.939	&0.760	&0.709	&0.89 (0.86,0.92)	&1.00	\\
DA-TR-Net (Ours)	&\bf{0.980}	&\bf{0.750}	&\bf{0.712}	&\bf{0.992}	&\bf{0.792}	&\bf{0.975}	&\bf{0.837}	&\bf{0.781}	&\bf{0.93(0.90,0.95)}	&\bf{0.69}	\\



\hline
\hline
\end{tabular}
}
\end{center}
\vspace{-1ex}
\end{table}


\subsection{Results and Ablation Study}

% The Adam algorithm \cite{kingma2014adam} with an initial learning rate of $1e-4$ and a multi-step learning rate scheduler with rate halving at 50\%, 70\%, and 90\% of the total epochs, were used for training. 
% A mini-batch size of 32 was used for training, and training was stopped after 50 epochs. 
% We used three random seeds to train three models for each fold and the final prediction result was determined by ensembing of logits from three models, followed by a Sigmoid function to produce probability. 
% A sensitivity analysis was performed and found out that three random seeds were well in terms of the computational cost versus the performance gain as a balance of all performance metrics.

\textbf{Lesion-wise Results}: 
To evaluate the performance of each method and produce clinically relevant results, pROC curves with false positive rates (FPRs) in the range of $(0,0.1)$ and PR curves of the different validation folds were interpolated using piece-wise constant interpolation and averaged to show the overall performance at the lesion level. 
For each curve, AUC was computed directly from the interpolated and averaged curves. 
The binary indicators of rim+/rim- lesions were generated by thresholding the model probabilities to maximize the $F_1$ score, where $F_1=2\cdot\dfrac{precision\cdot sensitivity}{precision+sensitivity}$. 
In addition, accuracy, F1 score, sensitivity, specificity, and precision were used to characterize the performance of each method. 
Table \ref{tab:overall} and Fig. \ref{fig:curves} show the lesion-wise performance metrics of the proposed methods in comparison with the other methods. 
DA-TR-Net outperformed the other competitors in all evaluation metrics. 
With a slightly higher overall accuracy and specificity with other methods, DA-TR-Net resulted in a 5.5\%, 15.4\% and 39.4\% improvement in $F_1$ score, 10.1\%, 13.6\% and 30.0\% improvement in pROC (FPR$<0.1$) AUC, and 10.2\%, 18.5\% and 54.0\% improvement in PR AUC compared to QSMRimNet, RimNet and APRL, respectively. 

\textbf{Subject-wise Results}: We also evaluated the performance at the subject-level. 
Pearson’s correlation coefficient was used to measure the correlation model predicted count and human expert count.
Mean Squared Error (MSE) was also used to measure the averaged accuracy for the model predicted count.
Fig. \ref{fig:subject-wise} shows the scatter-plot for the predicted count v.s. the human expert count, along with the identity line, and the Pearson’s correlation coefficient ($\rho$) for DA-TR-Net was $\rho=0.93 (95\%CI:0.90,0.95)$
As can be seen from Table \ref{tab:overall}, the Pearson’s correlations and MSE for the proposed DA-TR-Net was found higher than other competitors.
This demonstrates that the performance of DA-TR-Net at the subject-level is statistically significantly higher than that of APRL, Rim-Net, and QSMRim-Net. 


\textbf{Ablation Study}:
We conducted an ablation study to investigate the effects of each component accompanied with DA-TR.
First, we examined the effects of applying the proposed DA-TR to the latent feature maps and raw images.
Second, we examined the effects of using $\mathbf{V}_u$ and $\mathbf{V}_s$, because rim+ lesions differ from rim- lesions in both gradient magnitudes and values at the edge of the lesion.
We then investigated how multi-radius rim parameterization can affect the results, as the size of rim+ lesions vary greatly with a radius from 5 to 15 among different subjects. 
Results from models $\#1$, $\#2$ and $\#4$ show that the rim parametrization DA-TR is useful for rim+ identification, and DA-TR used in the latent feature map space performs even better.
Comparing model $\#3$ and $\#4$, one can see that accumulating both gradient magnitudes and feature values is beneficial.
The consistent performance improvement from model $\#4$ to $\#5$ and from model $\#5$ to $\#6$ has demonstrated the effectiveness of applying multi-radius rim parameterization.
More results on backbone networks can be found in the appendix.

\begin{table}[!t]

\caption{ Ablation study on the effects for each component in DA-TR. 
Multiple check marks for sets of $N$ denote the union of the checked sets.
Pre-Convs denotes a convolution block with six $3\times 3 \times 3$ convolution layers.  
}

\label{tab:ablation}
%\vskip 0.15in
\begin{center}
\resizebox{1\columnwidth}{!}{
%\setlength{\tabcolsep}{4pt}
\begin{tabular}{ccccccccccc}
\hline
\hline
\#	&Pre-Convs	&$\mathbf{V}_u$	&$\mathbf{V}_s$	&$N \in \{5,7,9\}$	&$N \in \{11,13\}$	&$N \in \{15\}$	&$F_1$	&ROC AUC	&pROC AUC (FPR$<0.1$)	&PR AUC	\\

\hline
1	&$\times$	&$\times$	&$\times$	&$\times$	&$\times$	&$\times$	&0.685	&0.945	&0.753	&0.689	\\
2	&$\times$	&$\checkmark$	&$\checkmark$	&$\times$	&$\times$	&$\checkmark$	&0.701	&0.971	&0.790	&0.720	\\
3	&$\checkmark$	&$\checkmark$	&$\times$	&$\times$	&$\times$	&$\checkmark$	&0.703	&0.967	&0.795	&0.714	\\
4	&$\checkmark$	&$\checkmark$	&$\checkmark$	&$\times$	&$\times$	&$\checkmark$	&0.702	&0.976	&0.817	&0.736	\\
5	&$\checkmark$	&$\checkmark$	&$\checkmark$	&$\times$	&$\checkmark$	&$\checkmark$	&0.727	&0.975	&0.825	&0.743	\\
6	&$\checkmark$	&$\checkmark$	&$\checkmark$	&$\checkmark$	&$\checkmark$	&$\checkmark$	&0.750	&0.975	&0.837	&0.781	\\


\hline
\hline
\end{tabular}
}
\end{center}

\end{table}