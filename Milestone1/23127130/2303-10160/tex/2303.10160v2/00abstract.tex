% Automatic Speech Recognition(ASR) text correction is a post-processing step to correct the obtained ASR transcripts to present it in a more grammatical and coherent form to the end user. Previous works on ASR correction focus mainly on using the audio information and the text for its correction. But for videos, there is another important modality, visual information which has not been explored for ASR text correction, which we aim to do so in this paper. Our main contribution in this paper is two fold. Firstly, we provide the dataset where the visual information of the video helps in the ASR text correction. Secondly, we present two novel ways of using the visual information for ASR text correction. 

% \shanbo{Aiming at improving the Automatic Speech Recognition (ASR) outputs with a post-processing step, ASR error correction (EC) techniques have been widely developed due to their efficiency in using parallel text data. Previous work mainly focus on using text or/ and speech data, thus hinders the performance gain when not only text and speech information, but other modalities, such as visual information are critical for EC. The challenges are mainly two folds: one is that previous work fails to emphasize on the visual information, thus rare exploration has been studied. The other is that the community lacks of a high quality benchmark where visual information matters for the EC models. Therefore, this paper provides 1) simple yet effective methods, namely \method to incorporate visual information to help EC; 2) large-scale benchmark datasets, namely \dataset, where each item in the training data consists of visual, speech, and text information, and the test data are carefully selected by human annotators to ensure that even human could make mistakes when the visual information are missing. Experimental results show that \method could effectively use the visual information and surpass state-of-the-art methods by xxxx, which also indicates that visual information is critical in our proposed \dataset.}

Aiming to improve the Automatic Speech Recognition (ASR) outputs with a post-processing step, ASR error correction (EC) techniques have been widely developed due to their efficiency in using parallel text data. Previous works mainly focus on using text or/ and speech data, which hinders the performance gain when not only text and speech information, but other modalities, such as visual information are critical for EC. The challenges are mainly two folds: one is that previous work fails to emphasize visual information, thus rare exploration has been studied. The other is that the community lacks a high-quality benchmark where visual information matters for the EC models. Therefore, this paper provides 1) simple yet effective methods, namely gated fusion and image captions as prompts to incorporate visual information to help EC; 2) large-scale benchmark dataset, namely \dataset\footnote{\url{https://github.com/VanyaBK/visual_ASR_EC}}, where each item in the training data consists of visual, speech, and text information, and the test data are carefully selected by human annotators to ensure that even humans could make mistakes when visual information is missing. Experimental results show that using captions as prompts could effectively use the visual information and surpass state-of-the-art methods by upto 1.2\% in Word Error Rate(WER), which also indicates that visual information is critical in our proposed \dataset dataset.
% \begin{figure*}[t]
% \centering
% {%
%   \includegraphics[width=0.97\linewidth]{prompt.pdf}%
% } 

% \caption{}
% \label{fig:framework}
% \end{figure*}
%