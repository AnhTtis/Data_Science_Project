@InProceedings{C2,
  author = 	 "Jones, C.D. and Smith, A.B. and Roberts, E.F.",
  title =        "Article Title",
  booktitle =        "Proceedings Title",
  organization = "IEEE",
  year = 	 "2003",
  volume = 	 "II",
  pages = 	 "803-806"
}


@Inproceedings{NEURIPS2021_ea159dc9,
 author = {Baevski, Alexei and Hsu, Wei-Ning and CONNEAU, Alexis and Auli, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {27826--27839},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Speech Recognition},
 url = {https://proceedings.neurips.cc/paper/2021/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{46687,
title	= {State-of-the-art Speech Recognition With Sequence-to-Sequence Models},
author	= {Chung-Cheng Chiu and Tara Sainath and Yonghui Wu and Rohit Prabhavalkar and Patrick Nguyen and Zhifeng Chen and Anjuli Kannan and Ron J. Weiss and Kanishka Rao and Katya Gonina and Navdeep Jaitly and Bo Li and Jan Chorowski and Michiel Bacchiani},
year	= {2018},
URL	= {https://arxiv.org/pdf/1712.01769.pdf}
}


@INPROCEEDINGS{9746763,  author={Zhang, Fan and Tu, Mei and Liu, Song and Yan, Jinyao},  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={ASR Error Correction with Dual-Channel Self-Supervised Learning},   year={2022},  volume={},  number={},  pages={7282-7286},  doi={10.1109/ICASSP43922.2022.9746763}}


@inproceedings{inproceedings,
author = {Guo, Jinxi and Sainath, Tara and Weiss, Ron},
year = {2019},
month = {05},
pages = {},
title = {A Spelling Correction Model for End-to-end Speech Recognition},
doi = {10.1109/ICASSP.2019.8683745}
}


  
@inproceedings{inproceedings_sc,
author = {Zhang, Shuai and Yi, Jiangyan and Tian, Zhengkun and Bai, Ye and Tao, Jianhua and Liu, Xuefei and Wen, Zhengqi},
year = {2021},
month = {08},
pages = {266-270},
title = {End-to-End Spelling Correction Conditioned on Acoustic Feature for Code-Switching Speech Recognition},
doi = {10.21437/Interspeech.2021-1242}
}

@misc{https://doi.org/10.48550/arxiv.2204.13206,
  doi = {10.48550/ARXIV.2204.13206},
  
  url = {https://arxiv.org/abs/2204.13206},
  
  author = {Oneata, Dan and Cucu, Horia},
  
  keywords = {Sound (cs.SD), Audio and Speech Processing (eess.AS), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Improving Multimodal Speech Recognition by Data Augmentation and Speech Representations},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}



@article{KR2022SubwordLL,
  title={Sub-word Level Lip Reading With Visual Attention},
  author={Prajwal K R and Triantafyllos Afouras and Andrew Zisserman},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={5152-5162}
}

@article{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.03762}
}



@inproceedings{li-etal-2022-vision,
    title = "On Vision Features in Multimodal Machine Translation",
    author = "Li, Bei  and
      Lv, Chuanhao  and
      Zhou, Zefan  and
      Zhou, Tao  and
      Xiao, Tong  and
      Ma, Anxiang  and
      Zhu, JingBo",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.438",
    doi = "10.18653/v1/2022.acl-long.438",
    pages = "6327--6337",
    abstract = "Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models. In this work, we investigate the impact of vision models on MMT. Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models (such as Vision Transformer) and enhanced features (such as object-detection and image captioning). We develop a selective attention model to study the patch-level contribution of an image in MMT. On detailed probing tasks, we find that stronger vision models are helpful for learning translation from the visual modality. Our results also suggest the need of carefully examining MMT models, especially when current benchmarks are small-scale and biased.",
}

@article{Sanabria2018How2AL,
  title={How2: A Large-scale Dataset for Multimodal Language Understanding},
  author={Ramon Sanabria and Ozan Caglayan and Shruti Palaskar and Desmond Elliott and Lo{\"i}c Barrault and Lucia Specia and Florian Metze},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.00347}
}

@inproceedings{Hernandez2018TEDLIUM3T,
  title={TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation},
  author={Fran√ßois Hernandez and Vincent Nguyen and Sahar Ghannay and Natalia A. Tomashenko and Y. Est{\`e}ve},
  booktitle={SPECOM},
  year={2018}
}

@inproceedings{Yadav2020EndtoendNE,
  title={End-to-end Named Entity Recognition from English Speech},
  author={Hemant Yadav and Sreyan Ghosh and Yi Yu and Rajiv Ratn Shah},
  booktitle={INTERSPEECH},
  year={2020}
}

@INPROCEEDINGS{7178964,  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Librispeech: An ASR corpus based on public domain audio books},   year={2015},  volume={},  number={},  pages={5206-5210},  doi={10.1109/ICASSP.2015.7178964}}

@inproceedings{Chen2021GigaSpeechAE,
  title={GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10, 000 Hours of Transcribed Audio},
  author={Guoguo Chen and Shuzhou Chai and Guan-Bo Wang and Jiayu Du and Weiqiang Zhang and Chao Weng and Dan Su and Daniel Povey and Jan Trmal and Junbo Zhang and Mingjie Jin and Sanjeev Khudanpur and Shinji Watanabe and Shuaijiang Zhao and Wei Zou and Xiangang Li and Xuchen Yao and Yongqing Wang and Yujun Wang and Zhao You and Zhiyong Yan},
  booktitle={Interspeech},
  year={2021}
}

@inproceedings{zhao-etal-2021-neurst,
    title = "{N}eur{ST}: Neural Speech Translation Toolkit",
    author = "Zhao, Chengqi  and
      Wang, Mingxuan  and
      Dong, Qianqian  and
      Ye, Rong  and
      Li, Lei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-demo.7",
    doi = "10.18653/v1/2021.acl-demo.7",
    pages = "55--62",
    abstract = "NeurST is an open-source toolkit for neural speech translation. The toolkit mainly focuses on end-to-end speech translation, which is easy to use, modify, and extend to advanced speech translation research and products. NeurST aims at facilitating the speech translation research for NLP researchers and building reliable benchmarks for this field. It provides step-by-step recipes for feature extraction, data preprocessing, distributed training, and evaluation. In this paper, we will introduce the framework design of NeurST and show experimental results for different benchmark datasets, which can be regarded as reliable baselines for future research. The toolkit is publicly available at \url{https://github.com/bytedance/neurst} and we will continuously update the performance of with other counterparts and studies at \url{https://st-benchmark.github.io/}.",
}



@article{Dosovitskiy2021AnII,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal={ArXiv},
  year={2021},
  volume={abs/2010.11929}
}




@inproceedings{LiYL022,
	author    = {Yafu Li and
	Yongjing Yin and
	Jing Li and
	Yue Zhang},
	
	title     = {Prompt-Driven Neural Machine Translation},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2022,
	Dublin, Ireland, May 22-27, 2022},
	pages     = {2579--2590},
	publisher = {Association for Computational Linguistics},
	year      = {2022},
	url       = {https://aclanthology.org/2022.findings-acl.203},
	timestamp = {Thu, 19 May 2022 16:52:59 +0200},
	biburl    = {https://dblp.org/rec/conf/acl/LiYL022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{https://doi.org/10.48550/arxiv.2204.14198,
  doi = {10.48550/ARXIV.2204.14198},
  
  url = {https://arxiv.org/abs/2204.14198},
  
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Zisserman, Andrew and Simonyan, Karen},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Flamingo: a Visual Language Model for Few-Shot Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Mazaheri2018VisualTC,
  title={Visual Text Correction},
  author={Amir Mazaheri and Mubarak Shah},
  booktitle={ECCV},
  year={2018}
}

