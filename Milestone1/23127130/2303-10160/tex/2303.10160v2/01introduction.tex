% A lot of progress has been made in the field of automatic speech recognition as observed in \cite{Dong2018SpeechTransformerAN,NEURIPS2021_ea159dc9,46687}. But there are still many errors in the final output which are grammatically incoherent and misspelled due to similar sounding words.
% \shanbo{Over the past years, automatic speech recognition (ASR) models have achieved great success \cite{Dong2018SpeechTransformerAN,NEURIPS2021_ea159dc9,46687}. However, there are still many errors in the ASR outputs  caused by the inherent difficulty, such as grammatically incoherent, homophone errors, etc.}


% \begin{figure*}[t]
% \centering

% {%
%   \includegraphics[width=0.80\linewidth]{image_ASR.pdf}%
% }


\begin{figure*}[htb!]
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figs/fusion.pdf}  
  \caption{Fusion-based}
  \label{fig:fusion}
\end{subfigure}
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figs/prompt.pdf}  
  \caption{Prompt-based}
  \label{fig:prompt}
\end{subfigure}
\caption{Illustration of the proposed methods for Visual ASR Correction. For the gated fusion method(left), the visual features and text embeddings are concatenated and sent to a decoder to get the corrected output. For the prompt-based method(right), given a source sentence to be corrected, we first generate the corresponding image caption as additional context to the source input, and then use a sequence-to-sequence model to generate the final result.}
\label{fig:caption}
\end{figure*}

Over the past years, automatic speech recognition (ASR) models have achieved great success \cite{NEURIPS2021_ea159dc9,46687}. However, there are still many errors in the ASR outputs caused by inherent difficulties, such as grammatical incoherence, homophone errors, etc.

% Previous works use an error correction module as post processing step to correct the ASR errors using either only text as in \cite{tanaka18_interspeech,inproceedings,9053213} or along with acoustics \cite{inproceedings_sc}. 

% Hence there have been many works utilizing audio information in terms of phonetics for the ASR text correction \cite{Wang2020ASREC,raghuvanshi-etal-2019-entity,Garg2020HierarchicalMW,9746763} which helps in identifying the errors in the ASR text as well as disambiguation of similar sounding words.

To alleviate grammatical errors, some previous work propose to use an error correction module, typically sequence-to-sequence models, to correct the ASR outputs, with the help of large-scale text data\cite{inproceedings}. But since the error patterns vary irregularly based on context, pronunciation and language understanding, it is challenging to construct good quality of ASR hypotheses to reference text pairs to be used for supervised training. As the homophone errors are more difficult and might not be corrected with only text information, other works propose to utilize audio information in terms of phonetics for correction \cite{inproceedings_sc,9746763}. Although great works have been studied, there are still certain errors that cannot be properly corrected with audio and text information. For example, the sentence "\texttt{if you plan a little bit}" and "\texttt{if you plant a little bit}" sounds quite similar to each other, and both are grammatically correct. In such cases, equipping the correction models with only audio and text information is not sufficient, and other modalities, for example, visual information, is critical. 

% However for videos, there is another important modality, visual features which can be used to correct the ASR transcripts. There are previous works which use visual information for Automatic speech recognition directly as in \cite{https://doi.org/10.48550/arxiv.2204.13206} or for lip reading tasks \cite{Xu2020DiscriminativeMS,KR2022SubwordLL}. But the use of the visual information has not been extended to the post processing step of ASR correction, which we intend to do so in this work.

As visual information is crucial for ASR in certain cases, there are some previous works that use visual information for ASR directly, as in \cite{https://doi.org/10.48550/arxiv.2204.13206} or for lip reading task \cite{KR2022SubwordLL}. However, even though visual information could help ASR, the performance gain is limited by the amount of training data, where each training instance should be the triplet of vision, audio, and text information. We believe ASR error correction models, which could leverage visual data and large-scale text data, will improve the performance further.

Error correction can be viewed as similar to machine translation  (MT) task where a sequence-to-sequence model like transformers \cite{Vaswani2017AttentionIA} with large amounts of high-quality data can lead to excellent results. Inspired by multi-modal MT and other related tasks like visual text correction \cite{li-etal-2022-vision,Mazaheri2018VisualTC}, in this paper, we propose a multi-modal ASR error correction method which utilizes visual information. %As we focus on the visual information in this work, we use visual information instead of the previously used audio information.

%So taking inspiration from previous multimodal machine translation tasks utilizing visual information like \cite{10.1145/3394171.3413715,caglayan-etal-2021-cross,li-etal-2022-vision}, we extend similar ideas to ASR text correction. 

%There are also previous works which use visual information for correction of textual description of videos \cite{Mazaheri2018VisualTC}, which involves two stages of inaccuracy detection and correction using LSTMs and gated fusion. 

The contributions of this paper are:
%\begin{itemize}
    (1)We present the multi-modal dataset of ASR transcripts along with images to help its correction. The datasets were obtained from two sources, how2 dataset \cite{Sanabria2018How2AL} and from the publicly available YouTube videos. Both the sources have reference transcripts annotated by humans. And the ASR transcripts were obtained from the Huggingface wav2vec model and the Google ASR API to show that the method is independent of the ASR model;
%\end{itemize}
%\begin{itemize}
    (2)We propose two ways to utilize the visual information for ASR text correction. Firstly, a gated fusion method where the image features are concatenated with the textual embeddings, similar to previous works \cite{li-etal-2022-vision}. Secondly, we propose a prompt-based method to better utilize large-scale text data, where the captions from the images are used as prompts for ASR correction to provide more context.
%\end{itemize}
% \begin{figure*}[t]
% \centering

% {%
%   \includegraphics[width=0.80\linewidth]{Gated_fusion.pdf}%
% }
% \caption{Gated Fusion method}
% \label{fig:framework}
% \end{figure*}
%using a \texttt{tanh} gated mechanism, 