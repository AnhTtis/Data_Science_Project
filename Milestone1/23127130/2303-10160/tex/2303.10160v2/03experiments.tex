%All the experiments are conducted using two different ASR models, Google ASR ASPI and Huggingface's wav2vec model to verify the generality of the proposed methods. 
\vspace{-0.5em}
\subsection{Baseline}
\label{sec:baseline}
\vspace{-0.5em}
The baseline used for this work is the standard BART-base model trained using Fairseq\footnote{\url{https://github.com/facebookresearch/fairseq}}. This model is first pre-trained with 5 million synthetic data obtained from \textit{TED-LIUM3}, \textit{DATA2} and \textit{LibriSpeech}. Then it is fine-tuned on the high-quality annotated dataset of 185k samples which were filtered based on the similarity metric of sentence-transformers model as explained in section \ref{subsec:dataset}. GPT-2 based BPE was applied in the pre-processing stage. %The source and the target dictionaries were obtained from the pretrained BART model.% and hidden size of 512
\vspace{-0.5em}
\subsection{Prompt Based Method}
\vspace{-0.5em}
For the prompt-based method, a similar setup to that of the baseline was followed. The only change from the baseline was the addition of caption data as prompts in the source sentence in all three train, valid and test sets.
\vspace{-0.5em}
\subsection{Gated Fusion Method}
\vspace{-0.5em}
This method was implemented based on \href{https://github.com/libeineu/fairseq_mmt}{Multimodal Machine Translation} where the sigmoid gate function was replaced with tanh. All the parameters were kept constant as in \cite{li-etal-2022-vision}, except for the learning rate which was changed to 0.001 and the max updates to 800,000. For evaluation, the average of last 10 checkpoints was used for more reliable results. %For this experiment, the Transformer-Tiny configuration was used, consisting of 4 encoder and decoder layers.%The width of beam size was set to 5
\vspace{-1em}
\section{Results and Analysis}
\vspace{-0.5em}
All the experiments are conducted on two different ASR models, Google ASR API and Huggingface wav2vec to verify the generality of the methods. As can be seen from Table \ref{tab:main}, the best results are obtained from the \textbf{prompt-based method}, thus verifying that visual information helps in ASR EC. %Let us now analyze why.
%The performance metrics of error correction were measured using the WER and sentence error rate(SER) for all the proposed methods.