






\begin{table*}[]
\centering
\begin{tabular}{c|l|cccccc}

\multirow{1}{*}{ID} & \multirow{1}{*}{Models and Variants}     & \multicolumn{2}{c|}{Google ASR API}                           & \multicolumn{2}{c|}{Huggingface wav2vec}         \\  \cline{3-6}
& & WER                 & \multicolumn{1}{c|}{SER}            & WER                      & \multicolumn{1}{c|}{SER} \\ \hline
1 & Original & 36.80 &\multicolumn{1}{c|}{100}&31.94&\multicolumn{1}{c|}{97.79} \\
2  &Transformer    &    34.14                      & \multicolumn{1}{c|}{99.42}  &   22.36     &                       \multicolumn{1}{c|}{91.45}                         \\
3  & Prompt-based       &     \textbf{33.5}         & \multicolumn{1}{c|}{\textbf{98.94}}          &       \textbf{21.13}                     & \multicolumn{1}{c|}{\textbf{90.59}}                                \\
4  & Gated fusion &           36.58        &  \multicolumn{1}{c|}{99.90}   &    25.44      &     \multicolumn{1}{c|}{95.39}                                                \\
5                                                                                                 &   Transformer + Gated fusion   &       34.8                    & \multicolumn{1}{c|}{99.42}                              &            22.92                & \multicolumn{1}{c|}{92.8}                              \\ 

6  & Transformer + Gated fusion (Filter)             &         34.54        & \multicolumn{1}{c|}{99.42}          &           22.66          & \multicolumn{1}{c|}{92.51}                         \\
7  &  Prompt-based + Gated Fusion             & 34.21      & \multicolumn{1}{c|}{98.94} &     21.80                 & \multicolumn{1}{c|}{92.12}                      \\
8  &   Prompt-based + Gated Fusion (Filter)       &       33.9          & \multicolumn{1}{c|}{99.04}          & 21.49          & \multicolumn{1}{c|}{91.74}        \\ \hline
9& Random Image captions(Caption Prompt) & 34.2 & \multicolumn{1}{c|}{99.14} &22.11&\multicolumn{1}{c|}{91.16} \\ \hline
10 & Original(Random test set) & 34.59&\multicolumn{1}{c|}{99.56}  & 30.70&\multicolumn{1}{c|}{97.6} \\
11  &Transformer(Random test set)    &    32.49    &                      \multicolumn{1}{c|}{98.68 }    &   21.48     &                       \multicolumn{1}{c|}{90.4}                  \\
12  & Prompt-based  (Random test set)             &         \textbf{30.79}             & \multicolumn{1}{c|}{\textbf{97.8}}    &       \textbf{20.6}                     & \multicolumn{1}{c|}{\textbf{89.10}}  

\end{tabular}
\caption{Measurement of error correction performance}
\label{tab:main}
\end{table*}




\vspace{-1em}
\subsection{Prompt Based Method}
\vspace{-0.5em}
Prompt-based methods have gained wide recognition following the success in several NLP tasks \cite{LiYL022}.
In this method, we use the caption data of the images as prompt for the ASR text correction task in order to provide more context. The caption data is obtained from the Flamingo model \cite{https://doi.org/10.48550/arxiv.2204.14198} trained for image captioning on the Google's Conceptual Captions datasets\footnote{\url{https://github.com/dhansmair/flamingo-mini}}. For example, consider Fig.\ref{fig:caption}b), here the caption data, "the tortoise is being cared for", is sent as a prompt by modifying the source sentence as, "the tortoise is being cared for  [SEP] thats really how we choose our tourist it for today". The modified source sentence and the target sentence are used as the parallel data for training and the modified source sentence is also used as the input while testing. Because of the presence of the caption data as prompt, more context is available to the model to correct \texttt{tourist} to \texttt{tortoise}, which otherwise would be difficult for humans too.
\vspace{-1em}
\subsection{Gated Fusion Method}
\vspace{-0.5em}
Gated fusion techniques are widely used in combining the representations from different modalities as is done in some of the previous works \cite{li-etal-2022-vision}. 
In this method, for any input sample which consists of image $I$, source text $S$ and target text $T$, the image features are obtained using the OpenAI CLIP's Vision Transformer(ViT) model \cite{Dosovitskiy2021AnII} as ViT($I$) and the textual embeddings are obtained from the standard transformer encoder as $H^S$. These two representations are fused by a vector concatenation noted as :
\begin{equation}
    H^{\mathrm{fused}} = g([H^{\mathrm{S}} ; \textbf{H}^{\mathrm{I} } ])
\end{equation}
where $H^I \in R^{L \times D}$ is the projected form of ViT($I$) to that of the length of the text representation $H^S \in R^{L \times D}$ using a linear projection layer. [L - sequence length; D - hidden dimension]. The fused representation $H^\mathrm{fused} \in R^{L \times D}$ is then passed through a \texttt{tanh} gate to control the amount of visual information used as :
\begin{equation}\label{gate}
 \Lambda = \tanh(f( [H^{\mathrm{S}} ; H^{\mathrm{fused}}  ]))
\end{equation}
The gated fused information is then added to the original textual embeddings to get the multimodal fusion representation as :
\begin{equation}\label{main}
H^{\mathrm{out}} = H^{\mathrm{S}} + \Lambda H^{\mathrm{fused}}
\end{equation}
Here tanh gate is used instead of sigmoid since it is centered at zero. This means that when $H^{\mathrm{fused}}$, is close to zero, the output of $\lambda$ will be close to zero, which aligns naturally with the situation when the image is absent as in the synthetic data. Although we found that the results were very similar for both tanh and sigmoid gates. \\
Since the high-quality annotated set with images is of a smaller size(185k samples) to correct grammatical errors, this method is applied sequentially after first obtaining the result from the baseline transformers model(Transformers+Gated fusion method in Table \ref{tab:main}). Finally, the changes made by the gated fusion method to the output from the baseline method (section \ref{sec:baseline}), is filtered by including only those changes where the similarity probability(calculated by CLIP's ViT) between the image and the changed text is higher than that with the original text(i.e output from the baseline method)[referred to as Transformers+Gated fusion(Filter) in Table \ref{tab:main}]. This is done to retain only image based corrections, since the grammar corrections are already done by the baseline method. Similar filtering and sequential correction is also applied after the prompt-based method [referred to as Prompt-based + Gated Fusion and Prompt-based + Gated Fusion(Filter) respectively in Table \ref{tab:main}].
\vspace{-0.5em}
\subsection{Dataset Retrieval}
\vspace{-0.5em}
\label{subsec:dataset}
The datasets were obtained from mainly two sources, the how2 dataset and the youtube videos. The how2 dataset consists of 300h of videos with annotated transcripts in English which resulted in 220,000 samples. The youtube videos were collected using the Youtube-DL toolkit\footnote{\url{https://github.com/ytdl-org/youtube-dl/}}, where each of these videos had annotated transcripts and audio in English. The youtube videos accounted for 2.5 million samples of data with annotated transcripts. The images for these samples were obtained by capturing the frame of the video at exactly the mid of the start and end timestamps of that sample. To obtain high-quality annotated dataset from how2 and youtube videos for visual ASR correction, we filtered the dataset to only include the examples where the similarity between the caption data and the reference transcript was greater than 0.2 as measured by Huggingface's sentence-transformers. Similarity score of 0.2 was chosen after testing performance for other thresholds\ref{tab:threshold}.  After this filtering, 58k samples were obtained from how2 dataset(accounting for 26\% of the total dataset) and 127k samples from the youtube videos(accounting for 5\% of the total dataset), indicating that a significant portion of the dataset could be corrected using the visual information. Totally, this high-quality annotated dataset consists of 185k samples with audio, images and ASR correction parallel data.

\begin{table}[h]
%\tiny
    \centering
    \begin{tabular}{c|c|cc}
        {Similarity Score} &{Size of train set} &\multicolumn{2}{c}{WER\hspace{0.5em} SER}   \\
        \hline
         0.1 & 380441 & 33.94&98.46\\ 
         0.2 & 183306 &\textbf{33.5} & \textbf{98.94}\\
         0.3 & 85181 & 33.89&99.23\\
         %0.4 & 38116 &34.21&99.33\\
         %0.5 & 16732 & 34.84&99.42\\
    \end{tabular}
    \caption{Prompt-based method across similarity thresholds}
    \label{tab:threshold}
\end{table}

% \ningxin{To construct a high-quality ASR error correction dataset based on videos, we collected and carefully selected a large number of videos with speech audios and human annotated transcripts from the How2 dataset\cite{Sanabria2018How2AL} (300h subset) and YouTube videos. The How2 dataset consists of 13,662 videos with human-annotated transcripts in English, which comprising \textbf{\textit{xxxxx (exact number)}} samples. For the YouTube videos, we collected over 127,000 videos with human-annotated transcripts using the Youtube-DL toolkit. For each video clip, we sampled one image by capturing the middle frame between the start and end timestamps as visual information. A weak checkpoint trained on the GigaSpeech small dataset \cite{Chen2021GigaSpeechAE} using Neural Speech Translation toolkit \cite{zhao-etal-2021-neurst} was used to obtain the synthetic ASR transcripts.}


%\ningxin{The statistics of our dataset is shown in Table\ref{tab:dataset_stats}} 



% \begin{table}[!htb]
%     \centering
%     \begin{tabular}{cccccc}
%     \toprule
%         \textbf{Source} & \textbf{How2} & \textbf{YouTube}  \\
%         %~ & ~ & ~ & train & dev & test \\ \midrule
%         \#Video & 13,662 & 33,145 & ~ & ~ & ~  \\ 
%         \#Sample & 191,297 & ??? & ~ & ~ & ~  \\ 
%         \#Words & 20.2 & 18.28 & ~ & ~ & ~ \\ \bottomrule
%     \end{tabular}
%     \caption{Statistics of Dataset}
%     \label{tab:dataset_stats}
% \end{table}

% \ningxin{In order to increase sample capacity, we also added some synthetic ASR data into the train set. The synthetic data was obtained from 3 sources: }


\begin{table*}[!htb]
    \centering
    \begin{tabular}{ccl}
    \toprule
      \multirow{4}{*}{\textbf{Ex.1}} & \textbf{Source sentence} & to the best strings for whatever \textbf{gucci} youre going to restring what i have on here\\
        &\textbf{Baseline} & to the best strings for whatever \textbf{gucci} youre going to restring what i have on here \\
        &\textbf{Caption prompt} & to the best strings for whatever \textbf{guitar} player youre going to restring what i have on here\\ 
        &\textbf{Target sentence} & to the best strings for whatever the \textbf{guitar} youre going to restring what i have on here \\\hline
        \multirow{4}{*}{\textbf{Ex.2}}&\textbf{Source sentence} & boxing and yeah it does because boxing kickboxing rg condola training methods\\
        &\textbf{Baseline} & boxing and yeah it does because boxing kickboxing are condola training methods \\
        &\textbf{Caption prompt} & boxing and yeah it does because boxing kickboxing are condola training methods\\ 
        &\textbf{Target sentence} & boxing and ya it does because boxing kick boxing are \textbf{jeet kune do} training methods  \\\hline
       
    \end{tabular}
    \caption{Sample Analysis}
    \label{tab:example}
\end{table*}
The training set consists of this high-quality annotated dataset along with synthetic EC parallel data. This synthetic dataset is obtained from 3 sources : 1) \textbf{TED-LIUM3} - The TED-LIUM3 corpus \cite{Hernandez2018TEDLIUM3T} is built from the TED videos; 2) \textbf{DATA2} - The DATA2 corpus \cite{Yadav2020EndtoendNE} is built for the named-entity recognition tasks; 3) \textbf{LibriSpeech} - The LibriSpeech corpus \cite{7178964} is derived from audiobooks of the LibriVox project.

A weak checkpoint trained on the GigaSpeech small dataset \cite{Chen2021GigaSpeechAE} using Neural Speech Translation toolkit \cite{zhao-etal-2021-neurst} was used to obtain the synthetic ASR transcripts by setting the beam size in the range of 8-16. Using this method, 5 million synthetic parallel data of candidate ASR transcripts and reference transcripts were obtained. 

The test set was constructed to contain 1,041 ASR transcripts from two ASR models, Google ASR API and the Huggingface wav2vec model such that the corresponding image would help in the correction of these transcripts. 

% \begin{table}
% \centering
% %\begin{tabular}{lcs}
% \hline
% \textbf{Source sentence} : sometimes if you plan a little bit in the \newline\noindent\phantom{so}wrong time and i didnt \\ 
% \hline
% \textbf{Target sentence} : sometimes if you plant them a little bit in \newline\noindent\phantom{so}the wrong time it might take them a little \\
% \hline
% \textbf{Caption} : a woman standing next to a tree \\
% \hline
% %\end{tabular}
% \caption{Example for prompt-based method}
% \label{tab:example}
% \end{table}