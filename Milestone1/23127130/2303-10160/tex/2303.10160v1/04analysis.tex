\vspace{-0.5em}
\subsection{Relevance of Images}
\vspace{-0.5em}
To study the relevance of image information in ASR correction, we further conduct more experiments by assigning a random image to the parallel data and seeing if it can help in the ASR correction. As we can see from Table \ref{tab:main}, including random image's caption as prompt to the transformer model, leads to a decrease in performance of WER from the prompt-based method, and is on par with the baseline transformer model. This shows that the right image of the video captured during the speech is essential to improve performance.
\vspace{-0.5em}
\subsection{Why is Caption Prompt Based Method Better?}
\vspace{-0.5em}
Because it's easier to use large amounts of synthetic data for text-only methods, the prompt-based method works better than gated fusion. In the gated fusion method it is hard to correct the grammatical errors because of the lack of a large amount of parallel data with the images. In order to emphasize why caption is better, we perform experiments where, with an increase in synthetic data(Table \ref{tab:synthetic}), the prompt-based method performs better for Google ASR text correction. This shows that a better representation of the caption is learnt with increasing synthetic data and hence it can be better used as context for ASR text correction.  
\vspace{-0.5em}
\subsection{Sample Analysis}
\vspace{-0.5em}
From Ex.1 of Table \ref{tab:example}, it is clear that with additional information from the videos, it is easy to correct \texttt{gucci} to \texttt{guitar} which otherwise would be hard to correct for the transformers model. We also note the limitations of the captioning model which leads to a decrease in the performance of ASR EC. For instance, Ex.2 of Table \ref{tab:example} shows that neither the transformer model nor the caption prompt method is able to predict "jeet kune do because the caption prompt for this particular example is "person a former professional boxer is a trainer and trainer", which do not help in predicting "jeet kune do". Instead, if we have a human annotator providing the caption as "person practicing jeet kune do", it would better help in the ASR EC. Thus, due to the limitations of the captioning model, we observe that the performance of the ASR EC can be further improved with better captioning data.
\vspace{-0.5em}
\subsection{Random Test Set}
\vspace{-0.5em}
Instead of constructing the test set to have cherry-picked examples where images are needed for ASR EC, we test our proposed methods on a test set built by sampling 1000 random sentences from the how2 dataset. From the results of Table \ref{tab:main}, we can see that the gap between the baseline and caption prompt methods increases by 2x for the google ASR API and decreases only slightly for huggingface wav2vec(with still a considerable gap of 0.88\%). This shows that the caption prompt model has the potential to improve the performance of ASR text correction for instructional videos in general.




\begin{table}[]
\centering
\begin{tabular}{c|l|cccccc}

\multirow{1}{*}{ID} & \multirow{1}{*}{Method}     & \multicolumn{2}{c|}{2M Synthetic}                           & \multicolumn{2}{c|}{5M Synthetic}         \\  \cline{3-6}% & \multicolumn{2}{c|}{TED+LibriSpeech(3.8M)}
& & WER                 & \multicolumn{1}{c|}{SER}             & WER                      & \multicolumn{1}{c|}{SER} \\ \hline%& WER                      & \multicolumn{1}{c|}{SER}
%1 & Original & 36.80&\multicolumn{1}{c|}{100}  & 36.80&\multicolumn{1}{c|}{100} \\
% 2  &Transformer    &        &                       \multicolumn{1}{c|}{}    &        &                       \multicolumn{1}{c|}{} &   22.36     &                       \multicolumn{1}{c|}{91.45}                     \\
1  & Prompt-based               &          33.94               & \multicolumn{1}{c|}{99.33}    &       \textbf{33.5}                     & \multicolumn{1}{c|}{\textbf{98.94}}                              \\ %&  21.13                          & \multicolumn{1}{c|}{90.68}
\end{tabular}
\caption{EC performance with different size of synthetic data}
\label{tab:synthetic}
\end{table}


% \begin{table}[]
% \centering
% \begin{tabular}{c|l|cccccc}

% \multirow{}{*}{ID} & \multirow{}{*}{Models}     & \multicolumn{2}{c|}{Google ASR}                           & \multicolumn{2}{c|}{HF wav2vec}         \\  \cline{3-6}% & \multicolumn{2}{c|}{TED+LibriSpeech(3.8M)}
% & & WER                 & \multicolumn{1}{c|}{SER}             & WER                      & \multicolumn{1}{c|}{SER} \\ \hline%& WER                      & \multicolumn{1}{c|}{SER}
% 1 & Original & 34.59&\multicolumn{1}{c|}{99.56}  & 30.70&\multicolumn{1}{c|}{97.6} \\
% 2  &Transformer    &    32.49    &                      \multicolumn{1}{c|}{98.68 }    &   21.48     &                       \multicolumn{1}{c|}{90.4}                  \\
% 3  & Prompt-based               &         \textbf{30.79}             & \multicolumn{1}{c|}{\textbf{97.8}}    &       \textbf{20.6}                     & \multicolumn{1}{c|}{\textbf{89.10}}                              \\ %&  21.13                          & \multicolumn{1}{c|}{90.68}
% \end{tabular}
% \caption{Experiments with random test set}
% \label{tab:synthetic}
% \end{table}