
\onecolumn

\begin{center}
\Large
\textbf{Appendix}
\end{center}

\appendix

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{figures/sup_qual.pdf}
\caption{\textbf{Comparison between Stable-DreamFusion~\cite{stable-dreamfusion,poole2022dreamfusion}, SJC~\cite{wang2022score}, and ours.} The baseline is original SJC~\cite{wang2022score}. Our debiasing methods qualitatively reduce view inconsistencies in zero-shot text-to-3D and the so-called \emph{Janus problem}.}
\label{fig:sup-qual}
\end{figure}

\newpage

\section{Background}

\subsection{Diffusion models}
Denoising diffusion models~\cite{ho2020denoising,song2021denoising} generate images through progressive denoising process. During training, denoising diffusion probabilistic models (DDPM)~\cite{ho2020denoising} optimize the following simplified objective:
\begin{equation}
L_{\textrm{DDPM}} := \mathbb{E}_{\bm{\epsilon} \sim \mathcal{N}(0, \mathbf{I}), \mathbf{x}_0, t} \left[\big\|\bm{\epsilon} - \bm{\epsilon}_\phi(\mathbf{x}_t, t)\big\|^2\right],
\end{equation}
where $\bm{\epsilon}_\phi$ is a network of the diffusion model, $t \in \{T, T-1, ..., 1\}$ is a timestep, $\mathbf{x}_0$ is an original image, and $\mathbf{x}_t$ denotes a perturbed image according to the timestep $t$. During inference, starting from $x_T$, DDPMs sample a previous sample $\mathbf{x}_{t-1}$ from a normal distribution $p_{\phi}(\mathbf{x}_{t-1}|\mathbf{x}_t)$ defined by $\phi$.

Some works fit DDPM into the generalized frameworks, \eg, non-Markovian~\cite{song2021denoising}, score-based~\cite{song2020score,karras2022elucidating}, \etc. Notably, denoising diffusion models have a tight relationship with score-based models~\cite{song2020score,karras2022elucidating} in the continuous form. Furthermore, the work~\cite{karras2022elucidating} show that denoising diffusion models can be refactored into the canonical form of denoising score matching using the same network parameterization. This formulation further facilitates the direct computation of 2D scores~\cite{song2019generative, karras2022elucidating} with the following equation:
\begin{equation}
\nabla_{\mathbf{x}} \log p(\mathbf{x}; \sigma) = \frac{D_{\phi}(\mathbf{x}; \sigma) - \mathbf{x}}{\sigma^2},
\end{equation}
where $D_{\phi}$ is an optimal denoiser network trained for every $\sigma$. With some preconditioning, a diffusion model $\bm{\epsilon}_\phi$~\cite{ho2020denoising, song2021denoising, nichol2021improved, rombach2022high} turns into a denoiser $D_{\phi}$. In other words, 2D scores of diffusion models can be calculated this way.

Recent advancements in diffusion models have sparked increased interest in text-to-image generation~\cite{ramesh2022hierarchical,rombach2022high,saharia2022photorealistic,nichol2021glide}. Diffusion guidance techniques~\cite{dhariwal2021diffusion,ho2021classifier,nichol2021glide,hong2022improving} have been developed to enable the control of the generation process based on various conditions such as class labels~\cite{ho2021classifier,dhariwal2021diffusion}, text captions~\cite{nichol2021glide}, or internal information~\cite{hong2022improving}. In particular, our work conditions text prompts with classifier-free guidance~\cite{ho2021classifier}, which is formulated as follows given a conditional diffusion model $\bm{\epsilon}_\phi(\mathbf{x}_t, t, c)$:
\begin{equation}
\tilde{\bm{\epsilon}} = \bm{\epsilon}_\phi(\mathbf{x}_t, t, c) + s\cdot(\bm{\epsilon}_\phi(\mathbf{x}_t, t, c) - \bm{\epsilon}_\phi(\mathbf{x}_t, t)),
\end{equation}
where $\tilde{\bm{\epsilon}}$ is the guided output, and $s$ is the guidance scale.

\subsection{Score distillation}
Diffusion models have shown remarkable performance in text-to-image modeling~\cite{nichol2021glide,ramesh2022hierarchical,saharia2022photorealistic,hong2022improving,rombach2022high}. On top of this, DreamFusion~\cite{poole2022dreamfusion} proposes the score-distillation sampling (SDS) method that uses text-to-image diffusion models to optimize neural fields, achieving encouraging results. The score-distillation sampling utilizes the gradient computed by the following equation:
\begin{equation}
\nabla_{\theta} L_{\textrm{SDS}} \triangleq \mathbb{E}_{\bm{\epsilon} \sim \mathcal{N}(0, \mathbf{I}), \mathbf{z}_\theta, t} \left[w(t)(\bm{\epsilon}_\phi(\mathbf{z}_t, t) - \bm{\epsilon})\frac{\partial \mathbf{z}_\theta}{\partial \theta}\right],
\label{eq:sds}
\end{equation}
where $\mathbf{z}_t$ denotes the $t$-step noised version of $\mathbf{z}_\theta$ which is a rendered image, and $w(t)$ is a scaling function only dependent on $t$. This gradient omits the Jacobian of the diffusion backbone, leading to tractable optimization of differential neural parameters~\cite{poole2022dreamfusion}.

On the other hand, in light of the interpretation of diffusion models as denoisers, SJC~\cite{wang2022score} presents a new approach directly using the score estimation for which the authors call perturb-and-average scoring (PAAS). The work shows that the U-Net Jacobian emerging in DreamFusion is not even necessary, as well as forming a strong baseline using publicly open Stable Diffusion~\cite{rombach2022high}. The perturb-and-average score approximates to a score with an inflated noise level:
\begin{equation}
\nabla_{\mathbf{z}_\theta} \log p_{\sqrt{2}\sigma}(\mathbf{z}_\theta) \approx \mathbb{E}_{n \sim \mathcal{N}(0, \mathbf{I}), \mathbf{z}_\theta} \left[\frac{D(\mathbf{z}_\theta+\sigma n; \sigma) - \mathbf{z}_\theta}{\sigma^2}\right],
\label{eq:paas}
\end{equation}
where the expectation is practically estimated by Monte Carlo sampling. This score estimate is then directly plugged into the 2D-to-3D chain rule and produces:
\begin{equation}
\nabla_{\theta} L_{\textrm{PAAS}} \triangleq \mathbb{E}_{\mathbf{z}_\theta} \left[\nabla_{\mathbf{z}_\theta} \log p_{\sqrt{2}\sigma}(\mathbf{z}_\theta)\frac{\partial \mathbf{z}_\theta}{\partial \theta}\right].
\label{eq:sjc}
\end{equation}
Although the derivation is different from SDS in DreamFusion~\cite{poole2022dreamfusion}, it is straightforward to show that the estimation $\nabla_{\theta} L_{\textrm{PAAS}}$ is same as $\nabla_{\theta} L_{\textrm{SDS}}$ with a different weighting rule and sampler~\cite{karras2022elucidating}.

\section{Implementation Details}

\subsection{Common settings}
We build our debiasing methods upon the public repository of SJC~\cite{wang2022score}. For all the results, including SJC and ours, we run 10000 steps to optimize the 3D fields. We set the hyperparameters of SJC to specific constants and do not change them throughout the experiments.

\subsection{Prompt debiasing}
To compute the pointwise mutual information (PMI), we use the uncased model of BERT~\cite{devlin2018bert} to obtain the conditional probability. Additionally, we set $P(u)=1$ for words that should not be erroneously omitted. Otherwise, we set $P(u)=1/2$. To use a general language model for the image-related task, we concatenated ``This image is depicting a" when evaluating the PMI between the view prompt and user prompt. We first get $u, v$ pairs such that $\frac{P(v, u)}{P(v)P(u)} < 1$. Then, given a view prompt, we remove words whose PMI for that view prompt, normalized across all view prompts, is below $0.95$.

For the view prompt augmentation, we typically follow the view prompt assignment rule of DreamFusion~\cite{poole2022dreamfusion} and SJC~\cite{wang2022score}. However, we slightly modify the view prompts and azimuth ranges for each prompt as mentioned in Sec.~\ref{sec:discrepancy}. For example, we assign an azimuth range of $[-22.5^{\circ}, 22.5^{\circ}]$ for the ``front view." Also, we empirically find that using a view prompt augmentation $v \in \{``front\ view", ``back\ view", ``side\ view", ``top\ view"\}$ without $``of"$ depending on a viewpoint gives us improved results for Stable Diffusion v1.5~\cite{rombach2022high}.

\subsection{Score debiasing}

In terms of score debiasing, we gradually increase the truncation threshold from one fourth of the pre-defined threshold to the pre-defined threshold, according to the optimization step. Specifically, we linearly increase the threshold from $2.0$ to $8.0$ for all experiments that leverage dynamic thresholding of 2D-to-3D scores.

\subsection{Evaluation metrics ($\textmd{A-LPIPS}_\textmd{VGG, Alex}$)}

Quantitatively evaluating a zero-shot text-to-3D framework is challenging due to the absence of ground truth 3D scenes that correspond to the text prompts. Existing works employ CLIP R-Precision~\cite{jain2022zero, poole2022dreamfusion}. However, it measures retrieval accuracy through projected 2D images and text input, making it unsuitable for quantifying the view consistency of a scene.

To address this issue, a concurrent work~\cite{seo2023let} proposes a new metric that utilizes COLMAP~\cite{schonberger2016structure} to measure the consistency of a generated 3D scene. However, we find that this metric largely depends on the accuracy of the sequential reconstruction provided by off-the-shelf COLMAP, which we empirically determine to be inaccurate for rendered images from SJC~\cite{wang2022score}. For instance, it occasionally omits camera poses for some images if features are not matched, resulting in extremely large variances between distances.

Therefore, to measure the view consistency of generated 3D objects quantitatively, we compute the average LPIPS~\cite{zhang2018unreasonable} between adjacent images, which we refer to as A-LPIPS. We sample 100 uniformly spaced camera poses from an upper hemisphere of a fixed radius, all directed towards the sphereâ€™s center at an identical elevation, and render 100 images from a 3D scene. Then, we average the LPIPS values evaluated for all adjacent pairs of images in the 3D scene, finally aggregating those averages across the scenes. The intuition behind this is that if there exist artifacts or view inconsistencies in a generated 3D scene, the perceptual loss will be large near those points.

\section{More Results}

\subsection{Dynamic thresholding of 2D-to-3D scores}
To show some examples of the effect of dynamic thresholding, we compare results of dynamic thresholding with those of static thresholding and no thresholding in Fig.~\ref{fig:dynamic}. It demostrates that dynamic clipping reduce artifacts with better realism.

\subsection{Qualitative results}
We present additional qualitative results in Fig.~\ref{fig:sup-qual} and ablation results in Fig.~\ref{fig:sup-ablation}.
In addition to the results of SJC~\cite{wang2022score}, which serves as the baseline for our experiments, we include those of Stable-DreamFusion~\cite{stable-dreamfusion}, an unofficial re-implementation of DreamFusion~\cite{poole2022dreamfusion} that utilizes Stable Diffusion~\cite{rombach2022high}. The results demonstrate that our methods significantly reduces the Janus or view inconsistency problem.


\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/thres_1.pdf}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/thres_2.pdf}
\caption{\textbf{Dynamic thresholding of 2D-to-3D scores.}}
\label{fig:dynamic}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/sup_abl1.pdf}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/sup_abl2.pdf}
\caption{\textbf{Improvement of view consistency through prompt and score debiasing.} The baseline is original SJC~\cite{wang2022score}, and \textit{Prompt} and \textit{Score} denote prompt and score debiasing, respectively. The given user prompts are \textit{``a cute and chubby panda munching on bamboo,"} and \textit{``an unicorn with a rainbow horn."}}
\label{fig:sup-ablation}
\end{figure}
