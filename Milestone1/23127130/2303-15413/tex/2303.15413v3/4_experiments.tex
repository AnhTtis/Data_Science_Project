\begin{figure}[t]
    \centering
    \begin{subfigure}{
        \includegraphics[width=0.4746\textwidth]{figures/clip_similarity_0.pdf}
        \label{fig:clip-front}
    }
    \end{subfigure}
    \hfill
    \begin{subfigure}{
        \includegraphics[width=0.464\textwidth]{figures/clip_similarity_2.pdf}
        \label{fig:clip-back}
    }
    \end{subfigure}
    \vspace{-10pt}
    \caption{\textbf{Average CLIP similarities of rendered images for each azimuth, calculated using view-augmented prompts.} The shaded areas, starting from the left, represent the $90^{\circ}$ regions for the front view and back view, respectively.}
    \label{fig:clip-both}\vspace{-5pt}
\end{figure}



\begin{table}[t]
\footnotesize  
\centering
\begin{tabular}{l c c c} 
\toprule
 Method & $\text{A-LPIPS}_\text{VGG} \downarrow$ & $\text{A-LPIPS}_\text{Alex} \downarrow$ \\
\midrule
Baseline~\cite{wang2022score} & 0.2054 & 0.1526 \\
Debiased (Preserved) & \underline{0.1963} & \underline{0.1450} \\
Debiased (Ours) & \textbf{0.1940} & \textbf{0.1445} \\
\bottomrule
\end{tabular}
\vspace{+5pt}
\caption{\textbf{Quantitative evaluation.} The best values are in bold, and the second best are underlined. \emph{Preserved} means user prompts are preserved, i.e., $P(u)=1$ for all $u$.}
\label{tab:alpips}\vspace{-20pt}
\end{table}



\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/main_qual.pdf}
\caption{\textbf{Comparison between Stable-DreamFusion~\cite{stable-dreamfusion,poole2022dreamfusion}, SJC~\cite{wang2022score}, and ours.} The baseline is original SJC~\cite{wang2022score}. Our debiasing methods qualitatively reduce view inconsistencies in zero-shot text-to-3D and the so-called \emph{Janus problem}.}\vspace{-10pt}
\label{fig:qual}
\end{figure}



\begin{table}[t]
\small  
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{l ccc} 
\toprule
Method   & View consistency & Faithfulness & Overall quality \\
\midrule
D-SDS (Ours)            & \textbf{90.42\%} & \textbf{83.93\%}      & \textbf{90.33\%} \\
SJC~\cite{wang2022score} & 9.58\% & 16.07\%      & 9.67\%  \\
\bottomrule
\end{tabular}
\vspace{+5pt}
\caption{\textbf{User study.}}\vspace{-20pt}
\label{tab:user}
\end{table}




\section{Experiments}

\subsection{Implementation details}

We build our debiasing methods on the high-performing public repository of SJC~\cite{wang2022score}. For all the results, including SJC and ours, we run 10,000 steps to optimize the 3D fields, which takes about 20 minutes using a single NVIDIA 3090 RTX GPU and adds almost no overhead compared to the baseline. We set the hyperparameters of SJC to specific constants~\cite{wang2022score} and do not change them throughout the experiments.

\subsection{Evaluation Metrics}

Quantitatively evaluating a zero-shot text-to-3D framework is challenging due to the absence of ground truth 3D scenes that correspond to the text prompts. Existing works employ CLIP R-Precision~\cite{jain2022zero, poole2022dreamfusion}. However, it measures retrieval accuracy through projected 2D images and text input, making it unsuitable for quantifying the view consistency of a scene.

Therefore, to measure the view consistency of generated 3D objects quantitatively, we compute the average LPIPS~\cite{zhang2018unreasonable} between adjacent images, which we refer to as A-LPIPS. We sample 100 uniformly spaced camera poses from an upper hemisphere of a fixed radius, all directed towards the sphereâ€™s center at an identical elevation, and render 100 images from a 3D scene. Then, we average the LPIPS values evaluated for all adjacent pairs of images in the 3D scene, finally aggregating those averages across the scenes. The intuition behind this is that if there exist artifacts or view inconsistencies in a generated 3D scene, the perceptual loss will be large near those points.

In addition, to assess the faithfulness to the view-augmented prompt, we present a graph that illustrates the average CLIP similarities of rendered images for each azimuth, as determined by view-augmented prompts. This metric is designed to be high when the score-distillation pipeline effectively generates an accurate view of an object.

\begin{figure}[t]
    \centering
    \begin{subfigure}{
        \includegraphics[width=0.47\textwidth]{figures/abl_cat.pdf}
    }
    \end{subfigure}
    \hfill
    \begin{subfigure}{
        \includegraphics[width=0.47\textwidth]{figures/abl_panda.pdf}
    }
    \end{subfigure}
    \caption{\textbf{Improvement of view consistency through prompt and score debiasing.} We start from the baseline (SJC~\cite{wang2022score}) and apply score debiasing and prompt debiasing sequentially for each prompt, "a smiling cat" and "a cute and chubby panda munching on bamboo", respectively.}
    \label{fig:ablation}\vspace{-10pt}
\end{figure}


\subsection{Comparison with the baseline}

\paragraph{Quantitative results.}
We present quantitative results from 70 user prompts for the baseline~\cite{wang2022score}, our combined method, and the method without the removal of contradicting words from a user prompt. Our method produces more consistent 3D objects than the baseline, as demonstrated in Table~\ref{tab:alpips}. Note that removing contradictions in prompts indeed leads to better results with respect to A-LPIPS, meaning that the generated objects overall in each azimuth are consistent with our debiasing methods.

We also present adherence to the view-augmented prompts in Fig.~\ref{fig:clip-both}. The diagram illustrates shaded sections depicting the 90-degree front and back view zones, beginning from the left side. When comparing our unbiased outcomes to the baseline, we observe a clear and preferable pattern in CLIP similarities associated with the view-augmented prompts. In this pattern, the similarity with the view-augmented prompts reaches its highest point in the desired region. In contrast, the standard method exhibits minor fluctuations in CLIP similarities as we examine different angles in relation to the view prompts, implying less faithfulness to the viewing direction.

\begin{wraptable}{r}{5.0cm}
\vspace{-10pt}
\footnotesize  
\centering
\begin{tabular}{c c}
\toprule
Success-Baseline & Success-Ours \\
\midrule
29.3\% & 68.3\% \\
\bottomrule
\end{tabular}
\caption{\textbf{Success rate.}}
\label{tab:success}\vspace{-10pt}
\end{wraptable}

In addition to the user study outlined in Table~\ref{tab:user}, which evaluates view consistency, faithfulness to user prompts, and overall quality, we also report the success rate of the generation in Table~\ref{tab:success}. The success rate applies to 41 out of 70 prompts featuring countable faces. We marked as successful only those objects that do not exhibit the Janus problem, \textit{i.e.}, those with an accurate number of faces. Our method significantly outperforms the baseline in terms of success rate.

Overall, the experiments corroborate that our debiasing methods improve the realism and alleviate the Janus problem of generated 3D objects, without requiring any 3D guide~\cite{seo2023let} or introducing significant overhead or additional optimization steps to the zero-shot text-to-3D setting.

\vspace{-5pt}
\paragraph{Qualitative results.}
We present qualitative results in Fig.~\ref{fig:qual}.
In addition to the results of SJC~\cite{wang2022score}, which serves as the baseline for our experiments, we include those of Stable-DreamFusion~\cite{stable-dreamfusion}, an unofficial re-implementation of DreamFusion~\cite{poole2022dreamfusion} that utilizes Stable Diffusion~\cite{rombach2022high}. The results demonstrate that our methods significantly reduce the Janus, or view inconsistency problem. For example, given a user prompt "a majestic giraffe with a long neck," the whole body is consistently generated using our debiasing method, compared to the baseline with the Janus problem. Additionally, as a notable example, when considering "a mug with a big handle," our method successfully generates a mug with a single handle, while the counterparts generate multiple handles.

Additionally, to show that our method is not only applicable to SJC~\cite{wang2022score} with Stable Diffusion~\cite{rombach2022high}, but also to any text-to-3D frameworks that leverage score distillation, we present results on DreamFusion~\cite{poole2022dreamfusion} with DeepFloyd-IF, and on concurrent frameworks such as Magic3D~\cite{lin2022magic3d} and ProlificDreamer~\cite{wang2023prolificdreamer} in Appendix~\ref{appendix.other3dframework}, which show impressive results.\vspace{-5pt}

\subsection{Ablation study}
\paragraph{Ablation on debiasing methods.}
We present ablation results in Fig.~\ref{fig:ablation}, where we sequentially added prompt debiasing and score debiasing on top of the baseline. This demonstrates that they gradually improve the view consistency and reduce artifacts as intended.

Using prompt debiasing alone can resolve the multi-face problem to some extent. In the case of the prompt "a smiling cat", prompt debiasing eliminates the word "smiling" from the prompt. As can be seen in column 1 and column 3, the cat has a more realistic appearance compared with the baseline. However, the cat retains an additional ear. Sometimes, such as in the instance with the panda, it can even generate a new ear. Therefore, using prompt debiasing alone does not solve the problem of creating additional artifacts like ears. Applying score debiasing removes these extra ears in both cases, leading to more view-consistent text-to-3D generation in combination with prompt debiasing.\vspace{-5pt}


\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/monkey_final.pdf}
  \caption{\textbf{Dynamic clipping of 2D-to-3D scores.} The given user prompt is "a monkey eating ramen". Using static clipping, there is a tough compromise between 3D consistency and 2D faithfulness. Dynamic clipping achieves a better tradeoff between pixelation and many artifacts in the result.}
  \label{fig:dynamic}
  \vspace{-10pt}
\end{figure}

\vspace{-5pt}
\paragraph{Ablation on dynamic clipping.}
To show some examples of the effect of dynamic clipping, we compare the results with those of static clipping and no clipping in Fig.~\ref{fig:dynamic}. It demonstrates that naive static clipping can struggle to find a good compromise between 3D consistency and 2D faithfulness, which means lowering the threshold can eliminate more artifacts like extra ears or eyes to achieve better realism, but it also returns fairly pixelated and collapsed appearance, as can be seen in (c). Conversely, employing dynamic clipping produces visually appealing outcomes with artifacts eliminated, closely resembling the consistency of static clipping at a low threshold. Moreover, it preserves intricate shapes and details without any pixelation or degradation of the object's visual presentation.