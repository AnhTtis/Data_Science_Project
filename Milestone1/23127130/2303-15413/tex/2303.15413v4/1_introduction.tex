

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{figures/janus.pdf}
\caption{\textbf{Comparison between the baseline (SJC~\cite{wang2022score}) and ours (Debiased Score Distillation Sampling; D-SDS).} Our debiasing methods qualitatively reduce view inconsistencies in zero-shot text-to-3D generation and the so-called \emph{Janus problem}.}
\label{fig:qualitative}
\end{figure}

\section{Introduction}
Recently, significant advancements have been made in the field of zero-shot text-to-3D generation~\cite{jain2022zero}, particularly with the integration of score-distillation techniques~\cite{lin2022magic3d, wang2022score, poole2022dreamfusion, metzer2022latent} and diffusion models~\cite{karras2022elucidating, song2020score, song2019generative, ho2020denoising, saharia2022photorealistic, rombach2022high, song2021denoising, dhariwal2021diffusion} to optimize neural radiance fields~\cite{mildenhall2021nerf}. These methods provide a solution for generating a wide range of 3D objects from a textual input, without requiring 3D supervision. Despite their considerable promise, these approaches often encounter the view inconsistency problem. One of the most notable problems is the multi-face issue, also referred to as \emph{the Janus problem}~\cite{poole2022dreamfusion}, which is illustrated in the "Baseline" of Fig.~\ref{fig:qualitative}. This problem constrains the applicability of the methods~\cite{lin2022magic3d, wang2022score, poole2022dreamfusion, metzer2022latent}, but the Janus problem is rarely formulated or carefully analyzed in previous literature.

To address the problem of view inconsistency, we delve into the formulation of score-distilling text-to-3D generation presented in \cite{poole2022dreamfusion, wang2022score, metzer2022latent, lin2022magic3d}. We generalize and expand upon the assumptions about the gradients concerning the parameters of a 3D scene in previous works such as DreamFusion~\cite{poole2022dreamfusion} and Score Jacobian Chaining (SJC)~\cite{wang2022score}, and identify the main causes of the problem within the estimated score. The score can be further divided into the unconditional score and pose-prompt gradient, both of which interrupt the estimation of unbiased gradients concerning the 3D scene. Additionally, since a naive text prompt describes a canonical view of an image such as front view, prior text-to-3D generation works ~\cite{seo2023let,poole2022dreamfusion,wang2022score,metzer2022latent,lin2022magic3d} append a view prompt (\textit{e.g.}, "front view", "side view", "back view", "overhead view", \textit{etc}.) to the user's input, depending on the sampled camera angle, to better reflect its appearance from a different view. We present an analysis of the score with user prompts and view prompts and their effect on 3D, arguing that refining them is necessary for generating more realistic and view-consistent 3D objects.

Building on this concept and drawing inspiration from gradient clipping~\cite{mikolov2012statistical} and dynamic thresholding~\cite{saharia2022photorealistic}, we propose a score debiasing method that performs dynamic score clipping. Specifically, our method cuts off the score estimated by 2D diffusion models to mitigate the impact of erroneous bias (Fig.~\ref{fig:method} and Fig.~\ref{fig:score}). With this debiasing approach, we reduce artifacts in the generated 3D objects and alleviate the view inconsistency problem by striking a balance between faithfulness to 2D models and 3D consistency. Furthermore, by gradually increasing the truncation value, which aligns with the coarse-to-fine nature of generating 3D objects~\cite{dupont2022data,mildenhall2021nerf}, we achieve a better trade-off for 3D consistency without significantly compromising faithfulness.

While the first attempt to address the bias issue in the scores, we further present a prompt debiasing method. In contrast to prior works~\cite{poole2022dreamfusion,wang2022score,lin2022magic3d} that simply concatenate a view prompt and user prompt, our method reduces inherent contradiction between them by leveraging a language model trained with a masked language modeling (MLM) objective~\cite{devlin2018bert}, computing the point-wise mutual information. Additionally, we decrease the discrepancy between the assignment of the view prompt and camera pose by adjusting the range of view prompts. These enable text-to-image models~\cite{saharia2022photorealistic, rombach2022high, nichol2021glide} to predict accurate 2D scores, resulting in 3D objects that possess more realistic and consistent structures.