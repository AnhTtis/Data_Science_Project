\section{Conclusion}
This paper introduces our submission to the ABAW5 competition for the tasks of AU detection, expression recognition, and VA estimation. Our approach begins by pre-training a MAE in a self-supervised manner, using a large-scale facial image dataset. This enables the MAE to learn a variety of general features associated with human faces. Subsequently, we finetune the MAE using static images from Aff-wild2 dataset. Then we propose Temporal and Multi-modal Fusion~(TMF) to exploit the multi-modal information from the vision and audio temporal features. In participating in the ABAW5 competition, we won the first prizes in the AU track and EXPR track and second prize in the VA track. The quantities ablation studies indicate that each module and procedure of our approach can improve the model performance for affective tasks.

\section*{Acknowledgments}
The experiments and the data management and storage are supported by Netease Fuxi  Youling platform, based on Fuxi Agent-Oriented Programming~(AOP) system that is carefully designed to facilitate task modeling. This work is also supported by the 2022 Hangzhou Key Science and Technology Innovation Program (No. 2022AIZD0054), and the Key Research and Development Program of Zhejiang Province (No. 2022C01011). 

% In this paper, we introduce our transformer-based multimodal information fusion framework for AU detection and expression recognition in the ABAW3 Competition. We propose to exploit the static vision feature and three kinds of dynamic multimodal features to fully collect the human emotion clues. Besides, to integrate the static features and dynamic multimodal features, we utilize the transformer decoder structure. In participating in the ABAW3 competition, we won the first prizes in the AU track and EXPR track. The competition results prove the superiority of our method. Also, the quantities ablation studies indicate that each multimodal feature and fusion module can improve the model performance for affective tasks.
