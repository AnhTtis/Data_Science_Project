\begin{figure*}
    \centering
    \includegraphics[ width=1\linewidth]{images/pipeline3.pdf}
    %\vspace{-1.5em}
    \caption{Illustration of our pipeline. In the first stage, we pre-train a MAE in  SSL manner. The input face image is randomly masked 75\% and MAE learns to reconstruct the missing pixels. Then, the MAE encoder is fine-tuned on the Aff-wild2 for AU detection, EXPR classification, and VA estimation. In the second stage, we propose a transformer-based structure TMF that utilizes temporal vision and audio information to further facilitate the challenge tasks. }
    \label{fig:pipeline}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
This section will present our method proposed for AU detection, expression recognition and VA estimation in ABAW5 competition. The pipeline can be seen in Fig.~\ref{fig:pipeline}. It consists of two steps: MAE pre-train and Temporal and Multi-modal Fusion~(TMF). We first introduce the MAE structure with good generalization to extract the vision features. Subsequently, we employ the TMF to integrate the temporal acoustic features with the temporal visual features to extract more effective information for completing the task. Finally, we adopt some smoothing policies to generate the final predictions. 

%%%%%%%%%%%%%%%%
\subsection{MAE Pre-training}
Different from the traditional MAE, our MAE is pre-trained on the facial image dataset to focus on learning the facial vision features. We construct a large-scale facial image dataset that contains images from the existing facial image datasets, e.g., AffectNet~\cite{AffectNet}, CASIA-WebFace~\cite{CASIA-Webface}, CelebA~\cite{CelebA} and IMDB-WIKI~\cite{IMDB-WIKI}. The total number of images is about 2.17M.
Then we pre-train the MAE model on the dataset in a self-supervised manner. Specifically, our MAE consists of a ViT-Base encoder and a ViT decoder based on the structure of Vision Transformer (ViT)~\cite{ViT}. 
The MAE pre-training procedure follows a masking-then-reconstruct method, whereby images are first divided into a series of patches (16x16) and 75\% of them are randomly masked. 
These masked images are sent to the MAE encoder and the complete images should be reconstructed by the MAE decoder~(See Fig.~\ref{fig:pipeline} left Step1). The loss function of MAE pre-training is the pixel-wise L2 loss to make the reconstructed images close to the target images.

% \begin{figure}
%     \centering
%     \includegraphics[ width=1\linewidth]{images/MAE1.pdf}
%     %\vspace{-1.5em}
%     \caption{Description of MAE self-supervised pre-training and fine-tuning on the downstream tasks. (a) The input face image is randomly masked 75\% and MAE learns to reconstruct the missing pixels. (b) After SSL, the MAE encoder is fine-tuned on the Aff-wild2 for AU detection, EXPR classification, and VA estimation.}
%     \label{fig:mae}
% \end{figure}

Once the self-supervised learning is complete, we remove the MAE decoder and replace it with a fully connected layer attached to the MAE encoder (See Fig.~\ref{fig:pipeline} left Step2). This allows us to fine-tune downstream tasks: AU detection, expression recognition, and VA estimation on the Aff-wild2 dataset. It is important to note that this process is based on frame-wise training, without taking into account temporal or other modal information. The corresponding loss functions for these three tasks are as follows:

\begin{equation}
    \mathcal{L}_{\textit{AU\_CE}}=-\frac{1}{12}\sum_{j=1}^{12} W_{au_j}[y_{j}\log\hat{y}_{j} 
     + (1-y_{j})\log(1-\hat{y}_{j})]. 
\label{eq:1}
\end{equation}
\begin{equation}
    \mathcal{L}_{\textit{EXPR\_CE}}=-\frac{1}{8}\sum_{j=1}^{8}W_{exp_j} z_{j}\log\hat{z}_{j}.
\label{eq:2}
\end{equation}

\begin{equation}
\begin{split}
\mathcal{L}_{\textit{VA\_CCC}}= 1 - CCC(\hat{v}_{batch_i},v_{batch_i}) \\
     + 1 - CCC(\hat{a}_{batch_i},a_{batch_i}) 
\end{split}
\label{eq:3}
\end{equation}
\begin{equation}
\label{CCC}
    \textit{CCC}(\mathcal{X},\mathcal{\hat{X}})=\frac{2\rho_{\mathcal{X\hat{X}}}\delta_{\mathcal{X}}\delta_{\mathcal{\hat{X}}}}{\delta_{\mathcal{X}}^2+\delta_{\mathcal{\hat{X}}}^2+(\mu_{\mathcal{X}}-\mu_{\mathcal{\hat{X}}})^{2}}.
\end{equation}
where $\hat{y}$, $\hat{z}$, $\hat{v}$ and $\hat{a}$ denote the model's predictions for AU, expression category, Valence, and Arousal, respectively. The symbols without hats refer to the ground truth. $\delta_{\mathcal{X}}$,$\delta_{\mathcal{\hat{X}}}$ indicate the standard deviations of $\mathcal{X}$ and $\mathcal{\hat{X}}$, respectively. $\mu_{\mathcal{X}}$ and $\mu_{\mathcal{\hat{X}}}$ are the corresponding means and  $\rho_{\mathcal{X\hat{X}}}$ is the correlation coefficient.
For the AU and EXPR tasks, we utilize weighted cross-entropy as the loss function. The weights for different categories, represented by $W_{au_j}$ and $W_{exp_j}$, are inversely proportional to the class number in the training set. 

%%%%%%%%%%%%%%%%
\subsection{Temporal and Multi-modal Fusion}
To further exploit the temporal and multi-modal features for AU, EXPR and VA tasks, we design the sequence-based model which combines the audio features. 
To concretely, we first divide the videos into several short clips, each having an equivalent frame number of K. 
We construct the Temporal and Multi-modal Fusion~(TMF) to perform the sequence-wise training which can be seen in Fig.~\ref{fig:pipeline} right. 

Given a video clip $C_i$ and the corresponding audio clips $A_i$, we use the finetuned MAE encoder and some existing pre-trained audio feature extractor models~(e.g. Hubert~\cite{hubert}, Wav2vec2~\cite{baevski2020wav2vec}, vggish~\cite{vggish}.) to extract the vision and acoustic features $F^{i}_{vis}$ and $F^{i}_{aud}$ for each frame separately. 
Then we concatenate $F^{i}_{vis}$ and $F^{i}_{aud}$ and sent them into a Transformer~\cite{vaswani2017attention} encoder structure to exploit the temporal correlations between them. 
The Transformer encoder comprises of four encoder layers with a dropout ratio of 0.3. The output of the Transformer encoder is then directed towards a fully connected layer to resize the final output size, which is tailored to fit various tasks. 
In the training process, the parameters of MAE encoder and audio feature extractor are fixed without training. During the computation of loss, we flatten the sequence result of a clip and use the same loss function as Equ.~\ref{eq:1}, \ref{eq:2}, \ref{eq:3}.

%\subsection{Post-processing}
Since the final prediction is made frame-by-frame, applying a smoothing policy can enhance the stability of the predictions. 
We find that the cropped face images from videos occasionally miss some frames potentially due to the frames not being able to detect faces. 
Therefore, to maintain continuity, we use the nearest frame to replace the lost frame and produce the prediction for each frame.
Then, we use the Gaussian filter to smooth the probability for AU or EXPR and VA values. The sigma of Gaussian filter is set according to different tasks. The detailed setting can be seen in the experiment section. 

%In AU track, we find that a fixed threshold of 0.5 is not the optimal choice for detecting the occurrence of each AU. Through searching the threshold space, we apply the dynamic thresholds for each AU, which can further enhance the F1 scores. 






%%%%%%%%%%%%%%%%
% \subsection{Dual Branch structure}
% To further enhance the model generalization, we propose a two-branch structure to perform collaboration training. 
% After the BLB training, we fix its parameters and share the multi-modal feature extraction module~(MAE Encoder and Audio Feature Extractor) with the Collaboration Learning Branch~(CLB). CLB has the same structure as BLB but different train data distribution $D_{CLB}$. Specifically, we first filter the hard training samples which are hard to converge after training. For example, we exclude samples that continue to display a large sequence loss even after the completion of training. We add these hard samples into $D_{CLB}$ in order to focus on learning more hard samples.
% To preserve the original training data distribution of $D_{init}$, we augment $D_{CLB}$ with a selection of random samples from $D_{init}$ until $D_{CLB}$ contains the same number of samples as $D_{init}$. 


% After building up $D_{CLB}$, we commence with the collaboration training by our proposed Dual-branch collaboration Learning~(DCL). Given a sample~($C_i$,$A_i$) from $D_{init}$ and a sample~($C_j$,$A_j$) from $D_{CLB}$, the corresponding logits outputs of BLB and CLB are $h^{i}_{BLB}$ and $h^{j}_{CLB}$, respectively. Then we perform the randomly linear interpolation in the logits space.
% The interpolated logit can be denoted as:
% \begin{equation}
% \begin{split}
% h = \alpha h^{i}_{BLB} \oplus(1-\alpha) h^{j}_{CLB},
% \end{split}
% \label{eq:res_integrate}
% \end{equation}
% where $\oplus$ denotes the element-wise sum and $\alpha$ is randomly sampled from the Beta distribution controlled by the hyper-parameters $\tau_1$ and $\tau_2$. We denote the final output after logit $h$ as $\hat{o}$. The final loss function of DCL is as follows:
% \begin{equation}
% \begin{split}
% \mathcal{L}_{DLC} = \alpha L(\hat{o},y_{i}) + (1-\alpha) L(\hat{o},y_{j}),
% \label{eq: loss_stage2}
% \end{split}
% \end{equation}
% where $y$ denotes the corresponding labels of different tasks, $L(,)$ represents the corresponding loss functions~\ref{eq:1}, \ref{eq:2}, \ref{eq:3} mentioned before. 

% By performing this linear interpolation, we effectively augment the logits space and thereby construct a greater number of potential unseen samples. This approach has the benefit of enriching the feature space and enhancing the model generalization.
