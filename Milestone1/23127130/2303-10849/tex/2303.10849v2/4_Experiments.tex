
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment}

%%%%%%%%%%%%%%%%
\subsection{Experimental Setting}

We pre-processed all videos in the Aff-Wild2 datasets into frames by OpenCV and employ the OpenFace~\cite{baltrusaitis2018openface} detector to crop all facial images into $224\times 224$ scale. We find that OpenFace incorrectly detects some images and there are no faces in them. 
We use the NetEase Fuxi Youling crowdsourcing platform to check and remove incorrect images.
We pre-train MAE on large-scale facial image datasets for 800 epochs using the AdamW~\cite{loshchilov2017adamw} optimizer. We set the batch size as 4096 and the learning rate as 0.0024. Our training process is implemented based on PyTorch and trained on 8 NVIDIA A30 GPUs. 
In the MAE fine-tuning phase, we set the batch size to 512 and the learning rate to 0.0001, while using AdamW as the optimizer.
During TMF training, we set the video clip length to 100, with a batch size of 32 and a learning rate of 0.0001. The TMF training process was completed in approximately 20 epochs using the AdamW optimizer. All the experiments are carried out with the Fuxi Youling Platform that is based Agent-Oriented Programming (AOP) in order to facilitate task modeling.
% In the DCL training, we set the $\alpha$ to follow the distribution of $Beta(2,2)$, other experimental settings are the same as single BLB training.

%Moreover, we employed several training techniques to improve the model's performance. Specifically, we implemented a cosine annealing scheduler with a warmup to adjust the learning rate. We also utilized exponential moving average~(EMA) to stabilize the model weights during training and achieve a more robust result. Lastly, we leveraged the model soup approach~\cite{wortsman2022model} to further enhance the model's performance on the validation set. All the experiments are carried out with the Fuxi Youling Platform that is based Agent-Oriented Programming (AOP) in order to facilitate task modeling.

%%%%%%%%%%%%%%%%
\textbf{Metrics.} For AU detection and expression classification, we calculate the F1-Score~(F1) for each class to evaluate the prediction results. For VA estimation, we calculate the Concordance Correlation Coefficient (CCC) for valence and arousal respectively. The definition of CCC can be seen equ.~\ref{CCC}. In the case of ERI estimation, we utilize Pearson's Correlation Coefficient (PCC) for each class as the metric. The specific definitions for each challenge are as follows:
\begin{equation}
S_{AU} = \frac{1}{N_{au}}\sum_{}^{}  F1_{au_{i}} 
\label{equ:s_au}
\end{equation}

\begin{equation}
S_{EXP}= \frac{1}{N_{exp}}\sum_{}^{}  F1_{exp_{i}}
\label{equ:s_exp}
\end{equation}

\begin{equation}
S_{VA} = 0.5*(CCC(\hat{v},v)+CCC(\hat{a},a))
\end{equation}

\begin{equation}
S_{ERI} = \frac{1}{N_{exp}}\sum_{}^{} PCC(\hat{p}_{exp_{i}},p_{exp_{i}})
\end{equation}

\begin{equation}
    PCC = \frac{Cov({x},{\hat{x}})}{\delta _{{x}}\delta _{{\hat{x}}}}
    \label{equ:pcc}
\end{equation}
where $Cov(,)$ represents the covariance.

%%%%%%%%%%%%%%%%
\subsection{Results on Validation set}

\begin{table*}[!t]
\renewcommand\arraystretch{1.05} 
\centering
{
  %\setlength{\tabcolsep}{1.5mm}{
  \begin{tabular}{l|cccccccccccc|c}
    \hline
    \textbf{Val Set} &\textbf{AU1} &\textbf{AU2} &\textbf{AU4} &\textbf{AU6} &\textbf{AU7} &\textbf{AU10} &\textbf{AU12} &\textbf{AU15} &\textbf{AU23} &\textbf{AU24} &\textbf{AU25} &\textbf{AU26} &\textbf{Avg.}\\[1pt]
    \hline \hline
    Official & 55.26 & 51.35 & 56.70 & 67.25 & 75.75 & 75.11 & 75.82 & 31.21 & 17.34 & \textbf{33.77} & 83.90 & 42.26 & 55.86\\
    fold-1   & \textbf{64.94} & 52.31 & \textbf{71.84} & 74.59 & 69.10 & 69.92 & 74.05 & 35.38 & \textbf{28.51} & 21.56 & 77.97 & 42.14 & 56.86 \\
    fold-2   & 63.90 & \textbf{53.86} & 71.59 & \textbf{75.72} & \textbf{77.15} & \textbf{76.26} & \textbf{79.39} & 27.78 & 28.28 & 23.86 & \textbf{86.20} & 40.56 & \textbf{58.71}\\
    fold-3   & 61.80 & 51.10 & 60.85 & 70.70 & 73.08 & 75.15 & 74.84 & 29.79 & 23.71 & 27.35 & 79.22 & \textbf{48.33} & 56.33 \\
    fold-4   & 51.59 & 39.27 & 53.40 & 64.69 & 66.73 & 71.83 & 72.80 & \textbf{44.44} & 23.87 & 27.60 & 76.70 & 39.12 & 52.67 \\
    fold-5   & 52.69 & 44.59 & 60.90 & 70.67 & 70.11 & 73.25 & 76.04 & 43.84 & 23.86 & 10.30 & 76.96 & 39.96 & 53.60 \\
    \hline
  \end{tabular}}
  \caption{The AU F1 scores~(in \%) of models that are trained and tested on different folds (including the original training/validation set of \textit{Aff-Wild2} dataset). The highest and lowest scores are both indicated in bold.}
  \label{tab:AU_F1_val}
  %}
\end{table*}

\begin{table*}[!t]
\renewcommand\arraystretch{1.05} 
\centering
{  
  %\setlength{\tabcolsep}{1.5mm}{
  \begin{tabular}{l|cccccccc|c}
    \hline
    \textbf{Val Set} &\textbf{Neutral} &\textbf{Anger} &\textbf{Disgust} &\textbf{Fear} &\textbf{Happiness} &\textbf{Sadness} &\textbf{Surprise} &\textbf{Other} &\textbf{Avg.}\\[1pt]
    \hline \hline
    Official&  65.11 & 44.61 & 48.91 & 18.85 & 56.46 & 60.95 & 32.20 & 64.32 & 48.93 \\
    fold-1 &   61.92 & 34.22 & 46.96 & 16.96 & 53.65 & 77.77 & 30.08 & 50.12 & 46.46 \\
    fold-2 &   74.60 & 16.22 & 41.49 & \textbf{70.22} & \textbf{65.26} & 57.42 & \textbf{40.29} & 60.31 & 53.23 \\
    fold-3  &  62.16 & 36.00 & 21.99 & 19.40 & 60.12 & 65.68 & 32.38 & \textbf{72.16} & 46.24 \\
    fold-4 &   63.67 & \textbf{57.74} & \textbf{56.09} & 15.09 & 64.97 & \textbf{88.98} & 25.18 & 72.01 & \textbf{55.47} \\
    fold-5 &   \textbf{77.54} & 25.86 & 13.12 & 19.50 & 57.36 & 42.75 & 29.51 & 50.57 & 39.53 \\
    
    \hline
  \end{tabular}}
  \caption{The expression F1 scores~(in \%) of models that are trained and tested on different folds (including the original training/validation set of \textit{Aff-Wild2} dataset). The highest and lowest scores are both indicated in bold.}
  \label{tab:exp_F1_val}
  %}
\end{table*}

\textbf{Action Unit Detection Challenge.}
For AU detection, We evaluate the performance of our model using the F1 scores in equ.~\ref{equ:s_au}. To improve the generalization capability of our model, we also perform 5-fold cross-validation based on randomly split videos from the existing labeled data. We present the F1 scores of each AU and their average F1 in Tab.~\ref{tab:AU_F1_val}. From the table, we find that the performances of AU4, AU15, and AU24 in different data splits have a large variance~(AU4: 53.40\% $ \sim $ 71.84\%, AU15: 27.78\% $ \sim $ 44.44\%, AU24: 17.34\% $ \sim $ 28.51\%). The reason for this could be that the number of these AUs is relatively low, which means the learning for these AUs is not sufficient and heavily relies on the data splits.   

% \begin{table}[]
% \small
%     \centering
%     \setlength{\tabcolsep}{1.0mm}{
%     \begin{tabular}{c|cccccc}
%     \hline
%        \multirow{2}{*}{Method} & \multicolumn{6}{c}{Validation Set} \\ \cline{2-7} 
%         & Official & fold1 & fold2 & fold3 & fold4 & fold5 \\
%        \hline\hline
%        Our-MAE  & \textbf{0.5527} & 0.5430 & 0.5724 & 0.5440 & 0.5179  &  \textbf{0.5416} \\
%        Our-BLB & 0.5501 & \textbf{0.5547} & \textbf{0.5842} & \textbf{0.5441} & \textbf{0.5240} &  0.5337 \\
%        % Our-BLB & 0.5501 & 0.5547 & 0.5842 & 0.5441 & 0.5240 &  0.5337 \\
%        % Our-DCL & \textbf{0.5667} & \textbf{0.5647} & \textbf{0.5929} & \textbf{0.5460} & \textbf{0.5345} & 0.5411 \\
%        \hline
%     \end{tabular}
%     \caption{AU: F1-score on the official and 5-fold validation set.}
%     \label{tab:val_AU}    
%     }
% \end{table}

\textbf{Expression Classification Challenge.}
For the EXPR challenge, we also perform 5-fold cross-validation based on randomly selected video clips from existing labeled data. We show our experimental results on the official and 5-fold validation sets in Tab. \ref{tab:exp_F1_val}.  We evaluate the model by the average F1 metric in equ.~\ref{equ:pcc}. From the table, we also find the model performances for Anger, Disgust, and Fear are relatively unstable~(Anger: 16.22\% $ \sim $ 57.74\%, Disgust: 13.12\% $ \sim $ 56.09\%, Fear: 15.09\% $ \sim $ 70.22\%). At the same time, these three categories have the lowest frequency of occurrence in the dataset, which leads to their poor performances.

% \begin{table}[]
% \small
%     \centering
%     \setlength{\tabcolsep}{1.0mm}{
%     \begin{tabular}{c|cccccc}
%     \hline
%        \multirow{2}{*}{Method} & \multicolumn{6}{c}{Validation Set} \\ \cline{2-7} 
%         & Official & fold1 & fold2 & fold3 & fold4 & fold5 \\
%        \hline\hline
%        Our-MAE  & 0.4679 & 0.4203 & 0.4709 & 0.4241 & 0.5066  & 0.3493 \\
%        Our-BLB & \textbf{0.4817} & \textbf{0.4646} & \textbf{0.5323} & \textbf{0.4624} & \textbf{0.5547} & \textbf{0.3953} \\
%        % Our-BLB & 0.4817 & 0.4646 & 0.5323 & 0.4624 & 0.5547 & 0.3953 \\
%        % Our-DCL & \textbf{0.4952} & \textbf{0.4758} & \textbf{0.5376} & \textbf{0.4634} & \textbf{0.5589} & \textbf{0.3981} \\
%        \hline
%     \end{tabular}
%     \caption{EXPR: F1-score on the official and 5-fold validation set.}
%     \label{tab:val_expr}    
%     }
% \end{table}

\textbf{Valence-Arousal Estimation Challenge.}
For VA estimation, We evaluate the model by the CCC of Valence and Arousal in equ.~\ref{CCC}. To enhance the model generalization, we also perform the 5-fold cross-validation according to random video split in the existing labeled data. The experimental results can be seen in Tab. \ref{tab:va_F1_val}.



\begin{table}[!t]
\renewcommand\arraystretch{1.05} 
\centering
{
  %\setlength{\tabcolsep}{1.5mm}{
  \begin{tabular}{l|cc|c}
    \hline
    \textbf{Val Set} &\textbf{Valence} &\textbf{Arousal} & \textbf{Avg.}\\[1pt]
    \hline \hline
    Official&  0.4643 & 0.6407 &  0.5525\\
    fold-1 &   0.5927 & 0.6542 &  0.6234\\
    fold-2 &   0.5647 & 0.6267 &  0.5957\\
    fold-3  &  0.5679 & 0.6959 &  0.6319\\
    fold-4 &   0.5567 & 0.6456 &  0.6011\\
    fold-5 &   \textbf{0.6478} & \textbf{0.7056} &  \textbf{0.6767}\\
    
    \hline
  \end{tabular}}
  \caption{The VA CCC scores of models that are trained and tested on different folds (including the original training/validation set of \textit{Aff-Wild2} dataset). The highest and lowest scores are both indicated in bold.}
  \label{tab:va_F1_val}
  %}
\end{table}


% \begin{table}[]
% \small
%     \centering
%     \setlength{\tabcolsep}{1.0mm}{
%     \begin{tabular}{c|cccccc}
%     \hline
%        \multirow{2}{*}{Method} & \multicolumn{6}{c}{Validation Set} \\ \cline{2-7} 
%         & Official & fold1 & fold2 & fold3 & fold4 & fold5 \\
%        \hline\hline
%        Our-MAE & \textbf{0.4758} & 0.5496 & 0.5097 & 0.5333 & 0.5005 & 0.6000 \\
%        Our-BLB & 0.4643 & \textbf{0.5927} & \textbf{0.5647} & \textbf{0.5679} & \textbf{0.5567} & \textbf{0.6478} \\
%        % Our-BLB & 0.4643 & \textbf{0.5927} & \textbf{0.5647} & 0.5679 & 0.5567 & 0.6478 \\
%        % Our-DCL & 0.4698 & 0.5861 & 0.5632 & \textbf{0.5703} & \textbf{0.5581} & \textbf{0.6641} \\
%        \hline
%     \end{tabular}
%     \caption{Valence: CCC on the official and 5-fold validation set.}
%     \label{tab:val_expr}    
%     }
% \end{table}

% \begin{table}[]
% \small
%     \centering
%     \setlength{\tabcolsep}{1.0mm}{
%     \begin{tabular}{c|cccccc}
%     \hline
%        \multirow{2}{*}{Method} & \multicolumn{6}{c}{Validation Set} \\ \cline{2-7} 
%         & Official & fold1 & fold2 & fold3 & fold4 & fold5 \\
%        \hline\hline
%        Our-MAE & 0.6208 & 0.6216 & 0.5805 & 0.6692 & 0.6054 & 0.6657 \\
%        Our-BLB & \textbf{0.6407} & \textbf{0.6542} & \textbf{0.6267} & \textbf{0.6959} & \textbf{0.6456} & \textbf{0.7056} \\
%        % Our-BLB & 0.6407 & \textbf{0.6542} & \textbf{0.6267} & \textbf{0.6959} & 0.6456 & 0.7056 \\
%        % Our-DCL & \textbf{0.6443} & 0.6513 & 0.6247 & 0.6729 & \textbf{0.6478} & \textbf{0.7110} \\
%        \hline
%     \end{tabular}
%     \caption{Arousal: CCC on the official and 5-fold validation set.}
%     \label{tab:val_expr}    
%     }
% \end{table}




\begin{table}[]
\small
    \centering
    \setlength{\tabcolsep}{4.0mm}
    {
    \begin{tabular}{c|cc}
    \hline
       \multirow{2}{*}{Team} & \multicolumn{2}{c}{Test Set} \\ \cline{2-3} 
        & Rank & F1-score  \\
       \hline\hline
       PRL~\cite{ABAW5_AU5} & \#5 & 51.01  \\
       SZFaceU~\cite{ABAW5_AU4} & \#4 & 51.28  \\
       USTC-IAT-United~\cite{ABAW5_AU3} & \#3 & 51.44  \\
       SituTech~\cite{ABAW5_AU2} & \#2 & 54.22  \\
       Ours & \#1 & \textbf{55.49}  \\
       \hline
    \end{tabular}
    \caption{Final competition results ~(average F1 score in \%) on the AU test set of ABAW5.}
    \label{tab:AU_test_set}    
    }
\end{table}


\subsection{Results on Test Set}
\textbf{Action Unit Detection Challenge}
In ABAW5 competition, we need to predict the labels of the official test set. Our proposed method achieves an average F1 score of 55.49\% for AU detection, thereby winning first place in this track (See Tab.~\ref{tab:AU_test_set}). 
The second-place team SituTech and the fourth-place team SZFaceU in this track also
leverage MAE features. Similar to our approach, they use a large-scale facial image dataset for MAE pre-training and use it as the feature extractor. 
Apart from this, SituTech incorporates additional visual features from DenseNet~\cite{iandola2014densenet}, IResNet100~\cite{iresnet}, and MobileNet~\cite{howard2017mobilenets}, pre-trained on the expression recognition task based on AffectNet~\cite{AffectNet}, FER+~\cite{barsoum2016training}, and RAF-DB~\cite{RAF}. In contrast, we fine-tune MAE on Aff-Wild2 and incorporate it into our multi-modal information training framework.  
SZFaceU proposes a Spatial-Temporal Graph Learning module to construct a graph representation for spatial-Temporal features. 
The third-place team proposes to use LANet~\cite{LANet} to extract the local features for AU. They model the correlations between AUs through Graph Neural Networks.
PRL utilizes the Video Vision Transformer~\cite{arnab2021vivit} to capture the temporal expression movements in the video.

\textbf{Expression Classification Challenge}
In Tab.~\ref{tab: EXP_test_set}, our approach achieves an average F1 score of 41.21\% in EXPR track, winning first place in this track. Our approach, as well as the second-place team (SituTech), the third-place team (CtyunAI) incorporate multi-modal information from both vision and acoustic modalities. 
Besides the above-mentioned vision features, SituTech uses the audio features of Hubert~\cite{hubert}, wav2vec~\cite{baevski2020wav2vec} and ECAPA-TDNN~\cite{ecapa}.
CtyunAI use three kinds of vision features: arcface~\cite{deng2019arcface}, EfficientNet-b2~\cite{savchenko2022video} and DAN~\cite{wen2021dan} and two kinds of audio features: wav2vec2~\cite{baevski2020wav2vec} and wav2vec2-emotion~\cite{pepino2021wav2vec2_emo}. 
HFUT-MAC uses POSTER2~\cite{mao2023poster} as the feature extractor and employs an affine module to align features of varying dimensions to a uniform dimension. Subsequently, a transformer is utilized to integrate the temporal features. 
HSE-NN-SberAI applies EfficientNet CNN from HSEmotion library~\cite{savchenko2022hsemotion} to extract effective prior expression knowledge and use MLP for classification.




\begin{table}[]
\small
    \centering
    \setlength{\tabcolsep}{4.0mm}
    {
    \begin{tabular}{c|cc}
    \hline
       \multirow{2}{*}{Team} & \multicolumn{2}{c}{Test Set} \\ \cline{2-3} 
        & Rank & F1-score  \\
       \hline\hline
       HSE-NN-SberAI~\cite{ABAW5_EXPR5} & \#5 & 32.92  \\
       HFUT-MAC~\cite{ABAW5_EXPR4} & \#4 & 33.37  \\
       CtyunAI~\cite{ABAW5_EXPR3} & \#3 & 35.32  \\
       SituTech~\cite{ABAW5_AU2} & \#2 & 40.72  \\
       Ours & \#1 & \textbf{41.21}  \\
       \hline
    \end{tabular}
    \caption{Final competition results ~(average F1 score in \%) on the EXPR test set of ABAW5.}
    \label{tab: EXP_test_set}    
    }
\end{table}

\textbf{VA Estimation Challenge}
In Table~\ref{tab: VA_test_set}, our approach achieves an average CCC score of 0.6372 in the VA track, securing second place. Notably, our results are highly competitive with the first-place team's score of 0.6414. 
After extracting three modalities of information: vision, acoustic and text, CBCR use TCN to capture the temporal features and channel attention network (CAN) for features fusion.
The methods of other teams are roughly similar to the analysis of other challenges. 




\begin{table}[]
\small
    \centering
    \setlength{\tabcolsep}{4.0mm}
    {
    \begin{tabular}{c|cc}
    \hline
       \multirow{2}{*}{Team} & \multicolumn{2}{c}{Test Set} \\ \cline{2-3} 
        & Rank & CCC  \\
       \hline\hline
       HFUT-MAC~\cite{ABAW5_EXPR4} & \#5 & 0.5342  \\
       CtyunAI~\cite{ABAW5_EXPR3} & \#4 & 0.5666  \\
       CBCR~\cite{ABAW5_VA3} & \#3 & 0.5913  \\
       SituTech~\cite{ABAW5_AU2} & \#1 & \textbf{0.6414}  \\
       Ours & \#2 & 0.6372  \\
       \hline
    \end{tabular}
    \caption{Final competition results ~(average CCC) on the VA test set of ABAW5.}
    \label{tab: VA_test_set}    
    }
\end{table}



\subsection{Ablation Studies}
In this section, we conduct several experiments to discuss the significance of each module of our approach, including MAE finetuning training, Temporal and Multi-modal Fusion~(TMF), the selection of features and the policies of post-processing. All the experiments are perform on the official training and validation set.


\begin{table}[]
\small
    \centering
    \setlength{\tabcolsep}{1.0mm}
    {
    \begin{tabular}{c|c|c|c}
    \hline
        Method & AU  & EXPR  & VA \\ 
       \hline\hline
       w/o. MAE finetune & 51.04 & 41.34 & 0.5169\\
       w/o TMF & 55.27 & 46.79 & 0.5483\\
       Ours & \textbf{55.86} & \textbf{48.93} & \textbf{0.5525}\\
       \hline
    \end{tabular}
    \caption{Ablation studies that discuss the significance of MAE fine-tune and Temporal and Multi-modal Fusion.  The evaluation metrics used for AU, EXPR, and VA are average F1 (\%), average F1 (\%), and CCC, respectively. }
    \label{tab: abla1}    
    }
\end{table}


\textbf{MAE finetuning training.}
To prove the effectiveness of our procedure of finetuning MAE on Aff-wild2~(See Fig.~\ref{fig:pipeline} (left: step 2)), we conduct the experiment that directly uses the initial MAE features to operate the TMF training. The result can be seen in Tab.~\ref{tab: abla1} (w/o. MAE finetune). It can be found that there is a significant drop in the performance metrics across all three tracks. The average F1 score for AU decreases from 55.86\% to 51.04\%, and for EXPR it decreased from 48.93\% to 41.34\%. The average CCC of VA also decreases from 0.5525 to 0.5169. This illustrates that MAE finetuning can effectively exploit the static vision features based on a single image, which provides valuable prior knowledge for learning the temporal vision feature.

\textbf{Temporal and Multi-modal Fusion.}
To illustrate the significance of TMF, we conduct an experiment that removes the TMF and uses the model trained on the single frame for evaluation. The results are presented in Tab.~\ref{tab: abla1}~(w/o TMF). It is evident that the inclusion of TMF leads to an  improvement in AU F1, EXPR F1, and VA CCC scores by 0.59\%, 2.14\% and 0.0042, respectively. This proves that the temporal and multi-modal features can further exploit more hidden clues for these three tasks. Notably, TMF is particularly effective for EXPR compared to other challenges.

\textbf{The choice of multi-modal features.}
In this section, we analyze the impact of using different multi-modal features, as presented in Tab.~\ref{tab: abla2}. Specifically, we experiment with two types of vision features: expression embedding from DLN~\cite{DLN} and the aforementioned MAE features. Our results demonstrated that using MAE features alone has a more significant advantage in all three challenges. Hence, for multi-modal information fusion, we only utilized the features extracted by MAE.
In terms of audio features, we try three kinds of features: Hubert~\cite{hubert}, wav2vec2~\cite{baevski2020wav2vec} and Vggish~\cite{vggish}. From Tab.~\ref{tab: abla2}, we observe that the  utilization of audio features does not enhance the performance of the AU task. However, the incorporation of audio features enhances the performance of the EXPR and VA challenges. 
For EXPR and VA tracks, our experiments show that the best performance is achieved by combining Hubert and Vggish features. 


\begin{table}[]
\small
    \centering
    \setlength{\tabcolsep}{0.8mm}
    {
    \begin{tabular}{c|c|c|c|c}
    \hline
        Vis\_fea & Audio\_fea & AU  & EXPR  & VA \\ 
       \hline\hline
        MAE & - & \textbf{55.86} & 47.73 & 0.5483\\
        DLN & - & 50.49 & 39.12 & 0.4789 \\
        MAE + DLN & - & 54.76 & 44.69  & 0.5169\\
       \hline\hline
       MAE & Hubert  & 54.89 &  48.03 & 0.5405\\ 
       MAE & Wav2vec2 & 53.14 &  47.91  & 0.5364\\
       MAE & Vggish  & 51.68 &  47.71 & 0.5157\\
       MAE & Hubert+Vggish & 52.28 & \textbf{48.93} & \textbf{0.5525} \\
       MAE & Hubert+wav2vec2 & 52.55 & 48.18  & 0.5471 \\
       MAE & Hubert+wav2vec2+Vggish  & 53.54 & 47.89 & 0.5521\\
    \hline
    \end{tabular}
    \caption{Ablation studies that discuss the significance of different selection of vision and audio features. The evaluation metrics used for AU, EXPR, and VA are average F1 (\%), average F1 (\%), and CCC, respectively.}
    \label{tab: abla2}    
    }
\end{table}



%\subsubsection{Post-processing policies}
%\noindent\textbf{Smooth Policy}
\textbf{Smooth Policy}
Due to the frame-by-frame prediction of the challenges, smooth policies can effectively eliminate some noisy predictions and enhance prediction stability. In this part, we discuss the different policies we used for smoothing in Tab.~\ref{tab:abla3}. From the table, we can find that the metrics have varying degrees of improvement by using smooth policies. We use Scipy library to realize these smooth approaches. In different challenges, there exist slight differences in hyper-parameter settings. For instance, we set the sliding window size of median\_filter and average\_filter in AU, EXP and VA tracks as 10, 25 and 50, respectively. We set the sigma of gaussian\_filter in AU, EXP and VA tracks as 5, 25 and 25, respectively. This is because human facial physical movements of expressions tend to change frequently over a short period of time, whereas emotional states often exhibit a small range of change. Therefore, AU changes are more sensitive compared to EXP and VA. Our final prediction for the official test set combines some of the mentioned smooth policies. 

\begin{table}[]
\small
    \centering
    \setlength{\tabcolsep}{0.8mm}
    {
    \begin{tabular}{c|c|c|c}
    \hline
        Policy & AU &  EXP  &  VA \\ 
       \hline\hline
        w/o. smooth & 55.86 & 48.93 & 0.5525 \\
        gaussian\_filter & \textbf{56.01} & \textbf{49.16} & \textbf{0.5640} \\
        median\_filter & 55.91 & 48.98 & 0.5537 \\
        average\_filter  & 55.95 & 49.08 & 0.5595 \\
    \hline
    \end{tabular}
    \caption{Ablation studies that discuss the influence of different smooth policies. The evaluation metrics used for AU, EXPR, and VA are average F1 (\%), average F1 (\%), and CCC, respectively.}
    \label{tab:abla3}    
    }
\end{table}

%\noindent\textbf{Thresholds for AU}
%Due to the data imbalance in the dataset~\cite{kollias2022abaw}, the learning of different AUs is of varying quality. Adjusting the predicted thresholds for different AUs can help improve the F1 metric on the validation set. By searching the thresholds, we find the most appropriate thresholds for AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25 and AU26 are 0.5, 0.6, 0.5, 0.3, 0.4, 0.4, 0.4, 0.5, 0.5, 0.5, 0.4 and 0.5. The final average F1 score can be increased from 55.86\% to 56.66\%. While the average F1 score on the validation set shows a clear improvement, it is important to note that there is a potential risk of overfitting if the data distribution of the test set significantly differs from that of the validation set.

% \subsubsection{ERI Estimation}
% We show our experimental results of different stages of our framework on the official validation set in Tab. \ref{tab:val_expr}.  We evaluate the model by the Pearson's Correlations Coefficient (PCC) metric in equ.~\ref{equ:pcc}. To enhance the model generalization, we also perform the 5-fold cross-validation according to random video split in the existing labeled data. The final prediction of test set comes from the ensemble results of these models. 

% \begin{table}[]
% \small
%     \centering
%     \setlength{\tabcolsep}{1.0mm}{
%     \begin{tabular}{c|cccccc}
%     \hline
%        \multirow{2}{*}{Method} & \multicolumn{6}{c}{Validation Set} \\ \cline{2-7} 
%         & Official & fold1 & fold2 & fold3 & fold4 & fold5 \\
%        \hline\hline
%     Ours  & 0.4120 & 0.4229 & 0.4199 & 0.4229&  0.4266  & 0.4049 \\

%        \hline
%     \end{tabular}
%     \caption{PCC on the official and 5-fold validation set.}
%     \label{tab:val_eri}    
%     }
% \end{table}

