
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

In recent years, there has been a growing interest in the research of human affective behavior analysis due to its potential to provide a more accurate understanding of human emotions, which can be applied to design more friendly human-computer interaction. 
The commonly used human expression representations include Action Unit~(AU), basic expression categories~(EXPR), and Valence-Arousal~(VA). Specifically, AU is first proposed by Paul Ekman and Wallace Friesen in the 1970s~\cite{ekman1978facs}. It depicts the local regional movement of faces which can be used as the smallest unit to describe the expression. Basic expression categories divide expressions into a limited number of groups according to the emotion categories, e.g., happiness, sadness, etc. VA contains two continuous values Valence~(V) and Arousal~(A), which are ranged between -1 and 1. They can be used to describe the human emotional state. V represents the degree of positivity or negativity of emotion; A describes the level of intensity or activation of emotion.
% ERI typically comprises a sequence of values representing multiple emotional dimensions that reflect the intensity of an individual's emotional response to a specific stimulus.


The fifth Competition on Affective Behavior Analysis in-the-wild~(ABAW5)~\cite{kollias2023abaw} is organized to focus on handling the obstacles in the process of human affective behavior analysis. It makes great efforts to construct large-scale multi-modal video datasets Aff-wild~\cite{zafeiriou2017aff,kollias2019deep,kollias2019face} and Aff-wild2~\cite{kollias2019expression,kollias2020analysing,kollias2021affect,kollias2021analysing,kollias2021distribution,kollias2022abaw2}. The proposal of these datasets has greatly promoted the development of facial expression analysis in the wild and accelerated the practical implementation of related industries.
Aff-wild2 contains 598 videos and most of them have the three kinds of frame-wise annotated labels: AU, basic expression categories and VA. ABAW5 proposes three challenges of detecting these three kinds of expression representations. 

% Besides, ABAW5 builds up a Hume-Reaction dataset which consists of about 75 hours of video recordings, recorded via a webcam, in the subjectsâ€™ homes. Each video in it has been self-annotated by the subjects themselves for the ERI intensity of 7 emotional experiences.


In this paper, we introduce our submission to the ABAW5. First of all, we pre-train a Masked Autoencoder (MAE)~\cite{he2022masked,ma2022maeface} on a large-scale facial dataset in a self-supervised manner. Then, we choose the MAE encoder as our visual feature extractor to capture the visual features of the faces. Due to the extensive quantity of faces included in the dataset, the features extracted with the MAE encoder have strong generalization capabilities. We also finetune the MAE encoder for the specific tasks of AU detection, EXPR classification, and VA estimation. This training process only use the static and vision modal data.
To further exploit temporal and multi-modal information, we design a Temporal and Multi-modal Fusion~(TMF) that divides the videos into several short clips and performed clip-wise training on the downstream tasks mentioned above. 
During this process, we utilized the finetuned MAE encoder to extract visual features from each frame, while also incorporating pre-trained audio models such as Hubert~\cite{hubert}, Wav2vec2~\cite{baevski2020wav2vec}, and Vggish~\cite{vggish} to capture acoustic features.
The visual and acoustic features are concatenated and fed into a Transformer structure to capture temporal information for the downstream tasks. Additionally, we proposed several effective post-processing policies aimed at smoothing predictions and further enhancing model performance.


In sum, the contributions of this work are two-fold:
\begin{itemize}
    \item  Our approach employs a highly effective MAE feature extractor to capture visual features. The MAE model is pre-trained on a large-scale facial image dataset and exhibits remarkable generalization ability for diverse downstream face-related tasks.
    \item  Our approach employs a Temporal and Multi-modal Fusion (TMF) to leverage the benefits of temporal and multi-modal information. With a strategic selection of optimal vision and audio features, our approach further improves the performance of the model. 
    \item  In the ABAW5 competition, our approach ranks first in both AU and EXPR tracks and ranks second in VA track. The final test set score and quantitative experiments can prove the superiority of our method.  
\end{itemize}
% Moreover, we design a dual-branch structure that contains Basic Learning Branch~(BLB) and Collaboration Learning Branch~(CLB). BLB and CLB have the same structure and shared feature extractors. By randomly interpolating the logit space of BLB and CLB, the model can enrich the feature space by implicitly creating some potential samples, which further enhance the model generalization.
