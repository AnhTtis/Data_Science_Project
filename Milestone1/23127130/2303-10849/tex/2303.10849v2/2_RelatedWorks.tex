
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}

In this section, we introduce some recent works on the relevant tasks in the CVPR2023: ABAW5 competition, including AU detection, expression recognition and VA estimation in the wild. We also briefly introduce the self-supervised learning method in facial affective analysis. 

%%%%%%%%%%%%%%%%
\subsection{AU Detection}

For AU detection in the wild, there exist some challenges regarding the limited identity information and interference of diversity poses, illumination or occlusions. These disturbances restrain the model generation and cause the overfitting to the noise information.
Several studies propose the use of a multi-task framework to incorporate additional auxiliary information as regularization, which introduces extra label constraints. Specifically, Zhang et al. \cite{multi-task1} proposed a streaming model that simultaneously performs AU detection, expression recognition, and VA regression. Similarly, Jin et al. \cite{multi-task2} and Thinh et al. \cite{multi-task3} combine the tasks of AU detection with expression recognition. JAA-Net~\cite{shao2021jaa} performs landmarks detection and AU detection at the same time. 

Another effective approach to enhance the model generalization is to utilize the related tasks' pre-trained backbones. 
Jiang et al. \cite{AU_2} use IResnet100~\cite{iresnet} that pre-trains on Glint360K~\cite{an2022pfc} and some private commercial datasets before conducting the AU detection task in the ABAW3. 
Savchenko et al.~\cite{EXPR4} utilize the EfficientNet~\cite{tan2019efficientnet} that pre-trained on the face recognition task as the backbone.
Zhang et al.\cite{multi-task1,zhang2022} introduce the pre-trained expression embedding model as the backbone and win the first prizes in ABAW2 and ABAW3.

Multi-modal information is also involved in ABAW competitions. 
Zhang et al.~\cite{zhang2022} capture three modalities of information - vision, acoustic, and text - and fused them using a transformer decoder structure. 
Jin et al.~\cite{jin2021multi} extract the vision features from IResNet100 and the audio features from Mel Spectrogram. They also use the transformer structure for the fusion of multi-modal features.

%%%%%%%%%%%%%%%%
\subsection{Expression Recognition}

The goal of expression recognition is to classify an input image into one of the basic emotion classes, such as happiness or sadness.
Similar approaches to exploit the extra information regularization are mentioned in Sec.~2.1. Zhang et al.~\cite{multi-task1} utilize the prior expression embedding model and propose a multi-task framework. 
Phan et al.\cite{EXPR5} employ the pre-trained model RegNet\cite{radosavovic2020designing} as the backbone and add the Transformer~\cite{vaswani2017attention} structure to extract the temporal information. 
Kim \textit{et al.} \cite{EXPR6} use Swin transformer \cite{liu2021swin} as the backbone and exploit the extra auxiliary from the audio modal. 
Wang et al. \cite{wang2021multi} propose a semi-supervised framework to predict pseudo-labels for unlabeled data, which helps improve the model's generalization to some extent.
Xue \textit{et al.}~\cite{EXPR3} develop a CFC network that uses different branches to train the easy-distinguished and hard-distinguished emotion categories.  

%%%%%%%%%%%%%%%%
\subsection{VA Estimation}
For VA estimation, several studies~\cite{multi-task1,ABAW2_VA1,ABAW2_VA3,ABAW2_VA4} leverage the correlation between VA and AU or VA and EXP, proposing multi-task frameworks. This enables these methods to extract supplementary information from other tasks, particularly for data without VA labels but possessing AU or EXPR labels.
Moreover, many works~\cite{ABAW3_VA1,ABAW3_VA2,ABAW5_AU1,ABAW5_EXPR5,ABAW3_VA5,ABAW5_AU3} propose multi-modal frameworks, which leverage hidden features from vision, audio, or text. The Transformer structure is also commonly used in VA tasks for feature fusion. Several works~\cite{ABAW3_VA1,ABAW5_AU1,ABAW5_AU2,ABAW5_EXPR3} utilize it for this purpose.



%%%%%%%%%%%%%%%%
\subsection{Self-supervised Learning in Affective Analysis}
It has been pointed out~\cite{zafeiriou2017aff} that annotating the corresponding emotion/AU/VA labels from real-world facial images costs a large amount of time/labor, which limits the development of the affective analysis. It is a potential solution to make use of the self-supervised learning~(SSL) method to exploit more knowledge from the existing large-scale unlabelled data. There have been several works to develop SSL methods to enhance the accuracy in the area of affective analysis. 
Shu et al.\cite{shu2022revisiting} explore different strategies in the choice of positives and negatives to enforce the expression-related features and reduce the interference of other facial attributes. They improve the accuracy of expression recognition based on the contrastive SSL methods (e.g. SimCLR~\cite{chen2020simple}).
Ma et al.~\cite{ma2022maeface} pre-train the Masked Autoencoder~(MAE) structure on the large-scale face images and finetune it on the AU detection, which achieves the state-of-the-art performance on the BP4D~\cite{zhang2014bp4d} and DISFA~\cite{mavadati2013disfa}.
In the ABAW5 competition, Zhang et al.~\cite{ABAW5_AU1}, Liu et al.~\cite{ABAW5_AU2}, and Wang et al.~\cite{ABAW5_AU4} all employ the pre-trained MAE to extract vision features and secure a position among the top few performers. 
