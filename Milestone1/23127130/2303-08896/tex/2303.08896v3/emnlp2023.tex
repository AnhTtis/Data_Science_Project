% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{EMNLP2023}
\usepackage{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}
% \usepackage[colorinlistoftodos]{todonotes}
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage[bottom]{footmisc}
\usepackage{diagbox}

% Formatting tables and figures
\usepackage{multirow, booktabs}
\usepackage{caption}
\captionsetup{font={small}, labelfont={bf}}
% \captionsetup{font={footnotesize}}
% \captionsetup{labelfont={bf}}

% coloring highlight rows in tables
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}

% \newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
% \setlength{\columnseprule}{0.4pt}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\blue}[1]{\textcolor{blue}{#1}}



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{SelfCheckGPT}: Zero-Resource Black-Box Hallucination Detection\\ for Generative Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Potsawee Manakul, Adian Liusie, Mark J. F. Gales \\
  ALTA Institute, Department of Engineering, University of Cambridge \\
  \texttt{pm574@cam.ac.uk, al826@cam.ac.uk, mjfg@eng.cam.ac.uk}}


\begin{document}
\maketitle
\begin{abstract}

Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.\footnote{Code and dataset can be found on the project page at \url{https://github.com/potsawee/selfcheckgpt}.} 


%%% General Problem/Motivation %%%
% Generative Large language models (LLMs) such
% as GPT-3 are capable of generating highly fluent
% responses to a variety of user prompts. Yet, LLMs may produce non-factual information, especially in situations where they do not have much information about the topic.  

% %%% Specific setup we are interested %%%

% Without external knowledge/database, uncertainty metrics such as token probabilities and entropies could be used to assess or generated texts. However, these uncertainty metrics require access to the generative model for the additional information beyond text-based responses, which may not always be available, especially in widely-used ChatGPT's web interface and its API.

% %%% Proposed Method %%%
% In this work, we propose "SelfCheckGPT" -- a sampling-based black-box method that requires only access to text responses -- for self fact-checking LLM generated texts. The method is based on the idea that if LLM has knowledge about the topic, the sampled responses should be similar, whereas if the information in sampled responses is inconsistent, it could indicate that LLM has less knowledge.

% %%% Results %%%
% We use GPT-3 to generate passages about persons in the WikiBio dataset, and we annotate the passages on the factuality aspect at the sentence level. Two factuality tasks are carried out: (i) detecting non-factual and factual sentences; (ii) ranking generated passages based on the factuality score. On these tasks, our results show that our SelfCheckGPT method considerably outperforms black-box baselines, and it is also comparable to grey-box methods.


\end{abstract}

\section{Introduction}
Large Language Models (LLMs) such as GPT-3 \citep{brown2020language} and PaLM \cite{chowdhery2022palm} are capable of generating fluent and realistic responses to a variety of user prompts. They have been used in many applications such as automatic tools to draft reports, virtual assistants and summarization systems. Despite the convincing and realistic nature of LLM-generated texts, a growing concern with LLMs is their tendency to hallucinate facts. It has been widely observed that models can confidently generate fictitious information, and worryingly there are few, if any, existing approaches to suitably identify LLM hallucinations.

\begin{figure}
\includegraphics[width=\linewidth,keepaspectratio]{fig/diagram.drawio.pdf}
    \caption{SelfCheckGPT with Prompt. Each LLM-generated sentence is compared against stochastically generated responses with no external database. A comparison method can be, for example, through LLM prompting as shown above.}
    \label{fig:diagram}
\end{figure}

A possible approach of hallucination detection is to leverage existing intrinsic uncertainty metrics to determine the parts of the output sequence that the system is least certain of \cite{yuan2021bartscore, gptscore}. However, uncertainty metrics such as token probability or entropy require access to token-level probability distributions, information which may not be available to users for example when systems are accessed through limited external APIs. An alternate approach is to leverage fact-verification approaches, where evidence is retrieved from an external database to assess the veracity of a claim \citep{Thorne18Fact, guo-etal-2022-survey}. However, facts can only be assessed relative to the knowledge present in the database. Additionally, hallucinations are observed over a wide range of tasks beyond pure fact verification \cite{kryscinski-etal-2020-evaluating, maynez-etal-2020-faithfulness}. 

%For example, summaries from automatic systems can contain information not present in the context \citep{kryscinski-etal-2019-neural, kryscinski-etal-2020-evaluating, maynez-etal-2020-faithfulness}. 

In this paper, we propose SelfCheckGPT, a sampling-based approach that can detect whether responses generated by LLMs are hallucinated or factual. To the best of our knowledge, SelfCheckGPT is the first work to analyze model hallucination of general LLM responses, and is the first zero-resource hallucination detection solution that can be applied to black-box systems. The motivating idea of SelfCheckGPT is that when an LLM has been trained on a given concept, the sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and may contradict one another. By sampling multiple responses from an LLM, one can measure information consistency between the different responses and determine if statements are factual or hallucinated. Since SelfCheckGPT only leverages sampled responses, it has the added benefit that it can be used for black-box models, and it requires no external database. Five variants of SelfCheckGPT for measuring informational consistency are considered: BERTScore, question-answering, $n$-gram, NLI, and LLM prompting. Through analysis of annotated articles generated by GPT-3, we show that SelfCheckGPT is a highly effective hallucination detection method that can even outperform grey-box methods, and serves as a strong first baseline for an increasingly important problem of LLMs.


\section{Background and Related Work}
% \subsection{Large Language Models}
% There has been rapid growth in current large language models (LLMs) literature with larger and better models being constantly released \cite{chowdhery2022palm}. These models are commonly used as the backbone for a range of NLP tasks \cite{wang-etal-2018-glue}. Traditionally, these LLMs are fine-tuned to a specific task and/or domain \cite{devlin-etal-2019-bert, radford2019language, raffel2020exploring}, however, a fascinating finding is that as models scale up, they inherit abilities to naturally solve a wide range of natural language tasks in a zero-shot fashion \cite{brown2020language, weiemergent}. 


\subsection{Hallucination of Large Language Models}
Hallucination has been studied in text generation tasks, including summarization \cite{Huang_hallucination_survey} and dialogue generation \cite{shuster-etal-2021-retrieval-augmentation}, as well as in a variety of other natural language generation tasks \cite{hallucination_survey_2023}. Self-consistency decoding has shown to improve chain-of-thought prompting performance on complex reasoning tasks \cite{wang2023selfconsistency}. Further, \citet{liu-etal-2022-token} introduce a hallucination detection dataset, however, texts are obtained by perturbing factual texts and thus may not reflect true LLM hallucination. 

%Also, a self-consistency decoding method has been shown to improve chain-of-thought LLM prompting on complex reasoning tasks \cite{wang2023selfconsistency}.  

% this initial research mainly focused on domain-specific, small LMs, and the extent to which this generalizes to large generative LLMs remains unclear. 

Recently, \citet{azaria2023internal} trained a multi-layer perception classifier where an LLM's hidden representations are used as inputs to predict the truthfulness of a sentence. However, this approach is a white-box approach that uses the internal states of the LLM, which may not be available through API calls, and requires labelled data for supervised training. Another recent approach is self-evaluation \cite{kadavath2022language}, where an LLM is prompted to evaluate its previous prediction, e.g., to predict the probability that its generated response/answer is true.

\subsection{Sequence Level Uncertainty Estimation}
Token probabilities have been used as an indication of model certainty. For example, OpenAI's GPT-3 web interface allows users to display token probabilities (as shown in Figure \ref{fig:openai_playground}), and further uncertainty estimation approaches based on aleatoric and epistemic uncertainty have been studied for autoregressive generation \cite{xiao-wang-2021-hallucination, malinin2021uncertainty}. Additionally, conditional language model scores have been used to evaluate properties of texts \cite{yuan2021bartscore, gptscore}. Recently, semantic uncertainty has been proposed to address uncertainty in free-form generation tasks where probabilities are attached to concepts instead of tokens \cite{kuhn2023semantic}.

%meanings of text -> concepts?

\begin{figure}[!ht]
    \includegraphics[width=\linewidth,keepaspectratio]{fig/playground_example0.png}
    \caption{Example of OpenAI's GPT-3 web interface with output token-level probabilities displayed.}
    \label{fig:openai_playground}
\end{figure}


\subsection{Fact Verification}
Existing fact-verification approaches follow a multi-stage pipeline of claim detection, evidence retrieval and verdict prediction \citep{guo-etal-2022-survey, zhong-etal-2020-reasoning}. Such methods, however, require access to external databases and can have considerable inference costs. 

\section{Grey-Box Factuality Assessment}
This section will introduce methods that can be used to determine the factuality of LLM responses in a zero-resource setting when one has full access to output distributions.\footnote{Alternate white-box approaches such as that of \citet{azaria2023internal} require access to full internal states, and is less practical and so not considered in this work.} We will use `factual' to define when statements are grounded in valid information, i.e. when hallucinations are avoided, and `zero-resource' when no external database is used. 

% We first motivate the intuition for why standard uncertainty-based metrics may be good proxies for factuality, then describe how systems' output token-level probability distributions can be used to determine sentence-level hallucinations.

\subsection{Uncertainty-based Assessment}
To understand how the factuality of a generated response can be determined in a zero-resource setting, we consider LLM pre-training. During pre-training, the model is trained with next-word prediction over massive corpora of textual data. This gives the model a strong understanding of language \cite{jawahar-etal-2019-bert, raffel2020exploring}, powerful contextual reasoning \cite{zhang2020semantics}, as well as world knowledge \cite{liusie2022world}. Consider the input "\texttt{Lionel Messi is a \_}". Since Messi is a world-famous athlete who may have appeared multiple times in pre-training, the LLM is likely to know who Messi is. Therefore given the context, the token "\texttt{footballer}" may be assigned a high probability while other professions such as "\texttt{carpenter}" may be considered improbable. However, for a different input such as "\texttt{John Smith is a \_}", the system will be unsure of the continuation which may result in a flat probability distribution. During inference, this is likely to lead to a non-factual word being generated. 

This insight allows us to understand the connection between uncertainty metrics and factuality. Factual sentences are likely to contain tokens with higher likelihood and lower entropy, while hallucinations are likely to come from positions with flat probability distributions with high uncertainty.

\subsubsection*{Token-level Probability} %$p$
Given the LLM's response $R$, let $i$ denote the $i$-th sentence in $R$, $j$ denote the $j$-th token in the $i$-th sentence, $J$ is the number of tokens in the sentence, and $p_{ij}$ be the probability of the word generated by the LLM at the $j$-th token of the $i$-th sentence. Two probability metrics are used:
\begin{align*}
    \text{Avg}(- \log p) &= - \frac{1}{J} \sum_j \log  p_{ij}  \\
    \text{Max}(- \log p) &= \underset{j}{\text{max}} \left( - \log p_{ij} \right)
\end{align*}
$\text{Max}(- \log p)$ measures the sentence's likelihood by assessing the \textit{least} likely token in the sentence.
% \footnote{Minimum token probability is equivalent to maximum negative log probability.}

\subsubsection*{Entropy} %$\mathcal{H}$
The entropy of the output distribution is:
\begin{equation*}
    \mathcal{H}_{ij} = - \sum_{\tilde{w} \in \mathcal{W}} p_{ij}(\tilde{w}) \log p_{ij}(\tilde{w})
\end{equation*}
where $p_{ij}(\tilde{w})$ is the probability of the word $\tilde{w}$ being generated at the $j$-th token of the $i$-th sentence, and $\mathcal{W}$ is the set of all possible words in the vocabulary. Similar to the probability-based metrics, two entropy-based metrics are used: 
\begin{equation*}
    \text{Avg}(\mathcal{H}) = \frac{1}{J} \sum_j \mathcal{H}_{ij}; \hspace{3mm}
    \text{Max}(\mathcal{H}) = \max_j \left( \mathcal{H}_{ij} \right) 
\end{equation*}

\section{Black-Box Factuality Assessment}
%\noindent\textbf{Motivation}.  
A drawback of grey-box methods is that they require output token-level probabilities. Though this may seem a reasonable requirement, for massive LLMs only available through limited API calls, such token-level information may not be available (such as with ChatGPT). Therefore, we consider black-box approaches which remain applicable even when only text-based responses are available. 

\subsection*{Proxy LLMs}
A simple approach to approximate the grey-box approaches is by using a proxy LLM, i.e. another LLM that we have full access to, such as LLaMA \cite{touvron2023llama}. A proxy LLM can be used to approximate the output token-level probabilities of the black-box LLM generating the text. In the next section, we propose SelfCheckGPT, which is also a black-box approach. 

\section{SelfCheckGPT}
% 1. Motivation of SelfCheckGPT
% \noindent\textbf{Motivation}.


SelfCheckGPT is our proposed black-box zero-resource hallucination detection scheme, which operates by comparing multiple sampled responses and measuring consistency.

%Also, a self-consistency decoding method has been shown to improve chain-of-thought LLM prompting on complex reasoning tasks \cite{wang2023selfconsistency}.  

\textbf{Notation}: Let $R$ refer to an LLM response drawn from a given user query. SelfCheckGPT draws a further $N$ stochastic LLM response samples $\{S^1,S^2,...,S^n,...,S^N\}$ using the same query, and then measures the consistency between the response and the stochastic samples. We design SelfCheckGPT to predict the hallucination score of the $i$-th sentence, $\mathcal{S}(i)$, such that $\mathcal{S}(i) \in [0.0, 1.0]$, where $\mathcal{S}(i) \rightarrow 0.0$ if the $i$-th sentence is grounded in valid information and $\mathcal{S}(i) \rightarrow 1.0$ if the $i$-th sentence is hallucinated.\footnote{With the exception of SelfCheckGPT with $n$-gram as the score of the $n$-gram language model is not bounded.} The following subsections will describe
each of the SelfCheckGPT variants.

\subsection{SelfCheckGPT with BERTScore}
\label{section:self_check_bertscore}
 Let $\mathcal{B}(.,.)$ denote the BERTScore between two sentences. SelfCheckGPT with BERTScore finds the average BERTScore of the $i$-th sentence with the most similar sentence from each drawn sample:
% \begin{align}
    % \tilde{\mathcal{S}}_\text{BERT}(i) &= \frac{1}{N} \sum_{n=1}^N \underset{k}{\text{max}} \left( \mathcal{B} (r_i, s^n_k) \right) \\
    % {\mathcal{S}}_\text{BERT}(i) &= 1 - \tilde{\mathcal{S}}_\text{BERT}(i)  
% \end{align}
\begin{equation}
    \mathcal{S}_\text{BERT}(i) = 1-\frac{1}{N} \sum_{n=1}^N \underset{k}{\text{max}} \left( \mathcal{B} (r_i, s^n_k) \right)   
\end{equation}
where $r_i$ represents the $i$-th sentence in $R$ and $s^n_k$ represents the $k$-th sentence in the $n$-th sample $S^n$. This way if the information in a sentence appears in many drawn samples, one may assume that the information is factual, whereas if the statement appears in no other sample, it is likely a hallucination. In this work, RoBERTa-Large \cite{liu2019roberta} is used as the backbone of BERTScore.

\subsection{SelfCheckGPT with Question Answering}
\label{section:self_check_qa}
We also consider using the automatic multiple-choice question answering generation (MQAG) framework \cite{manakul2023mqag} to measure consistency for SelfCheckGPT. MQAG assesses consistency by generating multiple-choice questions over the main generated response, which an independent answering system can attempt to answer while conditioned on the other sampled responses. If questions on consistent information are queried, the answering system is expected to predict similar answers. MQAG consists of two stages: question generation \texttt{G} and question answering \texttt{A}. For the sentence $r_i$ in the response $R$, we draw questions $q$ and options $\mathbf{o}$:
\begin{equation}
    q, \mathbf{o} \sim P_{{\tt G}}(q,\mathbf{o}|r_i, R)
    \label{eq:question_gen}
\end{equation}

% where $\mathbf{o} = \{a, \mathbf{o}_{\backslash a}\}= \{o_1, ..., o_4\}$. To filter out bad (e.g., unanswerable) questions, we define an answerability score \cite{raina-gales-2022-answer}:
% \begin{equation}
%     \alpha = P_{{\tt U}}(\text{answerable}|q,\text{context})
% \end{equation}
% where the context is either the response $R$ or sampled passages $S^n$, and $\alpha \rightarrow 0.0$ for unanswerable and $\alpha \rightarrow 1.0$ for answerable. We use $\alpha$ to filter out unanswerable questions which have $\alpha$ lower than a threshold. 

\noindent The answering stage \texttt{A} selects the answers:
\begin{align}
    a_R &= \underset{k}{\text{argmax}} \left[ P_{{\tt A}}(o_k | q, R, \mathbf{o})  \right]  \\
    a_{S^n} &= \underset{k}{\text{argmax}} \left[  P_{{\tt A}}(o_k | q, S^n, \mathbf{o}) \right]
\end{align}
We compare whether $a_R$ is equal to $a_{S^n}$ for each sample in $\{S^1,...,S^N\}$, yielding \#matches $N_{\tt m}$ and \#not-matches $N_{\tt n}$. A simple inconsistency score for the $i$-th sentence and question $q$ based on the match/not-match counts is defined:
$
    \mathcal{S}_\text{QA}(i,q) = \frac{N_{\tt n}}{N_{\tt m} + N_{\tt n}} 
$.
To take into account the answerability of generated questions, we show in Appendix \ref{appendix:selfcheck_qa} that we can modify the inconsistency score by applying soft-counting, resulting in: 
\begin{equation}
    \mathcal{S}_\text{QA}(i,q) = \frac{\gamma_2^{N'_{\tt n}}}{\gamma_1^{N'_{\tt m}} + \gamma_2^{N'_{\tt n}} }
\end{equation}
where $N'_{\tt m}$ = the effective match count, $N'_{\tt n}$ = the effective mismatch count, with $\gamma_1$ and $\gamma_2$ defined in Appendix \ref{appendix:bayes_qa}. Ultimately, SelfCheckGPT with QA is the average of inconsistency scores across $q$,
\begin{equation}
    \mathcal{S}_\text{QA}(i) = \mathbb{E}_q \left[ \mathcal{S}_\text{QA}(i,q) \right]
\end{equation}


\subsection{SelfCheckGPT with n-gram}
\label{section:self_check_ngram}
Given samples $\{S^1,...,S^N\}$ generated by an LLM, one can use the samples to create a new language model that approximates the LLM. In the limit as $N$ gets sufficiently large, the new language model will converge to the LLM that generated the responses. We can therefore approximate the LLM's token probabilities using the new language model. 

In practice, due to time and/or cost constraints, there can only be a limited number of samples $N$. Consequently, we train a simple $n$-gram model using the samples $\{S^1,...,S^N\}$ as well as the main response $R$ (which is assessed), where we note that including $R$ can be considered as a smoothing method where the count of each token in $R$ is increased by 1. We then compute the average of the log-probabilities of the sentence in response $R$,
\begin{equation}
    \mathcal{S}_{n\text{-gram}}^\text{Avg}(i) = - \frac{1}{J} \sum_j \log \tilde{p}_{ij} 
\end{equation}
where $\tilde{p}_{ij}$ is the probability (of the $j$-th token of the $i$-th sentence) computed using the $n$-gram model. Similar to the grey-box approach, we can also use the maximum of the negative log probabilities,
\begin{equation}
    \mathcal{S}_{n\text{-gram}}^\text{Max}(i) = \max_j \left( -\log \tilde{p}_{ij} \right)
\end{equation}



\subsection{SelfCheckGPT with NLI}
\label{section:self_check_nli}

Natural Language Inference (NLI) determines whether a hypothesis follows a premise, classified into either entailment/neutral/contradiction. NLI measures have been used to measure faithfulness in summarization, where \citet{maynez-etal-2020-faithfulness} use a textual entailment classifier trained on MNLI \cite{williams-etal-2018-broad} to determine if a summary contradicts a context or not. Inspired by NLI-based summary assessment, we consider using the NLI contradiction score as a SelfCheckGPT score.

For SelfCheck-NLI, we use DeBERTa-v3-large \cite{he2023debertav} fine-tuned to MNLI as the NLI model. The input for NLI classifiers is typically the \texttt{premise} concatenated to the \texttt{hypothesis}, which for our methodology is the sampled passage $S^n$ concatenated to the sentence to be assessed $r_i$. Only the logits associated with the `entailment' and `contradiction' classes are considered,
\begin{equation}
    P(\text{contradict}|r_i,S^n) = \frac{\exp (z_c)}{\exp (z_e) + \exp(z_c)}
\end{equation}
where $z_e$ and $z_c$ are the logits of the `entailment' and `contradiction' classes, respectively. This normalization ignores the neutral class and ensures that the probability is bounded between 0.0 and 1.0. The SelfCheckGPT with NLI score for each sample $S^n$ is then defined as,
\begin{equation}
    \mathcal{S}_{\text{NLI}}(i) = \frac{1}{N} \sum_{n=1}^NP(\text{contradict}|r_i,S^n)
\end{equation}
% Natural Language Inference (NLI) measures have been used in measuring faithfulness in summarization. For example, \citet{maynez-etal-2020-faithfulness} trained a textual entailment classifier on the Multi-NLI dataset \cite{williams-etal-2018-broad}. Given a context, the entailment model is to classify the hypothesis into one of the three classes (entail/neutral/contradict). Alternatively, \citet{kryscinski-etal-2020-evaluating, bao-etal-2022-suenes} used simulated data, such as real or fake summaries created by pre-defined transformations to train classifiers to detect inconsistent summaries. 

% Inspired by summary assessment with NLI, we use the contradiction score of the NLI model as a SelfCheckGPT score. During training (e.g., on the Multi-NLI dataset) the input to the NLI model is \texttt{context} concatenated by \texttt{hypothesis}. When applying the NLI model in the SelfCheckGPT framework, the input is the sampled passage $S^n$ concatenated by the sentence to be assessed $r_i$.

% In practice, DeBERTa-v3-large \cite{he2023debertav} fine-tuned to Multi-NLI is used as the NLI model. Only the outputs associated with `entailment' and `contradiction' classes are used, and the SelfCheckGPT with NLI score for each sample $S^n$ is defined as follows:


\subsection{SelfCheckGPT with Prompt}
\label{section:selfcheck_prompt}
LLMs have recently been shown to be effective in assessing information consistency between a document and its summary in zero-shot settings \cite{luo2023chatgpt}. Thus, we query an LLM to assess whether the $i$-th sentence is supported by sample $S^n$ (as the context) using the following prompt.

\begin{small}
\begin{verbatim}
------------------------------------------------
Context: {}
Sentence: {}
Is the sentence supported by the context above? 
Answer Yes or No:
------------------------------------------------
\end{verbatim}
\end{small}

\noindent Initial investigation showed that GPT-3 (text-davinci-003) will output either \texttt{Yes} or \texttt{No} 98\% of the time, while any remaining outputs can be set to \texttt{N/A}. The output from prompting when comparing the $i$-th sentence against sample $S^n$ is converted to score $x^n_i$ through the mapping \{\texttt{Yes}: 0.0, \texttt{No}: 1.0, \texttt{N/A}: 0.5\}. The final inconsistency score is then calculated as:
\begin{equation}
    \mathcal{S}_\text{Prompt}(i) = \frac{1}{N}\sum_{n=1}^N x^n_i
\end{equation}
SelfCheckGPT-Prompt is illustrated in Figure \ref{fig:diagram}. Note that our initial investigations found that less capable models such as GPT-3 (text-curie-001) or LLaMA failed to effectively perform consistency assessment via such prompting.


\section{Data and Annotation}
As, currently, there are no standard hallucination detection datasets available, we evaluate our hallucination detection approaches by 1) generating synthetic Wikipedia articles using GPT-3 on the individuals/concepts from the WikiBio dataset \cite{wikibio_dataset}; 2) manually annotating the factuality of the passage at a sentence level; 3) evaluating the system's ability to detect hallucinations. 

%Test- 72k
 WikiBio is a dataset where each input contains the first paragraph (along with tabular information) of Wikipedia articles of a specific concept. We rank the WikiBio test set in terms of paragraph length and randomly sample 238 articles from the top 20\% of longest articles (to ensure no very obscure concept is selected). {GPT-3 (text-davinci-003)} is then used to generate Wikipedia articles on a concept, using the prompt "\texttt{This is a Wikipedia passage about \{concept\}}:". Table \ref{tab:data_statistics} provides the statistics of GPT-3 generated passages. 
 
 \begin{table}[!ht]
  \centering
  \begin{tabular}{ccc}
    \toprule
    \#Passages &\#Sentences &\#Tokens/passage \\
    \midrule
    238  &1908   &184.7{\footnotesize$\pm$36.9}   \\
    \bottomrule
  \end{tabular}
  \caption{The statistics of \textbf{WikiBio GPT-3 dataset} where the number of tokens is based on the OpenAI GPT-2 tokenizer.}
  \label{tab:data_statistics}
\end{table}
\noindent We then annotate the sentences of the generated passages using the guidelines shown in Figure \ref{fig:labelling_diagram} such that each sentence is classified as either:
% A total of 1908 sentences from 238 passages were annotated, with 
\begin{itemize}
    \item \textbf{Major Inaccurate} (Non-Factual, \textbf{1}): The sentence is entirely hallucinated, i.e. the sentence is unrelated to the topic.
    \item \textbf{Minor Inaccurate} (Non-Factual, \textbf{0.5}): The sentence consists of some non-factual information, but the sentence is related to the topic.
    \item \textbf{Accurate} (Factual, \textbf{0}): The information presented in the sentence is accurate.
\end{itemize}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth,keepaspectratio]{fig/annotation.drawio.pdf}
    \caption{Flowchart of our annotation process}
    \label{fig:labelling_diagram}
\end{figure}

\noindent Of the 1908 annotated sentences, 761 (39.9\%) of the sentences were labelled major-inaccurate, 631 (33.1\%) minor-inaccurate, and 516 (27.0\%) accurate. 201 sentences in the dataset had annotations from two different annotators. To obtain a single label for this subset, if both annotators agree, then the agreed label is used. However, if there is disagreement, then the worse-case label is selected (e.g., \{minor inaccurate, major inaccurate\} is mapped to major inaccurate). The inter-annotator agreement, as measured by Cohen's $\kappa$ \cite{cohen1960ACO}, has $\kappa$ values of 0.595 and 0.748, indicating \textit{moderate} and \textit{substantial} agreement \cite{viera2005understanding} for the 3-class and 2-class scenarios, respectively.\footnote{3-class refers to when selecting between accurate, minor inaccurate, major inaccurate. 2-class refers to when minor/major inaccuracies are combined into one label.}

%\footnote{The label distribution could be easily shifted by selecting more obscure/well-known concepts} 

%The inter-annotator agreement, as measured by Cohen's $\kappa$ \cite{cohen1960ACO}, for the 201 sentences with annotation overlap is shown in Table \ref{tab:cohens_kappa}.

% \begin{table}[!ht]
%   \centering
%   \begin{tabular}{rcc}
%     \toprule
%     Annotation        &3-label &2-label \\
%     \midrule
%     Cohen's $\kappa$  &0.595   &0.748   \\
%     \bottomrule
%   \end{tabular}
%   \caption{Inter-annotator agreement where 3-label means selecting from accurate, minor inaccurate, major inaccurate. 2-label is calculated by combining minor/major into one label.}
%   \label{tab:cohens_kappa}
% \end{table}


Furthermore, passage-level scores are obtained by averaging the sentence-level labels in each passage. The distribution of passage-level scores is shown in Figure \ref{fig:histogram_annotation_doc}, where we observe a large peak at +1.0. We refer to the points at this peak as \textit{total hallucination}, which occurs when the information of the response is unrelated to the real concept and is entirely fabricated by the LLM. 


\begin{figure}[!ht]
    \centering
\includegraphics[width=0.9\linewidth,keepaspectratio]{fig/histogram_annotation.pdf}
    \caption{Document factuality scores histogram plot}
    \label{fig:histogram_annotation_doc}
\end{figure}

\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_nonfact_all_sentences.pdf}
        \caption{Non-Factual Sentences}
        \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_nonfact_good_sentences.pdf}
        \caption{Non-Factual* Sentences}
        \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_fact_all_sentences.pdf}
        \caption{Factual Sentences}
    \end{subfigure}
    \caption{PR-Curve of detecting non-factual and factual \textit{sentences} in the GPT-3 generated WikiBio passages.}
    \label{fig:detection_pr_curve}
\end{figure*}
\section{Experiments}

The generative LLM used to generate passages for our dataset is \textbf{GPT-3} (text-davinci-003), the state-of-the-art system at the time of creating and annotating the dataset. To obtain the main response, we set the temperature to 0.0 and use standard beam search decoding. For the stochastically generated samples, we set the temperature to 1.0 and generate $N$=20 samples. For the proxy LLM approach, we use LLaMA \cite{touvron2023llama}, one of the best-performing open-source LLMs currently available. For SelfCheckGPT-Prompt, we consider both GPT-3 (which is the same LLM that is used to generate passages) as well as the newly released ChatGPT (gpt-3.5-turbo). More details about the systems in SelfCheckGPT and results using other proxy LLMs can be found in the appendix.

\subsection{Sentence-level Hallucination Detection}
\begin{table*}[!t]
  \centering
  % \scalebox{1.0}{
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Method}  &\multicolumn{3}{c}{Sentence-level (AUC-PR)} &\multicolumn{2}{c}{Passage-level (Corr.)}  \\
    &NonFact &NonFact* &Factual           &Pearson &Spearman     \\
    \midrule
    Random          &72.96 &29.72 &27.04    &-   &-        \\
    % \midrule
    \rowcolor{Gray}
    \multicolumn{6}{l}{{\texttt{GPT-3 (text-davinci-003)}'s probabilities} (\textit{LLM, grey-box})} \\
    Avg($-$log$p$)                   &83.21 &38.89 &53.97 &57.04 &53.93 \\
    Avg($\mathcal{H}$)$^\dagger$     &80.73 &37.09 &52.07 &55.52 &50.87 \\
    Max($-$log$p$)                   &87.51 &35.88 &50.46 &57.83 &55.69 \\
    Max($\mathcal{H}$)$^\dagger$     &85.75 &32.43 &50.27 &52.48 &49.55 \\
    \rowcolor{Gray}
    \multicolumn{6}{l}{{\texttt{LLaMA-30B}'s probabilities} (\textit{Proxy LLM, black-box})} \\
    Avg($-$log$p$)                  &75.43  &30.32  &41.29  &21.72 &20.20  \\
    Avg($\mathcal{H}$)              &80.80  &39.01  &42.97  &33.80 &39.49 \\
    Max($-$log$p$)                  &74.01  &27.14  &31.08  &-22.83 &-22.71 \\
    Max($\mathcal{H}$)              &80.92  &37.32  &37.90  &35.57 &38.94   \\
    \rowcolor{Gray}
    \multicolumn{6}{l}{\textbf{SelfCheckGPT} (\textit{black-box)}} \\
    w/ BERTScore                     &81.96 &45.96 &44.23 &58.18 &55.90  \\
    % Counting                       &83.97 &40.07 &47.78 &57.39 &55.15\\
    % + Bayes'                       &83.04 &38.58 &47.41 &56.43 &55.03 \\
    w/ QA                            &84.26 &40.06 &48.14 &61.07 &59.29 \\ % this is bayes + gamma
    w/ Unigram (max)                 &85.63 &41.04 &58.47 &64.71 &64.91 \\ % unigram3 (max -logprob)
    % Combination                    &87.33 &44.37 &61.83 &69.05 &67.77 \\
    w/ NLI &92.50 &45.17 &66.08 &74.14 &73.78 \\
    w/ Prompt                        &\textbf{93.42} &\textbf{53.19} &\textbf{67.09} &\textbf{78.32} &\textbf{78.30} \\ 
    \bottomrule
  \end{tabular}
  % }
  \caption{AUC-PR for sentence-level detection tasks. Passage-level ranking performances are measured by Pearson correlation coefficient and Spearman's rank correlation coefficient w.r.t. human judgements. The results of other proxy LLMs, in addition to LLaMA, can be found in the appendix. $^{\dagger}$GPT-3 API returns the top-5 tokens' probabilities, which are used to compute entropy.}
  \label{tab:big_table}
\end{table*}

First, we investigate whether our hallucination detection methods can identify the factuality of sentences. In detecting non-factual sentences, both major-inaccurate labels and minor-inaccurate labels are grouped together into the \textit{non-factual} class, while the \textit{factual} class refers to accurate sentences. In addition, we consider a more challenging task of detecting major-inaccurate sentences in passages that are \textit{not} total hallucination passages, which we refer to as \textit{non-factual$^*$}.\footnote{There are 206  non-factual$^*$ passages (1632 sentences).} Figure \ref{fig:detection_pr_curve} and Table \ref{tab:big_table} show the performance of our approaches, where the following observations can be made:




% We look both at fact verification (i.e. identifying factual sentences) and hallucination detection (i.e. identifying non-factual sentences) performance. 


\vspace{1mm}
\textbf{1) LLM's probabilities $p$ correlate well with factuality}. Our results show that probability measures (from the LLM generating the texts) are strong baselines for assessing factuality. Factual sentences can be identified with an AUC-PR of 53.97, significantly better than the random baseline of 27.04, with the AUC-PR for hallucination detection also increasing from 72.96 to 83.21. This supports the hypothesis that when the LLMs are uncertain about generated information, generated tokens often have higher uncertainty, paving a promising direction for hallucination detection approaches. Also, the probability $p$ measure performs better than the entropy $\mathcal{H}$ measure of top-5 tokens. 

\textbf{2) Proxy LLM perform noticeably worse than LLM (GPT-3)}. The results of proxy LLM (based on LLaMA) show that the entropy $\mathcal{H}$ measures outperform the probability measures. This suggests that using richer uncertainty information can improve factuality/hallucination detection performance, and that previously the entropy of top-5 tokens is likely to be insufficient. In addition, when using other proxy LLMs such as GPT-NeoX or OPT-30B, the performance is near that of the random baseline. We believe this poor performance occurs as different LLMs have different generating patterns, and so even common tokens may have a low probability in situations where the response is dissimilar to the generation style of the proxy LLM. We note that a weighted conditional LM score such as BARTScore \cite{yuan2021bartscore} could be incorporated in future investigations. 

% Table \ref{tab:big_table} shows, however, that this approach is \textit{ineffective}.

% LLM's probability measures require access to LLM output probabilities, which may not always be available. We propose using a proxy system (e.g., GPT-NeoX-20B) whose model weights' are publicly available, to approximate the output probabilities of GPT-3

% Further, ablations in the appendix (Table \ref{tab:big_table_appendix}) show the impact of the initialisation/size of the proxy LLM, where it is shown that larger models typically yield better factual/non-factual detection performance, but none of which rival the previous grey box approach. 

\textbf{3) SelfCheckGPT outperforms grey-box approaches}. It can be seen that SelfCheckGPT-Prompt \textit{considerably} outperforms the grey-box approaches (including GPT-3's output probabilities) as well as other black-box approaches. Even other variants of SelfCheckGPT, including BERTScore, QA, and $n$-gram, outperform the grey-box approaches in most setups. Interestingly, despite being the least computationally expensive method, SelfCheckGPT with unigram (max) works well across different setups. Essentially, when assessing a sentence, this method picks up the token with the \textit{lowest} occurrence given all the samples. This suggests that if a token only appears a few times (or once) within the generated samples ($N$=20), it is likely non-factual. 

%These results demonstrate that self-consistency can be effective for hallucination detection, in addition to its observed benefits when used for decoding on complex reasoning tasks \cite{wang2023selfconsistency}

%Also, a self-consistency decoding method has been shown to improve chain-of-thought LLM prompting on complex reasoning tasks \cite{wang2023selfconsistency}.  

\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.33\linewidth}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_baseline1.pdf}
    \caption{\texttt{GPT-3} Avg($-\log p$)}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_llama_30b_2.pdf}
    \caption{\texttt{LLaMA-30B} Avg($\mathcal{H}$)}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
    \centering    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_selfcheck_prompt_chatgpt.pdf}
    \caption{SelfCheckGPT-Prompt}
    \end{subfigure}
\caption{Scatter plot of passage-level scores where Y-axis = Method scores, X-axis = Human scores. Correlations are reported in Table \ref{tab:big_table}. The scatter plots of other SelfCheckGPT variants are provided in Figure \ref{fig:scatter_plot_passage_appendix} in the appendix.}
\label{fig:scatter_plot_passage}
\end{figure*}

\textbf{4) SelfCheckGPT with $n$-gram}. When investigating the $n$-gram performance from 1-gram to 5-gram, the results show that simply finding the least likely token/$n$-gram is more effective than computing the average $n$-gram score of the sentence, details in appendix Table \ref{tab:ngram_results}. Additionally, as $n$ increases, the performance of SelfCheckGPT with $n$-gram (max) drops.

\textbf{5) SelfCheckGPT with NLI}. The NLI-based method outperforms all black-box and grey-box baselines, and its performance is close to the performance of the Prompt method. As SelfCheckGPT with Prompt can be computationally heavy, SelfCheckGPT with NLI could be the most practical method as it provides a good trade-off between performance and computation.



\subsection{Passage-level Factuality Ranking}
Previous results demonstrate that SelfCheckGPT is an effective approach for predicting sentence-level factuality. An additional consideration is whether SelfCheckGPT can also be used to determine the overall factuality of passages. Passage-level factuality scores are calculated by averaging the sentence-level scores over all sentences. 
\begin{equation}
    \mathcal{S}_{\text{passage}} = \frac{1}{|R|} \sum_i \mathcal{S}(i)
    \label{eq:doc_level_score}
\end{equation}
where $\mathcal{S}(i)$ is the sentence-level score, and $|R|$ is the number of sentences in the passage. Since human judgement is somewhat subjective, averaging the sentence-level labels would lead to ground truths with less noise. Note that for Avg($-\log p$) and  Avg($\mathcal{H}$), we compute the average over all tokens in a passage. Whereas for Max($-\log p$) and Max($\mathcal{H}$), we first take the maximum operation over tokens at the sentence level, and we then average over all sentences following Equation \ref{eq:doc_level_score}. 

Our results in Table \ref{tab:big_table} and Figure \ref{fig:scatter_plot_passage} show that all SelfCheckGPT methods correlate far better with human judgements than the other baselines, including the grey-box probability and entropy methods. SelfCheckGPT-Prompt is the best-performing method, achieving the highest Pearson correlation of 78.32. Unsurprisingly, the proxy LLM approach again achieves considerably lower correlations. 

% Further, the three variants of SelfCheckGPT appear complementary, with the combined approach being the best-performing system, achieving the highest Pearson correlation of 69.05. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Ablation Studies}
\subsubsection*{External Knowledge (instead of SelfCheck)}
If external knowledge is available, one can measure the informational consistency between the LLM response and the information source. In this experiment, we use the first paragraph of each concept that is available in WikiBio.\footnote{This method is no longer zero-resource as it requires retrieving relevant knowledge from external data.}

\begin{table}[!ht]
  \centering
  \tabcolsep=1.18mm
    \small
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Method}  &\multicolumn{3}{c}{Sent-lvl AUC-PR} &\multicolumn{2}{c}{Passage-lvl}  \\
    &NoFac &NoFac* &Fact           &Pear. &Spear.     \\
    \midrule
    SelfCk-BERT      &81.96 &45.96 &44.23 &58.18 &55.90  \\
    WikiBio+BERT     &81.32 &40.62 &49.15 &58.71 &55.80\\
    \midrule
    SelfCk-QA        &84.26 &40.06 &48.14 &61.07 &59.29 \\
    WikiBio+QA       &84.18 &45.40 &52.03 &57.26 &53.62\\
    \midrule
    SelfCk-1gm       &85.63 &41.04 &58.47 &64.71 &64.91 \\ % unigram3 (max -logprob)
    WikiBio+1gm      &80.43 &31.47 &40.53 &28.67 &26.70 \\
    \midrule
    SelfCk-NLI   &92.50 &45.17 &66.08 &74.14 &73.78 \\ 
    WikiBio+NLI  &91.18 &48.14 &71.61 &78.84 &80.00 \\
    \midrule
    SelfCk-Prompt    &93.42 &53.19 &67.09 &78.32 &78.30 \\ 
    WikiBio+Prompt   &93.59 &65.26 &73.11 &85.90 &86.11 \\
    \bottomrule
  \end{tabular}
  \caption{The performance when using SelfCheckGPT samples versus external stored knowledge.}
  \label{tab:ablation_external_knowledge}
\end{table}

\noindent Our findings in Table \ref{tab:ablation_external_knowledge} show the following. First, SelfCheckGPT with BERTScore/QA, using self-samples, can yield comparable or even better performance than when using the reference passage. Second, SelfCheckGPT with $n$-gram shows a large performance drop when using the WikiBio passages instead of self-samples. This failure is attributed to the fact that the WikiBio reference text alone is not sufficient to train an $n$-gram model. Third, in contrast, SelfCheckGPT with NLI/Prompt can benefit considerably when access to retrieved information is available. Nevertheless, in practice, it is infeasible to have an external database for every possible use case of LLM generation.








\subsubsection*{The Impact of the Number of Samples}
\noindent Although sample-based methods are expected to perform better when more samples are drawn, this has higher computational costs. Thus, we investigate performance as the number of samples is varied. Our results in Figure \ref{fig:number_of_samples_doc_level} show that the performance of SelfCheckGPT increases smoothly as more samples are used, with diminishing gains as more samples are generated. SelfCheckGPT with $n$-gram requires the highest number of samples before its performance reaches a plateau. 

\begin{figure}[!ht]
    \centering
\includegraphics[width=0.8\linewidth,keepaspectratio]{fig/ablation_number_samples_doclevel.pdf}
    \caption{The performance of SelfCheckGPT methods on ranking passages (Spearman's) versus the number of samples.}
    \label{fig:number_of_samples_doc_level}
\end{figure}



% \subsubsection*{Model Choice for Proxy LLM}
%  Figure \ref{fig:ablation_model_size} (in Appendix \ref{sec:appendix_results}) illustrates that  LLaMA is far better than other LLMs, and the performance of the proxy LLM method increases with model size. Similarly, average probability, Avg($p$), is closer to that of GPT-3 when using a larger proxy LLM as shown in Table \ref{tab:llm_token_prob} in the appendix.

\subsubsection*{The Choice of LLM for SelfCheckGPT-Prompt}
We investigate whether the LLM generating the text can self-check its own text. We conduct this ablation using a reduced set of the samples ($N$=4).

\begin{table}[!ht]
  \centering
    \small
  \begin{tabular}{llccc}
    \toprule
    % \multicolumn{2}{c}{LLM} &\multirow{2}{*}{$N$} &\multicolumn{2}{c}{Passage-level}  \\
    Text-Gen &SelfCk-Prompt  &$N$ &Pear. &Spear.     \\
    \midrule
    GPT-3 &ChatGPT &20 &78.32 &78.30 \\
    GPT-3 &ChatGPT &4  &76.47 &76.41 \\
    GPT-3 &GPT-3   &4  &73.11 &74.69 \\
    \midrule
    % \multicolumn{2}{l}{SelfCk w/ QA}            &20 &61.07 &59.29 \\
    \multicolumn{2}{l}{$^{\dagger}$SelfCheck w/ unigram (max)} &20 &64.71 &64.91 \\
    \multicolumn{2}{l}{$^{\dagger}$SelfCheck w/ NLI} &20 &74.14 &73.78 \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of GPT-3 (text-davinci-003) and ChatGPT (gpt-3.5.turbo) as the prompt-based text evaluator in SelfCheckGPT-Prompt. $^{\dagger}$Taken from Table \ref{tab:big_table} for comparison.}
  \label{tab:ablation_prompt_llm}
\end{table}
\label{section:ablation_llm_choice}

\noindent The results in Table \ref{tab:ablation_prompt_llm} show that GPT-3 can self-check its own text, and is better than the unigram method even when using only 4 samples. However, ChatGPT shows a slight improvement over GPT-3 in evaluating whether the sentence is supported by the context. More details are in Appendix \ref{appendix:ablation_prompt_llm}.





\section{Conclusions}
This paper is the first work to consider the task of hallucination detection for general large language model responses. We propose SelfCheckGPT, a zero-resource approach that is applicable to any black-box LLM without the need for external resources, and demonstrate the efficacy of our method. SelfCheckGPT outperforms a range of considered grey-box and black-box baseline detection methods at both the sentence and passage levels, and we further release an annotated dataset for GPT-3 hallucination detection with sentence-level factuality labels. 
%% This initial study and methods, based on self-chacking, can inspire future solutions for better dealing with the increasingly important issue of hallucination detection of LLMs.


\section*{Limitations}
In this study, the 238 GPT-3 generated texts were predominantly passages about individuals in the WikiBio dataset. To further investigate the nature of LLM's hallucination, this study could be extended to a wider range of concepts, e.g., to also consider generated texts about locations and objects. Further, this work considers factuality at the sentence level, but we note that a single sentence may consist of both factual and non-factual information. For example, the following work by \citet{min2023factscore} considers a fine-grained factuality evaluation by decomposing sentences into atomic facts. Finally, SelfCheckGPT with Prompt, which was convincingly the best selfcheck method, is quite computationally heavy. This might lead to impractical computational costs, which could be addressed in future work to be made more efficient.

\section*{Ethics Statement}
%There are no ethical risks for this piece of work.

% Our work identifies LLM hallucinations, and though we demonstrate that SelfCheckGPT can detect hallucinations effectively, the method may still make errors. We therefore advise users to be mindful that the classification made is not a guarantee of lack of hallucination. 

% Although we demonstrate that our approach can detect hallucinations effectively, some non-factual information may still be wrongly flagged as factual, which could lead to misinformation.

% This work studies hallucination in large language models, and there is no known ethical concern.

As this work addresses the issue of LLM's hallucination, we note that if hallucinated contents are not detected, they could lead to misinformation.

\section*{Acknowledgments}
This work is supported by Cambridge University Press \& Assessment (CUP\&A), a department of The Chancellor, Masters, and Scholars of the University of Cambridge, and the Cambridge Commonwealth, European \& International Trust. We would like to thank the anonymous reviewers for their helpful comments.



% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\newpage
\appendix
\section{Models and Implementation}

\subsection{Entropy}
The entropy of the output distribution is implemented as follows,
\begin{equation}
    \mathcal{H}_{ij} = 2^{ - \sum_{\tilde{w} \in \mathcal{W}} p_{ij}(\tilde{w}) \log_2 p_{ij}(\tilde{w})}
\end{equation}
where $\mathcal{W}$ is the set of all possible words in the vocabulary.

\subsection{Proxy LLMs}
The proxy LLMs considered are LLaMA-\{7B, 13B, 30B\} \cite{touvron2023llama}, OPT-\{125m, 1.3B, 13B, 30B\} \cite{zhang2022opt}, GPT-J-6B \cite{gpt_j} and GPT-NeoX-20B \cite{black-etal-2022-gpt}.



\subsection{SelfCheckGPT's Systems}

\noindent \textbf{Question Answering}: The generation systems \texttt{G1} and \texttt{G2} are T5-Large fine-tuned to SQuAD \cite{rajpurkar-etal-2016-squad} and RACE \cite{lai-etal-2017-race}, respectively. The answering system \texttt{A} is Longformer \cite{longformer} fine-tuned to the RACE dataset. The answerability system \texttt{U} is also Longformer, but fine-tuned to SQuAD2.0. \\

\noindent \textbf{LLM for Prompting}: We consider two LLMs, GPT-3 (text-davinci-003) and ChatGPT (gpt-3.5-turbo) We note that during the data creation and annotation, GPT-3 (text-davinci-003) was the state-of-the-art LLM available; hence, GPT-3 was used as the main LLM generating WikiBio passages. 


\section{SelfCheckGPT with QA}
\label{appendix:selfcheck_qa}
Previous work showed that implementing question generation (in Equation \ref{eq:question_gen}) with two generators (\texttt{G1} generates the question and associated answer, and \texttt{G2} generates distractors) yields higher-quality distractors \cite{manakul2023mqag}. Thus, a two-stage generation is adopted in this work as follows:
\begin{equation}
    q, a \sim P_{{\tt G1}}(q,a|r_i); \hspace{2mm} 
    \mathbf{o}_{\backslash a} \sim P_{{\tt G2}}(\mathbf{o}_{\backslash a}|q,a,R)
\end{equation}
where $\mathbf{o} = \{a, \mathbf{o}_{\backslash a}\}= \{o_1, ..., o_4\}$. In addition, to filter out bad (unanswerable) questions, we define an answerability score \cite{raina-gales-2022-answer}:
\begin{equation}
    \alpha = P_{{\tt U}}(\text{answerable}|q,\text{context})
\end{equation}
where the context is either the response $R$ or sampled passages $S^n$, and $\alpha \rightarrow 0.0$ for unanswerable and $\alpha \rightarrow 1.0$ for answerable. We use $\alpha$ to filter out unanswerable questions which have $\alpha$ lower than a threshold. Next, we derive how Bayes' theorem can be applied to take into account the number of answerable/unanswerable questions.

\subsection{SelfCheckGPT-QA with Bayes}
% \subsubsection*{Theory}
\label{appendix:bayes_qa}
Let $P(\text{F})$ denote the probability of the $i$-th sentence being non-factual, and $P(\text{T})$ denote the probability of the $i$-th sentence being factual. For a question $q$, the probability of $i$-th sentence being non-factual given a set of matched answers ${L}_{\tt m}$ and a set of not-matched answers ${L}_{\tt n}$ is:
\begin{align}
    &P(\text{F} | {L}_{\tt m}, {L}_{\tt n}) \nonumber \\
    &= \frac{ P({L}_{\tt m}, {L}_{\tt n} | \text{F}) P(\text{F})}{P({L}_{\tt m}, {L}_{\tt n} | \text{F}) P(\text{F}) + P({L}_{\tt m}, {L}_{\tt n} | \text{T}) P(\text{T})} \nonumber \\
    &= \frac{ P({L}_{\tt m}, {L}_{\tt n} | \text{F})}{P({L}_{\tt m}, {L}_{\tt n} | \text{F})  + P({L}_{\tt m}, {L}_{\tt n} | \text{T}) } \label{eq:bayes}
\end{align}
where we assume the sentence is equally likely to be False or True, i.e. $P(\text{F}) = P(\text{T})$. The probability of observing ${L}_{\tt m}, {L}_{\tt n}$ when the sentence is False (non-factual):
\begin{align}
&P({L}_{\tt m}, {L}_{\tt n} | \text{F}) \nonumber \\
&= \underset{a \in {L}_{\tt m}}{\prod} P(a = a_R | F) \underset{a' \in {L}_{\tt n}}{\prod} P(a' \neq a_R | F) \nonumber \\
&= (1-\beta_1)^{{N}_{\tt m}} (\beta_1)^{{N}_{\tt n}}
\end{align}
and probability of observing ${L}_{\tt m}, {L}_{\tt n}$ when the sentence is True (factual):
\begin{align}
&P({L}_{\tt m}, {L}_{\tt n} | \text{T}) \nonumber \\
&= \underset{a \in {L}_{\tt m}}{\prod} P(a = a_r | T) \underset{a' \in {L}_{\tt n}}{\prod} P(a' \neq a_r | T) \nonumber \\
&= (\beta_2)^{{N}_{\tt m}} (1-\beta_2)^{{N}_{\tt n}}
\end{align}
where ${N}_{\tt m}$ and ${N}_{\tt n}$ are the number of matched answers and the number of not-matched answers, respectively. Hence, we can simplify Equation \ref{eq:bayes}:
\begin{equation}
    P(\text{F} | {L}_{\tt m}, {L}_{\tt n}) = \frac{\gamma_2^{{N}_{\tt n}}}{\gamma_1^{{N}_{\tt m}} + \gamma_2^{{N}_{\tt n}} }
\end{equation}
where $\gamma_1 = \frac{\beta_2}{1-\beta_1}$ and $\gamma_2 = \frac{\beta_1}{1-\beta_2}$. Lastly, instead of rejecting samples having an answerability score below a threshold,\footnote{$\alpha$ is between 0.0 (unanswerable) and 1.0 (answerable). Standard-counting ${N}_{\tt m}$ and ${N}_{\tt n}$ can be considered as a special case of soft-counting where $\alpha$ is set to 1.0 if $\alpha$ is greater than the answerability threshold and otherwise $\alpha$ is 0.0.} we find empirically that soft-counting (defined below) improves the detection performance. We set both $\beta_1$ and $\beta_2$ to 0.8.

\begin{equation}
    {N}'_{\tt m} = \underset{n \hspace{0.3em}\text{s.t.}\hspace{0.3em} a_n \in {L}_{\tt m}}{\sum}\alpha_n;  \hspace{2mm}
    {N}'_{\tt n} = \underset{n \hspace{0.3em}\text{s.t.}\hspace{0.3em} a_n \in {L}_{\tt n}}{\sum}\alpha_n
    \label{eq:soft_counting}
\end{equation}
where $\alpha_n = P_{{\tt U}}(\text{answerable}|q, S^n)$. Therefore, the SelfCheckGPT with QA score, $\mathcal{S}_\text{QA}$, is:
\begin{equation}
    \mathcal{S}_\text{QA} = P(\text{F} | {L}_{\tt m}, {L}_{\tt n}) = \frac{\gamma_2^{N'_{\tt n}}}{\gamma_1^{N'_{\tt m}} + \gamma_2^{N'_{\tt n}} }
\end{equation}

% \subsubsection*{Results}
\noindent In Table \ref{tab:qa_bayes_results}, we show empically that applying Bayes' theorem and soft counting $\alpha$ (in Equation \ref{eq:soft_counting}) improves the performance of the SelfCheckGPT with QA method.
\begin{table}[!ht]
  \centering
  \small
  \tabcolsep=1.7mm
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Varaint}  &\multicolumn{3}{c}{Sentence-lvl} &\multicolumn{2}{c}{Passage-lvl} \\
    &NoF &NoF* &Fact &PCC &SCC \\
    \midrule
    SimpleCount         &83.97 &40.07 &47.78 &57.39 &55.15\\
    + Bayes             &83.04 &38.58 &47.41 &56.43 &55.03 \\
    + Bayes + $\alpha$  &84.26 &40.06 &48.14 &61.07 &59.29 \\
    \bottomrule
  \end{tabular}
  \caption{Performance of SelfCheckGPT-QA's variants.}
  \label{tab:qa_bayes_results}
\end{table}

\section{SelfCheckGPT with Prompt}
% \subsection{Prompt}
\label{appendix:ablation_prompt_llm}
We use the prompt template provided in the main text (in Section \ref{section:selfcheck_prompt}) for both GPT-3 (text-davinci-003) and ChatGPT (gpt-3.5-turbo). For ChatGPT, a standard system message "\texttt{You are a helpful assistant.}" is used in setting up the system.

At the time of conducting experiments, the API costs per 1,000 tokens are \$0.020 for GPT-3 and \$0.002 for ChatGPT. The estimated costs for running the models to answer \texttt{Yes/No} on all 1908 sentences and 20 samples are around \$200 for GPT-3 and \$20 for ChatGPT. Given the cost, we conduct the experiments on 4 samples when performing the ablation about LLM choice for SelfCheckGPT-Prompt (Section \ref{section:ablation_llm_choice}). Table \ref{tab:llm_choice_agreement} shows the breakdown of predictions made by GPT-3 and ChatGPT. 


\begin{table}[!ht]
  \centering
  \small
  \begin{tabular}{c|cc}
    \toprule
    \backslashbox{GPT-3}{ChatGPT} &\texttt{Yes} &\texttt{No} \\
    \midrule
    \texttt{Yes}  &3179  &1038 \\
    \texttt{No}   &367   &3048 \\
    \bottomrule
  \end{tabular}
  \caption{Breakdown of predictions made by GPT-3/ChatGPT when prompted to answer \texttt{Yes}(supported)/\texttt{No}(not-supported).}
  \label{tab:llm_choice_agreement}
\end{table}

\section{Additional Experimental Results}
Here, we provide experimental results that are complementary to those presented in the main paper. 
\label{sec:appendix_results}



\begin{table}[!ht]
  \centering
    \small
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{$n$-gram}  &\multicolumn{3}{c}{Sent-lvl AUC-PR} &\multicolumn{2}{c}{Passage-lvl}  \\
    &NoFac &NoFac* &Fact           &Pear. &Spear.     \\
    \midrule
    \rowcolor{Gray}
    \multicolumn{6}{l}{Avg($-$log$p$)} \\
    1-gram &81.52  &40.33  &41.76   &40.68 &39.22  \\
    2-gram &82.94  &44.38  &52.81   &58.84 &58.11  \\
    3-gram &83.56  &44.64  &53.99   &62.21 &63.00  \\
    4-gram &83.80  &43.55  &54.25   &61.98 &63.64  \\
    5-gram &83.45  &42.31  &53.98   &60.68 &62.96  \\
    \rowcolor{Gray}
    \multicolumn{6}{l}{Max($-$log$p$)} \\
    1-gram &85.63 &41.04 &58.47 &64.71 &64.91 \\
    2-gram &85.26 &39.29 &58.29 &62.48 &66.04 \\
    3-gram &84.97 &37.10 &57.08 &57.34 &60.49 \\
    4-gram &84.49 &36.37 &55.96 &55.77 &57.25 \\
    5-gram &84.12 &36.19 &54.89 &54.84 &55.97 \\    
    \bottomrule
  \end{tabular}
  \caption{The performance using different $n$-gram models in the SelfCheckGPT with $n$-gram method.}
  \label{tab:ngram_results}
\end{table}





\begin{figure}[!ht]
    \centering
\includegraphics[width=0.8\linewidth,keepaspectratio]{fig/ablation_number_samples_detect_nonfact.pdf}
    \caption{The performance of SelfCheckGPT methods on sentence-level non-factual detection (AUC-PR) versus the number of samples. This Figure extends the passage-level results in Figure \ref{fig:number_of_samples_doc_level}.}
    \label{fig:number_of_samples_sentence_level}
\end{figure}

% \begin{figure}[!ht]
%     \begin{subfigure}[b]{\linewidth}
% \centering
% \includegraphics[width=0.84\linewidth,keepaspectratio]{fig/ablation_model_size_sentence.pdf}
%       \caption{Sentence-level Detection}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{\linewidth}
% \centering
% \includegraphics[width=0.84\linewidth,keepaspectratio]{fig/ablation_model_size_passage.pdf}
%       \caption{Passage-level Ranking}
%     \end{subfigure}
%     \caption{Performance of the Avg($\mathcal{H}$) method using a proxy LLM where the model sizes are: LLaMA=\{7B, 13B, 30B\}, OPT=\{125m, 1.3B, 13B, 30B\}, GPT-J=6B, and NeoX=20B. The full results are provided in Table \ref{tab:big_table_appendix}.}
%     \label{fig:ablation_model_size}
% \end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth,keepaspectratio]{fig/ablation_model_size_passage.pdf}
    \caption{Passage-level ranking performance of the Avg($\mathcal{H}$) method using proxy LLM where the sizes are: LLaMA=\{7B, 13B, 30B\}, OPT=\{125m, 1.3B, 13B, 30B\}, GPT-J=6B, NeoX=20B. The full results are provided in Table \ref{tab:big_table_appendix}.}
    \label{fig:ablation_model_size}
\end{figure}

% \begin{table}[!ht]
%   \centering
%   \begin{tabular}{rlc}
%     \toprule
%     LLM            &Size  &Avg($p$)  \\
%     \midrule
%     \texttt{GPT-3} &175B      &72.02 \\
%     \midrule
%     \texttt{LLaMA}          &30B &65.25 \\
%     \texttt{LLaMA}          &13B &64.71 \\
%     \texttt{LLaMA}          &7B  &63.70 \\
%     \midrule
%     \texttt{OPT}   &30B       &53.81 \\
%     \texttt{NeoX}  &20B       &54.49 \\
%     \texttt{OPT}   &13B       &52.80 \\
%     \texttt{GPT-J} &6B        &52.50 \\
%     \texttt{OPT}   &1.3B      &49.20 \\
%     \texttt{OPT}   &125m      &40.91 \\
%     \bottomrule
%   \end{tabular}
%   \caption{Average token probability, Avg($p$), over all tokens in GPT-3 generated passages.}
%   \label{tab:llm_token_prob}
% \end{table}


\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_bertscore.pdf}
    \caption{SelfCheckGPT-BERTScore}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_method3.pdf}
    \caption{SelfCheckGPT-QA}
    \end{subfigure}
    % \begin{subfigure}[b]{0.31\linewidth}
    % \centering    
    % \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_combine.pdf}
    % \caption{SelfCheckGPT-Combination}
    % \end{subfigure}
    \begin{subfigure}[b]{0.245\linewidth}
    \centering    
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_unigram3.pdf}
    \caption{SelfCheckGPT-1gram(max)}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\linewidth}
    \centering    
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_nli.pdf}
    \caption{SelfCheckGPT-NLI}
    \end{subfigure}
\caption{Scatter plot of passage-level scores where Y-axis = Method scores, X-axis = Human scores. Correlations are reported in Table \ref{tab:big_table}. This figure provides results in addition to Figure \ref{fig:scatter_plot_passage}.}
\label{fig:scatter_plot_passage_appendix}
\end{figure*}

\begin{table*}[!ht]
  \centering
  \small
  \begin{tabular}{rlccccc}
    \toprule
    \multirow{2}{*}{LLM} &\multirow{2}{*}{Size}   &\multicolumn{3}{c}{Sentence-level (AUC-PR)}  &\multicolumn{2}{c}{Passage-level (Corr.)}  \\
    & &NonFact &NonFact* &Factual &Pearson  &Spearman \\
    \midrule
    Random         &- &72.96 &29.72 &27.04 &- &-   \\
    \rowcolor{Gray}
    \multicolumn{7}{l}{Avg($-$log$p$) Method} \\
    \texttt{LLaMA} &30B  &75.43  &30.32  &41.29  &21.72 &20.20  \\
    \texttt{LLaMA} &13B  &74.16  &30.01  &37.36  &13.33 &12.89  \\
    \texttt{LLaMA} &7B   &71.69  &27.87  &31.30  &-2.71 &-2.59   \\
    
    \texttt{OPT}   &30B  &67.70  &24.43  &25.04  &-32.07 &-31.45  \\
    \texttt{NeoX}  &20B  &69.00  &24.38  &26.18  &-31.79 &-34.15  \\
    \texttt{OPT}   &13B  &67.46  &24.39  &25.20  &-33.05 &-32.79  \\
    \texttt{GPT-J} &6B   &67.51  &24.28  &24.26  &-38.80 &-40.05  \\
    \texttt{OPT}   &1.3B &66.19  &24.47  &23.47  &-35.20 &-38.95  \\
    \texttt{OPT}   &125m &66.63  &25.31  &23.07  &-30.38 &-37.54  \\
    \rowcolor{Gray}
    \multicolumn{7}{l}{Avg($\mathcal{H}$) Method} \\
    \texttt{LLaMA} &30B  &80.80  &39.01  &42.97  &33.80 &39.49 \\
    \texttt{LLaMA} &13B  &80.63  &38.98  &40.59  &29.43 &33.12 \\
    \texttt{LLaMA} &7B   &78.67  &37.22  &33.81  &19.44 &21.79  \\
    
    \texttt{OPT}   &30B  &77.13  &33.67  &29.55 &-0.43 &3.43  \\
    \texttt{NeoX}  &20B  &77.40  &32.78  &30.13 &5.41 &7.43   \\
    \texttt{OPT}   &13B  &76.93  &33.71  &29.68 &0.25 &1.39  \\
    \texttt{GPT-J} &6B   &76.15  &33.29  &28.30 &-2.50 &-1.37   \\
    \texttt{OPT}   &1.3B &74.05  &31.91  &26.33 &-10.59 &-10.00 \\
    \texttt{OPT}   &125m &71.51  &30.88  &25.36 &-14.16 &-13.76 \\
    \rowcolor{Gray}
    \multicolumn{7}{l}{Max($-$log$p$) Method} \\
    \texttt{LLaMA} &30B  &74.01  &27.14  &31.08  &-22.83 &-22.71 \\
    \texttt{LLaMA} &13B  &71.12  &26.78  &28.82  &-34.93 &-31.70 \\
    \texttt{LLaMA} &7B   &69.57  &25.91  &26.54  &-42.57 &-38.24 \\
    
    \texttt{OPT}   &30B  &67.32  &24.40  &24.32 &-49.51 &-45.50 \\
    \texttt{NeoX}  &20B  &67.51  &23.88  &24.82 &-47.96 &-44.54 \\
    \texttt{OPT}   &13B  &67.36  &24.67  &24.46 &-50.15 &-44.42 \\
    \texttt{GPT-J} &6B   &67.58  &23.94  &23.93 &-51.23 &-47.68 \\
    \texttt{OPT}   &1.3B &68.16  &25.85  &24.66 &-45.60 &-42.39 \\
    \texttt{OPT}   &125m &69.23  &27.66  &24.14 &-39.22 &-37.18 \\
    \rowcolor{Gray}
    \multicolumn{7}{l}{Max($\mathcal{H}$) Method} \\
    \texttt{LLaMA} &30B  &80.92  &37.32  &37.90  &35.57 &38.94   \\
    \texttt{LLaMA} &13B  &80.98  &37.94  &36.01  &32.07 &34.01 \\
    \texttt{LLaMA} &7B   &79.65  &35.57  &31.32  &22.10 &22.53  \\
    
    \texttt{OPT}   &30B  &76.58  &33.44  &29.31 &1.63 &6.41 \\
    \texttt{NeoX}  &20B  &76.98  &31.96  &29.13 &5.97 &9.31 \\
    \texttt{OPT}   &13B  &76.26  &32.81  &29.25 &1.42 &2.82 \\
    \texttt{GPT-J} &6B   &75.30  &32.51  &28.13 &-2.14 &1.41 \\
    \texttt{OPT}   &1.3B &73.79  &31.42  &26.38 &-9.84 &-9.80 \\
    \texttt{OPT}   &125m &71.32  &31.65  &25.36 &-18.05 &-17.37 \\
    \bottomrule
  \end{tabular}
  \caption{AUC-PR for Detecting Non-Factual and Factual Sentences in the GPT-3 generated WikiBio passages. Passage-level PCC and SCC with LLMs used
to assess GPT-3 responses. This table is an extension to Table \ref{tab:big_table}.}
  \label{tab:big_table_appendix}
\end{table*}


\end{document}
