% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}


%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}
% \usepackage[colorinlistoftodos]{todonotes}
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage[bottom]{footmisc}

% Formatting tables and figures
\usepackage{multirow, booktabs}
\usepackage{caption}
\captionsetup{font={small}, labelfont={bf}}
% \captionsetup{font={footnotesize}}
% \captionsetup{labelfont={bf}}

% coloring highlight rows in tables
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}

% \newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
% \setlength{\columnseprule}{0.4pt}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{SelfCheckGPT}: Zero-Resource Black-Box Hallucination Detection\\ for Generative Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Potsawee Manakul, Adian Liusie, Mark J. F. Gales \\
  Department of Engineering, University of Cambridge \\
  \texttt{pm574@cam.ac.uk, al826@cam.ac.uk, mjfg@eng.cam.ac.uk}}


\begin{document}
\pagenumbering{arabic}
\pagestyle{plain}
\maketitle
\begin{abstract}

Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that in sentence hallucination detection, our approach has AUC-PR scores comparable to or better than grey-box methods, while SelfCheckGPT is best at passage factuality assessment.\footnote{Coda and dataset can be found on the project page at \url{https://github.com/potsawee/selfcheckgpt}.} 


%%% General Problem/Motivation %%%
% Generative Large language models (LLMs) such
% as GPT-3 are capable of generating highly fluent
% responses to a variety of user prompts. Yet, LLMs may produce non-factual information, especially in situations where they do not have much information about the topic.  

% %%% Specific setup we are interested %%%

% Without external knowledge/database, uncertainty metrics such as token probabilities and entropies could be used to assess or generated texts. However, these uncertainty metrics require access to the generative model for the additional information beyond text-based responses, which may not always be available, especially in widely-used ChatGPT's web interface and its API.

% %%% Proposed Method %%%
% In this work, we propose "SelfCheckGPT" -- a sampling-based black-box method that requires only access to text responses -- for self fact-checking LLM generated texts. The method is based on the idea that if LLM has knowledge about the topic, the sampled responses should be similar, whereas if the information in sampled responses is inconsistent, it could indicate that LLM has less knowledge.

% %%% Results %%%
% We use GPT-3 to generate passages about persons in the WikiBio dataset, and we annotate the passages on the factuality aspect at the sentence level. Two factuality tasks are carried out: (i) detecting non-factual and factual sentences; (ii) ranking generated passages based on the factuality score. On these tasks, our results show that our SelfCheckGPT method considerably outperforms black-box baselines, and it is also comparable to grey-box methods.


\end{abstract}

\section{Introduction}
Large Language Models (LLMs) such as GPT-3 \citep{brown2020language}, PaLM \cite{chowdhery2022palm}, and Chinchilla \cite{chinchilla} are capable of generating highly fluent and realistic responses to a variety of user prompts. They have been used in many applications such as automatic tools to draft reports, virtual assistants that retrieve information, summarization systems, as well as a multitude of other generative applications. Despite the convincing and realistic nature of LLM-generated texts, a concern with LLMs is their tendency to hallucinate facts and make up information. 

\begin{figure}
\includegraphics[width=\linewidth,keepaspectratio]{fig/diagram.drawio.pdf}
    \caption{SelfCheckGPT with Question Answering.}
\end{figure}


A method for hallucination detection is to leverage existing intrinsic uncertainty metrics such as token probability or entropy since these metrics can be used to determine the parts of the output sequence the system is least certain of \cite{yuan2021bartscore, gptscore}. However, all current uncertainty metrics require access to the output token-level probability distribution information that may not necessarily be available to users, e.g. when systems are accessed used through limited external APIs such as ChatGPT. Further, there is an active field of fact-verification where evidence is retrieved from an external database to assess the veracity of a claim \citep{Thorne18Fact, guo-etal-2022-survey}. However, facts can only be assessed relative to the knowledge present in the database. Though corpora such as Wikipedia can cover a great deal of general knowledge and serve as a useful database for fact verification, hallucination is observed over a wide range of tasks beyond pure fact verification. For example, summaries from automatic systems can contain information not present in the context \citep{kryscinski-etal-2019-neural, kryscinski-etal-2020-evaluating, maynez-etal-2020-faithfulness}. 

In this paper, we propose SelfCheckGPT, a simple sampling-based approach that can detect whether responses generated by LLMs are hallucinated or factual. SelfCheckGPT only uses sampled responses and can therefore be used on black box models, while it also operates in a zero-resource fashion, i.e. with no external database. The motivating idea of SelfCheckGPT is that when a LLM knows a given concept well, the sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and may completely contradict one another. By sampling multiple responses from a LLM, one can measure information consistency between the different responses and determine which statements are factual and which have been hallucinated. Three variants of SelfCheckGPT for measuring informational consistency are considered: BERTScore, question-answering, and n-gram. Through analysis of annotated articles generated by GPT-3, we show that SelfCheckGPT can determine factual documents effectively in a black-box, zero-resource manner.

\section{Related Work}
\subsection{Large Language Models}
There has been rapid growth in current large language models (LLMs) literature with larger and better models being constantly released \cite{chowdhery2022palm}. These models are commonly used as the backbone for a range of NLP tasks \cite{wang-etal-2018-glue}. Traditionally, these LLMs are fine-tuned to a specific task and/or domain \cite{devlin-etal-2019-bert, radford2019language, raffel2020exploring}, however, a fascinating finding is that as models scale up, they inherit abilities to naturally solve a wide range of natural language tasks in a zero-shot fashion \cite{brown2020language, weiemergent}. 

% For the simple downstream application of generating synthetic Wikipedia articles, this paper only leverages vanilla LLMs. 

\subsection{Hallucination of Large Language Models}
Hallucination has been studied in text generation tasks, including summarization \cite{Huang_hallucination_survey} and dialogue generation \cite{shuster-etal-2021-retrieval-augmentation}. A survey of hallucination in a variety of natural language generation tasks has been conducted \cite{hallucination_survey_2023}. Further, \citet{liu-etal-2022-token} compiled a hallucination detection dataset, but the texts were obtained by perturbing factual texts; thus, this dataset may not reflect actual LLM hallucination. 

% this initial research mainly focused on domain-specific, small LMs, and the extent to which this generalizes to large generative LLMs remains unclear. 

Recently, \citet{azaria2023internal} trains a multi-layer perception classifier using LLM's hidden representations as the input to predict a truthfulness of a sentence. This approach requires labelled data for supervised training as well as the internal states of the LLM, which may not be available through APIs. Another recent approach is self-evaluation \cite{kadavath2022language}, which is a method where the LLM is prompted to answer about its previous prediction, e.g. the probability of its generated response/answer is true.


\subsection{Sequence Level Uncertainty Estimation}
Token probabilities have been used as an indication of model certainty. For example, OpenAI's GPT-3 web interface allows users to display token probabilities as shown in Figure \ref{fig:openai_playground}. Additionally, uncertainty estimation based on aleatoric and epistemic uncertainty for autoregressive generation has been studied \cite{xiao-wang-2021-hallucination, malinin2021uncertainty}. Further, conditional language model scores have been used to evaluate properties of texts \cite{yuan2021bartscore, gptscore}. Recently, semantic uncertainty is proposed to address uncertainty in free-form generation tasks where probabilities are attached to meanings of text instead of tokens \cite{kuhn2023semantic}.


\begin{figure}[!ht]
    \includegraphics[width=\linewidth,keepaspectratio]{fig/playground_example0.png}
    \caption{Example of OpenAI's GPT-3 interface with token probabilities displayed.}
    \label{fig:openai_playground}
\end{figure}


\subsection{Fact Verification}
Existing fact-verification approaches follow a multi-stage pipeline of claim detection, evidence retrieval and verdict prediction \citep{guo-etal-2022-survey, zhong-etal-2020-reasoning}. Such methods, however, require access to external databases and can have considerable inference costs. 

\section{Grey-Box Factuality Assessment}
This section will introduce methods that can be used to determine the factuality of LLM responses in a zero-resource setting when one has full access to output distributions.\footnote{Alternatively, white-box approaches, such as the method in \citet{azaria2023internal}, require access to full internal states of the LLM in addition to output distributions. As a result, they are less practical and not considered in this work.} We will use `factual' to define when statements are grounded in valid information, i.e. when hallucinations are avoided, and `zero-resource' when no external database is used. 

% We first motivate the intuition for why standard uncertainty-based metrics may be good proxies for factuality, then describe how systems' output token-level probability distributions can be used to determine sentence-level hallucinations.

\subsection{Uncertainty-based Assessment}
\noindent\textbf{Motivation}. To consider how the factuality of a generated response can be determined in a zero-resource setting, we consider LLM pre-training. During pre-training, the model is trained with next-word prediction over massive corpora of textual data. This gives the model a strong understanding of language \cite{jawahar-etal-2019-bert, raffel2020exploring}, powerful contextual reasoning \cite{zhang2020semantics}, as well as world knowledge \cite{liusie2022world}. Consider the input "\texttt{Lionel Messi is a \_}". Since Messi is a world-famous athlete who may have appeared multiple times in pre-training, the LLM is likely to know who Messi is. Therefore, given the context, the token "\texttt{footballer}" may be assigned a very high probability while some other professions such as "\texttt{carpenter}" will be considered very improbable. However, for the input "\texttt{John Smith is a \_}", the system may be unsure of how the sentence should continue, and have a flat probability distribution. During decoding, this will lead to a random word being generated- causing the system to hallucinate. 

This insight allows us to realize the connection between uncertainty metrics and factuality. Factual sentences are likely to contain tokens with higher likelihood and lower entropy, while hallucinations are likely to come from positions with flat probability distributions with high uncertainty.

\subsubsection*{Token-level Probability $p$}
Given the LLM's response $R$, let $i$ denote the $i$-th sentence in $R$, $j$ denote the $j$-th token in the $i$-th sentence, $J$ is the number of tokens in the sentence, and $p_{ij}$ be the probability of the word generated by the LLM at the $j$-th token of the $i$-th sentence. Two probability metrics are used:
\begin{align}
    \text{Avg}(- \log p) &= - \frac{1}{J} \sum_j \log  p_{ij}  \\
    \text{Max}(- \log p) &= \underset{j}{\text{max}} \left( - \log p_{ij} \right)
\end{align}
$\text{Max}(- \log p)$ measures the sentence's likelihood by assessesing the \textit{least} likely token in the sentence.
% \footnote{Minimum token probability is equivalent to maximum negative log probability.}

\subsubsection*{Entropy $\mathcal{H}$}
The entropy of the output distribution is:
\begin{equation}
    \mathcal{H}_{ij} = - \sum_{\tilde{w} \in \mathcal{W}} p_{ij}(\tilde{w}) \log p_{ij}(\tilde{w})
\end{equation}
where $p_{ij}(\tilde{w})$ is the probability of the word $\tilde{w}$ being generated at the $j$-th token of the $i$-th sentence, and $\mathcal{W}$ is the set of all possible words in the vocabulary. Similar to the probability-based metrics, two entropy-based metrics are used: 
\begin{align}
    \text{Avg}(\mathcal{H}) &= \frac{1}{J} \sum_j \mathcal{H}_{ij} \\
    \text{Max}(\mathcal{H}) &= \max_j \left[ \mathcal{H}_{ij} \right] 
\end{align}

\section{Black-Box Factuality Assessment}
\noindent\textbf{Motivation}.  A drawback of the previous grey-box methods is that they require output token-level probabilities. Though this may seem a reasonable requirement, for massive LLMs only available through limited API calls, such token-level information might not be available (such as with ChatGPT). Therefore, we consider black-box approaches because they remain applicable even when only text-based responses can be derived from the LLM. 

\subsection*{Proxy LLMs}
A simple baseline to consider is using a proxy LLM, i.e. another LLM that we have full access to such as LLaMA \cite{touvron2023llama}. With no access to the full outputs of the LLM generating the text, a proxy LLM could be used to approximate the output token-level probabilities. In the next section, we propose SelfCheckGPT, which is also a black-box approach. 


\section{SelfCheckGPT}
% 1. Motivation of SelfCheckGPT
% \noindent\textbf{Motivation}.
\textbf{Notation}. Let $R$ refer to the LLM response drawn from a given user query. SelfCheckGPT operates by drawing a further $N$ stochastic LLM response samples $\{S^1,S^2,..,S^n,...,S^N\}$ from the same query, followed by measuring the consistency between the response and stochastic samples. As a hallucination score of the $i$-th sentence, we design SelfCheckGPT $\mathcal{S}(i)$ such that $\mathcal{S}(i) \in [0.0, 1.0]$ and $\mathcal{S}(i) \rightarrow 1.0$ if the $i$-th sentence is hallucinated, and $\mathcal{S}(i) \rightarrow 0.0$ if it is grounded in valid information.

\subsection{SelfCheckGPT with BERTScore}
\label{section:self_check_bertscore}
 Let $\mathcal{B}(.,.)$ denote the BERTScore between two sentences. SelfCheckGPT with BERTScore finds the averages BERTScore of a sentence with the most similar sentence of each drawn sample:
% \begin{align}
    % \tilde{\mathcal{S}}_\text{BERT}(i) &= \frac{1}{N} \sum_{n=1}^N \underset{k}{\text{max}} \left( \mathcal{B} (r_i, s^n_k) \right) \\
    % {\mathcal{S}}_\text{BERT}(i) &= 1 - \tilde{\mathcal{S}}_\text{BERT}(i)  
% \end{align}
\begin{equation}
    \mathcal{S}_\text{BERT}(i) = 1-\frac{1}{N} \sum_{n=1}^N \underset{k}{\text{max}} \left( \mathcal{B} (r_i, s^n_k) \right)   
\end{equation}
where $r_i$ represent the $i$-th sentence in $R$ and $s^n_k$ represent the $k$-th sentence in the $n$-th sample $S^n$. This way if the information in a sentence appears in many drawn samples, one may assume that the information is factual, whereas if the statement appears in no other sample, it is likely a hallucination.  


\subsection{SelfCheckGPT with Question Answering}
\label{section:self_check_qa}
Based on the idea that information consistency could be assessed using question answering (QA), we apply the automatic multiple-choice question answering generation (MQAG) framework \cite{manakul2023mqag} to SelfCheckGPT. MQAG assesses consistency by generating multiple-choice questions that an answering system can independently answer given each passage. If facts on consistent concepts are queried, the answering system is expected to predict similar answers. The MQAG framework consists of a question-answer generation system $\texttt{G1}$, distractor generation system $\texttt{G2}$, and answering system \texttt{A}. For the sentence $r_i$ in the response $R$, we draw questions $q$, associated answers $a$, and distractors $\mathbf{o}_{\backslash a}$ as follows:
\begin{equation}
    q, a \sim P_{{\tt G1}}(q,a|r_i); \hspace{2mm} 
    \mathbf{o}_{\backslash a} \sim P_{{\tt G2}}(\mathbf{o}_{\backslash a}|q,a,R)
\end{equation}
where $\mathbf{o} = \{a, \mathbf{o}_{\backslash a}\}= \{o_1, ..., o_4\}$. To filter out bad (e.g. unanswerable) questions, we define an answerability score \cite{raina-gales-2022-answer}:
\begin{equation}
    \alpha = P_{{\tt U}}(\text{answerable}|q,\text{context})
\end{equation}
where the context is either the response $R$ or sampled passages $S^n$, and $\alpha \rightarrow 0.0$ for unanswerable and $\alpha \rightarrow 1.0$ for answerable. We use $\alpha$ to filter out unanswerable questions which have $\alpha$ lower than a threshold. Subsequently, we use the answering system \texttt{A} to answer all answerable questions:
\begin{align}
    a_R &= \underset{k}{\text{argmax}} \left[ P_{{\tt A}}(o_k | q, R, \mathbf{o})  \right] \\
    a_{S^n} &= \underset{k}{\text{argmax}} \left[  P_{{\tt A}}(o_k | q, S^n, \mathbf{o}) \right]
\end{align}
We compare whether $a_R$ is equal to $a_{S^n}$ for all samples $\{S^1,...,S^N\}$, yielding the number of matches $N_{\tt m}$ and the number of not-matches $N_{\tt n}$. Subsequently, a simple inconsistency score for the $i$-th sentence and question $q$ based on the match/not-match counts is calculated:
$
    \mathcal{S}_\text{QA}(i,q) = \frac{N_{\tt n}}{N_{\tt m} + N_{\tt n}} \label{eq:simple_counting}
$.
To take into account the number of answerable questions (i.e. the evidence to assess the sentence), we use Bayes' theorem (derivation provided in Appendix \ref{appendix:bayes_qa}) to improve Equation \ref{eq:simple_counting} to
\begin{equation}
    \mathcal{S}_\text{QA}(i,q) = \frac{\gamma_2^{N'_{\tt n}}}{\gamma_1^{N'_{\tt m}} + \gamma_2^{N'_{\tt n}} }
\end{equation}
where $N'_{\tt m}$ = the effective match count, $N'_{\tt n}$ = the effective mismatch count, $\gamma_1$, and $\gamma_2$ are defined in Appendix \ref{appendix:bayes_qa}. Ultimately, SelfCheckGPT with QA is the average of inconsistency scores across $q$,
\begin{equation}
    \mathcal{S}_\text{QA}(i) = \mathbb{E}_q \left[ \mathcal{S}_\text{QA}(i,q) \right]
\end{equation}


\subsection{SelfCheckGPT with n-gram}
\label{section:self_check_ngram}
Given samples $\{S^1,S^2,...,S^N\}$ generated by a LLM, one could train a new language model using these samples to approximate the LLM. As $N$ gets larger, this new language model is closer to the LLM generating the response samples. 
Therefore, we can approximate the LLM's token probabilities using the newly trained language model. 

In practice, the number of samples $N$ is limited due to time and/or cost constraints. Consequently, we train a simple n-gram model using the samples $\{S^1,...,S^N\}$ and the main response $R$ (which will be assessed). We note that by including $R$  in training the n-gram model can be considered as a smoothing method where the count of each token in $R$ is increased by 1. Then, we compute the average of log-probabilities on the response $R$,
\begin{equation}
    \mathcal{S}_\text{n-gram}^\text{Avg}(i) = - \frac{1}{J} \sum_j \log \tilde{p}_{ij} 
\end{equation}
where $\tilde{p}_{ij}$ is the probability (of the $j$-th token of the $i$-th sentence) computed using the n-gram model. Alternatively, we can also use the maximum of negative log probabilities of the n-gram model,
\begin{equation}
    \mathcal{S}_\text{n-gram}^\text{Max}(i) = \max_j \left( -\log \tilde{p}_{ij} \right)
\end{equation}
\subsection{SelfCheckGPT Combination}
Lastly, given the differences in the natures of the variants of SelfCheckGPT, we expect them to be complementary. As a result, we consider SelfCheckGPT-Combination, which is a simple combination of the normalized scores of the three variants, including $\mathcal{S}_\text{BERT}$, $\mathcal{S}_\text{QA}$, and $\mathcal{S}_\text{n-gram}$.


\section{Data and Annotation}
We evaluate hallucination detection approaches by 1) generating synthetic Wikipedia articles using GPT-3 on the individuals from the Wikibio dataset \cite{wikibio_dataset}; 2) manually annotating the factuality of the passage at a sentence level; 3) evaluating the system's ability to detect hallucinations. 

%Test- 72k
 WikiBio is a dataset of the first paragraph (along with tabular information) of Wikipedia biographies. We rank the WikiBio test set in terms of paragraph length and randomly sample 238 articles from the top 20\% of longest articles (to ensure no obscure concept is selected). {GPT-3 (text-davinci-003)} is used to generate Wikipedia articles on a concept using the prompt "\texttt{This is a Wikipedia passage about \{concept\}}". Table \ref{tab:data_statistics} provides the statistics of GPT-3 generated passages. 
 
 \begin{table}[!ht]
  \centering
  \begin{tabular}{ccc}
    \toprule
    \#Passages &\#Sentences &\#Tokens/passage \\
    \midrule
    238  &1908   &184.7{\footnotesize$\pm$36.9}   \\
    \bottomrule
  \end{tabular}
  \caption{The statistics of \textbf{WikiBio GPT-3 dataset} where the number of tokens is based on the OpenAI GPT-2 tokenizer.}
  \label{tab:data_statistics}
\end{table}
We then annotate the sentences of the generated passages using the guidelines shown in Figure \ref{fig:labelling_diagram} such that each sentence classified as:
% A total of 1908 sentences from 238 passages were annotated, with 
\begin{itemize}
    \item \textbf{Major Inaccurate} (Non-Factual, \textbf{1}): The sentence is entirely hallucinated, i.e. the sentence is unrelated to the topic.
    \item \textbf{Minor Inaccurate} (Non-Factual, \textbf{0.5}): The sentence consists of some non-factual information, but the sentence is related to the topic.
    \item \textbf{Accurate} (Factual, \textbf{0}): The information presented in the sentence is accurate.
\end{itemize}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth,keepaspectratio]{fig/annotation.drawio.pdf}
    \caption{Flowchart of our annotation process}
    \label{fig:labelling_diagram}
\end{figure}

Of the 1908 annotated sentences, 761 (39.9\%) of the sentences were labelled major-inaccurate, 631 (33.1\%) were minor-inaccurate, and 516 (27.0\%) were accurate.\footnote{When selecting more obscure or more well-known concepts/individuals, the label distribution can be shifted to contain more or fewer hallucinations.} Passage-level scores are obtained by averaging the sentence-level labels in each passage. The distribution of passage-level scores is shown in Figure \ref{fig:histogram_annotation_doc}, where we observe a large peak at +1.0. We refer to the points at this peak as \textit{total hallucination}, i.e. the individual/concept was entirely made up and is unrelated to the real concept. 

\begin{figure}[!ht]
    \centering
\includegraphics[width=0.9\linewidth,keepaspectratio]{fig/histogram_annotation.pdf}
    \caption{Document factuality scores histogram plot}
    \label{fig:histogram_annotation_doc}
\end{figure}

% \noindent\textbf{Inter-annotator Agreement}
A subset of the dataset consisting of 201 sentences was annotated by two annotators. To obtain a single label for this subset, if both annotators agree, we use the agreed label. However, if they disagree, we use the worse-case label, e.g. \{minor inaccurate, major inaccurate\} is mapped to major inaccurate. We report inter-annotator agreement as measured by Cohen's $\kappa$ \cite{cohen1960ACO} in Table \ref{tab:cohens_kappa}. Cohen's $\kappa$ values of 0.595 and 0.748 indicate \textit{moderate} and \textit{substantial} agreement \cite{viera2005understanding} for the 3-label and 2-class scenarios, respectively.  

\begin{table}[!ht]
  \centering
  \begin{tabular}{rcc}
    \toprule
    Annotation        &3-label &2-label \\
    \midrule
    Cohen's $\kappa$  &0.595   &0.748   \\
    \bottomrule
  \end{tabular}
  \caption{Inter-annotator agreement where 3-label means selecting from accurate, minor inaccurate, major inaccurate. 2-label is calculated by combining minor/major into one label.}
  \label{tab:cohens_kappa}
\end{table}

\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_nonfact_all_sentences.pdf}
        \caption{Non-Factual Sentences}
        \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_nonfact_good_sentences.pdf}
        \caption{Non-Factual* Sentences}
        \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_fact_all_sentences.pdf}
        \caption{Factual Sentences}
    \end{subfigure}
    \caption{PR-Curve of detecting non-factual and factual \textit{sentences} in the GPT-3 generated WikiBio passages.}
    \label{fig:detection_pr_curve}
\end{figure*}
\section{Experiments}

The main generative LLM is \textbf{GPT-3} (text-davinci-003), which is the state-of-the-art system at the time of conducting our experiments. To obtain the main response, we set the generation temperature to 0.0 and use beam search decoding. For the stochastically generated samples, we set the temperature to 1.0 and generate $N$=20 samples. 

For the proxy LLM approach, the main text shows the results on LLaMA, which is one of the best-performing open-source LLMs. The results on other proxy LLMs can be found in the appendix. Also, the details about QG and QA systems are described in the appendix.



\subsection{Sentence-level Hallucination Detection}
\begin{table*}[!t]
  \centering
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Method}  &\multicolumn{3}{c}{Sentence-level (AUC-PR)} &\multicolumn{2}{c}{Passage-level (Corr.)}  \\
    &NonFact &NonFact* &Factual           &Pearson &Spearman     \\
    \midrule
    Random          &72.96 &29.72 &27.04    &-   &-        \\
    % \midrule
    \rowcolor{Gray}
    \multicolumn{6}{l}{{\texttt{GPT-3}'s probabilities} (\textit{LLM, grey-box})} \\
    Avg($-$log$p$)                   &83.21 &38.89 &53.97 &57.04 &53.93 \\
    Avg($\mathcal{H}$)$^\dagger$     &80.73 &37.09 &52.07 &55.52 &50.87 \\
    Max($-$log$p$)                   &87.51 &35.88 &50.46 &57.83 &55.69 \\
    Max($\mathcal{H}$)$^\dagger$     &85.75 &32.43 &50.27 &52.48 &49.55 \\
    \rowcolor{Gray}
    \multicolumn{6}{l}{{\texttt{LLaMA-30B}'s probabilities} (\textit{Proxy LLM, black-box})} \\
    Avg($-$log$p$)                  &75.43  &30.32  &41.29  &21.72 &20.20  \\
    Avg($\mathcal{H}$)              &80.80  &39.01  &42.97  &33.80 &39.49 \\
    Max($-$log$p$)                  &74.01  &27.14  &31.08  &-22.83 &-22.71 \\
    Max($\mathcal{H}$)              &80.92  &37.32  &37.90  &35.57 &38.94   \\
    \rowcolor{Gray}
    \multicolumn{6}{l}{\textbf{SelfCheckGPT} (\textit{black-box)}} \\
    w/ BERTScore                     &81.96 &45.96 &44.23 &58.18 &55.90  \\
    % Counting                       &83.97 &40.07 &47.78 &57.39 &55.15\\
    % + Bayes'                       &83.04 &38.58 &47.41 &56.43 &55.03 \\
    w/ QA                            &84.26 &40.06 &48.14 &61.07 &59.29 \\ % this is bayes + gamma
    w/ Unigram (max)                 &85.63 &41.04 &58.47 &64.71 &64.91 \\ % unigram3 (max -logprob)
    Combination                      &87.33 &44.37 &61.83 &69.05 &67.77 \\
    \bottomrule
  \end{tabular}
  \caption{AUC-PR for sentence-level detection tasks. Passage-level ranking performances are measured by Pearson correlation coefficient and Spearman's rank correlation coefficient w.r.t. human judgements. The results of other proxy LLMs, in addition to LLaMA, can be found in the appendix. $^{\dagger}$GPT-3 API returns the top-5 tokens' probabilities, which are used to compute entropy.}
  \label{tab:big_table}
\end{table*}

First, we investigate whether our hallucination detection methods are capable of identifying the factuality of sentences. In detecting non-fact sentences, both major-inaccurate labels and minor-inaccurate labels are grouped together into the \textit{non-factual} class, while the \textit{factual} class refers to accurate sentences. In addition, we consider a more challenging task of detecting major-inaccurate sentences in passages that are \textit{not} total hallucination passages, which we refer to as \textit{non-factual$^*$}.\footnote{In non-factual$^*$, 206 passages (1632 sentences) remain.} Figure \ref{fig:detection_pr_curve} and Table \ref{tab:big_table} show the performance of our approaches, where the following observations can be made:




% We look both at fact verification (i.e. identifying factual sentences) and hallucination detection (i.e. identifying non-factual sentences) performance. 




\textbf{1) LLM's probabilities $p$ correlate well with factuality}. Our results show that probability measures (from the LLM generating the texts) are strong baselines for assessing factuality. Factual sentences can be identified with an AUC-PR of 53.97, significantly better than the random baseline of 27.04, with the AUC-PR for hallucination detection also increasing from 72.96 to 83.21. This supports the hypothesis that when the LLMs are uncertain about generated information, generated tokens often have higher uncertainty, paving a promising direction for hallucination detection approaches. Also, the probability $p$ measure performs better than the entropy $\mathcal{H}$ measure of top-5 tokens. 

\textbf{2) Proxy LLM perform noticeably worse than LLM (GPT-3)}. Nevertheless, as opposed to the LLM, the results of proxy LLM show that the entropy $\mathcal{H}$ measures outperform the probability measures. This suggests that using richer uncertainty information could improve factuality/hallucination detection tasks, while the entropy of top-5 tokens is likely insufficient. In addition, when using other proxy LLMs such as GPT-NeoX or OPT-30B, the performance is worse than or marginally better than the random baseline. We believe this poor performance occurs as different LLMs have different generating patterns, and therefore even common uninformative tokens may have a low probability if they do not follow the style of the proxy LLM. We note that a weighted conditional language model score such as BARTScore \cite{yuan2021bartscore} could be incorporated in future investigations of the proxy LLM approach. 

% Table \ref{tab:big_table} shows, however, that this approach is \textit{ineffective}.

% LLM's probability measures require access to LLM output probabilities, which may not always be available. We propose using a proxy system (e.g. GPT-NeoX-20B) whose model weights' are publicly available, to approximate the output probabilities of GPT-3

% Further, ablations in the appendix (Table \ref{tab:big_table_appendix}) show the impact of the initialisation/size of the proxy LLM, where it is shown that larger models typically yield better factual/non-factual detection performance, but none of which rival the previous grey box approach. 

\textbf{3) SelfCheckGPT rivals grey-box approaches}. SelfCheckGPT considerably outperforms the proxy LLM approach in all detection setups. Furthermore, SelfCheckGPT outperforms the grey-box probability-based approach in most setups. We also observe a performance gain when combining the variants of SelfCheckGPT. 

Interestingly, despite being the simplest method, SelfCheckGPT with unigram (max) works well across different setups. Essentially, when assessing a sentence, this method picks up the token with the \textit{lowest} occurrence given all the samples. For instance, if this token only appears a few times (or once) in the samples ($N$=20), it is likely non-factual. Next, we investigate its performance as we vary from 1-gram to 5-gram. The results in Table \ref{tab:ngram_results} show that simply finding the least likely token/n-gram is more effective than computing the average n-gram language model score of the sentence (i.e. Avg($-\log p$). As $n$ increases the performance of SelfCheckGPT with n-gram (max) drops, because the space of n-grams increases exponentially with $n$; hence, requiring exponentially more samples.





\subsection{Passage-level Factuality Ranking}

The previous results show that SelfCheckGPT is an effective approach for predicting sentence-level factuality. An additional consideration, though, is whether SelfCheckGPT can be used to determine the overall factuality of passages. Passage-level factuality scores are obtained by averaging the sentence-level scores over all sentences. 
\begin{equation}
    f_{\text{passage}}(i) = \frac{1}{|R|} \sum_i f(i)
    \label{eq:doc_level_score}
\end{equation}
where $f(i)$ is the sentence-level score, and $|R|$ is the number of sentences in the passage. Note that for Avg($-\log p$) and  Avg($\mathcal{H}$), we compute the average over all tokens in a passage. Whereas for Max($-\log p$) and Max($\mathcal{H}$), we first take the maximum operation over tokens at the sentence level, and we then average over all sentences following Equation \ref{eq:doc_level_score}. Since human judgement is somewhat subjective, averaging the sentence-level labels would lead to ground truths with less noise.

Our results in Table \ref{tab:big_table} and Figure \ref{fig:scatter_plot_passage} show that all of the SelfCheckGPT methods correlate far better with human judgements than all other methods, including the grey-box probability and entropy methods. Further, the three variants of SelfCheckGPT appear complementary, with the combined approach being the best-performing system, achieving the highest Pearson correlation of 69.05. Unsurprisingly, the proxy LLM approach again achieves considerably lower correlations. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_baseline1.pdf}
    \caption{\texttt{GPT-3} Avg($-\log p$)}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_llama_30b_2.pdf}
    \caption{\texttt{LLaMA-30B} Avg($\mathcal{H}$)}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\linewidth}
    \centering    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_unigram3.pdf}
    \caption{SelfCheckGPT-Unigram(max)}
    \end{subfigure}
\caption{Scatter plot of passage-level scores where Y-axis = Method scores, X-axis = Human scores. The scatter plots of other SelfCheckGPT variants are provided in Figure \ref{fig:scatter_plot_passage_appendix} in the appendix.}
\label{fig:scatter_plot_passage}
\end{figure*}

\subsection{Ablation Studies}

\subsubsection*{External Knowledge (instead of SelfCheck)}
% \textbf{4) External Knowledge improves AUC}. 
If external knowledge is available, one could measure the informational consistency with the LLM response and the real-world document (instead of LLM self-samples). In this experiment, we have the related WikiBio passage and so can extract the first Wikipedia paragraph for each concept/individual.\footnote{This method is no longer zero-resource as it requires retrieving relevant knowledge, and so this approach may only be applicable to fact verification.} Our results in Table \ref{tab:ablation_external_knowledge} show that for the QA and BERTScore methods, using SelfCheckGPT samples can yield a comparable or even better performance compared to using the WikiBio reference passage. This illustrates that SelfCheckGPT is a strong hallucination detection approach that is comparable to methods using stored external information. Lastly, the n-gram model shows a significant drop in performance when using the WikiBio passages instead of LLM self-samples. This failure is attributed to the fact that just the WikiBio reference text is not sufficient to train an n-gram model, and we investigate the number of samples in more detail in the next ablation.


\begin{table}[!ht]
  \centering
  \tabcolsep=1.3mm
    \small
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Method}  &\multicolumn{3}{c}{Sent-lvl AUC-PR} &\multicolumn{2}{c}{Passage-lvl}  \\
    &NoFac &NoFac* &Fact           &Pear. &Spear.     \\
    \midrule
    SelfCk-BERT      &81.96 &45.96 &44.23 &58.18 &55.90  \\
    WikiBio+BERT     &81.32 &40.62 &49.15 &58.71 &55.80\\
    \midrule
    SelfCk-QA        &84.26 &40.06 &48.14 &61.07 &59.29 \\
    WikiBio+QA       &84.18 &45.40 &52.03 &57.26 &53.62\\
    \midrule
    SelfCk-1gm       &85.63 &41.04 &58.47 &64.71 &64.91 \\ % unigram3 (max -logprob)
    WikiBio+1gm      &80.43 &31.47 &40.53 &28.67 &26.70 \\
    \bottomrule
  \end{tabular}
  \caption{The performance when using SelfCheckGPT samples versus external stored knowledge.}
  \label{tab:ablation_external_knowledge}
\end{table}



\subsubsection*{Impact of the Number of Samples}
Typically, sampling-based methods are expected to achieve better performance when more samples are drawn. However, drawing a higher number of samples leads to higher computational costs and/or API costs. Thus, we investigate the behaviour of SelfCheckGPT as we vary the number of samples drawn from 1 to 20. Our results in Figure \ref{fig:number_of_samples_doc_level} (and Figure \ref{fig:number_of_samples_sentence_level} in the appendix) show that the performance of SelfCheckGPT increases as more samples are used, with the performance gain diminishing as we generate more samples. SelfCheckGPT with n-gram requires the highest number of samples before its performance reaches a plateau. 


\begin{figure}[!ht]
    \centering
\includegraphics[width=0.73\linewidth,keepaspectratio]{fig/ablation_number_samples_doclevel.pdf}
    \caption{The performance of SelfCheckGPT methods on ranking passages (Spearman's) versus the number of samples.}
    \label{fig:number_of_samples_doc_level}
\end{figure}

\subsubsection*{Model Choice for Proxy LLM}
 Figure \ref{fig:ablation_model_size} (in Appendix \ref{sec:appendix_results}) illustrates that  LLaMA is far better than other LLMs, and the performance of the proxy LLM method increases with model size. Similarly, average probability, Avg($p$), is closer to that of GPT-3 when using a larger proxy LLM as shown in Table \ref{tab:llm_token_prob} in the appendix.
 
 % and there is a significant gap between Avg($p$) of GPT-3 and Avg($p$) of proxy LLM 

% However, it is still worse than LLM's probabilities or SelfCheckGPT.

\section{Conclusions}
This work proposes SelfCheckGPT, a zero-resource black-box approach, that can be used to detect LLM hallucinations and determine the trustworthiness of generated responses without the need for any external resources. SelfCheckGPT has been shown to be an effective approach for LLM hallucination assessment at both sentence and passage levels, and the approach is applicable to any LLM and any topic that the LLM is prompted to generate. Through experimental analysis of annotated GPT-3 responses, we show that SelfCheckGPT has a competitive performance to the grey-box probability-based approach, and it also significantly outperforms the proxy LLM approach. In addition, this work releases a dataset for GPT-3 hallucination detection, consisting of 238 annotated passages.

\section{Limitations}
In this study, the scope of GPT-3 generated texts is 238 passages about individuals in the WikiBio dataset, as a result, a wider range of concepts, e.g. locations and objects, could be investigated to better understand the nature of LLM's hallucination. 


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\newpage
\appendix

\section{Models and Implementation}

\subsection*{Entropy}
The entropy of the output distribution is implemented as
$
    2^{ - \sum_{\tilde{w} \in \mathcal{W}} p_{ij}(\tilde{w}) \log_2 p_{ij}(\tilde{w})}
$
where $\mathcal{W}$ is the set of all possible words in the vocabulary.

\subsection*{Proxy LLMs}
The proxy LLMs considered are LLaMA-\{7B, 13B, 30B\} \cite{touvron2023llama}, OPT-\{125m, 1.3B, 13B, 30B\} \cite{zhang2022opt}, GPT-J-6B \cite{gpt_j} and GPT-NeoX-20B \cite{black-etal-2022-gpt}.



\subsection*{SelfCheckGPT's Systems}
Our BERTScore has the \texttt{roberta-large} model as the backbone. For the QA method, the generation systems \texttt{G1} and \texttt{G2} (described in Section \ref{section:self_check_qa}) are T5-large finetuned to SQuAD and RACE, respectively. The answering system \texttt{A} is Longformer \cite{longformer} finetuned to the RACE dataset. The answerability system \texttt{U} is also Longformer, but fine-tuned to SQuAD2.0. These models have been made available on HuggingFace.

\section{SelfCheckGPT-QA with Bayes}
\subsection*{Theory}
\label{appendix:bayes_qa}
Let $P(\text{F})$ denote the probability of the $i$-th sentence being non-factual, and $P(\text{T})$ denote the probability of the $i$-th sentence being factual. For a question $q$, the probability of $i$-th sentence being non-factual given a set of matched answers ${L}_{\tt m}$ and a set of not-matched answers ${L}_{\tt n}$ is:
\begin{align}
    &P(\text{F} | {L}_{\tt m}, {L}_{\tt n}) \nonumber \\
    &= \frac{ P({L}_{\tt m}, {L}_{\tt n} | \text{F}) P(\text{F})}{P({L}_{\tt m}, {L}_{\tt n} | \text{F}) P(\text{F}) + P({L}_{\tt m}, {L}_{\tt n} | \text{T}) P(\text{T})} \nonumber \\
    &= \frac{ P({L}_{\tt m}, {L}_{\tt n} | \text{F})}{P({L}_{\tt m}, {L}_{\tt n} | \text{F})  + P({L}_{\tt m}, {L}_{\tt n} | \text{T}) } \label{eq:bayes}
\end{align}
where we assume the sentence is equally likely to be False or True, i.e. $P(\text{F}) = P(\text{T})$. The probability of observing ${L}_{\tt m}, {L}_{\tt n}$ when the sentence is False (non-factual):
\begin{align}
&P({L}_{\tt m}, {L}_{\tt n} | \text{F}) \nonumber \\
&= \underset{a \in {L}_{\tt m}}{\prod} P(a = a_R | F) \underset{a' \in {L}_{\tt n}}{\prod} P(a' \neq a_R | F) \nonumber \\
&= (1-\beta_1)^{{N}_{\tt m}} (\beta_1)^{{N}_{\tt n}}
\end{align}
and probability of observing ${L}_{\tt m}, {L}_{\tt n}$ when the sentence is True (factual):
\begin{align}
&P({L}_{\tt m}, {L}_{\tt n} | \text{T}) \nonumber \\
&= \underset{a \in {L}_{\tt m}}{\prod} P(a = a_r | T) \underset{a' \in {L}_{\tt n}}{\prod} P(a' \neq a_r | T) \nonumber \\
&= (\beta_2)^{{N}_{\tt m}} (1-\beta_2)^{{N}_{\tt n}}
\end{align}
where ${N}_{\tt m}$ and ${N}_{\tt n}$ are the number of matched answers and the number of not-matched answers, respectively. Hence, we can simplify Equation \ref{eq:bayes}:
\begin{equation}
    P(\text{F} | {L}_{\tt m}, {L}_{\tt n}) = \frac{\gamma_2^{{N}_{\tt n}}}{\gamma_1^{{N}_{\tt m}} + \gamma_2^{{N}_{\tt n}} }
\end{equation}
where $\gamma_1 = \frac{\beta_2}{1-\beta_1}$ and $\gamma_2 = \frac{\beta_1}{1-\beta_2}$. Lastly, instead of rejecting samples having an answerability score below a threshold,\footnote{$\alpha$ is between 0.0 (unanswerable) and 1.0 (answerable). Standard-counting ${N}_{\tt m}$ and ${N}_{\tt n}$ can be considered as a special case of soft-counting where $\alpha$ is set to 1.0 if $\alpha$ is greater than the answerability threshold and otherwise $\alpha$ is 0.0.} we find empirically that soft-counting (defined below) improves the detection performance. We set both $\beta_1$ and $\beta_2$ to 0.8.

\begin{equation}
    {N}'_{\tt m} = \underset{n \hspace{0.3em}\text{s.t.}\hspace{0.3em} a_n \in {L}_{\tt m}}{\sum}\alpha_n;  \hspace{2mm}
    {N}'_{\tt n} = \underset{n \hspace{0.3em}\text{s.t.}\hspace{0.3em} a_n \in {L}_{\tt n}}{\sum}\alpha_n
    \label{eq:soft_counting}
\end{equation}
where $\alpha_n = P_{{\tt U}}(\text{answerable}|q, S^n)$. Therefore, the SelfCheckGPT with QA score, $\mathcal{S}_\text{QA}$, is:
\begin{equation}
    \mathcal{S}_\text{QA} = P(\text{F} | {L}_{\tt m}, {L}_{\tt n}) = \frac{\gamma_2^{N'_{\tt n}}}{\gamma_1^{N'_{\tt m}} + \gamma_2^{N'_{\tt n}} }
\end{equation}

\subsection*{Results}
In Table \ref{tab:qa_bayes_results}, we show empirically that using applying Bayes' theorem and soft counting $\alpha$ (in Equation \ref{eq:soft_counting}) improves the performance of the SelfCheckGPT with QA approach.
\begin{table}[!ht]
  \centering
  \small
  \tabcolsep=1.7mm
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Varaint}  &\multicolumn{3}{c}{Sentence-lvl} &\multicolumn{2}{c}{Passage-lvl} \\
    &NoF &NoF* &Fact &PCC &SCC \\
    \midrule
    SimpleCount         &83.97 &40.07 &47.78 &57.39 &55.15\\
    + Bayes             &83.04 &38.58 &47.41 &56.43 &55.03 \\
    + Bayes + $\alpha$  &84.26 &40.06 &48.14 &61.07 &59.29 \\
    \bottomrule
  \end{tabular}
  \caption{Performance of SelfCheckGPT-QA's variants.}
  \label{tab:qa_bayes_results}
\end{table}

\section{Additional Results}
Here, we provide experimental results that are complementary to those presented in the main paper. 
\label{sec:appendix_results}



\begin{table}[!ht]
  \centering
    \small
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{n-gram}  &\multicolumn{3}{c}{Sent-lvl AUC-PR} &\multicolumn{2}{c}{Passage-lvl}  \\
    &NoFac &NoFac* &Fact           &Pear. &Spear.     \\
    \midrule
    \rowcolor{Gray}
    \multicolumn{6}{l}{Avg($-$log$p$)} \\
    1-gram &81.52  &40.33  &41.76   &40.68 &39.22  \\
    2-gram &82.94  &44.38  &52.81   &58.84 &58.11  \\
    3-gram &83.56  &44.64  &53.99   &62.21 &63.00  \\
    4-gram &83.80  &43.55  &54.25   &61.98 &63.64  \\
    5-gram &83.45  &42.31  &53.98   &60.68 &62.96  \\
    \rowcolor{Gray}
    \multicolumn{6}{l}{Max($-$log$p$)} \\
    1-gram &85.63 &41.04 &58.47 &64.71 &64.91 \\
    2-gram &85.26 &39.29 &58.29 &62.48 &66.04 \\
    3-gram &84.97 &37.10 &57.08 &57.34 &60.49 \\
    4-gram &84.49 &36.37 &55.96 &55.77 &57.25 \\
    5-gram &84.12 &36.19 &54.89 &54.84 &55.97 \\    
    \bottomrule
  \end{tabular}
  \caption{The performance using different n-gram models.}
  \label{tab:ngram_results}
\end{table}





\begin{figure}[!ht]
    \centering
\includegraphics[width=0.73\linewidth,keepaspectratio]{fig/ablation_number_samples_detect_nonfact.pdf}
    \caption{The performance of SelfCheckGPT methods on sentence-level non-factual* detection (AUC-PR) versus the number of samples. This Figure extends the passage-level results in Figure \ref{fig:number_of_samples_doc_level}.}
    \label{fig:number_of_samples_sentence_level}
\end{figure}

\begin{figure}[!ht]
    \begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.84\linewidth,keepaspectratio]{fig/ablation_model_size_sentence.pdf}
      \caption{Sentence-level Detection}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.84\linewidth,keepaspectratio]{fig/ablation_model_size_passage.pdf}
      \caption{Passage-level Ranking}
    \end{subfigure}
    \caption{Performance of the Avg($\mathcal{H}$) method using a proxy LLM where the model sizes are: LLaMA=\{7B, 13B, 30B\}, OPT=\{125m, 1.3B, 13B, 30B\}, GPT-J=6B, and NeoX=20B. The full results are provided in Table \ref{tab:big_table_appendix}.}
    \label{fig:ablation_model_size}
\end{figure}

\begin{table}[!ht]
  \centering
  \begin{tabular}{rlc}
    \toprule
    LLM            &Size  &Avg($p$)  \\
    \midrule
    \texttt{GPT-3} &175B      &72.02 \\
    \midrule
    \texttt{LLaMA}          &30B &65.25 \\
    \texttt{LLaMA}          &13B &64.71 \\
    \texttt{LLaMA}          &7B  &63.70 \\
    \midrule
    \texttt{OPT}   &30B       &53.81 \\
    \texttt{NeoX}  &20B       &54.49 \\
    \texttt{OPT}   &13B       &52.80 \\
    \texttt{GPT-J} &6B        &52.50 \\
    \texttt{OPT}   &1.3B      &49.20 \\
    \texttt{OPT}   &125m      &40.91 \\
    \bottomrule
  \end{tabular}
  \caption{Average token probability, Avg($p$), over all tokens in GPT-3 generated passages.}
  \label{tab:llm_token_prob}
\end{table}


\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_bertscore.pdf}
    \caption{SelfCheckGPT-BERTScore}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_method3.pdf}
    \caption{SelfCheckGPT-QA}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\linewidth}
    \centering    
    \includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_combine.pdf}
    \caption{SelfCheckGPT-Combination}
    \end{subfigure}
\caption{Scatter plot of passage-level scores where Y-axis = Method scores, X-axis = Human scores. This figure provides results in addition to Figure \ref{fig:scatter_plot_passage}.}
\label{fig:scatter_plot_passage_appendix}
\end{figure*}

\begin{table*}[!ht]
  \centering
  \small
  \begin{tabular}{rlccccc}
    \toprule
    \multirow{2}{*}{LLM} &\multirow{2}{*}{Size}   &\multicolumn{3}{c}{Sentence-level (AUC-PR)}  &\multicolumn{2}{c}{Passage-level (Corr.)}  \\
    & &NonFact &NonFact* &Factual &Pearson  &Spearman \\
    \midrule
    Random         &- &72.96 &29.72 &27.04 &- &-   \\
    \rowcolor{Gray}
    \multicolumn{7}{l}{Avg($-$log$p$) Method} \\
    \texttt{LLaMA} &30B  &75.43  &30.32  &41.29  &21.72 &20.20  \\
    \texttt{LLaMA} &13B  &74.16  &30.01  &37.36  &13.33 &12.89  \\
    \texttt{LLaMA} &7B   &71.69  &27.87  &31.30  &-2.71 &-2.59   \\
    
    \texttt{OPT}   &30B  &67.70  &24.43  &25.04  &-32.07 &-31.45  \\
    \texttt{NeoX}  &20B  &69.00  &24.38  &26.18  &-31.79 &-34.15  \\
    \texttt{OPT}   &13B  &67.46  &24.39  &25.20  &-33.05 &-32.79  \\
    \texttt{GPT-J} &6B   &67.51  &24.28  &24.26  &-38.80 &-40.05  \\
    \texttt{OPT}   &1.3B &66.19  &24.47  &23.47  &-35.20 &-38.95  \\
    \texttt{OPT}   &125m &66.63  &25.31  &23.07  &-30.38 &-37.54  \\
    \rowcolor{Gray}
    \multicolumn{7}{l}{Avg($\mathcal{H}$) Method} \\
    \texttt{LLaMA} &30B  &80.80  &39.01  &42.97  &33.80 &39.49 \\
    \texttt{LLaMA} &13B  &80.63  &38.98  &40.59  &29.43 &33.12 \\
    \texttt{LLaMA} &7B   &78.67  &37.22  &33.81  &19.44 &21.79  \\
    
    \texttt{OPT}   &30B  &77.13  &33.67  &29.55 &-0.43 &3.43  \\
    \texttt{NeoX}  &20B  &77.40  &32.78  &30.13 &5.41 &7.43   \\
    \texttt{OPT}   &13B  &76.93  &33.71  &29.68 &0.25 &1.39  \\
    \texttt{GPT-J} &6B   &76.15  &33.29  &28.30 &-2.50 &-1.37   \\
    \texttt{OPT}   &1.3B &74.05  &31.91  &26.33 &-10.59 &-10.00 \\
    \texttt{OPT}   &125m &71.51  &30.88  &25.36 &-14.16 &-13.76 \\
    \rowcolor{Gray}
    \multicolumn{7}{l}{Max($-$log$p$) Method} \\
    \texttt{LLaMA} &30B  &74.01  &27.14  &31.08  &-22.83 &-22.71 \\
    \texttt{LLaMA} &13B  &71.12  &26.78  &28.82  &-34.93 &-31.70 \\
    \texttt{LLaMA} &7B   &69.57  &25.91  &26.54  &-42.57 &-38.24 \\
    
    \texttt{OPT}   &30B  &67.32  &24.40  &24.32 &-49.51 &-45.50 \\
    \texttt{NeoX}  &20B  &67.51  &23.88  &24.82 &-47.96 &-44.54 \\
    \texttt{OPT}   &13B  &67.36  &24.67  &24.46 &-50.15 &-44.42 \\
    \texttt{GPT-J} &6B   &67.58  &23.94  &23.93 &-51.23 &-47.68 \\
    \texttt{OPT}   &1.3B &68.16  &25.85  &24.66 &-45.60 &-42.39 \\
    \texttt{OPT}   &125m &69.23  &27.66  &24.14 &-39.22 &-37.18 \\
    \rowcolor{Gray}
    \multicolumn{7}{l}{Max($\mathcal{H}$) Method} \\
    \texttt{LLaMA} &30B  &80.92  &37.32  &37.90  &35.57 &38.94   \\
    \texttt{LLaMA} &13B  &80.98  &37.94  &36.01  &32.07 &34.01 \\
    \texttt{LLaMA} &7B   &79.65  &35.57  &31.32  &22.10 &22.53  \\
    
    \texttt{OPT}   &30B  &76.58  &33.44  &29.31 &1.63 &6.41 \\
    \texttt{NeoX}  &20B  &76.98  &31.96  &29.13 &5.97 &9.31 \\
    \texttt{OPT}   &13B  &76.26  &32.81  &29.25 &1.42 &2.82 \\
    \texttt{GPT-J} &6B   &75.30  &32.51  &28.13 &-2.14 &1.41 \\
    \texttt{OPT}   &1.3B &73.79  &31.42  &26.38 &-9.84 &-9.80 \\
    \texttt{OPT}   &125m &71.32  &31.65  &25.36 &-18.05 &-17.37 \\
    \bottomrule
  \end{tabular}
  \caption{AUC-PR for Detecting Non-Factual and Factual Sentences in the GPT-3 generated WikiBio passages. Passage-level PCC and SCC with LLMs used
to assess GPT-3 responses. This table is an extension to Table \ref{tab:big_table}.}
  \label{tab:big_table_appendix}
\end{table*}


\end{document}
