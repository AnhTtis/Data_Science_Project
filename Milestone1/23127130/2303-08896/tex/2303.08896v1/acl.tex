% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}


%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}
% \usepackage[colorinlistoftodos]{todonotes}
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage[bottom]{footmisc}
\usepackage{booktabs}
\usepackage{multirow}

% \newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
% \setlength{\columnseprule}{0.4pt}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection\\ for Generative Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Potsawee Manakul, Adian Liusie, Mark J. F. Gales \\
  Department of Engineering, University of Cambridge \\
  \texttt{pm574@cam.ac.uk, al826@cam.ac.uk, mjfg@eng.cam.ac.uk}}


\begin{document}
\pagenumbering{arabic}
\pagestyle{plain}
\maketitle
\begin{abstract}

Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to token-level output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using  GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several existing baselines and show that in sentence hallucination detection, our approach has AUC-PR scores comparable to grey-box methods, while SelfCheckGPT is best at passage factuality assessment.\footnote{Code, data, and model weights are available at \url{https://github.com/potsawee/selfcheckgpt}.} 


%%% General Problem/Motivation %%%
% Generative Large language models (LLMs) such
% as GPT-3 are capable of generating highly fluent
% responses to a variety of user prompts. Yet, LLMs may produce non-factual information, especially in situations where they do not have much information about the topic.  

% %%% Specific setup we are interested %%%

% Without external knowledge/database, uncertainty metrics such as token probabilities and perplexities could be used to assess or generated texts. However, these uncertainty metrics require access the generative model for the additional information beyond text-based responses, which may not always be available, especially in widely-used ChatGPT's web interface and its API.

% %%% Proposed Method %%%
% In this work, we propose "SelfCheckGPT" -- a sampling-based black-box method that requires only access to text responses -- for self fact-checking LLM generated texts. The method is based on the idea that if LLM has knowledge about the topic, the sampled responses should be similar, whereas if the information in sampled responses is inconsistent, it could indicate that LLM has less knowledge.

% %%% Results %%%
% We use GPT-3 to generate passages about persons in the WikiBio dataset, and we annotate the passages on the factuality aspect at the sentence level. Two factuality tasks are carried out: (i) detecting non-factual and factual sentences; (ii) ranking generated passages based on the factuality score. On these tasks, our results show that our SelfCheckGPT method considerably outperforms black-box baselines, and it is also comparable to grey-box methods.


\end{abstract}

\section{Introduction}
Large Language Models (LLMs) such as GPT-3 \citep{brown2020language}, PaLM \cite{chowdhery2022palm}, and Chinchilla \cite{chinchilla} are capable of generating highly fluent and realistic responses to a variety of user prompts. They have been used in many applications such as automatic tools to draft reports, virtual assistants that retrieve information, zero-shot summarization systems, as well as a multitude of other generative applications. Despite the convincing and realistic nature of LLM-generated texts, a concern with LLMs is their observed tendency to hallucinate facts and make up information. %Hallucinated facts will lead to unfactual outputs, may mislead users leading to the spread of misinformation.

\begin{figure}
\includegraphics[width=\linewidth,keepaspectratio]{fig/diagram.drawio.pdf}
    \caption{SelfCheckGPT with Question Answering.}
\end{figure}


A method for hallucination detection is to leverage existing intrinsic uncertainty metrics such as token probability and perplexity, since these metrics can be used to determine the parts of the output sequence the system is least certain of \cite{yuan2021bartscore, gptscore}. However, all current uncertainty metrics require access to the full output token-level probability distribution- information that may not necessarily be available to users, e.g. when systems are accessed used through limited external APIs such as ChatGPT. Further, there is an active field of fact-verification where evidence is retrieved from an external database to assess the veracity of a claim \citep{Thorne18Fact, guo-etal-2022-survey}. However, facts can only be assessed relative to knowledge present in the database. Though corpora such as Wikipedia can cover a great deal of general knowledge and serve as a useful database for fact verification, hallucination is observed over a wide range of tasks beyond pure fact verification. For example, summaries from automatic systems can contain information not present in the context \citep{kryscinski-etal-2019-neural, maynez-etal-2020-faithfulness, kryscinski-etal-2020-evaluating}. 

In this paper, we propose SelfCheckGPT, a simple sampling-based approach that can be used to detect whether responses generated by LLMs are hallucinated or factual. SelfCheckGPT only uses sampled responses and can therefore be used on black box models, while it also operates in a zero-resource fashion, i.e. with no external database. The motivating idea of SelfCheckGPT is that when a LLM knows a given concept well, the sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and may completely contradict one another. By sampling multiple responses from a LLM, one can measure information consistency between the different responses and determine which statements are factual and which have been hallucinated. Two variants of SelfCheckGPT for measuring informational inconsistency are considered: multiple choice question answering agreement and BERTScore. Through analysis of annotated articles generated by GPT3, we show that SelfCheckGPT can determine factual documents effectively in a black-box, zero-resource manner.

\section{Related Work}
\subsection{LLMs}
There has been rapid growth in current large language models (LLMs) literature with larger and better models being constantly released \cite{chowdhery2022palm}. These models are commonly used as the backbone for a range of natural language processing tasks \cite{wang-etal-2018-glue}. Traditionally, these LLMs are fine-tuned to a specific task and/or domain \cite{devlin-etal-2019-bert, radford2019language, raffel2020exploring}, however a fascinating finding is that as models scale up, they inherit abilities to naturally solve a wide range of natural language tasks in a zero-shot fashion \cite{brown2020language, weiemergent}. For the simple downstream application of generating synthetic Wikipedia articles, this paper only leverages vanilla LLMs. 

\subsection{Hallucination of Large Language Models}
Hallucination has been studied in text generation tasks, including summarization \cite{Huang_hallucination_survey} and dialogue generation \cite{shuster-etal-2021-retrieval-augmentation}. Recently, a survey of hallucination in a variety of natural language generation tasks has been conducted \cite{hallucination_survey_2023}. However, initial research mainly focused for domain-specific, small LMs, and the extent to which this generalises to large generative LLMs remains unclear. 

\subsection{Fact Verification}
Existing fact-verification approaches follow a multi-stage pipeline of claim detection, evidence retrieval and verdict prediction \citep{guo-etal-2022-survey, zhong-etal-2020-reasoning}. Such methods, however, require access to external databases and can have considerable inference costs. 

\subsection{Sequence Level Uncertainty Estimation}
Token probabilities have been used as indication of model certainty. For example, OpenAI's GPT-3 web interface allows users to display token probabilities as shown in Figure \ref{fig:openai_playground}. Additionally, uncertainty estimation based on aleatoric and epistemic uncertainty for autoregressive generation has been studied \cite{xiao-wang-2021-hallucination, malinin2021uncertainty}, though these methods require access to full output distributions. Further, conditional language model scores $p_{\theta}(y|x)$ have been used to evaluate properties of texts \cite{yuan2021bartscore, gptscore}. 

\begin{figure}[!h]
    \includegraphics[width=\linewidth,keepaspectratio]{fig/playground_example0.png}
    \caption{Example of OpenAI's GPT-3 interface with token probabilities displayed.}
    \label{fig:openai_playground}
\end{figure}

\section{Grey-Box Factuality Assessment}
This section will introduce methods that can be used to determine the factuality of LLM responses in a zero-resource setting when one has full access to output distributions. We will use `factual' to define when statements are grounded in valid information, i.e. when hallucinations are avoided, and `zero-resource' when no external database is used. We first motivate the intuition for why standard uncertainty based metrics may be good proxies for factuality, then describe how systems' output token-level probability distributions can be used to determine sentence-level hallucinations.

\subsection{Motivation of Approach}
To consider how the factuality of a generated response can be determined in a zero-resource setting, we consider LLM pre-training. During pre-training, the model is trained with next word prediction over massive corpora of textual data. This gives the model a strong understanding of language \cite{jawahar-etal-2019-bert, raffel2020exploring}, powerful contextual reasoning \cite{zhang2020semantics}, as well as world knowledge \cite{liusie2022world}. Consider the input "Lionel Messi is a \_". Since Messi is a world-famous athlete who may have appeared multiple times in pre-training, the LLM is likely to know who Messi is. Therefore, given the context, the token "footballer" may be assigned a very high probability while some other professions such as "carpenter" will be considered very improbable. However, for the input "John Smith is a \_", the system may be unsure of how the sentence should continue, and have a flat probability distribution. During decoding, this will lead to a random word being generated- causing the system to hallucinate. 

\subsection{Uncertainty-based Assessment}
This insight allows us to realize the connection between standard uncertainty metrics and the factuality of sentences. Factual sentences are likely to contain tokens with higher likelihood and lower entropy, while hallucinations are likely to come from positions with flat probability distributions with high uncertainty. \\

\noindent\textbf{Perplexity} Let $r$ be the LLM response to a given user query, $r_i$ the $i$-th sentence in $r$, and $r_{ij}$ the $j$-th token in $r_i$. The objective is to find a sentence-level hallucination function $f(r_i) \in [0.0, 1.0]$ such that $f(r_i) \rightarrow 1.0$ if $r_i$ is hallucinated (non-factual), and 0.0 if $r_i$ is grounded in valid information. Standard uncertainty metrics such as perplexity can be considered for the function $f$; if $\operatorname{PPL}(r_{ij})$ is the perplexity at token $r_{ij}$ given all previous tokens and $|r_i|$ the number of tokens in sentence $r_i$, then one can calculate the average perplexity of the sentence as follows:

\begin{equation}
    f(r_i) = \frac{1}{|r_i|} \sum_j \operatorname{PPL}(r_{ij})
\end{equation}

\noindent\textbf{Minimum probability}\footnote{Minimum token probability is equivalent to maximum negative log probability.} Let $P(r_{ij})$ be the LM probability of generating token $r_{ij}$ given all previous tokens. One can select the token in the sentence with minimum probability, and calculate the negative log probability.  

\begin{equation}
    f(r_i) = - \underset{j}{\text{min}} \log P(r_{ij})
\end{equation}


\section{Black-Box Factuality Assessment}
A drawback of the previous grey-box methods is that they require output token-level probabilities. Though this may seem a reasonable requirement, for massive LLMs only available through limited API calls, such token-level information might not be available (such as with ChatGPT). We therefore consider how the previous methods can be extended to work even when only responses can be sampled from the LLM. 


\subsection{Proxy LLMs}
A simple baseline to consider is using a proxy LLM. With no access to the full outputs of the LLM $\theta$ generating the text, a proxy LLM $\tilde{\theta}$ could be used to approximate the output values.  



% If we do not have access to the outputs using the system generating the text $\theta$, would we be able to approximate these values using a different system $\tilde{\theta}$ which we do have full access of.

\subsection{SelfCheckGPT via BERTScore}
Let $r$ refer to the LLM response drawn from a given user query. SelfCheckGPT operates by drawing a further $N$ stochastic LLM response samples $\{s^1,s^2,..,s^n,...,s^N\}$ from the same query, followed by measuring the consistency between the response and stochastic samples. Let $\mathcal{B}(.,.)$ denote the BERTScore between two sentences (where we use \texttt{roberta-large} as the backbone of BERTScore). SelfCheckGPT via BERTScore finds the averages BERT score of a sentence with the most similar sentence of each drawn sample:
\begin{align}
    \tilde{f}(r_i) &= \frac{1}{N} \sum_{n=1}^N \underset{j}{\text{max}} \left( \mathcal{B} (r_i, s^n_j) \right) \\
    {f}(r_i) &= 1 - \tilde{f}(r_i)  
\end{align}
where $r_i$ represent the $i$-th sentence in $r$ and $s^n_i$ represent the $i$-th sentence in $s^n$. This way if the information in a sentence appears in many drawn samples, one may assume that the information is factual, whereas if the statement appears in no other sample, it is likely a hallucination.  


%The task is design a non-factuality function $f(r_i) \in [0.0, 1.0]$ such that $f(r_i) \rightarrow 1.0$ if $r_i$ is non-factual.

%In this experiment, we set the temperature to $0.0$ to obtain $r$, and this response is the most likely output from beamsearch decoding. To obtain $\{s^1,...,s^N\}$, we set temperature to $1.0$. 


% Passage-level score can be obtained by:
% \begin{equation}
%     f_{\text{passage}}(r) = \sum_i f(r_i) \label{eq:doc_level_score}
% \end{equation}
% Next, we discuss how $f(r_i)$ is computed for each black-box approach.
% \subsection{SelfCheckGPT via BERTScore}
% \begin{align}
%     \tilde{f}(r_i) &= \frac{1}{N} \sum_{n=1}^N \underset{j}{\text{max}} \left( \mathcal{B} (r_i, s^n_j) \right) \\
%     {f}(r_i) &= 1 - \tilde{f}(r_i)  
% \end{align}
% where $\mathcal{B}(.,.)$ denotes the BERTScore where two sentences (we use \texttt{roberta-large} as the backbone of BERTScore). 


\subsection{SelfCheckGPT via MQAG}
\label{section:self_check_mqag}
Alternatively, one could assess consistency using question answering. We base this approach on the multiple-choice question answering generation (MQAG) framework \cite{manakul2023mqag}. MQAG assesses consistency by generating multiple-choice question that an external answering system can independently answer given each passage. If facts on consistent concepts are queried, the answering system is expected to predict similar answers. The MQAG framework consists of a question-answer generation system $\texttt{G1}$, distractor (incorrect options) generation system $\texttt{G2}$, and answering system \texttt{A}. For each sentence in the response $r_i$, we draw questions $q$, associated answers $a$, and distractors $\mathbf{o}_{\backslash a}$:
\begin{align}
    q, a &\sim P_{\texttt{G1}}(q,a|r_i) \\
    \mathbf{o}_{\backslash a} &\sim P_{\texttt{G2}}(\mathbf{o}_{\backslash a}|q,a,r_1,...,r_M)
\end{align}
where $\mathbf{o} = \{a, \mathbf{o}_{\backslash a}\}= \{o_1, ..., o_4\}$. To filter out bad (e.g. unanswerable) questions, we define an answerability score \cite{raina-gales-2022-answer}:
\begin{equation}
    \alpha = P_{\texttt{U}}(\text{answerable}|q,\text{context})
\end{equation}
where the context can be the response or sampled passages. $\alpha \rightarrow 0.0$ for unanswerable and $\alpha \rightarrow 1.0$ for answerable. We use $\alpha$ to filter out unanswerable questions, i.e. those which have an answerability score lower than a threshold. Subsequently, we use the answering system for all answerable questions:
\begin{align}
    a_r &= \underset{k}{\text{argmax}} \left[ P_{\texttt{A}}(o_k | q, r, \mathbf{o})  \right] \\
    a_{s_n} &= \underset{k}{\text{argmax}} \left[  P_{\texttt{A}}(o_k | q, s_n, \mathbf{o}) \right]
\end{align}
We compare whether $a_r$ matches $a_{s_n}$ for all sampled passages, yielding a set of matches $\mathcal{S}_{=} = \{s_n \hspace{0.4em}\text{s.t.}\hspace{0.4em} a_{s_n} = a_r\}$ and a set of mismatches $\mathcal{S}_{\neq} = \{s_n \hspace{0.4em}\text{s.t.}\hspace{0.4em} a_{s_n} \neq a_r\}$. Thus, we can define a simple inconsistency for sentence $r_i$ and question $q$ by the match count and mismatch count:
\begin{equation}
    f(r_i, q) = \frac{|\mathcal{S}_{\neq}|}{|\mathcal{S}_{=}| + |\mathcal{S}_{\neq}|} \label{eq:simple_counting}
\end{equation}
 % Using all answerable questions, we define the \textit{inconsistency} score for $r_i$ as the percentage of mismatches $\frac{c_2}{c_1+c_2}$.
To take into account the number of answerable questions given sampled passages (i.e. evidence to assess $r_i$), we use the Bayes' theorem (the derivation provided in Appendix \ref{appendix:bayes_mqag}) to modify the definition of inconsistency score in Equation \ref{eq:simple_counting}:
\begin{equation}
    f(r_i, q) = \frac{\gamma_2^{|\mathcal{S}_{\neq}|}}{\gamma_1^{|\mathcal{S}_{=}|} + \gamma_2^{|\mathcal{S}_{\neq}|} }
\end{equation}
% where $\tilde{c}_1 = \sum_{k \in Q_\text{match}} (\alpha_k)$ and $\tilde{c}_2 = \sum_{k \in Q_\text{mismatch}} (\alpha_k)$ are effective match count and mismatch count. 
where $\tilde{c}_1$ = effective match count and $\tilde{c}_2$ = effective mismatch count defined in Appendix \ref{appendix:bayes_mqag}. Thus, SelfCheckGPT-MQAG is obtained by averaging the inconsistency score across all generated questions:
\begin{equation}
    f(r_i) = \mathbb{E}_q \left[ f(r_i, q) \right]
\end{equation}
Note that SelfCheck-BERTScore and SelfCheck-MQAG score can be combined which we refer to as SelfCheckGPT-Combination.

\section{Experimental Setup}
\subsection{Data and Annotation}
We evaluate our hallucination detection approaches by 1) generating synthetic Wikipedia articles using GPT-3, 2) manually annotating the factuality of the passage at a sentence level; and 3) measuring system's ability in detecting hallucinations/validity. We also aggregate sentence-level scores to assess overall passage factuality. \\

%Test- 72k
WikiBio \cite{wikibio_dataset} is dataset of the first paragraph (along with tabular information) of Wikipedia biographies. We rank the WikiBio test set in terms of paragraph length and randomly sample 65 articles from the top 5\% of longest articles (to ensure no obscure concept is selected). We use GPT-3 to generate Wikipedia articles on a concept using the prompt \texttt{This is a Wikipedia passage about \{concept\}}. We then annotate the sentences of the generated passages using the guidelines illustrated in Figure \ref{fig:labelling_diagram}. A total of 526 sentences from 65 passages were annotated, with each sentence classified as: \\

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth,keepaspectratio]{fig/annotation.drawio.pdf}
    \caption{Flowchart of our annotation process}
    \label{fig:labelling_diagram}
\end{figure}

\noindent\textbf{Major Inaccurate} (Non-Factual, \textbf{1}): The sentence is a total hallucination unrelated to the topic. \\
\noindent\textbf{Minor Inaccurate} (Non-Factual, \textbf{0.5}): The sentence consists of non-factual information, though related to the topic. \\ 
\noindent\textbf{Accurate} (Factual, \textbf{0}): The information presented in the sentence is accurate. \\



Of the 526 annotated sentences, 240 (45.7\%) of the sentences were labelled major-inaccurate, 158 (30.0\%) were minor-inaccurate, and 128 (24.3\%) were accurate. Note that by selecting more obscure/well-known concepts, the distribution of labels can be shifted to contain more/fewer hallucinations. Passage-level scores can be obtained by averaging the sentence-level labels in each passage. The distribution of passage-level scores is shown in Figure \ref{fig:histogram_annotation_doc}, where we observe a large peak at +1.0. We refer to these points as \textit{total hallucination}, i.e. the individual/concept was entirely made up and is unrelated to the real concept. We also consider the more challenging task of detection \textit{ignoring} cases of total hallucination. In this setup, all passages with an average score greater than 0.75 are filtered out, leaving 41 passages and 322 sentences. 

\begin{figure}[!h]
    \centering
\includegraphics[width=0.8\linewidth,keepaspectratio]{fig/histogram_annotation.pdf}
    \caption{Document factuality scores histogram plot}
    \label{fig:histogram_annotation_doc}
\end{figure}
\subsection{Passage-level factuality} 
Sentence-level scores can be used to determine passage-level factuality scores by simply calculating the average over all sentences. 

\begin{equation}
    f_{\text{passage}}(r) = \frac{1}{|r|} \sum_i f(r_i)
    \label{eq:doc_level_score}
\end{equation}
For the average of $\text{log}p$ and perplexity, we \textit{directly} compute the average over all tokens in a passage. For human ground-truth and other methods, we follow the average defined in Equation \ref{eq:doc_level_score}.


\subsection{LLM}
We use GPT-3 (text-davinci-003), the state-of-the-art system at the time of conducting our experiments, as the main generative LLM. To obtain the main response, we set the generation temperature to 0.0 and use beamsearch decoding. For the stochastically generated samples, we set the temperature to 1.0 and generate 20 sampled passages.

\subsubsection*{Proxy LLM}
The proxy LLM architectures considered are OPT-\{125m, 1.3B, 13B, 30B\} \cite{zhang2022opt}, GPT-J-6B \cite{gpt_j} and GPT-NeoX-20B \cite{black-etal-2022-gpt}. Results in the main paper focus on GPT-NeoX, however all other results can be found in the appendix. 

\subsection{Question Generation and Answering}
% We follow the multiple-choice question-answering setting described in \cite{manakul2023mqag}.
The generation systems \texttt{G1} and \texttt{G2} (described in Section \ref{section:self_check_mqag}) are T5-large finetuned to SQuAD \cite{rajpurkar-etal-2016-squad} and RACE \cite{lai-etal-2017-race}, respectively. The answering system \texttt{A} is Longformer \cite{longformer} finetuned to the RACE dataset. The answerability system \texttt{U} is also Longformer, but fine-tuned to SQuAD2.0. These models have been made available on HuggingFace and full details can be found on our project page: \url{https://github.com/potsawee/selfcheckgpt}.

\section{Experimental Results}
\subsection{Sentence-level Hallucination Detection}
\begin{table*}[!t]
  \centering
  \begin{tabular}{l|c|ccc}
    \toprule
    \multirow{2}{*}{Method} &\multirow{2}{*}{$p_{\theta}$}  &\multicolumn{3}{c}{Sentence-level detection (GPT-3 passages)}  \\
    & &Non-Factual &Non-Factual* &Factual \\
    \midrule
    Random        &-    &75.67 &60.25 &24.33 \\
    \midrule
    \texttt{GPT-3} Avg($-$log$p$) &$\theta$    &83.67 &70.63 &48.67\\
    \texttt{GPT-3} Avg(PPL5$^\dagger$)    &$\theta$       &82.02 &67.70 &48.45\\
    \texttt{GPT-3} Max($-$log$p$) &$\theta$    &86.16 &76.51 &44.22\\
    \texttt{GPT-3} Max(PPL5$^\dagger$)    &$\theta$       &83.82 &72.15 &45.26\\
    \midrule
    \texttt{NeoX-20B} Avg($-$log$p$) &$\tilde{\theta}$ &71.34 &55.68 &21.39\\
    \texttt{NeoX-20B} Avg(PPL)     &$\tilde{\theta}$ &79.21 &65.92 &24.76\\
    \texttt{NeoX-20B} Max($-$log$p$) &$\tilde{\theta}$ &67.91 &53.47 &19.97\\
    \texttt{NeoX-20B} Max(PPL)     &$\tilde{\theta}$ &78.39 &65.54 &23.62\\
    \midrule
    SelfCheckGPT-BERTScore  &\xmark    &87.51 &74.01 &46.43 \\
    % Counting       &\xmark     &84.81 &73.88 &42.09\\
    % + Bayes'       &\xmark     &84.32 &73.00 &43.92\\
    SelfCheckGPT-MQAG    &\xmark    &86.30 &75.42 &44.15  \\ % this is bayes + gamma
    SelfCheckGPT-Combination &\xmark    &86.99 &76.16 &46.31  \\
    \midrule
    WikiBio-MQAG$^{\ddagger}$   &\xmark     &87.52  &76.13 &52.78\\
    \bottomrule
  \end{tabular}
  \caption{AUC-PR for Detecting Non-Factual and Factual Sentences in the GPT-3 generated WikiBio passages. $^{\dagger}$GPT3 only returns the top 5 probabilities, which are used to approximate perplexity. $^{\ddagger}$The first paragraph from Wikipedia is used for comparing facts. }
  \label{tab:big_table}
\end{table*}

Our first set of experiments investigates whether our fact hallucination detection methods are capable of identifying the factuality of sentences. We group both major-inaccurate labels and minor-inaccurate labels together into the \textit{non-factual} class, while the \textit{factual} class refers to accurate sentences. We look both at fact verification (i.e. identifying factual sentences) and hallucination detection (i.e. identifying non-factual sentences) performance. Table \ref{tab:big_table} and Figure \ref{fig:detection_pr_curve} illustrate the performance of different detection approaches, where the following observations were made:

% \item If token probabilities (or perplexities) information are not available, i.e. when using a black-box LLM, one could approximate the probabilities/perplexities by using different LLMs. We approximate GPT-(175B, text-davinci-003)  
% with a range of LLMs including OPT from 125m to 30B, GPT-J-6B, and 

\newpage
\textbf{1) $p_\theta$ scores correlate well with factuality}: We show that probability and perplexity measures are strong baselines for assessing factuality. Factual sentences can be identified with an AUC-PR of 48.67, significantly better than the random baseline of 24.33, with the AUC-PR for hallucination detection also increasing from 75.67 to 86.16. This supports the hypothesis that when the LLMs are uncertain about generated information, generated tokens often have higher uncertainty, paving a promising direction for hallucination detection approaches. 

\textbf{2) Proxy LLM $p_{\tilde{\theta}}$ scores fail}: the previous probability measures require access to LLM output model probabilities, which may not always be available. We proposed using a proxy system (GPT-NeoX-20B) which one has access to, to approximate the output model probabilities of GPT-3. Table \ref{tab:big_table} shows, however, that this approach is ineffective. Detection performance using proxy models is near random in most settings, with only the perplexity measure performs better than random. We believe this poor performance occurs as different LLMs have different generating patterns, and therefore even common uninformative tokens may have low $p_{\tilde{\theta}}$ if they don't follow the style of the proxy LLM. We note though that a weighted conditional language model score (as done in BARTScore \cite{yuan2021bartscore}) could be incorporated in future investigations to improve this approach. Further, ablations in the appendix (Table \ref{tab:big_table_appendix}) show the impact of the initialisation/size of the proxy LLM, where it is shown that larger models typically yield better factual/non-factual detection performance, but none of which rival the previous grey box approach. %Results in Table \ref{tab:llm_token_prob} also show that larger models have a higher expected token probability. 

\textbf{3) SelfCheckGPT rivals grey-box approaches}: SelfCheckGPT considerably outperforms the proxy LLM baseline approach in all detection set ups. Furthermore, SelfCheckGPT approach has similar performance to the grey-box uncertainty based approaches, with SelfCheckGPT approach even outperforming all other methods for hallucination detection.

\textbf{4) External Knowledge improves AUC}: If external knowledge is available, one could measure the informational consistency with the LLM response and the real-world document. In this experiment, we have the related WikiBio Passage and so can extract the first Wikipedia paragraph for each concept/individual. We show that by using SelfCheckGPT via MQAG with the reference passage (and without any other samples) we achieve even better performance and have high performance over all detection tasks. However, we note that this method is no longer zero-resource as it requires retrieving relevant knowledge, and so this approach may only be applicable to fact verification.  

\begin{figure*}[!t]
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_nonfact_all_sentences.pdf}
        \caption{Non-Factual Sentences}
        \end{subfigure}
        \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_nonfact_good_sentences.pdf}
        \caption{Non-Factual* Sentences}
        \end{subfigure}
        \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth,keepaspectratio]{fig/pr_curve_gpt3_fact_all_sentences.pdf}
        \caption{Factual Sentences}
    \end{subfigure}
    \caption{PR-Curve for Detecting Non-factual and Factual sentences in GPT-3 generated WikiBio passages.}
    \label{fig:detection_pr_curve}
\end{figure*}

\subsection{Passage-level Ranking}
The previous results show that SelfCheckGPT is an effective approach for predicting sentence-level factuality. An additional consideration, though, is whether SelfCheckGPT can be used to determine the overall factuality of passages. Sentence-level labels can be averaged to generate passage-level scores, and since human judgement is somewhat subjective, averaging the sentence-level labels would lead to ground truths with less noise. Our results in Table \ref{tab:doc_level_results} and Figure \ref{fig:scatter_plot_passage} show that SelfCheckGPT (both BERTScore and MQAG) correlates far better with human judgements than all other methods, including the grey-box probability/perplexity methods. Further, the two variants of SelfCheckGPT appear complementary, with the combined approach being the best-performing system. Unsurprisingly, the proxy LLM approach again achieves low/negative correlations. A final observation is that, interestingly, SelfCheckGPT even outperforms MQAG with external knowledge, illustrating that SelfCheckGPT is a strong hallucination detection approach that can even outperform methods using stored external information.

\begin{table}[!h]
  \centering
    \tabcolsep=1mm
  \begin{tabular}{lc|cc}
    \toprule
    Method &$p_{\theta}$           &PCC &SCC  \\
    \midrule
    \texttt{GPT-3} Avg($-$log$p$) &$\theta$ &43.48 &32.72  \\
    \texttt{GPT-3} Avg(PPL5)      &$\theta$ &43.44 &31.00  \\
    \midrule
    \texttt{NeoX} Avg($-$log$p$) &$\tilde{\theta}$ &-31.56 &-31.60  \\
    \texttt{NeoX} Avg(PPL)       &$\tilde{\theta}$ &7.49 &5.90  \\
    \midrule
    SelfCheckGPT-BERTScore   &\xmark    &64.84 &64.58  \\
    SelfCheckGPT-MQAG        &\xmark    &57.46 &54.61  \\
    SelfCheckGPT-Combination     &\xmark    &66.10 &65.01   \\
    \midrule
    WikiBio-MQAG$^{\dagger}$ &\xmark    &61.09 &60.11 \\
    \bottomrule
  \end{tabular}
  \caption{Passage-level ranking performance as measured by Pearson correlation coefficient (PCC) and Spearman's rank correlation coefficient (SCC) w.r.t. human judgements. $^{\dagger}$The first paragraph from Wikipedia is used as external knowledge.}
  \label{tab:doc_level_results}
\end{table}

\subsection{Impact of the Number of Samples}
Typically, sampling-based methods are expected to achieve better performance when more samples are drawn. However, drawing a higher number of samples leads to higher computational costs and/or API costs. Thus, we investigate the behaviour of SelfCheckGPT as we vary the number of samples drawn from 1 to 20. Our results in Figure \ref{fig:number_of_samples} illustrate that the performance of SelfCheckGPT increases as more samples are used, with the performance gain diminishing as we generate more samples. 

\begin{figure}[!h]
    \begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/ablation_number_samples_detect_nonfact.pdf}
      \caption{Detect Non-Factual*}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/ablation_number_samples_doclevel.pdf}
      \caption{Rank Passages}
    \end{subfigure}
    \caption{Performance (AUC-PR for detecting non-factual sentences and SCC for ranking passages) versus the number of samples.}
    \label{fig:number_of_samples}
\end{figure}

% \subsection{Is OurMethod \textit{implicit} perplexity?}



\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_baseline1.pdf}
      \caption{\texttt{GPT-3} Avg($-$log$p$)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_bertscore.pdf}
      \caption{SelfCheckGPT-BERTScore}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_method3.pdf}
      \caption{SelfCheckGPT-MQAG}
    \end{subfigure}
    \caption{Scatter plot of scores at the passage level where Y-axis = Method scores, X-axis = Human scores. We report the correlations in Table \ref{tab:doc_level_results} and Table \ref{tab:doc_level_appendix}.}
    \label{fig:scatter_plot_passage}
\end{figure*}

\section{Conclusions}
This work focuses on the task of hallucination detection in LLMs, and investigates methods that can be used to determine the factuality of sentences as well as the overall factuality of a passage. We propose SelfCheckGPT, a zero-resource black-box approach, that can be used to detect LLM hallucinations and determine the trustworthiness of generated responses. Through experimental analysis of annotated GPT-3 responses, we show that SelfCheckGPT has a competitive performance to the grey-box probablity-based approach, and SelfCheckGPT significantly outperforms the proxy LLM approach. 
% Overall, we present initial findings and demonstrate the effectiveness of SelfCheckGPT.  


 
% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Limitations}
In this study, the scope of GPT-3 generated texts is limited to 65 passages based on individuals in the WikiBio dataset, and we suggest that a larger annotated and wider range of concepts, e.g. locations and objects should be investigated to better understand the nature of LLM's hallucination. 



\section{SelfCheckGPT-MQAG with Bayes}
\label{appendix:bayes_mqag}
Let $P(\text{F})$ be $P(r_i = \text{non-factual})$ and $P(\text{T})$ be $P(r_i = \text{factual})$. For a question $q$, the probability of sentence $r_i$ being non-factual given matched answers $\mathcal{S}_=$ and mismatched answers $\mathcal{S}_{\neq}$ is:
\begin{align}
    &P(\text{F} | \mathcal{S}_=, \mathcal{S}_{\neq}) \nonumber \\
    &= \frac{ P(\mathcal{S}_=, \mathcal{S}_{\neq} | \text{F}) P(\text{F})}{P(\mathcal{S}_=, \mathcal{S}_{\neq} | \text{F}) P(\text{F}) + P(\mathcal{S}_=, \mathcal{S}_{\neq} | \text{T}) P(\text{T})} \nonumber \\
    &= \frac{ P(\mathcal{S}_=, \mathcal{S}_{\neq} | \text{F})}{P(\mathcal{S}_=, \mathcal{S}_{\neq} | \text{F})  + P(\mathcal{S}_=, \mathcal{S}_{\neq} | \text{T}) } \label{eq:bayes}
\end{align}
where we assume that $r_i$ is equally likely to be False or True, i.e. $P(\text{F}) = P(\text{T})$. The probability of getting $\mathcal{S}_=, \mathcal{S}_{\neq}$ when $r_i$ is False:
\begin{align}
&P(\mathcal{S}_=, \mathcal{S}_{\neq} | \text{F}) \nonumber \\
&= \underset{s_n \in \mathcal{S}_=}{\prod} P(a_{s_n} = a_r | F) \underset{s_{n'} \in \mathcal{S}_{\neq}}{\prod} P(a_{s_{n'}} \neq a_r | F) \nonumber \\
&= (1-\beta_1)^{|\mathcal{S}_=|} (\beta_1)^{|\mathcal{S}_{\neq}|}
\end{align}
and probability of getting $\mathcal{S}_=, \mathcal{S}_{\neq}$ when $r_i$ is True:
\begin{align}
&P(\mathcal{S}_=, \mathcal{S}_{\neq} | \text{T}) \nonumber \\
&= \underset{s_n \in \mathcal{S}_=}{\prod} P(a_{s_n} = a_r | T) \underset{s_{n'} \in \mathcal{S}_{\neq}}{\prod} P(a_{s_{n'}} \neq a_r | T) \nonumber \\
&= (\beta_2)^{|\mathcal{S}_=|} (1-\beta_2)^{|\mathcal{S}_{\neq}|}
\end{align}
Therefore, we can simplify Equation \ref{eq:bayes} to:
\begin{equation}
    P(\text{F} | \mathcal{S}_=, \mathcal{S}_{\neq}) = \frac{\gamma_2^{|\mathcal{S}_{\neq}|}}{\gamma_1^{|\mathcal{S}_{=}|} + \gamma_2^{|\mathcal{S}_{\neq}|} }
\end{equation}
where $\gamma_1 = \frac{\beta_2}{1-\beta_1}$ and $\gamma_2 = \frac{\beta_1}{1-\beta_2}$. Lastly, instead of rejecting samples having an answerability score below a threshold\footnote{$\alpha$ is between 0.0 (unanswerable) and 1.0 (answerable). Standard-counting $|\mathcal{S}_{=}|$ and $|\mathcal{S}_{\neq}|$ can be considered as a special case of soft-counting where $\alpha$ is set to 1.0 if $\alpha$ is greater than the answerability threshold and otherwise $\alpha$ is 0.0.}, we find empirically that soft-counting (defined below) improves the detection performance. We set both $\beta_1$ and $\beta_2$ to 0.8.

\begin{align}
    |{\mathcal{S}}_{=}| &= \underset{n \hspace{0.3em}\text{s.t.}\hspace{0.3em} s_n \in \mathcal{S}_{=}}{\sum}\alpha_n  \\
    |{\mathcal{S}}_{\neq}| &= \underset{n \hspace{0.3em}\text{s.t.}\hspace{0.3em} s_n \in \mathcal{S}_{\neq}}{\sum}\alpha_n
    % \label{eq:soft_counting}
\end{align}
where $\alpha_n = P_{\texttt{U}}(\text{answerable}|q, s_n)$.

\section{Additional Results}
\label{sec:appendix_results}

\begin{table}[!ht]
  \centering
      \tabcolsep=1.7mm
  \begin{tabular}{lcc|cc}
    \toprule
    LLM &size &Metric &PCC &SCC  \\
    \midrule
    \texttt{OPT}   &30B  &Avg($-$log$p$)      &-31.65 &-35.21 \\
    \texttt{NeoX}  &20B  &Avg($-$log$p$)      &-31.56 &-31.60  \\
    \texttt{OPT}   &13B  &Avg($-$log$p$)      &-33.29 &-37.39  \\
    \texttt{GPT-J} &6B   &Avg($-$log$p$)      &-38.16 &-38.47  \\
    \texttt{OPT}   &1.3B &Avg($-$log$p$)      &-41.55 &-42.72  \\
    \texttt{OPT}   &125m &Avg($-$log$p$)      &-40.80 &-41.93  \\
    \midrule
    \texttt{OPT}   &30B  &Avg(PPL)     &3.26 &7.77   \\
    \texttt{NeoX}  &20B  &Avg(PPL)     &7.49 &5.90  \\
    \texttt{OPT}   &13B  &Avg(PPL)     &5.34 &7.75  \\
    \texttt{GPT-J} &6B   &Avg(PPL)     &2.28 &4.76  \\
    \texttt{OPT}   &1.3B &Avg(PPL)     &2.09 &2.27  \\
    \texttt{OPT}   &125m &Avg(PPL)     &-8.03 &-8.53  \\
    \bottomrule
  \end{tabular}
  \caption{Passage-level PCC and SCC with LLMs used to assess GPT-3 responses. This table extends Table \ref{tab:doc_level_results}.}
  \label{tab:doc_level_appendix}
\end{table}


\begin{table}[!ht]
  \centering
  \tabcolsep=0.7mm
  \begin{tabular}{l|ccc|cc}
    \toprule
    \multirow{2}{*}{Varaint}  &\multicolumn{3}{c}{Sentence-lvl} &\multicolumn{2}{c}{Passage-lvl} \\
    &Non-F &Non-F* &Fact &PCC &SCC \\
    \midrule
    SimpleCount        &84.81 &73.88 &42.09 &47.15 &44.23  \\
    + Bayes             &84.32 &73.00 &43.92 &50.85 &50.16  \\
    + Bayes + $\alpha$  &86.30 &75.42 &44.15 &57.46 &54.61  \\
    \bottomrule
  \end{tabular}
  \caption{Ablation study on the variants of SelfCheckGPT-MQAG.}
\end{table}


\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.195\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_baseline2.pdf}
      \caption{\texttt{GPT-3} Avg(PPL)}
    \end{subfigure}
    \begin{subfigure}[b]{0.195\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_neox1.pdf}
      \caption{\texttt{NeoX} Avg($-$log$p$)}
    \end{subfigure}
    \begin{subfigure}[b]{0.195\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_neox2.pdf}
      \caption{\texttt{NeoX} Avg(PPL)}
    \end{subfigure}
    \begin{subfigure}[b]{0.195\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_combine.pdf}
      \caption{SelfCheckGPT-Cb}
    \end{subfigure}
    \begin{subfigure}[b]{0.195\linewidth}
\centering
\includegraphics[width=\linewidth,keepaspectratio]{fig/scatter_doclevel_pseudolabel.pdf}
      \caption{WikiBio-MQAG}
    \end{subfigure}
    \caption{Scatter plot of scores at the passage level where Y-axis = Method scores, X-axis = Human scores. Extended results in addition in Figure \ref{fig:scatter_plot_passage}.}
    \label{fig:scatter_plot_passage_2}
\end{figure*}

\begin{table}[!ht]
  \centering
  %\tabcolsep=1.7mm
  \begin{tabular}{lc|c}
    \toprule
    LLM            &Size  &AvgTokenProb  \\
    \midrule
    \texttt{GPT-3} &175B &62.67 \\
    \midrule
    \texttt{OPT}   &30B       &35.33 \\
    \texttt{NeoX}  &20B       &35.21 \\
    \texttt{OPT}   &13B       &33.67 \\
    \texttt{GPT-J} &6B        &33.23 \\
    \texttt{OPT}   &1.3B      &28.27 \\
    \texttt{OPT}   &125m      &17.82 \\
    \bottomrule
  \end{tabular}
  \caption{Average token probability over all tokens in GPT-3 generated passages.}
  \label{tab:llm_token_prob}
\end{table}

\begin{table*}[!t]
  \centering
  \begin{tabular}{lcc|ccc}
    \toprule
    \multirow{2}{*}{LLM} &\multirow{2}{*}{Size} &\multirow{2}{*}{Metric}  &\multicolumn{3}{c}{Sentence-level detection (GPT-3 passages)}  \\
    && &Non-Factual &Non-Factual* &Factual \\
    \midrule
    Random         &- &-  &75.67 &60.25 &24.33 \\
    \midrule
    \texttt{OPT}   &30B  &Avg($-$log$p$)      &70.55 &55.53 &21.32\\
    \texttt{NeoX}  &20B  &Avg($-$log$p$)      &71.34 &55.68 &21.39\\
    \texttt{OPT}   &13B  &Avg($-$log$p$)      &70.00 &54.68 &20.84\\
    \texttt{GPT-J} &6B   &Avg($-$log$p$)      &69.99 &55.04 &20.11\\
    \texttt{OPT}   &1.3B &Avg($-$log$p$)      &69.09 &54.52 &20.53\\
    \texttt{OPT}   &125m &Avg($-$log$p$)      &69.94 &55.09  &19.74\\
    \midrule
    \texttt{OPT}   &30B  &Max($-$log$p$)      &67.96 &54.30 &19.92\\
    \texttt{NeoX}  &20B  &Max($-$log$p$) &67.91 &53.47 &19.97\\
    \texttt{OPT}   &13B  &Max($-$log$p$)      &68.43 &55.31 &19.80\\
    \texttt{GPT-J} &6B   &Max($-$log$p$)     &68.16 &54.73 &19.59\\
    \texttt{OPT}   &1.3B &Max($-$log$p$)     &69.52 &56.62 &20.32\\
    \texttt{OPT}   &125m &Max($-$log$p$)     &72.19 &58.58 &20.26\\
    \midrule
    \texttt{OPT}   &30B  &Avg(PPL)      &79.03 &65.44 &25.78\\
    \texttt{NeoX}  &20B  &Avg(PPL)      &79.21 &65.92 &24.76\\
    \texttt{OPT}   &13B  &Avg(PPL)      &78.23 &63.94 &24.39\\
    \texttt{GPT-J} &6B   &Avg(PPL)      &78.15 &64.62 &23.23\\
    \texttt{OPT}   &1.3B &Avg(PPL)      &77.71 &64.52 &22.66\\
    \texttt{OPT}   &125m &Avg(PPL)      &75.32 &61.04  &21.83\\
    \midrule
    \texttt{OPT}   &30B  &Max(PPL)     &77.24 &63.51 &24.57\\
    \texttt{NeoX}  &20B  &Max(PPL)     &78.39 &65.54 &23.62\\
    \texttt{OPT}   &13B  &Max(PPL)     &76.59 &62.59 &23.54\\
    \texttt{GPT-J} &6B   &Max(PPL)     &76.06 &62.13 &22.32\\
    \texttt{OPT}   &1.3B &Max(PPL)     &77.10 &64.26 &22.61\\
    \texttt{OPT}   &125m &Max(PPL)     &73.60 &60.08  &21.69\\
    \bottomrule
  \end{tabular}
  \caption{AUC-PR for Detecting Non-Factual and Factual Sentences in the GPT-3 generated WikiBio passages. This table is an extension to Table \ref{tab:big_table}}
  \label{tab:big_table_appendix}
\end{table*}


\end{document}
