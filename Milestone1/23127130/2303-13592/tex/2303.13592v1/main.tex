% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{array}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{transparent}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{soul}
\newcommand\todo[1]{\textcolor{red}{#1}}
\newcommand\sam[1]{\textcolor{olive}{sam:#1}}
\input{math_commands.tex}
\usepackage{listings,multicol}

%%%% bubble text
\usepackage[many]{tcolorbox}
\usepackage{xcolor}
\usepackage{varwidth}
\usepackage{environ}
\usepackage{subcaption}
\usepackage{xparse}
\usepackage{textbubbles} % textbubbles.sty
%%%% bubble text

\usepackage[inline,shortlabels]{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em} %leftmargin=1em,iciteposstemindent=-1em
\setenumerate{noitemsep,leftmargin=1em,itemindent=13pt,topsep=0em}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% \usepackage[T1]{fontenc}
% For Vietnamese characters
\usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
% \usepackage[UTF8]{ctex}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{CJKutf8}
\definecolor{blue}{HTML}{016BA4}
\definecolor{orange}{HTML}{FF800E}
\definecolor{gray}{HTML}{898989}
\definecolor{sky}{HTML}{A3C8EC}


\usepackage{tikz}
\newcommand\bubblebox[2][]{\tikz[overlay]\node[fill=blue!20,inner sep=2pt, anchor=text, rectangle, rounded corners=1mm,#1] {#2};\phantom{#2}}

\title{Prompting Large Language Models to Generate Code-Mixed Texts: \\The Case of South East Asian Languages}

\author{
\textbf{Zheng-Xin Yong}$^{1}$\quad
\textbf{Ruochen Zhang}$^{1}$\quad
\textbf{Jessica Zosa Forde}$^{1}$\quad
\textbf{Skyler Wang}$^2$\quad
\\
\textbf{Samuel Cahyawijaya}$^3$\quad
\textbf{Holy Lovenia}$^3$\quad
% \textbf{Genta Indra Winata}$^5$\quad % wait until approval
\textbf{Lintang Sutawika}$^{4,5}$\quad
\\
\textbf{Jan Christian Blaise Cruz}$^6$\quad
\textbf{Long Phan}$^7$\quad
\textbf{Yin Lin Tan}$^{8,9}$\quad
\textbf{Alham Fikri Aji}$^{10}$\quad % can I propose myself as last author lol
\\
$^1$Brown University \;
$^2$UC Berkeley \;
$^3$HKUST \;
% $^4$Bloomberg \; % wait until approval
$^4$Datasaur.ai \;
$^5$EleutherAI \;
\\
$^6$Samsung R\&D Institute Philippines \; 
$^7$VietAI Research \;
\\
$^{8}$Stanford University \;
$^{9}$National University of Singapore \;
$^{10}$MBZUAI \;
}


\begin{document}
\begin{CJK*}{UTF8}{gbsn}
\maketitle

\begin{abstract}
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA)---Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish.
We find that ChatGPT shows the most potential, capable of producing code-mixed text 68\% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96\% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semantic inaccuracies. Other multilingual models such as BLOOMZ and Flan-T5-XXL are unable to produce code-mixed texts altogether. By highlighting the limited promises of LLMs in a specific form of low-resource data generation, we call for a measured approach when applying similar techniques to other data-scarce NLP contexts.
\end{abstract}

\section{Introduction}
Code-mixing is the linguistic practice of alternating between two or more languages in an utterance or conversation~\citep{poplack1978syntactic}. It allows individuals to express culturally-specific ideas, connect with or differentiate from other interlocutors, and reify their identities~\citep{bhatia2004,grosjean1982,toribio2006,chen1996code}. Despite its prevalence across many parts of the world, computational research into the area has only picked up steam recently~\cite{winata2022decades}. The growing visibility of code-mixing in the global media landscape, the proliferation of data-hungry machine-learning methods, and calls for natural language processing (NLP) research to devote more attention to multilingualism and low-resource languages collectively contribute to this emerging interest~\cite{aziz2019types,barman2014code,https://doi.org/10.48550/arxiv.2207.04672}.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{assets/alt-world.pdf}
\caption{\label{fig:SEA} Depiction of SEA regions, which consist of a total of 11 countries. We prompt LLMs to generate code-mixed data of languages used in six South East Asian countries (colored in dark blue): Brunei, Indonesia, Malaysia, Philippines, Singapore, and Vietnam.}
\end{figure}

That said, acquiring high-quality and low-cost code-mixed data presents itself as a challenge to NLP researchers in this field. For one, code-mixing is observed more frequently in colloquial settings and spoken communication, which makes procuring and curating extensive datasets logistically demanding and costly~\citep{chan2009automatic,winata2021multilingual}. Moreover, despite code-mixing's prevalence across social media or digital messaging platforms, consolidating such data may be curtailed by legal guardrails and scalability issues.

Recognizing these challenges, this article explores the feasibility of using Large Language Models (LLMs) to ameliorate data scarcity in code-mixing research. Known for their generative capabilities, we test whether multilingual LLMs can be prompted to create code-mixed data, and if so, to what degree. Using a code-mixing scale of 0 to 3 (0-no code-mixing; 1-loanword usage; 2-topic-related entities; 3-beyond entities), we compare performance across different models on different languages and assess whether these outputs are of a quality decent enough for research deployment. 

To do this, we hone in on languages in South East Asia (SEA). Home to more than 680 million people and over 1200 languages, code-mixing is particularly prevalent in this region due to their extended histories of language and cultural cross-fertilization (Figure~\ref{fig:SEA})~\cite{goddard2005languages,bautista2006southeast,reid-etal-2022-m2d2}. Marked by its distinctive multiracial and multilingual composition today, SEA exhibits unparalleled linguistic diversity and presents an opportunity to further research on multiple under-explored languages and practices\footnote{Major languages in SEA countries belong to different language families such as Indo-European, Thai, Austronesian, Sino-Tibetan, Dravidian, and Austro-Asiatic. Furthermore, there are at least thousands of major and minor SEA languages.}~\cite{migliazza1996mainland,goddard2005languages, joshi-etal-2020-state, aji-etal-2022-one,cahyawijaya2023nusacrowd}. 
Despite so, multilingual NLP research has devoted scant attention to examining code-mixing in this region, and publicly available code-mixed datasets relevant to its communities remain limited \cite{lyu2010seame,winata2022decades}. 

In this effort, we prompt LLMs such as ChatGPT, InstructGPT (davinci-002 and davinci-003) \cite{ouyang2022rlhf}, BLOOMZ \cite{muennighoff2022bloomz}, and Flan-T5-XXL \cite{chung2022flant5} to automatically generate code-mixed text that bilingually mixes English with either \textbf{Malay, Indonesian, Chinese, Tagalog, or Vietnamese}. All of these five SEA languages (alongside English) are used across six SEA countries, namely Singapore, Malaysia, Brunei, Philippines, Indonesia, and Vietnam. Furthermore, they belong to different language families---Indo-European, Austronesian, Sino-Tibetan, and Austro-Asiatic. An example of such a prompt is: "Write an English and Tagalog code-mixed sentence about Artificial Intelligence." In addition, we prompt these LLMs to generate texts in \textbf{Singlish} (a portmanteau of Singapore and English), a creole language that borrows from multiple languages. An example of a prompt involving Singlish looks like this: "Imitate the speaking style of a person who can speak Singlish in one sentence about family." We then ask native speakers to annotate the level of code-mixing in these generated outputs. We provide the complete set of prompts and code-mixed responses here: \url{https://github.com/Southeast-Asia-NLP/LLM-Code-Mixing}.

To the best of our knowledge, this marks the first attempt at studying the generation of synthetic code-mixed data through prompting LLMs in a zero-shot fashion without any monolingual reference texts or explicit linguistic constraints \citep{tarunesh-etal-2021-machine,rizvi-etal-2021-gcm,mondal-etal-2022-cocoa}. We discover that---for certain prompts---ChatGPT is able to follow instructions and generate syntactically sound code-mixed texts for the five SEA languages aforementioned up to 68\% success rate. For Singlish, ChatGPT and InstructGPT (davinci-003)'s performances are particularly noteworthy, clocking at 96\% across all prompts. In comparison, models such as BLOOMZ and Flan-T5-XXL are unable to produce code-mixed texts altogether (despite being advertised as multilingual). This leads us to conclude that code-mixing, at least as of today, is not considered an essential component of many multilingual LLMs. Moreover, the opaque creation of models like ChatGPT makes it difficult to ascertain the mechanisms that enable code-mixing generation.

Meanwhile, despite ChatGPT and InstructGPT's relative success in terms of performance, the code-mixing capabilities of these models are dampened by word choice errors that lead to semantic inaccuracies. In other words, while the grammatical arrangement of words in the output sentences may be correct, further scrutiny reveals misuses of words or concepts that ultimately diminish the fluency of these utterances. 
Furthermore, while ChatGPT can explain how the sentences are code-mixed, the explanations may be inaccurate. 
Thus, relying on LLMs to generate synthetic code-mixed data without extensive human supervision is discouraged. By highlighting the limited promises of LLMs in a specific form of low-resource data generation, we hope that NLP researchers think more critically 
about using such systems to produce synthetic data. 


\section{Methodology}
\begin{figure*}[!ht]
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_a.pdf}
		}
		\caption{Template: Assume as bilingual speaker}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_b.pdf}
		}
		\caption{Template: Two bilingual speakers}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_c.pdf}
		}
		\caption{Template: Imitate speaking style}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_d.pdf}
		}
		\caption{Template: Explicitly define CM}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_e.pdf}
		}
		\caption{Template: Native speaker}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_f.pdf}
		}
		\caption{Template: Write a CM sentence}
	\end{subfigure}
 \caption{Prompt templates with different \textcolor{blue}{languages} and \textcolor{orange}{topic} fields and responses from different LLMs containing code-mixed / non-code-mixed sentences. We also include a template where we specify the \textcolor{sky}{\ul{nationality}} of the speaker. Note that the explanations are a part of ChatGPT's original generation.
}
\label{fig:code-mixing-prompt-templates}
\end{figure*}


%%%% METHODOLOGY %%%%%%%
\subsection{Prompting Language Models}
We collect synthetic code-mixed data by prompting LLMs with natural language requests shown in Figure~\ref{fig:code-mixing-prompt-templates} along two axes: languages and topics (food, family, traffic, Artificial Intelligence, and weather).
Specifically, we explore ChatGPT, InstructGPT (davinci-002 and davinci-003) \cite{ouyang2022rlhf}, BLOOMZ \cite{muennighoff2022bloomz}, and Flan-T5-XXL \cite{chung2022flant5}. We use OpenAI and HuggingFace's API for prompting (see Appendix~\ref{app:hf-api}), except in the case of ChatGPT, for which we manually queried through the model's web interface\footnote{ChatGPT's API was not publicly released when we conducted this study.}. 


\begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{assets/overall_plot-2.pdf}
\caption{Comparison of performance of different LLMs in generating code-mixed data through zero-shot prompting. We distribute the result across different code-mixing levels: (1) Loanword, (2) Topic-related entities, and (3) Code-mixing beyond entity. }
\label{fig:plot-all-models}
\end{figure*}

In our prompts, we specify code-mixing between English with either Indonesian, Malay, Mandarin, Tagalog, or Vietnamese. We focused on code-mixing English with SEA languages for two reasons: (1) extensive literature on code-mixed English provides a relevant point of comparison, and (2) English is one of the most widely used languages in code-mixing across most SEA countries \cite{kirkpatrick2014english}. We additionally prompt sentences in Singlish, a creole language, to evaluate how sensitive LLMs are to the diversity of language practices in the SEA region. In total, we submitted 180 unique prompts per language model.

\begin{figure*}[!t]
\centering
\begin{subfigure}[b]{\textwidth}
     \centering
     \captionsetup{labelformat=empty}
     \caption{InstructGPT (davinci-003)}
     \includegraphics[width=\textwidth]{assets/subplots_davinci003.pdf}
     %\label{fig:davinci-003}
\end{subfigure}
\hfill
\begin{subfigure}[b]{\textwidth}
     \centering
     \captionsetup{labelformat=empty}
     \caption{ChatGPT}
     \includegraphics[width=\textwidth]{assets/subplots_chatgpt.pdf}
     %\label{fig:chatgpt}
\end{subfigure}
% \vspace{-1cm}
\caption{Analysis of code-mixed data generated by InstructGPT davinci-003 \textbf{(top)} and ChatGPT \textbf{(bottom)}.}
\label{fig:chatgpt-davinci-003-breakdown}
\end{figure*}

\subsection{Evaluation}

\subsubsection*{Code-Mixing Scale}
To evaluate the outputs, we ask whether LLMs are capable of producing \textit{intrasentential} code-mixed text. We adopt the definition of intrasentential code-mixing from \citet{berk1986linguistic}, which covers the mixing of small constituents---such as noun and verb phrases---and large constituents---such as coordinate clauses and prepositional phrases.  Native speakers are then tasked to manually annotated the collected responses on a scale from 0 to 3 using the following coding guideline to denote varying degrees of code-mixing:

\begin{itemize}
    \item \textbf{0 - No code-mixing:} The generated text is written purely in one language or only exhibits \textit{intersentential} code-mixing.
    \item \textbf{1 - Loanword usage:} The generated text uses loanwords for common terminologies. We define a foreign word as a loanword if the word is listed in Wiktionary\footnote{\url{https://en.wiktionary.org}}. For example: I like eating \textit{pho}.
    \item \textbf{2 - Topic-related entities:} The generated text mixes languages on entities/terms that are not considered loanwords—for example: 今天的 \textit{traffic} 真的很糟糕，我开了一个小时才到了办公室 (Chinese: The traffic today is really terrible. I spent an hour driving to get to the office).
    \item \textbf{3 - Beyond entity:} The generated text mixes languages beyond the entity level—for example: My family \textit{ay nagplano ng isang malaking} family reunion \textit{sa park} this coming weekend (Tagalog: My family has planned a big family reunion at the park this coming weekend). This category also includes intraword code-mixing, For example: Kapag busy ang trapiko, mag-ingat ka sa \textit{pagda-drive}\footnote{The prefix "pag-" in Tagalog is affixed to the English word "drive", resulting in the word "pagda-drive" (the act of driving). This example demonstrates the application of Tagalog infixing rules to English words.} para maiwasan mo ang mga masamang pangyayari (Tagalog: When traffic is busy, be careful while driving to avoid accidents).
    
\end{itemize}


The higher end of this scale reflects more complex code-mixing. For example, code-mixing with loanwords is arguably less challenging insofar as they are often used in a monolingual context to begin with. Likewise, code-mixing a single entity is not as complex as there is presumably a correspondence between the word forms in the two languages. However, code-mixing beyond the entity level, especially in cases like intraword code-mixing in the Tagalog-English example, is more difficult and requires a good grasp of the morphosyntactic structures of both languages.

As an additional quality check, native speakers were also tasked to flag outputs suffering from semantic inaccuracies or fluency issues. 

\subsubsection*{Accurateness} 
Additionally, we annotate the outputs' \textit{accurateness} to account for task failure or the generation of incorrect explanations. Accurateness annotation is binary, where inaccurate outputs were defined by: 
\begin{itemize}
    \item \textbf{Failure to follow instructions:} Some LLMs fail to accurately carry out the prompt instruction (for example, generating monolingual sentences). We also observe some code-mixed outputs that include additional languages unspecified in the prompts.
    \item \textbf{Inaccurate explanations:} ChatGPT, among other LLMs, tends to explain the code-mixed phrases, but occasionally provides wrong or nonfactual explanations (see Figure~\ref{fig:wrong-explanations}).
\end{itemize}


\section{Results}

%%%%%%% RESULTS %%%%%%%%%
\subsection{English-SEA Languages}
Figure~\ref{fig:plot-all-models} demonstrates that ChatGPT outperforms other language models in generating code-mixed data across five different language pairs, especially in its capacity to code-mix beyond topic-related entities (3 on the scale). For InstructGPT, the davinci-003 model demonstrates some capability to generate code-mixed texts, whereas davinci-002 is only able to code-mix with loanwords. Conversely, the other two publicly available multilingual LLMs perform poorly on the generation task. More specifically, while Flan-T5-XXL can code-mix with loanwords for the Indonesian-English language pair, most of its non-English monolingual outputs suffer from severe fluency issues (see Appendix~\ref{app:flant5-fluency}). BLOOMZ fails to produce code-mixed text despite its multilingual pretraining data ROOTS \cite{lauren2022roots} and instruction-tuning data xP3 \cite{muennighoff2022bloomz}, both of which cover Indonesian, Chinese, and Vietnamese.

%%%%% example %%%%
\begin{figure}[!t]
\centering
\scriptsize
\begin{leftbubbles}
Imagine there are two English-Vietnamese bilingual speakers talking to each other, can you imagine a conversation between them talking about family?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} Person A: Hi, how's your family doing?\\Person B: Chào bạn, gia đình tôi đang khỏe. Còn gia đình bạn thì sao?
\\Person A: They're good, thanks for asking. How about yours?
\\Person B: Gia đình tôi cũng khỏe, cảm ơn bạn. Mới đây, tôi đã đi chơi với ông bà và các em họ của mình.
\end{rightbubbles}
\hrule

\begin{leftbubbles}
Speaker 1: Yesterday raining cats and dogs, today like oven temperature. So shiok ah, Singapore weather.
\end{leftbubbles}
\begin{rightbubbles}
Speaker 2: Ya loh, very sian one. But nevermind lah, just drink more water and stay hydrated can already.
\end{rightbubbles}
\caption{Failure cases when we prompt ChatGPT to imagine two bilingual speakers talking about a specific topic.}
\label{fig:failure-two-bilingual-speakers}
\end{figure}

We further break down the performance of the best two models, ChatGPT and InstructGPT (davinci-003) in Figure~\ref{fig:chatgpt-davinci-003-breakdown}.\footnote{Detailed analysis for davinci-002, Flan-T5-XXL and BLOOMZ can be found in the Appendix (Figure~\ref{fig:davinci-002}, Figure~\ref{fig:flant5xxl}, and Figure~\ref{fig:bloomz}).} In Figure~\ref{fig:chatgpt-davinci-003-breakdown}(a) and Figure~\ref{fig:chatgpt-davinci-003-breakdown}(d), we see that while both ChatGPT and davinci-003 share similar performances in generating sentences with code-mixed topic-related entities, ChatGPT fares better at code-mixing beyond entity levels through mixing verb and adverbial phrases from another language. Furthermore, ChatGPT frequently generates translations and explanations for the code-mixed outputs, as illustrated in Figure~\ref{fig:code-mixing-prompt-templates}(a) and Figure~\ref{fig:code-mixing-prompt-templates}(f). %We find that ChatGPT tends to use AI-related English loanwords such as "Artificial Intelligence" and food-related Indonesian loanwords such as "nasi lemak" for English-Indonesian code-mixed sentences. \todo{(Tagalog)} 

In general, both ChatGPT and davinci-003 are capable of using either English or a SEA language as the matrix language, which is defined as the main language of a sentence per Myers-Scotton’s Matrix Language Frame model \citep{myers1997duelling}. However, we observe fluency issues with the generated code-mixed sentences. For instance, in Figure~\ref{fig:code-mixing-prompt-templates}(d), the phrase "炸鸡 (zhá jī) fried chicken", which translates into "fried chicken fried chicken", would not occur in a natural-sounding English-Chinese code-mixed sentence.

Figure~\ref{fig:chatgpt-davinci-003-breakdown}(b) and Figure~\ref{fig:chatgpt-davinci-003-breakdown}(e) present the code-mixedness according to different topics. We observe that both ChatGPT and davinci-003 tend to code-mix with loanwords when the topic is about "AI" by mixing the English loanwords "Artificial Intelligence." For food, both models tend to code-mix with food-related terms—which are topic-related entities—in SEA languages such as "bánh mì". We also observe some representative biases in certain language-topic pairs. For instance, when it comes to food, ChatGPT mentions the word "nasi goreng" for all English-Indonesian responses. For other topics such as traffic and weather, both models show the propensity to code-mix phrases related to traffic congestion and hot weather. We believe a larger scale of data collection is warranted to measure the effects of representative issues when zero-shot prompting LLMs for code-mixed data.


%%%%%%% END EXAMPLE %%%%%%%%%%%

In Figure~\ref{fig:chatgpt-davinci-003-breakdown}(f), we find that the prompt template that returns the highest quality results is the one where the term code-mixing is explicitly defined. Doing so gives ChatGPT the ability to produce code-mixed texts across different languages and topics a 68\% success rate to reach code-mixing beyond entities. In contrast, the worst-performing template consists of asking the model to imagine two bilingual speakers conversing, where the term code-mixing is not mentioned. In Figure~\ref{fig:failure-two-bilingual-speakers}, we see that ChatGPT may generate unnatural conversations where one interlocutor speaks in English and the other speaks in either Malay, Indonesian, Chinese, Tagalog, or Vietnamese exclusively (top example). Furthermore, ChatGPT may assume there are four speakers even though the prompt asks for a conversation between two speakers (bottom example).

\subsection{Singlish}
\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{assets/singlish.pdf}
\caption{Models' performance in generating Singlish expressions through zero-shot prompting.}
\label{fig:singlish-all-models}
\end{figure}

%%%%% example %%%%
\begin{figure}[!ht]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are a Singaporean speaker. How would you talk about food using Singlish in a single sentence?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} "Wah, the hawker center got so many sedap food, like chicken rice, laksa, and rojak, can die for sia!" (Translation: "Wow, the hawker center has so many delicious foods, like chicken rice, laksa, and rojak, they are to die for!")
\end{rightbubbles}
\caption{Singlish sentence and its translation to Standard English, both generated by ChatGPT.}
\label{fig:singlish-translations}
\end{figure}
%%%%%%% END EXAMPLE %%%%%%%%%%%

Given that Singlish is an English-based creole language that has lexicalized a lot of words from many languages such as Malay, Chinese, Tamil, Cantonese, and Hokkien, it is difficult to disambiguate loanwords and code-mixing beyond entities (such as Hokkien expressions "lah", "leh", and "loh"). Therefore, we classify the model success based on whether the generated sentences contain Singlish expressions (e.g. interjections, lexicalized phrases of languages without standardized writing system and etc\footnote{A reference list can be seen at \url{https://en.wikipedia.org/wiki/Singlish_vocabulary}}). In Figure~\ref{fig:singlish-all-models}, we see that ChatGPT and InstructGPT (davinci-003) have up to a 96\% success rate in generating Singlish sentences, whereas Flan-T5-XXL and BLOOMZ have a near-zero success rate. Furthermore, we find that ChatGPT is also capable of translating Singlish expressions into Standard English expressions (Figure~\ref{fig:singlish-translations}). 

OpenAI's documentation of model differences\footnote{\url{https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-text-davinci-003-differ}}---more specifically, that davinci-003 can produce higher-quality writing, process more complex instructions, and generate longer content than davinci-002---does not adequately explain why ChatGPT and davinci-003 significantly outperform davinci-002 and other multilingual LLMs in generating Singlish text. We hypothesize that the performance gap may be due to the larger presence of Singlish in OpenAI's training data. Singlish is not only one of the most well-researched non-standard Englishes \citep{sin2017}, but also uses the English alphabet and Latin script systems (unlike many other SEA languages), which potentially makes it easier for LLMs to scrape and parse data in this language. 
% However, we note that it is often difficult to disambiguate Singlish from a code-switch containing Singlish \citep{leimgruber2012}, as Singlish has lexicalized many words from other languages.

Despite ChatGPT's and davinci-003's ability to generate Singlish sentences, we notice semantic inaccuracies in their word choices. In Figure~\ref{fig:code-mixing-prompt-templates}(c), ChatGPT outputs Singlish expressions "lah" and "very important one leh" as well as the (British) English-Malay code-mixed phrase "sotong and chilli sauce" where "sotong" is a Malay word for "squid". Upon first inspection, the sentence appears syntactically sound but  native speakers found the analogy between familial connections and the dish (squid with chilli sauce) to be semantically confusing.  


\subsection{Accurateness}
We also notice that ChatGPT may fail to produce correctly code-mixed text when the nationality of the speaker is mentioned. Invoking nationality may bias ChatGPT to generate code-mixed texts with an \textit{additional} national language of the country (Figure~\ref{fig:failure-nationality-translations}, top example). In the example, because the national language of Malaysia is Malay, ChatGPT not only returns a code-mixed response that includes the Malay word "teruk" when prompted for English-Chinese utterance by a Malaysian, but also misclassifies the word "teruk" as Chinese.


%%%%% example %%%%
\begin{figure}[!t]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are a Malaysian speaker. How would you talk about traffic using both English and Chinese in a single sentence?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} Assuming that you are asking for a sentence that combines both English and Chinese in a natural and commonly used way by Malaysian speakers, here is an example:
\\"今天的~traffic~很~\ul{teruk}，我要赶紧走去~KLCC for meeting。" 
\\This sentence incorporates the English word "traffic" and \ul{the Chinese word "teruk"} (which means "bad" or "terrible" in English) to describe the situation of the traffic being very bad. The speaker then switches back to English to express their intention to hurry to KLCC for a meeting. This kind of code-switching is common in Malaysia, where speakers often mix languages in their daily conversations. 
\end{rightbubbles}
% \hrule
% \begin{leftbubbles}
% Imagine that you are a Filipino speaker. How would you talk about family using both English and Tagalog in a single sentence?
% \end{leftbubbles}
% \begin{rightbubbles}
% \textbf{(ChatGPT)} \ul{Sa aking pamilya, ang aking mga magulang at kapatid ay napakabait at mahal nila ako. (In my family, my parents and siblings are very kind and they love me.)} 
% \end{rightbubbles}
\caption{We underline the text that shows ChatGPT fails to follow the prompt. }
\label{fig:failure-nationality-translations}
\end{figure}
%%%%%%% END EXAMPLE %%%%%%%%%%%

While ChatGPT can explain how its generated text is code-mixed, we find its justifications unreliable for two reasons. First, it may hallucinate words that do not exist in the code-mixed text. For instance, in the top example in Figure~\ref{fig:wrong-explanations}, the English word "cold" does not exist in the code-mixed text. Second, it may assign words or phrases to the wrong language. In the bottom example in Figure~\ref{fig:wrong-explanations}, the phrase "nak minum air sejuk je" is in Malay, not English. This observation corroborates \citeposs{bang2023multitask} finding that suggests ChatGPT performs poorly on language identification tasks for low-resource languages.

%%%%%% EXAMPLE %%%%%%%
\begin{figure}[!ht]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are an Indonesian speaker. How would you talk about weather using both English and Indonesian in a single sentence? 
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} If I were to talk about weather using both English and Indonesian in a single sentence, I might say something like: "\textit{Hari ini sangat hot dan panas sekali, saya rasa aku butuh minum air dingin.}" \ul{This sentence includes the English words "hot" and "cold"} to describe the weather, and the Indonesian words "panas" and "dingin" which have similar meanings. Additionally, the sentence also includes a personal statement in Indonesian, "saya rasa aku butuh minum air dingin," which means "I feel like I need to drink cold water."
\end{rightbubbles}
\hrule
\begin{leftbubbles}
Code-mixing refers to a phenomenon of combining two or more languages in a single sentence. Can you generate a code-mixed English-Malay sentence about weather?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} "\textit{The weather hari ni sangat panas, I rasa macam nak minum air sejuk je.}" (Translation: "The weather today is very hot, I feel like drinking cold water.")
\\In this sentence, "hari ni" and "sangat panas" are in Malay, while "I" and \ul{"nak minum air sejuk je" are in English.}
\end{rightbubbles}
\caption{Underlined texts show wrong explanations about the code-mixed text. We italicize the \textit{code-mixed sentences} to make it explicit to the reader.}
\label{fig:wrong-explanations}
\end{figure}

\section{Discussion}

In this section, we walk through the various implications of our work for understanding the deployment of LLMs for synthetic data generation in low-resource settings, multilinguality in LLMs, research transparency, and multimodal applications.

\paragraph{Deploying LLMs for Low-Resourced Data Generation} By putting LLMs' generative capabilities to test, we ask in this work if they could generate high-quality and low-cost code-mixed texts for researchers working on a topic plagued by limited data availability. While we conclude that models such as ChatGPT and InstructGPT have shown relative success in generating code-mixed texts for SEA languages when prompted in particular zero-shot manners, we ask researchers to exercise caution when using this data generation technique. Even for Singlish, which outperforms the other languages examined, we find that syntactically-sound responses frequently contain word choice errors and semantic inaccuracies. For non-native speakers, such fluency deviations may be difficult to detect. Due to this lack of reliability, we strongly suggest researchers who wish to pursue this method of data generation implement extensive human checks with native speakers. 

\paragraph{Multilingual ≠ Code-Mix Compatible} The inability of models such as BLOOMZ and Flan-T5-XXL in producing code-mixed data demonstrates that code-mixing is not recognized as an essential component of many multilingual LLMs today. In other words, for some models, multilinguality simply means the system can process tasks and generate outputs in multiple languages, but not in the same sentence. By highlighting this limitation, we compel researchers to consider code-mixing as a core feature of many people's linguistics repertoire across the world. Building LLMs that include code-mixing not only allows NLP researchers to more accurately capture the dynamic elements of many languages, but it also helps improve one's understanding of tone, formality, and other cultural aspects embedded in conversations. Finally, we hope that by centering \textit{true} multilinguality, that is, multilinguality that goes beyond language inclusion and encompassing language-\textit{mixing}, we can develop representative and equitable LLMs that cater to the needs of underserved linguistic communities across the world.

\paragraph{Research Transparency} Aside from showing that ChatGPT and InstructGPT's \textit{can} code-mix, we cannot confidently identify \textit{how} the models do so due to the lack of transparency in how these systems are developed. Without a window into the kind of training data and engineering processes that went into models like ChatGPT, we can only speculate that they were not only fed multilingual texts but were also explicitly trained to handle code-mixing. To help facilitate greater levels of transparency and accountability, we urge forthcoming models that have code-mixing capabilities to be more open about how the models were developed.

\paragraph{Beyond Written Code-Mixing} 
As the LLM landscape continues to broaden, we anticipate that emerging models will likely be increasingly multimodal. Text-based LLMs that serve as conversational agents, the likes of ChatGPT, will likely start to include speech or visual units (e.g., GPT-4). Owing to the prevalence of code-mixing in vocal speech, we believe that including the ability to recognize and generate code-mixed outputs in LLMs will provide a wide range of downstream benefits. For instance, voice assistants supported by code-mixing-friendly automatic speech recognition (ASR) systems could help people communicate in ways that are potentially more naturalistic and authentic to their self-expression. Removing the need for people to adjust their speech patterns to become legible to machines could further ameliorate the effects of linguistics profiling \cite{baugh2005linguistic} and Western-centrism technological designs.

\section{Related Work}
\paragraph{Code-Mixed Data in SEA} Unlike monolingual data, there is only a limited number of human-curated code-mixed data. This resource limitation is more severe in SEA due to its historical lack of representation~\cite{winata2022decades}. Existing code-mixing studies in SEA cover several language pairs, i.e., English-Tagalog~\cite{oco-roxas-2012-pattern}, English-Indonesian~\cite{barik-etal-2019-normalization,yulianti2021emotcmt}, Javanese-Indonesian~\cite{tho2021cm-jv-id} and Chinese-English~\cite{lyu2010seame}\footnote{To exacerbate the situation, some of the SEA code-mixed datasets are no longer publicly available.}. The current corpus does not even scratch the surface of the sheer amount of code-mixedness in SEA~\cite{redmond2009-wl}, where deployable data is practically non-existent. In this work, we try to close this gap by exploring the potential of generating synthetic code-mixed data for the SEA region by way of LLMs.

\paragraph{Synthetic Code-Mixing} 
The effectiveness of using synthetically generated code-mixed data for improving the code-mixing capability of language models has been previously explored. \citet{winata2019code} and \citet{tan-joty-2021-code} have attempted to generate synthetic code-mixed sentences through word alignment and candidate selection from a parallel corpus.
%between matrix language (base language) and embedding language.
\citet{liu2020attention} and \citet{adilazuarda-etal-2022-indorobusta} have similarly generated synthetic code-mixed sentences by replacing words in a monolingual sentence with their machine-translated counterparts.
%by replacing words in matrix language with machine-translated ones in the embedded language. 
Despite the ability to produce sentences with high code-mixed index (CMI)~\cite{gambck2014cmi}, the generated sentences from these methods are inadequate in reproducing the naturalness of human-generated code-mixed utterances. In this work, we assess a novel way of generating synthetic code-mixed sentences without the need for parallel corpora or machine translation models through the prompting of LLMs, and provide a comprehensive analysis of such an approach. 
 

\section{Conclusion}
We demonstrate that ChatGPT and InstructGPT outperform other multilingual LLMs when prompted to generate code-mixed texts for South East Asian languages and that their performances are particularly noteworthy for Singlish. We also discover that publicly available multilingual LLMs such as BLOOMZ and Flan-T5-XXL are incapable of generating code-mixed data through zero-shot prompting. Nonetheless, we discover issues with accurateness, reliability, and fluency for code-mixed data generated by models such as ChatGPT. Therefore, we caution against using LLM-generated synthetic code-mixed data without the involvement of native speakers for annotating and editing.

\section{Limitations}
\subsection{Sample Size}
Our observations were limited to 180 prompts across five topics and six SEA languages. With the recent release of ChatGPT API, our immediate next step is to automatically generate more code-mixed samples and present a more robust and generalizable conclusion about ChatGPT's ability to generate synthetic code-mixed data.

\subsection{Language Models}
Our work only covers instruction-tuned language models. In future work, we will include a comparison between multilingual models that are not finetuned with instructions---for example, GPT3 (davinci) \cite{brown2020gpt3} and BLOOM \cite{scao2022bloom}---to explore the effects of instruction tuning in generating code-mixed data.

\subsection{Evaluation} 
While we annotated the degree of code-mixedness and checked for semantic or fluency issues, we were unable to systematically compare LLM-generated outputs against human-generated code-mixed data. In future efforts, the involvement of native speakers in such tasks is key in ensuring adequate fluency and naturalness levels. We can also further analyze the data with automated metrics such as CMI and M-/I-index, which requires human efforts in annotating each word token with the most accurate language labels.

\subsection{Prompt Templates}
Our study only uses prompt templates written in English to prompt language models in a zero-shot manner. In future follow-ups, we will (1) use code-mixed prompt templates such as "Generate an English-Bahasa sentence" instead of "Generate an English-Malay sentence" and (2) investigate LLMs' capability in generating code-mixed data with in-context few-shot examples. 

\subsection{Languages}
Our study focuses on generating code-mixed data for English-SEA language pairs. For future studies, we plan to investigate generating code-mixed data for non-English language pairs, such as Malay-Chinese and Indonesian-Javanese, and more SEA languages, such as Thai and Burmese. 


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Languages Spoken in SEA}
\label{sec:Languages}
There are more than  1,200 languages spoken in SEA~\cite{redmond2009-wl,maliwat2021}, 700 of which are spoken in Indonesia~\cite{aji-etal-2022-one,cahyawijaya2023nusacrowd}. 
We describe the languages the SEA languages used in the study in the following paragraphs.

%\paragraph{Burmese} Burmese is the national language of Myanmar.

\paragraph{Mandarin Chinese} Mandarin Chinese language (zh-Hans), which belongs to the Sino-Tibetan language family and uses the Hanzi script, is widely spoken in SEA due to the migration of Chinese people from the coastal provinces of southeastern China, such as Fujian, Guangdong, and Hainan. People of Chinese heritage in SEA frequently use the term "华人" (huá rén) to express their cultural identity as an ethnic group, instead of "中国人" (zhōng guó rén) which is primarily associated with nationality, even though both terms can be translated as "Chinese (people)." Singapore has the largest Chinese ethnic group among all SEA countries and Mandarin Chinese is considered one of the official languages in Singapore.

The language is characterized as linguistically "isolating" in that each Chinese character corresponds to one morpheme and that the language uses very little grammatical inflection. It uses a logographic writing system, which uses pictograms (Chinese characters) to represent meaning. Chinese is also a tonal language with four pitched tones and one neutral tone. It commonly displays a basic SVO word order and, instead of conjugating the verbs to express tenses, uses aspect particles such as 了 (le) and 着 (zhe) to indicate the temporal location of the sentence.


\paragraph{Indonesian}
Indonesian (ind) is the national language of Indonesia~\cite{indonesia2002undang}. It is spoken by around 300 million speakers worldwide. Indonesian is developed from the literary `Classical Malay’ of the Riau-Johor sultanate \cite{sneddon2003} and has many regional variants. Indonesian is written in Latin script with a lexical similarity of over 80\% to Standard Malay. Indonesian is non-tonal and has 19 consonants, 6 vowels, and 3 diphthongs. The stress is on the penultimate syllable and the word order is SVO. It has three optional noun classifiers. Indonesian has two social registers and a rich affixation system, including a variety of prefixes, suffixes, circumfixes, and reduplication. Most of the affixes in Indonesian are derivational~\cite{pisceldo-etal-2008-two}.


%\paragraph{Javanese}
%Javanese (jav) is a language spoken mainly on Java island. It is the de facto language of provincial identity in central and eastern Java. The word order is SVO. It has 21 consonants and 8 vowels.  Javanese used to be written in Javanese script but since 20th century is mostly written in Latin script. Javanese differs from most other languages of western Indonesia in contrasting dental and retroflex stops, and in the feature of breathy voice or murmur as a phonetic property of its voiced obstruents. Javanese also differs from most languages of the Philippines and western Indonesia in allowing a number of word-initial consonant clusters. It has an elaborate system of speech levels~\cite{blust2013austronesian}.

%\paragraph{Khmer} Khmer is the national language of Cambodia.

%\paragraph{Lao} Lao is the national language of Laos.

\paragraph{Standard Malay} 
Standard Malay (msa) is the national language of Malaysia, Brunei, and Singapore, and the language is spoken by approximately 290 million speakers worldwide. The word order of Standard Malay is SVO with four types of affixes, i.e., prefixes (awalan), suffixes (akhiran), circumfixes (apitan), and infixes (sisipan). Even though Standard Malay and Indonesian originate from the same Malay language and are mutually intelligible, they can differ in spelling and vocabulary. One example is loanwords. Due to the different colonial influences from the Dutch and British, Indonesian primarily absorbs Dutch loanwords whereas Malay absorbs English loanwords. Both languages can also differ in the meanings of the same written words, which are commonly referred to as interlingual homographs. For instance, "polisi" means "police" in Indonesian but "policy" in Standard Malay.

%\paragraph{Sundanese} 
%Sundanese (sun) is a language spoken mainly in the Banten and West Java provinces. It is the de facto language of provincial identity in western Java. The main dialects are Bogor (Krawang), Pringan, and Cirebon.  Sunanese is non-tonal and has 18 consonant and 7 vowel phonemes. The stress is on the penultimate syllable. Sundanese has elaborate coding of respect levels. Sundanese  was previously written in Arabic, Javanese, and Sundanese scripts, but it is written in Latin script since the middle of the 19th century. Sundanese is a predominantly SVO language. It has voice marking and incorporates some (optional) actor-verb agreement, i.e., number and person~\cite{kurniawan2013sundanese}.

\paragraph{Tagalog} Tagalog (tgl) is an Austronesian language spoken in the Philippines by around ~82 million native speakers. It is both agglutinative and pitch-accented, giving it rich and complex morphology \cite{kroeger1993phrase}. Tagalog's standardized form, known as \textit{Filipino}, is the country's official national language. The difference between Filipino and Tagalog is more sociopolitical than sociolinguistic: Commonwealth Act No. 184 of 1936 created a national committee whose purpose is to ``develop a national language.'' This resulted in the standardization of the Tagalog language into Filipino. In practice, Filipino is indistinguishable from Tagalog, albeit with the addition of letters f, j, c, x, and z, plus loanwords \cite{phgazette1936}.

%\paragraph{Tetum} Tetum is the national language of Timor-Leste.

%\paragraph{Thai} Thai (tha) is the national language of Thailand.

\paragraph{Vietnamese} Vietnamese (vie), the national language of Vietnam, is spoken by around 85 million people worldwide. It is a tonal language belonging to the Austroasiatic language family and uses accents to denote six distinctive tones. The sentence structure of Vietnamese displays the SVO word order, and due to heavy influence from Chinese, it also uses a rich set of classifiers that are required in the presence of quantifiers. For instance, instead of writing "bốn gà," which literally translates into "four chickens," it should be "bốn con gà" where "con" is a classifier for non-human animate things.

\paragraph{Singlish}
Singlish is a widely-used conversational language in Singapore. It is an English-based creole language that arose out of prolonged language contact between speakers of many different languages in the country, including Hokkien, Malay, Teochew, Cantonese, and Tamil. Singlish is spoken by around 4 million speakers, and one unique feature of the language is its heavy use of pragmatic particles borrowed from Southern Chinese dialects. One example of this is "lah," which in the sentence, "Her dress is too short lah," emphasizes the statement.  


\section{HuggingFace Inference API}
\label{app:hf-api}
We use HuggingFace's Inference API to prompt multilingual LLMs since we do not have sufficient local compute to host models with hundreds of billions of parameters. The text-to-text task is treated identically as a text-generation task, and we set \texttt{max\_new\_tokens} (amount of new tokens to be generated) to 100, \texttt{temperature} to 0.7, and \texttt{repetition\_penalty} to 1.2. 

\section{OpenAI Inference API}

We use OpenAI's official API to prompt both davinci-003 and davinci-002. Specifically, we use \texttt{openai.Completion.create} with a maximum generation length of 128. We use the default values for all other parameters.

\section{Flan-T5-XXL Non-English Outputs}
\label{app:flant5-fluency}
We observe that when Flan-T5-XXL generates non-English outputs, most of them are nonsensical. Here are some of the examples and their translations. 

\noindent \textbf{Indonesian}: Ini adalah sebuah udara untuk pengobatan minyak dan di sekitar kehidupan. 

\noindent \textit{Translation: This is an air for oil treatment and around life.}

\noindent \textbf{Malay}:  Artificial Intelligence adalah sebuah kantor keamanan yang digunakan untuk mengidentifikasi penduduk yang memiliki anak-anak dalam diri. 

\noindent \textit{Translation: Artificial intelligence is a security office used for identifying residents who have childen inside.}

\noindent \textbf{Tagalog}: Weather niya ang nagsimula sa pagsasagawa ng kaniyang kargahan ng panahon.

\noindent \textit{Translation: It was his weather that started carrying out his weather load.}

\noindent \textbf{Vietnamese}: Nhà ng tài ra mt ngi dy xut trn o trng h nhng ngi  ng thng u c thit v.

\noindent \textit{Translation: The artist has created an outstanding talent in the field of talented people.}

\begin{figure*}[htp]
\centering
\includegraphics[width=\textwidth]{assets/subplots_davinci002.pdf}
\caption{Analysis of davinci-002's capability of generating code-mixed data.}
\label{fig:davinci-002}
\end{figure*}

\begin{figure*}[htp]
\centering
\includegraphics[width=\textwidth]{assets/subplots_bloomz.pdf}
\caption{Analysis of BLOOMZ's capability of generating code-mixed data.}
\label{fig:bloomz}
\end{figure*}

\begin{figure*}[htp]
\centering
\includegraphics[width=\textwidth]{assets/subplots_flant5xxl.pdf}
\caption{Analysis of Flan-T5-XXL's capability of generating code-mixed data.}
\label{fig:flant5xxl}
\end{figure*}

\end{CJK*}
\end{document}
