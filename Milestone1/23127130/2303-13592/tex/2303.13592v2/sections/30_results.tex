\section{Results}

%%%%%%% RESULTS %%%%%%%%%
\subsection{English-SEA Languages}
Figure~\ref{fig:plot-all-models} demonstrates that ChatGPT outperforms other language models in generating code-mixed data across five different language pairs, especially in its capacity to code-mix beyond topic-related entities (3 on the scale). For InstructGPT, the davinci-003 model demonstrates some capability to generate code-mixed texts, whereas davinci-002 is only able to code-mix with loanwords. Conversely, the other two publicly available multilingual LLMs perform poorly on the generation task. More specifically, while Flan-T5-XXL can code-mix with loanwords for the Indonesian-English language pair, most of its non-English monolingual outputs suffer from severe fluency issues (see Appendix~\ref{app:flant5-fluency}). BLOOMZ fails to produce code-mixed text despite its multilingual pretraining data ROOTS \cite{lauren2022roots} and instruction-tuning data xP3 \cite{muennighoff2022bloomz}, both of which cover Indonesian, Chinese, and Vietnamese.

%%%%% example %%%%
\begin{figure}[!t]
\centering
\scriptsize
\begin{leftbubbles}
Imagine there are two English-Vietnamese bilingual speakers talking to each other, can you imagine a conversation between them talking about family?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} Person A: Hi, how's your family doing?\\Person B: Chào bạn, gia đình tôi đang khỏe. Còn gia đình bạn thì sao?
\\Person A: They're good, thanks for asking. How about yours?
\\Person B: Gia đình tôi cũng khỏe, cảm ơn bạn. Mới đây, tôi đã đi chơi với ông bà và các em họ của mình.
\end{rightbubbles}
\hrule
% \begin{leftbubbles}
% Imagine there are two English-Chinese bilingual speakers talking to each other, can you imagine a conversation between them talking about weather?
% \end{leftbubbles}
% \begin{rightbubbles}
% \textbf{(ChatGPT)} English Speaker 1: Hey, have you been outside today? It's really hot!
% \\Chinese Speaker 1: 是啊，今天天气真的很热。
% \\English Speaker 2: Yeah, it's been pretty sunny all day.
% \\Chinese Speaker 2: 是啊，今天一整天都很晴朗。
% \end{rightbubbles}

\begin{leftbubbles}
Speaker 1: Yesterday raining cats and dogs, today like oven temperature. So shiok ah, Singapore weather.
\end{leftbubbles}
\begin{rightbubbles}
Speaker 2: Ya loh, very sian one. But nevermind lah, just drink more water and stay hydrated can already.
\end{rightbubbles}
\caption{Failure cases when we prompt ChatGPT to imagine two bilingual speakers talking about a specific topic.}
\label{fig:failure-two-bilingual-speakers}
\end{figure}

We further break down the performance of the best two models, ChatGPT and InstructGPT (davinci-003) in Figure~\ref{fig:chatgpt-davinci-003-breakdown}.\footnote{Detailed analysis for davinci-002, Flan-T5-XXL and BLOOMZ can be found in the Appendix (Figure~\ref{fig:davinci-002}, Figure~\ref{fig:flant5xxl}, and Figure~\ref{fig:bloomz}).} In Figure~\ref{fig:chatgpt-davinci-003-breakdown}(a) and Figure~\ref{fig:chatgpt-davinci-003-breakdown}(d), we see that while both ChatGPT and davinci-003 share similar performances in generating sentences with code-mixed topic-related entities, ChatGPT fares better at code-mixing beyond entity levels through mixing verb and adverbial phrases from another language. Furthermore, ChatGPT frequently generates translations and explanations for the code-mixed outputs, as illustrated in Figure~\ref{fig:code-mixing-prompt-templates}(a) and Figure~\ref{fig:code-mixing-prompt-templates}(f). %We find that ChatGPT tends to use AI-related English loanwords such as "Artificial Intelligence" and food-related Indonesian loanwords such as "nasi lemak" for English-Indonesian code-mixed sentences. \todo{(Tagalog)} 

In general, both ChatGPT and davinci-003 are capable of using either English or a SEA language as the matrix language, which is defined as the main language of a sentence per Myers-Scotton’s Matrix Language Frame model \citep{myers1997duelling}. However, we observe fluency issues with the generated code-mixed sentences. For instance, in Figure~\ref{fig:code-mixing-prompt-templates}(d), the phrase "炸鸡 (zhá jī) fried chicken", which translates into "fried chicken fried chicken", would not occur in a natural-sounding English-Chinese code-mixed sentence.

Figure~\ref{fig:chatgpt-davinci-003-breakdown}(b) and Figure~\ref{fig:chatgpt-davinci-003-breakdown}(e) present the code-mixedness according to different topics. We observe that both ChatGPT and davinci-003 tend to code-mix with loanwords when the topic is about "AI" by mixing the English loanwords "Artificial Intelligence." For food, both models tend to code-mix with food-related terms—which are topic-related entities—in SEA languages such as "bánh mì". We also observe some representative biases in certain language-topic pairs. For instance, when it comes to food, ChatGPT mentions the word "nasi goreng" for all English-Indonesian responses. For other topics such as traffic and weather, both models show the propensity to code-mix phrases related to traffic congestion and hot weather. We believe a larger scale of data collection is warranted to measure the effects of representative issues when zero-shot prompting LLMs for code-mixed data.


%%%%%%% END EXAMPLE %%%%%%%%%%%

In Figure~\ref{fig:chatgpt-davinci-003-breakdown}(f), we find that the prompt template that returns the highest quality results is the one where the term code-mixing is explicitly defined. Doing so gives ChatGPT the ability to produce code-mixed texts across different languages and topics a 68\% success rate to reach code-mixing beyond entities. In contrast, the worst-performing template consists of asking the model to imagine two bilingual speakers conversing, where the term code-mixing is not mentioned. In Figure~\ref{fig:failure-two-bilingual-speakers}, we see that ChatGPT may generate unnatural conversations where one interlocutor speaks in English and the other speaks in either Malay, Indonesian, Chinese, Tagalog, or Vietnamese exclusively (top example). Furthermore, ChatGPT may assume there are four speakers even though the prompt asks for a conversation between two speakers (bottom example).

\subsection{Singlish}
\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{assets/singlish.pdf}
\caption{Models' performance in generating Singlish expressions through zero-shot prompting.}
\label{fig:singlish-all-models}
\end{figure}

%%%%% example %%%%
\begin{figure}[!ht]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are a Singaporean speaker. How would you talk about food using Singlish in a single sentence?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} "Wah, the hawker center got so many sedap food, like chicken rice, laksa, and rojak, can die for sia!" (Translation: "Wow, the hawker center has so many delicious foods, like chicken rice, laksa, and rojak, they are to die for!")
\end{rightbubbles}
\caption{Singlish sentence and its translation to Standard English, both generated by ChatGPT.}
\label{fig:singlish-translations}
\end{figure}
%%%%%%% END EXAMPLE %%%%%%%%%%%

Given that Singlish is an English-based creole language that has lexicalized a lot of words from many languages such as Malay, Chinese, Tamil, Cantonese, and Hokkien, it is difficult to disambiguate loanwords and code-mixing beyond entities (such as Hokkien expressions "lah", "leh", and "loh"). Therefore, we classify the model success based on whether the generated sentences contain Singlish expressions (e.g. interjections, lexicalized phrases of languages without standardized writing system and etc\footnote{A reference list can be seen at \url{https://en.wikipedia.org/wiki/Singlish_vocabulary}}). In Figure~\ref{fig:singlish-all-models}, we see that ChatGPT and InstructGPT (davinci-003) have up to a 96\% success rate in generating Singlish sentences, whereas Flan-T5-XXL and BLOOMZ have a near-zero success rate. Furthermore, we find that ChatGPT is also capable of translating Singlish expressions into Standard English expressions (Figure~\ref{fig:singlish-translations}). 

OpenAI's documentation of model differences\footnote{\url{https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-text-davinci-003-differ}}---more specifically, that davinci-003 can produce higher-quality writing, process more complex instructions, and generate longer content than davinci-002---does not adequately explain why ChatGPT and davinci-003 significantly outperform davinci-002 and other multilingual LLMs in generating Singlish text. We hypothesize that the performance gap may be due to the larger presence of Singlish in OpenAI's training data. Singlish is not only one of the most well-researched non-standard Englishes \citep{sin2017}, but also uses the English alphabet and Latin script systems (unlike many other SEA languages), which potentially makes it easier for LLMs to scrape and parse data in this language. 
% However, we note that it is often difficult to disambiguate Singlish from a code-switch containing Singlish \citep{leimgruber2012}, as Singlish has lexicalized many words from other languages.

Despite ChatGPT's and davinci-003's ability to generate Singlish sentences, we notice semantic inaccuracies in their word choices. In Figure~\ref{fig:code-mixing-prompt-templates}(c), ChatGPT outputs Singlish expressions "lah" and "very important one leh" as well as the (British) English-Malay code-mixed phrase "sotong and chilli sauce" where "sotong" is a Malay word for "squid". Upon first inspection, the sentence appears syntactically sound but  native speakers found the analogy between familial connections and the dish (squid with chilli sauce) to be semantically confusing.  


\subsection{Accurateness}
We also notice that ChatGPT may fail to produce correctly code-mixed text when the nationality of the speaker is mentioned. Invoking nationality may bias ChatGPT to generate code-mixed texts with an \textit{additional} national language of the country (Figure~\ref{fig:failure-nationality-translations}, top example). In the example, because the national language of Malaysia is Malay, ChatGPT not only returns a code-mixed response that includes the Malay word "teruk" when prompted for English-Chinese utterance by a Malaysian, but also misclassifies the word "teruk" as Chinese.


%%%%% example %%%%
\begin{figure}[!t]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are a Malaysian speaker. How would you talk about traffic using both English and Chinese in a single sentence?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} Assuming that you are asking for a sentence that combines both English and Chinese in a natural and commonly used way by Malaysian speakers, here is an example:
\\"今天的~traffic~很~\ul{teruk}，我要赶紧走去~KLCC for meeting。" 
\\This sentence incorporates the English word "traffic" and \ul{the Chinese word "teruk"} (which means "bad" or "terrible" in English) to describe the situation of the traffic being very bad. The speaker then switches back to English to express their intention to hurry to KLCC for a meeting. This kind of code-switching is common in Malaysia, where speakers often mix languages in their daily conversations. 
\end{rightbubbles}
% \hrule
% \begin{leftbubbles}
% Imagine that you are a Filipino speaker. How would you talk about family using both English and Tagalog in a single sentence?
% \end{leftbubbles}
% \begin{rightbubbles}
% \textbf{(ChatGPT)} \ul{Sa aking pamilya, ang aking mga magulang at kapatid ay napakabait at mahal nila ako. (In my family, my parents and siblings are very kind and they love me.)} 
% \end{rightbubbles}
\caption{We underline the text that shows ChatGPT fails to follow the prompt. }
\label{fig:failure-nationality-translations}
\end{figure}
%%%%%%% END EXAMPLE %%%%%%%%%%%

While ChatGPT can explain how its generated text is code-mixed, we find its justifications unreliable for two reasons. First, it may hallucinate words that do not exist in the code-mixed text. For instance, in the top example in Figure~\ref{fig:wrong-explanations}, the English word "cold" does not exist in the code-mixed text. Second, it may assign words or phrases to the wrong language. In the bottom example in Figure~\ref{fig:wrong-explanations}, the phrase "nak minum air sejuk je" is in Malay, not English. This observation corroborates \citeposs{bang2023multitask} finding that suggests ChatGPT performs poorly on language identification tasks for low-resource languages.

%%%%%% EXAMPLE %%%%%%%
\begin{figure}[!ht]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are an Indonesian speaker. How would you talk about weather using both English and Indonesian in a single sentence? 
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} If I were to talk about weather using both English and Indonesian in a single sentence, I might say something like: "\textit{Hari ini sangat hot dan panas sekali, saya rasa aku butuh minum air dingin.}" \ul{This sentence includes the English words "hot" and "cold"} to describe the weather, and the Indonesian words "panas" and "dingin" which have similar meanings. Additionally, the sentence also includes a personal statement in Indonesian, "saya rasa aku butuh minum air dingin," which means "I feel like I need to drink cold water."
\end{rightbubbles}
\hrule
\begin{leftbubbles}
Code-mixing refers to a phenomenon of combining two or more languages in a single sentence. Can you generate a code-mixed English-Malay sentence about weather?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} "\textit{The weather hari ni sangat panas, I rasa macam nak minum air sejuk je.}" (Translation: "The weather today is very hot, I feel like drinking cold water.")
\\In this sentence, "hari ni" and "sangat panas" are in Malay, while "I" and \ul{"nak minum air sejuk je" are in English.}
\end{rightbubbles}
\caption{Underlined texts show wrong explanations about the code-mixed text. We italicize the \textit{code-mixed sentences} to make it explicit to the reader.}
\label{fig:wrong-explanations}
\end{figure}

