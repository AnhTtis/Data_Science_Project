\section{Methodology}
% \begin{figure*}[!ht]
% \small
% \begin{fullleftbubbles}
% Imagine that you are an Indonesian speaker. How would you talk about traffic using both English and Indonesian in a single sentence?
% \end{fullleftbubbles}

% \begin{fullrightbubbles}
% \textbf{(ChatGPT)} As an Indonesian speaker, I would say, "Kemacetan di jalan raya sangat parah, saya \ul{stuck} di sini karena \ul{traffic}nya sangat congested." which translates to "The traffic jam on the highway is very bad, I'm stuck here because the traffic is very congested."
% \end{fullrightbubbles}

% \begin{fullrightbubbles}
% \textbf{(InstructGPT)} \ul{Traffic}nya sangat padat, lalu lintasnya sangat macet.
% \end{fullrightbubbles}

% \begin{fullrightbubbles}
% \textbf{(GPT3)} I was stuck in traffic for two hours. Terpaksa berjam-jam di jalan karena macet.
% \end{fullrightbubbles}

% \begin{fullrightbubbles}
% \textbf{(BLOOMZ)} Traffic is heavy on the highway between Jakarta and Bogor at this time of day.
% \end{fullrightbubbles}

% \begin{fullrightbubbles}
% \textbf{(Flan-T5-XXL)} Para pengacara harus membuka kejadian di atas sel kebijakan.
% \end{fullrightbubbles}
% \caption{Responses from different language models (ChatGPT, InstructGPT (davinci-003), GPT-3 (davinci-002), BLOOMZ and Flan-T5-XXL) to the same task prompt for generating code-mixed text. \ul{Intrasentential and intraword code-mixing} are underlined. \todo{(different colored bubble box for different models)}}
% \end{figure*}

\begin{figure*}[!ht]
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_a.pdf}
		}
		\caption{Template: Assume as bilingual speaker}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_b.pdf}
		}
		\caption{Template: Two bilingual speakers}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_c.pdf}
		}
		\caption{Template: Imitate speaking style}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_d.pdf}
		}
		\caption{Template: Explicitly define CM}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_e.pdf}
		}
		\caption{Template: Native speaker}
	\end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_f.pdf}
		}
		\caption{Template: Write a CM sentence}
	\end{subfigure}
 \caption{Prompt templates with different \textcolor{blue}{languages} and \textcolor{orange}{topic} fields and responses from different LLMs containing code-mixed / non-code-mixed sentences. We also include a template where we specify the \textcolor{sky}{\ul{nationality}} of the speaker. Note that the explanations are a part of ChatGPT's original generation.
}
\label{fig:code-mixing-prompt-templates}
\end{figure*}


%%%% METHODOLOGY %%%%%%%
\subsection{Prompting Language Models}
We collect synthetic code-mixed data by prompting LLMs with natural language requests shown in Figure~\ref{fig:code-mixing-prompt-templates} along two axes: languages and topics (food, family, traffic, Artificial Intelligence, and weather).
Specifically, we explore ChatGPT, InstructGPT (davinci-002 and davinci-003) \cite{ouyang2022rlhf}, BLOOMZ \cite{muennighoff2022bloomz}, and Flan-T5-XXL \cite{chung2022flant5}. We use OpenAI and HuggingFace's API for prompting (see Appendix~\ref{app:hf-api}), except in the case of ChatGPT, for which we manually queried through the model's web interface\footnote{ChatGPT's API was not publicly released when we conducted this study.}. 


\begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{assets/overall_plot-2.pdf}
\caption{Comparison of performance of different LLMs in generating code-mixed data through zero-shot prompting. We distribute the result across different code-mixing levels: (1) Loanword, (2) Topic-related entities, and (3) Code-mixing beyond entity. }
\label{fig:plot-all-models}
\end{figure*}

In our prompts, we specify code-mixing between English with either Indonesian, Malay, Mandarin, Tagalog, or Vietnamese. We focused on code-mixing English with SEA languages for two reasons: (1) extensive literature on code-mixed English provides a relevant point of comparison, and (2) English is one of the most widely used languages in code-mixing across most SEA countries \cite{kirkpatrick2014english}. We additionally prompt sentences in Singlish, a creole language, to evaluate how sensitive LLMs are to the diversity of language practices in the SEA region. In total, we submitted 180 unique prompts per language model.

\begin{figure*}[!t]
\centering
\begin{subfigure}[b]{\textwidth}
     \centering
     \captionsetup{labelformat=empty}
     \caption{InstructGPT (davinci-003)}
     \includegraphics[width=\textwidth]{assets/subplots_davinci003.pdf}
     %\label{fig:davinci-003}
\end{subfigure}
\hfill
\begin{subfigure}[b]{\textwidth}
     \centering
     \captionsetup{labelformat=empty}
     \caption{ChatGPT}
     \includegraphics[width=\textwidth]{assets/subplots_chatgpt.pdf}
     %\label{fig:chatgpt}
\end{subfigure}
% \vspace{-1cm}
\caption{Analysis of code-mixed data generated by InstructGPT davinci-003 \textbf{(top)} and ChatGPT \textbf{(bottom)}.}
\label{fig:chatgpt-davinci-003-breakdown}
\end{figure*}

\subsection{Evaluation}

\subsubsection*{Code-Mixing Scale}
To evaluate the outputs, we ask whether LLMs are capable of producing \textit{intrasentential} code-mixed text. We adopt the definition of intrasentential code-mixing from \citet{berk1986linguistic}, which covers the mixing of small constituents---such as noun and verb phrases---and large constituents---such as coordinate clauses and prepositional phrases.  Native speakers are then tasked to manually annotated the collected responses on a scale from 0 to 3 using the following coding guideline to denote varying degrees of code-mixing:

\begin{itemize}
    \item \textbf{0 - No code-mixing:} The generated text is written purely in one language or only exhibits \textit{intersentential} code-mixing.
    \item \textbf{1 - Loanword usage:} The generated text uses loanwords for common terminologies. We define a foreign word as a loanword if the word is listed in Wiktionary\footnote{\url{https://en.wiktionary.org}}. For example: I like eating \textit{pho}.
    \item \textbf{2 - Topic-related entities:} The generated text mixes languages on entities/terms that are not considered loanwords—for example: 今天的 \textit{traffic} 真的很糟糕，我开了一个小时才到了办公室 (Chinese: The traffic today is really terrible. I spent an hour driving to get to the office).
    \item \textbf{3 - Beyond entity:} The generated text mixes languages beyond the entity level—for example: My family \textit{ay nagplano ng isang malaking} family reunion \textit{sa park} this coming weekend (Tagalog: My family has planned a big family reunion at the park this coming weekend). This category also includes intraword code-mixing, For example: Kapag busy ang trapiko, mag-ingat ka sa \textit{pagda-drive}\footnote{The prefix "pag-" in Tagalog is affixed to the English word "drive", resulting in the word "pagda-drive" (the act of driving). This example demonstrates the application of Tagalog infixing rules to English words.} para maiwasan mo ang mga masamang pangyayari (Tagalog: When traffic is busy, be careful while driving to avoid accidents).
    % \item \textbf{4 - Intraword:} The generated text combines both languages in a single word at the morpheme level. For example: Kapag busy ang trapiko, mag-ingat ka sa \textit{pagda-drive}\footnote{The prefix "pag-" in Tagalog is affixed to the English word "drive", resulting in the word "pagda-drive" (the act of driving). This example demonstrates the application of Tagalog infixing rules to English words.} para maiwasan mo ang mga masamang pangyayari!
    
\end{itemize}


The higher end of this scale reflects more complex code-mixing. For example, code-mixing with loanwords is arguably less challenging insofar as they are often used in a monolingual context to begin with. Likewise, code-mixing a single entity is not as complex as there is presumably a correspondence between the word forms in the two languages. However, code-mixing beyond the entity level, especially in cases like intraword code-mixing in the Tagalog-English example, is more difficult and requires a good grasp of the morphosyntactic structures of both languages.

As an additional quality check, native speakers were also tasked to flag outputs suffering from semantic inaccuracies or fluency issues. 

% We suggest that the higher end of the scale reflects more complex code-mixing. For example, code-mixing with loan words is arguably not difficult since they are often used in monolingual contexts. Similarly, code-mixing a single entity is not as complex, as one could use a dictionary to translate it and the grammar usually works out. However, code-mixing beyond the entity level (especially intraword code-mixing) is much more difficult, as the person must have a good grasp of the grammar of both languages.

\subsubsection*{Accurateness} 
Additionally, we annotate the outputs' \textit{accurateness} to account for task failure or the generation of incorrect explanations. Accurateness annotation is binary, where inaccurate outputs were defined by: 
\begin{itemize}
    \item \textbf{Failure to follow instructions:} Some LLMs fail to accurately carry out the prompt instruction (for example, generating monolingual sentences). We also observe some code-mixed outputs that include additional languages unspecified in the prompts.
    \item \textbf{Inaccurate explanations:} ChatGPT, among other LLMs, tends to explain the code-mixed phrases, but occasionally provides wrong or nonfactual explanations (see Figure~\ref{fig:wrong-explanations}).

    % \item \textbf{Wrong Language:} We note that some output produces incorrect languages.
    % \item \textbf{Empty output:} BLOOM and BLOOMZ often produce no output at all, which we marked as incorrect.
\end{itemize}


% We evaluated the collected text on the following criteria: 

% \paragraph{Correctness} We judged that the language model is not correct when it fails to follow the instructions. For instance, the response includes a language that is not specified in the prompt. 

% \paragraph{Code-mixed Category} The level of sophistication of the code-mixed output is categorized by whether the code-mixed words or phrases are as follows: 
% \begin{enumerate}
%    \item loanwords\footnote{We define loanwords as a word from a foreign language that can be found in the dictionary of the local language. For SEA foreign languages to English language, we use Wiktionary as the reference.} (e.g., Vietnamese dish \textit{pho})
%    \item topic-related entities (e.g., \textit{deep learning} when prompted about the AI topic)
%    \item beyond entity level (e.g., Keluarga saya suka \textit{spending quality time} bersama-sama setiap hari (Malay: My family likes spending quality time together everyday.))
%\end{enumerate}
%All the evaluations were carried out by native speakers of the SEA languages.

%We argue that the level of sophistication correlates with the cross-lingual generalization ability of the language model to code-mixing. Loanwords are usually present in the monolingual corpora, whereas code-mixing beyond the entity level requires the language model to be able to model the grammatical structure of the two code-mixing languages. \todo{(need more eyes on this paragraph.)}
