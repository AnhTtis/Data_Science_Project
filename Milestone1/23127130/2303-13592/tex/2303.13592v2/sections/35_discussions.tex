\section{Discussion}

In this section, we walk through the various implications of our work for understanding the deployment of LLMs for synthetic data generation in low-resource settings, multilinguality in LLMs, research transparency, and multimodal applications.

\paragraph{Deploying LLMs for Low-Resourced Data Generation} By putting LLMs' generative capabilities to test, we ask in this work if they could generate high-quality and low-cost code-mixed texts for researchers working on a topic plagued by limited data availability. While we conclude that models such as ChatGPT and InstructGPT have shown relative success in generating code-mixed texts for SEA languages when prompted in particular zero-shot manners, we ask researchers to exercise caution when using this data generation technique. Even for Singlish, which outperforms the other languages examined, we find that syntactically-sound responses frequently contain word choice errors and semantic inaccuracies. For non-native speakers, such fluency deviations may be difficult to detect. Due to this lack of reliability, we strongly suggest researchers who wish to pursue this method of data generation implement extensive human checks with native speakers. 

\paragraph{Multilingual â‰  Code-Mix Compatible} The inability of models such as BLOOMZ and Flan-T5-XXL in producing code-mixed data demonstrates that code-mixing is not recognized as an essential component of many multilingual LLMs today. In other words, for some models, multilinguality simply means the system can process tasks and generate outputs in multiple languages, but not in the same sentence. By highlighting this limitation, we compel researchers to consider code-mixing as a core feature of many people's linguistics repertoire across the world. Building LLMs that include code-mixing not only allows NLP researchers to more accurately capture the dynamic elements of many languages, but it also helps improve one's understanding of tone, formality, and other cultural aspects embedded in conversations. Finally, we hope that by centering \textit{true} multilinguality, that is, multilinguality that goes beyond language inclusion and encompassing language-\textit{mixing}, we can develop representative and equitable LLMs that cater to the needs of underserved linguistic communities across the world.

\paragraph{Research Transparency} Aside from showing that ChatGPT and InstructGPT's \textit{can} code-mix, we cannot confidently identify \textit{how} the models do so due to the lack of transparency in how these systems are developed. Without a window into the kind of training data and engineering processes that went into models like ChatGPT, we can only speculate that they were not only fed multilingual texts but were also explicitly trained to handle code-mixing. To help facilitate greater levels of transparency and accountability, we urge forthcoming models that have code-mixing capabilities to be more open about how the models were developed.

\paragraph{Beyond Written Code-Mixing} 
As the LLM landscape continues to broaden, we anticipate that emerging models will likely be increasingly multimodal. Text-based LLMs that serve as conversational agents, the likes of ChatGPT, will likely start to include speech or visual units (e.g., GPT-4). Owing to the prevalence of code-mixing in vocal speech, we believe that including the ability to recognize and generate code-mixed outputs in LLMs will provide a wide range of downstream benefits. For instance, voice assistants supported by code-mixing-friendly automatic speech recognition (ASR) systems could help people communicate in ways that are potentially more naturalistic and authentic to their self-expression. Removing the need for people to adjust their speech patterns to become legible to machines could further ameliorate the effects of linguistics profiling \cite{baugh2005linguistic} and Western-centrism technological designs.

%\paragraph{Future research}
%Although we see code-mixed data collected from native speakers as the 'gold standard', we recognize that time and resource-constrained researchers may use LLMs like ChatGPT for data generation. Since our work demonstrates that LLMs are sensitive to potential biases in prompts and may generate sentences that suffer from semantic inaccuracies, it is crucial to design a filtering mechanism (such as human annotators) to control the quality of the synthetic data. \todo {we need more stuff here}

% Nudge OpenAI to be open. - suggests data include CM data.

% Non-transparency has been actively studied - inability to do something well. We are looking for ability to do something. 

% Ethics consideration generation - abusive languages / cultural respectful - people more likely to believe texts come from people. 

% Publicly available MLM bad because they are not finetuned on these CM tasks where ChatGPT is probably exposed to. \url{https://twitter.com/ShayneRedford/status/1630252835404218371?s=20}