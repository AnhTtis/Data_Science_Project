\section{Related Work}
\paragraph{Code-Mixed Data in SEA} Unlike monolingual data, there is only a limited number of human-curated code-mixed data. This resource limitation is more severe in SEA due to its historical lack of representation~\cite{winata2022decades}. Existing code-mixing studies in SEA cover several language pairs and creoles, i.e., English-Tagalog~\cite{oco-roxas-2012-pattern}, English-Indonesian~\cite{barik-etal-2019-normalization,yulianti2021emotcmt}, Javanese-Indonesian~\cite{tho2021cm-jv-id}, Chinese-English~\cite{lyu2010seame} and Singlish~\cite{chen2015national,lent-etal-2021-language}\footnote{To exacerbate the situation, some of the SEA code-mixed datasets are no longer publicly available.}. The current corpus does not even scratch the surface of the sheer amount of code-mixedness in SEA~\cite{redmond2009-wl}, where deployable data is practically non-existent. In this work, we try to close this gap by exploring the potential of generating synthetic code-mixed data for the SEA region by way of LLMs.

\paragraph{Synthetic Code-Mixing} 
The effectiveness of using synthetically generated code-mixed data for improving the code-mixing capability of language models has been previously explored. \citet{winata2019code} and \citet{tan-joty-2021-code} have attempted to generate synthetic code-mixed sentences through word alignment and candidate selection from a parallel corpus.
%between matrix language (base language) and embedding language.
\citet{liu2020attention} and \citet{adilazuarda-etal-2022-indorobusta} have similarly generated synthetic code-mixed sentences by replacing words in a monolingual sentence with their machine-translated counterparts.
%by replacing words in matrix language with machine-translated ones in the embedded language. 
Despite the ability to produce sentences with high code-mixed index (CMI)~\cite{gambck2014cmi}, the generated sentences from these methods are inadequate in reproducing the naturalness of human-generated code-mixed utterances. In this work, we assess a novel way of generating synthetic code-mixed sentences without the need for parallel corpora or machine translation models through the prompting of LLMs, and provide a comprehensive analysis of such an approach. 
% Given that LLMs encode most of the available online textual data, our method naturally produces synthetic code-mixed sentences that better represent real code-mixed sentences.
%When LLMs are able to encode most of the available textual data that is ever created, we expect that our method will naturally produce better synthetic code-mixed sentences that better represent real code-mixed sentences.
% as adversarial attacks and training for mBERT and XLM-R models. 
