\section{Methodology}
% \begin{figure*}[!ht]
% \small
% \begin{fullleftbubbles}
% Imagine that you are an Indonesian speaker. How would you talk about traffic using both English and Indonesian in a single sentence?
% \end{fullleftbubbles}

% \begin{fullrightbubbles}
% \textbf{(ChatGPT)} As an Indonesian speaker, I would say, ``Kemacetan di jalan raya sangat parah, saya \ul{stuck} di sini karena \ul{traffic}nya sangat congested.'' which translates to ``The traffic jam on the highway is very bad, I'm stuck here because the traffic is very congested.''
% \end{fullrightbubbles}

% \begin{fullrightbubbles}
% \textbf{(InstructGPT)} \ul{Traffic}nya sangat padat, lalu lintasnya sangat macet.
% \end{fullrightbubbles}

% \begin{fullrightbubbles}
% \textbf{(GPT3)} I was stuck in traffic for two hours. Terpaksa berjam-jam di jalan karena macet.
% \end{fullrightbubbles}

% \begin{fullrightbubbles}
% \textbf{(BLOOMZ)} Traffic is heavy on the highway between Jakarta and Bogor at this time of day.
% \end{fullrightbubbles}

% \begin{fullrightbubbles}
% \textbf{(Flan-T5-XXL)} Para pengacara harus membuka kejadian di atas sel kebijakan.
% \end{fullrightbubbles}
% \caption{Responses from different language models (ChatGPT, InstructGPT (davinci-003), GPT-3 (davinci-002), BLOOMZ and Flan-T5-XXL) to the same task prompt for generating code-mixed text. \ul{Intrasentential and intraword code-mixing} are underlined. \todo{(different colored bubble box for different models)}}
% \end{figure*}



%%%% METHODOLOGY %%%%%%%
\subsection{Prompting Language Models}
We collect synthetic code-mixed data by prompting LLMs with requests along two axes: languages and topics (food, family, traffic, Artificial Intelligence, and weather). See Figure~\ref{fig:code-mixing-prompt-templates} for examples of different prompt templates.
Specifically, we explore ChatGPT, InstructGPT (davinci-002 and davinci-003) \cite{ouyang2022rlhf}, 176B-parameter BLOOMZ \cite{muennighoff2022bloomz}, and Flan-T5-XXL \cite{chung2022flant5}. We use OpenAI and HuggingFace's API for prompting (see Appendix~\ref{app:hf-api}), except in the case of ChatGPT, which we manually queried through its web interface\footnote{ChatGPT's API was not publicly released when we conducted this study.}. 

In our prompts, we specify code-mixing between English and either Indonesian, Malay, Mandarin, Tagalog, Vietnamese, or Tamil. We focused on code-mixing English with SEA languages for two reasons: (1) extensive literature on code-mixed English provides a relevant point of comparison, and (2) English is one of the most widely used languages in code-mixing across SEA countries \cite{kirkpatrick2014english}. We additionally prompt with sentences in Singlish, a creole language, to evaluate how sensitive LLMs are to the diversity of language practices in the SEA region. In total, we submitted 210 unique prompts per language model.


\begin{figure*}[!t]
% \centering
% \begin{subfigure}[b]{\textwidth}
%      \centering
%      \captionsetup{labelformat=empty}
%      \caption{InstructGPT (davinci-003)}
%      \includegraphics[width=\textwidth]{assets/subplots_davinci003.pdf}
%      %\label{fig:davinci-003}
% \end{subfigure}
% \hfill
% \begin{subfigure}[b]{\textwidth}
%      \centering
%      \captionsetup{labelformat=empty}
%      \caption{ChatGPT}
%      \includegraphics[width=\textwidth]{assets/subplots_chatgpt.pdf}
%      %\label{fig:chatgpt}
% \end{subfigure}
% \vspace{-1cm}
\centering
\includegraphics[width=\textwidth]{assets/subplots_chatgpt.pdf}
\caption{Analysis of code-mixed data generated by ChatGPT.}
\label{fig:chatgpt-davinci-003-breakdown}
\end{figure*}

\subsection{Evaluation}

\subsubsection*{Level of Code-Mixing}
\label{sec:cm-scale}
To evaluate outputs, we ask whether LLMs can produce \textit{intrasentential} code-mixed text. We adopt the definition of intrasentential code-mixing from \citet{berk1986linguistic}, which covers mixing small constituents---such as noun and verb phrases---and large constituents---such as coordinate clauses and prepositional phrases.  Native speakers are then tasked to manually annotate the collected responses on a scale from 0 to 3 using the following coding guidelines to denote the degree of code-mixedness:

\begin{itemize}
    \item \textbf{0 - No code-mixing:} The generated text is written purely in one language or only exhibits \textit{intersentential} code-mixing (i.e., switching at sentence boundaries including interjection, idiom, and tags). We adopt the definition from \citet{berk1986linguistic}.
    \item \textbf{1 - Loanwords:} The generated text uses loanwords for common terminologies. We consider a word as a loanword if it is listed in Wiktionary\footnote{\url{https://en.wiktionary.org}}. For example: In the sentence, ``I like eating \textit{pho},'' ``pho'' is a loanword.
    \item \textbf{2 - Topic-related nouns:} The generated text uses nouns related to the topic specified in the prompt in another language. For instance, for the topic of traffic, an example would be ``今天的 \textit{ traffic} 真的很糟糕，我开了一个小时才到了办公室.'' (Chinese: ``The traffic today is really terrible. I spent an hour driving to get to the office.'')
    \item \textbf{3 - Linguistic Elements:} The generated text mixes linguistic elements beyond loanwords and topic-related nouns at the phrasal or clausal level. One example is verb phrases: ``My family \textit{ay nagplano ng isang malaking} family reunion \textit{sa park} this coming weekend.'' (Tagalog: ``My family has planned a big family reunion at the park this coming weekend.'') This category also includes intraword code-mixing, e.g., ``Kapag busy ang trapiko, mag-ingat ka sa \textit{pagda-drive}\footnote{The prefix ``pag-'' in Tagalog is affixed to the English word ``drive'', resulting in the word ``pagda-drive'' (the act of driving). This example demonstrates the application of Tagalog inflection rules to English words.} para maiwasan mo ang mga masamang pangyayari.'' (Tagalog: ``When traffic is busy, be careful while driving to avoid accidents.'')
    
\end{itemize}

We use this scale instead of popular word-level metrics such as CMI \cite{gamback2014measuring} because our scale more holistically evaluates the ability of LLMs to code-mix. The lower end of this scale reflects a lower complexity of code-mixing. Code-mixing with loanwords is arguably less challenging, as they are often used in a monolingual context to begin with. Likewise, code-mixing topic-related nouns is not as complex as there is presumably a correspondence between the nouns in the two languages and is primed by the prompts. 

On the other hand, code-mixing prefixes/suffixes, phrases and clauses requires a good grasp of the intricate morphosyntactic structures of both languages and can produce syntactically diverse code-mixed data. Therefore, we consider the LLM to have \textit{successfully} generated code-mixed text only if the text belongs to this category. 

\subsubsection*{Naturalness} 

We asked native speakers to annotate the naturalness of the generated text on a rating scale of 1---not natural at all, 2---an annotator may see someone else from other regions/cultures speaking it, and 3---an annotator may see themselves speaking it. If the text is not considered natural (rating of 1), we requested the annotators to document their reasons, including fluency issues, syntactical errors, and semantic incorrectness. 
% For languages with more than one annotator, we also report their interannotator agreement scores.\footnote{All annotators are authors of this paper.}

Code-mixing practices vary across race, gender, class, and geography, among other axes. Therefore, in line with sociolinguistic research, we emphasize that the ``naturalness'' of code-mixed text (as perceived by an annotator) is subjective, and depends on the background and social location of the annotator. All annotators are AI researchers and native speakers of their respective SEA languages. We refer to Appendix~\ref{app:interannotator} for more information about our annotators and their inter-annotator agreement scores.

\subsubsection*{Accurateness} 

We additionally annotate the \textit{accurateness} of the generations to account for task failure or incorrect explanations. Inaccurate outputs were defined by: 
\begin{itemize}
    \item \textbf{Failure to follow instructions:} Some LLMs fail to accurately carry out the prompt instruction (for example, generating monolingual sentences). We also observe some code-mixed outputs that include additional languages unspecified in the prompts.
    \item \textbf{Inaccurate explanations:} ChatGPT, among other LLMs, tends to explain the code-mixed phrases, but occasionally provides incorrect explanations (see Figure~\ref{fig:wrong-explanations}).

    % \item \textbf{Wrong Language:} We note that some output produces incorrect languages.
    % \item \textbf{Empty output:} BLOOM and BLOOMZ often produce no output at all, which we marked as incorrect.
\end{itemize}


% We evaluated the collected text on the following criteria: 

% \paragraph{Correctness} We judged that the language model is not correct when it fails to follow the instructions. For instance, the response includes a language that is not specified in the prompt. 

% \paragraph{Code-mixed Category} The level of sophistication of the code-mixed output is categorized by whether the code-mixed words or phrases are as follows: 
% \begin{enumerate}
%    \item loanwords\footnote{We define loanwords as a word from a foreign language that can be found in the dictionary of the local language. For SEA foreign languages to English language, we use Wiktionary as the reference.} (e.g., Vietnamese dish \textit{pho})
%    \item topic-related entities (e.g., \textit{deep learning} when prompted about the AI topic)
%    \item beyond entity level (e.g., Keluarga saya suka \textit{spending quality time} bersama-sama setiap hari (Malay: My family likes spending quality time together everyday.))
%\end{enumerate}
%All the evaluations were carried out by native speakers of the SEA languages.

%We argue that the level of sophistication correlates with the cross-lingual generalization ability of the language model to code-mixing. Loanwords are usually present in the monolingual corpora, whereas code-mixing beyond the entity level requires the language model to be able to model the grammatical structure of the two code-mixing languages. \todo{(need more eyes on this paragraph.)}
