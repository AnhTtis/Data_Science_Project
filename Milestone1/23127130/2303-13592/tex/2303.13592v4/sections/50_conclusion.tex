\section{Conclusion}
To ameliorate the scarcity of code-mixed data for South East Asian languages, we explore generating synthetic code-mixed data using state-of-the-art multilingual Large Language Models (LLMs). On one hand, we find that publicly available LLMs such as BLOOMZ and Flan-T5-XXL have limited capability in generating syntactically diverse code-mixed data. On the other hand, closed-source models such as ChatGPT and InstructGPT are better at generating natural code-mixed text, but their performance varies substantially depending on the prompt template and language pairing. Furthermore, many outputs suffer from syntactic, semantic, and reliability issues. Therefore, we caution against using LLM-generated synthetic code-mixed data without the involvement of native speakers for annotating and editing.

% We demonstrate that ChatGPT outperforms other multilingual LLMs when prompted to generate code-mixed texts for South East Asian languages and that their performances are particularly noteworthy for Singlish. We also discover that publicly available multilingual LLMs such as BLOOMZ and Flan-T5-XXL are incapable of generating code-mixed data through zero-shot prompting. Nonetheless, we discover issues with the accuracy, reliability, and naturalness of code-mixed data generated by models such as ChatGPT. Therefore, we caution against using LLM-generated synthetic code-mixed data without the involvement of native speakers for annotating and editing.