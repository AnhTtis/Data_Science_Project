\section{Introduction}
Code-mixing, also known as code-switching, is the linguistic practice of alternating between two or more languages in an utterance or conversation~\citep{poplack1978syntactic}. It allows individuals to express culturally-specific ideas, connect with or differentiate from other interlocutors, and reify their identities~\citep{bhatia2004,grosjean1982,toribio2006,chen1996code, dogruoz-etal-2021-survey}. Despite its prevalence across many parts of the world, computational research into this area remains understudied~\cite{ws-2014-approaches-code,aguilar-etal-2020-lince, winata2021multilingual,winata2022decades,zhang2023multilingual}.

%Despite its prevalence across many parts of the world, computational research into the area has only picked up steam recently~\cite{winata2022decades}. The growing visibility of code-mixing in the global media landscape, the proliferation of data-hungry machine-learning methods, and calls for natural language processing (NLP) research to devote more attention to multilingualism and low-resource languages collectively contribute to this emerging interest~\cite{aziz2019types,barman2014code,https://doi.org/10.48550/arxiv.2207.04672}.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{assets/sea_map.pdf}
\caption{\label{fig:SEA} Depiction of SEA regions, which consist of a total of 11 countries. We prompt LLMs to generate code-mixed data of languages used in six South East Asian countries (colored in dark blue): Brunei, Indonesia, Malaysia, Philippines, Singapore, and Vietnam.}
\end{figure}

% That said, acquiring high-quality and low-cost code-mixed data presents itself as a challenge to NLP researchers in this field. For one, code-mixing is observed more frequently in colloquial settings and spoken communication, which makes procuring and curating extensive datasets logistically demanding and costly~\citep{chan2009automatic,winata2021multilingual}. Moreover, despite code-mixing's prevalence across social media or digital messaging platforms, consolidating such data may be curtailed by legal guardrails and scalability issues.

\begin{figure*}[t]
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_a_3_models.pdf}
		}
		\caption{Template: Assume to be bilingual speaker}
	\end{subfigure}
	% %
	% \begin{subfigure}[c]{0.5\textwidth}
	% 	\resizebox{0.99\textwidth}{!}{
	% 		\includegraphics[]{assets/template_b.pdf}
	% 	}
	% 	\caption{Template: Two bilingual speakers}
	% \end{subfigure}
	%
	\begin{subfigure}[c]{0.5\textwidth}
		\resizebox{0.99\textwidth}{!}{
			\includegraphics[]{assets/template_c_3_models.pdf}
		}
		\caption{Template: Imitate speaking style}
	\end{subfigure}
	% %
	% \begin{subfigure}[c]{0.5\textwidth}
	% 	\resizebox{0.99\textwidth}{!}{
	% 		\includegraphics[]{assets/template_d.pdf}
	% 	}
	% 	\caption{Template: Explicitly define CM}
	% \end{subfigure}
	% %
	% % \begin{subfigure}[c]{0.5\textwidth}
	% % 	\resizebox{0.99\textwidth}{!}{
	% % 		\includegraphics[]{assets/template_e.pdf}
	% % 	}
	% % 	\caption{Template: Native speaker}
	% % \end{subfigure}
	% % %
	% % \begin{subfigure}[c]{0.5\textwidth}
	% % 	\resizebox{0.99\textwidth}{!}{
	% % 		\includegraphics[]{assets/template_f.pdf}
	% % 	}
	% % 	\caption{Template: Write a CM sentence}
	% % \end{subfigure}
 \caption{Example prompt templates with different \textcolor{blue}{languages} and \textcolor{orange}{topic} fields and responses from LLMs containing code-mixed / non-code-mixed sentences. Note that the explanations are a part of ChatGPT's original generation. ``CM'' indicates the level of code-mixing (Section~\ref{sec:cm-scale}). See Figure~\ref{fig:app-code-mixing-prompt-templates} in Appendix for all prompt templates and responses from other LLMs such as BLOOMZ and Flan-T5-XXL.
}
\label{fig:code-mixing-prompt-templates}
\end{figure*}

 One longstanding challenge in this area involves acquiring high-quality and low-cost code-mixed data. For one, code-mixing is observed more frequently in colloquial settings and spoken communication, which makes procuring and curating extensive datasets logistically demanding and costly~\citep{chan2009automatic,winata2021multilingual}. Moreover, despite code-mixing's prevalence across social media and digital messaging platforms, consolidating such data may be curtailed by legal guardrails and scalability issues. Recognizing these challenges, we explore the feasibility of using generative Large Language Models (LLMs) to ameliorate data scarcity in code-mixing research. As recent work shows that LLMs can successfully generate synthetic data \cite{alpaca,he2023annollm,tang2023does, whitehouse2023llm}, here we evaluate whether multilingual LLMs can be prompted to create code-mixed data that look natural to native speakers (and if so, to what extent). 
%Using a code-mixing scale of 0 to 3 (0-no code-mixing; 1-loanword usage; 2-topic-related nouns; 3-beyond entities), we compare performance across different models on different languages and assess whether these outputs are of a quality decent enough for research deployment.

To this end, we hone in on languages in South East Asia (SEA). Home to more than 680 million people and over 1200 languages, code-mixing is particularly prevalent in this region due to its countries' extended histories of language and cultural cross-fertilization and colonialism (Figure~\ref{fig:SEA})~\cite{goddard2005languages,bautista2006southeast,reid-etal-2022-m2d2}. Marked by its distinctive multilingual and multiracial composition today, SEA presents an opportunity to further research numerous marginalized languages and linguistic practices in NLP research\footnote{Major languages in SEA countries belong to different language families such as Indo-European, Thai, Austronesian, Sino-Tibetan, Dravidian, and Austro-Asiatic. Furthermore, there are at least thousands of major and minor SEA languages.}~\cite{migliazza1996mainland,goddard2005languages, joshi-etal-2020-state, aji-etal-2022-one, winata2023nusax, cahyawijaya2023nusacrowd}. 
% Despite so, multilingual NLP research has devoted scant attention to examining code-mixing in this region, and
Nonetheless, publicly available code-mixed datasets relevant to SEA communities remain limited \cite{lyu2010seame,winata2022decades}. 

%One explanation for this is that code-mixing is observed more often in colloquial settings and speech-based communication, leading to challenges in collecting extensive datasets that can be used for language processing research. While it remains critical to better understand code-mixing computationally to allow better downstream performance in real-world contexts involving diverse ethnic, cultural, and linguistic expressions~\citep{sitaram2019survey,lovenia-etal-2022-ascend,hershcovich-etal-2022-challenges}, there is a limited number of publicly available code-mixed datasets representative of the SEA communities \cite{lyu2010seame,winata2022decades}.
% and even these public datasets have largely focused on more extremely high-resource languages like English and Mandarin~\citep{lyu2010seame}. 

%Our focus on SEA presents an opportunity to contribute to a region with diverse under-studied and under-resourced languages \cite{joshi-etal-2020-state,aji-etal-2022-one}. 

We prompt five multilingual LLMs, i.e., ChatGPT, InstructGPT (davinci-002 and davinci-003) \cite{ouyang2022rlhf}, BLOOMZ \cite{muennighoff2022bloomz}, and Flan-T5-XXL \cite{chung2022flant5} to generate code-mixed text that bilingually mixes English with either \textbf{Malay, Indonesian, Chinese, Tagalog, Vietnamese, or Tamil}. All of these six SEA languages (alongside English) are used across six SEA countries, namely Singapore, Malaysia, Brunei, Philippines, Indonesia, and Vietnam. Furthermore, they belong to different language families---Indo-European, Austronesian, Sino-Tibetan, Austro-Asiatic, and Dravidian. An example of a prompt we used is: ``Write an English and Tamil code-mixed sentence about Artificial Intelligence.'' In addition, we prompt these LLMs to generate texts in \textbf{Singlish}, an English-based creole widely spoken in Singapore that combines multiple SEA languages such as Malay, Chinese and Tamil. 
% An example of a prompt involving Singlish looks like this: ``Imitate the speaking style of a person who can speak Singlish in one sentence about family.'' 
We ask native speakers to annotate the \textit{naturalness} (i.e., whether a native speaker would speak as such) and the \textit{level of code-mixing} in the outputs. 
%We provide the complete set of prompts and code-mixed responses here: \url{https://github.com/Southeast-Asia-NLP/LLM-Code-Mixing}.


% \begin{figure*}[!t]
% \centering
% \includegraphics[width=1\textwidth]{assets/overall_plot-2.pdf}
% \caption{Comparison of performance of different LLMs in generating code-mixed data through zero-shot prompting. We distribute the result across different code-mixing levels: (1) Loanword, (2) Topic-related entities, and (3) Code-mixing beyond entity. }
% \label{fig:plot-all-models}
% \end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{assets/code-switchness-all-subdued.pdf}
\caption{Comparison of performance of different LLMs in generating code-mixed data through zero-shot prompting. We distribute the result across different code-mixing levels: (0) No code-mixing (Non-CM), (1) Loanword, (2) Topic-related nouns, and (3) Linguistic Elements.}
\label{fig:plot-all-models}
\end{figure*}

To the best of our knowledge, our work marks the first attempt at studying the generation of synthetic code-mixed data through prompting LLMs in a zero-shot fashion without any monolingual reference texts or explicit linguistic constraints \citep{solorio-liu-2008-learning,tarunesh-etal-2021-machine,rizvi-etal-2021-gcm,mondal-etal-2022-cocoa}. We find that publicly available multilingual language models such as BLOOMZ and Flan-T5-XXL are only capable of code-mixing with loanwords or topic-related nouns. Most of the time, they fail to code-mix (despite being advertised as multilingual). While ChatGPT stands out in its ability to generate code-mixed texts, it is extremely sensitive to the prompt template and exhibits a considerable variance of success in generating natural-sounding code-mixed texts across different language pairs. Additionally, it may erroneously introduce additional languages not specified in the prompt and wrongly explain the code-mixing of the text. 

Our results lead us to conclude that code-mixing, at least as of today, is not considered an essential component of many multilingual LLMs. Moreover, the opaque creation of models like ChatGPT makes it difficult to ascertain the mechanisms that enable code-mixing. By highlighting the limited promises of LLMs in a specific form of low-resource data generation, we advise NLP researchers against using existing systems to produce synthetic code-mixed data without extensive human evaluation. 

%---for certain prompts---ChatGPT is able to follow instructions and code-mix linguistic elements, such as verb and adverbial phrases, for the aforementioned six SEA languages up to 76\% success rate. 
%For Singlish, ChatGPT and InstructGPT (davinci-003)'s performances are particularly noteworthy, clocking at 96\% across all prompts. 
% We also observe surprisingly high capability of ChatGPT in generating Singlish text. In comparison, models . 

% Meanwhile, despite ChatGPT and InstructGPT's relative success in terms of performance, the code-mixing capabilities of these models are dampened by grammatical errors and inaccuracies. In other words, while the grammatical arrangement of words in the output sentences may be correct, further scrutiny reveals misuses of words or concepts that ultimately diminish the fluency of these utterances. 
% Furthermore, while ChatGPT can explain how the sentences are code-mixed, the explanations may be inaccurate. 

%With little transparency into the training data supporting models such as ChatGPT and InstructGPT, it is difficult to ascertain the consistency of their code-mixing capabilities. Even for languages where, we recommend researchers to rely on human annotation 



% \begin{itemize}
%     \item How natural is ChatGPT on generating Code-mixed sentences?
%     \item How is the generation diversity over different language pairs? Are they culturally related with the languages?
%     \item Is there any common code-mixing pattern that we can observe from ChatGPT? Does it cover all the possible code-mixed sentences in these languages?
% \end{itemize}


%%% DESCRIBE FINDINGS HERE


% Despite the prevalence of code-mixing, multilingual natural language processing research has traditionally avoided studying code-mixed text. Among the reasons for this is that code-mixing is observed more often in spoken than written form, leading to an absence of extensive data sets that can be used for language processing research~\citep{winata2021multilingual}. Text that utilizes multiple languages instead of consistently utilizing a single language can be labelled as 'contaminated' and discarded. Moreover, the linguistic structure of a code-mixed utterance is distinct from that of its component languages as it combines the grammatical structure of each language. Modelling the dynamics of a code-mixed utterance is thus not necessarily equivalent to modelling the dynamics of each language, making it even more difficult for current models to process the data. Furthermore, code-mixed text presents challenges in machine translation or masked-language-modeling. Understanding code-mixing allows better downstream performance in real-world data involving demographics of diverse culture, ethnicity, and language that code-mix~\citep{sitaram2019survey,hershcovich-etal-2022-challenges,winata2022decades}.

% As large language models demonstrate broad zero-shot performance across a wide range of tasks, their mastery of multiple languages is increasingly tested. Typically, tasks such as machine translation or multilingual summarization are used, all of which expect monolingual output. Evaluation of code-mixed tasks remains understudied.