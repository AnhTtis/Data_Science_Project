\section{Related Work}
\paragraph{Code-Mixed Data in SEA} Unlike monolingual data, there is only a limited number of human-curated code-mixed datasets. This resource limitation is more severe in SEA due to its marginalization in NLP research~\cite{winata2022decades}. Popular current code-mixing evaluation benchmarks \cite{aguilar-etal-2020-lince, khanuja-etal-2020-gluecos} do not include SEA languages, and existing code-mixing studies in SEA only cover a limited number of language pairs and creoles, e.g., English-Tagalog~\cite{oco-roxas-2012-pattern}, English-Indonesian~\cite{barik-etal-2019-normalization,yulianti2021emotcmt}, Javanese-Indonesian~\cite{tho2021cm-jv-id}, Chinese-English~\cite{lyu2010seame,lovenia2022ascend, zhang2023crocosum} and Singlish~\cite{chen2015national,lent-etal-2021-language}\footnote{To exacerbate the situation, some of the SEA code-mixed datasets are no longer publicly available.}. The current corpus does not even scratch the surface of the sheer amount of code-mixedness in SEA~\cite{redmond2009-wl}, where deployable data is practically non-existent. In this work, we try to close this gap by exploring the potential of generating synthetic code-mixed data for the SEA region by prompting LLMs.

\paragraph{Synthetic Code-Mixing} 
Generation of
synthetic code-mixed data to address data scarcity problem has been previously explored. \citet{solorio-liu-2008-learning}, \citet{winata2019code}, and \citet{tan-joty-2021-code} have attempted to generate synthetic code-mixed sentences through word alignment and candidate selection from a parallel corpus.
%between matrix language (base language) and embedding language.
\citet{liu2020attention} and \citet{adilazuarda-etal-2022-indorobusta} have similarly generated synthetic code-mixed sentences by replacing words in monolingual sentences with their machine-translated counterparts, whereas \citet{pratapa-etal-2018-language}, \citet{rizvi-etal-2021-gcm} and \citet{santy-etal-2021-bertologicomix} leveraged parse tree structure for such replacements. Another approach is to perform neural machine translation to translate monolingual sentences to code-mixed ones~\cite{appicharla-etal-2021-iitp,gautam-etal-2021-comet,jawahar-etal-2021-exploring,dowlagar-mamidi-2021-gated}.
%by replacing words in matrix language with machine-translated ones in the embedded language. 
% Despite the ability to produce sentences with high code-mixed index (CMI)~\cite{gamback2014measuring}, the generated sentences from these methods are inadequate in reproducing the naturalness of human-generated code-mixed utterances. 
In this work, we assess a novel way of generating synthetic code-mixed sentences through prompting multilingual LLMs.
% Given that LLMs encode most of the available online textual data, our method naturally produces synthetic code-mixed sentences that better represent real code-mixed sentences.
%When LLMs are able to encode most of the available textual data that is ever created, we expect that our method will naturally produce better synthetic code-mixed sentences that better represent real code-mixed sentences.
% as adversarial attacks and training for mBERT and XLM-R models. 
