\section{Limitations}
% \subsection{Sample Size}
% Our observations were limited to 210 unique prompts per language model across five topics and seven SEA languages. With the recent release of ChatGPT API, our immediate next step is to automatically generate more code-mixed samples and present a more robust and generalizable conclusion about ChatGPT's ability to generate synthetic code-mixed data.

\subsection{Effectiveness of Synthetic Code-Mixed Data on Downstream Tasks}
In our study, we did not evaluate how much our synthetically generated code-mixed data improve the ability of language models to handle code-mixed text in downstream NLP tasks. While previous findings have shown that finetuning models with synthetic code-mixed data yields less performance gains than with naturally occurring code-mixed data \cite{santy-etal-2021-bertologicomix}, we believe that this performance gap will diminish as the quality of synthetic data generation gets better with future multilingual LLMs. 

\subsection{Lack of Human-Generated Data} 
While we annotated the degree of code-mixedness and naturalness, we did not have human-generated, naturally occurring, code-mixed sentences in response to the prompt topics. Therefore, we could not systematically compare the data distribution of our synthetic data against the human-generated data. However, since there are multiple ways in which a sentence can be code-mixed, our focus in this work is on how human-like are the sentences, and this, we believe, was adequately captured by our evaluation.

\subsection{Monolingual Zero-Shot Prompting}
Our study only uses prompt templates written in English to prompt language models in a zero-shot manner. In future follow-ups, we will (1) use code-mixed prompt templates such as ``Generate an English-Bahasa sentence'' instead of ``Generate an English-Malay sentence'' and (2) investigate LLMs' capabilities in generating code-mixed data with in-context few-shot examples. 

\subsection{Instruction-Tuned Language Models}
Our work only covers instruction-tuned language models. In future work, we will include a comparison between multilingual models that are not finetuned with instructions---for example, GPT3 (davinci) \cite{brown2020gpt3} and BLOOM \cite{scao2022bloom}---to explore the effects of instruction tuning in generating code-mixed data.

\subsection{English-Centric Code-Mixing}
Our study focuses on generating code-mixed data only for English-SEA language pairs. For future studies, we plan to investigate generating code-mixed data for non-English language pairs commonly spoken in SEA countries (such as Malay-Chinese and Indonesian-Javanese).

\subsection{Failures of BLOOM and Flan-T5-XXL}
Given the lack of research transparency on why ChatGPT performs better at code-mixed text generation, we assume that the publicly available models such as BLOOM and Flan-T5-XXL are unable to code-mix due to the lack of code-mixed texts in the pretraining corpora and code-mixing tasks in the instruction-tuning datasets. Further investigation is warranted to understand the effects of code-mixed text in pretraining and instruction-tuning data on code-mixed text generation. 

\subsection{Presence of Synthetic Code-Mixed Data in Future Pretraining Data}
As we advocate for the code-mixing ability in future generations of LLMs, we are aware of the potential risks of \textit{data feedback}, where generative models that recursively train on data generated by previous generations may amplify biases and lose information about the tails of the original distribution \cite{shumailov2023curse,taori2022data}. Since these negative effects can be mitigated through human-generated content \cite{shumailov2023curse}, it becomes imperative for the NLP community to collect natural code-mixed data for low-resource languages. 