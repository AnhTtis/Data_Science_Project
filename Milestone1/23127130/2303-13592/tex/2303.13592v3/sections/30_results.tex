\section{Results}

%%%%% example %%%%
\begin{figure}[!t]
\centering
\scriptsize
\begin{leftbubbles}
Imagine there are two English-Vietnamese bilingual speakers talking to each other, can you imagine a conversation between them talking about family?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} Person A: Hi, how's your family doing?\\Person B: Chào bạn, gia đình tôi đang khỏe. Còn gia đình bạn thì sao?
\\Person A: They're good, thanks for asking. How about yours?
\\Person B: Gia đình tôi cũng khỏe, cảm ơn bạn. Mới đây, tôi đã đi chơi với ông bà và các em họ của mình.
\end{rightbubbles}
\hrule
\begin{leftbubbles}
Imagine there are two English-Chinese bilingual speakers talking to each other, can you imagine a conversation between them talking about weather?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} English Speaker 1: Hey, have you been outside today? It's really hot!
\\Chinese Speaker 1: 是啊，今天天气真的很热。
\\English Speaker 2: Yeah, it's been pretty sunny all day.
\\Chinese Speaker 2: 是啊，今天一整天都很晴朗。
\end{rightbubbles}

% \begin{leftbubbles}
% Speaker 1: Yesterday raining cats and dogs, today like oven temperature. So shiok ah, Singapore weather.
% \end{leftbubbles}
% \begin{rightbubbles}
% Speaker 2: Ya loh, very sian one. But nevermind lah, just drink more water and stay hydrated can already.
% \end{rightbubbles}
\caption{Failure cases when we prompt ChatGPT to imagine two bilingual speakers conversing about a specific topic.}
\label{fig:failure-two-bilingual-speakers}
\end{figure}

%%%%%%% RESULTS %%%%%%%%%
\subsection{English-SEA Languages}
Figure~\ref{fig:plot-all-models} demonstrates that ChatGPT outperforms other language models in generating code-mixed data across six different language pairs, and is particularly proficient at code-mixing linguistic expressions. For InstructGPT, the davinci-003 model demonstrates some ability to generate syntactically varied code-mixed texts, whereas davinci-002 mostly code-mixes with loanwords. Conversely, the other two publicly available multilingual LLMs have extremely limited code-mixing capabilities. More specifically, Flan-T5-XXL can only code-mix with loanwords for the Indonesian-English language pair, and most of its non-English monolingual outputs suffer from severe fluency issues (see Appendix~\ref{app:flant5-fluency}). BLOOMZ is only capable of code-mixing topic-related nouns for Tamil-English even though its multilingual pretraining data ROOTS \cite{lauren2022roots} and instruction-tuning data xP3 \cite{muennighoff2022bloomz} cover Indonesian, Chinese, Tamil, and Vietnamese. We observe no direct effects of the proportions of these languages in the training sets on BLOOMZ's ability to code-mix (Appendix~\ref{effects-bloomz-lang-proportion}).

We further break down the performance of ChatGPT in Figure~\ref{fig:chatgpt-davinci-003-breakdown}\footnote{Detailed analysis for davinci-002, davinci-003, Flan-T5-XXL and BLOOMZ can be found in the Appendix (Figure~\ref{fig:davinci-002}, Figure~\ref{fig:davinci-003}, Figure~\ref{fig:bloomz} and Figure~\ref{fig:flant5xxl}).}. In Figure~\ref{fig:chatgpt-davinci-003-breakdown}(a), we see that ChatGPT is least proficient at mixing linguistic elements for English-Tagalog. This may be due to syntactic differences between the two languages; for example, English exhibits Subject-Verb-Object (SVO) word order, whereas Tagalog exhibits a verb-initial structure. Moreover, English demonstrates nominative-accusative alignment, whereas Tagalog, being a symmetrical-voice language, utilizes a case system with a typological classification that ``remains controversial among Austronesian linguists'' \cite[192]{aldridge2012}. In contrast, ChatGPT performs the best for English-Indonesian code-mixing, which may be due to training data distribution and similarities between the two languages regarding word order and 
morphosyntactic alignment. We also find that ChatGPT is capable of using either English or a SEA language as the matrix language, i.e., as the main language of a sentence as per the Matrix Language Frame model \citep{myers1997duelling}. 

Figure~\ref{fig:chatgpt-davinci-003-breakdown}(b) shows ChatGPT's code-mixing proficiency based on topics. ChatGPT tends to code-mix with loanwords when the topic is about ``AI'' by mixing the English loanwords ``Artificial Intelligence,'' or its short form ``AI.'' For food, it tends to code-mix with food-related terms—which are topic-related nouns—in SEA languages such as ``bánh mì'' (Vietnamese sandwich). We also observe some representation biases in specific language-topic pairs. For instance, when it comes to food, ChatGPT uses the word ``nasi goreng'' (fried rice) for all English-Indonesian responses. For other topics, such as traffic and weather, it tends to code-mix phrases related to traffic congestion and hot weather. 
%We believe a larger scale of data collection is warranted to measure the representation biases in synthetic data generated by LLMs.

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth]{assets/fluency.pdf}
\caption{Naturalness of code-mixed (CM) text generated from ChatGPT. ``Semi-Natural'' indicates that the annotators see someone else from other region/culture speaking it, whereas ``Natural'' indicates that the annotators see themselves speaking it.}
\label{fig:naturalness}
\end{figure}

In Figure~\ref{fig:chatgpt-davinci-003-breakdown}(c), we find the prompt template with the highest quality results is the one where the term code-mixing is explicitly defined. In contrast, the worst-performing template consists of asking the model to generate conversations between two bilingual speakers, where the term code-mixing is unmentioned. In Figure~\ref{fig:failure-two-bilingual-speakers}, we see that ChatGPT generates an uncommon pattern of conversations where one interlocutor speaks in English and the other speaks in another language entirely (top example). Furthermore, ChatGPT may assume there are four speakers though the prompt asks for a conversation between two speakers (bottom example).

In terms of naturalness, we observe a considerable variance in ChatGPT's ouputs, with English-Tamil being the least natural (Figure~\ref{fig:naturalness}). Further analysis shows that ChatGPT either commits grammatical mistakes (such as comma splice and redundancy) or generates semantically confusing sentences. We also observe unnatural text patterns that mix two different script systems of the same language in the single sentence (for example, the Tamil script and its transliterated Latin script). We document these naturalness and fluency issues in Table~\ref{tab:naturalness-issues} (Appendix~\ref{app:naturalness-issues}) and report the interannotator agreement scores in Appendix~\ref{app:interannotator}.

%%%%%%% END EXAMPLE %%%%%%%%%%%



\subsection{Singlish}
\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{assets/singlish.pdf}
\caption{Models' performance in generating Singlish expressions through zero-shot prompting.}
\label{fig:singlish-all-models}
\end{figure}

%%%%% example %%%%
\begin{figure}[!ht]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are a Singaporean speaker. How would you talk about food using Singlish in a single sentence?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} ``Wah, the hawker center got so many sedap food, like chicken rice, laksa, and rojak, can die for sia!'' (Translation: ``Wow, the hawker center has so many delicious foods, like chicken rice, laksa, and rojak, they are to die for!'')
\end{rightbubbles}
\caption{Singlish sentence and its translation to Standard English, both generated by ChatGPT.}
\label{fig:singlish-translations}
\end{figure}
%%%%%%% END EXAMPLE %%%%%%%%%%%

Given that Singlish is an English-based creole language that has lexicalized a lot of words from many languages such as Malay, Chinese, Tamil, Cantonese, and Hokkien (e.g., ``lah,'' ``leh,'' and ``loh,'' etc.), it is difficult to disambiguate loanwords and linguistic elements. Therefore, we classify the model success based on a binary scale of whether the generated sentences contain Singlish expressions (e.g., interjections, lexicalized phrases of languages without a standardized writing system)\footnote{A reference list can be found at \url{https://en.wikipedia.org/wiki/Singlish_vocabulary} }. In Figure~\ref{fig:singlish-all-models}, we see that ChatGPT and InstructGPT (davinci-003) have up to a 96\% success rate in generating Singlish sentences, whereas Flan-T5-XXL and BLOOMZ have a near-zero success rate. Furthermore, we find that ChatGPT is also capable of translating Singlish expressions into Standard American English  (Figure~\ref{fig:singlish-translations}). 

While ChatGPT is capable of generating fluent and natural Singlish (Figure~\ref{fig:naturalness}), we observe semantic inaccuracies in their word choices that are discernible by native speakers. In Figure~\ref{fig:code-mixing-prompt-templates}(b), ChatGPT generates an English-Malay code-mixed phrase ``sotong and chilli sauce'' where ``sotong'' is a Malay word for ``squid.'' Upon first inspection, the sentence appears syntactically sound but native speakers found the analogy between familial connections and the dish (squid with chili sauce) to be semantically confusing.

OpenAI's documentation of model differences\footnote{\url{https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-text-davinci-003-differ}}---more specifically, that davinci-003 can produce higher-quality writing, process more complex instructions, and generate longer content than davinci-002---does not adequately explain why ChatGPT and davinci-003 significantly outperform davinci-002 and other multilingual LLMs in generating Singlish text. We hypothesize that the performance gap may be due to the larger presence of Singlish in OpenAI's training data. Singlish is not only one of the most well-researched dialects of English \citep{sin2017}, but it also uses the English alphabet and Latin script systems (unlike many other SEA languages), which potentially makes it easier for LLMs to scrape and parse data in this language. 
% However, we note that it is often difficult to disambiguate Singlish from a code-switch containing Singlish \citep{leimgruber2012}, as Singlish has lexicalized many words from other languages.

% Despite ChatGPT's and davinci-003's ability to generate Singlish sentences, we notice semantic inaccuracies in their word choices. In Figure~\ref{fig:code-mixing-prompt-templates}(c), ChatGPT outputs Singlish expressions ``lah'' and ``very important one leh'' as well as the (British) English-Malay code-mixed phrase ``sotong and chilli sauce'' where ``sotong'' is a Malay word for ``squid''. Upon first inspection, the sentence appears syntactically sound but  native speakers found the analogy between familial connections and the dish (squid with chilli sauce) to be semantically confusing.  

%%%%% example %%%%
\begin{figure}[!t]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are a Malaysian speaker. How would you talk about traffic using both English and Chinese in a single sentence?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} Assuming that you are asking for a sentence that combines both English and Chinese in a natural and commonly used way by Malaysian speakers, here is an example:
\\``今天的~traffic~很~\ul{teruk}，我要赶紧走去~KLCC for meeting。'' 
\\This sentence incorporates the English word ``traffic'' and \ul{the Chinese word ``teruk''} (which means ``bad'' or ``terrible'' in English) to describe the situation of the traffic being very bad. The speaker then switches back to English to express their intention to hurry to KLCC for a meeting. This kind of code-switching is common in Malaysia, where speakers often mix languages in their daily conversations. 
\end{rightbubbles}
% \hrule
% \begin{leftbubbles}
% Imagine that you are a Filipino speaker. How would you talk about family using both English and Tagalog in a single sentence?
% \end{leftbubbles}
% \begin{rightbubbles}
% \textbf{(ChatGPT)} \ul{Sa aking pamilya, ang aking mga magulang at kapatid ay napakabait at mahal nila ako. (In my family, my parents and siblings are very kind and they love me.)} 
% \end{rightbubbles}
\caption{ChatGPT's failure to follow the prompt. Underlined texts show its language misclassification. }
\label{fig:failure-nationality-translations}
\end{figure}
%%%%%%% END EXAMPLE %%%%%%%%%%%

\subsection{Failures of ChatGPT}
We notice that ChatGPT may fail to produce correctly code-mixed text when the nationality of the speaker is mentioned. Invoking nationality may induce ChatGPT to generate code-mixed texts with an \textit{additional} national language of the country. In the Figure~\ref{fig:failure-nationality-translations} example, because the national language of Malaysia is Malay\footnote{Mandarin Chinese is widely spoken in the country.}, ChatGPT not only returns a code-mixed response that includes the Malay word ``teruk'' when prompted for English-Chinese utterance by a Malaysian, but also misclassifies the word ``teruk'' as Chinese.


While ChatGPT can explain how its generated text is code-mixed (as illustrated in Figure~\ref{fig:code-mixing-prompt-templates}(a)), we find its justifications unreliable for two reasons. First, it may hallucinate words that do not exist in the code-mixed text. For instance, in the top example in Figure~\ref{fig:wrong-explanations}, the English word ``cold'' does not exist in the code-mixed text. Second, it may assign words or phrases to the wrong language. In the bottom example in Figure~\ref{fig:wrong-explanations}, the phrase ``nak minum air sejuk je'' is in Malay, not English. This observation corroborates \citeposs{bang2023multitask} finding suggesting ChatGPT performs poorly on language identification tasks for low-resource languages. 
%\arjun{Jess: add something about tokenization?}

%%%%%% EXAMPLE %%%%%%%
\begin{figure}[!ht]
\centering
\scriptsize
\begin{leftbubbles}
Imagine that you are an Indonesian speaker. How would you talk about weather using both English and Indonesian in a single sentence? 
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} If I were to talk about weather using both English and Indonesian in a single sentence, I might say something like: ``\textit{Hari ini sangat hot dan panas sekali, saya rasa aku butuh minum air dingin.}'' \ul{This sentence includes the English words ``hot'' and ``cold''} to describe the weather, and the Indonesian words ``panas'' and ``dingin'' which have similar meanings. Additionally, the sentence also includes a personal statement in Indonesian, ``saya rasa aku butuh minum air dingin,'' which means ``I feel like I need to drink cold water.''
\end{rightbubbles}
\hrule
\begin{leftbubbles}
Code-mixing refers to a phenomenon of combining two or more languages in a single sentence. Can you generate a code-mixed English-Malay sentence about weather?
\end{leftbubbles}
\begin{rightbubbles}
\textbf{(ChatGPT)} ``\textit{The weather hari ni sangat panas, I rasa macam nak minum air sejuk je.}'' (Translation: ``The weather today is very hot, I feel like drinking cold water.'')
\\In this sentence, ``hari ni'' and ``sangat panas'' are in Malay, while ``I'' and \ul{``nak minum air sejuk je'' are in English.}
\end{rightbubbles}
\caption{Underlined texts show wrong explanations about the code-mixed text. We italicize the \textit{code-mixed sentences} to make it explicit to the reader.}
\label{fig:wrong-explanations}
\end{figure}

