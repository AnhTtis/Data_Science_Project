\section{Related Work}

\subsection{Pixel-supervised VIS}
There are two major VIS benchmarks, the YTVIS series \cite{yang2019video} and OVIS \cite{qi2021occluded}, which have very different video types in terms of object motions and scenes. The YTVIS series focus mainly on segmenting sparse objects in shorter videos, while the OVIS aims to segment crowded instances with occlusions in longer videos. Based on these facts, we categorize the pixel-supervised VIS models into YTVIS-oriented ones and OVIS-oriented ones.

\textbf{YTVIS-oriented VIS models.}
By introducing a tracker into the representative IIS methods \cite{he2017mask,bolya2019yolact,tian2020conditional}, the early proposed VIS methods \cite{yang2019video, cao2020sipmask,Li_2021_CVPR,Athar_Mahadevan20stemseg,liu2021sg,yang2021crossover,QueryInst,ke2021pcan} have achieved decent performance on YTVIS series. 
The frame-to-frame trackers \cite{yang2019video,cao2020sipmask,Li_2021_CVPR,QueryInst} integrate the clues such as category score, box/mask IoU and instance embedding similarity. 
The clip-to-clip trackers \cite{Li_2021_CVPR,Athar_Mahadevan20stemseg,bertasius2020classifying,lin2021video, qi2021occluded,wang2020vistr,seqformer,hwang2021video} propagate the predicted instance masks from a key frame to other frames\cite{bertasius2020classifying,dai2017deformable,lin2021video,Li_2021_CVPR,qi2021occluded,wang2021end}.
% which can be implemented by deformable convolution \cite{bertasius2020classifying,dai2017deformable}, non-local block \cite{lin2021video}, correlation \cite{Li_2021_CVPR,qi2021occluded}, graph neural network \cite{wang2021end}, \etc. 
% By exploiting the temporal redundancy among overlapped frames, clip-to-clip trackers improve much the performance over per-frame methods on YTVIS datasets.
Recently, query-based \cite{carion2020end} VIS methods \cite{wang2020vistr,hwang2021video,seqformer,wu2022trackletquery} have achieved impressive progress. VisTR \cite{wang2020vistr} views the VIS task as an end-to-end parallel sequence prediction problem. 
% but it consumes a large amounts of memory to store spatial-temporal features. 
To reduce the storing memory of spatial-temporal features, IFC\cite{hwang2021video} transfers inter-frame information via efficient memory tokens, and SeqFormer \cite{seqformer} locates an instance in each frame and aggregates them to predict video-level instances. 

\textbf{OVIS-oriented VIS models.}
The aforementioned YTVIS-oriented VIS models often fail to handle the challenging long videos with crowded and similar-looking objects in OVIS dataset, resulting in significant performance degradation. Inspired by contrastive learning \cite{chen2020simplectt, pang2021quasi,khosla2020supervisedctt,wang2021densectt}, IDOL \cite{IDOL} learns discriminative embeddings for multiple object tracking frame by frame.
Mask2Former \cite{cheng2021mask2former} achieves impressive performance on IIS tasks by calculating attention only in the region of objects.
M2F-VIS \cite{cheng2021mask2former-video} and MinVIS \cite{huang2022minvis} extend Mask2Former to VIS task, where they respectively take per-frame and per-clip inputs.
% Mask2Former-VIS \cite{cheng2021mask2former-video} with per-clip input takes the mask IoU of the overlapping frames as the tracker, while MinVIS \cite{huang2022minvis} with per-frame input employs instance embedding similarity by using Hungarian matching as the tracker. 
VITA \cite{heo2022vita} integrates object embeddings of all frames in the video to produce video-level instance masks.

\textbf{Remarks.} Though the pixel-supervised VIS methods have achieved much progress, their generalization capability is limited. For example, the VIS models trained on YTVIS21 often fail to handle the challenging videos in OVIS, due to the limited number of type of videos in each dataset. However, it is labour-extensive to label the pixel-wise masks in videos. Inspired by the success of box-supervised IIS methods, we explore VIS task with only box annotations in this paper.

\input{figs/boxvis_baseline}
\subsection{Weakly-supervised Segmentation}

\textbf{Box-supervised IIS.}
A few box-supervised IIS methods have been proposed. BoxCaseg \cite{wang2021boxcaseg} leverages a saliency model to generate pseudo object masks. BoxInst  \cite{tian2020boxinst} achieves impressive performance by proposing a projection loss and a pairwise affinity loss for box-supervision. BoxLevelSet \cite{li2022boxlevelset} utilizes the traditional level set evolution to predict the object boundaries and instance masks. DiscoBox \cite{lan2021discobox} and BoxTeacher \cite{cheng2022boxteacher} employ an exponential moving average (EMA) teacher to produce high-quality pseudo masks and introduce pseudo pixel-wise mask supervision, bringing significant performance improvement. 

\textbf{Weakly-supervised Video Segmentation.}
% It is a challenging task to output pixel-wise segmentation masks with only image-level or box-level supervision. 
For the video object segmentation (VOS) task, BoxVOS \cite{hannan2022box} and QMRA \cite{lin2021query} utilize the motion map and feature aggregation from consecutive frames to segment objects in videos with only box supervision. 
% QMRA \cite{lin2021query} aggregates the rich information in memory frames and box annotations.
For the VIS task, FlowIRM \cite{liu2021weakly} presents a class-supervised VIS baseline with relatively low performance. In this work, we propose the first box-supervised VIS framework and demonstrate its effectiveness. 
