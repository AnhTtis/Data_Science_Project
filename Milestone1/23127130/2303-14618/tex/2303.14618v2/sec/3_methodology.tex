\section{Methodology}

We first extend the pixel-supervised VIS method to a box-supervised VIS (BoxVIS) baseline in \cref{sec:boxvis_baseline}, then propose a box-center guided spatial-temporal pairwise affinity (STPA) loss and a large scale box-annotated VIS dataset (BVISD) in \cref{sec:st_pair} and \cref{sec:bvisd}, respectively.

\subsection{BoxVIS baseline}\label{sec:boxvis_baseline}
% \subsection{Preliminaries}\label{sec:boxvis_baseline}
In this subsection, we first briefly introduce the box-supervised segmentation loss \cite{tian2020boxinst} and the pseudo mask supervision loss with the high-quality pseudo masks \cite{lan2021discobox,cheng2022boxteacher}, then extend the per-clip input based M2F-VIS model \cite{cheng2021mask2former-video} to its box-supervised counterpart, namely M2F-VIS-box, as a BoxVIS baseline.

\textbf{Box-supervised segmentation loss \cite{tian2020boxinst}} consists of the projection loss and the pairwise affinity loss. Projection loss supervises  the  horizontal  and  vertical  projections of predicted masks using the ground-truth bounding boxes via Dice loss \cite{dice1945dice, milletari2016dice_vnet}, termed as ${L}_{proj}$.
% which ensures that the tightest box covering the predicted mask  matches  the  ground-truth  box. 
% The projection loss is formulated as:
% \begin{equation}
% \Scale[0.96] { {L}_{proj} =  {L}({\max\limits_y (M),\ \max\limits_y (M_{\bar{B}}}) + {L}({\max\limits_x (M),\ \max\limits_x (M_{\bar{B}})}), } \nonumber 
% & = {\displaystyle{L}({proj_x (M),\ proj_x (\bar{B})}) + {L}({proj_y (M),\ proj_y (\bar{B})}),} \nonumber \\ 
% \end{equation}
% where $M_{\bar{B}}$ denotes the ground-truth boxed mask, where points in the inner bounding box are 1, and otherwise 0.
Pairwise affinity loss is designed to supervise pixel-wise mask prediction without pixel-wise annotation. 
For two pixels, if their color similarity is grater than a threshold, they are assumed to have the same mask label, denoted as $y_e = 1$. Their pairwise mask affinity is $ P(y_e=1) = {M}_i\cdot {M}_j + (1-{M}_i)\cdot(1-{M}_j)$,
where ${M}_i$ and ${M}_j$ represent the predicted masks of the two pixels, respectively.
% For two pixels located at $(x_i, y_i)$ and $(x_j, y_j)$, their color similarity is defined as $S_{lab} = \text{exp}(-\frac{\parallel c_i - c_j\parallel}{\theta})$, where $c_i$ and $c_j$ are the color vectors in the LAB color space, and $\theta$ is a hyper-parameter (set as 2 by default). If the color similarity of two pixels is grater than a threshold $\tau$, they are assumed to have same mask labels, denoted as $y_e = 1$. Hence, the pairwise mask affinity is formulated as 
% $ P(y_e=1) = {M}_i\cdot {M}_j + (1-{M}_i)\cdot(1-{M}_j)$,
% where ${M}_i$ and ${M}_j$ represent predicted masks of the two pixels, respectively.
The pairwise affinity loss is
\begin{equation}\label{eq:pair}
   \vspace{-1mm}
   {L}_{pair}  =  -\frac{1}{N} \sum\limits_{e\in E_{in}} \mathds{1}_{\{S_{lab} \geq \tau_{lab}\}} \log P(y_e = 1).
\end{equation}
where $S_{lab}$ and $\tau_{lab}$ are the color similarity and the color threshold in the LAB space, respectively. $E_{in}$ indicates the set of paired pixels in the inner bounding box, and $N$ is the total number of the pixel pairs with the same labels in $E_{in}$.

\textbf{Pseudo mask supervision loss.} Exponential moving average (EMA) teacher \cite{lan2021discobox,cheng2022boxteacher} can generate high-quality pseudo instance masks of the ground-truth boxes. By replacing the ground-truth masks with the pseudo masks, the pixel-wise mask supervision loss can be extended accordingly to the pseudo mask supervision loss, termed as $L_{pse\text{-}mask}$, which usually consists of the binary cross-entropy (BCE) loss and the Dice loss \cite{dice1945dice, milletari2016dice_vnet}.
% The box-supervised IIS models with EMA employ a sophisticated Teacher Net and a Student Net in training. The Student Net is normally optimized with the aforementioned box-supervised losses and pseudo-mask loss, and the Teacher Net is progressively updated via EMA without back-propagating gradients.

\textbf{BoxVIS baseline.} By using the above two box-supervised loss terms and the pseudo mask supervision loss, we extend the pixel-supervised VIS method M2F-VIS \cite{cheng2021mask2former-video} to a BoxVIS baseline, namely M2F-VIS-box.
The overall architecture of the BoxVIS baseline is shown in \cref{fig:boxvis_baseline}(a). 
During training, a short video clip will be fed to the sophisticated Teacher Net and the Student Net to simultaneously predict object masks. 
On one hand, the predicted masks from the Teacher Net will be taken as the pseudo masks of the ground-truth boxes by the Hungarian matching algorithm, whose cost matrix consists of the classification loss and the projection loss. 
On the other hand, the predicted masks from the Student Net will be matched with the ground-truth boxes and the generated pseudo masks, whose matching cost matrix takes the assigned pseudo masks into account. 
Finally, the matched masks from the Student Net are supervised by the ground-truth boxes and the pseudo masks via the box-supervised segmentation loss and the pseudo-mask supervision loss, respectively. 
Note that the parameters of the Teacher Net are progressively updated via EMA.
During inference, the Teacher Net will be discarded, and we only use the predicted masks by the Student Net.

Beside, the low-quality pseudo masks may introduce some incorrect pixel-wise supervision for the pseudo-mask loss, resulting in performance degradation. The confidence scores for estimating the mask quality are computed as the product of the classification score and the projection score. To select high-quality pseudo masks, we adopt a dynamic threshold, which is determined by the ratio of the number of processed iterations to the total number of training iterations: $\epsilon = 1/(1+e^{-2r})$, where $r={\text{iters}}/{\text{total iters}}$.


% --------------------------------------------------------------------------------
\subsection{Spatial-temporal Pairwise Affinity Loss} \label{sec:st_pair}
Segmenting instances from videos is more challenging than segmenting objects from individual images, as videos often have unexpected motion blur, uncommon-camera-to-object motion, appearance changes, heavy occlusion and so on.  
The pairwise affinity loss in Eq. (\ref{eq:pair}) only considers the paired pixels within a single frame, which may however fail for BoxVIS to constrain the temporal consistency of segmented objects. 
One naive solution is to directly select the neighbours at the same positions in the next frame to perform temporal pairwise affinity loss. Unfortunately, objects in a video often change their positions due to camera jitters or object motion. The assumption that two adjacent pixels in an image will have similar colors and hence the same mask labels does not hold well for adjacent frames in a video. That is, pixels from two adjacent frames may have similar colors but they can belong to different instances.
To solve this issue, we propose a box-center guided spatial-temporal pairwise affinity (STPA) loss, which uses the box-center guided shifting to generate the temporally paired pixels, and uses the color similarity and the patch feature correlation to stably compute the pairwise affinity.

\textbf{Spatial-temporally paired pixels.}
With an undirected graph, combining a pixel's left, top, right and bottom neighbours is equal to combining its right and bottom neighbours, but the latter is more efficient in training.
For a pixel in a video clip, we consequently only consider its right and bottom neighbours in the current frame, namely the set $E_{s}$ of spatial neighbours. 
We then discuss how to use the ground-truth bounding boxes to identify inter-frame paired pixels. The ground-truth bounding boxes in consecutive frames can indicate coarse inter-frame movements of the objects. 
For an instance that appears in two adjacent frames $t_i$ and $t_j$, we denote by $(t_i, x_i^c, y_i^c)$ and $(t_j, x_j^c, y_j^c)$ its bounding box centers, and by $(dx^c, dy^c) = (x_j^c-x_i^c, y_j^c-y_i^c)$ the location offsets of the two centers.
Then, for any pixel at position $(t_i, x_i, y_i)$ in frame $t_i$, if it locates in the inner bounding box of the instance, we can shift its position to the nearby area of the instance bounding box in frame $t_j$ by $(t_j, x_i+dx^c, y_i+dy^c)$.
Consequently, the set $E_{t}$ of temporal neighbours can be produced by shifting the center pixel and its four spatial neighbours in frame $t_i$ to the corresponding positions in frame $t_j$. 
Overall, for each pixel, we group its seven neighbours as paired pixels to compute the proposed STPA loss. The schematic and the generated spatial-temporal pixels are illustrated  in \cref{fig:boxvis_baseline}(b).

\textbf{Patch feature correlation.}
It is not reliable enough to utilize pixel-to-pixel color similarity to determine whether two pixels have the same labels or not. Therefore, we introduce an extra patch feature correlation to compute stable and reliable temporal pairwise affinity. 
% The color similarity of two patches is defined as
% \begin{equation}
 % { {S}_{lab} = \frac{1}{|\Omega|} \sum_{o \in \Omega} \text{exp} (-\frac{\parallel c_{i+o} - c_{j+o} \parallel }{\theta}), }
% \end{equation}
% where $c_{i+o} $ and $c_{j+o}$ are the color vectors of the two pixels at positions $(t_i, x_i+o_x, y_i+o_y)$ and $(t_j, x_j+o_x, y_j+o_y)$ in the LAB color space.  $\theta$ is a hyper-parameter, being 2 as default. 
For a pixel at position $(t_i, x_i, y_i)$, we represent the pixels in its centered patch as $(t_i, x_i+o_x, y_i+o_y)$, where the displacements $(o_x, o_y) \in {\normalsize [-k, k]\times [-k, k]}$ and $k$ controls the patch size. The feature correlation of two patches is 
\begin{equation}
 \begin{normalsize}
 % { {S}_{corr} = \frac{1}{|\Omega|} \sum_{o \in \Omega} 1 - \text{exp} (-\frac{<f_{i+o}, f_{j+o}>}{d}), }
 {S}_{corr} = \frac{1}{|\Omega|} \sum_{o \in \Omega}\ \frac{<f_{i+o},\ f_{j+o}>}{\parallel f_{i+o} \parallel \cdot \parallel f_{j+o} \parallel }, 
 \end{normalsize}
\end{equation}
where $f_{i+o}$ and $f_{j+o}$ are the feature vectors at positions $(t_i, x_i+o_x, y_i+o_y)$ and $(t_j, x_j+o_x, y_j+o_y)$ in the last pixel encoder layer of Mask2Former framework. $|\Omega|$ is the total number of pixels within the patch $\Omega = {\normalsize [-k, k]\times [-k, k]}$, and $k$ is set as 1 by default.

The overall pairwise affinity is $S_e = S_{lab} + 0.5S_{corr}$.
To remove the paired pixels with low affinity, we introduce a color similarity threshold $\tau_{lab}$ and a correlation threshold $\tau_{corr}$, respectively. The affinity threshold used in our paper is $\tau=\tau_{lab}+0.5\tau_{corr}$. Without otherwise specified, we set $\tau_{lab}=0.3$ and $\tau_{corr}=0.9$ by default, \ie, $\tau=0.75$.
% We still adopt the mask pairwise affinity $ P(y_e=1) = {M}_i\cdot {M}_j + (1-{M}_i)\cdot(1-{M}_j)$, and $M_i$ and $M_j$ are the predicted masks at positions $(t_i, x_i, y_i)$ and $(t_j, x_j, y_j)$, respectively.
Finally, the STPA loss can be formulated as:
\begin{align}\label{eq:stpair}
\begin{normalsize}
   {L}_{stpa}  =  -\frac{1}{N} \sum\limits_{e\in E_{stin}} \mathds{1}_{\{S_e \geq \tau\}} \log P(y_e = 1).
\end{normalsize}
\end{align}
where $P(y_e=1)$ is the mask pairwise affinity \cite{tian2020boxinst}, and $E_{stin}$ indicates the set of the spatial-temporally paired pixels, among which at least one pixel locates in the inner bounding box of the instance in current frame. $N$ is the total number of the paired pixels with affinity higher than the threshold $S_e$ in the set $E_{stin}$. 
% Note that we only take the right pixel $(x_i+s, y_i)$ and bottom pixel $(x_j, y_j+s)$ into account, where $s$ is the stride. 

% --------------------------------------------------------------------------
\subsection{Box-annotated VIS dataset (BVISD) } \label{sec:bvisd}
It is very costly to annotate fine-grained masks for instances in videos. 
We propose a larger scale box-annotated VIS dataset (BVISD) by merging the videos from current VIS benchmarks (\ie, YouTube-VIS and OVIS), and converting images from COCO to short pseudo video clips. 
For YouTube-VIS benchmark, we adopt the latest YTVIS21, which contains 2,985 training videos over 40 categories, and the video length is less than 36 frames. OVIS \cite{qi2021occluded} includes 607 training videos over 25 categories, but each frame has crowded objects with different occlusion levels. 

\input{figs/bvisd_cate}

There are 25 overlapping categories between COCO and VIS benchmarks, including around 90k images and 450k objects. To augment the single image from COCO to a video clip, we first resize the image by adjusting its short edge in the range of [600, 800], and then randomly crop a region with the short edge within [320, 512]. Finally, the cropped regions are randomly rotated within the degree of [-15, 15]. We repeat the above augmentation process for $T$ times to obtain a pseudo video clip with $T$ frames. During training, the short video clip augmented from COCO will be further resized into the input resolution, \ie 360p. An example of pseudo clip generation is illustrated in the \textbf{supplemental materials}.

One key step to consolidate the above three datasets is how to properly  merge the object categories to avoid semantic conflicts.
Fortunately, the 25 categories of OVIS are mostly contained in the 40 categories of YTVIS21, except for the `sheep' category. Besides, the categories `car' and `truck' in YTVIS21 are merged into a super-category `vehicle' in OVIS. 
In BVISD, we first merge the similar categories into their super-category to avoid ambiguity, and keep the remaining object categories in YTVIS21 and OVIS. For COCO, we use images that contain at least one object in the categories of BVISD. The distribution of the objects in BVISD is illustrated in \cref{fig:bvisd_cate}. 
% where the number of `person' category is divided by 5 for better display.

The statistics of YTVIS21, OVIS, COCO and BVISD are shown in \cref{tab:bvisd_data}. Overall, the proposed BVISD consists of 222k frames, 978k objects and 40 non-conflicting object categories with only object box annotations.
It has been shown \cite{papadopoulos2017extreme,lin2014microsoft} that labeling the bounding box of an object takes only 7 seconds, while labeling its polygon-based mask needs 79.2 second in COCO \cite{lin2014microsoft}. One can estimate that annotating the object bounding boxes in BVISD takes $7 \times 978,000$ seconds (about 80 worker days), while labeling the pixel-wise annotations takes $79.2 \times 978,000$ seconds (about 898 worker days). 

\input{tables/bvisd}

% Since the ground-truth labels of current VIS benchmarks are not released, we have to submit the predicted results to the CodeLab website to evaluate the instance segmentation performance. 
To more easily test the models, we split the BVISD dataset into two separated parts: the training set and the valid set, with the valid set containing around 10\% videos of BVISD. 
As shown in \cref{fig:bvisd_cate}, some categories are only presented in YTVIS21 but not OVIS and COCO, which may result in unbalanced categories. During training, therefore, we sample the clips from the three datasets with different sampling weights to alleviate the issue. 
During inference, we test our proposed BoxVIS with three valid sets: official YTVIS21 valid, official OVIS valid and our BVISD valid. When testing on official YTVIS21 and OVIS valid sets, the predicted categories will be mapped back to the original object categories of the source dataset.
% Unlike the single type of videos in YTVIS21 or OVIS valid sets, the proposed BVISD valid-sub includes their joint video types: diverse video lengths from 19 to 120 frames, crowded or sparse objects, and fast-moving or steady camera shifts. 
% Consequently, the instance segmentation performance on BVISD valid-sub can better reflect model generalization ability. 
The code to generate BVISD can be found at \url{https://github.com/MinghanLi/BoxVIS}.


