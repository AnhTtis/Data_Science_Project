%---------------------- CoCo -----------------------------
\begin{table*}
\begin{center}
\begin{tabular}{p{0.05\textwidth}<{\centering}p{0.08\textwidth}<{\centering}p{0.09\textwidth}|p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.035\textwidth}<{\centering}|p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.035\textwidth}<{\centering}}
\hline
Sup. & Matcher & Criterion & \multicolumn{6}{c|}{Box} & \multicolumn{6}{c}{Mask}  \\
     & Mask    &  Mask  & AP & AP$_{50}$ & AP$_{75}$ & AP$_s$  & AP$_m$ & AP$_l$ & AP & AP$_{50}$ & AP$_{75}$ & AP$_s$  & AP$_m$ & AP$_l$\\
\hline\hline
\multicolumn{3}{l}{CondInst}\\
Mask       & biou   & bce + dice  & - & - & - & - & - & - & 35.6 & 56.3 & 37.8 & 16.9 & 38.9 & 51.0 \\
Box        & biou   & proj        & - & - & - & - & - & - & 21.2 & 45.2 & 17.7 & 10.0 & 21.4 & 32.5 \\
Box        & biou   & proj + pair & - & - & - & - & - & - & 30.7 & 52.2 & 31.1 & 13.8 & 33.1 & 45.7 \\
% BoxTeacher   
Box        & biou   & ++pseudo    & - & - & - & - & - & - & 34.2 & 56.0 & 35.4 & - & - & - \\
\hline\hline
\multicolumn{3}{l}{Mask2former} \\
Mask       & bce+dice   & bce + dice  & 45.9 & 65.2 & 49.1 & 27.2 & 48.5 & 62.8 & 43.1 & 65.4 & 46.4 & 22.5 & 46.2 & 64.2\\
Box        & proj   & proj        & 43.3 & 61.1 & 46.3 & 24.8 & 46.9 & 59.9 & 33.6 & 56.7 & 34.2 & 15.2 & 36.0 & 52.0 \\
% & box        & proj   & proj + tv  & 44.8 & 64.3 & 47.9 & 26.3 & 48.5 & 61.0 & 35.0 & 59.1 & 35.4 & 16.4 & 37.4 & 53.2 \\
% & box        & proj   & ++EMA      & 45.9 & 64.4 & 49.4 & 26.7 & 49.5 & 63.6 & 37.5 & 60.9 & 38.4 & 18.7 & 39.4 & 57.1 \\
% \cline{2-16} 
Box        & proj    & proj + pair & 45.8 & 64.4 & 49.1 & 27.4 & 49.4 & 61.7 & 37.6 & 60.7 & 39.4 & 19.2 & 40.5 & 55.1 \\
Box        & proj    & +pseudo    & 46.5 & 65.2 & 50.0 & 28.3 & 49.9 & 62.1 & 39.2 & 62.1 & 41.4 & 20.3 & 41.5 & 57.9 \\
\hline
\end{tabular}
\end{center}
\caption{Results on COCO val2017 set. In mask2former, cross-attn module achieves implicitly a pairwise loss in feature similarity, which performs better than the original lab color similarity of BoxInst\cite{tian2020boxinst}. }
\end{table*}


% ------------------------------------------------------------------------
\begin{table*}
\begin{center}
\begin{tabular}{p{0.09\textwidth}<{\centering}p{0.07\textwidth}<{\centering}|p{0.05\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.04\textwidth}<{\centering}|p{0.06\textwidth}<{\centering}p{0.05\textwidth}<{\centering}p{0.05\textwidth}<{\centering}|p{0.06\textwidth}<{\centering}p{0.05\textwidth}<{\centering}|p{0.06\textwidth}<{\centering}}
\hline
\multirow{2}{*}{Methods} & \multirow{2}{*}{Sup.} & \multicolumn{3}{c|}{Training Data} & \multicolumn{3}{c|}{YTVIS21} & \multicolumn{2}{c|}{{ \color{red} OVIS}} & JVIS\\
                         & & ytvis21 & ovis & coco & val-sub & val & val22 & val-sub & val  & val-sub\\
\hline
\multicolumn{2}{l}{MinVIS} \\
\hline
& Mask & $\checkmark$ &   &  & 48.3  & 43.3 & 31.0 & -    & -    & 34.0 \\
& Mask & & $\checkmark$   &  & -     & -    & -    & 21.5 & 28.8 & 13.6 \\
& Mask & 2  & 1 &            & 49.4  & 45.7 & 33.5 & 21.4 & 29.2 & 35.3 \\
& Mask & 2  & 1 & 1          & 51.6  & 46.0 & 29.4 & 23.2 & 30.0 & 36.8 \\
\hline
& Box  & $\checkmark$ &   &   & 46.8 & 40.5 & 25.1 & -    & -    & 31.1 \\
& Box  & & $\checkmark$   &   & -    & -    & -    & 16.7 & 22.0 & 12.8 \\
& Box  & 2  & 1 &             & 48.3 & 42.0 & 26.0 & 18.7 & 23.9 & 33.9 \\
& Box  & 2  & 1 & 1           & 48.9 & 42.8 & 30.4 & 19.2 & 25.0 & 34.4 \\
\hline
\multicolumn{2}{l}{Mask2Former-video} \\
\hline
& Mask & $\checkmark$ &   &   & 47.2  & 43.1 & 33.5  & -    & -    & 31.9 \\
& Mask & & $\checkmark$   &   & -     & -    & -     & 22.7 & 27.9 & 13.7 \\
& Mask & 2 & 1 &              & 50.1  & 43.8 & 31.7  & 22.9 & 30.9 & 35.3 \\
& Mask & 2 & 1 & 1            & 50.3  & 44.7 & 37.5  & 26.1 & 31.8 & 36.6 \\
\hline
& Box  & $\checkmark$ &   &   & 47.8  & 43.1 & 32.8  & -    & -    & 32.5 \\
& Box  & & $\checkmark$   &   & -     & -    & -     & 20.9 & 25.7 & 13.8 \\
& Box  & 2 & 1 &              & 47.5  & 40.3 & 29.7  & 21.8 & 27.9 & 34.2 \\
& Box  & 2 & 1 & 1            & 48.7  & 42.9 & 31.7  & 23.3 & 28.9 & 35.0 \\
\hline
BoxVIS \\
\hline
\multirow{4}{*}{+temp-pair} 
& Box  & $\checkmark$ &   &   & 48.0  &   &   & -    & - \\
& Box  & & $\checkmark$   &   & -     & - & - & 20.7 &   \\
\cline{2-11}
& Box  & 2 & 1 &   & 48.4  &      &      & 21.7 &   \\
& Box  & 2 & 1 & 1 & 48.8  &      &      & 24.2 & 27.9 \\
coco-pse2
& Box  & 2 & 1 & 1 & 48.7  &      &      & 24.7 & 29.0 \\
\hline 
\multirow{2}{*}{+attnst3}
& Box  & 2 & 1 &   & 47.2 &       &      & 23.2 & \\
& Box  & 2 & 1 & 1 & 49.2 & 43.2  & 27.7 & 24.0 & 28.3 \\
\hline  
\end{tabular}
\end{center}
\caption{Results on YouTubeVIS-2021 and OVIS valid sets. YouTubeVIS-2021 with 360p, while OVIS with 480p. The default pseudo coco clip is first resize coco image [400, 500, 600] and crop size (480, 600), while PseClip2 means that resize coco image [600, 700, 800] and crop size (320, 480). The later can produce the pseudo clip with large camera motion.}
\end{table*}


\begin{table*}
\begin{center}
\begin{tabular}{p{0.075\textwidth}<{\centering}p{0.075\textwidth}<{\centering}p{0.095\textwidth}<{\centering}|p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.04\textwidth}<{\centering}|p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.04\textwidth}<{\centering}|p{0.04\textwidth}<{\centering}}
\hline
\multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Finetune-VIS} & \multicolumn{5}{c|}{YT21} & \multicolumn{5}{c|}{YT22}  \\
       & Sup. & Data  & AP & AP$_{50}$ & AP$_{75}$ & AR$_1$  & AR$_{10}$ & AP & AP$_{50}$ & AP$_{75}$ & AR$_1$  & AR$_{10}$ & Mean\\
\hline\hline
\multirow{3}{*}{MinVIS} 
& Mask & yt21      & 43.3 & 64.0 & 46.4 & 39.2 & 51.3 & 31.0 & 56.1 & 31.6 & 27.4 & 33.8 & 37.1\\
& Mask & 2yt+1ovis & 45.7 & 65.8 & 50.2 & 39.4 & 51.6 & 33.5 & 55.8 & 32.4 & 28.7 & 38.0 \\
& Mask & ++1coco   & 46.0 & 66.0 & 51.4 & 40.7 & 53.0 & 29.4 & 49.3 & 31.1 & 28.1 & 33.8 \\
\hline
\multirow{3}{*}{MinVIS-box}
& Box  & yt21      & 40.5 & 64.1 & 44.2 & 37.1 & 47.2 & 25.1 & 47.7 & 25.4 & 24.1 & 30.0 & 33.0 \\
& Box  & 2yt+1ovis & 42.0 & 64.1 & 44.0 & 37.7 & 49.3 & 26.0 & 51.1 & 26.7 & 23.1 & 30.1 & 34.0 \\
& box  & ++1coco   & 42.8 & 64.2 & 46.9 & 37.3 & 48.6 & 30.4 & 53.2 & 31.5 & 26.5 & 36.0 \\
\hline\hline
\multirow{3}{*}{M2F-VIS}
& Mask & yt21      & 43.1 & 63.3 & 46.2 & 39.1 & 51.0 & 33.5 & 55.8 & 39.1 & 30.4 & 39.5 \\
& Mask & 2yt+1ovis & 43.8 & 65.5 & 46.7 & 39.6 & 52.0 & 31.7 & 57.2 & 31.4 & 27.1 & 37.7 \\
& Mask & ++1coco   & 44.7 & 64.8 & 48.5 & 39.3 & 52.2 & 37.5 & 61.1 & 38.2 & 32.5 & 44.8 \\
\hline
\multirow{3}{*}{M2F-VIS-box}
& Box  & yt21      & 43.1 & \\
& Box  & 2yt+1ovis & 40.3 & 61.1 & 43.4 & 37.1 & 48.1 & 29.7 & 51.5 & 29.4 & 28.0 & 35.3 & 35.0\\
& Box  & ++1coco   & 42.9 \\

\hline\hline
\multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Finetune-VIS} & \multicolumn{8}{c}{OVIS}   \\
       & Sup. & Data  & AP & AP$_{50}$ & AP$_{75}$ & AP$_{so}$ & AP$_{mo}$ & AP$_{ho}$ & AR$_1$  & AR$_{10}$ \\
\hline
MinVIS 
& Mask & ovis      & 28.8 & 51.2 & 28.9 & 44.8 & 33.0 & 11.8 & 14.3 & 33.4 \\
\multirow{2}{*}{BoxVIS}
& Box  & ovis      & 22.0 & 41.0 & 20.9 & 39.0 & 27.2 &  6.6 & 12.8 & 26.5 \\
& Box  & 2ovis+1yt & 23.4 & 43.1 & 22.6 & 43.8 & 27.1 &  7.6 & 13.2 & 26.7 \\
\hline
\multirow{2}{*}{BoxVIS-clip}
& Mask & 2yt+1ovis & \\
& Box  & 2yt+1ovis & 27.9 & 49.4 & 28.8 & 45.5 & 32.0 & 10.3 & 14.5 & 32.4 \\
\hline
\end{tabular}
\end{center}
\caption{Results of BoxInst and MinVIS on YouTube-VIS 2021 and 2022 valid sets. }
\end{table*}

\subsection{Coarse-to-fine pairwise affinity loss}
For one of query features denoted as $q$, we reformulate the masked attention \cref{eq:mask-attn} as $X^l_q = \Delta X^l_q + X^{l-1}_q $ , where the derived value is
\begin{align}\label{eq:deltax}
    \Delta X^l_q  := W^l_q V^l = \text{softmax}(\mathcal{M}^{l-1}_{a, q}+q^l{K^l}^T)V^l.
\end{align}
At a specific feature location $(x,y)$, if the predicted mask ${M}^{l-1}_q(x, y) <= 0.5$, then the attention mask $\mathcal{M}^{l-1}_{a,q}(x, y)=-\infty$, finally leading to $W_{q}^l = 0$. This demonstrates that only the features in the activated mask area contributes to the query feature $q$ in the $l$ layer. 
% \begin{equation}
    % W_{n,i}^l = \frac{\exp(\mathcal{M}_{n,i}^l + Q_n^l {K_i^l}^T)}{\sum_{j=1}^{H^lW^l} \exp(\mathcal{M}_{n,j}^l + Q_n^l {K_j^l}^T)} = 0.
% \end{equation}
Therefore, we group the predicted object locations as an activate set 
$\Omega^{l-1}_q := \{(x, y) | M^{l-1}_q(x, y) > 0.5\}.$
Then Eq. (\ref{eq:deltax}) can be reformulated as
\begin{align}
    \Delta X^l_n & = \sum\nolimits_{i \in \Omega^{l-1}_n} W_{n,i}^l V_i^l \nonumber \\
                 & = \sum\nolimits_{i \in \Omega^{l-1}_n} \frac{1} {\sigma_n^l} \exp(Q_n^l {K^l_i}^T) V_i^l
\end{align}
where $\sigma_n^l = \sum_{i \in \Omega^{l-1}_n} \exp(Q_n^l {K_i^l}^T)$, and the normalization scale ensures that the sum of the weights $W_{n,i}^l$ is 1. Actually the derived value is set to a weighted average of the masked features that are predicted as instances in the last decoder layer.

\textbf{3D Total Variation generalization loss.}
We assume that two neighbouring pixels at feature locations $(x_i, y_i), (x_j, y_j)$ have high feature similarity. The 3D total variation norm (3DTV) with an anisotropic version is 
\begin{align}
    L_\text{3DTV} & = | M(x_i+1, y_i,   t_i  )-M(x_i, y_i, t_i)| \nonumber \\
                  & + | M(x_i,   y_i+1, t_i  )-M(x_i, y_i, t_i)| \nonumber \\
                  & + | M(x_i,   y_i,   t_i+1)-M(x_i, y_i, t_i)|
\end{align}

The total training losses can be written as:
\begin{align}
    {L} & = \lambda_1 {L}_{cls} + \lambda_2 {L}_{box\text{-}mask} \nonumber \\
        & =  \lambda_1 {L}_{cls} + \lambda_2 ({L}_{proj} + \gamma L_\text{3DTV} + (1-\gamma) {L}_\text{affinity}).
\end{align}
where $\gamma = 1$ in the first 12 epochs, otherwise $0$.

Since the total training loss has fast convergence, the first 12 epochs employ total variation generalization to split the input image into coarse regions (contributes to \textbf{fast convergence}), while the last 24 epochs employ pairwise affinity loss to encode fine relationships for paired pixels, at least one of which is in the bounding boxes. 


\subsection{Gaussian blur}
The Gaussian blur is a type of image-blurring filter that uses a Gaussian function for calculating the transformation to apply to each pixel in the image. The formula of a Gaussian blur in three dimensions is 
\begin{equation}
    G(d_x, d_y, d_t) = \frac{1}{2\pi \sigma^2} \exp(-\frac{d_x^2+d_y^2+d_t^2}{2\sigma^2}),
\end{equation}
where $d_x, d_y, d_t$ are the distances from the origin to the destination in the horizontal axis, the vertical axis and the temporal axis respectively, and $\sigma$ is the standard deviation of the Gaussian distribution. Values from this distribution are used to build a convolution matrix which is applied to the original image. Each pixel's new value is set to a weighted average of the pixel's neighborhood. The original pixel's value receives the heaviest weight (having the highest Gaussian value) and neighboring pixels receive smaller weights as their distance to the original pixel increases.

\subsection{Masked-attention}
The masked attention modulates the attention matrix via
\begin{align}
    X^l = \text{softmax}(\mathcal{M}_a^{l-1}+Q^l{K^l}^T)V^l+X^{l-1},
    \label{eq:mask-attn}
\end{align}
where the attention mask $\mathcal{M}_a^{l-1}$ at feature location $(x,y)$ is 
\begin{equation}
    \mathcal{M}_a^{l-1}(x,y) = 
        \begin{cases}
            0,        & \text{if}\ M^{l-1}(x,y) > 0.5, \\
            -\infty,  & \text{otherwise}.  
        \end{cases}   
\end{equation}  
Here, $l$ is the layer index, $X^l \in R^{N\times C}$ refers to $N$ $C$-dimensional query features at the $l^{th}$ layer and $Q^l=f_Q(X^{l-1}) \in R^{N\times C}$. $K^l, V^l \in R^{H^l W^l\times C}$ are the image features under transformation $f_K(\cdot)$ and $f_V(\cdot)$ respectively, and $H^l$ and $W^l$ are the spatial resolution of image features. $f_Q, f_K, f_V$ are linear transformations.


\subsection{Mask generation}
The query-based mask prediction in the $l-1$ layer is 
\begin{align}
    M^{l-1} & = \sigma (F \cdot \text{MLP}(X^{l-1})^T) > 0.5 \nonumber \\
            & = (F \cdot \text{MLP}(X^{l-1})^T > 0 \nonumber
\end{align}
where $F\in R^{T\times H\times W\times D}$ refers to the mask features, and $ \text{MLP}(X^{l-1}) \in R^{N\times D}$ denotes the $N$ predicted mask parameters. The activated function $\sigma$ employs the Sigmoid function.
The $i^{th}$ predicted masks at feature location $(t, x, y)$ is
\begin{align}
    M^{l-1}_n(t, x, y) & =(F(t, x, y) \cdot \text{MLP}(X^{l-1}_i)^T \nonumber \\
                       & = |F(t, x, y)| \cdot |\text{MLP}(X^{l-1}_n)| \nonumber\\
                       &\quad\ cos(F(t, x, y), \text{MLP}(X^{l-1}_n))
\end{align}
Due to $|\cdot| \geq 0$, $M^{l-1}_n(t, x, y) > 0$ equals to $cos(F(t, x, y), \text{MLP}(X^{l-1}_n)) > 0$. Given the predicted masks of the last layer, masked attention knows all possible locations of the instance. 

\begin{table*}
\begin{center}
\begin{tabular}{p{0.15\textwidth}p{0.07\textwidth}<{\centering}|p{0.05\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.03\textwidth}<{\centering}p{0.09\textwidth}<{\centering}p{0.09\textwidth}<{\centering}|p{0.07\textwidth}p{0.07\textwidth}}
\hline
\multirow{2}{*}{Methods} & \multirow{2}{*}{Sup.} & \multicolumn{5}{c|}{Training Data} & \multicolumn{1}{l}{ytvis21} & \multicolumn{1}{l}{ovis}\\
                         & & ytvis21 & ovis & coco & num. insts & anno. time & val-sub & val-sub   \\
\hline\hline
\multirow{2}{*}{MinVIS} 
& Mask & $\checkmark$  & $\checkmark$ &               & 504k  & 462d & 49.4  & 21.4  \\
& Mask & $\checkmark$  & $\checkmark$ & $\checkmark$  & 831k  & 762d & 51.6{\color{blue} $^{+2.2}$}  & 23.2{\color{blue} $^{+1.8}$}  \\
\hline
\multirow{2}{*}{Mask2Former-video} 
& Mask & $\checkmark$  & $\checkmark$ &               & 504k  & 462d & 50.1  & 22.9  \\
& Mask & $\checkmark$  & $\checkmark$ & $\checkmark$  & 831k  & 762d & 50.3{\color{blue} $^{+0.2}$}  & 26.1{\color{blue} $^{+3.2}$}  \\
\hline 
\multirow{2}{*}{BoxVIS-frame}
& Box  & $\checkmark$  & $\checkmark$ &               & 504k  & 41d & 48.3 & 18.7 \\
& Box  & $\checkmark$  & $\checkmark$ & $\checkmark$  & 831k  & 67d & 48.9{\color{blue} $^{+0.6}$} & 19.2{\color{blue} $^{+0.5}$} \\
\hline
\multirow{2}{*}{BoxVIS-clip}
& Box  & $\checkmark$  & $\checkmark$ &              & 504k  & 41d & 47.5 & 21.8 \\
& Box  & $\checkmark$  & $\checkmark$ & $\checkmark$ & 831k  & 67d & 48.7{\color{blue} $^{+1.3}$}  & 23.3{\color{blue} $^{+1.5}$} \\
\hline
\multirow{2}{*}{+temp. pair}
& Box  & $\checkmark$  & $\checkmark$ &              & 504k  & 41d &  \\
& Box  & $\checkmark$  & $\checkmark$ & $\checkmark$ & 831k  & 67d &  48.8 & 24.2 \\
\hline\hline
\end{tabular}
\end{center}
\caption{Results comparison of SOTA methods with our proposed joint training on BJVIS data set. }
\end{table*}

\begin{table}
\centering
\begin{tabular}{p{0.04\textwidth}<{\centering}p{0.02\textwidth}<{\centering}p{0.04\textwidth}<{\centering}p{0.02\textwidth}<{\centering}|p{0.06\textwidth}<{\centering}p{0.06\textwidth}<{\centering}p{0.04\textwidth}<{\centering}}
\Xhline{0.8pt}
\multicolumn{4}{c|}{Affinity} & \multicolumn{3}{c}{Mask AP}\\
$S_{lab}$ & $k_{lab}$  & $S_{corr}$ & $k_{corr}$ & {\small YTVIS21} &{\small OVIS}  & {\small BVISD}\ \\
\Xhline{0.8pt}
 $\checkmark$ & 1 &              &   &      &      & 33.7 \\
 $\checkmark$ & 3 &              &   &      &      & \\
              &   & $\checkmark$ & 1 &      &      & 32.1 \\ 
              &   & $\checkmark$ & 3 &      &      &      \\ 
 $\checkmark$ & 1 & $\checkmark$ & 1 &      &      & 33.0 \\
 $\checkmark$ & 1 & $\checkmark$ & 3 &      &      & 33.7 \\  
 $\checkmark$ & 3 & $\checkmark$ & 3 &      &      & 32.1 \\ 
 $\checkmark$ & 3 & ms & 3 &      &      & 33.0 \\
% 3 & $\checkmark$ & $\checkmark$ & $\checkmark$ & \\
\Xhline{0.8pt}
\end{tabular}
\caption{Spatial-temporal pairwise affinity loss.}
\end{table}