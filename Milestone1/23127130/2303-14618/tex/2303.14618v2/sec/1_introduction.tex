\section{Introduction}
Video instance segmentation (VIS) aims to predict the pixel-wise masks and categories of instances over the input video. The recently proposed VIS methods \cite{wang2020vistr,hwang2021video,seqformer,heo2022vita,IDOL, huang2022minvis} have made great progress on the YouTube-VIS \cite{yang2019video} and OVIS \cite{qi2021occluded} benchmark datasets. Generally speaking, these methods first partition the whole video into individual frames or short clips to extract per-frame or per-clip features and predict instance masks, and then associate the predicted masks across frames/clips based on the embedding similarity of instances. However, all the current VIS methods require pixel-wise annotations, where the pixels belonging to the target instances are labeled as 1 and others as 0, to train the VIS model for pixel-wise mask prediction.

\begin{table}
\centering
% \begin{tabular}{p{0.105\textwidth}p{0.035\textwidth}<{\centering}p{0.045\textwidth}<{\centering}p{0.05\textwidth}<{\centering}p{0.05\textwidth}<{\centering}p{0.04\textwidth}<{\centering}}
\begin{tabular}{p{0.12\textwidth}p{0.045\textwidth}<{\centering}p{0.05\textwidth}<{\centering}p{0.075\textwidth}p{0.065\textwidth}p{0.05\textwidth}<{\centering}}
\Xhline{0.8pt}
{\normalsize Methods} & {\normalsize Superv.}  & { Annot.} & {\normalsize YTVIS21} &  {\normalsize OVIS} \\ % & \multicolumn{1}{l}{{\small BVISD}} \\
\Xhline{0.8pt}
{\normalsize M2F-VIS}        & Pixel & { 485d} & 43.2 & {24.5 } \\  % 213+272
{\normalsize M2F-VIS-box}    & Box   & {  43d}  & 41.2$^{{ -2.0}}$ & 23.9$^{{-0.6}}$ \\  % 19+24
{\normalsize BoxVIS (ours)}   & Box   & {  80d}  & 43.2$^{{ +0.0}}$  & 29.0$^{{+4.5}}$ \\
% {\small M2F-VIS-box}    & box   & {\small 43d}  & 41.2$^{{\color{blue} -2.0}}$ & 23.9$^{{\color{blue}-0.6}}$ & - \\  % 19+24
% {\small BoxVIS (ours)}   & box   & {\small 80d}  & 43.2$^{{\color{red} +0.0}}$  & 29.0$^{{\color{red}+4.5}}$ & 35.9 \\
% \multicolumn{1}{c}{$\Delta$}  & - & - & \multicolumn{1}{c}{{\color{red} +0.0}} & \multicolumn{1}{c}{{\color{red}+4.5}} & -\\
\Xhline{0.8pt}
\end{tabular}
\vspace{-2mm}
\caption{ Performance (Mask AP) comparison of pixel-supervised Mask2Former-VIS (M2F-VIS) \cite{cheng2021mask2former-video}, its box-supervised counterpart M2F-VIS-box, and our BoxVIS on YTVIS21 and OVIS valid sets.}\label{tab:min_boxvis}
\vspace{-2mm}
\end{table}


Compared with image instance segmentation (IIS), VIS requires a larger number of annotated training data due to the existence of object motion and complex trajectories in videos. Unfortunately, it is an expensive and labour-extensive task to label the pixel-wise object masks in a video. As a result, the amount of pixel-wise annotations in existing VIS datasets is small. For example, there are only 8k and 5k labeled instances in YouTube-VIS 2021 (YTVIS21) and OVIS, respectively, which limits the generalization capabilities of trained VIS models. However, it has been demonstrated \cite{papadopoulos2017extreme,lin2014microsoft} that labeling the bounding box of an object takes only 8.8\% (7s vs. 79.2s) the time of labeling its polygon-based mask in COCO \cite{lin2014microsoft}. 
Actually, a few box-supervised IIS methods \cite{lan2021discobox,tian2020boxinst,li2022boxlevelset,cheng2022boxteacher,lan2023MAutoLabeler} have been recently developed and achieved competitive performance with pixel-supervised IIS methods.
Therefore, one natural and interesting question is: can we use bounding boxes to label instances to train the VIS models?  

To validate the feasibility of VIS with only box annotations, we adapt the state-of-the-art pixel-supervised VIS model to a box-supervised VIS (BoxVIS) baseline. Specifically, inspired by the box-supervised IIS methods BoxInst \cite{tian2020boxinst} and BoxTeacher \cite{cheng2022boxteacher}, we introduce the box-supervised segmentation loss terms into the pixel-supervised VIS method Mask2Former-VIS (M2F-VIS) \cite{cheng2021mask2former-video}, and replace the ground-truth masks with the produced pseudo masks to supervise the model learning. 
(Please refer to Section \ref{sec:boxvis_baseline} for details.)  
The adapted BoxVIS model from M2F-VIS is termed as M2F-VIS-box. As shown in \cref{tab:min_boxvis}, compared with M2F-VIS, M2F-VIS-box drops only 2.0\% and 0.6\% mask AP on YTVIS21 and OVIS valid sets, respectively. We think there are two main reasons for the slight performance degradation. 
First, all current VIS models have been pre-trained on the COCO dataset before fine-tuned on the VIS datasets. There is a large overlap of categories between COCO and VIS datasets so that the pre-training enables a good baseline VIS model. Second, when fine-tuning on videos, ground-truth boxes and pseudo masks actually provide comparable supervision to pixel-wise annotation to learn object motion and appearance changes. 

Nonetheless, there are two challenging issues for BoxVIS.
First, without pixel-wise labeling, the box-level annotation cannot explicitly tell the segmenter the precise object boundary and the temporal association of objects. It is essential to investigate how to make the VIS models have good object spatial-temporal consistency with only box annotations.
Second, objects in videos often have significant position shifts, appearance changes, heavy occlusion, uncommon camera-to-object views, \etc. To deal with the diverse variations of videos, a larger scale box-annotated dataset is required to train robust BoxVIS models. 

With the above considerations, we propose to improve the BoxVIS performance from two aspects: modeling the object spatial-temporal consistency and increasing the amount of box-annotated video clips. 
First, we propose a box-center guided spatial-temporal pairwise affinity (STPA) loss to predict instance masks with better spatial and temporal consistency.
Second, we collect a larger box-annotated VIS dataset (BVISD) by consolidating the videos from current VIS benchmarks and converting some images from COCO to short pseudo video clips. 
The trained BoxVIS model with the proposed STPA loss and BVISD  demonstrates promising instance segmentation results, achieving 43.2\% and 29.0\% mask AP on YouTube-VIS 2021 and OVIS valid sets, respectively. It obtains comparable or even better generalization performance than the pixel-supervised VIS competitors by using only 16\% of their annotation time and cost, exhibiting great potentials.


