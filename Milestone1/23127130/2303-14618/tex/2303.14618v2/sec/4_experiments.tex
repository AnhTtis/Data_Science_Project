\section{Experiments}

\textbf{Datasets.} There are two VIS datasets with pixel-wise annotations: YouTube-VIS series \cite{yang2019video} and OVIS \cite{qi2021occluded}. For YouTube-VIS \cite{yang2019video}, we use the updated 2021 version (YTVIS21) in this paper. YTVIS21 contains 2,985 training, 421 validation, and 453 test videos over 40 categories. All the videos are annotated for every 5 frames. The number of frames per video is between 19 and 36. 
% The recently released 2022 version extends YouTube-VIS 2021 with 71/50 additional long videos in validation/test sets, respectively. The length of long videos is between 36 and 84 frames.
OVIS \cite{qi2021occluded} includes 607 training, 140 validation and 154 test videos, scoping 25 object categories. Different from YTVIS21, videos in OVIS are longer (up to 292 frames) and have more objects with different occlusion levels per frame. 
% The proportions of objects with no occlusion, slight occlusion, and severe occlusion are 18.2\%, 55.5\%, and 26.3\%, respectively. 
% Note that 80.2\%  of the instances are severely occluded in at least one frame, and only 2\% of the instances are not occluded in any frame. 

Our proposed BVISD with only box annotations contains 3524 videos from the above two VIS benchmarks and around 90k short pseudo video clips converted from COCO in the training set, and 362 videos in the validation set. BVISD combines all video types in both YTVIS21 and OVIS, and consists of 40 object categories.

\textbf{Evaluation metrics.} The metrics, including average precision (AP$_{*}$) at different IoU thresholds, average recall (AR$_{*}$) with different object numbers and the mean value of AP (AP), are adopted for VIS model evaluation. OVIS divides instances into slight occlusion (AP$_\text{so}$), moderate occlusion (AP$_\text{mo}$) and heavy occlusion (AP$_\text{ho}$).
% , whose occlusion scores are in the range of [0, 0.25], [0.25, 0.5] and [0.5, 0.75], respectively.

\textbf{Implementation details.} Unless otherwise stated, we employ the same hyper-parameters as Mask2Former-VIS \cite{cheng2021mask2former-video}, which is implemented on top of detectron2 \cite{wu2019detectron2}. All VIS models have been pre-trained on COCO image instance segmentation \cite{lin2014microsoft} with pixel-wise annotations. 
% All models with ResNet50 backbone \cite{he2016deep} are trained on eight GeForce RTX 3090 GPUs with 24G RAM, and models with Swin Large backbone \cite{Liu_2021_ICCV} are trained on four NVIDIA RTX A6000 with 48G RAM. 

The initial learning rate is set to 0.0001, and it decays by a factor of 0.1 at 10k and 12k iterations. The batch size adopts 16 video clips, and each video clip consists of $3$ frames.
During training, we resize the video clips with their shorter edge size being 320 or 512, and keep their longer edge size below 800. During inference, all frames are resized to a shorter edge size of $360$ on YTVIS21 and $480$ on OVIS and BVISD.
We employ the tracking by query embedding matching method proposed in \cite{huang2022minvis} to associate the instances across clips, and adopt the average of its predicted masks in the overlapping clips as the final masks. 

\input{tables/abl_study_baseline}

% ----------------------------------------------------------------------------------------------------------------
\subsection{Ablation study}
\textbf{Losses in BoxVIS baseline (M2F-VIS-box).} 
We study the effectiveness of the two box-supervised loss items and pseudo-mask loss in \cref{tab:abl_baseline}. 
% The BoxVIS baseline is trained on the YTVIS21 or OVIS dataset. 
One can see that BoxVIS baseline trained with only the projection loss $L_{proj}$ performs poorly, while the introduction of the pairwise affinity loss $L_{pair}$ significantly improves the performance by around 3\% AP. 
The pseudo-mask loss $L_{pse\text{-}mask}$ with the high-quality pseudo masks can further improve the performance by 1.2\% AP, which is comparable to its pixel-supervised counterpart M2F-VIS (see \cref{tab:min_boxvis}).  The use of dynamic threshold for selecting high-quality pseudo masks brings another 0.7\% AP increase in performance.

\textbf{Components of the STPA loss.} In \cref{tab:abl_stpa}, we explore the effects of the spatial-temporally paired pixels $E_s$ and $E_t$, the patch correlation $S_{corr}$, and box-center guided shifting in the proposed STPA loss. We only adopt the box-supervised loss and exclude the pseudo-mask loss to better investigate the performance changes.

The pairwise affinity loss $L_{pair}$ employs the spatially paired pixels ($|E_s|=2$) and the color similarity $S_{lab}$, obtaining 41.1\%, 25.5\% and 31.9\% AP on YTVIS21, OVIS and BVISD datasets, respectively. 
When using the default number of spatially paired pixels in BoxInst \cite{tian2020boxinst} ($|E_s|=8$), the performance is nearly the same. 
Introducing the temporally paired pixels $E_t$ into the STPA loss brings 0.7-0.9\% AP improvements on all VIS datasets. 
However, directly adding the patch correlation $S_{corr}$ to the pairwise affinity leads to significant performance drop on OVIS. This is because on object-crowded OVIS videos, patches with high feature correlation may come from different objects, resulting in incorrect pixel-wise supervision.
Fortunately, by introducing the box-center guided shifting, this issue can be alleviated and the STPA loss achieves good performance on all VIS datasets. 
% Without using patch feature correlation but with box-center guided shifting, the performance drops significantly. 
In addition, increasing the number of temporally paired pixels $|E_t|$ can slightly increase the performance. 
Overall, compared with the pairwise affinity loss used in IIS tasks, our STPA loss designed for VIS tasks can bring about 2\% AP improvement on all VIS datasets.

\input{tables/abl_study_stpa}
\input{tables/abl_study_bvisd}

\input{tables/sota_yt21_ovis}

\textbf{Sampling weights in BVISD.} Due to the unbalanced number of objects per category in BVISD (see \cref{fig:bvisd_cate}), we propose to sample the video clips from the three datasets with different sampling weights during training. Taking the last row of \cref{tab:abl_bvisd} as an example, among the 8 short video clips, 4, 2 and 2 are from YTVIS21, OVIS and COCO, respectively.
BoxVIS trained {\it separately} on YTVIS21 or OVIS obtains 42.8\% and 26.2\% AP, respectively, while the model trained jointly on YTVIS21 and OVIS with equal sampling weights shows a performance drop on YTVIS21 but a slight performance increase on OVIS. 
This is because some object categories presented in YTVIS21 are not shown in OVIS, and the object-crowded videos in OVIS contribute more to gradient back-propagation in training. By setting a higher sampling weight on YTVIS21, the performance is much improved on all VIS datasets.
By introducing the pseudo video clips from COCO and setting higher sampling weights on YTVIS21 and COCO, higher performance can be achieved. In addition, the generated pseudo video clips from COCO can bring about 1\% AP improvement.
Unless specified, the sampling weights in BVISD are set as $1/2, 1/4$ and $1/4$ for YTVIS21, OVIS and COCO, respectively.


\input{tables/sota_swinl_yt21_ovis}
\input{figs/visualizer}
% ----------------------------------------------------------------------------------------------------------------
\subsection{Main results}
With ResNet50 backbone and Swin Large backbone, we compare the proposed BoxVIS with the state-of-the-art pixel-supervised VIS methods on YTVIS21 and OVIS datasets in \cref{tab:sota_yt21_ovis} and \cref{tab:sota_swinl}, respectively.
% In addition, we report the performance comparison between recently proposed pixel-supervised VIS methods with Swin Large backbone on YTVIS21 and OVIS datasets in \cref{tab:sota_swinl}. 
% Due to the limit of space, the experimental comparison on YTVIS22 valid set can be found in the \textbf{supplemental materials}.

\textbf{YTVIS21 valid set.} 
From \cref{tab:sota_yt21_ovis}, we can see that the accuracy of early pixel-supervised VIS methods \cite{yang2021crossover, wang2020vistr, hwang2021video, seqformer} is below 41\% AP.%, mainly because the segmentation heads and tracking strategies are not well explored. 
The recently developed pixel-supervised VIS methods with self-supervised learning or masked-attention transformer bring about 4\% and 5\% AP improvement. Trained jointly on VIS and COCO datasets, VITA \cite{heo2022vita} and MDQE \cite{li2023mdqe} achieve 45.7\% and 44.5\% AP, respectively.
M2F-VIS-box (our BoxVIS baseline) obtains 41.2\% AP, 1.9\% lower than its pixel-supervised counterpart M2F-VIS \cite{cheng2021mask2former-video}. In contrast, our BoxVIS with the STPA loss can increase the performance to 42.8\% AP, and our BoxVIS trained on BVISD improves the performance to 43.2\% AP. Overall, our BoxVIS outperforms its pixel-supervised counterpart M2F-VIS \cite{cheng2021mask2former-video} and exhibits comparable performance to the state-of-the-art pixel-supervised competitors.


\textbf{OVIS valid set.} As shown in \cref{tab:sota_yt21_ovis}, the early proposed pixel-supervised VIS methods fail to handle the challenging videos in OVIS dataset, resulting in poor performance (below 15\% AP). The recent masked-attention based methods M2F-VIS \cite{cheng2021mask2former-video} and MinVIS \cite{huang2022minvis} bring approximately 10\% AP improvement, achieving 24.5\% and 26.3\% AP, respectively. By introducing clip-level input and contrastive learning for object embeddings, MDQE \cite{li2023mdqe} obtains the state-of-the-art 29.2\% AP.
However, VITA \cite{heo2022vita}, which is the state-of-the-art on YTVIS21, obtains only 19.6\% AP on OVIS, showing limited generalization capability.
The box-supervised baseline M2F-VIS-box and our BoxVIS trained only on OVIS obtain 23.9\% and 26.2\% AP, respectively, validating that the proposed STPA loss can predict instance masks with better spatial and temporal consistency. Finally, our BoxVIS trained on BVISD improves the performance to 29.0\% AP, obtaining competitive performance with the state-of-the-art pixel-supervised VIS competitors. The encouraging results demonstrate great potentials of box annotation in VIS tasks.


\textbf{Swin Large backbone.} VIS models with the stronger Swin Large backbone can have higher detection and segmentation abilities. Due to the limited space, only the recently proposed high-performance methods are reported in \cref{tab:sota_swinl}. 
Among the pixel-supervised VIS methods, VITA \cite{heo2022vita} shows the best accuracy (57.5\% AP) on YTVIS21, while MinVIS \cite{huang2022minvis}, IDOL \cite{IDOL} and MDQE \cite{li2023mdqe} achieve 41.6\%, 42.6\% and 42.6\% AP, respectively, where IDOL \cite{IDOL} and MDQE \cite{li2023mdqe} take 720p videos as input.

Our BoxVIS with the frame-level tracker \cite{huang2022minvis} achieves 52.8\% and 34.4\% AP on YTVIS21 and OVIS, respectively, outperforming its pixel-supervised counterpart M2F-VIS \cite{cheng2021mask2former-video}.
However, there is a performance gap on OVIS dataset compared to the top-performance pixel-supervised VIS methods. We believe one reason is that for challenging videos on OVIS, the transformer network with attention mechanism can learn more discriminative object embeddings from pixel-wise annotation than box-level annotation.
Fortunately, since our BoxVIS follows the clip-level input pipeline, it can introduce the mask IoU on overlapping frames into the tracker to more accurately associate objects across clips. In particular, by using the clip-level tracker proposed in MDQE \cite{li2023mdqe}, the performance of BoxVIS can be significantly improved to 53.9\% and 40.6\% AP on YTVIS21 and OVIS, respectively, which is competitive with the pixel-supervised VIS methods.
It is worth mentioning that our BVISD consumes only 16\% the cost of pixel-wise annotation, and there is still a big room for BoxVIS to improve its performance.

\textbf{Visualization.} \cref{fig:visualizer} displays the predicted instance masks by BoxVIS on OVIS videos. One can see that with only box annotations, BoxVIS can still predict high-quality instance masks for challenging videos with occluded objects and complex boundaries. Visualization comparison on more videos are provided in the \textbf{supplemental materials}. 

\textbf{Parameters and Speed.} We follow Detectron2 \cite{wu2019detectron2} to calculate the number of parameters and FPS for all VIS methods in \cref{tab:sota_yt21_ovis}. BoxVIS inherits the network architecture of M2F-VIS \cite{cheng2021mask2former-video} and thus has the same model size (44M) as it. The tracking strategy affects mostly the inference speed of VIS models.
SeqFormer\cite{seqformer} and M2F-VIS \cite{cheng2021mask2former-video} take the whole video as input without extra tracking, running at around 70FPS. IDOL \cite{IDOL} and MinVIS \cite{huang2022minvis} with frame-by-frame inference run at 30.6FPS and 52.4FPS, respectively. 
Our BoxVIS adopts clip-by-clip inference to obtain averaged masks on the overlapping clips, resulting in a slower inference speed of 37.8FPS. If we set the clip length as 1, BoxVIS can run as fast as M2F-VIS (\ie, 70FPS) at the price of about 1\% AP drop on all VIS datasets.





