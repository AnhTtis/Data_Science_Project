\section{Related Works}

\subsection{Box-supervised Image Instance Segmentation}
BoxCaseg \cite{wang2021boxcaseg} leverages a salience model to generate pseudo object masks for training Mask R-CNN along with the multiple instance learning (MIL) loss.  Recently, BoxInst  \cite{tian2020boxinst} achieves impressive performance by proposing two box-supervised loss items, the projection loss and the pairwise affinity loss. 
BoxLevelSet \cite{li2022boxlevelset} utilizes the traditional level set evolution to predict the object boundaries and instance masks.
DiscoBox \cite{lan2021discobox} and BoxTeacher \cite{cheng2022boxteacher} employ an exponential moving average teacher to produce high-quality pseudo instance masks for introduction the pseudo pixel-wise mask supervision, bringing significant performance improvement.

\input{figs/boxvis_baseline}

\subsection{Pixel-supervised Video Instance Segmentation}
\textbf{Per-frame input based VIS methods.} A popular VIS pipeline \cite{yang2019video, cao2020sipmask,Li_2021_CVPR,liu2021sg,yang2021crossover,QueryInst,ke2021pcan,huang2022minvis,IDOL} is to extend the representative pixel-supervised IIS methods \cite{he2017mask,bolya2019yolact,tian2020conditional,carion2020end,cheng2021mask2former} by adapting a frame-to-frame instance tracker. For example, in \cite{yang2019video,QueryInst,huang2022minvis}, the clues such as category score, box/mask IoU and instance embedding similarity are integrated into the tracker. 
However, these trackers may fail to distinguish instances with similar appearance, resulting in error-prone frame association. Inspired by the contrastive learning \cite{chen2020simplectt, pang2021quasi,khosla2020supervisedctt,wang2021densectt}, IDOL \cite{IDOL} learns discriminative instance embeddings for multiple object tracking frame by frame.
Besides, clip-to-clip trackers \cite{bertasius2020classifying, lin2021video, qi2021occluded} have been proposed via propagating the predicted instance masks from a key frame to other frames.
% including instance masks propagation from each frame to its neighbouring frames or from several selected key frames to all the other frames. 
The commonly used techniques include deformable convolution \cite{bertasius2020classifying,dai2017deformable}, non-local block \cite{lin2021video}, correlation \cite{Li_2021_CVPR,qi2021occluded}, graph neural network \cite{wang2021end}, \etc. 
% By exploiting the temporal redundancy among overlapped frames, clip-to-clip trackers improve much the performance over per-frame methods.

\textbf{Per-clip input based VIS methods.} StemSeg \cite{Athar_Mahadevan20stemseg} first propose a clip-in clip-out VIS pipeline by modeling a video clip as a single 3D spatio-temporal pixel embedding. In recent years, transformer based per-clip methods \cite{wang2020vistr,wu2022trackletquery,hwang2021video,yang2022TempEffi,seqformer} have achieved impressive progress. VisTR \cite{wang2020vistr} views the VIS task as a direct end-to-end parallel sequence prediction problem, but it needs a large memory to store spatio-temporal features. To solve the issue, DETR-based IFC\cite{hwang2021video} transfers inter-frame information via efficient memory tokens, and SeqFormer \cite{seqformer} locates an instance in each frame and aggregates temporal information to predict video-level instances. 
Besides, to keep the temporal consistency, EfficientVIS \cite{wu2022trackletquery} transfers inter-clip query embeddings via a temporal dynamic convolution.

Although the pixel-supervised VIS methods have achieved impressive performance on current VIS benchmarks YTVIS21 and OVIS respectively, but their generalization capability may be limited, because the amount of pixel-wise annotations is small. For example, the VIS model trained only on YTVIS21 dataset fails to handle the videos with crowded objects like OVIS. It is expensive and labour-extensive to label the pixel-wise object masks in a video. Inspired by the successful of box-supervised IIS methods, in this paper we extend the state-of-the-art pixel-supervised VIS methods to their box-supervised counterparts, and propose a box-center guided spatial-temporal pairwise affinity loss to predict instance masks for better spatial and temporal consistency, finally introduce a larger scale box annotated VIS dataset (BVISD) to improve the generalization capability of box-supervised VIS models.