\section{Related Work}

\subsection{Pixel-supervised VIS}
There are two major VIS benchmarks, the YTVIS series \cite{yang2019video} and OVIS \cite{qi2021occluded}, which have very different video types in terms of object motions and scenes. The YTVIS series focus mainly on segmenting sparse objects in shorter videos, while the OVIS aims to segment crowded instances with occlusions in longer videos. Based on these facts, we categorize the pixel-supervised VIS models into YTVIS-oriented ones and OVIS-oriented ones.

\textbf{YTVIS-oriented VIS models.}
By introducing a tracker into the representative IIS methods \cite{he2017mask,bolya2019yolact,tian2020conditional}, the early proposed VIS methods \cite{yang2019video, cao2020sipmask,Li_2021_CVPR,Athar_Mahadevan20stemseg,liu2021sg,yang2021crossover,QueryInst,ke2021pcan} have achieved decent performance on YTVIS series. 
The frame-to-frame trackers \cite{yang2019video,cao2020sipmask,Li_2021_CVPR,QueryInst} integrate the clues such as category score, box/mask IoU and instance embedding similarity. 
The clip-to-clip trackers \cite{Li_2021_CVPR,Athar_Mahadevan20stemseg,bertasius2020classifying,lin2021video, qi2021occluded,wang2020vistr,seqformer,hwang2021video} propagate the predicted instance masks from a key frame to other frames, which can be implemented by deformable convolution \cite{bertasius2020classifying,dai2017deformable}, non-local block \cite{lin2021video}, correlation \cite{Li_2021_CVPR,qi2021occluded}, graph neural network \cite{wang2021end}, \etc. 
% By exploiting the temporal redundancy among overlapped frames, clip-to-clip trackers improve much the performance over per-frame methods on YTVIS datasets.

Recently, query-based \cite{carion2020end} VIS methods \cite{wang2020vistr,hwang2021video,seqformer,wu2022trackletquery} have achieved impressive progress. VisTR \cite{wang2020vistr} views the VIS task as an end-to-end parallel sequence prediction problem, but it consumes a large amounts of memory to store spatial-temporal features. To solve the issue, IFC\cite{hwang2021video} transfers inter-frame information via efficient memory tokens, and SeqFormer \cite{seqformer} locates an instance in each frame and aggregates them to predict video-level instances. 

\textbf{OVIS-oriented VIS models.}
The above mentioned YTVIS-oriented VIS models often fail to handle the challenging long videos with crowded and similar-looking objects in OVIS dataset, resulting in significant performance degradation. Inspired by contrastive learning \cite{chen2020simplectt, pang2021quasi,khosla2020supervisedctt,wang2021densectt}, IDOL \cite{IDOL} learns discriminative embeddings for multiple object tracking frame by frame.
Masked-attention transformer \cite{cheng2021mask2former} brings significant performance improvement on OVIS dataset by calculating attention only in the region of the objects.
Mask2Former-VIS \cite{cheng2021mask2former-video} with per-clip input takes the mask IoU of the overlapping frames as the tracker, while MinVIS \cite{huang2022minvis} with per-frame input employs instance embedding similarity by using Hungarian matching as the tracker. VITA \cite{heo2022vita} integrates the predicted object embeddings of all frames in the video to produce video-level instance masks.

\textbf{Remarks.} Though the pixel-supervised VIS methods have achieved much progress, their generalization capability is limited. For example, the VIS models trained on YTVIS21 often fail to handle the challenging videos in OVIS, due to the limited number of type of videos in each dataset. However, it is labour-extensive to label the pixel-wise masks in videos. Inspired by the success of box-supervised IIS methods, we explore video instance segmentation with only box annotations in this paper.

% a box-supervised VIS (BoxVIS) method and a box-annotated VIS dataset (BVISD) to reduce the annotation cost and improve the model generalization capability.
% extend the state-of-the-art pixel-supervised VIS methods to their box-supervised counterparts, and propose a box-center guided spatial-temporal pairwise affinity loss to predict instance masks for better spatial and temporal consistency, finally collect a larger box-annotated VIS dataset (BVISD) to improve the generalization capability of box-supervised VIS models.

\input{figs/boxvis_baseline}

\subsection{Box-supervised IIS}
A few box-supervised IIS methods have been proposed. BoxCaseg \cite{wang2021boxcaseg} leverages a salience model to generate pseudo object masks to train a Mask R-CNN along with the multiple instance learning (MIL) loss. Recently, BoxInst  \cite{tian2020boxinst} achieves impressive performance by proposing a projection loss and a pairwise affinity loss for box-supervision. 
BoxLevelSet \cite{li2022boxlevelset} utilizes the traditional level set evolution to predict the object boundaries and instance masks.
DiscoBox \cite{lan2021discobox} and BoxTeacher \cite{cheng2022boxteacher} employ an exponential moving average teacher to produce high-quality pseudo instance masks and introduce pseudo pixel-wise mask supervision, bringing significant performance improvement. In this work, we propose the first  box-supervised VIS framework and demonstrate its effectiveness. 