\section{Methodology}

We first extend the pixel-supervised VIS methods to a box-supervised VIS (BoxVIS) baseline in \cref{sec:boxvis_baseline}, then propose a box-center guided spatial-temporal pairwise affinity loss and a large scale box-annotated VIS dataset (BVISD) in \cref{sec:st_pair} and \cref{sec:bvisd}, respectively.


\subsection{A BoxVIS Baseline}\label{sec:boxvis_baseline}
In this subsection, we first introduce the two box-supervised instance segmentation loss items \cite{tian2020boxinst} and the pseudo mask supervision loss with the high-quality pseudo masks generated by exponential moving average teacher \cite{lan2021discobox,cheng2022boxteacher}, then extend the per-clip input based Mask2Former-VIS model \cite{cheng2021mask2former-video} to its box-supervised counterpart, namely M2F-VIS-box, as a BoxVIS baseline.

\textbf{Projection loss} \cite{tian2020boxinst} supervises  the  horizontal  and  vertical  projections  of the predicted mask using the ground-truth box annotation, which ensures that the tightest box covering the predicted mask  matches  the  ground-truth  box. The projection loss is formulated as:
\begin{equation}
\Scale[0.96] { {L}_{proj} =  {L}({\max\limits_y (M),\ \max\limits_y (\bar{B})}) + {L}({\max\limits_x (M),\ \max\limits_x (\bar{B})}), } \nonumber 
% & = {\displaystyle{L}({proj_x (M),\ proj_x (\bar{B})}) + {L}({proj_y (M),\ proj_y (\bar{B})}),} \nonumber \\ 
\end{equation}
where $M$ indicates the predicted mask, and $\bar{B}$ denotes the ground-truth box mask, where points in the inner bounding box are 1, and otherwise 0.

\textbf{Pairwise affinity loss} \cite{tian2020boxinst} is designed to supervise pixel-wise mask prediction without pixel-wise annotation. If two adjacent pixels have similar colors, they are very likely to have the same mask labels.
For two pixels located at $(x_i, y_i)$ and $(x_j, y_j)$, their color similarity is defined as 
$S_{lab} = \text{exp}(-\frac{\parallel c_i - c_j\parallel}{\theta})$,
where $c_i$ and $c_j$ are the color vectors in the LAB color space, and $\theta$ is a hyper-parameter (set as 2 by default).
If the color similarity of two pixels is grater than a threshold $\tau$, they are assumed to have same mask labels, \ie, the edge weight of the two pixels is set to 1.
Hence, the pairwise mask affinity is formulated as 
$ P(y_e=1) = {M}_i\cdot {M}_j + (1-{M}_i)\cdot(1-{M}_j)$,
where ${M}_i$ and ${M}_j$ represent the predicted masks of the two pixels, respectively.
The pairwise affinity loss is formulated as:
\begin{normalsize}
\begin{equation}\label{eq:pair}
   \vspace{-1mm}
   {L}_{pair}  =  -\frac{1}{N} \sum\limits_{e\in E_{in}} \mathds{1}_{\{S_{lab} \geq \tau\}} \log P(y_e = 1).
\end{equation}
\end{normalsize}
where $E_{in}$ indicates the set of paired pixels in the inner bounding box, and $N$ is the total number of the pixel pairs with the same labels in $E_{in}$. 

\textbf{Pseudo mask supervision loss.} Exponential moving average (EMA) teacher \cite{lan2021discobox,cheng2022boxteacher} can generate high-quality instance masks as the pseudo masks of the ground-truth boxes. By replacing the ground-truth masks with the pseudo masks, the pixel-wise mask supervision loss in the pixel-supervised IIS methods can be extended to the pseudo mask supervision loss, termed as $L_{pse\text{-}mask}$, which usually consists of the binary cross-entropy loss and the Dice loss \cite{dice1945dice, milletari2016dice_vnet}.
The box-supervised IIS models with EMA employ a sophisticated Teacher Net and a Student Net in training. The Student Net is normally optimized with the aforementioned box-supervised losses and pseudo-mask loss, and the Teacher Net is progressively updated via EMA without back-propagating gradients.

\textbf{BoxVIS baseline.} By using the above two box-supervised loss terms and the pseudo mask supervision loss, we extend the pixel-supervised VIS method Mask2Former-VIS (M2F-VIS) \cite{cheng2021mask2former-video} to a BoxVIS baseline, namely M2F-VIS-box.
The overall architecture of the BoxVIS baseline is shown in \cref{fig:boxvis_baseline}(a). It is composed of a sophisticated Teacher Net $f_\theta$ and a Student Net $f_\phi$. 
During training, a short video clip will be fed to the Teacher Net $f_\theta$ and Student Net $f_\phi$ to predict object masks. The predicted masks from the teacher net will be taken as the pseudo masks of the ground-truth boxes by the Hungarian matching algorithm, whose cost matrix consists of the classification loss and the projection loss. 
On the other hand, the predicted masks from the student net will be matched with the ground-truth boxes and the generated pseudo masks, whose matching cost matrix takes the assigned pseudo masks into account. After that, the matched masks are supervised by the ground-truth boxes and pseudo masks via the aforementioned box-supervised loss and pseudo-mask loss respectively. Finally, the teacher is progressively updated via EMA.
During inference, the teacher net will be discarded, and we only use the predicted masks from the student net.

Beside, the low-quality pseudo masks may introduce some incorrect pixel-wise supervision for the pseudo-mask loss, resulting in performance degradation. The confidence scores for estimating the mask quality are computed as the product of the classification score and the projection score. Hence, to select the high-quality pseudo masks, we adopt a dynamic threshold, which is determined by the ratio of the number of processed iterations to the total number of training iterations: $\epsilon = 1/(1+e^{-2(1-r)}),\quad r={\text{iters}}/{\text{total iters}}$.




% --------------------------------------------------------------------------------
\subsection{Spatial-temporal Pairwise Affinity Loss} \label{sec:st_pair}
Segmenting instances from videos is more challenging than segmenting objects from individual images, as videos often have unexpected motion blur, uncommon-camera-to-object, appearance changes, heavy occlusion and so on. 
% For these challenging videos, adjacent frames can help the model to exploit richer spatial-temporal features to distinguish pixels belonging to the target instance. 
The pairwise affinity loss in Eq. (1) only considers the paired pixels within a single frame, which may however fail for BoxVIS to constrain the temporal consistency of segmented objects. 
One naive solution is to directly select the neighbours at the same positions in the next frame to perform temporal pairwise affinity loss. 
Unfortunately, objects in a video often change their positions due to camera jitters or object motion. The assumptions that two adjacent pixels in an image will have similar colors and hence the same mask labels do not hold well for adjacent frames in a video. Two pixels from two adjacent frames may have similar colors but they can belong to different instances.
To solve this issue, we propose a box-center guided spatial-temporal pairwise affinity (STPA) loss, which uses the box-center guided shifting to generate the temporally paired pixels, and uses the color similarity and patch correlation in feature space to stably compute the pairwise affinity.

\textbf{Spatial-temporally paired pixels.}
For a pixel in a video clip, we only consider its two spatial neighbours in the current frame for efficiency: the right and bottom pixels, named the set $E_{s}$ of spatial neighbours. 
We then discuss how to use the bounding box centers to identify inter-frame paired pixels.
The object bounding boxes in consecutive frames can indicate coarse inter-frame movements of the objects. 
For a given instance appeared in two adjacent frames $t_i$ and $t_j$, we denote by $(t_i, x_i^c, y_i^c)$ and $(t_j, x_j^c, y_j^c)$ its bounding box centers, and by $(dx^c, dy^c) = (x_i^c-x_j^c, y_i^c-y_j^c)$ the location offsets of the two centers.
Then, for any pixel at position $(t_i, x_i, y_i)$ in frame $t_i$, if it locates in the inner bounding box of the instance, we can shift its position to the nearby area of the instance bounding box in frame $t_j$ by $(t_j, x_i+dx^c, y_i+dy^c)$.
Consequently, the set $E_{t}$ of temporal neighbours can be produced by shifting the center pixel and its four spatial neighbours in frame $t_i$ to the corresponding positions in frame $t_j$. 
Overall, for each pixel, we group its seven neighbours as paired pixels to compute the proposed STPA loss. The schematic and the generated spatial-temporal pixels are illustrated  in \cref{fig:boxvis_baseline}(b).

\textbf{Patch correlation.}
It is not reliable enough to utilize pixel-to-pixel color similarity to determine whether two pixels have the same labels or not. Therefore, we introduce an extra patch correlation in the feature space to compute stable and reliable temporal pairwise affinity. 
% The color similarity of two patches is defined as
% \begin{equation}
 % { {S}_{lab} = \frac{1}{|\Omega|} \sum_{o \in \Omega} \text{exp} (-\frac{\parallel c_{i+o} - c_{j+o} \parallel }{\theta}), }
% \end{equation}
% where $c_{i+o} $ and $c_{j+o}$ are the color vectors of the two pixels at positions $(t_i, x_i+o_x, y_i+o_y)$ and $(t_j, x_j+o_x, y_j+o_y)$ in the LAB color space.  $\theta$ is a hyper-parameter, being 2 as default. 
For a pixel at position $(t_i, x_i, y_i)$, we represent the pixels in its centered patch as $(t_i, x_i+o_x, y_i+o_y)$, where the displacements $(o_x, o_y) \in {\normalsize [-k, k]\times [-k, k]}$ and $k$ controls the patch size (set as 1 by default).
The feature correlation of two patches is defined as
\begin{normalsize}
\begin{equation}
 % { {S}_{corr} = \frac{1}{|\Omega|} \sum_{o \in \Omega} 1 - \text{exp} (-\frac{<f_{i+o}, f_{j+o}>}{d}), }
 {S}_{corr} = \frac{1}{|\Omega|} \sum_{o \in \Omega}\ \frac{<f_{i+o},\ f_{j+o}>}{\parallel f_{i+o} \parallel \cdot \parallel f_{j+o} \parallel }, 
\end{equation}
\end{normalsize}
where $f_{i+o}$ and $f_{j+o}$ are the feature vectors at positions $(t_i, x_i+o_x, y_i+o_y)$ and $(t_j, x_j+o_x, y_j+o_y)$ in the last pixel encoder layer of Mask2Former framework. $|\Omega|$ is the total number of pixels within the patch $\Omega = {\normalsize [-k, k]\times [-k, k]} $.

The overall pairwise affinity is $S_e = S_{lab} + 0.5S_{corr}$.
To remove the paired pixels with low affinity, we introduce a color similarity threshold $\tau_{lab}$ and a correlation threshold $\tau_{corr}$, respectively. The affinity threshold used in our paper is $\tau=\tau_{lab}+0.5\tau_{corr}$. Without otherwise specified, we set $\tau_{lab}=0.3$ and $\tau_{corr}=0.9$ by default, \ie, $\tau=0.75$.
We still adopt the mask pairwise affinity $ P(y_e=1) = {M}_i\cdot {M}_j + (1-{M}_i)\cdot(1-{M}_j)$, and $M_i$ and $M_j$ are the predicted masks at positions $(t_i, x_i, y_i)$ and $(t_j, x_j, y_j)$, respectively.
Finally, the STPA loss can be formulated as:
\begin{normalsize}
    \begin{align}\label{eq:stpair}
       {L}_{stpa}  =  -\frac{1}{N} \sum\limits_{e\in E_{stin}} \mathds{1}_{\{S_e \geq \tau\}} \log P(y_e = 1).
    \end{align}
\end{normalsize}
where $E_{stin}$ indicates the set of the spatial-temporally paired pixels, among which at least one pixel locates in the inner bounding box of the instance in current frame. $N$ is the total number of the paired pixels with affinity higher than the threshold $S_e$ in the set $E_{stin}=E_s \cup E_t $. 
% Note that we only take the right pixel $(x_i+s, y_i)$ and bottom pixel $(x_j, y_j+s)$ into account, where $s$ is the stride. 

\input{tables/bvisd}
% --------------------------------------------------------------------------
\subsection{Box-annotated VIS dataset (BVISD) } \label{sec:bvisd}
It is very costly to annotate fine-grained masks for instances in videos. 
We propose a larger scale box-annotated VIS dataset (BVISD) by merging the videos from current VIS benchmarks, YouTube-VIS and OVIS, and converting images from the COCO dataset to short pseudo video clips. 
For YouTube-VIS benchmark, we adopt the latest YTVIS21, which contains 2,985 training videos over 40 categories, and the video length is less than 36 frames. OVIS \cite{qi2021occluded} includes 607 training videos over 25 object categories, but each frame has more objects than YTVIS21 with different occlusion levels. 

There are 25 overlapping categories between COCO and VIS benchmarks, including around 90k images and 450k objects. To augment the single image from COCO to a video clip, we first resize the image by adjusting its short edge in the range of [600, 800], and then randomly crop a region with the short edge within [320, 512]. Finally, the cropped region will be randomly rotated with the degree of [-15, 15]. We repeat the above augmentation process for $T$ times to obtain a pseudo video clip with $T$ frames. During training, the short video clip augmented from COCO will be further resized into the input resolution, \ie 360p.

One key step to consolidate the above three datasets is how to properly  merge the object categories to avoid semantic conflicts.
Fortunately, the 25 categories of OVIS are mostly contained in the 40 categories of YTVIS21, except for the `sheep' category. Besides, the categories `car' and `truck' in YTVIS21 are merged into a super-category `vehicle' in OVIS. 
In BVISD, we first merge the similar categories as its super-category to avoid ambiguity, and keep the remaining object categories in YTVIS21 and OVIS. For COCO, we use images that contain at least one object in the categories of BVISD. The distribution of the objects in BVISD is illustrated in \cref{fig:bvisd_cate}.
% where the number of `person' category is divided by 5 for better display.

\input{figs/bvisd_cate}

The statistics of YTVIS21, OVIS, COCO and BVISD are shown in \cref{tab:bvisd_data}. Overall, the proposed BVISD consists of 222k frames, 978k objects and 40 non-conflicting object categories. Note that only object box annotations are provided in BVISD.
It has been shown \cite{papadopoulos2017extreme,lin2014microsoft} that labeling the bounding box of an object takes only 7 seconds, while labeling its polygon-based mask needs 79.2 second in COCO \cite{lin2014microsoft}. One can then estimate that annotating the object bounding boxes in BVISD takes $7 \times 978,000$ seconds, about 80 worker days, while labeling the pixel-wise annotations takes $79.2 \times 978,000$ seconds, about 898 worker days. 


% Since the ground-truth labels of current VIS benchmarks are not released, we have to submit the predicted results to the CodeLab website to evaluate the instance segmentation performance. 
For easier to test the models, we split the BVISD data into two separated parts: the training set and the valid set, while the valid set contains around 1/10 the videos of BVISD. 
As shown in \cref{fig:bvisd_cate}, some categories are only presented in YTVIS21 but not OVIS and COCO, which may result in unbalanced categories. During training, therefore, we sample the clips from the three datasets with different sampling weights to alleviate the issue. 
During inference, we test our proposed BoxVIS in the three valid sets: official YTVIS21 valid, official OVIS valid and our BVISD valid. For the testing on official YTVIS21 and OVIS valid sets, the predicted categories will be mapped back to the original object categories of the source dataset.
% Unlike the single type of videos in YTVIS21 or OVIS valid sets, the proposed BVISD valid-sub includes their joint video types: diverse video lengths from 19 to 120 frames, crowded or sparse objects, and fast-moving or steady camera shifts. 
% Consequently, the instance segmentation performance on BVISD valid-sub can better reflect model generalization ability. 


