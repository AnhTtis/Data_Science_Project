
\section{Introduction}

Meta-learning \cite{schmidhuber1987evolutionary,naik1992meta,bengio2013optimization} aims to learn useful prior knowledge (\eg, sensitive initialization) from a collection of similar tasks to facilitate the learning of new unseen tasks. Most meta-learning methods \cite{finn2017model,rajeswaran2019meta,bertinetto2018meta,wang2019bayesian,sun2021towards,bronskill2021memory,chen2021generalization,vanschoren2019meta,nichol2018first,hsu2018unsupervised,wang2022meta,wang2022learning} assume the access to the training and testing data of each task. However, this assumption is not always satisfied: many individuals and institutions only release the pre-trained models instead of the data. This is due to data privacy, safety, or ethical issues in real-world scenarios, making the task-specific data difficult or impossible to acquire. For example, many pre-trained models with arbitrary architectures are released on GitHub without training data. However, when facing a new task, we need some prior knowledge learned from those pre-trained models so that the model can be adapted fast to the new task with few labeled examples. Thus, meta-learning from several pre-trained models without data becomes a critical problem, named \textit{Data-free Meta-Learning (DFML)} \cite{wang2022metalearning}.

\begin{figure}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=0.8\linewidth]{figs/eci.pdf}

   \caption{Episode Curriculum Inversion can improve the efficiency of pseudo episode training. At each episode, EI may repeatedly synthesize the tasks already learned well, while ECI only synthesizes harder tasks not learned yet.}
   \label{fig:curriculum}
   \vspace{-0.4cm}
\end{figure}

\begin{figure}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=0.8\linewidth]{figs/icfil.pdf}

   \caption{Task-distribution shift between meta training and testing. The pseudo data distilled from pre-trained models only contains partial semantic information learned by pre-trained models.}
   \label{fig:shift}
   \vspace{-0.5cm}
\end{figure}



Existing data-free meta-learning methods address the problem in the parameter space. Wang \etal \cite{wang2022metalearning} propose to meta-learn a black-box neural network by predicting the model parameters given the task embedding without data, which can be generalized to unseen tasks. The predicted model parameters with the average task embedding as input are served as the meta initialization for meta testing. However, this method has several drawbacks. First, they only merge the model in parameter space and ignore the underlying data knowledge that could be distilled from the pre-trained models. Second, their method can only be applied to small-scale pre-trained models since they use a neural network to predict the model parameters. Furthermore, their application scenarios are restricted to the case where all pre-trained models have the same architecture, limiting the real-world applicable scenarios.  


In this work, we try to address all the above issues simultaneously in a unified framework, named \textbf{PURER} (see \cref{fig:pipeline}), which contains: (1) e\textbf{P}isode c\textbf{U}rriculum inve\textbf{R}sion (ECI) during data-free meta training; and (2) inv\textbf{E}rsion calib\textbf{R}ation following inner loop (ICFIL) during meta testing, thus significantly expanding the application scenarios of DFML. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. We progressively synthesize a sequence of pseudo episodes (tasks) by distilling the training data from each pre-trained model. ECI adaptively increases the difficulty level of pseudo episode according to the real-time feedback of the meta model. Specifically, we first introduce a small learnable dataset, named \textit{dynamic dataset}. We initialize the dynamic dataset as Gaussian noise and progressively update it to better quality via one-step gradient descent for every iteration. For each episode, we construct a pseudo task by first sampling a subset of labels, and then sampling corresponding pseudo support data and query data. To improve the efficiency of pseudo episode training (see \cref{fig:curriculum}), we introduce the \textit{curriculum mechanism} to synthesize episodes with an increasing level of difficulty. We steer the dynamic dataset towards appropriate difficulty so that only tasks not learned yet are considered at each iteration, which avoids repeatedly synthesizing the tasks already learned well. We design a \textit{Gradient Switch} controlled by the real-time feedback from current meta model, to synthesize harder tasks only when the meta model has learned well on most tasks sampled from current dynamic dataset (see \cref{fig:pipeline}).  Finally, we formulate the optimization process of meta training with ECI as an adversarial form in an end-to-end manner. We further propose a simple plug-and-play supplement---ICFIL---only used during meta testing to narrow the gap between meta training and meta testing task distribution (see \cref{fig:shift}). Overall, our proposed PURER can solve the 
DFML problem using the underlying data knowledge regardless of the dataset, scale and architecture of pre-trained models.









Our method is architecture, dataset and model-scale agnostic, thus substantially expanding the application scope of DFML in real-world applications. We perform extensive experiments in various scenarios, including \textbf{(i)} \textbf{SS}: DFML with \textbf{S}ame dataset and \textbf{S}ame model architecture; \textbf{(ii)} \textbf{SH}: DFML with \textbf{S}ame dataset and \textbf{H}eterogeneous model architectures; \textbf{(iii)} \textbf{MH}: DFML with \textbf{M}ultiple datasets and \textbf{H}eterogeneous model architectures. For benchmarks of SS, SH and MH on CIFAR-FS and MiniImageNet, our method achieves significant performance gains in the range of $6.92\%$ to $17.62\%$, $6.31\%$ to $27.49\%$ and $7.39\%$ to $11.76\%$, respectively.


We summarize the main contributions as three-fold:
\begin{itemize}
    \item We propose a new orthogonal perspective with respect to existing works to solve the data-free meta-learning problem by exploring the underlying data knowledge. Furthermore, our framework is architecture, dataset and model-scale agnostic, \ie, it can be easily applied to various real-world scenarios.
    \item We propose a united framework, PURER, consisting of: (i) ECI to perform pseudo episode training with an increasing level of difficulty during meta training; (ii) ICFIL to narrow the gap between meta training and testing task distribution during meta testing.
    \item Our method achieves superior performance and outperforms the SOTA baselines by a large margin on various benchmarks of SS, SH and MH, which shows the effectiveness of our method.
\end{itemize}