\section{Methodology}
\label{sec:methodology}

In this section, we propose a unified framework PURER to solve DFML as illustrated in \cref{fig:pipeline}. In Section \ref{subsec:episodeCurriculumInversion}, we propose ECI to perform pseudo episode training by progressively synthesizing a sequence of pseudo episodes with an increasing level of difficulty during meta training. In Section \ref{subsec:IFCIL}, we further propose ICFIL to eliminate the task-distribution shift issue during meta testing.


\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=0.7\linewidth]{figs/pipeline.pdf}

   \caption{The overall pipeline of our proposed PURER consisting of ECI and ICFIL. For each episode during meta training, a pseudo episode is sampled from the dynamic dataset. The split pseudo support set and query set are used for the inner loop and outer loop of meta-learning. The real-time feedback of meta model controls the Gradient Switch. When the feedback is positive, the dynamic dataset is updated to synthesize harder tasks with larger outer loss for the next iteration by minimizing the reversed outer loss through gradient descent. During meta testing, the adapted base model after inner loop is calibrated via ICFIL. For brevity, we leave out the calibration for linear classifier head.}
   \label{fig:pipeline}
   \vspace{-0.4cm}
\end{figure*}


\subsection{Preliminary: Episode Training}
\label{subsec:startFromEpisodeTraining}




Our work aims to synthesize a sequence of pseudo episodes to perform pseudo episode training. Thus, we would like to introduce episode training briefly. 
In the case of MAML\cite{finn2017model}, each episode involves:



\begin{itemize}
    \item \textit{Outer Loop}: Update the \textit{meta model} with the goal of improving the performance of \textit{base model} on query set. The meta model works across episodes and learns good generalization from many episodes.
    
    \item \textit{Inner Loop}: Perform \textit{fast adaptation}. The \textit{base model} takes the \textit{meta model} as initialization and performs few steps of gradient descent over the support set. The \textit{base model} works at the level of individual task.
\end{itemize}


Considering a per datum classification loss $l: \mathcal{X} \times \mathcal{Y} \times \Theta \rightarrow \mathbb{R}_{+}$, the \textit{empirical loss} over a finite dataset $\mathcal{B}$ is defined as $\mathcal{L}(\mathcal{B};\boldsymbol{\theta})\triangleq\frac{1}{\left| \mathcal{B} \right|} \sum_{(\boldsymbol{x},y) \in \mathcal{B}} l(\boldsymbol{x},y; \boldsymbol{\theta})$. Thus, for a given task $\mathcal{T}=\{\mathcal{S},\mathcal{Q}\}$, it becomes $\mathcal{L}(\mathcal{S}; \boldsymbol{\theta})$ and $\mathcal{L}(\mathcal{Q}; \boldsymbol{\theta})$. 

 The bi-level optimization can be formulated as follows 

\begin{subequations}
\small
\begin{align}
    \min_{\boldsymbol{\theta}}&\ \mathcal{L}_{outer}(\mathcal{T};\boldsymbol{\theta})\triangleq\mathcal{L}\left(\mathcal{Q}; \boldsymbol{\theta}^{*}\right),\\
     {\rm s.t.}\ 
    \boldsymbol{\theta}^{*} 
    &=\boldsymbol{\theta}-\alpha_{inner}\nabla_{\boldsymbol{\theta}}\mathcal{L}_{inner}(\mathcal{T};\boldsymbol{\theta}) \nonumber\\
    &\triangleq\boldsymbol{\boldsymbol{\theta}}-\alpha_{inner}\nabla_{\boldsymbol{\theta}}\mathcal{L}(\mathcal{S};\boldsymbol{\boldsymbol{\theta}}).\label{subeq:inner}
\end{align}
\label{eq:lossMeta}
\end{subequations}

where $\alpha_{inner}$ is the step size in the inner loop.



\subsection{Episode Curriculum Inversion (ECI)}
\label{subsec:episodeCurriculumInversion}

We propose an ECI component to perform pseudo episode training for learning to adapt fast to new unseen tasks (see \cref{fig:pipeline}). We synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. ECI adaptively increases the difficulty level of pseudo episodes according to the real-time feedback of the meta model. 
 
 
 
\noindent
\textbf{Episode Inversion.}\ We first propose the basic Episode Inversion (EI) to synthesize the pseudo task for each episode. At the beginning of meta training, we first introduce a small dynamic dataset $\mathcal{D}$ initialized by Gaussian noise with mean $\mu=0$ and standard deviation $\sigma=1$. For each class, it only contains $K+M$ instances, consistent with $N$-way $K$-shot target task configuration with $M$ instances each class in $\mathcal{Q}$. At each iteration, $\mathcal{D}$ is dynamically updated to better quality by taking one-step gradient descent to minimize
\begin{equation}
\small
\mathcal{L}_{inv}(\mathcal{D})=\sum_{(\boldsymbol{\hat{x}},y) \in \mathcal{D}}l(\boldsymbol{\hat{x}},y;\boldsymbol{\psi})+\mathcal{R}_{prior}(\boldsymbol{\hat{x}})+\mathcal{R}_{feature}(\boldsymbol{\hat{x}}).
\label{eq:lossData}
\end{equation}
$\boldsymbol{\psi}$ is the parameter of the corresponding pre-trained model. For each pair $(\boldsymbol{\hat{x}},y)$, $\boldsymbol{\hat{x}}$ is the pseudo image, and $y$ is the manually assigned label to which we desire $\boldsymbol{\hat{x}}$ to belong. $l(\cdot)$ is the per datum classification loss (\eg, cross-entropy loss) mentioned in \cref{subsec:startFromEpisodeTraining}. $\mathcal{R}_{prior}(\cdot)$ is borrowed from DeepDream \cite{deepdream} to steer $\hat{x}$ away from unreal images:

\begin{equation}
\small
\mathcal{R}_{prior}(\boldsymbol{\hat{x}})=\alpha_{TV}\mathcal{R}_{TV}(\boldsymbol{\hat{x}})+\alpha_{l_{2}}\mathcal{R}_{l_{2}}(\boldsymbol{\hat{x}}),
\label{eq:prior}
\end{equation}
where $\mathcal{R}_{TV}$ and $\mathcal{R}_{l_{2}}$ are the total variance and $l_2$ norm of $\boldsymbol{\hat{x}}$, with scaling factor $\alpha_{TV}$ and $\alpha_{l_{2}}$.
$\mathcal{R}_{feature}(\cdot)$ is a feature distribution regularization term used in DeepInversion \cite{yin2020dreaming} to minimize the distance between feature maps of pseudo images and original training images in each convolutional layer:
\begin{equation}
\small
\begin{aligned}
\mathcal{R}_{feature}(\boldsymbol{\hat{x}})=&\sum_{l}\Vert\mu_{l}(\boldsymbol{\hat{x}})-{\rm BN}_{l}\rm{(running\_mean)}\Vert+\\
&\sum_{l}\Vert\sigma_l^2(\boldsymbol{\hat{x}})-{\rm BN}_{l}{\rm (running\_variance)}\Vert.
\end{aligned}
\label{eq:feature}
\end{equation}
We feed a batch of pseudo images into the given pre-trained model to obtain feature maps in each convolutional layer. $\mu_{l}(\boldsymbol{\hat{x}})$ and $\sigma_l^2(\boldsymbol{\hat{x}})$ are the batch-wise mean and variance of those feature maps in the $l^{th}$ convolutional layer. ${\rm BN}_{l}\rm{(running\_mean)}$ and ${\rm BN}_{l}{\rm (running\_variance)}$ are the running average mean and variance stored in the $l^{th}$ BatchNorm layer of pre-trained model, which is calculated during the pre-training period using original training images and can be obtained without access to original training images.


For each episode, we construct a pseudo task by first sampling a subset of labels, and then sampling the pseudo support data and query data from $\mathcal{D}$.

%\vspace{-0.03cm}
\noindent
\textbf{Curriculum Mechanism.}\ Directly synthesizing the pseudo images without any feedback may repeatedly generate not optimal or not useful data for meta training (see \cref{fig:curriculum}). For example, we may repeatedly synthesize the tasks already learned well, thus increasing the unnecessary inversion cost. To find an effective episode inversion path, we propose to perform EI with curriculum mechanism, \ie ECI. We aim to steer the dynamic dataset $\mathcal{D}$ towards appropriate difficulty so that only tasks not learned yet are considered at each iteration, which avoids repeatedly synthesizing the tasks already learned well. We characterize the difficulty of $\mathcal{D}$ for current meta model (parameterized by $\boldsymbol{\theta}$) as the expected outer loss over candidate tasks, \ie $\underset{\mathcal{T} \in \mathcal{D}}{\mathbb{E}}\left[\mathcal{L}_{outer}(\mathcal{T};\boldsymbol{\theta})\right]$. Specifically, a large outer loss presents a hard task; a task which has been learned well by current meta model, however, leads to a small outer loss. 
Thus, as illustrated in \cref{fig:pipeline}, we design a loss-based criteria to judge whether the meta model has learned well on tasks sampled from current $\mathcal{D}$. Specifically, at each iteration, for a batch of episodes $\{\mathcal{T}_{i}\}$, if the sum of the outer loss (training accuracy) over $\{\mathcal{T}_{i}\}$ has not decreased (increased) for 6 consecutive iterations, the meta model will send a positive feedback, indicating that harder tasks are needed for the next iteration; else negative. To synthesize harder tasks only when the feedback is positive, we design the \textit{Gradient Switch} controlled by the real-time feedback. Specifically, if the meta model has converged, the positive feedback will turn on the Gradient Switch so that the reversed gradient of outer loss can flow backwards to $\mathcal{D}$ (see \cref{fig:pipeline}). In this way, we will update $\mathcal{D}$ to maximize the outer loss through gradient descent, which means we can obtain harder tasks with a larger outer loss for the next iteration.
% Thus, as illustrated in \cref{fig:pipeline}, we design a \textit{Gradient Switch} \ls{Previous sentence indicate that loss value is the criteria. Why we call it "Gradient Switch"? } $\mathbb{I}$ controlled by the \textit{feedback} $\Omega$ to decide when harder tasks should be synthesized according to the real-time feedback of meta model. Specifically, after each batch of episodes, we check whether the current meta model has learned well on candidate tasks sampled from $\mathcal{D}$. If the meta model is converged over $\mathcal{D}$ \zy{unclear statement, what is the meaning of converged, how to construct the feedback $\Omega$}, the feedback $\Omega$ turns to be positive; else negative.
The Gradient Switch controlled by feedback $\Omega$ works as an indicator function, \ie, 
\begin{equation}
\small
\mathbb{I}(\Omega)=
\left\{
\begin{aligned}
    &1,\  if\ \Omega\ is\ positive; \\
    &0,\  if\ \Omega\ is\ negative.
\end{aligned}
\right.
\label{eq:switch}
\end{equation}
In this way, ECI finds an effective episode inversion path by ignoring the tasks already learned well, thus improving the efficiency of pseudo episode training.



\begin{algorithm}[t]
\small
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Require}{Require}
%\Require{The meta-model parameters $\boldsymbol{\theta}$; The Dynamic dataset $\mathcal{D}$; The step size $\beta$ and $\gamma$. The scaling factor $\lambda_{adv}$;}
% \Input{The set of pretrained model parameters $\phi_0, \phi_1,\cdots, \phi_{M}$.}
% \Output{The updated meta-model parameters $\boldsymbol{\theta}$.}
\textbf{REQUIRE} Meta-model parameters $\boldsymbol{\theta}$;  dynamic dataset $\mathcal{D}$; step size $\alpha_{inner}$, $\alpha_{outer}$ and $\beta$; scaling factor $\lambda$.\;
Initialize $\mathcal{D}$\ and $\boldsymbol{\theta}$\;
%by $\frac{1}{B}\sum_{i}^{B}\mathcal{M}_{i}$\;
\While(){not done}{
\tcp{Episode Curriculum Inversion}
%$\textit{//\ Stage\ 1: Episode\  Curriculum\ Inversion}$\;
%Freeze $\boldsymbol{\theta}$\;
Evaluate $\-\mathcal{L}_{inv}(\mathcal{D})$ \wrt \cref{eq:lossData}\;
\If {feedback $\Omega$\ is\ Positive}{
Randomly sample a task $\mathcal{T}$\ from $\mathcal{D}$\;
%Split $\mathcal{T}$ into $(\mathcal{S}, \mathcal{Q})$\;
Evaluate $\mathcal{L}_{outer}(\mathcal{T}; \boldsymbol{\theta})$ \wrt \cref{eq:lossMeta}\;
Update $\mathcal{D} \leftarrow \mathcal{D} - \beta*\nabla_{\mathcal{D}}\left(\mathcal{L}_{inv}(\mathcal{D})-\lambda*\mathcal{L}_{outer}(\mathcal{T}; \boldsymbol{\theta})\right)$
}
\Else{Update $\mathcal{D} \leftarrow \mathcal{D} - \beta*\nabla_{\mathcal{D}}\left(\mathcal{L}_{inv}(\mathcal{D})\right)$}
%Unfreeze $\boldsymbol{\theta}$ \;
\tcp{Meta Training}
%$\mathbf{//\ Stage\ 2: Meta\ training}$\;
%Freeze $\mathcal{D}$\;
Randomly generate a batch of tasks $\mathcal{T}_i$ from $\mathcal{D}$\;
\For{$\mathbf{all}$ $\mathcal{T}_i$}{
%Split $\mathcal{T}_i$ into $(\mathcal{S}_i, \mathcal{Q}_i)$\;
Evaluate $\mathcal{L}_{outer}(\mathcal{T}_i; \boldsymbol{\theta})$ \wrt \cref{eq:lossMeta}\;
}
Update $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha_{outer}*\nabla_{\boldsymbol{\theta}}\sum_{\mathcal{T}_i}\mathcal{L}_{outer}(\mathcal{T}_i; \boldsymbol{\theta})$\;
\tcp{Update Feedback}
Check \textit{feedback} $\Omega$\;
%Unfreeze $\mathcal{D}$\;
}
\caption{Data-free Meta Training with ECI\label{IR}}
\label{alg:eci}
\end{algorithm}

 \noindent
\textbf{Adversarial Optimization.}\ 
% \zy{add transition sentences to connect the descriptions in this section and previous sections so that the section transition is smooth. For example, to achieve the goal in Section
% XXX}. 
To adaptively synthesize episodes from easy to hard along with the meta training process,
we perform pseudo episode training by adversarially updating the meta model (parameterized by $\boldsymbol{\theta}$) and dynamic dataset $\mathcal{D}$ in an end-to-end manner. We search the optimal meta model parameters $\boldsymbol{\theta}$ by minimizing the expected outer loss  over candidate tasks from $\mathcal{D}$ , \ie, $\underset{\mathcal{T} \in \mathcal{D}}{\mathbb{E}}[\mathcal{L}_{outer}(\mathcal{T}; \boldsymbol{\theta})]$, while 
 we update the dynamic dataset $\mathcal{D}$ to synthesize harder tasks with higher quality by minimizing reversed outer loss $-\underset{\mathcal{T} \in \mathcal{D}}{\mathbb{E}}[\mathcal{L}_{outer}(\mathcal{T}; \boldsymbol{\theta})]$ (when feedback $\Omega$ is positive) and inversion loss $\mathcal{L}_{inv}(\mathcal{D})$. Thus, we formulate the overall optimization process as an adversarial form:
\begin{equation}
\small
\begin{aligned}
\min_{\boldsymbol{\theta}}\max_{\mathcal{D}}\underset{\mathcal{T} \in \mathcal{D}}{\mathbb{E}}[-\mathcal{L}_{inv}(\mathcal{D})
+\mathbb{I}(\Omega)*\mathcal{L}_{outer}(\mathcal{T};\boldsymbol{\theta})].
\end{aligned}
\label{eq:lossAll}
\end{equation}
To the end, we summarize the detailed procedure of ECI for data-free meta training in \cref{alg:eci}.


\begin{algorithm}[t]
\small
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Require}{Require}
\textbf{REQUIRE} The meta testing task $\mathcal{T}_{test}=\{\mathcal{S}_{test},\mathcal{Q}_{test}\}$; meta initialization $\boldsymbol{\theta}$.\;
\tcp{Inner Loop}
Obtain $\{\boldsymbol{\varphi}_{test}^{*},\mathbf{W}_{test}^{*}\}$ via fast adaptation over $\mathcal{S}_{test}$\;
% $\{\boldsymbol{\varphi}_{test}^{*},\mathbf{W}_{test}^{*}\}=InnerLoop(\mathcal{S}_{test};\{\phi,\mathbf{W}\})$\;
Obtain pseudo support set $\hat{\mathcal{S}}_{test}$ \wrt \cref{eq:lossData}\;
\tcp{Calibration}
Calibrate $\boldsymbol{\varphi}_{test}^{*}$ by minimizing \cref{eq:contrast}\;
% \tcp{Calibration for the classifier}
% Calibrate $\mathbf{W}_{test}^{*}$ by minimizing $\mathcal{L}_{classifier} (\mathbf{W}_{test}^{*})$\;
Train a new linear classifier head\;
\tcp{Prediction}
Make predictions on the query set $\mathcal{Q}_{test}$
\caption{Meta Testing with ICFIL}
\label{alg:icfil}
\end{algorithm}


\subsection{Inversion Calibration following Inner Loop} \label{subsec:IFCIL}
%\zy{briefly discuss why need to propose this compoent with one or two sentence}
Pseudo images synthesized via ECI can not cover all semantic information of real images, thus leading to a gap between meta training and testing task distribution (see \cref{fig:shift}). In other words, the task-distribution shift issue in the data-free setting is much more significant than that in the setting of data-based meta-learning. To address this issue, we propose a simple plug-and-play supplement---ICFIL---only used during meta testing, to further narrow the gap between meta training and testing task distribution. 


For an $N$-way target task $\mathcal{T}_{test}=\{\mathcal{S}_{test},\mathcal{Q}_{test}\}$ during meta testing, we are given the meta initialization $\boldsymbol{\theta}$ according to \cref{alg:eci}. The base model initialized by $\boldsymbol{\theta}$ comprises a backbone $\phi:\mathbb{R}^{N_{i n}} \rightarrow \mathbb{R}^{N_f}$ (parameterized by $\boldsymbol{\varphi}$) and a linear classifier head parametrized by the weight matrix $\mathbf{W} \in \mathbb{R}^{N_f \times N}$, where $N_{in}$, $N_f$ are the dimension of the input and embedding, respectively. We first perform fast adaptation over $\mathcal{S}_{test}$. The adapted parameters are $\boldsymbol{\theta}_{test}^{*}=\{\boldsymbol{\varphi}_{test}^{*},\mathbf{W}_{test}^{*}\}$. Note that this is precisely identical to the standard meta testing procedure for regular meta-learning. The ICFIL does not assume access to query set $\mathcal{Q}_{test}$ or any additional data and it can be viewed as a simple plug-and-play supplement for the standard meta testing period. We use ICFIL to calibrate the adapted base model before it predicts on the query set $\mathcal{Q}_{test}$. As illustrated in \cref{fig:pipeline}, we first synthesize a batch of pseudo images $\hat{\mathcal{S}}_{test}$ from the adapted base model according to \cref{eq:lossData}. Inspired by the supervised contrastive learning \cite{khosla2020supervised}, for a given image $\boldsymbol{x}$ in $\mathcal{S}_{test}$, we define all pseudo images $\boldsymbol{\hat{x}}$ with the same label as positive samples $\boldsymbol{\hat{x}}^{+}$ while the others as negative samples $\boldsymbol{\hat{x}}^{-}$. The core idea behind ICFIL is to force the base model to focus on the overlapping information in both pseudo and real images, and ignore the others. 
% Thus, it can extract consistent features for both pseudo and real images with the same label. 
The calibration loss is formulated as:
\begin{equation}
\small
\!\!\! \mathcal{L}_{calibration}(\phi)\!=\!-\!\sum_{\boldsymbol{x} \in \mathcal{S}_{test}}\sum_{\boldsymbol{\hat{x}}^{+}}\log\frac{\exp\left[\phi(\boldsymbol{x})^{T}\phi(\boldsymbol{\hat{x}}^{+})\//\tau\right]}{\sum_{\boldsymbol{\hat{x}}}\exp\left[\phi(\boldsymbol{x})^{T}\phi(\boldsymbol{\hat{x}})\//\tau\right]}.
\label{eq:contrast}
\end{equation}
where temperature parameter $\tau$ is widely used to help discriminate positive and negative samples. We then freeze the backbone and train a new linear classifier head by minimizing a standard classification loss (\eg, cross-entropy loss) over $\mathcal{S}_{test}$. The whole procedure for meta testing with ICFIL is provided in \cref{alg:icfil}.











