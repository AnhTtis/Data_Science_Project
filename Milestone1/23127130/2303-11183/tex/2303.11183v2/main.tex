% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{appendix}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{xcolor,colortbl}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\crefname{equation}{Eq.}{Eqs.}
\crefname{algorithm}{Alg.}{Algs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{8321} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\newcommand{\ls}[1]{{\color{red}{\bf\sf [LS: #1]}}}
\newcommand{\zy}[1]{{\color{blue}{\bf\sf [ZY: #1]}}}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning }


% \iffalse
\author{
Zixuan Hu\textsuperscript{\rm 1}
\quad 
Li Shen\textsuperscript{\rm 2,}\thanks{Corresponding authors: Li Shen and Chun Yuan} 
\quad 
Zhenyi Wang\textsuperscript{\rm 3}
\quad 
Tongliang Liu\textsuperscript{\rm 4}
\quad 
Chun Yuan\textsuperscript{\rm 1,*}
\quad 
Dacheng Tao\textsuperscript{\rm 2}
\\
\textsuperscript{\rm 1}Tsinghua Shenzhen International Graduate School, China; 
\textsuperscript{\rm 2}JD Explore Academy, China \\
\textsuperscript{\rm 3}State University of New York at Buffalo, USA;
\textsuperscript{\rm 4}The University of Sydney, Australia\\
{\tt\small huzixuan21@mails.tsinghua.edu.cn; mathshenli@gmail.com; zhenyiwa@buffalo.edu}\\
{\tt\small tongliang.liu@sydney.edu.au; yuanc@sz.tsinghua.edu.cn; dacheng.tao@gmail.com
}
}
% \fi



\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}

The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed \textbf{PURER}, which contains: (1) e\textbf{P}isode c\textbf{U}rriculum inve\textbf{R}sion (ECI) during data-free meta training; and (2) inv\textbf{E}rsion calib\textbf{R}ation following inner loop (ICFIL) during meta testing. 
During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes according to the real-time feedback of the meta model. We formulate the optimization process of meta training with ECI as an adversarial form in an end-to-end manner. 
During meta testing, we further propose a simple plug-and-play supplement---ICFIL---only used during meta testing to narrow the gap between meta training and meta testing task distribution. Extensive experiments in various real-world scenarios show the superior performance of ours.

\end{abstract}


%%%%%%%%% BODY TEXT
\input{texfile/introduction.tex}
\input{texfile/relatedwork.tex}
\input{texfile/problemdefinition.tex}
\input{texfile/methodology.tex}
\input{texfile/experiment.tex}
\input{texfile/conclusion.tex}

\clearpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{texfile/ref}
}

\clearpage



\begin{appendix}
%\appendix
\appendixpage
\section{Implementation Details}
\label{app:implementationdetails}
\noindent
\textbf{Implementation details for SS.}\ For SS scenario, all pre-trained models are with the same architecture. We take Conv4 
as the architecture of all pre-trained models and the meta model, the same as regular meta-learning works\cite{finn2017model,chen2020variational,liu2020adaptive,wang2019simpleshot}. Conv4 is a four-block convolution neural network, where each block consists of 32 $3 \times 3$ filters, a BatchNorm, a ReLU and an additional $2 \times 2$ max-pooling. 
The episode-batch size for each iteration is 4. We adopt the Adam optimizer to learn the dynamic dataset, base model and meta model with a learning rate of 0.25, 0.01 and 0.001, respectively. We take one-step gradient descent to perform fast adaptation for both meta training and testing. After 4000 iterations (outer loops), we add curriculum mechanism to episode inversion. At each iteration, the meta model sends a positive feedback if the sum of training accuracy over the episode batch has not increased for 6 consecutive iterations. We empirically set the scaling factor $\lambda$ as 10.  For meta testing, we calibrate the backbone for 1 iteration via Adam optimizer with a learning rate of $1e-5$. We set the temperature parameter as 0.1. Then we freeze the backbone and train a new classifier over the entire support set for 100 iterations via Adam optimizer with a learning rate of 0.01. For episode inversion, we set $\alpha_{TV}=1e-4$  and $\alpha_{l_2}=1e-5$.


\noindent
\textbf{Implementation details for SH.}\ For SH scenario, all pre-trained models are trained on the same dataset but with heterogeneous architectures. For each task, we pre-train the model with an architecture randomly selected from Conv4, ResNet-10 and ResNet-18. Compared to Conv4, ResNet-10 and ResNet-18 are larger-scale neural networks. The ResNet-10 has 4 residual stages of 1 block each which gradually decreases the spatial resolution. The ResNet-18 also has 4 residual stages but with 2 blocks per stage, which is a deeper neural network. The other configurations are the same as SS.


\noindent
\textbf{Implementation details for MH.}\ For MH scenario,  all pre-trained models are trained on multiple datasets with heterogeneous architectures. During meta training, we construct each task  from the meta training dataset randomly selected from CIFAR-FS and MiniImageNet. We pre-train the model with an architecture randomly selected from Conv4, ResNet-10 and ResNet-18. We take Conv4 as the meta model architecture. During meta testing, we report the average accuracy over 600 tasks sampled from both CIFAR-FS and MiniImageNet meta testing datasets. The other configurations are the same as SS.


\section{More Results}
\label{app:moreresults}

\noindent
\textbf{Hyperparameter Sensitivity.}\ 
We evaluate the model performance sensitivity with different values $\lambda$ in \cref{tab:sentivity}. As can be seen, the achieved performance of our method is stable with the changes of $\lambda$ value, although there are some variations among different $\lambda$ values. This advantage makes it easy to apply our method in practice.


\begin{table}[tbp]
  \centering
  \small
  \caption{Hyperparameter sensitivity on  DFML CIFAR-FS 5-way classification in SS scenario.}
    \label{tab:sentivity}
      \vspace{-0.2cm}
      \scalebox{0.99}{
  \begin{tabular}{ccc}
    \toprule

    $\boldsymbol{\lambda}$&   5-way 1-shot & 5-way 5-shot \\
    \midrule
    $\lambda$=10.0 & 38.66 $\pm$ 0.78 & 51.95 $\pm$ 0.79\\
    $\lambda$=2.0 & 37.87 $\pm$ 0.70 & 49.09 $\pm$ 0.75\\
    $\lambda$=0.5 & 38.04 $\pm$ 0.79 &48.91 $\pm$ 0.75\\
     \bottomrule
  \end{tabular}}
    %\vspace{-0.4cm}
\end{table}


\begin{table}[]
    \centering
    \caption{Effect of the number of pre-trained models in SS scenario.}
    \begin{tabular}{ccc}
    \toprule
         \shortstack{\textbf{num}}& 5-way 1-shot & 5-way 5-shot\\
         \midrule
         2& 34.28 $\pm$ 0.75 & 45.40 $\pm$ 0.74\\
         6& 36.78 $\pm$ 0.75 & 47.71 $\pm$ 0.80 \\
         13& 38.66 $\pm$ 0.78 & 51.95 $\pm$ 0.79\\
    
    \bottomrule
    \end{tabular}
    
    \label{tab:numofpretrained}
\end{table}

\noindent
\textbf{Effect of the number of pre-trained models.} We perform the experiments about DFML with the different numbers of pre-trained models in SS scenario. We train each pre-trained model for a 5-way classification problem. \cref{tab:numofpretrained} shows the results. By increasing the number from 2 to 6, we can observe $2.5\%$ and $2.31\%$ performance gains for 1-shot and 5-shot learning, respectively. The gains increase to $4.38$ and $6.55\%$ by increasing the number from 2 to 13. The reason is that more pre-trained models can provide more underlying data knowledge of different classes. With broader data knowledge, meta-learning can acquire better generalization for target tasks with new unseen classes.



\end{appendix}

\end{document}
