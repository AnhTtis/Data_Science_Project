\section{Experiments}
\label{sec:experiments}

We conduct extensive experiments in various real-world scenarios to demonstrate the effectiveness of our proposed PURER framework.
In \cref{subsec:setup}, we present the basic experiment setup. In \cref{subsec:ss,subsec:sh,subsec:mh}, we provide experiments about DFML in various scenarios, including SS, SH and MH. In \cref{subsec:ablation}, we provide more analysis of PURER.
% \zy{add one overall summary sentence to describe what each section you did. For example, in Section, we present the experiment setup. In Section, we provide the experiments about}

\subsection{Experiments Setup}
\label{subsec:setup}
\noindent
\textbf{Baselines.}\ We compare PURER with four typical baselines: \textbf{ (i) Random.}\ Randomly initialize the base model before the adaptation of each target task. \textbf{(ii) Average.}\ Average all pre-trained models to initialize the base model before the adaptation of each target task. \textbf{(iii) OTA\cite{singh2020model}.}\ Calculate the weighted average of all pre-trained models via optimal transport as the initialization of the base model before the adaptation of each target task. 
% This method assumes all pre-trained models solve the same task.
\textbf{(iv) DRO\cite{wang2022metalearning}.}\ Meta-learn a neural network to predict the model parameters given the task embedding. The predicted model parameters with the average task embedding as input are served  as the meta initialization. This is the first work to solve DFML by directly predicting the meta initialization in parameter space.

\noindent
\textbf{Datasets for meta testing.}\ 
CIFAR-FS \cite{bertinetto2018meta} and MiniImageNet \cite{vinyals2016matching} are commonly used in meta-learning, which consist of 100 classes with 600 images per class, respectively. We adopt a standard dataset split as \cite{wang2022metalearning}: 64 classes for meta training, 16 classes for meta validation and 20 classes for meta testing. All splits are non-overlapping. 
% The image resolution of CIFAR-FS is $32 \times 32$ and $84 \times 84$ for MiniImageNet. 
Note that we have no access to the meta training data in DFML setting.


\noindent
\textbf{Evaluation metric.}\ 
% Each $N$-way $K$-shot target task has a  support set  to adapt the meta-learned initialization to each target task. We take one-step gradient descent to perform fast adaptation in inner loop. 
We evaluate the performance by the average accuracy and standard deviation over 600 unseen target tasks sampled from meta testing dataset.

\noindent
\textbf{Scenarios.}\ (i) \textbf{SS}. All pre-trained models are trained on \textbf{S}ame dataset with \textbf{S}ame architecture. (ii) \textbf{SH}. All pre-trained models are trained on \textbf{S}ame dataset but with \textbf{H}eterogeneous architectures. (iii) \textbf{MH}. All pre-trained models are trained on \textbf{M}ultiple datasets with \textbf{H}eterogeneous architectures. Existing DFML work, \ie, baseline DRO \cite{wang2022meta} and baselines Average and OTA 
merely apply to SS because they require pre-trained models have the same architecture. We argue that SH and MH are more widely applicable scenarios in real-world applications and our PURER is a unified framework for addressing SS, SH and MH simultaneously.



\subsection{Experiments of DFML in SS}
\label{subsec:ss}
\label{subsec:same}
\noindent
\textbf{Overview.}\ We first perform experiments in \textbf{SS} scenario. 
We construct a collection of $N$-way tasks from the meta-training dataset and pre-train the model via standard supervised learning for each task. The collection of pre-trained models is used as the given resource models for DFML.

\noindent
\textbf{Implementation details.}\ We take Conv4 
as the architecture of all pre-trained models and the meta model, the same as regular meta-learning works\cite{finn2017model,chen2020variational,liu2020adaptive,wang2019simpleshot}.  We take one-step gradient descent to perform fast adaptation for both meta training and testing. More implementation details are provided in \cref{app:implementationdetails}.








\begin{table}[htbp]
  \centering
  \small
   \caption{Compare to baselines in SS scenario.} 
   \vspace{-0.2cm}
  \begin{tabular}{clcc}
    \toprule
     \textbf{SS}&\textbf{Method}  & 1-shot &5-shot \\
    \midrule
    \multirow{5}{*}{\shortstack{\textbf{CIFAR-FS}\\\textbf{5-way}}} & Random &21.65  $\pm$ 0.45 & 21.59  $\pm$ 0.45\\
     & Average & 28.12  $\pm$ 0.62& 32.15  $\pm$ 0.64\\
     & OTA & 29.10 $\pm$ 0.65 & 34.33 $\pm$ 0.67\\
     %& MFGP & & \\
     & DRO & 23.92 $\pm$ 0.49 & 24.34 $\pm$ 0.49\\
     
     & Ours & \textbf{38.66 $\pm$ 0.78} & \textbf{51.95 $\pm$ 0.79}\\
    \midrule
    \midrule
    \multirow{5}{*}{\shortstack{\textbf{MiniImageNet}\\\textbf{5-way}}} & Random &22.45  $\pm$ 0.41 & 23.48  $\pm$ 0.45\\
     & Average & 22.87  $\pm$  0.39&26.13  $\pm$  0.43\\
     & OTA & 24.22 $\pm$ 0.53& 27.22 $\pm$ 0.59 \\
     %& MFGP & & \\
     & DRO & 23.96 $\pm$ 0.42 &25.81 $\pm$ 0.41\\

     & Ours & \textbf{31.14 $\pm$ 0.63} & \textbf{40.86 $\pm$ 0.64}\\
     \bottomrule
  \end{tabular}
  \label{tab:dfmeta5way}
  %\vspace{-0.2cm}
\end{table}


\noindent
\textbf{Results.}\ \cref{tab:dfmeta5way} shows the results for 5-way classification on CIFAR-FS and MiniImageNet under the scenario of SS. For CIFAR-FS, our method outperforms the best baseline by $9.56\%$ and $17.62\%$ for 1-shot and 5-shot learning, respectively. For MiniImageNet,  our method outperforms the best baseline by $6.92\%$ and $13.64\%$ for 1-shot and 5-shot learning, respectively. The results show that random initialization is insufficient to perform fast adaptation. Simply averaging and OTA perform worse because: i) they fuse models layer-wise, but each pre-trained model are trained to solve different tasks, thus lacking the precise correspondence in parameter space; ii) they lack meta-learning objectives so that the fused model can not generalize well to new unseen tasks. DRO also performs not better due to the small number of pre-trained models. DRO trains a neural network to predict the meta initialization, thus requiring a relatively large number of pre-trained models as training resources. Our proposed method performs best because we leverage the fruitful underlying data knowledge contained in each pre-trained model instead of focusing on parameter space. We perform pseudo episode training with meta-learning objectives, thus learning to adapt fast to new unseen tasks with few labeled data.




\subsection{Experiments of DFML in SH}
\label{subsec:sh}
\noindent
\textbf{Overview.} To verify the broad applicability of our proposed method, we perform experiments under a more realistic and challenging scenario, \textbf{SH}. For each task, we pre-train the model with a randomly selected architecture.


\noindent
\textbf{Implementation details.}\ For each task, we pre-train the model with an architecture randomly selected from Conv4, ResNet-10 and ResNet-18. Compared to Conv4, ResNet-10 and ResNet-18 are larger-scale neural networks. We take Conv4 as the meta model architecture. More implementation details are provided in \cref{app:implementationdetails}.


\noindent
\textbf{Results.}\ \cref{tab:dfmetamix} shows the results for 5-way classification on CIFAR-FS and MiniImageNet under the scenario of SH. For CIFAR-FS, our proposed method outperforms the baseline by $17.50\%$ and $27.49\%$ for 1-shot and 5-shot learning, respectively. For MiniImageNet, our method outperforms the baseline by $6.31\%$ and $11.71\%$ for 1-shot and 5-shot learning, respectively. Ours can apply to this real-world scenario and perform the best because we leverage the underlying data knowledge regardless of the scale and architecture of pre-trained models.


\begin{table}[htbp]
  \centering
  \small
  \caption{Compare to baselines in SH scenario. Pre-trained models are trained with heterogeneous architectures (Conv4, ResNet-10 and ResNet-18).}
  \label{tab:dfmetamix}
    \vspace{-0.2cm}
  \begin{tabular}{clcc}
    \toprule
     \textbf{SH}&\textbf{Method}  & 1-shot & 5-shot \\
    \midrule
    \multirow{2}{*}{\shortstack{\textbf{CIFAR-FS}\\\textbf{5-way}}} & Random &21.65  $\pm$ 0.45& 21.59  $\pm$ 0.45  \\
     
     & Ours &\textbf{39.15 $\pm$ 0.70} & \textbf{49.08 $\pm$ 0.74} \\
    \midrule
    \midrule
    \multirow{2}{*}{\shortstack{\textbf{MiniImageNet}\\\textbf{5-way}}} & Random & 22.45  $\pm$  0.41&23.48  $\pm$  0.45 \\
     & Ours & \textbf{28.76 $\pm$ 0.60} & \textbf{35.19 $\pm$ 0.64}\\
     \bottomrule
  \end{tabular}
  %\vspace{-0.2cm}
\end{table}



\subsection{Experiments of DFML in MH}
\label{subsec:mh}
\noindent
\textbf{Overview.}\ We further perform experiments in \textbf{MH} scenario, which provides a more challenging test.  We construct a collection of tasks from different meta training datasets. For each task, we pre-train the model with a randomly selected architecture. In this way, we obtain a collection of pre-trained models trained on various datasets with different architectures.

\noindent
\textbf{Implementation details.}\ During meta training, we construct each task from the meta training dataset randomly selected from CIFAR-FS and MiniImageNet. We pre-train the model with an architecture randomly selected from Conv4, ResNet-10 and ResNet-18. We take Conv4 as the meta model architecture. During meta testing, we report the average accuracy over 600 tasks sampled from both CIFAR-FS and MiniImageNet meta testing datasets. More implementation details are provided in \cref{app:implementationdetails}.

\noindent
\textbf{Results.}\ \cref{tab:dfmetamixdata} shows the results for 5-way classification under the scenario of MH. Our proposed method outperforms the baseline by $7.39\%$ and $11.76\%$ for 5-way 1-shot and 5-shot learning, respectively. Ours can apply to this more challenging scenario because our method is dataset-agnostic and architecture-agnostic. In other word, PURER works well on pre-trained models trained on various datasets with different architectures.

\begin{table}[htbp]
  \centering
  \small
  \caption{Compare to baselines in MH scenario. Pre-trained models are from multiple datasets (CIFAR-100 and MiniImageNet)  with heterogeneous architectures (Conv4, ResNet-10 and ResNet-18).}
  \label{tab:dfmetamixdata}
    \vspace{-0.2cm}
  \begin{tabular}{clcc}
    \toprule
     \textbf{MH}&\textbf{Method}  &  1-shot & 5-shot \\
    \midrule
    \multirow{2}{*}{\shortstack{\textbf{5-way}}} & Random &21.11  $\pm$ 0.41& 21.34  $\pm$ 0.40  \\
     
     & Ours &\textbf{28.50 $\pm$ 0.63} & \textbf{33.10 $\pm$ 0.69} \\
     \bottomrule
  \end{tabular}
  %\vspace{-0.2cm}
\end{table}




\subsection{Ablation Study}
\label{subsec:ablation}


\noindent
\textbf{Effectiveness of each component in PUERE.}\  \cref{tab:ablation} shows the results of ablation studies in SS scenario. We first introduce a vanilla baseline (EI) performing pseudo episode training without curriculum mechanism and meta testing without ICFIL. EI still outperforms the best baselines in \cref{tab:dfmeta5way} by $3.8\%$ and $6.55\%$ for 1-shot and 5-shot learning, respectively, which verifies the effectiveness of the basic idea of DFML leveraging the underlying data knowledge. To further evaluate the effectiveness of curriculum mechanism, we append curriculum to EI, \ie, ECI. We can observe that with curriculum mechanism, the performance can be improved by $1.09\%$ and $2.93\%$ for 1-shot and 5-shot learning, respectively. Similarly, we adopt ICFIL during meta testing for EI, \ie, EI + ICFIL, and the performance can be improved by $1.84\%$ and $2.89\%$ for 1-shot and 5-shot learning, respectively. By introducing both curriculum mechanism for episode inversion and ICFIL for meta testing, we achieve the best performance with a boosting gain of $3.12\%$ and $7.09\%$ for 1-shot and 5-shot learning, respectively, which demonstrates the effectiveness of the joint schema. 


\begin{table}[h]
  \centering
  \small
  \caption{Ablation studies on MiniImageNet in SS scenario.}
    %\vspace{-0.6cm}
  \scalebox{0.7}{
  \begin{tabular}{ccccc}
    \toprule
    \small
    \multirow{2}{*}{\textbf{SS}} & \multicolumn{2}{c}{Component} & \multicolumn{2}{c}{Accuracy} \\
    \cmidrule(r){2-3}
    \cmidrule(r){4-5}
    &\textbf{Curriculum}&\textbf{ICFIL}&5-way 1-shot&5-way 5-shot\\
    \midrule
    \textbf{EI}&&&28.02 $\pm$ 0.58&33.77 $\pm$ 0.63\\
    \midrule
    &\checkmark&& 29.11$\pm$ 0.59 &36.70 $\pm$ 0.64 \\
    &&\checkmark&29.86 $\pm$ 0.66&36.66 $\pm$ 0.66\\
    \midrule
    \textbf{Ours}&\checkmark&\checkmark&\textbf{31.14$\pm$ 0.63}&\textbf{40.86 $\pm$ 0.64}\\
     \bottomrule
  \end{tabular}}
  
  \label{tab:ablation}
  %\vspace{-0.6cm}
\end{table}




\begin{table}[tbp]
  \centering
  \small
  \caption{Effect of the number of training class for each task in SS.}
    \label{tab:dfmeta10way}
      \vspace{-0.2cm}
      \scalebox{0.8}{
  \begin{tabular}{clcc}
    \toprule

     \textbf{SS}&\textbf{Method}  &  1-shot &  5-shot \\
    \midrule
    \multirow{5}{*}{\shortstack{\textbf{CIFAR-FS}\\\textbf{10-way}}} & Random &  10.26 $\pm$ 0.22 & 10.32 $\pm$ 0.22 \\
     & Average  & 12.23 $\pm$ 0.30 &14.86 $\pm$ 0.30 \\
     & OTA & 13.40 $\pm$ 0.32 &15.17 $\pm$ 0.36\\

     & DRO & 11.29 $\pm$ 0.25 & 11.56 $\pm$ 0.26 \\

     & Ours & \textbf{22.88 $\pm$ 0.41} & \textbf{36.19 $\pm$ 0.45}\\

    \midrule
    \midrule
    \multirow{5}{*}{\shortstack{\textbf{MiniImageNet}\\\textbf{10-way}}} & Random & 
 11.20 $\pm$ 0.23 & 11.34 $\pm$ 0.24  \\
     & Average & 12.83 $\pm$ 0.27 & 13.94 $\pm$ 0.27 \\
     & OTA & 11.92 $\pm$ 0.25&13.25 $\pm$ 0.29 \\
     %& MFGP & & \\
     & DRO &11.78 $\pm$ 0.20 & 13.08 $\pm$ 0.24 \\
     
     & Ours & \textbf{18.16 $\pm$ 0.37}&\textbf{ 26.92 $\pm$ 0.37}\\

     \bottomrule
  \end{tabular}}
    \vspace{-0.2cm}
\end{table}

\begin{table}[tbp]
  \centering
  \small
  \caption{Compare to MAML in SS scenario. 
  $^{\dag}$: use real meta-training dataset of equal size to the dynamic dataset used in DFML. Grey: data-based meta-learning method.}
  \label{tab:maml}
    \vspace{-0.2cm}
  \scalebox{0.9}{
  \begin{tabular}{clcc}
    \toprule
     \textbf{SS}&\textbf{Method}  & 1-shot & 5-shot \\
    \midrule
    \multirow{2}{*}{\shortstack{\textbf{CIFAR-FS}\\\textbf{5-way}}} & \cellcolor{gray!40}MAML$^{\dag}$ &\cellcolor{gray!40} 34.18 $\pm$ 0.75 &\cellcolor{gray!40} 41.20 $\pm$ 0.71 \\
    &Ours &\textbf{38.66 $\pm$ 0.78} & \textbf{51.95 $\pm$ 0.79}\\
    \midrule
    \midrule
    \multirow{2}{*}{\shortstack{\textbf{CIFAR-FS}\\\textbf{10-way}}} &\cellcolor{gray!40}MAML$^{\dag}$ &\cellcolor{gray!40} 19.05 $\pm$ 0.38 &\cellcolor{gray!40} 21.04 $\pm$ 0.35 \\
    &Ours &\textbf{22.88 $\pm$ 0.41} & \textbf{36.19 $\pm$ 0.45}\\
     \bottomrule
  \end{tabular}}
  \vspace{-0.38cm}
\end{table}







\noindent
\textbf{Effect of the number of training class for each task.} To evaluate the performance difference for the different number of training class for each task, we perform the experiments for 10-way classification problem in SS scenario. \cref{tab:dfmeta10way} shows the results. For CIFAR-FS, our method outperforms the best baseline by $9.48\%$ and $21.02\%$ for 1-shot and 5-shot learning, respectively. For MiniImageNet, our method outperforms the best baseline by $5.33\%$ and $12.98\%$ for 1-shot and 5-shot learning, respectively. Compared to 5-way classification, the accuracy of 10-way classification is lower because it is more challenging. Ours outperforms all baselines in both 5-way and 10-way classification problems.



\noindent
\textbf{Comparison to MAML.}\ 
To evaluate the effectiveness of DFML, we compare our method with regular data-based meta-learning work MAML\cite{finn2017model} under the scenario of SS. The results are shown in \cref{tab:maml}. We introduce MAML$^{\dag}$, which uses the real meta training dataset of equal size to the dynamic dataset used in DFML.  For 5-way classification on CIFAR-FS, our method outperforms MAML$^{\dag}$ by $4.48\%$ and $10.75\%$ for 1-shot and 5-shot learning, respectively. For 10-way classification on CIFAR-FS, our method outperforms MAML$^{\dag}$ by $3.83\%$ and $15.15\%$ for 1-shot and 5-shot learning, respectively. MAML$^{\dag}$ performs worse because  MAML$^{\dag}$ uses a fixed meta training dataset, which only provides a limited number of tasks in a random order. In contrast, PURER can provide a more diverse collection of tasks with an increasing level of difficulty because the dynamic dataset can be adaptively updated according to the real-time feedback of meta-learning model.







\noindent
\textbf{Contributions from curriculum mechanism.}\ 
\cref{fig:cur} shows the testing accuracy with and without curriculum mechanism, respectively. After 4000 iterations, we adopt curriculum mechanism to adaptively synthesize harder tasks. Without curriculum mechanism, the performance converges to a lower peak and even begins to decline, which can be explained as overfitting to the tasks of single-level difficulty. In contrast, with curriculum mechanism, the meta model can keep learning from harder tasks and finally achieves a peak with higher testing performance.







\noindent
\textbf{Visualizations of ECI.}\ 
\cref{fig:vis} shows visualizations of 5-way pseudo episodes synthesized by ECI from models pre-trained on CIFAR-FS with architectures of Conv4 and ResNet-18, respectively. We can observe that compared with Conv4, a deeper pre-trained model (ResNet-18) can synthesize images of better quality.
% \ls{discussions are also required.}

\begin{figure}[tbp]
  \centering
    \includegraphics[width=0.99\linewidth]{figs/efficiency.pdf}
      \vspace{-0.3cm}
   \caption{Effect of curriculum mechanism on testing performance on CIFAR-FS in SS scenario. 
   Solid curve: smoothed performance curve. Transparent curve: original performance curve.}
   \label{fig:cur}
   \vspace{-0.2cm}
\end{figure}

\begin{figure}[tbp]
  \centering
    \includegraphics[width=0.9\linewidth]{figs/vis.pdf}
     \vspace{-0.3cm}
   \caption{  (left) pseudo images synthesized from Conv4. (right) pseudo images synthesized from ResNet-18. Each column corresponds to one class.}
   \label{fig:vis}
   \vspace{-0.4cm}
\end{figure}

\noindent
\textbf{Hyperparameter sensitivity.}\ 
Results in \cref{app:moreresults} show that the performance of our method is stable with the changes of $\lambda$ value.
