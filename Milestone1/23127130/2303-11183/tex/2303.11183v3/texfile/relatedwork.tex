\section{Related Works}
\label{relatedWorks}

\noindent
\textbf{Meta-Learning.}
Meta-Learning \cite{schmidhuber1987evolutionary,naik1992meta,bengio2013optimization} aims to learn general prior knowledge from a large collection of tasks such that the prior can be adapted fast to new unseen tasks. Most existing works \cite{finn2017model, vinyals2016matching,wang2021meta,finn2019online,finn2018probabilistic,li2018learning,yao2021meta,wang2022meta,harrison2020continuous,hou2019cross,zhang2018metagan,zhang2019variational,li2019few,ye2020few,li2020boosting,yang2021free,simon2022meta,yin2019meta,liu2019learning,yoon2018bayesian,hsu2018unsupervised,khodak2019adaptive} assume that the data associated with each task is available during meta training. Recently, data-free meta-learning \cite{wang2022metalearning} problem has attracted researchers' attention. Wang \etal \cite{wang2022metalearning} propose to predict the meta initialization through a meta-trained black-box neural network. However, this method has several drawbacks. First, they focus on parameter space and ignore the underlying data knowledge that could be distilled from the pre-trained models. Second, their method can only be applied for small-scale pre-trained models since they use a neural network to predict the meta initialization. Furthermore, their application scenarios are only restricted to the case where all the pre-trained models have the same architecture, which reduces the applicable scenarios in real applications.  

\noindent
\textbf{Model Inversion.}
Model inversion (MI) \cite{fredrikson2015model,wu2016methodology,zhang2020secret} aims to reconstruct training data from a pre-trained model. Existing works \cite{fredrikson2015model,wu2016methodology,zhang2020secret,deng2021graph,lopes2017data,chawla2021data,zhu2021data,liu2021data,zhang2022fine,fang2021contrastive} synthesize images from pre-trained models via gradient descent. Those works solely synthesize pseudo images without using external feedback, which causes the generated data to be suboptimal or unuseful for meta training. In contrast, our method uses the feedback from meta-learning model to adaptively adjust the synthesized images in an end-to-end way.


\noindent
\textbf{Curriculum Learning.}
Curriculum learning (CL) \cite{bengio2009curriculum,asurvey,soviany2022curriculum} aims to train a model in a meaningful order during loading training data, from easy to hard. 
Kumar \etal \cite{kumar2010self} propose self-paced learning (SPL) to provide automatic curriculum with a loss-based difficulty measure. Recently, Zhang \etal \cite{zhang2021curriculum} propose a curriculum-based meta-learning method by forming harder tasks from each cluster, which merely applies to data-based meta-learning because it needs K-means clustering \cite{lloyd1982least} before training. While in this work, we adaptively synthesize the curriculum on the fly under data-free setting to improve the efficiency of pseudo episode training. 