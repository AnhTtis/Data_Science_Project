\section{Related Work}



In reinforcement learning there are two forms of generalization and overfitting.
Inter-task overfitting describes overfitting to a specific environment such that performance on slightly different environments drops significantly. This appears in the context of sim-to-real, where the simulation is different from the target environment on which a well performing policy is desired, or when the environment changes slightly, for example, because of a different visual appearance \cite{zhang2018study,packer2018assessing,zhang2018dissection,raileanu2020automatic,Song2020Observational}.
In contrast, intra-task overfitting appears in the context of learning from limited data in a fixed environment when the model fits the data too perfectly and generalizes poorly to new data.
We consider intra-task opposed to inter-task generalization. 


In model-based reinforcement learning, there is also the problem of policy overfitting on an inaccurate dynamics model \cite{arumugam2018mitigating,jiang2015dependence}. 
As a result, the policy optimizes over the inaccuracies of the model and finds exploits that do not work on the actual environment.
One approach is to use uncertainty estimates coming from an ensemble of dynamics models to be more conservative when the estimated uncertainty is high \cite{chua2018deep}. 
Another approach to prevent the policy from exploiting the model is to use different kinds of regularization on the plans the policy considers \cite{arumugam2018mitigating}.
In contrast to these previous works, we directly tackle the source of the problem by learning a better dynamics model. 
Consequently, our method is orthogonal to and can easily be combined with the just mentioned line of work.

Directly targeting the overfitting of the dynamics model can be done through the usage of a Bayesian dynamics model and the uncertainties that come with such a model. 
Gaussian processes have been used successfully in this context \cite{deisenroth2011pilco} although it is difficult to scale this to high-dimensional problems.
Another way to reduce overfitting of the dynamics model is to use techniques from supervised learning. 
This includes for example regularization of the weights, dropout \cite{dropout}, or data augmentation \cite{laskin2020reinforcement,schwarzer2021dataefficient}.
All of these are also orthogonal to our method and can be combined with it to learn an even better dynamics model.
Another popular approach is early stopping \cite{strand1974theory,anderssen1981formal,morgan1989generalization}, where the training is stopped before the training loss converges.
Our method can be regarded as the analogy of early stopping in a dynamic dataset scenario.

Reducing the number of model parameters can prevent overfitting but can decrease performance compared to the right amount of training steps with more parameters. Our method overcomes this problem by automatically choosing the right amount of training steps for a given network.

Hyperparameter optimization for RL algorithms is also related to our work.
For example, AlphaStar \cite{silver2018general} has been improved by using Bayesian optimization \cite{chen2018bayesian}. 
\citet{zhang2021importance} demonstrated that model-based RL algorithms can be greatly improved through automatic hyperparameter optimization.
A recent overview on automated RL is given by \citet{parker2022automated}.
However, most of these approaches improve hyperparameters by training the RL agent  on the environment in an inner loop while keeping the hyperparameters fixed during each run.
Our work deviates from that by adapting a hyperparameter online during training of a single run.
The approach of \citet{schaul2019adapting} also falls into this category and dynamically adapts behavior-related parameters such as stochasticity and optimism.
Similarly, the algorithm Agent57 \cite{badia2020agent57} adaptively chooses from a set of policies with different exploration strategies and achievs human level performance on all $57$ Atari games \cite{bellemare2013arcade}.
Another approach adapts a hyperparameter that controls under- and overestimation of the value function online resulting in a model-free RL algorithm with strong performance on continuous control tasks \cite{dorka2021adaptively}.

In contrast to these approaches, our method directly learns a better world model by detecting under- and overfitting online on a validation set and dynamically adjusts the number of update steps accordingly.
This renders the need to tune the UTD ratio hyperparameter unnecessary and further allows to automatically have its value being adapted to the needs of the different training stages.


