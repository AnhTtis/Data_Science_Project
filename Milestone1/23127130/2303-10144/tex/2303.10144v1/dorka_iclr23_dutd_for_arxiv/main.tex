
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{graphicx}
\usepackage{subfigure}

% \usepackage{pgfplots}
\usepackage{pgf}

% text wrap figures
\usepackage{wrapfig}

\usepackage{enumitem}



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% shortcuts
% \newcommand{\todo}[1]{\textbf{{\color{red} TODO:} #1}}
\newcommand{\etal}{{\em et al. }}

% math
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cD}{\mathcal{D}}

\newcommand{\cH}{\mathcal{H}} %entropy
\newcommand{\cL}{\mathcal{L}} %loss fct

\newcommand{\st}{s_t}
\newcommand{\at}{a_t}
\newcommand{\rt}{r_t}
\newcommand{\stone}{s_{t+1}}
\newcommand{\atone}{a_{t+1}}

\newcommand{\sa}{(s, a)}
\newcommand{\stat}{(\st, \at)}
\newcommand{\sttatt}{(\stone, \atone)}

\DeclareMathOperator*{\E}{\mathbb{E}} % expectation
\newcommand{\idc}[1]{\mathbbm{1}\left[#1\right]}  % indicator function

% \mathbb{1}
\newcommand{\pluseq}{\mathrel{+}\mathrel{\mkern-2mu}=}
\newcommand{\minuseq}{\mathrel{-}\mathrel{\mkern-2mu}=}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{\arg\,min}

\usepackage{booktabs}
\RequirePackage{algorithm}
\RequirePackage{algorithmic}

%  for comment on right side in Algorithm
% taken from
% https://tex.stackexchange.com/questions/133158/comment-in-algorithmic-on-a-line-beginning-with-if
\usepackage{array}
\usepackage{eqparbox}
\renewcommand\algorithmiccomment[1]{%
  \hfill\#\ \eqparbox{COMMENT}{#1}%
}
\usepackage{etoolbox}  % patch def of algorithmic environment
\makeatletter
\patchcmd{\algorithmic}{\addtolength{\ALC@tlm}{\leftmargin} }{\addtolength{\ALC@tlm}{\leftmargin}}{}{}
\makeatother







\title{Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


% \author{%
%   Anonymous Author(s) \\
%   Affiliation\\
%   Address\\
%   email\\
%   }
% \author{%
%   Nicolai Dorka\\ 
% %   Nicolai Dorka \& Tim Welschehold\\ 
%   University of Freiburg\\
%   \texttt{dorka@cs.uni-freiburg.de} \\
%      \And
%   Tim Welschehold \\
%   University of Freiburg\\
%   \And
%   Wolfram Burgard \\
%   Technical University of Nuremberg \\
% %   University of Technology Nuremberg  \\
% }

\author{
Nicolai Dorka\textsuperscript{1} ~~~~~~~ Tim Welschehold\textsuperscript{1} ~~~~~~~ Wolfram Burgard\textsuperscript{2}\\
~\textsuperscript{1}University of Freiburg ~~~~~ \textsuperscript{2}University of Technology Nuremberg \\
~\texttt{dorka@cs.uni-freiburg.de}
}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}
Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving.
As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training.
We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari $100$k benchmark.
The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search which is not feasible for many applications.
Our method eliminates the need to set the UTD hyperparameter by hand and even leads to a higher robustness with regard to other learning-related hyperparameters further reducing the amount of necessary tuning.
\end{abstract}



\input{secs/introduction}
\input{secs/related_work}
\input{secs/method}
\input{secs/experiments}
\input{secs/discussion}



\subsubsection*{Acknowledgments}
This work was supported by the European Unionâ€™s Horizon 2020 Research and Innovation Program
under Grant 871449-OpenDR.

\bibliography{bibliography}
\bibliographystyle{iclr2023_conference}

\clearpage
\appendix
\input{secs/appendix}


\end{document}
