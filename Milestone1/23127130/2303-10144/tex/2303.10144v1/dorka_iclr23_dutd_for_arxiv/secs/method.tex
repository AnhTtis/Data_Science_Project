\vspace{-0.1cm}
\section{The DUTD Algorithm}
\vspace{-0.1cm}



In this section, we will first introduce the general setup, explain early stopping in the context of finding the right data fit and propose a new method that transfers this technique to the online learning setting.
Lastly, we explain how the method can be applied to DreamerV2.
\vspace{-0.1cm}
\subsection{Model-Based Reinforcement Learning}
\vspace{-0.1cm}
We use the classical RL framework \cite{introdrl2018} assuming a Markov decision process $(\cS, \cA, \cP, \cR)$. In this framework, the agent sequentially observes the current state $\st \in \cS$ in which it executes an action $\at \in \cA$, receives a scalar reward $r_t$ according to the reward function $\cR$,  and transitions to a next state $s_{t+1}$ generated by the unknown transition dynamics $\cP$. The goal is to learn a policy that selects actions in each state such that the total expected return $ \sum_{i=t}^{T} r_i$ is maximized.

Model-based RL approaches learn a world model $\hat{\cP}(\stone \mid \st, \at)$ -- also called dynamics model -- and a reward model $\hat{R}(\rt \mid \st)$ that attempt to reflect their real but unknown counterparts.
These models can then be used to learn a good policy by differentiating through the world model or by generating imaginary rollouts on which an RL algorithm can be trained. 
Alternatively,  the learned model can be used in a planning algorithm to select an action in the environment.








\subsection{Under- and Overfitting}
A well-known problem in supervised learning is that of overfitting, which typically corresponds to a  low error on the training data and a high error on test data not seen during training. 
Usually, this happens if the model fits the training data too perfectly.
In contrast to this, underfitting corresponds to the situation in which the model even poorly fits the training data and is characterized by both a high training and test error.
To measure the  performance of the model on unseen data, the available data is often split into a training and a validation set.
Generally, only the training set is used to train the model while the validation set is used to evaluate its performance on new data.

For iterative training methods -- like gradient descent based methods -- overfitting is often detected by observing the learning curves for training and validation error against the number of training steps.
A typical behavior is that in the beginning of the training both training and validation loss are decreasing. This is the region where the model is still underfitting. 
At some point, when the model starts overfitting the training data, only the training loss decreases further while the validation loss starts to increase.
The aforementioned early stopping method balances under- and overfitting by stopping the training once the validation loss starts to increase.

While in supervised learning one can easily select a well fit model by using the validation loss, in reinforcement learning one cannot apply this technique as the dataset is not fixed but dynamic and is constantly growing or changing.
Furthermore, the quality of the current policy influences the quality of the data collected in the future.
Even though learning a world model is in principle a supervised task, this
problem also occurs in the model-based RL framework.



\subsection{Dynamic Update-to-Data Ratio}

A typical hyperparameter in many RL algorithms is the update-to-data (UTD) ratio which specifies the number of update steps performed per environment step (i.e., per new data point).
This ratio can in principle be used to balance under- and overfitting as one can control it in a way that not too few or too many updates steps are done on the currently available data.
However, several problems arise while optimizing this parameter.
First, it is very costly to tune this parameter as it requires to run the complete RL training several times making it infeasible for many potential applications.
Second, the assumption that one fixed value is the optimal choice during the entire training duration does not necessarily hold. For example, if data from a newly explored region of the environment is added to the replay buffer it might be beneficial to increase the number of update steps.


To address these problems, we propose -- DUTD -- a new method  that dynamically adjusts the UTD ratio during training.
It is inspired by the early stopping criterion and targets at automatically balancing under- and overfitting online by adjusting the number of update steps.
As part of the method, we store some of the experience in a separate validation buffer not used for training.
Precisely, every $d$ environment steps we collect $s$ consecutive transitions from a few separate episodes dedicated to validation and every $k$ environment steps the world model is evaluated on the validation buffer, where $k$ should be much smaller than $d$.
As the world model learning task is supervised this is easily done by recording the loss of the world model on the given validation sequences.
The current validation loss is then compared to the validation loss of the previous evaluation.
If the loss has decreased, we assume the model is still in the underfitting regime and increase the UTD rate by a specified amount. 
If the loss has increased, we assume the model to be in an overfitting regime and hence reduce the UTD rate.
To allow for a finer resolution at the high-update side of the allowed interval we adjust the UTD rate in log-space, meaning it is increased or decreased by multiplying it with a value of $c$ or $1/c$ respectively, where $c$ is slightly larger than $1$.
The update formula at time step $t$ then becomes
\begin{align}
	utd\_ratio_{t} = utd\_ratio_{t-k} \cdot b; ~~~~~  
	b=
	\begin{cases}
		c, & \text{if }  \text{validation\_loss}_{t} < \text{validation\_loss}_{t-k}, \\
		\frac{1}{c}, & \text{if }  \text{validation\_loss}_{t} \geq \text{validation\_loss}_{t-k}.
	\end{cases}
\end{align}


DUTD is a general method that can be applied to any model-based RL algorithm that learns a world model in a supervised way.
The implementation can be either in terms of the UTD ratio or the data-to-update ratio which is its inverse and which we call \textbf{IUTD} (i.e., the number of environment steps per update step).
It is more convenient to use the UTD ratio if several updates are performed per environment step and the IUTD if an update step is only performed after some environment steps.
Methodologically, the two settings are the same as the two ratios describe the same quantity and are just the inverse of each other.

A high-level overview of DUTD is shown in Figure~\ref{fig:autd_method} and the pseudocode is described in Algorithm~\ref{alg:AUTD}, both explained in terms of the IUTD ratio as we will apply DUTD to the DreamerV2 algorithm \cite{hafner2021mastering} for which several update steps per environment step become computationally very costly. 
However, in both framework both scenarios can be addressed by letting the ratio be a fractional.


\begin{wrapfigure}{R}{0.56\textwidth}
	\vspace{-.8cm}
	\begin{minipage}{\linewidth}%{0.6\textwidth}
		\begin{algorithm}[H]
			\caption{DUTD (in terms of inverted UTD ratio)}
			\label{alg:AUTD}
			\begin{algorithmic}
				\STATE {\bfseries Input:} Initial \textit{inverted} UTD ratio \emph{iutd\_ratio}; number of steps after which additional validation data is collected $d$, number of validation transitions collected $s$, steps after which the \textit{iutd\_ratio} is updated $k$, iutd update increment $c$
				\FOR{$t=1$ {\bfseries to} total\_num\_of\_env\_steps}
				\STATE Act according to policy $\pi(a \mid s)$ and observe next state
				\IF{$t ~ \text{mod} ~ d == 0 $}
				\STATE Collect $s$ transitions and store experience in a separate validation buffer; increment $t = t + s$
				\ENDIF
				\IF{$t ~ \text{mod} ~ \mathit{iutd\_ratio} == 0 $}
				\STATE Perform one training step of the transition model
				\ENDIF
				\IF{$t ~ \text{mod} ~ k == 0 $}
				\STATE Compute model loss $L$ on validation dataset 
				\IF[Overfitting]{$L \geq L_{\mathit{previous}}$}% 
				\STATE $\mathit{iutd\_ratio} = \mathit{iutd\_ratio} \cdot c$   
				\ELSE[Underfitting]
				\STATE $\mathit{iutd\_ratio} = \mathit{iutd\_ratio} / c$   
				\ENDIF
				\STATE $ L_{\mathit{previous}} ~=~ L$
				\ENDIF
				\ENDFOR
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}
	\vspace{-.5cm}
\end{wrapfigure}


\vspace{-0.1cm}
\subsection{Applying DUTD to DreamerV2}
\vspace{-0.05cm}
We apply DUTD to DreamerV2 \cite{hafner2021mastering}, which is a model-based RL algorithm that builds on Dreamer \cite{Hafner2020Dream} which again builds on PlaNet \cite{hafner2019learning}.
DreamerV2 learns a world model through latent imagination. 
The policy is learned purely in the latent space of this world model through an actor-critic framework. 
It is trained on imaginary rollouts generated by the world model. 
The critic is regressed onto $\lambda$-targets \cite{schulman2015high,introdrl2018}
and the actor is trained by a combination of Reinforce \cite{williams1992simple}
and a dynamics backpropagation loss.
The world model learns an image encoder that maps the input to a categorical latent state on which a Recurrent State-Space Model \cite{hafner2019learning} learns the dynamics.
Three predictors for image, reward, and discount factor are learned on the latent state.
The total loss for the world model is a combination of losses for all three predictors and a Kullbackâ€“Leibler loss between the latents predicted by the dynamics and the latents from the encoder.

To apply DUTD we evaluate the image reconstruction loss on the validation set.
Other choices are also possible but we speculate that the image prediction is the most difficult and important part of the world model.
One could also use a combination of different losses but then one would potentially need a scaling factor for the different losses.
As we want to keep our method simple and prevent the need of hyperparameter tuning for our method, we employ the single image loss. 
The source code of our implementation is publicly available \footnote{\url{https://github.com/Nicolinho/dutd}}.


