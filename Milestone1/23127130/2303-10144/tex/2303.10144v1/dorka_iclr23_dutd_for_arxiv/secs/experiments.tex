\vspace{-0.15cm}
\section{Experiments}
\vspace{-0.15cm}


We evaluate DUTD applied to DreamerV2 on the Atari $100$k benchmark \cite{Kaiser2020Model} and the DeepMind Control Suite \cite{tassa2018deepmind}.
For each of the two benchmarks we use the respective hyperparameters provided by the authors in their original code base.
Accordingly, the baseline IUTD ratio is set to a value of $5$ for the control suite and $16$ for Atari which we also use as initial value for our method.
This means an update step is performed every $5$ and $16$ environment steps respectively.
For both benchmarks we set the increment value of DUTD to $c=1.3$ and the IUTD ratio is updated every $500$ steps which corresponds to the length of one episode in the control suite (with a frame-skip of $2$).
Every $100,000$ steps DUTD collects $3,000$ transitions of additional validation data.
We cap the IUTD ratio in the interval $[1,15]$ for the control suite and in $[1,32]$ for Atari.
This is in principle not necessary and we find that most of the time the boundaries, especially the upper one, is not reached. A boundary below $1$ would be possible by using fractions and doing several updates per environment step, but this would be computationally very expensive for DreamerV2. 
All other hyperparameters are reported in the Appendix. 
They were not extensively tuned and we observed that the performance of our method is robust with respect to the specific choices.
The environment steps in all reported plots also include the data collected for the validation set.

The Atari 100k benchmark \cite{Kaiser2020Model} includes $26$ games from the Arcade Learning Environment \cite{bellemare2013arcade} and the agent is only allowed $100,000$ steps of environment interaction per game, which are $400,000$ frames with a frame-skip of $4$ and corresponds to roughly two hours of real-time gameplay. 
The final performance per run is obtained by averaging the scores of $100$ rollouts with the final policy after training has ended. 
We compute the human normalized score of each run as $\frac{\text{agent score} - \text{random score}}{\text{human score} - \text{random score}}$.
The DeepMind Control Suite provides several environments for continuous control.
Agents receive pixel inputs and operate with a frame-skip of $2$ as in the original DreamerV2.
We trained for $2$ million frames on most environments and to save computation cost for $1$ million frames if standard DreamerV2 already achieves its asymptotic performance well before that mark.
The policy is evaluated every $10,000$ frames for $10$ episodes.
For both benchmarks, each algorithm is trained with $5$ different seeds on every environment. 



Our experiments are designed to demonstrate the following:
\begin{itemize}[leftmargin=2em]
	\item The UTD ratio can be automatically adjusted using our DUTD approach
	\item DUTD generally increases performance (up to $300$\% on Atari100k) by learning an improved world model compared to the default version of DreamerV2
	\item DUTD increases the robustness of the RL agent with regard to learning-related hyperparameters
	\item DUTD is competitive with the best UTD hyperparameter found by an extensive grid search
\end{itemize}



\begin{figure*}[t]
	\centering 
	\includegraphics[width=0.98\textwidth]{figs/atari_results.pdf}
	\caption{Aggregated metrics over $5$ random seeds on the $26$ games of Atari $100$k with $95$\% confidence intervals according to the method presented in \citet{agarwal2021deep}. The intervals are estimated by the percentile bootstrap with statified sampling. Higher mean, median, interquantile mean (IQM) and lower optimality gap are better.}
	\label{fig:atari_results}
\end{figure*}




\subsection{Performance of DUTD compared to Standard DreamerV2}

\begin{wrapfigure}{r}{0.37\textwidth}
	\vspace{-0.3cm}
	\centering 
	\includegraphics[width=\linewidth]{figs/aggregated_curves_logdutd.pdf}
	\vspace{-0.32cm}
	\caption{Sample efficiency curves aggregated from the results for ten environments of the DeepMind Control Suite for DreamerV2 with the default UTD ratio and when it is adjusted with DUTD. The IQM score at different training steps is plotted against the number of environment steps. Shaded regions denote pointwise $95$\% stratified bootstrap confidence intervals according to the method by  \citet{agarwal2021deep}.
	}
	\vspace{-0.1cm}
	\label{fig:dmc_results_dutd_default_dreamer}
\end{wrapfigure}


For Atari100k, Figure~\ref{fig:atari_results} shows results aggregated over the $26$ games with the method of \citet{agarwal2021deep}, where the interquantile mean (IQM) ignores the bottom and top $25$\%  of the runs across all games and computes the mean over the remaining. 
The optimality gap describes the amount by which a minimal value of human level performance is not reached.
In Figure~\ref{app:fig:atari100k_learning_curves_all_envs} we present the learning curves for each environment.
The results show that  DUTD achieves a drastically stronger performance  on all considered metrics compared to DreamerV2 with the fixed default IUTD ratio of $16$.
It increases the interquantile mean (IQM) score by roughly $300\%$ and 
outperforms the human baseline in terms of mean score without any data augmentation. 


Figure~\ref{fig:dmc_results_dutd_default_dreamer} shows the aggregated results for two million frames over ten environments of the Control Suite, which we list in the Appendix.
The curves per environment are presented in 
Figure~\ref{app:fig:dm_control_performance_all_envs}
of the Appendix further including results for ten more environments on which the algorithms run until one million frames.
Compared to the manually set default UTD ratio,  DUTD matches or improves the performance on every environment.
Overall, DUTD improves the performance significantly although its average IUTD rate over all games and checkpoints is  $5.84$ similar to the default rate of $5$ showing that DUTD better exploits the performed updates.



\subsection{Increased Robustness with DUTD}

\begin{figure*}[t]
	\centering 
	\includegraphics[width=\textwidth]{figs/dutd_log_lr1e-4_no_xlabel.pdf}
	\includegraphics[width=\textwidth]{figs/dutd_log_lr1e-3_no_title.pdf}
	\vspace{-0.6cm}
	\caption{Learning curves for five environments of the Control Suite for DUTD-DreamerV2 and standard DreamerV2 when non-default learning rates are used. The first row shows the results for a lower than default learning rate of $0.0001$ and the second row for a higher one of $0.001$.
		The default learning rate is $0.0003$ and its results are shown
		in Figure \ref{app:fig:dm_control_performance_all_envs}.
		The solid line represents the mean and the shaded region a pointwise standard deviation in each direction computed over $5$ runs.}
	\label{fig:robustness_lr}
	\vspace{-0.4cm}
\end{figure*}


As DUTD dynamically adjusts the UTD ratio which allows to modify the training process online, we formed the hypothesis that with DUTD the underlying RL algorithm is more robust to suboptimal learning hyperparameters.
Similar to supervised learning on a fixed dataset the optimal number of updates to tradeoff between under- and overfitting will be highly dependent on hyperparameters like the learning rate.
To investigate this, we evaluated DreamerV2 with and without our method for different learning rates of the dynamics model.
The standard learning rate on the control suite is $0.0003$.
Hence, we trained with both a higher learning rate of $0.001$ and a lower one of $0.0001$ on a subset of the environments.
The resulting learning curves are displayed in Figure~\ref{fig:robustness_lr}.
Compared to the default learning rate  the performance of DreamerV2 with the standard fixed IUTD ratio of $5$ is overall lower and decreases substantially for some of the environments for both non-default learning rates.
However, using DUTD the algorithm achieves considerably stronger results.
This shows that using DUTD the algorithm is more robust to the learning rate, which is an important property when the algorithm is applied in real world settings such as robotic manipulation tasks, since multiple hyperparameter sweeps are often infeasible in such scenarios.
The need for more robustness as offered by DUTD is demonstrated by the performance drop of DreamerV2 with a learning rate differing by a factor of $3$ and the fact that on Atari a different learning rate is used.




\subsection{Comparing DUTD with Extensive Hyperparameter Tuning}


\begin{figure*}[t]
	\centering 
	\includegraphics[width=0.98\textwidth]{figs/atari_different_iutds.pdf}
	\vspace{-0.4cm}
	\caption{Aggregated metrics over $5$ random seeds on the $26$ games of Atari $100$k, cf. Figure \ref{fig:atari_results} for the methodology. DUTD is compared to Dreamer with different choices for the IUTD rate.
	}
	\vspace{-0.5cm}
	\label{fig:atari_results_different_iutds}
\end{figure*}


\begin{wrapfigure}{R}{0.40\textwidth}
	\vspace{-0.4cm}
	\centering 
	\includegraphics[width=\linewidth]{figs/ablation_different_iutds_aggregated_curves_logdutd.pdf}
	\vspace{-0.5cm}
	\caption{Sample efficiency curves showing the IQM score aggregated from the results for ten environments of the DeepMind Control Suite for DreamerV2 with different choices for the IUTD ratio. Shaded regions denote pointwise $95$\% stratified bootstrap confidence intervals.
	}
	\label{fig:dmc_results_different_iutds}
\end{wrapfigure}



\begin{figure*}[ht]
	\centering 
	\includegraphics[width=\textwidth]{figs/utd_rate_selection_dutd_log.pdf}
	\vspace{-0.4cm}
	\caption{
		IUTD ratio against environment steps for DUTD and the standard DreamerV2 on five environments. For each environment the mean over $5$ runs is plotted as the solid line and the shaded region represents one pointwise standard deviation in each direction.}
	\vspace{-0.5cm}
	\label{fig:num_utd_updates}
\end{figure*}



In the previous sections, we showed that DUTD improves the performance of DreamerV2 with its default IUTD ratio significantly.
Now we want to investigate how well DUTD compares to the best hyperparameter value for IUTD that can be found through an extensive grid search on each benchmark. 
While for many applications such a search is not feasible we are interested in what can be expected of DUTD relative to what can be regarded as the highest achievable performance.

On the Atari $100$k benchmark we evaluate DreamerV2 with IUTD rates of $1, 2, 4, 7, 10$ and $16$ (the default value) and denote the algorithms with DreamerV2-IUTD\_1, DreamerV2-IUTD\_2, etc.
The aggregated results over all games and seeds in Figure \ref{fig:atari_results_different_iutds} show an increase in performance when the number of updates increases up to an IUTD rate of $2$. 
Increasing it further to $1$ leads to declining results.
Thus, there is a sweet spot and one can not simply set the IUTD rate very low and expect good results. 
Averaged over all runs and checkpoints the IUTD rate of DUTD is at $3.91$ which is in the region of the best performing hyperparameters of $2$ and $4$. 
This is also reflected by the fact that DUTD achieves similar performance to these two optimal choices.

We further evaluate DreamerV2 with IUTD ratios of $2$, $5$ (the default one), $10$, and $15$ on ten environments of the control suite.
An IUTD value below $2$ is not possible as a single run would take roughly two weeks to run on our hardware.
The aggregated sample efficiency curves in Figure \ref{fig:dmc_results_different_iutds} further support the hypothesis that DUTD is competitive with the results of an extensive grid search. 
Only an IUTD choice of $2$ gives slightly better sample efficiency but reaches a lower final performance. 
To further investigate the behaviour of DUTD we report the adjusted \textbf{inverted} UTD ratio over time for five environments in Figure~\ref{fig:num_utd_updates}, and for all environments in
Figure~\ref{app:fig:num_utd_updates} in the Appendix.
Interestingly, the behavior is similar for all the environments.
At the start of the training, the ratio is very low and then it quickly oscillates around a value of roughly $5$ for most environments and an even higher value for a few others.
On cheetah\_run and hopper\_hop, the IUTD oscillates around the default value of $5$ most of the time and still, DUTD reaches a higher performance than Dreamer as can be seen in the single environment plot in
Figure \ref{app:fig:dm_control_performance_all_envs}  of the Appendix. 
This result supports the hypothesis that a static IUTD rate can be suboptimal for some environments and that DUTD successfully balances over- and underfitting during the training process.


\subsection{Evaluation for a High Number of Samples}

Next we investigate the behaviour of DUTD if training is continued for many environment steps. We randomly selected $5$ games from the Atari benchmark and trained the algorithms for $40$ million frames.
The resulting learning curves displayed in Figure \ref{fig:atari_long} show that DUTD maintains its advantage also in this setting.
The significantly improved performance is achieved with an IUTD ratio of $13.51$ averaged over all games and checkpoints. In Figure \ref{app:fig:num_utd_updates_atari_long} of the Appendix we show the development of the IUTD ratio over time for each environment. We can see that with DUTD after an initial phase with a lower IUTD ratio it oscillates around a value not too far from the highly tuned default ratio of $16$. This means DUTD significantly improves performance over plain DreamerV2 without requiring substantially more updates.
The experiment further highlights the benefits of DUTD. Evaluating different choices for a fixed IUTD ratio in this setting is highly expensive and for low values of the IUTD ratio almost impossible as a single run with the default value takes already several days to train. DUTD improves upon the highly tuned default choice and removes the need to tune this hyperparameter in an inner loop.

\begin{figure*}[t]
	\centering 
	\includegraphics[width=\textwidth]{figs/atari_long.pdf}
	\vspace{-0.5cm}
	\caption{Learning curves for DreamerV2 with and without DUTD on $5$ randomly selected environments of the Atari benchmark. For each environment the mean over $3$ runs is plotted as the solid line and the shaded region represents one pointwise standard deviation in each direction.}
	\label{fig:atari_long}
	\vspace{-0.5cm}
\end{figure*}


\subsection{Generality of DUTD}
To demonstrate the generality of DUTD we applied it to PlaNet \cite{hafner2019learning} which is another model-based RL algorithm. 
We evaluated the resulting method on three environments of the DeepMind Control Suite using the same hyperparameters for DUTD as for Dreamer.
The results in Figure \ref{app:fig:planet_experiment} of the appendix show that DUTD also improves the performance of PlaNet validating that DUTD is a general method and indicating its usefulness for different base algorithms.


