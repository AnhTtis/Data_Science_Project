\section{Discussion}

We presented a novel and general method denoted as DUTD that is designed to detect under- and overfitting on evolving datasets and is able to dynamically adjust the typically hand-set UTD ratio in an automated fashion.
As in early stopping, the underlying rationale is that too many updates can lead to overfitting while too few updates can lead to underfitting.
DUTD quickly identifies such trends by tracking the development of the world model performance on a validation set.
It then accordingly increases or decreases the UTD ratio in the case of underfitting or overfitting.

In our experiments, we demonstrated how to successfully apply DUTD to a model-based RL algorithm like DreamerV2.
The experiments show that DUTD can automatically balance between the under- and overfitting of the world model by adjusting the UTD ratio.
As a result, DUTD removes the burden of manually setting the UTD ratio, which otherwise needs to be tuned for new environments making it prohibitively expensive to apply such algorithms in many domains.
At the same time, DUTD increases the performance of DreamerV2 significantly compared to its default UTD rate and is competitive with the best hyperparameter found for each domain through an extensive hyperparameter search.
Moreover, a notable property of DUTD-DreamerV2 is its robustness to changes in the learning rate.
This is important, as the learning rate often has to be tuned for new environments.
For example, in DreamerV2 the default learning rate differs between Atari and the DeepMind Control Suite.
In the context of real world problems such tuning is undesirable and often too costly.
At the same time, the hyperparameters of DUTD can easily be set and do not have a big influence on the final performance. 
We recommend updating the UTD rate after a fixed time interval that is similar to the average episode length.
The data used for validation should not exceed $10$\% of all data. 

An interesting avenue for future work would be to explore non-supervised objectives for model-free RL algorithms that can be used for evaluation on the validation set.
This would allow the usage of DUTD to adjust the UTD ratio of such algorithms.
Another potential way to further boost the performance of our method is to use k-fold cross-validation with an ensemble of world models such that every transition can be used for training.

We are convinced that DUTD is a further step in the direction of autonomy and the easy applicability of RL algorithms to new real world problems without the need to tune any hyperparameters in an inner loop.
More generally, our work shows that it might be fruitful to use knowledge about the underlying learning dynamics to design algorithms that dynamically adjust parts of the learning algorithm.
