



\section{Further Results}
\subsection{Applying DUTD to PlaNet}
To demonstrate the generality of DUTD we additionally applied it to PlaNet \cite{hafner2019learning} with the same hyperparameters for DUTD as we also used for DreamerV2. As base source code on which we implemented DUTD we used \cite{pineda2021MBRL}. 
We evaluated the resulting algorithm on three environments of the DeepMind Control Suite that were also used in the original publication of PlaNet. We used $5$ seeds and evaluated the algorithms every $25000$ environment frames. 
The results in Figure \ref{app:fig:planet_experiment} show that DUTD also improves the performance of PlaNet. This is further evidence for the generality of DUTD.

\begin{figure*}[th]
	\vspace{-0.2cm}
	\centering 
	\includegraphics[width=0.8\textwidth]{figs/planet_experiment_dutd.pdf}
	\vspace{-0.4cm}
	\caption{Learning curves for PlaNet with and without DUTD on three environments of the DeepMind Cotrol Suite. The solid line is the mean over $5$ seeds and the shaded area represents one pointwise standard deviation. We used a uniform filter of size $3$.}
	\label{app:fig:planet_experiment}
	\vspace{-0.2cm}
\end{figure*}




\subsection{Detailed Results for Applying DUTD to DreamerV2}
The ten environments of the DeepMind Control Suite used to generate the aggregated curves in the Figures \ref{fig:dmc_results_dutd_default_dreamer} and \ref{fig:dmc_results_different_iutds} are:
acrobot\_swingup, cheetah\_run, finger\_turn\_easy, finger\_turn\_hard, hopper\_hop, quadruped\_run, quadruped\_walk, reacher\_hard, walker\_walk, and walker\_run.

We evaluated on all $20$ environments used in the original Dreamer paper \cite{Hafner2020Dream} but to save computation stopped training for ten environments at 1 million steps because standard Dreamer already reaches its asymptotic performance well before that mark. The aggregated curves are generated from the other $10$ environments for which training ran until $2$ million steps. 
Figure \ref{app:fig:dm_control_performance_all_envs} shows the single learning curves for all environments.
Please note, that on the $1$ million steps environments with DUTD the asymptotic performance is reached much faster - often twice as fast.  

In the Figures \ref{app:fig:performance_different_utds_each_env}, \ref{app:fig:atari100k_learning_curves_all_envs}, \ref{app:fig:dm_control_performance_all_envs},
\ref{app:fig:num_utd_updates}, and \ref{app:fig:atari100k_learning_curves_all_envs_all_iutds}
we present the more detailed results of our experiments for each single environment.



\begin{figure*}[th]
	\vspace{-0.3cm}
	\centering 
	\includegraphics[width=\textwidth]{figs/ablation_different_utds_all_envs.pdf}
	\vspace{-0.5cm}
	\caption{Learning curves for different choices of the IUTD ratio for each of the environments. The solid line is the mean over $5$ seeds and the shaded area represents one pointwise standard deviation.}
	\label{app:fig:performance_different_utds_each_env}
	\vspace{-0.9cm}
\end{figure*}


\begin{figure*}[th]
	\centering 
	\includegraphics[width=\textwidth]{figs/logdutd_atari100k_single_runs_one_plot.pdf}
	\caption{Learning curves for DreamerV2 with and without DUTD on the $26$ environments of the Atari $100$k benchmark. The solid line is the mean over $5$ seeds and the shaded area represents one pointwise standard deviation.}
	\label{app:fig:atari100k_learning_curves_all_envs}
\end{figure*}




\begin{figure*}[th]
	\centering 
	\includegraphics[width=\textwidth]{figs/dutd_log_dm_control_performance_20_envs.pdf}
	\caption{Learning curves for DreamerV2 with and without DUTD for $20$ environments of the DeepMind Control Suite. The solid line is the mean over $5$ seeds and the shaded area represents one pointwise standard deviation.}
	\label{app:fig:dm_control_performance_all_envs}
\end{figure*}


\begin{figure*}[ht]
	\centering 
	\includegraphics[width=\textwidth]{figs/utd_rate_all_envs_dutd_log.pdf}
	\caption{
		IUTD ratio against environment steps for DUTD and the standard DreamerV2 on all environments. For each environment the mean over $5$ runs is plotted as the solid line and the shaded region shows represents one pointwise standard deviation in each direction.}
	\label{app:fig:num_utd_updates}
\end{figure*}

\begin{figure*}
	\centering 
	\includegraphics[width=\textwidth]{figs/atari_long_utd_ratio.pdf}
	\caption{
		IUTD ratio against environment steps for DUTD and the standard DreamerV2 on $5$ environments of Atari for which the algorithms were trained until $40$ million frames. For each environment the mean over $3$ runs is plotted as the solid line and the shaded region shows represents one pointwise standard deviation in each direction.}
	\label{app:fig:num_utd_updates_atari_long}
\end{figure*}

\begin{figure*}[th]
	\centering 
	\includegraphics[width=\textwidth]{figs/single_runs_atari100k_all_IUTDs.pdf}
	\caption{Learning curves for different choices of the IUTD ratio for each of the $26$ environments of the Atari $100$k benchmark. The solid line is the mean over $5$ seeds and the shaded area represents one pointwise standard deviation.}
	\label{app:fig:atari100k_learning_curves_all_envs_all_iutds}
\end{figure*}





\clearpage

\section{Hyperparameters}
In Table~\ref{app:tab:hyperparameter} we give an overview of all hyperparameters related to DUTD.
All other hyperparameters are the standard DreamerV2 hyperparameters as given in the open source codebase \footnote{\url{https://github.com/danijar/dreamerv2}}.
On the DM Control Suite we reduced the number of steps $d$ after which to collect new data for the validation set by a half during the first $400$k steps as for some environments a strong policy is learned very quickly and hence a validation set with more recent transitions that better represent the kind of transitions the agent encounter makes more sense. 
We have because we started our first experiments with this but from some limited additional experiments it seems not to have a big impact on performance.


\begin{table}[h]
	\caption{Hyperparameters values for DUTD applied to DreamerV2 and the corresponding hyperparameter in the original DreamerV2.}
	\label{app:tab:hyperparameter}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lcc}
					\toprule
					Hyperparameter & Atari & DM Control \\
					\midrule
					Initial IUTD ratio & 16 & 5 \\
					\vspace{0.5cm}
					Lower boundary for the IUTD ratio & 1 & 1 \\
					Upper boundary for the IUTD ratio & 32 & 15 \\
					IUTD update increment | $c$ & 1.3 & 1.3 \\
					Number of steps after which to update the IUTD ratio | $k$ & 500 & 500 \\
					Validation set maximum size | $k$ & 12,000 & 10,000 \\
					Number of steps after which to collect new data for  & & \\
					the validation set | $d$ & 100,000 & 100,000 \\
					Number of additional transitions for the   &  &  \\
					validation set each time new validation data is collected | $s$ & 3,000 & 3,000 \\
					\midrule
					\midrule
					& \multicolumn{2}{c}{Standard DreamerV2} \\
					\midrule
					IUTD ratio & 16 & 5 \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}






\section{Hyperparameter Sensitivity of DUTD}



Most hyperparameters of our method are straightforward to set and do not need any tuning. Updating the UTD ratio after the maximum episode length of $500$ in DM Control Suite (DMC) is a value that we directly transferred to the Atari benchmark without further tuning. The initial value for the UTD ratio has no effect, as it gets quickly adjusted. The lower and upper limits for the UTD ratio are not reached often and hence do not affect performance given they are chosen lavish enough.  We did not tune those. We tried a few choices for the number of additional transitions each time new validation data is collected and the number of steps after which we do so but did not find it to affect performance a lot and fixed one choice for both benchmarks.

The multiplicative factor \textit{c} is the most important hyperparameter of our method and we hence conducted an additional experiment evaluating its sensitivity on the Atari100k benchmark over $5$ random seeds. We show the aggregated metrics for different multiplicative factors in Figure \ref{app:fig:atari100k_hyperparameter_sensitivity_dutd}.

The results show that seen over all metrics and relative to the baseline results the performance is not very sensitive with respect to the choice of the multiplicative factor. For the mean our default factor of $1.3$ even gives slightly worse results than all other factors.
Further, we argue the fact that the same setting of hyperparameters of DUTD works for very different benchmarks, Atari and DMC, shows that DUTD is not very sensitive to its hyperparameters and that the default values given by us will most likely work for a wide range of tasks.
While an extensive hyperparameter search for the optimal UTD ratio might give slightly better results than DUTD with some fixed multiplicative factor, DUTD is still favorable for many real world applications where such tuning is too costly.


\begin{figure*}[t]
	\centering 
	\includegraphics[width=\textwidth]{figs/hyper_sensitivity.pdf}
	\caption{Aggregated metrics over $5$ random seeds on the $26$ games of Atari $100$k, cf. Figure \ref{fig:atari_results} for the methodology. We investigate the sensitivity of DUTD to its own most important hyperparameter \textit{c} for values of $1.1$, $1.2$, $1.3$ (default one used in the main experiments), $1.4$, and $1.5$ .}
	\label{app:fig:atari100k_hyperparameter_sensitivity_dutd}
\end{figure*}





