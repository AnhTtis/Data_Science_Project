\section{Introduction}
	\begin{wrapfigure}{R}{0.41\textwidth}
		\centering 
		\includegraphics[width=0.41\textwidth]{figs/dutd.pdf}
		\caption{
			Overview of DUTD. A small subset of the experience collected from the environment is stored in a validation set not used for training. The world model is trained for one update after every $\frac{1}{\emph{UTD ratio}}$ many environment steps. From time to time, \textit{e.g.}, after an episode ended, the \emph{UTD ratio} is adjusted depending on the detection of  under- or overfitting of the world model on the validation data. The policy is obtained from the world model either by planning or learning and collects new data in the environment.}
		\label{fig:autd_method}
\end{wrapfigure}




In model-based reinforcement learning (RL) the agent learns a predictive world model to derive the policy for the given task through interaction with its environment. Previous work has shown that model-based  approaches can achieve equal or even better results than their model-free counterparts~\cite{silver2018general,schrittwieser2020mastering,chua2018deep,hafner2021mastering}.
An additional advantage of using a world model is, that once it has been learned for one task, it can directly or after some finetuning be used for different tasks in the same environment potentially making the training of multiple skills for the agent considerably cheaper.
Learning a world model is in principle a supervised learning problem. However, in contrast to the standard supervised learning setting, in model-based RL the dataset is not fixed and given at the beginning of training but is gathered over time through the interaction with the environment which raises additional challenges.

A typical problem in supervised learning is overfitting on a limited amount of data.
This is well studied and besides several kinds of regularizations a common solution is to use a validation set that is not used for training but for continual evaluation of the trained model during training.
By considering the learning curve on the validation set it is easy to detect if the model is under- or overfitting the training data.
For neural networks a typical behavior is that too few updates lead to underfitting while too many updates lead to overfitting.
In this context, the validation loss is a great tool to balance those two and to achieve a small error on unseen data.

For learning a world model on a dynamic dataset there unfortunately is no established method to determine if the model is under- or overfitting the training data available at the given point in time.
Additionally, in model-based RL a poorly fit model can have a dramatic effect onto the learning result as from it the agent derives the policy, which influences the future collected experience which again influences the learning of the world model.
So far, in model-based RL this is commonly addressed with some form of regularization and by setting an update-to-data (UTD) ratio that specifies how many update steps the model does per newly collected experience, similar to selecting the total number of parameter updates in supervised learning.
Analogously to supervised learning, a higher UTD ratio is more prone to overfit the data and a lower one to underfit it.
State-of-the-art methods set the UTD ratio at the beginning of the training and do not base the selection on a dynamic performance metric.
Unfortunately, tuning this parameter is very costly as the complete training process has to be traversed several times.
Furthermore, a fixed UTD ratio is often sub-optimal because different values for this parameter might be preferable at different stages of the training process.

In this paper, we propose a general method -- called \textbf{D}ynamic \textbf{U}pdate-\textbf{t}o-\textbf{D}ata ratio (\textbf{DUTD}) -- that adjusts the UTD ratio during training. 
DUTD is inspired by using early stopping to balance under- and overfitting.
It stores a small portion of the collected experience in a separate validation buffer not used for training but instead used to track the development of the world models accuracy in order to detect under- and overfitting.
Based on this, we then dynamically adjust the UTD ratio.

We evaluate DUTD applied to DreamerV2 \cite{hafner2021mastering} on the DeepMind Control Suite and the Atari100k benchmark.
The results show that DUTD increases the overall performance relative to the default DreamerV2 configuration.
Most importantly, DUTD makes searching for the best UTD rate obsolete and is competitive with the best value found through extensive hyperparameter tuning of DreamerV2.
Further, our experiments show that with DUTD the world model becomes considerably more robust with respect to the choice of the learning rate.


In summary, this paper makes the following contributions: 
i) we introduce a method to detect under- and overfitting of the world model online by evaluating it on hold-out data; 
ii) We use this information to dynamically adjust the UTD ratio to optimize world model performance;
iii) Our method makes tuning the UTD hyperparameter obsolete; 
iv) We  exemplarily apply our method to a state-of-the-art model-based RL method and show that it leads to an improved overall performance and higher robustness compared to its default setting and reaches a competitive performance to an extensive hyperparameter search.


