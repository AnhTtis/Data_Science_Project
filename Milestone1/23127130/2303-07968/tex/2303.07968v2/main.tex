\documentclass[prd,twocolumn,showpacs,preprintnumbers,superscriptaddress,floatfix,nofootinbib]{revtex4-1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx,,booktabs}
\usepackage{amssymb}
\usepackage{amsmath,txfonts,ulem}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{color}
\usepackage{bm}
\usepackage{ulem}
\usepackage{makecell}
% \usepackage{subcaption}
\usepackage[colorlinks,
            citecolor=blue,
            anchorcolor=green,
            menucolor=orange,
            linkcolor=red,
            filecolor=red,
            runcolor=pink,
            urlcolor=blue,
            frenchlinks=red]{hyperref}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=Latex.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{LastRevised=Wednesday, July 15, 2020 22:19:32}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\allowdisplaybreaks[4]


\begin{document}


\title{Determination of the distribution of strong coupling constant with machine learning}
    \author{Xiao-Yun Wang}
\email{xywang@lut.edu.cn}
\affiliation{Department of physics, Lanzhou University of Technology,
Lanzhou 730050, China}
\affiliation{Lanzhou Center for Theoretical Physics, Key Laboratory of Theoretical Physics of Gansu Province, Lanzhou University, Lanzhou, Gansu 730000, China}

\author{Chen Dong}
\email{dongphysics@yeah.net}
\affiliation{Department of physics, Lanzhou University of Technology,
Lanzhou 730050, China}

\author{Quanjin Wang}
\affiliation{Department of physics, Lanzhou University of Technology,
Lanzhou 730050, China}
	
	
	
	



\begin{abstract}
In this work, we use the artificial neural network (ANN) method to study and predict the distribution of strong coupling constants by fitting the existing data. Our approach takes advantage of the ability of ANN to learn complex nonlinear relations and excellent generalization,  and allows for a systematic treatment of the uncertainties associated with the data. To ensure the reliability of our results,
we apply three evaluation indexes to evaluate the accuracy of model during training.
Finally, we obtained the predicted values of the strong coupling constants at different energy scales, and compared and verified them with the existing experimental data. Our approach represents a promising way to improve the determination of the strong coupling constant at low energies, and could have important implications for future experimental and theoretical studies in quantum chromodynamics.




\end{abstract}

%%\pacs{14.60.Pq, 11.30.Er, 12.15.Ff, 14.60.Lm}

\maketitle

\section{Introduction}
In recent years, with the continuous development of deep learning \cite{Abadi:2016kic,AlecRadford,Donahue,Jrgen} has been gradually applied to the research of high-energy particle physics with its advantages of scalability, excellent nonlinear modelling ability, adaptability, robustness and interpretability. In order to detect the signals of rare particles, the problem of not capturing all available signals and background classification in high-energy physics experiments is solved \cite{Baldi:2014kfa}. The researchers overcame these shortcomings using deep learning and achieved remarkable results. In addition, in the project of the Ring Imaging Cherenkov detectors (RICH) detector experiment \cite{Blago:2023oeo}, the Convolutional Neural Network (CNN) method is introduced to improve the particle recognition technology, which will provide the accuracy of simulated collision data for the LHC operation in $2022$.
A new scheme \cite{Thomadakis:2023ebe} is proposed using machine learning to reconstruct orbit parameters from the CLAS12 probe with a different algorithm. And employing neural networks to reconstruct particles can recognize physical reactions in real-time.
Of course, machine learning can also be employed to investigate the physics property of nucleons.  In Ref \cite{Dutrieux:2021nlz}, the authors described in detail the use of artificial neural networks for the global fitting of deeply virtual Compton scattering data. Moreover, the D-term, which is the most important for comprehending the QCD momentum energy tensor, is effectively extracted from this scheme to analyze the distribution of pressure and shear force \cite{Polyakov:2018zvc} inside the proton. Recently, an interesting work has been developed based on machine learning \cite{Zhang:2022uqk}, which uses physical constraints to successfully rediscover the Gell-Mann–Okubo formula using the symbolic regression technique of the genetic algorithm. These works strongly inspires us to study numerous uncertain physical quantities in QCD theory with machine learning.

Quantum chromodynamics (QCD) is a fundamental theory describing the strong interaction between quarks and gluons \cite{Yu:2021yvw}. In QCD theory, the strong coupling constant $\alpha_s(Q)$ is essential to represent the strong interaction, characterising the strength of the previous strong interaction between quarks and gluons. At high energy $Q$, the accurate $\alpha_s(Q)$ are required to study the growing accurateness of hadron scattering experiments and to test high-energy models that unify the strong and weak forces \cite{Deur:2016tte}. At low energy $Q$, one can be employed to explore and understand the internal structure of hadrons \cite{Wang:2022zwz}, quark confinement, hadronic processes, and vector meson photoproduction models \cite{Zeng:2020coc}. In addition, $\alpha_s(Q)$ also play a vital part in proton mass-scale research \cite{Dong:2022ids, Wang:2022tzw}.
However, much theoretical and experimental research has been done on strong coupling constants, but the value for partial energies still needs to be determined.

In the high-energy scale, the $\alpha_s(Q)$ becomes smaller due to the asymptotically free property \cite{Gross:1973id}. This means that the interaction between quarks and gluons is weakened and can be studied using the renormalization group equation. Moreover, $\alpha_s(Q)$ in the high-energy scale can also be determined through high-energy physics experiments \cite{Begel:2022kwp, Bethke:2022cfc}, such as LHeC, HERA, CMS, JADE and H1.
However, problems arise when the research is at the low-energy scale. The QCD interaction becomes very strong and cannot be effectively calculated by perturbation theory, complicating the theoretical calculation. The $\alpha_s(Q)$ at low-energy scale can be studied experimentally \cite{Deur:2008rf,Deur:2022msf,Deur:2016tte,Deur:2005cf,Deur:2008rf,Deur:2021klh,Kim:1998kia,HERMES:1997hjr,HERMES:1998pau,HERMES:1998cbu,HERMES:2002mes,HERMES:2006jyl}
and relevant results have been obtained theoretically through Lattice QCD \cite{Komijani:2020kst,Maezawa:2016vgv}, Holographic QCD \cite{Brodsky:2006uqa,deTeramond:2008ht,deTeramond:2013it}, effective field theory \cite{Harada} and light-front theory \cite{Yu:2021yvw}. However, the uncertainty of the result still exists as a problem at the low-energy scale.

In order to determine the value of strong coupling constant $\alpha_s(Q)$ under different energies, the Recurrent Neural Network (RNN) algorithm in Artificial Neural Network (ANN) is adopted in this work to analyze $\alpha_s(Q)$ experimental data accumulated in laboratories around the world.  The mean square error (MSE) is employed as the loss function to evaluate the model training results. Three evaluation indexes are selected to assess the quality of the model training, which are Expected Variance (EV), $R2$ score and Mean Absolute Error (MAE). The strong coupling constant behavior in the $Q$ range of a new input sequence is predicted after all indexes have reached acceptable values.

The structure of this paper is organized as follows. The principle of the RNN neural is described in Sec. \ref{sec2}.
RNN network construction based on strong coupling constant, data processing and selection of model evaluation functions are shown in Sec. \ref{sec3}.
The prediction of alphas and evaluation results are presented in Sec. \ref{sec4}. A simple summary and discussion is given in Sec. \ref{sec5}.

\section{The principle of Recurrent neural network} \label{sec2}
Recurrent neural network (RNN) \cite{Hammer,Graves} is a branch of artificial neural network (ANN) and an important part of deep learning. RNN can be regarded as a class of neural networks with short-term memory ability. In RNN, neurons can not only receive information from other neurons,  but also receive information from themselves, forming a network structure with loops. Because RNN network can receive its neuron information, it has a more robust memory capacity. Therefore, RNN is very suitable for processing time series data, text data, etc.

The simple network diagram of RNN is shown in Fig. \ref{fig:Rnn}, which shows the basic link structure of RNN. For the hidden state $h_t$ at time $t$, it can be represented as,
\begin{equation}
    h_t = \sigma (W_{ih} x_t + b_{ih} +W_{hh}h_{t-1}+b_{hh}) \label{eq:1}
\end{equation}
where $h_t$ is the hidden state at time $t$, $x_t$ is the input quantity at time $t$, $h_{t-1}$ is the hidden state at time $t-1$, $W_{ih}$ is the weight of input to the hidden layer, $W_{hh}$ is the weight from hidden layer to hidden layer, $b_{ih}$ is the bias from output layer to hidden layer, $b_{hh}$ is the bias from hidden layer to hidden layer, and $\sigma$ is the activation function. Either $tanh$ or $relu$ is usually chosen for the choice of activation functions.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[scale=0.48]{RNN.eps}
		\caption{The simple principle of recurrent neural network.}\label{fig:Rnn}
	\end{center}	
\end{figure}

In practical applications, RNN has a certain memory ability for information when modeling sequence data. However, with the increase of recursion times, simple RNN will have the problem of exponential explosion or disappearance of weight, which makes it challenging to capture long-term correlation. It also leads to the difficulty of convergence in RNN training.
A special kind of RNN is introduced to solve this problem, that is, long short-term memory network (LSTM) \cite{Gers}.

Compared with simple RNN networks, LSTM can obtain better analysis results in longer sequences. It is easy to solve the problems of gradient disappearing and gradient explosion in the training process. The network construction principle of LSTM is shown in Fig. \ref{fig:LSTM}.
\begin{figure*}[htbp]
	\begin{center}
		\includegraphics[scale=0.48]{LSTM.eps}
		\caption{The simple principle of the long short-term memory network.}\label{fig:LSTM}
	\end{center}	
\end{figure*}

In the LSTM network, each LSTM cell performs the following function calculations for the input \cite{Msi},
\begin{equation}
\begin{aligned}
& i_t=\sigma\left(W_{i i} x_t+b_{i i}+W_{h i} h_{t-1}+b_{h i}\right) \\
& f_t=\sigma\left(W_{i f} x_t+b_{i f}+W_{h f} h_{t-1}+b_{h f}\right) \\
& g_t=\tanh \left(W_{i g} x_t+b_{i g}+W_{h g} h_{t-1}+b_{h g}\right) \\
& o_t=\sigma\left(W_{i o} x_t+b_{i 0}+W_{h o} h_{t-1}+b_{h o}\right) \\
& c_t=f_t \times c_{t-1}+i_t \times g_t \\
& h_t=o_t \times \tanh \left(c_t\right)
\end{aligned}
\end{equation}
where $i_t$, $f_t$, $g_t$ and $o_t$ respectively represent input gate, forget gate, selection gate and output gate. $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$. $h_{t-1}$ is the hidden state at time $t-1$. Here, set the hidden status to $0$ at the initial moment. In the transmission process of each cell, the status $c_{t-1}$ is usually transferred to the status $c_t$ after some parameters are attached, and its change speed is slow. In contrast, the value of $h_t$ changes significantly, often very different from that of other nodes.

Regarding information processing, LSTM is generally divided into three stages: forgetting, selective memory, and output.
\begin{enumerate}[label=\roman*.]
    \item  Forgetting stage : This stage mainly forgets the input transmitted by the previous node selectively. The value of $f_t$ controls the part that needs to be forgotten and remembered in $c_{t-1}$.
    \item Selective memory stage : This stage inputs $x_t$ for selective memory. Record important data. The input content of the current cell is the calculated $i_t$, which can be selectively output by $g_t$.
    \item Output stage : This phase determines what output is as the current state. The control is mainly through $o_t$, and $c_t$ is scaled using the $tanh$ activation function.
\end{enumerate}
%----------------------------------------------------------------
\section{Construction of LSTM network based on strongly coupled constant data} \label{sec3}
It is realized that the strong coupling constant $\alpha_s(Q)$ varies with the energy $Q$ from the experimental data accumulated by various accelerators in the world. This relationship is a kind of temporal data, where the time axis represents different energy $Q$.
Due to the unique properties of LSTM, it can capture the change rule of $\alpha_s(Q)$ over time (energy) in these time series data and predict the value of $\alpha_s(Q)$ under different $Q$. This ability makes LSTM a powerful tool for studying $\alpha_s(Q)$ variations in QCD theory.

To obtain reliable results, the quality and quantity of experimental data must be considered first. For low-energy scale, the $\alpha_s(Q)$ data at $Q\in(0.1, 4)$ GeV obtained by JLab \cite{Deur:2008rf,Deur:2022msf,Deur:2016tte,Deur:2005cf,Deur:2008rf,Deur:2021klh} and other laboratories \cite{Kim:1998kia,HERMES:1997hjr,HERMES:1998pau,HERMES:1998cbu,HERMES:2002mes,HERMES:2006jyl}.
For the high-energy scale, the data of $Q\in(6, 2000)$ GeV are derived from Ref. \cite{Begel:2022kwp}.
In addition, the value of $\alpha_s(M_\tau)$, $\alpha_s(2.85)$ and $\alpha_s(9.50)$ obtained by QCD spectral sum rules approach is also introduced \cite{Narison:2018dcr,Braaten:1991qm}.
Then, all the data is divided into the training set and the test set in a ratio of $7$:$3$, and the data sets are normalized. Data normalization aims to limit the pre-processed data to a certain range to eliminate the adverse effects caused by the singular sample data. At the same time, it can speed up the optimal solution of gradient descent and improve the accuracy. In addition, noise is added to existing data to achieve the data enhancement effect and improve the effectiveness of training results.

For the construction of the model, we employ the ``Keras'' deep learning framework to build the LSTM network. During the build, a hidden layer and an output layer are set up, and the activation functions are $tanh$ and $relu$, respectively.  Add a dropout layer between the two layers to randomly drop $30\%$ of the neurons, mitigating the overfitting of the network. In other words, in the process of forward propagation of the network, the activation value of a neuron stops working with a certain probability, which makes the model more generalized and ultimately makes the trained network more robust and does not over-rely on some local features.

The correct choice of optimizer can affect the convergence speed, generalization ability, and stability of the model. By analyzing the characteristics of $\alpha_s(Q)$ changing with energy $Q$, it is also necessary to avoid the problem of gradient disappearing or explosion. The ``Adam'' adaptive optimizer is a suitable choice, which can avoid optimal local problems in addition to its ability to satisfy the needs.

Up to now, we need to clarify a problem. The network established in this work belongs to the regression model, so the mean square error (MSE) is used as the loss function,
\begin{equation}
MSE=\frac{1}{n} \sum_{i=1}^{n}(y_i-\hat{y}_{i})^2.
\end{equation}
where $y$ is the predicted result, $\hat{\mathrm{y}}$ is the true value, and $n$ is the number of samples.
The function is convenient for intuitively understanding the results and has broad applicability and excellent mathematical properties. In addition, the evaluation of the LSTM model is an indispensable and essential part. This is because it can help to analyze the difference between the prediction of the model and the actual results, and determine the accuracy and reliability of the model prediction. Meanwhile, the value of evaluation indicators can enable us to identify the defects and limitations of the model to enhance and modify the model. Here, three evaluation indexes are employed to evaluate the model performance of the LSTM: expected variance ($EV$), R$2$ score, and mean absolute error ($MSE$):
\begin{enumerate}[label=\roman*.]
    \item  Expected Variance ($EV$) : An index used to describe the degree of data dispersion in a probability distribution, which can be expressed as
    \begin{equation}
    EV = 1 - \frac{Var(y - \hat{y})}{Var(y)} \in [0, 1] \label{eq:4},
    \end{equation}
    which measures the model's ability to explain the target variable. That is, the percentage of variance that the model can account for in the value of the target variable. The $Var$ in Eq. (\ref{eq:4}) represents variance.The criterion of expected variance is intuitive, and the result returned by it belongs to $[0, 1]$. The result is closer to $1$, the more accurate the prediction result of the model is.
    \item $R2$ score : Represents the degree to which the regression model analyzes the variability of the observed value. $R2$ fractions range from $0$ to $1$, which is similar to $EV$. The closer it is to $1$, the better the interpretation of the observation, and the closer it is to $0$, the worse the performance. When $R2=1$, it means that the model can fit the data perfectly. The calculation formula is,
    \begin{equation}
    R^2=1-\frac{\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2} \in[0,1],
    \end{equation}
    where $\bar{y}$ is the average of the true values.

    \item Mean Absolute Error ($MAE$) : It is an index of the average error between the predicted result and the true value in the regression model. The smaller the $MAE$ value is, the closer the predicted result of the model is to the true value. The calculation formula is as follows,
    \begin{equation}
       M A E=\frac{1}{n} \sum_{i=1}^n\left|y_i-\hat{y}_i\right| \quad \in[0,+\infty) .
    \end{equation}
\end{enumerate}
%----------------------------------------------------------------
\section{Result analysis and discussion}\label{sec4}
\begin{figure*}[t]
    \centering
    \subfigure{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.45]{Loss-C-pre.eps}
         %\caption*{}
        \end{minipage}%
        }
    \subfigure{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.45]{Loss-D-pre.eps}
        % \caption*{}
        \end{minipage}
        }	
	\caption{The values of the strong coupling constants predicted by the LSTM model, (a) is the Machine learning (result \uppercase\expandafter{\romannumeral1}), and (b) is the Machine learning (result \uppercase\expandafter{\romannumeral2}).
    Solid circles (E03006/E05111) and solid upper triangles (E03006/E97110) are obtained from Ref. \cite{Deur:2021klh}, the solid square, pentagram and inverted triangle are received from Ref. \cite{Deur:2005cf,Deur:2008rf}, the solid hexagon is the result of $\alpha_{F3}/\pi$ \cite{Kim:1998kia}, the solid diamond shape is given by Ref. \cite{HERMES:1997hjr,HERMES:1998pau,HERMES:1998cbu,HERMES:2002mes,HERMES:2006jyl} and the half-white, half-solid square is the result of the QCD spectral sum rules approach \cite{Narison:2018dcr,Braaten:1991qm}. The hollow diamond is the result from LHeC, the hollow hexagon is the data from H1, the hollow upper triangles and stars are the result derived from JADE experiment \cite{Bethke:2022cfc}, the hollow circle is the result of CMS and the hollow pentagon is the result of HERA. These experimental data at high-energy scale can be obtained from Ref. \cite{Begel:2022kwp,Bethke:2022cfc}.}
    \label{fig:pre}	
\end{figure*}
Considering the accumulated experimental data of strong coupling constants (low-energy \cite{Deur:2008rf,Deur:2022msf,Deur:2016tte,Deur:2005cf,Deur:2008rf,Deur:2021klh,Kim:1998kia,HERMES:1997hjr,HERMES:1998pau,HERMES:1998cbu,HERMES:2002mes,HERMES:2006jyl} and high-energy scale \cite{Begel:2022kwp,Bethke:2022cfc})
and a few data derived by the QCD spectral sum rules \cite{Narison:2018dcr,Braaten:1991qm}. Two hidden layers are added and the neuron units are set as $1100$ and $800$ respectively. The training epoch number is set to $300$. The trained LSTM network predicts a new set of continuous sequences $Q\in[0.10,2000.00]$ GeV, which is represent as
 result \uppercase\expandafter{\romannumeral1} (the blue line) shown in Fig. \ref{fig:pre} (a).
The loss function of training data and test data obtained in the training process is shown in Fig. \ref{fig:Loss} (a). Realize that the loss function is controlled within a small range and that the value of the loss function decreases as the epoch increases.It can be observed that the loss functions of the training data and the test data decrease rapidly within epoch$\in[0,50)$. After epoch$>50$, the loss function becomes flat and very close. More importantly, the loss function of test data has no sudden surge in the convergence process, which means no overfitting exists in the training process. In addition, the values of the evaluation indexes for the result \uppercase\expandafter{\romannumeral1} are $EV=0.95$, $R2$ score = $0.96$ and $MSE=0.05$.
These circumstances indicate that the performance of the LSTM network on the training data and the test data is similar and demonstrates that the model has an acceptable generalization ability. It is also indicated that the trained model is effective in predicting the value of strong coupling constant.

However, careful analysis reveals that there exists a trough at $Q=1\sim5$ GeV. We suspect that the error of the last three sets of $\alpha_{F3}/\pi$ data (solid hexagon) \cite{Kim:1998kia} and the last five sets of $\alpha_{g1}/\pi$ CLAS EG1b (2014) data \cite{Deur:2008rf} (solid inverted triangle) are tremendous, which may lead to the result \uppercase\expandafter{\romannumeral1}. If the error bar is considered, these values can become negative.
Therefore, we consider a correction to reduce the weight of the data mentioned above. Especially the weight of $\alpha_{F3}/\pi$ is reduced to zero, then retraining the LSTM network.


\begin{table}\small
    \renewcommand\arraystretch{2}
	\caption{\label{tab:table2} The value of $\alpha_s(M_Z)$ obtained from the experimental data and theoretical models compared with machine learning results, where $M_Z=91.2$ GeV.}
	\begin{ruledtabular}
			\begin{tabular}{cc}
				
				Model  & $\alpha_s(M_Z)$\\
    \hline
            Machine learning (result \uppercase\expandafter{\romannumeral1})     &       $0.110$          \\
            Machine learning (result \uppercase\expandafter{\romannumeral2})&             $0.108$    \\
            Lattice \cite{Schilling:1993fg}    &     $0.102^{+0.0029}_{-0.0100}$   \\
            CELLO data \cite{CELLO:1989okb}      &      $0.0965^{+0.002}_{-0.003}$ \\
            SLD experiment at SLAC \cite{SLD:1994idb}   & $0.1200\pm 0.0025$   \\
            Deep inelastic Scattering Data \cite{Blumlein:1994kw}    &  $0.108\pm0.002$\\
            Standard QCD (from $J/\psi$ width) \cite{Clavelli:1994mf}  & $0.1056\pm0.0013$ \\
            Standard QCD (from $\Upsilon$ width) \cite{Clavelli:1994mf} & $0.110\pm0.002$ \\
			Brodsky lepage mackenzie (BLM) \cite{Brodsky:1982gc} & $0.107 \pm 0.003$  \\
			Minimum subtraction scheme  \cite{hooft} & $0.118\pm0.006$
				
			\end{tabular}
   \end{ruledtabular}

\end{table}

\begin{figure}[h]
  \centering
  \subfigure{\includegraphics[scale=0.40]{Loss-C.eps}

  }
  \subfigure{
  \includegraphics[scale=0.40]{Loss-D.eps}

  }
  \caption{The loss function of training and test data, the (a) is the Machine learning (result \uppercase\expandafter{\romannumeral1}), and (b) is for the result \uppercase\expandafter{\romannumeral2}.}
  \label{fig:Loss}
\end{figure}


The units of neurons in the hidden layer and epochs remained unchanged. The retrained and predicted result \uppercase\expandafter{\romannumeral2} is shown in Fig. \ref{fig:pre} (b), which is represented by the red dot-dashed line.  Compared to result \uppercase\expandafter{\romannumeral1}, the revised forecast result \uppercase\expandafter{\romannumeral2} is more expressive, showing a smoother continuous descending behavior.
The loss function shown in Fig. \ref{fig:Loss} (b) indicates a reasonable convergence trend. Almost all the loss functions of test and training data converge gently after epoch $>30$. And the value is minimal.
 The the evaluation indexes of result \uppercase\expandafter{\romannumeral2} are also appropriate, $EV=0.93$, $R2=0.94$ and $MSE=0.005$. These evaluation functions show that the training results after modifying the weight of data are effective and that the model has a strong generalization ability.

The above consequences illustrate that the selection and processing of initial data affect the accuracy of training in network training. More importantly, it affects the behavior of the prediction of the new input sequence.
However, regardless of the result in \uppercase\expandafter{\romannumeral1} or \uppercase\expandafter{\romannumeral2}, the values of the loss function and expected variance characterize the accuracy of LSTM network training and the feasibility of prediction.
It's just that the prediction at result \uppercase\expandafter{\romannumeral2} can reveal the regularity of the strong coupling constant varying with the energy $Q$, which is a continuous decreasing behavior. In any case, both of the results shown in Fig. \ref{fig:pre} are in good agreement with the experimental data in the high-energy scale. This indicates that the LSTM network is relatively reliable and accurate for learning and training. For the low-energy scale, the behavior shown in Result $2$ can better display the variation regularity of the strong coupling constant under different energies.
And the difference between the two results is minimal within $1$ GeV. However, when the energy is $1$ GeV and extended to $10$ GeV, result \uppercase\expandafter{\romannumeral1} shows a trough, while result \uppercase\expandafter{\romannumeral2} can better display the regularity of the strong coupling constant under different energies.
A detailed result at $Q\in[1,10]$ GeV for both results is listed in Tab. \ref{tab:table1}.

The Z pole experiment is one of the critical experiments in particle physics, reflecting the process of conducting experiments at the Z boson mass ($91.2$ GeV). It is also achievable to measure the strong coupling constant $\alpha_s(M_Z)$ of the Z boson by total hadronic the $e^+e^-$ annihilation rate $R$ \cite{Chetyrkin:1996ela} and other experiments.  This is of great significance for studying the standard model and exploring the properties of elementary particles. Moreover, multiple models have been developed theoretically for $\alpha_s(M_Z)$ research. Tab. \ref{tab:table2} lists and compares the strong coupling constant of this work with the results of related experiments and theoretical calculations for the value of $\alpha_s(M_Z)$. It can be realized that slight differences exist between different theoretical and experimental $\alpha_s(M_Z)$ values, which are within the acceptable range. Fortunately, our results agree with the values obtained based on the deep inelastic scattering data \cite{Blumlein:1994kw} and the standard QCD \cite{Clavelli:1994mf}. It also indirectly reflects the feasibility and reliability of LSTM network learning QCD strong coupling constant behavior based on machine learning.

\begin{table}\small
\renewcommand\arraystretch{2}
	\caption{\label{tab:table1} The result of $\alpha_s(Q)$  predicted at different energies $Q$ for the two machine learning results.}
	\begin{ruledtabular}
			\begin{tabular}{ccc}
				$\alpha_s(Q)$ &
    \makecell[c]{Machine learning \\ (result \uppercase\expandafter{\romannumeral1}) }
    & \makecell[c]{Machine learning \\ (result \uppercase\expandafter{\romannumeral2}) }\\
                \hline
                $Q=1$ GeV & $0.595$ & $0.570$ \\
                $Q=2$ GeV & $0.222$ & $0.303$ \\
                $Q=3$ GeV & $0.160$ & $0.240$ \\
                $Q=4$ GeV & $0.155$ & $0.218$ \\
                $Q=5$ GeV & $0.163$ & $0.206$ \\
                $Q=6$ GeV & $0.168$ & $0.196$ \\
                $Q=7$ GeV & $0.171$ & $0.187$ \\
                $Q=8$ GeV & $0.172$ & $0.180$ \\
                $Q=9$ GeV & $0.172$ & $0.174$ \\
                $Q=10$ GeV & $0.170$ & $0.169$ \\
				
			\end{tabular}
   \end{ruledtabular}

\end{table}


%------------------------------------------
\section{Conclusion} \label{sec5}
In this work, we employ deep learning to analyze the experimental data of QCD-strong coupling constant $\alpha_s(Q)$ for the first time. One treats the strong coupling constant and Q as a sequential sequence. The RNN-LSTM not only effectively processes the sequential data and trains strong coupling constants with complex nonlinearities but also solves the gradient disappearance or gradient explosion generated in the training procedure.
Based on the sensitivity of RNN-LSTM network to long and short time series data, the strong coupling constant sequence data under different energies are trained. Subsequently, the trained model is used to predict a new input sequence $Q\in[0.1, 2000]$ GeV, and two effective results are eventually obtained (results \uppercase\expandafter{\romannumeral1} and \uppercase\expandafter{\romannumeral2} in Fig. \ref{fig:pre}). By analyzing the prediction effect, loss function and evaluation index of the model, the two results exhibit strong regularity and accuracy in the high-energy scale. For low-energy scale, both results can be regarded as valid, while result \uppercase\expandafter{\romannumeral2} shows a reasonable, smooth and regular strong coupling constant behavior.

It should be realized that the processing of data weight is an inevitable link in the training process, but more accurate data is crucial for machine learning training.
The more accurate experimental data are still needed to predict the value of the strong coupling constant more accurately with machine learning. In particular, the data between several GeVs is an important area for studying the nature of the interaction strength between light quarks and gluons.
In addition, the neural network internal structure is complex. Increasing the neuron unit and epoch will significantly reduce the training efficiency of the model. Moreover, the internal working mechanism is analogy a black box, and the function structure generated within it cannot be comprehended.  Therefore, more machine learning algorithms should be researched and hired to train more accurate results, and standard functional structure forms should be studied in the training process.



%-----------------------------------------------------------------
\begin{acknowledgments}
We appreciate the kind communication with Prof. Stephan Narison. This work is supported by the National Natural Science Foundation of China under Grants No. 12065014 and No. 12047501, and by the Natural Science Foundation of Gansu province under Grant No. 22JR5RA266. We acknowledge the West Light Foundation of The Chinese Academy of Sciences, Grant No. 21JR7RA201.
\end{acknowledgments}


\begin{thebibliography}{99}
%1
%\cite{Abadi:2016kic}
\bibitem{Abadi:2016kic}
M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S.~Corrado, A.~Davis, J.~Dean and M.~Devin, \textit{et al.}
``TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed  Systems,''
[arXiv:1603.04467].
%269 citations counted in INSPIRE as of 14 Mar 2023

%2
%\cite{Alec Radford}
\bibitem{AlecRadford}
Alec Radford, Luke Metz, Soumith Chintala.
``Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,''
[arXiv:1511.06434].
%Computerence (2015).

%3
%\cite{Donahue}
\bibitem{Donahue}
J Donahue, LA Hendricks, S Guadarrama, M Rohrbach, K Saenko.
``Long-term recurrent convolutional networks for visual recognition and description,"
[arXiv:1411.4389].
%2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) IEEE, 2015.

%4
%\cite{Jürgen}
\bibitem{Jrgen}
Jurgen Schmidhuber,
``Deep learning in neural networks,"
Neural Netw, Volume \textbf{61}, Pages 85-117, ISSN 0893-6080 (2015).

%5
%\cite{Baldi:2014kfa}
\bibitem{Baldi:2014kfa}
P.~Baldi, P.~Sadowski and D.~Whiteson,
``Searching for Exotic Particles in High-Energy Physics with Deep Learning,''
Nature Commun. \textbf{5}, 4308 (2014).
%doi:10.1038/ncomms5308
%[arXiv:1402.4735 [hep-ph]].
%217 citations counted in INSPIRE as of 14 Mar 2023

%6
%\cite{Blago:2023oeo}
\bibitem{Blago:2023oeo}
M.~P.~Blago,
``Deep learning particle identification in LHCb RICH,''
J.Phys. Conf. Ser. \textbf{2438}, no.1, 012076 (2023).
%doi:10.1088/1742-6596/2438/1/012076
%0 citations counted in INSPIRE as of 14 Mar 2023

%7
%\cite{Thomadakis:2023ebe}
\bibitem{Thomadakis:2023ebe}
P.~Thomadakis, K.~Garner, G.~Gavalian and N.~Chrisochoides,
``Charged particle reconstruction in CLAS12 using Machine Learning,''
Comput. Phys. Commun. \textbf{287}, 108694 (2023).
%0 citations counted in INSPIRE as of 20 Mar 2023

%8
%\cite{Dutrieux:2021nlz}
\bibitem{Dutrieux:2021nlz}
H.~Dutrieux, C.~Lorc\'e, H.~Moutarde, P.~Sznajder, A.~Trawi\'nski and J.~Wagner,
``Phenomenological assessment of proton mechanical properties from deeply virtual Compton scattering,''
Eur. Phys. J. C \textbf{81}, no.4, 300 (2021).
%doi:10.1140/epjc/s10052-021-09069-w
%[arXiv:2101.03855 [hep-ph]].
%29 citations counted in INSPIRE as of 14 Mar 2023

%9
%\cite{Polyakov:2018zvc}
\bibitem{Polyakov:2018zvc}
M.~V.~Polyakov and P.~Schweitzer,
``Forces inside hadrons: pressure, surface tension, mechanical radius, and all that,''
Int. J. Mod. Phys. A \textbf{33}, no.26, 1830025 (2018).
%doi:10.1142/S0217751X18300259
%[arXiv:1805.06596 [hep-ph]].
%201 citations counted in INSPIRE as of 14 Mar 2023

%10
%\cite{Zhang:2022uqk}
\bibitem{Zhang:2022uqk}
Z.~Zhang, R.~Ma, J.~Hu and Q.~Wang,
``Approach the Gell-Mann-Okubo Formula with Machine Learning,''
Chin. Phys. Lett. \textbf{39}, no.11, 111201 (2022).
%doi:10.1088/0256-307X/39/11/111201
%[arXiv:2208.03165 [hep-ph]].
%3 citations counted in INSPIRE as of 14 Mar 2023

%11
%\cite{Yu:2021yvw}
\bibitem{Yu:2021yvw}
Q.~Yu, H.~Zhou, X.~D.~Huang, J.~M.~Shen and X.~G.~Wu,
``Novel and Self-Consistency Analysis of the QCD Running Coupling \ensuremath{\alpha} $_{s}$(Q) in Both the Perturbative and Nonperturbative Domains,''
Chin. Phys. Lett. \textbf{39}, 071201 (2022).
%doi:10.1088/0256-307X/39/7/071201
%[arXiv:2112.01200 [hep-ph]].
%7 citations counted in INSPIRE as of 14 Mar 2023

%12
%\cite{Deur:2016tte}
\bibitem{Deur:2016tte}
A.~Deur, S.~J.~Brodsky and G.~F.~de Teramond,
``The QCD Running Coupling,''
Nucl. Phys. \textbf{90}, 1 (2016).
%doi:10.1016/j.ppnp.2016.04.003
%[arXiv:1604.08082 [hep-ph]].
%328 citations counted in INSPIRE as of 14 Mar 2023

%13
%\cite{Wang:2022zwz}
\bibitem{Wang:2022zwz}
X.~Y.~Wang, F.~Zeng, Q.~Wang and L.~Zhang,
``First extraction of the proton mass radius and scattering length $\vert\alpha_{\rho^{0}p}\vert$ from \ensuremath{\rho}$^{0}$ photoproduction,''
Sci. China Phys. Mech. Astron. \textbf{66}, 232012 (2023).
%doi:10.1007/s11433-022-2024-9
%[arXiv:2206.09170 [nucl-th]].
%4 citations counted in INSPIRE as of 24 Mar 2023

%14
%\cite{Zeng:2020coc}
\bibitem{Zeng:2020coc}
F.~Zeng, X.~Y.~Wang, L.~Zhang, Y.~P.~Xie, R.~Wang and X.~Chen,
``Near-threshold photoproduction of $J/\psi$ in two-gluon exchange model,''
Eur. Phys. J. C \textbf{80}, 1027 (2020).
%doi:10.1140/epjc/s10052-020-08584-6
%[arXiv:2008.13439 [hep-ph]].
%14 citations counted in INSPIRE as of 14 Mar 2023

%15
%\cite{Dong:2022ids}
\bibitem{Dong:2022ids}
C.~Dong, J.~Zhang, J.~Bu, H.~Zhou and X.~Y.~Wang,
``Exploration of trace anomaly contribution to proton mass based on light vector meson photoproduction,''
Eur. Phys. J. C \textbf{83}, 122 (2023).
%doi:10.1140/epjc/s10052-023-11260-0
%[arXiv:2209.04979 [hep-ph]].
%1 citations counted in INSPIRE as of 14 Mar 2023

%16
%\cite{Wang:2022tzw}
\bibitem{Wang:2022tzw}
X.~Y.~Wang, J.~Bu and F.~Zeng,
``Analysis of the contribution of the quantum anomaly energy to the proton mass,''
Phys. Rev. D \textbf{106}, 094029 (2022).
%doi:10.1103/PhysRevD.106.094029
%[arXiv:2210.01994 [hep-ph]].
%0 citations counted in INSPIRE as of 14 Mar 2023

%17
%\cite{Gross:1973id}
\bibitem{Gross:1973id}
D.~J.~Gross and F.~Wilczek,
``Ultraviolet Behavior of Nonabelian Gauge Theories,''
Phys. Rev. Lett. \textbf{30}, 1343-1346 (1973).
%doi:10.1103/PhysRevLett.30.1343
%5984 citations counted in INSPIRE as of 14 Mar 2023

%18
%\cite{Begel:2022kwp}
\bibitem{Begel:2022kwp}
M.~Begel, S.~Hoeche, M.~Schmitt, H.~W.~Lin, P.~M.~Nadolsky, C.~Royon, Y.~J.~Lee, S.~Mukherjee, C.~Baldenegro and J.~Campbell, \textit{et al.}
``Precision QCD, Hadronic Structure \& Forward QCD, Heavy Ions: Report of Energy Frontier Topical Groups 5, 6, 7 submitted to Snowmass 2021,''
[arXiv:2209.14872 [hep-ph]].
%9 citations counted in INSPIRE as of 04 Apr 2023

%19
%\cite{Bethke:2022cfc}
\bibitem{Bethke:2022cfc}
S.~Bethke and A.~Wagner,
``The JADE Experiment at the PETRA $e^+e^-$ collider -- history, achievements and revival,''
Eur. Phys. J. H \textbf{47}, 16 (2022).
%doi:10.1140/epjh/s13129-022-00047-8
%[arXiv:2208.11076 [hep-ex]].
%2 citations counted in INSPIRE as of 04 Apr 2023

%20
%\cite{Deur:2005cf}
\bibitem{Deur:2005cf}
A.~Deur, V.~Burkert, J.~P.~Chen and W.~Korsch,
``Experimental determination of the effective strong coupling constant,''
Phys. Lett. B \textbf{650}, 244-248 (2007).
%doi:10.1016/j.physletb.2007.05.015
%[arXiv:hep-ph/0509113 [hep-ph]].
%125 citations counted in INSPIRE as of 21 Mar 2023

%21
%\cite{Deur:2008rf}
\bibitem{Deur:2008rf}
A.~Deur, V.~Burkert, J.~P.~Chen and W.~Korsch,
``Determination of the effective strong coupling constant alpha(s,g(1))(Q**2) from CLAS spin structure function data,''
Phys. Lett. B \textbf{665}, 349-351 (2008).
%doi:10.1016/j.physletb.2008.06.049
%[arXiv:0803.4119 [hep-ph]].
%124 citations counted in INSPIRE as of 14 Mar 2023

%22
%\cite{Deur:2022msf}
\bibitem{Deur:2022msf}
A.~Deur, V.~Burkert, J.~P.~Chen and W.~Korsch,
``Experimental determination of the QCD effective charge $\alpha_{g_1}(Q)$,''
Particles \textbf{5}, 171 (2022).
%doi:10.3390/particles5020015
%[arXiv:2205.01169 [hep-ph]].
%7 citations counted in INSPIRE as of 14 Mar 2023

%23
%\cite{Deur:2021klh}
\bibitem{Deur:2021klh}
A.~Deur, J.~P.~Chen, S.~E.~Kuhn, C.~Peng, M.~Ripani, V.~Sulkosky, K.~Adhikari, M.~Battaglieri, V.~D.~Burkert and G.~D.~Cates, \textit{et al.}
``Experimental study of the behavior of the Bjorken sum at very low Q2,''
Phys. Lett. B \textbf{825}, 136878 (2022).
%doi:10.1016/j.physletb.2022.136878
%[arXiv:2107.08133 [nucl-ex]].
%10 citations counted in INSPIRE as of 21 Mar 2023

%24
%\cite{Kim:1998kia}
\bibitem{Kim:1998kia}
J.~H.~Kim, D.~A.~Harris, C.~G.~Arroyo, L.~de Barbaro, P.~de Barbaro, A.~O.~Bazarko, R.~H.~Bernstein, A.~Bodek, T.~Bolton and H.~S.~Budd, \textit{et al.}
``A Measurement of alpha(s)(Q**2) from the Gross-Llewellyn Smith sum rule,''
Phys. Rev. Lett. \textbf{81}, 3595-3598 (1998).
%doi:10.1103/PhysRevLett.81.3595
%[arXiv:hep-ex/9808015 [hep-ex]].
%106 citations counted in INSPIRE as of 21 Mar 2023

%25
%\cite{HERMES:1997hjr}
\bibitem{HERMES:1997hjr}
K.~Ackerstaff \textit{et al.} [HERMES],
``Measurement of the neutron spin structure function g1(n) with a polarized He-3 internal target,''
Phys. Lett. B \textbf{404}, 383-389 (1997).
%doi:10.1016/S0370-2693(97)00611-4
%[arXiv:hep-ex/9703005 [hep-ex]].
%435 citations counted in INSPIRE as of 21 Mar 2023

%26
%\cite{HERMES:1998pau}
\bibitem{HERMES:1998pau}
K.~Ackerstaff \textit{et al.} [HERMES],
``Determination of the deep inelastic contribution to the generalized Gerasimov-Drell-Hearn integral for the proton and neutron,''
Phys. Lett. B \textbf{444}, 531-538 (1998).
%doi:10.1016/S0370-2693(98)01396-3
%[arXiv:hep-ex/9809015 [hep-ex]].
%64 citations counted in INSPIRE as of 21 Mar 2023

%27
%\cite{HERMES:1998cbu}
\bibitem{HERMES:1998cbu}
A.~Airapetian \textit{et al.} [HERMES],
``Measurement of the proton spin structure function g1(p) with a pure hydrogen target,''
Phys. Lett. B \textbf{442}, 484-492 (1998).
%doi:10.1016/S0370-2693(98)01341-0
%[arXiv:hep-ex/9807015 [hep-ex]].
%335 citations counted in INSPIRE as of 21 Mar 2023

%28
%\cite{HERMES:2002mes}
\bibitem{HERMES:2002mes}
A.~Airapetian \textit{et al.} [HERMES],
``Evidence for quark hadron duality in the proton spin asymmetry A(1),''
Phys. Rev. Lett. \textbf{90}, 092002 (2003).
%doi:10.1103/PhysRevLett.90.092002
%[arXiv:hep-ex/0209018 [hep-ex]].
%88 citations counted in INSPIRE as of 21 Mar 2023

%29
%\cite{HERMES:2006jyl}
\bibitem{HERMES:2006jyl}
A.~Airapetian \textit{et al.} [HERMES],
``Precise determination of the spin structure function g(1) of the proton, deuteron and neutron,''
Phys. Rev. D \textbf{75}, 012007 (2007).
%doi:10.1103/PhysRevD.75.012007
%[arXiv:hep-ex/0609039 [hep-ex]].
%536 citations counted in INSPIRE as of 21 Mar 2023

%30
%\cite{Komijani:2020kst}
\bibitem{Komijani:2020kst}
J.~Komijani, P.~Petreczky and J.~H.~Weber,
``Strong coupling constant and quark masses from lattice QCD,''
Prog. Part. Nucl. Phys. \textbf{113}, 103788 (2020).
%doi:10.1016/j.ppnp.2020.103788
%[arXiv:2003.11703 [hep-lat]].
%20 citations counted in INSPIRE as of 14 Mar 2023
%\cite{Maezawa:2016vgv}

%31
\bibitem{Maezawa:2016vgv}
Y.~Maezawa and P.~Petreczky,
``Quark masses and strong coupling constant in 2+1 flavor QCD,''
Phys. Rev. D \textbf{94}, 034507 (2016).
%doi:10.1103/PhysRevD.94.034507
%[arXiv:1606.08798 [hep-lat]].
%48 citations counted in INSPIRE as of 14 Mar 2023

%32
%\cite{Brodsky:2006uqa}
\bibitem{Brodsky:2006uqa}
S.~J.~Brodsky and G.~F.~de Teramond,
``Hadronic spectra and light-front wavefunctions in holographic QCD,''
Phys. Rev. Lett. \textbf{96}, 201601 (2006).
%doi:10.1103/PhysRevLett.96.201601
%[arXiv:hep-ph/0602252 [hep-ph]].
%455 citations counted in INSPIRE as of 24 Mar 2023

%33
%\cite{deTeramond:2008ht}
\bibitem{deTeramond:2008ht}
G.~F.~de Teramond and S.~J.~Brodsky,
``Light-Front Holography: A First Approximation to QCD,''
Phys. Rev. Lett. \textbf{102}, 081601 (2009).
%doi:10.1103/PhysRevLett.102.081601
%[arXiv:0809.4899 [hep-ph]].
%339 citations counted in INSPIRE as of 24 Mar 2023

%34
%\cite{deTeramond:2013it}
\bibitem{deTeramond:2013it}
G.~F.~de Teramond, H.~G.~Dosch and S.~J.~Brodsky,
``Kinematical and Dynamical Aspects of Higher-Spin Bound-State Equations in Holographic QCD,''
Phys. Rev. D \textbf{87}, 075005 (2013).
%doi:10.1103/PhysRevD.87.075005
%[arXiv:1301.1651 [hep-ph]].
%83 citations counted in INSPIRE as of 24 Mar 2023

%35
\bibitem{Harada}
Harada, M. ,  Y. Kikukawa , and  K. Yamawaki,
``Strong Coupling Gauge Theories and Effective Field Theories." Strong Coupling Gauge Theories,''
Effective Field Theories 2003.

%36
\bibitem{Graves}
Graves. A,
``Supervised Sequence Labelling with Recurrent Neural Networks,'' (2013).

%37
\bibitem{Hammer}
Hammer. B, ``Learning with recurrent neural networks,''
Phd Thesis Fachbereich Mathematik/informatik Universitat Osnabruck, 2000, 21(2):357--368.


%38
\bibitem{Gers}
Gers, F. A., Schmidhuber,
``Learning to forget: Continual prediction with LSTM,''
Neural computation, 12(10), 2451-2471.

%39
\bibitem{Msi}
Msi. A,  Sssm. A,  Sa B, ``Sequence-to-sequence Bangla Sentence Generation with LSTM Recurrent Neural Networks - ScienceDirect,''
Procedia Computer Science, 2019, 152(C):51-58.


%40
%\cite{Narison:2018dcr}
\bibitem{Narison:2018dcr}
S.~Narison,
``QCD parameter correlations from heavy quarkonia,''
Int. J. Mod. Phys. A \textbf{33}, 1850045 (2018).
%doi:10.1142/S0217751X18500458
%[arXiv:1801.00592 [hep-ph]].
%45 citations counted in INSPIRE as of 21 Mar 2023

%41
%\cite{Braaten:1991qm}
\bibitem{Braaten:1991qm}
E.~Braaten, S.~Narison and A.~Pich,
``QCD analysis of the tau hadronic width,''
Nucl. Phys. B \textbf{373}, 581-612 (1992).
%doi:10.1016/0550-3213(92)90267-F
%721 citations counted in INSPIRE as of 21 Mar 2023



%42
%\cite{Schilling:1993fg}
\bibitem{Schilling:1993fg}
K.~Schilling and G.~S.~Bali,
``alpha-s from the lattice potential,''
Nucl. Phys. B Proc. Suppl. \textbf{34}, 147-152 (1994).
%doi:10.1016/0920-5632(94)90331-X
%[arXiv:hep-lat/9311054 [hep-lat]].
%8 citations counted in INSPIRE as of 02 Apr 2023

%43
%\cite{CELLO:1989okb}
\bibitem{CELLO:1989okb}
H.~J.~Behrend \textit{et al.} [CELLO],
``Model Independent Limits on $\Lambda$ ({QCD}) From $e^+ e^-$ Annihilation in the Energy Range From 14-{GeV} to 46-{GeV},''
Z. Phys. C \textbf{44}, 63 (1989).
%doi:10.1007/BF01548583
%40 citations counted in INSPIRE as of 02 Apr 2023

%44
%\cite{SLD:1994idb}
\bibitem{SLD:1994idb}
K.~Abe \textit{et al.} [SLD],
``Measurement of alpha-s (M(Z)**2) from hadronic event observables at the Z0 resonance,''
Phys. Rev. D \textbf{51}, 962-984 (1995).
%doi:10.1103/PhysRevD.51.962
%[arXiv:hep-ex/9501003 [hep-ex]].
%224 citations counted in INSPIRE as of 02 Apr 2023

%45
%\cite{Blumlein:1994kw}
\bibitem{Blumlein:1994kw}
J.~Blumlein and J.~Botts,
``Do deep inelastic scattering data favor a light gluino?,''
Phys. Lett. B \textbf{325}, 190-196 (1994).
%[erratum: Phys. Lett. B \textbf{331}, 450 (1994)]
%doi:10.1016/0370-2693(94)90091-4
%[arXiv:hep-ph/9401291 [hep-ph]].
%40 citations counted in INSPIRE as of 02 Apr 2023

%46
%\cite{Clavelli:1994mf}
\bibitem{Clavelli:1994mf}
L.~Clavelli and P.~W.~Coulter,
``Low-energy measurements of the strong coupling constant and the question of a light gluino,''
Phys. Rev. D \textbf{51}, 1117-1124 (1995).
%doi:10.1103/PhysRevD.51.1117
%23 citations counted in INSPIRE as of 02 Apr 2023

%47
%\cite{Brodsky:1982gc}
\bibitem{Brodsky:1982gc}
S.~J.~Brodsky, G.~P.~Lepage and P.~B.~Mackenzie,
``On the Elimination of Scale Ambiguities in Perturbative Quantum Chromodynamics,''
Phys. Rev. D \textbf{28}, 228 (1983).
%doi:10.1103/PhysRevD.28.228
%1259 citations counted in INSPIRE as of 02 Apr 2023

%48
\bibitem{hooft}
G.~'t Hooft,
``Symmetry Breaking Through Bell-Jackiw Anomalies,'' Phys. Rev. Lett. \textbf{37}, 8-11 (1976).
%49
%\cite{Chetyrkin:1996ela}
\bibitem{Chetyrkin:1996ela}
K.~G.~Chetyrkin, J.~H.~Kuhn and A.~Kwiatkowski,
``QCD corrections to the $e^{+} e^{-}$ cross-section and the $Z$ boson decay rate,''
Phys. Rept. \textbf{277}, 189-281 (1996).
%doi:10.1016/S0370-1573(96)00012-9
%[arXiv:hep-ph/9503396 [hep-ph]].
%279 citations counted in INSPIRE as of 02 Apr 2023


\end{thebibliography}



\end{document}
