\documentclass[prd,twocolumn,showpacs,preprintnumbers,superscriptaddress,floatfix,nofootinbib]{revtex4-1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx,,booktabs}
\usepackage{amssymb}
\usepackage{amsmath,txfonts,ulem}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{color}
\usepackage{bm}
\usepackage{ulem}
% \usepackage{subcaption}
\usepackage[colorlinks,
            citecolor=blue,
            anchorcolor=green,
            menucolor=orange,
            linkcolor=red,
            filecolor=red,
            runcolor=pink,
            urlcolor=blue,
            frenchlinks=red]{hyperref}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=Latex.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{LastRevised=Wednesday, July 15, 2020 22:19:32}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\allowdisplaybreaks[4]


\begin{document}


\title{Determination of the distribution of strong coupling constant with machine learning}
    \author{Xiao-Yun Wang}
\email{xywang@lut.edu.cn}
\affiliation{Department of physics, Lanzhou University of Technology,
Lanzhou 730050, China}
\affiliation{Lanzhou Center for Theoretical Physics, Key Laboratory of Theoretical Physics of Gansu Province, Lanzhou University, Lanzhou, Gansu 730000, China}

\author{Chen Dong}
\email{dongphysics@yeah.net}
\affiliation{Department of physics, Lanzhou University of Technology,
Lanzhou 730050, China}

\author{Quanjin Wang}
\affiliation{Department of physics, Lanzhou University of Technology,
Lanzhou 730050, China}
	
	
	
	



\begin{abstract}
In this work, we use the artificial neural network (ANN) method to study and predict the distribution of strong coupling constants by fitting the existing data. Our approach takes advantage of the ability of ANN to learn complex nonlinear relations and excellent generalization,  and allows for a systematic treatment of the uncertainties associated with the data. To ensure the reliability of our results, we apply cross-validation methods during the training process. Finally, we obtained the predicted values of the strong coupling constants at different energy scales, and compared and verified them with the existing experimental data. Our approach represents a promising way to improve the determination of the strong coupling constant at low energies, and could have important implications for future experimental and theoretical studies in quantum chromodynamics.




\end{abstract}

%%\pacs{14.60.Pq, 11.30.Er, 12.15.Ff, 14.60.Lm}

\maketitle

\section{Introduction}
In recent years, with the continuous development of deep learning \cite{Abadi:2016kic,AlecRadford,Donahue,Jrgen} has been gradually applied to the research of high-energy particle physics with its advantages of scalability, excellent nonlinear modelling ability, adaptability, robustness and interpretability. In order to detect the signals of rare particles, the problem of not capturing all available signals and background classification in high-energy physics experiments is solved \cite{Baldi:2014kfa}. The researchers overcame these shortcomings using deep learning and achieved remarkable results. In addition, in the project of the Ring Imaging Cherenkov detectors (RICH) detector experiment \cite{Blago:2023oeo}, the Convolutional Neural Network (CNN) method is introduced to improve the particle recognition technology, which will provide the accuracy of simulated collision data for the LHC operation in $2022$. Of course, machine learning can also be employed to investigate the physics property of nucleons.  In Ref \cite{Dutrieux:2021nlz}, the authors described in detail the use of artificial neural networks for the global fitting of deeply virtual Compton scattering data. Moreover, the D-term, which is the most important for comprehending the QCD momentum energy tensor, is effectively extracted from this scheme to analyze the distribution of pressure and shear force \cite{Polyakov:2018zvc} inside the proton. Recently, an interesting work has been developed based on machine learning \cite{Zhang:2022uqk}, which uses physical constraints to successfully rediscover the Gell-Mann-Okubo formula using the symbolic regression technique of the genetic algorithm. These works strongly inspires us to study numerous uncertain physical quantities in QCD theory with machine learning.

Quantum chromodynamics (QCD) is a fundamental theory describing the strong interaction between quarks and gluons \cite{Yu:2021yvw}. In QCD theory, the strong coupling constant $\alpha_s(Q)$ is essential to represent the strong interaction, characterising the strength of the previous strong interaction between quarks and gluons. The exact value of $\alpha_s(Q)$ is crucial for understanding the nature of strong interactions and can be used to study the cross section of vector mesons photoproduction \cite{Zeng:2020coc} and the mass source of nucleons \cite{Dong:2022ids, Wang:2022tzw}. However, much theoretical and experimental research has been done on strong coupling constants, but the values for partial energies are not accurate enough.


In the high-energy region, the $\alpha_s(Q)$ becomes smaller due to the asymptotically free property \cite{Gross:1973id}. This means that the interaction between quarks and gluons is weakened and can be studied using the renormalization group equation. Moreover, $\alpha_s(Q)$ in the high-energy region can also be determined through high-energy physics experiments, such as LHeC \cite{LHeC:2020van}, HERA \cite{Begel:2022kwp}, CMS \cite{CMS:2018uag}, JADE \cite{Bethke:2022cfc} and H1 \cite{H1:2015ubc}.
However, problems arise when the energy decreases below $10$ GeV. In the low-energy region, the QCD interaction becomes very strong and cannot be effectively calculated by perturbation theory, complicating the theoretical calculation. Although $\alpha_s(Q)$ at low energy can be studied experimentally, relevant results have been obtained theoretically through Lattice QCD \cite{Komijani:2020kst,Maezawa:2016vgv},Holographic QCD, effective field theory \cite{Harada}, the light-front theory \cite{Yu:2021yvw}. However, the uncertainty of the result is still a problem in the study of QCD theory.

In order to determine the value of strong coupling constant $\alpha_s(Q)$ under different energies, the Recurrent Neural Network (RNN) algorithm in Artificial Neural Network (ANN) is adopted in this work to analyze $\alpha_s(Q)$ experimental data accumulated in laboratories around the world. The trained model is used to predict $\alpha_s(Q)$, and the mean square error (MSE) is used and expected variance to evaluate the model, which ensures model reliability, accuracy and generalization.

The structure of this paper is organized as follows. The principle of the RNN neural is described in Sec. \ref{sec2}.
RNN network construction based on strong coupling constant, data processing and selection of model evaluation functions are shown in Sec. \ref{sec3}.
The prediction of alphas and evaluation results are presented in Sec. \ref{sec4}. A simple summary and discussion is given in Sec. \ref{sec5}.

\section{The principle of Recurrent neural network} \label{sec2}

Recurrent neural network (RNN) \cite{Graves} is a branch of artificial neural network (ANN) and an important part of deep learning. RNNS can be regarded as a class of neural networks with short-term memory ability. In RNN, neurons can not only receive information from other neurons,  but also receive information from themselves, forming a network structure with loops. Because RNN network can receive its own neuron information, it has a more robust memory capacity. Therefore, RNN is very suitable for processing time series data, text data, etc.

The simple network diagram of RNN is shown in Fig. \ref{fig:Rnn}, which shows the basic link structure of RNN. For the hidden state $h_t$ at time $t$, it can be represented as,
\begin{equation}
    h_t = \sigma (W_{ih} x_t + b_{ih} +W_{hh}h_{t-1}+b_{hh}) \label{eq:1}
\end{equation}
where $h_t$ is the hidden state at time $t$, $x_t$ is the input quantity at time $t$, $h_{t-1}$ is the hidden state at time $t-1$, $W_{ih}$ is the weight of input to the hidden layer, $W_{hh}$ is the weight from hidden layer to hidden layer, $b_{ih}$ is the bias from output layer to hidden layer, $b_{hh}$ is the bias from hidden layer to hidden layer, and $\sigma$ is the activation function. Either $tanh$ or $relu$ is usually chosen for the choice of activation functions.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[scale=0.48]{RNN.eps}
		\caption{The simple principle of recurrent neural network.}\label{fig:Rnn}
	\end{center}	
\end{figure}

In practical applications, RNN has a certain memory ability for information when modeling sequence data. However, with the increase of recursion times, simple RNN will have the problem of exponential explosion or disappearance of weight, which makes it challenging to capture long-term correlation. It also leads to the difficulty of convergence in RNN training.
A special kind of RNN is introduced to solve this problem, that is, long short-term memory network (LSTM) \cite{Gers}.

Compared with simple RNN networks, LSTM can obtain better analysis results in longer sequences. It is easy to solve the problems of gradient disappearing and gradient explosion in the training process. The network construction principle of LSTM is shown in Fig. \ref{fig:LSTM}.
\begin{figure*}[htbp]
	\begin{center}
		\includegraphics[scale=0.48]{LSTM.eps}
		\caption{The simple principle of the long short-term memory network.}\label{fig:LSTM}
	\end{center}	
\end{figure*}

In the LSTM network, each LSTM cell performs the following function calculations for the input,
\begin{equation}
\begin{aligned}
& i_t=\sigma\left(W_{i i} x_t+b_{i i}+W_{h i} h_{t-1}+b_{h i}\right) \\
& f_t=\sigma\left(W_{i f} x_t+b_{i f}+W_{h f} h_{t-1}+b_{h f}\right) \\
& g_t=\tanh \left(W_{i g} x_t+b_{i g}+W_{h g} h_{t-1}+b_{h g}\right) \\
& o_t=\sigma\left(W_{i o} x_t+b_{i 0}+W_{h o} h_{t-1}+b_{h o}\right) \\
& c_t=f_t \times c_{t-1}+i_t \times g_t \\
& h_t=o_t \times \tanh \left(c_t\right)
\end{aligned}
\end{equation}
where $i_t$, $f_t$, $g_t$ and $o_t$ respectively represent input gate, forget gate, selection gate and output gate. $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$. $h_{t-1}$ is the hidden state at time $t-1$. Here, set the hidden status to $0$ at the initial moment. In the transmission process of each cell, the status $c_{t-1}$ is usually transferred to the status $c_t$ after some parameters are attached, and its change speed is slow. In contrast, the value of $h_t$ changes significantly, often very different from that of other nodes.

Regarding information processing, LSTM is generally divided into three stages: forgetting, selective memory, and output.
\begin{enumerate}[label=\roman*.]
    \item  Forgetting stage : This stage mainly forgets the input transmitted by the previous node selectively. The value of $f_t$ controls the part that needs to be forgotten and remembered in $c_{t-1}$.
    \item Selective memory stage : This stage inputs $x_t$ for selective memory. Record important data. The input content of the current cell is the calculated $i_t$, which can be selectively output by $g_t$.
    \item Output stage : This phase determines what output is as the current state. The control is mainly through $o_t$, and $c_t$ is scaled using the $tanh$ activation function.
\end{enumerate}
%----------------------------------------------------------------
\section{Construction of LSTM network based on strongly coupled constant data} \label{sec3}
It is realized that the strong coupling constant $\alpha_s(Q)$ varies with the energy $Q$ from the experimental data accumulated by various accelerators in the world. This relationship is a kind of time series data, where the time axis represents different energy $Q$.
Due to the unique properties of LSTM, it can capture the change rule of $\alpha_s(Q)$ over time (energy) in these time series data and predict the value of $\alpha_s(Q)$ under different $Q$. This ability makes LSTM a powerful tool for studying $\alpha_s(Q)$ variations in QCD theory.

To obtain reliable results, the quality and quantity of experimental data must be considered first. For low energy region, the $\alpha_s(Q)$ data at $Q\in(0.1, 4)$ GeV obtained by CLAS, Hall A and other laboratories are selected \cite{Deur:2008rf,Deur:2022msf,Deur:2016tte}. For the high energy region, the data of $Q\in(6, 2000)$ GeV obtained by LHeC \cite{LHeC:2020van}, HERA \cite{Begel:2022kwp}, CMS \cite{CMS:2018uag}, JADE \cite{Bethke:2022cfc} and H1 \cite{H1:2015ubc} experiments are adopted. Then, all the data is divided into the training set and the test set in a ratio of $7$:$3$, and the data sets are normalized. Data normalization aims to limit the pre-processed data to a certain range to eliminate the adverse effects caused by the singular sample data. At the same time, it can speed up the optimal solution of gradient descent and improve the accuracy. In addition, noise is added to existing data to achieve the data enhancement effect and improve the effectiveness of training results.

For the construction of the model, we employ the "Keras" deep learning framework to build the LSTM network. During the build, a hidden layer and an output layer are set up, and the activation functions are $tanh$ and $relu$, respectively.  Add a dropout layer between the two layers to randomly drop $30\%$ of the neurons, mitigating the overfitting of the network. In other words, in the process of forward propagation of the network, the activation value of a neuron stops working with a certain probability, which makes the model more generalized and ultimately makes the trained network more robust and does not over-rely on some local features.

The correct choice of optimizer can affect the convergence speed, generalization ability, and stability of the model. By analyzing the characteristics of $\alpha_s(Q)$ changing with energy $Q$, it is also necessary to avoid the problem of gradient disappearing or explosion. The "Adam" adaptive optimizer is a suitable choice, which can avoid optimal local problems in addition to its ability to satisfy the needs.

Up to now, we need to clarify a problem. The model established in this work belongs to the regression model, so the mean square error (MSE) is used as the loss function,
\begin{equation}
\mathrm{MSE}=\frac{1}{\mathrm{n}} \sum_{\mathrm{i}=1}^{\mathrm{n}}\left(\mathrm{y}_{\mathrm{i}}-\hat{\mathrm{y}}_{\mathrm{i}}\right)^2.
\end{equation}
where $y$ is the predicted result, $\hat{\mathrm{y}}$ is the true value, and $n$ is the number of samples.
The function is convenient for intuitively understanding the results and has broad applicability and excellent mathematical properties. Moreover, the expected variance (EV) is selected as the evaluation index of the model,
\begin{equation}
EV = 1 - \frac{Var(y - \hat{y})}{Var(y)} \label{eq:4}
\end{equation}
which measures the model's ability to explain the target variable. That is, the percentage of variance that the model can account for in the value of the target variable. The $Var$ in Eq. (\ref{eq:4}) represents variance.The criterion of expected variance is intuitive, and the result returned by it belongs to $[0, 1]$. The result is closer to $1$, the more accurate the prediction result of the model is.
%----------------------------------------------------------------
\section{Result analysis and discussion}\label{sec4}
Set the units of hidden layer nerve to $1100$, and the output layer's to $800$. In order to fully demonstrate the reliability and accuracy of the LSTM model after training, we divided the training into three scenarios:
\begin{enumerate}[label=\roman*.]
    \item Scenario A : Training low-energy data only, $Q\in(0.1, 4)$ GeV;
    \item Scenario B : Training high-energy data only, $Q\in(6,2000)$ GeV;
    \item Scenario C : Training high-energy and low-energy overall data, $Q\in(0.1,2000)$ GeV.
    \item Scenario D: Discard the three groups of $\alpha_{F3/\pi}$ with the largest error in the low-energy data, and combine the data in the high-energy data.
\end{enumerate}
Note that the data involved in each Scenario is divided into training sets and test sets in a ratio of $7$:$3$.

For Scenario A, set the training epoch as $300$, and obtain the training and test data loss function as shown in Fig. \ref{fig:Loss} (a). The EV=$0.87$, suggesting that the training accuracy of the model is appropriate.

It can be observed that the loss functions of the training data and the test data decrease rapidly within epoch$\in[0,50]$.  After epoch$>50$, the loss function becomes flat and very close. More importantly, the loss function of test data does not suddenly increase. That is to mean there is no overfitting in the training process. These circumstances indicate that the performance of the LSTM model on the training data and the test data is similar and demonstrates that the model has an acceptable generalization ability.
Subsequently, the trained model is used to predict the values of the strong coupling constant $\alpha_s(Q)$ under different energies $Q$, as shown in Fig. \ref{fig:pre} (a).


The brown line shows the result predicted by LSTM, which is in good agreement with the experimental data measurements in the low-energy region. However, the experimental data of $\alpha_s(Q)$  showed an increasing and then decreasing trend when $Q$ increased, while LSTM displayed no decrease after the increasing effect. This indicates that the results of Scenario A can effectively predict the strong coupling constant at low-energy but not at high-energy.

The number of epochs set for Scenario B is identical to Scenario A. With the increase of epoch, the loss function of the training data will fluctuate at first and then rapidly decline to convergence. The loss function of test data drops rapidly to convergence, as shown in Fig. \ref{fig:Loss} (b). The overall loss function is small, indirectly indicating that LSTM fits well and the empirical risk of the model is minimized.

In addition, EV=$0.98$ obtained by Scenario B is very close to $1$, which means that the test result is quite accurate.
Next, the LSTM model trained in Scenario B is used to predict the result of the high-energy region, which is almost consistent with the experimental data. However, when extending the prediction to low-energy region, the advantages of the training model could not be shown at all. The LSTM directly presents a prediction similar to the normal distribution. The predicted results are shown in Fig. \ref{fig:pre} (b). The green line is the predicted results of LSTM of Scenario B.

The number of epochs of Scenario C is also the same. Considering all experimental data, the loss function of training data and test data is shown in Fig. \ref{fig:Loss} (c). The EV is calculated as $0.95$.
As shown in Fig. \ref{fig:pre} (c), the behavior of the strong coupling constant predicted by the LSTM model (the black line) after the training almost reappears the experimental data when considering the error of the experimental data. Because the EV and the loss function together exhibit excellent consequences, the predicted results of Scenario C are reliable. However, at $Q=0.1$ GeV, the $\alpha_s(Q)$ predicted by Scenario C are rather prominent. At the same time, when $Q$ is approximately between $2$ GeV and $6$ GeV, there is a trough, which is a sudden increase in the continuous decline. That causes the overall result to be abnormal.

After carefully analyzing the experimental data, we found that the last three data of $\alpha_{F3/\pi}$ (the solid black hexagon) have large error bars, which are relatively strange compared to other experimental data. Therefore, we consider discarding these three sets of data during training, that is, Scenario D. Eventually, EV=$0.96$ is obtained. And the loss function and prediction results are shown in Fig. \ref{fig:Loss} and \ref{fig:pre} (d), respectively.

The orange line represents the predicted results for the Scenario D. After the three data are discarded, the fluctuation of the transition zone at high energy and low energy disappears. The three data are preserved in the drawing to better show the results after the training.
The strong coupling constant behaves more reasonably as $Q$ increases. In addition, the value of $\alpha_s(Q)$ predicted in $Q\in (2,6)$ GeV is also within the error range of the three data of $\alpha_{F3/\pi}$. Moreover, the predicted $\alpha_s(Q)$ values around Q=$0.1$ GeV are more consistent, unlike the slight bump in Fig. \ref{fig:pre} (c).

The above four Scenarios show that different results can be obtained by analyzing strong coupling constants from diverse angles. From all the results obtained, Scenario D has strong performance in low and high-energy regions. In fact, one can realize that, except for Scenarios A, the results are basically identical in the high-energy region. However, in the low-energy area, the results of different Scenarios are slightly different due to the error of experimental data, especially in the $1\leq Q \leq10$ GeV range.  In addition, the results of the loss function and expected variance are also consistent with the evaluation criteria of the training model. Therefore, the result of Scenario D is considered reasonable and reliable. And the value of $\alpha_s(Q)$ at $Q\in[1,10]$ is shown in Tab. \ref{tab:table1}.

\begin{table*}[]\small
	\caption{\label{tab:table1} The result of $\alpha_s(Q)$  predicted by Scenario D at different energies $Q$.}
	\resizebox{\linewidth}{!}{
			\begin{tabular}{ccccccccccc}
				\hline
				\hline
				Q (GeV)  & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$           \\
				\hline
				$\alpha_s(Q)$ & $0.5626$ & $0.2370$ & $0.2001$ & $0.1920$ & $0.1934$ & $0.1945$ & $0.1908$  & $0.1859$ & $0.1803$ & $0.1740$  \\
				
			
				\hline
				\hline
			\end{tabular}
}
\end{table*}
\begin{figure*}[t]
    \centering
    \subfigure[\hspace{2mm} Scenario A]{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.40]{Loss-A.eps}
         %\caption{(A)}
        \end{minipage}%
        }
    \subfigure[\hspace{2mm} Scenario B]{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.40]{Loss-B.eps}
        % \caption*{(b)}
        \end{minipage}%
        }

    \subfigure[\hspace{2mm} Scenario C]{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.40]{Loss-C.eps}
        % \caption*{(c)}
        \end{minipage}%
        }
    \subfigure[\hspace{2mm} Scenario D]{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.40]{Loss-C2.eps}
        % \caption*{(d)}
        \end{minipage}%
        }	
	\caption{ The loss function of training and test data for Scenario A, B, C and D.}\label{fig:Loss}	
\end{figure*}

\begin{figure*}[t]
    \centering
    \subfigure[\hspace{2mm} Scenario A]{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.40]{Loss-A-pre.eps}
         %\caption{(A)}
        \end{minipage}%
        }
    \subfigure[\hspace{2mm} Scenario B]{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.40]{Loss-B-pre.eps}
        % \caption*{(b)}
        \end{minipage}%
        }

    \subfigure[\hspace{2mm} Scenario C]{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.40]{Loss-C-pre.eps}
        % \caption*{(c)}
        \end{minipage}%
        }
    \subfigure[\hspace{2mm} Scenario D]{
        \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[scale=0.40]{Loss-D-pre.eps}
        % \caption*{(d)}
        \end{minipage}%
        }	
	\caption{The values of the strong coupling constants predicted by the LSTM model trained in Scenario A, B, C and D at different Q.}\label{fig:pre}	
\end{figure*}
%------------------------------------------
\section{Conclusion} \label{sec5}
In this work, we employ deep learning to analyze the experimental data of QCD-strong coupling constant. Based on the sensitivity of RNN-LSTM model to long and short time series data, different training Scenarios are set. The results from these Scenarios indicate that the selection and quality of experimental data nevertheless have a certain degree of influence on the model after training. Scenario A conducted training for the low-energy region, and the trained model effectively predicted the $\alpha_s(Q)$ value of the low-energy region, but not for the high-energy region. Scenario B presents the opposite result of Scenario A.
Scenario C and D both employ the overall data for training, but after discarding part of the data with large errors, the results presented by the two schemes are distinct, especially if $Q$ is between $1$ GeV and $10$ GeV. It is possible that the error of experimental data in this part may lead to a slight error of judgment during the training process, and the implementation of Scenario D eliminates this influence. More importantly, the expected variance and loss function results indicate that the accuracy of the test data and prediction data during the training process is very suitable and also suggest that the model is vigorous in the ability to predict the results. Therefore, the strong coupling constants at different energies predicted by Scenario D are reliable.

RNN-LSTM can effectively train and predict the strong coupling constant with complex nonlinear, it can also effectively solve the gradient disappearance or gradient explosion generated in the training process. However, its internal structure is complex. Increasing the neuron unit and epoch will significantly reduce the training efficiency of the model. In addition, its predictive ability is still weak for unknown sequence segment data. Therefore, more accurate experimental data are still needed to utilize deep learning to predict the value of the strong coupling constant more accurately. Especially in the low-energy region, $Q$ is between $1$ GeV and $10$ GeV. At the same time, more machine learning should be researched and hired to train more accurate results.



%-----------------------------------------------------------------
\begin{acknowledgments}
This work is supported by the National Natural Science Foundation of China under Grants No. 12065014 and No. 12047501, and by the Natural Science Foundation of Gansu province under Grant No. 22JR5RA266. We acknowledge the West Light Foundation of The Chinese Academy of Sciences, Grant No. 21JR7RA201.
\end{acknowledgments}


\begin{thebibliography}{99}
%\cite{Abadi:2016kic}
\bibitem{Abadi:2016kic}
M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S.~Corrado, A.~Davis, J.~Dean and M.~Devin, \textit{et al.}
``TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed  Systems,''
[arXiv:1603.04467 [cs.DC]].
%269 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Alec Radford}
\bibitem{AlecRadford}
Alec Radford, Luke Metz, Soumith Chintala.
``Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,''
Computer ence (2015).

%\cite{Donahue}
\bibitem{Donahue}
J. DonahueA et. al.
``Long-term recurrent convolutional networks for visual recognition and description,"
2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) IEEE, 2015.

%\cite{JÃ¼rgen}
\bibitem{Jrgen}
Jrgen Schmidhuber,
``Deep learning in neural networks,"
Neural Netw (2015).

%\cite{Baldi:2014kfa}
\bibitem{Baldi:2014kfa}
P.~Baldi, P.~Sadowski and D.~Whiteson,
``Searching for Exotic Particles in High-Energy Physics with Deep Learning,''
Nature Commun. \textbf{5}, 4308 (2014).
%doi:10.1038/ncomms5308
%[arXiv:1402.4735 [hep-ph]].
%217 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Blago:2023oeo}
\bibitem{Blago:2023oeo}
M.~P.~Blago,
``Deep learning particle identification in LHCb RICH,''
J.Phys. Conf. Ser. \textbf{2438}, no.1, 012076 (2023).
%doi:10.1088/1742-6596/2438/1/012076
%0 citations counted in INSPIRE as of 14 Mar 2023


%\cite{Dutrieux:2021nlz}
\bibitem{Dutrieux:2021nlz}
H.~Dutrieux, C.~Lorc\'e, H.~Moutarde, P.~Sznajder, A.~Trawi\'nski and J.~Wagner,
``Phenomenological assessment of proton mechanical properties from deeply virtual Compton scattering,''
Eur. Phys. J. C \textbf{81}, no.4, 300 (2021).
%doi:10.1140/epjc/s10052-021-09069-w
%[arXiv:2101.03855 [hep-ph]].
%29 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Polyakov:2018zvc}
\bibitem{Polyakov:2018zvc}
M.~V.~Polyakov and P.~Schweitzer,
``Forces inside hadrons: pressure, surface tension, mechanical radius, and all that,''
Int. J. Mod. Phys. A \textbf{33}, no.26, 1830025 (2018).
%doi:10.1142/S0217751X18300259
%[arXiv:1805.06596 [hep-ph]].
%201 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Zhang:2022uqk}
\bibitem{Zhang:2022uqk}
Z.~Zhang, R.~Ma, J.~Hu and Q.~Wang,
``Approach the Gell-Mann-Okubo Formula with Machine Learning,''
Chin. Phys. Lett. \textbf{39}, no.11, 111201 (2022).
%doi:10.1088/0256-307X/39/11/111201
%[arXiv:2208.03165 [hep-ph]].
%3 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Yu:2021yvw}
\bibitem{Yu:2021yvw}
Q.~Yu, H.~Zhou, X.~D.~Huang, J.~M.~Shen and X.~G.~Wu,
``Novel and Self-Consistency Analysis of the QCD Running Coupling \ensuremath{\alpha} $_{s}$(Q) in Both the Perturbative and Nonperturbative Domains,''
Chin. Phys. Lett. \textbf{39}, 071201 (2022).
%doi:10.1088/0256-307X/39/7/071201
%[arXiv:2112.01200 [hep-ph]].
%7 citations counted in INSPIRE as of 14 Mar 2023


%\cite{Zeng:2020coc}
\bibitem{Zeng:2020coc}
F.~Zeng, X.~Y.~Wang, L.~Zhang, Y.~P.~Xie, R.~Wang and X.~Chen,
``Near-threshold photoproduction of $J/\psi$ in two-gluon exchange model,''
Eur. Phys. J. C \textbf{80}, 1027 (2020).
%doi:10.1140/epjc/s10052-020-08584-6
%[arXiv:2008.13439 [hep-ph]].
%14 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Dong:2022ids}
\bibitem{Dong:2022ids}
C.~Dong, J.~Zhang, J.~Bu, H.~Zhou and X.~Y.~Wang,
``Exploration of trace anomaly contribution to proton mass based on light vector meson photoproduction,''
Eur. Phys. J. C \textbf{83}, 122 (2023).
%doi:10.1140/epjc/s10052-023-11260-0
%[arXiv:2209.04979 [hep-ph]].
%1 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Wang:2022tzw}
\bibitem{Wang:2022tzw}
X.~Y.~Wang, J.~Bu and F.~Zeng,
``Analysis of the contribution of the quantum anomaly energy to the proton mass,''
Phys. Rev. D \textbf{106}, 094029 (2022).
%doi:10.1103/PhysRevD.106.094029
%[arXiv:2210.01994 [hep-ph]].
%0 citations counted in INSPIRE as of 14 Mar 2023


%\cite{Gross:1973id}
\bibitem{Gross:1973id}
D.~J.~Gross and F.~Wilczek,
``Ultraviolet Behavior of Nonabelian Gauge Theories,''
Phys. Rev. Lett. \textbf{30}, 1343-1346 (1973).
%doi:10.1103/PhysRevLett.30.1343
%5984 citations counted in INSPIRE as of 14 Mar 2023


%\cite{LHeC:2020van}
\bibitem{LHeC:2020van}
P.~Agostini \textit{et al.} [LHeC and FCC-he Study Group],
``The Large Hadron\textendash{}Electron Collider at the HL-LHC,''
J. Phys. G \textbf{48}, no.11, 110501 (2021).
%doi:10.1088/1361-6471/abf3ba
%[arXiv:2007.14491 [hep-ex]].
%197 citations counted in INSPIRE as of 14 Mar 2023







%\cite{Begel:2022kwp}
\bibitem{Begel:2022kwp}
M.~Begel, S.~Hoeche, M.~Schmitt, H.~W.~Lin, P.~M.~Nadolsky, C.~Royon, Y.~J.~Lee, S.~Mukherjee, C.~Baldenegro and J.~Campbell, \textit{et al.}
``Precision QCD, Hadronic Structure \& Forward QCD, Heavy Ions: Report of Energy Frontier Topical Groups 5, 6, 7 submitted to Snowmass 2021,''
[arXiv:2209.14872 [hep-ph]].
%8 citations counted in INSPIRE as of 14 Mar 2023

%\cite{CMS:2018uag}
\bibitem{CMS:2018uag}
A.~M.~Sirunyan \textit{et al.} [CMS],
``Combined measurements of Higgs boson couplings in proton\textendash{}proton collisions at $\sqrt{s}=13\,\text {Te}\text {V} $,''
Eur. Phys. J. C \textbf{79}, 421 (2019).
%doi:10.1140/epjc/s10052-019-6909-y
%[arXiv:1809.10733 [hep-ex]].
%605 citations counted in INSPIRE as of 14 Mar 2023


%\cite{Bethke:2022cfc}
\bibitem{Bethke:2022cfc}
S.~Bethke and A.~Wagner,
``The JADE Experiment at the PETRA $e^+e^-$ collider -- history, achievements and revival,''
Eur. Phys. J. H \textbf{47}, 16 (2022).
%doi:10.1140/epjh/s13129-022-00047-8
%[arXiv:2208.11076 [hep-ex]].
%0 citations counted in INSPIRE as of 14 Mar 2023

%\cite{H1:2015ubc}
\bibitem{H1:2015ubc}
H.~Abramowicz \textit{et al.} [H1 and ZEUS],
``Combination of measurements of inclusive deep inelastic ${e^{\pm }p}$ scattering cross sections and QCD analysis of HERA data,''
Eur. Phys. J. C \textbf{75}, 580 (2015).
%doi:10.1140/epjc/s10052-015-3710-4
%[arXiv:1506.06042 [hep-ex]].
%799 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Komijani:2020kst}
\bibitem{Komijani:2020kst}
J.~Komijani, P.~Petreczky and J.~H.~Weber,
``Strong coupling constant and quark masses from lattice QCD,''
Prog. Part. Nucl. Phys. \textbf{113}, 103788 (2020).
%doi:10.1016/j.ppnp.2020.103788
%[arXiv:2003.11703 [hep-lat]].
%20 citations counted in INSPIRE as of 14 Mar 2023
%\cite{Maezawa:2016vgv}
\bibitem{Maezawa:2016vgv}
Y.~Maezawa and P.~Petreczky,
``Quark masses and strong coupling constant in 2+1 flavor QCD,''
Phys. Rev. D \textbf{94}, 034507 (2016).
%doi:10.1103/PhysRevD.94.034507
%[arXiv:1606.08798 [hep-lat]].
%48 citations counted in INSPIRE as of 14 Mar 2023

\bibitem{Harada}
Harada, M. ,  Y. Kikukawa , and  K. Yamawaki,
``Strong Coupling Gauge Theories and Effective Field Theories." Strong Coupling Gauge Theories,''
Effective Field Theories 2003.

\bibitem{Graves}
Graves, A,
``Supervised Sequence Labelling with Recurrent Neural Networks,'' (2013).

\bibitem{Gers}
Gers, F. A., Schmidhuber,
``Learning to forget: Continual prediction with LSTM,''
Neural computation, 12(10), 2451-2471.
























%\cite{Deur:2008rf}
\bibitem{Deur:2008rf}
A.~Deur, V.~Burkert, J.~P.~Chen and W.~Korsch,
``Determination of the effective strong coupling constant alpha(s,g(1))(Q**2) from CLAS spin structure function data,''
Phys. Lett. B \textbf{665}, 349-351 (2008).
%doi:10.1016/j.physletb.2008.06.049
%[arXiv:0803.4119 [hep-ph]].
%124 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Deur:2022msf}
\bibitem{Deur:2022msf}
A.~Deur, V.~Burkert, J.~P.~Chen and W.~Korsch,
``Experimental determination of the QCD effective charge $\alpha_{g_1}(Q)$,''
Particles \textbf{5}, 171 (2022).
%doi:10.3390/particles5020015
%[arXiv:2205.01169 [hep-ph]].
%7 citations counted in INSPIRE as of 14 Mar 2023

%\cite{Deur:2016tte}
\bibitem{Deur:2016tte}
A.~Deur, S.~J.~Brodsky and G.~F.~de Teramond,
``The QCD Running Coupling,''
Nucl. Phys. \textbf{90}, 1 (2016).
%doi:10.1016/j.ppnp.2016.04.003
%[arXiv:1604.08082 [hep-ph]].
%328 citations counted in INSPIRE as of 14 Mar 2023




\end{thebibliography}



\end{document}
