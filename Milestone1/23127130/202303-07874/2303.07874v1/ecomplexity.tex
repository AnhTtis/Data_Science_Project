\paragraph{Variational Complexity}
Let us now introduce a \say{complexity} measure for a piece-wise linear function $g$. 
We start by introducing a complexity measure for a particular choice of the network parameters. The complexity of the function will then be the minimum complexity of the network that represents this function. 
We choose
\begin{align} \label{equ:complexitymeasure}
C_k(\theta) = \frac12 \| \theta_w\|^2 = \frac12\left( \|\w^{(1)}\|^2 + \|\w^{(2)}\|^2 \right),
\end{align}
i.e., it is half the squared Euclidean norm of the {\em weight parameters}. 

If we use the representation (\ref{equ:firstrepresentation}) in its natural form, i.e.,  $w^{(2)}_i =a_i$ and $W^{(1)}_i = 1$, then we have $C_k(\theta) = \frac12 \sum_{i=1}^{k} (a_i^2+1)$. But we can do better. Write
\begin{align} \label{equ:secondrepresentation}
f(x) = c + \sum_{i=1}^{k} w^{(2)}_i [W^{(1)}_i(x-x_i)]_+,
\end{align}
where $w^{(2)}_i =a_i/\sqrt{|a_i|}$ and $W^{(1)}_i = |w^{(2)}_i |$. This gives us a complexity measure $C_k(\theta) = \sum_{i=1}^{k} |a_i| = \sum_{i=1}^{k} |\alpha_i-\alpha_{i-1}|$, where $\alpha_0=0$. Indeed, it is not very hard to see, and it is proved in \citet{srebronormbound}, that this is the best one can do even if we keep $f(x)$ fixed and are allowed to let the number $k$ of hidden nodes tend to infinity. In other words, for the function $f$ described in (\ref{equ:polygone}) we have
\begin{align*}
C(f) = \inf_{k \in \N, \theta: f_\theta = f} C_k(\theta) = \variation(f'),
\end{align*}
where $\variation(f')$ denotes the total variation of $f'$, the derivative of $f$. Why total variation?
Note that $\alpha_i$ denotes the derivative of the function so that $|\alpha_i-\alpha_{i-1}|$ is the change in the derivative at the point $x_i$. Therefore, $\sum_{i=1}^{k} |\alpha_i-\alpha_{i-1}|$ is the total variation associated to this derivative. 

% need to incorporate this material
%Background: proof of Weierstrass theorem (see Pinkus, 2000)
%Any f ∈ C[0; 1] can be uniformly approximated to arbitrary precision by a polygonal line (cf. Shektman, 1982)
%Lebesgue (1898): polygonal line with m pieces can be written
%m−1
%g(x) = ax + b + �� ci(x − xi)+
%i=1
%�� knots: 0=x0 <x1 <···<xm−1 <xm =1 �� m+1parametersa,b,ci ∈R
%�� ReLU function approximation in 1d
%% end

If we consider a general function $f: [0, 1] \xrightarrow{} \R$ then for every $\epsilon>0$, $f$ can be uniformly approximated by a piecewise linear function, see \citet{shekhtman82}. As $\epsilon$ tends to $0$ for the \say{best} approximation the variation of the piece-wise linear function converges to the total variation of $f'$. This can equivalently be written as the integral of 
$|f''|$.
It is therefore not surprising that if we look at general functions $f: \R \xrightarrow{} \R$ and let the network width tend to infinity then the lowest cost representation has a complexity of
\begin{align} \label{equ:complexity}
C(f) = \max \left(\int |f''(x)| dx, |f'(-\infty) + f'(+\infty)| \right).
\end{align}
As we previously mentioned, this concept of the complexity of a function was introduced in \citet{srebronormbound} and this paper also contains a rigorous proof of (\ref{equ:complexity}). (Note: The second term in \eqref{equ:complexity} is needed
when we go away from a function that is supported on a finite domain to $\R$. To see this consider the complexity of $f(x) = \alpha x$. It is equal to $2\alpha$ ($f(x) = \sqrt{\alpha} [\sqrt{\alpha} x]_+ - \sqrt{\alpha} [-\sqrt{\alpha} x]_+$) but $\int |f''(x)| dx = 0$.)

%\begin{align*}
%f(x) = \int_{\R} \left( \alpha(1,b) [x-b]_+ + \alpha(-1,b) [b-x]_+ \right) db + c.
%\end{align*}
%Then taking the second derivative of $f$ we get
%$$
%f(x) = \alpha(1,x) + \alpha(-1,x) = \alpha_+(x).
%$$
%We see that the measure $\alpha$ is almost uniquely defined by $f$. 


 %\citet{srebronormbound} it was shown that for 2-layer NN with ReLU activation %functions (in the infinite width limit) the minimal network euclidean norm for %representing $f$ is equal
%$$
%\max \left(\int |f''(x)| dx, |f'(-\infty) + f'(+\infty)| \right).
%$$
%Formally we consider functions of the form
%$$
%\sum_{i=1}^k w_i^{(2)} \left[\langle \w_i^{(1)}, \x \rangle + \bias_i^{(1)} \right]_+ + %b^{(2)}.
%$$
%We want to minimize $\|\w^{(2)}\|_2^2 + \|W^{(1)}\|_F^2$. Minimizing this norm is %equivalent to constraining norms of rows of $W^{(1)}$ and minimizing L1 norm of %$\w^{(2)}$. That is ultimately we minimize
%$$
%R(f) := \inf_{\theta \in \Theta}\|\w^{(2)}\|_1 \text{ s.t. } h_{\theta} = f, %\forall_{i} \|\w_i^{(1)}\|_2 = 1
%$$

%As we discussed when we introduced the complexity measure, the motivation for doing so is to impose a complexity hierarchy on the functions that are representable by NNs from \say{simple} to \say{most complex}. The thesis is then that NNs, when learning with SGLD (and to some extend SGD), will not pick a function that best approximates the given data. Rather, the criterion to be minimized consists of two parts, namely the approximation error plus the complexity. In other words, we will learn the lowest complexity function that reasonably approximates the given data. This explains why NNs do not overfit despite the large inherent overparametrization.

%Let us summarize. Minimizing (\ref{equ:regularizedopt}) corresponds to minimizing the  sum of the empirical mean and a natural complexity measure of the function. Therefore, in this case, the regularization has a very pleasing interpretation as favoring \say{low complexity} functions.


\paragraph{Sharp versus Variational Complexity.} Now we explain how the notion of sharp complexity is, in some regimes, equivalent to the variational complexity. This gives a concrete example of our promise that sharp complexity aligns well with natural complexity measures.

%Recall that we consider a NN with $k$ nodes in the intermediate layer, a scalar input and a scalar output. We therefore have $ 2k$ weights, namely $k$ weights from the scalar {\em input} to the intermediate nodes (which we henceforth call {\em input} weights) and $k$ weights from the intermediate nodes to the {\em output} (which in the sequel we will call {\em output} weights). 
%Let us start by looking at a simplified model where we only consider the $k$ output weights. 
%We still assume that the incoming weights behave as they should (as we will see shortly, they will take on values equal to the outgoing weights) but we ignore them in the probabilistic expression.
%We discuss the real model at the end. Not much will change.

Assume at first that the target function is of the form $g(x) = b + \sum_{i=1}^c v_i[x - t_i]_+$
and requires only a single change of the derivative. I.e., the piece-wise linear function consists of two pieces and we require only one term in the sum, $g(x) = a[x - t]_+ + b$ Call this function $g_1$, where the $1$ indicates that there is only a single change of the derivative and the change is of magnitude $a$. 
%Further, assume that the prior $\alpha(\theta)$ is Gaussian  where the components are independent. The vector of means is $\mu_{\theta} = (\mu_{\theta_w}=(0,\cdots, 0), \mu_{\theta_b})$ and the vector of variances is $\sigma^2_{\theta} = (\sigma^2_{\theta_w}=(\sigma^2_w,\cdots, \sigma^2_w), \sigma^2_{\theta_b}=(0,\cdots, 0))$. 
%The prior for the weights (without biases) $P(\theta_w) \propto e^{-\frac{1}{2 \sigma_w^2} \| \theta_w\|^2}$. 


We now ask what is the value of $\chi^\#(g_1, \dist_x, \e^2)$, for $\dist_x = U([0,1])$ - as this is what appears in \eqref{eq:mainthm}. We claim that for small $\e$, specific choices of $M$ and $\sigma_w^2, \sigma_b^2$ and particular regimes of parameters we have
\begin{equation}\label{eq:promise}
\chi^\#(g_1, U([0,1]), \e^2) = \Theta(a / \sigma_w^2) = \Theta(C(g_1) / \sigma_w^2).
\end{equation}
This means that the sharp complexity is closely related to the variational complexity of $g_1$. The more formal version of \eqref{eq:promise} of which a proof is in Appendix~\ref{app:proofs} reads
\begin{lemma}\label{lem:complexityforonechange} 
Let $t,\e \in (0,1), a,b\in \R$. Define $g_1(x) := b + a[x - t]_+$. If $k \leq M \leq \frac{1}{\sigma_w^2}, \sigma_b^2 \leq \frac{1}{\sigma_w^2}$ and $\Omega(\e^{1/4}),\Omega(\log(k/\sigma_w) \sigma_w^2) \leq |a| < 2, \Omega(\e^{1/4}) \leq  |b|, \Omega(\e^{1/2}) \leq \min(t,1-t)$ then
$$
\frac{|a|}{3 \sigma_w^2} \leq \chi^\#(g_1, U([0,1]), \e^2) \leq 2\left(\frac{|a|}{\sigma_w^2} + \frac{|b|}{\sigma_b^2} \right) + 11 - 3\log(\e).
$$
%\begin{itemize}
%    \item 
%    $$\chi^\#(U([0,1]), g_1, \e^2) \leq 2ak + 4 - 2 \log(\e),$$
%    \item if $a = \Omega(\e^2)$ then
%    %$a > 2\pi e^3 \e^2 $ then 
%    $$\frac{ak}{2} - \log(\e) - 3 \leq \chi^\#(U([0,1]), g_1, \e^2).$$
%\end{itemize}
\end{lemma}

The above lemma is stated with the most general setting of parameters. To get more insight into the meaning of the lemma we give the following corollary.

\begin{example}\label{exm:onechange}
For every sufficiently small $\sigma_e^2$ and $M = k, \sigma_w^2 = \frac{1}{k}, \sigma_b^2 = 1, |b| = \Theta \left(\sigma_e^{1/2} \right)$, $\Omega \left( \sigma_e^{1/4} \right), \Omega \left(\frac{\log(k)}k \right) \leq |a| < 2$ if we define $g_1(x) := b + a[x-\frac12]_+$ then
$$
\chi^\#(g_1,U[0,1],\sigma_e^2) \leq 3|a|k + 3 \log \left(\frac{1}{\sigma_e} \right).
$$
\end{example}

\begin{proof}
One can easily verify that the assumptions of Lemma~\ref{lem:complexityforonechange} are satisfied. Applying the lemma we get
\begin{align*}
\chi^\#(g_1,U[0,1],\sigma_e^2) 
&\leq 2\left(\frac{|a|}{\sigma_w^2} + \frac{|b|}{\sigma_b^2} \right) + 11 + 3\log \left(\frac{1}{ 
\sigma_e}\right)  \\
&\leq 2|a| k + \Theta \left(\sigma_e^{1/2} \right) + 11 + 3\log \left(\frac{1}{ 
\sigma_e} \right) \\
&\leq 3|a|k + 3 \log \left(\frac{1}{\sigma_e} \right) && \text{As } \Omega \left( \frac{\log(k)}{k} \right) \leq |a|.
\end{align*}
\end{proof}

\paragraph{Generalization bound.} Now we want to understand what Example~\ref{exm:onechange} gives us for the generalization bound from Theorem~\ref{thm:generalizationbound}. Setting $\beta = 1$ in Theorem~\ref{thm:generalizationbound} and applying Example~\ref{exm:onechange},  we can bound
\begin{align}
&\expectation_{\samp \sim \dist^N}[L_\dist(Q)] \nonumber \\
&\leq
2\sigma_e^2 + \frac{C}{\sqrt{2}}\sqrt{\frac{\chi^\#(g_1, \dist_x, \sigma_e^2)}{N}} \nonumber \\
&\leq 2\sigma_e^2 + \frac{C}{\sqrt{2}}\sqrt{\frac{3|a|k + 3 \log \left(\frac{1}{\sigma_e} \right)}{N} } \label{eq:generalizationboundforonechange}.
%&\lesssim \sigma_e^2 + \sqrt{\frac{2\left(\frac{|a|}{\sigma_w^2} + \frac{|b|}{\sigma_b^2} \right) + 3 \log(N) + 3\log(\frac{1}{\sigma_e^2})}{N} } \label{eq:generalizationboundforonechange}.
\end{align}

Now we interpret \eqref{eq:generalizationboundforonechange}. First note that the setting of parameters in Example~\ref{exm:onechange} is natural. The choice of $\sigma_w^2 = \frac{1}{k}$ and $\sigma_b^2 = 1$ are among standard choices for initialization schemes. We pick $|b| = \Theta \left(\sigma_e^{1/2} \right)$ and $t = 1/2$ in order to analyze functions $g_1(x)\approx a\left[x - \frac12 \right]_+$, where the bias term $b$ is nonzero because of the assumptions of Lemma~\ref{lem:complexityforonechange}. Note that depending on the relation between $k$ and $\sigma_e^2$ one of the terms dominates \eqref{eq:generalizationboundforonechange}: either $3|a|k$ or $3 \log \left(\frac{1}{\sigma_e} \right)$. 

If $\sigma_e^2 \ll k$ then $3 \log \left(\frac{1}{\sigma_e} \right)$ dominates and the generalization bound depends mostly on the noise level $\sigma_e^2$ \footnote{As a side note, notice that the $3$ in $3 \log \left(\frac{1}{\sigma_e} \right)$ corresponds to the $2c+1$ bound on the limiting complexity in Example~\ref{lem:cchangeslimit}, as we consider a function with one change of slope and a very small $\e^2$ for computing $\chi^\#$. This illustrates the relationship between sharp and limiting complexity.}.

If $\sigma_e^2 \gg k$ then $3|a|k$ dominates. In this case we get the promised dependence of the generalization bound on $|a|$, which we recall is equal to $C(g_1)$. Note that there is a wide range of $|a|$ for which the bound holds, i.e. $ \Omega \left(\frac{\log(k)}{k} \right) \leq |a| \leq 2$. We see that the simpler $g_1$, measured in terms of $C$, the better a generalization bound we get. 
%We emphasize that the bound, in some regimes, is non-vacuous also in the over-parametrized regime. Indeed consider a case where $N = \Theta(\sqrt{k})$ and $|a| \leq \frac{1}{\sqrt{k}}$ \com{Not sure if we should talk about it. I mean it's weird that it only works when $|a|$ needs to get smaller with $k$}.


%we choose the simplest set of parameters such that assumptions of Lemma~\ref{lem:complexityforonechange} hold and the bound conveys an interesting message. 

%Let $\sigma_e^2$ be sufficiently small and $k \geq \Omega(1/\sigma_e)$ and $M = k, \sigma_w^2 = \frac{1}{k}, \sigma_b^2 = 1, \beta = 1, |b| = \Theta \left(\sigma_e^{1/2} \right), t = 1/2$. The choice of $\sigma_w^2 = \frac{1}{k}, \sigma_b^2 = 1$ are among standard choices for initialization schemes. The choice $|b| = \Theta \left(\sigma_e^{1/2} \right), t = 1/2$ aims at focusing on functions $g_1(x)\approx a \left[x - \frac12 \right]_+$, where the bias term $b$ is nonzero because of the assumptions of the lemma. One can verify that the assumptions are satisfied if $\Omega \left(\sigma_e^{1/4} \right), \Omega \left(\frac{\log(k)}k \right) \leq |a| < 2$ with these choices \eqref{eq:generalizationboundforonechange} becomes
%$$
%2\sigma_e^2 + \sqrt{\frac{2|a| k + \Theta \left(\sigma_e^{1/2} \right) + 11 + \frac{3}{2}\log \left(\frac{1}{ 
%\sigma_e^2} \right)}{N} + \frac{\ln(N)}{\delta N}}
%$$

%\com{The choice of $M = k$ is less standard as it means that the variance of the bias terms is bigger than for more standard initialization schemes where ... Check what people do exactly. Does it relate somehow to NTK. Explain why it makes sense to look at it still.}.

