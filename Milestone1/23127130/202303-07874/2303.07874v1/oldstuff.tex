\section{Calculus} \label{sec:calculus}
Before we go on to study various families $\model$ in detail, let us show that under mild assumptions our (asymptotic) complexity notion fulfills some natural axioms.

We will be interested in parametric families of functions from $\inspace$ to $\outspace$. More precisely families of the form $\mathcal{H}_{\theta} := \left\{\pred_{\theta} : \inspace \xrightarrow{} \outspace, \theta \in \R^m \right\}$,
where $\theta$ is the vector of parameters. Often we will restrict ourselves to families that are \textit{scale invariant}, i.e., families that satisfy
$$\bigcup_{\alpha \in \R} \alpha \cdot \mathcal{H}_{\theta} = \mathcal{H}_\theta,$$
where the multiplication is understood as multiplying functions by a scalar. A stronger property of a family that we will consider is that of \textit{strongly scale invariant}, which is satisfied by a family $\mathcal{H}_\theta$ iff 
$$\exists_{S \subseteq [m]} \ \forall_{\alpha \in \R, \theta \in \R^m} \  \alpha \cdot f_\theta = f_{\alpha_S \cdot \theta} ,$$
where $(\alpha_S \cdot \theta)_i := \begin{cases}  \alpha \cdot \theta_i \mbox{,} & \mbox{if } i \in S \\ \theta_i \mbox{,} & \mbox{otherwise} \end{cases}$. 
\comm{RU}{Can we say why this is an interesting property and perhaps give an example?}
This definition covers for instance a case of neural networks without bias terms in the last layer. It is the case because we can pick for the set $S$ the weights in the last layer of the network.

\begin{lemma}[Scale Invariance of Asymptotic Complexity]
Assume that we are given $({\cal H}, \dist_\x, \ell)$, where ${\cal H}$ is {\em strictly scale-invariant}, and a Gaussian prior $\alpha(\theta)$.  For any function $g$ and any scalar $\kappa \neq 0$,
$\chi({\cal H}, \dist_\x, \ell, \kappa g) = \chi({\cal H}, \dist_\x, \ell, g)$.
\end{lemma}
\begin{proof}
%Note that the family is assumed to be strictly scale-invariant. Hence the scaling only changes the operating point for some of the weights. 
Let $S \subseteq [m]$ be the subset that certifies that $\mathcal{H}$ is strictly scale-invariant. Let $\e > 0$ and $A_\e := \{ \theta \in \R^m : \expectation_{\x \sim \dist_\x}[ \ell(f_{\theta}(\x), (\x, g(\x))] \leq \ell(0, (\x, \epsilon)) \}$. Moreover define $A^\kappa_\e := \bigcup_{\theta \in A_\e} \kappa_S \cdot \theta$. By definition $A_\e$ is exactly the set of parameters whose probability is computed to get $\chi({\cal H}, \dist_\x, \ell, g, \e)$. Moreover $A_\e^\kappa$ is the set of parameters whose probability is computed to get $\chi({\cal H}, \dist_\x, \ell, \kappa g, \e)$. It is because multiplication by $\kappa_S$ gives a bijection between these two sets. Now note
\begin{align*}
\prob[A_\e] &= \int_{A_\e} f(\theta) \ d \theta \\
&= \int_{A_\e^\kappa} f( (1/\kappa)_S \cdot \theta) \ d \theta \\
&= 
\end{align*}

\end{proof}

If we are building up a function by using several ``components'' then intuitively the resulting function is more complex than then components (unless there are substantial cancellations). The following lemma makes this precise.
\begin{lemma}[Subadditivity Lower Bound for Asymptotic Complexity]
We are given a ${\cal H}, \dist_\x, \ell)$ with Gaussian
prior $\alpha(\theta)$.  
Let $g_1$ and $g_2$ have complexities $\chi(\model, \dist_\x, \ell, g_1)$ and $\chi(\model, \dist_\x, \ell, g_2)$.
If $\chi(\model, \dist_\x, \ell, g_1) \neq \chi(\model, \dist_\x, \ell, g_2)$ then $\chi(\model, \dist_\x, \ell, g_1+g_2) + \chi(\model, \dist_\x, \ell, g_1-g_2) \geq \max\{\chi(\model, \dist_\x, \ell, g_1),
\chi(\model, \dist_\x, \ell, g_2)\}$.  
\end{lemma}
\begin{proof}
...
\end{proof}

In general there cannot be an upper bound. To see this consider the simple case of two functions $g_1$ and $g_2$, both of which are realizable. E.g., both functions might be piece-wise linear and can therefore be exactly represented by a sufficient number of nodes with ReLU activation functions. Then the sum is also a piece-wise function but there might simply not be enough nodes in the hidden layer to represent the resulting functions exactly.

\section{Old stuff}

If we represent this change of derivative by a single node then we know that we can assign $\sqrt{a}$ to the outgoing weight of this node and $\sqrt{a}$ to the incoming weight of this node. This corresponds to a network where all outgoing weights are zero except one that is $\sqrt{a}$ (and the same for the incoming weights). But we can also take the weight $\sqrt{a}$ and \say{spread it out} over the $k$ nodes in any fashion we want, assuming only that the sum of squares of the weights equals $a$. In fact, take a vector $\vv$ of length $k$ of Euclidean norm squared equal to $1$. Let the vector have components $\vv_i$ and assign the value $\sqrt{a} \vv_i$ to the outgoing weight of node  $i$ (and of course pick the bias terms accordingly and also the incoming weights). Then together all these $k$ nodes will represent the same change in the derivative and the complexity will also be the same. In principle all the components $\vv_i$ should be non-negative so that we are working in the positive orthant. In order to avoid this complication let us assume that we have the following further over-parametrization and that along each connection we not only have the weight but we have in addition the possibility to multiply by an element of $\{\pm1\}$ \comm{GG}{Does this plus minus change anything}. In this way we have no restriction on the sign of the weight. Of course, the two models have equal expressive power. For the rest of this section we will assume this model.

In words, if we limit ourselves to the outgoing weights then we have a $k$-dimensional sphere of radius $\sqrt{a}$, where every single point on this sphere represents $f_1$ exactly. Further, if we do not insist that $f_1$ is represented exactly but allow a small deviation then we can extend the sphere to a spherical shell. Every point in this shell gives an approximate representation of $f_1$, each having roughly the same $\|\theta_w\|^2$. Figure~\ref{fig:sphericalshell} depicts this situation for the case $k=3$. In order to show the spherical shell the sphere is cut open.
\begin{figure}[tb]
\begin{center} \includegraphics[width=8cm]{eps/sphericalshell.eps}
\end{center}
\caption{Spherical shell of parameters that approximately represent $f_1$. The blue surface shows the sphere of parameters that give exact representations. The green and the orange surface indicate the outer and the inner boundary of the spherical shell giving approximate representations.}
\label{fig:sphericalshell} 
\end{figure}

What is the volume of all these vectors $\theta$? (Recall, to simplify our current presentation we limit $\theta$ to the outgoing weights.) As discussed, the exact representations lie on the surface of a $k$-dimensional sphere of radius $\sqrt{a}$. The area of this sphere is equal to $
    a^{\frac{k-1}{2}} \frac{2 \pi^{k/2}}{\Gamma(k/2)}$.
Thus the probability is proportional to
\begin{align}
    a^{\frac{k-1}{2}} \frac{2 \pi^{k/2}}{\Gamma(k/2)} \cdot \e  \cdot e^{ - \frac{1}{ 2\sigma_w^2} a}. \label{eq:probabilityofg1}
\end{align}
We arrive at this expression by starting with the surface area, multiplying this by the \say{thickness} of the spherical shell, which we call $\e$ and finally multiplying with the \say{height} of the density inside the shell, which is proportional to $e^{-\frac{1}{2\sigma_w^2}a}$ as discussed. 

Taking $-\log$ of \eqref{eq:probabilityofg1} we get
\begin{equation}\label{eq:onechange}
\approx \frac{a}{2\sigma_w^2}  -\frac{k}{2} \log(a) + k \log(k) - \log(\e). 
\end{equation}

Note that the term $-\log(\e)$ is the term responsible for the fact that the limiting sharp complexity is in this case $\chi^\#(g) = 1$. We claim that in some regimes of interest \eqref{eq:onechange} is approximately $\frac{a}{2\sigma_w^2}$. This happens when $a \gg k$ and...
\com{We kind of want $\sigma_w^2 = \frac{a}{k-1}$ so that the chi distribution has maximum at $\sqrt{a}$. Or maybe $\leq$? But there's also an interplay of $\sigma_\text{alg}^2$ with $\sigma_e^2$. The starting point is $\sigma_e^2$ I think. Then we should kind of say that $\sigma_w^2 \approx \sigma_e^2$ and go from there. But then we would like to have $\sigma_e^2 \leq \frac{a}{k-1}$}

\subsection{Old}

Let us write this in the form 

\begin{comment}

\begin{align*}
    4 (\sigma_w^2\pi)^{k/2} \underbrace{\left(\frac{(2a)^{k/2-1} e^{-\frac{(2a)}{ 2\sigma_w^2}}}{
    (2 \sigma_w^2)^{k/2} \Gamma(k/2)} \right)}_{(*)} \cdot \epsilon \sqrt{a} \cdot e^{-\frac{1}{ \sigma_w^2} A(f, \samp)}.
\end{align*}

Recall that the density of a chi-square distribution with $k$ degrees of freedom, i.e.,
the distribution of the sum of the squares of $k$ independent Gaussians, has the form
\begin{align*}
\frac{x^{k/2-1} e^{-x/2}}{2^{k/2} \Gamma(k/2)}.
\end{align*}
Further recall that this density is unimodal for $k>1$, i.e., it has a unique maximum and it first increases up to this maximum and then decreases back to zero thereafter. The maximum is at $x=k-2$.

Perhaps we should use instead the chi-distribution that has density 
\begin{align*}
\frac{x^{k-1} e^{-x^2/2}}{2^{k/2-1} \Gamma(k/2)}.
\end{align*}
This is the density of the square root of the sum of the squares instead of the sum of the squares. In this case our density can be written as 

\end{comment}

\begin{align*}
    2 \sqrt{2} (\sigma_w^2\pi)^{k/2} \underbrace{\left(\frac{\sqrt{2a}^{k-1} e^{-\frac{\sqrt{2 a}^2}{ 2\sigma_w^2}}}{\sigma_w^2
    (2 \sigma_w^2)^{k/2-1} \Gamma(k/2)} \right)}_{(*)} \cdot \epsilon.
\end{align*}
\comm{GG}{Theres a weird sigmaw inside the denominator}
Recall that the density of a chi-distribution with $k$ degrees of freedom, i.e.,
the distribution of the square root of the sum of the squares of $k$ independent Gaussians, has the form
\begin{align*}
\frac{x^{k-1} e^{-x^2/2}}{2^{k/2-1} \Gamma(k/2)}.
\end{align*}
Further recall that this density is unimodal for $k>1$, i.e., it has a unique maximum and it first increases up to this maximum and then decreases back to zero thereafter. The maximum is at $x=\sqrt{k-1}$.

%The mode is at ${\sqrt{k-1}}$. Now we do not have the annoying factor. So want to have $\sqrt{k-1} = \sqrt{2 a}/\sigma$.

We conclude that $(*)$ corresponds to a scaled chi-distribution with $k$ degrees of freedom where instead of zero-mean unit-variance Gaussians we pick zero-mean Gaussians with variance $\sigma_w^2$. Note that such a scaled chi-distribution takes on its maximum value at $\sqrt{k-1} \sigma_w$. The standard scaling of the variance of the weights is $\sigma_w^2 \propto 1/k$. For simplicity let's assume that $\sigma_w^2 = 1/(k-1)$ \comm{GG}{is it ok?}. Then the maximum value is attained at $1$.

%Let us therefore assume that we set $\sqrt{2 a} = \sqrt{k-1} \sigma_w$, or $\sigma_w^2 = (2 a)/(k-1)$. Note that in practice we do not know the variational complexity of the function. But this choice can always be achieved by performing a one-dimensional hyper-parameter search. Note further that these rules leads to the standard scaling of the variance of the weights, namely $\sigma_w^2 \propto 1/k$.

Taking $-\log$ of \eqref{eq:probabilityofg1} we get that
\begin{align*}
\chi^\#(\dist_\x, g_1, \e) 
&\leq \frac{a}{2\sigma_w^2} -\frac{k-1}{2}\log(a) - \log(\frac{2\pi^{k/2}}{\Gamma(k/2)}) - \log(\e') \\
&= \frac{C(g_1)}{2\sigma_w^2} - \poly(k,\log(a),\log(\e')) \\
&= \frac{(k-1)C(g_1)}{4} - \poly(k,\log(a),\log(\e'))
\end{align*}
Why do we have an inequality instead of equality in the first line? The reason is that
there can be other functions that still count towards the probability but are of different form. We will show however that these other functions contribute negligibly to $\chi^\#$.

So consider a second function, call it $f_d$ which in addition to the one change of its derivative also has some other \say{wiggles}, so that the total number of slope changes is $d$. Necessarily its complexity is higher, call it $a'$. Consider the probability of functions close to $f_d$. Recall that $a'>a$. Hence the squared radius of the sum of the squares of the outgoing weights \comm{GG}{Is radius $a$ or $2a$} representing $f_d$ is $a'>a$. Thus the density of $\alpha(\theta)$ is smaller for these functions than for $g_1$. But there is a second, even more important, factor that reduce the probability of $f_d$ compared to $g_1$. Not all the points on the sphere of square radius $a'$ correspond to exact representations of $f_d$. In fact, the points that correspond to exact representations of $f_d$ lie on a lower dimensional sub-manifold, reducing the probability further significantly. We claim that this probability has the form
\begin{align*}
   S(f, k-d) \cdot e^{- \frac{2 a}{ 2 \sigma_w^2}} \cdot \epsilon^{d} \cdot e^{-\frac{1}{ \sigma_w^2} A(f, \samp)},
\end{align*}
where $S(f, k-d)$ denotes the area of the subset of the sphere in $k$ dimensions that represents this function exactly, seen as a $k-d$ dimensional manifold. 

All this is easiest seen by looking at the case $k=3$. As we discussed, for $f_1$ the set of exact representations forms a sphere in $\R^3$ since all three nodes can be used to implement this change but there is one global constraint. This leads to the spherical shell of approximate representations as seen in Figure~\ref{fig:sphericalshell}. Next consider $f_2$, which contains one more change of the derivative. At least one of the three nodes will be needed to implement this second change of the slope, leaving only two nodes to implement the main change of the derivative.
So the exact representations will now lie on a slightly bigger sphere of squared radius $2 a'$ in $\R^3$ and they will correspond to circles of squared radius $2 a$ (each such of the six circles corresponds to picking two of the three nodes and implementing the change of the slope already present in $f_1$ via these two nodes, hence a circle since we can freely distribute the weight between these two nodes). If we \say{expand} one such circle by allowing representations \say{close by} we get a torus.
Hence the approximate implementations will now correspond to tori that are embedded on the surface of a sphere. This is shown in Figure~\ref{fig:tori}. As mentioned, the sphere itself is slightly larger, due to the larger complexity of the function. \comm{GG}{Here we kind of need that chi distribution business} But as we discussed, even if we took the whole spherical shell that corresponds to this slightly larger sphere it would contain less probability. In addition, we only take a small subset of the probability, due to the extra combinatorial constraint. In summary, if we assume that we sample from the posterior distribution, there is a strong regularization term that gives preference to \say{simple} functions, hence avoiding over-fitting.

\begin{figure}[tb]
\begin{center} \includegraphics[width=8cm]{eps/tori.eps}
\end{center}
\caption{The six tori indicate the region of the parameter space (outgoing weights) that approximately represent the function $f_2$, a function with two changes of the derivative. Note that the exact representations correspond to six circles, i.e., it has dimension $1$, whereas for a single change of the derivative it has dimension $2$. If we want to implement a function with three changes of the derivative, then the exact representations are points (dimension $0$).}
\label{fig:tori} 
\end{figure}

So far we considered a simplified model where we only looked at the outgoing weights. But not much changes if we look at the outgoing {\em and} incoming weights.  The manifold that contains the exact representations is now embedded in $2k$-dimensional instead of $k$-dimensional space since
for each node the incoming weight is identical to the outgoing weight. If we apply a suitable unitary transform we see that we can get to essentially the original case but where now all the coordinates that are involved in the manifold are stretched by a factor $\sqrt{2}$, and of course we are in an ambient space of dimension $2k$ instead of $k$. E.g., if we consider the function $f_1$ then 
the associated posterior probability of all it's approximate realization is proportional to  
\begin{align*}
   (2a)^{\frac{k-1}{2}} \frac{2 \pi^{k/2}}{\Gamma(k/2)} \cdot \epsilon^{k+1} \cdot e^{-\frac{1}{ \sigma_w^2} A(f, \samp) - \frac{1}{ \sigma_w^2} a}.
\end{align*}

Let us write this as
%\begin{align*}
%    \frac{2\Gamma(k) (\sigma_w^2)^k \pi^{k/2}}{\Gamma(k/2)}   \underbrace{\left( \frac{(2 a)^{k-1} e^{-\frac{(2a)}{ 2\sigma_w^2}}}{{
%    (2 \sigma_w^2)^{k} \Gamma(k)}} \right)}_{(**)} \cdot (\frac{\epsilon}{\sqrt{2a}})^{k-1} 
%    \epsilon^2 
%    e^{-\frac{1}{ \sigma_w^2} A(f, \samp)}.
%\end{align*}
\begin{align*}
    2(2\sigma_w^2)^{k/2-1} \pi^{k/2}   \underbrace{\left( \frac{(\sqrt{2 a})^{k-1} e^{-\frac{(2a)}{ 2\sigma_w^2}}}{{
    (2 \sigma_w^2)^{k/2-1}} \Gamma(k/2)} \right)}_{(**)} \cdot 
    \epsilon^{k+1} 
    e^{-\frac{1}{ \sigma_w^2} A(f, \samp)}.
\end{align*}
%Recall that the density of a chi-distribution with $k$ degrees of freedom is
%\begin{align*}
%\frac{x^{k-1} e^{-x^2/2}}{2^{k/2-1} \Gamma(k/2)}.
%\end{align*}

$$S = \{[0,b_{i_{\text{min}}}], [b_{i_{\text{min}}},b_{i_{\text{min} + 1}}], \dots, [b_{k-1},b_k],[b_k,1]\}.$$ 
Define $S_b := \{[l,r] \in S : r - l < |W(l)| \}$. By an averaging argument there exists $[l_0,r_0] \in S$ such that $r_0-l_0 \geq \frac{1}{k+1}$. Thus if $S_b = S$ then $\|f_\theta\|^2 \geq \frac{1}{12(k+1)^5}$ by Lemma~\ref{lem:l2lwrbnd} and the definition of $S_b$. This contradicts the assumption on $\|f_\theta\|^2$, which implies that $S_b \neq S$. Let $[l^*,r^*] \in S \setminus S_b$.

Let $\{[l_0,r_0],[l_1,r_1], \dots, [l_i,r_i]\} \subseteq S_b$ be a maximal continuous subset of intervals in $S_b$. That is, for all $j \in [i]$, $r_j = l_{j+1}$ and the intervals ending at $l_0$ and starting at $r_i$ are not in $S_b$. If $[l_0,r_0]$, is on the left of $[l^*,r^*]$, i.e. $r_0 \leq l^*$, then perform the following: for all $b_j \in [l_0,r_i]$ set $b_j \leftarrow l_0$. If $[l_0,r_0]$, is on the right of $[l^*,r^*]$, i.e. $l_0 \geq r^*$, then perform the following: for all $b_j \in [l_0,r_i]$ set $b_j \leftarrow r_i$. We do this operation for all maximal, continuous subsets of $S_b$. This finishes the first phase. Call the resulting vector of parameters $\theta^1$.
