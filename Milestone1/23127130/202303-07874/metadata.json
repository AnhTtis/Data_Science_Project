{
    "arxiv_id": "2303.07874",
    "paper_title": "Bayes Complexity of Learners vs Overfitting",
    "authors": [
        "Grzegorz GÅ‚uch",
        "Rudiger Urbanke"
    ],
    "submission_date": "2023-03-13",
    "revised_dates": [],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "stat.ML"
    ],
    "abstract": "We introduce a new notion of complexity of functions and we show that it has\nthe following properties: (i) it governs a PAC Bayes-like generalization bound,\n(ii) for neural networks it relates to natural notions of complexity of\nfunctions (such as the variation), and (iii) it explains the generalization gap\nbetween neural networks and linear schemes. While there is a large set of\npapers which describes bounds that have each such property in isolation, and\neven some that have two, as far as we know, this is a first notion that\nsatisfies all three of them. Moreover, in contrast to previous works, our\nnotion naturally generalizes to neural networks with several layers.\n  Even though the computation of our complexity is nontrivial in general, an\nupper-bound is often easy to derive, even for higher number of layers and\nfunctions with structure, such as period functions. An upper-bound we derive\nallows to show a separation in the number of samples needed for good\ngeneralization between 2 and 4-layer neural networks for periodic functions.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07874v1"
    ],
    "publication_venue": null
}