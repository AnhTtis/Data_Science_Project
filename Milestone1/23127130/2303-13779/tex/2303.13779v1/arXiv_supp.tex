
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\makeatletter
\renewcommand\@makefnmark{\hbox{\@textsuperscript{\normalfont\color{black}\@thefnmark}}}
\renewcommand\@makefntext[1]{%
  \parindent 1em\noindent
            \hb@xt@1.8em{%
                \hss\@textsuperscript{\normalfont\@thefnmark}}#1}
\makeatother
\onecolumn{
%%%%%%%%% TITLE - PLEASE UPDATE
\begin{center}
\title{\vspace{+0.5cm}\Large{\textbf{Supplementary material for \\Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR \\}}}
\vspace{+0.6cm}
\author{
    Aneeshan Sain\textsuperscript{1,2}  \hspace{.2cm}
    Ayan Kumar Bhunia\textsuperscript{1} \hspace{.3cm}
    Subhadeep Koley\textsuperscript{1,2}  \hspace{.2cm}
    Pinaki Nath Chowdhury\textsuperscript{1,2}  \hspace{.2cm}\\
    Soumitri Chattopadhyay\footnote{Interned with SketchX}  \hspace{.2cm}
    Tao Xiang\textsuperscript{1,2}\hspace{.2cm}  
    Yi-Zhe Song\textsuperscript{1,2} \\
    \textsuperscript{1}SketchX, CVSSP, University of Surrey, United Kingdom.  \\
    \textsuperscript{2}iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.\\
    {\tt\small \{a.sain, a.bhunia, p.chowdhury,  t.xiang, y.song\}@surrey.ac.uk} 
}
\end{center}
\vspace{+0.2cm}
}

\renewcommand\thesection{\Alph{section}}
\setcounter{section}{0}

\section{Alternative to Triplet Loss} 
Factually, for photos, intra-modal triplet loss is a self-supervised objective. For a cross-modal problem such as ours however, triplet loss can offer some leeway in better conditioning the joint photo-sketch embedding \cite{yu2016sketch,song2017deep,bhunia2020sketch}. Empirically, when compared with contrastive loss \cite{chaudhari2019attentive} as a self-supervised objective (while keeping everything else the same), triplet loss performs better (45.68\% on ShoeV2) further justifying our case.

\section{On the need for Knowledge Distillation}
During student-training, three objectives are learnt -- cross-modal separation, intra-modal separation between photos and that between sketches. As learning everything together is difficult, we decouple the process by first training a teacher completely with intra-modal triplet loss on photos, and then use the trained teacher's photo discrimination knowledge to better guide the student (FG-SBIR) during its training. 

\section{On using additional datasets} 
TU-Berlin and QuickDraw are sketch-only datasets designed towards sketch classification. Some \cite{collomosse2019livesketch,dey2019doodle} did augment them for \textit{category-level }SBIR \cite{bui2018deep,dutta2019semantically}, by sourcing unpaired photos. These however do not work for our instance-level setting - we need instance-level sketch-photo correspondences. The idea of abstraction-influence is very interesting, which shall be considered as a future work. 

\section{Dealing with scarcity of sketch-data} 
Distilling from unlabelled photos is beneficial as they are abundantly available, unlike sketches that require time and human effort to collect \cite{bhunia2021more}. On distilling from only sketches there is minimal increment from teacher supervision (44.51\% vs. 44.18 \% on ShoeV2) as compared to that from photos (48.35\% vs. 44.18\% on ShoeV2). Faithful sketch-generation in photo-to-sketch generation tasks is challenging; it's difficult to quantify its generation-quality, and they hardly generalise to human-sketches \cite{bhunia2021more}. Using CLIPasso \cite{vinker2022clipasso} for sketch-generation instead of teacher-supervision, hence delivers a poor result of 38.57\% on ShoeV2. Although works have explored augmenting sketches via stroke-dropping/deformations \cite{yu2016sketch}, or as line-drawings \cite{chan2022learning}, resulting sketches mostly follow edge-maps thus being less reliable. On using \cite{chan2022learning} instead of teacher-supervision, we obtained a poorer result of 39.23\% compared to our 48.35\% on ShoeV2, thus proving our method to be simpler and more efficient.

\section{Clarity on training teacher} The teacher comprises an ImageNet pre-trained PVT backbone trained on 60,502 additional photos from Sketchy (ext) \cite{liu2017deep} for Sketchy; 50,025 photos of UT-Zap50k \cite{yu2014fine} for ShoeV2 \cite{yu2016sketch}; and 7,800 photos from websites like IKEA, etc \cite{pang2020solving} for ChairV2 \cite{sain2020cross}.

\section{Optimisation for multi-task objectives} 
We used the available toolbox of \textit{WandB Sweeps} for quick tuning of hyper-parameters, which provided 48.35\% Acc@1 on ShoeV2. Even on using complex loss balancing approach of \cite{groenendijk2021multi}, we obtain a close 47.94\%. Furthermore, changing the hyper-parameter values by $\pm10\%$, causes a mere $\pm0.5\%$ change in Acc@1 on ShoeV2 \cite{yu2016sketch}. This proves that our method despite needing five loss objectives, is quick to tune, thus being easily reproducible.

\section{Clarity on training stability} 
\noindent $\bullet$ Learning rate decay: We used exponential rate decay with initial learning rate of 0.001 and decay factor of 0.2.\\
$\bullet$ Large batch-size: We used 256 batch-size via gradient accumulation \cite{patel2022recall} on 4 V100-GPU machines.\\
$\bullet$ Reducing augmentations: We used augmentation (random horizontal flipping only) on just 30\% of training data.\\
Plots below show the above methods' implementation on top of CNN-Baseline. Least gittering in \textit{Ours} shows our EMA approach to be superior.

\begin{figure}[!hbt]
    \includegraphics[width=\linewidth]{figures/transformer_graph_4.png}
    \vspace{-0.6cm}
    \caption*{Evaluation accuracy at every $100^\text{th}$ training-step [Best if zoomed]. 
   }
   \label{fig:plot_instability}
   \vspace{-0.4cm}
\end{figure}


\section{Further clarity on experimental results}
As we intended to show PVT \cite{wang2021pyramid} is a better backbone than the earlier CNN-based ones, we compared prior state-of-arts to our method using different backbones. Furthermore, for the methods having code available, we replaced their backbones with PVT, only to obtain inferior results (32.68\% for \cite{yu2016sketch} and 34.12\% for \cite{song2017deep}), thus proving ours as better. Furthermore,  our method surpasses by 8.33\% on ShoeV2, against a contemporary method of TC-Net \cite{lin2019tc}, despite having a lesser complexity of training and simpler loss objectives than the latter.



% ------------------------------------------------------------------------
% (iii) Citation-version will be corrected, and results of TC-Net [\citegreen{37}] shall be added along with discussions upon acceptance. -------- sensitive
% Our method outperforms TC-Net [\citegreen{37}] by 8.33\% on ShoeV2, despite having a lesser complexity of training and simpler loss objectives.
%-------------------------------------------------------------------------



% {\small\noindent \textbf{\\References:\\} 
% % \\\textbf{[A]} Liu et. al. ``Semantic-Aware..." ICCV'19.
% % \\\textbf{[B]} Wang et. al. ``Transferable..." T-PAMI'21.
% \noindent \textbf{[A]} Yash Patel et. al. ``Recall@k Surrogate Loss with Large Batches and Similarity Mixup" CVPR'22.
% \\\textbf{[B]} Yael Vinker et. al. ``CLIPasso: Semantically-Aware Object Sketching" SIGGRAPH'22.
% \\\textbf{[C]} Caroline Chan et. al. ``Learning to generate line drawings that convey geometry and semantics" CVPR'22.
% \\\textbf{[D]} Rick Groenendijk et. al. ``Multi-Loss Weighting with Coefficient of Variations" WACV'21.
% \\\textbf{[E]} Hangyu Lin ``Tc-net for isbir: Triplet classification network for instance-level sketch based image retrieval" WACV'21.
% }


% \end{document}
