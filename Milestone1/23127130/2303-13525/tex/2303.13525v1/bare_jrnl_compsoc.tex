


\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{makecell}








% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
\usepackage[numbers]{natbib}





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex

\usepackage{xurl}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Uncertainty-Aware Workload Prediction in Cloud Computing}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Andrea~Rossi,
        Andrea~Visentin,%~\IEEEmembership{Member,~IEEE,}
        ~Steven~Prestwich,%~\IEEEmembership{Member,~IEEE,}
        ~and~Kenneth~N.~Brown%~\IEEEmembership{Member,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem A. Rossi is with the Centre for Research Training in Artificial Intelligence, University College Cork, Ireland.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: a.rossi@cs.ucc.ie
\IEEEcompsocthanksitem A. Visentin, S. Prestwich and K. N. Brown are with the Insight Centre for Data Analytics, University College Cork, Ireland.\protect\\
E-mail: a.visentin@ucc.ie, \{s.prestwich, k.brown\}@cs.ucc.ie}}% <-this % stops an unwanted space

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers

% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}

% Predicting future demand in Cloud Computing is essential for managing a Cloud data centre and guaranteeing a minimum Quality of Service (QoS) level to the customers. Modelling the uncertainty of the future resource demand leads to a more accurate quality of the prediction and a reduction of the waste due to overallocation. In this paper, we propose Bayesian deep learning models trained with multiple datasets to predict the distribution of the future demand of processing units and memory and model the uncertainty in the context of cloud workload prediction. We compare them to the same models trained with one single dataset for both univariate and bivariate cases, showing that more data help the model to generalize and make it more accurate. Moreover, we provide an ablation study of a bivariate model trained with multiple datasets and compare the results to the univariate counterpart to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate the effect of the transfer learning approach in the context of cloud workload prediction.
% Results show the benefits of training the networks with multiple datasets, without having seen the same data before for both the univariate and bivariate cases, with the advantages of using pretrained models to predict related but unseen time series.
% For this study, we preprocessed in a consistent and detailed way 12 datasets from the public Google Cloud and Alibaba Cloud Computing systems' traces that we used for the experiments and made them available to facilitate the research in this field. 

% Predicting future demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing a minimum Quality of Service (QoS) level. Modelling the uncertainty of the future resource demand leads to a more accurate quality of the prediction and a reduction of the waste due to overallocation. In this paper, we propose Bayesian deep learning models trained with multiple datasets to predict the distribution of future resource demand and model its uncertainty. We compare them to the version trained with one single dataset for univariate and bivariate cases, showing that more data help the model generalize and make it more accurate. Moreover, we design extensive experiments, providing an ablation study of a bivariate model trained with multiple datasets and comparing the results to the univariate counterpart to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate the transfer learning effect to assess this approach's applicability in the context of cloud computing.
% Experimental results show the benefits of training the networks with multiple datasets without having seen the same data before, with the advantages of using pretrained models to predict related but unseen time series.

Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities.
Extensive experiments show that pretraining with multiple datasets boosts performances while fine-tuning does not. Our models generalise well on related but unseen time series, proving transfer learning capabilities. Runtime performance analysis shows that the models are deployable in real-world applications. For this study, we preprocessed twelve datasets from real-world traces in a consistent and detailed way and made them available to facilitate the research in this field.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Bayesian Neural Networks, Cloud Computing, Workload Prediction, Uncertainty, Deep Learning, Transfer Learning.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{T}{he} advent of cloud computing services is relatively new. However, in recent years they have gained enormous popularity because of benefits including reduction of the cost of the business and increase in productivity \cite{muller2015benefits}. These advantages gained even more importance during the COVID-19 pandemic \cite{alashhab2021impact}, where work from home was widely adopted. The advances in Artificial Intelligence (AI) and Big Data have led more companies to use cloud computing systems, increasing their demand. Workload forecasting and scheduling play a fundamental role in the cost of operating the data centres, but predicting the resource demand is a challenging task \cite{tirmazi2020borg}.
Moreover, data centres have a high environmental impact. In this regard, the energy consumption of data centres will grow from 292 TWh in 2016 to 353 TWh in 2030 due to the increase in the number of users \cite{koot2021usage} and, if it is left uncontrolled, the greenhouse gas emission due to ICT technologies might increase over 14\% in 2040, compared to 1-1.6\% from 2007 to 2016 \cite{belkhir2018assessing}. Cloud computing providers aim to preconfigure the machines to guarantee a high Quality of Service (QoS). The benefits of predicting future demand include better resource utilisation and a reduction of the overallocation with the opportunity of serving more customers, which leads to an increase in profit and an overall decrease in energy consumption, CO2 emission and maintenance costs. 
Machine learning (ML) and deep learning (DL) approaches have been widely used to forecast future demand in the cloud environment \cite{xi2021attention, zhang2018efficient, kumar2021self}, with a focus on probabilistic and uncertainty aspects as well \cite{minarolli2014cross, minarolli2017tackling}.

In this paper, we evaluate the performance of probabilistic forecasts based on Hybrid Bayesian Neural Networks (HBNNs), built upon our previous work \cite{rossi2022bayesian}. We consider bivariate time series to predict processing units and memory usage simultaneously, training the models using multiple datasets to increase our model's generalisation capabilities and analyse the impact of Bayesian components in the network. We assess the performance of the predictive models using four publicly available datasets from Google Cloud \cite{reiss2011google, clusterdata:Wilkes2020a} and Alibaba clusters \cite{jiang2020characterizing, weng2022mlaas}. We preprocess them to overcome the inconsistency and often undetailed versions in the literature for replicability.

Moreover, we investigate the transfer learning (TL) approach to predict the future workload of traces that are not part of the training set. To the best of our knowledge, this is the first time that TL has been applied in the context of workload prediction in cloud computing.

The contributions in this paper are as follows:
\begin{itemize}
    \item We preprocess and make available twelve different datasets of the resource demand divided into windows of 5 minutes from cluster cells of four public traces from Alibaba and Google Cloud Computing systems. These are the most used datasets in the cloud workload computing literature.
    \item We validate the results by applying the model to multiple datasets and extending them to variations of the model, which includes bivariate predictions.
    \item We provide a comparison of a bivariate model to the univariate counterpart, training with one or more traces, showing the importance of the training with multiple datasets.
    \item We investigate the generalisation capabilities of the model with a deep analysis of the training scenarios in the context of data-driven applications for workload prediction in Cloud Computing, focusing on the TL approach.
    \item All the investigations are centred on the probability distribution estimation and modelling of the uncertainty of the prediction, with a focus on the epistemic uncertainty and, specifically, the data variability problem (distributional representation of the training set).
\end{itemize}

The remainder of the paper is structured as follows. Section \ref{sec:RelatedWork} introduces related work. We describe the baseline and the proposed models in Section \ref{sec:predmodels}. Section \ref{sec:TrainingStrat} presents the training scenarios that are compared in this paper. Section \ref{sec:experiments} describes the experiments' methodology and discusses the proposed methods' results based on real-world load traces data. Section \ref{sec:conclusion} concludes the paper and discusses the future works.
% \newpage

% \IEEEPARstart{F}{indings summary:}
% \begin{itemize}
%     \item \textbf{Extensions}
%     \begin{itemize}
%         \item For each combination, each model has the same accuracy in terms of MSE/MAE (Diebold-Mariano statistical test)
%         \item Models trained with multiple datasets outperform models trained on single datasets for both resource efficiency and accuracy.
%         \item The single bivariate model has not enough data to be trained.
%         \item In particular, for the bivariate prediction, the models trained with multiple datasets achieve performance similar to the univariate, improving the performance up to around 25\% in terms of total predicted resources for both CPU and memory demand.
%         \item Experiments confirm that the univariate prediction is an easier task compared to the bivariate version.
%     \end{itemize}
%     \item \textbf{Transfer Learning}
%     \begin{itemize}
%         \item Transfer learning is effective also for the prediction of unseen data in Cloud workload prediction.
%         \item Using multiple datasets for the training allows the model to achieve better performance.
%         \item It is better to use datasets from the same probability distribution (in terms of resource prediction, but not accuracy).
%         \item For both accuracy and resource efficiency, the fine-tuning (FT) leads to overfitting (performance degrades).
%         \item However, fine-tuning from a pretrained model is better than training a model from scratch using a random initialization
%     \end{itemize}
% \end{itemize}


% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)


% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


\section{Related Work}\label{sec:RelatedWork}

In cloud computing management, workload prediction plays an important role that has been broadly studied over the past 20 years. Starting with statistical methods such as Autoregressive Integrated Moving Average (ARIMA) by Calherios \textit{et al.} \cite{calheiros2014workload}, in the last few years, research has focused on ML and DL approaches that have been shown to outperform statistical methods. For this reason, this section is focused on ML and DL techniques. Some of the work is centred on prediction at the machine level, others at the cluster level and design and evaluate models in a wide variety of datasets for univariate and multivariate forecasts.

A large selection of ML methods has tackled cloud workload forecasting. Khan \textit{et al.} \cite{khan2012workload} combined clustering algorithms to group Virtual Machines (VMs) with similar patterns and used Hidden Markov Modelling to forecast changes in workload patterns. Banerjee \textit{et al.} \cite{banerjee2021efficient} proposed a multi-step-ahead prediction framework composed of a set of supervised learning approaches such as Linear Regression, k-Nearest Neighbours, Support Vector Regressor (SVR), Gradient Boosting and decision tree. The authors applied a prediction-based VM placement algorithm to minimise resource utilisation and power consumption. Kim \textit{et al.} \cite{kim2020forecasting} designed an ensemble model based on eight ML predictors for an online forecast to reduce the Service Level Agreement violations. The experiments are run on Google Cloud Trace 2011 and Facebook Hadoop trace. 

With the advent of DL, many architectures have been investigated. Leka \textit{et al.} \cite{leka2021hybrid} implemented a hybrid neural network with a Convolutional Neural Network (CNN) followed by a Long Short-Term Memory (LSTM), which improved the accuracy compared to the single components alone in a real-world dataset with 12 samples of VMs with the most common workload patterns. Dang-Quang \textit{et al.} \cite{dang2021multivariate} proposed a multivariate Bidirectional LSTM (Bi-LSTM), which improves the accuracy of the prediction w.r.t. the univariate counterpart using a real trace workload dataset from the Delft University of Technology called GWA-T-12 Bitbrains. This is due to the strong correlation between the CPU and HTTP workload traces, which do not hold in our traces for processing unit and memory demand. Ouhame \textit{et al.} \cite{ouhame2021efficient} proposed a hybrid multivariate CNN and LSTM model which performs better than ARIMA-LSTM, Vector Autoregressive GRU and Vector Autoregressive Multilayer Perceptron models on Bitbrains dataset. Huidan Xi \textit{et al.} \cite{xi2021attention} implemented an attention-based LSTM and compare it with an LSTM without the attention mechanism. They predicted the CPU workload at the machine level over three machines on the Alibaba CDC dataset, showing that attention improves accuracy. Qian \textit{et al.} \cite{qian2021attention} evaluated the performance of an encoder-decoder model based on the attention mechanism at the machine level over a sample of 1,024 machines from the Google Cloud trace 2011. They compared their model to a standard LSTM and an Echo State Network at different prediction steps, showing that their model outperformed the baseline. Patel \textit{et al.} \cite{patel2022hybrid} predicted CPU usage with a hybrid prediction method composed of 1-dimensional convolution (1DConv) and LSTM on three real-world datasets, Google Cloud trace 2011, Alibaba 2018 and Bitbrains. The first part of the model combines three different CNN blocks, which capture patterns from dilated versions of the trace. The model outperformed the sequential versions of the network with one single CNN block. Ruan \textit{et al.} \cite{ruan2022cloud} proposed a turning point-based trend prediction using a cloud feature-enhanced deep learning model. The model is based on LSTM components and applied to real-world datasets by Google Cloud trace 2011. Karim \textit{et al.} \cite{karim2021bhyprec} implemented a multi-step-ahead prediction model called BHyPreC, which combined a Bi-LSTM on the top of the stacked LSTM and GRU blocks, outperforming baselines such as ARIMA, LSTM, GRU and Bi-LSTM. The authors showed that combining different RNN components enhances the accuracy of CPU workload prediction.

Combining the architectures mentioned above to build ensemble models has also become trendy in cloud workload prediction. This is because different models can capture various aspects of the trace to improve the accuracy of the forecast. Valarmathi \textit{et al.}\cite{valarmathi2021resource} proposed an ensemble of Random Forests (RF) followed by an LSTM for predicting the CPU utilisation of VMs in the Alibaba 2018 dataset. It outperformed ML models such as linear regression, SVR, Gradient Boosting, RF and Gaussian Process Regression. An outlier detection mechanism is applied to the RF ensemble's output before training the LSTM module. Yazdanian \textit{et al.} \cite{yazdanian2021e2lg} proposed a hybrid model named E2LG, which decomposes first the workload time series into components of different frequency bands and then they use an ensemble of Generative Adversarial Network and LSTM to predict each sub-band. In this architecture, LSTM blocks are used as generators, and 1DConv blocks are used as a discriminator. The model is experimentally tested on HTTP workload datasets for both one-step-ahead and multi-step-ahead predictions.

The research has also moved on to the probabilistic aspects of time series forecasting in recent years. In our previous work \cite{rossi2022bayesian}, we extend the DL model to a probabilistic approach employing an HBNN that captures the epistemic and aleatory uncertainty of the prediction. We showed the advantages of forecasting a probability distribution in contrast to a point estimate but limited to univariate forecast and using only one dataset at a time in the training phase. While the aleatory uncertainty cannot be eliminated, more training data can reduce epistemic uncertainty \cite{hullermeier2021aleatoric}. Salinas \textit{et al.} \cite{salinas2020deepar} designed a probabilistic DL method called DeepAR trained on large related time series (electricity demand and traffic forecast), reaching state-of-the-art performance.

Another common technique used for improving the accuracy of the predictive models is transfer learning (TL). This technique aims to learn a task (target domain) by transferring the knowledge of another model trained for a different but related task (source domain). Generally, it can also apply using a pretrained model to predict related but unseen datasets \cite{sarkar2018hands}. In this context, the source domain comprises the datasets used in the pretrained model, while the unseen datasets are the target domain. Fawaz \textit{et al.} \cite{fawaz2018transfer} investigated the TL approach in the context of time series classification showing that the performance can improve or degrade according to the dataset used for the transfer. Hao \textit{et al.} \cite{hao2021transfer} built a QoS Bayesian Network to efficiently estimate the QoS of VMs by quantifying the uncertain relationships among their relevant features on Alibaba published datasets. Khan \textit{et al.} \cite{khan2022workload} applied TL to clustering algorithms to estimate the energy state of the VMs. 

We surveyed a total of 72 works on Cloud Computing Workload. For the sake of brevity, we did not include them all. None of them used the Google Cloud Trace 2019, and Alibaba Cluster traces 2020 in the context of workload prediction. A GPU trace from the latter is also available, which has become essential in DL and AI applications. Most papers use Google Cloud Trace 2011, Alibaba 2018, Bitbrains and other HTTP server traces. Still, the preprocessing steps of the datasets are usually inconsistent or not well described, limiting the possibility of replicating the research.
Furthermore, none of the previous works investigated the concept of transfer learning in the context of probabilistic workload prediction and the generalisation capabilities of DL models exploiting multiple datasets, a milestone step in the era of big data.
    
\section{Predictive Model}\label{sec:predmodels}

In the resource management scheme in a cloud computing environment (see Fig. \ref{fig:setup}), the resource manager is the leading actor, which uses the future demand prediction to configure the VM in the cloud servers. The forecast is provided by the predictive model, which is the focus of this paper. Predictive models are trained based on the historical workload data. Every time a new configuration occurs, the workload history is updated to feed the predicted model with newly available data. We can classify the models based on the training datasets, the prediction type, and the DL architecture used to forecast future demand.


\begin{figure*}[htbp]
\centerline{\includegraphics[width=0.8\linewidth]{images/setup.png}}
\caption{Workload Prediction Scheme}
\label{fig:setup}
\end{figure*}

We propose a wide variety of models. We add a prefix to their name to identify them better.

The model can be trained using one trace (S) from the twelve preprocessed datasets or all of them (M).

Regarding the type of prediction, we can distinguish between univariate (U) models and bivariate (B) models. In the former case, we predict just one resource at a time; in the latter, we simultaneously predict both processing units and memory demand.

The architectures used as a predictive model are an LSTM-based model (LSTM), used as the baseline, while the proposed models are the HBNN and the LSTMD, where D stands for distribution. Each model in the analysis is given by combining these three categories. For instance, M-B-HBNN refers to an HBNN model trained with multiple datasets for a bivariate time series. In the following sections, we describe these models in more detail. A graphical representation of the three compared architectures is depicted in Fig. \ref{fig:architectures}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{images/architectures.png}}
\caption{Network architectures comparison}
\label{fig:architectures}
\end{figure}

\subsection{LSTM}

An LSTM-based model is used as the baseline of our experiments. The network consists of an input layer with a size of 288, corresponding to the past 24 hours of workload. The input size has been found experimentally. The sequence is given to a succession (between one and three) of 1DConv layers followed by an LSTM layer, frequently used to deal with sequential data. The combination of these two types of layers has been proven effective in time series forecasting \cite{leka2021hybrid, patel2022hybrid}. The LSTM is followed by a sequence of dense layers whose number varies with the training set's size and the prediction type. For instance, for the M-B-LSTM, the number of dense layers is 3, while for the S-U-LSTM is 2. The last layer has one single neuron in the case of univariate models or two in the case of bivariate models. The Mean Squared Error (MSE) is used as the metric for the optimization via the Adam algorithm \cite{kingma2014adam}. The training converges in a few hundred epochs, and an early stopping strategy is applied to prevent overfitting. More details about the training/test split can be found in the following sections. The network is implemented in Keras\footnote{\url{https://keras.io/}} and the hyperparameter tuning, which also includes the number of layers in the network, has been performed using the Talos\footnote{\url{https://github.com/autonomio/talos}} optimization library.

\subsection{HBNN}

Similarly to our previous work \cite{rossi2022bayesian}, we design a Bayesian Last Layer network to capture the epistemic uncertainty. The architecture differs from the LSTM model only on the last two layers so that we can understand the impact of the Bayesian layer. The sequential input is fed to one or more 1DConv layers, followed by an LSTM layer. As in the previous model, the LSTM is placed behind a sequence of dense layers and a Bayesian dense layer, which replaces the last dense layer of the baseline. A dense layer follows the Bayesian one with two neurons for the univariate prediction or 4 for the bivariate forecast to predict the mean and the variance of one or two Normal distributions embedded in a distributional layer which is the output of our model. The output distribution captures the aleatory uncertainty given by the noise of the workload demand. The network is optimized by minimizing the loss function, which is the negative log-likelihood between the target demand and the distribution predicted by the model. Except for these differences in architecture and loss function, a similar training strategy to the baseline is applied.
The Bayesian layer and the distribution layer are implemented using TensorFlow probability\footnote{\url{https://www.tensorflow.org/probability}}, the rest of the network is implemented in Keras, and the hyperparameterization has been run in the same way as the LSTM model.

\subsection{LSTMD}

The LSTMD model's architecture is the same as the HBNN, but a standard dense layer replaces the Bayesian dense layer. The output is still a Gaussian distribution in the case of a univariate prediction or two Gaussian distributions in the case of a bivariate prediction. Also, in this case, the weights are optimized with the negative log-likelihood as the loss function.

\section{Training Scenarios}\label{sec:TrainingStrat}

\begin{figure*}[htbp]
\centerline{\includegraphics[width=0.7\linewidth]{images/training4.png}}
\caption{Training scenarios and transfer learning}
\label{fig:ddan}
\end{figure*}

The first part of the experiments focuses on comparing univariate and bivariate predictive models trained with one or more traces. We keep the same training procedure as our previous work \cite{rossi2022bayesian}, with a bigger hyperparameter space, a different scaling and the use of data shuffling to improve the overall performance of the models. Moreover, we preprocess and extend the work to more datasets. Furthermore, we investigate the impact of training a bivariate version of the predictive model in contrast with the univariate case and the benefits of training a deep learning model using multiple datasets.

The second part is focused on the TL approach applied to the M-B-HBNN model, using the same network's architecture (i.e. with the hyperparameters found on the optimisation for the M-B-HBNN model). We apply different training scenarios, including fine-tuning (FT) the network's weights, i.e. starting from the weights of the pretrained model, we proceed with further training of the network on the datasets we want to predict. We can enumerate seven different approaches:
\begin{itemize}
    \item \textbf{All}: the model is trained with all 12 clusters. We then predict the specific datasets without any further training.
    \item \textbf{All FT}: We start from \textit{All}. We then fine-tune the network on the specific dataset before making the prediction. This assesses whether the FT process leads to a better weight configuration for the dataset we want to predict.
    \item \textbf{All-but-one}: the model is trained with 11 out of 12 datasets (source domain). We then predict the remaining dataset (target domain). With this experiment, we want to further investigate the M-B-HBNN model's generalisation capabilities and evaluate models on unseen clusters (zero-shot TL). This would be very helpful when a new cluster is available, and we want to deploy a predictive model immediately.
    \item \textbf{All-but-one FT}: We start with the \textit{All-but-one} version. We then fine-tune the network on the remaining dataset before making the prediction (one-shot TL). This investigates the TL approach with the FT on newly available data.
    \item \textbf{GC19}: This applies only to the datasets from the Google Cloud Trace 2019. This approach is similar to \textit{All}, but we used only Google Cloud Trace 2019 datasets. This assesses whether pretraining a model on related datasets from the same providers helps the predictive capabilities.
    \item \textbf{GC19 FT}: We start with \textit{GC19} but with a fine-tuning on the specific dataset to predict. This is similar to \textit{GC19} but with the evaluation of the FT effect.
    \item \textbf{Random}: this corresponds to the S-U-HBNN with random initialisation of the weights and trained on a single dataset at a time. This assesses whether starting from a pretrained model's weights or a random initialisation is better.
\end{itemize}

We also train \textit{GC19-but-one} and \textit{GC19-but-one FT} versions, specific for the Google Cloud Trace 2019, but we omit the results due to the poor performance compared to the other models. Fig. \ref{fig:ddan} depicts a graphical representation of these training scenarios.

\section{Experiments}\label{sec:experiments}

In this section, we first describe the experimental setup and the datasets used in our experiments. We then evaluate the models in terms of point estimate accuracy, the efficiency of the predicted resources and runtime performance. In each subsection, we first assess the extension to bivariate models and those trained with multiple datasets. We then discuss the results based on the TL approach.

\subsection{Experimental Setup}

The training of the models has been run with a CPU Intel\textregistered Xeon\textregistered Gold 6240 at 2.60GHz and GPU NVIDIA Quadro RTX 8000 with 48 GB of memory where Ubuntu 20.04 is installed.
The experiments are conducted over twelve real-world traces to train an LSTM model, used as a baseline, and the proposed models.
For each dataset, the first 80\% is used as the training set, 20\% of which is used as the validation set. The remaining 20\% of each dataset is used as the test set. In the case of multiple datasets, 80\% of each dataset is concatenated and shuffled to make each batch more representative of each cluster. Still, there is never a time overlap between data in the training and test sets, such that the prediction of one trace cannot exploit information of a particular timestamp in another trace.
We share a GitHub repository\footnote{\url{https://github.com/andreareds/UncertaintyAwareWorkloadPrediction}} that contains further information on the models' architecture and the search space for the hyperparameters optimisation. In particular, the search space is based on the number of layers, the number of neurons for each layer, the batch size, the activation functions, the learning rate, the momentum and decay coefficients and the number of kernels in the 1DConv layer. Once the hyperparameters are tuned, we train the models ten times for each cluster and resource using various random seeds as initialisation to assess the optimisation algorithm's convergence. We forecast the 5-minute interval of demand 10 minutes in the future, where 10 minutes is a sufficient time interval for most applications \cite{mao2010cloud, baldan2016forecasting}, e.g. resource allocation, vertical scaling etc.

\subsection{Datasets}

The twelve datasets used in our experiments include one cluster from Google Cloud 2011, eight clusters from Google Cloud trace 2019, one from Alibaba Cluster Trace 2018 and two from Alibaba Cluster Trace 2020. More details on the datasets and the preprocessing phase are given for reproducibility in the following sections. All the preprocessed datasets can be downloaded from the shared repository.

\subsubsection{Google Cloud Trace 2011 and 2019}

The Google Cloud Trace 2011 \cite{reiss2011google} and 2019 \cite{clusterdata:Wilkes2020a} are publicly available datasets published by Google from the Google Cloud Platform and contain details about the resource utilisation of the cluster cells. In particular, Google Cloud Trace 2011 is composed of 29 days of resource usage collected in May 2011 from 12,500 machines in a single cluster cell, while Google Cloud Trace 2019 is composed of data of 29 days from 8 different cluster cells distributed around the world with around 10,000 machines for each cell. While Google Cloud Trace 2011 is preprocessed offline, the Google Cloud Trace 2019, about 2.4TiB compressed, is preprocessed using Google BigQuery. For each trace, we create a time series dataset that includes the average CPU and average memory usage for all the machines with a 5-minute interval, as done in other works in workload forecasting in cloud computing \cite{kumar2019efficient, liu2016quantitative, herbst2014self} with 8352 data points in total. Missing records are neglected for simplicity. For the tasks that run only partially in a 5-minute window, we multiply the average resource by a weight corresponding to the fraction of the window in which the task is in execution. Data is finally scaled in the range [0, 1] using a MinMax scaling strategy for speeding up the convergence of the training.

\subsubsection{Alibaba Cluster Trace 2018 and 2020}

Consistently with the preprocessing phase for Google Cloud Trace, we preprocessed the Alibaba Cluster Trace 2018 \cite{jiang2020characterizing} and 2020 \cite{weng2022mlaas}. The 2018 version includes the workload history for CPU and memory of about 4,000 machines in 8 days. The 2020 version is a longer trace of about two months from about 1,800 machines that contain over 6,500 GPUs. From this trace, we compile two datasets, one related to the CPU and memory usage and one for the GPU and GPU memory usage. As for the Google Cloud Trace, the average resource usage is aggregated in windows of 5 minutes each. Tasks that run only partially in the specified time interval are weighted with the fraction of time in which the task is running. Missing records are neglected for simplicity, and the data are scaled in the range [0, 1].

\subsection{Point Estimate Accuracy}

\begin{table*}[htbp]
\parbox{.45\linewidth}{
\begin{center}
\caption{Average MSE/MAE comparison for CPU demand. In bold, the best model overall, in italic the best model in their groups}
\begin{tabular}{cc|clcl||clcl|}
\cline{3-10}
 &  & \multicolumn{4}{c||}{\textbf{Univariate}} & \multicolumn{4}{c|}{\textbf{Bivariate}} \\ \cline{3-10} 
 &  & \multicolumn{2}{c|}{\textbf{MSE}} & \multicolumn{2}{c||}{\textbf{MAE}} & \multicolumn{2}{c|}{\textbf{MSE}} & \multicolumn{2}{c|}{\textbf{MAE}} \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\textbf{Single}}} & \textbf{LSTM} & \multicolumn{2}{c|}{0.0041} & \multicolumn{2}{c|}{0.0457} & \multicolumn{2}{c|}{\textit{0.0047}} & \multicolumn{2}{c|}{\textit{0.0500}} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & \textbf{LSTMD} & \multicolumn{2}{c|}{\textbf{0.0040}\textit{}} & \multicolumn{2}{c|}{\textit{0.0455}} & \multicolumn{2}{c|}{0.0061} & \multicolumn{2}{c|}{0.0556} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & \textbf{HBNN} & \multicolumn{2}{c|}{0.0047} & \multicolumn{2}{c|}{0.0502} & \multicolumn{2}{c|}{0.0299} & \multicolumn{2}{c|}{0.1285} \\ \hline \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\textbf{Multi}}} & \textbf{LSTM} & \multicolumn{2}{c|}{0.0044} & \multicolumn{2}{c|}{0.0474} & \multicolumn{2}{c|}{0.0044} & \multicolumn{2}{c|}{0.0479} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & \textbf{LSTMD} & \multicolumn{2}{c|}{\textit{0.0041}} & \multicolumn{2}{c|}{\textit{0.0464}} & \multicolumn{2}{c|}{0.0046} & \multicolumn{2}{c|}{\textit{\textbf{0.0446}}} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & \textbf{HBNN} & \multicolumn{2}{c|}{0.0047} & \multicolumn{2}{c|}{0.0485} & \multicolumn{2}{c|}{\textit{0.0042}} & \multicolumn{2}{c|}{0.0471} \\ \hline
\end{tabular}%
\label{tab:cpuerrors}
\end{center}
}
\hfill
\parbox{.45\linewidth}{
\begin{center}
\caption{Average MSE/MAE comparison for memory demand. In bold, the best model overall, in italic the best model in their groups}
\begin{tabular}{cc|clcl||clcl|}
\cline{3-10}
 &  & \multicolumn{4}{c||}{\textbf{Univariate}} & \multicolumn{4}{c|}{\textbf{Bivariate}} \\ \cline{3-10} 
 &  & \multicolumn{2}{c|}{\textbf{MSE}} & \multicolumn{2}{c|}{\textbf{MAE}} & \multicolumn{2}{c|}{\textbf{MSE}} & \multicolumn{2}{c|}{\textbf{MAE}} \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\textbf{Single}}} & \textbf{LSTM} & \multicolumn{2}{c|}{0.0042} & \multicolumn{2}{c|}{0.0425} & \multicolumn{2}{c|}{\textit{0.0054}} & \multicolumn{2}{c|}{\textit{0.0487}} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & \textbf{LSTMD} & \multicolumn{2}{c|}{\textit{\textbf{0.0041}}} & \multicolumn{2}{c|}{\textbf{0.042}\textit{}} & \multicolumn{2}{c|}{0.0088} & \multicolumn{2}{c|}{0.0631} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & \textbf{HBNN} & \multicolumn{2}{c|}{0.0044} & \multicolumn{2}{c|}{0.0455} & \multicolumn{2}{c|}{0.0386} & \multicolumn{2}{c|}{0.1533} \\ \hline \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\textbf{Multi}}} & \textbf{LSTM} & \multicolumn{2}{c|}{0.0048} & \multicolumn{2}{c|}{0.0447} & \multicolumn{2}{c|}{0.0044} & \multicolumn{2}{c|}{0.0438} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & \textbf{LSTMD} & \multicolumn{2}{c|}{\textit{0.0044}} & \multicolumn{2}{c|}{\textit{0.0439}} & \multicolumn{2}{c|}{0.0046} & \multicolumn{2}{c|}{0.0446} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & \textbf{HBNN} & \multicolumn{2}{c|}{0.0052} & \multicolumn{2}{c|}{0.0498} & \multicolumn{2}{c|}{\textit{0.0043}} & \multicolumn{2}{c|}{\textit{0.0436}} \\ \hline
\end{tabular}%
\label{tab:memerrors}
\end{center}
}
\end{table*}



\begin{table*}[htbp]
\parbox{.45\linewidth}{
\begin{center}
\caption{Average MSE comparison for resource demand}
\begin{tabular}{cc|cccc|}
\cline{3-6}
 &  & \multicolumn{4}{c|}{\textbf{Initialization}} \\ \cline{2-6} 
\multicolumn{1}{c|}{} & \makecell{\textbf{Fine-} \\ \textbf{Tuning}} & \multicolumn{1}{c|}{\textbf{Random}} & \multicolumn{1}{c|}{\textbf{All}} & \multicolumn{1}{c|}{\makecell{\textbf{All-but-} \\ \textbf{one}}} & \makecell{\textbf{GC19} \\ \textbf{only}} \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{CPU}}} & \textbf{Yes} & \multicolumn{1}{c|}{0.1292} & \multicolumn{1}{c|}{0.0058} & \multicolumn{1}{c|}{0.0057} & 0.0059 \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & \textbf{No} & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{\textbf{0.0042}} & \multicolumn{1}{c|}{0.0044} & 0.0043 \\ \hline \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Memory}}} & \textbf{Yes} & \multicolumn{1}{c|}{0.0347} & \multicolumn{1}{c|}{0.0062} & \multicolumn{1}{c|}{0.0058} & 0.0063 \\ \cline{2-6} 
\multicolumn{1}{|c|}{} &\textbf{No} & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{\textbf{0.0043}} & \multicolumn{1}{c|}{0.0045} & 0.0044 \\ \hline
\end{tabular}
\label{tab:msedd}
\end{center}
}
\hfill
\parbox{.45\linewidth}{
\begin{center}
\caption{Average MAE comparison for resource demand}
\begin{tabular}{cc|cccc|}
\cline{3-6}
 &  & \multicolumn{4}{c|}{\textbf{Initialization}} \\ \cline{2-6} 
\multicolumn{1}{c|}{} & \makecell{\textbf{Fine-} \\ \textbf{Tuning}} & \multicolumn{1}{c|}{\textbf{Random}} & \multicolumn{1}{c|}{\textbf{All}} & \multicolumn{1}{c|}{\makecell{\textbf{All-but-} \\ \textbf{one}}} & \makecell{\textbf{GC19} \\ \textbf{only}} \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{CPU}}} & \textbf{Yes} & \multicolumn{1}{c|}{0.2108} & \multicolumn{1}{c|}{0.0561} & \multicolumn{1}{c|}{0.0563} & 0.0562 \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & \textbf{No} & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{\textbf{0.0471}} & \multicolumn{1}{c|}{0.0481} & 0.0474 \\ \hline \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Memory}}} & \textbf{Yes} & \multicolumn{1}{c|}{0.1224} & \multicolumn{1}{c|}{0.0561} & \multicolumn{1}{c|}{0.0538} & 0.0546 \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & \textbf{No} & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{\textbf{0.0436}} & \multicolumn{1}{c|}{0.0448} & 0.0444 \\ \hline
\end{tabular}
\label{tab:maedd}
\end{center}
}
\end{table*}

In this section, we compare the models in terms of prediction errors. The metrics used for this evaluation are MSE and Mean Absolute Error (MAE), as usually done in time series forecasting approaches. In the case of HBNN and LSTMD models, the error is computed w.r.t. the mean of the predicted distribution, while for the LSTM, the error is calculated based on the point prediction.
Tables \ref{tab:cpuerrors} and \ref{tab:memerrors} show the results for the four combinations of single/multiple datasets and univariate/bivariate for CPU and memory demand, respectively. For each model combination, the average MSE and MAE are computed based on the results achieved on the twelve traces.
As confirmed from the previous findings \cite{he2022multivariate}, learning patterns from multivariate time series is very hard, especially when the size of the training data is small, when there is no strong correlation between time series \cite{du2003univariate} and when we are focused on more short-term predictions \cite{chayama2016univariate}. In our traces, we analyse the Pearson correlation between the CPU/GPU and memory demand, finding no strong correlation between the time series and determining that homoscedasticity holds according to the Breusch-Pagan test \cite{breusch1979simple}. We can appreciate the improvement achieved by training the model using multiple dataset traces. All the bivariate models improve their performance compared to the univariate case. We can see that the HBNN model is the one which benefits most from more training data.
The model trained on multiple datasets does not require an extra tuning phase on the specific dataset we want to make the prediction: on the contrary, MSE and MAE metrics get worse if we apply FT. Overall the LSTMD-based models are the ones that achieve the best score in terms of these metrics. On the contrary, the S-B-HBNN failed to converge; the S-B-LSTMD struggles and does not achieve the same performance as the univariate, while S-B-LSTM worsens to a lesser extent.

However, for each possible combination, except for the single bivariate models, the models have the same accuracy according to the statistical difference test for time series forecasting models at 95\% of confidence level (Diebold-Mariano Test \cite{diebold2002comparing}). For this reason, we cannot limit the analysis to these metrics.


Tables \ref{tab:msedd} and \ref{tab:maedd} show the results of the average MSE and MAE, respectively, for the TL approach. Again, the average is w.r.t. the clusters used to train the models.
We can see the importance of training the model on multiple traces. A pretrained network on similar datasets is more impactful than training on the specific dataset we want to make the prediction, showing the model's TL capabilities when predicting a probability distribution.
Moreover, the fine-tuning process never improves the performance of the pretrained models. We believe this is because the model achieves a local optimum that can hardly be improved with fine-tuning on much smaller datasets than those used to pretrain it. Specifically, the TL approach is the one that degrades the accuracy less when FT is applied.
Among the considered training scenarios, M-B-HBNN is the one that achieves the best performance. We believe this is because when more data is available, the network has the chance to learn more patterns from the time series used in training due to a bigger and richer training set.

% \begin{table*}[htbp]
% \caption{MSE and MAE for CPU and memory demand}
% \begin{center}
% \begin{tabular}{clc|cc||cc|}
% \cline{4-7}
%  &  &  & \multicolumn{2}{c||}{\multirow{2}{*}{\textbf{MSE}}} & \multicolumn{2}{c|}{\multirow{2}{*}{\textbf{MAE}}} \\
%  &  &  & \multicolumn{2}{c||}{} & \multicolumn{2}{c|}{} \\ \cline{4-7} 
%  &  &  & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{CPU/GPU}}} & \multirow{2}{*}{\textbf{MEMORY}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{CPU/GPU}}} & \multirow{2}{*}{\textbf{MEMORY}} \\
%  &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline
% \multicolumn{2}{|c|}{\multirow{4}{*}{\rotatebox{90}{\textbf{Statistical}}}} & \multirow{2}{*}{ARIMA} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{GARCH} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline \hline
% \multicolumn{2}{|c|}{\multirow{8}{*}{\rotatebox{90}{\textbf{Single Dataset}}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}S-\\ LSTM\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}S-\\ HBNN\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}S-\\ MHA\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{AVG-S} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline \hline
% \multicolumn{2}{|c|}{\multirow{8}{*}{\rotatebox{90}{\textbf{Multiple Dataset}}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}M-\\ LSTM\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}M-\\ HBNN\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}M-\\ MHA\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{AVG-M} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline \hline
% \multicolumn{2}{|c|}{\multirow{8}{*}{\rotatebox{90}{\textbf{Bivariate}}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BI-\\ LSTM\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BI-\\ HBNN\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BI-\\ MHA\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{AVG-BI} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline \hline
% \multicolumn{2}{|c|}{\multirow{8}{*}{\rotatebox{90}{\begin{tabular}[c]{@{}c@{}}\textbf{Multiple Dataset}\\ \textbf{Bivariate}\end{tabular}}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}M-BI-\\ LSTM\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}M-BI-\\ HBNN\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}M-BI-\\ MHA\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-7} 
% \multicolumn{2}{|c|}{} & \multirow{2}{*}{AVG-M-BI} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} \\
% \multicolumn{2}{|c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline
% \end{tabular}
% \label{tab:errors}
% \end{center}
% \end{table*}

\subsection{Impact of Prediction on QoS}

\begin{table*}%[htbp]
\caption{CPU and Memory allocation statistics of the models}
\begin{center}
\begin{tabular}{cc|cccc||cccc|}
\cline{3-10}
 &  & \multicolumn{4}{c||}{\multirow{2}{*}{\textbf{CPU/GPU}}} & \multicolumn{4}{c|}{\multirow{2}{*}{\textbf{Memory}}} \\
 &  & \multicolumn{4}{c||}{} & \multicolumn{4}{c|}{} \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Target QoS}}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{SR}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{OP}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{UP}}} & \multirow{2}{*}{\textbf{TPR}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{SR}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{OP}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{UP}}} & \multirow{2}{*}{\textbf{TPR}} \\ 
\multicolumn{1}{|c|}{} &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  \\ \hhline{==========}
\multicolumn{1}{|c|}{\multirow{16}{*}{95\%}} & S-U-LSTM & \multicolumn{1}{c|}{93.86} & \multicolumn{1}{c|}{\textit{181.68}} & \multicolumn{1}{c|}{5.31} & \textit{1181.36} & \multicolumn{1}{c|}{93.62} & \multicolumn{1}{c|}{\textit{\textbf{118.94}}} & \multicolumn{1}{c|}{3.69} & \textit{\textbf{1061.05}} \\ \cline{2-10}
\multicolumn{1}{|c|}{} & S-U-HBNN & \multicolumn{1}{c|}{93.86} & \multicolumn{1}{c|}{181.83} & \multicolumn{1}{c|}{\textit{5.09}} & 1181.72 & \multicolumn{1}{c|}{93.54} & \multicolumn{1}{c|}{128.57} & \multicolumn{1}{c|}{\textit{3.61}} & 1070.75 \\\hhline{|~=========} 
\multicolumn{1}{|c|}{} & S-U-LSTMD & \multicolumn{1}{c|}{93.14} & \multicolumn{1}{c|}{171.98} & \multicolumn{1}{c|}{\textit{5.64}} & 1171.32 & \multicolumn{1}{c|}{93.26} & \multicolumn{1}{c|}{\textit{\textbf{111.61}}} & \multicolumn{1}{c|}{3.87} & \textit{\textbf{1053.54}} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-U-LSTM & \multicolumn{1}{c|}{93.1} & \multicolumn{1}{c|}{\textit{\textbf{166.55}}} & \multicolumn{1}{c|}{6.36} & \textit{\textbf{1165.17}} & \multicolumn{1}{c|}{93.26} & \multicolumn{1}{c|}{117.54} & \multicolumn{1}{c|}{\textit{3.79}} & 1059.55 \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-U-LSTM & \multicolumn{1}{c|}{91.45} & \multicolumn{1}{c|}{157.07} & \multicolumn{1}{c|}{7.62} & 1154.44 & \multicolumn{1}{c|}{95.74} & \multicolumn{1}{c|}{146.99} & \multicolumn{1}{c|}{\textit{2.57}} & 1090.21 \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-U-HBNN & \multicolumn{1}{c|}{91.41} & \multicolumn{1}{c|}{\textbf{\textit{154.71}}} & \multicolumn{1}{c|}{\textit{7.02}} & \textit{\textbf{1152.67}} & \multicolumn{1}{c|}{95.78} & \multicolumn{1}{c|}{\textit{143.77}} & \multicolumn{1}{c|}{2.85} & \textit{1086.72} \\ \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-U-LSTMD & \multicolumn{1}{c|}{94.62} & \multicolumn{1}{c|}{\textit{196.45}} & \multicolumn{1}{c|}{\textit{4.30}} & \textit{1197.14} & \multicolumn{1}{c|}{95.18} & \multicolumn{1}{c|}{\textit{137.45}} & \multicolumn{1}{c|}{3.07} & \textit{1080.18} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-U-LSTM & \multicolumn{1}{c|}{94.62} & \multicolumn{1}{c|}{197.69} & \multicolumn{1}{c|}{4.65} & 1198.03 & \multicolumn{1}{c|}{95.22} & \multicolumn{1}{c|}{141.04} & \multicolumn{1}{c|}{\textit{2.86}} & 1083.98 \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & S-B-LSTM & \multicolumn{1}{c|}{91.93} & \multicolumn{1}{c|}{863.11} & \multicolumn{1}{c|}{32.83} & 1835.26 & \multicolumn{1}{c|}{91.89} & \multicolumn{1}{c|}{676.14} & \multicolumn{1}{c|}{\textit{12.58}} & 1609.35 \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-B-HBNN & \multicolumn{1}{c|}{91.93} & \multicolumn{1}{c|}{\textit{825.14}} & \multicolumn{1}{c|}{\textit{29.46}} & \textit{1800.66} & \multicolumn{1}{c|}{91.89} & \multicolumn{1}{c|}{\textit{675.94}} & \multicolumn{1}{c|}{13.53} & \textit{1608.21} \\ \hhline{|~=========} 
\multicolumn{1}{|c|}{} & S-B-LSTMD & \multicolumn{1}{c|}{92.61} & \multicolumn{1}{c|}{\textit{846.79}} & \multicolumn{1}{c|}{30.21} & \textit{1821.57} & \multicolumn{1}{c|}{92.45} & \multicolumn{1}{c|}{\textit{679.97}} & \multicolumn{1}{c|}{13.18} & \textit{1612.59} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-B-LSTM & \multicolumn{1}{c|}{92.61} & \multicolumn{1}{c|}{895.31} & \multicolumn{1}{c|}{\textit{30.16}} & 1870.14 & \multicolumn{1}{c|}{92.45} & \multicolumn{1}{c|}{686.47} & \multicolumn{1}{c|}{\textit{11.70}} & 1620.56 \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-B-LSTM & \multicolumn{1}{c|}{95.66} & \multicolumn{1}{c|}{227.28} & \multicolumn{1}{c|}{3.91} & 1228.35 & \multicolumn{1}{c|}{97.43} & \multicolumn{1}{c|}{188.17} & \multicolumn{1}{c|}{\textit{\textbf{1.28}}} & 1132.69 \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-B-HBNN & \multicolumn{1}{c|}{95.7} & \multicolumn{1}{c|}{\textit{216.83}} & \multicolumn{1}{c|}{\textit{\textbf{3.81}}} & \textit{1218.01} & \multicolumn{1}{c|}{97.39} & \multicolumn{1}{c|}{\textit{184.19}} & \multicolumn{1}{c|}{1.51} & \textit{1128.48} \\ \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-B-LSTMD & \multicolumn{1}{c|}{96.95} & \multicolumn{1}{c|}{283.94} & \multicolumn{1}{c|}{\textit{\textbf{2.46}}} & 1286.46 & \multicolumn{1}{c|}{99.0} & \multicolumn{1}{c|}{\textit{229.80}} & \multicolumn{1}{c|}{\textit{\textbf{0.54}}} & 1175.06 \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-B-LSTM & \multicolumn{1}{c|}{96.99} & \multicolumn{1}{c|}{\textit{259.69}} & \multicolumn{1}{c|}{2.70} & \textit{1261.98 }& \multicolumn{1}{c|}{99.0} & \multicolumn{1}{c|}{\textit{229.80}} & \multicolumn{1}{c|}{0.56} & \textit{1175.03} \\ \hline \hline \hline
\multicolumn{1}{|c|}{\multirow{16}{*}{97\%}} & S-U-LSTM & \multicolumn{1}{c|}{95.18} & \multicolumn{1}{c|}{\textit{201.68}} & \multicolumn{1}{c|}{4.13} & \textit{1202.54} & \multicolumn{1}{c|}{95.02} & \multicolumn{1}{c|}{\textit{\textbf{131.64}}} & \multicolumn{1}{c|}{2.93} & \textit{\textbf{1074.50}} \\ \cline{2-10}
\multicolumn{1}{|c|}{} & S-U-HBNN & \multicolumn{1}{c|}{95.22} & \multicolumn{1}{c|}{202.32} & \multicolumn{1}{c|}{\textit{3.92}} & 1203.89 & \multicolumn{1}{c|}{95.06} & \multicolumn{1}{c|}{143.98} & \multicolumn{1}{c|}{\textit{2.66}} & 1087.12 \\ \hhline{|~=========} 
\multicolumn{1}{|c|}{} & S-U-LSTMD & \multicolumn{1}{c|}{94.58} & \multicolumn{1}{c|}{\textit{\textbf{192.27}}} & \multicolumn{1}{c|}{\textit{4.37}} & \textit{\textbf{1192.88}} & \multicolumn{1}{c|}{94.86} & \multicolumn{1}{c|}{\textit{\textbf{125.88}}} & \multicolumn{1}{c|}{\textit{2.94}} & \textit{\textbf{1068.74}} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-U-LSTM & \multicolumn{1}{c|}{94.54} & \multicolumn{1}{c|}{194.59} & \multicolumn{1}{c|}{4.51} & 1195.06 & \multicolumn{1}{c|}{94.9} & \multicolumn{1}{c|}{130.45} & \multicolumn{1}{c|}{3.00} & 1073.25 \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-U-LSTM & \multicolumn{1}{c|}{93.82} & \multicolumn{1}{c|}{183.61} & \multicolumn{1}{c|}{5.51} & 1183.08 & \multicolumn{1}{c|}{96.95} & \multicolumn{1}{c|}{164.98} & \multicolumn{1}{c|}{\textit{1.89}} & 1108.89 \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-U-HBNN & \multicolumn{1}{c|}{93.82} & \multicolumn{1}{c|}{\textit{\textbf{180.01}}} & \multicolumn{1}{c|}{\textit{5.04}} & \textit{\textbf{1179.95}} & \multicolumn{1}{c|}{96.91} & \multicolumn{1}{c|}{\textit{162.14}} & \multicolumn{1}{c|}{2.22} & \textit{1105.72} \\ \hhline{|~=========}  
\multicolumn{1}{|c|}{} & M-U-LSTMD & \multicolumn{1}{c|}{95.95} & \multicolumn{1}{c|}{219.12} & \multicolumn{1}{c|}{\textit{3.16}} & 1220.95 & \multicolumn{1}{c|}{96.39} & \multicolumn{1}{c|}{\textit{154.76}} & \multicolumn{1}{c|}{2.36} & \textit{1098.19} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-U-LSTM & \multicolumn{1}{c|}{95.99} & \multicolumn{1}{c|}{\textit{216.67}} & \multicolumn{1}{c|}{3.70} & \textit{1217.96} & \multicolumn{1}{c|}{96.39} & \multicolumn{1}{c|}{157.76} & \multicolumn{1}{c|}{\textit{2.14}} & 1101.42 \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & S-B-LSTM & \multicolumn{1}{c|}{93.42} & \multicolumn{1}{c|}{957.91} & \multicolumn{1}{c|}{25.50} & 1937.39 & \multicolumn{1}{c|}{94.78} & \multicolumn{1}{c|}{\textit{755.57}} & \multicolumn{1}{c|}{7.32} & \textit{1694.05} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-B-HBNN & \multicolumn{1}{c|}{93.46} & \multicolumn{1}{c|}{\textit{918.34}} & \multicolumn{1}{c|}{\textit{22.26}} & \textit{1901.07} & \multicolumn{1}{c|}{94.78} & \multicolumn{1}{c|}{767.76} & \multicolumn{1}{c|}{\textit{7.13}} & 1706.43 \\ \hhline{|~=========} 
\multicolumn{1}{|c|}{} & S-B-LSTMD & \multicolumn{1}{c|}{93.58} & \multicolumn{1}{c|}{\textit{940.42}} & \multicolumn{1}{c|}{\textit{23.47}} & \textit{1921.93} & \multicolumn{1}{c|}{94.58} & \multicolumn{1}{c|}{765.92} & \multicolumn{1}{c|}{\textit{7.35}} & 1704.37 \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-B-LSTM & \multicolumn{1}{c|}{93.58} & \multicolumn{1}{c|}{964.9} & \multicolumn{1}{c|}{25.02} & 1944.87 & \multicolumn{1}{c|}{94.58} & \multicolumn{1}{c|}{\textit{734.34}} & \multicolumn{1}{c|}{8.52} & \textit{1671.63} \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-B-LSTM & \multicolumn{1}{c|}{96.75} & \multicolumn{1}{c|}{254.86} & \multicolumn{1}{c|}{2.85} & 1256.99 & \multicolumn{1}{c|}{98.03} & \multicolumn{1}{c|}{\textit{200.34}} & \multicolumn{1}{c|}{\textit{\textbf{1.00}}} & \textit{1145.14} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-B-HBNN & \multicolumn{1}{c|}{96.75} & \multicolumn{1}{c|}{\textit{242.97}} & \multicolumn{1}{c|}{\textit{\textbf{2.81}}} & \textit{1245.15} & \multicolumn{1}{c|}{98.03} & \multicolumn{1}{c|}{206.17} & \multicolumn{1}{c|}{1.03} & 1150.93 \\ \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-B-LSTMD & \multicolumn{1}{c|}{97.75} & \multicolumn{1}{c|}{316.68} & \multicolumn{1}{c|}{\textit{\textbf{1.59}}} & 1320.07 & \multicolumn{1}{c|}{99.44} & \multicolumn{1}{c|}{\textit{258.94}} & \multicolumn{1}{c|}{0.31} & \textit{1204.43} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-B-LSTM & \multicolumn{1}{c|}{97.75} & \multicolumn{1}{c|}{\textit{293.65}} & \multicolumn{1}{c|}{1.78} & \textit{1296.85} & \multicolumn{1}{c|}{99.44} & \multicolumn{1}{c|}{276.75} & \multicolumn{1}{c|}{\textit{\textbf{0.19}}} & 1222.36 \\ \hline \hline \hline\multicolumn{1}{|c|}{\multirow{16}{*}{99\%}} & S-U-LSTM & \multicolumn{1}{c|}{97.03} & \multicolumn{1}{c|}{247.30} & \multicolumn{1}{c|}{2.32} & 1249.87 & \multicolumn{1}{c|}{97.59} & \multicolumn{1}{c|}{\textit{\textbf{171.30}}} & \multicolumn{1}{c|}{\textit{1.49}} & \textit{\textbf{1115.60}} \\ \cline{2-10}
\multicolumn{1}{|c|}{} & S-U-HBNN & \multicolumn{1}{c|}{97.03} & \multicolumn{1}{c|}{\textit{241.64}} & \multicolumn{1}{c|}{\textit{2.31}} & \textit{1244.31} & \multicolumn{1}{c|}{97.55} & \multicolumn{1}{c|}{173.76} & \multicolumn{1}{c|}{1.55} & 1118.02 \\ \hhline{|~=========} 
\multicolumn{1}{|c|}{} & S-U-LSTMD & \multicolumn{1}{c|}{96.35} & \multicolumn{1}{c|}{231.27} & \multicolumn{1}{c|}{\textit{2.67}} & 1233.59 & \multicolumn{1}{c|}{97.07} & \multicolumn{1}{c|}{\textit{\textbf{153.41}}} & \multicolumn{1}{c|}{\textit{1.77}} & \textit{\textbf{1097.44}} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-U-LSTM & \multicolumn{1}{c|}{96.31} & \multicolumn{1}{c|}{\textit{\textbf{229.15}}} & \multicolumn{1}{c|}{2.95} & \textit{\textbf{1231.18}} & \multicolumn{1}{c|}{97.11} & \multicolumn{1}{c|}{159.16} & \multicolumn{1}{c|}{1.81} & 1103.15 \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-U-LSTM & \multicolumn{1}{c|}{96.47} & \multicolumn{1}{c|}{\textit{\textbf{228.65}}} & \multicolumn{1}{c|}{3.22} & \textit{\textbf{1230.41}} & \multicolumn{1}{c|}{98.11} & \multicolumn{1}{c|}{\textit{195.35}} & \multicolumn{1}{c|}{\textit{1.12}} & \textit{1140.03} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-U-HBNN & \multicolumn{1}{c|}{96.47} & \multicolumn{1}{c|}{229.15} & \multicolumn{1}{c|}{\textit{2.67}} & 1231.46 & \multicolumn{1}{c|}{98.07} & \multicolumn{1}{c|}{197.21} & \multicolumn{1}{c|}{1.41} & 1141.60 \\ \hhline{|~=========}  
\multicolumn{1}{|c|}{} & M-U-LSTMD & \multicolumn{1}{c|}{97.79} & \multicolumn{1}{c|}{\textit{262.69}} & \multicolumn{1}{c|}{\textit{1.75}} & \textit{1265.92} & \multicolumn{1}{c|}{97.63} & \multicolumn{1}{c|}{187.88} & \multicolumn{1}{c|}{1.47} & 1132.21 \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-U-LSTM & \multicolumn{1}{c|}{97.75} & \multicolumn{1}{c|}{275.80} & \multicolumn{1}{c|}{1.80} & 1278.99 & \multicolumn{1}{c|}{97.67} & \multicolumn{1}{c|}{\textit{183.17}} & \multicolumn{1}{c|}{\textit{1.39}} & \textit{1127.57} \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & S-B-LSTM & \multicolumn{1}{c|}{94.458} & \multicolumn{1}{c|}{1138.38} & \multicolumn{1}{c|}{14.11} & 2129.20 & \multicolumn{1}{c|}{98.11} & \multicolumn{1}{c|}{\textit{904.42}} & \multicolumn{1}{c|}{1.73} & \textit{1848.49} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-B-HBNN & \multicolumn{1}{c|}{94.58} & \multicolumn{1}{c|}{\textit{1096.57}} & \multicolumn{1}{c|}{\textit{10.89}} & \textit{2090.67} & \multicolumn{1}{c|}{98.07} & \multicolumn{1}{c|}{947.24} & \multicolumn{1}{c|}{\textit{1.14}} & 1891.91 \\ \hhline{|~=========}  
\multicolumn{1}{|c|}{} & S-B-LSTMD & \multicolumn{1}{c|}{94.42} & \multicolumn{1}{c|}{1118.85} & \multicolumn{1}{c|}{\textit{12.37}} & 2111.46 & \multicolumn{1}{c|}{97.91} & \multicolumn{1}{c|}{933.59} & \multicolumn{1}{c|}{\textit{1.71}} & 1877.68 \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & S-B-LSTM & \multicolumn{1}{c|}{94.42} & \multicolumn{1}{c|}{\textit{1117.13}} & \multicolumn{1}{c|}{15.33} & \textit{2106.78} & \multicolumn{1}{c|}{97.87} & \multicolumn{1}{c|}{\textit{898.32}} & \multicolumn{1}{c|}{1.86} & \textit{1842.26} \\ \hhline{|~=========} \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-B-LSTM & \multicolumn{1}{c|}{97.95} & \multicolumn{1}{c|}{300.96} & \multicolumn{1}{c|}{1.62} & 1304.32 & \multicolumn{1}{c|}{99.04} & \multicolumn{1}{c|}{\textit{244.60}} & \multicolumn{1}{c|}{\textit{\textbf{0.42}}} & \textit{1189.98} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-B-HBNN & \multicolumn{1}{c|}{97.95} & \multicolumn{1}{c|}{\textit{292.95}} & \multicolumn{1}{c|}{\textit{\textbf{1.55}}} & \textit{1296.3}9 & \multicolumn{1}{c|}{99.04} & \multicolumn{1}{c|}{248.08} & \multicolumn{1}{c|}{0.54} & 1193.33 \\ \hhline{|~=========} 
\multicolumn{1}{|c|}{} & M-B-LSTMD & \multicolumn{1}{c|}{99.04} & \multicolumn{1}{c|}{379.16} & \multicolumn{1}{c|}{\textit{\textbf{0.59}}} & 1383.55 & \multicolumn{1}{c|}{99.8} & \multicolumn{1}{c|}{\textit{314.17}} & \multicolumn{1}{c|}{0.10} & \textit{1258.87} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & M-B-LSTM & \multicolumn{1}{c|}{99.04} & \multicolumn{1}{c|}{\textit{367.20}} & \multicolumn{1}{c|}{0.61} & \textit{1371.58} & \multicolumn{1}{c|}{99.8} & \multicolumn{1}{c|}{321.44} & \multicolumn{1}{c|}{\textit{\textbf{0.04}}} & 1267.20 \\ \hline 
\end{tabular}
\label{tab:statistics}
\end{center}
\end{table*}

\begin{table*}[htbp]
\caption{CPU and Memory allocation statistics of the models}
\begin{center}
\begin{tabular}{cc|cccc||cccc|}
\cline{3-10}
 &  & \multicolumn{4}{c||}{\multirow{2}{*}{\textbf{CPU/GPU}}} & \multicolumn{4}{c|}{\multirow{2}{*}{\textbf{Memory}}} \\
 &  & \multicolumn{4}{c||}{} & \multicolumn{4}{c|}{} \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Target QoS}}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{SR}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{OP}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{UP}}} & \multirow{2}{*}{\textbf{TPR}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{SR}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{OP}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{UP}}} & \multirow{2}{*}{\textbf{TPR}} \\ 
\multicolumn{1}{|c|}{} &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  \\ \hhline{==========}
\multicolumn{1}{|c|}{\multirow{7}{*}{95\%}} & All & \multicolumn{1}{c|}{\textbf{94.58}} & \multicolumn{1}{c|}{206.29} & \multicolumn{1}{c|}{5.08} & 1206.2  & \multicolumn{1}{c|}{95.74} & \multicolumn{1}{c|}{156.71} & \multicolumn{1}{c|}{2.41} & 1100.1\\  \cline{2-10}
\multicolumn{1}{|c|}{} & All FT & \multicolumn{1}{c|}{87.47} & \multicolumn{1}{c|}{167.17} & \multicolumn{1}{c|}{11.69} & 1160.46  & \multicolumn{1}{c|}{85.87} & \multicolumn{1}{c|}{92.17} & \multicolumn{1}{c|}{7.83} & 1030.13\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & All-but-one & \multicolumn{1}{c|}{95.7} & \multicolumn{1}{c|}{216.83} & \multicolumn{1}{c|}{3.81} & 1218.01  & \multicolumn{1}{c|}{97.39} & \multicolumn{1}{c|}{184.19} & \multicolumn{1}{c|}{1.51} & 1128.48\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & All-but-one FT & \multicolumn{1}{c|}{94.1} & \multicolumn{1}{c|}{204.32} & \multicolumn{1}{c|}{4.71} & 1204.6  & \multicolumn{1}{c|}{\textbf{95.14}} & \multicolumn{1}{c|}{160.2} & \multicolumn{1}{c|}{2.68} & 1103.32\\  \cline{2-10} 
\multicolumn{1}{|c|}{} & GC19 & \multicolumn{1}{c|}{98.55} & \multicolumn{1}{c|}{108.99} & \multicolumn{1}{c|}{0.38} & 1126.04  & \multicolumn{1}{c|}{97.18} & \multicolumn{1}{c|}{116.24} & \multicolumn{1}{c|}{1.26} & 1032.09\\  \cline{2-10} 
\multicolumn{1}{|c|}{} & GC19 FT & \multicolumn{1}{c|}{77.7} & \multicolumn{1}{c|}{49.82} & \multicolumn{1}{c|}{8.55} & 1058.69  & \multicolumn{1}{c|}{89.36} & \multicolumn{1}{c|}{74.61} & \multicolumn{1}{c|}{3.84} & 987.87\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & Random & \multicolumn{1}{c|}{92.29} & \multicolumn{1}{c|}{190.49} & \multicolumn{1}{c|}{6.53} & 1188.94  & \multicolumn{1}{c|}{85.75} & \multicolumn{1}{c|}{559.49} & \multicolumn{1}{c|}{28.22} & 1477.07\\  \hline \hline
\multicolumn{1}{|c|}{\multirow{7}{*}{97\%}} & All & \multicolumn{1}{c|}{95.95} & \multicolumn{1}{c|}{230.14} & \multicolumn{1}{c|}{3.97} & 1231.16  & \multicolumn{1}{c|}{\textbf{97.11}} & \multicolumn{1}{c|}{175.27} & \multicolumn{1}{c|}{1.75} & 1119.31\\  \cline{2-10}
\multicolumn{1}{|c|}{} & All FT & \multicolumn{1}{c|}{91.01} & \multicolumn{1}{c|}{197.1} & \multicolumn{1}{c|}{8.45} & 1193.64  & \multicolumn{1}{c|}{90.85} & \multicolumn{1}{c|}{112.9} & \multicolumn{1}{c|}{5.17} & 1053.52\\  \cline{2-10} 
\multicolumn{1}{|c|}{} & All-but-one & \multicolumn{1}{c|}{\textbf{96.75}} & \multicolumn{1}{c|}{242.97} & \multicolumn{1}{c|}{2.81} & 1245.15  & \multicolumn{1}{c|}{98.03} & \multicolumn{1}{c|}{206.17} & \multicolumn{1}{c|}{1.03} & 1150.93\\  \cline{2-10} 
\multicolumn{1}{|c|}{} & All-but-one FT & \multicolumn{1}{c|}{95.46} & \multicolumn{1}{c|}{227.05} & \multicolumn{1}{c|}{3.5} & 1228.53  & \multicolumn{1}{c|}{96.15} & \multicolumn{1}{c|}{176.18} & \multicolumn{1}{c|}{2.03} & 1119.95\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & GC19 & \multicolumn{1}{c|}{99.42} & \multicolumn{1}{c|}{125.95} & \multicolumn{1}{c|}{0.23} & 1143.15  & \multicolumn{1}{c|}{98.19} & \multicolumn{1}{c|}{133.11} & \multicolumn{1}{c|}{0.92} & 1049.29\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & GC19 FT &\multicolumn{1}{c|}{84.0} & \multicolumn{1}{c|}{60.71} & \multicolumn{1}{c|}{5.94} & 1072.2  & \multicolumn{1}{c|}{92.9} & \multicolumn{1}{c|}{87.48} & \multicolumn{1}{c|}{2.61} & 1001.97\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & Random & \multicolumn{1}{c|}{93.98} & \multicolumn{1}{c|}{212.67} & \multicolumn{1}{c|}{4.96} & 1212.69  & \multicolumn{1}{c|}{89.64} & \multicolumn{1}{c|}{626.76} & \multicolumn{1}{c|}{18.71} & 1553.85\\  \hline \hline
\multicolumn{1}{|c|}{\multirow{7}{*}{99\%}} & All & \multicolumn{1}{c|}{97.19} & \multicolumn{1}{c|}{275.76} & \multicolumn{1}{c|}{2.45} & 1278.3  & \multicolumn{1}{c|}{98.35} & \multicolumn{1}{c|}{210.79} & \multicolumn{1}{c|}{1.0} & 1155.59\\  \cline{2-10}
\multicolumn{1}{|c|}{} & All FT & \multicolumn{1}{c|}{94.5} & \multicolumn{1}{c|}{255.78} & \multicolumn{1}{c|}{4.47} & 1256.29  & \multicolumn{1}{c|}{96.07} & \multicolumn{1}{c|}{154.39} & \multicolumn{1}{c|}{2.49} & 1097.7\\  \cline{2-10} 
\multicolumn{1}{|c|}{} & All-but-one & \multicolumn{1}{c|}{97.95} & \multicolumn{1}{c|}{292.95} & \multicolumn{1}{c|}{1.55} & 1296.39  & \multicolumn{1}{c|}{\textbf{99.04}} & \multicolumn{1}{c|}{248.08} & \multicolumn{1}{c|}{0.54} & 1193.33\\  \cline{2-10} 
\multicolumn{1}{|c|}{} & All-but-one FT & \multicolumn{1}{c|}{97.35} & \multicolumn{1}{c|}{270.77} & \multicolumn{1}{c|}{2.01} & 1273.74  & \multicolumn{1}{c|}{97.71} & \multicolumn{1}{c|}{206.75} & \multicolumn{1}{c|}{1.2} & 1151.35\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & GC19 & \multicolumn{1}{c|}{\textbf{99.86}} & \multicolumn{1}{c|}{158.17} & \multicolumn{1}{c|}{0.14} & 1175.46  & \multicolumn{1}{c|}{98.84} & \multicolumn{1}{c|}{165.23} & \multicolumn{1}{c|}{0.54} & 1081.79\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & GC19 FT & \multicolumn{1}{c|}{92.76} & \multicolumn{1}{c|}{83.18} & \multicolumn{1}{c|}{2.89} & 1097.72  & \multicolumn{1}{c|}{96.6} & \multicolumn{1}{c|}{112.81} & \multicolumn{1}{c|}{1.3} & 1028.61\\ \cline{2-10} 
\multicolumn{1}{|c|}{} & Random & \multicolumn{1}{c|}{96.31} & \multicolumn{1}{c|}{255.44} & \multicolumn{1}{c|}{2.87} & 1257.56  & \multicolumn{1}{c|}{94.5} & \multicolumn{1}{c|}{760.59} & \multicolumn{1}{c|}{7.53} & 1698.85\\ \hline
\end{tabular}
\label{tab:trainingstatistics}
\end{center}
\end{table*}

This section evaluates the models using the metrics defined in \cite{rossi2022bayesian}. The goal is to assess the quality of the prediction w.r.t. a target QoS level that the providers should ensure to the customers. In particular, these metrics are:

\begin{itemize}
    \item Success Rate (SR): the percentage of future demand within the confidence interval
    \item Overprediction (OP): the total amount of overprediction defined as the difference between the upper bound of the prediction and the real demand for the requests within the confidence interval
    \item Underprediction (UP): the total amount of underprediction defined as the difference between the real demand and the upper bound of the prediction for the requests greater than the upper bound
    \item Total Predicted Resources (TPR): the sum of all the upper bounds of the predictions
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{images/tprcpu.png}}
\caption{Total Predicted CPU for S-U, M-U and M-B models}
\label{fig:tprcpu}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{images/tprcpubi.png}}
\caption{Total Predicted CPU for M-B and S-B models}
\label{fig:tprcpubi}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{images/tltprcpu2.png}}
\caption{Total Predicted CPU for transfer learning experiments}
\label{fig:tltprcpu}
\end{figure}

In the case of HBNN and LSTMD models that predict a probability distribution, these scores are computed w.r.t. the upper bound of the confidence interval with a confidence level of 95\%, 97\% and 99\%. To compute an interval from a point estimate, we can predict the output of the network plus a fixed threshold, e.g. 5\%, as done in \cite{minarolli2014cross, minarolli2017tackling}. The HBNN and LSTMD are compared to their LSTM counterpart with a fixed threshold s.t. its accuracy is close to the SR of the compared model. Table \ref{tab:statistics} lists all the results for all the possible combinations of models at the aforementioned confidence levels.
We also draw a graphical representation of the total predicted resources versus the success rate for CPU demand in Fig. \ref{fig:tprcpu}. The graph is drawn by varying the confidence level between 90\% and 99.5\% and computing the SR and TPR, respectively. If one curve is above the other, the model achieves better performance because, with the same SR, it predicts a lower TPR. On the other hand,  with the same TPR, it reaches a higher SR, achieving a higher number of correctly forecasted values within the upper bound of the distribution but with a lower amount of total predicted resources. For the readability of the graph, we removed the plot of the single bivariate models, which achieve poor performance under this metric.
From the aforementioned table and figure, we can see that the LSTMD and HBNN models consistently outperform the LSTM counterpart, with HBNN performing well for the CPU and LSTMD performing better for the memory prediction and showing the advantage of predicting the probability.
Training a model with multiple datasets compared to the model trained with one single dataset has more advantages, despite the single univariate model seeming superior in the case of memory prediction at a 99\% confidence level. Again, it is clear that the univariate prediction is an easier task compared to the bivariate version.

Fig. \ref{fig:tprcpubi} compares the bivariate models trained with single and multiple datasets. As the point estimate experiment shows, if the bivariate model is trained with a single dataset, it struggles to converge. For this picture, it is clear that the model benefits significantly if it is trained with more data. In particular, the M-B models achieve between around 25\% and 60\% saving in terms of TPR for both CPU and memory demand, with a higher SR.


Regarding the TL part, Table \ref{tab:trainingstatistics} lists the results for the different training scenarios. Similarly, we plot the TPR versus SR for the CPU demand in Fig. \ref{fig:tprcpubi}.
This experiment shows the advantages of using multiple datasets for training the network compared to training with random initialization with the specific dataset. Moreover, we can see that with the FT on the specific dataset we want to make the prediction, the quality of prediction under our metrics gets worse due to the overfitting of the pretrained model.
These results also show the benefits of having a pretrained model that can be used to predict the datasets that have never been seen before by the model. To the best of our knowledge, this is the first time this technique has been applied to cloud workload prediction.
Interestingly, training the model only on the datasets from the same distribution (eight clusters from GC2019) allows the training to achieve better performance in terms of these metrics.
Overall, we believe that training the model with multiple datasets leads to many benefits in terms of performance, also in the case of prediction on unseen datasets in the training set. More available data allows the model to capture more patterns shared among similar traces. Also, it will enable the service provider to accelerate the deployment process by avoiding the need to retrain new models from scratch or wait for new data when additional clusters join the cloud computing system.

\subsection{Accuracy of the Predicted Uncertainty}

\begin{table*}[htbp]
\parbox{.45\linewidth}{
\begin{center}
\caption{Average MSE comparison for resource demand accuracy}
\begin{tabular}{cc|clcl||clcl|}
\cline{3-10}
 &  & \multicolumn{4}{c|}{\textbf{Univariate}} & \multicolumn{4}{c|}{\textbf{Bivariate}} \\ \cline{3-10} 
 &  & \multicolumn{2}{c|}{\makecell{\textbf{CPU/} \\ \textbf{GPU}}} & \multicolumn{2}{c|}{\textbf{Memory}} & \multicolumn{2}{c|}{\makecell{\textbf{CPU/} \\ \textbf{GPU}}} & \multicolumn{2}{c|}{\textbf{Memory}} \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Single}}} & \textbf{LSTMD} & \multicolumn{2}{c|}{2.96} & \multicolumn{2}{c|}{3.07} & \multicolumn{2}{c|}{8.15} & \multicolumn{2}{c|}{6.51} \\ \cline{2-10} 

\multicolumn{1}{|c|}{} & \textbf{HBNN} & \multicolumn{2}{c|}{1.69} & \multicolumn{2}{c|}{1.80} & \multicolumn{2}{c|}{11.80} & \multicolumn{2}{c|}{11.81} \\ \hline \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Multi}}} & \textbf{LSTMD} & \multicolumn{2}{c|}{\textbf{1.24}} & \multicolumn{2}{c|}{\textbf{1.14}} & \multicolumn{2}{c|}{6.89} & \multicolumn{2}{c|}{21.05} \\ \cline{2-10}
\multicolumn{1}{|c|}{} & \textbf{HBNN} & \multicolumn{2}{c|}{12.07} & \multicolumn{2}{c|}{2.42} & \multicolumn{2}{c|}{1.97} & \multicolumn{2}{c|}{8.66} \\ \hline
\end{tabular}%
\label{tab:cpuaccuracyerrors}
\end{center}
}
\hfill
\parbox{.45\linewidth}{
\begin{center}
\caption{Average MAE comparison for resource demand accuracy}
\begin{tabular}{cc|clcl||clcl|}
\cline{3-10}
 &  & \multicolumn{4}{c|}{\textbf{Univariate}} & \multicolumn{4}{c|}{\textbf{Bivariate}} \\ \cline{3-10} 
 &  & \multicolumn{2}{c|}{\makecell{\textbf{CPU/} \\ \textbf{GPU}}} & \multicolumn{2}{c|}{\textbf{Memory}} & \multicolumn{2}{c|}{\makecell{\textbf{CPU/} \\ \textbf{GPU}}} & \multicolumn{2}{c|}{\textbf{Memory}} \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Single}}} & \textbf{LSTMD} & \multicolumn{2}{c|}{1.47} & \multicolumn{2}{c|}{1.67} & \multicolumn{2}{c|}{2.66} & \multicolumn{2}{c|}{2.34} \\ \cline{2-10} 

\multicolumn{1}{|c|}{} & \textbf{HBNN} & \multicolumn{2}{c|}{1.10} & \multicolumn{2}{c|}{1.27} & \multicolumn{2}{c|}{3.32} & \multicolumn{2}{c|}{3.03} \\ \hline \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Multi}}} & \textbf{LSTMD} & \multicolumn{2}{c|}{\textbf{0.98}} & \multicolumn{2}{c|}{\textbf{0.92}} & \multicolumn{2}{c|}{2.08} & \multicolumn{2}{c|}{3.94} \\ \cline{2-10}
\multicolumn{1}{|c|}{} & \textbf{HBNN} & \multicolumn{2}{c|}{3.34} & \multicolumn{2}{c|}{1.25} & \multicolumn{2}{c|}{1.11} & \multicolumn{2}{c|}{2.39} \\ \hline
\end{tabular}%
\label{tab:memaccuracyerrors}
\end{center}
}
\end{table*}

In this section, we evaluate the prediction accuracy of the probabilistic models HBNN and LSTMD. To do so, we plot the targeted confidence levels versus the success rate achieved by the models by varying the confidence level between 90\% and 99.5\%. A perfect model would achieve an SR equal to the targeted confidence level. This evaluation does not apply to LSTM, which predicts a point estimate, so we should compute a fixed threshold differently. Moreover, we compute the MSE and MAE of the curve w.r.t. the line $y=x$ to aggregate the plot results in a single numerical value to evaluate the overall performance. The results are depicted in Fig. \ref{fig:accmem} and Tables \ref{tab:cpuaccuracyerrors} and \ref{tab:memaccuracyerrors} for MSE and MAE, respectively.
Although w.r.t. these metrics, the M-U-LSTMD is the model that overall achieves the best accuracy, there are interesting differences between each combination of the model, with the HBNN outperforming the LSTMD counterpart in the case of S-U and M-B versions.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{images/accmem2.png}}
\caption{Memory prediction accuracy}
\label{fig:accmem}
\end{figure}

\begin{table}[htbp]
\begin{center}
\caption{Average MSE/MAE comparison for resource prediction accuracy for transfer learning experiments}
\begin{tabular}{c|cc||cc|}
\cline{2-5}
 & \multicolumn{2}{c|}{\textbf{CPU/GPU}} & \multicolumn{2}{c|}{\textbf{Memory}} \\ \hline
\multicolumn{1}{|c|}{\textbf{Model}} & \multicolumn{1}{c|}{\textbf{MSE}} & \textbf{MAE} & \multicolumn{1}{c|}{\textbf{MSE}} & \textbf{MAE} \\ \hline \hline
\multicolumn{1}{|c|}{\textbf{All}} & \multicolumn{1}{c|}{1.22} & \textbf{0.93} & \multicolumn{1}{c|}{2.76} & \textbf{1.27} \\ \hline
\multicolumn{1}{|c|}{\textbf{All FT}} & \multicolumn{1}{c|}{48.83} & 6.67 & \multicolumn{1}{c|}{99.44} & 8.88 \\ \hline
\multicolumn{1}{|c|}{\textbf{All-but-one}} & \multicolumn{1}{c|}{1.97} & 1.11 & \multicolumn{1}{c|}{8.66} & 2.39 \\ \hline
\multicolumn{1}{|c|}{\textbf{All-but-one FT}} & \multicolumn{1}{c|}{\textbf{1.19}} & 0.94 & \multicolumn{1}{c|}{\textbf{2.40}} & \textbf{1.27} \\ \hline
\multicolumn{1}{|c|}{\textbf{GC19}} & \multicolumn{1}{c|}{10.21} & 2.92 & \multicolumn{1}{c|}{6.44} & 2.07 \\ \hline
\multicolumn{1}{|c|}{\textbf{GC19 FT}} & \multicolumn{1}{c|}{327.91} & 16.54 & \multicolumn{1}{c|}{33.72} & 5.32 \\ \hline
\multicolumn{1}{|c|}{\textbf{Random}} & \multicolumn{1}{c|}{5.36} & 2.17 & \multicolumn{1}{c|}{83.73} & 8.52 \\ \hline
\end{tabular}%
\label{tab:accuracycomparison}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{images/tlaccmem.png}}
\caption{Memory prediction accuracy for transfer learning experiments}
\label{fig:tlaccmem}
\end{figure}

Similarly, we show the results for the TL experiments in Table \ref{tab:accuracycomparison} and Fig. \ref{fig:tlaccmem}. From the accuracy point of view, we can see the benefits of training a model using multiple datasets and more data, even for prediction on unseen datasets. Again, the FT degrades the quality of the prediction also from an accuracy point of view. Moreover, in contrast with what we discussed in the previous experiments, we do not see any advantage in training the model using datasets from the same cluster type. However, the models generalize better when more datasets are used for the training.

Furthermore, we should also consider that MSE and MAE are symmetric metrics. Regarding the QoS, we should prefer models that achieve a higher SR than the target confidence level rather than a lower one, even if the MSE and MAE are lower. For instance, we should favour a model that achieves a 96\% SR compared to 94\% w.r.t. a 95\% confidence level, despite the score being the same in terms of MSE and MAE. For this reason, asymmetric metrics would be more suitable to evaluate the model's accuracy combined with the QoS we aim to provide to the customers.


% \begin{figure*}[htbp]
% \centerline{\includegraphics[width=0.9\linewidth]{images/multibivariate.png}}
% \caption{Multibivariate model}
% \label{fig:multibivariate}
% \end{figure*}

% \begin{figure*}[htbp]
% \centerline{\includegraphics[width=0.9\linewidth]{images/multiunivariate.png}}
% \caption{Multiunivariate model}
% \label{fig:multiunivariate}
% \end{figure*}

% \begin{figure*}[htbp]
% \centerline{\includegraphics[width=0.9\linewidth]{images/singlebivariate.png}}
% \caption{Single bivariate model}
% \label{fig:singlebivariate}
% \end{figure*}

% \begin{figure*}[htbp]
% \centerline{\includegraphics[width=0.9\linewidth]{images/singleunivariate.png}}
% \caption{Single univariate model}
% \label{fig:singleunivariate}
% \end{figure*}

\subsection{Runtime Performance}

\begin{table*}[htbp]
\begin{center}
\caption{Runtime performance analysis. In italics the best model for each subgroup.}
\begin{tabular}{c|cccc|cccc|c|}
\cline{2-10}
 & \multicolumn{4}{c|}{ \makecell{\textbf{Training} \\ \textbf{Time [s]}}} & \multicolumn{4}{c|}{ \makecell{\textbf{Fine-Tuning} \\ \textbf{Time [s]}}} &  \makecell{\textbf{Inference} \\ \textbf{Time [s]}} \\ \hline
\multicolumn{1}{|c|}{\textbf{Model}} & \multicolumn{1}{c|}{20\%} & \multicolumn{1}{c|}{40\%} & \multicolumn{1}{c|}{60\%} & 80\% & \multicolumn{1}{c|}{6} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{18} & 24 & 1 sample \\ \hline
\multicolumn{1}{|c|}{\textbf{S-U-LSTM}} & \multicolumn{1}{c|}{\textit{12}}& \multicolumn{1}{c|}{\textit{21}}& \multicolumn{1}{c|}{\textit{29}}& \multicolumn{1}{c|}{\textit{48}}& \multicolumn{1}{c|}{5.38}& \multicolumn{1}{c|}{7.4}& \multicolumn{1}{c|}{12.77}& \multicolumn{1}{c|}{9.29}& \multicolumn{1}{c|}{0.0002} \\ \hline
\multicolumn{1}{|c|}{\textbf{S-U-HBNN}} & \multicolumn{1}{c|}{35}& \multicolumn{1}{c|}{140}& \multicolumn{1}{c|}{268}& \multicolumn{1}{c|}{325}& \multicolumn{1}{c|}{\textit{1.57}}& \multicolumn{1}{c|}{\textit{1.63}}& \multicolumn{1}{c|}{\textit{1.7}}& \multicolumn{1}{c|}{\textit{1.64}}& \multicolumn{1}{c|}{0.0058} \\ \hline
\multicolumn{1}{|c|}{\textbf{S-U-LSTMD}} & \multicolumn{1}{c|}{15}& \multicolumn{1}{c|}{25}& \multicolumn{1}{c|}{38}& \multicolumn{1}{c|}{52}& \multicolumn{1}{c|}{2.49}& \multicolumn{1}{c|}{2.32}& \multicolumn{1}{c|}{2.22}& \multicolumn{1}{c|}{1.68}& \multicolumn{1}{c|}{\textit{0.00002}} \\ \hline \hline
\multicolumn{1}{|c|}{\textbf{S-B-LSTM}} & \multicolumn{1}{c|}{11}& \multicolumn{1}{c|}{21}& \multicolumn{1}{c|}{32}& \multicolumn{1}{c|}{\textit{53}}& \multicolumn{1}{c|}{3.03}& \multicolumn{1}{c|}{\textit{1.61}}& \multicolumn{1}{c|}{\textit{1.34}}& \multicolumn{1}{c|}{\textit{1.54}}& \multicolumn{1}{c|}{0.0002} \\ \hline
\multicolumn{1}{|c|}{\textbf{S-B-HBNN}} & \multicolumn{1}{c|}{57}& \multicolumn{1}{c|}{106}& \multicolumn{1}{c|}{154}& \multicolumn{1}{c|}{172}& \multicolumn{1}{c|}{\textit{1.84}}& \multicolumn{1}{c|}{2.01}& \multicolumn{1}{c|}{1.88}& \multicolumn{1}{c|}{2.01}& \multicolumn{1}{c|}{\textit{0.0001}} \\ \hline
\multicolumn{1}{|c|}{\textbf{S-B-LSTMD}} & \multicolumn{1}{c|}{\textit{10}}& \multicolumn{1}{c|}{\textit{16}}& \multicolumn{1}{c|}{\textit{25}}& \multicolumn{1}{c|}{237}& \multicolumn{1}{c|}{3.17}& \multicolumn{1}{c|}{4.06}& \multicolumn{1}{c|}{2.98}& \multicolumn{1}{c|}{2.26}& \multicolumn{1}{c|}{\textit{0.0001}} \\ \hline \hline
\multicolumn{1}{|c|}{\textbf{M-U-LSTM}} & \multicolumn{1}{c|}{1121}& \multicolumn{1}{c|}{947}& \multicolumn{1}{c|}{869}& \multicolumn{1}{c|}{1050}& \multicolumn{1}{c|}{2.54}& \multicolumn{1}{c|}{2.44}& \multicolumn{1}{c|}{2.37}& \multicolumn{1}{c|}{3.41}& \multicolumn{1}{c|}{\textit{0.0001}} \\ \hline
\multicolumn{1}{|c|}{\textbf{M-U-HBNN}} & \multicolumn{1}{c|}{\textit{486}}& \multicolumn{1}{c|}{\textit{425}}& \multicolumn{1}{c|}{\textit{530}}& \multicolumn{1}{c|}{\textit{621}}& \multicolumn{1}{c|}{\textit{1.7}}& \multicolumn{1}{c|}{\textit{1.63}}& \multicolumn{1}{c|}{\textit{1.63}}& \multicolumn{1}{c|}{1.79}& \multicolumn{1}{c|}{\textit{0.0001}} \\ \hline
\multicolumn{1}{|c|}{\textbf{M-U-LSTMD}} & \multicolumn{1}{c|}{714}& \multicolumn{1}{c|}{518}& \multicolumn{1}{c|}{706}& \multicolumn{1}{c|}{836}& \multicolumn{1}{c|}{4.92}& \multicolumn{1}{c|}{2.3}& \multicolumn{1}{c|}{5.35}& \multicolumn{1}{c|}{\textit{1.49}}& \multicolumn{1}{c|}{\textit{0.0001}} \\ \hline \hline
\multicolumn{1}{|c|}{\textbf{M-B-LSTM}} & \multicolumn{1}{c|}{1441}& \multicolumn{1}{c|}{\textit{1323}}& \multicolumn{1}{c|}{\textit{1603}}& \multicolumn{1}{c|}{1602}& \multicolumn{1}{c|}{\textit{2.29}}& \multicolumn{1}{c|}{\textit{1.73}}& \multicolumn{1}{c|}{11.81}& \multicolumn{1}{c|}{5.45}& \multicolumn{1}{c|}{0.0002} \\ \hline
\multicolumn{1}{|c|}{\textbf{M-B-HBNN}} & \multicolumn{1}{c|}{2123}& \multicolumn{1}{c|}{3210}& \multicolumn{1}{c|}{3230}& \multicolumn{1}{c|}{2281}& \multicolumn{1}{c|}{2.54}& \multicolumn{1}{c|}{2.3}& \multicolumn{1}{c|}{2.6}& \multicolumn{1}{c|}{\textit{2.78}}& \multicolumn{1}{c|}{\textit{0.0001}} \\ \hline
\multicolumn{1}{|c|}{\textbf{M-B-LSTMD}} & \multicolumn{1}{c|}{\textit{954}}& \multicolumn{1}{c|}{1559}& \multicolumn{1}{c|}{1477}& \multicolumn{1}{c|}{\textit{1592}}& \multicolumn{1}{c|}{3.41}& \multicolumn{1}{c|}{3.0}& \multicolumn{1}{c|}{\textit{2.09}}& \multicolumn{1}{c|}{3.33}& \multicolumn{1}{c|}{\textit{0.0001}} \\ \hline
\end{tabular}%
\label{tab:runtime}
\end{center}
\end{table*}

The applicability of these DL models to real-world scenarios strongly depends on the time necessary for training and deploying the model in a cloud resource management setup.
The three critical aspects in determining the usability of DL models are the training time, the fine-tuning time (i.e. how often we refresh the network weights with newly available data) and the inference time. The training time depends, for instance, on the size of the training set; the fine-tuning time depends on how often we retune the weights of the deep learning model, and the inference time is related to the forecast time once the model is trained. We measure these three metrics by varying the size of the training set in 20\%, 40\%, 60\% and 80\%, by changing the number of steps among 6, 12, 18 and 24 for the fine-tuning time, which correspond to 30, 60, 90 and 120 minutes frequency and we measure the inference time of predicting one sample. The results are computed as an average of 10 runs for all the preprocessed traces. Table \ref{tab:runtime} lists all the time measurements in seconds for all the combinations of the models. We omit the measures for the TL part, where the speed of convergence of the network is strictly correlated to the size of the training set, and it is applied only to the HBNN model (the DL architecture is the same). As we can see, the models take more or less the same time for the fine-tuning and inference steps, with the HBNN often being the faster architecture. The training time, instead, varies with the type of training and the prediction. The HBNN is the slowest model, except for the univariate version trained with multiple datasets. However, the training phase generally is infrequent and done offline, e.g. overnight, so it is not a critical factor. At the same time, fine-tuning and inference are the most frequent actions in resource management operations. We would also like to underline that the results for the S-U versions refer to the training of one single (trace, resource) pair, which means that we need to run this phase 24 times (12 clusters $\times$ 2 resources). This also applies to the S-B (12 times) and M-U versions (2 times). With this runtime analysis, we observe that all the models can be practically deployed in real-world scenarios, with the advantage of having a model trained with multiple datasets, which does not require any parallelization of the systems for each possible cluster cell.

% \begin{figure*}[htbp]
% \centerline{\includegraphics[width=0.9\linewidth]{images/comparison.png}}
% \caption{Training strategies comparison}
% \label{fig:comparison}
% \end{figure*}

% \begin{figure*}[htbp]
% \centerline{\includegraphics[width=0.9\linewidth]{images/comparison2.png}}
% \caption{Training strategies comparison without random initialization}
% \label{fig:comparisonnorandom}
% \end{figure*}

% \begin{figure*}[htbp]
% \centerline{\includegraphics[width=0.9\linewidth]{images/comparison3.png}}
% \caption{Training strategies comparison without random initialization and GC19 FT}
% \label{fig:comparisonnogc19ft}
% \end{figure*}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{images/tot_res-cpu.eps}}
% \caption{Total Predicted Resources for CPU}
% \label{fig:tprcpu}
% \end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{images/tot_res-mem.eps}}
% \caption{Total Predicted Resources for memory}
% \label{fig:tprmem}
% \end{figure}



% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{images/succVSconf-cpu.eps}}
% \caption{Success Rate VS Confidence Level for CPU}
% \label{fig:svsccpua}
% \end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{images/succVSconf-mem.eps}}
% \caption{Success Rate VS Confidence Level for memory}
% \label{fig:svscmema}
% \end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{images/std-cpu_a.eps}}
% \caption{Standard deviation comparison for CPU}
% \label{fig:stdcpua}
% \end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{images/std-mem_a.eps}}
% \caption{Standard deviation comparison for memory}
% \label{fig:stdmema}
% \end{figure}

\section{Conclusion and Future Work}\label{sec:conclusion}

Probabilistic forecasting and uncertainty modelling provide a broader picture to the resource manager in a cloud computing environment, helping the scheduling decision process. In this paper, we analysed the performance of probabilistic models that capture the uncertainty in the prediction by forecasting a probability distribution of the future demand. We evaluated univariate and bivariate models for predicting processing units and memory usage by training the models with one or more related time series from Google Cloud and Alibaba traces. We also investigated the TL technique and the generalisation capabilities of the models for related but unseen historical workload data. We showed that the training phase using multiple datasets allows the models to achieve better performance regarding resource prediction efficiency and accuracy w.r.t. a target QoS level and to reach good results for more challenging tasks such as bivariate forecasting.
Moreover, the TL method allows predictions on unseen traces in the training phase, opening the chance of not retraining from scratch a model when new cluster cells are available or waiting for enough historical data. Finally, we analysed the runtime performance of the models for their deployment in practical applications. This analysis showed that the models could be practically applied to predict the demand for future resources.

In future, we would like to analyse further the performance of the HBNN model in the case of different loss functions, including non-symmetrical loss, to address the target QoS level in the training phase and compare it to a full Bayesian version of the network, also in the case of multi-step-ahead prediction. We will also focus on extending the TL concept to a machine-level workload forecast to understand and analyse the amount of training data and their representativeness necessary for the model to reach high generalisation capabilities. Finally, the predictions can be exploited for scheduling, resource allocation, scaling and workload balancing problem for managing VMs in a cloud computing environment, integrating the predictive model and the resource allocator in the same pipeline.



% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{First}

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


This work was conducted with the financial support of Science Foundation Ireland under Grant Nos. 18/CRT/6223, 16/RC/3918 and 12/RC/2289-P2. which are co-funded under the European Regional Development Fund; by TAILOR (GA No. 952215), a project funded by EU Horizon 2020 research and innovation programme, and by the Google Cloud Research Credits program with the award GCP203677602.
The authors thank Dr Diego Carraro from Insight Centre of Data Analytics, University College Cork, for helpful conversation and feedback. Andrea Rossi is the corresponding author.

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliographystyle{IEEEtranN}
\bibliography{IEEEabrv, references}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\newpage

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/AndreaRossi.jpeg}}]{Andrea Rossi}
% \begin{IEEEbiography}{Andrea Rossi}
is a PhD student at the SFI Centre for Research Training in Artificial Intelligence at University College Cork since October 2019.
He got a bachelor's in Information Engineering and a master's in Computer Engineering at the University of Padua (Italy). His research is focused on time series analysis and predictive models for resource management problems in cloud computing environments.\end{IEEEbiography}

\vspace{-10cm}

% if you will not have a photo at all:
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/AndreaVisentin.jpeg}}]{Andrea Visentin}
has a BSc and an MSc in Computer Engineering from the University of Padua (Italy) and completed a PhD at the Insight Centre for Data Analytics at UCC. He is currently a permanent lecturer at the School of Computer Science \& IT. Moreover, he is a researcher at the Confirm Centre for Smart Manufacturing and the Insight Centre for Data Analytics.
\end{IEEEbiography}

\vspace{-10cm}

% insert where needed to balance the two columns on the last page with
% biographies
% \newpage

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/StevenPrestwich.jpeg}}]{Steven Prestwich}
is a Lecturer in the School of Computer Science \& IT, University College Cork, Ireland. He has a PhD in Computer Science from the University of Manchester (UK) and an MA in mathematics from the University of Oxford (UK). He is currently an investigator in the Insight Centre for Data Analytics and the Confirm smart manufacturing centre. His current work includes deep learning, forecasting and supply chain optimisation and constraint acquisition.
\end{IEEEbiography}

\vspace{-10cm}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/KenBrown.jpeg}}]{Kenneth N. Brown}
is a Professor in the School of Computer Science \& IT, Deputy Director of Insight@UCC and he co-leads the research challenge on decision making. He is a Funded Investigator on Confirm, the national centre for smart manufacturing. He is a PI and executive committee member of Enable, the inter-centre spoke on smart cities and IoT. His research interests are in the areas of artificial intelligence, optimisation and constraint-based reasoning.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}
\end{document}


