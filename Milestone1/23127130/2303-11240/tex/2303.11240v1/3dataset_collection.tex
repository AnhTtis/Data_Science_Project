

We  now  discuss  our  methodology  in building  our dataset. We use a custom-built crawler utilizing selenium due to TruthSocial's lack of public API. Our crawler builds a network of Truth Social users, with each user connected to other users via following edges. Additionally, a user's Truths (posts) are linked to that user via a foreign-key relationship. Finally, when crawling a user's Truths, information related to media, external URLs, hashtags, users tagged, replies, ReTruths, and quotes is collected.

\textbf{Crawling.}
Our data collection procedure works as follows. Utilizing a self-populating queue of users (initialized with just the user @realDonaldTrump) as a driver, we generate a new thread for each user in the queue and perform the following actions:
  \begin{enumerate}
      \item Collect information about the user's follower count, following count, creation date.
      \item Iterate through users following the user, creating an edge for each user-to-user relationship, and adding that user to the queue if that user has not been scraped.
      \item Iterate through users the user follows, creating an edge for each user-to-user relationship, and adding that user to the queue if that user has not been scraped.
      \item Iterate through the user's Truths.   
  \end{enumerate}
After the initial crawl, foreign relationships between Truths and ReTruths, Quotes, and Replies needed to be further resolved. This is due to the fact that during the crawling phase, resolving every foreign relationship (e.g., every time a ReTruth entry was created, finding the original Truth and creating an entry for it), was found to be inefficient and often led to an increase in queries that caused Truth Social to temporarily deny the web scraper access. Thus, when ReTruths, Quotes, or Replies entries were created, the external (Truth Social's) ID of Truth they were directed towards was saved. Then, utilizing these external IDs, the respective Truth entries were later created, and the foreign relationship resolved.

During this crawling process, data points were saved in a PostgreSQL server, separated into the following tables:
\begin{itemize}
    \item users
    \item follows
    \item truths
    \item quotes
    \item replies
    \item media
    \item hashtags
    \item external\_urls
    \item truth\_media\_edges
    \item truth\_hashtag\_edges
    \item truth\_external\_url\_edges
    \item truth\_user\_tag\_edges
\end{itemize}

A data-cleaning phase followed the initial crawl; the tables from the PostgreSQL table were exported to tsv files, and python was used to clean.
First, relationships between Truths and ReTruths were resolved. If a Truth entry is a ReTruth of another Truth, the ReTruthing Truth’s “truth\_retruthed” field utilizes a foreign key to point to the ID of the original Truth. It was found that resolving this relationship during the initial crawling phase (finding and creating an entry for the original Truth that the current Truth entry is a ReTruth of) was inefficient and often led to an increase in queries that caused Truth Social to temporarily deny the web scraper access to its services. The workaround  to this issue was to save the original Truth’s data in the “text”, “like\_count”, “reply\_count”, “original\_author\_username”, and “retruth\_count” fields in the ReTruthing Truth’s entry (this was possible due to the fact that a ReTruth does not have any of these attributes, so saving this data did not degrade the information about the ReTruthing Truth entry), and to then resolve these relationships during the data-cleaning phase, creating entries for the original Truths and updating their information as necessary. It should be noted that the timestamp of the original Truth was unable to be saved in the initial crawling phase without facing these data query issues, and so their timestamp fields were set to “-1” to indicate that the information is not available. Additionally, this “-1” sentinel is employed in the “truth\_retruthed” field for Truth entries that are not ReTruths. Finally, a user entry was then created for the original Truth’s author if the user did not already exist. The process of creating user entries during the data-cleaning phase is discussed below.

Second, following relationships between users were then resolved. During the initial crawling phase, followed-following edge entries included foreign keys to both users if entries both users existed. However, similar to the issue faced with resolving ReTruth relationships during the crawling phase, it was found that—due to query limits—creating new, complete entries for unseen users was problematic because of the increase in queries this caused when crawling the often thousands of following relationships for each user. As such, the usernames of the following and the followed users were saved in each edge entry. Then, during the data-cleaning phase, if either of the usernames (which, should be noted, must be unique) did not correspond to a user entry, a new user entry was created, and the foreign key to the user was resolved. During the creation of these user entries, a “-1” sentinel was employed to indicate when a field lacked information.
 


% \begin{algorithm}
%     \caption{User Scrape}
%     \begin{algorithmic}[1]
%         \For {user in user\_queue}
%             \State Save basic user info
%             \State Get list of followers
%             \For {follower\_user in followers}
%             \State Create follower edge
%             \If {follower\_user not scraped}
%                 \State Add follower\_user to queue
%             \EndIf
%             \EndFor
%             \State Get list of following
%             \For {following\_user in following}
%             \State Create follower edge
%             \If {following\_user not scraped}
%                 \State Add following\_user to queue
%             \EndIf
%             \EndFor
%             \State Get list of truths
%             \For {truth in truths}
%             \State Save truth information
%             \EndFor
%         \EndFor
%     \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% 	\caption{Tim Weninger Sucks}
% 	\begin{algorithmic}[1]
% 		\For {Every time step}
% 		\State Calculate How Much Tim Weninger Sucks
% 		\For {All map measurements from $\mathbf{x}_{Map}$}
% 		\State Denormalize \textbf{Suck} Score (Eq.: 3.14)
% 		\State Add margin of \textbf{Suck} Safety (Eq.: 3.15)
% 		\State Calculate \textbf{Suck} difference $\Delta h_{ObsSafe_{j}}$ to aircraft (Eq.: 3.16)
% 		\If {$\Delta h_{ObsSafe_{j}}>0$}
% 		\State Add \textbf{Suck} measurement to set of critical measurements $\mathcal{M}_{crit}$ (Eq.: 3.17)
% 		\EndIf
% 		\EndFor
% 		\For {All \textbf{Suck} measurements in $\mathcal{M}_{crit}$}
% 		\State Calculate local obstacle \textbf{Suck} vector (Eq.: 3.20)
% 		\EndFor
% 		\State Sum over all local \textbf{Suck} vectors (Eq.: 3.22)
% 		\State Transform to global \textbf{Suck} frame to receive $\mathbf{x}_{oaCmd}$ (Eq.: 3.23)
% 		\State Calculate obstacle \textbf{Suck} weight $w_{oa}$ based on critical zone weight (Eq.: 3.24)
% 		\State Calculate target \textbf{Suck} weight $w_{ts}$ as $1-w_{oa}$ (Eq.: 3.13)
% 		\State Calculate \textbf{Suck} vector $\mathbf{x}_{HSaCmd}=w_{oa}\mathbf{x}_{oaCmd}+w_{ts}\mathbf{x}_{tsCmd}$ (Eq.: 3.11)
% 		\EndFor
% 	\end{algorithmic} 
% \end{algorithm} 




% \textbf{Data.}

\textbf{Limitations.}
The crawler was met with several key limitations.
\begin{enumerate}
    \item \textbf{Access to a user's followers is limited.} While Truth Social allows clients on its web application to scroll through the entirety of a user's following list, it limits clients' access to a user's followers list to only 50 followers. It has not been determined why this limitation exists and how these 50 followers are selected.
    \item \textbf{Query Limits.} When scraping Truth Social, we found that we had to limit the number and frequency of queries to avoid our queries being rejected by Truth Social's security. As such, we had to limit the speed of our crawler.
\end{enumerate}
% \begin{itemize}
%     \item Cannot get all followers.
%     \item BFS structure -- may not have all connections from retruth to truth
%     \item Not all users are accompanied by their respective truths -- created user entries after the fact
% \end{itemize}

\textbf{Ethical Considerations.}
The dataset is intended as an academic resource only, and was created for analysis of  extremism in social networks. 



% We use a custom-built crawlerthat accesses the (undocumented, but open) Parler API. Thiscrawler was based on Parler API discoveries that allowed forfaster crawling (donkenby 2020).