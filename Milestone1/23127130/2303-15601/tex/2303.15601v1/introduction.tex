\section{Introduction}\label{sec:Intro}
\subsection{Lady tasting tea, revisited.} Muriel Bristol, a biologist at Rothamsted Research at the dawn of last century, once claimed that she could taste whether a cup of tea was prepared by pouring milk first. Ronald Fisher, in an attempt to disprove her claim, arranged the following simple experiment: Bristol was presented with eight cups of tea, half of which prepared by pouring milk first, and she was asked to taste them one by one and identify the correct ones. This episode became widely known as the ``lady tasting tea" experiment, the very first example appearing in Fisher's seminal book \cite{fisher1936design} on the design of statistical experiments. Its analysis of the experiment is now a cornerstone of scientific thinking, being also the first appearance of the expression ``null-hypothesis" in Fisher's work. In this case, it assumes that Bristol's guesses are random, so that that the distribution of the score follows an hypergeometric distribution. In average, one expects four correct guesses, the underlying probability distribution being well-understood.
\\ \\
In the original experiment, Bristol did not receive any kind of feedback during the experiment: what if she was told the correct answer after each attempt? Clearly, since she knows the exact number of cups of each type, she can always guess the one that appeared the smallest number of times so far, and therefore increase her likelihood of a correct guess at each step. This does not require any special ability on her side, other than a clever exploitation of the information she is provided with. There has been a substantial flurry of interest in variations of this kind, owing to the connection with randomized clinical trials \cite{Blackwell1957} and the testing of claims of extra-sensory perceptions \cite{diaconis1978statistical}, which we will review later. While most of the focus has been on the asymptotic expected score for large experiments, it is clear that a rigorous analysis of the experiments requires the understanding of the fluctuations of the score. This is the focus of our paper. 
\subsection{Model and main result}\label{subs:MainResult}
Consider a well-shuffled deck of card consisting of $n$ different types of cards where the card type $i$ appears $m_i=m_i(n)$ times. Assume that, for all $n$, the deck is shuffled so that all arrangements of the decks are equally likely. Let ${\bf m}={\bf m} (n)$ denote the vector ${\bf m}=(m_1, \ldots, m_n)$ of multiplicity of the types in the deck. We also denote by $\abs{\bf{m}}$ the total number of cards in the deck, that is, $\abs{\bf{m}}=\sum_{i=1}^{n}m_i$.
Consider the following \emph{complete feedback game}: a player is asked to guess the type of the card appearing on top of the deck. After each guess the top card is revealed to the player. The game continues until the deck is exhausted. Let $S_{{\bf m}(n)}$ be the total number of correct guesses (also referred to as the score) at the end of the game. Obviously $S_{{\bf m}(n)}$ depends upon player's strategy. For example, if the guesses keeps guessing the card type to be $i$, then $S_{{\bf m}(n)}=m_i$. It is shown in \cite{DG81} that the the greedy algorithm maximizes the expected number of correct guesses, that is to say, a player should guess a card that has the maximum multiplicity in the remaining deck. We will refer to the greedy algorithm as the `optimal strategy' throughout this paper, and we will tacitly assume that the player is performing this strategy. Our main result is a central limit theorem (CLT) for the optimal score $S_{{\bf m}(n)}$ with a Berry-Esseen bound.
\begin{theorem}
\label{thm:CLT}
Let ${\bf m}(n)$ be a sequence of decks for which $m(n)=\max_{i\in [n]} m_i(n)\leq m$ for some fixed $m$, and for which the fraction $\epsilon(n)$ of types $i$ that appear with multiplicity $m(n)$ satisfies $\epsilon(n)\geq \epsilon$ for some positive $\epsilon$. Let $S_{{\bf m}(n)}$ be the total number of correct guesses under the greedy/optimal strategy. Then:
\begin{itemize}
    \item The mean $\mu_n:=\E{S_{{\bf m}(n)}}$ and the variance $\sigma_n^2\coloneqq \Var{S_{{\bf m}(n)}}$ satisfy 
    \begin{align*}
        \mu_n\sim \sigma^2_n\sim \left(1+\ldots+\frac{1}{m(n)}\right)\ln n.
    \end{align*}
    as $n\rightarrow +\infty$.
    \item There exists a constant $C=C(\epsilon, m)$ such that
    \begin{align*}
    \sup_{x\in\mathbb R}\left|\mathbb P\left(\frac{S_{{\bf m}(n)}-\mu_n}{\sigma_n}\leq x\right)-\Phi(x)\right|\leq C\,\frac{\ln\ln n}{\sqrt{\ln n}}
    \end{align*}
where $\Phi$ is the cumulative distribution function of a standard normal random variable.
\end{itemize}
\begin{remark}
In particular, the Theorem applies to the case $m_i(n)\equiv m$ for a fixed $m$. The result about the mean was already shown in \cite{DG81, he2021card}.
\end{remark}
\end{theorem}
%\begin{remark}
%Our proof should also hold when $m=m(n)$ grow at a suitably slow rate. Indeed, the main ingredient comes from \cite{he2021card}, in particular Theorem \ref{thm:poissonapp}, that continues to hold for certain sequences $m=m(n)$ for which their error term go to zero. Since the dependence on $m$ of the implicit constant in their work is not explicit, it is hard to guess when the method would break down. Because of the  presence of the $n^{-1/m}$ term in the error term for $j=m-1$, it is natural to guess that $m\approx \ln n$ is already a natural barrier for at least part of our argument (though, in principle, the implicit constant may help). 
%\end{remark}
%\textcolor{red}{Probably the theorem can be strengthened to obtain a rate of convergence from our proof! Also, from the remark it seems that one may allow $m=o(\ln(n))$. If proof works as long as $m=o(\ln(n))$, it is worth pointing out. Is there an obvious reason why, say $m=\log(n)$ case things should break?}


\subsection{Related literature}\label{subsec:Lit}
The complete feedback game was originally motivated by clinical trials. For an in-depth discussion about the problem, a good reference is \cite{efron1971forcing}, though the first appearance is in a work by Blackwell and Hodges \cite{Blackwell1957}. They considered the case where two types of treatments have to be assigned to a fixed number of people, say $2m$, who arrive one by one at the clinic. They were interested in the case where both treatments are provided in the same quantity and in a random order. However, they assume that the hospital may decide, to their discretion, whether to rule out some of the subjects because of their medical conditions. Since they have information on the treatments provided up to that point, they may decide to bias the result of the experiment toward a specific treatment. This can be done by ruling out a particularly sick subject if they know that it is more likely that their favorable treatment has to appear next. \\ \\
In our language, this is precisely the complete feedback case with $n=2$, with ${\bf m} =(m,m)$. The authors in \cite{Blackwell1957} gave an asymptotic formula for the optimal expected score (in their language, the selection bias), which was then extended by \cite{DG81} to the generic case ${\bf m}=(m_1, \ldots, m_n)$ with $n$ fixed. As for the fluctuations, the latter reference shows that, for $n=2$ and the ${\bf m}=(m,m)$ with $m$ large, the limiting optimal score satisfies a central limit theorem. On the other hand, in the unbalanced case where ${\bf m}=(m_1, m_2)$ where $m_1, m_2$ grow with $m_1/m_2\rightarrow p\neq 1/2$, they show that the fluctuations of the optimal score are not Gaussian. Related results in the case $m=2$ also appeared in \cite{kuba2023card}.\\ \\
Another occurrence of the complete feedback game is related to the rigorous analysis for extra-sensory perception claims. In fact, one of the most celebrated experiment in this direction corresponds precisely to the complete feedback game with a deck of twenty-five cards (Zener cards), with five symbols each appearing five times. For an historical account, the interested reader is referred to \cite{diaconis1978statistical}. Motivated by this, in \cite{DG81} the authors suggest to study the asymptotic optimal expected score for the complete feedback game with decks ${\bf m}=(m_1, \ldots, m_n)$ where $n$ grows. In the case $m_i\equiv 1$, the analysis becomes much simpler since the sequence of guesses become independent. In particular, it is easy to deduce that one obtains about $\ln n$ correct guesses in expectation, with a variance of the same order and normal fluctuations.\\ \\
The case where some of the $m_i$ are greater than one is more subtle, since the chance of a correct guess will depend on the history of the draws up to that moment. In \cite{diaconis_partialfeedback}, the authors analyzed the case where $m_i\equiv m$ is fixed and $n$ grows to infinity, showing that asymptotically the expected optimal score is $\left(1+\ldots+ 1/m\right)\ln n$. The result was substantially refined by the first author and He in \cite{he2021card}, where the expected score is determined for decks ${\bf m}=(m_1,\ldots, m_n)$ under the same assumptions of Theorem \ref{thm:CLT}. Moreover, their asymptotic result matches the optimal expected score up to an explicit error that goes to zero. Their main tool is the analysis of a certain variation of the birthday problem via Stein's methods, which will be our main tool here as well. In the case $m_i\equiv m$ where both $m$ and $n$ are growing, the asymptotic for the expected optimal score was obtained by the first author and Steinerberger in \cite{ottolini2022guessing}, covering a variety of regimes that include the case $n=m$ (the original Zener's setting).
\\ \\
Variations of the game also include different types of feedback, the most relevant case being that of a yes/no feedback (i.e., the card is shown only when a guess is correct). This becomes much harder to analyze even for balanced decks ${\bf m}=(m_1,\ldots, m_n)$ with $m_i\equiv m$ types, as it is known \cite{DG81} that the optimal strategy is not the greedy one as soon as $m>1$ and $n>2$. Some limiting results were recently obtained in \cite{diaconis_partialfeedback, nie2022number}, where they show, for instance, that the expected optimal score for $n\gg m \gg 1$ is of the form $m+\Theta(\sqrt m)$ uniformly in $n$. Results on the fluctuations are currently unknown -- except in the case $m=1$ shown in \cite{DG81}, where the limiting distribution has a non-normal behavior. Since the optimal strategy is rather hard to implement, a fact ultimately due to its connection with permanents \cite{chung1981permanents, diaconis2001statistical}, there has also been some interest in near-optimal strategies that are easier to implement \cite{diaconis2022guessing}. 
\\ \\
Finally, we mention some other variations of the game on a similar flavour. The problem of minimizing the expected number of correct guesses was also addressed in \cite{DG81, he2021card, diaconis_partialfeedback}, for both the complete feedback and the yes/no feedback. Another natural set of questions comes from considering decks that have not been properly shuffled, such as the case of a deck which has been riffle shuffled \cite{liu2021card}.