\section{Proofs}\label{sec:proof}
In the remainder of the paper, we will drop at times the dependence on the deck $\bf m$ and on $n$. The implicit constants in the notation $O, \Omega, \Theta, \lesssim$ will depend on $m$ and $\epsilon$ only, unless we specify otherwise. We will often identify $m(n)$ and $\epsilon(n)$ with their upper/lower bound $m$ or $\epsilon$, unless there is ambiguity.
\subsection{Main idea}
It will be convenient to define the following random variables.
\begin{itemize}
    \item $T_j=\max\{t\in \{0,1,
    \ldots, \abs{\bf{m}}\}:\text{No card among the \emph{last} $t$ appear more than $j$ times}\}$. Here $0\leq j\leq m$, with the convention $T_0=0$ and $T_m=\abs{\bf{m}}$. 
    \item $W_{j,t}=\sum_{{\bf t}\leq t} Y_{\bf t}$, where for each index $\bf t=(t_1,\ldots, t_{j+1})$ the binary random variable $Y_{\bf t}$ is one if and only if the cards at positions $\bf t$ are equal (here, positions are considered from the \emph{bottom} of the deck). Notice that $T_j>t$ if and only if $W_{j,t}=0$. Here, $1\leq t\leq \abs{\bf{m}}$ and $0\leq j\leq m$.
    \item $\widetilde W_j=W_{j-1, T_j}$ denotes the number of cards that appear $j$ times before some card appear $j+1$ times. Again, this is done from the \emph{bottom} of the deck. Here, $1\leq j\leq m$.
\end{itemize}
\begin{example}
Consider a deck ${\bf m}=(3,3,2)$, and assume that the sequence of cards extracted, listed from the last to the first, is 
\begin{align*}
    (1, 2, 2, 1, 3, 1, 2, 3).
\end{align*}
In this case, $T_1=2, T_2=5, \widetilde W_1=2, \widetilde W_2=2, \widetilde W_3=2$.
\end{example}
\begin{remark}\label{rem:mult}
Notice that $\widetilde W_m$ is equal to the fraction of types that appear with multiplicity $m$, which is at least $\epsilon n$ under the assumption of Theorem \ref{thm:CLT}.
\end{remark}
The random variables $W_{j, t}$ are important tools in understanding the asymptotic behaviour of total score. In fact, the main ingredient in \cite{he2021card} is an asymptotic result for the $W_{t, j}$, which behave like Poisson random variables with suitable parameters. This should come as no surprise, since the $Y_{\bf t}$s are indicator of rare events, most of which are weakly dependent. They obtain the following result.
\begin{theorem}[Theorem 1.8 \cite{he2021card}]
\label{thm:poissonapp}
Let $1\leq j<m$. Then, there exists $\lambda=\Theta\left(\frac{t^{j+1}}{n^j}\right)$ such that
\begin{align*}
    d_{tv}(W_{j,t}, P_{\lambda})\lesssim \frac{t}{n}\lesssim \left(\frac{\lambda}{n}\right)^{\frac{1}{j+1}}
\end{align*}
Here, $P_{\lambda}$ represents a Poisson random variable with mean $\lambda$, and $d_{tv}$ represents the total variation distance between probability measures (where we identify the a random variable with its law).
Moreover, the implicit constants in the error term and the definition of $\lambda$ can be chosen to depend only on $j$ and $\epsilon$, the fraction of types that appear with multiplicity $m$. 
\end{theorem}
\begin{remark}
In the case $m_i\equiv m$, one has $\lambda=\frac{t^{j+1}}{n^j}\frac{\binom{m}{j+1}}{m^{j+1}}$. Notice that, for fixed $j$, the second term converges to $\frac{1}{(j+1)!}$ as $m\rightarrow +\infty$.
\end{remark}
\begin{remark}
The theorem above can be thought as a variant of the classical birthday problem: in fact, for $j=1$ and $m_i(n)\equiv m(n)\rightarrow +\infty$, we obtain the best possible approximation for the classical birthday problem.
\end{remark}
In this section we state and prove some lemmas that will be useful in the proof of Theorem~\ref{thm:CLT}. We begin with a discussion of the idea of the proof. The total score $S_{\bf m}$ can be written as the sum of $\abs{\bf{m}}$ (the total size of the deck) binary random variables, namely, the indicators that the at a given time the player obtains a correct guess. Were these random variables independent, the CLT for the total score will follow at once. \\ \\ 
However, even in the case $m=2, n=2$, it is clear that if the second guess is correct, then the remaining two cards are distinct (hence the third guess will be correct with probability $1/2$) while if the second guess is wrong, then the remaining two cards are the same (hence the third guess will be correct with probability $1$). Intuitively, the dependence becomes weak for large $n$, and it should be related to the concentration properties of the random variables introduced above. Indeed, the strategy will change depending on how many card appear with a given multiplicity at a given time. The first crucial step towards the proof of CLT will be the observation that, conditioned on $\widetilde{W}_j$, the score can be written as a sum of independent random variables (Lemma~\ref{lem:TotalScore}). This allows us to prove a CLT for the conditional score with a uniform Berry-Essen bound (Lemma~\ref{lem:berryEssen}). \\ \\
The final issue is to understand the behavior of $\widetilde W_j$ and show that they enjoy suitable concentration. The main difficulties is that Theorem~\ref{thm:poissonapp} requires a \emph{fixed} time, rather than the random time $T_j$ appearing in the definition of the $\widetilde W_j$. Moreover, the random variables $T_j$ and $W_{j-1, t}$ are dependent meaning that we cannot expect a straightforward limiting result expressed in terms of a compound Poisson random variables. It is worth noticing that, while in \cite{he2021card} there is a multivariate version of Theorem \ref{thm:poissonapp}, it does not suffice for our purposes. To circumvent the problem, we will use a suitable monotonicity and concentration argument, which allows to prove the main Lemma \ref{lem:finalReduction}.
\subsection{A CLT for the conditional score}
We start by showing a useful representation of the optimal score. 
\begin{lemma}
\label{lem:TotalScore}
For any deck ${\bf m}$, the optimal score $S_{\bf m}$ can be written as
\begin{align*}
    S_{\mathbf{m}}=\sum_{j=1}^{m}\sum_{s=1}^{\widetilde W_j}X_{j,s}, 
\end{align*}
where the $X_{j,s}$ are conditionally independent -- given the $\widetilde W_j$'s -- Bernoulli random variables with $\mathbb P(X_{j,s}=1)=\frac{1}{s}$. 
\end{lemma}
\begin{proof}
Let $\tau_{j, s}$ be the time at which the maximum multiplicity of a symbol left in the deck is equal to $j$, there are exactly $s$ symbols with this multiplicity, and one of those is selected. Notice that a correct guess can only occur at the times $\tau_{j, s}$. If $X_{j, s}$ denotes the indicator that the guess at time $\tau_{j, s}$ is correct, then these are independent and
\begin{align*}
\mathbb P\left(X_{j,s}=1\right)=\frac{1}{s},
\end{align*}
regardless of the strategy chosen (to fix ideas, imagine that a randomly chosen symbol among the $s$ ones with the right multiplicity is chosen). Moreover, they are independent of the times $\tau_{s,j}$. Finally, observe that for each $j$, the number of relevant $\tau_{j,s}$ is $1\leq j\leq m$ and $1\leq s\leq \widetilde W_{j}$, so that we obtain conditional independence given the $\widetilde W_j$s. 
\end{proof}

\begin{remark}
Following Remark \ref{rem:mult}, our assumption on $\epsilon$ guarantees that the expected number of correct guesses is lower bounded by $\ln n+O(1)$. This can be seen by looking at $\widetilde W_m$ (i.e., guesses early on in the game). 
\end{remark}
\begin{remark}
Since $\widetilde W_{j}\geq 1$ for all $j$, we have the deterministic bound $S_{\bf m}\geq m$. These correspond to the correct guesses when there is only one card appearing with the maximum multiplicity, which eventually will result in a correct guess with certainty. %Of course, the bound $S_{\bf m}\geq m$ can also be deduced by the fact that even guessing always the same card with multiplicity $m$ will guarantee that number of correct guesses.
\end{remark}
For convenience, we will denote by $S_{\bf m}'$ the total score conditioned on the random variables $\widetilde{W}_j$ for $1\leq j\leq m$. Lemma~\ref{lem:TotalScore} says that $S_{\bf m}'$ is sum of independent Bernoulli random variable. We can leverage this to obtain the following result. 
\begin{lemma}
\label{lem:berryEssen}
Consider a deck as in the assumption of Theorem \ref{thm:CLT}. Let $S_{\bf m}'$ denote the total score conditioned on the $\widetilde W_j$s, and let $\mu'_n$ and $\sigma'_n$ denote the conditional mean and standard deviation. Then, uniformly over $\widetilde W_j$s, one has
\begin{align*}
    \left|\mathbb P\left(\frac{S'_{\bf m}-\mu'_n}{\sigma'_n}\leq x\right)-\Phi(x)\right|\lesssim \frac{1}{(\ln n)^{\frac{3}{2}}}
\end{align*}
where $\Phi(x)$ denotes the CDF of a standard normal random variable. 
\end{lemma}
\begin{proof}
By means of Lemma \ref{lem:TotalScore} and linearity of expectation, we can write
\begin{align*}
&\mu_n'=\sum_{j=1}^m\left(1+\ldots+\frac{1}{\widetilde W_j}\right), \\ &\sigma_n'^2=\sum_{j=1}^m\left[\left(1+\ldots+\frac{1}{\widetilde W_j}\right)-\left(1+\ldots+\frac{1}{
\widetilde W_j^2}\right)\right].
\end{align*}
Using the well-known facts 
\begin{align*}
    1+\ldots+\frac{1}{k}=\ln k+O(1), \quad \sum_{n=1}^{+\infty}\frac{1}{k^2}=\frac{\pi^2}{6}<\infty
\end{align*}
we deduce that
\begin{equation}\label{eq:condmean}
    \mu'_n=\sum_{j=1}^m \ln \widetilde W_j+O(1)
    ,\quad  
    \sigma'^2_n=\mu'_n+O(1).
\end{equation}
Moreover, using the fact that $\widetilde W_{m}\geq \epsilon n$ (see Remark \ref{rem:mult}) we deduce that
\begin{equation}\label{eq:useful}
    {\sigma'_n}^2\geq \ln n+O(1)
\end{equation}
uniformly over all realizations of the $\widetilde W_j$s. Since second and third moment of a Bernoulli random variable are the same, a standard Berry-Essen bound for non identically distributed random variables (see, e.g., \cite{shevtsova2010improvement}) gives  
\begin{align*}
    \left|\mathbb P\left(\frac{S'_{\bf m}-\mu'_n}{\sigma'_n}\leq x\right)-\Phi(x)\right|\lesssim \frac{1}{\sigma'^3_n}\lesssim \frac{1}{(\ln n)^{\frac{3}{2}}}
\end{align*}
\end{proof}
\begin{remark}\label{rem:mixture}
Notice that the proof already shows that the limiting fluctuations for the score $S_{{\bf m}(n)}$ are distributed as mixtures of normal random variables. In order to prove the result, it will suffice to show suitable concentration results for the conditional mean and variance. 
\end{remark}
\iffalse
The uniform error bound in the Berry-Essen theorem shows that it suffices to control.
\begin{align*}
    \frac{\mu'-\mu}{\sigma'}\rightarrow 0, \quad \frac{\sigma^2-\sigma'^2}{\sigma'^2}\rightarrow 0.
\end{align*}
Using the uniform lower bound on $\sigma'$ and $\sigma^2=\Var{\mu'}+\mathbb E(\sigma'^2)$, and $\mu'-(\sigma')^2=O(1)$, it suffices to show
\begin{align*}
    \frac{\Var{\mu'}}{\ln n}\rightarrow 0.
\end{align*}
\fi

%We also need to setup some notations before we proceed. We denote the unconditioned mean and expectation of $S_{\bf m}$ by $\mu$ and $\sigma$ respectively. Let $X_i$ be the score obtained from $i$th guess. Then, $S_{\bf m}=\sum_{i}X^{(n)}_i$. Let $\widetilde{X}_i:=\E{X^{(n)}_i\given (W_j)_j}$ and let $\widetilde{S}_n=\sum_{i}\widetilde{X}_i$. That is, $\widetilde{S}_i$ is the conditional score of the guesser given $W_j$s. We denote the expectation and variance of the conditional total score by $\mu_c, \sigma_c$ respectively. Then
\subsection{Remove the conditioning}
As pointed out in Remark \ref{rem:mixture}, the conditional CLT for $S_{\bf m}'$ shown in Lemma~\ref{lem:berryEssen} will suffice to our purposes if we can show a suitable concentration for the conditional means and variance $\mu_n'$ and $\sigma_n'$. In fact, the main ingredient will be the following. 
 %The proof uses the fact that $\mu_n'$ can be written in terms of $\ln(\widetilde{W}_j)$, and by Theorem~\ref{thm:poissonapp}, we can can replace $\widetilde{W_j}$ by a suitable Poisson random variable. 
\begin{lemma}
\label{lem:finalReduction}
Consider a deck ${\bf m}$ following the assumptions of Theorem \ref{thm:CLT}. Let $\mu_n'$ be the conditional mean of the total score $S_{\bf m}$ given the random variables $\widetilde{W}_j's$. Then, 
\[\Var{\mu_n'}=O\left(\left(\ln \ln n\right)^2\right)\]
\end{lemma}
\begin{proof}
First, we claim that if suffices to show
\begin{equation}\label{eq:goal}
    \mathbb E\left(\ln \widetilde W_j-c_j\right)^2=O\left((\ln\ln n)^2\right)
\end{equation}
where $c_j=c_j(n)$ is some arbitrary sequence, and $1\leq j\leq m-1$ (the case $j=m$ is obvious since $\widetilde W_m$ is deterministic). Indeed, once \eqref{eq:goal} is shown, one can simply exploit the first bound in \eqref{eq:condmean}, together with a triangular inequality and the well-known fact that the variance minimizes the square discrepancy from any constant. \\ \\
The goal now is to reduce concentration properties of $\widetilde W_j$ to those of $W_{j-1,t}$ and $T_j$, for which we can exploit Theorem~\ref{thm:poissonapp}. To this aim, consider two sequences $0\leq g_n\leq f_n$ to be chosen later (eventually, we will have $g_n\rightarrow 0$ and $f_n\rightarrow \infty$). Using Theorem~\ref{thm:poissonapp}, we know that
\begin{align*}
    \left|\mathbb P\left(\frac{T_j}{Cn^{\frac{j}{j+1}}}\not\in [f_n, g_n]\right)-\left(e^{-f_n}+1-e^{-g_n}\right)\right|\lesssim \frac{f_n}{n^{\frac{1}{j+1}}}
\end{align*}
Let $t_{-}:=Cn^{\frac{j}{j+1}}g_n, t_{+}:=Cn^{\frac{j}{j+1}}f_n$. Since $W_{j-1, t}$ is weakly increasing in $t$, in the regime $T_j\in [t_{-}, t_{+}]$, one has
\begin{align*}
  W_{j-1, t_{-}}\leq  \widetilde W_j\leq W_{j-1, t_{+}}.
\end{align*}
In particular, on the event $T_j\in [t_{-}, t_{+}]$ we can bound
\begin{equation}\label{eq:removetilde}
    \left|\ln \widetilde W_j-c_j\right|^2\leq |\ln W_{j-1, t_{-}}-c_j|^21_{W_{j-1, t_{-}}>0}+|\ln W_{j-1, t_{+}}-c_j|^21_{W_{j-1, t_{+}}>0},
\end{equation}
where the indicators are harmless since on the event $T_j\in [t_-, t_+]$, we are guaranteed that $W_{j, t_{-}}$ and $W_{j,t_{+}}$ are both greater than zero.\\ \\
The key gain is that we overcome the complicate dependence mechanism behind the definition of the $\widetilde W_j$ -- recall that $T_j$ and $W_{j-1,t}$ are \emph{not} independent. Using again Theorem~\ref{thm:poissonapp}, we know 
\begin{align*}
    d_{tv}\left(W_{j-1, t_{-}}, Poi\left(\lambda_{-}\right)\right)\lesssim\frac{t_{-}}{n}\lesssim \frac{g_n}{n^{\frac{1}{j+1}}}, \quad d_{tv}\left(W_{j-1, t_{+}}, Poi\left(\lambda_{+}\right)\right)\lesssim \frac{t_{+}}{n}\lesssim\frac{f_n}{n^{\frac{1}{j+1}}}. 
\end{align*}
where $\lambda{+}$ and $\lambda_{-}$ satisfy
\begin{equation}\label{eq:lambda}
\lambda_+=\Theta\left(n^{\frac{1}{j+1}}f_n\right), \quad \lambda_-=\Theta\left(n^{\frac{1}{j+1}}g_n\right).
\end{equation}
Choose now $c_j=\frac{1}{j+1}\ln n$, and notice that $|\ln \widetilde W_j-c_j|\leq \ln n$ since $1\leq \widetilde W_j\leq n$. In particular, we can replace $W_{j-1, t_{\pm}}$ with $Poi(\lambda_{\pm})$ in \eqref{eq:removetilde} up to an error of order
\begin{align*}
  \ln n\left(e^{-f_n}+1-e^{-g_n}+\frac{f_n}{n^{\frac{1}{j+1}}}+\frac{g_n}{n^{\frac{1}{j+1}}}\right).
\end{align*}
Consider now the choices of $f_n=\ln n, g_n=\frac{1}{\ln n}$. Then, using $1-e^{-x}\leq x$, the expression above is $O(1)$, and we are left to show
\begin{align*}
    \mathbb E\left[\left(\ln \left(Poi(\lambda_{+})\right)-c_j\right)^21_{Poi(\lambda_{+})>0}\right]+\mathbb E\left[\left(\ln \left(Poi(\lambda_{-})\right)-c_j\right)^21_{Poi(\lambda_{-})>0}\right]=O\left(\left(\ln\ln n\right)^2\right).
\end{align*}
Thanks to \eqref{eq:lambda} we can replace $c_j$ with $\lambda_{\pm}$ up to a price
\begin{align*}
    |c_j-\ln (\lambda_{-})|^2\lesssim (\ln g_n)^2=\left(\ln\ln n\right)^2,
    \quad |c_j-\ln (\lambda_{+})|^2\lesssim (\ln f_n)^2=\left(\ln\ln n\right)^2
\end{align*}
and thus the result follows upon showing
\begin{align*}
    \mathbb E\left[\left(\ln \left(Poi(\lambda_{+})\right)-\ln\lambda_{+}\right)^21_{Poi(\lambda_{+})>0}\right]+\mathbb E\left[\left(\ln \left(Poi(\lambda_{-})\right)-\ln\lambda_{-}\right)^21_{Poi(\lambda_{-})>0}\right]\lesssim \left(\ln\ln n\right)^2.
\end{align*}
In fact, we can do better than this, as it follows from Lemma \ref{lem:simple} (notice that, because of \eqref{eq:lambda} and our choices of $f_n$ and $g_n$, we know that $\lambda_{\pm}\geq 1$ for all $n$ sufficiently large). 
\end{proof}

\begin{lemma}\label{lem:simple}
Let $X=\frac{Poi(\lambda)}{\lambda}$ for $\lambda>1$. Then,
\begin{align*}
    \mathbb E[(\ln X)^21_{X>0}]\leq C
\end{align*}
for some absolute constant $C$.
\end{lemma}
\begin{proof}
Let $A=\{|X-1|\geq 1/2, X\neq 0\}$. Then, we have
\begin{align*}
    \mathbb E[(\ln X)^21_{X>0}]\leq (\ln 2)^2+\mathbb E[(\ln X)^21_A].
\end{align*}
Since $\lambda>1$, on the event $A$ we have $X>1$ and we can bound $(\ln X)^2\leq X$. Therefore, combining with the Cauchy-Schwartz inequality, 
\begin{align*}
    \mathbb E[(\ln X)^21_A]\leq (\mathbb E[X^2])^{1/2}\left(\mathbb P(A)\right)^{1/2}.
\end{align*}
For the first term, we have
\begin{align*}
    \mathbb E[X^2]=\frac{\lambda^2+\lambda}{\lambda}\leq 2\lambda.
\end{align*}
On the other hand, the Chernoff bound for Poisson random variables gives
\begin{align*}
    \mathbb P(A)\leq \mathbb P\left(|Poi(\lambda)-\lambda|\geq \frac{\lambda}{2} \right)\leq 2e^{-\frac{\lambda}{12}}
\end{align*}
from which the claim follows at once. 
\end{proof}




%\begin{lemma}
%\label{lem:SimpleComputation}
%Let $\lambda>0$ and let $F(\lambda):=\E{\ln\left(\frac{Poi(\lambda)}{\lambda}\right)^21_{Poi(\lambda)>0}}$. Then, $F(\lambda)\leq 7.$
%\end{lemma}
%\begin{proof}
%Note that there exists $0<\delta<1$ such that $\ln x\leq x^2$ if $x\in [\delta, \infty)$ ($\delta\approx$ 0.4). Fix such a $\delta$ and observe that
%\begin{align}
%\label{eqn:UpperTail}
  %  \mathbb E\ln\left(\frac{Poi(\lambda)1_{\{Poi(\lambda)\geq \delta\lambda\}}}{\lambda}\right)^2 &= e^{-\lambda}\sum_{k\geq \delta\lambda} \left(\frac{k}{\lambda}\right)^2\frac{\lambda^k}{k!}\nonumber\\ 
  %  &\leq e^{-\lambda}\left(1+\frac{1}{\delta\lambda-1}\right)\sum_{k\geq \delta\lambda} \frac{\lambda^{k-2}}{(k-2)!} \nonumber\\
 %   &\leq 2.
%\end{align}
%On the other hand if $ k\leq \delta \lambda$ then $\lambda/k\in [1/\delta, \infty)\subseteq [\delta, \infty)$ because $\delta\leq 1$. Therefore, we have  $(\lambda/k)^2\geq (\log(\lambda/k))^2$. Thus,
%\begin{align}
%\label{eqn:LowerTail}
   % E\ln\left(\frac{Poi(\lambda)1_{\{1\leq Poi(\lambda)< \delta\lambda\}}}{\lambda}\right)^2 &= e^{-\lambda}\sum_{1\leq k< \delta\lambda}\left(\ln\left(\frac{k}{\lambda}\right)\right)^2\frac{\lambda^k}{k!}\nonumber\\
    %&= e^{-\lambda}\sum_{1\leq k< \delta\lambda}\left(\ln\left(\frac{\lambda}{k}\right)\right)^2\frac{\lambda^k}{k!}\nonumber\\
    %&\leq e^{-\lambda}\sum_{1\leq k< \delta\lambda}\left(\frac{\lambda}{k}\right)^2\frac{\lambda^k}{k!}\nonumber\\
   % &=e^{-\lambda}\sum_{1\leq k< \delta\lambda}\left(1+\frac{1}{k}\right)\left(1+\frac{2}{k}\right)\frac{\lambda^{k+1}}{(k+2)!}\nonumber\\
   % &\leq 5.
%\end{align}
%Combining \eqref{eqn:UpperTail} and \eqref{eqn:LowerTail} we obtain the result.
%\end{proof}
We are now ready to prove the main result, which will be an easy consequence of the lemmas we proved so far.
\begin{proof}[Proof of Theorem~\ref{thm:CLT}]
We start with the first part. The asymptotic result for the mean is the main result in \cite{he2021card}. As for the variance $\sigma^2_n$, it can be written in terms of the conditional mean and variance (see Lemma \ref{lem:berryEssen}) as
\begin{align*}
    \sigma_n^2=\Var{\mu_n'}+\mathbb E[{\sigma'_n}^2]. 
\end{align*}
Therefore, the asymptotic result follows by using \eqref{eq:condmean}, which shows that the second term is asymptotically the same of $\mu_n$, together with Lemma \eqref{lem:finalReduction}, which shows that the first term is negligible.\\ \\
We now move to the second part, namely, the proof of the CLT. Observe that
\begin{align}
    \left| \mathbb P\left(\frac{S_{{\bf m}(n)}-\mu_n}{\sigma_n}\leq x\right) - \Phi(x) \right| &= \left| \E{ \mathbb P\left(\frac{S'_{{\bf m}(n)}-\mu_n}{\sigma_n}\leq x\Bigg|\widetilde W_1, \ldots \widetilde W_m \right) - \Phi(x)} \right|\nonumber\\
    &= \left| \E{ \mathbb P\left(\frac{S'_{{\bf m}(n)}-\mu'_n}{\sigma'_n}\leq y\Bigg|\widetilde W_1, \ldots \widetilde W_m \right) - \Phi(x)} \right|\nonumber \\
    &\leq \|\widetilde{F}-\Phi\|_{\infty} + \abs{\E{\Phi(y)-\Phi(x)}}\;,
\label{eqn:FirstStep}
\end{align}
where $\widetilde{F}(z):=P\left(\frac{S_{\bf m}'-\mu'}{\sigma'}\leq z\right)$ and $y = y_n(x)=\frac{\sigma}{\sigma'}x+\frac{\mu-\mu'}{\sigma'}$ is random. We know that $$\|F-\Phi\|_{\infty}\lesssim \frac{1}{\left(\ln n\right)^{\frac{3}{2}}}$$ from Lemma~\ref{lem:berryEssen}, so that it suffices to bound $\abs{\E{\Phi(y)-\Phi(x)}}$. To this aim, notice that we have the bound
\begin{align*}
    |\Phi(y)-\Phi(x)|\lesssim |x-y|e^{-\frac{\min(x^2,y^2)}{2}}.
\end{align*}
We observe that, from \eqref{eq:condmean} and \eqref{eq:useful}, we can estimate
\begin{align*}
    \sigma_n-\sigma'_n=\frac{\sigma_n^2-{\sigma'_n}^2}{\sigma_n+\sigma_n'}=o\left(|\mu_n-\mu_n'|+1\right)
\end{align*}
with probability one, where we used again \eqref{eq:useful}. This entails
\begin{equation}\label{eq:y-x}
    |y(x)-x|\leq \frac{|\sigma_n'-\sigma_n|}{\sigma'_n}|x|+\frac{|\mu_n-\mu'|}{\sigma'_n}\lesssim \left(1+|x|\right)\frac{|\mu_n-\mu_n'|+1}{\sqrt{\ln n}},
\end{equation}
with probability one. Therefore, we have with probability one and for all $n$ sufficiently large,
\begin{align*}
    |\Phi(y)-\Phi(x)|\lesssim (1+|x|)e^{-\min(x^2, y^2)}\frac{|\mu_n-\mu_n'|+1}{\sqrt{\ln n}}
\end{align*}
Let $A_n$ be the event where the random variable $y=y(x)$ satisfies  $|y(x)-x|\leq (1+|x|)/2$. On this event, the pre-factor depending on $x$ is uniformly bounded from above and thus
\begin{align*}
    |\Phi(y)-\Phi(x)|1_{A_n}\lesssim \frac{|\mu_n-\mu_n'|+1}{\sqrt{\ln n}},
\end{align*}
and thus Lemma \eqref{lem:finalReduction}, together with a Chebyshev's inequality, allows to conclude
\begin{align*}
   \mathbb E[|\Phi(y)-\Phi(x)|1_{A_n}]\leq \frac{\ln\ln n}{\sqrt{\ln n}}.
\end{align*}
On the complementary event, use the bound $|\Phi(x)-\Phi(y)|\leq 1$ to deduce
\begin{align*}
    \mathbb E[|\Phi(y)-\Phi(x)|1_{A^c_n}]\leq \mathbb P(A_n^c).
\end{align*}
Therefore, we conclude using once more with Lemma \eqref{lem:finalReduction}, together with a Chebyshev inequality, since using \eqref{eq:y-x} we have, for some fixed constant $K>0$, 
\begin{align*}
    \mathbb P(A_n^c)\leq \mathbb P\left(\frac{|\mu_n-\mu_n'|+1}{\sqrt{\ln n}}\geq K \right)\lesssim \frac{\ln\ln n}{\sqrt{\ln n}}
\end{align*}

\end{proof}
