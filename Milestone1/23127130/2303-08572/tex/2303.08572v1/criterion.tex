\section{Applying the UCM Principle from Data}
\label{sec:criterion_data}
Applying the proposed causal inference principle amounts to performing  hypothesis testing concerning the UC nature of the conditional pmf estimates $\hat{\boldsymbol\theta}^{X\rightarrow Y}$ and $\hat{\boldsymbol\theta}^{Y\rightarrow X}$. This is closely related to classical tests for two-way contingency tables \citep{Agresti, Read}. Given a table of counts $N_{x,y}$, let the \textit{null hypothesis} $H_0$ be that these counts can be explained by a UCM in the $X\rightarrow Y$ direction. To test this hypothesis, consider the corresponding maximum log-likelihood (noting that $p_{X,Y}(x,y) = \mathbb{P}[X=x,Y=y] = \theta_x^{X} \, \gamma_{\sigma_x(y)}$, for a UCM),
\begin{equation}
\mathcal{L}_{H_0} = \sum\limits_{x \in \mathcal{X}}\sum\limits_{y \in \mathcal{Y}} N_{x,y} \log\bigl( \hat{\theta}_x^{X} \, \hat{\gamma}_{\hat{\sigma}_x(y)}\bigr) = \sum\limits_{x \in \mathcal{X}} N_x \log  \hat{\theta}_x^{X} + \sum\limits_{x \in \mathcal{X}}\sum\limits_{y \in \mathcal{Y}} N_{x,y} \log\bigl( \hat{\gamma}_{\hat{\sigma}_x(y)}\bigr) ,
\end{equation}
where $\hat{\boldsymbol\theta}^X$, $\hat{\sigma_1},...,\hat{\sigma}_{|\mathcal{X}|}$, and $\hat{\boldsymbol\gamma}$ are the ML estimates obtained as shown in Section \ref{sec:channel}.

The alternative hypothesis is that the channel is arbitrary, with maximum log-likelihood 
\begin{equation}
\mathcal{L}_{\bar{H}_0} = \sum\limits_{x \in \mathcal{X}} N_x \log  \hat{\theta}_x^{X} + \sum\limits_{x \in \mathcal{X}}\sum\limits_{y \in \mathcal{Y}} N_{x,y} \log\bigl( N_{x,y}/N_x\bigr),
\end{equation}
since the ML estimates of the conditional pmf parameters are as given in \eqref{estimate_thetaxy}, and the ML estimate of the marginal $\boldsymbol\theta^X$  is the same, regardless of the channel being uniform or not. These models are nested: a UCM is a particular case of the set of all valid channels, thus it is always true that $\mathcal{L}_{H_0} \leq \mathcal{L}_{\bar{H}_0}$.

The \textit{likelihood-ratio statistic} (LRS), denoted $G^2$, is then given by 
\begin{equation}
G_{X\rightarrow Y}^2 = 2 (\mathcal{L}_{\bar{H}_0} - \mathcal{L}_{H_0}) =  2 \sum\limits_{x \in \mathcal{X}}\sum\limits_{y \in \mathcal{Y}} N_{x,y} \log\Bigl( \frac{N_{x,y}}{\hat{\gamma}_{\hat{\sigma}_x(y)} N_x}\Bigr);
\end{equation}
notice that $\hat{\gamma}_{\hat{\sigma}_x(y)} N_x$ is the expected value of $N_{x,y}$ under the null hypothesis. This is the LRS in the $X\rightarrow Y$ direction, which we indicate with the subscript $X\rightarrow Y$. The LRS in the reverse direction, denoted $G_{Y\rightarrow X}^2$, is computed in the same way, after swapping the roles of $X$ and $Y$.

%Alternatively, we may use the so-called Pearson statistic (usually denoted as $X^2$, but here denoted as $P^2$ since we are using $X$ to denote one of the random variables), 
%\begin{equation}
%P^2 = \sum\limits_{x \in \mathcal{X}}\sum\limits_{y \in \mathcal{Y}} \frac{(N_{x,y} - \hat{\gamma}_{\hat{\sigma}_x(y))} N_x)^2}{\hat{\gamma}_{\hat{\sigma}_x(y)} N_x}.
%\end{equation}

It is well known that $G^2$ is asymptotically $\chi^2$-distributed with df $=(|\mathcal{X}|-1)(|\mathcal{Y}|-1)$ \textit{degrees of freedom}, yielding the ${\tt p}$-value 
\[
{\tt p} = \mathbb{P}[ \chi^2_{\mbox{df}} \geq G^2] = 1-\mathbb{P}[ \chi^2_{\mbox{df}} < G^2],
\]
where $\mathbb{P}[ \chi^2_{\mbox{df}} < G^2]$ is the cumulative distribution function of a $\chi_{\mbox{df}}^2$ distribution. If ${\tt p}$ is less than some significance level (\textit{i.e.}, the test statistic $G^2$ is too large), the null hypothesis is rejected.

Let ${\tt p}^{X\rightarrow Y}$ and ${\tt p}^{Y\rightarrow X}$ be the ${\tt p}-$values of the LRS for testing the uniformity of the channels in both directions, and let $\alpha$ be a significance level for the test \citep{Agresti, Read}, \textit{i.e.}, the null hypothesis is rejected if the ${\tt p}-$value is less than $\alpha$. Having a statistical test of whether an estimated conditional pmf corresponds to a UCM, we adopt a procedure similar to the one proposed by \citet{anm2011}.
    \begin{itemize}
        \item If ${\tt p}^{X\rightarrow Y} \geq \alpha$ and  ${\tt p}^{Y\rightarrow X} < \alpha$, declare $X\rightarrow Y$.
    \item If ${\tt p}^{X\rightarrow Y} < \alpha$ and  ${\tt p}^{Y\rightarrow X} \geq \alpha$, declare $Y\rightarrow X$.
     \item If ${\tt p}^{X\rightarrow Y} < \alpha$ and  ${\tt p}^{Y\rightarrow X} < \alpha$, declare "undecided: wrong model".
     \item If ${\tt p}^{X\rightarrow Y} \geq \alpha$ and  ${\tt p}^{Y\rightarrow X} \geq \alpha$, declare "undecided: both directions possible".
     \end{itemize}

The fourth case (\textit{i.e.}, the hypotheses that the channel is uniform in both directions cannot be rejected) is asymptotically improbable, unless $X$ and $Y$ are independent, due to the identifiability guarantee. Alternatively, to force the method to make a decision between the two causal directions, one may simply decide for $X\rightarrow Y$, if ${\tt p}^{X\rightarrow Y} > {\tt p}^{Y\rightarrow X}$, and for $Y\rightarrow X$, otherwise. 

%The method is summarized in Algorithm \ref{alg:causalinferencemethod}.

%\begin{algorithm}
%\small
%\SetAlgoLined
%\KwIn{$|\mathcal{X}|\times |\mathcal{Y}|$ count matrix $\mathbf{N}= [N_{x,y}]$}
%\KwOut{Causal direction between $X$ and $Y$}
%\begin{enumerate}
%    \item Construct the conditional probability matrix $\boldsymbol \theta^{Y|X}$. Estimate the associated uniformly dispersive channel  using Algorithm \eqref{alg:symmetric} (or \eqref{alg:cyclic_symmetric} if $Y$ is cyclic), by setting $\hat{\gamma}$ to the row-vector $\boldsymbol \theta^{Y|X}_1$.
%    \item  Construct the conditional probability matrix $\boldsymbol \theta^{X|Y}$. Estimate the associated uniformly dispersive channel  using Algorithm \eqref{alg:symmetric} (or \eqref{alg:cyclic_symmetric} if $X$ is cyclic), by setting $\hat{\gamma}$ to the row-vector $\boldsymbol \theta^{X|Y}_1$.
%    \item Calculate $D^{X\rightarrow Y}$ and $D^{Y\rightarrow X}$.
%   \item Decide the causal direction:\\
%    If $D^{X \rightarrow Y}  < D^{Y \rightarrow X}$, output $X \rightarrow Y$;\\[0.1cm]
%    If $D^{X \rightarrow Y}  > D^{Y \rightarrow X}$, output $Y \rightarrow X$;\\
%    Else, output "Inconclusive".
%    \end{enumerate}
% \caption{Causal Inference Method via Least Difference in Log-likelihood (\acs{LDL})}
% \label{alg:causalinferencemethod}
%\end{algorithm}

