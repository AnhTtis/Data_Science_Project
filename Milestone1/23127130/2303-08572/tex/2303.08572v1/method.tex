\section{Uniform Channel Models -- UCM -- for Categorical Variables}
\label{sec:UCModel}
This section describes the proposed causal inference principle for categorical variables, after introducing notation and reviewing the notion of uniform channel. Finally, we prove an identifiability theorem for the proposed model and show its equivalence with an SCM. 

\label{sec:method}
\subsection{Categorical Variables and Uniform Channels}
Let $X \in \mathcal{X} = \{1,...,|\mathcal{X}|\}$ and $Y \in \mathcal{Y} = \{1,...,|\mathcal{Y}|\}$ be two \textit{categorical random variables} (although their outcomes are shown as integers, no role is played by their order). The joint pmf $p_{X,Y}$ can be factored in two different ways, $
 p_{X,Y}(x,y) = \; p_{Y|X}(y|x) \; p_X(x)  =  p_{X|Y}(x|y)\; p_Y(y)$,  corresponding to a \textit{Markov equivalence class}. If $X  \perp \!\!\! \perp Y$, the joint pmf factors trivially $p_{X,Y}(x,y) = \; p_X(x)\, p_{Y}(y)$. Let the vector of parameters of the first factorization be denoted as $\boldsymbol \theta = (\boldsymbol \theta^X, \boldsymbol \theta^{X\rightarrow Y})$, \textit{i.e.},
\begin{align*}
    \theta_x^X = p_X(x) = \mathbb{P}[X=x] \quad \text{and} \quad  \theta_{x,y}^{X\rightarrow Y} = p_{Y|X}(y|x) = \mathbb{P}[Y=y | X = x],
\end{align*}
where $\boldsymbol \theta^X \in \Delta_{|\mathcal{X}| - 1}$, with $\Delta_{m-1}$ being the probability simplex in $\mathbb{R}^m$. The conditional probabilities are arranged in a  $|\mathcal{X}| \times |\mathcal{Y}|$ row-stochastic matrix $\boldsymbol \theta^{X\rightarrow Y}$, with the  $x$-th row denoted as $\boldsymbol \theta_x^{X\rightarrow Y}$. 
%In summary, $\boldsymbol \theta = (\boldsymbol \theta^X, \boldsymbol \theta^{X\rightarrow Y}) \in \Delta_{|\mathcal{X}| - 1} \times (\Delta_{|\mathcal{Y}| - 1})^{|\mathcal{X}|}$.

\vspace{0.05cm}


\begin{definition}[Discrete Memoryless Channel] \citep{code,Hamming}
A discrete memoryless channel (DMC) is a probabilistic system with a discrete input alphabet $\mathcal{X} = \{1,\ldots, |\mathcal{X}|\}$ and a discrete output alphabet $\mathcal{Y}= \{1,\ldots, |\mathcal{Y}|\}$, specified by the conditional probabilities $p_{Y|X}( y| x)$, for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$.  The adjective ``memoryless" means that, given a sequence of random inputs, the corresponding outputs are conditionally independent.

%\begin{equation}
%     p( {\bf Y}={\bf y} | {\bf X}={\bf x})=  \prod_{n = 1}^{N} p(Y_n = y_n| X_n = x_n).
% \end{equation}
\end{definition}


\begin{definition}[Uniform channel (UC)] \citep{Hamming}
A UC is a DMC in which each row of the conditional probability (channel) matrix $\boldsymbol \theta^{X\rightarrow Y}$ is a permutation of every other row. 
\end{definition}


\begin{definition}[Cyclic uniform channel (CUC)] A CUC is a UC where each row of the channel matrix $\boldsymbol \theta^{X\rightarrow Y}$ is a cyclic permutation of every other row.
\end{definition}

Let $\mathbb{S}_{|\mathcal{Y}|}$ denote the set of all $|\mathcal{Y}|!$ permutations of $(1,..., |\mathcal{Y}|)$. In a UC, each row $\boldsymbol \theta_x^{X\rightarrow Y}$ is a row-specific permutation $\sigma_x \in \mathbb{S}_{|\mathcal{Y}|}$ of a common vector $\boldsymbol\gamma \in \Delta_{|\mathcal{Y}|-1}$, \textit{i.e.},
\begin{equation}
\boldsymbol \theta_x^{X\rightarrow Y} = (\gamma_{\sigma_x (1)}, \ldots,   \gamma_{\sigma_x (|\mathcal{Y}|)}) \;\;\; \Leftrightarrow \;\;\; p_{Y|X}(y|x) = \gamma_{\sigma_x(y)}. \label{eq_gama_perm}
\end{equation}
In the case of a CUC, $\sigma_x \in \mathbb{C}_{|\mathcal{Y}|}$, the set of all $|\mathcal{Y}|$ cyclic permutations of $(1,..., |\mathcal{Y}|)$.

\subsection{Uniform Channel Model -- UCM -- for Categorical Variables}
We propose a new principle to infer the most likely causal direction between two categorical variables by following the rationale behind ANMs. Recall that the ANM principle for real variables is as follows: if $Y$ satisfies an ANM $Y= f_Y(X) + N_Y$, where $N_Y \perp \!\!\! \perp X$ (\textit{i.e.}, the \textit{exogenous} noise is independent of $X$), but the same is not true in the reverse direction, then the most likely causal direction is $X\rightarrow Y$. In the ANM for real variables, the conditional \textit{probability density function} (pdf) has identical \textit{shape} for all values of $x$, simply being \textit{shifted} by $f_Y(x)$, \textit{i.e.}, $p_{Y|X}(y|x) = p_{N_Y} (y-f_Y(x))$, where $p_{N_Y}$ is the pdf of the noise variable $N_Y$. Consequently, the \textit{conditional differential entropy} $h(Y|X)$ does not depend on the pfd of $X$, as shown next. 

\begin{proposition}\label{conditioned_h}
If real-valued variables $X$ and $Y$ admit an ANM from $X$ to $Y$, then the \textit{conditional differential entropy} $h(Y|X) = h(N_Y)$, independently of the distribution of $X$.
\end{proposition}

{\noindent\bf Proof}: Using the shift-invariance  (a) of differential entropy \citep{code},
\begin{align}
h(Y|X) & =  \; \mathbb{E}_{X,Y} [ -\log p_{Y|X}(Y|X) ] \; = \; \mathbb{E}_{X} \bigl[  \mathbb{E}_{Y|X}[ -\log p_{Y|X}(Y|X) ] \bigr] \nonumber \\ 
& = \;  \mathbb{E}_{X} \bigl[  \mathbb{E}_{Y|X}[ -\log p_{N_Y}(Y-f_Y(X)) ] \bigr] \; \stackrel{(a)}{=}\; \mathbb{E}_{X} \bigl[ h(N_Y) \bigr]  \;  = \; h(N_Y).  \tag*{\mbox{$\blacksquare$}}
\end{align}

For categorical variables, the sets $\mathcal{X}$ and $\mathcal{Y}$ lack any meaningful order, thus there is no notion of \textit{addition}, and an ANM is not directly applicable. However, the conditional entropy invariance property of ANMs can be preserved by considering the transformation group under which discrete entropy is invariant: \textit{permutations}. Consequently, our proposed causal inference principle is:

\begin{center}
\fbox{
\begin{minipage}{13.5cm}
\begin{description}[leftmargin=*]
\item[UCM causal inference principle for categorical variables:] given two categorical variables $X$ and $Y$, if the conditional pmf $\boldsymbol \theta^{X \rightarrow Y}$ corresponds to a UCM, but the conditional pmf $\boldsymbol \theta^{Y\rightarrow X}$ does not, then we infer the causal direction to be $X\rightarrow Y$.
\end{description}
\end{minipage}}
\end{center}


Paralleling Proposition \ref{conditioned_h}, the following result is a simple consequence of the invariance of (discrete) entropy to symbol permutations. 

\begin{proposition}\label{conditioned_H}
If $\boldsymbol \theta^{X\rightarrow Y}$ corresponds to a UC (each row of $\boldsymbol \theta^{X\rightarrow Y}$ is a permutation of a vector $\boldsymbol\gamma \in \Delta_{|\mathcal{Y}|-1}$), then the conditional entropy $H(Y|X) = H({\boldsymbol \gamma})$, independently of $p_X$.
\end{proposition}

{\noindent\bf Proof}: Due to the permutation-invariance property (a) of entropy \citep{code},
\begin{align}
H(Y|X) & = \mathbb{E}_{X,Y} [ -\log p_{Y|X}(Y|X) ] \; = \; \mathbb{E}_{X} \bigl[  \mathbb{E}_{Y|X}[ -\log p_{Y|X}(Y|X) ] \bigr] \nonumber \\
& = \; \mathbb{E}_{X} \bigl[  \mathbb{E}_{Y|X}[ -\log \gamma_{\sigma_X(Y)} ] \bigr] \;  
 \stackrel{(a)}{=} \; \mathbb{E}_{X} \bigl[ H({\boldsymbol\gamma}) \bigr] 
\; = \; H({\boldsymbol\gamma}).  \tag*{\mbox{$\blacksquare$}}
\end{align}

The proposed causal inference problem can be seen as an instance of the \textit{independence of cause and mechanism} principle,  with \textit{independence} corresponding to the following property: the conditional pmf of the effect given the cause has the same collection of probability values, only their positions depend on the cause. Thus, the conditional uncertainty (entropy) of the effect, given the cause, is independent of the distribution of the cause. 

A relevant fact that provides further support to the proposed principle is that the UCM can be written as an SCM \citep{pearlcausality}, as shown in the following proposition.
\begin{proposition}
Let $X\in\mathcal{X}$ and $Y\in\mathcal{Y}$ be a pair of dependent random variables such that the conditional pmf $\boldsymbol \theta^{X\rightarrow Y}$ corresponds to a UCM specified by $\boldsymbol\gamma$ and $\sigma_1,...,\sigma_{|\mathcal{X}|}$, as in \eqref{eq_gama_perm}, and the marginal $\boldsymbol \theta^{X}$ be arbitrary. Then, the joint pmf of $X$ and $Y$ is entailed by the following SCM:
\begin{equation}
X:= U_X, \hspace{1cm} Y := f_Y( X, U_Y), \label{eq:SCM}
\end{equation}
with $U_X \perp \!\!\! \perp U_Y$ (independent \textit{exogenous}  variables), $U_X \in \mathcal{X}$ has pmf $\boldsymbol \theta^{X}$, $U_Y \in \mathcal{Y}$ has pmf $\boldsymbol{\gamma}$, and $f_Y:\mathcal{X} \times \mathcal{Y} \rightarrow \mathcal{Y}$ is a  function given by $ f_Y(x,u) = \tau_x (u)$, with $\tau_x = \sigma_x^{-1}$ (inverse\footnote{Given a permutation $\sigma \in \mathbb{S}_{|\mathcal{Y}|}$, its inverse $\sigma^{-1} \in \mathbb{S}_{|\mathcal{Y}|}$ is such that $\sigma^{-1} (\sigma (i)) = i$, for any $i = 1,..., |\mathcal{Y}|$.} permutation of $\sigma_x$). Conversely, if the conditional pmf $\boldsymbol \theta^{X\rightarrow Y}$ does not correspond to a UCM, it is  impossible to write an SCM of the form \eqref{eq:SCM}, with $U_Y\in \mathcal{Y}$, entailing the joint pmf of $X$ and $Y$.

\end{proposition}
{\noindent\bf Proof}: 
Given $x\in\mathcal{X}$, $Y = f_Y(x,U)$ is a categorical random variable with conditional pmf
\[
p_{Y|X}(y|x) = \mathbb{P}[Y=y | X=x] = \mathbb{P}[ U_Y = \tau_x^{-1}(y) ] = \gamma_{\sigma_x(y)}
\]
(which coincides with \eqref{eq_gama_perm}), where the second equality stems from the definition of $f_Y$ and permutations being bijections. Conversely, if the conditional pmf $\boldsymbol{\theta}^{X\rightarrow Y}$ does not correspond to a UCM (neither all its rows are equal to each other, by assumption), depending on what value/category $X$ takes, the conditional pmf of $Y$ takes different probability values, not just a permutation of a common pmf, making it impossible to write an SCM of the form \eqref{eq:SCM} with $U_Y \in \mathcal{Y}$ independent of $U_X$. \hfill$\blacksquare$

\vspace{0.2cm}
It is the restriction $U_Y\in \mathcal{Y}$ that makes this result non-trivial. In fact, \citet{kocaoglu_entropic_2017} showed that, given any joint pmf $p_{X,Y}$, it is possible to write an SCM $Y := f_Y( X, U_Y)$, with $U_X \perp \!\!\! \perp U_Y$, that induces $p_{X,Y}$ and such that $U_Y$ takes values in a set of cardinality O($|\mathcal{X}|\, |\mathcal{Y}|$). 

%\vspace{0.05cm}
\begin{remark}
If none, or both, of the conditional pmfs, $\boldsymbol \theta^{X\rightarrow Y}$ and $\boldsymbol \theta^{Y\rightarrow X}$, correspond to a UCM, the proposed criterion does not select a causal direction. Of course, in practice, the conditional pmfs are estimated from a finite dataset, thus there is a very small chance that one of these estimates corresponds exactly to a UCM. In Section \ref{sec:criterion_data}, we come back to this issue, proposing statistical tests do decide if a conditional pmf estimate can be considered to correspond to a UCM. In the following subsection, we assume that we have an infinite amount of data (equivalently, the true underlying pmf) and address the identifiability issue in this ideal condition. If this model was not identifiable in this ideal setting, it would be hard to argue that it could be useful with a finite amount of data.
\end{remark}


\begin{remark}
Our UCM contains as a particular case the model for ``structureless" sets proposed by \citet{anm2011}. Their model assumes a function $\phi:\mathcal{X}\rightarrow\mathcal{Y}$ and  $p_{Y|X}(y|x) = p$, if $y=\phi(x)$, and $p_{Y|X}(y|x) = (1-p)/(|\mathcal{Y}|-1)$, if $y\neq \phi(x)$. This corresponds to a UC (in fact, a CUC), with
\[
\boldsymbol\gamma = \bigl( p,(1-p)/(|\mathcal{Y}|-1),...,(1-p)/(|\mathcal{Y}|-1) \bigr)
\]
and any set of permutations such that $\sigma_x(y) = 1$, for $y=f(x)$. Our UC and CUC models are much more general, as they do not constrain the conditional pmf to have only two different values.
\end{remark}

\subsection{Identifiability}\label{Indentify}
For the proposed criterion to be useful, it should be supported by an identifiability guarantee, \textit{ i.e.}, that the set of joint probability mass functions $p_{X,Y}$ such that both $p_{Y|X}$ and $p_{X|Y}$ correspond to uniform channels should be as small as possible, ideally have zero Lebesgue measure in the space of valid parameters, thus zero probability under any continuous density \citep{anm2011}.  Before stating and proving the general identifiability result, we illustrate it for the case where both variables are binary: $\mathcal{X} = \mathcal{Y}= \{1, 2\}$. Let $p_X(1) = \theta_1^X = \beta$ and let $\boldsymbol \theta^{X\rightarrow Y}$ correspond to a UCM (in this case, simply a \textit{binary symmetric channel}) with error probability $\alpha$ \citep{code}:
\begin{align*}
    \boldsymbol \theta^{X\rightarrow Y} = \begin{bmatrix}
    1 - \alpha &  \alpha\\
    \alpha & 1 - \alpha
    \end{bmatrix}.
\end{align*}
Of course, a channel matrix where the two rows are equal to $(1-\alpha,\alpha)$ is also a UC, but in that case $X$ and $Y$ are independent, which is an uninteresting case. The channel in the reverse direction, \textit{i.e.}, $\boldsymbol \theta^{Y\rightarrow X}$, can be easily derived using Bayes law, yielding
\begin{align*}
{\small    \boldsymbol \theta^{Y\rightarrow X} = \begin{bmatrix}
    {\displaystyle \frac{(1-\alpha) \beta}{(1-\alpha) \beta + \alpha (1-\beta)}} &{\displaystyle  \frac{\alpha(1 - \beta)}{(1-\alpha) \beta + \alpha(1-\beta)}} \\
 {\displaystyle \frac{\alpha\beta}{\alpha\beta + (1-\alpha) (1 - \beta)} } & {\displaystyle \frac{(1-\alpha) (1 - \beta)}{\alpha\beta + (1-\alpha) (1 - \beta)}}
    \end{bmatrix} .}
\end{align*}
Notice that in matrix $\boldsymbol \theta^{Y\rightarrow X}$, the variable $Y$ indexes rows and $X$ indexes columns, so that it is  row-stochastic as is standard for channel matrices. Matrix $\boldsymbol\theta^{Y\rightarrow X}$ represents a UC if and only if 
one (or both) of two conditions are satisfied: the diagonal elements are equal to each other; the elements in the first column are equal to each other (in which case, $X \perp\!\!\! \perp Y$). Simple algebraic manipulation allows showing that this is equivalent to having $(\alpha,\beta) \in \{(\alpha,\beta)\in[0,1]^2: \, \alpha = 0 \, \vee\,  \alpha = 1/2 \, \vee\, \alpha = 1 \, \vee\, \beta = 0 \, \vee\, \beta = 1/2 \, \vee\, \beta = 1\}$, which has zero Lebesgue measure. The following theorem generalizes this result for arbitrary $|\mathcal{X}|$ and $|\mathcal{Y}|$.

 \begin{theorem}\label{identifiability}
Let $X \in \mathcal{X}$ and $Y\in \mathcal{Y}$ be two categorical random variables with a joint pmf such that the conditional $\boldsymbol \theta^{X\rightarrow Y}$ corresponds to a UC. Assume also that the marginals have full support\footnote{There is no loss of generality in this assumption; if there are zeros in the marginals, we simply redefine $\mathcal{X}$ or/and $\mathcal{Y}$ by removing the zero-probability elements.}: $p_Y(y) \neq 0$, for any $y\in\mathcal{Y}$, and $p_X(x) \neq 0$, for any $x\in\mathcal{X}$. Further assume that the rows of the channel matrix $\boldsymbol \theta^{X\rightarrow Y}$ are not all equal to each other (\textit{i.e.}, $X$ and $Y$ are not independent\footnote{If all the rows are equal to each other, then $Y \perp \!\!\! \perp X$; since independence is a symmetrical relationship, the reverse channel $\boldsymbol \theta^{Y\rightarrow X}$ will also have all its rows equal to each other, thus being a special case of a UC channel.}).
%Let $\boldsymbol{\theta}^X$ be the marginal in $X$. 
Then, the set of parameters such that the reverse channel $\boldsymbol \theta^{Y\rightarrow X}$ is also a UCM has zero Lebesgue measure.
\end{theorem}

The proof, presented in Appendix \ref{app_proof}, essentially boils down to showing that the UC condition on the reverse channel  $\boldsymbol \theta^{Y\rightarrow X}$ corresponds to the zero set of a polynomial that is not identically zero, which thus has zero Lebesgue measure, a classical result from polynomial theory \citep{Federer1969}.




