\section{Results}
\label{sec:resul}
We compare the proposed approach, on synthetic, benchmark, and real data, with two state-of-the-art methods for categorical variables, for which code is publicly available: DC \citep{dc} ({\small \url{eda.mmci.uni-saarland.de/prj/cisc/}}, with $\epsilon = 0$) and HCR \citep{hcr} (a Python version of the $R$ code available at {\small \url{cran.r-project.org/web/packages/HCR/index.html}}). The code for all the experiments will be made available upon acceptance of the manuscript. 

% https://github.com/catsoliveira/LDLforCausalInference

In the ML estimates underlying our approach (namely \eqref{estimate_thetaxy} and \eqref{approximategamma}), to avoid the problem of zero or vanishing probabilities, we use a small amount ($10^{-3}$) of additive (a.k.a. Dirichlet) smoothing. 

%\subsection{Synthetic Data}

\begin{comment}
\subsubsection*{Experiment 1: Estimating Uniform Channels}
We first investigate the performance of Algorithms \ref{alg:symmetric} and \ref{alg:cyclic_symmetric} on synthetic datasets. We simulate data by generating it from a given UC or CUC. Afterwards, we assess the ability of the method to estimate it. We use the Kullbackâ€“Leibler divergence (KLD)   \cite{code} to compare the true UC and the estimated one. We use the standard symmetrized KLD, averaged over the rows of $\boldsymbol \theta^{Y|X}$, weighted by the (marginal) probabilities of the rows: 
\begin{equation*}
\small
    \text{KL} \!= \! \frac{1}{2}\sum\limits_{x=1}^{|\mathcal{X}|} 
     \left( \sum\limits_{y=1}^{|\mathcal{Y}|} \theta_{x,y}^{Y|X} \log \frac{\theta_{x,y}^{Y|X}}{\hat{\theta}_{x,y}^{Y|X}} + \sum\limits_{y=1}^{|\mathcal{Y}|} \hat{\theta}_{x,y}^{Y|X} \log \frac{\hat{\theta}_{x,y}^{Y|X}}{\theta_{x,y}^{Y|X}}  \right) \theta_{x}^X.
\end{equation*}

We consider synthetic data for an increasing number of sampled observations $N \in \{50$ , $100$, $200$, $300$, $400$, $500$, $1000$, $1500$, $2000$, $2500$, $3000\}$ and increasingly larger supports $\mathcal{X}$ and $\mathcal{Y}$. We choose the support sizes to be $(|\mathcal{X}|, |\mathcal{Y}|) \in \{(2,2)$, $(3,3)$, $(4,4)$, $(5,5)$, $(2,3)$, $(2,4)$, $(2,5)$, $(3,2)$, $(4,2)$, $(5,2)\}$. For each support setting and each number of samples $N$, we generate $50$ independent datasets. Consequently, the performance results obtained for each $N$, are the average across them.

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.45]{kl.png}
\qquad 
\includegraphics[scale=0.45]{kl_cyclic.png}
\caption{average KL divergence  for the numbers of samples previously mentioned and $|\mathcal{X}| = |\mathcal{Y}| = 2, 3, 4, 5$. Left: UDC. Right: CUDC.}
\label{fig:performance_measure_cyclic}
\end{figure*}

Figure \ref{fig:performance_measure_cyclic}  shows that the KLD between the true UDCs or CUDCs and the estimated ones  increases with the size of the support of the variables, in a manner compatible with the increase of the number of parameters being estimated, while, as expected, it decrease approximately with the inverse of the number of samples. We can thus conclude that Algorithms \ref{alg:symmetric} and \ref{alg:cyclic_symmetric} accurately estimate the underlying UDCs or CUDCs. For support settings in $\{(2,3), (2,4), (2,5), (3,2), (4,2), (5,2)\}$, a similar behaviour is found, thus we omit the corresponding plots. 
%However, one thing worth noticing is that, in general, a larger difference between the variables' support sizes induces a higher KL divergence between the original pmf and the estimated one, although these values only differ in decimal places.

\end{comment}

\subsection{Identifying the UCM Direction}
The first set of experiments is a sanity check, assessing the ability of the proposed criterion to identify the UCM direction, using synthetic data, with different sample sizes $N$ 
%$N \in \{50$ , $100$, $200$, $300$, $400$, $500$, $1000$, $1500$, $2000$, $2500$, $3000\}$, 
and different sizes of the support sets, $|\mathcal{X}|$, and $|\mathcal{Y}|$.
%$(|\mathcal{X}|, |\mathcal{Y}|) \in \{(2,2)$, $(3,3)$, $(4,4)$, $(5,5)$, $(2,3)$, $(2,4)$, $(2,5)$, $(3,2)$, $(4,2)$, $(5,2)\}$.
For each pair $(|\mathcal{X}|, |\mathcal{Y}|)$ and each $N$, we generate $100$ independent datasets using randomly generated UCMs in the $X\rightarrow Y$ direction and the results reported for each $N$ are the corresponding averages. The decision rule is simply to choose $X\rightarrow Y$, if ${\tt p}^{X\rightarrow Y} \geq {\tt p}^{Y\rightarrow X}$ (equivalently, $G_{X\rightarrow Y}^2 \leq G_{Y\rightarrow X}^2$), and $Y\rightarrow X$ (which is wrong), otherwise. The results in Fig. \ref{fig:prob} show that the accuracy achieves high values, close to 100\%, for $N> 500\sim 1000$, without a clear effect of the sizes of the support sets or difference between the non-cyclic and the cyclic cases.  


\begin{figure*}[ht]
\center
\includegraphics[scale=0.49]{accuracy_difsup.png}
\qquad 
\includegraphics[scale=0.49]{accuracy_difsup_cyclic.png}
\vspace{-0.1cm}
\caption{Accuracy in selecting the UCM direction, using the proposed criterion, for different sample and support sizes. Left plot: general UCMs; right plot: cyclic UCMs (CUCM).}
\label{fig:prob}
\end{figure*}

\subsection{Benchmark Data}
We use the 112 pairs in the \textit{cause-effect pairs} benchmark set \citep{causeeffectwebsite} where both variables are categorical and that have as ground truth that either $X \rightarrow Y$ or $Y \rightarrow X$. We set $\alpha = 0.05$ and compare UCM with the two methods mentioned above: DC \citep{dc} and HCR \citep{hcr}. Decisions of ``undecided'' are counted as wrong. The average accuracies of UCM, DC, and HCR reported in Table \ref{tab:results_bench} shows that UCM outperforms both HCR and DC on this dataset. Notice that a random decision would yield accuracy equal to 1/3.


\begin{table*}[hbt]
\centering
     \caption{Average accuracy results on the 112 pairs of the benchmark dataset.}
\vspace{0.5cm}
     \label{tab:results_bench}
     \makebox[\linewidth]{
     \begin{tabular}{| c | c |  c |}
     \hline
     UCM & DC & HCR \\
     \hline\hline
     0.61 & 0.41 & 0. 47\\
     \hline
\end{tabular} }
\end{table*}

\begin{comment}

As is common practice, we plot the accuracy against the fraction of decisions that the methods are forced to make. The pairs are sorted in decreasing order of absolute score difference $|D^{X \rightarrow Y} - D^{Y \rightarrow X}|$ (which quantifies the confidence in the decision), and the accuracy is computed over the top $k\%$ pairs. If $D^{X \rightarrow Y} = D^{Y \rightarrow X}$, the decision is considered wrong. For the other methods, we follow a similar procedure. 

\begin{figure}[hbt]
    \centering
    \includegraphics[scale=0.5]{benchmark2.png}
    \vspace{-0.2cm}
    \caption{Accuracy vs decision rate for categorical pairs in the \textit{Cause-effect pairs} benchmark dataset.}
    \label{fig:dec_rate}
\end{figure}

Fig. \ref{fig:dec_rate} shows that all methods correctly infer the causal direction for the pair for which each method has the highest confidence. At rate 100\% (all pairs), LDL has an accuracy of roughly $59\%$, whereas DC and HCR achieve global accuracy of $41\%$ and $47\%$, respectively. Moreover, LDL outperforms both competing methods for all decision rate above 60\%. Also, it achieves acceptable results even when the decision rate is 1. In conclusion, it can be claimed that, on this dataset, LDL outperforms the competing alternatives.

\end{comment}

\subsection{Real Data}\label{sec:real_data}
Finally, we evaluate the UCM method (again with $\alpha = 0.05$) on real data from the \textit{UCI Machine Learning Repository} \citep{uciwebsite}. We use pairs of variables from the following datasets: Adult, Pittsburgh Bridges, Accute Inflamation, Temperature, and Horse Colic. The datasets and selected pairs, as well as the criteria used to decide what is the ground truth causal direction, are described in Appendix \ref{app_data}. We include only pairs for which a test of independence, at significance level 0.05 \citep{Agresti}, rejects the null hypothesis of independence. Furthermore, we include only pairs for which at least one of the three tested methods chooses one of the causal directions. 
Table \ref{tab:results} shows that the UCM and HCR approaches found the ``correct" causal direction in 5 out of 9 pairs, and DC in 4 pairs. UCM returned only correct decisions or abstained from deciding. This small number of experiments does now allow for reaching any strong conclusions but suggests that UCM performs on par, arguably somewhat better, with DC and HCR.  


\begin{table*}[hbt]
\centering
\normalsize
{
     \caption{Results on real data. Wrong decisions are shown in red; UWM stands for "undecided: wrong model". Month is a cyclic variable, thus a CUC was used in the $Y\rightarrow X$ direction.}
\vspace{-0.2cm}
     \label{tab:results}
     \makebox[\linewidth]{
     \begin{tabular}{ c | c  c  c  c c}
     Dataset & $X$ & $Y$ & UCM & DC & HCR\\[0.1cm]
     \hline
     Adult & Occupation & Income & UWM & $X
\rightarrow Y$ &$X \rightarrow Y$ \\
     Adult & Work Class & Income & UWM & $X
\rightarrow Y$   &$X \rightarrow Y$    \\
      Acute Inflammation & Inflam. of urinary bladder & Lumbar pain & $Y
\rightarrow X$ & Inconcl. &  Inconcl.\\
      Acute Inflammation & Inflam. of urinary bladder & Nausea & $Y
\rightarrow X$ & Inconcl. &  Inconcl.\\
 Acute Inflammation & Inflam. of urinary bladder & Burning urethra & $Y
\rightarrow X$ & Inconcl. &  Inconcl.\\
     Pittsburgh Bridges & Material & Lanes & $X \rightarrow Y$ &
\color{red} $Y \rightarrow X$ & $X \rightarrow Y$ \\
     Pittsburgh Bridges & Purpose & Type & UWM &
\color{red} $Y \rightarrow X$ & \color{black} $X \rightarrow Y$ \\
     Temperature & Month & Temperature & $X \rightarrow Y$ &
$X \rightarrow Y$ & \color{red} $Y \rightarrow X$ \\
     Horse Colic & Abdomen Status & Surgical Lesion & UWM &
$X \rightarrow Y$ & $X \rightarrow Y$ \\
\hline 
     \end{tabular} }}
\end{table*}

