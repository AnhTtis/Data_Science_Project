\section{Channel Estimation from Data}
\label{sec:channel}
This section addresses the problem of estimating channel parameters $\boldsymbol \theta^{X\rightarrow Y}$ from $N$ independent and identically distributed samples of $(X,Y): (x_1,y_1),..., (x_N, y_N)$. This will play a key role in translating the causal inference criterion proposed in Section \ref{sec:UCModel} to the realistic scenario where there is only access to a finite amount of data, rather than perfect knowledge of the joint pmf $p_{X,Y}$.

Before estimating the channel parameters (conditional pmf), notice that estimating the marginal pmf $\boldsymbol \theta^{X}$ is trivial: with $N_x$ denoting the number of samples $(x_i, y_i)$ with $x_i = x$, the \textit{maximum likelihood} (ML) estimate of $\boldsymbol \theta^X$ is given by 
\begin{equation}
    \hat{\boldsymbol \theta}^{X} = \; \arg \max_{{\boldsymbol \theta} \in \Delta _{|\mathcal{X}|-1}} \sum_{x\in\mathcal{X}} N_x \log  \theta^X_{x} = \Bigl( \frac{N_1}{N},...,\frac{N_{|\mathcal{X}|}}{N}\Bigr).
    \label{eq:ML_thetax}
\end{equation}

For estimating $\boldsymbol \theta^{X\rightarrow Y}$, we consider the following 4 scenarios:  \textbf{(1)} arbitrary channel; \textbf{(2)} UCM, with known permutations; \textbf{(3)} UCM,  with \textit{unknown} permutations; \textbf{(4)} CUCM, with \textit{unknown} cyclic permutations. Scenarios 1 and 2 are trivial and considered only as they provide the building blocks to address scenarios 3 and 4.

\subsection{Scenario 1: Arbitrary Channel}
\label{subsec: firstscenario}
Let $N_{x,y}$ be the number of samples $(x_i, y_i)$ such that $x_i = x$ and $y_i=y$. In the absence of constraints other than each row of $\boldsymbol \theta^{X\rightarrow Y}$ must be a valid pmf, the ML estimate is 

\begin{equation}
    \hat{\boldsymbol{\theta}}^{X\rightarrow Y}  =  \underset{\boldsymbol{\theta} \in (\Delta_{|\mathcal{Y}|-1})^{|\mathcal{X}|}}{\text{arg max}} \sum_{x\in \mathcal{X}} \sum_{y\in \mathcal{Y}}  N_{x,y}\log \theta_{x,y}.
    \label{objectivefunction}
\end{equation}
Since both the objective function and the constraints in \eqref{objectivefunction} are separable across $x= 1,..., |\mathcal{X}|$, the problem is also separable into a collection of problems, each yielding the classical ML estimates
\begin{equation}
    \hat{\theta}^{X\rightarrow Y}_{x,y} = N_{x,y}/N_x, \hspace{0.3cm} \mbox{for $(x,y)\in \mathcal{X}\times \mathcal{Y}$}.
\label{estimate_thetaxy}
\end{equation}

\subsection{Scenario 2: UCM with Known Permutations}
\label{2ndscenario}
In a UC, each row $\boldsymbol \theta_x^{X\rightarrow Y}$ is as given in \eqref{eq_gama_perm}. If the permutations $\sigma_1, \ldots, \sigma_{|\mathcal{X}|}$ are known, the ML estimate of $\boldsymbol \gamma$ is given by 
\begin{equation}
    \hat{\boldsymbol{\gamma}} = \;  \underset{\boldsymbol{\gamma} \in \Delta _{|\mathcal{Y}|-1} }{\text{arg max}} \sum_{x \in \mathcal{X}} \sum_{y\in \mathcal{Y}}  N_{x,y}\log \gamma_{\sigma_x (y)}.
    \label{objectivefunction_symmetric}
\end{equation}
This problem is not separable, as all the rows of the channel matrix share the same probability values, although with different permutations. Swapping the summation order  and using the inverse permutations $\tau_x = \sigma_x^{-1}$ to do a change of variable in the sum over $\mathcal{Y}$, problem \eqref{objectivefunction_symmetric}  can be rewritten as
\begin{equation}
    \hat{\boldsymbol{\gamma}} = \; \underset{\boldsymbol{\gamma} \in \Delta _{|\mathcal{Y}|-1} }{\text{arg max}}  \sum\limits_{z\in \mathcal{Y}} \log \gamma_z \sum_{x\in \mathcal{X}} N_{x,\tau_{x}(z)}.
    \label{eq:UDC_known}
\end{equation}
Problem \eqref{eq:UDC_known} has the same form as \eqref{eq:ML_thetax}, the solution being simply
\begin{equation}
    {\displaystyle \hat{\gamma}_y =  \frac{1}{N}\sum\limits_{x\in \mathcal{X}} N_{x, \tau_x (y)}}, \;\; \; \mbox{for $y \in \mathcal{Y}$}.
\label{approximategamma}
\end{equation}
Notice that $N_{x, \tau_x (y)}$ is the number of samples $(x_i,y_i)$ such that $x_i=x$ and $y_i = \tau_x (y)$.

\subsection{Scenario 3: UC with Unknown Permutations}
\label{subsec: thirdscenario}
In this case, the log-likelihood is maximized, not only w.r.t. $\boldsymbol\gamma$, but also the permutations. Although, at first sight, this may look like a very hard problem, as there are $(|\mathcal{Y}|!)^{|\mathcal{X}|}$ combinations of permutations, we show next that it can be solved very efficiently. The optimization problem in hand (formulated w.r.t. the inverse permutations, denoted as $\tau_1,\ldots, \tau_{|\mathcal{X}|} \in \mathbb{S}_{|\mathcal{Y}|}$) is
\begin{align}
    \hat{\boldsymbol \gamma}, \hat{\tau}_1, \ldots, \hat{\tau}_{|\mathcal{X}|} \;  = \hspace{-0.5cm} \underset{ \begin{array}{c} \boldsymbol \gamma \in \Delta _{|\mathcal{Y}|-1} \\ 
    \tau_1,...,\tau_{|\mathcal{X}|} \in \mathbb{S}_{|\mathcal{Y}|}\end{array} }{\text{arg max}} \hspace{-0.5cm} \mathcal{L}(\boldsymbol \gamma, \tau_1,...,\tau_{|\mathcal{X}|} ), \label{thirdscenario}
\end{align}
where 
\begin{align}
    \mathcal{L}(\boldsymbol \gamma, \tau_1,...,\tau_{|\mathcal{X}|} ) = \sum_{x\in \mathcal{X}} \sum_{y\in \mathcal{Y}}  N_{x,\tau_x (y)}\log \gamma_y. \label{thirdscenario_b}
\end{align}
The following proposition (proved in Appendix \ref{proof_estimate}) provides the solution to this problem.

\begin{proposition}\label{proof_UCestimate}
A globally optimal solution to the problem specified in \eqref{thirdscenario}--\eqref{thirdscenario_b} is given as follows. For $x\in \mathcal{X}$, $\hat{\tau}_x$ is any permutation that sorts $\{N_{x,1},..., N_{x,|\mathcal{Y}|}\}$ into non-increasing order,
\begin{equation}
\hat{\tau}_x\;\; \mbox{is such that}\;\; N_{x, \hat{\tau}_x (1)} \geq \cdots \geq N_{x, \hat{\tau}_x (|\mathcal{Y}|)},\label{sortedNx_a}
\end{equation}
and, for  $y \in \mathcal{Y}$,
\begin{equation}
    {\displaystyle \hat{\gamma}_y = \frac{1}{N}\sum\limits_{x\in \mathcal{X}} N_{x, \hat\tau_x (y)}}.
\label{approximategamma2_a}
\end{equation}
\end{proposition}

The solution in \eqref{sortedNx_a}--\eqref{approximategamma2_a} is a global, but not unique, optimum; in fact, any pmf $\hat{\boldsymbol{\xi}}$ that is a permutation of $\hat{\boldsymbol{\gamma}}$, \textit{i.e.}, $\hat{\gamma}_y = \hat{\xi}_{\rho(y)}$, where $\rho \in \mathbb{S}_{|\mathcal{Y}|}$, yields 
\[
\hat{\theta}_{x,y}^{X\rightarrow Y} = \hat{\gamma}_{\hat{\sigma}_x(y)} = \hat{\xi}_{\rho(\hat{\sigma}_x(y))}.
\]
That is, $\boldsymbol\gamma$ is identifiable only up to a permutation, since any permutation of $\hat{\boldsymbol{\gamma}}$, combined with the inverse of that permutation composed with each $\sigma_x$, yields the same conditional pmf estimate $\hat{\boldsymbol{\theta}}^{X\rightarrow Y}\!\!\!$, thus the same maximum value of the log-likelihood. Finally, notice that the cost of computing this solution scales as $O(|\mathcal{X}|\, |\mathcal{Y}|\, \log |\mathcal{Y}|)$, due to the number $|\mathcal{X}|$ of sorting operations, each of size $|\mathcal{Y}|$.

\subsection{Scenario 4: CUC with Unknown Permutations}
\label{subsec:fourth}
The difference between this and the previous case is that the permutations are now cyclic. Thus, the corresponding optimization problem is identical to \eqref{thirdscenario}--\eqref{thirdscenario_b}, but with the constraint $\tau_1,...,\tau_{|\mathcal{X}|} \in \mathbb{S}_{|\mathcal{Y}|}$ replaced with $\tau_1,...,\tau_{|\mathcal{X}|} \in \mathbb{C}_{|\mathcal{Y}|}$, where $\mathbb{C}_{|\mathcal{Y}|}$ is the set of cyclic permutations of $\{1,...,|\mathcal{Y}|\}$. 

Although the cardinality of $\mathbb{C}_{|\mathcal{Y}|}$ is $|\mathcal{Y}|$, much smaller than that of $\mathbb{S}_{|\mathcal{Y}|}$, which is $|\mathcal{Y}|!$, this problem is harder than  \eqref{thirdscenario}--\eqref{thirdscenario_b}. Whereas the cost of the exact solution of \eqref{thirdscenario}--\eqref{thirdscenario_b} scales with $O(|\mathcal{X}|\, |\mathcal{Y}|\, \log |\mathcal{Y}|)$, exactly solving this problem by exhaustive search costs $O(|\mathcal{Y}|^{|\mathcal{X}|})$. It happens that this problem is a variant of a class of problems known as \textit{multireference alignment}, which is known to be NP-hard \citep{Bandeira}. Exact solutions are thus out of the question for large problems. Here, we propose an alternating maximization approach with two steps:
\begin{itemize}[leftmargin=0.8cm,itemsep=0cm]
    \item Given the current permutation estimates $\hat{\tau}_1, ..., \hat{\tau}_{|\mathcal{X}|}$, update $\hat{\boldsymbol{\gamma}}$ according to \eqref{approximategamma}, with $\tau_x = \hat{\tau}_x$. 
    \item Given the current $\hat{\boldsymbol \gamma}$, maximize w.r.t. the permutations, which is separable across $\tau_1,...,\tau_{|\mathcal{X}|}$:
    \begin{equation}
    \hat{\tau}_x = \underset{\tau \in \mathbb{C}_{|\mathcal{Y}|}}{\text{arg max}} \sum\limits_{y=1}^{|\mathcal{Y}|} N_{x,\tau(y)} \log \hat{\gamma}_y, \;\;\mbox{for $x=1,...,|{\mathcal X}|$.}
    \label{eq: cyclic}
\end{equation}
    This maximization is carried out exactly by considering all the $|\mathcal{Y}|$ cyclic permutations. 
    \end{itemize}
The costs of both steps of this algorithm scale as $O (|\mathcal{X}|\, |\mathcal{Y}|)$. Convergence can be proved via the same approach that is used to prove convergence of the $K$-means algorithm \citep{kmeans}, since both algorithms share a common structure: alternate between exact maximization with respect to real quantities (cluster centers, in K-means, $\boldsymbol{\gamma}$ in our algorithms) and an exact combinatorial optimization (the point-to-cluster assignments in K-means, the cyclic permutations in the proposed algorithms). 

\begin{comment}
\begin{algorithm}
\caption{Algorithm to estimate a UC with unknown permutations}
\label{alg:symmetric}
\KwIn{$|\mathcal{X}|\times |\mathcal{Y}|$ count matrix $\mathbf{N}= [N_{x,y}]$ }
\KwOut{$\hat{\gamma}$ and $\hat{\tau}_x$, for $x= 1,\ldots, |\mathcal{X}|$}
 Find the permutation $\rho_x $ that sorts the entries of the $x$-th row of $\mathbf{N}$ in increasing order, for $x= 2,\ldots, |\mathcal{X}|$\;
Set $\hat{\tau}_1 = (1,2,\ldots, |\mathcal{Y}|)$

 Set initial parameter estimate $\hat{\gamma}$
 
 \Repeat{objective function stops changing}{
 \textbf{Update the permutations:} 
 
 Find the permutation $\eta$ that sorts the components of $\hat{\gamma}$ in increasing order. 
 
 Set $\hat{\tau}_x = \rho_x \circ \eta^{-1}$, for $x= 2,\ldots, |\mathcal{X}|$\;
 
 \textbf{Update the parameters:} Update $\hat{\gamma}$ according to: 
 \begin{equation*}
     \hat{\gamma}_y = \frac{1}{N} \sum_{x\in \mathcal{X}} N_{x, \hat{\tau}_x (y)}, \quad \text{for} \quad y = 1,\ldots,|\mathcal{Y}|.
 \end{equation*}
 
 }
\end{algorithm}
\end{comment}


\begin{comment}
\begin{algorithm}
\small
\SetAlgoLined
\KwIn{$|\mathcal{X}|\times |\mathcal{Y}|$ count matrix $\mathbf{N}= [N_{x,y}]$ }
\KwOut{$\hat{\gamma}$ and $\hat{\tau}_x$, for $x= 1,\ldots, |\mathcal{X}|$}
Set $\hat{\tau}_1 = (1,2,\ldots, |\mathcal{Y}|)$\;
 Set initial parameter estimate $\hat{\gamma}$\;
 \Repeat{objective function stops changing}{
 \textbf{Update the permutations:} 
 
 Find $\hat{\tau}_x $ according to \eqref{eq: cyclic}, for $x=2,\ldots,|\mathcal{X}|$\;
 \textbf{Update $\hat{\gamma}$:} Update $\hat{\gamma}$ according to: 
 \begin{align*}
      \hat{\gamma}_y = \frac{1}{N} \sum_{x\in \mathcal{X}} N_{x, \hat{\tau}_x (y)}, \quad \text{for} \quad y = 1,\ldots,|\mathcal{Y}|\;    
 \end{align*}
 }
 \caption{Algorithm to estimate a CUDC with unknown cyclic permutations}
 \label{alg:cyclic_symmetric}
\end{algorithm}

\end{comment}

\begin{comment}

\subsection{Algorithm Convergence}\label{sec:convergence}
We study the convergence of the algorithms described in Subsection \ref{sec:estimating} by exploiting their structural similarity to the K-means clustering algorithm \citep{kmeans}: alternate between maximization w.r.t. real quantities (cluster centers, in K-means, $\boldsymbol{\gamma}$ in our algorithms) and a combinatorial optimization (the point-to-cluster assignments in K-means, the permutations in our algorithms). 

Before stating and proving the result, we formally present the algorithms in a common framework. Denote the collection of all the unknown permutations as $\boldsymbol \tau = (\tau_2, ..., \tau_{|\mathcal{X}|})$ (recall that $\tau_1 = (1, ..., |\mathcal{Y}|)$) and the objective function in \eqref{thirdscenario}  as $\mathcal{L}(\boldsymbol \gamma, \boldsymbol \tau)$.  The problems of estimating a UC or a CUC can both be written as
\begin{equation}
         \underset{\boldsymbol \gamma, \boldsymbol \tau}{\text{max}} \quad \mathcal{L}(\boldsymbol \gamma, \boldsymbol \tau), \hspace{1cm}\; \text{subject to} \quad  \boldsymbol\tau \in \mathbb{D}, \quad \boldsymbol\gamma \in \Delta_{|\mathcal{Y}| - 1},
    \label{problemP}
\end{equation}
where $\mathbb{D}= (\mathbb{S}_{|\mathcal{Y}|})^{|\mathcal{X}| -1}$, for a UC, or  $\mathbb{D}=(\mathbb{C}_{|\mathcal{Y}|})^{|\mathcal{X}| -1}$, for a CUC. Notice that if $Y$ is binary, there is no difference between using a UC or a CUC, since $\mathbb{S}_2 = \mathbb{C}_2$. With this notation, the algorithms can both be written as Algorithm 1, which also describes more in detail the stopping criterion.

\begin{algorithm}
{\footnotesize
\SetAlgoLined
\caption{Algorithm to Estimate a UC or a CUC}
\label{alg:common}
\KwResult{Estimates $\hat{\boldsymbol \tau}$ and $\hat{\boldsymbol \gamma}$}
Set $t=0$, ${\tt stop} = 0$, and an initial parameter estimate $\hat{\gamma}^{(0)}$\;

Find the initial permutation: $
\hat{\boldsymbol \tau}^{(0)} = \underset{\boldsymbol \tau \in \mathbb{D}}{\text{arg max}} \quad \mathcal{L}(\hat{\boldsymbol \gamma}^{(0)}, \boldsymbol \tau)$\;

 \Repeat{\texttt{stop} = 1}{
Update the parameter estimate: 
\vspace{-0.2cm}\begin{equation}
\hat{\boldsymbol \gamma}^{(t+1)} =\underset{\boldsymbol \gamma \in \Delta_{|\mathcal{Y}|-1}}{\text{arg max}} \quad \mathcal{L}(\boldsymbol \gamma, \hat{\boldsymbol \tau}^{(t)}). \label{b:gamma} 
\end{equation}

\eIf{$\mathcal{L}(\hat{\boldsymbol \gamma}^{(t+1)}, \hat{\boldsymbol \tau}^{(t)}) = \mathcal{L}(\hat{\boldsymbol \gamma}^{(t)}, \hat{\boldsymbol \tau}^{(t)}) $}
{Set $\hat{\boldsymbol \tau} = \hat{\boldsymbol \tau}^{(t)}$ and $\hat{\boldsymbol \gamma} = \hat{\boldsymbol \gamma}^{(t)}$

Set \texttt{stop} = 1
}{Update the permutations:

\vspace{-0.2cm}\begin{equation}
\hat{\boldsymbol \tau}^{(t+1)} = \underset{\boldsymbol \tau \in \mathbb{D}}{\text{arg max}} \quad \mathcal{L}(\hat{\boldsymbol \gamma}^{(t+1)}, \boldsymbol \tau) \label{a:tau}
\end{equation}

\If{$\mathcal{L}(\hat{\boldsymbol \gamma}^{(t+1)}, \hat{\boldsymbol \tau}^{(t+1)}) = \mathcal{L}(\hat{\boldsymbol \gamma}^{(t+1)}, \hat{\boldsymbol \tau}^{(t)}) $}{Set $\hat{\boldsymbol \tau} = \hat{\boldsymbol \tau}^{(t)}$ and $\hat{\boldsymbol \gamma} = \hat{\boldsymbol \gamma}^{(t+1)}$\; 
Set \texttt{stop} = 1}
}}}
\end{algorithm}



\begin{theorem}
Algorithm 1 converges in a finite number of iterations to a POS \textit{(partial optimal solution)} \citep{kmeans},
which is defined as a point $(\boldsymbol \gamma^*, \boldsymbol \tau^*)$ satisfying
\[
  \mathcal{L}(\boldsymbol \gamma^*, \boldsymbol \tau^*) \geq   \mathcal{L}(\boldsymbol \gamma, \boldsymbol \tau^*), \;\; \forall\boldsymbol\gamma \in \Delta_{|\mathcal{Y}| - 1} \hspace{0.5cm} \mbox{and} \hspace{0.5cm}
             \mathcal{L}(\boldsymbol \gamma^*, \boldsymbol \tau^*) \geq   \mathcal{L}(\boldsymbol \gamma^*, \boldsymbol \tau), \;\; \forall\boldsymbol\tau \in \mathbb{D}. 
\]
\end{theorem}

\begin{proof} Since the two steps of the algorithm are solved exactly, it is clear that
\begin{equation}
\mathcal{L}(\hat{\boldsymbol \gamma}^{(t+1)}, \hat{\boldsymbol \tau}^{(t+1)}) \geq \mathcal{L}(\hat{\boldsymbol \gamma}^{(t+1)}, \hat{\boldsymbol \tau}^{(t)}) \geq \mathcal{L}(\hat{\boldsymbol \gamma}^{(t)}, \hat{\boldsymbol \tau}^{(t)}).
\end{equation}
If any of these two inequalities is not strict,  the algorithm stops.
%Moreover, since $\gamma_y \leq 1$, for $y = 1,\ldots, |\mathcal{Y}|$, then $\log \gamma_y \leq 0$. Thus, we have that
%\begin{equation}
%  \mathcal{L}(\boldsymbol \gamma, \boldsymbol \tau) = \sum_{x\in \mathcal{X}}  \sum_{y\in \mathcal{Y}}  N_{x, \tau_x (y)} %\log \gamma_y \leq 0.
%\end{equation}
%Therefore, the sequence $\mathcal{L}(\hat{\boldsymbol \gamma}^{(t)}, \hat{\boldsymbol \tau}^{(t)}), t=1,2,\ldots$ is %monotonically increasing and bounded above. Thus, according to the monotone convergence theorem, the sequence is convergent. 
Note that $(\boldsymbol\gamma^*, \boldsymbol\tau^*)$ is a POS of \eqref{problemP} if and only if $\boldsymbol\gamma^*$ solves \eqref{b:gamma} for $\hat{\boldsymbol\tau}^{(t)} = \boldsymbol\tau^*$ and $\boldsymbol\tau^*$ solves \eqref{a:tau} with $\hat{\boldsymbol\gamma}^{(t+1)} = \boldsymbol\gamma^*$. The set POS thus coincides with that of fixed point of the algorithm. To show convergence to a POS in a finite number of iterations it suffices to show that any point of $\mathbb{D}$ (which is finite) is visited at most once before the algorithm stops. Assume that this was not true: that $\hat{\boldsymbol \tau}^{(t)} = \hat{\boldsymbol \tau}^{(t')}$, for some $t \neq t'$. Updating $\hat{\boldsymbol \gamma}$ according to \eqref{b:gamma} yields $\hat{\boldsymbol \gamma}^{(t+1)}$ and $\hat{\boldsymbol \gamma}^{(t'+1)}$, which satisfy
\begin{equation}
\mathcal{L}(\hat{\boldsymbol \gamma}^{(t+1)}, \hat{\boldsymbol \tau}^{(t)})  =  \mathcal{L}(\hat{\boldsymbol \gamma}^{(t+1)}, \hat{\boldsymbol \tau}^{(t')}) = \mathcal{L}(\hat{\boldsymbol \gamma}^{(t'+1)}, \hat{\boldsymbol \tau}^{(t')}).
\label{sub:wrong}
\end{equation}
where the first equality results from assuming that $\hat{\boldsymbol \tau}^{(t)} = \hat{\boldsymbol \tau}^{(t')}$ and the second one from the fact that $\hat{\boldsymbol \gamma}^{(t+1)}$ and $\hat{\boldsymbol \gamma}^{(t'+1)}$ both result from solving \eqref{b:gamma} (which has a unique solution) with the same $\hat{\boldsymbol \tau}^{(t)} = \hat{\boldsymbol \tau}^{(t')}$. However, as seen above, unless the algorithm stops, we have strict monotonicity at each step, thus \eqref{sub:wrong} cannot be true, contradicting that we can have $\hat{\boldsymbol \tau}^{(t)} = \hat{\boldsymbol \tau}^{(t')}$, for some $t \neq t'$. Finally, $\mathbb{D}$ is  finite, thus the algorithm reaches a POS and stops a finite number of steps.
\end{proof}

\vspace{-0.6cm}
\end{comment}
