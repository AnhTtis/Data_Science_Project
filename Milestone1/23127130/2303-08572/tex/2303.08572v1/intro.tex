\section{Introduction}
\label{sec:intro}
Causal inference is a key problem in many areas of science and data analysis \citep{pearlcausality}. In principle, distinguishing statistical dependencies from causal relationships requires interventions \citep{pearlcausality,elementsofcausalinference}. However, intervening is often impossible (\textit{e.g.}, analyzing past data), impractical, or unethical (\textit{e.g.}, forcing people to smoke), which has stimulated much research aimed at inferring causal relationships (\textit{causal discovery}) from purely observational data \citep{Janzing2019, Mooij2016, elementsofcausalinference}, or mixed observational-interventional data \citep{pmlr-v177-faria22a}

There is a vast literature  on methods to learn \textit{directed acyclic graphs} (DAGs) from  data, usually by inferring \textit{conditional independence} (CI) properties among variables \citep{Chickering2002b,Heckerman,Koller_Friedman}. However, without additional assumptions or criteria, those methods cannot distinguish different DAGs entailing the same CIs (in the same \textit{Markov equivalence class} -- MEC). The simplest instance of this problem involves a pair of variables $(X,Y)$: purely statistical methods (\textit{e.g.}, maximum likelihood estimation) cannot recover the causal graph because $X \rightarrow Y$ and $Y \rightarrow X$ constitute a MEC, corresponding to the two possible factorizations of the joint distribution: $p_{X,Y}(x,y) = p_{Y|X}(y|x) \, p_X(x) = p_{X|Y}(x|y)\, p_Y(y)$. Although there are several methods that select particular elements of a MEC, they all include additional assumptions beyond \textit{faithfulness}\footnote{Fiathfulness holds if every conditional independence in the joint probability distribution corresponds to a separation property in the graph \citep{Koller_Friedman,Sadeghi}.} and, for one reason or another they cannot be used to distinguish between $X \rightarrow Y$ and $Y \rightarrow X$, where $X$ and $Y$ are a pair of categorical variables; \textit{e.g.}, they are only applicable to quantitative data \citep{Park2018} or only make sense for more than two variables \citep{Gao2021}.

Without interventions, choosing an element of a MEC requires additional assumptions about the underlying data-generating mechanism. For instance, \textit{additive noise models} (ANM) \citep{anm1,anm3,anm2011,anm4} assume the effect is a function of the cause plus a noise term independent of the cause ($Y = f_Y(X) + N_Y, X \perp \!\!\! \perp N_Y$); if the same doesn't hold in the reverse direction, the model is said to be \textit{identifiable}. ANMs are generically identifiable \citep{anm2011,anm4} in the following sense: if the joint density $p_{X,Y}$ corresponds to an ANM, the conditional density $p_{Y|X}(\cdot|x)$ has identical \textit{shape} for any $x$, simply being \textit{shifted} by $f_Y(x)$, but $p_{X|Y}(\cdot|y)$ typically depends on $y$ in a more complicated way. Under the ANM criterion, if such a  model exists in one direction but not the other, the former is selected as the causal direction.

The ANM can be seen as an instance of the principle of \textit{independence of cause and mechanism} (ICM) \citep{Janzing2010,kolm3}, according to which the cause-effect mechanism (a deterministic function followed by addition of noise, in an ANM) is \textit{independent}\footnote{The term ``independent'' here does not have a probabilistic sense, but a functional sense: changing the distribution of the cause does not affect the causal mechanim, and vice-versa.} of the cause, thus of its distribution. The ICM principle has been exploited using  different tools to define and assess the notion of \textit{independence}: information geometry \citep{Daniusis, kolm3}; algorithmic information theory, namely Kolmogorov complexity (which is not computable, but is approximable \citep{Vitanyi}), by \citet{Janzing_TIT_2010} and \citet{Mian_Marx_Vreeken_2021}; stochastic complexity, via the \textit{minimum description length} (MDL \citep{mdl}) principle \citep{Budhathoki2017,Marx2019,Tagasovska} or the \textit{minimum message length} \citep{Wallace} criterion, by \citet{Stegle}.

Relatively few methods have been proposed to address the cause-effect problem with categorical variables. \citet{anm2011,Peters_AISTATS_2010}  extended ANMs to the discrete case and proved identifiability, but only for variables taking values in a set equipped with a meaningful order, in which an operation similar to addition (a shift) is defined. They consider the rings $\mathbb{Z}$, for variables without cyclic structure, and $\mathbb{Z}/n\mathbb{Z}$ with modulo-$n$ addition, for cyclic variables (\textit{e.g.},  seasons or months of the year), or subsets of these rings. However, purely categorical variables live in sets with no order, thus no meaningful notion of addition or shift, precluding the direct use of ANMs. \citet{anm2011} consider what they call ``structureless'' sets, but only with a particular form of the conditional pmf, not generally applicable. Some of the MDL-based methods mentioned above can be used with categorical variables: \citet{Budhathoki2017} proposed CISC (\textit{causal inference by stochastic complexity}); \cite{hcr} proposed HCR (\textit{hidden compact representation}), based on BIC (\textit{Bayesian information criterion}). \citet{dc} assess mechanism independence via a \textit{distance correlation} (DC) between the cause pmf and the conditional pmf of the effect. \citet{kocaoglu_entropic_2017}  select the causal direction in which the sum of the marginal entropy of the cause with that of the exogenous variable in the corresponding \textit{structural causal model} (SCM) is minimal. Recently, \citet{ni2022} addressed the cause-effect problem for categorical variables by formulating the conditional distribution of the effect given the cause as ordinal regression, with optimal label permutation, and choosing the direction in which this model has the highest likelihood. 

As is standard when focusing on the cause-effect problem, we assume \textit{causal sufficiency} (\textit{i.e.}, absence of unobserved \textit{confounders}), no selection bias, and no feedback \citep{Mooij2016}. We propose a new approach to the cause-effect problem for categorical variables, inspired by viewing the causal mechanism as a communication channel. This view allows extending to the categorical case a key feature of ANMs: the conditional (differential) entropy of the effect given the cause is independent\footnotemark[1] of the distribution of the cause \citep{kolm3}. For categorical variables (\textit{i.e.}, symbols, in channel terminology), a memoryless channel corresponds to the conditional \textit{probability mass function} (pmf) of the output given the input, the \textit{channel matrix} $\boldsymbol \theta^{X\rightarrow Y}$, where  $\boldsymbol \theta_{x,y}^{X\rightarrow Y} = p_{Y|X}(y|x) = \mathbb{P}[Y=y | X=x]$. In a so-called \textit{uniform channel} (UC \citep{Hamming}), the rows of this matrix are permutations of each other\footnote{A uniform channel is not necessarily a \textit{symmetric channel}, which requires additionally that all the columns are also permutations of each other \citep{code}.}, implying (as shown below) that the conditional entropy $H(Y|X)$ is independent of the distribution of $X$. Paralleling the ANM rationale, given a pair of categorical variables ($X,Y$), if the conditional pmf in one direction, say of $Y$ given $X$, corresponds to a UC and the same is not true in the other direction, then the causal structure is declared to be $X\rightarrow Y$. This criterion, which we refer to as the UCM (\textit{uniform channel model}) is supported by an \textit{identifiability} result proved in this paper: if a joint distribution corresponds to a UCM in one direction, in general (\textit{i.e.}, with probability one under any continuous density on the model parameters), it does not correspond to a UCM in the reverse direction. 

The proposed UCM approach is further supported by the fact (proved below) that if, and only if, $\boldsymbol \theta^{X\rightarrow Y}$ corresponds to a UCM, is it possible to write a \textit{structural causal model} (SCM) \citep{pearlcausality} of the form $Y = f_Y(X,U_Y)$, where $f_Y$ is a deterministic function and $U_Y$ is an exogenous random variable, taking values in in the same set as $Y$ and independent of $X$. The importance of this independence was recently highlighted by \citet{Papineau}: ``the probabilistic independence of exogenous terms in (...) structural equations holds the key to causal direction''. 

A final question is how to instantiate the UCM principle with a finite amount of data. This question parallels that of how to estimate the underlying function and noise distribution in an ANM. Naturally, with a finite dataset, we only have an estimate of the underlying distribution and the probability that this estimate corresponds exactly to a UCM in one of the two directions is vanishingly small. Although other ways to address this issue are conceivable, we resort to statistical hypothesis testing to decide in which direction, if any, the conditional pmf can be considered a UCM. A key building block of this approach is estimating a channel under the constraint that it is uniform; this is a problem that, to the best of our knowledge, had not be studied before and for which we derive a closed-form solution. We also extend the approach to the case where the rows of the channel matrix are cyclic permutations of each other (a \textit{cyclic UCM} -- CUCM), applicable when the effect variable has cyclic nature, but in this case the channel estimate has to be obtained iteratively.

It is important to stress that using the UCM (or ANM, or any other restricted model class, for that matter) to identify a causal relation does not imply any assumption that this is a realistic model of the true underlying relation. As clearly argued by \citet[Section 4.1.2]{elementsofcausalinference}, the rationale is simply that if there is such a model in one direction, but not the other, it is more likely that the former is the causal direction. 

The main contributions of this paper are the following:

\begin{itemize}
    \item A new instantiation, for categorical variables, of the principle of \textit{independence of cause and mechanism}: the \textit{uniform channel model} (UCM) principle.
    \item A proof of identifiability of the UCM. 
    \item A proof that the joint distribution of a pair of categorical random variables is entailed by an SCM in which the exogenous noise has the same cardinality as the effect variable if and only if it corresponds to a UCM.
    \item An instantiation of the UCM principle using statistical hypothesis testing, supported on a closed-form estimate of a UCM (which, to the best of our knowledge, is a new result, possibly of independent interest).
\end{itemize}

The paper is organized as follows. Section \ref{sec:UCModel} describes the UCM and presents the corresponding identifiability theorem and equivalence to an SCM. Section \ref{sec:channel} addresses the problem of estimating uniform and cyclic uniform channels from  data. Section \ref{sec:criterion_data} 
describes how the criterion is applied to observed data. Experimental results are reported in Section \ref{sec:resul}, and Section \ref{sec:concl} concludes the paper.
