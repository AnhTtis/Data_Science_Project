\section{Discussion}
% \vspace{0.5ex}\noindent
% \textbf{Limitations and Future Works.}
% Currently, our method uses a constant compression rate $k$ that controls the amount of feature units removed during the compression step.
% However, the number of unreliable units may not be the same for all adversarial examples; the same compression rate may be too low to remove sufficient number of unreliable units for some adversarial examples, while it may be too high for others and undesirably remove reliable cues that should be preserved.
% This could also explain why our method shows marginal performance improvements compared to existing methods that apply suppression techniques.
% As a future work, we plan to adaptively control the compression rate for each adversarial example to solve this problem.
Since the goal of our work is to improve the robustness of adversarial training models against adversarial examples, it lies on the assumption that the input images contain malicious perturbations designed to fool the model.
Thus, we deliberately design the Separation stage to disentangle activations that specifically lead the model to mispredictions.
However, natural images may not contain such malicious cues, and thus, our FSR module occasionally decreases the natural accuracy by a small amount.
%%%%%%%%%%%%%%%%%%%%%%% Submitted version %%%%%%%%%%%%%%%%%%%%%%%
% A potential direction for future work could determine the amount of non-robust activations prior to feature separation adaptively for each input image.
%%%%%%%%%%%%%%%%%%%%%%% Camera-ready %%%%%%%%%%%%%%%%%%%%%%%
A potential direction for future work could be applying curriculum learning~\cite{curriculum} to make FSR be better aware of the relationship between feature robustness and attack strength.


% \vspace{0.5ex}\noindent
% \textbf{Potential Negative Societal Impact.} 
Adversarial attack poses a huge threat to the deployment of deep neural networks (DNN) in real-world applications~\cite{driving, arcface, face}.
In this regard, our work contributes positively to the research field by designing an easy-to-plugin module to robustify the DNN models against adversarial attacks.
Thanks to its simplicity, we expect that our method could also serve as a basis to design more robust DNN models in diverse and real-world applications.


\section{Conclusion}
In this paper, we have proposed the novel Feature Separation and Recalibration (FSR) module that recalibrates the non-robust activations to restore discriminative cues that help the model make correct predictions under adversarial attack.
To achieve this goal, FSR first disentangles the intermediate feature map into the robust activations that capture useful cues for correct model decisions and the non-robust activations that are responsible for incorrect predictions.
It then recalibrates the non-robust activations to capture potentially useful cues that could provide additional guidance for more robust predictions on adversarial examples.
We have empirically demonstrated the ability of our method to improve the robustness of various models when applied to different adversarial training strategies across diverse datasets.
We have also verified the superiority of our method to existing approaches that simply deactivate such non-robust activations.
