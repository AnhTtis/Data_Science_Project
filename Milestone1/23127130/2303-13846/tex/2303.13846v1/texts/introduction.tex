\section{Introduction}
Despite the advancements of deep neural networks (DNNs) in computer vision tasks~\cite{resnet, yolo, vqa, pplr}, they are vulnerable to adversarial examples~\cite{lbfgs, fgsm} that are maliciously crafted to subvert the decisions of these models by adding imperceptible noise to natural images.
Adversarial examples are also known to be successful in real-world cases, including autonomous driving~\cite{driving} and biometrics~\cite{arcface, face}, and to be effective even when target models are unknown to the attacker~\cite{delving, lbfgs, ada}.
Thus, it has become crucial to devise effective defense strategies against this insecurity.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% v1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To this end, many defensive techniques have been proposed, including defensive distillation~\cite{distillation}, input denoising~\cite{hgd}, and attack detection~\cite{robust-detection, squeezing}.
% Among these methods, adversarial training~\cite{fgsm, pgd}, which robustifies a model by training it on a set of worst-case adversarial examples, has been considered to be the most successful and popular approach.
% Even with adversarial training, however, it has been found that models learn non-robust features from the dataset and thus are vulnerable to adversarial attacks~\cite{NotBugsFeatures}.
% Furthermore, small adversarial perturbations on the pixel-level accumulate to a much larger degree in the intermediate feature space and ruin the final output of the model~\cite{fd}.
% To make the model robust against these feature-level perturbations, recent advanced methods have deactivated the highly disrupted activations that cause model mispredictions.
% Xie \etal~\cite{fd} applied classical denoising techniques to deactivate disrupted activations, and Bai \etal~\cite{cas} and Yan \etal~\cite{cifs} deactivated channels that are irrelevant to correct model decisions.


% These approaches, however, inevitably neglect discriminative cues that potentially lie in these non-robust activations.
% Ilyas \etal~\cite{NotBugsFeatures} have shown that there exist non-robust features in the image level that are responsible for model mispredictions upon adversarial attack but provide discriminative information in the natural setting.
% Similarly on the feature level, we argue that the highly disrupted feature units with non-robust activations that are responsible for model mispredictions could have captured discriminative cues for model predictions on natural images.
% With appropriate adjustments, these activations can thus capture useful cues for model predictions, and deactivating them could lead to loss of these potentially discriminative information that can provide the model with better guidance to make correct predictions.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% v1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% v2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To this end, numerous defense techniques have been proposed, including defensive distillation~\cite{distillation}, input denoising~\cite{hgd}, and attack detection~\cite{robust-detection, squeezing}.
Among these methods, adversarial training~\cite{fgsm, pgd}, which robustifies a model by training it on a set of worst-case adversarial examples, has been considered to be the most successful and popular.


Even with adversarial training, however, small adversarial perturbations on the pixel-level accumulate to a much larger degree in the intermediate feature space and ruin the final output of the model~\cite{fd}.
% Additionally, Ilyas \etal~\cite{NotBugsFeatures} \YK{showed}\Skip{have shown} that there exist non-robust features in the input space that are responsible for the success of adversarial examples \YK{by disentangling the robust and non-robust features.}\Skip{and disentangled the robust and non-robust features.}
To solve this problem, recent advanced methods disentangled and deactivated the non-robust feature activations that cause model mispredictions.
Xie \etal~\cite{fd} applied classical denoising techniques to deactivate disrupted activations, and Bai \etal~\cite{cas} and Yan \etal~\cite{cifs} deactivated channels that are irrelevant to correct model decisions.
%%%
These approaches, however, inevitably neglect discriminative cues that potentially lie in these non-robust activations.
% In addition to the findings of the non-robust features on the input space, Ilyas \etal~\cite{NotBugsFeatures} \YK{also showed}\Skip{have also shown} that these features can still provide discriminative information in the natural setting.
Ilyas \etal~\cite{NotBugsFeatures} have shown that a model can learn discriminative information from non-robost features in the input space.
Based on this finding, we argue that there exist potential discriminative cues in the non-robust activations, and deactivating them could lead to loss of these useful information that can provide the model with better guidance for making correct predictions.
%%%%%%%%%%
% \JS{To fully excavate such inherent information, we argue for the first time that the }
%%%%%%%%%%
% Likewise, we argue that the highly disrupted feature units with non-robust activations that are responsible for model mispredictions could have captured discriminative cues for model predictions on natural images.
% \Skip{With appropriate adjustments, these activations can thus capture useful cues for model predictions,} and deactivating them could lead to loss of these potentially discriminative information that can provide the model with better guidance for making correct predictions.
% \JS{Based on these findings, we argue for the first time that the non-robust activations in the feature space could recapture discriminative cues via appropriate adjustment.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% v2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For the first time, we argue that with appropriate adjustment, the non-robust activations that lead to model mispredictions could recapture discriminative cues for correct model decisions.
To this end, we propose a novel Feature Separation and Recalibration (FSR) module that aims to improve the \textit{feature robustness}.
We first separate the intermediate feature map of a model into the malicious \textit{non-robust} activations that are responsible for model mispredictions and the \textit{robust} activations that still provide useful cues for correct model predictions even under adversarial attacks.
Exploiting only the robust feature just like the existing methods~\cite{fd, cas, cifs}, however, could lead to loss of potentially useful cues in the non-robust feature.
Thus, we recalibrate the non-robust activations to capture cues that provide additional useful guidance for correct model decisions. 
These additional cues can better guide the model to make correct predictions and thus boost its robustness.


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/fig_gradcam5.pdf}
    \caption{ 
        Visualization of attention maps~\cite{gradcam} on the features of natural images on a naturally trained model ($f_{nat}$) and the robust ($f^{+}$), non-robust ($f^{-}$), and recalibrated features ($\tilde{f}^{-}$) of adversarial examples ($x'$) obtained from an adversarial training~\cite{pgd} model equipped with our FSR module on CIFAR-10 dataset.
        The robust feature captures discriminative cues regarding the ground truth class, while the non-robust feature captures irrelevant cues.
        To further boost feature robustness, we recalibrate the non-robust feature ($f^{-} \rightarrow \tilde{f}^{-}$) and capture additional useful cues for model predictions.
        Adversarial images $x'$ are upscaled for clearer visual representations.
    }
    \label{fig:intro-gradcam}
    \vspace{-3mm}
\end{figure}


% \sout{For the first time, we argue that with appropriate adjustment, the non-robust activations that lead to model mispredictions could recapture discriminative cues for correct model decisions.}
% To this end, we propose a novel Feature Separation and Recalibration (FSR) module that aims to improve the \textit{feature robustness}.
% We first separate the intermediate feature map of a model into the malicious \textit{non-robust} activations that are responsible for model mispredictions and the \textit{robust} activations that still provide useful cues for correct model predictions even under adversarial attack.
% Exploiting only the robust feature just like the existing methods~\cite{fd, cas, cifs}, however, could lead to loss of potentially useful cues in the non-robust feature.
% Thus, we recalibrate the non-robust activations to capture cues that provide additional useful guidance for correct model decisions. 
% These additional cues will better guide the model to make correct predictions and thus boost its robustness.


Fig.~\ref{fig:intro-gradcam} visualizes the attention maps~\cite{gradcam} on the features of natural images by a naturally trained model ($f_{nat}$) and the robust ($f^{+}$), non-robust ($f^{-}$), and recalibrated features ($\tilde{f}^{-}$) on adversarial examples ($x'$) obtained from an adversarial training~\cite{pgd} model equipped with our FSR module. 
Given an adversarial example, while the non-robust feature ($f^{-}$) captures cues irrelevant to the ground truth class, the robust feature ($f^{+}$) captures discriminative cues (\eg, horse's leg). 
% In order to capture useful cues from the distorted non-robust feature, which is otherwise neglected by the existing methods, our FSR module recalibrates the non-robust activations and restores additional useful cues not captured by the robust activations ($f^{-} \rightarrow \tilde{f}^{-}$).
Our FSR module recalibrates the non-robust activations ($f^{-} \rightarrow \tilde{f}^{-}$), which are otherwise neglected by the existing methods, and restores additional useful cues not captured by the robust activations (\eg, horse's body).
With these additional cues, FSR further boosts the model's ability to make correct decisions on adversarial examples.


Thanks to its simplicity, our FSR module can be easily plugged into any layer of a CNN model and is trained with the entire model in an end-to-end manner.
We extensively evaluate the robustness of our FSR module on benchmark datasets against various white-box and black-box attacks and demonstrate that our approach improves the robustness of different variants of adversarial training (Sec.~\ref{sec:exp-robustness}) with small computational overhead (Sec.~\ref{sec:exp-comp}).
We also show that our approach of recalibrating non-robust activations is superior to existing techniques~\cite{cas, fd, cifs} that simply deactivate them (Sec.~\ref{sec:exp-robustness}).
Finally, through ablation studies, we demonstrate that our Separation stage can effectively disentangle feature activations based on their effects on model decision and that our Recalibration stage successfully recaptures useful cues for model predictions (Sec.~\ref{sec:exp-ablation}).


In summary, our contributions are as follow:
\begin{itemize}
    \item In contrast to recent methods that deactivate distorted feature activations, we present a novel point of view that these activations can be recalibrated to capture useful cues for correct model decisions. 
    % we claim that these activations can be recalibrated to recapture useful cues for correct model decisions (see Fig.~\ref{fig:intro-gradcam}). 

    \item We introduce an easy-to-plugin \textit{Feature Separation and Recalibration (FSR)} module, which separates non-robust activations from feature maps and recalibrates these feature units for additional useful cues.
    % to instead capture useful cues\Skip{for model predictions}. \Skip{Our method is highly modularized and can thus be easily plugged into any layer of a model with small computational overhead.}

    % \item Experimental results demonstrate the effectiveness of our FSR module on various white- and black-box attacks with small computational overhead. 
    \item Experimental results demonstrate the effectiveness of our FSR module on various white- and black-box attacks with small computational overhead and verify our motivation that recalibration restores discriminative cues in non-robust activations.
    
    \Skip{and verify our novel point of view.}
    \Skip{Extensive experiments show the superiority of our FSR module against existing feature deactivation methods and verify its effectiveness on improving the robustness of various adversarial training techniques under both white-box and black-box attacks.} 
    
\end{itemize}