\section{Methods}
\label{sec:methods}
While the distorted feature activations upon adversarial attack are known to be responsible for model mispredictions, we argue that with adjustment, we can recapture useful cues for model predictions.
To fully utilize such potentially useful cues, we present a novel Feature Separation and Recalibration (FSR) module (Fig.~\ref{fig:overview}) that can restore these cues through a separation-and-recalibration scheme.
During the Separation stage, we disentangle the feature map into the \textit{robust} and \textit{non-robust} features by masking out the non-robust and robust activations, respectively.
Then, during the Recalibration stage, we recalibrate the activations of the non-robust features such that they provide useful information for correct model prediction.
Thanks to its simplicity, as shown in Fig.~\ref{fig:overview}, FSR module can be inserted to any layer of a model to improve its robustness.
We elaborate on the details of our FSR module in the following subsections.


\subsection{Feature Separation}
\label{ssec:3.1}
% \textcolor{red}{
% During the Separation stage, we learn to disentangle the input feature into the robust feature consisting of the robust activations and the non-robust feature consisting of the non-robust activations.
% To do so, we introduce the Separation Network, which learns the robustness of each individual activation of the input intermediate feature map. 
% Talk more about how Robustness Network and its output reliability map works.
% }
% During the Separation stage, we disentangle the input intermediate feature map into the robust feature and the non-robust feature by masking out the non-robust and the robust activations, respectively.
As compared to recent approaches \cite{fd, cas, cifs} that robustify feature maps by deactivating non-robust activations, we aim to recalibrate them to restore potentially useful cues for model predictions.
To this end, during the Separation stage, we extract the non-robust activations to be recalibrated from the input intermediate feature map. 
% into the robust and non-robust features by masking out the non-robust and robust activations, respectively.}
To determine which activation is robust or non-robust, we introduce the Separation Net $S$, which learns the robustness of each feature unit.
Given the intermediate feature map $f \in \mathbb{R}^{C \times H \times W}$ as input, where $C$, $H$, and $W$ represent the channel, height, and width dimensions of $f$, respectively, the Separation Net outputs a robustness map $r \in \mathbb{R}^{C \times H \times W}$ that represents a robustness scores of the corresponding units of $f$, where a higher score means a more robust feature activation.
% \Skip{\textcolor{blue}{. In other words, it captures a higher robustness score for a more robust feature activation and a lower robustness score for a less robust activation.}}


% \textcolor{red}{
% Given the output reliability map, we need to generate a mask that is applied on the feature map to disentangle the non-robust activations from the robust activations.
% However, discrete sampling on the feature map could cause discontinuity [Gumbel, Soft Top-k], which could result in gradient masking [obfuscated gradients, adaptive attack - athalye].
% Thus, we employ Gumbel softmax to generate a differentiable soft mask.
% Talk about how we formulate Gumbel softmax to obtain two features.
% }
In order to extract the non-robust feature, we disentangle the feature map into the robust feature and the non-robust feature in an element-wise manner based on the robustness score.
One way to achieve this goal is to apply to the feature map a binary mask $b\in \{0, 1\}^{C \times H \times W}$ generated based on the robustness score.
However, such discrete sampling is non-differentiable~\cite{gumbel} and discontinuous~\cite{soft-topk}, which could cause gradient masking that would give a false sense of robustness~\cite{practical, obfuscated}.
% \YK{[Why is gradient masking a problem? Short explain.]}


To avoid such problem, we approximate a binary mask $b$ with a differentiable soft mask $m \in [0, 1]^{C \times H \times W}$ using Gumbel softmax~\cite{gumbel} such that:
\begin{equation}
    % \begin{aligned}
        m = \frac{e^{((\log{(\sigma(r))} + g_1) / \tau)}}
        {e^{((\log{(\sigma(r))} + g_1) / \tau)} + e^{((\log{(1 - \sigma(r))} + g_2) / \tau)}},
    % \end{aligned}
    \label{eq:mask}
\end{equation}
where $r$ is the robustness map, and $\sigma$ is a sigmoid function used to normalize the robustness map.
$g_1$ and $g_2$ represent the samples drawn from the Gumbel distribution such that $g = -\log(-\log(u))$, where $u \sim \text{Uniform}(0, 1)$, and $\tau$ is a temperature used to control the effects of $g_1$ and $g_2$.
Note that during inference, to avoid stochasticity from sampling $g_1$ and $g_2$, we fix them as $-\log(-\log(u_c))$, where $u_c \in \mathbb{R}^{C \times H \times W}$ is the expected value of Uniform distribution.
% where $u_c = [0.5]^{c \times h \times w}$ is the expected value of Uniform distribution.
By computing two-class Gumbel softmax between the normalized robustness map and its inverted version, we obtain a mask $m$ with values close to 1 for high robustness scores and values close to 0 for low robustness scores.
% The higher the robustness score of a feature unit is (\ie, high magnitude in $r$), the more likely it will be assigned a value close to 1 in the resulting mask.
Then, to mask out the non-robust activations from the input feature $f$ and obtain the robust feature $f^{+}$, we compute element-wise product between the feature $f$ and the positive mask $m^{+} = m$ such that $f^{+} = m^{+} \otimes f$.
% Thus, we can mask out the non-robust activations from the input feature $f$ and obtain the robust feature $F^{+}$ by computing element-wise product between the mask $m$ and feature $f$ such that $F^{+} = m \otimes f$.
\Skip{Similarly, we can mask out the robust activations and obtain the non-robust feature $f^{-}$ by computing the element-wise product between the feature $f$ and the negative mask $m^{-} = 1 - m$ such that $f^{-} = m^{-} \otimes f$.}
Similarly, we obtain non-robust feature $f^{-} = m^{-} \otimes f$ by masking out the robust activations with the negative mask $m^{-} = 1 - m$.

% As a result, units with high robustness scores are assigned a mask value close to 1, and units with low scores are assigned a mask value close to 0.
% % To remove stochasticity during inference, we set $g_1$ and $g_2$ to 
% We then compute element-wise product between the mask $m$ and the input feature $f$ to obtain the robust feature $F^{+}$, where we mask out the non-robust activations with low robustness scores, and the element-wise product between $1 - m$ and $f$ to obtain the non-robust feature $F^{-}$, where we mask out the robust activations with high robustness scores.


% \textcolor{red}{
% To ensure that the two robust and non-robust features are well disentangled (i.e., the Separation Network correctly learns the Separation map), we design two objectives that determine how ``robust" each feature is.
% To do so, we attach an MLP-based auxiliary classifier after the two features.
% Talk about the design of auxiliary layer and auxiliary losses.
% }
% To \textcolor{blue}{ensure that the robust and non-robust features are well disentangled}, we design two objectives that guide the Separation Net to learn correct robustness scores.
% \textcolor{red}{To ensure that we disentangle the robust and non-robust featues based on their influence on model predictions, we design an objective that guide the Separation Net to learn correct robustness score for each feature activation.}
Without any guidance, however, the Separation Net may not learn the correct robustness score for each activation.
To this end, we design an objective that guides the Separation Net to learn robustness scores specifically based on the influence of feature activations on model making a correct or incorrect prediction.
We attach an MLP-based auxiliary layer $h$ that takes each of the two feature $f^{+}$ and $f^{-}$ as inputs and outputs prediction scores $p^{+}$ and $p^{-}$, respectively.
Then, we compute the separation loss $\mathcal{L}_{sep}$ as follows:
\begin{equation}
    \mathcal{L}_{sep} = - \sum^{N}_{i=1} (y_i \cdot \log(p^{+}_i) + y'_i \cdot \log(p^{-}_i)),
    \label{eq:l_sep}
\end{equation}
where $N$ is the number of classes, $y$ is the ground truth label, and $y'$ is the label corresponding to the wrong class with the highest prediction score from the final model output.
By training the auxiliary layer to make correct predictions based on the activations that are preserved from the positive mask $m^{+}$, $\mathcal{L}_{sep}$ guides the Separation Net to assign high robustness scores to units that help the auxiliary layer make correct predictions.
% We also compute the non-robustness loss $\mathcal{L}_{nr}$ as follows:
% \begin{equation}
%     \mathcal{L}_{nr} = - \sum^{N}_{i=1} y'_i \cdot \log(p^{-}_i),
%     \label{eq:l_nr}
% \end{equation}
% where $y'$ is the label corresponding to the wrong class with the highest prediction score from the final model output.
At the same time, we aim to disentangle the highly disrupted activations that cause the model to make mispredictions upon adversarial attack.
% By training the auxiliary layer to make misprediction based on the activations that are masked out, 
$\mathcal{L}_{sep}$ also guides the Separation Net to assign low robustness scores to non-robust activations that are specifically responsible for the most probable misprediction.
% At the same time, by making the most probable misprediction, $\mathcal{L}_{sep}$ also guides the Separation Net to assign low robustness scores to activations selected by the negative mask $m^{-}$.
% \YK{[Discussion and brief introduction for the following subsection.] With $\mathcal{L}_{sep}$, Seperation Net can effectively disentangle the robust feature which provide..., however, it is not sufficient that ... In the following subsection, we describe how to capture ...}


With $\mathcal{L}_{sep}$, the Separation Net can effectively separate the robust and the non-robust feature activations.
Discarding the non-robust activations is one way to improve feature robustness; however, this approach would ignore potentially useful cues that can be recaptured through recalibration (see Sec.~\ref{sec:exp-ablation}).
In the following subsection, we discuss how we recalibrate the disentangled non-robust activations to capture additional useful cues for improved feature robustness.
% The disentangled non-robust feature is then passed into the Recalibration stage, which is discussed in the following subsection.


\subsection{Feature Recalibration}
% \textcolor{red}{
% During the Recalibration stage, we adjust the non-robust feature activations to make them capture useful cues for correct predictions.
% To this end, we introduce the Recalibration Network.
% Talk about how we design the Recalibration Network including its inputs \& outputs.
% }
Exploiting only the robust feature obtained through the Separation stage just like recent techniques~\cite{fd, cas, cifs} could lead to loss of potentially useful cues for model predictions that could further boost model robustness.
% These approaches, however, inevitably ignore the potential discriminative cues that can guide the model to make correct predictions.
Therefore, for the first time, we adjust the non-robust feature activations to capture the additional useful cues during the Recalibration stage.
We first introduce the Recalibration Net $R$ that takes the non-robust feature $f^{-}$ as input and outputs recalibrating units that are designed to adjust the activations of $f^{-}$ accordingly.
To recalibrate the non-robust activations designated by the robustness map, we apply the negative mask $m^{-}$ to the recalibrating units.
% to extract only the units corresponding to the non-robust activations of $F^{-}$.
Finally, we compute the recalibrated feature $\tilde{f}^{-}$ by adding the result to $f^{-}$, \ie, 
$\tilde{f}^{-} = f^{-} + m^{-} \otimes R(f^{-})$.


% \textcolor{red}{
% To train the Recalibration Network, we attach the same auxiliary layer after the recalibrated feature and compute the Recalibration loss.
% Talk about how we design the Recalibration loss and its goal.
% }
The goal of the Recalibration stage is to make the non-robust activations recapture cues that can help the model make correct decisions.
To guide the Recalibration Net to achieve this goal, we again attach the auxiliary layer $h$ after the recalibrated feature $\tilde{f}^{-}$ and compute the recalibration loss $\mathcal{L}_{rec}$ as follows:
\begin{equation}
    \mathcal{L}_{rec} = - \sum^{N}_{i=1} y_i \cdot \log(\tilde{p}^{-}_i),
    \label{eq:l_rec}
\end{equation}
where $\tilde{p}^{-}$ is the output prediction score of the auxiliary layer given $\tilde{f}^{-}$ as input.
By training the same auxiliary layer to make correct decisions based on the recalibrated feature, we guide the Recalibration Net to adjust the non-robust activations such that they provide cues relevant to the ground truth class.
After the Recalibration stage, we add the robust feature $f^{+}$ and the recalibrated non-robust feature $\tilde{f}^{-}$ in an element-wise manner to obtain the output feature map $\tilde{f} = f^{+} + \tilde{f}^{-}$, which is passed to subsequent layers of the model.
% \YK{[Discuss the benefits of FSR module (or recalibration stage).] ThNoverrough the recalibration stage, we can additionally capture useful cues in non-robust features, which is neglected in previous methods...}
Through the Recalibration stage, we can capture additional useful cues from the non-robust activations, which are neglected in previous approaches.


\begin{table*}[t]
	\begin{center}
	    \resizebox{0.95\linewidth}{!}
		{\begin{tabular}{c|c c c c c c|c c c c c c}
        \hline
        \textbf{\textit{ResNet-18}} & \multicolumn{6}{c|}{CIFAR-10} & \multicolumn{6}{c}{SVHN} \\ \hline 
        % \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{6}{c|}{CIFAR-10} & \multicolumn{6}{c}{SVHN} \\ \cline{2-13}
        Method & Natural & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble & Natural & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble \\
        \hline \hline
        {AT}            & \textbf{85.02} & 56.21 & 48.22 & 46.37 & 47.38 & 45.51         
                        & 91.21 & 55.55 & 40.85 & 37.54 & 40.61 & 37.41 \\
        {AT + FSR}      & 81.46 & \textbf{58.07} & \textbf{52.47} & \textbf{51.02} & \textbf{49.44} & \textbf{48.34}    
                        & \textbf{91.28} & \textbf{60.46} & \textbf{43.94} & \textbf{39.01} & \textbf{43.22} & \textbf{38.81} \\
        \hline
        {TRADES}        & \textbf{86.31} & 57.21 & 50.74 & 49.44 & 48.66 & 47.89         
                        & 90.99 & 61.31 & 47.12 & 43.55 & 45.48 & 42.99 \\
        {TRADES + FSR}  & 84.49 & \textbf{58.29} & \textbf{52.27} & \textbf{51.28} & \textbf{49.92} & \textbf{49.28}         
                        & \textbf{91.39} & \textbf{68.85} & \textbf{51.49} & \textbf{47.50} & \textbf{46.70} & \textbf{46.17} \\
        \hline
        {MART}          & 82.73 & 56.65 & 50.88 & 49.15 & 47.21 & 45.98         
                        & \textbf{90.50} & 58.21 & 43.61 & 40.43 & 42.20 & 40.07 \\
        {MART + FSR}    & \textbf{83.28} & \textbf{59.55} & \textbf{54.80} & \textbf{53.69} & \textbf{48.98} & \textbf{48.36}
                        & 89.87 & \textbf{61.06} & \textbf{46.51} & \textbf{42.94} & \textbf{43.89} & \textbf{42.40} \\
                        
        \hline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Robustness (accuracy (\%)) of adversarial training strategies (AT, TRADES, MART) with (+ FSR) and without our FSR module against diverse white-box attacks on ResNet-18. Better results are marked in \textbf{bold}.
	}
	\label{table:robustness-resnet}
\end{table*}


\begin{table*}[t]
	\begin{center}
	    \resizebox{0.95\linewidth}{!}
		{\begin{tabular}{c|c c c c c c|c c c c c c}
		% \thickhline
        \hline
         \textbf{\textit{VGG16}} & \multicolumn{6}{c|}{CIFAR-10} & \multicolumn{6}{c}{SVHN} \\ \hline
        % \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{6}{c|}{CIFAR-10} & \multicolumn{6}{c}{SVHN} \\ \cline{2-13}
        Method & Natural & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble & Natural & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble \\
        \hline
        \hline
        AT              & \textbf{80.56} & 53.47 & 47.17 & 45.58 & 45.82 & 43.71         
                        & 89.59 & 54.88 & 40.27 & 36.90 & 39.46 & 36.62 \\
        AT + FSR        & 80.06 & \textbf{54.40} & \textbf{49.82} & \textbf{48.82} & \textbf{47.28} & \textbf{46.24}         
                        & \textbf{91.44} & \textbf{65.01} & \textbf{45.99} & \textbf{39.07} & \textbf{43.08} & \textbf{38.15} \\
        \hline
        TRADES          & \textbf{82.44} & 53.92 & 47.39 & 46.20 & 44.80 & 44.20         
                        & 90.48 & 61.50 & 45.99 & 40.00 & 42.82 & 39.27 \\
        TRADES + FSR    & 80.78 & \textbf{55.48} & \textbf{49.95} & \textbf{49.03} & \textbf{46.28} & \textbf{45.90}         
                        & \textbf{91.89} & \textbf{69.25} & \textbf{54.56} & \textbf{47.81} & \textbf{46.66} & \textbf{44.10} \\
        \hline
        MART            & 76.11 & 54.86 & 51.06 & 50.16 & 43.53 & 43.01     
                        & 89.95 & 59.03 & 42.89 & 38.73 & 39.12 & 37.64 \\
        MART + FSR      & \textbf{79.18} & \textbf{56.41} & \textbf{52.69} & \textbf{52.13} & \textbf{44.49} & \textbf{44.20}         
                        & \textbf{90.60} & \textbf{62.28} & \textbf{47.17} & \textbf{42.50} & \textbf{43.44} & \textbf{40.73} \\
        % \thickhline
        \hline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Robustness (accuracy (\%)) of adversarial training strategies (AT, TRADES, MART) with (+ FSR) and without our FSR module against diverse white-box attacks on VGG16. Better results are marked in \textbf{bold}.
	}
        \vspace{-3mm}
	\label{table:robustness-vgg}
\end{table*}


\subsection{Model Training}
The proposed FSR module can be easily inserted to any layer of a model and is trained with the entire model in an end-to-end manner thanks to its simplicity.
We can also apply the proposed method with any classification loss $\mathcal{L}_{cls}$ for different types of adversarial training~\cite{pgd, trades, mart}, and the overall objective function is as follows:
% \Skip{Given the following classification loss $\mathcal{L}_{cls}$ from Eq.~\ref{eq:1} of the vanilla adversarial training framework~\cite{pgd}, we develop the following \YK{objective function}\Skip{joint loss} used to train the entire model:}
\begin{equation}
    \mathcal{L} = \mathcal{L}_{cls} + 
    \frac{1}{\vert L \vert}\sum_{l \in L} \left( {\lambda_{sep}} \cdot \mathcal{L}^l_{sep} + 
                          {\lambda_{rec}} \cdot \mathcal{L}^l_{rec} \right),
    % \lambda_{sep} \cdot \mathcal{L}_{sep} +
    % \lambda_{rec} \cdot \mathcal{L}_{rec}.
\end{equation}
where $L$ represents the set of positions in which the FSR module is inserted, and $\mathcal{L}^l_{sep}$ and $\mathcal{L}^l_{rec}$ each represents the separation loss and the recalibration loss applied on the FSR module at $l$-th layer.
The hyperparameters $\lambda_{sep}$ and $\lambda_{rec}$ are used to control the weights of $\mathcal{L}_{sep}$ and $\mathcal{L}_{rec}$, respectively.
Addition of this simple FSR module can improve the robustness of adversarial training methods against both white-box and black-box attacks with small computational overhead, as described in the following section. % (Sec.~\ref{sec:exp}).
