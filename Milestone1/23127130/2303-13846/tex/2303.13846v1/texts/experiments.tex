\section{Experiments}
\label{sec:exp}

\subsection{Experimental Setups}
\vspace{0.5ex}\noindent
\textbf{Evaluation Protocols.}
We evaluate our method on CIFAR-10/100~\cite{cifar10}, SVHN~\cite{svhn}, and Tiny ImageNet~\cite{learnable} datasets using ResNet-18~\cite{resnet}, VGG16~\cite{vgg}, and WideResNet-34-10~\cite{wideresnet} as the baseline models.
% following the common protocol~\cite{cas, cifs}. 
% In order to show that our method is applicable to various adversarial training defense techniques, 
We apply our method to PGD adversarial training (AT)~\cite{pgd} and other variants of adversarial training, \ie, TRADES~\cite{trades} and MART~\cite{mart}, to verify its wide applicability.
% For fair comparisons, during training and evaluation, we craft adversarial examples \textit{adaptively} with our defense method known to the attacker by performing attacks on the joint loss of our proposed objectives (Eq.~\ref{eq:l_total}).
For training, we use PGD-10~\cite{pgd} with perturbation bound $\epsilon$ = 8/255 (step size $\epsilon$/4 for CIFAR-10/100 and Tiny ImageNet, $\epsilon$/8 for SVHN) under $\ell_\infty$-norm to craft adversarial examples.
For evaluation, we use FGSM~\cite{fgsm}, PGD-20~\cite{pgd} (step size $\epsilon$/10), PGD-100~\cite{pgd} (step size $\epsilon$/10), and C\&W~\cite{c&w} (PGD optimization for 30 steps with step size $\epsilon$/10) bounded within $\epsilon$ = 8/255 under $\ell_\infty$-norm.
As suggested by Carlini \etal~\cite{evaluating}, to better compare the robustness of different defense techniques, we also report the average per-example Ensemble robustness of the model as formulated below~\cite{adt}:
\begin{equation}
    % \text{Ensemble} = \frac{1}{N_{test}}\sum^{N_{test}}_{i=1}\min_{x'_i \in \mathcal{X}'_i} \mathbbm{1}(F_\theta(x'_i) = y_i),
    % \text{Ensemble} = \frac{1}{\vert \mathcal{D}_{test} \vert}\sum_{(x, y) \sim \mathcal{D}_{test}}\min_{x' \in \mathcal{X}'} \mathbbm{1}(F_\theta(x') = y),
    \text{Ensemble} = \frac{1}{N_{test}}\sum^{N_{test}}_{i=1}\min_{a \in \mathcal{A}} \mathbbm{1}(F_\theta(a(x)) = y),
\end{equation}
% where $N_{test}$ is the number of images in the test dataset, $\mathbbm{1}(\cdot)$ is an indicating function, $f$ is a target model with parameter $\theta$, $y_i$ is a ground truth label, and $\mathcal{X}'_i$ is a set of adversarial examples crafted on a natural image $x_i$ using different adversarial attacks (FGSM, PGD-20, PGD-100, and C\&W).
where $N_{test}$ is the number of images in the test dataset, $\mathbbm{1}(\cdot)$ is an indicating function, $F_\theta$ is a target model with parameter $\theta$, $y$ is the ground truth label, and $\mathcal{A}$ is a set of adversarial attacks (FGSM, PGD-20, PGD-100, and C\&W).


\vspace{0.5ex}\noindent
\textbf{Implementation Details.}
We train all models for 100 epochs using an SGD optimizer (momentum 0.9, weight decay $5\times10^{-4}$).
We set the initial learning rate to 0.1 for CIFAR-10/100 and Tiny ImageNet and to 0.01 for SVHN, and reduce it by a factor of 10 after 75-th and 90-th epochs.
We empirically set $\lambda_{sep} = 1$, $\lambda_{rec} = 1$, and $\tau$ for Gumbel softmax as $0.1$.
We implement the Separation Net as a series of three blocks each comprised of a convolutional layer, a batch normalization layer, and a ReLU activation, except for the last block which consists of a single convolutional layer.
The Recalibration Net also consists of a series of three blocks each comprised of a convolutional layer, a batch normalization layer, and a ReLU activation.
We insert our FSR module after \texttt{block4} for ResNet-18, \texttt{block4} for VGG16, and \texttt{block3} for WideResNet-34-10.


\subsection{Robustness Evaluation}
\label{sec:exp-robustness}
% In this section, we evaluate the robustness of our FSR module under various white-box and black-box attacks.


% \textcolor{red}{
% Application to AT. 
% We first compare various adversarial training techniques and the applications of our FCD module to these defense strategies.
% Make Table 1 (AT, TRADES, and MART on CIFAR10 and SVHN on resnet18). 
% Table 2 could be in supplementary.
% Explain about what Ensemble is. Refer to the paper that lists out the guideline for testing defense.
% Mention how we improve the robustness against various attacks.
% Mention why we occasionally face drop in natural accuracy.
% }
\vspace{0.5ex}\noindent
\textbf{Defense against White-box Attacks.}
To evaluate the ability of our FSR module to improve the model robustness of various adversarial training techniques, we report in Table~\ref{table:robustness-resnet} the effectiveness of applying FSR to three different methods (AT, TRADES, and MART) on ResNet-18.
Applying our FSR module consistently improves the robustness of all defense techniques under all of the individual attacks and the Ensemble attack.
Similar trends are observed in the SVHN dataset (Table~\ref{table:robustness-resnet}) and on VGG16 (Table~\ref{table:robustness-vgg}); for example, FSR improves the robustness of TRADES on VGG16 under PGD-20 by 8.57\% on the SVHN dataset.
By recalibrating the malicious non-robust activations, our method provides the model with additional useful cues for improved robustness.
With only a slight increase in the number of computations (see Sec.~\ref{sec:exp-comp}), we can improve the robustness of various adversarial training methods regardless of the dataset and the model.
Results on WideResNet-34-10, CIFAR-100, Tiny ImageNet are provided in the supplementary materials.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% submission for review %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \YK{[I think we can discuss it in the rebuttal period if necessary, or in the limitation section.]}
% One observation is that our FSR module occasionally drops the accuracy on natural images.
% This is because our method is designed to disentangle malicious, non-robust activations that specifically cause a model to make mispredictions.
% However, there may not exist such malicious cues in natural images, and trying to separate and recalibrate these ``non-robust" activations from the corresponding feature maps may lead to loss of discriminative information.
% Nevertheless, the natural accuracy drops only by a small amount or even improves upon the addition of FSR module (\eg, on SVHN), showing that this phenomenon is also dependent on the dataset.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% camera-ready %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
One observation is that our FSR module occasionally drops the accuracy on natural images.
This is because our method is designed to disentangle and recalibrate the ``malicious" activations that are \textit{intentionally} crafted to fool model predictions.
In natural images, since there are no intentionally disrupted feature activations, trying to identify and recalibrate these malicious cues can lead to potential loss of discriminative information and occasional accuracy drop.
Nevertheless, the natural accuracy drops only by a small amount or even improves upon the addition of FSR module (\eg, on SVHN), showing that this phenomenon is also dependent on the dataset.


\begin{table}[t]
	\begin{center}
	    \resizebox{\linewidth}{!}
		{\begin{tabular}{c|c c c c}
		% \thickhline
        \hline
        Method & TI-FGSM & DI-FGSM & $\mathcal{N}$Attack & AutoAttack \\
        \hline
        \hline
        AT              & 59.03 & 46.56 & 39.55 & 44.11 \\
        AT + FSR        & \textbf{62.53} & \textbf{50.50} & \textbf{52.65} & \textbf{46.41} \\
        \hline
        TRADES          & 59.95 & 49.23 & 43.45 & 46.81 \\
        TRADES + FSR    & \textbf{61.45} & \textbf{50.54} & \textbf{50.43} & \textbf{48.45} \\
        \hline
        MART            & 59.73 & 48.38 & 44.68 & 44.27 \\
        MART + FSR      & \textbf{62.68} & \textbf{51.42} & \textbf{53.76} & \textbf{46.55} \\
        % \thickhline
        \hline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    % Robustness (accuracy (\%)) of adversarial training strategies (AT, TRADES, MART) on ResNet-18 with (+ FSR) and without our FSR module against diverse black-box attacks and AutoAttack on CIFAR-10. Better results are marked in \textbf{bold}.
     Robustness (accuracy (\%)) of adversarial training strategies (AT, TRADES, MART) with (+ FSR) and without our FSR module against diverse black-box attacks and AutoAttack on CIFAR-10 using ResNet-18. Better results are marked in \textbf{bold}.
	}
	\label{table:black-box}
\end{table}

% \textcolor{red}{
% Defense against Black-box Attacks and AutoAttack. 
% Table 2. We show that our method can also improve robustness against black-box attacks and AutoAttack.
% For black-box attacks, we use two transfer-based attacks that use naturally trained ResNet18 as surrogate model and NAttack, which is a strong query-based attack.
% Explain about AutoAttack -- ensemble of four attacks.
% }
\vspace{0.5ex}\noindent
\textbf{Defense against Black-box Attacks and AutoAttack.}
To show that FSR improves adversarial training methods even under different types of attacks, we evaluate our method against a variety of black-box attacks and AutoAttack~\cite{autoattack}.
For black-box attacks, we use two transfer-based attacks -- TI-FGSM~\cite{tifgsm} and DI-FGSM~\cite{difgsm} -- crafted on a naturally trained ResNet-50, and $\mathcal{N}$Attack~\cite{nattack}, which is a strong query-based attack.
Following CAS~\cite{cas}, to evaluate each method against $\mathcal{N}$Attack, we sample 1,000 images from CIFAR-10 test set and limit the number of queries to 40,000.
% \YK{[Is it common?]}
AutoAttack, which is an ensemble of two Auto-PGD attacks~\cite{autoattack}, Fast Adaptive Boundary attack~\cite{fab}, Square attack~\cite{square}, has been shown to evaluate the robustness of defense techniques more reliably.
As shown in Table~\ref{table:black-box}, our method improves the robustness against diverse black-box attacks, especially against the stronger $\mathcal{N}$Attack.
Our method also improves the robustness against AutoAttack, showing that the improvement upon the addition of FSR module is truly thanks to making the model robust instead of obfuscated gradients~\cite{obfuscated} or improper evaluation.


% \begin{table*}
% 	\begin{center}
% 	    \resizebox{0.6\textwidth}{!}
% 		{\begin{tabular}{c|c c c c c c c}
%         % \thickhline
%         \hline
%         Method & No attack & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble & AutoAttack \\
%         \hline
%         \hline
%         AT      & 85.02 & 56.21 & 48.22 & 46.37 & 47.38 & 45.51 & 44.11 \\
%         \hline
%         FD      & 85.14 & 56.81 & 48.54 & 46.70 & 47.72 & 45.82 & 44.57 \\
%         CAS     & \textbf{85.78} & 55.57 & 50.42 & 49.91 & \textbf{53.47} & 46.46 & 44.23 \\
%         CIFS    & 79.87 & 56.53 & 49.80 & 48.17 & 49.89 & 47.26 & 43.94 \\
%         FSR (Ours)    & 81.46 & \textbf{58.07} & \textbf{52.47} & \textbf{51.02} & 49.44 & \textbf{48.34} & \textbf{46.41} \\
%         % \thickhline
%         \hline
%         \end{tabular}}
% 	\end{center}
% 	\caption{
% 	    Comparison of robustness (accuracy (\%)) between existing methods and our method. 
% 	    All models are trained using AT with ResNet-18 on CIFAR-10 dataset.
% 	}
% 	\label{table:existing-methods}
% \end{table*}

% \begin{table*}[]
% \begin{center}
% \resizebox{0.8\textwidth}{!}
% {
% \begin{tabular}{c|c|cccc:c|c}
% \hline
% Method     & No attack & FGSM           & PGD-20         & PGD-100        & C\&W  & \textbf{Ensemble} & \textbf{AutoAttack} \\ \hline \hline
% AT   & 85.02          & 56.21 & 48.22 & 46.37 & 47.38          & 45.51 & 44.11 \\ \hline
% FD   & 85.14          & 56.81 & 48.54 & 46.70 & 47.72          & 45.82 & 44.57 \\
% CAS  & \textbf{85.78} & 55.57 & 50.42 & 49.91 & \textbf{53.47} & 46.46 & 44.23 \\
% CIFS & 79.87          & 56.53 & 49.80 & 48.17 & 49.89          & 47.26 & 43.94 \\
% FSR (Ours) & 81.46     & \textbf{58.07} & \textbf{52.47} & \textbf{51.02} & 49.44 & \textbf{\hl{48.34}}    & \textbf{\hl{46.41}}      \\ \hline
% \end{tabular}
% }
% \end{center}
% \caption{Comparison of robustness (accuracy (\%)) between existing methods and our method. All models are trained using AT with ResNet-18 on CIFAR-10 dataset. 
% The best results are marked in \textbf{bold} and also highlighted if evaluated under more comprehensive attacks (Ensemble and AutoAttack).
% }
% \label{table:existing-methods}
% \end{table*}

\begin{table*}
\begin{center}
\resizebox{0.65\textwidth}{!}
{
\begin{tabular}{c|c|cccc|c|c}
\hline
% \multicolumn{2}{c|}{Method} & Natural & FGSM           & PGD-20         & PGD-100        & C\&W  & \cellcolor{mygray}Ensemble & \cellcolor{mygray}AutoAttack \\ \hline \hline
Method & Natural & FGSM           & PGD-20         & PGD-100        & C\&W  & \cellcolor{mygray}Ensemble & \cellcolor{mygray}AutoAttack \\ \hline \hline
% Method     & & Natural & FGSM           & PGD-20         & PGD-100        & C\&W  & \cellcolor{mygray}Ensemble & \cellcolor{mygray}AutoAttack \\ \hline \hline
AT   & 85.02          & 56.21 & 48.22 & 46.37 & 47.38          & \cellcolor{mygray}45.51 & \cellcolor{mygray}44.11 \\ 
FD   & 85.14          & 56.81 & 48.54 & 46.70 & 47.72          & \cellcolor{mygray}45.82 & \cellcolor{mygray}44.57 \\
CAS  & \textbf{85.78} & 55.57 & 50.42 & 49.91 & \textbf{53.47} & \cellcolor{mygray}46.46 & \cellcolor{mygray}44.23 \\
CIFS & 79.87          & 56.53 & 49.80 & 48.17 & 49.89          & \cellcolor{mygray}47.26 & \cellcolor{mygray}43.94 \\ \hline
FSR (Ours) & 81.46    & \textbf{58.07} & \textbf{52.47} & \textbf{51.02} & 49.44 & \cellcolor{lightgray}\textbf{48.34}   & \cellcolor{lightgray}\textbf{46.41}      \\ \hline
\end{tabular}
}
\end{center}
\vspace{-3mm}
\caption{Comparison of robustness (accuracy (\%)) between existing methods and our method. All models are trained using AT with ResNet-18 on CIFAR-10. The best results are marked in \textbf{bold}, and more comprehensive Ensemble and AutoAttack are highlighted in grey.}
\label{table:existing-methods}
\end{table*}


% \textcolor{red}{
% Comparison with Existing Methods.
% Table 3. We show the superiority of our method against existing methods that simply deactivate or suppress non-robust activations.
% We outperform all methods in both various white-box attacks and in AutoAttack.
% This shows that it does indeed help to recalibrate non-robust activations.
% }
\vspace{0.5ex}\noindent
\textbf{Comparison with Existing Methods.}
To verify the effectiveness of recalibrating non-robust activations, we report in Table~\ref{table:existing-methods} the comparison of our method to existing feature manipulation methods (FD, CAS, and CIFS).
We leave out from evaluation kWTA~\cite{kWTA} and SAP~\cite{sap}, which also manipulate feature activations for robustness, because they are known to cause gradient masking~\cite{adaptive, obfuscated}.
We report the robustness against various white-box attacks, the Ensemble of these attacks, and AutoAttack.
Our FSR outperforms the three methods under most white-box attacks.
CAS and CIFS excel at defending against C\&W because they exploit the weights of auxiliary classifiers or the gradients of their logit outputs to manipulate feature activations and thus enlarge the prediction margins on the feature space~\cite{cas}.
Our method does not adopt such technique and slightly lags behind CAS and CIFS under C\&W attack.
Nevertheless, our method achieves the highest Ensemble robustness, verifying that overall, our method is the most robust.
FSR also outperforms all methods under AutoAttack, showing that it is more reliable even under an ensemble of various white-box and black-box attacks.
As opposed to these approaches that deactivate the non-robust feature activations, our method instead recalibrates them to capture additional useful cues that help the model make correct predictions, thus improving the model robustness.


\subsection{Ablation Studies}
\label{sec:exp-ablation}
We first evaluate whether the individual components of FSR module work as desired by measuring the following: (a) classification accuracy of model and (b) weighted $k$-NN accuracy.
% by different features from FSR module.
% To evaluate (a) the classification accuracy of model, we modify a pre-trained AT model with FSR such that instead of the final feature $\tilde{f}$, different features from FSR module are passed into the subsequent layers during inference. 
To evaluate (a) the classification accuracy of model, we pass down different features from FSR into the subsequent layers instead of the final feature $\tilde{f}$.
By doing so, we compare the \textit{model robustness} brought by these different features.


Different from the model robustness, we also explicitly measure the \textit{feature robustness} based on how well each feature captures discriminative cues corresponding to the ground truth class. 
To do so, we embed each feature among the features of natural images and measure (b) the weighted $k$-NN accuracy~\cite{unsup1, unsup2}.
For each arbitrary feature $f$, we first compute the weight $w_i$ for each neighbor $\bm{v_i}$ as its cosine similarity to $f$ with the temperature parameter $\gamma$ such that $w_i = \exp(\cos(f, \bm{v_i}) / \gamma)$.
Then, we compute the prediction score $s_c$ for each class $c$ by weighting the vote of each of the $k$-nearest neighbors $\mathcal{N}_k$ as $s_c = \sum_{i \in \mathcal{N}_k} w_i \cdot \mathbbm{1}(c_i = c)$.
Through this way, we measure robustness of each feature by determining how close it lies to the unperturbed features.
% corresponding to the ground truth class.


% \begin{table}
% 	\begin{center}
% 	    \resizebox{\columnwidth}{!}
% 		{\begin{tabular}{c|c|c|c|c c}
%                 % \thickhline
%                 \hline
%                 \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multirow{2}{*}{Ablation} & \multicolumn{2}{c|}{(a) Classification} & \multicolumn{2}{c}{(b) Weighted $k$-NN} \\
%                 \cline{3-6}
%                 ~ & ~ & Ensemble & AutoAttack & \enspace 5-NN \enspace  & \enspace  20-NN \enspace  \\
%                 \hline
%                 \hline
%                 $f^{+}$ & (i) \& (ii) &47.89 & 45.82 & 66.21 & 61.58 \\
%                 $f^{-}$ & (i) \& (ii) &33.11 & 28.39 & 54.69 & 53.89 \\
%                 % $f^{+} \leftrightarrow f^{-}$   & (i) & 45.87 & 43.70 & 58.73 & 55.08 \\
%                 $\tilde{f}^{-}$                 & (ii) &46.93 & 44.52 & 66.34 & 65.64 \\
%                 $\tilde{f}$ (Ours)  & (i) \& (ii) & 48.34 & 46.41 & 70.91 & 65.88 \\ % $f^{+} + \tilde{f}^{-}$
%                 % \thickhline
%                 \hline
%                 \end{tabular}}
%         \end{center}
%         \vspace{-3mm}
%         	\caption{
%                 Ablation studies on the robustness of various feature maps obtained throughout our framework on CIFAR-10 using ResNet-18.
%         	(a) Robustness (\%) of the model upon replacing the final feature map $\tilde{f}$ with different feature maps.
%                 (b) Top-1 accuracy (\%) of weighted $k$-NN on different feature maps. 
%                 We refer to different features marked in the column ``Ablation" during (i) Evaluation on Separation and (ii) Evaluation on Recalibration.
%                 % \JS{Column ``Ablation" refers to the evaluation stage in which each method is used}: (i) Evaluation on Separation and/or (ii) Evaluation on Recalibration.
%                 % Column ``Ablation'' denotes for which evaluation each method is used (Separation, Recalibration, or both).
%                 }
%                 % 각 feature가 무엇을 의미하는지 명시
%                 % \tilde{F}^{-}가 처음에 본문에 안나오는데 따로 명시?
% 	\label{table:abl-features}
% \end{table}

\begin{table}
	\begin{center}
	    \resizebox{0.9\columnwidth}{!}
		{\begin{tabular}{c|c c|c c}
                \hline
                \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{(a) Classification} & \multicolumn{2}{c}{(b) Weighted $k$-NN} \\
                \cline{2-5}
                ~ &  Ensemble & AutoAttack & \enspace 5-NN \enspace  & \enspace 20-NN \enspace \\
                \hline
                \hline
                $f^{+}$ & 47.89 & 45.82 & 66.21 & 61.58 \\
                $f^{-}$ & 33.11 & 28.39 & 54.69 & 53.89 \\
                % $f^{+} \leftrightarrow f^{-}$   & (i) & 45.87 & 43.70 & 58.73 & 55.08 \\
                $\tilde{f}^{-}$ & 46.93 & 44.52 & 66.34 & 65.64 \\
                $\tilde{f}$ (Ours) & 48.34 & 46.41 & 70.91 & 65.88 \\ % $f^{+} + \tilde{f}^{-}$
                % \thickhline
                \hline
                \end{tabular}}
        \end{center}
        \vspace{-3mm}
        	\caption{
                Ablation studies on the robustness of various feature maps obtained throughout our framework on CIFAR-10 using ResNet-18.
        	(a) Robustness (\%) of the model upon replacing the final feature map $\tilde{f}$ with different feature maps.
                (b) Top-1 accuracy (\%) of weighted $k$-NN on different feature maps. 
                }
        \vspace{-3mm}
	\label{table:abl-features}
\end{table}


% \begin{table}[t]
% 	\begin{center}
% 	    \resizebox{\linewidth}{!}
% 		{\begin{tabular}{c|c c c c c}
% 		\thickhline
%         % \textbf{\textit{ResNet-18}} & \multicolumn{5}{c|}{CIFAR-10} \\
%         % \hline
%         & No Attack & FGSM & PGD-20 & PGD-100 & Ensemble \\
%         \hline
%         \hline
%         AT + FSR (w/o sep)      & & & & & \\
%         AT + FSR                & & & & & \\
%         \hline
%         TRADES + FSR (w/o sep)  & & & & & \\
%         TRADES + FSR            & & & & & \\
%         \hline
%         MART + FSR (w/o sep)    & & & & & \\
%         MART + FSR              & & & & & \\
%         \thickhline
%         \end{tabular}}
% 	\end{center}
% 	\caption{
% 	    Caption.
% 	}
% 	\label{table:abl-wo_sep}
% \end{table}


% \textcolor{red}{
% Effectiveness of Separation.
% Table 4. First, we show that our Separation does appropriately disentangle robust and non-robust features, which is an essential step in our method to recalibrate non-robust activations.
% We do so by first replacing the robustness map generated by our Separation Net w/ a uniform map and a random map.
% We also try inverting the mask generated by our method such that the non-robust feature is passed into the final feature, and the robust feature is passed through Recalibration.
% }
\vspace{0.5ex}\noindent
\textbf{Evaluation on Separation.}
We show that our Separation stage does appropriately disentangle the non-robust feature from the input feature, which is an essential step in our method before recalibrating non-robust activations.
To do so, we evaluate how well the robust feature $f^{+}$ and the non-robust feature $f^{-}$ capture useful cues for correct model predictions.
We measure the classification accuracy of the model against the Ensemble (of FGSM, PGD-20, PGD-100, and C\&W) and AutoAttack upon using each feature (\ie, $\tilde{f} = f^{+}$ or $\tilde{f} = f^{-}$), whose results are shown in the left side (a) of Table~\ref{table:abl-features}.
We can observe that $f^{+}$ leads to much higher robustness against both Ensemble and AutoAttack than $f^{-}$, which drastically decreases the adversarial robustness.
This implies that while $f^{+}$ captures robust cues that lead to correct model decisions on adversarial examples, $f^{-}$ captures non-robust cues that are responsible for mispredictions upon attack, showing that our Separation stage well disentangles the input feature activations as intended.


%%%%%%%%%%%%%%%%%%%%%%%%%%%% Swapping f+ w/ f- %%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We additionally test the robustness when we switch $f^{+}$ and $f^{-}$ during inference ($f^{+} \leftrightarrow f^{-}$).
% That is, we instead pass $f^{+}$ through the Recalibration stage and directly pass $f^{-}$ into the final feature map such that:
% \begin{equation}
%     \tilde{f} = f^{-} + (f^{+} + m^{+} \otimes R(f^{+})).
% \end{equation}
% In this case, as shown in the left side (a) of Table~\ref{table:abl-features}, we suffer from drop in robustness even compared to passing down only $f^{+}$, implying that the uncalibrated non-robust activations provide malicious cues that misguide the model to make incorrect predictions.
% This again verifies that our Separation stage well disentangles such non-robust activations from the robust activations.


% Measuring the weighted $k$-NN accuracy of each target feature on the embedding space with unperturbed features of natural images can be another reliable way to determine how well it captures discriminative cues corresponding to ground truth class without relying on an external classifier.
% For each arbitrary feature $f$, we first compute the weight $w_i$ for each neighbor $\bm{v_i}$ as its cosine similarity to $f$ with the temperature parameter $\gamma$ such that $w_i = \exp(\cos(f, \bm{v_i}) / \gamma)$.
% Then, we compute the prediction score $s_c$ for each class $c$ by weighting the vote of each of the $k$-nearest neighbors $\mathcal{N}_k$ as $s_c = \sum_{i \in \mathcal{N}_k} w_i \cdot \mathbbm{1}(c_i = c)$.
On the right side (b) of Table~\ref{table:abl-features}, we report the 5-NN and 20-NN accuracies on $f^{+}$ and $f^{-}$.
Both cases show that $f^{+}$ results in higher accuracies than $f^{-}$.
This is because we disentangle $f^{+}$ as activations that help the model make correct predictions, and it thus captures more similar representations as the features of natural images than $f^{-}$ does.
% We also measure the $k$-NN accuracy of the final feature resulting from switching $f^{+}$ and $f^{-}$ ($f^{+} \leftrightarrow f^{-}$).
% Feature map generated by switching $f^{+}$ and $f^{-}$ still captures the uncalibrated non-robust activations and thus results in lower $k$-NN accuracy than $f^{+}$.


% In order to verify that the Separation stage is vital for selectively recalibrating the non-robust activations, we experiment on our method without the Separation stage.
% That is, we instead recalibrate the entire input feature map such that $\tilde{F} = R(f)$.
% As shown in Table~\ref{table:abl-wo_sep}, this leads to a drop in robustness compared to recalibrating only the non-robust activations.
% This is because recalibrating the already robust activations could result in loss of information that provides useful cues for correct model predictions.
% Rather, adjusting only the non-robust activations leads to higher robustness.


% \textcolor{red}{
% Effectiveness of Recalibration.
% Table 5. We also show that our Recalibration appropriately adjusts the non-robust activations such that they capture cues that help the model make correct predictions and also that it is vital at improving the robustness of our model.
% On the left side of Table 5, we first report the classification accuracy of the model trained using our method when three different features -- $f_{r}$, $f_{nr}$, and $f_{rec}$ -- are passed into the subsequent layers instead of the final feature, which is combination of $f_{r}$ and $f_{rec}$.
% Explain about the meaning of each result.
% }
\vspace{0.5ex}\noindent
\textbf{Evaluation on Recalibration.}
We also evaluate the ability of Recalibration to adjust the non-robust activations such that they capture cues that help the model make correct predictions.
We again test the classification accuracy of our model upon replacing the final feature map $\tilde{f}$ with the non-robust feature $f^{-}$ or the recalibrated non-robust feature $\tilde{f}^{-}$.
As shown in the left side (a) of Table~\ref{table:abl-features}, $\tilde{f}^{-}$ leads to huge improvements in robustness compared to $f^{-}$, showing that our Recalibration stage appropriately adjusts the non-robust activations to capture cues that help the model make correct predictions.


% We can also observe that the robustness upon passing down only $f^{+}$, which is equivalent to simply suppressing the non-robust activations with our mask $m^{+}$, lags behind using both the robust and recalibrated features, \ie, $\tilde{f}=f^{+}+\tilde{f}^{-}$ (Ours).
We can also observe that $f^{+}$, which is equivalent to simply suppressing the non-robust activations with our mask $m^{+}$, lags behind using both the robust and recalibrated features, \ie, $\tilde{f}=f^{+}+\tilde{f}^{-}$ (Ours).
This shows that recalibrating the non-robust activations restores additional useful cues for model decisions that are not captured by the robust activations and further improves the adversarial robustness, advocating for the necessity of our Recalibration stage.


Similar trends can also be observed on the $k$-NN accuracy as shown in the right side (b) of Table~\ref{table:abl-features}.
The recalibrated non-robust feature $\tilde{f}^{-}$ leads to significantly higher $k$-NN accuracy than $f^{-}$, and the accuracy of $f^{+}$ lags behind that of $\tilde{f} = f^{+} + \tilde{f}^{-}$ (Ours), again verifying the effectivness of our Recalibration stage.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% v1 - bad w/o rec performance %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}
%     \begin{center}
%     \resizebox{\columnwidth}{!}
%     {
%         \begin{tabular}{c|c c c c c c}
%         \hline
%         Method     & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble & AutoAttack \\ 
%         \hline 
%         \hline
%         AT          & 56.21 & 48.22 & 46.37 & 47.38 & 45.51 & 44.11 \\ \hline
%         + FSR w/o sep   & 57.51 & 50.71 & 48.98 & 49.32 & 47.60 & 45.47 \\
%         + FSR w/ sep  & \textbf{58.07} & \textbf{52.47} & \textbf{51.02} & \textbf{49.44} & \textbf{48.34} & \textbf{46.41} \\
%         \hline
%         \end{tabular}
%         }
%         \end{center}
%         \vspace{-3mm}
%         \caption{
%         Comparison of robustness (\%) of FSR applied on AT with or without Separation. 
%         Model and dataset used are ResNet-18 and CIFAR-10, respectively.
%         Best results are marked in \textbf{bold}.
%         }
%     \label{table:abl-sep}
% \end{table}


% \vspace{0.5ex}\noindent
% \textbf{Effectiveness of Separation.}
% We now empirically verify our design choice of applying recalibration selectively on non-robust activations by comparing the model robustness uopn removing the Separation stage from our FSR (\ie, we recalibrate the entire input feature map).
% % Removing the Separation stage improves the natural accuracy; as mentioned in Sec.~\ref{sec:exp-robustness}, this is because with Separation, we specifically disentangle the non-robust activations that may not exist in natural images.
% As shown in Table~\ref{table:abl-sep}, removing the Separation stage leads to drop in adversarial robustness.
% This is because recalibrating the robust activations may lead to loss of already useful cues, verifying the need to selectively recalibrate the non-robust activations.
% Nevertheless, FSR without Separation still improves the robustness of AT, showing that Recalibration effectively captures useful cues for model predictions.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% v1 - bad w/o rec performance %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% v2 - good w/o rec performance %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
    \begin{center}
    \resizebox{\columnwidth}{!}
    {
        \begin{tabular}{c|c c c c c c}
        \hline
        Method     & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble & AutoAttack \\ 
        \hline 
        \hline
        AT          & 56.21 & 48.22 & 46.37 & 47.38 & 45.51 & 44.11 \\ 
        \hline
        + FSR       & \textbf{58.07} & \textbf{52.47} & \textbf{51.02} & \textbf{49.44} & \textbf{48.34} & \textbf{46.41} \\
        w/o Sep     & 57.51 & 50.71 & 48.98 & 49.32 & 47.60 & 45.47 \\
        w/o Rec     & 57.67 & 50.06 & 48.54 & 49.41 & 47.32 & 44.96 \\
        \hline
        \end{tabular}
        }
        \end{center}
        \vspace{-3mm}
        \caption{
        Comparison of robustness (\%) of FSR applied on AT upon removing the Separation or the Recalibration stage. 
        Model and dataset used are ResNet-18 and CIFAR-10, respectively.
        Best results are marked in \textbf{bold}.
        }
        \vspace{-3mm}
    \label{table:abl-sep}
\end{table}


\vspace{0.5ex}\noindent
\textbf{Effectiveness of Separation and Recalibration.}
We verify the necessity of the Separation and the Recalibration stages by comparing the model robustness upon removing each of them from FSR.
As shown in Table~\ref{table:abl-sep}, removing the Separation stage (\ie, recalibrating the entire input feature map) leads to drop in adversarial robustness.
% Since the robust activations already capture useful cues for model predictions, learning to recalibrate them would easily optimize the recalibration loss.
% This can make the Recalibration Net converge in a suboptimal manner such that it focuses on recalibrating the robust activations instead of the non-robust activations that need to be recalibrated.
This is because recalibrating the entire feature map that contains both robust and non-robust activations leads to a suboptimal training of the Recalibration Net.
More specifically, the Recalibration Net learns to minimize the recalibration loss through the discriminative information that is already captured by the robust activations instead of recalibrating the non-robust activations.
Nevertheless, FSR without Separation still improves the robustness of AT, showing that Recalibration effectively captures useful cues for model predictions.


We also compare robustness as we remove the Recalibration stage and pass down only the robust feature into subsequent layers.
As shown in Table~\ref{table:abl-sep}, removing the Recalibration stage also decreases robustness.
This demonstrates the necessity of the Recalibration stage and also confirms that the non-robust activations contain additional useful cues which further boost model robustness.
Still, FSR without Recalibration improves the robustness compared to vanilla AT.
This shows that our Separation stage well disentangles the intermediate feature map based on feature robustness and outputs robust activations that provide useful cues for model predictions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% v2 - good w/o rec performance %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In the supplementary materials, we report additional ablation studies including the effects of different hyperparameters ($\lambda_{sep}$, $\lambda_{rec}$, $\tau$), effectiveness of Gumbel softmax, addition of FSR on different layers, choice of $y'$ in Eq.~\ref{eq:l_sep}, and effects of replacing robustness map $m$ with other strategies.


% (1) Different values of tau -- Too large would not disentangle activations properly, while too small or 0 tau would behave like discrete sampling \\
% (2) Various values of $\lambda$ \\
% (3) Positions where FSR is inserted \\
% (4) Label choice of $y'$ \\


% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{../figures/fig_gradcam.pdf}
%     \caption{ 
%         Adversarial examples ($x'$) and the GradCAM heatmaps visualized on the robust feature $f^{+}$, the non-robust feature $f^{-}$, and the recalibrated feature $\tilde{f}^{-}$ from ResNet-18 with FSR module.
%         While the non-robust feature captures cues irrelevant to the ground truth class, the robust feature and the recalibrated feature each captures unique, relevant cues that lead to correct model decisions.
%     }
%     \label{fig:gradcam}
% \end{figure}


% \subsection{Qualitative Analysis}
% % GradCAM \\
% % Show examples of (1) Recalibration restoring useful cues from non-robust feature, and (2) recalibrated feature capturing new useful cues that are not captured by robust feature. \\
% In Fig.~\ref{fig:gradcam}, we show the GradCAM~\cite{gradcam} visualizations of the robust feature $f^{+}$, the non-robust feature $f^{-}$, and the recalibrated non-robust feature $\tilde{f}^{-}$ on adversarial examples generated using PGD-20 with $\epsilon=8/255$ and step size of 0.1.
% Comparing the heatmaps of $f^{+}$ and $f^{-}$, we can observe that the robust feature captures information relevant to the ground truth class (\eg, the leg of a horse), while the non-robust feature captures irrelevant cues that lead the model to incorrect decisions.
% This shows that our Separation stage well disentangles the feature activations depending on their effects on model decisions.


% Meanwhile, comparing the heatmaps of $f^{-}$ and $\tilde{f}^{-}$, we can observe that the recalibrated feature map captures cues relevant to the ground truth class that the non-robust feature has failed to capture.
% This exhibits the ability of our Recalibration stage to appropriately recapture discriminative cues that help the model make correct predictions.
% Another interesting observation is that the recalibrated feature $\tilde{f}^{-}$ captures discriminative cues that are NOT captured by the robust feature $f^{+}$.
% For example, as shown in the first row of Fig.~\ref{fig:gradcam}, while the robust feature captures cues regarding the leg of the horse, the recalibrated feature captures cues regarding its body.
% This shows that our Recalibration could also restore discriminative cues that are not captured by the robust feature, advocating for the need to recalibrate non-robust activations instead of deactivating them.


\subsection{Computational Efficiency}
\label{sec:exp-comp}
In Table~\ref{table:computation}, we provide the computational analysis of our method compared to a vanilla model in terms of the number of parameters (\# params (M)) and the number of floating point operations (FLOPs (G)).
With only a slightly more number of computations, we can improve the robustness of adversarial training and its variants with a significant margin.
Additionally, on the CIFAR-10 dataset, one training epoch of PGD-10 adversarial training on the vanilla ResNet-18 takes 114 seconds, while it takes 120 seconds on ResNet-18 with FSR module.
One epoch of evaluation on PGD-20 adversarial examples takes 19 seconds for the vanilla ResNet-18, while it takes 20 seconds for ResNet-18 equipped with FSR module.
With slight computational overhead, our FSR module improves the robustness of traditional adversarial training models.


\begin{table}
	\begin{center}
	\resizebox{\columnwidth}{!}
	{\begin{tabular}{c|c c|c c}
        % \thickhline
            \hline
            \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{\textit{\textbf{VGG16}}} & \multicolumn{2}{c}{\textit{\textbf{ResNet-18}}}\\
            \cline{2-5}
             ~ & \# Params (M) & FLOPs (G) & \# Params (M) & FLOPs (G) \\
            \hline
            \hline
            Vanilla     & 15.25         & 0.6299    & 11.17         & 1.1133    \\
            + FSR       & 16.52         & 0.6701    & 12.43         & 1.1535    \\
            % \thickhline
            \hline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Comparison of computational costs (\# params and FLOPs) on a vanilla model and a model with our FSR module.
	}
        \vspace{-3mm}
	\label{table:computation}
\end{table}


% \subsection{Supplementary}
% \subsubsection{More Ablation Studies}
% (4) Gumbel softmax prevents gradient masking. Test the robustness against white-box attacks AutoAttack w/ and w/o Gumbel softmax (binary thresholding w/ value 0.5 w/o Gumbel softmax). \\
% (5) Test the effects of temperature tau. \\
% (6) Effects of placing our FSR module in different layers. \\
% (7) Label choice $y'$.


% \subsubsection{Analysis on Obfuscated Gradients}
% (1) Defense against black-box attacks shown in Sec. 4.2. \\
% (2) Trend of robustness w/ varying perturbation constraint $\epsilon$. \\
% (3) Trend of robustness w/ varying \# attack iterations.


% \subsubsection{Computational Efficiency}
% (1) \# params and FLOPs.