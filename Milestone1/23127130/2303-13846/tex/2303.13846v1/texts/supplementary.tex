\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\renewcommand\thesection{\Alph{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\addcontentsline{toc}{chapter}{First unnumbered chapter}
\setcounter{section}{0}
\renewcommand*{\theHsection}{chX.\the\value{section}}


\noindent
\textbf{\Large Appendix}


\section{Additional Robustness Evaluation}
In this section, we report the robustness of our FSR on additional datasets (CIFAR-100~\cite{cifar10}, Tiny ImageNet~\cite{learnable}) and model (WideResNet-34-10~\cite{wideresnet}).


\vspace{0.5ex}\noindent
\textbf{Experiments on Other Datasets.}
Table~\ref{table:cifar100} shows the robustness improvements when our FSR module is applied on AT, TRADES, and MART in CIFAR-100 dataset.
While the performance improvements are not as large as in CIFAR-10 and SVHN, applying our FSR module consistently improves the model robustness of all three adversarial training techniques, showing that our method is still effective on more challenging datasets.
We noted that the reason for limited accuracy gain on CIFAR-100 is actually due to its low-resolution data not providing sufficient information for learning the inter-class relationship among cues relevant to various similar classes (e.g., boy and man)~\cite{fine-grained}.


Thus, we also evaluate our method on a more challening Tiny ImageNet dataset with fine-grained classes and higher-resolution images.
As shown by the results in Table~\ref{table:tinyimagenet}, we observed 2.08\% improvement on average for Ensemble robustness compared to vanilla methods, which is significantly higher than that of CIFAR-100 (0.67\%, Table~\ref{table:cifar100}) and on par with CIFAR-10 (2.20\%, Table 1) and SVHN (2.30\%, Table 2). 
This shows that our FSR module is also effective on larger, more complex models and datasets and is not limited by the over-parameterization of the model.


\vspace{0.5ex}\noindent
\textbf{Experiments on Other Model.}
In addition to ResNet-18 and VGG16 in the main paper, we also evaluate our FSR module on WideResNet-34-10.
As shown in Table~\ref{table:wideresnet}, our FSR module leads to consistent robustness improvement on WideResNet-34-10.


\section{Additional Ablation Studies}

\vspace{0.5ex}\noindent
\textbf{Position of FSR module.}
Table~\ref{table:position} reports the model robustness when our FSR module is inserted to different layers of ResNet-18. 
As shown in the table, inserting our FSR module after \texttt{Block4} of the model shows the best model robustness under attacks.
This is because the model learns features that are more related to the global semantic information of the image and the final class prediction in the deeper layers, while it learns more low-level features with less semantic information in shallower layers~\cite{cas}.
Recalibrating the non-robust activations in the deeper layers that are more related to the final predictions is more effective at boosting the model robustness.


\begin{table}[t]
	\begin{center}
	    \resizebox{\columnwidth}{!}
		{\begin{tabular}{c|c c c c c c}
        \hline
        \textbf{\textit{ResNet-18}} & \multicolumn{6}{c}{CIFAR-100} \\ \hline 
        Method & Natural & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble \\
        \hline \hline
        {AT}            & \textbf{59.25} & 28.80 & 24.39 & 23.43 & 23.92 & 22.46 \\
        {AT + FSR}      & 58.23 & \textbf{29.58} & \textbf{25.33} & \textbf{24.30} & \textbf{24.54} & \textbf{22.95} \\
        \hline
        {TRADES}        & \textbf{61.87} & 30.77 & 26.37 & 25.76 & 24.08 & 23.45 \\
        {TRADES + FSR}  & 57.27 & \textbf{31.66} & \textbf{27.70} & \textbf{27.27} & \textbf{24.82} & \textbf{24.40} \\
        \hline
        {MART}          & \textbf{57.13} & 31.32 & 27.40 & 26.80 & 25.24 & 24.42 \\
        {MART + FSR}    & 56.51 & \textbf{32.08} & \textbf{27.90} & \textbf{27.28} & \textbf{25.91} & \textbf{24.98} \\
        \hline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Robustness (accuracy (\%)) of adversarial training strategies (AT, TRADES, MART) with (+ FSR) and without our FSR module against diverse white-box attacks on ResNet-18 and on CIFAR-100 dataset. Better results are marked in \textbf{bold}.
	}
	\label{table:cifar100}
\end{table}


\begin{table}[t]
	\begin{center}
	    \resizebox{\columnwidth}{!}
		{\begin{tabular}{c|c c c c c c}
        \hline
        \textbf{\textit{ResNet-18}} & \multicolumn{6}{c}{Tiny ImageNet} \\ \hline 
        Method & Natural & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble \\
        \hline \hline
        {AT}            & 51.13 & 22.54 & 18.69 & 17.87 & 17.83 & 16.34 \\
        {AT + FSR}      & \textbf{51.77} & \textbf{24.19} & \textbf{20.95} & \textbf{20.06} & \textbf{19.32} & \textbf{18.02} \\
        \hline
        {TRADES}        & \textbf{50.41} & 23.79 & 21.16 & 20.72 & 17.24 & 17.02 \\
        {TRADES + FSR}  & 49.53 & \textbf{24.87} & \textbf{23.22} & \textbf{23.09} & \textbf{19.22} & \textbf{19.04} \\
        \hline
        {MART}          & \textbf{46.21} & 23.84 & 21.75 & 21.35 & 18.34 & 17.71 \\
        {MART + FSR}    & 46.02 & \textbf{26.02} & \textbf{24.05} & \textbf{23.82} & \textbf{20.63} & \textbf{20.24} \\
        \hline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Robustness (accuracy (\%)) of adversarial training strategies (AT, TRADES, MART) with (+ FSR) and without our FSR module against diverse white-box attacks on ResNet-18 and on Tiny ImageNet dataset. Better results are marked in \textbf{bold}.
	}
	\label{table:tinyimagenet}
\end{table}


\begin{table}[t]
	\begin{center}
	    \resizebox{\columnwidth}{!}
		{\begin{tabular}{c|c c c c c c}
        \hline
        \textbf{\textit{WideResNet-34-10}} & \multicolumn{6}{c}{CIFAR-10} \\ \hline 
        Method & Natural & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble \\
        \hline \hline
        {AT}            & \textbf{87.49} & 59.47 & 50.72 & 48.75 & 50.42 & 48.52 \\
        {AT + FSR}      & 87.02 & \textbf{61.40} & \textbf{53.78} & \textbf{52.04} & \textbf{52.35} & \textbf{50.36} \\
        \hline
        {TRADES}        & 86.06 & 60.78 & 51.77 & 49.66 & 51.34 & 49.27 \\
        {TRADES + FSR}  & \textbf{86.88} & \textbf{62.97} & \textbf{54.37} & \textbf{51.98} & \textbf{53.19} & \textbf{51.34} \\
        \hline
        {MART}          & 85.81 & 61.22 & 52.49 & 49.88 & 49.67 & 48.81 \\
        {MART + FSR}    & \textbf{86.21} & \textbf{62.61} & \textbf{54.23} & \textbf{52.00} & \textbf{51.25} & \textbf{50.10} \\
        \hline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Robustness (accuracy (\%)) of adversarial training strategies (AT, TRADES, MART) with (+ FSR) and without our FSR module against diverse white-box attacks on WideResNet-34-10 and on CIFAR-10 dataset. Better results are marked in \textbf{bold}.
	}
	\label{table:wideresnet}
\end{table}


\vspace{0.5ex}\noindent
\textbf{Design Choice of $\mathcal{L}_{sep}$.}
As explained in Sec. 3.1 of the main paper, in order to disentangle the non-robust activations through the separation loss $\mathcal{L}_{sep}$ (Eq. 3 of the main paper):
\begin{equation}
    \mathcal{L}_{sep} = - \sum^{N}_{i=1} (y_i \cdot \log(p^{+}_i) + y'_i \cdot \log(p^{-}_i)),
\end{equation}
we minimize the cross entropy loss of the prediction score with respect to $y'$, which we define as the label corresponding to the wrong class with the highest prediction score.
In Table~\ref{table:label}, we report the comparison of robustness as we employ different schemes for such disentanglement.
``Uniform" represents replacing $y'$ with a uniform vector implemented through label smoothing,
``Entropy max." represents maximizing the entropy of the output prediction $p^{-}$ on the non-robust feature,
``Avg. targeted loss" represents the average of cross-entropy loss with respect to all class labels except for the ground truth class,
and ``Mispredicted" represents our original design.
All four schemes lead to meaningful improvement compared to the vanilla AT method, as they guide the Separation Net to learn low robustness scores on feature units that are responsible for predictions other than the ground truth class.
Still, our design of using the mispredicted class output achieves the highest robustness under all attacks.
This implies that through this scheme, the Separation Net learns to assign low robustness scores to the most harmful feature units that lead to the most probable model mistake and thus improves the feature robustness by the largest margin.


\begin{table}
	\begin{center}
	    \resizebox{\columnwidth}{!}
		{\begin{tabular}{c|c c c c c c}
        \thickhline
        & No attack & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble \\
        \hline
        \hline
        $\texttt{Block1}$                       & \textbf{84.58}    & 56.41             & 48.29             & 46.28             & 46.96 & 44.89 \\
        $\texttt{Block2}$                       & 83.76             & 56.34             & 48.86             & 47.03             & 47.32 & 45.28 \\
        $\texttt{Block3}$                       & 82.60             & 56.62             & 50.43             & 49.11             & 47.84 & 46.33 \\
        $\texttt{Block4}$                       & 81.46             & \textbf{58.07}    & \textbf{52.47}    & \textbf{51.02}    & \textbf{49.44} & \textbf{48.34} \\
        $\texttt{Block3}$ + $\texttt{Block4}$   & 82.18             & 56.93             & 50.72             & 49.32             & 48.63 & 46.91 
\\
        \thickhline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Comparison of accuracy (\%) as we insert our FSR module after different layers of ResNet-18.
	}
	\label{table:position}
\end{table}


\begin{table}
	\begin{center}
	    \resizebox{\columnwidth}{!}
		{\begin{tabular}{c|c c c c c c}
        \thickhline
                            & No attack         & FGSM              & PGD-20            & PGD-100           & C\&W      & Ensemble  \\
        \hline
        \hline
        AT                  & 85.02             & 56.21             & 48.22             & 46.37             & 47.38             & 45.51 \\
        \hline
        Uniform             & \textbf{85.16}    & 58.05             & 50.87             & 48.91             & \textbf{49.99}    & 47.90 \\
        Entropy max.        & 84.69             & \textbf{58.35}    & 50.66             & 48.93             & 49.90             & 47.88 \\
        Avg. targeted loss  & 84.50             & 57.98             & 50.41             & 48.55             & 49.80             & 47.42 \\
        Mispredicted (Ours) & 81.46             & 58.07             & \textbf{52.47}    & \textbf{51.02}    & 49.44             & \textbf{48.34} \\
        \thickhline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Comparison of accuracy (\%) for different design choices of the separation loss $\mathcal{L}_{sep}$ (Eq. 3).
	}
	\label{table:label}
\end{table}


\vspace{0.5ex}\noindent
\textbf{Effects of Gumbel Softmax.}
We verify the effects of applying Gumbel softmax to generate a differentiable soft mask $m$ that divides the input feature map into the robust activations and the non-robust activations.
We compare the robustness upon replacing $m$ with a binary mask $b$ (Sec. 3.1 of the main paper) that divides the activations in a discrete manner.
We implement the binary mask $b$ by first applying a sigmoid normalization function to the robustness map $r$ generated by the Separation Net and setting all values less than 0.5 to 0 and all values greater than or equal to 0.5 to 1.
In other words, for an $i$-th unit of the robustness map $r$, we set $b_i$ as follows:
\begin{equation}
    b_i = \begin{cases}
        0, & \text{if}\ \sigma(r)_i < t \\
        1, & \text{if}\ \sigma(r)_i \ge t,
    \end{cases}
\end{equation}
where $t = 0.5$, and $\sigma(\cdot)$ is the sigmoid normalization function.


In Table~\ref{table:gumbel}, we show the comparison of robustness of our method upon using either $b$ (Binary) or $m$ (Gumbel).
Using the differentiable mask $m$ through the Gumbel softmax leads to higher robustness against all white-box attacks and especially against the AutoAttack than using the binary mask $b$.
Using the Gumbel softmax allows us to learn the mask to better capture the feature robustness, and it also prevents gradient masking, thus showing higher robustness against AutoAttack.


\begin{table}
	\begin{center}
	    \resizebox{\columnwidth}{!}
		{\begin{tabular}{c|c c c c c c}
        \thickhline
                & FGSM              & PGD-20            & PGD-100           & C\&W              & Ensemble  & AutoAttack  \\
        \hline
        \hline
        Binary  & 55.78             & 49.21             & 47.79             & 48.74             & 46.91     & 44.26 \\
        Gumbel  & \textbf{58.07}    & \textbf{52.47}    & \textbf{51.02}    & \textbf{49.44}    & \textbf{48.34}    & \textbf{46.41} \\
        \thickhline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Comparison of accuracy (\%) on using mask generated by discrete binary sampling or through Gumbel softmax.
	}
	\label{table:gumbel}
\end{table}


\begin{table}[t]
	\begin{center}
	    \resizebox{\columnwidth}{!}
		{\begin{tabular}{c|c c c c c}
        \thickhline
        & FGSM & PGD-20 & PGD-100 & C\&W & Ensemble \\
        \hline
        \hline
        Greedy          & 57.75             & 49.48             & 47.59             & 48.36             & 46.42 \\
        Random          & 56.60             & 50.04             & 48.46             & 49.08             & 46.77 \\
        w/o Separation        & 57.51             & 50.71             & 48.98             & 49.32             & 47.60 \\
        w/ Separation (Ours)  & \textbf{58.07}    & \textbf{52.47}    & \textbf{51.02}    & \textbf{49.44}    & \textbf{48.34} \\
        \thickhline
        \end{tabular}}
	\end{center}
        \vspace{-3mm}
	\caption{
	    Comparison of accuracy (\%) as we replace the Separation Net with different strategies.
	}
	\label{table:separation}
\end{table}


\vspace{0.5ex}\noindent
\textbf{Experiments on Effectiveness of the Separation Net.}
In order to verify whether our Separation Net is learning appropriate robustness scores for each feature activaiton, we tried replacing the output mask $m$ from the Separation Net (Eq. 2) with different strategies.
We tested random selection and a greedy method of recalibrating the lowest activations, both of which would recalibrate feature activations unaware of their robustness. 
Table~\ref{table:separation} shows that both strategies significantly lag behind our method without Separation, which is equivalent to recalibrating all activations (refer to Table 6). 
This is because they do not fully recapture the discriminative cues underlying in non-robust activations. 
Our method with Separation leads to the highest robustness, showing that FSR well identifies the non-robust activations and recaptures discriminative cues from them.


\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/figures/tau.pdf}
        \caption{$\tau$}
        \label{fig:tau}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/figures/lam_sep.pdf}
        \caption{$\lambda_{sep}$}
        \label{fig:lam-sep}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/figures/lam_rec.pdf}
        \caption{$\lambda_{rec}$}
        \label{fig:lam-rec}
    \end{subfigure}
    \caption{
        Analysis on the robustness with various values of hyperparameters used in FSR module.
        (a) Study on $\tau$ that controls the temperature on Gumbel softmax.
        (b) Study on $\lambda_{sep}$ that controls the weight on the separation loss $\mathcal{L}_{sep}$.
        (c) Study on $\lambda_{rec}$ that controls the weight on the recalibration loss $\mathcal{L}_{rec}$.
    }
    \label{fig:hyperparameter}    
\end{figure*}


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.5\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/figures/iterations.pdf}
        \caption{Varying \# iterations}
        \label{fig:iter}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.5\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/figures/epsilon.pdf}
        \caption{Varying $\epsilon$}
        \label{fig:eps}
    \end{subfigure}
    \caption{
        Analysis on obfuscated gradients.
        (a) Change in model robustness as we vary the number of iterations in PGD attack.
        (b) Change in model robustness as we vary the perturbation bound $\epsilon$.
    }
    \label{fig:obfuscated}    
\end{figure}


\vspace{0.5ex}\noindent
\textbf{Hyperparameter Study.}
We also compare the robustness as we vary the temperature $\tau$ (Eq. 2) that controls how ``discrete" the mask is.
For low temperature values, the output mask becomes more discrete (\ie, most values are close to either 0 or 1), and for high temperature values, it becomes more uniform (\ie, most values are far away from 0 or 1)~\cite{gumbel}.
As shown in Fig.~\ref{fig:tau}, we achieve the highest robustness when $\tau = 0.1$.
From this observation, we can see that too small $\tau$ will degenerate the Gumbel softmax into binary sampling and make the mask become a binary mask, which could result in no gradients or improper training~\cite{spatial}.
In contrast, too large $\tau$ will make the mask become more uniformly distributed and reduce the gap between the mask values applied on robust or non-robust activations, thus making our goal of disentanglement less feasible.


In Fig.~\ref{fig:lam-sep} and Fig.~\ref{fig:lam-rec}, we visualize the trends of model robustness as we vary the weights on our proposed loss functions $\mathcal{L}_{sep}$ (Eq. 3) and $\mathcal{L}_{rec}$ (Eq. 4).
Higher value of $\lambda_{sep}$ generally improves robustness under all attacks with the best performance achieved when $\lambda_{sep} = 1$, showing that our proposed objectives help the model learn more robust feature representations.
Similar trends can also be observed for $\lambda_{rec}$; higher value of $\lambda_{rec}$ generally improves robustness with the best performance achieved when $\lambda_{rec} = 1$.
Setting $\lambda_{sep}$ and $\lambda_{rec}$ to be too high, however, tends to degrade robustness.
This is because of the trade-off between the vanilla classification loss $\mathcal{L}_{cls}$ on the final classifier layer and the two auxiliary loss.
As we focus more on the objectives on the auxiliary layer, the two auxiliary losses may deviate the model from learning the classification task based on $\mathcal{L}_{cls}$.


\section{Analysis on Obfuscated Gradients}
In this section, we verify that the robustness of our method is not a result of obfuscating gradients.
We test our method under the following criteria~\cite{obfuscated} to demonstrate that our method does not obfuscate gradients:
\begin{enumerate}[label=(\roman*)]
    \itemsep0em 
    \item\label{item:first} White-box attacks are stronger than black-box attacks,
    \item\label{item:second} Robustness decreases with the increased number of iterations in gradient-based attacks,
    \item\label{item:third} Robustness decreases with increased perturbation bound $\epsilon$, and unbounded attacks achieve 100\% attack success rate.
\end{enumerate}


Tables 1 and 3 of the main paper show the robustness of our method under both white-box and black-box attacks when applied to ResNet-18 on the CIFAR-10 dataset.
Comparing the two tables, we can observe that the strongest black-box attacks (\eg, DI-FGSM and $\mathcal{N}$Attack) are still weaker than white-box attacks (\eg, C\&W), meeting the requirement~\ref{item:first}.
Fig.~\ref{fig:iter} shows robustness of our method and vanilla PGD adversarial training under PGD attacks with various number of iterations. 
The robustness does indeed decrease with increasing number of iterations, meeting the requirement~\ref{item:second}.
Fig.~\ref{fig:eps} shows robustness of the two methods under PGD attacks with various perturbation bounds $\epsilon$ under $\ell_\infty$-norm.
Similarly, the robustness decreases with increasing $\epsilon$, and it reaches 0\% accuracy under unbounded attacks, thus meeting the requirement~\ref{item:third}.