\section{Related Works}

\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.8\textwidth]{../figures/frameworkv2.pdf}
    % \includegraphics[width=0.8\textwidth]{../figures/frameworkv3.pdf}
    \includegraphics[width=0.8\textwidth]{figures/framework_final.pdf}
    \vspace{-3mm}
    \caption{ 
        (a) Overview of the Feature Separation and Recalibration module.
        During the (b) Separation stage, we disentangle the input feature $f$ into the robust feature $f^{+}$ and the non-robust feature $f^{-}$ by applying the learnable positive mask $m^{+}$ and the negative mask $m^{-}$, respectively.
        Then, during the (c) Recalibration stage, we recalibrate the activations of $f^{-}$ into $\tilde{f}^{-}$ to restore the useful cues for correct model predictions.
        Finally, we combine the recalibrated feature and the robust feature to obtain the output feature $\tilde{f}$ and pass it down to subsequent layers of the model.
    }
    \vspace{-3mm}
    \label{fig:overview}
\end{figure*}

\subsection{Adversarial Training as Adversarial Defense}
Adversarial training guides a model to be robust against adversarial attacks by training it with adversarially generated data and has been widely considered as one of the most effective defense strategies.
It solves the following minimax optimization problem:
\begin{equation}
    \min_{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\delta} \mathcal{L}_{cls}(F_{\theta}(x + \delta), y) \right],
    \label{eq:1}
\end{equation}
\Skip{where $F$ is a DNN model with parameter $\theta$,}
% \YK{
where $F_{\theta}$ is a model parameterized by $\theta$,
% }
$x$ is a natural image with label $y$ from dataset $\mathcal{D}$, $\delta$ is a perturbation bounded within the $\ell_p$-norm of magnitude $\epsilon$ such that $\left\| \delta \right\|_p \le \epsilon$, and $\mathcal{L}_{cls}(\cdot, \cdot)$ represents the classification loss. 
The inner maximization aims to find the strongest possible perturbation $\delta$ that maximizes the classification loss, and the outer minimization learns the model to minimize the loss with respect to the worst-case adversarial examples.
To optimize the inner maximization, Goodfellow \etal~\cite{fgsm} used the Fast Gradient Sign Method (FGSM), and Madry \etal~\cite{pgd} used the Projected Gradient Descent (PGD) attack.


Many variants of adversarial training have also been studied in recent years.
ALP~\cite{alp} reduced the distance between the logits from a natural image and its adversarial counterpart.
TRADES~\cite{trades} decomposed the prediction error on adversarial examples into the natural error and the boundary error to improve both robustness and accuracy.
MART~\cite{mart} additionally considered misclassified  examples during training.
Inspired by curriculum learning~\cite{curriculum}, CAT~\cite{cat} and FAT~\cite{fat} trained models with increasingly stronger adversarial examples to improve generalization.
SEAT~\cite{seat} proposed a self-ensemble method that combines the weights of different models through the training process, and S$^2$O~\cite{s2o} applied the second-order statistics to the model weights to improve adversarial training robustness.
Thanks to its simplicity, our method can be easily plugged into any of these adversarial training methods to further improve their robustness.


\subsection{Adversarial Defense on Feature Space}
% \YK{[The connection between previous subsection and following sentence is not clear.]}
In a parallel line of research, it has been found that some prior models learn non-robust features from the dataset~\cite{NotBugsFeatures} and that input perturbations of adversarial examples are often accumulated through intermediate layers to misguide the final prediction~\cite{fd}.
To solve these problems, several works tried to learn robust feature representations by modifying the network structure or applying regularizations.
Galloway \etal~\cite{bn-galloway}, Benz \etal~\cite{bn-benz}, and Wang \etal~\cite{bn-wang} studied the adversarial vulnerability from the perspective of batch normalization. 
% Zhang \etal~\cite{general-activation} proposed a general framework to certify the robustness of neural networks with different activation functions.
Dhillon \etal~\cite{sap} applied pruning to a random set of activations, especially those with small magnitudes, and Madaan \etal~\cite{anp-vs} pruned out activations that are vulnerable to adversarial attacks.
Mustafa \etal~\cite{pcl} proposed a class-wise feature disentanglement and pushed the centers of each class from each other to learn more discriminative feature representations.
 \Skip{\YK{for $\sim$}.}


There also have been attempts to reduce abnormalities in the feature maps by explicitly manipulating the feature activations.
% explicitly manipulate the feature outputs to reduce abnormalities in the feature maps by masking or suppressing unreliable feature units.
Xiao \etal~\cite{kWTA} proposed \textit{k}-Winner-Takes-All activation to deactivate all feature units except for \textit{k} units with the largest magnitudes.
Xu \etal~\cite{pse} interpreted the effects of adversarial perturbations on the pixel, image, and network levels and masked out feature units sensitive to perturbations.
Zoran \etal~\cite{attention} applied attention mechanism to emphasize important regions on the feature map.
Xie \etal~\cite{fd} proposed Feature Denoising (FD) that applies classical denoising techniques to deactivate abnormal activations.
Bai \etal~\cite{cas} and Yan \etal~\cite{cifs} studied the effects of perturbations on the feature activation from the channel perspective and proposed Channel Activation Suppression (CAS) and Channel-wise Importance-based Feature Selection (CIFS), respectively, to deactivate the activation of non-robust channels.


% \Skip{In contrast to these methods, we propose that with recalibration, these non-robust activations could recapture potential cues that help the model make correct decisions. 
% By simply deactivating the non-robust feature units, these methods ignore such potentially discriminative cues that can improve model robustness. 
% We propose a novel method (see Sec.~\ref{sec:methods}) to further utilize these discriminative cues for improved model robustness by effectively recalibrating them.}
% In contrast to these methods, we propose a recalibration strategy that aims to capture potentially discriminative cues from non-robust features.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% submitted version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In contrast to these methods, we propose a recalibration strategy that aims to capture potentially discriminative cues from non-robust features.
% While existing deactivation strategy simply discards non-robust feature units and ignores these cues, we adjust such activations to restore additional useful information for improved model robustness.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Camera-ready version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In contrast to these methods, we propose a recalibration strategy.
Existing deactivation strategies simply discard non-robust feature activations responsible for model mistakes.
Taking a step further, we adjust such activations to instead \textit{recapture} potentially discriminative cues and thus boost model robustness.
