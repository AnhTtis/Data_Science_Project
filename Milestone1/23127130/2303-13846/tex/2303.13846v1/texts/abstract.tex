\begin{abstract}
% Despite the advancements of deep neural networks in computer vision, they are known to learn non-robust feature representations and are vulnerable to adversarial examples.
% Numerous attempts have tried to learn robust representations by suppressing non-robust feature activations, but the suppressed units still retain cues irrelevant to correct predictions that may deteriorate the model robustness.
% To overcome such limitation, we propose a novel, easy-to-plugin framework named \emph{Feature Compression Decompression (FCD)} that reconstructs robust feature maps in which most units instead capture relevant cues for predictions.
% To achieve this goal, \emph{FCD} consists of two main parts: compression and decompression. 
% The compression part extracts reliable units that capture information relevant to correct model predictions, and the decompression part reconstructs a new feature map by rearranging these reliable cues in a way that they provide meaningful information to subsequent layers.
% Extensive experiments verify the superiority of our \emph{FCD} framework compared to traditional suppression techniques and demonstrate that it improves the robustness of existing adversarial training methods.
% Codes are provided in the supplementary materials.

Deep neural networks are susceptible to adversarial attacks due to the accumulation of perturbations in the feature level, and numerous works have boosted model robustness by deactivating the non-robust feature activations that cause model mispredictions.
However, we claim that these malicious activations still contain discriminative cues and that with recalibration, they can capture additional useful information for correct model predictions.
To this end, we propose a novel, easy-to-plugin approach named \emph{Feature Separation and Recalibration (FSR)} that recalibrates the malicious, non-robust activations for more robust feature maps through Separation and Recalibration.
The Separation part disentangles the input feature map into the robust feature with activations that help the model make correct predictions and the non-robust feature with activations that are responsible for model mispredictions upon adversarial attack.
The Recalibration part then adjusts the non-robust activations to restore the potentially useful cues for model predictions.
Extensive experiments verify the superiority of \emph{FSR} compared to traditional deactivation techniques and demonstrate that it improves the robustness of existing adversarial training methods by up to 8.57\% with small computational overhead.
Codes are available at \url{https://github.com/wkim97/FSR}.


\Skip{
Deep neural networks are susceptible to adversarial attacks due to the accumulation of perturbations in the feature level, and numerous works have boosted model robustness by deactivating the non-robust feature activations that cause model mispredictions. However, we claim that these malicious activations still contain discriminative cues and that with recalibration, they can capture additional useful information for correct model predictions. To this end, we propose a novel, easy-to-plugin approach named Feature Separation and Recalibration (FSR) that recalibrates the malicious, non-robust activations for more robust feature maps through Separation and Recalibration. The Separation part disentangles the input feature map into the robust feature with activations that help the model make correct predictions and the non-robust feature with activations that are responsible for model mispredictions upon adversarial attack. The Recalibration part then adjusts the non-robust activations to restore the potentially useful cues for model predictions. Extensive experiments verify the superiority of FSR compared to traditional deactivation techniques and demonstrate that it improves the robustness of existing adversarial training methods by up to 8.57% with small computational overhead. Codes are provided in the supplementary materials and will be made publicly available.
}


\end{abstract}