% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[pagenumbers]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{multirow}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage[table]{xcolor}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{20} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{An Extended Study of Human-like Behavior under Adversarial Training}
%\title{Do Vision Models Benefit From Adversarial Training  to Generalize to a Wider Range of Out-Of-Distribution Samples?  }
\author{
Paul Gavrikov$^{1}$\thanks{Funded by the Ministry for Science, Research and Arts, Baden-Wuerttemberg, Grant 32-7545.20/45/1 (Q-AMeLiA).} \qquad Janis Keuper$^{1,2}$\footnotemark[1] \qquad Margret Keuper$^{3,4}$\\
$^{1}$IMLA, Offenburg University, $^{2}$Fraunhofer ITWM, $^{3}$University of Siegen\\
$^{4}$Max Planck Institute for Informatics, Saarland Informatics Campus\\
{\tt\small \{paul.gavrikov,janis.keuper\}@hs-offenburg.de,  
keuper@mpi-inf.mpg.de}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract} % Interpreting and understanding model robustness
Neural networks have a number of shortcomings. Amongst the severest ones is the sensitivity to distribution shifts which allows models to be easily fooled into wrong predictions by small perturbations to inputs that are often imperceivable to humans and do not have to carry semantic meaning. Adversarial training poses a partial solution to address this issue by training models on worst-case perturbations. Yet, recent work has also pointed out that the reasoning in neural networks is different from humans. Humans identify objects by shape, while neural nets mainly employ texture cues. Exemplarily, a model trained on photographs will likely fail to generalize to datasets containing sketches. Interestingly, it was also shown that adversarial training seems to favorably increase the shift toward shape bias. In this work, we revisit this observation and provide an extensive analysis of this effect on various architectures, the common $\ell_2$- and $\ell_\infty$-training, and Transformer-based models. Further, we provide a possible explanation for this phenomenon from a frequency perspective.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
ImageNet~\cite{imagenet} trained convolutional neural networks (CNNs) have been shown to predominantly classify images by the observed texture, whereas, humans rather tend to consider global object shapes as the predominant cues~\cite{geirhos2018imagenettrained}. In this context, \textit{Geirhos} \etal provided an initial analysis of robust models and provided initial evidence, that the initial texture bias in CNNs is shifted towards shape-based decisions under adversarial training (AT) \cite{Geirhos2021_modelsvshumasn}. However,  the authors have limited their analysis to a ResNet-50 trained on ImageNet using AT against an $\ell_2$-bound adversary. To allow for a more conclusive evaluation, we expand their analysis to the more common $\ell_\infty$-setting for AT and analyze additional CNNs like ResNet-18 and Wide-ResNet-50-2, as well as Transformers (XCiT-S/M/L12), which are known to behave differently from CNNs regarding their inductive bias. In our study, we evaluate models trained on clean data as well as under AT with different norms and budgets with respect to their generalization ability to out-of-domain (OOD) data \cite{Geirhos2021_modelsvshumasn,Wang2019Learning}, with special emphasis on the shape-texture \textit{cue-conflict}, that has been used as a measure of the misalignment between human and neural network based image classification. In this context, we provide an extensive evaluation and discussion of the different behavior of CNN and Transformer models. 

Further, we analyze the generalization ability of adversarially-trained networks from a frequency perspective. Specifically, we investigate the frequency spectra of different OOD image categories and provide possible explanations for the following two questions: (i) Why does adversarial training lead to an accuracy decay on some OOD datasets? and (ii) Why is the \textit{cue-conflict} between shape and texture affected by AT?

We summarize our key findings as follows:
\begin{itemize}
    \item Training against $\ell_\infty$-bound adversaries generally results in similar trends regarding human-like behavior with respect to the shape-texture bias as $\ell_2$-bound adversarial training. However, $\ell_\infty$-robust models perform better on high-frequency, and worse on low-frequency data.
    \item Observations made by prior work on $\ell_2$-bound ResNet-50 scale to other CNNs and Transformers relative to parameter sizes.
    \item Although Transformers also experience a drop in OOD performance after adversarial training, they perform better in OOD generalization and are more human-like than robust CNNs, and even outperform humans on many benchmarks.
    \item From the analysis of the images frequency spectra, we provide a possible explanation of why adversarial training can lead to a decay of model accuracy on OOD data.
    \item We also provide a possible explanation of why adversarial training reduces texture bias and increases shape bias.
\end{itemize}

%-------------------------------------------------------------------------
\section{Related Work}
\label{sec:related_work}
This work focuses on the intersection between adversarial robustness and ``human-like'' behavior which we briefly sketch in this section.
\paragraph{Adversarial robustness.}
Neural networks have a tendency to overfit the training data distribution, which makes them fail to generalize beyond it. As a result, their predictions are often highly sensitive to small changes in input \cite{biggio2013,SzegedyZSBEGF13}, even if those changes are imperceptible and meaningless to humans. This phenomenon can be formally described as an adversarial attack, where the goal is to find an additive perturbation to the input sample that maximizes the loss function \cite{goodfellow2015explaining,madry2018towards,croce2020reliable,croce2020minimally}. To constraint attacks, perturbations are only sought within a specified radius $\epsilon$ (budget) of the original input. The radius is typically bounded by the $\ell_2$ or $\ell_\infty$-norm.

Adversarial attacks can be found in both white-box \cite{madry2018towards,croce2020minimally,croce2020reliable} and black-box \cite{liu2017delving,ilyas2018blackbox,Bhagoji2018Oct,andriushchenko2020} settings, with gradient-based attacks being particularly effective. Models that are not trained with adversarial defenses are typically only robust to low budgets attacks, if at all. Adversarial training (AT) \cite{madry2018towards} is a solution to this problem, as it trains the model on worst-case perturbations found during training, effectively making out-of-domain attacks become in-domain samples. However, this approach can result in overfitting to attacks used during training. Early stopping \cite{Zhang2020AttacksWD} and the addition of external (synthetic) data \cite{rebuffi2021data,gowal2021improving,wang2023better} have been proposed as effective solutions to address this problem.

However, adversarial robustness does not necessarily correlate with improved generalization and can even hurt it \cite{SaikiaSB21}. Supposedly again due to overfitting of training data, \eg models can still be susceptible to adverse weather conditions, image artifacts due to image compression, changes in lighting, etc.~\cite{hendrycks2019,lopes2020improving}.
%
\begin{figure}
    \centering
    \resizebox{\columnwidth}{!}{
         \begin{tabular}{@{}ccccc@{}}
         \textbf{colour (B\&W)} & \textbf{contrast} &\textbf{eidolon I} & \textbf{eidolon II} & \textbf{eidolon III}\\
         \includegraphics[width=0.32\columnwidth]{images/color_0011_cl_dnn_bw_elephant_40_n02504458_3485.png} & 
         \includegraphics[width=0.32\columnwidth]{images/contrast_0013_cop_dnn_c30_elephant_10_n02504458_3225.png} & 
         \includegraphics[width=0.32\columnwidth]{images/eidolonI_0141_eid_dnn_8-10-10_elephant_10_n02504458_651.png} &
         \includegraphics[width=0.32\columnwidth]{images/eidolonII_0043_eid_dnn_8-3-10_elephant_10_n02504013_8930.png} & 
         \includegraphics[width=0.32\columnwidth]{images/eidolonIII_0051_eid_dnn_16-0-10_elephant_10_n02504458_3570.png} \\
         \textbf{false-colour} & \textbf{high-pass} & \textbf{low-pass} & \textbf{phase-scrambling} & \textbf{power-equalisation} \\
         \includegraphics[width=0.32\columnwidth]{images/false-colour_0008_fc_dnn_false_elephant_35_n02504013_5027.png} &
         \includegraphics[width=0.32\columnwidth]{images/high-pass_0069_hp_dnn_0.7_elephant_10_n02504458_6135.png} & 
         \includegraphics[width=0.32\columnwidth]{images/low-pass_0094_lp_dnn_7_elephant_10_n02504458_1848.png} &
         \includegraphics[width=0.32\columnwidth]{images/phase-scrambling_0099_ps_dnn_60_elephant_10_n02504013_10389.png} & 
         \includegraphics[width=0.32\columnwidth]{images/power-equalisation_0095_pow_dnn_pow_elephant_35_n02504013_18988.png} \\
         \textbf{rotation} & \textbf{uniform-noise} & & & \\
         \includegraphics[width=0.32\columnwidth]{images/rotation_0103_rot_dnn_270_elephant_20_n02504013_7883.png} &
         \includegraphics[width=0.32\columnwidth]{images/uniform-noise_0044_nse_dnn_0.20_elephant_10_n02504013_2464.png} & & &\\
         \textbf{edge} & \textbf{silhouette} & \textbf{sketch} & \textbf{stylized}  & \textbf{cue-conflict (dog)} \\
         \includegraphics[width=0.32\columnwidth]{images/edge_elephant1.png} & 
         \includegraphics[width=0.32\columnwidth]{images/silhouette_elephant1.png} &
         \includegraphics[width=0.32\columnwidth]{images/sketch_0552_ske_dnn_0_elephant_00_elephant-0002-sketch-1.png} & 
         \includegraphics[width=0.32\columnwidth]{images/stylized_0551_sty_dnn_0_elephant_00_elephant-0002-ILSVRC2012-val-00001850.png} &
         \includegraphics[width=0.32\columnwidth]{images/cue-conflict_elephant1-dog2.png} 
         \end{tabular}
     }
     \caption{OOD examples from \cite{geirhos2018imagenettrained,Geirhos2018Generalisation,Wang2019Learning,Geirhos2021_modelsvshumasn} for the ImageNet class ``elephant''.}
    \label{fig:examples}
\end{figure}
%
\paragraph{Measuring ``human-like'' behavior.}

\textit{Geirhos}~\etal propose to measure ``human-like'' reasoning via out-of-distribution (OOD) generalization to datasets and consistency in predictions with humans \cite{Geirhos2021_modelsvshumasn}.

Regarding OOD, they propose to benchmark against a set of 12 ImageNet modification datasets \cite{Geirhos2018Generalisation} at various intensities/conditions. At first glance, this may sound familiar to ImageNet-C \cite{hendrycks2019}, but benchmarks a different set of modifications: \textit{(the absence of) colour}, \textit{contrast (changes)}, \textit{eidolon~I/II/III}, \textit{false-colour}, \textit{high/low-pass (frequency filtering)}, \textit{phase-scrambling}, \textit{power-equalisation}, \textit{rotation}, \textit{uniform-noise}. Additionally, they propose to benchmark against a set of five OOD datasets aiming to identify the shape-texture-bias \cite{geirhos2018imagenettrained}: \textit{stylized}, \textit{edge}, \textit{silhouette}, \textit{texture/shape cue-conflict}, and \textit{sketch} (the latter provided by \cite{Wang2019Learning}). All datasets contain samples that belong to 16 ImageNet classes and are therefore classifiable by ImageNet models. For all datasets, the authors include a baseline obtained in lab settings over 4-10 human annotators. The \textit{cue-conflict} dataset is of particular interest, as neural networks are not only prone to overfit but - at least in the vision domain - they also tend to compute predictions based on details such as the texture of images rather than shapes, which does not align with human vision \cite{geirhos2018imagenettrained}. For example, an image of an elephant with an overlaid lion texture will most likely result in a prediction as ``lion'', while most humans would predict ``elephant'' as the true label when given the choice between both. It is worth noting that the authors also mention that ImageNet can be largely accurately classified solely based on texture. As such, ImageNet performance is insufficient as an indicator of ``human-like'' decision making, and \textit{Geirhos}~\etal propose to additionally report the \textit{cue-conflict} score to quantify this phenomenon. Examples of all datasets are shown in \cref{fig:examples}.

As an additional metric to accuracy, \cite{Geirhos2020error} propose to evaluate the agreement in predictions. In particular, they analyze false predictions (\textit{error consistency}) as well as the intersection rate of predictions where both humans and models have made a correct prediction (\textit{observed consistency}). 

The authors maintain a leaderboard of the most ``human-like'' models, which is currently dominated by Transformers such as \textit{ViT} \cite{dosovitskiy2021an,Zhai_2022_CVPR} and \textit{CLIP} \cite{pmlr-v139-radford21a}, or large convolutional neural networks \cite{yalniz2019billionscale, Kolesnikov2020Oct} - all being pre-trained on massive datasets. 
%-------------------------------------------------------------------------
\section{Method}
\label{sec:method}
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{plots/fft_to_azimuth.drawio.pdf}
    \caption{Visualization of how we obtain the spectrum plots. Each frequency measurement in the spectrum plot corresponds to the integral over the FFT power spectrum (frequency increases from the center to outer edges) up to that particular frequency. }
    \label{fig:fft_to_azimuth}
\end{figure}
%
To study the likeliness to human-like behavior of adversarially-trained models in greater detail, we use publicly available checkpoints and perform an analysis according to the setting proposed in 
\cite{Geirhos2021_modelsvshumasn}. We analyze pre-trained \textit{ResNet-18, ResNet-50, WideResNet-50-2} models trained against $\ell_\infty$-bound adversaries with $\epsilon\in\{0.5/255,1/255, 2/255,4/255,8/255\}$, and against $\ell_2$-bound adversaries with $\epsilon\in\{0.01,0.03,0.05,0.1,0.25,0.5,1,3,5\}$, and clean baselines (all provided by \cite{NEURIPS2020_24357dd0}). Further, we analyze \textit{XCiT-S/M/L12} Transformer models trained against $\ell_\infty$-bound adversaries with $\epsilon=4/255$ provided by \cite{debenedetti2023light} and a clean \textit{XCiT-S}\footnote{Clean pre-trained \textit{XCiT-M/L12} with the same configuration were not available.} baseline provided by \cite{rw2019timm}. Lastly, to better understand the differences between CNNs and Transformers, we also analyze a clean \textit{ConvMixer-768-32} \cite{Trockman2022} checkpoint, again obtained from \cite{rw2019timm}.
All models were trained on ImageNet \cite{imagenet} without any additional pre-training.

For all models, we measure the accuracy of all datasets by reporting the mean overall conditions in the dataset where average human performance was above 20\% accuracy. Lastly, we determine the \textit{observed} and \textit{error consistency} against human annotators again as a mean over all datasets and conditions. As there are multiple annotators per dataset, we calculate consistencies against each annotator and report the mean.

We first provide a more extensive evaluation of models that have been trained using $\ell_2$-AT. Then, we provide insights on how models trained with $\ell_2$-AT behave compared to models that are trained using $\ell_\infty$-AT. Comparing these two training types is not straightforward, due to the different types of perturbations they cause. As the $\ell_2$-norm penalizes the euclidean distance, perturbations can locally be more severe than under $\ell_\infty$. Yet, if the perturbation magnitude increases the area of perturbations has to decrease under $\ell_2$-norm, while attacks under the $\ell_\infty$-norm can add perturbations to the entire image without constraints except for the magnitude. Thus, there are multiple options for choosing comparable budgets between the two norms. We choose a straightforward way and select budgets for both norms that approximately result in the same clean accuracy shown in \cref{tab:eps_cmp}. As we have more checkpoints for $\ell_2$-AT training, we only use a subset of those in the following analysis.
%
\iffalse
\begin{table}
    \centering
    \scriptsize
    \caption{Our chosen $\epsilon$ budgets for comparisons between $\ell_2$- and $\ell_\infty$-bound training.}
    \label{tab:eps_cmp}
    \begin{tabular}{c|c}
    \toprule
    $\ell_2$ & $\ell_\infty$\\
    \midrule
         $0.1$ & $0.5/255$  \\
         $1$ &  $1/255$\\
         $3$ &  $4/255$\\
         $5$ &  $8/255$\\
    \bottomrule
    \end{tabular}
\end{table}
\fi
\begin{table}
    \centering
    \scriptsize
    \caption{Our chosen $\epsilon$ budgets for comparisons between $\ell_2$- and $\ell_\infty$-bound training.}
    \label{tab:eps_cmp}
    \begin{tabular}{c|cccc}
    \toprule
    $\ell_2$ & $0.1$&$1$ &$3$ &$5$\\
    \midrule
    $\ell_\infty$ & $0.5/255$&$1/255$&  $4/255$ &  $8/255$\\     
    \bottomrule
    \end{tabular}
\end{table}
%

Next, we compare the behavior of CNN and Transformer models under these training settings. Based on these experiments, we then discuss whether AT is an effective tool to induce a more human-like behavior in trained models. Finally, we impose a frequency perspective on OOD performance and shape bias under AT. To back this analysis, we plot the frequency distribution for each OOD dataset, and clean ImageNet validation samples belonging to the same classes. Then we compare each OOD distribution to the clean distribution to understand where shifts in the frequency distribution are located. We obtain the frequency distribution plots as introduced in \cite{Durall_2020_CVPR}: we compute the log-scaled FFT power spectrum and compute the radial integral under increasing frequency resulting in a frequency power distribution (\cref{fig:fft_to_azimuth}). For comparability, we scale the resulting distributions by their integral.
%
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/budget_trend.pdf}
    \caption{Performance of $\ell_2$ vs. $\ell_\infty$-AT-trained \textit{ResNet-18, ResNet-50, WideResNet-50-2} on clean data, texture/shape bias cue-conflict datasets, the average mean of all OOD datasets, and observed/error consistency compared to humans under increased training attack budget $\epsilon$.}
    \label{fig:budget_trend}
\end{figure}
%
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{plots/linf_vs_l2.pdf}
    \caption{Comparison of performance on OOD datasets between robust \textit{WideResNet-50-2} trained against $\ell_2$- (upper $\epsilon$ values) and $\ell_\infty$-bound (lower $\epsilon$ values) adversaries under increasing training budget $\epsilon$. $\epsilon$ are selected in a way that clean accuracy approximately matches between the norms.}
    \label{fig:linf_vs_l2}
\end{figure*}
%
%-------------------------------------------------------------------------
\section{Results}
\label{sec:results}
\paragraph{Width and depth of $\ell_2$ CNNs.}
First, we want to evaluate the effect of $\ell_2$-training on CNN architectures: \textit{Wide-ResNet-50-2}, \textit{ResNet-50} and \textit{ResNet-18} (\cref{fig:budget_trend}, top row), which allows investigating the effect of $\ell_2$-AT depending on model depth and width. 

We observe that increasing both, width and depth improves clean performance, and performance on all OOD datasets, as well as the observed and error consistency. As such, \textit{Wide-ResNet-50-2} performed best on clean performance, OOD mean, and observed/error consistency. It is also worth noting, that switching from \textit{ResNet-50} to \textit{Wide-ResNet-50-2} has a smaller impact on performance than switching from \textit{ResNet-18} to \textit{ResNet-50}. Also, we observe that in some cases \textit{ResNet-18} shows opposite trends with respect to training budget than 50-layer deep \textit{ResNets}, \eg for \textit{power-equalisation}. Still, \textit{ResNet-18} performs best on the \textit{edge} dataset for large training budgets (not shown due to space limitations). Overall, %of all the tested CNNs we analyzed, 
this suggests, that increasing parameterization of $\ell_2$-bound adversarially-trained models correlates with an increase in human-like behavior.

\paragraph{$\ell_2$- vs. $\ell_\infty$-bound Adversarial Training.}
Next, we compare how $\ell_2$-AT relates to $\ell_\infty$-AT with respect to human-like reasoning. 
Exemplarily, we analyze the trend under comparable budgets of a \textit{WideResNet-50-2} (\cref{fig:linf_vs_l2}). We observe that on some datasets there is barely any perceivable difference as the budget increases (\textit{colour, contrast, false-colour, uniform-noise, rotation, cue-conflict}), but there are cases where one norm or the other clearly performs better. $\ell_\infty$-robust models seem to be more robust against \textit{high-pass}, \textit{phase-scrambling}, and \textit{power-equalisation}. On the other hand, $\ell_2$-robust models appear to perform better on \textit{low-pass}, and \textit{eidelonII/III}. Lastly, there are also some inconclusive settings where one or the other performs better depending on the budget (\textit{silhouette, eidolon~I, stylized}). Besides \textit{cue-conflict}, none of the OOD categories clearly benefit from AT for WideResNet-50-2. These observations only partly transfer to other CNN architectures in \cref{tab:my_label}. In general, \cref{fig:budget_trend} (bottom) shows a similar trend for $\ell_2$ and $\ell_\infty$-AT, and all results support the finding that the \textit{cue-conflict} score increases consistently under both types of AT, \ie the behavior becomes more human-like towards shape bias in both cases. Therefore, we conclude that the more commonly used $\ell_\infty$-AT is equally effective in inducing human-like behavior in CNNs, with respect to \textit{cue-conflict}, and \textit{consistency}. 
%
\begin{table}
    \centering
    \scriptsize
    \caption{Comparison between parameters of analyzed models.}
    \label{tab:params}
    \begin{tabular}{llr}
    \toprule
    \textbf{Model} & \textbf{Inductive Bias} & \textbf{Parameters}\\
    \midrule
         ResNet-18 & CNN & $10.4$ M\\
         ResNet-50 & CNN & $25.6$ M\\
         WideResNet-50-2 & CNN & $68.9$ M \\
         \midrule
         ConvMixer-768-32 & Hybrid & $21.2$ M\\
         \midrule
         XCiT-S12 & Transformer & $26.3$ M\\
         XCiT-M12 & Transformer & $46.4$ M\\
         XCiT-L12 & Transformer & $103.8$ M\\
    \bottomrule
    \end{tabular}
\end{table}
%
\paragraph{CNNs vs. Transformers.}
Finally, we expand our analysis to Transformer architectures (\textit{XCiT}) for which we only report clean and $\ell_\infty$-training performance. 
%
\input{tables/overview}%
%
On clean training, even the smallest Transformer (\textit{XCiT-S12}) which has a comparable number of parameters to \textit{ResNet-50} (\cref{tab:params}), performs significantly better than the largest CNN. Contrary to CNNs it is also able to surpass human performance on \textit{eidolon II/III}, \textit{high-pass} (with an impressive improvement of approx.~35\% above CNNs), \textit{phase-scrambling, power-equalisation, uniform-noise}, and \textit{stylized}.
Under AT, we largely see the same shift as with CNNs with one exception: while AT improves \textit{stylized} performance of CNNs, it decreased it on Transformers. Still, Transformers achieve higher accuracies than humans in this category.
Of all studied models, the adversarially-trained \textit{XCiT-L12} performs best on \textit{eidolon I-III}, \textit{silhouette, sketch, and cue-conflict}. However, it is also worth noting that it contains 50\% more parameters compared to the largest CNN we analyze. In general, we can not conclude that more parameters are always better as we \eg see some reduction in error consistency from robust \textit{XCiT-S/M12} to \textit{XCiT-L12}. Further, the clean \textit{ConvMixer} which contains no self-attention but patch-embeddings, shows also an increased \textit{cue-conflict}. Generally, there is a trade-off between CNNs and Transformers in almost all studied datasets. We hypothesize that patch-embeddings may naturally be slightly shifted toward shape bias compared to CNNs.

\paragraph{Is adversarial training a good option to achieve human-like reasoning?}
While AT does improve \textit{cue-conflict} significantly and shifts the internal decision process toward human-like shape bias behavior, it also decreases OOD  performance across many datasets. Most notably, AT causes a significant drop in robustness to changes in \textit{contrast}, \textit{rotation}, and \textit{uniform noise} compared to clean training. 
Interestingly, it also always reduces \textit{high-pass} performance. In the case of \textit{XCiT}-models this performance is slightly worse than for humans after AT, although the clean model significantly outperformed humans (by approx.~23\%). 
We see the largest OOD drops in \textit{XCiT}, while the \textit{ResNets} show only minor impairments. Based on these findings, AT alone is not sufficient to shift models toward human-like reasoning in all aspects. In the next section, we investigate the frequency spectra of OOD samples and show that they provide an indication of whether AT can, in principle, help to increase performance.
%
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{plots/ood_spectrum.pdf}
    \caption{Frequency distribution of the utilized OOD datasets in comparison to comparable ImageNet validation samples (clean). Distributions are normalized by their integral. Frequency increases along the X-axis.}
    \label{fig:ood_spectrum}
\end{figure*}
%
\section{A Frequency Perspective on Adversarial Training and Out-Of Distribution Data}
In \cref{fig:ood_spectrum}, we plot for all considered OOD image categories their frequency power spectra, radially integrated as described in \cref{fig:fft_to_azimuth}, and compare the frequency spectra to the spectrum of the clean training images. From this comparison, it is apparent that some OOD categories deviate heavily from the natural image distribution in terms of their spectra. This is obviously true for \textit{high-pass} and \textit{low-pass} images as well as for \textit{uniform-noise} and \textit{edge}, where the differences are particularly strong in the high-frequency regime, but it is also visible for \textit{contrast}, \textit{rotation}, and \textit{power equalization}, or \textit{phase scrambling}, with significant differences in the lowest frequencies. Although adversarial attacks might slightly alter the frequency spectrum of attacked images, they are $\epsilon$ bounded and will therefore not significantly change the frequency distribution over all samples. Thus, it would be natural that AT (\ie adding more training samples with a spectral distribution similar to the one of clean images) would harm the transferability of models to such out-of-domain categories. In fact, \cref{tab:my_label} shows exactly this trend: both types of AT cause a consistent decay in classification accuracy for the OOD categories  \textit{high-pass}, \textit{low-pass}, \textit{uniform-noise}, \textit{contrast}, \textit{rotation} and \textit{power equalization}. When the differences are in the low-frequency regime as for \textit{contrast}, the decay seems to be particularly strong. This observation supports the findings by \cite{SaikiaSB21} that AT can harm robustness to other corruption types and provides an initial explanation: Adding more training samples from the original spectral distribution can harm the generalization to other diverging spectral distributions. 

\cref{fig:ood_spectrum} also shows that some OOD categories have power spectra that are quite similar to the spectra of the original data (and thus of adversarial examples). For these categories, \eg \textit{eidolon I}, \textit{eidolon II}, \textit{eidolon II}, \textit{false colour} or \textit{cue-conflict}, AT does not lead to a decay in accuracy but can even lead to improvement in some cases. In the following, we will discuss in which cases we might expect this improvement.  

From the above observation, we see that the OOD data should share some important properties with the clean data to benefit from AT, \ie the frequency distribution should not differ too much. At the same time, it has been argued that convolutional neural networks tend to decide based on texture information \cite{geirhos2018imagenettrained}, which is local and rather mid to high-frequency. Thus, adversarial examples can attack such models by slightly altering the image in these frequency bands. While this may vary by dataset \cite{Maiya2021,AbelloHW21,BernhardMMBCSR21}, at least some high-frequency is always present as  \eg adversarial attacks can be detected in the frequency spectrum~\cite{Harder2021}.

To compensate for these attacks, robust models desensitize to high-frequency and instead shift their decisions towards global cues that involve low-frequency information, which can typically be observed in FFT-spectra of perturbations after AT (\eg \cite{Grabinski2022Nov}, Fig.~8). The desensitization of high-frequencies during training also results in more robust models, as shown from various perspectives such as injecting noise patches to inputs \cite{lopes2020improving}, blurring feature-maps \cite{NEURIPS2020_f6a673f0}, splitting and regularizing frequency information \cite{SaikiaSB21}, or low-pass filtering intermediate feature-maps \cite{Grabinskilowcut22} during training.
There seem to be sufficient indicators to reasonably assume that shifting the decisions toward low-frequency information by removing the focus from high-frequency is at least a necessary ingredient of robustness. Clearly, AT encourages this shift, which can also be seen in weights of convolution filters of robust models \cite{Gavrikov_2022_CVPR,Gavrikov_2022_CVPRW}.

Likewise, texture bias can also be analyzed from a frequency perspective. Textures contain high-frequency information while shapes can not be represented without low-frequency bands. As non-robust neural networks naturally prefer high-frequency information for predictions they reason based on textures. Under AT, models rely less on local high-frequency information and prioritize the lower-frequent information, that encodes global structures such as shapes. This effect can be well seen in the \textit{cue-conflict} performance where images contain both types of information in the images, and models can choose which information to prioritize. From an information perspective alone, both choices would acceptable.


Ultimately, this perspective does not explain all findings \underline{and other mechanics may influence the decision process}. For example, \textit{stylized} performance improves under AT for CNNs while the accuracy of Transformers, starting at a higher level, is decreasing. It can just provide an intuition of why the model decisions learn to shift towards a more global, shape bias  - given that the overall spectral distribution remains very similar to the original training data distribution in the \textit{cue-conflict} category. 
%-------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

We have extended previous experiments that studied the influence of $\ell_2$-AT on the reasoning of neural networks in comparison to human reasoning. Our findings indicate, that previous observations scale to $\ell_\infty$-AT, other CNNs, and even Transformers. In general, we find that robust Transformers appear to be more similar to human reasoning than CNNs as they perform better on OOD datasets and increasingly reason based on shape information. Still, AT results in degradation against some corruptions that do not seem to affect humans or models trained without AT.  Finally, we propose an explanation of why AT enforces shape bias from a frequency perspective: AT seems to hurt generalization against OOD datasets where the spectral distribution significantly diverges from the training data. In other cases, AT causes the model to shift its decision from local high-frequency information to global shape information, which better resembles the behavior of humans.



\clearpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieeetr_fullname}
\bibliography{main}
}

\end{document}
