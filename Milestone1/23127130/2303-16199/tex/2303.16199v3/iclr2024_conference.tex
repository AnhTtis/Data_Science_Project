
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{bbding}
\makeatletter
  \newcommand\figcaption{\def\@captype{figure}\caption}
  \newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{color, colortbl}
\definecolor{citecolor}{HTML}{2980b9}
\definecolor{linkcolor}{HTML}{c0392b}


\usepackage{multirow}
\usepackage{makecell}
\usepackage{caption}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\usepackage{adjustbox}
\usepackage{wrapfig}

\usepackage{float}
\usepackage{subfig}

\usepackage[hidelinks,breaklinks=true,colorlinks,bookmarks=false,citecolor=citecolor,linkcolor=linkcolor]{hyperref}


\usepackage{framed}
\usepackage{makecell}
\usepackage{listings}
\lstset{
    escapeinside={(|}{|)}
}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\lstdefinelanguage{JavaScript}{
  keywords={break, case, catch, continue, debugger, default, delete, do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  ndkeywords={class, export, boolean, throw, implements, import, this},
  keywordstyle=\color{blue}\bfseries,
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  sensitive=true
}
\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}


% \usepackage[hidelinks,breaklinks=true,colorlinks,bookmarks=false,citecolor=citecolor,linkcolor=linkcolor]{hyperref}

\title{LLaMA-Adapter:\\Efficient Fine-tuning of Large Language Models with Zero-initialized Attention}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Renrui Zhang$^{*1,2}$, Jiaming Han$^{*1,2}$, Chris Liu$^{*1}$,  Aojun Zhou$^2$, Pan Lu$^{3}$\vspace{0.1cm}\\ \textbf{Yu Qiao$^{\dagger 1}$, Hongsheng Li$^{\dagger 2,4}$, Peng Gao$^{\dagger \ddagger * 1}$}\vspace{0.3cm}\\
$^1$Shanghai Artificial Intelligence Laboratory\quad 
  $^2$CUHK MMLab\\
  $^3$University of California, Los Angeles\quad $^4$CPII of InnoHK\vspace{0.1cm}\\
\texttt{\{zhangrenrui, hanjiaming, gaopeng, qiaoyu\}@pjlab.org.cn}\\
\texttt{hsli@ee.cuhk.edu.hk}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\blfootnote{$^*$ Equal contribution\ \ $^{\dagger}$ Corresponding author\ \ $^{\ddagger}$ Project leader}

\vspace{-0.7cm}
\begin{abstract}
With the rising tide of large language models (LLMs), there has been a growing interest in developing general-purpose instruction-following models, e.g., ChatGPT.
To this end, we present \textbf{LLaMA-Adapter}, a lightweight adaption method for efficient instruction tuning of LLaMA. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces \textbf{1.2M} learnable parameters upon the frozen LLaMA 7B model, and costs less than \textbf{one hour} for fine-tuning. Specifically, a zero-initialized attention mechanism is proposed. It adopts a learnable zero gating to adaptively inject the instructional cues into LLaMA within self-attention layers, contributing to a stable training process and superior final performance.
In this way, LLaMA-Adapter can generate high-quality responses to diverse language instructions, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, by incorporating an image encoder, our approach can be simply extended to a \textbf{Multi-modal LLM} for image-conditioned instruction following, which achieves superior multi-modal reasoning capacity on several popular benchmarks (MME, MMBench, LVLM-eHub).
Furthermore, we also verify the proposed zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa, CLIP) on traditional vision and language tasks, demonstrating the effectiveness and generalizability of our approach. Code and models are released at \url{https://github.com/OpenGVLab/LLaMA-Adapter}.
\end{abstract}


\section{Introduction}

Large Language Models (LLMs)~\citep{dai2019transformer,radford2019language,zhang2022opt,raffel2020exploring,devlin2018bert} have stimulated widespread attention in both academia and industry. Driven by massive corpora and advanced hardware, LLMs exhibit remarkable understanding and generative ability, propelling language tasks to a higher level. Recently, significant progress has been made on instruction-following models, e.g., ChatGPT~\citep{OpenAI2023ChatGPT} and GPT-4~\citep{OpenAI2023GPT4TR}, which follow language instructions and generate contextual responses. However, the further prevalence of instruction models is largely impeded by the closed-source restriction and high development costs.


To alleviate this, Stanford Alpaca~\citep{alpaca} proposes to fine-tune an open-source LLM, i.e., LLaMA~\citep{touvron2023llama} into an instruction-following model, which is affordable and replicable. Starting from 175 human-written instruction-output pairs~\citep{selfinstruct}, Alpaca leverages GPT-3.5~\citep{brown2020language} to expand the training data to 52K in a self-instruct manner. Supervised by this, Alpaca fine-tunes the entire 7B parameters in LLaMA, producing an exceptional instruction model that performs similarly to GPT-3.5. Despite Alpaca's effectiveness, a complete fine-tuning of large-scale LLaMA is still time-consuming, computation-intensive, and cumbersome to transfer to different downstream scenarios.


In this paper, we introduce \textbf{LLaMA-Adapter}, an efficient fine-tuning method that adapts LLaMA into a well-performed instruction-following model. 
Trained by Alpaca's instruction-output data, our approach freezes the entire LLaMA model, and proposes a zero-initialized attention mechanism with superior resource efficiency.
Specifically, in LLaMA's higher transformer layers, we append a set of learnable adaption prompts as prefixes to the word tokens.
% These prompts learn to adaptively inject new language instructions into the frozen LLaMA.
Then, to avoid the noise from randomly initialized prompts at the early training stage, we equip the frozen self-attention layers with a learnable gating factor.
The gating mechanism is initialized by zeros, and controls the feature interaction between prompt and word tokens, within the process of attention calculation.
Such a strategy can first preserve the original knowledge in LLaMA, and progressively inject the new instructional signals during training. 
This contributes to a more stable learning process and better instruction-following capacity of the final model. 

% In addition to instruction-following models, our zero-initialized attention can be generalized to other vision and language models for parameter-efficient fine-tuning.
% For vision models, we utilize our approach to fine-tune a pre-trained ViT~\citep{dosovitskiy2020image} for downstream image classification, obtaining superior performance on VTAB-1k~\citep{zhai2019large} benchmark over various image distributions.
% For other language models, we evaluate our fine-tuning efficacy on ReBERTa~\citep{liu2019roberta} for extractive question answering, which achieves leading results on SQuAD~\citep{rajpurkar2016squad} v1.1 and v2.0 benchmarks.
% By the extension experiments, we demonstrate the effectiveness of the zero-initialized attention in LLaMA-Adapter for traditional vision and language tasks.


\begin{figure*}[t!]
  \centering
  \vspace{-0.2cm}
    \includegraphics[width=\textwidth]{figs/fig1_5.pdf}
   \caption{\textbf{Characteristics of LLaMA-Adapter.} Our lightweight adaption method efficiently fine-tunes LLaMA~\citep{touvron2023llama} 7B model with only 1.2M learnable parameters within one hour, which exhibits superior instruction-following and multi-modal reasoning capacity.}
    \label{fig1}
    % \vspace{-0.3cm}
\end{figure*}

Overall, our LLaMA-Adapter exhibits four main characteristics, as shown in Figure~\ref{fig1}.

\begin{itemize}
   \item \textbf{1.2M Parameters.} 
   Instead of updating the full 7B parameters, we freeze the pre-trained LLaMA and only learn the zero-initialized attention mechanism with 1.2M parameters. This, however, reveals comparable instruction-following proficiency with the 7B Alpaca.
   
   \item \textbf{One-hour Fine-tuning.}
   Thanks to our lightweight adaption modules with zero-initialized gating, the training convergence of LLaMA-Adapter costs less than one hour on 8 A100 GPUs, which are three times faster than Alpaca.
   

   \item \textbf{Plug with Expertise.}
   For different scenarios, it is flexible to insert their respective adapters to endow LLaMA with different expert knowledge or new modality input. Thus, it suffices to store a 1.8M adapter within each context, other than a complete copy of the 13G LLaMA.

   
   \item \textbf{Multi-modal Reasoning.}
   Besides language instruction, our approach can also incorporate an image encoder via zero-initialized attention to become a multi-modal LLM. Compared to concurrent works~\citep{llava,zhu2023minigpt}, LLaMA-Adapter showcases higher tuning efficiency with competitive reasoning capacity on MME~\citep{fu2023mme}, MMBench~\citep{liu2023mmbench}, and LVLM-eHub~\citep{xu2023lvlm} benchmarks. 
   
\end{itemize}

In addition to instruction tuning, our zero-initialized attention can be generalized to traditional vision and language tasks for parameter-efficient fine-tuning.
We apply our approach to the pre-trained ViT~\citep{dosovitskiy2020image}, ReBERTa~\citep{liu2019roberta}, and CLIP~\citep{radford2021learning}, respectively for fine-tuning vision, language, and vision-language models.
On a wide range of downstream tasks, we demonstrate the effectiveness of our proposed method for traditional tasks.

% \vspace{0.2cm}
\section{Related Work}


\paragraph{Instruction Tuning of Language Models.} 
The subfield of language models learning instruction-following capabilities aims to generate responses based on natural language commands.
These methods normally enhance the pre-trained LLMs by fine-tuning them with high-quality instruction-output data pairs. Early works, such as FLAN~\citep{wei2021finetuned}, PromptSource~\citep{bach2022promptsource}, and SUP-NATINST~\citep{wang2022super}, introduce effective instruction tuning methods and establish comprehensive evaluation benchmarks. 
% that outperforms non-tuned LLMs in unseen tasks. PromptSource~\citep{bach2022promptsource} provides a development environment with a web-based GUI, which manages natural language prompts for zero-shot and few-shot learning. SUP-NATINST~\citep{wang2022super} adopts multi-task training and 
% establish comprehensive evaluation benchmarks of diverse language tasks. 
InstructGPT~\citep{ouyang2022training} demonstrates significant improvement in the instruction-following power, but is closed-source to the community. To promote the open source of instruction models, Stanford Alpaca~\citep{alpaca} fine-tunes all the 7B parameters of LLaMA~\citep{touvron2023llama} with 52K self-instruct data.
However, this full-model fine-tuning can be inefficient in both time and computation resources, limiting its transferability to downstream applications.
In this paper, we propose LLaMA-Adapter to fine-tune only lightweight zero-initialized attention mechanisms on top of the frozen LLaMA, other than updating parameters of the entire model. 
There are several works \textbf{\textit{concurrent}} to ours, Alpaca-LoRA~\citep{alpaca_lora}, Vicuna~\citep{vicuna2023}, and LLaMA-GPT4~\citep{gpt4llm}, which aim to improve Alpaca from different aspects. Alpaca-LoRA utilizes the existing LoRA~\citep{hu2021lora} to efficiently fine-tune LLaMA, which is restricted to the original network structure and cannot be extended for image input. In contrast, our LLaMA-Adapter achieves higher training efficiency and can be simply generalized to a multi-modal LLM via zero-initialized attention. Vicuna and LLaMA-GPT4 target at constructing a more advanced instruction dataset using ChatGPT~\citep{OpenAI2023ChatGPT} and GPT-4~\citep{OpenAI2023GPT4TR}, instead of Alpaca's 52K data, which still adopt full fine-tuning without the potential for multi-modal instruction tuning.


% \paragraph{Large Vision-Language Models.}
% Over the past decade, we have witnessed a shift in vision-language research from task-specific models~\citep{vinyals2015show,karpathy2015deep,yang2016stacked,santoro2017simple,jiang2020defense} to large foundation models~\citep{radford2021learning,wang2021simvlm,bao2022vlmo,wang2022ofa,alayrac2022flamingo,wang2022git,wang2022image,li2023blip}. After pre-training on large-scale image-text data, the large vision-language models can be adapted to a variety of downstream tasks with powerful performance. A line of works~\citep{radford2021learning,wang2021simvlm,wang2022ofa} train both visual and textual encoders from scratch, leading to a high computation cost. Recently, another line of works~\citep{alayrac2022flamingo,wang2022git,li2023blip,zhai2022lit} adopt pre-trained unimodal models as initialization, and only train the newly introduced learnable modules. For example, LiT~\citep{zhai2022lit} utilizes a pre-trained vision backbone to speed up the CLIP~\citep{radford2021learning} training, and Frozen~\citep{tsimpoukelli2021multimodal} fine-tunes an image encoder to transform visual tokens into LLM's soft prompts. Similarly, CLIPCap~\citep{mokady2021clipcap} proposes a mapping network to connect the pre-trained image encoder with LLMs, and Flamingo~\citep{alayrac2022flamingo} inserts several cross-attention layers to inject visual knowledge into LLMs. With promising zero-shot results, BLIP2~\citep{li2023blip} connects pre-trained image encoders and LLMs with a Q-Former~\citep{}. CLIP-Adapter~\citep{gao2021clip}, Tip-Adapter~\citep{zhang2021tip,zhang2023prompt} and PointCLIP~\citep{zhu2022pointclip,zhang2022pointclip} introduce customized adapters upon CLIP for 2D and 3D few-shot learning. In summary, existing methods either adopt mapping networks or cross-attention mechanisms to connect two modalities with pre-trained vision-language models. Different from them, our LLaMA-Adapter only introduces a few learnable parameters and adapts an LLM, e.g., LLaMA~\citep{touvron2023llama}, to learn a multi-modal instruction model. With a simple but effective zero-initialized attention, we progressively inject visual information into the pre-trained language model, which contributes to superior vision-language reasoning.


\paragraph{Parameter-efficient Fine-tuning.} 
The pre-training and fine-tuning paradigms have been proven to be highly effective in different language and vision tasks. 
Compared to full fine-tuning, Parameter-Efficient Fine-Tuning (PEFT)~\citep{peft} methods freeze most parameters of pre-trained models, and aim to exhibit comparable capabilities on downstream tasks~\citep{wang2018glue,puzikov2018e2e}. 
% Various PEFT techniques have been explored, including prompt tuning~\citep{li2021prefix,lester2021power,liu2021gpt,liu2021p-tuning,qin2021learning}, Low-Rank Adaptation (LoRA)~\citep{hu2021lora, zhang2023adaptive,hedegaard2022structured}, and adapters~\citep{houlsby2019parameter,pfeiffer2020adapterfusion,lin2020exploring, chen2022vision,rebuffi2017learning}. 
Therein, prompt tuning appends a collection of trainable tokens to pre-trained large models, which are inserted either to the input embeddings~\citep{lester2021power, liu2021gpt} or every intermediate layer~\citep{li2021prefix, liu2021p-tuning}.
LoRA~\citep{hu2021lora,zhang2023adaptive,hedegaard2022structured} introduces trainable rank decomposition matrices into each network weights~\citep{karimi2021compacter}, indicating promising fine-tuning ability on large generative models~\citep{stable-diffusion-lora,alpaca_lora}. Adapters~\citep{houlsby2019parameter} insert lightweight adaption modules into each block of the transformer and have been extended across numerous domains~\citep{gesmundo2022munet,gao2021clip,zhang2021tip}.
Different from previous efforts, we propose the LLaMA-Adapter with zero-initialized attention specially designed for instruction tuning and multi-modal reasoning of LLaMA~\citep{touvron2023llama}.
% Our zero gating strategy can progressively inject the instructional cues in adaption prompts into the frozen LLaMA~\citep{touvron2023llama}.
Some existing works also adopt gating techniques in prompt tuning~\citep{yoo2023improving,goswami2023switchprompt}, but conduct a naive gated combination of different prompt tokens with randomly initialized factors. Instead, our gating factor learns from zero during training, and is delicately integrated into self-attention layers. Another branch of work applies zero initialization to convolutional networks~\citep{zhao2021zero}, text-to-image diffusion models (ControlNet~\citep{controlnet}), or vision-language learning (Flamingo~\citep{alayrac2022flamingo}). They are not PEFT methods requiring large-scale parameters, and have motivations for better network-level initialization or feature-level fusion via residual connections, very different from our interaction controlling within attention layers.



% or zero initialization techniques. However, 

% Moreover, we verify the generalization capacity of our approach for fine-tuning traditional vision and language models, which also obtains superior performance.


\section{LLaMA-Adapter}

In Section~\ref{s1}, we first introduce to insert learnable adaption prompts into LLaMA's~\citep{touvron2023llama} transformer. Then, we present the details of zero-initialized attention mechanisms with zero gating in Section~\ref{s2}, and generalize LLaMA-Adapter for multi-modal reasoning in Section~\ref{s3}. Finally, we extend our approach for efficient fine-tuning of language and vision models in Section~\ref{s4}.


\subsection{Learnable Adaption Prompts}
\label{s1}

Given a pre-trained LLaMA with an $N$-layer transformer, we first insert a set of learnable adaption prompts into its topmost $L$ layers ($L\leq N$).
We denote the prompts as $\{P_l\}_{l=1}^{L}$, where $P_l \in \mathbb{R}^{K\times C}$ with $K$ denoting the prompt length for each layer, and $C$ equaling the feature dimension of LLaMA's transformer. The prompting at last $L$ layers can better tune the language representations with higher-level semantics.

Taking the $l$-th inserted layer as an example ($l\leq L$), we denote the $M$-length word tokens as $T_l \in \mathbb{R}^{M\times C}$, which represent the input instruction and the already generated response. The learnable adaption prompt is concatenated with $T_l$ along the token dimension as prefixes, formulated as 
\begin{align}
    [P_l;\ T_l]\  \in \mathbb{R}^{(K+M)\times C}.
\end{align}
In this way, the instruction knowledge learned within $P_l$, can effectively guide $T_l$ to generate the subsequent contextual response via our zero-initialized attention layers in the transformer block.


\subsection{Zero-initialized Attention}
\label{s2}

\begin{wrapfigure}{r}{0.5\textwidth}
  \centering
  \vspace{-0.3cm}
    \includegraphics[width=0.99\linewidth]{figs/fig2_2.pdf}
   \caption{\textbf{Details of Zero-initialized Attention.} We insert learnable adaption prompts into the last $L$ out of $N$ transformer layers of LLaMA. To progressively learn the instructional knowledge, we adopt a zero gating factor within the attention for stable training in the early training stages.}
    \label{fig2}
    \vspace{0.5cm}
\end{wrapfigure}

If the adaption prompts are randomly initialized, they might bring disturbance to the word tokens at the beginning of training, which harms the fine-tuning stability and effectiveness. Considering this, we modify the vanilla self-attention at the last $L$ layers to be zero-initialized variants, as shown in Figure~\ref{fig2}. 
Suppose the model is generating the $(M+1)$-th word on top of $[P_l;\ T_l]$ at the $l$-th inserted layer, we denote the corresponding $(M+1)$-th word token as $t_l \in \mathbb{R}^{1\times C}$. In the attention mechanism, several linear projection layers are first applied to transform the input tokens into queries, keys, and values as
% \begin{align}
%     Q_l = \operatorname{Linear_{q}}(\ t_l\ );\ \ \
%     K_l, V_l = \operatorname{Linear_{kv}}(\ [P_l;\ T_l;\ t_l]\ ).
% \end{align}
\begin{align}
    Q_l &= \operatorname{Linear_{q}}(\ t_l\ );\\
    K_l &= \operatorname{Linear_{k}}(\ [P_l;\ T_l;\ t_l]\ );\\
    V_l &= \operatorname{Linear_{v}}(\ [P_l;\ T_l;\ t_l]\ ).
\end{align}
Then, the attention scores of $Q_l$ and $K_l$ before the softmax function are calculated as
\begin{align}
    S_l &= Q_l K_l^T/\sqrt{C}\ \in \mathbb{R}^{1\times (K+M+1)},
\end{align}
which records the feature similarities between the new word $t_l$ and all $K+M+1$ tokens. Meanwhile, $S_l$ can be reformulated by two components as 
\begin{align}
    S_l &= [S_l^K;\ S_l^{M+1}]^T,
\label{eq1}
\end{align}
where $S_l^K \in \mathbb{R}^{K\times 1}$ and $S_l^{M+1} \in \mathbb{R}^{(M+1)\times 1}$ denote the attention scores of $K$ adaption prompts and $M+1$ word tokens, respectively.
The former $S_l^K$ represents how much information the learnable prompt contributes to generating $t_l$, which probably causes disturbance in the early training stage.


To this end, we adopt a learnable gating factor, denoted as $g_l$, to adaptively control the importance of $S_l^K$ in the attention. Initialized by zero, $g_l$ can firstly eliminate the influence of under-fitted prompts, and then increase its magnitude for providing more instruction semantics to LLaMA.
Therefore, we independently apply the softmax functions to the two components in Equation~\eqref{eq1}, and multiply the first term by $g_l$, formulated as
\begin{align}
    S_l^g = \ [\operatorname{softmax}(S_l^K)\cdot \operatorname{tanh}(g_l);\ \ \operatorname{softmax}(S_l^{M+1})]^T,
\end{align}
where an activation function $\operatorname{tanh}(\cdot)$ is adopted to regulate the scale of $g_l$ to into -1$\sim$1.
The separate softmax functions ensure the second term to be irrelevant to the adaption prompts, and we do not multiply any coefficient to $\operatorname{softmax}(S_l^{M+1})$ to prevent the pre-trained knowledge from being disturbed, i.e., preserving its original probability distribution.
When $g_l$ is close to zero, it can mostly convey the originally pre-trained knowledge of LLaMA to token $t_l$ for a creditable generation. In practice, we adopt multiple $g_l$ to be independently learned for different heads within the attention, benefiting the learning diversity of multi-head mechanisms.

Finally, we calculate the output of the $l$-th attention layer with a linear projection layer as
\begin{align}
    t_{l}^o = \operatorname{Linear_{o}}(S_l^g V_l)\ \in \mathbb{R}^{1\times C}.
\end{align}
With our proposed zero-initialized attention, the adaption prompts can progressively inject the newly acquired instructional signals into the transformer, while simultaneously incorporating the pre-trained knowledge of LLaMA to provide high-quality responses.


\begin{figure*}[t!]
  \centering
    \includegraphics[width=\textwidth]{figs/fig3_3.pdf}
% \vspace{0.05cm}
   \caption{\textbf{Multi-modal LLaMA-Adapter.} By connecting a pre-trained image encoder, LLaMA-Adapter can be extended to a multi-modal LLM for image-conditioned instruction following. Given an image input, we element-wisely add the image tokens with adaption prompts, and utilize our zero-initialized attention mechanism to inject visual semantics into LLaMA~\citep{touvron2023llama}.}
    \label{fig3}
    % \vspace{-0.3cm}
\end{figure*}

\subsection{Multi-modal Reasoning}
\label{s3}

Apart from language instructions, LLaMA-Adapter is capable of answering a question based on image input with simple modifications. This fully unleashes the multi-modal reasoning power of LLMs for extensive application scenarios, e.g., image captioning, object counting, and OCR. The overall framework of our multi-modal LLaMA-Adapter is shown in Figure~\ref{fig3}.

% ScienceQA benchmark~\citep{scienceqa} as examples. Given \textcolor{zrrgreen}{\textbf{visual}} and \textcolor{zrrgreen}{\textbf{textual contexts}}, along with the corresponding \textcolor{zrrblue}{\textbf{question}} and \textcolor{zrrblue}{\textbf{options}}, the model is required to conduct multi-modal understanding to give the correct \textcolor{zrrred}{\textbf{answer}}.


\paragraph{Multi-modal Architecture.} For an input image, we first leverage a pre-trained visual encoder, e.g., CLIP~\citep{radford2021learning}, to extract its multi-scale global features, denoted as $\{I_m\}_{m=1}^{M}$, where $I_m \in \mathbb{R}^{1\times C_m}$ and $M$ denotes the scale number. Then, we concatenate the $M$-scale features along the channel dimension, and apply a learnable projection network to transform them into word embedding space, formulated as
\begin{align}
    I_p = \operatorname{Projection}\Big(\operatorname{Concat}\big(\{I_m\}_{m=1}^M\big)\Big),
\end{align}
where $I_p \in \mathbb{R}^{1\times C}$ and is regarded as the overall image token with the same feature dimension as our adaption prompts. 
After this, we repeat $I_p$ for $K$ times, and element-wisely add it onto the $K$-length adaption prompts at all $L$ inserted transformer layers. For the $l$-th layer, we denote the acquired multi-modal prompt as
\begin{align}
    P_l^v = P_l + \operatorname{Repeat}(I_p)\ \in \mathbb{R}^{K\times C},
\end{align}
where $P_l^v$ denotes the prompt incorporating visual information from the given image. After this, our zero-initialized attention can learn to increasingly infuse the image-conditional semantics into LLaMA by the zero gating factor $g_l$.
In this way, an LLM can be efficiently tuned to understand vision-language input, and tackle more challenging generative tasks with multi-modal reasoning.

% As a general framework, LLaMA-Adapter with additional input condition can also be extended to video and audio modalities. Using the pre-trained modal-specific encoders, we can integrate instructional signals of different modalities into the adaption prompts, which further maximizes the comprehension and generative capacity of LLaMA. We leave this as a future work.

\paragraph{Training Strategy.}
Instead of using Alpaca's data~\citep{alpaca} for language-only instruction tuning, we fine-tune LLaMA-Adapter with multi-modal instruction data, and evaluate the performance with two popular scenarios:

\begin{itemize}
   \item \textbf{ScienceQA~\citep{scienceqa} Evaluation.} ScienceQA includes a large-scale science question answering data collected from a wide range of knowledge domains. Each sample contains a visual context, a textual context, a question with multiple options, and an answer. We directly utilize ScienceQA's multi-modal training set to fine-tune LLaMA-Adapter, and conduct in-domain testing. We freeze both the image encoder and LLaMA, and only train the lightweight projection network and zero-initialized attention mechanisms.
   
   \item \textbf{Zero-shot Multi-modal Evaluation.} To verify the out-of-domain generation ability of our approach, we conduct a two-stage multi-modal training, and then evaluate three benchmarks (MME~\citep{fu2023mme}, MMBench~\citep{liu2023mmbench}, LVLM-eHub~\citep{xu2023lvlm}) in a zero-shot manner. For the first stage, we utilize the raw image-caption data from LAION-400M~\citep{schuhmann2021laion} to tune the projection network and zero-initialized attention modules. This step is mainly for the alignment of embedding space between visual features and word tokens. For the second stage, we freeze the projection network, and only tune the zero-initialized attention within LLaMA by a combination of Alpaca's data and LLaVA-I~\citep{llava}. The second stage aims to endow LLMs with the capability to generate detailed responses based on human instructions.
   
   
\end{itemize}


% \subsection{Zero-initialized Attention for other Large Models}
% \label{s4}

% Our approach, i.e., adaption prompts with zero-initialized attention, is not limited to the domain of instruction models, and can be further utilized to fine-tune large models in traditional vision and language tasks, exerting superior generalization capacity.

% \paragraph{Vision Models.}
% We select a pre-trained ViT~\citep{dosovitskiy2020image} as the foundation vision model for downstream image classification tasks. Similar to LLaMA, we insert the adaption prompts as prefix into the topmost $L$ transformer layers in ViT, and modify the attention operations to be zero-initialized at all inserted layers. By increasingly injecting the downstream visual semantics, we only introduce a few parameters on top of the frozen ViT, and attain comparable classification accuracy to full fine-tuning on VTAB-1k~\citep{zhai2019large} benchmark, which indicates our attention operator's efficacy in vision domains.

% \paragraph{Language Models.}
% We utilize RoBERTa~\citep{liu2019roberta} pre-trained on large-scale unlabeled text corpus, and evaluate our proposed zero-initialized attention on SQuAD~\citep{rajpurkar2016squad} benchmark for extractive question answering. We implement the zero-initialized attention on top of P-tuning v2~\citep{liu2021p-tuning}, a prompt tuning method for efficiently adapting large language models. Likewise, we only enable the prompt tokens in P-tuning v2 and our zero gating factors to be learnable during fine-tuning. The leading results demonstrate our superiority for traditional language tasks. Please refer to Supplementary Material for applying zero-initialized attention mechanisms to more large models and tasks.


% \vspace{-0.2cm}
\section{Experiment}

In Section~\ref{s4.1}, we first evaluate the language instruction-following capacity of LLaMA-Adapter. Then, we present our multi-modal reasoning performance on several benchmarks in Section~\ref{s4.2}, and conduct ablation studies on ScienceQA's validation set in Section~\ref{s4.3}. Finally, we report the fine-tuning results of our approach on traditional vision and language models in Section~\ref{s4.4}.

% \input{tables/instruction_comparisons_short}

\begin{figure*}[t!]
    \centering
    % \vspace{0.3cm}
    \includegraphics[width=0.93\textwidth]{figs/compare_v2.pdf}
    \caption{\textbf{Language Instruction-following Capacity.} Our LLaMA-Adapter performs comparably with Alpaca~\citep{alpaca} for question answering, language translation, and code generation.}
    \label{tab:instruction_comparisons_short}
    % \vspace{-1cm}
\end{figure*}

\subsection{Instruction-following Evaluation}
\label{s4.1}

\paragraph{Settings.} 
Following Stanford Alpaca~\citep{alpaca}, we utilize 52K instruction-following data for training.
We fine-tune LLaMA-Adapter on 8 A100 GPUs for 5 epochs. The warmup epochs, batch size, learning rate, and weight decay are set to 2, 64, 0.009, and 0.02, respectively. By default, we utilize the pre-trained LLaMA model with 7B parameters and $N=32$ transformer layers. We adopt a prompt length $K=10$ and insert the adaption prompts into the last $L=30$ layers. 
% In the generation stage, we adopt \textit{top-p} sampling~\citep{top_p_sample} as the default decoding method with a temperature $0.1$ and a $\textit{top-p}=0.75$. 
For quantitative results, we compare with methods both trained by the 52K instruction data, Alpaca~\citep{alpaca} and Alpaca-LoRA~\citep{alpaca_lora}, and evaluate with one widely adopted schemes, GPT-4 evaluating benchmark~\citep{vicuna2023}. It adopts GPT-4~\citep{OpenAI2023GPT4TR} to assess the quality of two compared responses from different models on 80 questions.

\paragraph{Performance.} 
We first show some generated responses of LLaMA-Adapter and Alpaca in Figure~\ref{tab:instruction_comparisons_short}.
For different kinds of instructions, our approach can output reasonable responses comparable to the fully fine-tuned Alpaca, including question answering, language translation, and code generation. 
Please refer to the Appendix for a full comparison with Alpaca-LoRA, GPT-3~\citep{brown2020language}, and LLaMA-I~\citep{touvron2023llama}.
For GPT-4 assessment in Figure~\ref{fig:quantitative_comparison}, LLaMA-Adapter obtains more `win' compared to Alpaca and Alpaca-LoRA, respectively. 
This fully demonstrates the effectiveness of our adaption method with zero-initialized attention mechanisms. 

\input{tables/training_efficiency}

% \begin{table}[t]
% \caption{\textbf{Results on Open LLM Benchmark~\citep{open-llm-leaderboard}.}}
% \centering
% % \resizebox{\textwidth}{!}{%
% \small
% \begin{tabular}{l|c|cccc}
% \toprule
% Method        & Avg  & ARC  & HellaSwag & MMLU & TruthfulQA \\
% \midrule
% Alpaca        & 48.7 & 49.1 & 77.7      & 33.8 & 36.3       \\
% Alpaca-LoRA   & 49.3 & 49.5 & 77.3      & 35.8 & 34.5       \\
% LLaMA-Adapter & \textbf{49.6} & 51.5 & 75.4      & 35.1 & 36.2       \\
% \bottomrule
% \end{tabular}%
% % }
% \label{tab:open_llm_benchmark}
% \end{table}


\paragraph{Efficiency.} 
In Table~\ref{tab:training_efficiency}, we compare the learnable parameters, storage space, and training time of different instruction-following methods. 
As a lightweight plug-and-play module, LLaMA-Adapter enjoys superior training efficiency with only 1.2M parameters, 4.9M storage, and one-hour training. This enables more efficient storage of large-scale language models on mobile devices.
LLaMA-Adapter's efficiency advantages can be further revealed by multi-node training, since only the gradients of 1.2M parameters are required to be transferred among nodes, other than Alpaca's 7B.

\subsection{Multi-modal Evaluation}
\label{s4.2}

\input{tables/scienceqa}

% \begin{table}[t]
% \caption{\textbf{Zero-Shot Multimodal Evaluation on MME~\citep{fu2023mme}, MMbench~\citep{liu2023mmbench} and LVLM-eHub~\citep{xu2023lvlm} Benchmarks.} Note that we only give the overall performance of each benchmark. Please refer to our appendix for detailed metrics on each subtask.}
% \centering
% % \resizebox{\textwidth}{!}{%
% \small
% \begin{tabular}{l|cccc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{2}{c}{MME} & \multirow{2}{*}{MMbench} & \multirow{2}{*}{LVLM-eHub} \\
%                        & Perception  & Cognition &                          &                            \\
% \midrule
% LLaVA                  & 503         & 215       & 36.2                     & 0.64                       \\
% Mini-GPT4              & 867         & 292       & 23.0                     & 0.55                       \\
% \textbf{LLaMA-Adapter}          & \textbf{973}         & \textbf{249}       & \textbf{39.5}                     & \textbf{0.67}                      \\
% \bottomrule
% \end{tabular}%
% % }
% \label{tab:mm_results}
% \end{table}

\begin{table}[t]
\vspace{-0.4cm}
\caption{\textbf{Zero-shot Multi-modal Evaluation} on MME~\citep{fu2023mme}, MMBench~\citep{liu2023mmbench} and LVLM-eHub~\citep{xu2023lvlm} benchmarks.
 P: Perception; C: Cognition. LR: Logical Reasoning; AR: Attribute Reasoning; RR: Relation Reasoning; FP-C/S: Fine-grained Perception (Cross Instance/Single Instance); CP: Coarse Perception. VP: Visual Perception; VKA: Visual Knowledge Acquisition; VR: Visual Reasoning; VC: Visual Commonsense.}
\centering
% \footnotesize
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cc|c|cccccc|cccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{MME}     & \multicolumn{7}{c|}{MMbench}                             & \multicolumn{4}{c}{LVLM-eHub}             \\
                       & P            & C            & All           & LR   & AR   & RR   & FP-S & FP-C & CP   & VP   & VKA  & VR   & VC   \\
                       \midrule
LLaVA                  & 503          & 215          & 36.2          & 15.9 & 53.6 & 28.6 & 41.8 & 20.0 & 40.4 & 0.62 & 0.38 & 0.77 & 0.79 \\
Mini-GPT4              & 867          & 292          & 23.0          & 13.6 & 32.9 & 8.9  & 28.7 & 11.2 & 28.3 & 0.73 & 0.35 & 0.53 & 0.57 \\
\textbf{LLaMA-Adapter} &973 & 249          & 39.5 & 13.1 & 47.4 & 23.0 & 45.0 & 33.2 & 50.6 & 0.81 & 0.44 & 0.83 & 0.59 \\
\bottomrule
\end{tabular}%
}
\label{tab:mm_results}
\end{table}

\begin{figure*}[t]
\centering
\vspace{0.2cm}
\includegraphics[width=\textwidth]{figs/fig1_f.pdf}
\caption{\textbf{Multi-modal Reasoning Examples} on zero-shot open-domain questions. We compare our approach with LLaVA~\citep{llava} and MiniGPT-4~\citep{zhu2023minigpt} for object counting, OCR, and common sense reasoning.}
\label{fig:multimodal}
\end{figure*}

\begin{figure*}[t]
% \quad\quad\hspace{0.2cm}
\begin{minipage}[t]{0.3\linewidth}
\centering
 \small
\tabcaption{\textbf{Number of Insertion Layers} to the pre-trained transformer of LLaMA.}
\label{tab:layers_vs_acc}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{c|cc}
\toprule
Layers & Params & Val Acc.  \\
\midrule
10                & 0.97          & 55.95             \\
20                & 1.37          & 73.36        \\
30                & 1.79          & \textbf{83.85}  \\
32                & 1.83          & 81.03\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{minipage}
\qquad
\begin{minipage}[t]{0.23\linewidth}
\centering
 \small
\tabcaption{Effectiveness of \textbf{Zero-initialized Attention} in our method.}
\label{tab:zero_init_acc}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{l|c}
\toprule
Setting &  Val Acc.\\
\midrule
Rand-Init.   & 40.77           \\
Zero-Init.\vspace{0.05cm}   & \textbf{83.85} \\
\textit{\textcolor{blue}{Gain}} &\textcolor{blue}{+43.08}\\
\bottomrule
\end{tabular}%
\end{adjustbox}
\end{minipage}
\qquad
\begin{minipage}[t]{0.35\linewidth}
\centering
 \captionof{figure}{\textbf{Loss Curves} of LLaMA-Adapter with (blue) and without (orange) zero-initialized attention.
}
\includegraphics[width=1\textwidth]{figs/65b_demo4.pdf}
\label{fig:zero_init_loss}
\end{minipage}
\end{figure*}


% \begin{figure*}[t]
% \hspace{0.1cm}
% \begin{minipage}[t]{0.42\linewidth}
% \centering
%  \captionof{figure}{\textbf{Loss Curves} with (blue) and without (orange) zero-initialized attention.
% }
% \includegraphics[width=1\textwidth]{figs/65b_demo4.pdf}
% \label{fig:zero_init_loss}
% \end{minipage}
% \quad\quad\quad\hspace{0.3cm}
% \begin{minipage}[t]{0.43\linewidth}
% \centering
%  \small
% % \vspace{0.03cm}
% \tabcaption{\textbf{Robustness to Over-fitting.} We compare the training loss, validation loss, and validation accuracy of LLaMA-Adapter in different training epochs.}
% \label{tab:loss_vs_acc}
% \begin{adjustbox}{width=\linewidth}
% \begin{tabular}{ccc|c}
% \toprule
% Epoch      & Train Loss    & Val Loss    & Val Acc (\%) \\ \midrule
% 15         & 0.022         & 0.136         & 82.08             \\
% 30         & 0.004         & 0.241         & 83.85             \\
% 60         & 0.001         & 0.282         & \textbf{83.94}            \\ \bottomrule
% \end{tabular}%
% \end{adjustbox}
% \end{minipage}
% \end{figure*}


\begin{figure*}[t]
\begin{minipage}[t]{0.32\linewidth}
\centering
 \small
\tabcaption{\raggedright{\textbf{Vision Model Fine- tuning}} with ViT on VTAB-1k benchmark.}
\label{t_6}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{l cc c}
	\toprule
        \makecell*[l]{Method}
        &Natural &Special. &Struct.\\
		\cmidrule(lr){1-1}
  \cmidrule(lr){2-4}
         Full  & 75.88 & 83.36 & 47.64\\
         % Bias  & 73.30 & 78.25 & 44.09\\
         Adapter  & 70.39 & 77.11 & 33.43\\
         Sidetune &58.21 &68.12 &23.41\\
         VPT\vspace{0.05cm} & 78.48 & 82.43 & 54.98\\
         % \cmidrule(lr){1-4}
         \bf Zero-init.  &\bf81.74 & \bf84.43 & \bf56.75\\
        \bottomrule
	\end{tabular}
\end{adjustbox}
\end{minipage}
% \hfill
\quad
% \hspace{0.4cm}
\begin{minipage}[t]{0.32\linewidth}
\centering
 \small
% \vspace{0.03cm}
\tabcaption{\textbf{\raggedright{Language Model Fine-}tuning} with RoBERTa on SQuAD benchmark.}
\label{t_7}
\begin{adjustbox}{width=\linewidth}
             \begin{tabular}{l cccc}
                \toprule
                    \multirow{2}{*}{Method}
                    &\multicolumn{2}{c}{SQuAD 1.1} &\multicolumn{2}{c}{SQuAD 2.0}\\
                    \cmidrule(lr){2-3}  \cmidrule(lr){4-5}
                    &EM &F1 &EM &F1\\
                    \cmidrule(lr){1-1}
                    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                     Full                                 & 88.9     & 94.6               & 86.5     & 89.4               \\
                     PT            & 1.2      & 12.0               & 50.2     & 50.2               \\
                     PT2     & 88.5     & 94.4               & 82.1     & 85.5               \\
                     % PT2$^*$                     & 88.1     &94.2               & 81.3     & 84.7               \\
                     \bf Zero-init.                        & \bf 88.8 & \bf 94.6           & \bf 83.9 &  \bf 87.2           \\  
                     
                    \bottomrule
            \end{tabular}
\end{adjustbox}
\end{minipage}
\quad
\begin{minipage}[t]{0.28\linewidth}
\centering
 \small
\tabcaption{\textbf{Vision-language Fine-tuning} with CLIP on base-to-novel benchmark.}
\label{t_8}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{l cc c}
	\toprule
        \makecell*[l]{Method}
        &Base &Novel &HM\\
		\cmidrule(lr){1-1}
  \cmidrule(lr){2-4}
         CLIP  & 75.88 & 83.36 & 47.64\\
         CoOp  & 70.39 & 77.11 & 33.43\\
         CoCop &58.21 &68.12 &23.41\\
         MaPLe\vspace{0.05cm} & 78.48 & 82.43 & 54.98\\
         % \cmidrule(lr){1-4}
         \bf Zero-init.  &\bf81.74 & \bf84.43 & \bf56.75\\
        \bottomrule
	\end{tabular}
\end{adjustbox}
\end{minipage}
\end{figure*}


% \begin{figure*}[t]
% \quad\quad
% \begin{minipage}[t]{0.36\linewidth}
% \centering
%  \captionof{figure}{\textbf{Loss Curves} 
% We
% }
% \includegraphics[width=\textwidth]{figs/65b_demo3.pdf}
% \label{fig:zero_init_loss}
% \end{minipage}
% \quad\quad\quad\quad\hspace{0.1cm}
% \begin{minipage}[t]{0.43\linewidth}
% \centering
%  \small
% % \vspace{0.03cm}
% \tabcaption{\textbf{Ablation on Zero-initialized Attention.}}
% \label{t_ab2}
% \begin{adjustbox}{width=\linewidth}
% \begin{tabular}{ccc|c}
% \toprule
% Epoch      & Train Loss    & Val Loss    & Val Acc (\%) \\ \midrule
% 15         & 0.022         & 0.136         & 82.08             \\
% 30         & 0.004         & 0.241         & 83.85             \\
% 60         & 0.001         & 0.282         & \textbf{83.94}            \\ \bottomrule
% \end{tabular}%
% \end{adjustbox}
% \end{minipage}
% \end{figure*}


\paragraph{Settings.} 
We adopt CLIP~\citep{radford2021learning} as the image encoder to extract multi-scale visual features, and leverage a simple bottleneck MLP layer as the learnable projection network. We keep other hyperparameters the same as the language instruction-following LLaMA-Adapter. For ScienceQA~\citep{scienceqa}, we concatenate the given question, textual context, and options sequentially in one sentence as LLaMA's input. 
% We compare with multiple traditional visual question answering (VQA) methods, along with the zero-shot performance of GPT series~\citep{OpenAI2023ChatGPT,OpenAI2023GPT4TR}. 
For zero-shot multi-modal evaluation, we select three benchmarks, MME~\citep{fu2023mme}, MMBench~\citep{liu2023mmbench}, and LVLM-eHub~\citep{xu2023lvlm}, covering a wide range of VQA tasks. We compare with two concurrent multi-modal LLMs: LLaVA~\citep{llava} and MiniGPT-4~\citep{zhu2023minigpt}.


% We train the multi-modal LLaMA-Adapter on ScienceQA~\citep{scienceqa}, a large-scale multi-modal science question answering dataset collected from a wide range of knowledge domains.
% % \footnote{\url{https://scienceqa.github.io}}. 
% Each example contains a \textcolor{zrrgreen}{\textbf{visual context}}, a \textcolor{zrrgreen}{\textbf{textual context}}, a \textcolor{zrrblue}{\textbf{question}}, multiple \textcolor{zrrblue}{\textbf{options}}, and an \textcolor{zrrred}{\textbf{answer}}.
% We omit the lecture and explanation in some data samples for simplicity.
% In our LLaMA-Adapter, we concatenate the given question, textual context, and options sequentially in one sentence as LLaMA's input.
% In the generation stage, we adopt greedy search as the decoding method. 
% We keep other hyperparameters the same as the instruction-following LLaMA-Adapter.

% \vspace{0.1cm}

\paragraph{Performance.}
In Table~\ref{tab:scienceqa} for the ScienceQA performance, our single-modal `LLaMA-Adapter$_T$' attains 78.31\% accuracy, surpassing several traditional VQA methods with large parameters. By further injecting visual conditions with a 0.6M projection network, our multi-modal `LLaMA-Adapter' improves +6.88\% accuracy, attaining leading results superior to the GPT series.
In Table~\ref{tab:mm_results} for the three multi-modal benchmarks, compared to the concurrent works, our approach achieves competitive scores with a much more efficient tuning strategy. This is because, LLaVA requires fine-tuning the entire 7B LLM, and Mini-GPT4 adopts Vicuna~\citep{vicuna2023} that also fully fine-tunes LLaMA with 13B parameters.
We also show some multi-modal reasoning examples in Figure~\ref{fig:multimodal}. Our approach exhibits better object counting, OCR, and commence reasoning performance.


% GPT-3 can perform zero-shot and few-shot question answering by constructing a suitable prompt, but its total parameters are much large than LLaMA-Adapter (175B \emph{vs.} 7B). Besides, as a language model, GPT-3 can not leverage any visual information. In contrast, LLaMA-Adapter can easily switch between single modal and multi-modal. 

% As shown in Tab.~\ref{tab:scienceqa}, our single modal method (LLaMA-Adapter$_T$) achieves 78.31\% accuracy. By injecting visual tokens into LLaMA-Adapter, our multi-modal method can further bring 7\% accuracy improvement. 

% Moreover, we notice that MM-CoT~\citep{zhang2023multicot} is on par with our method, but it relies on a complex two-stage inference method. We believe our LLaMA-Adapter can also be boosted and leave the exploration of chain-of-thought for future research.

% \vspace{0.2cm}
\subsection{Ablation Study}
\label{s4.3}
% We investigate the designs in LLaMA-Adapter 
\paragraph{Insertion Layers.} 
We first investigate the number of transformer layers to be inserted by zero-initialized attention in LLaMA-Adapter. As shown in Table~\ref{tab:layers_vs_acc}, increasing the layer numbers introduces more parameters, but leads to a large improvement in the answering accuracy of ScienceQA's validation set. There also exists an optimal insertion number from the higher layers, since too many layers would adversely disturb the early encoding of input words. If one has limited resources to identify the best number, simply inserting into all transformer layers is generally a good solution.


\paragraph{Zero-initialized Attention.} 
Our proposed zero-initialized attention is essential for the early-stage training stability and final generation capacity. As shown in Table~\ref{tab:zero_init_acc}, it contributes to a significant +43.08\% gain on ScienceQA's validation set. In contrast, the randomly initialized baseline only achieves 40.77\% accuracy, nearly the same as `Random Choice' (Table~\ref{tab:scienceqa}'s first row).
In Figure~\ref{fig:zero_init_loss}, we plot the loss curves with and without the zero initialization. The loss of `Zero-initialized' declines much faster at the beginning, and finally converges to zero. In contrast, the `Random-initialized' slowly approaches 0.15, which is not fully converged and causes a large performance drop.



% \paragraph{Robustness to Over-fitting.} 
% As the fine-tuning data of large language models is normally much smaller-scale than the pre-training data, researchers have to carefully tune a set of hyperparameters to avoid over-fitting. In Table~\ref{tab:loss_vs_acc}, we show our LLaMA-Adapter is relatively robust to the over-fitting issue. Similar to the conclusion in~\citep{ouyang2022training}, even if our model has over-fitted the fine-tuning data, e.g., the validation loss marginally varies from 0.136 (15 epochs) to 0.282 (60 epochs), the validation accuracy is still increasing, e.g., from 82.08\% to 83.94\%. This is because, LLaMA-Adapter keeps the pre-trained LLaMA 7B model frozen, and only learns lightweight adapters with a few parameters. 
\subsection{Zero-initialized Attention for other Large Models}
\label{s4.4}

Our approach, i.e., zero-initialized attention, is not limited to the domain of tuning instruction models, and can be further utilized to fine-tune large models in traditional vision and language tasks.

\paragraph{Vision Models.}
We select a pre-trained ViT/16~\citep{dosovitskiy2020image} as the vision model and evaluate on VTAB-1k~\citep{zhai2019large} benchmark, which contains 19 visual tasks with three domains: Natural, Specialized, and Structured. As shown in Table~\ref{t_6}, for various image distributions, e.g., natural images, medical and satellite imagery, our approach performs much better than the full fine-tuning, and also surpasses existing parameter-efficient methods~\citep{jia2022visual,houlsby2019parameter,zhang2020side}, indicating our generalization ability for vision tasks.


\paragraph{Language Models.}
We utilize a pre-trained RoBERTa$_\mathrm{large}$~\citep{liu2019roberta} and adopt SQuAD~\citep{rajpurkar2016squad} v1.1 and v2.0 benchmarks for extractive question answering evaluation. Exact Match (EM) and F1 scores on the dev set are reported. We refer to the Appendix for other language tasks.
As shown in Table~\ref{t_7}, the leading results among previous methods~\citep{lester2021power,liu2021p-tuning} demonstrate our superiority over traditional language tasks.

\paragraph{Vision-language Models.}
We adopt CLIP~\citep{radford2021learning} as the pre-trained vision-language model, and test on base-to-novel generalization~\citep{zhou2022conditional} benchmark, where `HM' denotes harmonic mean. As shown in Table~\ref{t_8}, compared to previous works~\citep{cocoop,zhou2022learning,khattak2022maple}, our approach achieves the best average classification accuracy on both base and novel categories, demonstrating our fine-tuning capability for large vision-language models.




% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\textwidth]{figs/zero_init_loss.pdf}
%     \caption{\textbf{Loss Curves with and without Zero-init Attention.} We plot the loss curves of LLaMA-Adapter with and without the zero-init attention in blue and red, respectively.}
%     \label{fig:zero_init_loss}
% \end{figure}

% \input{tables/loss_vs_acc}

% \footnotetext[1]{Reported by PT2's~\href{https://github.com/THUDM/P-tuning-v2}{official GitHub repository}, over which we implement our method.}

\section{Conclusion}
In this paper, we propose LLaMA-Adapter, an efficient adaption method for tuning instruction-following models. 
For better training stability and final performance, we introduce the zero-initialized attention mechanism with a learnable gating factor, which increasingly incorporates instructional signals, while preserving the pre-trained knowledge in LLaMA. 
With only 1.2M parameters and one-hour training, our approach effectively fine-tunes LLaMA with superior efficiency compared to the 7B-parameter Alpaca.
LLaMA-Adapter can be generalized to image-conditioned generation as a multi-modal LLM, achieving competitive results on various visual question answering benchmarks.
On traditional vision and language tasks, our zero-initialized attention also attains favorable fine-tuning performance, which indicates strong generalization capacity.
% \textbf{Limitation:} as our multi-modal variant presents a generic paradigm for incorporating external semantics, we will further extend LLaMA-Adapter to serve as a unified multi-modal framework, conditioned on a wide range of instructions, such as video, audio, and point clouds. We do not foresee negative social impact from the proposed work.

\section{Acknowledgement}

This work is partially supported by the National Key R\&D Program of China (NO.2022ZD0161100), the National Natural Science Foundation of China (No.62206272), the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)s InnoHK, and General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\newpage
\appendix

% \vspace{-0.4cm}
\section{Overview}
\begin{itemize}
    \item Section~\ref{A}: Detailed results of zero-shot multi-modal evaluation.
    \item Section~\ref{E}: Additional related work.
    \item Section~\ref{B}: Detailed results of fine-tuning traditional vision and language models.
    \item Section~\ref{F}: Additional experiments and discussion.
    \item Section~\ref{C}: Full comparison of instruction-following models.
    \item Section~\ref{D}: Comparison of LLaMA-Adapter and LLaMA-I.
\end{itemize}

\section{More Details of Multi-modal Evaluation}
\label{A}

\paragraph{ScienceQA~\citep{scienceqa} Evaluation.} The data sample in ScienceQA contains a visual context, a textual context, a question, multiple options, and a correct answer, as shown in Figure~\ref{fig:scienceqa_examples}.
We omit the lecture and explanation in some data samples for simplicity.

\paragraph{Zero-shot Multi-modal Evaluation.} We test our approach on the three benchmarks~\citep{fu2023mme,liu2023mmbench,xu2023lvlm} following their official procedures. In Tables~\ref{tab:mme_perception},~\ref{tab:mme_cognition} and~\ref{tab:lvlm_ehub}, we respectively report the detailed results of MME and LVLM-eHub benchmarks. As shown, across a wide range of visual question-answering problems, our approach can consistently achieve competitive results. We also show more examples of the multi-modal LLaMA-Adapter for open-domain zero-shot visual questions in Figure~\ref{fig:mutlimodal_demo_a1}, where our approach can generate detailed and high-quality responses in natural language.
The experiments fully demonstrate the generalization capacity of our proposed multi-modal LLM. We also give some qualitative examples in Figures~\ref{fig:mutlimodal_demo_a1} and~\ref{fig:mutlimodal_demo_a2}, where our LLaMA-Adapter can answer open-ended questions for web images.


\begin{table}[h]
\vspace{0.3cm}
\caption{\textbf{Perception Results on MME Benchmark~\citep{fu2023mme}.}}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|cccccccccc}
\toprule
Model         & ALL & Existence & Count & Position & Color & Poster & Celebrity & Scene & Landmark & Artwork & OCR \\
\midrule
LLaVA         & 503 & 50        & 50    & 50       & 55    & 50     & 49        & 50    & 50       & 49      & 50  \\
MiniGPT-4     & 867 & 115       & 123   & 82       & 110   & 56     & 65        & 96    & 69       & 56      & 83  \\
LLaMA-Adapter & 973 & 120       & 50    & 48       & 75    & 100    & 86        & 149   & 150      & 70      & 125 \\
\bottomrule
\end{tabular}%
}
\label{tab:mme_perception}
\end{table}

\begin{table}[h]
\caption{\textbf{Cognition Results on MME Benchmark~\citep{fu2023mme}.}}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|cccc}
\toprule
Model         & ALL & Commonsense Reasoning & Numerical Calculation & Text Translation & Code Reasoning \\
\midrule
LLaVA         & 215 & 57                    & 50                    & 58               & 50             \\
MiniGPT-4     & 292 & 72                    & 55                    & 55               & 110            \\
LLaMA-Adapter & 249 & 81                    & 63                    & 50               & 55            \\
\bottomrule
\end{tabular}%
}
\label{tab:mme_cognition}
\end{table}

% \begin{table}[h]
% \caption{\textbf{Zero-shot Multi-modal Results on MMbench~\citep{liu2023mmbench}.} LR: Logical Reasoning; AR: Attribute Reasoning; RR: Relation Reasoning; FP-C/S: Fine-grained Perception (Cross Instance/Single Instance); CP: Coarse Perception.}
% \centering
% % \resizebox{\textwidth}{!}{%
% \small
% \begin{tabular}{l|c|cccccc}
% \toprule
% Model         & Overall & LR   & AR   & RR   & FP-S & FP-C & CP   \\
% \midrule
% MiniGPT-4     & 23.0    & 13.6 & 32.9 & 8.9  & 28.7 & 11.2 & 28.3 \\
% LLaVA         & 36.2    & 15.9 & 53.6 & 28.6 & 41.8 & 20.0 & 40.4 \\
% LLaMA-Adapter & 39.5    & 13.1 & 47.4 & 23.0 & 45.0 & 33.2 & 50.6 \\
% \bottomrule
% \end{tabular}%
% % }
% \label{tab:mmbench}
% \end{table}

\begin{table}[h]
\caption{\textbf{Zero-shot Multi-modal Results on LVLM-eHub Benchmark~\citep{xu2023lvlm}.} OC: Object Counting; MCI: Multi-Class Identification; KIE: Key Information Extraction; VE: Visual Entailment; KGID: Knowledge-grounded Image Description; VCR: Visual Commonsense Reasoning.}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|c|ccc}
\toprule
LVLM-eHub                    & Tasks             & \#Datasets & LLaVA & MiniGPT-4 & LLaMA-Adapter \\
\midrule
Visual Perception            & ImgCls, OC, MCI   & 8         & 0.62  & 0.73      & 0.81          \\
Visual Knowledge Acquisition & OCR, KIE, Caption & 17        & 0.38  & 0.35      & 0.44          \\
Visual Reasoning             & VQA, KGID, VE     & 13        & 0.77  & 0.53      & 0.83          \\
Visual Commonsense           & ImageNetVC, VCR   & 6         & 0.79  & 0.57      & 0.59         \\
\midrule
Average                      & -                 & 44        & 0.64  & 0.55      & 0.67        \\
\bottomrule
\end{tabular}%
}
\label{tab:lvlm_ehub}
\end{table}


\begin{figure*}[h]
    \centering
    \vspace{-0.6cm}
    \includegraphics[width=\textwidth]{figs/scienceqa_examples.pdf}
    \caption{\textbf{Multi-modal Reasoning Examples in ScienceQA~\citep{scienceqa}.} }
    \label{fig:scienceqa_examples}
    % \vspace{-1cm}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    % \vspace{0.3cm}
    \includegraphics[width=\textwidth]{figs/multimodal_demo_a1.pdf}
    \caption{\textbf{Zero-shot Multi-modal Understanding Examples of LLaMA-Adapter: Part 1.} }
    \label{fig:mutlimodal_demo_a1}
    % \vspace{-1cm}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    % \vspace{0.3cm}
    \includegraphics[width=\textwidth]{figs/multimodal_demo_a2.pdf}
    \caption{\textbf{Zero-shot Multi-modal Understanding Examples of LLaMA-Adapter: Part 2.} }
    \label{fig:mutlimodal_demo_a2}
    % \vspace{-1cm}
\end{figure*}

\section{Additional Related Work}
\label{E}

\paragraph{Multi-modal Language Models.}
With the continuous improvement of data scale and computing power, the advancement of Multi-Modal Language Models (MMLMs) has gained momentum. Initiatives like CLIP~\citep{radford2021learning}, ALIGN~\citep{jia2021scaling}, and their derivatives~\citep{li2022grounded,gao2021clip,zhai2022lit} employ vision-language contrastive pre-training on vast datasets, showcasing robust generalization in zero-shot evaluation. With the rise of LLMs~\citep{OpenAI2023ChatGPT,OpenAI2023GPT4TR}, modern MMLMs merge these LLM architectures with visual comprehension capacities. BLIP-2~\citep{li2023blip}, for instance, introduces a Q-Former network, bridging frozen image encoders with LLMs. Flamingo~\citep{alayrac2022flamingo} uses interleaved image-text data for few-shot learning, enhancing vision-language inferences. Kosmos~\citep{huang2023kosmos} trains an MMLM on web-scale multi-modal data from scratch, enabling powerful visual perception capacities. While models like Bard~\citep{bard} and GPT-4~\citep{OpenAI2023GPT4TR} remain influential, their closed-source nature has led to the development of MMLMs such as those based on open-source LLaMA~\citep{llava,zhu2023minigpt,ye2023mplug,li2023otter,2023vpgtrans}. Typically, these MMLMs utilize a two-stage training process. In the initial phase, a substantial quantity of image-text pairs are leveraged to align vision models with LLMs. The subsequent phase involves fine-tuning on a limited set of high-quality datasets to follow human instructions. However, these models are either highly dependent on a fine-tuned instruction model (Vicuna~\citep{vicuna2023} in Mini-GPT4~\citep{zhu2023minigpt}), or require updating the entire parameters of LLMs (LLaVA~\citep{llava}). Follow-up works like SPHINX series~\citep{lin2023sphinx,gao2024sphinx}, ImageBind-LLM~\citep{han2023imagebind}, Point-LLM~\citep{guo2023point}, and MathVerse~\citep{zhang2024mathverse} further explore more diverse potentials of multi-modal capabilities within LLMs.
As a concurrent work to LLaVA and Mini-GPT4, our LLaMA-Adapter utilizes zero-initialized attention mechanisms for parameter-efficiently fine-tuning the original LLaMA model, largely saving the expensive full-parameter fine-tuning.

\paragraph{Comparison to Flamingo~\citep{alayrac2022flamingo}.}
As a strong in-context MLLMs, Flamingo adopts a gating strategy for injecting external knowledge into LLMs. Compared to our zero-initialized attention, there are three main differences as follows.

\begin{itemize}
   \item \textbf{Inserted Position.} 
   Our gating works delicately within the self-attention layer of an LLM, more specifically, after the query-key attention scores and before multiplying with value. In contrast, Flamingo's gating is outside and before feeding into LLM's layers, which works right after the newly added cross-attention layer and feed-forward networks.

   \item \textbf{Detailed Mechanism.} Our gating directly reweighs the attention scores of adaption prompts, controlling how much information of prompts is aggregated by the generating word token. Flamingo's gating naively reweighs the residual connection, which controls how much information of visual features is added to all language features.

    \item \textbf{Parameter Efficiency.} Our gating mechanism only introduces 1.2M parameters of efficient learnable prompts. Flamingo's gating is based on newly added large-scale cross-attention layers and FFNs, having over 3B parameters.

    \item \textbf{Application Scenarios.} Due to our lightweight designs, the zero-initialized attention can be adopted either for incorporating language instruction knowledge, or multi-modal image conditions, while Flamingo is specially designed for vision-language tasks by newly adding heavyweight cross-attention modules.
\end{itemize}

% Therefore, our zero gating is more efficient and functions in a different way to Flamingo.

\section{More Detailed Results of Model Fine-tuning}
\label{B}

In this section, we provide more detailed experiments and analysis of applying our zero-initialized attention to fine-tune vision models, language models, and vision-language models, respectively.

\subsection{Detailed Results on Vision Models}

In Table~\ref{tab:vtab_per_task}, we compare the detailed fine-tuning results on VTAB-1k~\citep{zhai2019large} benchmark with 19 downstream visual tasks, which can be categorized into Natural (7 tasks), Specialized (4 tasks), and Structured (8 tasks), according to image domains. As shown, our zero-initialized attention outperforms VPT~\citep{jia2022visual} on most datasets (16 out of 19), and surpasses full fine-tuning along with other fine-tuning methods by large margins. This demonstrates the general efficacy of the proposed mechanism on a variety of image distributions.

\begin{table*}[h]
\centering
\caption{\textbf{Detailed Fine-tuning Results on VTAB-1k Benchmark.} We report the top-1 accuracy and adopt ViT-B/16~\citep{dosovitskiy2020image} pre-trained on supervised ImageNet-21k~\citep{deng2009imagenet} as the base model. We compare our zero-initialized attention with Bias~\citep{zaken2022bitfit}, Adapter~\citep{houlsby2019parameter}, Sidetune~\citep{zhang2020side} and VPT~\citep{jia2022visual}.}
\label{tab:vtab_per_task}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|rrrrrrrr|rrrrr|rrrrrrrrr}
\toprule
%     \multirow{2}{*}{Method} & \multicolumn{8}{c}{Natural} & \multicolumn{5}{|c|}{Specialized} & \multicolumn{9}{c}{Structure} \\
% \cmidrule(lr){2-9} \cmidrule(lr){10-14} \cmidrule(lr){15-23}
    & \rotatebox{90}{CIFAR100} & \rotatebox{90}{Caltech101} & \rotatebox{90}{DTD} & \rotatebox{90}{Flowers102} & \rotatebox{90}{OxfordPets} & \rotatebox{90}{SVHN} & \rotatebox{90}{SUN397} & \rotatebox{90}{\bf Mean} & \rotatebox{90}{Patch Camelyon} & \rotatebox{90}{EuroSAT} & \rotatebox{90}{Resisc45} & \rotatebox{90}{Retinopathy} & \rotatebox{90}{\bf Mean} & \rotatebox{90}{Clevr/count} & \rotatebox{90}{Clevr/distance} & \rotatebox{90}{DMLab} & \rotatebox{90}{KITTI/distance} & \rotatebox{90}{dSprites/location} & \rotatebox{90}{dSprites/orientation} & \rotatebox{90}{SmallNORB/azimuth} & \rotatebox{90}{SmallNORB/elevation} & \rotatebox{90}{\bf Mean}  \\
\midrule
    Full       & 68.9     & 87.7    & 64.3 & 97.2       & 86.9 & 87.4 & 38.8   & 75.9 & 79.7           & 95.7    & 84.2     & 73.9        & 83.4 & 56.3        & 58.6           & 41.7  & 65.5  & 57.5              & 46.7                 & 25.7          & 29.1          & 47.6 \\
\midrule
    Bias       & 72.8     & 87.0    & 59.2 & 97.5       & 85.3 & 59.9 & 51.4   & 73.3 & 78.7           & 91.6    & 72.9     & 69.8        & 78.3 & 61.5        & 55.6           & 32.4  & 55.9  & 66.6              & 40.0                 & 15.7          & 25.1          & 44.1 \\
    Adapter    & 74.1     & 85.7    & 62.7 & 97.8       & 87.2 & 34.6 & 50.7   & 70.4 & 76.3           & 87.5    & 73.7     & 70.9        & 77.1 & 45.2        & 41.8           & 31.2  & 56.4  & 31.9              & 25.4                 & 13.5          & 22.0          & 33.4 \\
    Sidetune   & 60.7     & 60.8    & 53.6 & 95.5       & 66.7 & 34.9 & 35.3   & 58.2 & 58.5           & 87.7    & 65.2     & 61.0        & 68.1 & 27.6        & 22.6           & 31.3  & 51.7  & 8.2               & 14.4                 & 9.8           & 21.8          & 23.4 \\
    VPT        & 78.8     & 90.8    & 65.8 & 98.0       & 88.3 & 78.1 & 49.6   & 78.5 & 81.8           & 96.1    & 83.4     & 68.4        & 82.4 & 68.5        & 60.0           & 46.5  & 72.8  & 73.6              & 47.9                 & 32.9          & 37.7          & 55.0 \\
\midrule
    \bf Zero-init. & 82.2     & 92.4    & 70.3 & 98.4       & 89.8 & 84.9 & 54.3   &\bf 81.7 & 83.6           & 95.3    & 85.0     & 73.8        &\bf 84.4 & 69.3        & 60.2           & 51.1  & 79.7  & 80.7              & 49.0                 & 30.6          & 33.6          &\bf 56.8 \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{More Experiments on Language Tasks}

For a more comprehensive evaluation of zero-initialized attention, we fine-tune RoBERTa$_\mathrm{large}$~\citep{liu2019roberta} on other two natural language processing tasks in addition to extractive question answering of the main paper, which are named entity recognition (NER) and semantic role labeling (SRL). We adopt CoNLL03~\citep{CONLL03}, CoNLL04~\citep{CONLL04}, CoNLL05~\citep{CONLL05}, and CoNLL12~\citep{CONLL12} as the evaluation datasets. As shown in Table~\ref{tab:CoNLL}, compared to P-tuning V2 (PT2)~\citep{liu2021p-tuning}, our zero-initialized attention can steadily perform better on all datasets with varying magnitudes, which indicates our effectiveness for different language tasks and applications.
% \vspace{0.2cm}

\begin{table*}[h]
\centering
\caption{\textbf{Language Model Fine-tuning} with RoBERTa$_\mathrm{large}$~\citep{liu2019roberta} on named entity recognition (NER) and semantic role labeling (SRL) tasks. We report the micro-f1 score. * denotes our reproduced results.}
\label{tab:CoNLL}
\small
\resizebox{0.97\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
           \makecell*[l]{Method} & \multicolumn{1}{l}{CoNLL03} & \multicolumn{1}{l}{CoNLL04} & \multicolumn{1}{l}{CoNLL12} & \multicolumn{1}{l}{CoNLL05$_{Brown}$} & \multicolumn{1}{l}{CoNLL05$_{WSJ}$} \\
\midrule
Full       & 92.6                          & 88.8                        & 86.5                        & 85.6                              & 90.2                            \\
PT~\citep{lester2021power}         & 86.1                          & 76.2                        & 67.2                        & 70.7                              & 76.8                            \\
PT2~\citep{liu2021p-tuning}        & 92.8                          & 88.4                        & 84.6                        & 84.3                              & 89.2                            \\
PT2$^*$\vspace{0.05cm}      & 91.8                          & 88.4                        & 84.7                        & 83.9                              & 89.4                            \\
\bf Zero-init. &\bf 92.4                          &\bf 88.8                        &\bf 85.2                        &\bf 84.7                              &\bf 89.6                           \\
\bottomrule
\end{tabular}
}
\end{table*}

% \vspace{0.5cm}
\subsection{Detailed Results on Vision-Language Models}

Besides ViT and RoBERTa, we also evaluate our approach on CLIP~\citep{radford2021learning}, a vision-language model pre-trained by 400 million text-image pairs. In detail, we adopt CLIP with a ViT-B/16 as the visual encoder and a 12-layer transformer~\citep{li2019attention} as the textual encoder. We test our fine-tuning results on base-to-novel generalization~\citep{zhou2022conditional} benchmark with three datasets, i.e., ImageNet~\citep{deng2009imagenet}, Caltech101~\citep{caltech101}, and Flowers102~\citep{flowers102}, where the model is trained only on the base classes in a few-shot
setting and evaluated on both base and novel categories. We freeze the entire CLIP and insert the adaption prompts with zero-initialized attention into CLIP's encoders.
As shown in Table~\ref{tab:clip}, our approach achieves the best average classification accuracy on both base and novel categories, demonstrating our fine-tuning capability for large vision-language models.
% \vspace{0.2cm}


\begin{table*}[t]
\centering
\caption{\textbf{Vision-Language Model Fine-tuning} with ViT-B/16 CLIP~\citep{radford2021learning} on base-to-novel generalization~\citep{zhou2022conditional} benchmark. We report the classification accuracy (\%) and harmonic mean (HM).}
\label{tab:clip}
\begin{adjustbox}{width=\linewidth}
             \begin{tabular}{l cccccccccccc}
                \toprule
                    \multirow{2}{*}{Method\ \ \ \ \ \ \ }
                    &\multicolumn{3}{c}{\makecell*[l]{ImageNet}} &\multicolumn{3}{c}{Caltech101} &\multicolumn{3}{c}{Flowers102}&\multicolumn{3}{c}{Average}\\
                    \cmidrule(lr){2-4}  \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
                    &Base &Novel &HM &Base &Novel &HM  &Base &Novel &HM &Base &Novel &HM\\
                    \cmidrule(lr){1-1}
                    \cmidrule(lr){2-4}  \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
                     CLIP~\citep{radford2021learning}                                 &  72.43 &68.14 &70.22 &96.84 &94.00 &95.40 & 72.08 &77.80 &74.83  &80.45 &79.98 &80.15           \\
                     CoOp~\citep{zhou2022learning}           &76.47 &67.88 &71.92 &98.00 &89.81 &93.73 &97.60 &59.67 &74.06 &90.69 &72.45 &79.90 \\
                     CoCoOp~\citep{zhou2022conditional}      & 75.98 &70.43 &73.10  &97.96 &93.81 &95.84   &94.87 &71.75 &81.71 &89.60 &78.66 &83.55         \\
                     MaPLe~\citep{khattak2022maple}\vspace{0.05cm}                     & 76.66 &70.54 &73.47  & 97.74 &94.36 &96.02   &95.92 &72.46 &82.56 &90.11 &79.12 &84.02       \\
                     \bf Zero-init.     & \bf 76.70 & \bf 71.00       &\bf73.74    & \bf 98.10 & \bf94.53 &\bf96.28 & \bf 96.00 & \bf 74.67       & \bf84.00  & \bf90.27 & \bf80.07 & \bf 84.67       \\  
                     
                    \bottomrule
            \end{tabular}
\end{adjustbox}
\end{table*}

\section{Additional Experiment and Discussion}
\label{F}

\subsection{Evaluation on Counterfactual Reasoning}

As a core ability of human intelligence, counterfactual reasoning is a challenging assessment for multi-modal LLMs, which involves the processing of alternatives to observed states or past events. Here, we adopt the very recent C-VQA~\citep{zhang2023cvqa} benchmark for evaluating our counterfactual reasoning capability. C-VQA contains 2K counterfactual question and answer pairs, which are collected from VQAv2~\citep{goyal2017making} and supplemented by ChatGPT~\citep{OpenAI2023ChatGPT}. As shown in Table~\ref{tab:cvqa_eval}, for three groups of questions, LLaMA-Adapter performs comparably to the concurrent LLaVA. Especially for the `Numerical indirect' questions, our approach achieves the best counterfactual reasoning results (34.3) and the least performance loss (5.6$\downarrow$) than all other models.
\begin{table}[h]
\centering
\caption{\textbf{Counterfactual Reasoning Evaluation on C-VQA~\citep{zhang2023cvqa} Benchmark.}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
Method           & Numerical direct$\uparrow$ (Loss$\downarrow$) & Numerical indirect$\uparrow$ (Loss$\downarrow$) & Boolean$\uparrow$ (Loss$\downarrow$) \\
\midrule
ViperGPT~\citep{suris2023vipergpt}         & 80.6 (19.4$\downarrow$)                       & 31.6 (68.4$\downarrow$)                         & 21.6 (72.4$\downarrow$)              \\
% InstructBLIP~\citep{Dai2023InstructBLIPTG}     & 32.9 (14.4$\downarrow$)                       & 32.0 (19.6$\downarrow$)                         & 48.4 (17.7$\downarrow$)              \\
LLaVA-7B~\citep{llava}         & 27.0 (9.9$\downarrow$)                        & 25.0 (15.2$\downarrow$)                         & 58.5 (4.8$\downarrow$)               \\
LLaVA-13B~\citep{llava}         & 24.8 (11.9$\downarrow$)	&20.8 (21.2$\downarrow$)	&56.3 (4.7$\downarrow$)\\
\midrule
LLaMA-Adapter-7B & 30.1 (5.8$\downarrow$)                        & 34.3 (5.6$\downarrow$)                          & 45.8 (14.5$\downarrow$)              \\
\bottomrule
\end{tabular}%
}
\label{tab:cvqa_eval}
\end{table}


\subsection{Evaluation on Object Hallucination}

Similar to language generation, multi-modal LLMs also suffer from the hallucination issue, i.e., they might generate descriptions containing objects inconsistent with the target images. To validate our approach's performance, we adopt POPE~\citep{li2023evaluating} for object hallucination evaluation, which converts the object hallucination problem as a binary classification task and includes 500 images from MSCOCO~\citep{lin2014microsoft} with 6 questions per sample. As shown in Table~\ref{tab:pope_eval}, for different evaluation settings, LLaMA-Adapter with LLaMA-7B attains competitive accuracy compared to other multi-modal LLMs with LLaMA-13B, which indicates our relatively stronger robustness to object hallucination problems.
\begin{table}[h]
\centering
\caption{\textbf{Object Hallucination Evaluation on POPE~\citep{li2023pope} Benchmark.}}
\begin{tabular}{lccc}
\toprule
Method           & Random & Popular & Adversarial \\
\midrule
InstructBLIP-13B~\citep{Dai2023InstructBLIPTG} & 88.73  & 81.37   & 74.37       \\
mPLUG-Owl-7B~\citep{ye2023mplug}     & 53.30  & 50.63   & 50.67       \\
LLaVA-13B~\citep{llava}        & 54.43  & 52.43   & 50.77       \\
MM-GPT-7B~\citep{gong2023multimodalgpt}        & 50.03  & 50.00   & 50.00       \\
\midrule
LLaMA-Adapter-7B & 75.47  & 60.43   & 60.66      \\
\bottomrule
\end{tabular}
\label{tab:pope_eval}
\end{table}


\subsection{Tuning by More Instruction Data}
By default, we utilize a combination of Alpaca's data (52K)~\citep{alpaca} and LLaVA-I (158K)~\cite{llava} for visual instruction tuning. Here, we progressively add more question-answering data to enlarge the instruction datasets of LLaMA-Adapter: the sampled 83K VQAv2~\citep{goyal2017making} by LLaVA-1.5~\citep{liu2023improved} and the entire 204K VQAv2. We also compare our performance with very recent multi-modal LLMs with advanced visual reasoning capabilities: InstructBLIP~\citep{instructblip} and LLaVA-1.5. InstructBLIP collects extensive visual question-answering datasets (16M) to fine-tune BLIP-2~\citep{li2023blip}, which endows robust visual instruction-following capabilities. LLaVA-1.5 is an upgraded variant of LLaVA with a more powerful LLM, i.e., LLaMA-2~\citep{touvron2023llama}, and is also fine-tuned by a collection of 665K instruction-tuning datasets.
As shown in Table~\ref{tab:mm_eval_plus}, the increasing instruction tuning data leads to better multi-modal reasoning results on three benchmarks, demonstrating our methods favorable scalability to data size. Our LLaMA-Adapter also achieves comparable performance to the latest InstructBLIP and LLaVA-1.5, further indicating our effectiveness for multi-modal reasoning.
% \begin{table}[h]
% \centering
% \caption{\textcolor{blue}{\textbf{Instruction-tuning with More Datasets} on three zero-shot multi-modal Benchmarks: MME~\citep{fu2023mme}, MMBench~\citep{liu2023mmbench}, and MM-Vet~\citep{yu2023mmvet}.}}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|ccc|ccccccc|ccccccc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{MME} & \multicolumn{7}{c|}{MMBench}                    & \multicolumn{7}{c}{MM-Vet}                      \\
%                        & All    & P      & C     & All  & LR   & AR   & RR   & FP-S & FP-C & CP   & All  & Rec  & OCR  & Know & Gen  & Spat & Math \\
% \midrule
% BLIP-2                  & 1584   & 1294   & 290   & -    & -    & -    & -    & -    & -    & -    & 22.4 & 27.5 & 11.1 & 11.8 & 7.0  & 16.2 & 5.8  \\
% InstructBLIP           & 1505   & 1213   & 292   & 33.9 & 21.6 & 47.4 & 22.5 & 33.0 & 24.4 & 41.1 & 26.2 & 32.4 & 14.6 & 16.5 & 18.2 & 18.6 & 7.7  \\
% MiniGPT-4              & 1159   & 867    & 292   & 23.0 & 13.6 & 32.9 & 8.9  & 28.7 & 11.2 & 28.3 & 22.1 & 27.5 & 11.1 & 11.8 & 7.0  & 16.2 & 5.8  \\
% LLaVA                  & 718    & 503    & 215   & 36.2 & 15.9 & 53.6 & 28.6 & 41.8 & 20.0 & 40.4 & 23.8 & 28.0 & 17.1 & 16.3 & 18.9 & 21.2 & 11.5 \\
% LLaVA-1.5 &1826 & 1531 & 295 & 59.5 & 32.4 & 72.6 & 49.3 & 62.3 & 52.2 & 67.7 & 30.5 &- &- &- &- &- &- \\
% \midrule
% LLaMA-Adapter          & 1222   & 973    & 249   & 39.5 & 13.1 & 47.4 & 23.0 & 45.0 & 33.2 & 50.6 & 32.8 & 38.9 & 20.8 & 27.4 & 29.9 & 28.5 & 11.5  \\
% +VQAv2 (83K) &1256& 1007 & 249 &43.4& 22.9 & 44.7 & 31.3 & 46.7 & 46.9 & 50.3 &- &- &- &- &- &- &-\\
% +VQAv2 (204K)&1618& 1272 & 346 &60.1& 34.7 & 65.3 & 48.7 & 63.1 & 57.3 & 69.3 &- &- &- &- &- &- &-\\
% \bottomrule
% \end{tabular}%
% }
% \label{tab:mm_eval_plus}
% \end{table}
\begin{table}[h]
\centering
\caption{\textbf{Instruction-tuning with More Datasets} on three zero-shot multi-modal Benchmarks: MME~\citep{fu2023mme}, MMBench~\citep{liu2023mmbench}, and LVLM-eHub~\citep{xu2023lvlm}.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccc|ccccccc|ccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c|}{MME} & \multicolumn{7}{c|}{MMBench}                    & \multicolumn{5}{c}{LVLM-eHub}                     \\
                       & All    & P      & C     & All  & LR   & AR   & RR   & FP-S & FP-C & CP   & All    & VP   & VKA  & VR   & VC   \\
\midrule
BLIP-2                  & 1584   & 1294   & 290   & -    & -    & -    & -    & -    & -    & -  &0.77 & 0.86 & 0.93 & 0.76 & 0.54  \\
InstructBLIP           & 1505   & 1213   & 292   & 33.9 & 21.6 & 47.4 & 22.5 & 33.0 & 24.4 & 41.1 & 0.95 & 0.93 & 0.97 & 0.91 & 0.99\\
MiniGPT-4              & 1159   & 867    & 292   & 23.0 & 13.6 & 32.9 & 8.9  & 28.7 & 11.2 & 28.3 & 0.55 & 0.73 & 0.35 & 0.53 & 0.57\\
LLaVA                  & 718    & 503    & 215   & 36.2 & 15.9 & 53.6 & 28.6 & 41.8 & 20.0 & 40.4 & 0.64 & 0.62 & 0.38 & 0.77 & 0.79\\
LLaVA-1.5 &1826 & 1531 & 295 & 59.5 & 32.4 & 72.6 & 49.3 & 62.3 & 52.2 & 67.7 &- &- &- &- &-\\
\midrule
LLaMA-Adapter          & 1222   & 973    & 249   & 39.5 & 13.1 & 47.4 & 23.0 & 45.0 & 33.2 & 50.6 &0.6675& 0.81 & 0.44 & 0.83 & 0.59 \\
+VQAv2 (83K) &1256& 1007 & 249 &43.4& 22.9 & 44.7 & 31.3 & 46.7 & 46.9 & 50.3 &0.6925& 0.84 & 0.42 & 0.88 & 0.63\\
+VQAv2 (204K)&1618& 1272 & 346 &60.1& 34.7 & 65.3 & 48.7 & 63.1 & 57.3 & 69.3 &0.7175& 0.86 & 0.44 & 0.92 & 0.65 \\
\bottomrule
\end{tabular}%
}
\label{tab:mm_eval_plus}
\end{table}

\subsection{More Quantitative Comparison with Alpaca-LoRA}
Besides qualitative results, We have compared the language generative capabilities of our LLaMA-Adapter, Alpaca~\citep{alpaca}, and Alpaca-LoRA~\citep{alpaca_lora} on the GPT-4 evaluation benchmark~\citep{vicuna2023} in Figure~\ref{fig:quantitative_comparison}, which utilizes GPT-4 to assess the response quality on 80 questions. Here, we further evaluate the language processing capacity of the three methods on Open LLM benchmark~\citep{open-llm-leaderboard}. It evaluates LLMs' generative abilities in four different tasks: AI2 Reasoning Challenge~\citep{clark2018think}, HellaSwag~\citep{zellers2019hellaswag}, MMLU~\citep{hendrycks2021measuring}, and TruthfulQA~\citep{lin2022truthfulqa}. Each task contains challenging data samples over a wide range of knowledge domains.
% https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/.
%
As shown in Table~\ref{tab:open_llm_eval}, LLaMA-Adapter still achieves the best average performance than Alpaca's full fine-tuning and Alpaca-LoRA. This demonstrates the strong language instruction-following ability of our approach.
\begin{table}[h]
\centering
\caption{\textbf{Quantitative Evaluation on Open LLM Benchmark~\citep{open-llm-leaderboard}.}}
\begin{tabular}{lccccc}
\toprule
Method        & Avg   & ARC  & HellaSwag & MMLU & TruthfulQA \\
\midrule
Alpaca~\citep{alpaca}        & 49.23 & 49.1 & 77.7      & 33.8 & 36.3       \\
Alpaca-LoRA~\citep{alpaca_lora}   & 50.73 & 53   & 77.9      & 37.1 & 34.9       \\
\midrule
LLaMA-Adapter & 52.2  & 54.7 & 78.8      & 34.9 & 40.4      \\
\bottomrule
\end{tabular}
\label{tab:open_llm_eval}
\end{table}

\subsection{Comparison to Different LoRA Variants}
The default rank of Alpaca-LoRA~\citep{alpaca_lora} is 8, which contains 4.2M trainable parameters. In Table~\ref{tab:lora_rank}, we respectively show the results of Alpaca-LoRA with the ranks of 2, 4, and 16. We also evaluate their language processing capabilities on Open LLM benchmark~\citep{open-llm-leaderboard}. As shown, lower ranks of LoRA can effectively reduce the learnable parameters from 8.4M to 1.0M, and slightly lower the training time from 1.5h to 1.48h. However, our LLaMA-Adapter with 1.2M parameters and 1h still attains the best average result, demonstrating a good trade-off between performance and training efficiency.
\begin{table}[h]
\centering
\caption{\textbf{Comparison to Alpaca-LoRA~\citep{alpaca_lora} with Different Ranks} on Open LLM benchmark~\citep{open-llm-leaderboard}.}
\label{tab:lora_rank}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|ccc|ccccc}
\toprule
Model         & Rank & Param & Time & AVG  & ARC  & HellaSwag & MMLU & TruthfulQA \\
\midrule
\multirow{4}{*}{Alpaca-LoRA}   & 2    & 1.0   & 1.48 & 50.9 & 53.6 & 77.9      & 37.9 & 34.0       \\
  & 4    & 2.1   & 1.49 & 50.8 & 53.5 & 77.8      & 37.5 & 34.4       \\
  & 8    & 4.2   & 1.49 & 50.7 & 53.2 & 78.1      & 37.1 & 34.5       \\
 & 16   & 8.4   & 1.5  & 50.8 & 53.0 & 78.0      & 37.1 & 34.9       \\
\midrule
LLaMA-Adapter & -    & 1.2   & 1.0    & 52.2 & 54.7 & 78.8      & 34.9 & 40.4       \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{Full Comparison of Instruction-following Models}
\label{C}

In this section, we provide the full comparison of existing instruction-following models: Alpaca~\citep{alpaca}, Alpaca-LoRA~\citep{alpaca_lora}, GPT-3~\citep{brown2020language}, and our LLaMA-Adapter.
Our approach only fine-tunes 1.2M parameters within one hour, but generates responses comparable to the fully fine-tuned Alpaca and large-scale GPT-3, exhibiting a superior performance-efficiency trade-off.

\label{sec:instruct_full}
\input{tables/instruct_comparisons_full}

% \newpage
\section{Comparison with LLaMA-I}
% \vspace{-0.5cm}
\label{D}

In this section, we compare the generation quality of LLaMA-Adapter with LLaMA-I~\citep{touvron2023llama}, an instruction-fine-tuned LLaMA 65B model following~\citep{chung2022scaling}. Our LLaMA-Adapter also produces comparable responses, but only requires to fine-tune 1.2M parameters upon the LLaMA 7B model.

\label{sec:llama_i}
\input{tables/instruction_comparisons_llama}




% \begin{table}[t]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{lccc}
% \toprule
% Method           & Numerical direct$\uparrow$ (Loss$\downarrow$) & Numerical indirect$\uparrow$ (Loss$\downarrow$) & Boolean$\uparrow$ (Loss$\downarrow$) \\
% \midrule
% ViperGPT~\citep{suris2023vipergpt}         & 80.6 (19.4$\downarrow$)                       & 31.6 (68.4$\downarrow$)                         & 21.6 (72.4$\downarrow$)              \\
% InstructBLIP~\citep{Dai2023InstructBLIPTG}     & 32.9 (14.4$\downarrow$)                       & 32.0 (19.6$\downarrow$)                         & 48.4 (17.7$\downarrow$)              \\
% LLaVA-7B~\citep{llava}         & 27.0 (9.9$\downarrow$)                        & 25.0 (15.2$\downarrow$)                         & 58.5 (4.8$\downarrow$)               \\
% \midrule
% LLaMA-Adapter-7B & 30.1 (5.8$\downarrow$)                        & 34.3 (5.6$\downarrow$)                          & 45.8 (14.5$\downarrow$)              \\
% \bottomrule
% \end{tabular}%
% }
% \caption{\textbf{Evaluation on C-VQA~\citep{zhang2023cvqa}.}}
% \label{tab:cvqa_eval}
% \end{table}


% \begin{table}[t]
% \centering
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{lccc}
% \toprule
% Method           & Random & Popular & Adversarial \\
% \midrule
% InstructBLIP-13B~\citep{Dai2023InstructBLIPTG} & 88.73  & 81.37   & 74.37       \\
% mPLUG-Owl-7B~\citep{ye2023mplug}     & 53.30  & 50.63   & 50.67       \\
% LLaVA-13B~\citep{llava}        & 54.43  & 52.43   & 50.77       \\
% MM-GPT-7B~\citep{gong2023multimodalgpt}        & 50.03  & 50.00   & 50.00       \\
% \midrule
% LLaMA-Adapter-7B & 75.47  & 60.43   & 60.66      \\
% \bottomrule
% \end{tabular}%
% % }
% \caption{\textbf{Evaluation on POPE Benchmark~\citep{li2023pope}.}}
% \label{tab:pope_eval}
% \end{table}


% \begin{table}[t]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|ccc|ccccccc|ccccccc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{MME} & \multicolumn{7}{c|}{MMBench}                    & \multicolumn{7}{c}{MMVet}                      \\
%                        & ALL    & P      & C     & All  & LR   & AR   & RR   & FP-S & FP-C & CP   & ALL  & Rec  & OCR  & Know & Gen  & Spat & Math \\
% \midrule
% LLaVA                  & 718    & 503    & 215   & 36.2 & 15.9 & 53.6 & 28.6 & 41.8 & 20.0 & 40.4 & 23.8 & 28.0 & 17.1 & 16.3 & 18.9 & 21.2 & 11.5 \\
% InstructBLIP           & 1505   & 1213   & 292   & 33.9 & 21.6 & 47.4 & 22.5 & 33.0 & 24.4 & 41.1 & 26.2 & 32.4 & 14.6 & 16.5 & 18.2 & 18.6 & 7.7  \\
% BLIP2                  & 1584   & 1294   & 290   & -    & -    & -    & -    & -    & -    & -    & 22.4 & 27.5 & 11.1 & 11.8 & 7.0  & 16.2 & 5.8  \\
% Mini-GPT4              & 1159   & 867    & 292   & 23.0 & 13.6 & 32.9 & 8.9  & 28.7 & 11.2 & 28.3 & 22.1 & 27.5 & 11.1 & 11.8 & 7.0  & 16.2 & 5.8  \\
% \midrule
% LLaMA-Adapter          & 1222   & 973    & 249   & 39.5 & 13.1 & 47.4 & 23.0 & 45.0 & 33.2 & 50.6 & 32.8 & 38.9 & 20.8 & 27.4 & 29.9 & 28.5 & 11.5  \\
% \bottomrule
% \end{tabular}%
% }
% \caption{\textbf{Evaluation on Multi-modal Benchmarks}, including MME~\citep{fu2023mme}, MMBench~\citep{liu2023mmbench} and MMVet~\citep{yu2023mmvet}.}
% \label{tab:mm_eval_plus}
% \end{table}


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \begin{table}[]
% \centering
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{lccccc}
% \toprule
% Method        & AVG   & ARC  & HellaSwag & MMLU & TruthfulQA \\
% \midrule
% Alpaca~\citep{alpaca}        & 49.23 & 49.1 & 77.7      & 33.8 & 36.3       \\
% Alpaca-LoRA~\citep{alpaca_lora}   & 50.73 & 53   & 77.9      & 37.1 & 34.9       \\
% \midrule
% LLaMA-Adapter & 52.2  & 54.7 & 78.8      & 34.9 & 40.4      \\
% \bottomrule
% \end{tabular}%
% % }
% \caption{\textbf{Evaluation on Open LLM Leaderboard.}}
% \label{tab:open_llm_eval}
% \end{table}

% \begin{table}[t]
% \centering
% \caption{\textbf{Scaling Up Instruction Tuning Dataset}}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|ccc|ccccccc|ccccc}
% \toprule
% Data Size      & \multicolumn{3}{c|}{MME} & \multicolumn{7}{c|}{MMBench}                    & \multicolumn{5}{c}{LVLM-eHub}      \\
% \midrule
% LLaMA-Adapter  & All    & P      & C     & All  & LR   & AR   & RR   & FP-S & FP-C & CP   & All    & VP   & VKA  & VR   & VC   \\
% Alpaca+LLaVA-I & 1222   & 973    & 249   & 39.5 & 13.1 & 47.4 & 23.0 & 45.0 & 33.2 & 50.6 & 0.6675 & 0.81 & 0.44 & 0.83 & 0.59 \\
% +VQAv2 (83K)   & 1256   & 1007   & 249   & 43.4 & 22.9 & 44.7 & 31.3 & 46.7 & 46.9 & 50.3 & 0.6925 & 0.84 & 0.42 & 0.88 & 0.63 \\
% +VQAv2 (204K)  & 1618   & 1272   & 346   & 60.1 & 34.7 & 65.3 & 48.7 & 63.1 & 57.3 & 69.3 & 0.7175 & 0.86 & 0.44 & 0.92 & 0.65 \\
% \bottomrule
% \end{tabular}%
% }
% \label{tab:train_w_vqav2}
% \end{table}

% \begin{table}[t]
% \centering
% \caption{\textbf{Detailed Comparisons with Alpaca-LoRA~\cite{alpaca_lora} on Open LLM LeaderBoard.}}
% \label{tab:lora_rank}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{c|ccc|ccccc}
% \toprule
% Model         & Rank & Param & Time & AVG  & ARC  & HellaSwag & MMLU & TruthfulQA \\
% \midrule
% Alpaca-LoRA   & 2    & 1.0   & 1.48 & 50.9 & 53.6 & 77.9      & 37.9 & 34.0       \\
% Alpaca-LoRA   & 4    & 2.1   & 1.49 & 50.8 & 53.5 & 77.8      & 37.5 & 34.4       \\
% Alpaca-LoRA   & 8    & 4.2   & 1.49 & 50.7 & 53.2 & 78.1      & 37.1 & 34.5       \\
% Alpaca-LoRA   & 16   & 8.4   & 1.5  & 50.8 & 53.0 & 78.0      & 37.1 & 34.9       \\
% \midrule
% LLaMA-Adapter & -    & 1.2   & 1    & 52.2 & 54.7 & 78.8      & 34.9 & 40.4       \\
% \bottomrule
% \end{tabular}%
% }
% \end{table}

% \vspace{0.3cm}

\end{document}
