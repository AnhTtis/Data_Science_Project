\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}

\usepackage{bbding}
\makeatletter
  \newcommand\figcaption{\def\@captype{figure}\caption}
  \newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{color, colortbl}
\definecolor{citecolor}{HTML}{2980b9}
\definecolor{linkcolor}{HTML}{c0392b}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\definecolor{zrrgreen}{HTML}{008000}
\definecolor{zrrblue}{HTML}{4682B4}
\definecolor{zrrred}{HTML}{B22222}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{caption}

\usepackage{adjustbox}
\usepackage{wrapfig}

\usepackage{float}
\usepackage{subfig}

\usepackage[hidelinks,breaklinks=true,colorlinks,bookmarks=false,citecolor=citecolor,linkcolor=linkcolor]{hyperref}


\usepackage{framed}
\usepackage{makecell}
\usepackage{listings}
\lstset{
    escapeinside={(|}{|)}
}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\lstdefinelanguage{JavaScript}{
  keywords={break, case, catch, continue, debugger, default, delete, do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  ndkeywords={class, export, boolean, throw, implements, import, this},
  keywordstyle=\color{blue}\bfseries,
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  sensitive=true
}
\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}

% \usepackage[
% backend=biber,
% style=ieee,
% ]{biblatex}
% \addbibresource{neurips.bib}
\title{LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention}



\author{Renrui Zhang$^{*1,2}$, Jiaming Han$^{*1}$, Chris Liu$^{*1}$, Peng Gao$^{*\dagger \ddagger 1}$, Aojun Zhou$^2$\vspace{0.1cm}\\ \textbf{Xiangfei Hu$^1$, Shilin Yan$^1$, Lu Pan$^{3}$, Hongsheng Li$^{\dagger 2}$, Yu Qiao$^{\dagger 1}$}\vspace{0.3cm}\\
  $^1$Shanghai Artificial Intelligence Laboratory\quad
  $^2$CUHK MMLab\\
  $^3$University of California, Los Angeles\vspace{0.1cm}\\
  \texttt{\{zhangrenrui, hanjiaming, gaopeng, qiaoyu\}@pjlab.org.cn}\\
  % \texttt{hsli@ee.cuhk.edu.hk} \\
}


\begin{document}


\maketitle
\blfootnote{$^*$ Equal contribution\ \ $^{\dagger}$ Corresponding author\ \ $^{\ddagger}$ Project leader}

\vspace{-0.5cm}
\begin{abstract}

    We present \textbf{LLaMA-Adapter}, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces \textbf{1.2M} learnable parameters upon the frozen LLaMA 7B model, and costs less than \textbf{one hour} for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at \url{https://github.com/OpenGVLab/LLaMA-Adapter}.
    
\end{abstract}



\section{Introduction}

Large-scale Language Models (LLMs)~\cite{dai2019transformer,radford2019language,zhang2022opt,raffel2020exploring,devlin2018bert} have stimulated widespread attention in both academia and industry. Driven by massive corpora and advanced hardware, LLMs exhibit remarkable understanding and generative ability, propelling language tasks into a higher level. Recently, significant progress has been made on instruction-following models, e.g., ChatGPT~\cite{chatgpt} and GPT-3.5 (text-davinci-003)~\cite{brown2020language}. Following instructions in natural language, they can generate professional and contextual responses in a conversational way. However, the further prevalence of instruction models is largely impeded by the closed-source restriction and high development costs.


To alleviate this, Stanford Alpaca~\cite{alpaca} proposes to fine-tune an LLM, i.e., LLaMA~\cite{touvron2023llama} into an instruction-following model, which is affordable and replicable. Starting from 175 human-written instruction-output pairs~\cite{selfinstruct}, Alpaca leverages GPT-3.5 to expand the training data to 52K in a self-instruct manner. Supervised by this, Alpaca fine-tunes the entire 7B parameters in LLaMA, producing an exceptional instruction model that performs similarly to GPT-3.5. Despite Alpaca's effectiveness, a complete fine-tuning of large-scale LLaMA is still time-consuming, computation-intensive, multi-modality unsupported and cumbersome to transfer to different downstream scenarios.


In this paper, we introduce \textbf{LLaMA-Adapter}, an efficient fine-tuning method that adapts LLaMA into a well-performed instruction-following model. We also utilize the 52K instruction-output data for training purposes, but freeze the entire LLaMA model with superior resource efficiency. Specifically, in LLaMA's higher transformer layers, we append a set of learnable adaption prompts as prefix to the input instruction tokens.
These prompts learn to adaptively inject new instructions (conditions) into the frozen LLaMA.
To avoid noise from adaption prompts at the early training stage, we modify the vanilla attention mechanisms at inserted layers to be zero-initialized attention, with a learnable gating factor. Initialized by zero vectors, the gating can firstly preserve the original knowledge in LLaMA, and progressively incorporate instructional signals during training. This contributes to stable learning during the fine-tuning process and better instruction-following capacity of the final model. 

% In addition to instruction-following models, our zero-initialized attention can be generalized to other vision and language models for parameter-efficient fine-tuning.
% For vision models, we utilize our approach to fine-tune a pre-trained ViT~\cite{dosovitskiy2020image} for downstream image classification, obtaining superior performance on VTAB-1k~\cite{zhai2019large} benchmark over various image distributions.
% For other language models, we evaluate our fine-tuning efficacy on ReBERTa~\cite{liu2019roberta} for extractive question answering, which achieves leading results on SQuAD~\cite{rajpurkar2016squad} v1.1 and v2.0 benchmarks.
% By the extension experiments, we demonstrate the effectiveness of the zero-initialized attention in LLaMA-Adapter for traditional vision and language tasks.


\begin{figure*}[t!]
\vspace{-0.1cm}
  \centering
    \includegraphics[width=\textwidth]{figs/fig1_4.pdf}
   \caption{\textbf{Characteristics of LLaMA-Adapter.} Our lightweight adaption method efficiently fine-tunes LLaMA~\cite{touvron2023llama} 7B model with only 1.2M learnable parameters within one hour. After training, LLaMA-Adapter exhibits superior instruction-following and multi-modal reasoning capacity.}
    \label{fig1}
    % \vspace{-0.3cm}
\end{figure*}

Overall, our LLaMA-Adapter exhibits four main characteristics, as shown in Figure~\ref{fig1}.

\begin{itemize}
   \item \textbf{1.2M Parameters.} 
   Instead of updating the full 7B parameters, we freeze the pre-trained LLaMA and only learn the adaption prompts with 1.2M parameters on top. This, however, reveals comparable instruction-following proficiency with the 7B Alpaca.
   
   \item \textbf{One-hour Fine-tuning.}
   Thanks to our lightweight adaption modules with zero-initialized gating, the training convergence of LLaMA-Adapter costs less than one hour on 8 A100 GPUs, which are three times faster than Alpaca.
   

   \item \textbf{Plug with Expertise.}
   For different scenarios, it is flexible to insert their respective adapters and endow LLaMA with different expert knowledge. Thus, it suffices to store a 1.2M adapter within each context, other than a complete copy of the 7B model.

   
   \item \textbf{Multi-modal Instruction.}
   Besides textual instruction, our approach can also take images as input for multi-modal reasoning. By adding image tokens into adaption prompts, LLaMA-Adapter performs competitively on ScienceQA~\cite{scienceqa} and COCO Caption~\cite{chen2015microsoft} benchmarks.
   
\end{itemize}

In addition to instruction-following models, our zero-initialized attention can be generalized to other vision and language models for parameter-efficient fine-tuning.
For vision models, we utilize our approach to fine-tune a pre-trained ViT~\cite{dosovitskiy2020image} for downstream image classification, obtaining superior performance on VTAB-1k~\cite{zhai2019large} benchmark over various image distributions.
For other language models, we evaluate our fine-tuning efficacy on ReBERTa~\cite{liu2019roberta} for extractive question answering, which achieves leading results on SQuAD~\cite{rajpurkar2016squad} v1.1 and v2.0 benchmarks.
By these experiments, we demonstrate the effectiveness of LLaMA-Adapter for traditional vision and language tasks.

\vspace{-0.05cm}
\section{Related Work}
\vspace{-0.05cm}


\paragraph{Instruction-Following Language Models.} 
The subfield of language models learning instruction-following capabilities aims to generate responses based on natural language commands, which have been extensively researched in language~\cite{wei2021finetuned, wang2022super, bach2022promptsource, ouyang2022training}, and multi-modality~\cite{Shridhar2019ALFREDAB, Min2021FILMFI} domains.
These methods normally enhance the pre-trained LLMs by fine-tuning them using high-quality instruction-output data pairs. Such fine-tuning process boosts the model to better comprehend user intentions and follow instructions more accurately. Therein, FLAN~\cite{wei2021finetuned} introduces an instruction tuning method that outperforms non-tuned LLMs in unseen tasks. PromptSource~\cite{bach2022promptsource} provides a development environment with a web-based GUI, which creates and manages natural language prompts for zero-shot and gradient-based few-shot learning. SUP-NATINST~\cite{wang2022super} establishes a large benchmark of 1,616 diverse language tasks, and adopts a multi-task training on the T5 model. InstructGPT~\cite{ouyang2022training} demonstrates significant improvement of the instruction-following power, and is probably integrated into the closed-source GPT-3.5~\cite{brown2020language} and GPT-4~\cite{OpenAI2023GPT4TR}. Stanford Alpaca~\cite{alpaca} fine-tunes all the 7B parameters of an LLM, i.e., LLaMA~\cite{touvron2023llama} in an end-to-end manner, which is open-source and replicable.
However, this full-model fine-tuning can be inefficient in both time and memory, limiting its transferability to downstream applications.
In contrast, our LLaMA-Adapter aims to fine-tune only lightweight adapters on top of the frozen LLaMA, other than updating parameters of the entire model. Compared to a concurrent work Alpaca-LoRA~\cite{alpaca_lora}, our approach further reduces the computational demands, and can be generalized to follow visual instructions for multi-modal reasoning.


% \paragraph{Large Vision-Language Models.}
% Over the past decade, we have witnessed a shift in vision-language research from task-specific models~\cite{vinyals2015show,karpathy2015deep,yang2016stacked,santoro2017simple,jiang2020defense} to large foundation models~\cite{radford2021learning,wang2021simvlm,bao2022vlmo,wang2022ofa,alayrac2022flamingo,wang2022git,wang2022image,li2023blip}. After pre-training on large-scale image-text data, the large vision-language models can be adapted to a variety of downstream tasks with powerful performance. A line of works~\cite{radford2021learning,wang2021simvlm,wang2022ofa} train both visual and textual encoders from scratch, leading to a high computation cost. Recently, another line of works~\cite{alayrac2022flamingo,wang2022git,li2023blip,zhai2022lit} adopt pre-trained unimodal models as initialization, and only train the newly introduced learnable modules. For example, LiT~\cite{zhai2022lit} utilizes a pre-trained vision backbone to speed up the CLIP~\cite{radford2021learning} training, and Frozen~\cite{tsimpoukelli2021multimodal} fine-tunes an image encoder to transform visual tokens into LLM's soft prompts. Similarly, CLIPCap~\cite{mokady2021clipcap} proposes a mapping network to connect the pre-trained image encoder with LLMs, and Flamingo~\cite{alayrac2022flamingo} inserts several cross-attention layers to inject visual knowledge into LLMs. With promising zero-shot results, BLIP2~\cite{li2023blip} connects pre-trained image encoders and LLMs with a Q-Former~\cite{}. CLIP-Adapter~\cite{gao2021clip}, Tip-Adapter~\cite{zhang2021tip,zhang2023prompt} and PointCLIP~\cite{zhu2022pointclip,zhang2022pointclip} introduce customized adapters upon CLIP for 2D and 3D few-shot learning. In summary, existing methods either adopt mapping networks or cross-attention mechanisms to connect two modalities with pre-trained vision-language models. Different from them, our LLaMA-Adapter only introduces a few learnable parameters and adapts an LLM, e.g., LLaMA~\cite{touvron2023llama}, to learn a multi-modal instruction model. With a simple but effective zero-initialized attention, we progressively inject visual information into the pre-trained language model, which contributes to superior vision-language reasoning.


\paragraph{Parameter-Efficient Fine-Tuning.} 
The pre-training and fine-tuning paradigms have been proven to be highly effective in different language and vision tasks. 
Compared to full fine-tuning, Parameter-Efficient Fine-Tuning (PEFT)~\cite{peft} methods freeze most parameters of pre-trained models, and can still exhibit comparable capabilities on downstream tasks. Various PEFT techniques have been explored, including prompt tuning~\cite{li2021prefix,lester2021power,liu2021gpt,liu2021p-tuning,qin2021learning,zhang2023personalize}, Low-Rank Adaptation (LoRA)~\cite{hu2021lora, zhang2023adaptive,hedegaard2022structured}, and adapters~\cite{houlsby2019parameter,pfeiffer2020adapterfusion,lin2020exploring, chen2022vision,rebuffi2017learning}. 
Prompt tuning appends a collection of trainable prompt tokens to pre-trained large models, which are inserted either to the input embeddings only~\cite{lester2021power, liu2021gpt}, or to all of the intermediate layers~\cite{li2021prefix, liu2021p-tuning}.
LoRA~\cite{hu2021lora} introduces trainable rank decomposition matrices into each network weights~\cite{karimi2021compacter}, which have indicated promising fine-tuning ability on large generative models~\cite{stable-diffusion-lora,touvron2023llama}. Adapters~\cite{houlsby2019parameter} insert lightweight adaption modules into each layer of the pre-trained transformer and have been extended across numerous domains~\cite{gesmundo2022munet,gao2021clip,zhang2021tip,zhang2022pointclip}.
In this paper, we propose a new PEFT method, LLaMA-Adapter, specially designed for LLaMA~\cite{touvron2023llama} and instruction-following fine-tuning. 
Existing PEFT methods might potentially disturb the pre-trained linguistic knowledge by directly inserting randomly initialized modules. This leads to unstable fine-tuning with large loss values at early training stages. To this end, LLaMA-Adapter adopts a zero-initialized attention with gating factors to well mitigate such a issue, which progressively incorporates the instructional cues with the frozen LLaMA.
% Our LLaMA-Adapter can also be adapted to take images as input with favorable multi-modal understanding capacity.
Moreover, we verify the effectiveness of our approach to fine-tune large models in other domains. Aided by the adaption prompts with zero gating, our efficient fine-tuning of ViT~\cite{dosovitskiy2020image} and RoBERTa~\cite{liu2019roberta} exhibit competitive downstream performance respectively on vision and language tasks, demonstrating superior generalization capacity.

\vspace{-0.05cm}
\section{LLaMA-Adapter}

In Section~\ref{s1}, we first introduce how to insert the learnable adaption prompts into LLaMA's~\cite{touvron2023llama} transformer. Then, we present the details of zero-initialized attention mechanisms with zero gating in Section~\ref{s2}, and generalize LLaMA-Adapter for multi-modal reasoning in Section~\ref{s3}. Finally, we extend our approach for efficient fine-tuning of vision and vision-language models in Section~\ref{s4}.


\subsection{Learnable Adaption Prompts}
\label{s1}

Given 52K instruction-output data~\cite{selfinstruct} and a pre-trained LLaMA~\cite{touvron2023llama} with an $N$-layer transformer, we adopt a set of learnable adaption prompts for instruction-following fine-tuning. We denote the prompts for $L$ transformer layers as $\{P_l\}_{l=1}^{L}$, where $P_l \in \mathbb{R}^{K\times C}$ with $K$ denoting the prompt length for each layer, and $C$ equaling the feature dimension of LLaMA's transformer. Note that we insert the prompts into the topmost $L$ layers of the transformer ($L\leq N$). This can better tune the language representations with higher-level semantics.

Taking the $l$-th inserted layer as an example ($l\leq L$), we denote the $M$-length word tokens as $T_l \in \mathbb{R}^{M\times C}$, which represent the input instruction and the already generated response. The learnable adaption prompt is concatenated with $T_l$ along the token dimension as prefix, formulated as 
\begin{align}
    [P_l;\ T_l]\  \in \mathbb{R}^{(K+M)\times C}.
\end{align}
In this way, the instruction knowledge learned within $P_l$, can effectively guide $T_l$ to generate the subsequent contextual response via attention layers in the transformer block.


\subsection{Zero-initialized Attention}
\label{s2}

\begin{wrapfigure}{r}{0.5\textwidth}
  \centering
  \vspace{-0.95cm}
    \includegraphics[width=0.99\linewidth]{figs/fig2_2.pdf}
   \caption{\textbf{Details of LLaMA-Adapter.} We insert lightweight adapters with learnable prompts into $L$ out of $N$ transformer layers of LLaMA. To progressively learn the instructional knowledge, we adopt zero-initialized attention with gating mechanisms for stable training in early stages.}
    \label{fig2}
    % \vspace{0.2cm}
\end{wrapfigure}

If the adaption prompts are randomly initialized, they might bring disturbance to the word tokens at the beginning of training, which harms the fine-tuning stability and effectiveness. Considering this, we modify the vanilla attention mechanisms at the last $L$ transformer layers to be zero-initialized attention, as shown in Figure~\ref{fig2}. 
Suppose the model is generating the $(M+1)$-th word on top of $[P_l;\ T_l]$ at the $l$-th inserted layer, we denote the corresponding $(M+1)$-th word token as $t_l \in \mathbb{R}^{1\times C}$. In the attention mechanism, several linear projection layers are first applied to transform the input tokens into queries, keys, and values as
% \begin{align}
%     Q_l = \operatorname{Linear_{q}}(\ t_l\ );\ \ \
%     K_l, V_l = \operatorname{Linear_{kv}}(\ [P_l;\ T_l;\ t_l]\ ).
% \end{align}
\begin{align}
    Q_l &= \operatorname{Linear_{q}}(\ t_l\ );\\
    K_l &= \operatorname{Linear_{k}}(\ [P_l;\ T_l;\ t_l]\ );\\
    V_l &= \operatorname{Linear_{v}}(\ [P_l;\ T_l;\ t_l]\ ).
\end{align}
Then, the attention scores of $Q_l$ and $K_l$ before the softmax function are calculated as
\begin{align}
    S_l &= Q_l K_l^T/\sqrt{C}\ \in \mathbb{R}^{1\times (K+M+1)},
\end{align}
which records the feature similarities between the new word $t_l$ and all $K+M+1$ tokens. Meanwhile, $S_l$ can be reformulated by two components as 
\begin{align}
    S_l &= [S_l^K;\ S_l^{M+1}]^T,
\label{eq1}
\end{align}
where $S_l^K \in \mathbb{R}^{K\times 1}$ and $S_l^{M+1} \in \mathbb{R}^{(M+1)\times 1}$ denote the attention scores of $K$ adaption prompts and $M+1$ word tokens, respectively.
The former $S_l^K$ represents how much information the learnable prompt contributes to generating $t_l$, which probably causes disturbance in the early training stage.


To this end, we adopt a learnable gating factor, denoted as $g_l$, to adaptively control the importance of $S_l^K$ in the attention. Initialized by zero, $g_l$ can firstly eliminate the influence of under-fitted prompts, and then increase its magnitude for providing more instruction semantics to LLaMA.
Therefore, we independently apply the softmax functions to the two components in Equation~\eqref{eq1}, and multiply the first term by $g_l$, formulated as
\begin{align}
    S_l^g &= [\operatorname{softmax}(S_l^K)\cdot g_l;\ \ \operatorname{softmax}(S_l^{M+1})]^T.
\end{align}
The separate softmax functions ensure the second term to be irrelevant to the adaption prompts. 
When $g_l$ is close to zero, it can mostly convey the originally pre-trained knowledge of LLaMA to token $t_l$ for a creditable generation. In practice, we adopt multiple $g_l$ to be independently learned for different heads within the attention, benefiting the learning diversity of multi-head mechanisms.

Finally, we calculate the output of the $l$-th attention layer with a linear projection layer as
\begin{align}
    t_{l}^o = \operatorname{Linear_{o}}(S_l^g V_l)\ \in \mathbb{R}^{1\times C}.
\end{align}
With our proposed zero-initialized attention, the adaption prompts can progressively inject the newly acquired instructional signals into the transformer, while simultaneously incorporating the pre-trained knowledge of LLaMA to provide high-quality responses.


\begin{figure*}[t!]
  \centering
    \includegraphics[width=\textwidth]{figs/fig3.pdf}
% \vspace{0.05cm}
   \caption{\textbf{Multi-modal Reasoning of LLaMA-Adapter.} On ScienceQA benchmark~\cite{scienceqa}, LLaMA-Adapter is extended to a multi-modal variant for image-conditioned question answering. Given an image as the visual context, we acquire the global image token by multi-scale aggregation, and element-wisely add it onto the adaption prompts for visual instruction following.}
    \label{fig3}
    % \vspace{-0.3cm}
\end{figure*}

\subsection{Multi-modal Reasoning}
\label{s3}

Apart from textual instructions, LLaMA-Adapter is capable of answering a question based on input of other modalities, which augments the language model with rich cross-modal information. As shown in Figure~\ref{fig3}, we take the ScienceQA benchmark~\cite{scienceqa} as examples, which is analogous to the COCO Caption dataset~\cite{chen2015microsoft}. Given \textcolor{zrrgreen}{\textbf{visual}} and \textcolor{zrrgreen}{\textbf{textual contexts}}, along with the corresponding \textcolor{zrrblue}{\textbf{question}} and \textcolor{zrrblue}{\textbf{options}}, the model is required to conduct multi-modal understanding to give the correct \textcolor{zrrred}{\textbf{answer}}.


For an input image as the visual context, we first leverage a pre-trained visual encoder, e.g., CLIP~\cite{radford2021learning}, to extract its multi-scale global features, denoted as $\{I_m\}_{m=1}^{M}$, where $I_m \in \mathbb{R}^{1\times C_m}$ and $M$ denotes the scale number. Then, we concatenate the $M$-scale features along the channel dimension and apply a learnable projection network on top, formulated as
\begin{align}
    I_p = \operatorname{Projection}\Big(\operatorname{Concat}\big(\{I_m\}_{m=1}^M\big)\Big),
\end{align}
where $I_p \in \mathbb{R}^{1\times C}$ and is regarded as the overall image token with the same feature dimension as our adaption prompts. 
After this, we repeat $I_p$ for $K$ times, and element-wisely add it onto the $K$-length adaption prompts at all $L$ inserted transformer layers. For the $l$-th layer, we denote the acquired multi-modal prompt as
\begin{align}
    P_l^v = P_l + \operatorname{Repeat}(I_p)\ \in \mathbb{R}^{K\times C},
\end{align}
where $P_l^v$ denotes the adaption prompt incorporating visual information from the given image context. In this way, LLaMA is fine-tuned to generate responses conditioned on vision-language inputs, and can tackle more challenging generative tasks with multi-modal understanding.

% As a general framework, LLaMA-Adapter with additional input condition can also be extended to video and audio modalities. Using the pre-trained modal-specific encoders, we can integrate instructional signals of different modalities into the adaption prompts, which further maximizes the comprehension and generative capacity of LLaMA. We leave this as a future work.

\subsection{Zero-initialized Attention for other Large Models}
\label{s4}


Our approach, i.e., adaption prompts with zero-initialized attention, is not limited to the domain of instruction models, and can be further utilized to fine-tune large models in traditional vision and language tasks, exerting superior generalization capacity.

\paragraph{Vision Models.}
We select a pre-trained ViT~\cite{dosovitskiy2020image} as the foundation vision model for downstream image classification tasks. Similar to LLaMA, we insert the adaption prompts as prefix into the topmost $L$ transformer layers in ViT, and modify the attention operations to be zero-initialized at all inserted layers. By increasingly injecting the downstream visual semantics, we only introduce a few parameters on top of the frozen ViT, and attain comparable classification accuracy to full fine-tuning on VTAB-1k~\cite{zhai2019large} benchmark, which indicates our attention operator's efficacy in vision domains.

\paragraph{Language Models.}
We utilize RoBERTa~\cite{liu2019roberta} pre-trained on large-scale unlabeled text corpus, and evaluate our proposed zero-initialized attention on SQuAD~\cite{rajpurkar2016squad} benchmark for extractive question answering. We implement the zero-initialized attention on top of P-tuning v2~\cite{liu2021p-tuning}, a prompt tuning method for efficiently adapting large language models. Likewise, we only enable the prompt tokens in P-tuning v2 and our zero gating factors to be learnable during fine-tuning. The leading results demonstrate our superiority for traditional language tasks. Please refer to Supplementary Material for applying zero-initialized attention mechanisms to more large models and tasks.


% \vspace{-0.2cm}
\section{Experiment}

In Section~\ref{s4.1}, we first evaluate the instruction-following capacity of LLaMA-Adapter. Then, we present our multi-modal performance on ScienceQA~\cite{scienceqa} benchmark in Section~\ref{s4.2}, and conduct ablation study on ScienceQA's validation set in Section~\ref{s4.3}. Finally, we report the fine-tuning results of our approach on other vision and language models in Section~\ref{s4.4}.

% \input{tables/instruction_comparisons_short}

\begin{figure*}[t!]
    \centering
    % \vspace{0.3cm}
    \includegraphics[width=\textwidth]{figs/compare.pdf}
    \caption{\textbf{Instruction-following Comparison} between Alpaca~\cite{alpaca} and LLaMA-Adapter.}
    \label{tab:instruction_comparisons_short}
    % \vspace{-1cm}
\end{figure*}

\subsection{Instruction-following Evaluation}
\label{s4.1}

\paragraph{Settings.} 
Following Stanford Alpaca~\cite{alpaca}, we utilize 52K instruction-following data for training, which is extended from 175 instruction-output pairs~\cite{selfinstruct}. 
% Each sample contains three components: \{instruction\} is the description of a task; \{input\} is the context for the task; and \{output\} is the answer generated by GPT-3.5 (text-davinci-003)~\cite{brown2020language}.
% Note that only 40\% of the data examples include an \{input\}. 
We fine-tune LLaMA-Adapter on 8 A100 GPUs for 5 epochs. The warmup epochs, batch size, learning rate, and weight decay are set to 2, 64, 0.009, and 0.02, respectively. By default, we utilize the pre-trained LLaMA model with 7B parameters and $N=32$ transformer layers. We adopt a prompt length $K=10$ and insert the adaption prompts into the last $L=30$ layers. In the generation stage, we adopt \textit{top-p} sampling~\cite{top_p_sample} as the default decoding method with a temperature $0.1$ and a $\textit{top-p}=0.75$. For quantitative evaluation~\cite{vicuna2023}, we ask GPT-4~\cite{OpenAI2023GPT4TR} to assess the response quality of instruction-following models on 80 questions. Since we observed that GPT-4 has a preference to give higher scores to the first response in comparison, we also switch the position of two responses, resulting in a total of 160 evaluation items.
% \vspace{-0.3cm}


\paragraph{Performance.} 
We compare the generated responses of LLaMA-Adapter and Alpaca~\cite{alpaca} in Figure~\ref{tab:instruction_comparisons_short}, and report the quantitative results in Figure~\ref{fig:quantitative_comparison}. Please refer to Supplementary Material for a full comparison with Alpaca-LoRA~\cite{alpaca_lora}, GPT-3~\cite{brown2020language}, and LLaMA-I~\cite{touvron2023llama}.
For different kinds of instructions in Figure~\ref{tab:instruction_comparisons_short}, our approach can output reasonable responses comparable to the fully fine-tuned Alpaca, including question answering, language translation, and code generation. 
For the GPT-4 evaluation in Figure~\ref{fig:quantitative_comparison}, LLaMA-Adapter obtains more `win' compared to Alpaca and Alpaca-LoRA. 
This fully demonstrates the effectiveness of our adapters with zero-initialized attention mechanisms. 


\begin{figure*}[t!]
    \centering
    % \vspace{0.3cm}
    \includegraphics[width=\textwidth]{figs/scienceqa_examples.pdf}
    \caption{\textbf{Multi-modal Reasoning on ScienceQA~\cite{scienceqa}} dataset by LLaMA-Adapter.\vspace{0.3cm}}
    \label{fig:scienceqa_examples}
    % \vspace{-1cm}
\end{figure*}
\input{tables/training_efficiency}


\paragraph{Efficiency.} 
In Table~\ref{tab:training_efficiency}, we compare the learnable parameters, storage space, and training time of different instruction-following methods. 
As a lightweight plug-and-play module, LLaMA-Adapter enjoys superior training efficiency with only 1.2M parameters, 4.9M storage, and one-hour training. This enables us to fine-tune large-scale language models, e.g., LLaMA, on mobile devices. 
LLaMA-Adapter's efficiency advantages can be further revealed by multi-node training, since only the gradients of 1.2M parameters are required to be transferred among nodes, other than Alpaca's 7B.

\subsection{Multi-modal Evaluation}
\label{s4.2}

\input{tables/scienceqa}

\begin{figure*}[t]
\quad\quad\hspace{0.2cm}
\begin{minipage}[t]{0.36\linewidth}
\centering
 \small
\tabcaption{\textbf{Ablation on Inserted Layers} of LLaMA's transformer.}
\label{tab:layers_vs_acc}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{c|cc}
\toprule
Layers & Params & Val Acc (\%)  \\
\midrule
10                & 0.97          & 55.95             \\
20                & 1.37          & 73.36        \\
30                & 1.79          & \textbf{83.85}       \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{minipage}
\quad\quad\quad\quad
\begin{minipage}[t]{0.38\linewidth}
\centering
 \small
% \vspace{0.03cm}
\tabcaption{\textbf{Ablation on Zero-initialized Attention.} Blue highlights the gain.}
\label{tab:zero_init_acc}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{l|c}
\toprule
Setting &  Val Acc (\%) \\
\midrule
Rand-Init Attention   & 40.77           \\
Zero-Init Attention   & \textbf{83.85} \\
\textit{\textcolor{blue}{Gain}} &\textcolor{blue}{+43.08}\\
\bottomrule
\end{tabular}%
\end{adjustbox}
\end{minipage}
\end{figure*}


% \begin{figure*}[t]
% \quad\quad
% \begin{minipage}[t]{0.36\linewidth}
% \centering
%  \captionof{figure}{\textbf{Loss Curves} 
% We
% }
% \includegraphics[width=\textwidth]{figs/65b_demo3.pdf}
% \label{fig:zero_init_loss}
% \end{minipage}
% \quad\quad\quad\quad\hspace{0.1cm}
% \begin{minipage}[t]{0.43\linewidth}
% \centering
%  \small
% % \vspace{0.03cm}
% \tabcaption{\textbf{Ablation on Zero-initialized Attention.}}
% \label{t_ab2}
% \begin{adjustbox}{width=\linewidth}
% \begin{tabular}{ccc|c}
% \toprule
% Epoch      & Train Loss    & Val Loss    & Val Acc (\%) \\ \midrule
% 15         & 0.022         & 0.136         & 82.08             \\
% 30         & 0.004         & 0.241         & 83.85             \\
% 60         & 0.001         & 0.282         & \textbf{83.94}            \\ \bottomrule
% \end{tabular}%
% \end{adjustbox}
% \end{minipage}
% \end{figure*}


\paragraph{Settings.} 
For the multi-modal LLaMA-Adapter, we adopt CLIP's~\cite{radford2021learning} visual encoder to extract the multi-scale global features of input images, and leverage simple cascaded MLPs as the learnable projection network. 
We adopt greedy search as the decoding method for generation, and keep other hyperparameters the same as the instruction-following LLaMA-Adapter.
Two multi-modal datasets are utilized to train our model and evaluate the performance: ScienceQA~\cite{scienceqa} and COCO Caption~\cite{chen2015microsoft}.
ScienceQA is a large-scale multi-modal science question answering dataset collected from various knowledge domains.
Each example contains a \textcolor{zrrgreen}{\textbf{visual context}}, a \textcolor{zrrgreen}{\textbf{textual context}}, a \textcolor{zrrblue}{\textbf{question}}, multiple \textcolor{zrrblue}{\textbf{options}}, and an \textcolor{zrrred}{\textbf{answer}}.
% We omit the lecture and explanation in some data samples for simplicity.
We concatenate the given question, textual context, and options sequentially in one sentence as LLaMA-Adapter's input. 
COCO Caption dataset contains 0.6M training image-caption data (120k images with 5 captions per image) over a wide range of distributions. We utilize ``Generate caption for this image'' as the textual instruction input for LLaMA-Adapter.

% For the visual context, we adopt CLIP~\cite{radford2021learning}'s visual encoder to extract its multi-scale global features, and leverage simple cascaded MLPs as the learnable projection network.
% As the pre-trained visual encoder, i.e., CLIP, is strong enough, we do not utilize the captioning data provided in ScienceQA. 
% In the generation stage, we adopt greedy search as the decoding method. 
% We keep other hyperparameters the same as the instruction-following LLaMA-Adapter.

% \vspace{0.1cm}

\paragraph{Performance.}
In Table~\ref{tab:scienceqa}, we compare LLaMA-Adapter with existing popular VQA methods~\cite{yu2019mcan,li2019visualbert,li2020does} and large language models~\cite{khashabi2020unifiedqa,brown2020language,zhang2023multicot} on ScienceQA datset.
As shown, our single-modal variant (`LLaMA-Adapter$_T$') attains 78.31\% accuracy with only 1.2M parameters. By further injecting visual conditions with a 0.6M projection network, our multi-modal variant (`LLaMA-Adapter') exhibits a improvement of +6.88\% answering accuracy. 
Compared to traditional VQA methods, they are required to train the entire network by in-domain datasets with considerable resource budget, while LLaMA-Adapter only fine-tunes a few parameters with better performance.
Despite the GPT series~\cite{brown2020language,chatgpt,OpenAI2023GPT4TR} achieving zero-shot answering without fine-tuning, they contain much more parameters than our LLaMA 7B model with lightweight adapters. 
Besides, MM-CoT~\cite{zhang2023multicot} is on par with our approach, but it highly relies on a complex two-stage inference. Therefore, our LLaMA-Adapter demonstrates superior parameter efficiency while achieving competitive question answering capacity.
In Table~\ref{t_caption}, we report the results of image captioning on COCO Caption dataset. Both BLIP~\cite{li2022blip} and BLIP-2~\cite{li2023blip} adopt a costly pre-training stage on additional datasets for superior performance, including Visual Genome~\cite{krishna2017visual}, Conceptual Captions~\cite{sharma2018conceptual,changpinyo2021conceptual} and LAION~\cite{schuhmann2021laion}. In contrast, our LLaMA-Adapter only requires COCO Catption's training set of 0.6M data and attains better accuracy than ClipCap~\cite{mokady2021clipcap}.
% \vspace{0.5cm}
\begin{wrapfigure}{r}{0.55\textwidth}
\centering
\vspace{0.6cm}
\small
\tabcaption{\textbf{Performance (\%) on COCO Caption's~\cite{chen2015microsoft} validation set} following Karpathy et al.~\cite{karpathy2015deep}. PT denotes pre-training on additional datasets~\cite{chen2015microsoft,krishna2017visual,sharma2018conceptual,changpinyo2021conceptual,schuhmann2021laion}, FT denotes fine-tuning on COCO Caption.}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Data Scale} & \multicolumn{2}{c}{COCO Caption}\vspace{0.1cm} \\
                       & PT      & FT      & B@4          & CIDEr          \\
\midrule
BLIP~\cite{li2022blip}                   & 14M           & 0.6M           & 40.4            & 136.7          \\
BLIP-2~\cite{li2023blip}                  & 129M          & 0.6M           & 43.7            & 145.3          \\
\midrule
ClipCap~\cite{mokady2021clipcap}                & 0             & 0.6M           & 33.5            & 113.1          \\
\bf LLaMA-Adapter         &\bf 0             &\bf 0.6M           &\bf 36.2            &\bf 122.2\\
\bottomrule
\end{tabular}
\label{t_caption}
\end{adjustbox}
\end{wrapfigure}



% GPT-3 can perform zero-shot and few-shot question answering by constructing a suitable prompt, but its total parameters are much large than LLaMA-Adapter (175B \emph{vs.} 7B). Besides, as a language model, GPT-3 can not leverage any visual information. In contrast, LLaMA-Adapter can easily switch between single modal and multi-modal. 

% As shown in Tab.~\ref{tab:scienceqa}, our single modal method (LLaMA-Adapter$_T$) achieves 78.31\% accuracy. By injecting visual tokens into LLaMA-Adapter, our multi-modal method can further bring 7\% accuracy improvement. 

% Moreover, we notice that MM-CoT~\cite{zhang2023multicot} is on par with our method, but it relies on a complex two-stage inference method. We believe our LLaMA-Adapter can also be boosted and leave the exploration of chain-of-thought for future research.

% \vspace{0.2cm}
\subsection{Ablation Study}
\label{s4.3}

\paragraph{Insertion Layers.} 
We first investigate the number of transformer layers to be inserted in LLaMA-Adapter. As shown in Table~\ref{tab:layers_vs_acc}, increasing the layer numbers introduces more parameters, but leads to a large improvement in the accuracy of ScienceQA's validation set, e.g., +17.41\% from 10 to 30, and +10.49\% from 20 to 30. It indicates that more adaption prompts at different layers can provide stronger task-specific guidance to the pre-trained LLaMA.
\clearpage

\begin{figure*}[t]
\hspace{0.1cm}
\begin{minipage}[t]{0.42\linewidth}
\centering
 \captionof{figure}{\textbf{Loss Curves} with (blue) and without (orange) zero-initialized attention.
}
\includegraphics[width=1\textwidth]{figs/65b_demo4.pdf}
\label{fig:zero_init_loss}
\end{minipage}
\quad\quad\quad\hspace{0.3cm}
\begin{minipage}[t]{0.43\linewidth}
\centering
 \small
% \vspace{0.03cm}
\tabcaption{\textbf{Robustness to Over-fitting.} We compare the training loss, validation loss, and validation accuracy of LLaMA-Adapter in different training epochs.}
\label{tab:loss_vs_acc}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{ccc|c}
\toprule
Epoch      & Train Loss    & Val Loss    & Val Acc (\%) \\ \midrule
15         & 0.022         & 0.136         & 82.08             \\
30         & 0.004         & 0.241         & 83.85             \\
60         & 0.001         & 0.282         & \textbf{83.94}            \\ \bottomrule
\end{tabular}%
\end{adjustbox}
\end{minipage}
\end{figure*}


\begin{figure*}[t]
\begin{minipage}[t]{0.47\linewidth}
\centering
 \small
\tabcaption{\textbf{Vision Model Fine-tuning} with ViT-B/16~\cite{dosovitskiy2020image} on VTAB-1k~\cite{zhai2019large}. We report the average accuracy (\%) of three task groups.}
\label{t_6}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{l cc c}
	\toprule
        \makecell*[l]{Method}
        &Natural &Specialized &Structured\\
		\cmidrule(lr){1-1}
  \cmidrule(lr){2-4}
         Full  & 75.88 & 83.36 & 47.64\\
         Bias~\cite{zaken2022bitfit}  & 73.30 & 78.25 & 44.09\\
         Adapter~\cite{houlsby2019parameter}  & 70.39 & 77.11 & 33.43\\
         Sidetune~\cite{zhang2020side} &58.21 &68.12 &23.41\\
         VPT~\cite{jia2022visual}\vspace{0.05cm} & 78.48 & 82.43 & 54.98\\
         % \cmidrule(lr){1-4}
         \bf Zero-init.  &\bf81.74 & \bf84.43 & \bf56.75\\
        \bottomrule
	\end{tabular}
\end{adjustbox}
\end{minipage}\hfill
% \qquad
% \hspace{0.4cm}
\begin{minipage}[t]{0.46\linewidth}
\centering
 \small
% \vspace{0.03cm}
\tabcaption{\textbf{Language Model Fine-tuning} with RoBERTa$_\mathrm{large}$~\cite{liu2019roberta} on SQuAD~\cite{rajpurkar2016squad}. * denotes our reproduced results of P-Tuning v2~\cite{liu2021p-tuning}.}
\label{t_7}
\begin{adjustbox}{width=\linewidth}
             \begin{tabular}{l cccc}
                \toprule
                    \multirow{2}{*}{Method\ \ \ \ \ \ \ }
                    &\multicolumn{2}{c}{SQuAD 1.1 dev} &\multicolumn{2}{c}{SQuAD 2.0 dev}\\
                    \cmidrule(lr){2-3}  \cmidrule(lr){4-5}
                    &EM &F1 &EM &F1\\
                    \cmidrule(lr){1-1}
                    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                     Full                                 & 88.9     & 94.6               & 86.5     & 89.4               \\
                     PT~\cite{lester2021power}            & 1.2      & 12.0               & 50.2     & 50.2               \\
                     PT2~\cite{liu2021p-tuning}      & 88.5     & 94.4               & 82.1     & 85.5               \\
                     PT2$^*$                     & 88.1     &94.2               & 81.3     & 84.7               \\
                     \bf Zero-init.                        & \bf 88.8 & \bf 94.6           & \bf 83.9 &  \bf 87.2           \\  
                     
                    \bottomrule
            \end{tabular}
\end{adjustbox}
\end{minipage}
\end{figure*}

\paragraph{Zero-initialized Attention.} 
Our proposed attention mechanism is essential for the early-stage training stability and final generation capacity of LLaMA-Adapter. As shown in Table~\ref{tab:zero_init_acc}, it contributes to a significant +43.08\% performance gain on the validation set. In contrast, the randomly initialized baseline only achieves 40.77\% accuracy, nearly the same as `Random Choice' (see Table~\ref{tab:scienceqa}'s first row). This comparison demonstrates the decisive role of zero-initialized attention in our approach.
In Figure~\ref{fig:zero_init_loss}, we plot the loss curves with and without the zero initialization, where the `zero-init attention' converges faster and reaches lower loss bounds than `rand-init attention'.



\paragraph{Robustness to Over-fitting.} 
As the fine-tuning data of large language models is normally much smaller-scale than the pre-training data, researchers have to carefully tune a set of hyperparameters to avoid over-fitting. In Table~\ref{tab:loss_vs_acc}, we show our LLaMA-Adapter is relatively robust to the over-fitting issue. Similar to the conclusion in~\cite{ouyang2022training}, even if our model has over-fitted the fine-tuning data, e.g., the validation loss marginally varies from 0.136 (15 epochs) to 0.282 (60 epochs), the validation accuracy is still increasing, e.g., from 82.08\% to 83.94\%. This is because, LLaMA-Adapter keeps the pre-trained LLaMA 7B model frozen, and only learns lightweight adapters with a few parameters. 

\subsection{Zero-initialized Attention for other Large Models}
\label{s4.4}

\paragraph{Settings.}
For image classification, we fine-tune the ViT-B/16~\cite{dosovitskiy2020image} pre-trained on supervised ImageNet-21k~\cite{deng2009imagenet} dataset. We adopt VTAB-1k~\cite{zhai2019large} for evaluation, which is a collection of 19 diverse visual tasks and organized into three groups according to the image domains: Natural, Specialized, and Structured. For extractive question answering, we follow P-tuning v2 (PT2)~\cite{liu2021p-tuning} to fine-tune the RoBERTa$_\mathrm{large}$~\cite{liu2019roberta} model on SQuAD~\cite{rajpurkar2016squad} v1.1 and v2.0 benchmark. Exact Match (EM) and F1 scores on the dev set are reported. We defer the evaluation on the name entity recognition (NER) and the semantic role labeling (SRL) tasks to Supplementary Material.
% For vision-language recognition, we adopt CLIP~\cite{radford2021learning} with a ViT-B/16 visual encoder as the pre-trained model, and test by the \textcolor{red}{base-to-novel generalization~\cite{} benchmark} on three datasets: ImageNet~\cite{deng2009imagenet}, Caltech101~\cite{caltech101}, and Flowers102~\cite{flowers102}. Please refer to Supplementary Material for experimental details.


\paragraph{Performance.}
We present the results of fine-tuning ViT and RoBERTa in Tables~\ref{t_6} and~\ref{t_7}, respectively. For three dataset groups with various image distributions, e.g., natural images, medical and satellite imagery, our approach achieves +3.26\%, +2.00\%, and +1.77\% improvement over VPT~\cite{jia2022visual}. On both SQuAD v1.1 and v2.0 dev sets, zero-initialized attention can boost P-tuning v2 with different margins, indicating strong language understanding capability. This demonstrates our superiority on traditional vision and language tasks compared to existing fine-tuning methods.


% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\textwidth]{figs/zero_init_loss.pdf}
%     \caption{\textbf{Loss Curves with and without Zero-init Attention.} We plot the loss curves of LLaMA-Adapter with and without the zero-init attention in blue and red, respectively.}
%     \label{fig:zero_init_loss}
% \end{figure}

% \input{tables/loss_vs_acc}

% \footnotetext[1]{Reported by PT2's~\href{https://github.com/THUDM/P-tuning-v2}{official GitHub repository}, over which we implement our method.}

\section{Conclusion}
In this paper, we propose LLaMA-Adapter, an efficient adaption method for training instruction-following models. With only 1.2M parameters and one-hour training, our approach effectively fine-tunes LLaMA with superior efficiency compared to the 7B-parameter Alpaca. 
For better training stability and final performance, we introduce zero-initialized attention with gating mechanism, which adaptively incorporates instructional signals, while preserving the pre-trained knowledge in LLaMA. LLaMA-Adapter can be generalized to image conditions for multi-modal reasoning, achieving competitive results on ScienceQA and COCO Caption benchmarks.
On traditional vision and language tasks, our zero-initialized attention also attains favorable fine-tuning performance, which indicates strong generalization capacity. \textbf{Limitation:} as our multi-modal variant presents a generic paradigm for incorporating external semantics, we will further extend LLaMA-Adapter to serve as a unified multi-modal framework, conditioned on a wide range of instructions, such as video, audio, and point clouds. We do not foresee negative social impact from the proposed work.


\appendix
\section{Appendix Overview}

\begin{itemize}
    \item Section~\ref{B}: Additional experiments of zero-initialized attention.
    \item Section~\ref{C}: Full comparison of instruction-following models.
    \item Section~\ref{D}: Comparison of LLaMA-Adapter and LLaMA-I.
    % \item Section~\ref{E}: Discussion of limitation and future works.
\end{itemize}


\section{Additional Experiments}
\label{B}

In this section, we provide more detailed experiments and analysis of applying our zero-initialized attention to fine-tune vision models, language models, and vision-language models, respectively.

\subsection{Detailed Results on Vision Tasks}

In Table~\ref{tab:vtab_per_task}, we compare the detailed fine-tuning results on VTAB-1k~\cite{zhai2019large} benchmark with 19 downstream visual tasks, which can be categorized into Natural (7 tasks), Specialized (4 tasks), and Structured (8 tasks), according to image domains. As shown, our zero-initialized attention outperforms VPT~\cite{jia2022visual} on most datasets (16 out of 19), and surpasses full fine-tuning along with other fine-tuning methods by large margins. This demonstrates the general efficacy of the proposed mechanism on a variety of image distributions.

\begin{table*}[h]
\centering
\caption{\textbf{Detailed Fine-tuning Results on VTAB-1k Benchmark.} We report the top-1 accuracy (\%) and adopt ViT-B/16~\cite{dosovitskiy2020image} pre-trained on supervised ImageNet-21k~\cite{deng2009imagenet} as the base model.}
\label{tab:vtab_per_task}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|rrrrrrrr|rrrrr|rrrrrrrrr}
\toprule
%     \multirow{2}{*}{Method} & \multicolumn{8}{c}{Natural} & \multicolumn{5}{|c|}{Specialized} & \multicolumn{9}{c}{Structure} \\
% \cmidrule(lr){2-9} \cmidrule(lr){10-14} \cmidrule(lr){15-23}
    & \rotatebox{90}{CIFAR100} & \rotatebox{90}{Caltech101} & \rotatebox{90}{DTD} & \rotatebox{90}{Flowers102} & \rotatebox{90}{OxfordPets} & \rotatebox{90}{SVHN} & \rotatebox{90}{SUN397} & \rotatebox{90}{\bf Mean} & \rotatebox{90}{Patch Camelyon} & \rotatebox{90}{EuroSAT} & \rotatebox{90}{Resisc45} & \rotatebox{90}{Retinopathy} & \rotatebox{90}{\bf Mean} & \rotatebox{90}{Clevr/count} & \rotatebox{90}{Clevr/distance} & \rotatebox{90}{DMLab} & \rotatebox{90}{KITTI/distance} & \rotatebox{90}{dSprites/location} & \rotatebox{90}{dSprites/orientation} & \rotatebox{90}{SmallNORB/azimuth} & \rotatebox{90}{SmallNORB/elevation} & \rotatebox{90}{\bf Mean}  \\
\midrule
    Full       & 68.9     & 87.7    & 64.3 & 97.2       & 86.9 & 87.4 & 38.8   & 75.9 & 79.7           & 95.7    & 84.2     & 73.9        & 83.4 & 56.3        & 58.6           & 41.7  & 65.5  & 57.5              & 46.7                 & 25.7          & 29.1          & 47.6 \\
\midrule
    Bias~\cite{zaken2022bitfit}       & 72.8     & 87.0    & 59.2 & 97.5       & 85.3 & 59.9 & 51.4   & 73.3 & 78.7           & 91.6    & 72.9     & 69.8        & 78.3 & 61.5        & 55.6           & 32.4  & 55.9  & 66.6              & 40.0                 & 15.7          & 25.1          & 44.1 \\
    Adapter~\cite{houlsby2019parameter}    & 74.1     & 85.7    & 62.7 & 97.8       & 87.2 & 34.6 & 50.7   & 70.4 & 76.3           & 87.5    & 73.7     & 70.9        & 77.1 & 45.2        & 41.8           & 31.2  & 56.4  & 31.9              & 25.4                 & 13.5          & 22.0          & 33.4 \\
    Sidetune~\cite{zhang2020side}   & 60.7     & 60.8    & 53.6 & 95.5       & 66.7 & 34.9 & 35.3   & 58.2 & 58.5           & 87.7    & 65.2     & 61.0        & 68.1 & 27.6        & 22.6           & 31.3  & 51.7  & 8.2               & 14.4                 & 9.8           & 21.8          & 23.4 \\
    VPT~\cite{jia2022visual}        & 78.8     & 90.8    & 65.8 & 98.0       & 88.3 & 78.1 & 49.6   & 78.5 & 81.8           & 96.1    & 83.4     & 68.4        & 82.4 & 68.5        & 60.0           & 46.5  & 72.8  & 73.6              & 47.9                 & 32.9          & 37.7          & 55.0 \\
\midrule
    \bf Zero-init. & 82.2     & 92.4    & 70.3 & 98.4       & 89.8 & 84.9 & 54.3   &\bf 81.7 & 83.6           & 95.3    & 85.0     & 73.8        &\bf 84.4 & 69.3        & 60.2           & 51.1  & 79.7  & 80.7              & 49.0                 & 30.6          & 33.6          &\bf 56.8 \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{More Experiments on Language Tasks}

For a more comprehensive evaluation of zero-initialized attention, we fine-tune RoBERTa$_\mathrm{large}$~\cite{liu2019roberta} on other two natural language processing tasks in addition to extractive question answering of the main paper, which are named entity recognition (NER) and semantic role labeling (SRL). We adopt CoNLL03~\cite{CONLL03}, CoNLL04~\cite{CONLL04}, CoNLL05~\cite{CONLL05}, and CoNLL12~\cite{CONLL12} as the evaluation datasets. As shown in Table~\ref{tab:CoNLL}, equipping P-tuning V2 (PT2)~\cite{liu2021p-tuning} with our zero-initialized attention can steadily improve the performance on all datasets with varying magnitudes, which indicates our effectiveness for different language tasks and applications.
% \vspace{0.2cm}

\begin{table*}[h]
\centering
\caption{\textbf{Language Model Fine-tuning} with RoBERTa$_\mathrm{large}$~\cite{liu2019roberta} on named entity recognition (NER) and semantic role labeling (SRL). We report the micro-f1 score. * denotes our reproduced results.}
\label{tab:CoNLL}
\resizebox{0.97\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
           \makecell*[l]{Method} & \multicolumn{1}{l}{CoNLL03~\cite{CONLL03}} & \multicolumn{1}{l}{CoNLL04~\cite{CONLL04}} & \multicolumn{1}{l}{CoNLL12~\cite{CONLL12}} & \multicolumn{1}{l}{CoNLL05$_{Brown}$~\cite{CONLL05}} & \multicolumn{1}{l}{CoNLL05$_{WSJ}$~\cite{CONLL05}} \\
\midrule
Full       & 92.6                          & 88.8                        & 86.5                        & 85.6                              & 90.2                            \\
PT~\cite{lester2021power}         & 86.1                          & 76.2                        & 67.2                        & 70.7                              & 76.8                            \\
PT2~\cite{liu2021p-tuning}        & 92.8                          & 88.4                        & 84.6                        & 84.3                              & 89.2                            \\
PT2$^*$\vspace{0.05cm}      & 91.8                          & 88.4                        & 84.7                        & 83.9                              & 89.4                            \\
\bf Zero-init. &\bf 92.4                          &\bf 88.8                        &\bf 85.2                        &\bf 84.7                              &\bf 89.6                           \\
\bottomrule
\end{tabular}
}
\end{table*}

% \vspace{0.5cm}
\subsection{Fine-tuning Vision-Language Models}

Besides ViT and RoBERTa, we also evaluate our approach on CLIP~\cite{radford2021learning}, a vision-language model pre-trained by 400 million text-image pairs. In detail, we adopt CLIP with a ViT-B/16 as the visual encoder and a 12-layer transformer~\cite{li2019attention} as the textual encoder. We test our fine-tuning results on base-to-novel generalization~\cite{zhou2022conditional} benchmark with three datasets, i.e., ImageNet~\cite{deng2009imagenet}, Caltech101~\cite{caltech101}, and Flowers102~\cite{flowers102}, where the model is trained only on the base classes in a few-shot
setting and evaluated on both base and novel categories. We freeze the entire CLIP and insert the adaption prompts with zero-initialized attention into CLIP's visual encoder.
As shown in Table~\ref{tab:clip}, our approach achieves the best average classification accuracy on both base and novel categories, demonstrating our fine-tuning capability for large vision-language models.
% \vspace{0.2cm}


\begin{table*}[h]
\centering
\caption{\textbf{Vision-Language Model Fine-tuning} with ViT-B/16 CLIP~\cite{radford2021learning} on base-to-novel generalization~\cite{zhou2022conditional} benchmark. We report the classification accuracy (\%) and harmonic mean (HM).}
\label{tab:clip}
\begin{adjustbox}{width=\linewidth}
             \begin{tabular}{l cccccccccccc}
                \toprule
                    \multirow{2}{*}{Method\ \ \ \ \ \ \ }
                    &\multicolumn{3}{c}{\makecell*[l]{ImageNet~\cite{deng2009imagenet}}} &\multicolumn{3}{c}{Caltech101~\cite{caltech101}} &\multicolumn{3}{c}{Flowers102~\cite{flowers102}}&\multicolumn{3}{c}{Average}\\
                    \cmidrule(lr){2-4}  \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
                    &Base &Novel &HM &Base &Novel &HM  &Base &Novel &HM &Base &Novel &HM\\
                    \cmidrule(lr){1-1}
                    \cmidrule(lr){2-4}  \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
                     CLIP~\cite{radford2021learning}                                 &  72.43 &68.14 &70.22 &96.84 &94.00 &95.40 & 72.08 &77.80 &74.83  &80.45 &79.98 &80.15           \\
                     CoOp~\cite{zhou2022learning}           &76.47 &67.88 &71.92 &98.00 &89.81 &93.73 &97.60 &59.67 &74.06 &90.69 &72.45 &79.90 \\
                     CoCoOp~\cite{zhou2022conditional}      & 75.98 &70.43 &73.10  &97.96 &93.81 &95.84   &94.87 &71.75 &81.71 &89.60 &78.66 &83.55         \\
                     MaPLe~\cite{khattak2022maple}\vspace{0.05cm}                     & 76.66 &70.54 &73.47  & 97.74 &94.36 &96.02   &95.92 &72.46 &82.56 &90.11 &79.12 &84.02       \\
                     \bf Zero-init.     & \bf 76.70 & \bf 71.00       &\bf73.74    & \bf 98.10 & \bf94.53 &\bf96.28 & \bf 96.00 & \bf 74.67       & \bf84.00  & \bf90.27 & \bf80.07 & \bf 84.67       \\  
                     
                    \bottomrule
            \end{tabular}
\end{adjustbox}
\end{table*}


\section{Full Comparison of Instruction-following Models}
\label{C}

In this section, we provide the full comparison of existing instruction-following models: Alpaca~\cite{alpaca}, Alpaca-LoRA~\cite{alpaca_lora}, GPT-3~\cite{brown2020language}, and our LLaMA-Adapter.
Our approach only fine-tunes 1.2M parameters within one hour, but generates responses comparable to the fully fine-tuned Alpaca and large-scale GPT-3, exhibiting a superior performance-efficiency trade-off.

\label{sec:instruct_full}
\input{tables/instruct_comparisons_full}

% \newpage
\section{Comparison with LLaMA-I}
% \vspace{-0.5cm}
\label{D}

In this section, we compare the generation quality of LLaMA-Adapter with LLaMA-I~\cite{touvron2023llama}, an instruction-fine-tuned LLaMA 65B model following~\cite{chung2022scaling}. All examples below are copied from the appendix of LLaMA~\cite{touvron2023llama}. Our LLaMA-Adapter also produces comparable responses, but only requires to fine-tune 1.2M parameters upon the LLaMA 7B model.

\label{sec:llama_i}
\input{tables/instruction_comparisons_llama}

\vspace{0.3cm}

% \vspace{0.3cm}
% \section{Discussion}
% \label{E}

% We have compared our LLaMA-Adapter with the representative instruction-following methods, i.e., Alpaca~\cite{alpaca}, Alpaca-LoRA~\cite{alpaca_lora}, GPT-3~\cite{brown2020language}, and LLaMA-I~\cite{touvron2023llama}.
% As the field of instruction large language models (LLMs) is rapidly developing, we further discuss our relations to some \textbf{\textit{very recent works within two months}}. Compared to LLaMA-Adapter, they utilize larger-scale foundation language models with more advanced training data, and can be categorized into instruction LLMs, chatbot LLMs, and multi-modal LLMs.

% \paragraph{Instruction LLMs.} 
% LLaMA-Adapter, Alpaca~\cite{alpaca} and Alpaca-LoRA~\cite{alpaca_lora} are all developed based on 52K instruction-following data generated by GPT-3.5~\cite{brown2020language}. Recently, LLaMA-GPT4~\cite{gpt4llm} utilizes the powerful GPT-4~\cite{OpenAI2023GPT4TR} to update the 52K training data, leading to a significant improvement in data quality. Therefore, instruction models fine-tuned by such data are expected to perform better than the previous Alpaca data. As our LLaMA-Adapter is a data-independent fine-tuning method, we can also upgrade LLaMA-Adapter with higher-quality instruction-following data like LLaMA-GPT4. We leave this as a future work.

% \paragraph{Chatbot LLMs.} 
% Different from the vanilla instruction-following models, a chatbot LLM can have many rounds of conversations with humans, such as ChatGPT~\cite{chatgpt} and GPT-4~\cite{OpenAI2023GPT4TR}. To achieve this, Vicuna~\cite{vicuna2023} leverages 70K user-shared conversation data from ShareGPT\footnote{\url{https://sharegpt.com}} to fine-tune LLaMA into a chat model. 
% Similarly, Open-Assistant\footnote{\url{https://github.com/LAION-AI/Open-Assistant}} collects 22K human demonstrations of assistant conversations, and trains a chat model based on Pythia-12B~\cite{biderman2023pythia}. However, both Vicuna and Open-Assistant fine-tune the full parameters in LLMs, and the efficient fine-tuning of chat models is still rarely explored. We believe LLaMA-Adapter can take a further step in parameter-efficient chatbot LLMs if given the conversation data like Vicuna and Open-Assistant.

% \paragraph{Multi-modal LLMs.} 
% GPT-4~\cite{OpenAI2023GPT4TR} has demonstrated superior visual understanding capabilities such as meme explanation, document-level reasoning, and exam problem solving. The recent MiniGPT4~\cite{zhu2023minigpt} and LLaVA~\cite{llava} connect a pre-trained vision model (CLIP~\cite{radford2021learning}) with a chat model (Vicuna~\cite{vicuna2023}) by fine-tuning on visual instruction data generated from GPT-4. Our experiments on ScienceQA~\cite{scienceqa} benchmark have indicated that LLaMA-Adapter has the potential for visual understanding and multi-modal tasks. Therefore, it is natural to further develop a more advanced multi-modal LLaMA-Adapter based on the image-text data generated by GPT-4.

% \clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
\bibliographystyle{ieee_fullname}
\bibliography{neurips}
}

\end{document}
