% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
% \usepackage{emoji}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{floatrow}
\usepackage{caption}
\usepackage{xcolor}
\definecolor{citecolor}{HTML}{2980b9}
\definecolor{linkcolor}{HTML}{c0392b}
% \usepackage{subcaption}
% \usepackage{subfig}
% \usepackage{pgfplots}
% \floatsetup[table]{capposition=bottom,style = Plaintop}
% \floatsetup{heightadjust=object}
\usepackage{stfloats}

\usepackage{xcolor}
\usepackage{color, colortbl}
\definecolor{zrrgreen}{HTML}{008000}
\definecolor{zrrblue}{HTML}{4682B4}
\definecolor{zrrred}{HTML}{B22222}


\usepackage{adjustbox}
\usepackage{colortbl}
% \usepackage[table,dvipsnames, svgnames, x11names]{xcolor}

\makeatletter
  \newcommand\figcaption{\def\@captype{figure}\caption}
  \newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running .
% (Or just hit 'q' on the first  run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks,]{hyperref}
\usepackage[hidelinks,pagebackref=true,breaklinks=true,colorlinks,bookmarks=false,citecolor=citecolor,letterpaper=true,linkcolor=linkcolor]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\usepackage{framed}
\usepackage{makecell}
\usepackage{listings}
\lstset{
    escapeinside={(|}{|)}
}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\lstdefinelanguage{JavaScript}{
  keywords={break, case, catch, continue, debugger, default, delete, do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  ndkeywords={class, export, boolean, throw, implements, import, this},
  keywordstyle=\color{blue}\bfseries,
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  sensitive=true
}
\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{XXXX} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{LLaMA-Adapter: Efficient Fine-tuning of Language Models\\with Zero-init Attention}

% \title{LLaMA-Adapter: Instruction-following Language Models\\with Parameter-efficient Zero-init Attention}

\author{Renrui Zhang$^{*1,2}$, Jiaming Han$^{*1}$, Aojun Zhou$^{2}$, Xiangfei Hu$^{1}$, Shilin Yan$^{1}$\\Pan Lu$^{3}$, Hongsheng Li$^{2}$, Peng Gao$^{1}$, Yu Qiao$^{1}$\vspace{0.3cm}\\
  $^1$Shanghai Artificial Intelligence Laboratory\quad 
  $^2$CUHK MMLab\\
  $^3$University of California, Los Angeles\\
\texttt{\{zhangrenrui, hanjiaming, gaopeng, qiaoyu\}@pjlab.org.cn}
}
\maketitle
\blfootnote{$^*$ Equal contribution}
%%%%%%%%% ABSTRACT
\begin{abstract}
We present \textbf{LLaMA-Adapter}, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces \textbf{1.2M} learnable parameters upon the frozen LLaMA 7B model, and costs less than \textbf{one hour} for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, \emph{e.g.}, images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA.
We release our code at \url{https://github.com/ZrrSkywalker/LLaMA-Adapter}.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\begin{figure}[t]
  \centering
% \vspace{0.2cm}
\includegraphics[width=\textwidth]{figs/adapter-fig1-v3.pdf}
% \vspace{-0.1cm}
   \caption{\textbf{Characteristics of LLaMA-Adapter.} Our lightweight adaption method tunes LLaMA into an instruction-following model with only 1.2M learnable parameters and one-hour training. LLaMA-Adapter is plug-and-play for different downstream expertise and can be generalized to multi-modal reasoning.}
    \label{fig1}
\vspace{0.2cm}
\end{figure}

Large-scale Language Models (LLMs)~\cite{dai2019transformer,radford2019language,zhang2022opt,raffel2020exploring,devlin2018bert} have stimulated widespread attention in both academia and industry. Driven by massive corpora and advanced hardware, LLMs exhibit remarkable understanding and generative ability, propelling language tasks into a higher level. Recently, significant progress has been made on instruction-following models, \emph{e.g.}, ChatGPT\footnote{\url{https://chat.openai.com}} and GPT-3.5 (text-davinci-003)~\cite{brown2020language}. Following instructions or commands in natural language, they can generate professional and contextual responses in a conversational way. However, the further prevalence of instruction-following models is largely impeded by the closed-source restriction and high development costs.


To alleviate this, Stanford Alpaca~\cite{alpaca} proposes to fine-tune an LLM, \emph{i.e.}, LLaMA~\cite{touvron2023llama} into an instruction-following model, which is affordable and replicable. Starting from 175 human-written instruction-output pairs~\cite{selfinstruct}, Alpaca leverages GPT-3.5 to expand the training data to 52K in a self-instruct manner. Supervised by this, Alpaca fine-tunes the entire 7B parameters in LLaMA, producing an exceptional model that performs similarly to GPT-3.5. Despite Alpaca's effectiveness, a complete fine-tuning of large-scale LLaMA is still time-consuming, computation-intensive, multi-modality unsupported and cumbersome to transfer to different downstream scenarios.


In this paper, we introduce \textbf{LLaMA-Adapter}, an efficient fine-tuning method that adapts LLaMA into a well-performed instruction-following model. We also utilize the 52K instruction-output data for training purposes, but demonstrate superior resource efficiency to Alpaca. Specifically, in LLaMA's higher transformer layers, we append a set of learnable adaption prompts as prefix to the input instruction tokens.
These prompts learn to adaptively inject new instructions (conditions) into LLaMA.
To avoid noise from adaption prompts at the early training stage, we modify the vanilla attention mechanisms at inserted layers to be zero-init attention, with a learnable gating factor. Initialized by zero vectors, the gating can firstly preserve the original knowledge in LLaMA, and progressively incorporate instructional signals during training. This contributes to stable learning during the fine-tuning process and better instruction-following capacity of the final model. 

Overall, our LLaMA-Adapter exhibits four main characteristics, as shown in Figure~\ref{fig1}.

\begin{itemize}
   \item \textbf{1.2M Parameters.} 
   Instead of updating the full 7B parameters, we freeze the pre-trained LLaMA and only learn the adaption prompts with 1.2M parameters on top. This, however, reveals comparable instruction-following proficiency with the 7B Alpaca.
   
   \item \textbf{One-hour Fine-tuning.}
   Thanks to lightweight parameters and our zero-init gating, the convergence of LLaMA-Adapter costs less than one hour on 8 A100 GPUs, three times faster than Alpaca.
   

   \item \textbf{Plug with Expertise.}
   For different scenarios, it is flexible to insert their respective adapters and endow LLaMA with different expert knowledge. Thus, it suffices to store a 1.2M adapter within each context, other than a complete copy of the 7B model.

   
   \item \textbf{Multi-modal Condition.}
   Besides textual instruction, LLaMA-Adapter can be extended to image input for multi-modal reasoning. By simply adding images tokens into adaption prompts, LLaMA-Adapter performs competitively on the ScienceQA benchmark.
   
\end{itemize}



\section{Related Work}


\subsection{Instruction-Following Language Models} 

The subfield of language models focusing on the instruction-following capabilities is crucial for generating responses based on natural language commands. Instruction-following methods enhance pre-trained models by fine-tuning them using high-quality input-output tuples of task instructions and ground truth outputs. This fine-tuning helps the model better understand user intentions and follow instructions more accurately. Instruction-following methods have been extensively researched in language models~\cite{wei2021finetuned, wang2022super, bach2022promptsource, ouyang2022training} and multi-modality domains~\cite{Shridhar2019ALFREDAB, Min2021FILMFI}. Among those methods, FLAN~\cite{wei2021finetuned} introduces an instruction tuning method that outperforms non-tuned LLMs in unseen tasks. PromptSource~\cite{bach2022promptsource} provides development environment and repository that offers a web-based GUI for creating and managing natural language prompts for zero-shot or gradient-based few-shot learning. SUP-NATINST~\cite{wang2022super} establishes a large benchmark of 1,616 diverse NLP tasks and uses multi-task training on the T5 model, demonstrates strong generalization capabilities on unseen tasks. InstructGPT~\cite{ouyang2022training} demonstrates significant performance improvements and may be integrated into closed-source models like GPT-3.5 and GPT-4~\cite{OpenAI2023GPT4TR}. The open-source Stanford-Alpaca~\cite{alpaca} approach fine-tunes all parameters of LLMs in an end-to-end manner. However, this full-model fine-tuning can be computationally intensive and challenging to scale to larger pre-trained language models. 
In contrast, this paper aims to fine-tune lightweight adapters on top of the frozen large-scale pre-trained models, \emph{e.g.}, LLaMA~\cite{touvron2023llama}, rather than performing end-to-end fine-tuning of all parameters. Our approach reduces computational demands and facilitates the efficient adaptation of LLMs to instruction-following tasks while maintaining high performance.
%In our paper, we aim to finetune only a light-weight adapter upon the frozen large pre-trained models instead of end-to-end finetuning all parameters.
 
 %Reinforcement Learning from Human Feedback (RLHF) is a common method used to achieve this. By fine-tuning models in this way, they become more aligned with human intention.

\subsection{Large Vision-Language Models}
% early vl models, pre-trained model -> vl model: frozen, clipcap, flamingo, blipv2 etc.
Over the past decade, we have witnessed a shift in vision-language research from task-specific models~\cite{vinyals2015show,karpathy2015deep,yang2016stacked,santoro2017simple,jiang2020defense} to large foundation models~\cite{radford2021learning,wang2021simvlm,bao2022vlmo,wang2022ofa,alayrac2022flamingo,wang2022git,wang2022image,li2023blip}. After pre-training on large-scale image-text data, such large vision-language models can be adapted to a variety of downstream tasks with powerful performance. A line of works~\cite{radford2021learning,wang2021simvlm,wang2022ofa} train both visual and textual encoders from scratch, leading to a high computation cost. Recently, another line of works~\cite{alayrac2022flamingo,wang2022git,li2023blip,zhai2022lit} adopt pre-trained unimodal models as initialization and only train the newly introduced parameters. For example, LiT~\cite{zhai2022lit} utilizes pre-trained image encoder to speed up CLIP~\cite{radford2021learning} training. Frozen~\cite{tsimpoukelli2021multimodal} fine-tunes an image encoder to transform visual tokens into LLM's soft prompts. Similarly, CLIPCap~\cite{mokady2021clipcap} proposes a mapping network to connect the pre-trained image encoder with LLMs. Flamingo~\cite{alayrac2022flamingo} inserts several cross-attention layers to inject visual knowledge into LLMs. BLIP2~\cite{li2023blip} connects pre-trained image encoders and LLMs with a Q-Former. CLIP-Adapter~\cite{gao2021clip}, Tip-Adapter~\cite{zhang2021tip,zhang2023prompt} and PointCLIP~\cite{zhu2022pointclip,zhang2022pointclip} introduce customized adapters upon CLIP for 2D and 3D few-shot learning. To summary, these methods use mapping networks or cross-attention layers to connect vision and languages. Our work also belongs to the second line of works. Differently, our method only introduces a few learnable parameters and progressively injects visual features into pre-trained LLMs with a simple but efficient zero-init attention.


\subsection{Parameter-Efficient Fine-Tuning} 

Parameter-Efficient Fine-Tuning (PEFT)~\cite{peft} methods facilitate efficient adaptation of LLMs without the need to update all model parameters, thereby reducing the cost and improving the efficiency of fine-tuning large models. Various PEFT techniques include Prefix Tuning~\cite{li2021prefix}, Low-Rank adaptation (LoRA)~\cite{hu2021lora} and the insertion of adapter layers in pre-trained large language models~\cite{houlsby2019parameter,pfeiffer2020adapterfusion,lin2020exploring}.  Prefix Tuning~\cite{li2021prefix} appends a collection of prefixes to autoregressive language models, or alternatively, incorporates prefixes for both encoder and decoder components, similar methods proposed in ~\cite{lester2021power}. LoRA~\cite{hu2021lora} introduces trainable rank decomposition matrices into each layer~\cite{karimi2021compacter}. Adapters~\cite{houlsby2019parameter} involves inserting lightweight modules into each layer of pre-trained models, which only updates the adapters and has been extended across numerous domains~\cite{pfeiffer2020adapterfusion}.

% Existing PEFT methods mainly focus on traditional language tasks. 
In this paper, we fine-tune pre-trained language models for instruction-following capabilities (response to instructs), and are distinct from existing ones in two aspects.

\begin{itemize}
   \item \textbf{Zero-init Attention.} 
   Prevalent PEFT methods might potentially disturb the pre-trained linguistic knowledge by directly inserting randomly initialized modules. This leads to unstable fine-tuning with large loss value at early training stages. LLaMA-Adapter adopts a zero-init attention with gating to mitigate this.

   \item \textbf{Unified Multi-modal Tuning.}
   Previous PEFT methods are normally developed to address specific modalities, such as language, image, and audio. In contrast, LLaMA-Adapter can handle both language and multi-modality fine-tuning with a unified manner, demonstrating superior generalization ability.
   

\end{itemize}

% \vspace{0.1cm}
\section{LLaMA-Adapter}

In Section~\ref{s1}, we first introduce how to insert the learnable adaption prompts into LLaMA's transformer. Then in Section~\ref{s2}, we present the details of zero-init attention mechanisms with zero gating. Finally in Section~\ref{s3}, we generalize LLaMA-Adapter for multi-modal reasoning.

\begin{figure}[t]
  \centering
% \vspace{0.2cm}
\includegraphics[width=0.96\textwidth]{figs/adapter-fig2-v5.pdf}
\vspace{-0.2cm}
   \caption{\textbf{Details of LLaMA-Adapter.} We insert lightweight adapters with learnable prompts into $L$ out of $N$ transformer layers of LLaMA. Aided by zero-init attention and gating mechanisms, the adaption prompt progressively learns new instructional cues, without disturbing the original pre-trained knowledge.}
    \label{fig2}
\vspace{0.2cm}
\end{figure}

\subsection{Learnable Adaption Prompts}
\label{s1}

Given 52K instruction-to-output data~\cite{selfinstruct} and a pre-trained LLaMA~\cite{touvron2023llama} with an $N$-layer transformer, we adopt a set of learnable adaption prompts for instruction-following fine-tuning. We denote the prompts for $L$ transformer layers as $\{P_l\}_{l=1}^{L}$, where $P_l \in \mathbb{R}^{K\times C}$ with $K$ denoting the prompt length for each layer, and $C$ equaling the feature dimension of LLaMA's transformer. Note that we insert the prompts into the topmost $L$ layers of the transformer ($L\leq N$). This can better tune the language representations with higher-level semantics.

Taking the $l$-th inserted layer as an example, we denote the $M$-length word tokens as $T_l \in \mathbb{R}^{M\times C}$. Then, the adaption prompt is concatenated with $T_l$ along the token dimension as prefix, formulated as 
\begin{align}
    [P_l;\ T_l]\  \in \mathbb{R}^{(K+M)\times C}.
\end{align}
In this way, the instruction knowledge learned within $P_l$ can effectively guide $T_l$ to generate contextual responses.

\begin{figure*}[t!]
  \centering
    \includegraphics[width=\textwidth]{figs/adapter-fig3-v4.pdf}
% \vspace{0.05cm}
   \caption{\textbf{Multi-modal Reasoning of LLaMA-Adapter.} On the ScienceQA benchmark~\cite{scienceqa}, LLaMA-Adapter is extended to multi-modal variant for image-conditioned question answering. Given an image as the visual context, we acquire the global image token by multi-scale concatenation and projection. Then, we element-wisely add the image token onto adaption prompts of $L$ inserted layers. In this way, LLaMA-Adapter achieves competitive reasoning capability based on multi-modal conditions.}
    \label{fig3}
    % \vspace{-0.3cm}
\end{figure*}

\subsection{Zero-init Attention}
\label{s2}
If the adaption prompts are randomly initialized, they might bring disturbance to the word tokens at the beginning of training, which harms the fine-tuning stability and effectiveness. Considering this, we modify the vanilla attention mechanisms at the last $L$ transformer layers to be zero-init attention, as shown in Figure~\ref{fig2}. 
Suppose the model is generating the $(M+1)$-th word on top of $[P_l;\ T_l]$ at the $l$-th inserted layer, we denote the corresponding $(M+1)$-th word token as $t_l \in \mathbb{R}^{1\times C}$. In the attention mechanism, several linear projection layers are first applied to transform the input tokens into queries, keys, and values as
\begin{align}
    Q_l &= \operatorname{Linear_{q}}(\ t_l\ ),\\
    K_l &= \operatorname{Linear_{k}}(\ [P_l;\ T_l;\ t_l]\ ),\\
    V_l &= \operatorname{Linear_{v}}(\ [P_l;\ T_l;\ t_l]\ ).
\end{align}
Then, the attention scores before the softmax function are calculated as
\begin{align}
    S_l &= Q_l K_l^T/\sqrt{C}\ \in \mathbb{R}^{1\times (K+M+1)},
\end{align}
which records the feature similarities between $t_l$ and all $K+M+1$ tokens. Meanwhile, $S_l$ can be reformulated by two components as 
\begin{align}
    S_l &= [S_l^K;\ S_l^{M+1}]^T,
\label{eq1}
\end{align}
where $S_l^K \in \mathbb{R}^{K\times 1}$ and $S_l^{M+1} \in \mathbb{R}^{(M+1)\times 1}$ denote the attention scores of $K$ adaption prompts and $M+1$ word tokens, respectively.
The former $S_l^K$ represents how much information the learnable prompt contributes to $t_l$, which probably causes disturbance in the early training stage.


To this end, we adopt a learnable gating factor, denoted as $g_l$, to adaptively control the importance of $S_l^K$ in the attention. Initialized by zero, $g_l$ can firstly eliminate the influence of under-fitted prompts, and then increase its magnitude for providing more instruction semantics to LLaMA.
Therefore, we independently apply the softmax functions to the two components in Equation~\eqref{eq1}, and multiply the first term by $g_l$, formulated as
\begin{align}
    S_l^g &= [\operatorname{Softmax}(S_l^K)\cdot g_l;\ \ \operatorname{Softmax}(S_l^{M+1})]^T.
\end{align}
The separate softmax functions ensure the second term to be irrelevant to our adaption prompts. 
When $g_l$ is close to zero, it can convey the originally pre-trained knowledge of LLaMA to token $t_l$ for creditable generation. In practice, we adopt multiple $g_l$ to be independently learned for different heads of the attention mechanism.

Finally, we calculate the output of the attention layer with a linear projection layer as
\begin{align}
    t_{l}^o = \operatorname{Linear_{o}}(S_l^g V_l)\ \in \mathbb{R}^{1\times C}.
\end{align}
With our proposed zero-init attention, the adaption prompts progressively inject the newly acquired instructional knowledge into LLaMA, while effectively incorporates its pre-trained ability to provide high-quality responses.


\subsection{Multi-modal Reasoning}
\label{s3}

Not limited to textual instructions, LLaMA-Adapter is capable of answering a question based on input of other modalities, which augments the language model with rich cross-modal information. As shown in Figure~\ref{fig3}, we take the ScienceQA benchmark~\cite{scienceqa} as examples. Given \textcolor{zrrgreen}{\textbf{visual}} and \textcolor{zrrgreen}{\textbf{textual contexts}}, along with the corresponding \textcolor{zrrblue}{\textbf{question}} and \textcolor{zrrblue}{\textbf{options}}, the model is required to conduct multi-modal reasoning to give the correct \textcolor{zrrred}{\textbf{answer}}.


For an input image as the visual context, we first leverage a pre-trained visual encoder, \emph{e.g.}, CLIP~\cite{radford2021learning}, to extract its multi-scale global features, denoted as $\{I_m\}_{m=1}^{M}$, where $I_m \in \mathbb{R}^{1\times C_m}$ and $M$ denotes the scale number. Then, we concatenate the $M$-scale features along the channel dimension and apply a learnable projection network on top, formulated as
\begin{align}
    I_p = \operatorname{Projection}\Big(\operatorname{Concat}\big(\{I_m\}_{m=1}^M\big)\Big),
\end{align}
where $I_p \in \mathbb{R}^{1\times C}$ and is regarded as the overall image token with the same feature dimension as our adaption prompts. 
After this, we repeat $I_p$ for $K$ times, and element-wisely add it onto the $K$-length adaption prompts at all $L$ inserted transformer layers. For the $l$-th layer, we denote the acquired multi-modal prompt as
\begin{align}
    P_l^v = P_l + \operatorname{Repeat}(I_p)\ \in \mathbb{R}^{K\times C},
\end{align}
where $P_l^v$ denotes the multi-modal prompt incorporated with visual information from the given image context. In this way, LLaMA is fine-tuned to generate responses conditioned on vision-language inputs, and can tackle more challenging generative tasks with multi-modal understanding, such as the ScienceQA benchmark~\cite{scienceqa}.

As a general framework, LLaMA-Adapter with additional input condition can also be extended to video and audio modalities. Using the pre-trained modal-specific encoders, we can integrate instructional signals of different modalities into the adaption prompts, which further maximizes the comprehension and generative capacity of LLaMA. We leave this as a future work.

% We leverage the question-answering data from ScienceQA~\cite{} to train our image-conditioned LLaMA-Adapter, which demonstrates competitive performance with explicit multi-hop reasoning.

% \section{Experiment}

\vspace{0.2cm}
\section{Instruction-following Evaluation}
In this section, we evaluate the instruction-following capacity of LLaMA-Adapter by responding to instructions.

\vspace{0.1cm}
\subsection{Experimental Details}

\paragraph{Training Data.} We use 52K instruction-following data from Stanford Alphaca~\cite{alpaca} for training, denoted as Alphaca-52K. Each sample in Alphaca-52K contains the following fields:
\{instruction\} is the description of a task, \{input\} is the context for the task, and \{output\} is the answer generated by GPT-3.5 (text-davinci-003)~\cite{brown2020language}. Note that around 40\% of the examples include an input.
% TODO: whether to put the prompt details here?
% \noindent \fbox{\begin{minipage}{0.98\linewidth}
% Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
% \\

% \#\#\# Instruction: \\
% \{instruction\}
% \\

% \#\#\# Input: \\
% \{input\}
% \\

% \#\#\# Response:\\
% \{output\}

% \end{minipage}}


\input{tables/instruction_comparisons_short}

\paragraph{Implementation Details.} We build LLaMa-Adapter based on the original LLaMa codebase\footnote{\url{https://github.com/facebookresearch/llama}} with minor modifications. We train LLaMa-Adapter on 8 A100 GPUs for 5 epochs. The warmup epochs, batch size, learning rate, and weight decay are set to 2, 64, 9e-3, and 0.02, respectively. In general, we utilize the pre-trained LLaMA model with 7B parameters and $N=32$ transformer layers as the base model. We set the prompt length $K=10$ and insert prompts into the last $L=30$ layers by default. Other variants of LLaMA-Adapter with different inserted layers are also released in our code. In the generation stage, we adopt \textit{top-p} sampling as the default decoding method with a temperature $0.1$ and a $\textit{top-p}=0.75$.


\subsection{Performance} 
We compare LLaMA-Adapter with the representative instruction-following method, Alphaca~\cite{alpaca}, in Table~\ref{tab:instruction_comparisons_short}, and present the full comparison with Alpaca-LoRA~\cite{hu2021lora} and GPT-3~\cite{brown2020language} in Appendix~\ref{sec:instruct_full}. As there still lack of rigorous metrics for evaluation, we simply show some response examples for comparison. As shown, by only fine-tuning 1.2M parameters, our approach generates reasonable responses comparable to the fully fine-tuned Alpaca and the large-scale GPT-3. This fully demonstrates the effectiveness of our adapters with zero-init attention.

% We put full comparisons with other methods in Appendix~\ref{sec:instruct_full}. 

% other instruction-following models, \emph{i.e.}, Alphaca~\cite{alpaca}, Alpaca-LoRA~\cite{alpaca_lora} and GPT-3~\cite{}. 

% with Alpaca in Tab.~\ref{tab:instruction_comparisons_short}. LLaMA-Adapter can also generate reasonable responses like fully fine-tuned Alpaca. Due to space limit, we put full comparisons with other methods in Appendix~\ref{sec:instruct_full}. 

In Appendix~\ref{sec:llama_i}, we also compare LLaMA-Adapter with LLaMA-I, \emph{i.e.}, LLaMA-65B fine-tuned on large-scale instructional data. As shown, LLaMA-Adapter is capable of various non-trivial tasks, such as dialog generation, code generation, and question answering \emph{etc.} We believe LLaMa-Adapter can be further enhanced by combining larger LLaMA models, enlarging the amount of training data, and scaling up learnable parameters.

\subsection{Efficiency} 
In Table~\ref{tab:training_efficiency}, we compare the learnable parameters, storage space, and training time of different instruction-following methods. 
As a lightweight plug-and-play module, LLaMA-Adapter enjoys superior training efficiency with only 1.2M parameters, 4.9M storage, and one-hour training. This enables us to fine-tune large-scale language models (LLaMA) on cheap and mobile devices. LLaMA-Adapter's efficiency advantages can be further revealed by multi-node training, since only the gradients of 1.2M parameters are transferred among nodes, other than Alpaca's 7B.


% We only require to fine-tune the adaption prompts with the gating factor, instead of updating the whole LLaMA. In Tab.~\ref{tab:training_efficiency}, we compare the amount of trainable parameters and training time with Alphaca and Alphaca-LoRA, where our LLaMA-Adapter only introduces \textbf{1.2M} parameters and can be trained in \textbf{10 minutes}. This allows us to train large language models (LLaMA) on cheap devices more easily.


% \subsection{LLaMA-Adapter Variants}
% % TODO: consider move it to scienceQA section.
% As introduced in Sec.~\ref{s1}, changing the prompt length $K$ and prompting layers $L$ leads to different variants of LLaMA-Adapter. It gives us more flexibility to balance training cost and instruct performance.
% \input{tables/adapter_variants}

\input{tables/scienceqa}
\input{tables/training_efficiency}


\section{Multi-modal Evaluation}

In this section, we illustrate the multi-modal reasoning capability of LLaMA-Adapter on the ScienceQA benchmark~\cite{scienceqa} and conduct ablation studies to verify the effectiveness of our designs.

\subsection{Experimental Details}
\paragraph{Training Data.} 
We train the multi-modal LLaMA-Adapter on ScienceQA~\cite{scienceqa}, a large-scale multi-modal and multi-choice science question dataset collected from a wide range of domains\footnote{\url{https://scienceqa.github.io/explore}}. Figure~\ref{fig:scienceqa_examples} gives two examples in ScienceQA. Each example normally contains a \textcolor{zrrgreen}{\textbf{visual context}}, a \textcolor{zrrgreen}{\textbf{textual context}}, a \textcolor{zrrblue}{\textbf{question}}, multiple \textcolor{zrrblue}{\textbf{options}}, and an \textcolor{zrrred}{\textbf{answer}}.
We omit the lecture and explanation for simplicity. The model is required to output the correct answer based on the multi-modal context and choices. The official question answering accuracy is adopted as the evaluation metric.


\paragraph{Implementation Details.} 
We organize the textual input of LLaMA with one sentence, in an order of question, textual context, and options. For the visual context, we adopt the pre-trained CLIP~\cite{radford2021learning} as the visual encoder to extract its multi-scale and global features. We utilize simple MLPs as the projection network before adding the image tokens to the adaption prompts.
Since the pre-trained visual encoder is strong enough, we do not use the captioning data in ScienceQA. 
In the generation stage, we adopt greedy search as the decoding method. 
We keep other settings the same as single-modal LLaMA-Adapter if not specified.

\vspace{0.1cm}
\subsection{Performance}
In Table~\ref{tab:scienceqa}, we compare LLaMA-Adapter with popular VQA methods~\cite{yu2019mcan,Anderson2017up,Kim2018,gao2019dynamic,pmlr-v139-kim21k,lu2021iconqa,li2019visualbert,li2020does} and language models~\cite{khashabi2020unifiedqa,brown2020language,zhang2023multicot}.
As shown, our single-modal variant (`LLaMA-Adapter$_T$') attains 78.31\% accuracy with 1.2M parameters. By further injecting visual conditions with a 0.6M projection network, our multi-modal variant (`LLaMA-Adapter') is boosted by +6.88\% answering accuracy. 
Traditional VQA methods are required to train the entire network with considerable resource budget, while LLaMA-Adapter only fine-tunes a few parameters with better answering performance.
Compared to GPT-3~\cite{brown2020language}, despite its zero-shot answering capacity without fine-tuning, GPT-3 contains 175B total parameters, much larger than our 7B LLaMA with 1.2M adapters. Also, as a language model, GPT-3 can not leverage any additional visual information. In contrast, LLaMA-Adapter can be easily switched into multi-modal variant and achieves +10\% higher accuracy.
Besides, we notice that MM-CoT~\cite{zhang2023multicot} is on par with our approach, but it relies on the complex two-stage inference. We believe our LLaMA-Adapter can also be boosted and leave the exploration of chain-of-thought for future research.

\begin{figure*}[t!]
    \centering
    % \vspace{0.3cm}
    \includegraphics[width=\textwidth]{figs/scienceqa_examples.pdf}
    \caption{Two examples of \textbf{Multi-modal Reasoning on ScienceQA~\cite{scienceqa}} by LLaMA-Adapter.\vspace{0.2cm}}
    \label{fig:scienceqa_examples}
    % \vspace{0.3cm}
\end{figure*}


% GPT-3 can perform zero-shot and few-shot question answering by constructing a suitable prompt, but its total parameters are much large than LLaMA-Adapter (175B \emph{vs.} 7B). Besides, as a language model, GPT-3 can not leverage any visual information. In contrast, LLaMA-Adapter can easily switch between single modal and multi-modal. 

% As shown in Tab.~\ref{tab:scienceqa}, our single modal method (LLaMA-Adapter$_T$) achieves 78.31\% accuracy. By injecting visual tokens into LLaMA-Adapter, our multi-modal method can further bring 7\% accuracy improvement. 

% Moreover, we notice that MM-CoT~\cite{zhang2023multicot} is on par with our method, but it relies on a complex two-stage inference method. We believe our LLaMA-Adapter can also be boosted and leave the exploration of chain-of-thought for future research.

\vspace{0.2cm}
\subsection{Ablation Study}

\paragraph{Insertion Layers.} 
We first investigate the number of transformer layers to be inserted. As shown in Table~\ref{tab:layers_vs_acc}, increasing the layer numbers introduce more learnable parameters, but leads to a significant improvement on the accuracy of validation set, \emph{e.g.}, +17.41\% from 10 to 30, and +10.49\% from 20 to 30. It indicates that more adaption prompts can provide stronger task-specific guidance to the pre-trained LLaMA. 
This encourages us to adopt more inserted transformer layers for the larger LLaMA model with 65B parameters in the future. 
% On the other hand, since none-prompting layers do not requires gradients, we can reduce the training cost by adjusting the number of prompting layers.

\paragraph{Zero-init Attention.} 
Our proposed zero-init attention in LLaMA-Adapter is essential for the early-stage training stability and final generation capacity. As shown in Table~\ref{tab:zero_init_acc}, it contributes to a significant +43.27\% performance gain on SciceneQA's validation set. In contrast, the randomly initialized baseline only achieves 40.77\% accuracy, nearly the same as `Random Choice' (see Table~\ref{tab:scienceqa}'s first row). This comparison demonstrates the decisive role of zero-init attention in our approach.
In Figure~\ref{fig:zero_init_loss}, we plot the loss curves with and without the zero-init attention, where the `zero-init attention' converges faster and reaches lower loss bounds than `rand-init attention'.

\input{tables/layers_vs_acc}

\input{tables/zero_init_acc}



\paragraph{Robustness to Over-fitting.} 
Since the data for fine-tuning is much smaller-scale than that for pre-training, researchers have to carefully tune a set of hyperparameters to avoid over-fitting of LLMs. In Table~\ref{tab:loss_vs_acc}, we show our LLaMA-Adapter is relatively robust to the over-fitting issue. Similar to the conclusion in~\cite{ouyang2022training}, even if our model has over-fitted the fine-tuning data, \emph{e.g.}, the validation loss marginally varies from 0.136 (15 epochs) to 0.282 (60 epochs), the validation accuracy is still increasing, \emph{e.g.}, from 82.08\% to 83.94\%. One possible reason is that LLaMA-Adapter only introduces a few learnable parameters and keep the pre-trained 7B LLaMA frozen. Therefore, a small-scale dataset can also fully fine-tune our adapters.



\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/zero_init_loss.pdf}
    \caption{\textbf{Loss Curves with and without Zero-init Attention.} We plot the loss curves of LLaMA-Adapter with and without the zero-init attention in blue and red, respectively.}
    \label{fig:zero_init_loss}
\end{figure}

\input{tables/loss_vs_acc}



\section{Conclusion}
In this paper, we propose LLaMA-Adapter, an efficient adaption method for training instruction-following language models. With only 1.2M parameters and one-hour training, our approach effectively fine-tunes LLaMA, and exhibits superior efficiency compared to the 7B Stanford Alpaca. For better training stability and final performance, we propose a zero-init attention with a gating mechanism, which adaptively incorporates instructional signals, while preserving the pre-trained generative knowledge in LLaMA. Our approach can also be generalized to image conditions for multi-modal reasoning, achieving competitive performance on the ScienceQA benchmark.
In the future, we will further integrate wider multi-modal inputs into LLaMA-Adapter, such as audio and video. More experiments on larger LLaMA models (33B, 65B parameters), and diverse benchmarks (VQA v2~\cite{goyal2017making}, OK-VQA~\cite{lin2022retrieval}, TVQA~\cite{lei2018tvqa}, and DocVQA~\cite{mathew2021docvqa}) will be conducted.

% vqav2, ok-vqa, tvqa and docvqa. Larger LLaMa Model blah, blash
 
% \clearpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix
\onecolumn
\newpage
\section{Full Comparison of Instruction-Following Capability}
\label{sec:instruct_full}
\input{tables/instruct_comparisons_full}

\newpage
\section{Comparison with Instruct LLaMA (LLaMA-I)}
\label{sec:llama_i}
\input{tables/instruction_comparisons_llama}

\end{document}
