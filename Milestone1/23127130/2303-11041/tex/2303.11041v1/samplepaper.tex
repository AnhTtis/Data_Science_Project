% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
% Ahmed packages
\usepackage{subcaption,bbm}
\usepackage{booktabs,array,graphicx}
\usepackage{multirow}
\usepackage[textsize=scriptsize, textwidth=32mm]{todonotes}
\newcommand{\authoredtodo}[4][]{\ifhmode\unskip\fi\todo[linecolor=#3, backgroundcolor=#3!50!white,#1]{[#2] #4}}
\definecolor{turquoise}{rgb}{0.0, 0.6, 0.6}
\newcommand{\ahmed}[2][]{\authoredtodo[inline]{Ahmed}{turquoise}{#2}}
\definecolor{deepmagenta}{rgb}{0.8, 0.0, 0.8}
\newcommand{\noha}[2][]{\authoredtodo[inline]{Noha}{deepmagenta}{#2}}

% indicator function
\newcommand{\ind}[1]{\mathbbm{1}_{(#1)}}

\newcommand{\STAB}[1]{\begin{tabular}{@{}c@{}}#1\end{tabular}}
\usepackage{dcolumn}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\newcolumntype{B}[3]{>{\boldmath\DC@{#1}{#2}{#3}}c<{\DC@end}}
\newcommand\mc[1]{\multicolumn{1}{c}{#1}} % handy shortcut macro

\usepackage[detect-all]{siunitx}


\usepackage[acronym]{glossaries}
\newacronym{ice}{ICE}{Intracardiac Echocardiography}
\newacronym{af}{AFib}{Atrial Fibrillation}
\newacronym{la}{LA}{Left Atrium}
\newacronym{ct}{CT}{Computed Tomography}
\newacronym{cas}{CAS}{Clinical Application Specialist}
\newacronym{ce}{CE}{Cross-Entropy}

\newcommand{\ice}{\acrshort{ice} }

\newcommand\footnotenomarker[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\begin{document}
%
\title{From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation}
%
\titlerunning{A Practical Editing Approach for ICE Segmentation}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Ahmed H. Shahin \and
Yan Zhuang \and
Noha El-Zehiry}
%
\authorrunning{A. Shahin et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Siemens Healthineers, Princeton, New Jersey, USA\\
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{ahmedhshahen@gmail.com}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
    Accurate and safe catheter ablation procedures for patients with atrial fibrillation require precise segmentation of cardiac structures in Intracardiac Echocardiography (ICE) imaging. Prior studies have suggested methods that employ 3D geometry information from the ICE transducer to create a sparse ICE volume by placing 2D frames in a 3D grid, enabling training of 3D segmentation models. However, the resulting 3D masks from these models can be inaccurate and may lead to serious clinical complications due to the sparse sampling in ICE data, frames misalignment, and cardiac motion. To address this issue, we propose an interactive editing framework that allows users to edit segmentation output by drawing scribbles on a 2D frame.  The user interaction is mapped to the 3D grid and utilized to execute an editing step that modifies the segmentation in the vicinity of the interaction while preserving the previous segmentation away from the interaction. Furthermore, our framework accommodates multiple edits to the segmentation output in a sequential manner without compromising previous edits. This paper presents a novel loss function and a novel evaluation metric specifically designed for editing. Results from cross-validation and testing indicate that our proposed loss function outperforms standard losses and training strategies in terms of segmentation quality and following user input. Additionally, we show quantitatively and qualitatively that subsequent edits do not compromise previous edits when using our method, as opposed to standard segmentation losses. Overall, our approach enhances the accuracy of the segmentation while avoiding undesired changes away from user interactions and without compromising the quality of previously edited regions, leading to better patient outcomes.
    % The abstract should briefly summarize the contents of the paper in
% 150--250 words.

\keywords{Interactive editing \and Ultrasound \and Echocardiography.}
\end{abstract}

\section{Introduction}
% AF and ICE
\acrfull{af} is a prevalent cardiac arrhythmia affecting over 45 million individuals worldwide as of 2016~\cite{kornej2020}.
\footnotenomarker{Disclaimer: The concepts and information presented in this paper are based on research results that are not commercially available. Future commercial availability cannot be guaranteed.}
Catheter ablation, which involves the elimination of affected cardiac tissue, is a widely used treatment for \acrshort{af}. To ensure procedural safety and minimize harm to healthy tissue, \acrfull{ice} imaging is utilized to guide the intervention.

\acrlong{ice} imaging utilizes an ultrasound probe attached to a catheter and inserted into the heart to obtain real-time images of its internal structures. In ablation procedures for \acrfull{la} \acrshort{af} treatment, the \acrshort{ice} ultrasound catheter is inserted in the right atrium to image the left atrial structures. The catheter is rotated clockwise to capture image frames that show the \acrshort{la} body, the \acrshort{la} appendage and the pulmonary veins~\cite{russo2013}. Unlike other imaging modalities, such as transesophageal echocardiography, \ice imaging does not require general anesthesia~\cite{enriquez2018}. Therefore, it is a safer and more convenient option for cardiac interventions using ultrasound imaging.

% This imaging modality provides essential visualization of the heart's interior and enables accurate targeting of the affected tissue during the ablation procedure. Unlike other imaging modalities, such as transesophageal echocardiography, \ice imaging does not require general anesthesia, endotracheal intubation, or esophageal intubation~\cite{enriquez2018}. Therefore, it is a safer and more convenient option for cardiac interventions.

% This paragraph is very general. Let's make it more specific. For example, 
% "In ablation procedures for Left Atruim (LA) AFib treatment, the ICE uktrasound catheter is inserted in the right atruim to image the left atrial structures. The cather is rotated clockwise to capture image frames that show the Left atruim body, the left atruim appendage and the pulmonary veins" - let's add a reference  here. You can add this paper 
% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5153176/


% ICE segmentation
The precise segmentation of cardiac structures, particularly the \acrshort{la}, is crucial for the success and safety of catheter ablation. However, segmentation of the \acrshort{la} is challenging due to the constrained spatial resolution of 2D \ice images and the manual manipulation of the \ice transducer. Further, the sparse sampling of \ice frames makes it difficult to train automatic segmentation models. Consequently, there is a persistent need to develop interactive editing tools to help experts modify the automatic segmentation to reach a clinically satisfactory accuracy.

During a typical \ice imaging scan, a series of sparse 2D \ice frames are captured and a \acrfull{cas} annotates the boundaries of the desired cardiac structure in each frame\footnote{Annotations typically take the form of contours instead of masks, as the structures being segmented appear with open boundaries in the frames.} (Fig. \ref{fig:ice2d}). To construct dense 3D masks for training segmentation models, Liao et al. utilized the 3D geometry information from the \ice transducer, to project the frames and their annotations onto a 3D grid~\cite{liao2018}. They deformed a 3D template of the \acrshort{la} computed from 414 CT scans to align as closely as possible with the \acrshort{cas} contours, producing a 3D mesh to train a segmentation model~\cite{liao2018}. However, the resulting mesh may not perfectly align with the original \acrshort{cas} contours due to factors such as frames misalignment and cardiac motion (Fig. \ref{fig:ice3d}). Consequently, models trained with such 3D mesh as a ground truth do not produce accurate enough segmentation results, which can lead to serious complications (Fig. \ref{fig:ice_pred}).

% Interactive editing
A natural remedy is to allow clinicians to edit the segmentation output and create a model that incorporates and follows these edits. In the case of \ice data, the user interacts with the segmentation output by drawing a scribble on one of the 2D frames (Fig. \ref{fig:ice_edit}). Ideally, the user interaction should influence the segmentation in the neighboring frames while preserving the original segmentation in the rest of the volume. Moreover, the user may make multiple edits to the segmentation output, which must be incorporated in a sequential manner without compromising the previous edits.% This is a challenging task given the sparsity and non-uniform sampling of the \ice frames.

% Contributions
In this paper, we present a novel interactive editing framework for \ice data. This is the first study that addresses the specific challenges of interactive editing with \ice data. Most of the editing literature treats editing as an interactive segmentation problem and does not provide a clear distinction between interactive segmentation and interactive editing. We provide a novel method that is specifically designed for editing. The novelty of our approach is two fold: 1) We introduce an editing-specific novel loss function that guides the model to incorporate user edits \textit{while preserving the original segmentation in unedited areas}. 2) We present a novel evaluation metric that best reflects the editing formulation. Comprehensive evaluations of the proposed method on \ice data demonstrate that the presented loss function achieves superior performance in comparison to traditional interactive segmentation losses and training strategies, as evidenced by experimental data.

% figure of 4 subfigures horizontal
\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{1a2.pdf}
        \caption{}
        \label{fig:ice2d}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.75\textwidth]{1b3.pdf}
        \caption{}
        \label{fig:ice3d}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.75\textwidth]{1c3.pdf}
        \caption{}
        \label{fig:ice_pred}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{1d2.pdf}
        \caption{}
        \label{fig:ice_edit}
    \end{subfigure}
    \caption{Volumetric segmentation of \acrshort{ice} data. (a) 2D \acrshort{ice} frames with \acrshort{cas} contours outlining \acrshort{la} boundaries. (b) 2D frames (black shades) and \acrshort{cas} contours projected onto a 3D grid. Blue mesh represents the 3D segmentation mask obtained by deforming a \acrshort{ct} template to fit contours as closely as possible \cite{liao2018}. Note the sparsity of frames. (c) Predicted 3D segmentation mask generated by model trained with masks from (b). (d) Predicted mask projected onto 2D (green) and compared with the original \acrshort{cas} contours. Note the misalignment between the mask and \acrshort{cas} contours in some frames. Yellow indicates an example of a user corrective edit.}
\end{figure}

\section{Interactive Editing of \ice Data}
\subsection{Problem Definition}
\label{sec:setup}
The user is first presented with an \ice volume, $x \in R^{H \times W \times D}$, and its initial imperfect segmentation, $y_{\text{init}} \in R^{H \times W \times D}$, where $H$, $W$ and $D$ are the dimensions of the volume. To correct inaccuracies in the segmentation, the user draws a scribble on one of the 2D \ice frames. Our goal is to use this 2D interaction to provide 3D correction to $y_{\text{init}}$ in the vicinity of the user interaction. We project the user interaction from 2D to 3D and encode it as a 3D Gaussian heatmap, $u \in R^{H \times W \times D}$, centered on the scribble with a standard deviation of $\sigma_{enc}$~\cite{maninis2018}. The user iteratively interacts with the output until they are satisfied with the quality of the segmentation.

We train an editing model $f$ to predict the corrected segmentation output $\hat{y}^t \in R^{H\times W\times D}$ given $x$, $y_{\text{init}}^t$, and $u^t$, where $t$ is the iteration number. The goal is for $\hat{y}^t$ to accurately reflect the user's correction near their interaction, while preserving the initial segmentation elsewhere. Since $y_{\text{init}}^{t+1} \equiv \hat{y}^t$, subsequent user inputs $u^{\{t+1, \dots, T\}}$ should not corrupt previous corrections $u^{\{0, \dots, t\}}$.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{editing7.png}
    \caption{The proposed interactive editing framework involves user interaction with the segmentation output by drawing a scribble on one of the 2D frames. The editing model is trained to incorporate the user interaction while preserving the initial segmentation in unedited areas. Cyan shade: initial segmentation. Green contour: corrected segmentation. Yellow contour: user interaction.}
\end{figure}


\subsection{Loss Function}
\label{sec:loss}
Most interactive segmentation methods aim to incorporate user guidance to enhance the overall segmentation~\cite{reuben2021,khan2019,maninis2018}. However, in our scenario, this approach may undesirably modify previously edited areas and may not align with clinical expectations since the user have corrected these areas and changes are unexpected. To address the former issue, Bredell et al. proposed an iterative training strategy in which user edits are synthesized and accumulated over a fixed number of steps with every training iteration~\cite{bredell2018}. However, this approach comes with a significant increase in training time and does not explicitly instruct the model to preserve regions away from the user input.

We propose an \textbf{editing-specific} loss function $\mathcal{L}$ that encourages the model to preserve the initial segmentation while incorporating user input. The proposed loss function incentivizes the model to match the prediction $\hat{y}$ with the ground truth $y$ in the vicinity of the user interaction. In regions further away from the user interaction, the loss function encourages the model to match the initial segmentation $y_{\text{init}}$, instead.

We define the vicinity of the user interaction as a 3D Gaussian heatmap, $A \in R^{H \times W \times D}$, centered on the scribble with a standard deviation of $\sigma_{edit}$. Correspondingly, the regions far from the interaction are defined as $\bar{A}=1-A$. The loss function is defined as the sum of the weighted cross entropy losses $\mathcal{L}_{\text{edit}}$ and $\mathcal{L}_{\text{preserve}}$ w.r.t $y$ and $y_{\text{init}}$, respectively, as follows
% 
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{edit}} + \mathcal{L}_{\text{preserve}}
\end{equation}
where
% $\mathcal{L}_{\text{edit}}$ and $\mathcal{L}_{\text{preserve}}$ are weighted by the value of $A$ and $\bar{A}$ at each voxel, respectively 
\begin{align}
    \mathcal{L}_{\text{edit}} &= -\sum_{i=1}^{H} \sum_{j=1}^{W} \sum_{k=1}^{D} A_{i,j,k} \left[ y_{i,j,k} \log \hat{y}_{i,j,k} + (1-y_{i,j,k}) \log (1-\hat{y}_{i,j,k}) \right]\\
    \mathcal{L}_{\text{preserve}} &= -\sum_{i=1}^{H} \sum_{j=1}^{W} \sum_{k=1}^{D} \bar{A}_{i,j,k} \left[ y_{{init}_{i,j,k}} \log \hat{y}_{i,j,k} + (1-y_{{init}_{i,j,k}}) \log (1-\hat{y}_{i,j,k}) \right]
\end{align}
% 
The Gaussian heatmaps facilitate a gradual transition between the edited and unedited areas, resulting in a smooth boundary between the two regions.

\subsection{Evaluation Metric}
\label{sec:metric}
The evaluation of segmentation quality typically involves metrics such as the Dice coefficient and the Jaccard index, which are defined for binary masks, or distance-based metrics, which are defined for contours~\cite{taha2015}. In our scenario, where the ground truth is \acrshort{cas} contours, we use distance-based metrics\footnote{Contours are inferred from the predicted mask $\hat{y}$.}. However, the standard utilization of these metrics computes the distance between the predicted and ground truth contours, which misleadingly incentivizes alignment with the ground truth contours in \textbf{all regions}. This approach incentivizes changes in the unedited regions, which is undesirable from a user perspective, as users want to see changes only in the vicinity of their edit. Additionally, this approach incentivizes the corruption of previous edits.

% Following the same logic of our proposed loss function, 
We propose a novel editing-specific evaluation metric that assess how well the prediction $\hat{y}$ matches the \acrshort{cas} contours $y_{\text{cas}}$ in the vicinity of the user interaction, and the initial segmentation $y_{\text{init}}$ in the regions far from the interaction.
% 
\begin{equation}
    \mathcal{D} = \mathcal{D}_{\text{edit}} + \mathcal{D}_{\text{preserve}}
\end{equation}
where, $\forall (i,j,k) \in \{1, \dots, H\} \times \{1, \dots, W\} \times \{1, \dots, D\}$, $\mathcal{D}_{\text{edit}}$ is the distance from $y_{\text{cas}}$ to $\hat{y}$ in the vicinity of the user edit, as follows
% 
\begin{equation}
    \mathcal{D}_{\text{edit}} = 
        \ind{y_{\text{cas}_{i,j,k}}=1} \cdot A_{i,j,k} \cdot d(y_{\text{cas}_{i,j,k}}, \hat{y})
\end{equation}
% 
where $d$ is the minimum Manhattan distance from $y_{\text{cas}_{i,j,k}}$ to any point on $\hat{y}$. For $\mathcal{D}_{\text{preserve}}$, we compute the average symmetric distance between $y_{\text{init}}$ and $\hat{y}$, since the two contours are of comparable length. The average symmetric distance is defined as the average of the minimum Manhattan distance from each point on $y_{\text{init}}$ contour to $\hat{y}$ contour and vice versa, as follows
% 
\begin{equation}
    \mathcal{D_{\text{preserve}}} = \frac{\bar{A}}{2} \cdot \left[ \ind{y_{\text{init}_{i,j,k}}=1} \cdot d(y_{\text{init}_{i,j,k}}, \hat{y}) + \ind{\hat{y}_{i,j,k}=1} \cdot d(\hat{y}_{i,j,k}, y_{\text{init}}) \right]
\end{equation}
% 
The resulting $\mathcal{D}$ represents a distance map $\in R^{H \times W \times D}$ with defined values only on the contours $y_{\text{cas}}, y_{\text{init}}, \hat{y}$. Statistics such as the $95^{\text{th}}$ percentile and mean can be computed over the corresponding values of these contours on the distance map.

\section{Experiments}
\subsection{Dataset}
\label{sec:dataset}
Our dataset comprises \ice scans for 712 patients, each with their \acrshort{la} \acrshort{cas} contours $y_{cas}$ and the corresponding 3D meshes $y$ generated by~\cite{liao2018}. Scans have an average of 28 2D frames. Using the 3D geometry information, frames are projected to a 3D grid with a resolution of $128\times128\times128$ and voxel spacing of $1.1024\times1.1024\times1.1024$ mm. We perform five-fold cross-validation on $85\%$ of the dataset (605 patients) and use remaining $15\%$ (107 patients) for testing.

\subsection{Implementation Details}
\label{sec:implementation}
% To obtain the initial imperfect segmentations $y_{\text{init}}$, we train a U-Net~\cite{ronneberger2015} of four encoding levels on the 3D meshes $y$ using a \acrfull{ce} loss. Our editing model also follows the same U-Net architecture. The encoding block consists of two 3D convolutional layer followed by a max pooling layer. Each convolutional layer is followed by batch normalization layer and ReLU non-linearity layers. Number of filters in the segmentation model convolutional layers are $16,32,64,128$ for each encoding block, and half of them in the editing model. The decoder follows similar architecture. The input to the editing model consists of three channels: the \ice volume $x$, the initial segmentation $y_{\text{init}}$, and the user input $u$. During training of the editing model, we synthesize the user interaction on the frame with maximum error between $y_{\text{init}}$ and $y$.\footnote{We do not use the \acrshort{cas} contours during training and only use them for testing because the \acrshort{cas} contours do not align with the labels $y$.} To simulate the interaction, we select the region of maximum error between $y_{\text{init}}$ and $y$ on the frame and draw a scribble on the boundary of the groundtruth $y$ in that region. During testing, we use the real \acrshort{cas} contours and choose the contour with the maximum distance from the predicted segmentation $\hat{y}$ as the user interaction. We set the values of $\sigma_{enc}$ and $\sigma_{edit}$ to $20$, chosen by cross-validation. We use the Adam optimizer~\cite{kingma2014} with a learning rate of $0.005$ and a batch size of 4 to train the editing model for $100$ epochs.

To obtain the initial imperfect segmentation $y_{\text{init}}$, a U-Net model~\cite{ronneberger2015} is trained on the 3D meshes $y$ using a \acrfull{ce} loss. The same U-Net architecture is used for the editing model. The encoding block consists of two 3D convolutional layers followed by a max pooling layer. Each convolutional layer is followed by batch normalization and ReLU non-linearity layers~\cite{ioffe2015}. The number of filters in the segmentation model convolutional layers are 16, 32, 64, and 128 for each encoding block, and half of them for the editing model. The decoder follows a similar architecture.

The input to the editing model consists of three channels: the input \ice volume $x$, the initial segmentation $y_{\text{init}}$, and the user input $u$. During training, the user interaction is synthesized on the frame with maximum error between $y_{\text{init}}$ and $y$. \footnote{We do not utilize the \acrshort{cas} contours during training and only use them for testing because the \acrshort{cas} contours do not align with the segmentation meshes $y$.} The region of maximum error is selected and a scribble is drawn on the boundary of the ground truth in that region to simulate the user interaction. During testing, the real contours of the \acrshort{cas} are used and the contour with the maximum distance from the predicted segmentation is chosen as the user interaction. The values of $\sigma_{enc}$ and $\sigma_{edit}$ are set to 20, chosen by cross-validation. The Adam optimizer is used with a learning rate of 0.005 and a batch size of 4 to train the editing model for 100 epochs~\cite{kingma2014}.

\subsection{Results}
\label{sec:results}
We use the editing evaluation metric $\mathcal{D}$ (Sec. \ref{sec:metric}) for evaluation of the different methods. For better interpretability of the results, we report the overall error, the error near the user input, and the error far from the user input. We define near and far regions by thresholding the Gaussian heatmap $A$ at $0.5$.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \begin{table}[!t]
%     \centering
%     \caption{Results on Cross-Validation (CV) and test set. We use the editing evaluation metric $\mathcal{D}$ and report the $95^{\text{th}}$ percentile of the overall editing error, the error near the user input, and far from the user input (mm). The near and far regions are defined by thresholding $A$ at $0.5$. For the CV results, we report the mean and standard deviation over the five folds. The statistical significane w.r.t the CE loss -- $\dagger$: p-value $< 0.01$, $\ddagger$: p-value $< 0.005$.}
%     \label{tab:results_single}
%     \begin{tabular}{@{}llccc@{}}
%     \toprule
%     Data              & Method             & Overall $\downarrow$          & Near $\downarrow$  & Far $\downarrow$ \\ \midrule
%     \multirow{5}{*}{\STAB{\rotatebox[origin=c]{90}{CV}}}
%     & No Editing         & 3.962 $\pm$ 0.148 & -                   & -                       \\
%                       & \acrshort{ce} Loss & 0.969 $\pm$ 0.108 & 0.381 $\pm$ 0.049 & 0.769 $\pm$ 0.108 \\
%                       & Dice Loss & 0.981 $\pm$ 0.083 & 0.402 $\pm$ 0.05 & 0.788 $\pm$ 0.074 \\
%                       & InterCNN           & 0.755 $\pm$ 0.018 & \textbf{0.267 $\pm$ 0.049} & 0.558 $\pm$ 0.108 \\
%                       & Editing Loss       & \textbf{0.645 $\pm$ 0.032}&0.387 $\pm$ 0.037         & \textbf{0.158 $\pm$ 0.012}              \\ \midrule
%     \multirow{5}{*}{\STAB{\rotatebox[origin=c]{90}{Test Set}}}
%     & No Editing         & 4.126       & -               & -                   \\
%     & \acrshort{ce} Loss & 1.022       & 0.48            & 0.79                \\
%     & Dice Loss          & 0.96        & 0.443           & 0.778               \\
%     & InterCNN           & $0.821^{\dagger}$       & \textbf{0.372}  & 0.567               \\
%     & Editing Loss       &\textbf{0.723}& 0.479          &\textbf{0.184}       \\ \bottomrule
%     \end{tabular}
% \end{table}

\begin{table}[!ht]
    \centering
    \caption{Results on Cross-Validation (CV) and test set. We use the editing evaluation metric $\mathcal{D}$ and report the $95^{\text{th}}$ percentile of the overall editing error, the error near the user input, and far from the user input (mm). The near and far regions are defined by thresholding $A$ at $0.5$. For the CV results, we report the mean and standard deviation over the five folds. The statistical significance is computed for the difference with InterCNN. $^\dagger$: p-value $<0.01$, $^\ddagger$: p-value $<0.001$.}
    \label{tab:results_single}
    % \begin{tabular}{@{}lcccccc@{}}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{} m{1.9cm} r@{\text{} $\pm$ \text{}}l r@{\text{} $\pm$ \text{}}l r@{\text{} $\pm$ \text{}}l r@{.}l r@{.}l r@{.}l @{}}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{6}{c}{CV} & \multicolumn{6}{c}{Test} \\ \cmidrule(l{3pt}r{3pt}){2-7} \cmidrule(l{3pt}r{3pt}){8-13} & \multicolumn{2}{c}{Overall $\downarrow$} & \multicolumn{2}{c}{Near $\downarrow$} & \multicolumn{2}{c}{Far $\downarrow$} & \multicolumn{2}{c}{Overall $\downarrow$} & \multicolumn{2}{c}{Near $\downarrow$} & \multicolumn{2}{c}{Far $\downarrow$} \\ \midrule
    No Editing & 3.962 & 0.148 &   \multicolumn{2}{c}{-} & \multicolumn{2}{c}{-} & 4&126 & \multicolumn{2}{c}{-} & \multicolumn{2}{c}{-} \\
    \acrshort{ce} Loss & 1.164 & 0.094 & 0.577 & 0.024 & 0.849 & 0.105 & 1&389 & 0&6 & 1&073 \\
    Dice Loss  & 1.188 & 0.173 & 0.57 & 0.089  & 0.892 & 0.155 & 1&039 & \textbf{0}&\textbf{46} & 0&818 \\
    InterCNN   & 0.945 & 0.049 & \textbf{0.517} & \textbf{0.052} & 0.561 & 0.006 & 0&94 & 0&509 & 0&569 \\
    Editing Loss & \textbf{0.809} & \textbf{0.05}$^\ddagger$ & 0.621 & 0.042 & \textbf{0.182} & \textbf{0.01}$^\ddagger$ & \textbf{0}&\textbf{844}$^\dagger$ & 0&662 & \textbf{0}&\textbf{184}$^\ddagger$ \\ \bottomrule
    \end{tabular}
    }
\end{table}

We compare our loss (editing loss) with the following baselines: (1) \textbf{No Editing}: the initial segmentation $y_{\text{init}}$ is used as the final segmentation $\hat{y}$, and the overall error in this case is the distance from the \acrshort{cas} contours to $y_{\text{init}}$. This should serve as an upper bound for error. (2) \textbf{\acrshort{ce} Loss}: an editing model trained using the standard \acrshort{ce} segmentation loss. (3) \textbf{Dice Loss}~\cite{Milletari2016}: an editing model trained using Dice segmentation loss. (4) \textbf{InterCNN}~\cite{bredell2018}: for every training sample, simulated user edits based on the prediction are accumulated with any previous edits and re-input to the model for 10 iterations, trained using \acrshort{ce} loss. We report the results after a single edit (the furthest \acrshort{cas} contour from $\hat{y}$) in Table \ref{tab:results_single}. A single training epoch takes $\approx 3$ minutes for all the models except for InterCNN, which takes $\approx 14$ minutes, on a single NVIDIA Tesla V100 GPU.

The results show that the proposed loss outperforms all the baselines in terms of overall error. Although all the editing methods exhibit comparable performance in the near region, in the far region where the error is calculated relative to $y_{\text{init}}$, our proposed loss outperforms all the baselines by a significant margin. This can be attributed to the fact that the baselines are trained using loss functions which aim to match the ground truth globally, resulting in deviations from the initial segmentation in the far region. In contrast, our loss takes into account user input in its vicinity and maintains the initial segmentation elsewhere. 

\subsubsection{Sequential Editing}
\label{sec:seq_editing} We also investigate the scenario where the user iteratively performs edits on the segmentation multiple times. We utilized the same models that were used in the single edit experiment and simulated 10 editing iterations. At each iteration, we selected the furthest \acrshort{cas} contour from $\hat{y}$, ensuring that the same edit was not repeated twice. For the interCNN model, we aggregated the previous edits and input them to the model, whereas for all other models, we input a single edit per iteration. We assessed the impact of the number of edits on the overall error. In Fig. \ref{fig:seq_editing}, we computed the distance from all the \acrshort{cas} contours to the predicted segmentation and observed that the editing loss model improved with more edits. In contrast, the CE and Dice losses degraded with more edits due to compromising the previous corrections, while the InterCNN had only marginal improvements.

Furthermore, in Fig. \ref{fig:seq_editing_2}, we present a qualitative example to understand the effect of follow-up edits on the first correction. Edits after the first one are on other frames and not shown in the figure. We observe that the CE and InterCNN methods did not preserve the first correction, while the editing loss model maintained it. This is a crucial practical advantage of our loss, which allows the user to make corrections without compromising the previous edits.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.4\textwidth]{seq2.pdf}
    \caption{$95^{\text{th}}$ percentile of the distance from the \acrshort{cas} contours to the prediction.}
    \label{fig:seq_editing}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{seq4.png}
    \caption{Impact of follow-up edits on the first correction. Yellow: first user edit. Green: output after each edit. Our editing loss maintains the integrity of the previous edits, while in the other methods the previous edits are compromised.}
    \label{fig:seq_editing_2}
\end{figure}

\section{Conclusion}
We presented an interactive editing framework for challenging clinical applications. We devised an editing-specific loss function that penalizes the deviation from the ground truth near user-interaction and penalizes deviation from the initial segmentation away from user-interaction. Our novel editing algorithm is more robust as it does not compromise previously corrected regions. We demonstrate the performance of our method on the challenging task of volumetric segmentation of sparse \ice data. However, our formulation can be applied to other editing tasks and different imaging modalities.

% This experiment aims to shed light on the impact of subsequent edits on the previous ones.

\bibliographystyle{splncs04}
\bibliography{refs}
%
\end{document}
