\section{The joint hierarchical Bayesian model} 
\label{sec:model} 

We present the joint-sparsity promoting Bayesian model considered in this investigation. 
The conditionally Gaussian prior in \cref{sub:prior} is particularly important for developing our new method.


\subsection{The likelihood function} 
\label{sub:likelihood}

The likelihood density function models the connection between the parameter and measurement vectors. 
Consider the linear MMV data model \cref{eq:MMV_IP} with MMVs $\mathbf{y}_l \in \R^{M_l}$, known forward operators $F_l \in \R^{M_l \times N}$, desired parameter vectors $\mathbf{x}_l \in \R^{N}$, and additive Gaussian noise $\mathbf{e}_l \sim \mathcal{N}(\mathbf{0},\Sigma_l)$. 
Since $\Sigma_l$ is a symmetric positive definite (SPD) covariance matrix, there exists a Cholesky decomposition of the form $\Sigma_l = C_l C_l^T$ with invertible $C_l$. 
The noise can then be whitened by multiplying both sides of \cref{eq:MMV_IP} with $C_l^{-1}$ from the left-hand side so that we can assume $\Sigma_l = I$, where $I$ is the $M_l \times M_l$ identity matrix.
The \emph{$l$th likelihood function} is then 
\begin{equation}\label{eq:likelihood} 
	\pi_{\mathbf{Y}_l | \mathbf{X}_l}( \mathbf{y}_l | \mathbf{x}_l ) 
        \propto \exp\left( -\frac{1}{2} \| F_l \mathbf{x}_l - \mathbf{y}_l \|_2^2 \right), 
        \quad l=1,\dots,L.
\end{equation} 
Assuming that $\mathbf{Y}_{1:L}$ are jointly independent conditioned on $\mathbf{X}_{1:L}$, the \emph{joint likelihood function} is   
\begin{equation}\label{eq:joint_likelihood} 
	\pi_{\mathbf{Y}_{1:L}|\mathbf{X}_{1:L}}(\mathbf{y}_{1:L}|\mathbf{x}_{1:L}) 
		= \prod_{l=1}^L \pi_{\mathbf{Y}_l | \mathbf{X}_l}( \mathbf{y}_l | \mathbf{x}_l ) 
		\propto \exp\left( -\frac{1}{2} \sum_{l=1}^L \| F_l \mathbf{x}_l - \mathbf{y}_l \|_2^2 \right).
\end{equation}
Note that \cref{eq:joint_likelihood} is a conditionally Gaussian density function, which is convenient for Bayesian inference.  A couple of remarks are in order.

\begin{remark}[Complex-valued forward operators] This framework also allows for complex-valued forward operators and observations. Specifically for $F \in \C^{M \times N}$, we can use the equivalent real-valued forward operator $[\operatorname{Re}(F);\operatorname{Im}(F)] \in \R^{2M \times N}$, where $\operatorname{Re}(F)$ and $\operatorname{Im}(F)$ denote the real and imaginary part of $F$.
\end{remark} 

\begin{remark}[Non-linear data models]
	For simplicity, we restrict our attention to linear data models. 
	However, the proposed approach can be extended to non-linear models using methods such as Kalman filtering \cite{evensen2009data,spantini2022coupling,kim2022hierarchical}. 
\end{remark}


\subsection{The joint-sparsity promoting conditionally Gaussian prior}
\label{sub:prior}

The prior density models our prior belief about the desired parameter vectors $\mathbf{x}_{1:L}$. 
Here we assume that they are {\em jointly} sparse, i.e., there {exist a linear transform $R \in \R^{K \times N}$ such that $R \mathbf{x}_1, \dots, R \mathbf{x}_L$} are sparse and have the same support (the indices of their non-zero values are the same). 
We start by modeling the sparsity of $R \mathbf{x}_l$ in a probabilistic setting by choosing the \emph{$l$th prior} as the conditionally Gaussian density  
\begin{equation}\label{eq:prior_sparsity}
    \pi_{\mathbf{X}_l | \boldsymbol{\Theta}_l}( \mathbf{x}_l | \boldsymbol{\theta}_l ) 
        \propto \det( D_{\boldsymbol{\theta}_l} )^{-1/2} \exp\left( -\frac{1}{2} \| D_{\boldsymbol{\theta}_l}^{-1/2} {R} \mathbf{x}_l \|_2^2 \right), \quad l=1,\dots,L, 
\end{equation}
with hyper-parameter vector $\boldsymbol{\theta}_l = [(\theta_l)_1,\dots,(\theta_l)_K]$, covariance matrix $D_{\boldsymbol{\theta}_l} = \diag(\boldsymbol{\theta}_l)$, and unknown variance parameters $(\theta_l)_k > 0$ for $k=1,\dots,K$ and $l=1,\dots,L$.

\begin{remark}\label{rem:motivation_condGaussian}
	Following \cite{calvetti2007gaussian,glaubitz2022generalized}, the conditional Gaussian prior \cref{eq:prior_sparsity} can be motivated by its asymptotic behavior: 
	Assume that $(\theta_l)_1=\dots=(\theta_l)_K$, then \cref{eq:prior_sparsity} favors $\mathbf{x}_l$ for which $\| R \mathbf{x}_l \|_2$ is close to zero, since such an $\mathbf{x}_l$ has a higher probability. 
	For instance, when $R \mathbf{x}_l$ corresponds to the increments of $\mathbf{x}_l$, i.e., $\left[ R \mathbf{x}_l \right]_k = (x_l)_{k+1} - (x_l)_{k}$, then \cref{eq:prior_sparsity} with $(\theta_l)_1=\dots=(\theta_l)_K$ favors $\mathbf{x}_l$ to have little variation. 
	However, if one of the hyper-parameters, say $(\theta_l)_k$, is significantly larger than the others, a jump between $(x_l)_{k+1}$ and $(x_l)_{k}$ becomes more likely. 
	In this way, \cref{eq:prior_sparsity} promotes sparsity of $R \mathbf{x}_l$. 
	Furthermore, we can connect the support of $R \mathbf{x}_l$ to the hyper-parameters $(\theta_l)_1,\dots,(\theta_l)_K$. 
	In particular, we expect the support of $R \mathbf{x}_l$ to coincide with the hyper-parameters significantly larger than most others. 
\end{remark}

We next model $R \mathbf{x}_1,\dots,R \mathbf{x}_L$ having the same support. 
To this end, motivated by \cref{rem:motivation_condGaussian}, we connect the supports of $R \mathbf{x}_1,\dots,R \mathbf{x}_L$ to the hyper-parameter vectors, $\boldsymbol{\theta}_1,\dots,\boldsymbol{\theta}_L$, by assuming that $\boldsymbol{\theta}_1 = \dots = \boldsymbol{\theta}_L$. 
Denoting the common hyper-parameter vector as $\boldsymbol{\theta}$, \cref{eq:prior_sparsity} then reduces to  
\begin{equation}\label{eq:prior_joint_sparsity}
    \pi_{\mathbf{X}_l | \boldsymbol{\Theta}}( \mathbf{x}_l | \boldsymbol{\theta} ) 
        \propto \det( D_{\boldsymbol{\theta}} )^{-1/2} \exp\left( -\frac{1}{2} \| D_{\boldsymbol{\theta}}^{-1/2} R \mathbf{x}_l \|_2^2 \right), \quad l=1,\dots,L. 
\end{equation}
That is, the priors are now all conditioned on the {\em same} hyper-parameters. 
Finally, assuming the $\mathbf{X}_{1:L}$ are jointly independent conditioned on $\boldsymbol{\Theta}$, the \emph{joint prior} is 
\begin{equation}\label{eq:joint_prior} 
	\pi_{\mathbf{X}_{1:L} | \boldsymbol{\Theta}}( \mathbf{x}_{1:L} | \boldsymbol{\theta} ) 
		= \prod_{l=1}^L \pi_{\mathbf{X}_l | \boldsymbol{\Theta}}( \mathbf{x}_l | \boldsymbol{\theta} ) 
		\propto \det( D_{\boldsymbol{\theta}} )^{-L/2} \exp\left( -\frac{1}{2} \sum_{l=1}^L \| D_{\boldsymbol{\theta}}^{-1/2} R \mathbf{x}_l \|_2^2 \right). 
\end{equation}

\begin{remark}[Other sparsity-promoting hierarchical priors] 
	The conditionally Gaussian prior in \cref{eq:joint_prior} not only enforces joint sparsity but also enables convenient Bayesian inference due to its compatibility with the Gaussian likelihood \cref{eq:joint_likelihood}. 
	However, the joint-sparsity-promoting approach using a common hyper-parameter vector $\boldsymbol{\theta}$ can be extended to other hierarchical sparsity-promoting priors, such as horseshoe \cite{carvalho2009handling,uribe2022horseshoe} and neural network priors \cite{neal1996priors,asim2020invertible,li2021bayesian}. 
\end{remark}


\subsection{The generalized gamma hyper-prior} 
\label{sub:hyper}

The price to pay for the hierarchical joint prior model \cref{eq:joint_prior} is that we now need to estimate not only the parameter vectors $\mathbf{x}_{1:L}$ but also the common hyper-parameter vector $\boldsymbol{\theta}$. 
By Bayes' theorem, the \emph{joint posterior density} of $(\mathbf{X}_{1:L},\boldsymbol{\Theta})$ given $\mathbf{Y}_{1:L}$ is 
\begin{equation}\label{eq:posterior} 
	\pi_{ \mathbf{X}_{1:L}, \boldsymbol{\Theta} | \mathbf{Y}_{1:L} }( \mathbf{x}_{1:L}, \boldsymbol{\theta} | \mathbf{y}_{1:L} ) 
		\propto \pi_{ \mathbf{Y}_{1:L} | \mathbf{X}_{1:L} }( \mathbf{y}_{1:L} | \mathbf{x}_{1:L} ) \, 
			\pi_{ \mathbf{X}_{1:L} | \boldsymbol{\Theta} }( \mathbf{x}_{1:L} | \boldsymbol{\theta} ) \, 
			\pi_{ \boldsymbol{\Theta} }( \boldsymbol{\theta} ). 
\end{equation}
From \cref{rem:motivation_condGaussian}, it is evident that to promote sparsity of $R \mathbf{x}_1,\dots,R \mathbf{x}_L$ the hyper-prior $\pi_{ \boldsymbol{\Theta} }$ should favor small values of $\theta_1,\dots,\theta_K$ while allowing occasional large outliers for the conditionally Gaussian prior \cref{eq:prior_joint_sparsity}.
Following \cite{calvetti2020sparse,calvetti2020sparsity}, this can be achieved by treating $\theta_1,\dots,\theta_K$ as random variables with an uninformative generalized gamma density function
\begin{equation}\label{eq:hyper_priors} 
	\pi_{\boldsymbol{\Theta}}(\boldsymbol{\theta}) 
		= \prod_{k=1}^K \mathcal{GG}( \theta_k | r, \beta, \vartheta_k ) 
		\propto \det(D_{\boldsymbol{\theta}})^{r \beta - 1} \exp\left( - \sum_{k=1}^K ( \theta_k/\vartheta_k )^r \right). 
\end{equation} 
Here, $\mathcal{GG}$ is the generalized gamma distribution  
\begin{equation}\label{eq:pdf_gamma} 
\begin{aligned}
    \mathcal{GG}( \theta_k | r, \beta, \vartheta_k ) 
    		\propto \theta_k^{r \beta - 1} \exp\left( - ( \theta_k/\vartheta_k )^r \right),
\end{aligned}
\end{equation} 
where $r \in \R\setminus\{0\}$, $\beta > 0$, and $\vartheta_k > 0$ for $k=1,\dots,K$.
\Cref{fig:graphical_model} provides a graphical illustration and summary of our joint-sparsity-promoting hierarchical Bayesian model. 

\begin{figure}[tb]
\centering
\resizebox{0.4\textwidth}{!}{%
\begin{tikzpicture}
  	% nodes 
    \node[obs] (y1) {$\mathbf{y}_1$}; % 
	\node[latent, right=1.5 of y1] (x1) {$\mathbf{x}_1$} ; % 
	%\node[latent, right=1.5 of x1] (beta1) {$\boldsymbol{\beta}^{(1)}$} ; %
	% 
	\node[const, right=1.75 of x1] (theta_aux) {}; %
	\node[latent, below=.725 of theta_aux] (theta) {$\boldsymbol{\theta}$} ; %
	%
	\node[obs, below=1.5 of y1] (y2) {$\mathbf{y}_2$}; % 
	\node[latent, right=1.5 of y2] (x2) {$\mathbf{x}_2$} ; % 
	%\node[latent, right=1.5 of x2] (beta2) {$\boldsymbol{\beta}^{(2)}$} ; %
	%\node[const, below=1.2 of y_1] (dots_1) {}; % 
	%\node[const, left=1.0 of dots_1] (dots_2) {$\vdots$}; % 
	%
	% edges 
	\edge {x1} {y1}; % 
	%
	\edge {x2} {y2}; % 
	%
	\edge {theta} {x1}; % 
	\edge {theta} {x2}; % 
	% boxes 
	\plate[inner sep=0.3cm, xshift=0cm, yshift=0cm] {plate_y1} {(y1)} {$M_1$}; %
	\plate[inner sep=0.3cm, xshift=0cm, yshift=0cm] {plate_x} {(x1)} {$N$}; % 
	\plate[inner sep=0.3cm, xshift=0cm, yshift=0cm] {plate_theta} {(theta)} {$K$}; %
	%
	\plate[inner sep=0.3cm, xshift=0cm, yshift=0cm] {plate_y2} {(y2)} {$M_2$}; %
	\plate[inner sep=0.3cm, xshift=0cm, yshift=0cm] {plate_x} {(x2)} {$N$}; % 
\end{tikzpicture} 
}%
\caption{
    Graphical representation of the hierarchical Bayesian model promoting joint sparsity for two ($L=2$) measurement and parameters vectors, $\mathbf{y}_1, \mathbf{y}_2$ and $\mathbf{x}_1, \mathbf{x}_2$, respectively. 
  	Shaded and plain circles represent observed and unobserved (hidden) random variables, respectively. 
	The arrows indicate how the random variables influence each other: 
	The parameter vectors $\mathbf{x}_1,\mathbf{x}_2$ are connected to the measurement vectors $\mathbf{y}_1,\mathbf{y}_2$, respectively, via the likelihood \cref{eq:joint_likelihood}; 
	The common hyper-parameters $\boldsymbol{\theta}$ are connected to $\mathbf{x}_1, \mathbf{x}_2$ via the joint-sparsity-promoting prior \cref{eq:joint_prior}. 
	Using common gamma hyper-parameters $\boldsymbol{\theta}$ (instead of separate ones for $\mathbf{x}_1, \mathbf{x}_2$) results in $R \mathbf{x}_1$ and $R \mathbf{x}_2$ having the same support. 
	}
\label{fig:graphical_model}
\end{figure}
