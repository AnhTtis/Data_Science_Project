\section{Bayesian inference} 
\label{sec:BI} 

We now address Bayesian inference for the joint-sparsity-promoting hierarchical Bayesian model proposed in \Cref{sec:model}. 
To this end, 
for given MMVs $\mathbf{y}_{1:L}$, 
we solve for the \emph{maximum a posterior (MAP) estimate} $(\mathbf{x}_{1:L}^{\MAP},\boldsymbol{\theta}^{\MAP})$,
which is the maximizer of the posterior density \cref{eq:posterior}.
Equivalently, the MAP estimate is the minimizer of the negative logarithm of the posterior, i.e.,  
\begin{equation}\label{eq:MAP_estimate}
	(\mathbf{x}_{1:L}^{\MAP},\boldsymbol{\theta}^{\MAP}) 
		= \argmin_{ \mathbf{x}_{1:L}, \boldsymbol{\theta} } \left\{ \mathcal{G}( \mathbf{x}_{1:L}, \boldsymbol{\theta} \right\}, 
\end{equation} 
where the objective function $\mathcal{G}$ is 
\begin{equation}\label{eq:obj_fun} 
	\mathcal{G}( \mathbf{x}_{1:L}, \boldsymbol{\theta} ) 
		= - \log \pi_{ \mathbf{X}_{1:L}, \boldsymbol{\Theta} | \mathbf{Y}_{1:L} }( \mathbf{x}_{1:L}, \boldsymbol{\theta} | \mathbf{y}_{1:L} ).
\end{equation} 
Substituting \cref{eq:joint_likelihood,eq:joint_prior,eq:hyper_priors} into \cref{eq:obj_fun}, we obtain
\begin{equation}\label{eq:G}
\begin{aligned} 
	\mathcal{G}( \mathbf{x}_{1:L}, \boldsymbol{\theta} ) 
		= \frac{1}{2} \left( \sum_{l=1}^L \| F_l \mathbf{x}_l - \mathbf{y}_l \|_2^2 + \| D_{\boldsymbol{\theta}}^{-1/2} R \mathbf{x}_l  \|_2^2 \right) + \sum_{k=1}^K \left( \frac{\theta_k}{\vartheta_k} \right)^r - \eta \sum_{k=1}^K \log( \theta_k ) 
\end{aligned}	
\end{equation}
up to constants that neither depend on $\mathbf{x}_{1:L}$ nor $\boldsymbol{\theta}$, where $\eta = r \beta - (L/2 + 1)$. 
In what follows we discuss how the minimizer of $\mathcal{G}$ --- and therefore the MAP estimate $(\mathbf{x}_{1:L}^{\MAP},\boldsymbol{\theta}^{\MAP})$ --- can be approximated. 


\subsection{The iterative alternating sequential algorithm} 
\label{sub:IAS}

We use a block-coordinate descent approach \cite{wright2015coordinate,beck2017first} to approximate the MAP estimate. 
In the context of conditionally Gaussian priors for which the covariance is assumed to follow a (generalized) gamma distribution, a prevalent block-coordinate descent method is the so-called iterative alternating sequential (IAS) algorithm \cite{calvetti2007gaussian,calvetti2015hierarchical,calvetti2019hierachical,calvetti2020sparse}.
The IAS algorithm computes the minimizer of the objective function $\mathcal{G}$ by alternatingly (i) minimizing $\mathcal{G}$ w.r.t.\ $\mathbf{x}_{1:L}$ for fixed $\boldsymbol{\theta}$ and (ii) minimizing $\mathcal{G}$ w.r.t.\ $\boldsymbol{\theta}$ for fixed $\mathbf{x}_{1:L}$. 
Given an initial guess for the hyper-parameter vector $\boldsymbol{\theta}$, the IAS algorithm proceeds through a sequence of updates of the form 
\begin{equation}\label{eq:IAS}
	\mathbf{x}_{1:L} = \argmin_{\mathbf{x}_{1:L}} \left\{ \mathcal{G}(\mathbf{x}_{1:L},\boldsymbol{\theta}) \right\}, \quad 
	\boldsymbol{\theta} = \argmin_{\boldsymbol{\theta}} \left\{ \mathcal{G}(\mathbf{x}_{1:L},\boldsymbol{\theta}) \right\},
\end{equation}
until a convergence criterion is met.\footnote{
In our implementation, we stop if the relative change in the $\mathbf{x}_{l}$ variables falls below a given threshold. 
For simplicity, we initialize the hyper-parameter vector as $\boldsymbol{\theta} = [1,\dots,1]$.
} 
Efficient implementation of the two update steps in \cref{eq:IAS} is discussed below.


\subsection{Updating the parameter vectors} 
\label{sub:x_update}

Updating $\mathbf{x}_{1:L}$ given $\boldsymbol{\theta}$ reduces to solving the quadratic optimization problems 
\begin{equation}\label{eq:x_update} 
	\mathbf{x}_l 
		= \argmin_{\mathbf{x}} \left\{ \| F_l \mathbf{x} - \mathbf{y}_l \|_2^2 + \| D_{\boldsymbol{\theta}}^{-1/2} R \mathbf{x} \|_2^2 \right\}, \quad 
		l=1,\dots,L,
\end{equation}
with $D_{\boldsymbol{\theta}} = \diag(\boldsymbol{\theta})$. 
Note that the optimization problems \cref{eq:x_update} are decoupled and can thus be solved efficiently in parallel. 
Furthermore, assuming the \emph{common kernel condition} (also see \cite{glaubitz2022generalized,xiao2023sequential})
\begin{equation}\label{eq:common_kernel}
    \kernel(F_l) \cap \kernel(R) = \{ \mathbf{0} \}, \quad l=1,\dots,L,
\end{equation} 
holds, each optimization problem in \cref{eq:x_update} has a unique solution. 
Here, $\kernel(G) = \{ \, \mathbf{x} \in \R^N \mid G \mathbf{x} = \mathbf{0} \, \}$ is the kernel of an operator $G: \R^N \to \R^M$, i.e., the set of vectors that are mapped to zero by $G$. 
The common kernel condition \cref{eq:common_kernel} guarantees that the combination of prior information and the given measurements will result in a well-posed problem, which is a commonly accepted assumption in regularized inverse problems \cite{kaipio2006statistical,tikhonov2013numerical}. 
Finally, we can efficiently solve the quadratic optimization problems \cref{eq:x_update} using various existing methods, including the fast iterative shrinkage-thresholding (FISTA) algorithm \cite{beck2009fast}, the preconditioned conjugate gradient (PCG) method \cite{saad2003iterative}, potentially combined with an early stopping based on Morozov's discrepancy principle \cite{calvetti2015hierarchical,calvetti2018bayes,calvetti2020sparse}, and the gradient descent approach \cite{glaubitz2022generalized}.  
There is no general advantage of one method over another, and the choice should be made based on the specific problem (and the structure of $F_l$ and $R$) at hand.  
 

\subsection{Updating the hyper-parameters} 
\label{sub:beta_update}

We next address the update for the hyper-parameters $\boldsymbol{\theta}$, for which we must solve 
\begin{equation}\label{eq:update_beta1} 
	\boldsymbol{\theta} = \argmin_{\boldsymbol{\theta}} \left\{ \mathcal{G}(\mathbf{x}_{1:L},\boldsymbol{\theta}) \right\} 
\end{equation}
for fixed parameter vectors $\mathbf{x}_{1:L}$. 
Substituting \cref{eq:G} into \cref{eq:update_beta1} and ignoring all terms that do not depend on $\boldsymbol{\theta}$, \cref{eq:update_beta1} is equivalent to 
\begin{equation}\label{eq:update_beta2} 
	\theta_k = \argmin_{\theta_k} \left\{ \theta_k^{-1} \left( \sum_{l=1}^L [R \mathbf{x}_l]_k^2/2 \right) + \left( \frac{\theta_k}{\vartheta_k} \right)^r - \eta \log( \theta_k ) \right\}, \quad 
\end{equation} 
where $\eta = r \beta - (L/2 + 1)$ and $[R \mathbf{x}_l]_k$ denotes the $k$-th entry of the vector $R \mathbf{x}_l \in \R^K$. 
Differentiating the objective function in \cref{eq:update_beta2} w.r.t.\ $\theta_k$ and setting this derivative to zero yields 
\begin{equation}\label{eq:update_beta3} 
	0 = - \theta_k^{-2} \left( \sum_{l=1}^L [R \mathbf{x}_l]_k^2/2 \right) + \theta_k^{r-1} \left( \frac{r}{\vartheta_k^r} \right) - \theta_k^{-1} \eta.
\end{equation} 
For some values of $r$, \cref{eq:update_beta3} admits an analytical solution. 
For instance, if $r=1$, \cref{eq:update_beta3} is equivalent to 
\begin{subequations}\label{eq:update_beta4}
\begin{equation}\label{eq:update_beta_rp1} 
	\theta_k = \frac{ \vartheta_k }{2} \left( \eta + \sqrt{ \eta^2 + 2 \vartheta^{-1} \sum_{l=1}^L [R \mathbf{x}_l]_k^2 } \right), \quad k=1,\dots,K,
\end{equation}  
where $\eta = r \beta - (L/2 + 1)$, 
and respectively for $r = -1$, we have
\begin{equation}\label{eq:update_beta_rm1} 
	\theta_k = \frac{ \sum_{l=1}^L [R \mathbf{x}_l]_k^2/2 + \vartheta_k}{ - \eta }, \quad k=1,\dots,K.
\end{equation}
\end{subequations}
By assuming $\eta > 0$ in \cref{eq:update_beta_rp1} and $\eta < 0$ in \cref{eq:update_beta_rm1}, the hyper-parameters are ensured to be positive.
We refer to \cite{calvetti2020sparse,calvetti2020sparsity} for details on how \cref{eq:update_beta3} can be solved numerically in the general case. 


\subsection{Proposed algorithm and its relationship to current methodology}
\label{sub:comparison_IAS} 

\cref{algo:MMV_IAS} summarizes the above procedure to approximate the MAP estimate $(\mathbf{x}_{1:L}^{\MAP},\boldsymbol{\theta}^{\MAP})$ of our joint-sparsit-promoting hierarchical Bayesian model proposed in \Cref{sec:model}. 
We will refer to this method as the \emph{MMV-IAS algorithm}.

\begin{algorithm}[h!]
\caption{The MMV-IAS algorithm}\label{algo:MMV_IAS} 
\begin{algorithmic}[1]
    \STATE{Choose model parameters $(r,\beta,\boldsymbol{\vartheta})$ and initialize $\boldsymbol{\theta}$} 
    \REPEAT
		\STATE{Update the parameter vectors $\mathbf{x}_{1:L}$ (in parallel) according to \cref{eq:x_update}}
		\STATE{Update the hyper-parameters $\boldsymbol{\theta}$ according to \cref{eq:update_beta4}} 
    \UNTIL{convergence or the maximum number of iterations is reached}
\end{algorithmic}
\end{algorithm} 


\subsubsection*{Relationship to the IAS algorithm}

Our MMV-IAS algorithm builds upon the standard IAS algorithm \cite{calvetti2020sparse,calvetti2020sparsity} and reduces to it when handling a single measurement and parameter vector ($L=1$). 
However, it is important to note that using the standard IAS to recover each $\mathbf{x}_{1:L}$ separately from the MMVs $\mathbf{y}_{1:L}$ is \emph{not} equivalent to the MMV-IAS algorithm as it does not consider joint sparsity. 
Our numerical examples in \Cref{sec:numerics} demonstrate that this can lead to suboptimal results.

\begin{remark}[Extensions of the IAS algorithm]
	Several advancements to the IAS algorithm have recently been made. 
	In \cite{calvetti2020sparsity}, hybrid versions were proposed to balance convex and non-convex models to enhance sparsity while avoiding stopping at non-global minima. 
	In \cite{si2022path}, path-following methods were used to smoothly transition from convex to non-convex models. 
	Additionally, in \cite{kim2022hierarchical}, the IAS algorithm was generalized for non-linear data models with ensemble Kalman methods. 
	While beyond the scope of this current investigation, it would be beneficial to integrate our joint-sparsity-promoting approach with these advancements as deemed appropriate for a particular application.
\end{remark}


\subsubsection*{Relationship to iteratively re-weighted least squares} 

The update steps \cref{eq:update_beta4,eq:x_update} of \cref{algo:MMV_IAS} can be understood as an iteratively re-weighted least squares (IRLS) algorithm \cite{chartrand2008iteratively,daubechies2010iteratively} with automatic inter-signal coupling. 
IRLS algorithms aim to recover sparse signals by assigning individual weights to the components of $\mathbf{x}$ and updating these weights iteratively. 
This concept is also applied in iteratively re-weighted $\ell^1$-regularization methods \cite{candes2008enhancing}. 
The MMV-IAS framework provides a Bayesian interpretation for weighting strategies and can be used to tailor these weights based on statistical assumptions of the problem. 


\subsection{Uncertainty quantification}
\label{sub:UQ_IAS} 

Although we only solve for the MAP estimate of the posterior density $\pi_{ \mathbf{X}_{1:L}, \boldsymbol{\Theta} | \mathbf{Y}_{1:L} = \mathbf{y}_{1:L} }$ for given MMV data $\mathbf{y}_{1:L}$, we can still partially quantify uncertainty in the recovered parameter vectors. 
Specifically, for fixed hyper-parameters $\boldsymbol{\theta}$, Bayes' theorem yields 
\begin{equation}\label{eq:posterior_x}
	\pi_{ \mathbf{X}_{1:L} | \boldsymbol{\Theta} = \boldsymbol{\theta}, \mathbf{Y}_{1:L} = \mathbf{y}_{1:L} }( \mathbf{x}_{1:L} )
		\propto \pi_{ \mathbf{Y}_{1:L} | \mathbf{X}_{1:L}}(  \mathbf{y}_{1:L} | \mathbf{x}_{1:L} ) \, \pi_{ \mathbf{X}_{1:L} | \boldsymbol{\Theta}}( \mathbf{x}_{1:L} | \boldsymbol{\theta} )
\end{equation}
for the fully conditional posterior for the parameter vectors $\mathbf{X}_{1:L}$. 
Here, $\pi_{ \mathbf{Y}_{1:L} | \mathbf{X}_{1:L}}$ is the likelihood density \cref{eq:joint_likelihood} and $\pi_{ \mathbf{X}_{1:L} | \boldsymbol{\Theta}}$ is the prior \cref{eq:joint_prior}.
Substituting \cref{eq:joint_likelihood,eq:joint_prior} into \cref{eq:posterior_x} yields 
\begin{equation}\label{eq:posterior_x2} 
	\pi_{ \mathbf{X}_{1:L} | \boldsymbol{\Theta} = \boldsymbol{\theta}, \mathbf{Y}_{1:L} = \mathbf{y}_{1:L} }( \mathbf{x}_{1:L} ) 
		\propto \exp\left( -\frac{1}{2} \sum_{l=1}^L \| F_l \mathbf{x}_l - \mathbf{y}_l \|_2^2 + \| D_{\boldsymbol{\theta}}^{-1/2} R \mathbf{x}_l \|_2^2 \right).
\end{equation}
Let $\Gamma_l = ( F_l^T F_l + {R^T D_{\boldsymbol{\theta}}^{-1} R} )^{-1}$. 
Some basic computations show that 
\begin{equation}\label{eq:simple_comp} 
	\| F_l \mathbf{x}_l - \mathbf{y}_l \|_2^2 + \| D_{\boldsymbol{\theta}}^{-1/2} R \mathbf{x}_l \|_2^2 
		= \left( \mathbf{x}_l - \Gamma_l F_l^T \mathbf{y}_l \right)^T \Gamma_l^{-1} \left( \mathbf{x}_l - \Gamma_l F_l^T \mathbf{y}_l \right)
\end{equation}
up to an additive constant that does not depend on $\mathbf{x}_l$.
Note that the right-hand side of \cref{eq:simple_comp} corresponds to the potential of a normal distribution with mean $\boldsymbol{\mu}_l = \Gamma_l F_l^T \mathbf{y}_l$ and covariance matrix $\Gamma_l$. 
Hence \cref{eq:posterior_x2,eq:simple_comp} imply that  
\begin{equation}\label{eq:posterior_x3} 
	\pi_{ \mathbf{X}_{l} | \boldsymbol{\Theta} = \boldsymbol{\theta}, \mathbf{Y}_{l} = \mathbf{y}_{l} }( \mathbf{x}_{l} ) 
		\propto \mathcal{N}( \mathbf{x}_l | \boldsymbol{\mu}_l, \Gamma_l ).
\end{equation}
That is, for fixed $\boldsymbol{\theta}$ and $\mathbf{y}_{l}$, 
the parameter vector $\mathbf{X}_l$ is normal distributed with mean $\boldsymbol{\mu}_l = \Gamma_l F_l^T \mathbf{y}_l$ and covariance matrix $\Gamma_l = ( F_l^T F_l + {R^T D_{\boldsymbol{\theta}}^{-1} R} )^{-1}$. 
The common kernel condition \cref{eq:common_kernel} ensures that $\Gamma_l$ is an SPD covariance matrix. 
We can now quantify uncertainty in $\mathbf{X}_l$ by sampling from the normal distribution and subsequently determining, for instance, the sample mean and credible intervals. 
We refer to \cite{vono2022high} for more details on sampling from high-dimensional Gaussian distributions. 
 
\begin{remark}[More meaningful uncertainty quantification]
	Although our approach quantifies uncertainty in the parameter vectors, it does not account for the uncertainty in the hyper-parameters. 
	To fully address uncertainty, Bayesian MAP estimation should be replaced with an alternative inference technique. 
	One option is the variational Bayesian approach proposed in \cite{agrawal2022variational}, which uses a mean-field approximation to estimate the posterior density. 
	While beyond the scope of the current investigation, another avenue for future work is to combine our joint-sparsity-promoting hierarchical Bayesian model with advanced sampling techniques such as those outlined in \cite{calvetti2008hypermodels,bardsley2012mcmc,owen2013monte}. 
	Such studies will provide a more meaningful uncertainty quantification. 
\end{remark}
