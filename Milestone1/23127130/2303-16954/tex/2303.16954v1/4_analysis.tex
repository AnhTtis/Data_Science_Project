\section{Analysis: Complexity, convexity, and convergence} 
\label{sec:analysis} 

We briefly analyze the computational complexity, convexity, and convergence of the MMV-IAS algorithm (\cref{algo:MMV_IAS}) and the underlying objective function.


\subsection{Computational complexity}
\label{sub:complexity}

Consider $L$ parameter vectors $\mathbf{x}_1,\dots,\mathbf{x}_L \in \R^N$. 
Different methods can be used to solve the $\mathbf{x}_l$-updates in \cref{algo:MMV_IAS}. 
Assuming that we use the PCG method, each $\mathbf{x}_l$-update has computational complexity $\mathcal{O}(\tilde{N}_l)$, where $\tilde{N}_l$ is the number of non-zero elements of the matrix $F_l^T F_l + R^T D_{\boldsymbol{\theta}}^{-1/2} R$. 
In the worst case ($\tilde{N}_l = N^2$ for all $l=1,\dots,L$), the computational cost for updating all parameter vectors is $\mathcal{O}(L N^2)$. 
As already noted, however, these updates can be performed in parallel. 
Furthermore, we perform the $\boldsymbol{\theta}$-update in \cref{algo:MMV_IAS} using one of the explicit formulas \cref{eq:update_beta4}. 
If $R \in \R^{K \times N}$ for $l=1,\dots,L$, then the $\boldsymbol{\theta}$-update has computational complexity $\mathcal{O}(K N)$. 
See \cite[Section 4.1]{glaubitz2022generalized} for more details.
In sum, if we run \cref{algo:MMV_IAS} for $I$ iterations, then its overall order of operations is (at most) $\mathcal{O}(I (L N^2 + K N) )$. 


\subsection{Convexity of the objective function}
\label{sub:convexity} 

We next investigate the convexity of the objective function $\mathcal{G}$ in \cref{eq:G}.  
\cref{thm:convexity} provides the choices of hyper-parameters $(r, \beta, \vartheta_{1:K})$ for which the objective function $\mathcal{G}$ is globally or locally convex, that is, when the convexity is restricted to specific values for $\boldsymbol{\theta}$. It also describes how the number of MMVs influences convexity.

\begin{theorem}[Convexity of the objective function]\label{thm:convexity}
	Let $\mathcal{G}$ be the objective function in \cref{eq:G} and $\eta = r \beta - (L/2 + 1)$. 
	\begin{enumerate}
		\item[(a)] 
		If $r \geq 1$ and $\eta > 0$, then $\mathcal{G}$ is globally convex. 
		
		\item[(b)]
		If $0<r<1$ and $\eta > 0$, or if $r < 0$, then $\mathcal{G}$ is convex provided that 
		\begin{equation}\label{eq:convexity_cond}
			\theta_k < \vartheta_k \left( \frac{\eta}{r |r-1|} \right)^{1/r}, 
				\quad k=1,\dots,K. 
		\end{equation}
		
	\end{enumerate}
\end{theorem}

\cref{thm:convexity} highlights the impact of MMV data on the convexity of the objective function $\mathcal{G}$ in our joint-sparsity-promoting hierarchical Bayesian model. 
In particular, as $L$ increases, the linear decrease of $\eta$ results in the condition $\eta > 0$ becoming more restrictive. 
Furthermore, since  $\eta$ decreases as $L$ increases, we see in part (b) that the right-hand side of \cref{eq:convexity_cond} is also smaller.
That is, the convex set in which $G$ is convex shrinks as $L$ increases, revealing a trade-off between promoting joint sparsity and decreasing convexity as the number of coupled MMV data and parameter vectors increases.
\cref{thm:convexity} is a natural extension of \cite[Theorem 4.1]{calvetti2020sparse} (also see \cite[Theorem 3.1]{calvetti2020sparsity}), which we recover as the special case of $L=1$ (and $R = I$). 
The proof is provided in \cref{app:convexity_proof}.

\begin{remark} [Convergence of MMV-IAS]\label{rem:convergence} 
	The convergence of the IAS algorithm has been established for single measurement vector cases with specific choices of hyper-hyper-parameters in previous studies \cite{calvetti2015hierarchical,calvetti2019hierachical}.  
	While investigating convergence, particularly in combination with the extension to MMV data, is highly desirable, it falls outside the scope of the present work and will be the subject of future research.
\end{remark}
