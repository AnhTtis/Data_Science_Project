\section{Proof of \cref{thm:convexity}}
\label{app:convexity_proof}

In this section, we prove \cref{thm:convexity}. 
To this end, we first present two auxiliary lemmas. 

\begin{lemma}\label{lem:derivatives} 
	Let $r \in \R\setminus\{0\}$ and $\beta, \vartheta_k > 0$ for $k=1,\dots,K$, and denote $\eta = r \beta - (L/2 + 1)$. 
	The objective function $\mathcal{G}$ in \cref{eq:G} has the following second derivatives: 
	\begin{equation}\label{eq:derivatives}
	\begin{aligned}
		\nabla_{\mathbf{x}_l} \nabla_{\mathbf{x}_m} \, \mathcal{G} 
			& = \delta_{l,m} \left( F_l^T F_l + R^T D_{\boldsymbol{\theta}}^{-1} R \right), \\ 
		\left[ \nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} \, \mathcal{G} \right]_{j,k} 
			& = \delta_{j,k} \left( \theta_k^{-3} \left( \sum_{l=1}^L [R \mathbf{x}_l]_k^2 \right) + \theta_k^{r-2} \left( \frac{r(r-1)}{\vartheta_k^r} \right) + \theta_k^{-2} \eta \right), \\ 
		\left[ \nabla_{\boldsymbol{\theta}} \nabla_{\mathbf{x}_l} \, \mathcal{G} \right]_{k,n}  
			& = - \theta_k^{-2} [R]_{k,n} [ R \mathbf{x}_l ]_k, 
	\end{aligned}
	\end{equation}	
	for $j,k=1,\dots,K$ and $n=1,\dots,N$. 
	Here, $\delta_{l,m}$ is the usual Kronecker delta with $\delta_{l,m} = 1$ if $l=m$ and $\delta_{l,m} = 0$ otherwise.
\end{lemma}

\begin{proof} 
	Simple calculations show that the first derivatives are 
	\begin{equation}\label{eq:derivatives_proof1}
	\begin{aligned}
		\nabla_{\mathbf{x}_l} \, \mathcal{G} 
			& = F_l^T \left( F_l \mathbf{x}_l - \mathbf{y}_l \right) + R^T D_{\boldsymbol{\theta}}^{-1} R \mathbf{x}_l, \\ 
		\left[ \nabla_{\boldsymbol{\theta}} \, \mathcal{G} \right]_k 
			& = - \theta_k^{-2} \left( \sum_{l=1}^L [ R  \mathbf{x}_l]_k^2/2 \right) + \theta_k^{r-1} \left( \frac{r}{\vartheta_k^r} \right) - \theta_k^{-1} \eta,
	\end{aligned}
	\end{equation} 
	for $l=1,\dots,L$ and $k=1,\dots,K$.
	Next, we can conclude from \cref{eq:derivatives_proof1} that 
	\begin{equation}
	\begin{aligned}
		\nabla_{\mathbf{x}_l} \nabla_{\mathbf{x}_m} \, \mathcal{G} 
			& = \delta_{l,m} \left( F_l^T F_l + R^T D_{\boldsymbol{\theta}}^{-1} R \right), \\ 
		\left[ \nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} \, \mathcal{G} \right]_{j,k} 
			& = \delta_{j,k} \left( \theta_k^{-3} \left( \sum_{l=1}^L [R \mathbf{x}_l]_k^2 \right) + \theta_k^{r-2} \left( \frac{r(r-1)}{\vartheta_k^r} \right) + \theta_k^{-2} \eta \right),
	\end{aligned}
	\end{equation} 
	for $l,m = 1,\dots,L$ and $j,k=1,\dots,K$. 
	To determine the mixed derivatives $\nabla_{\boldsymbol{\theta}} \nabla_{\mathbf{x}_l} \, \mathcal{G}$, note that 
	\begin{equation} 
		\left[ R^T D_{\boldsymbol{\theta}}^{-1} R \mathbf{x}_l \right]_n 
			= \sum_{j=1}^K \left[ R^T \right]_{n,j} \left[ D_{\boldsymbol{\theta}}^{-1} R \mathbf{x}_l \right]_j 
			= \sum_{j=1}^K \left[ R \right]_{j,n} \theta_k^{-1} \left[ R \mathbf{x}_l \right]_j
	\end{equation} 
	for $n=1,\dots,N$ and $l=1,\dots,L$.
	Hence we obtain
	\begin{equation} 
	\begin{aligned}
		\left[ \nabla_{\boldsymbol{\theta}} \nabla_{\mathbf{x}_l} \, \mathcal{G} \right]_{k,n} 
			= \partial_{\theta_k} \left[ R^T D_{\boldsymbol{\theta}}^{-1} R \mathbf{x}_l \right]_n 
			= - \theta_k^{-2} [R]_{k,n} [ R \mathbf{x}_l ]_k
	\end{aligned}
	\end{equation} 
	for $k=1,\dots,K$ and $n=1,\dots,N$.
\end{proof} 

The next lemma provides a lower bound in terms of the Hessian of the objective function $G$, allowing us to investigate its convexity.

\begin{lemma}\label{lem:Hessian}
	Let $r \in \R\setminus\{0\}$ and $\beta, \vartheta_k > 0$ for $k=1,\dots,K$. 
	Moreover, let 
	\begin{equation}\label{eq:Hessian}
		H = H( \mathbf{x}_{1:L}, \boldsymbol{\theta} ) = 
		\begin{bmatrix} 
			\nabla_{\mathbf{x}_{1:L}} \nabla_{\mathbf{x}_{1:L}} \, \mathcal{G} & \nabla_{\mathbf{x}_{1:L}} \nabla_{\boldsymbol{\theta}} \, \mathcal{G} \\ 
			\nabla_{\boldsymbol{\theta}} \nabla_{\mathbf{x}_{1:L}} \, \mathcal{G} & \nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} \, \mathcal{G}
		\end{bmatrix}
	\end{equation}
	be the Hessian of the objective function $\mathcal{G}$ in \cref{eq:G} and let $\mathbf{u} = [\mathbf{v}_{1:L}; \mathbf{w}]$ with $\mathbf{v}_l \in \R^N$, $l=1,\dots,L$, and $\mathbf{w} \in \R^K$. 
	Then, 
	\begin{equation}\label{eq:Hessian_ineq}
		\mathbf{u}^T H \mathbf{u} 
			\geq  \sum_{k=1}^K \theta_k^{-2} w_k^2 \left( \theta_k^{r} \left( \frac{r(r-1)}{\vartheta_k^r} \right) + \eta \right), 
	\end{equation} 
	where $\eta = r \beta - (L/2 + 1)$.
\end{lemma}

\begin{proof} 
	We start by noting that   
	\begin{equation}\label{eq:uTHu} 
		\mathbf{u}^T H \mathbf{u} 
			= \sum_{l=1}^L \mathbf{v}_l^T \left( \nabla_{\mathbf{x}_l} \nabla_{\mathbf{x}_l} \, \mathcal{G} \right) \mathbf{v}_l 
				+ 2 \sum_{l=1}^L \mathbf{w}^T \left( \nabla_{\boldsymbol{\theta}} \nabla_{\mathbf{x}_l} \, \mathcal{G} \right) \mathbf{v}_l 
				+ \mathbf{w}^T \left( \nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} \, \mathcal{G} \right) \mathbf{w}.
	\end{equation} 
	Substituting the second derivatives \cref{eq:derivatives} from \cref{lem:derivatives} yields  
	\begin{equation}\label{eq:ABC} 
	\begin{aligned}
		\mathbf{v}_l^T \left( \nabla_{\mathbf{x}_l} \nabla_{\mathbf{x}_l} \, \mathcal{G} \right) \mathbf{v}_l 
			& = \sum_{m=1}^M \left[ F_l \mathbf{v}_l \right]_m^2 + \sum_{k=1}^K \theta_k^{-1} \left[ R \mathbf{v}_l \right]_k^2, \\ 
		\mathbf{w}^T \left( \nabla_{\boldsymbol{\theta}} \nabla_{\mathbf{x}_l} \, \mathcal{G} \right) \mathbf{v}_l  
			& = - \sum_{k=1}^K \theta_k^{-2} w_k \left[ R \mathbf{x}_l \right]_k \left[ R \mathbf{v}_l \right]_k, \\ 
		\mathbf{w}^T \left( \nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} \, \mathcal{G} \right) \mathbf{w} 
			& = \sum_{k=1}^K w_k^2 \left( \theta_k^{-3} \left( \sum_{l=1}^L [R \mathbf{x}_l]_k^2 \right) + \theta_k^{r-2} \left( \frac{r(r-1)}{\vartheta_k^r} \right) + \theta_k^{-2} \eta \right).
	\end{aligned} 
	\end{equation} 
	Furthermore, substituting \cref{eq:ABC} into \cref{eq:uTHu} we obtain  
	\begin{equation}\label{eq:uTHu2} 
	\begin{aligned}
		\mathbf{u}^T H \mathbf{u} 
			= & \sum_{l=1}^L \sum_{m=1}^M \left[ F_l \mathbf{v}_l \right]_m^2 \\
			& + \sum_{l=1}^L \sum_{k=1}^K \left( \theta_k^{-1} \left[ R \mathbf{v}_l \right]_k^2 - 2 \theta_k^{-2} \left[ R \mathbf{v}_l \right]_k w_k \left[ R \mathbf{x}_l \right]_k + \theta_k^{-2} w_k^2 \left[ R \mathbf{x}_l \right]_k^2 \right) \\ 
			& + \sum_{k=1}^K \theta_k^{-2} w_k^2 \left( \theta_k^{r} \left( \frac{r(r-1)}{\vartheta_k^r} \right) + \eta \right). 
	\end{aligned}
	\end{equation} 
	Note that 
	\begin{equation} 
		\theta_k^{-2} \left[ R \mathbf{v}_l \right]_k w_k \left[ R \mathbf{x}_l \right]_k + \theta_k^{-2} w_k^2 \left[ R \mathbf{x}_l \right]_k^2 
			= \theta_k^{-3} \left( \theta_k \left[ R \mathbf{v}_l \right]_k - w_k \left[ R \mathbf{x}_l \right]_k \right)^2.
	\end{equation}
	Hence, we can rewrite \cref{eq:uTHu2} as 
	\begin{equation}\label{eq:uTHu3} 
	\begin{aligned}
		\mathbf{u}^T H \mathbf{u} 
			= & \sum_{l=1}^L \sum_{m=1}^M \left[ F_l \mathbf{v}_l \right]_m^2 
			+ \sum_{l=1}^L \sum_{k=1}^K \theta_k^{-3} \left( \theta_k \left[ R \mathbf{v}_l \right]_k - w_k \left[ R \mathbf{x}_l \right]_k \right)^2 \\ 
			& + \sum_{k=1}^K \theta_k^{-2} w_k^2 \left( \theta_k^{r} \left( \frac{r(r-1)}{\vartheta_k^r} \right) + \eta \right). 
	\end{aligned}
	\end{equation} 
	Finally, note that the first two sums on the right-hand side of \cref{eq:uTHu3} are non-negative, which yields the assertion.
\end{proof} 

We are now positioned to prove \cref{thm:convexity}. 

\begin{proof}[Proof of \cref{thm:convexity}]
	Recall that $\mathcal{G}$ is convex if and only if its Hessian $H$ satisfies $\mathbf{u}^T H \mathbf{u} \geq 0$ for all $\mathbf{u} = [\mathbf{v}_{1:L}; \mathbf{w}]$. 
	Let $\mathbf{u} = [\mathbf{v}_{1:L}; \mathbf{w}]$, then \cref{lem:Hessian} implies 
	\begin{equation}\label{eq:convexity_proof1}
		\mathbf{u}^T H \mathbf{u} 
			\geq  \sum_{k=1}^K \theta_k^{-2} w_k^2 \left( \theta_k^{r} \left( \frac{r(r-1)}{\vartheta_k^r} \right) + \eta \right).
	\end{equation} 
	The right-hand side of \cref{eq:convexity_proof1} is positive if 
	\begin{equation}\label{eq:convexity_proof2}
		\theta_k^{r} \left( \frac{r(r-1)}{\vartheta_k^r} \right) > -\eta, \quad k=1,\dots,K.
	\end{equation} 
	The proof for the different cases follows by enforcing condition \cref{eq:convexity_proof2}.
\end{proof} 
