 \section{Numerical results} 
\label{sec:numerics} 

We conduct numerical experiments to showcase the effectiveness of our joint-sparsity-promoting MMV-IAS and MMV-GSBL algorithms, detailed in \cref{algo:MMV_IAS,algo:MMV_GSBL}.
For a fair comparison, we also evaluate the individual signal recovery performance using the traditional IAS and GSBL algorithms with the same model parameters. 
The MATLAB code used to generate the numerical tests can be found in the code repository \url{https://github.com/jglaubitz/LeveragingJointSparsity}. 


\subsection{Hyper-prior parameter selection}
\label{sub:param_selection}

For all signal recovery problems, we chose $(r,\beta,\vartheta) = (-1,1,10^{-4})$ for the IAS and MMV-IAS algorithm and $(\beta,\vartheta) = (1,10^{4})$ for the GSBL and MMV-GSBL algorithm. 
For the imaging application in \cref{sub:parallel_MRI}, we chose $(r,\beta,\vartheta) = (-1,1,10^{-3})$ and $(\beta,\vartheta) = (1,10^{3})$. 
Similar parameters were used in \cite{glaubitz2022generalized,xiao2022sequential} and \cite{calvetti2020sparse,calvetti2020sparsity} for the GSBL and IAS algorithm, respectively. 
We did not attempt to optimize any of these parameters.  


\subsection{Signal deblurring}
\label{sub:deblurring}

We first consider (jointly) deblurring four piecewise-constant signals with a shared edge profile. 
The signals are generated by fixing five transition points in the interval $[0,1]$, dividing $[0,1]$ into six constant subintervals on which the signals are constant, and then randomly assigning signal values drawn from a uniform distribution.
The values are then normalized such that the maximum value of each signal is set to $1$. 
\cref{fig:deb_signal1_intro,fig:deb_signal2_intro} in \Cref{sec:introduction} show the first two signals and the given noisy blurred measurements. 
We aim to recover the nodal values $\mathbf{x}_{1:4}$ of all four signals at $N = 40$ equidistant grid points. 
The corresponding data model is 
\begin{equation}\label{eq:deblurring_model}
    \mathbf{y}_l = F \mathbf{x}_l + \mathbf{e}_l, \quad 
    l=1,\dots,4.
\end{equation} 
The discrete forward operator, $F$, represents the application of the midpoint quadrature to the convolution equation 
\begin{equation} 
	y(s) = \int_0^1 k(s-s') x(s) \intd s',
\end{equation}
where we assume a Gaussian convolution kernel of the form 
\begin{equation} 
	k(s) = \frac{1}{2 \pi \gamma^2} \exp\left( - \frac{s^2}{2 \gamma^2} \right)
\end{equation} 
with blurring parameter $\gamma = 3 \cdot 10^{-2}$. 
The forward operator is then given by 
\begin{equation}\label{eq:disc_convolution}
	[F]_{m,n} = h k( h[i-j] ), \quad i,j=1,\dots,n,
\end{equation}
where $h=1/n$ is the distance between consecutive grid points. 
Note that $F$ has full rank but quickly becomes ill-conditioned. 
The noise vectors $\mathbf{e}_{1:4}$ in \cref{eq:deblurring_model} are i.i.d., with zero mean and a common variance $\sigma^2 = 10^{-2}$. 
To reflect our prior knowledge that the signals are piecewise constant, we use  
\begin{equation}\label{eq:deblurring_R}
	R = 
    \begin{bmatrix}
        -1 & 1 & & \\ 
         & \ddots & \ddots & \\ 
         & & -1 & 1 
    \end{bmatrix} 
    \in \R^{(n-1) \times n} 
\end{equation} 
for the sparsifying operator. 
\cref{fig:deb_signal1_IAS_L4,fig:deb_signal2_IAS_L4} show the recovered first two signals using the IAS algorithm (to promote sparsity separately) and the proposed MMV-IAS algorithm (to promote sparsity jointly), respectively. 
\cref{fig:deb_signal1_GSBL_L4,fig:deb_signal2_GSBL_L4} report on the same test using the GSBL and MMV-GSBL algorithms.  
The results demonstrate that incorporating joint sparsity into the IAS and GSBL algorithms improves signal recovery accuracy. 

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_L4_theta} 
    	\caption{First signal, IAS and MMV-IAS}
    	\label{fig:deb_signal1_IAS_L4_theta}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_IAS_L4_theta} 
    	\caption{Second signal, IAS and MMV-IAS}
    	\label{fig:deb_signal2_IAS_L4_theta}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_GSBL_L4_theta} 
    	\caption{First signal, GSBL and MMV-GSBL}
    	\label{fig:deb_signal1_GSBL_L4_theta}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_GSBL_L4_theta} 
    	\caption{Second signal, GSBL and MMV-GSBL}
    	\label{fig:deb_signal2_GSBL_L4_theta}
  	\end{subfigure}%
  	\caption{ 
  	Top row: Normalized MAP estimate of the hyper-parameter $\theta$ for the first two signals using the IAS and MMV-IAS algorithms. 
	Bottom row: Normalized inverse MAP estimate of $\theta$ for the first two signals using the GSBL and MMV-GSBL algorithm. 
  	}
  	\label{fig:deb_signal_theta}
\end{figure}

The improved accuracy of the MMV-IAS and MMV-GSBL algorithms can be attributed to the use of a common hyper-parameter vector $\boldsymbol{\theta}$ that more accurately detects edge locations compared to separate hyper-parameter vectors $\boldsymbol{\theta}_{1:L}$ used in the IAS and GSBL algorithms. 
This is evident in \cref{fig:deb_signal_theta}, which shows the normalized estimated hyper-parameters produced by the MMV-IAS and MMV-GSBL algorithms for the first two signals. 
While the MMV-IAS and MMV-GSBL algorithms accurately capture all edge locations, the IAS and GSBL algorithms produce visibly erroneous hyper-parameter profiles. 
The impact of missed true edge locations and false detection of others are clearly visible in \cref{fig:deb_signal1_GSBL_L4}, which shows that the MMV-GSBL approach eliminates the artificial edges around $0.1$ on the horizontal axis. 
Similarly, \cref{fig:deb_signal2_GSBL_L4} demonstrates that the MMV-GSBL approach retains the existing edges around $0.2$ on the horizontal axis, which are missed by the GSBL approach. 

\begin{figure}[tb]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_L4_CI} 
    	\caption{First signal, IAS algorithm}
    	\label{fig:deb_signal1_IAS_L4_CI}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_GSBL_L4_CI} 
    	\caption{First signal, GSBL algorithm}
    	\label{fig:deb_signal1_GSBL_L4_CI}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_MMVIAS_L4_CI} 
    	\caption{First signal, MMV-IAS algorithm}
    	\label{fig:deb_signal1_MMVIAS_L4_CI}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_MMVGSBL_L4_CI} 
    	\caption{First signal, MMV-GSBL algorithm}
    	\label{fig:deb_signal1_MMVGSBL_L4_CI}
  	\end{subfigure}%
  	\caption{ 
  	The $99.9\%$ credible intervals (CIs) for the recovered first signal using the IAS, GSB, MMV-IAS, and MMV-GSBL algorithms 
  	}
  	\label{fig:deb_signal_CI}
\end{figure}

The proposed MMV-IAS and MMV-GSBL algorithms have the additional advantage of quantifying uncertainty in the recovered signals, as described in \cref{sub:UQ_IAS} and \cref{rem:GSBL_UQ}. 
This is demonstrated in \cref{fig:deb_signal_CI}, which shows the $99.9\%$ credible intervals of the fully conditional posterior densities $\pi_{\mathbf{X}_1|\boldsymbol{\Theta}=\boldsymbol{\theta},\mathbf{Y}_{1:L}=\mathbf{y}_{1:L}}$ of the first recovered signal for the IAS, MMV-IAS, GSBL, and MMV-GSBL model.
Here, $\mathbf{y}_{1:L}$ are the given noisy blurred MMVs and $\boldsymbol{\theta}$ is the estimated hyper-parameter vector.


\subsection{Error and success analysis}
\label{sub:analysis}

We now conduct a synthetic sparse signal recovery experiment to further assess the performance of the proposed joint-sparsity-promoting MMV-IAS and MMV-GSBL algorithms. 
We consider $L$ randomly generated signals, $\mathbf{x}_{1:L}$, each of size $N$. 
We fix the number of measurements, $M$, non-zero components, $s$, and trials, $T$. 
For each trial, $t=1,\dots,T$, we proceed as follows:
\begin{enumerate}
	\item[(i)] 
	Generate a support set $S \subset \{ 1,\dots,N \}$ uniformly at random with size $|S| = s$;
	
	\item[(ii)] 
	Define signal vectors $\mathbf{x}_1,\dots,\mathbf{x}_L$ such that $\mathrm{supp}(\mathbf{x}_1) = \dots = \mathrm{supp}(\mathbf{x}_L) = S$, where the non-zero entries are drawn from the standard normal distribution; 
	
	\item[(iii)] 
	Generate a forward operator $F$ as described below, fix a noise variance $\sigma^2$, and compute the measurement vectors $\mathbf{y}_l = F \mathbf{x}_l + \mathbf{e}_l$, where $\mathbf{e}_l$ is drawn from $\mathcal{N}(\mathbf{0},\sigma^2 I)$;
	
	\item[(iv)]
	Compute the reconstructions $\hat{\mathbf{x}}_1,\dots,\hat{\mathbf{x}}_L$ using the desired algorithm (e.g., IAS or MMV-IAS); 
	 
	\item[(v)] 
	Compute the normalized error $E_t = \sqrt{ \sum_{l=1}^L \| \mathbf{x}_l - \hat{\mathbf{x}}_l \|_2^2 / \sum_{l=1}^L \| \mathbf{x}_l \|_2^2 }$ for each algorithm.
	
\end{enumerate}
Finally, we evaluate the algorithm performance using the average error and empirical success probability (ESP). 
The \emph{average error} is $E = (E_1+\dots+E_T)/T$, i.e., the average of the individual trial errors. 
The \emph{ESP} is the fraction of trials that successfully recovered the vectors $\mathbf{x}_1,\dots,\mathbf{x}_L$ up to a given tolerance $\varepsilon_{\rm tol}$, i.e., $E_t < \varepsilon_{\rm tol}$. 
We use a subsampled discrete cosine transform (DFT) as the forward operator $F$. 
This mimics the situation in which Fourier data are collected (e.g., synthetic aperture radar and magnetic resonance imaging), but some of the data are determined to be unusable due to a system malfunction or obstruction.
Specifically, we generate a set $\Omega \subset \{1,\dots,N\}$ of size $M$ uniformly at random and let 
\begin{equation} 
	F = P_{\Omega} A, 
\end{equation} 
where $A \in \R^{N \times N}$ is the DCT matrix and $P_{\Omega} \in \R^{M \times N}$ is the operator selecting rows of $A$ corresponding to the indices in $\Omega$. 
The identity matrix is used as the sparsifying operator as the signals are assumed to be sparse. 
In our experiments we set $N=100$, {$s=20$}, $T=10$, $\sigma^2=10^{-6}$, and $\varepsilon_{\rm tol}=10^{-2}$. 

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_IAS_C4} 
    	\caption{Average errors, $L=4$}
    	\label{fig:comparision_error_IAS_C4}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_IAS_C8} 
    	\caption{Average errors, $L=8$}
    	\label{fig:comparision_error_IAS_C8}
  	\end{subfigure}%
  	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_IAS_C16} 
    	\caption{Average errors, $L=16$}
    	\label{fig:comparision_error_IAS_C16}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_IAS_C4} 
    	\caption{Success probability, $L=4$}
    	\label{fig:comparision_succ_IAS_C4}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_IAS_C8} 
    	\caption{Success probability, $L=8$}
    	\label{fig:comparision_succ_IAS_C8}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_IAS_C16} 
    	\caption{Success probability, $L=16$}
    	\label{fig:comparision_succ_IAS_C16}
  	\end{subfigure}% 
  	\caption{ 
	Comparison of the average error and success probability for the sparsity-promoting IAS algorithm (blue triangles) and joint-sparsity-promoting MMV-IAS algorithm (green squares). 
	We recover a signal of size $N=100$ with $s=20$ non-zero entries from an increasing number of measurements $m$. 
  	}
  	\label{fig:comparision_IAS}
\end{figure}

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_GSBL_C4} 
    	\caption{Average errors, $L=4$}
    	\label{fig:comparision_error_GSBL_C4}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_GSBL_C8} 
    	\caption{Average errors, $L=8$}
    	\label{fig:comparision_error_GSBL_C8}
  	\end{subfigure}%
  	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_GSBL_C16} 
    	\caption{Average errors, $L=16$}
    	\label{fig:comparision_error_GSBL_C16}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_GSBL_C4} 
    	\caption{Success probability, $L=4$}
    	\label{fig:comparision_succ_GSBL_C4}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_GSBL_C8} 
    	\caption{Success probability, $L=8$}
    	\label{fig:comparision_succ_GSBL_C8}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_GSBL_C16} 
    	\caption{Success probability, $L=16$}
    	\label{fig:comparision_succ_GSBL_C16}
  	\end{subfigure}% 
  	\caption{ 
	Comparison of the average error and success probability for the sparsity-promoting GSBL algorithm (blue triangles) and joint-sparsity-promoting MMV-GSBL algorithm (green squares). 
	We recover a signal of size $N=100$ with $s=20$ non-zero entries from an increasing number of measurements $m$. 
	}
  	\label{fig:comparision_GSBL}
\end{figure}  

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/phase_IAS_L4} 
    	\caption{IAS algorithm (separately recovered)}
    	\label{fig:phase_IAS_L4}
  	\end{subfigure}
  	%
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/phase_MMVIAS_L4} 
    	\caption{MMV-IAS algorithm, $L=4$}
    	\label{fig:phase_MMVIAS_L4}
  	\end{subfigure}
	\\
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/phase_MMVIAS_L8} 
    	\caption{MMV-IAS algorithm, $L=8$}
    	\label{fig:phase_MMVIAS_L8}
  	\end{subfigure}
  	%
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/phase_MMVIAS_L16} 
    	\caption{MMV-IAS algorithm, $L=16$}
    	\label{fig:phase_MMVIAS_L16}
  	\end{subfigure}
	%
  	\caption{ 
	Phase transition diagrams for the IAS and MMV-IAS algorithm for $L = 4,8,16$. 
	The diagrams show the success probability for values $1 \leq s \leq N$ and $1 \leq M \leq N$. 
  	}
  	\label{fig:phase}
\end{figure} 

The performance of the proposed joint-sparsity-promoting MMV-IAS and MMV-GSBL algorithms is evaluated and compared to the existing IAS and GSBL algorithms in \cref{fig:comparision_IAS,fig:comparision_GSBL}. 
These figures report the average errors and ESP for different numbers of MMVs ($L = 4, 8, 16$). 
The results show that the proposed algorithms outperform the existing ones regarding average errors and ESP in most cases. 
As the number of MMVs increases, the superiority of the proposed algorithms becomes more pronounced. 
For instance, when $L=16$, the MMV-IAS algorithm requires only around $40$ measurements per signal for successful recovery, while the IAS algorithm requires around $70$. 
The phase transition plots in \cref{fig:phase} further demonstrate the improved performance of the MMV-IAS algorithm, which exhibits a phase transition close to the optimal $m = s$ line. 
The phase transition profiles for the MMV-GSBL algorithm are similar to the MMV-IAS algorithm but are not included here for brevity.


\subsection{Application to parallel magnetic resonance imaging} 
\label{sub:parallel_MRI} 

We next apply the proposed MMV-IAS and MMV-GSBL algorithms to a parallel MRI test problem. 
Parallel MRI is a multi-sensor acquisition system that uses multiple coils to simultaneously acquire image measurements for recovery. 
Details on parallel MRI can be found in \cite{guerquin2011realistic,chun2015efficient,chun2017compressed,adcock2019joint}. 
A standard discrete data model for parallel MRI is the following: 
Let $\mathbf{x} \in \C^N$ be the vectorized image to be recovered and $L$ be the number of coils. 
For the $l$th coil, the measurements acquired are 
\begin{equation}\label{eq:pMRI_model}
	\mathbf{y}_l = P_{\Omega_l} F \mathbf{x} + \mathbf{e}_l, 
\end{equation} 
where $F \in \C^{N \times N}$ is the discrete Fourier transform (DFT) matrix, $P_{\Omega_l} \in \C^{M \times N}$ is a sampling operator that selects the rows of $F$ corresponding to the frequencies in $\Omega_l$, and $\mathbf{e}_l \in \C^M$ is noise. 
The image measurement acquired by each of the $L$ coils is intrinsic to the particular coil. 
A typical sampling procedure for parallel MRI is to use data taken as radial line sampling in the Fourier space. 
\cref{fig:pMRI_sampling} illustrates the radial sampling maps corresponding to four different coils. 

\begin{figure}[tb]
	\centering
  	\includegraphics[width=\textwidth]{figures/sampling_maps} 
  	\caption{Four different radial sampling maps}
  	\label{fig:pMRI_sampling}
\end{figure} 

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_ref} 
		\caption{Reference image}
    		\label{fig:pMRI_ref} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_IAS} 
		\caption{IAS reconstruction}
		\label{fig:pMRI_coil1_IAS} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_GSBL} 
		\caption{GSBL reconstruction}
		\label{fig:pMRI_coil1_GSBL}
  	\end{subfigure}
	\\ 
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_LS} 
		\caption{Least squares reconstruction}
		\label{fig:pMRI_coil1_LS} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_MMVIAS} 
		\caption{MMV-IAS reconstruction}
		\label{fig:pMRI_coil1_MMVIAS} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_MMVGSBL} 
		\caption{MMV-GSBL reconstruction}
		\label{fig:pMRI_coil1_MMVGSBL} 
  	\end{subfigure}
  	%
	\caption{ 
	Reference image and the reconstructed first coil images
  	}
  	\label{fig:pMRI_coil_images}
\end{figure} 

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_ref} 
		\caption{Reference image}
		\label{fig:pMRI_overall_ref} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_IAS} 
		\caption{IAS reconstruction}
		\label{fig:pMRI_overall_IAS}
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_GSBL} 
		\caption{GSBL reconstruction}
		\label{fig:pMRI_overall_GSBL}
  	\end{subfigure}
	\\ 
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_LS} 
		\caption{Least squares reconstruction}
		\label{fig:pMRI_overall_LS}
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_MMVIAS} 
		\caption{MMV-IAS reconstruction} 
		\label{fig:pMRI_overall_MMVIAS} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_MMVGSBL} 
		\caption{MMV-GSBL reconstruction} 
		\label{fig:pMRI_overall_MMVGSBL} 
  	\end{subfigure}
	%
  	\caption{ 
	Reference image and the reconstructed overall images. 
	For all methods, $20$ lines/angles (corresponding to around $16\%$ sampling of the k-space), $\sigma^2 = 10^{-3}$, and $L=4$ coils were used.
  	}
  	\label{fig:pMRI_overall_images}
\end{figure} 

\begin{figure}[tb]
	\centering
	\begin{subfigure}[b]{0.495\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/error_overall_IAS} 
		\caption{Relative overall error, IAS \& MMV-IAS} 
		\label{fig:error_overall_IAS}
  	\end{subfigure}
  	%
	\begin{subfigure}[b]{0.495\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/error_overall_GSBL} 
		\caption{Relative overall error, GSBL \& MMV-GSBL} 
		\label{fig:error_overall_GSBL}
  	\end{subfigure}
	%
  	\caption{ 
	Relative error of the recovered overall image using the least squares (LS) approach, the existing IAS/GSBL algorithm, and the proposed MMV-IAS/GSBL method.
	In all cases, we used $L=4$ coils, noise variance $\sigma^2 = 10^{-3}$, and a varying number of lines. 
  	}
  	\label{fig:pMRI_errors}
\end{figure} 

Many techniques have been proposed for parallel MRI, see \cite{chun2015efficient} and references therein. 
Here we focus on the coil-by-coil approach, first computing the approximate coil images $\mathbf{\hat{x}}_{1:L}$ from \cref{eq:pMRI_model} and then computing an approximation $\mathbf{\hat{x}}$ to the overall image by considering the average of the coil images, i.e., $\mathbf{\hat{x}} = (\mathbf{\hat{x}}_{1} + \dots + \mathbf{\hat{x}}_{L})/L$. 
In our experiment, we compare the recovery of the $256 \times 256$ Shepp--Logan phantom image in \cref{fig:pMRI_ref} using the least-squares (LS) approach, the existing IAS/GSBL algorithm, and the proposed MMV-GSBL/IAS algorithm to recover the coil images. 
We used the anisotropic first-order discrete gradient operator as the sparsifying operator. 
\cref{fig:pMRI_coil_images} shows the recovered first coil images for $20$ lines, noise variance $\sigma^2 = 10^{-3}$, and $L=4$ coils.
The recovered first coil images using the proposed MMV-IAS (\cref{fig:pMRI_coil1_MMVIAS}) and MMV-GSBL (\cref{fig:pMRI_coil1_MMVGSBL}) algorithms are visibly more accurate than using the corresponding IAS (\cref{fig:pMRI_coil1_IAS}) and GSBL (\cref{fig:pMRI_coil1_GSBL}) algorithms.  
Note the sharper transitions between internal structures. 
Consequently, the proposed MMV-IAS/GSBL algorithm also yields a more accurate approximation to the overall image, compared to the existing IAS/GSBL algorithm, which is demonstrated in \cref{fig:pMRI_overall_images}.
To further assess the performance of the proposed joint-sparsity-promoting MMV-IAS and MMV-GSBL algorithms, \cref{fig:pMRI_errors} reports the relative error of the recovered overall image for varying numbers of lines sampled in the Fourier space. 
The proposed MMV-IAS/GSBL algorithm to jointly recover the coil images consistently yields the smallest error. 