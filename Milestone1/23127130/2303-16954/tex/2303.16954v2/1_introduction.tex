\section{Introduction} 
\label{sec:introduction} 

% 1) Why should others care? 
Parameter estimation from observable measurements is of fundamental importance in science and engineering applications.
Multiple measurement vectors (MMVs) can often be obtained from various sources, each having distinct underlying parameter vectors due to differences in spatial or temporal conditions \cite{cotter2005sparse,wipf2007empirical,adcock2019joint}. 
This situation can be modeled as a set of linear inverse problems given by
\begin{equation}\label{eq:MMV_IP} 
	\mathbf{y}_l = F_l \mathbf{x}_l + \mathbf{e}_l, \quad l=1,\dots,L,
\end{equation}  
where $\mathbf{y}_1,\dots,\mathbf{y}_L$ are the available MMVs, $\mathbf{x}_1,\dots,\mathbf{x}_L$ represent the sought-after parameter vectors, $F_{1},\dots,F_{L}$ are explicitly known linear forward operators, and $\mathbf{e}_1,\dots,\mathbf{e}_L$ denote the unknown noise component. 
The linear forward operators are often poorly conditioned, and the measurements may be limited in number or resolution, and contaminated by noise, causing the set of inverse problems \cref{eq:MMV_IP} to be ill-posed.

A well-known effective strategy used to mitigate ill-posedness is to incorporate prior information regarding the unknown parameter vectors, 
and in this study, we assume that these parameter vectors exhibit {\em joint sparsity}. 
Specifically, we assume there exists a linear operator $R$ (e.g., a discrete gradient or wavelet transform) such that $R \mathbf{x}_1,\dots,R \mathbf{x}_L$ are sparse and have common support. 
For example, the parameter vectors could correspond to piecewise constant signals with the same interior edge locations but different values. 
\cref{fig:deb_signal1_intro,fig:deb_signal2_intro} depict this scenario for the first two of four jointly sparse piecewise constant signals.
Joint sparsity arises in various applications, including signal processing, source location, neuro-electromagnetic imaging, parallel MRI, hyper-spectral imaging, and SAR imaging. 
For further reading on this topic, see \cite{cotter2005sparse,wipf2007empirical,adcock2019joint,zhang2022empirical} and related references.

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1} 
    	\caption{First signal and measurements}
    	\label{fig:deb_signal1_intro}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_rm1_L4} 
    	\caption{Recovered first signal}
    	\label{fig:deb_signal1_IAS_rm1_L4_intro}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2} 
    	\caption{Second signal and measurements}
    	\label{fig:deb_signal2_intro}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_IAS_rm1_L4} 
    	\caption{Recovered second signal}
    	\label{fig:deb_signal2_IAS_rm1_L4_intro}
  	\end{subfigure}%
	%
  	\caption{ 
  	First column: The first two of four piecewise constant signals with a common edge profile and noisy blurred measurements. 
	Second column: Reconstructions of the signals using the existing IAS algorithm to separately recover them (blue triangles) and the proposed MMV-IAS algorithm to jointly recover them (green squares). 
	See \cref{sub:deblurring} for more details. 
  	}
  	\label{fig:deb_signal_intro}
\end{figure}  

% 2) What has been done already? What is the problem with what has already been done? 
\subsection*{Current methodology}

% Deterministic approaches 
Various deterministic methods address the ill-posedness in the set of linear inverse problems \cref{eq:MMV_IP} by transforming it into a set of nearby regularized optimization problems. 
Under the joint sparsity assumption, established compressive sensing methods \cite{donoho2006compressed,eldar2012compressed,foucart2017mathematical} can be used to {\em individually} recover the desired parameter vectors. 
By leveraging their joint sparsity structure, the compressive sensing methods in \cite{cotter2005sparse,eldar2009robust,adcock2019joint}  {\em jointly} recover these vectors.
The approach in \cite{adcock2019joint} significantly enhanced the recovery process's robustness and accuracy. 
For more recent works in this area, see \cite{gelb2019reducing,scarnati2020accurate,xiao2022sequential,xiao2023sequential} and their references.

% UQ 
However, regularized inverse problems often face two significant challenges: (i) determining the appropriate regularization parameters and (ii) quantifying uncertainty in the recovered solution. 
Because the available measurements in \eqref{eq:MMV_IP} are often insufficient and noisy, it is essential to quantify the subsequent uncertainty in the parameters of interest. 
In particular, uncertainty in the parameters leads to uncertainty in predictions and decision-making. 

% Hierarchical Bayesian approach 
In this work we employ a hierarchical Bayesian approach \cite{kaipio2006statistical,calvetti2007introduction,stuart2010inverse} to solve the MMV inverse problem \cref{eq:MMV_IP}, 
with the parameters of interest and the measurements modeled as random variables. 
The sought-after posterior distribution for the parameters of interest is characterized using Bayes' theorem, which connects the posterior density to the prior and likelihood densities.
The prior encodes information available on the parameters of interest before any data are observed, while the likelihood density incorporates the data model and a stochastic description of measurements. 
A primary benefit of this framework is that it enables uncertainty quantification while avoiding the need for fine-tuning regularization parameters. 

% Conditional Gaussian priors
One particularly effective class of priors for promoting sparsity is the conditional Gaussian prior. 
This choice has proven successful in various applications such as sparse basis selection \cite{tipping2001sparse,wipf2004sparse}, signal and image recovery \cite{chantas2006bayesian,babacan2010sparse,glaubitz2022generalized,xiao2022sequential}, and edge detection \cite{churchill2019detecting,xiao2023sequential2}. 
Additionally, conditional Gaussian priors are computationally convenient and lead to highly efficient inference algorithms, 
see \cite{calvetti2019hierachical,calvetti2020sparse,calvetti2020sparsity,vono2022high,glaubitz2022generalized} and references therein. 
Although existing sparsity-promoting hierarchical Bayesian algorithms can be used to infer the parameter vectors separately, such approaches do not exploit the \emph{joint sparsity} of the parameter vectors. 


\subsection*{Our contribution}

% 3) What is done here?
We present a hierarchical Bayesian learning approach that leverages joint sparsity in multiple parameter vectors described by the MMV data model \cref{eq:MMV_IP}. 
Our approach utilizes separate priors for each parameter vector while sharing common hyper-parameters drawn from (generalized) gamma distributions, which capture the sparsity profile of the parameter vectors.
% 4) What are the findings? 
The advantage of using joint-sparsity-promoting priors is demonstrated in comparison to well-established sparsity-promoting algorithms, such as the generalized sparse Bayesian learning (GSBL) \cite{tipping2001sparse,wipf2004sparse,glaubitz2022generalized} and iterative alternating sequential (IAS) \cite{calvetti2019hierachical,calvetti2020sparse,calvetti2020sparsity} algorithms. 
Our results indicate that the proposed MMV-GSBL and MMV-IAS algorithms, which by design use joint-sparsity-promoting priors, outperform the existing methods.
\cref{fig:deb_signal_intro} compares the existing IAS algorithm with the proposed MMV-IAS algorithm to recover the first two out of four piecewise-constant signals with a common edge profile from noisy blurred data. 
Observe that the joint sparsity enhancement in either approach consistently results in superior signal recovery compared to reconstructing signals individually. 
In particular,  while the performance of the IAS and GSBL algorithms may vary depending on the specific problem and model parameters, incorporating joint sparsity consistently offers advantageous outcomes. 
Further numerical experiments demonstrate that the proposed method increases the robustness and accuracy of the recovered parameter vectors, better catches the sparsity profile encoded in the hyper-parameters, and reduces uncertainty. 
% 5) What are the implications and applications? 
The findings of this research highlight the significant improvement in performance that can be achieved by exploiting joint sparsity in hierarchical Bayesian models. 
In particular, we demonstrate its potential application to parallel MRI. 
Additionally, we note that utilizing separate priors with shared hyper-parameters is not limited to conditionally Gaussian priors and that our approach can be adapted to other hierarchical prior models. 
% Note on connection to existing work 
Finally, our approach shares similarities with the one proposed in \cite{wipf2007empirical} for classical SBL that we discuss in \cref{sub:GSBL_connection}. 


% 6) Outline 
\subsection*{Outline} 

We present the joint-sparsity-promoting conditionally Gaussian priors and the resulting hierarchical Bayesian model in \Cref{sec:model}, with the Bayesian MAP estimation discussed in \Cref{sec:BI}. 
In \Cref{sec:analysis}, we analyze the new MMV-IAS algorithm and compare it to the existing IAS algorithm. 
\Cref{sec:GSBL} extends the idea of joint-sparsity-promoting priors to the GSBL framework to form the MMV-GSBL algorithm. 
Numerical experiments are showcased in \Cref{sec:numerics}, which include applications of the proposed MMV-IAS and -GSBL algorithms to parallel MRI. 
We summarize the work in \Cref{sec:summary}.


\subsection*{Notation} 

We use normal and boldface capital letters, such as $X$ and $\mathbf{X}$, to denote scalar- and vector-valued random variables, respectively. 
For a density $\pi$, we write $X \sim \pi$ when $X$ is distributed according to $\pi$. 
If $L \in \N$ and $\mathbf{X}_1,\dots,\mathbf{X}_L$ are random variables, then we denote their collection by $\mathbf{X}_{1:L} = (\mathbf{X}_1,\dots,\mathbf{X}_L)$. 
The same notation applies to dummy variables $\boldsymbol{x} \in \R^n$. 
