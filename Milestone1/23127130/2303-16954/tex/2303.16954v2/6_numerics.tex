 \section{Numerical results} 
\label{sec:numerics} 

We conduct numerical experiments to showcase the effectiveness of our joint-sparsity-promoting MMV-IAS and MMV-GSBL algorithms, detailed in \cref{algo:MMV_IAS,algo:MMV_GSBL}.
For a fair comparison, we also evaluate the individual signal recovery performance using the traditional IAS and GSBL algorithms with the same model parameters. 
The MATLAB code used to generate the numerical tests can be found in the code repository \url{https://github.com/jglaubitz/LeveragingJointSparsity}. 


\subsection{Hyper-prior parameter selection}
\label{sub:param_selection}

For all signal recovery problems, we either chose $(\beta,\vartheta) = ( 1, 1.501, 10^{-2} )$ for the IAS algorithm and $(\beta,\vartheta) =  ( 1, L/2+1.501, 10^{-2} )$ for the MMV-IAS algorithm, resulting in globally convex objective functions, or $(r,\beta,\vartheta) = (-1,1,10^{-4})$ for the IAS and MMV-IAS algorithm, resulting in non-convex objective functions. 
Moreover, we use $(\beta,\vartheta) = (1,10^{3})$ for the GSBL and MMV-GSBL algorithm, resulting in a non-convex objective function. 
Similar parameters were used in \cite{glaubitz2022generalized,xiao2022sequential} and \cite{calvetti2020sparse,calvetti2020sparsity} for the GSBL and IAS algorithm, respectively. 
We did not attempt to optimize any of these parameters.  


\subsection{Signal deblurring}
\label{sub:deblurring}

We first consider (jointly) deblurring four piecewise-constant signals with a shared edge profile. 
The signals are generated by fixing five transition points in the interval $[0,1]$, dividing $[0,1]$ into six constant subintervals on which the signals are constant, and then randomly assigning signal values drawn from a uniform distribution.
The values are then normalized such that the maximum value of each signal is set to $1$. 
\cref{fig:deb_signal1_intro,fig:deb_signal2_intro} in \Cref{sec:introduction} show the first two signals and the given noisy blurred measurements. 
We aim to recover the nodal values $\mathbf{x}_{1:4}$ of all four signals at $N = 40$ equidistant grid points. 
The corresponding data model is 
\begin{equation}\label{eq:deblurring_model}
    \mathbf{y}_l = F \mathbf{x}_l + \mathbf{e}_l, \quad 
    l=1,\dots,4.
\end{equation} 
The discrete forward operator, $F$, represents the application of the midpoint quadrature to the convolution equation 
\begin{equation} 
	y(s) = \int_0^1 k(s-s') x(s) \intd s',
\end{equation}
where we assume a Gaussian convolution kernel of the form 
\begin{equation} 
	k(s) = \frac{1}{2 \pi \gamma^2} \exp\left( - \frac{s^2}{2 \gamma^2} \right)
\end{equation} 
with blurring parameter $\gamma = 3 \cdot 10^{-2}$. 
The forward operator is then given by 
\begin{equation}\label{eq:disc_convolution}
	[F]_{m,n} = h k( h[i-j] ), \quad i,j=1,\dots,n,
\end{equation}
where $h=1/n$ is the distance between consecutive grid points. 
Note that $F$ has full rank but quickly becomes ill-conditioned. 
The noise vectors $\mathbf{e}_{1:4}$ in \cref{eq:deblurring_model} are i.i.d., with zero mean and a common variance $\sigma^2 = 10^{-2}$. 
To reflect our prior knowledge that the signals are piecewise constant, we use  
\begin{equation}\label{eq:deblurring_R}
	R = 
    \begin{bmatrix}
        -1 & 1 & & \\ 
         & \ddots & \ddots & \\ 
         & & -1 & 1 
    \end{bmatrix} 
    \in \R^{(n-1) \times n} 
\end{equation} 
for the sparsifying operator. 
\cref{fig:deb_signal1_IAS_rp1_L4,fig:deb_signal2_IAS_rp1_L4} show the recovered first two signals using the IAS algorithm (to promote sparsity separately) and the proposed MMV-IAS algorithm (to promote sparsity jointly) for $r=1$, resulting in a globally convex objective function. 
\cref{fig:deb_signal1_IAS_rm1_L4,fig:deb_signal2_IAS_rm1_L4} show the same results for $r=-1$, resulting in a non-convex objective function.
\cref{fig:deb_signal1_GSBL_L4,fig:deb_signal2_GSBL_L4} report on the same test using the GSBL and MMV-GSBL algorithms.  
The results demonstrate that incorporating joint sparsity into the IAS and GSBL algorithms improves signal recovery accuracy. 

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_rp1_L4} 
    	\caption{(MMV-)IAS for $r=1$}
    	\label{fig:deb_signal1_IAS_rp1_L4}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_rm1_L4} 
    	\caption{(MMV-)IAS for $r=-1$}
    	\label{fig:deb_signal1_IAS_rm1_L4}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_GSBL_L4} 
    	\caption{(MMV-)GSBL}
    	\label{fig:deb_signal1_GSBL_L4}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_IAS_rp1_L4} 
    	\caption{(MMV-)IAS for $r=1$}
    	\label{fig:deb_signal2_IAS_rp1_L4}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_IAS_rm1_L4} 
    	\caption{(MMV-)IAS for $r=-1$}
    	\label{fig:deb_signal2_IAS_rm1_L4}
  	\end{subfigure}%
	% 
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_GSBL_L4} 
    	\caption{(MMV-)GSBL}
    	\label{fig:deb_signal2_GSBL_L4}
  	\end{subfigure}%
	%
  	\caption{ 
	Different reconstructions of the first (top row) and second (bottom row) of four piecewise constant signals with a common edge profile and noisy blurred measurements using the existing IAS/GSBL algorithm to separately recover them (blue triangles) and the proposed MMV-IAS/-GSBL algorithm to jointly recover them (green squares). 
  	}
  	\label{fig:deb_signal}
\end{figure}

\begin{figure}[tb]
	\centering 
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_rp1_L4_theta} 
    	\caption{(MMV-)IAS for $r=1$}
    	\label{fig:deb_signal1_IAS_rp1_L4_theta}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_rm1_L4_theta} 
    	\caption{(MMV-)IAS for $r=-1$}
    	\label{fig:deb_signal1_IAS_rm1_L4_theta}
  	\end{subfigure}%
  	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_GSBL_L4_theta} 
    	\caption{(MMV-)GSBL}
    	\label{fig:deb_signal1_GSBL_L4_theta}
  	\end{subfigure}% 
	\\
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_IAS_rp1_L4_theta} 
    	\caption{(MMV-)IAS for $r=1$}
    	\label{fig:deb_signal2_IAS_rp1_L4_theta}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_IAS_rm1_L4_theta} 
    	\caption{(MMV-)IAS for $r=-1$}
    	\label{fig:deb_signal2_IAS_rm1_L4_theta}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal2_GSBL_L4_theta} 
    	\caption{(MMV-)GSBL}
    	\label{fig:deb_signal2_GSBL_L4_theta}
  	\end{subfigure}%
  	\caption{ 
	Normalized MAP estimate of the hyper-parameter $\theta$ for the first (top row) and second (bottom row) signal using the IAS and MMV-IAS algorithms with $r=\pm1$ and the GSBL and MMV-GSBL algorithms 
	}
  	\label{fig:deb_signal_theta}
\end{figure}

The improved accuracy of the MMV-IAS and MMV-GSBL algorithms can be attributed to the use of a common hyper-parameter vector $\boldsymbol{\theta}$ that more accurately detects edge locations compared to separate hyper-parameter vectors $\boldsymbol{\theta}_{1:L}$ used in the IAS and GSBL algorithms. 
This is evident in \cref{fig:deb_signal_theta}, which shows the normalized estimated hyper-parameters produced by the IAS/GSBL and MMV-IAS/GSBL algorithms for the first two signals. 
While the MMV-IAS and MMV-GSBL algorithms accurately capture all edge locations, the IAS and GSBL algorithms produce visibly erroneous hyper-parameter profiles. 
The impact of missed true edge locations and false detection of others are clearly visible in \cref{fig:deb_signal1_GSBL_L4}, which shows that the MMV-GSBL approach eliminates the artificial edges around $0.1$ on the horizontal axis. 
Similarly, \cref{fig:deb_signal2_GSBL_L4} demonstrates that the MMV-GSBL approach retains the existing edges around $0.2$ on the horizontal axis, which are missed by the GSBL approach. 

\begin{figure}[tb]
	\centering
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_rp1_L4_CI} 
    	\caption{IAS algorithm for $r=1$}
    	\label{fig:deb_signal1_IAS_rp1_L4_CI}
  	\end{subfigure}% 
  	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_IAS_rm1_L4_CI} 
    	\caption{IAS algorithm for $r=-1$}
    	\label{fig:deb_signal1_IAS_rm1_L4_CI}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_GSBL_L4_CI} 
    	\caption{GSBL algorithm}
    	\label{fig:deb_signal1_GSBL_L4_CI}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_MMVIAS_rp1_L4_CI} 
    	\caption{MMV-IAS algorithm for $r=1$}
    	\label{fig:deb_signal1_MMVIAS_rp1_L4_CI}
  	\end{subfigure}% 
  	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_MMVIAS_rm1_L4_CI} 
    	\caption{MMV-IAS algorithm for $r=-1$}
    	\label{fig:deb_signal1_MMVIAS_rm1_L4_CI}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/deb_signal1_MMVGSBL_L4_CI} 
    	\caption{MMV-GSBL algorithm}
    	\label{fig:deb_signal1_MMVGSBL_L4_CI}
  	\end{subfigure}%
  	\caption{ 
	The $99.9\%$ credible intervals (CIs) for the recovered first (top row) and second (bottom row) signal conditioned on the MAP estimate of the hyper-parameter vector $\boldsymbol{\theta}^{\rm MAP}$ using the IAS and MMV-IAS algorithm with $r=\pm1$ as well as the GSB and MMV-GSBL algorithm
  	}
  	\label{fig:deb_signal_CI}
\end{figure}

The proposed MMV-IAS and MMV-GSBL algorithms have the additional advantage of quantifying uncertainty in the recovered signals, as described in \cref{sub:UQ_IAS} and \cref{rem:GSBL_UQ}. 
This is demonstrated in \cref{fig:deb_signal_CI}, which shows the $99.9\%$ credible intervals of the fully conditional posterior densities $\pi_{\mathbf{X}_1|\boldsymbol{\Theta}=\boldsymbol{\theta},\mathbf{Y}_{1:L}=\mathbf{y}_{1:L}}$ of the first recovered signal for the IAS, MMV-IAS, GSBL, and MMV-GSBL model.
Here, $\mathbf{y}_{1:L}$ are the given noisy blurred MMVs and $\boldsymbol{\theta}$ is the estimated hyper-parameter vector.


\subsection{Error and success analysis}
\label{sub:analysis}

We now conduct a synthetic sparse signal recovery experiment to further assess the performance of the proposed joint-sparsity-promoting MMV-IAS and MMV-GSBL algorithms. 
We consider $L$ randomly generated signals, $\mathbf{x}_{1:L}$, each of size $N$. 
We fix the number of measurements, $M$, non-zero components, $s$, and trials, $T$. 
For each trial, $t=1,\dots,T$, we proceed as follows:
\begin{enumerate}
	\item[(i)] 
	Generate a support set $S \subset \{ 1,\dots,N \}$ uniformly at random with size $|S| = s$;
	
	\item[(ii)] 
	Define signal vectors $\mathbf{x}_1,\dots,\mathbf{x}_L$ such that $\mathrm{supp}(\mathbf{x}_1) = \dots = \mathrm{supp}(\mathbf{x}_L) = S$, where the non-zero entries are drawn from the standard normal distribution; 
	
	\item[(iii)] 
	Generate a forward operator $F$ as described below, fix a noise variance $\sigma^2$, and compute the measurement vectors $\mathbf{y}_l = F \mathbf{x}_l + \mathbf{e}_l$, where $\mathbf{e}_l$ is drawn from $\mathcal{N}(\mathbf{0},\sigma^2 I)$;
	
	\item[(iv)]
	Compute the reconstructions $\hat{\mathbf{x}}_1,\dots,\hat{\mathbf{x}}_L$ using the desired algorithm (e.g., IAS or MMV-IAS); 
	 
	\item[(v)] 
	Compute the normalized error $E_t = \sqrt{ \sum_{l=1}^L \| \mathbf{x}_l - \hat{\mathbf{x}}_l \|_2^2 / \sum_{l=1}^L \| \mathbf{x}_l \|_2^2 }$ for each algorithm.
	
\end{enumerate}
Finally, we evaluate the algorithm performance using the average error and empirical success probability (ESP). 
The \emph{average error} is $E = (E_1+\dots+E_T)/T$, i.e., the average of the individual trial errors. 
The \emph{ESP} is the fraction of trials that successfully recovered the vectors $\mathbf{x}_1,\dots,\mathbf{x}_L$ up to a given tolerance $\varepsilon_{\rm tol}$, i.e., $E_t < \varepsilon_{\rm tol}$. 
We use a subsampled discrete cosine transform (DFT) as the forward operator $F$. 
This mimics the situation in which Fourier data are collected (e.g., synthetic aperture radar and magnetic resonance imaging), but some of the data are determined to be unusable due to a system malfunction or obstruction.
Specifically, we generate a set $\Omega \subset \{1,\dots,N\}$ of size $M$ uniformly at random and let 
\begin{equation} 
	F = P_{\Omega} A, 
\end{equation} 
where $A \in \R^{N \times N}$ is the DCT matrix and $P_{\Omega} \in \R^{M \times N}$ is the operator selecting rows of $A$ corresponding to the indices in $\Omega$. 
The identity matrix is used as the sparsifying operator as the signals are assumed to be sparse. 
In our experiments we set $N=100$, {$s=20$}, $T=10$, $\sigma^2=10^{-6}$, and $\varepsilon_{\rm tol}=10^{-2}$. 

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_IAS_C4} 
    	\caption{Average errors, $L=4$}
    	\label{fig:comparision_error_IAS_C4}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_IAS_C8} 
    	\caption{Average errors, $L=8$}
    	\label{fig:comparision_error_IAS_C8}
  	\end{subfigure}%
  	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_IAS_C16} 
    	\caption{Average errors, $L=16$}
    	\label{fig:comparision_error_IAS_C16}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_IAS_C4} 
    	\caption{Success probability, $L=4$}
    	\label{fig:comparision_succ_IAS_C4}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_IAS_C8} 
    	\caption{Success probability, $L=8$}
    	\label{fig:comparision_succ_IAS_C8}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_IAS_C16} 
    	\caption{Success probability, $L=16$}
    	\label{fig:comparision_succ_IAS_C16}
  	\end{subfigure}% 
  	\caption{ 
	Comparison of the average error and success probability for the sparsity-promoting IAS algorithm (blue triangles) and joint-sparsity-promoting MMV-IAS algorithm (green squares), both for $r=-1$. 
	We recover a signal of size $N=100$ with $s=20$ non-zero entries from an increasing number of measurements $m$. 
  	}
  	\label{fig:comparision_IAS}
\end{figure}

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_GSBL_C4} 
    	\caption{Average errors, $L=4$}
    	\label{fig:comparision_error_GSBL_C4}
  	\end{subfigure}%
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_GSBL_C8} 
    	\caption{Average errors, $L=8$}
    	\label{fig:comparision_error_GSBL_C8}
  	\end{subfigure}%
  	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_error_GSBL_C16} 
    	\caption{Average errors, $L=16$}
    	\label{fig:comparision_error_GSBL_C16}
  	\end{subfigure}%
	\\
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_GSBL_C4} 
    	\caption{Success probability, $L=4$}
    	\label{fig:comparision_succ_GSBL_C4}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_GSBL_C8} 
    	\caption{Success probability, $L=8$}
    	\label{fig:comparision_succ_GSBL_C8}
  	\end{subfigure}% 
  	%
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comparision_succ_GSBL_C16} 
    	\caption{Success probability, $L=16$}
    	\label{fig:comparision_succ_GSBL_C16}
  	\end{subfigure}% 
  	\caption{ 
	Comparison of the average error and success probability for the sparsity-promoting GSBL algorithm (blue triangles) and joint-sparsity-promoting MMV-GSBL algorithm (green squares). 
	We recover a signal of size $N=100$ with $s=20$ non-zero entries from an increasing number of measurements $m$. 
	}
  	\label{fig:comparision_GSBL}
\end{figure}  

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/phase_IAS_rm1_L4} 
    	\caption{IAS algorithm (separately recovered)}
    	\label{fig:phase_IAS_rm1_L4}
  	\end{subfigure}
  	%
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/phase_MMVIAS_rm1_L4} 
    	\caption{MMV-IAS algorithm, $L=4$}
    	\label{fig:phase_MMVIAS_rm1_L4}
  	\end{subfigure}
	\\
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/phase_MMVIAS_rm1_L8} 
    	\caption{MMV-IAS algorithm, $L=8$}
    	\label{fig:phase_MMVIAS_L8}
  	\end{subfigure}
  	%
  	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/phase_MMVIAS_rm1_L16} 
    	\caption{MMV-IAS algorithm, $L=16$}
    	\label{fig:phase_MMVIAS_L16}
  	\end{subfigure}
	%
  	\caption{ 
	Phase transition diagrams for the IAS and MMV-IAS algorithm for $r=-1$ and $L = 4,8,16$. 
	The diagrams show the success probability for values $1 \leq s \leq N$ and $1 \leq M \leq N$. 
  	}
  	\label{fig:phase}
\end{figure} 

The performance of the proposed joint-sparsity-promoting MMV-IAS and MMV-GSBL algorithms is evaluated and compared to the existing IAS and GSBL algorithms in \cref{fig:comparision_IAS,fig:comparision_GSBL}. 
These figures report the average errors and ESP for different numbers of MMVs ($L = 4, 8, 16$). 
For brevity, we only report on the IAS and MMV-IAS results for $r=-1$. 
The results show that the proposed algorithms outperform the existing ones regarding average errors and ESP in most cases. 
As the number of MMVs increases, the superiority of the proposed algorithms becomes more pronounced. 
For instance, when $L=16$, the MMV-IAS algorithm requires only around $40$ measurements per signal for successful recovery, while the IAS algorithm requires around $70$. 
The phase transition plots in \cref{fig:phase} further demonstrate the improved performance of the MMV-IAS algorithm, which exhibits a phase transition close to the optimal $m = s$ line. 
The phase transition profiles for the MMV-GSBL algorithm are similar to the MMV-IAS algorithm but are not included here for brevity.


\subsection{Application to parallel magnetic resonance imaging} 
\label{sub:parallel_MRI} 

We next apply the proposed MMV-IAS and MMV-GSBL algorithms to a parallel MRI test problem. 
Parallel MRI is a multi-sensor acquisition system that uses multiple coils to simultaneously acquire image measurements for recovery. 
Details on parallel MRI can be found in \cite{guerquin2011realistic,chun2015efficient,chun2017compressed,adcock2019joint}. 
A standard discrete data model for parallel MRI is the following: 
Let $\mathbf{x} \in \C^N$ be the vectorized image to be recovered and $L$ be the number of coils. 
For the $l$th coil, the measurements acquired are 
\begin{equation}\label{eq:pMRI_model}
	\mathbf{y}_l = P_{\Omega_l} F \mathbf{x} + \mathbf{e}_l, 
\end{equation} 
where $F \in \C^{N \times N}$ is the discrete Fourier transform (DFT) matrix, $P_{\Omega_l} \in \C^{M \times N}$ is a sampling operator that selects the rows of $F$ corresponding to the frequencies in $\Omega_l$, and $\mathbf{e}_l \in \C^M$ is noise. 
The image measurement acquired by each of the $L$ coils is intrinsic to the particular coil. 
A typical sampling procedure for parallel MRI is to use data taken as radial line sampling in the Fourier space. 
\cref{fig:pMRI_sampling} illustrates the radial sampling maps corresponding to four different coils. 

\begin{figure}[tb]
	\centering
  	\includegraphics[width=\textwidth]{figures/sampling_maps} 
  	\caption{Four different radial sampling maps}
  	\label{fig:pMRI_sampling}
\end{figure} 

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_ref} 
		\caption{Reference image}
    		\label{fig:pMRI_ref} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_IAS} 
		\caption{IAS reconstruction}
		\label{fig:pMRI_coil1_IAS} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_GSBL} 
		\caption{GSBL reconstruction}
		\label{fig:pMRI_coil1_GSBL}
  	\end{subfigure}
	\\ 
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_LS} 
		\caption{Least squares reconstruction}
		\label{fig:pMRI_coil1_LS} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_MMVIAS} 
		\caption{MMV-IAS reconstruction}
		\label{fig:pMRI_coil1_MMVIAS} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{figures/pMRI_coil1_MMVGSBL} 
		\caption{MMV-GSBL reconstruction}
		\label{fig:pMRI_coil1_MMVGSBL} 
  	\end{subfigure}
  	%
	\caption{ 
	Reference image and the reconstructed first coil images
  	}
  	\label{fig:pMRI_coil_images}
\end{figure} 

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_ref} 
		\caption{Reference image}
		\label{fig:pMRI_overall_ref} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_IAS} 
		\caption{IAS reconstruction}
		\label{fig:pMRI_overall_IAS}
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_GSBL} 
		\caption{GSBL reconstruction}
		\label{fig:pMRI_overall_GSBL}
  	\end{subfigure}
	\\ 
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_LS} 
		\caption{Least squares reconstruction}
		\label{fig:pMRI_overall_LS}
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_MMVIAS} 
		\caption{MMV-IAS reconstruction} 
		\label{fig:pMRI_overall_MMVIAS} 
  	\end{subfigure}
	%
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/pMRI_overall_MMVGSBL} 
		\caption{MMV-GSBL reconstruction} 
		\label{fig:pMRI_overall_MMVGSBL} 
  	\end{subfigure}
	%
  	\caption{ 
	Reference image and the reconstructed overall images. 
	For all methods, $20$ lines/angles (corresponding to around $16\%$ sampling of the k-space), $\sigma^2 = 10^{-3}$, and $L=4$ coils were used.
  	}
  	\label{fig:pMRI_overall_images}
\end{figure} 

\begin{figure}[tb]
	\centering
	\begin{subfigure}[b]{0.495\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/error_overall_IAS} 
		\caption{Relative overall error, IAS \& MMV-IAS} 
		\label{fig:error_overall_IAS}
  	\end{subfigure}
  	%
	\begin{subfigure}[b]{0.495\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/error_overall_GSBL} 
		\caption{Relative overall error, GSBL \& MMV-GSBL} 
		\label{fig:error_overall_GSBL}
  	\end{subfigure}
	%
  	\caption{ 
	Relative error of the recovered overall image using the least squares (LS) approach, the existing IAS/GSBL algorithm, and the proposed MMV-IAS/GSBL method.
	In all cases, we used $L=4$ coils, noise variance $\sigma^2 = 10^{-3}$, and a varying number of lines. 
  	}
  	\label{fig:pMRI_errors}
\end{figure} 

Many techniques have been proposed for parallel MRI, see \cite{chun2015efficient} and references therein. 
Here we focus on the coil-by-coil approach, first computing the approximate coil images $\mathbf{\hat{x}}_{1:L}$ from \cref{eq:pMRI_model} and then computing an approximation $\mathbf{\hat{x}}$ to the overall image by considering the average of the coil images, i.e., $\mathbf{\hat{x}} = (\mathbf{\hat{x}}_{1} + \dots + \mathbf{\hat{x}}_{L})/L$. 
In our experiment, we compare the recovery of the $256 \times 256$ Shepp--Logan phantom image in \cref{fig:pMRI_ref} using the least-squares (LS) approach, the existing IAS/GSBL algorithm, and the proposed MMV-GSBL/IAS algorithm to recover the coil images. 
For brevity, we only report on the IAS and MMV-IAS results for $r=-1$.
We used the anisotropic first-order discrete gradient operator as the sparsifying operator. 
\cref{fig:pMRI_coil_images} shows the recovered first coil images for $20$ lines, noise variance $\sigma^2 = 10^{-3}$, and $L=4$ coils.
The recovered first coil images using the proposed MMV-IAS (\cref{fig:pMRI_coil1_MMVIAS}) and MMV-GSBL (\cref{fig:pMRI_coil1_MMVGSBL}) algorithms are visibly more accurate than using the corresponding IAS (\cref{fig:pMRI_coil1_IAS}) and GSBL (\cref{fig:pMRI_coil1_GSBL}) algorithms.  
Note the sharper transitions between internal structures. 
Consequently, the proposed MMV-IAS/GSBL algorithm also yields a more accurate approximation to the overall image, compared to the existing IAS/GSBL algorithm, which is demonstrated in \cref{fig:pMRI_overall_images}.
To further assess the performance of the proposed joint-sparsity-promoting MMV-IAS and MMV-GSBL algorithms, \cref{fig:pMRI_errors} reports the relative error of the recovered overall image for varying numbers of lines sampled in the Fourier space. 
The proposed MMV-IAS/GSBL algorithm to jointly recover the coil images consistently yields the smallest error. 


\subsection{Comparison with a sequential approach} 
\label{sub:num_seq}

Comparing the proposed MMV-IAS/GSBL algorithm solely with the traditional IAS/GSBL algorithm for separate recovery of parameter vectors may not be entirely equitable.  
The MMV-IAS/GSBL algorithm leverages information from all parameter vectors to reconstruct each individually. 
In contrast, the traditional IAS/GSBL does not facilitate information sharing across different parameter vectors. 
Consequently, we also compare the MMV-IAS/GSBL algorithm with a sequential variant of the IAS/GSBL algorithm. 
In the sequential IAS/GSBL algorithm, we determine the $l$th parameter vector $\mathbf{x}_l$ and its corresponding hyper-parameter vector $\boldsymbol{\theta}_l$ by approximating the MAP estimate of the $l$th posterior $\pi_{ \mathbf{X}_l, \boldsymbol{\Theta}_l }( \mathbf{x}_l, \boldsymbol{\theta}_l ) \propto \pi_{ \mathbf{Y}_{l} | \mathbf{X}_{l} }( \mathbf{y}_{l} | \mathbf{x}_{l} ) \, \pi_{\mathbf{X}_l | \boldsymbol{\Theta}_l}( \mathbf{x}_l | \boldsymbol{\theta}_l ) \, \pi_{ \boldsymbol{\Theta}_l }( \boldsymbol{\theta}_l )$---as it is done in the IAS/GSBL algorithm. 
However, unlike the traditional IAS/GSBL algorithm, the initial value for $\boldsymbol{\theta}_l$ in the corresponding block-coordinate descent method is chosen as the MAP estimate $\boldsymbol{\theta}_{l-1}^{\rm MAP}$ derived from the previously learned parameter vector. 
This approach is reminiscent of strategies employed in time-dependent problems where data are received in sequential batches. 
For the first parameter vector, the IAS/GSBL and the sequential IAS/GSBL algorithms start with the same initialization for $\boldsymbol{\theta}_1$. 
For this reason, we do not report on the first parameter vector in the subsequent numerical tests. 
However, from the second parameter vector onward, their initializations diverge. 
The sequential approach ensures that the insights obtained from the previous measurement vector $\mathbf{y}_l$ and the learned $\mathbf{x}_l,\boldsymbol{\theta}_l$ are not disregarded, and as such facilitate a reasonable comparison for the proposed MMV-IAS/GSBL algorithm.

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal2_IAS_rp1} 
    	\caption{Reconstructions, $2$nd signal}
    	\label{fig:comp_signal2_IAS_rp1}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal3_IAS_rp1} 
    	\caption{Reconstructions, $3$rd signal}
    	\label{fig:comp_signal3_IAS_rp1}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal4_IAS_rp1} 
    	\caption{Reconstructions, $4$th signal}
    	\label{fig:comp_signal4_IAS_rp1}
  	\end{subfigure}%
	\\
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal2_IAS_rp1_theta} 
    	\caption{Hyper-parameters, $2$nd signal}
    	\label{fig:comp_signal2_IAS_rp1_theta}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal3_IAS_rp1_theta} 
    	\caption{Hyper-parameters, $3$rd signal}
    	\label{fig:comp_signal3_IAS_rp1_theta}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal4_IAS_rp1_theta} 
    	\caption{Hyper-parameters, $4$th signal}
    	\label{fig:comp_signal4_IAS_rp1_theta}
  	\end{subfigure}%
	%
  	\caption{ 
	Reconstructions (top row) and normalized hyper-parameter estimates $\theta$ (bottom row) for the last three of four piecewise constant signals with a common edge profile. 
	We compare the IAS algorithm (blue triangles), the sequential IAS algorithm (red stars), and the MMV-IAS algorithm (green squares). 
	All methods use a generalized gamma hyper-prior with $r=1$, ensuring globally convex objective functions. 
  	}
  	\label{fig:comp_IAS_rp1}
\end{figure}

\cref{fig:comp_IAS_rp1} compares the reconstructions and normalized hyper-parameter estimates for the last three of four piecewise constant signals with a common edge profile for the IAS, sequential IAS, and MMV-IAS algorithms. 
All methods use a generalized gamma hyper-prior with $r=1$, ensuring globally convex objective functions. 
The results reveal only minor differences between the IAS and sequential IAS algorithms in this particular scenario. 
The primary reason for this similarity is the global convexity of the objective function common to both algorithms. 
Despite their differing hyper-parameter initialization, they are both theoretically guaranteed to converge to the same unique minimum as the iteration count approaches infinity. 
The slight discrepancies observed between the IAS and sequential IAS algorithms, such as those noted in \cref{fig:comp_signal3_IAS_rp1}, can be attributed to the early termination of the block-coordinate descent method. 
In comparison, when juxtaposed with the IAS and sequential IAS algorithms, the MMV-IAS algorithm demonstrates improved performance, particularly in terms of more precise edge detection and overall signal recovery.

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal2_IAS_rm1} 
    	\caption{Reconstructions, $2$nd signal}
    	\label{fig:comp_signal2_IAS_rm1}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal3_IAS_rm1} 
    	\caption{Reconstructions, $3$rd signal}
    	\label{fig:comp_signal3_IAS_rm1}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal4_IAS_rm1} 
    	\caption{Reconstructions, $4$th signal}
    	\label{fig:comp_signal4_IAS_rm1}
  	\end{subfigure}%
	\\
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal2_IAS_rm1_theta} 
    	\caption{Hyper-parameters, $2$nd signal}
    	\label{fig:comp_signal2_IAS_rm1_theta}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal3_IAS_rm1_theta} 
    	\caption{Hyper-parameters, $3$rd signal}
    	\label{fig:comp_signal3_IAS_rm1_theta}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal4_IAS_rm1_theta} 
    	\caption{Hyper-parameters, $4$th signal}
    	\label{fig:comp_signal4_IAS_rm1_theta}
  	\end{subfigure}%
	%
  	\caption{ 
	Reconstructions (top row) and normalized hyper-parameter estimates $\theta$ (bottom row) for the last three of four piecewise constant signals with a common edge profile. 
	We compare the IAS algorithm (blue triangles), the sequential IAS algorithm (red stars), and the MMV-IAS algorithm (green squares). 
	All methods use a generalized gamma hyper-prior with $r = -1$, leading to non-convex objective functions.
  	}
  	\label{fig:comp_IAS_rm1}
\end{figure}

\cref{fig:comp_IAS_rm1} offers a comparison akin to the previous one, but under a generalized gamma hyper-prior with $r=-1$, leading to non-convex objective functions. 
In this scenario, there are significant differences between the IAS and sequential IAS algorithms, with the latter appearing to underperform. 
For example, as shown in \cref{fig:comp_signal2_IAS_rm1}, the sequential IAS algorithm fails to detect the final edge at $0.85$, whereas the traditional IAS algorithm successfully identifies it. Similar trends are evident in \cref{fig:comp_signal3_IAS_rm1,fig:comp_signal4_IAS_rm1}. 
These results suggest that initializing the hyper-parameter vector $\boldsymbol{\theta}_l$ in the block-coordinate descent method with the previous MAP estimate $\boldsymbol{\theta}_{l-1}^{\rm MAP}$ might steer the algorithm towards a less favorable local minimum compared to a fresh initialization. 
Altering the recovery sequence of the parameter vectors could improve the sequential IAS algorithm's performance. 
Nonetheless, unless guided by the problem's context, identifying an optimal order poses a challenging and non-trivial task. 
In contrast, when compared with the IAS and sequential IAS algorithms, the MMV-IAS algorithm consistently demonstrates superior performance, especially in terms of more precise edge detection and overall signal recovery.

\begin{figure}[tb]
	\centering
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal2_GSBL} 
    	\caption{Reconstructions, $2$nd signal}
    	\label{fig:comp_signal2_GSBL}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal3_GSBL} 
    	\caption{Reconstructions, $3$rd signal}
    	\label{fig:comp_signal3_GSBL}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal4_GSBL} 
    	\caption{Reconstructions, $4$th signal}
    	\label{fig:comp_signal4_GSBL}
  	\end{subfigure}%
	\\
  	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal2_GSBL_theta} 
    	\caption{Hyper-parameters, $2$nd signal}
    	\label{fig:comp_signal2_GSBL_theta}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal3_GSBL_theta} 
    	\caption{Hyper-parameters, $3$rd signal}
    	\label{fig:comp_signal3_GSBL_theta}
  	\end{subfigure}%
	%
	\begin{subfigure}[b]{0.33\textwidth}
		\includegraphics[width=\textwidth]{%
      		figures/comp_signal4_GSBL_theta} 
    	\caption{Hyper-parameters, $4$th signal}
    	\label{fig:comp_signal4_GSBL_theta}
  	\end{subfigure}%
	%
  	\caption{ 
	Reconstructions (top row) and normalized hyper-parameter estimates $\theta$ (bottom row) for the last three of four piecewise constant signals with a common edge profile. 
	We compare the GSBL algorithm (blue triangles), the sequential GSBL algorithm (red stars), and the MMV-GSBL algorithm (green squares). 
  	}
  	\label{fig:comp_GSBL}
\end{figure}

In the concluding analysis, \cref{fig:comp_GSBL} provides a comparison analogous to the earlier ones, but this time for the GSBL, sequential GSBL, and MMV-GSBL algorithms, all of which lead to non-convex objective functions. 
In this case, moderate differences are observed between the GSBL and sequential GSBL algorithms. 
As illustrated in \cref{fig:comp_signal2_GSBL,fig:comp_signal4_GSBL}, the sequential GSBL algorithm produces improved outcomes compared to the GSBL algorithm, while in \cref{fig:comp_signal3_GSBL}, the GSBL algorithm demonstrates superior performance over its sequential counterpart. 
These findings suggest that initializing the hyper-parameter vector $\boldsymbol{\theta}_l$ in the block-coordinate descent method with the prior MAP estimate $\boldsymbol{\theta}_{l-1}^{\rm MAP}$ can variably influence the algorithm, leading it towards either a more or less favorable local minimum compared to a fresh initialization. 
In contrast, the MMV-GSBL algorithm exhibits superior overall performance.