\section{Analysis: Complexity, convexity, and convergence} 
\label{sec:analysis} 

We briefly analyze the computational complexity, convexity, and convergence of the MMV-IAS algorithm (\cref{algo:MMV_IAS}) and the underlying objective function.


\subsection{Computational complexity}
\label{sub:complexity}

Consider $L$ parameter vectors $\mathbf{x}_1,\dots,\mathbf{x}_L \in \R^N$. 
Different methods can be used to solve the $\mathbf{x}_l$-updates in \cref{algo:MMV_IAS}. 
Assuming that we use the PCG method, each $\mathbf{x}_l$-update has computational complexity $\mathcal{O}(\tilde{N}_l)$, where $\tilde{N}_l$ is the number of non-zero elements of the matrix $F_l^T F_l + R^T D_{\boldsymbol{\theta}}^{-1/2} R$. 
In the worst case ($\tilde{N}_l = N^2$ for all $l=1,\dots,L$), the computational cost for updating all parameter vectors is $\mathcal{O}(L N^2)$. 
As already noted, however, these updates can be performed in parallel. 
Furthermore, we perform the $\boldsymbol{\theta}$-update in \cref{algo:MMV_IAS} using one of the explicit formulas \cref{eq:update_beta4}. 
If $R \in \R^{K \times N}$ for $l=1,\dots,L$, then the $\boldsymbol{\theta}$-update has computational complexity $\mathcal{O}(K N)$. 
See \cite[Section 4.1]{glaubitz2022generalized} for more details.
In sum, if we run \cref{algo:MMV_IAS} for $I$ iterations, then its overall order of operations is (at most) $\mathcal{O}(I (L N^2 + K N) )$. 


\subsection{Convexity of the objective function}
\label{sub:convexity} 

We next investigate the convexity of the objective function $\mathcal{G}$ in \cref{eq:G}.  
\cref{thm:convexity} provides the choices of hyper-parameters $(r, \beta, \vartheta_{1:K})$ for which the objective function $\mathcal{G}$ is globally or locally convex, that is, when the convexity is restricted to specific values for $\boldsymbol{\theta}$. It also describes how the number of MMVs influences convexity.

\begin{theorem}[Convexity of the objective function]\label{thm:convexity}
	Let $\mathcal{G}$ be the objective function in \cref{eq:G} and $\eta = r \beta - (L/2 + 1)$. 
	\begin{enumerate}
		\item[(a)] 
		If $r \geq 1$ and $\eta > 0$, then $\mathcal{G}$ is globally convex. 
		
		\item[(b)]
		If $0<r<1$ and $\eta > 0$, or if $r < 0$, then $\mathcal{G}$ is convex provided that 
		\begin{equation}\label{eq:convexity_cond}
			\theta_k < \vartheta_k \left( \frac{\eta}{r |r-1|} \right)^{1/r}, 
				\quad k=1,\dots,K. 
		\end{equation}
		
	\end{enumerate}
\end{theorem}

\cref{thm:convexity} highlights the impact of MMV data on the convexity of the objective function $\mathcal{G}$ in our joint-sparsity-promoting hierarchical Bayesian model. 
In particular, as $L$ increases, the linear decrease of $\eta$ results in the condition $\eta > 0$ becoming more restrictive. 
Furthermore, since  $\eta$ decreases as $L$ increases, we see in part (b) that the right-hand side of \cref{eq:convexity_cond} is also smaller.
That is, the convex set in which $G$ is convex shrinks as $L$ increases, revealing a trade-off between promoting joint sparsity and decreasing convexity as the number of coupled MMV data and parameter vectors increases.
\cref{thm:convexity} is a natural extension of \cite[Theorem 4.1]{calvetti2020sparse} (also see \cite[Theorem 3.1]{calvetti2020sparsity}), which we recover as the special case of $L=1$ (and $R = I$). 
The proof is provided in \cref{app:convexity_proof}.

\begin{remark} [Convergence of MMV-IAS]\label{rem:convergence} 
	The findings in \cref{thm:convexity} impact the IAS algorithm's performance. 
	When the objective function $\mathcal{G}$ exhibits global convexity, the MMV-IAS algorithm is guaranteed to converge to the unique minimum of $\mathcal{G}$. 
	This follows from the standard theory for coordinate descent approaches \cite{wright2015coordinate,beck2017first}. 
	Although global convexity streamlines the computation of the MAP estimate, there is a strong justification for investigating alternative choices of $r$ that yield hierarchical priors with enhanced sparsity-promoting properties. 
	Empirical studies suggest that when the objective function is not globally convexity, the resulting sparsity of the minimizer is often increased. 
	Nevertheless, a non-convex $\mathcal{G}$ can lead to the emergence of misleading local minima, potentially causing the MMV-IAS algorithm to become entrapped in one. 
	To overcome the risk of the algorithm prematurely converging to an incorrect local minimum, \cite{calvetti2020sparsity} recommends the adoption of hybrid versions of the IAS algorithm in the single-measurement-vector case. 
	In such constructions, the global convergence traits of gamma hyper-priors ($r=1$) are leveraged initially to close in on the vicinity of the unique global minimum, after which there is a shift to a generalized gamma hyper-prior with $r < 1$ to provide a stronger sparsity stimulus.
\end{remark}