\section{Extension to generalized sparse Bayesian learning} 
\label{sec:GSBL}

We now briefly demonstrate how our method for fostering joint sparsity can be incorporated into the GSBL framework \cite{tipping2001sparse,wipf2004sparse,glaubitz2022generalized}. 
Sparse Bayesian learning (SBL), first introduced in \cite{tipping2001sparse}, is a statistical approach that employs Bayesian inference to recover sparse solutions from indirect, incomplete, and noisy data. 
This technique is characterized by combining a conditional zero-mean Gaussian prior with a gamma hyper-prior for the precision of the Gaussian prior. 
Traditional SBL methods have predominantly operated under the sparsity assumption in the parameter vector $\mathbf{x}$. 
However, the recent work \cite{glaubitz2022generalized} broadened this framework by proposing that sparsity can also apply to some linear transformation of the parameter vector, denoted as $R \mathbf{x}$. 
Here, $R$ is permitted to possess a non-trivial kernel, provided the common kernel condition $\ker(F) \cap \ker(R) = {\mathbf{0}}$ holds. 
This extension led to the development of the GSBL approach. 
Within this context, we now further evolve the GSBL method to encourage \emph{joint} sparsity in the case of MMVs corresponding to jointly sparse parameter vectors. 

As highlighted in the introduction, both IAS and GSBL are established cases of sparsity-promoting algorithms that can benefit from our joint sparsity-promoting priors in the presence of MMV data. 
Importantly, these priors are not restricted to the discussed algorithms, as they can also be employed to enhance the performance of other sparsity-promoting MAP estimators, demonstrating their versatile applicability.


\subsection{The hierarchical Bayesian model}
\label{sub:GSBL_model}

The main difference between the hierarchical model discussed in \Cref{sec:model} and the one underlying GSBL is that the latter treats the diagonal entries of the precision (inverse covariance) matrix $D_{\boldsymbol{\theta}}$ as gamma distributed random variables. 
In this case, the joint prior is 
\begin{equation}\label{eq:joint_prior_GSBL} 
	\pi_{\mathbf{X}_{1:L} | \boldsymbol{\Theta}}( \mathbf{x}_{1:L} | \boldsymbol{\theta} ) 
		\propto \det( D_{\boldsymbol{\theta}} )^{L/2} \exp\left( -\frac{1}{2} \sum_{l=1}^L \| D_{\boldsymbol{\theta}}^{1/2} R \mathbf{x}_l \|_2^2 \right) 
\end{equation}
rather than \cref{eq:joint_prior} and the gamma hyper-prior is 
\begin{equation}\label{eq:hyper_priors_GSBL} 
	\pi_{\boldsymbol{\Theta}}(\boldsymbol{\theta}) 
		= \prod_{k=1}^K \mathcal{GG}( \theta_k | 1, \beta, \vartheta_k ) 
		\propto \det(D_{\boldsymbol{\theta}})^{\beta - 1} \exp\left( - \sum_{k=1}^K \theta_k/\vartheta_k \right) 
\end{equation}
rather than \cref{eq:hyper_priors}.
We still assume the joint likelihood function \cref{eq:joint_likelihood}. 


\subsection{Bayesian inference}
\label{sub:GSBL_inference}

We perform Bayesian inference for the GSBL model by again solving for the MAP estimate of its posterior. 
To this end, a block-coordinate descent approach similar to the IAS algorithm was recently investigated in \cite{glaubitz2022generalized} (also see \cite{xiao2023sequential}).
For the GSBL model above, the objective function $\mathcal{G}$ that is minimized by the MAP estimate is 
\begin{equation}\label{eq:G_GSBL}
\begin{aligned} 
	\mathcal{G}( \mathbf{x}_{1:L}, \boldsymbol{\theta} ) 
		= \frac{1}{2} \left( \sum_{l=1}^L \| F_l \mathbf{x}_l - \mathbf{y}_l \|_2^2 
			+ \| D_{\boldsymbol{\theta}}^{1/2} R \mathbf{x}_l  \|_2^2 \right) 
			+ \sum_{k=1}^K \frac{\theta_k}{\vartheta_k} 
			+ ( -L/2 + 1 - \beta ) \sum_{k=1}^K \log( \theta_k ) 
\end{aligned}	
\end{equation}
up to constants that neither depend on $\mathbf{x}_{1:L}$ nor $\boldsymbol{\theta}$. 
We again minimize $\mathcal{G}$ by alternatingly (i) minimizing $\mathcal{G}$ w.r.t.\ $\mathbf{x}_{1:L}$ for fixed $\boldsymbol{\theta}$ and (ii) minimizing $\mathcal{G}$ w.r.t.\ $\boldsymbol{\theta}$ for fixed $\mathbf{x}_{1:L}$.  
In the case of GSBL, updating $\mathbf{x}_{1:L}$ given $\boldsymbol{\theta}$ reduces to solving the quadratic optimization problems 
\begin{equation}\label{eq:x_update_GSBL} 
	\mathbf{x}_l 
		= \argmin_{\mathbf{x}} \left\{ \| F_l \mathbf{x} - \mathbf{y}_l \|_2^2 + \| D_{\boldsymbol{\theta}}^{1/2} R \mathbf{x} \|_2^2 \right\}, \quad 
		l=1,\dots,L.
\end{equation}
Moreover, using the same arguments as in \cref{sub:beta_update}, the minimizer of $\mathcal{G}$ w.r.t.\ $\boldsymbol{\theta}$ for fixed $\mathbf{x}_{1:L}$ is 
\begin{equation}\label{eq:update_theta_GSBL} 
	\theta_k = \frac{ L/2 - 1 + \beta }{ \sum_{l=1}^L [R \mathbf{x}_l]_k^2/2 + \vartheta_k^{-1} }, \quad k=1,\dots,K.
\end{equation}  
We refer to \cite{glaubitz2022generalized} for more details. 
\cref{algo:MMV_GSBL} summarizes the above procedure to approximate the MAP estimate of the joint-sparsity-promoting GSBL model above. 
Henceforth we refer to this method as the \emph{MMV-GSBL algorithm}.

\begin{algorithm}[h!]
\caption{The MMV-GSBL algorithm}\label{algo:MMV_GSBL} 
\begin{algorithmic}[1]
    \STATE{Choose model parameters $(\beta,\boldsymbol{\vartheta})$ and initialize $\boldsymbol{\theta}$} 
    \REPEAT
		\STATE{Update the parameter vectors $\mathbf{x}_{1:L}$ (in parallel) according to \cref{eq:x_update_GSBL}}
		\STATE{Update the hyper-parameters $\boldsymbol{\theta}$ according to \cref{eq:update_theta_GSBL}} 
    \UNTIL{convergence or the maximum number of iterations is reached}
\end{algorithmic}
\end{algorithm}

\begin{remark}[Uncertainty quantification]\label{rem:GSBL_UQ}
	We can partially quantify uncertainty in the parameter vectors recovered by the MMV-GSBL method described in \cref{algo:MMV_GSBL} by following the discussion in \cref{sub:UQ_IAS}. 
	The only difference to the MMV-IAS algorithm is that the covariance matrices $\Gamma_l$ in \cref{eq:posterior_x3} become $\Gamma_l = ( F_l^T F_l + R^T D_{\boldsymbol{\theta}} R )$ in the MMV-GSBL framework. 
\end{remark}


\subsection{Analysis}
\label{sub:GSBL_analysis} 

The analysis carried out for the MMV-IAS model and algorithm in \Cref{sec:analysis} can be extended to the MMV-GSBL model and algorithm. 
Both algorithms share a similar computational complexity of $\mathcal{O}(I (L N^2 + K N))$. 
However, as mentioned in previous studies  \cite{wipf2004sparse,glaubitz2022generalized}, the GSBL cost function can exhibit non-convexity with multiple local minima. 
This non-convexity is expected to persist in the MMV-GSBL cost function as well. 


\subsection{Connection to existing methods} 
\label{sub:GSBL_connection}

Recovering jointly sparse signals from MMV data using SBL was considered in \cite{wipf2007empirical}. 
The MMV-SBL method proposed \cite{wipf2007empirical} has certain limitations, however, including restrictions on the forward operators, the noise distribution, and the requirement of sparse parameter vectors. 
Furthermore, the evidence approach used in \cite{wipf2007empirical} can slow performance for large problems. 
In contrast, the MMV-GSBL algorithm (\cref{algo:MMV_GSBL}) is more efficient and flexible. 
It allows for varying forward operators, different noise distributions, and more general regularization operators promoting sparsity in an arbitrary linear transformation of the parameter vectors, 
This makes the proposed MMV-GSBL algorithm suitable for various MMV problems and large-scale parameter vectors.
