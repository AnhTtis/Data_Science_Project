\section{Introduction}
Machine Reading Comprehension (MRC) is a challenging research field in Natural Language Processing, in which systems learn to predict answers for the questions inputted by users given a relevant context. MRC has many real-world applications such as Open Domain Question Answering \cite{chen-etal-2017-reading} and conversational Question Answering \cite{reddy-etal-2019-coqa}. Thanks to the rapid development of pre-trained large language models, performances of MRC systems show substantial progress. Pre-trained large language models are typically deep learning models designed based on the architecture of the Transformers model \cite{NIPS2017_3f5ee243}. These models are pre-trained on very large text corpora using unsupervised tasks such as Masked Language Model and Next Sentence Prediction \cite{devlin-etal-2019-bert}. After the pre-training phase, researchers can leverage the language understanding of these models by fine-tuning them on downstream tasks such as MRC. After being fine-tuned, these language models can achieve state-of-the-art performances on many benchmarks.

Researchers also pre-train multilingual models which are transformers-based models pre-trained with text corpora in over 100 languages \citet{conneau-etal-2020-unsupervised,devlin-etal-2019-bert}. Although multilingual models do not rely on direct cross-lingual supervision while being pre-trained, they can achieve surprisingly high performances on different tasks in multilingual settings. Besides, these multilingual models also excel in monolingual settings, especially in low-resource languages, where the number of high-quality works in developing monolingual language models is still limited. However, the abilities of multilingual language models are restricted by the curse of multilinguality \cite{conneau-etal-2020-unsupervised}: pre-training a multilingual model with a fixed capacity on an increasing number of languages only improves its performances up to a certain point. Therefore, pre-trained multilingual models often show many language weaknesses compared to monolingual counterparts in monolingual settings.

Following the success of pre-trained models in English \cite{devlin-etal-2019-bert,zhuang-etal-2021-robustly}, researchers all over the world carry out many high-quality works in pre-training monolingual language models such as CamemBERT \cite{chan-etal-2020-germans} in French, GELECTRA \cite{martin-etal-2020-camembert} in German, and PhoBERT \cite{nguyen-tuan-nguyen-2020-phobert} in Vietnamese. These monolingual models also achieve state-of-the-art performance on numerous benchmarks, directly empowering the field of Natural Language Processing to develop in their respective languages.

Facilitated by the development of pre-trained language models, MRC has recently also shown great progress in many languages. For example, RoBERTa \cite{roberta}, CamemBERT \cite{martin-etal-2020-camembert} and GELECTRA \cite{chan-etal-2020-germans} achieve near human performances on SQuAD \cite{rajpurkar-etal-2018-know}, FQuAD \cite{dhoffschmidt-etal-2020-fquad,fquad20} and GermanQuAD \cite{moller-etal-2021-germanquad}, respectively. However, for other low-resource languages, such as Vietnamese, the performances of pre-trained language models are significant far lower than that of humans \cite{viquad20}. We can explain these difficulties in research by the underdevelopment of Vietnamese monolingual language models. As a result, most researchers \cite{vireader,vlspmrc1,vlspmrc2,Nguyen_2020} in Vietnamese MRC have to use multilingual models, which have many limitations in monolingual settings, as the cores of their MRC systems. 

The difficulties that Vietnamese MRC researchers encounter, together with the limited number of works on Vietnamese monolingual models, suggest that more high-quality research into Vietnamese monolingual models is urgently needed. Therefore, in order to suggest new directions for these future works, we attempt to reveal the language weaknesses of monolingual models by analyzing the performances of monolingual models in comparison with those of multilingual ones.

In this work, we choose to investigate the performances of models on MRC because it is a suitable task for exploring the weaknesses of language models from multiple linguistic aspects. MRC allows us to examine the performance of models on lexical aspects, single-sentence level aspects, and multi-sentence level aspects of natural language. For instance, in order to answer "Who" questions, MRC models must be competent in recognizing the person's name in a sentence, demonstrating their proficiency in Named Entity Recognition. Besides, to fully understand the given context, MRC models are expected to acquire extraordinary Reading Comprehension skills such as coreference resolution and bridging, which are part of the multi-sentence level aspects of language understanding.

We focus our analysis on unanswerable questions because unanswerable questions proposed by \citet{viquad20} are much more challenging than answerable questions in the same dataset, which directly creates more materials for us to reveal the language weaknesses of models. Additionally, since \citet{viquad20} proposed a novel method for annotating unanswerable questions, which involves instructing annotators to use various techniques to transform answerable questions into unanswerable ones instead of generating unanswerable questions from scratch, UIT-ViQuAD 2.0 has successfully introduced many new types of unanswerable questions. Therefore, we have a more diverse range of language aspects to analyze the performances of models on.

we initially examine the performance of monolingual and multilingual models on the UIT-ViQuAD 2.0 development set. However, we concern that the development set of UIT-ViQuAD 2.0 may not be sufficiently challenging to expose the language weaknesses of models on specific language aspects. Hence, we annotate a new set of high-quality unanswerable questions on an out-of-domain corpus to further analyze the language proficiency of both monolingual and multilingual models.

Our contributions are summed as follows:
\begin{enumerate}
    \item Our work successfully discovers different language weaknesses and strengths of Vietnamese monolingual models. Results from our work provide good directions for future works on more robust Vietnamese monolingual models.
    \item To more accurately assess the language abilities of models, we propose a new method for annotating high-quality unanswerable questions that successfully further challenge current systems in MRC. 
    \item Results from our analysis reveal that new high-quality Vietnamese Machine Reading Comprehension benchmarks are urgently needed.
\end{enumerate}
