\section{Models and Analysis Method}
\subsection{Models}
In this work, to highlight the weaknesses of Vietnamese language models, we compare the performances of two Vietnamese monolingual language models with those of two multilingual language models.\\
\textbf{Multilingual Language Models.} We choose mBERT \cite{devlin-etal-2019-bert} and XLM-RoBERTa \cite{conneau-etal-2020-unsupervised} as two multilingual models. Because we are investigating the weaknesses of existing models of each language model type, we decide to use XLM-RoBERTa\textsubscript{LARGE}, which outperforms XLM-RoBERTa\textsubscript{BASE} in almost all tasks of natural language processing. XLM-RoBERTa\textsubscript{LARGE} has 24 transformer-based layers with 560M parameters and was trained on 2394.3 GiB of text in 100 languages, in which 137.3 GiB of 24.7 billion word tokens is Vietnamese text. On the other hand, mBERT has 12 transformer-based layers with 178M parameters and was trained in 104 languages, including Vietnamese.\\
\textbf{Monolingual Language Models. }We choose the large version of PhoBERT \cite{nguyen-tuan-nguyen-2020-phobert}, and Vietnamese WikiBERT \cite{pyysalo-etal-2021-wikibert} as two competitive monolingual models against multilingual counterparts. PhoBERT\textsubscript{LARGE} is a transformer-based model with 370M parameters and is trained with 20GiB of 3 billion Vietnamese word tokens. The critical difference of PhoBERT from multilingual models is that PhoBERT segments Vietnamese words before applying the Byte-Pair encoding methods \cite{sennrich-etal-2016-neural} to the pre-training data. For example, while multilingual models tokenize the word ``học sinh''(\textit{student}) as two tokens, ``học'' and ``sinh'', PhoBERT treats this whole word as a single token ``học\_sinh'' This is because white space in Vietnamese is used to separate the syllables instead of words.

On the other hand, Vietnamese WikiBERT has 101M parameters and is trained with 172M Vietnamese tokens. Because researchers developing Vietnamese WikiBERT are not Vietnamese native speakers, they do not acknowledge the unique linguistic features of the Vietnamese language as \citet{nguyen-tuan-nguyen-2020-phobert} do.

In this paper, for simplicity, we will refer to PhoBERT\textsubscript{LARGE}, XLM-RoBERTa\textsubscript{LARGE} and Vietnamese WikiBERT as PhoBERT, XLM-RoBERTa, and WikiBERT, respectively.
\subsection{Analysis Method}
Following previous works \cite{rajpurkar-etal-2016-squad,rajpurkar-etal-2018-know,nguyen-etal-2020-vietnamese}, we use two metrics, Exact Match (EM) and F1-score, to evaluate the overall performances of different models on Reading Comprehension task.
\begin{itemize}
    \item \textbf{EM}: (Exact Match) The percentage of answers predicted by the MRC system match exactly any one of the gold answer(s) annotated by the human reader.
    \item \textbf{F1}: F1-score measured the average overlap between predicted answers with those in the gold answers. For each question, we calculate the F1 score of predicted answer with each gold answer, and take the maximum F1 as the F1 of the corresponding question. 

\end{itemize}
Because we carry out our analysis on the test set that requires models having abilities to recognize unanswerable questions, we also take into consideration the performances of models in classifying answerable and unanswerable questions. Performances on classification tasks are reported in our analysis as Recall on answerable questions and unanswerable questions.
\begin{itemize}
    \item \textbf{Recall\textsubscript{unanswerable}}: The percentage of unanswerable questions that the model correctly predicts as not having the answer in the given context.
    \item \textbf{Recall\textsubscript{answerable}}: The percentage of answerable questions that model attempt to answer. In order to focus on the classification task, this metric does not consider whether the model predicts the correct answer.
\end{itemize}
Then, in order to analyze the performances of models on different language aspects, we annotate each unanswerable question into one of 7 unanswerable types, most of which are inspired by \cite{viquad20}.

Besides, as we focus on suggesting new directions for works in developing Vietnamese monolingual models, instead of pointing out the weaknesses of any single model, we focus on determining different hard language aspects that are challenging for all investigated Vietnamese language models. Thus, we define two new concepts for this purpose:
\begin{itemize}
    \item \textbf{Monolingual hard unanswerable questions}: Unanswerable questions that both WikiBERT and PhoBERT attempt to answer.
    \item \textbf{Multilingual hard unanswerable questions}: Unanswerable questions that both mBERT and XLM-RoBERTa attempt to answer.
\end{itemize}
These concepts of monolingual and multilingual hard unanswerable questions empower us to focus on the language weaknesses that both monolingual models have compared to the weaknesses of both mBERT and XLM-RoBERTa. Thus, we can encourage future research to follow effective methods from previous works and develop new methods to deal with the existing weaknesses. To compare the results between different experiments, we calculate the percentage of monolingual and multilingual hard unanswerable questions over the total number of unanswerable questions in each unanswerable type.
\subsection{Experimental Settings}
All models are trained with 28,457 questions in training set of UIT-ViQuAD 2.0 \cite{viquad20} in 2 epochs. We use Adam optimizer \cite{DBLP:journals/corr/KingmaB14} with learning rate of $2\cdot10^{-5}$, $\beta_1 = 0.9$, $\beta_2 = 0.999$, and 100 warm-up steps for all 4 models. We fine-tuned all four models on a single NVIDIA Tesla K80 provided by Google Colaboratory. Due to these limited resources in computation, we have to fine-tune our models with a small number of samples per batch. The fine-tuning batch size we use for XLM-RoBERTa, mBERT, WikiBERT is 4, while 8 is the batch size in fine-tuning PhoBERT. We then evaluate models on the development set of UIT-ViQuAD 2.0 in Section 4 and Parallel UIT-VinewsQA in Section 5. 