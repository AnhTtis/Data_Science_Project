\section{Analysis on Parallel UIT-VinewsQA}
\input{sections/tables/vinews_stat.tex}
\input{sections/tables/vinewsperform}
UIT-VinewsQA is an extractive question answering dataset on Vietnamese healthcare news articles, most of which are narrative articles instead of informative like articles on the Wikipedia platform. Moreover, healthcare articles in UIT-VinewsQA are written for people with different education levels, so the sentence structure used in these articles must be simpler than that of Wikipedia articles. Therefore, as presented in Table \ref{tab: language-vinews-viquad}, UIT-VinewsQA has some linguistic differences from UIT-ViQuAD, such as
\begin{itemize}
    \item UIT-VinewsQA has fewer entities per sentence than UIT-ViQuAD. This significantly reduces the challenging level of recognizing relations between entities in the given context of extractive question answering task. Therefore, unanswerable questions of types such as \textit{Entity Swap} and \textit{Relation Reverse} are not as challenging for language models in UIT-VinewsQA compared to UIT-ViQuAD 2.0.
    \item UIT-VinewsQA has fewer tokens per sentence than UIT-ViQuAD, which leads to simpler sentence structures across the corpus. 
\end{itemize}

\subsection{Benchmark Annotations}
When annotating new unanswerable questions on UIT-VinewsQA, we strictly follow the procedure proposed by \citet{viquad20}: we transform answerable questions extracted from the development set of UIT-VinewsQA into unanswerable questions. However, to promote the diversity of unanswerable questions, we intentionally sample our answerable questions based on their reasoning skills inspired by \citet{van2020new} (word matching, paraphrasing, single-sentence reasoning, multiple-sentence reasoning). For each answerable reasoning skill - unanswerable question type pair, we annotated ten unanswerable questions. Therefore, we have a benchmark of 280 unanswerable questions of four answerable reasoning skills and seven unanswerable question types in addition to 280 answerable questions extracted from the UIT-VinewsQA development set. We name this benchmark Parallel UIT-VinewsQA because each answerable question in the benchmark is accompanied by a corresponding unanswerable question.

Besides, during the annotating process, we do not show our annotators the answers to the original (answerable) questions and ask them to annotate answers for these questions before transforming original questions into unanswerable ones. We only include an unanswerable question into our benchmark if the annotator correctly answers the corresponding answerable question. This helps us strictly require our annotators to grasp a ``big picture'' of the given context instead of merely focusing on the sentences containing answers to the original questions. In later analysis, we find out that this process significantly improves the quality of questions of all unanswerable types.
\subsection{Performance on Parallel UIT-VinewsQA}
\input{sections/tables/vinewshard}
Table \ref{vinewsqa-performane} show that all considered models  achieve only from $33.57\%$ to $43.21\%$ on Recall\textsubscript{answerable} when evaluated on the 280 newly annotated unanswerable questions. his cannot be attributed solely to the out-of-domain context, as the models performed well on the 280 answerable questions extracted from UIT-ViNewsQA, achieving the highest F1 score of 81.80\% among the four models. This result indicates that while UIT-VinewsQA is considered one of the high-quality Vietnamese MRC datasets, it does not fully reveal the existing weaknesses of MRC systems.

Our newly generated unanswerable questions thus give us much more materials to analyze the weaknesses and strengths of monolingual models in MRC. We then analyze the weaknesses of language models by examining the percentage of monolingual and multilingual hard unanswerable questions out of the total number of unanswerable questions. 

Due to the linguistic features of the UIT-VinewsQA corpus shown in Table \ref{tab: language-vinews-viquad}, \textit{Entity Swap} and \textit{Relation Reverse} types of unanswerable questions are no longer challenging as they are in UIT-ViQuAD. On the other hand, the most notable result from our analysis is that \textit{Antonym} type is significantly more challenging for monolingual models than for multilingual models. As the unanswerable questions of \textit{Antonym} type in SQuAD 2.0 \cite{nguyen-etal-2020-vietnamese} often require language models good lexical knowledge to correctly recognize, monolingual models are believed to have advantages over multilingual counterparts. This is because \cite{vulic-etal-2020-probing} show that monolingual models often encode significantly more lexical information than monolingual models. However, because we are following the process of annotating unanswerable questions proposed by \citet{viquad20} on a different corpus, we hypothesize that there may be some significant changes in unanswerable questions of \textit{Antonym} type in our benchmark.
\subsection{Analysis on Antonym Type}
Closely examining the performances of models on each unanswerable question of \textit{Antonym} type, we see that monolingual models often fail to recognize an unanswerable question when the antonym used to create that question does not explicitly contradict the context. Based on this observation, we believe that these questions should be analyzed separately from other questions of \textit{Antonym} type to understand the language weaknesses of monolingual models fully. We then divide \textit{Antonym} type into two new types of \textit{Implicit Antonym} and \textit{Explicit Antonym} to further explore the effects each type have on two types of language models (see Figure \ref{implcit-explicit-example} in \ref{sec:appendix2} for examples). In short, language models can correctly predict unanswerable questions of \textit{Explicit Antonym} using only lexical knowledge. However, to recognize an unanswerable question of \textit{Implicit Antonym}, models must acquire an adequate amount of high-level semantic knowledge.

\begin{table*}[h]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
 &
  Full Benchmark &
  \begin{tabular}[c]{@{}c@{}}Hard Monolingual \\ Unanswerable questions (\%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Hard Multilingual\\ Unanswerable questions (\%)\end{tabular} \\ \midrule
Explicit Antonym &
  25 &
  40.00 &
  \textbf{32.00} \\
Implicit Antonym &
  15 &
  80.00 &
  \textbf{46.67} \\ \bottomrule
\end{tabular}
\caption{Number of monolingual and multilingual hard unanswerable questions alongside with the number of unanswerable questions in the Parallel UIT-VinewsQA in Implicit and Explicit Antonym types}
\label{tab:explicit-implicit}
\end{table*}
Our analysis (Table \ref{tab:explicit-implicit}) reveals that while monolingual models show comparable performance on \textit{Explicit Antonym} type to multilingual models, \textit{Implicit Antonym} type is significantly more challenging for monolingual models than for multilingual models. This result proves that monolingual models lack skills in representing the relations between context and the adjective describing the context, which is part of high-level semantic knowledge.