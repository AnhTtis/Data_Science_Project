\section{Introduction}
\begin{figure}[t]
  \centering  
  \includegraphics[width=\textwidth]{teaser.pdf}
  \caption{
   Given that both interactive behaviour and natural language are sequential and hierarchical, we explored their similarity by applying an NLP method (a language encoder) to model mouse and keyboard behaviour. %
  }  
  \label{fig:teaser}
\end{figure}
Computational modelling of interactive behaviour has emerged as a key component of intelligent user interfaces (IUIs) in human-computer interaction (HCI)\\\cite{xu2016spatio,zhang2022predicting,antal2021sapimouse,acien2020typenet,dhakal2018observations}.
For example, understanding interactive behaviour helps HCI researchers and user experience (UX) designers analyse and improve interactive systems~\cite{salmeron2014evaluation,bi2012multilingual}.
Mouse and keyboard input is particularly promising %
because it is readily available on a large number of devices and pervasively used in daily life~\cite{xu2016spatio,sun2016shared}.
Interactive behaviour consists of low-level, atomic input actions that cannot be further decomposed~\cite{motwani2015multimodal}, which may resemble characters in natural language.
Furthermore, a sequence of such actions (an activity) that can reflect higher-level interaction goals may resemble a (sub)word that is a sequence of characters with semantic meanings.
As such, interactive behaviour has both a sequential (actions happen one after another) and a hierarchical structure (a sequence of actions forms an activity driven by specific interaction goals), and hence may be similar to natural language (see Fig.~\ref{fig:teaser}).
On the other hand, NLP methods, leveraging the sequential and hierarchical structure of input data, have recently achieved groundbreaking success in various downstream tasks like machine translation and question-answering~\cite{Pennington2014GloVeGV,kim2016character,jawahar2019does,kunchukuttan2016learning}.
However, analysing the possible similarity and link between interactive behaviour and natural language remains under-explored in HCI. %
One notable exception is the work by Han et al.\ that %
encoded $n$ consecutive actions (like mouse clicks) into tokens to learn action embeddings~\cite{han2020modelling}.
However, at its core, the method uses n-gram, which limits the length of action sequences to a fixed length $n$ and requires a dedicated search for its optimal value.
Moreover, the vocabulary size grows exponentially as $n$ increases~\cite{subba2017host}.
Due to such drawback, n-gram has been dropped in NLP in favour of more flexible methods such as byte pair encoding (BPE)~\cite{raffel2020exploring,radford2019language}.
BPE and its variants are used in a significant number of large language models (LLMs) to encode text as subwords, allowing rare or unseen words to be handled without introducing new tokens every time~\cite{sennrich2015neural,schuster2012japanese}.
Additionally, subwords in the vocabulary generated by BPE can have various lengths, allowing a rich and flexible vocabulary.
In this work, we explore the similarity between mouse and keyboard behaviour and natural language, by using BPE to learn a vocabulary, i.e., a set of activities, which is further used to encode the behaviour to perform interactive task recognition.
Knowing which task the user is conducting is essential for adaptive interactive systems that aim to understand interactive behaviour and interaction goals~\cite{pasqual2014mouse,fu2017your,hu2022EHTask}.

Existing mouse and keyboard datasets were typically collected in controlled laboratory settings, although behaviour tends to be more natural in out-of-the-lab settings~\cite{mazilu2015wearable}.
We evaluate the method on two datasets that cover both settings and offer both modalities.
For the lab setting, we chose the Buffalo dataset collected by Sun et al.~\cite{sun2016shared} as it is the largest available dataset~\cite{murphy2017shared}.
For the out-of-the-lab setting, given a lack of suitable publicly available data, we collected a novel multimodal dataset named EMAKI (\underline{E}veryday \underline{M}ouse \underline{A}nd \underline{K}eyboard \underline{I}nteractions)\footnote{\textcolor{\reviewcolor}{The dataset and code are available here: \url{https://git.hcics.simtech.uni-stuttgart.de/public-projects/EMAKI}}}
EMAKI was collected from 39 participants performing three interactive tasks: \textit{text entry and editing}, \textit{image editing} and \textit{questionnaire completion}.
These tasks can be found in a wide range of applications and UIs, and cover varying types of mouse and keyboard actions.

On the two datasets, vocabulary analysis shows that BPE could learn explainable activities, 
e.g., reflecting graphical user interface (GUI) layouts %
and indicating interaction goals such as performing mouse dragging or keyboard shortcuts.
Results from interactive task recognition show that BPE %
outperformed other methods on both modalities and datasets.
In summary, our contributions are three-fold:
    (1) We collect EMAKI, a novel 39-participant out-of-the-lab mouse and keyboard dataset.
    (2) We explore the potential similarity between natural language and mouse and keyboard behaviour by learning meaningful activities via a commonly used NLP method, BPE.
    (3) We show that encoding with BPE also improves the performance of interactive task recognition.
As such, our work uncovers the similarity between natural language and interactive behaviour, showing the potential for applying the new pack of methodology, i.e., NLP methods, to computational interactive behaviour modelling in HCI.