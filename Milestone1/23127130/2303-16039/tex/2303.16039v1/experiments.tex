\section{Evaluations of the NLP Method}
\label{sec:evaluation}
As mentioned at the beginning of Section~\ref{sec:method}, BPE was evaluated in two ways: (1) we analysed its vocabulary to examine if the way of learning semantic subwords from characters could learn meaningful activities from interactive actions; and (2) we tested if encoding interactive behaviour in this NLP fashion benefited a downstream task, interactive task recognition, using a Transformer-based classifier.
\subsection{Analysis of the Learnt Vocabulary}
\label{sec:experiment-vocabulary}
We first examined statistics of the vocabulary including its size and activity lengths.
Then we analysed semantic meanings of the most frequent and long activities.
Frequent activities are short, low-level and pervasively exists in various activities, while long activities reflect high-level and complex goals.

\subsubsection{Vocabulary Statistics.}

\begin{figure}[t]
    \centering
    \vspace{-1cm}
    \includegraphics[width=\linewidth]{BPE_vocab_violin_vertical.pdf}
    \vspace{-0.5cm}
    \caption{Violin plots for the lengths of activities learnt by BPE after 300 (in red), 600 (in blue) and 900 (in green) iterations, of (a) mouse, (b) keyboard and (c) both modalities on EMAKI and Buffalo datasets. Each bar shows the range of activity lengths, while the middle line indicates the median length. The y-axes are scaled according to the range in each subplot.}
    \label{fig:violin}
\end{figure}

As Fig.~\ref{fig:violin}a shows, in EMAKI the maximum length of mouse activities reached 243 actions (BPE-900), while the median length was 16.
The longest keyboard activity had 53 actions, while the median length was 3 (Fig.~\ref{fig:violin}b).
When using both modalities jointly, the maximum activity length was 239 after 900 iterations, while the median length was 4 (Fig.~\ref{fig:violin}c).
In Buffalo, the lengths of activities had a maximum of 405 and a median of 39 from mouse behaviour (Fig.~\ref{fig:violin}a); a maximum of 16 and a median of 4 from keyboard behaviour (Fig.~\ref{fig:violin}b); and a maximum of 158 and a median of 4 from joint modalities (Fig.~\ref{fig:violin}c).
Mouse activities were longer than keyboard activities, indicating that the preprocessed mouse data were more similar compared to preprocessed keyboard data.
Comparisons between datasets show that mouse activities in EMAKI were more diverse, while Buffalo contained more diverse keyboard activities.

\begin{table}[t]
    \vspace{-1mm}
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|} 
        \hline
        Dataset & \multicolumn{3}{c|}{EMAKI} & \multicolumn{3}{c|}{Buffalo}\\
        \hline
        Method & BPE-300 & BPE-600 & BPE-900 & BPE-300 & BPE-600 & BPE-900 \\
        \hline
         Mouse & 322 & 622 & 921 & 310 & 609 & 909\\
        Keyboard & 513 & 808 & 1103 & 473 & 770 & 1067\\
        Both & 569 & 864 & 1163 & 496 & 790 & 1084\\
        \hline
    \end{tabular}    
    \vspace{2mm}
    \caption{Vocabulary sizes generated using BPE after 300, 600 and 900 iterations, on EMAKI and Buffalo datasets.}
    \label{tab:vocabsize}
    \vspace{-4mm}
\end{table}

Table \ref{tab:vocabsize} shows the vocabulary sizes generated by BPE on the two datasets.
Note that starting from BPE-$k$ and running the algorithm for $k$ more iterations, the vocabulary size increases by approximately $k$ elements -- showing that BPE overcomes the issue of exponential growth of vocabulary size in n-gram.

\subsubsection{Frequent Activities.}
\begin{table}[t]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        Rank & 1 & 2 &3&4&5&6&7&8&9&10 \\
        \hline
        EMAKI    & $\sqcup{\downarrow}$, $\sqcup{\uparrow}$
        & ${\Leftarrow}{\downarrow}{,}{\Leftarrow}{\uparrow}$
        & e${\downarrow}$, e${\uparrow}$
        & t${\downarrow}$, t${\uparrow}$
        & a${\downarrow}$, a${\uparrow}$
        & o${\downarrow}$, o${\uparrow}$
        & ${\Leftarrow}{\downarrow}{,}{\Leftarrow}{\uparrow}{,}{\Leftarrow}{\downarrow}{,}{\Leftarrow}{\uparrow}$
        & i${\downarrow}$, i${\uparrow}$
        & s${\downarrow}$, s${\uparrow}$
        & n${\downarrow}$, n${\uparrow}$\\
        \hline
        Buffalo & $\sqcup{\downarrow}$, $\sqcup{\uparrow}$
        & ${\Leftarrow}{\downarrow}{,}{\Leftarrow}{\uparrow}$
        & e${\downarrow}$, e${\uparrow}$
        & t${\downarrow}$, t${\uparrow}$
        & o${\downarrow}$, o${\uparrow}$
        & i${\downarrow}$, i${\uparrow}$
        & a${\downarrow}$, a${\uparrow}$
        & s${\downarrow}$, s${\uparrow}$
        & n${\downarrow}$, n${\uparrow}$
        & l${\downarrow}$, l${\uparrow}$\\
        \hline
    \end{tabular}
    \caption{The ten most frequent keyboard activities found by BPE. The $\sqcup$ symbol represents Space. The $\Leftarrow$ symbol means Backspace. The down arrow $\downarrow$ and up arrow $\uparrow$ denote \textit{KeyDown} and \textit{KeyUp}, respectively.}
    \label{tab:top10key}
\end{table}

The three BPE iterations learnt the same top-10 frequent keyboard action sequences as shown in Table~\ref{tab:top10key}.
Eight out of ten action sequences are the same on the two datasets, although they were collected from different participants in different experimental settings, indicating that generalised patterns underlie keyboard behaviour.
The interaction goal behind the most frequent activity is to press the spacebar, which is in line with the observation that spaces occur often when typing in various languages.
The second frequent activity reflects an intention of pressing Backspace which is frequently and widely used to %
correct what has been typed.
Most frequent activities correspond to character keystrokes, and reflect the top-7 most frequent English letters: "e" (12.15\%), "a" (8.67\%), "t" (8.60\%), "i" (7.53\%), "o" (7.38\%), "n" (7.34\%) and "s" (6.63\%)~\cite{grigas2018letter}.
The difference in their order may be due to that the datasets are limited to specific typing scenarios and not representative of the entire English language.
We also noticed that the left and right arrows, for redirecting typing locations, were also frequent on both datasets.

The most frequent ten mouse action sequences learnt by BPE were also the same on the two datasets.
All of them are mouse moves of pinpoint, implying that participants follow similar ways to interact with UI targets even in different tasks and settings.
These pinpointing regions were primarily in the top-left and bottom-left areas, while fewer pinpoints fell on the right side.
This matches the layouts of not only general GUIs but also those used in our user study.
For example, menu bars and sidebars are commonly at the top and to the left of interactive windows, respectively.
Also, our text formatting tools were at the top of the text editor.
The image editing tools were in the leftmost of the image editor.
Additionally, our questionnaires were left-aligned, so the choices for participants to click lay to the left.

\subsubsection{Interaction Goals behind Activities.}
We also analysed long activities to examine if BPE learnt a hierarchy, i.e., if atomic actions form meaningful activities driven by complex goals.
An example is ``Dot${\downarrow}$, Dot${\uparrow}$, Space${\downarrow}$, Space${\uparrow}$, Shift${\downarrow}$, i${\downarrow}$, i${\uparrow}$, Shift${\uparrow}$, Space${\downarrow}$, Space${\uparrow}$''.
The goal behind the whole sequence is to start a sentence with the word ``I'', in line with the common texting or typing scenario of writing about oneself.
It consisted of the low-level goal of pressing each aforementioned key, which was further composed of atomic actions \textit{KeyDown} and \textit{KeyUp}.
BPE also learnt ``Space${\downarrow}$, Space${\uparrow}$, Backspace${\downarrow}$, Backspace${\uparrow}$'' from EMAKI, suggesting that participants typed at a faster pace than their thought process.
Another example is ``Ctrl${\downarrow}$, s${\downarrow}$, s${\uparrow}$, Ctrl${\uparrow}$'' from Buffalo, representing the shortcut for saving files.
Looking at mouse behaviour, BPE captured drag behaviour, represented as a \textit{MouseDown} action followed by multiple \textit{MouseMove} actions and ending with a \textit{MouseUp} action.
Another learnt long activity %
had 37 actions %
with 35 moves and a click as pinpoint in area 0, reflecting the goal of adjusting the cursor to a target and then clicking.

\subsection{Interactive Task Recognition}
\label{sec:task-recognition}
We also evaluated the practical effectiveness of our approach on interactive task recognition.
Knowing which task a user is performing enables adaptive UIs to understand the interactive behaviour and goals~\cite{hu2022EHTask,hadnett2019effect,hu21_User}.
We compared our approach with two baselines: an ablated version which bypasses encoding (noted as NoEncoding) and replacing BPE with an autoencoder (AE).
Autoencoder, consisting of an encoder and a decoder, is trained in a self-supervised way to reconstruct the input with the lowest error.
Therefore, it needs no annotations and has a high generalisability, %
also used on language data~\cite{li2015hierarchical}.
To control variables, i.e., restrict the comparison to the encoding, we set two rules:
(1) to reduce the impact of sophisticated designs of the encoders, use vanilla AE and BPE; (2) use the same hyperparameter sets for the classifier.

We implemented an AE that includes four components: an embedding layer of dimension $d_{e}=128$ to handle discrete tokens; an encoder component composed of one to three fully connected (FC) layers with hidden dimensions $(64)$, $(64, 32)$ and $(64, 32, 16)$; a decoder component, which is symmetric to the encoder; and a reconstruction component consisting of an FC layer %
and a softmax layer. %
Dropout was added after FC layers to avoid overfitting.
We denote the autoencoder that has one, two, or three FC layers in the encoder and decoder components as AE-1, AE-2 and AE-3.
Cross entropy between the reconstructed sequences and the input was used as the loss function.
After training, the encoder component was used to encode interactive behaviour.

Our task classifier is based on a Transformer~\cite{vaswani2017attention}, which is well known for its success in NLP and capability to handle long dependencies in temporal signals.
The classifier is composed of $N=\{2, 4, 6\}$ Transformer encoder layers, then an FC and softmax layer.
Each Transformer encoder layer had $h=4$ attention heads, $d_\text{model}=\{16, 64\}$ expected features, $d_\text{ff}=4d_\text{model}$ dimension in feedforward layers and uses the ReLU activation function.
During training, we applied label smoothing with $\epsilon=0.1$~\cite{vaswani2017attention}.
We used AdamW optimizer with learning rate $lr=\{10^{-3}, 10^{-4}\}$ and $\beta=(0.9, 0.999)$~\cite{bruckner2022learning} and the cross entropy as loss function.
The training was done on a Tesla V100 GPU with a batch size of 64 and a dropout rate of 0.5.
The classifier was trained for 30 epochs, while the AE was trained for 10 epochs because of its faster convergence.
Because activities in the flexible vocabulary learnt by BPE have different lengths, we padded short samples %
and applied padding masks.

EMAKI has three main interactive tasks, posing a three-class classification problem, while Buffalo has two tasks, posing a binary classification problem.
The evaluation follows 5-fold participant-independent cross-validation, where data from 80\% of participants form the training set and the remaining participants form the test set.
This scheme can evaluate the performance of unseen users. %
Macro F1 score~\cite{hoppe2018eye} was chosen as evaluation metric because of the imbalanced classes, %
e.g., most keyboard data were from the text task on EMAKI.
For each model, we report the highest F1 score achieved among all the parameter sets.
Results show that on both datasets methods using BPE encoding outperformed the others (see Fig.~\ref{fig:TR-Ours} and~\ref{fig:TR-UB}).

\paragraph{\textbf{Results on EMAKI}}
\label{sec:TR-Ours}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{bar_TR_ours.pdf}
    \caption{F1 scores of recognising three interactive tasks on EMAKI from (a) mouse, (b) keyboard and (c) both modalities, segmented by different windows.
    Error bars represent the standard deviation from a 5-fold cross-validation.}
    \label{fig:TR-Ours}
\end{figure}

On mouse data, BPE-300 consistently outperformed other methods (Fig.~\ref{fig:TR-Ours}a).
A one-way ANOVA test showed that differences between methods are significant ($p{<}.001$): $F{=}9.697$ on $L_{win}{=}200$, $F{=}12.396$ on $L_{win}{=}100$ and $F{=}7.194$ on $L_{win}{=}20$. A post-hoc Tukey HSD test further confirmed that BPE-300 significantly outperformed the other methods on $L_{win}{=}200$, $L_{win}{=}100$ ($p{<}.001$ for AE and $p{<}.05$ for NoEncoding) and $L_{win}{=}20$ ($p{<}.01$ for both AE and NoEncoding). 
Fig.~\ref{fig:TR-Ours}b shows that BPE-600 achieved the best results for $L_{win}{=}100$ and $L_{win}{=}50$, whereas when $L_{win}{=}10$ the best was BPE-300.
Differences between methods are significant ($F{=}13.044$, $p{<}.001$ for $L_{win}{=}100$, $F{=}4.620$, $p{<}.01$ for $L_{win}{=}50$ and $F{=}4.220$, $p{<}.01$ for $L_{win}{=}10$). 
Post-hoc Tukey HSD tests confirmed that BPE-600 significantly outperformed NoEncoding ($p{<}.01$) and AE-1 ($p{<}.001$) for $L_{win}{=}100$. %
On joint modalities, BPE-300 performed the best, with %
the highest F1 score of 0.693 (Fig.~\ref{fig:TR-Ours}c). %
Differences between methods were again significant with $F{=}13.996$, $p{<}.001$ on $L_{win}{=}150$, $F{=}5.678$, $p{<}.001$ on $L_{win}{=}75$ and $F{=}2.665$, $p{<}.05$ on $L_{win}{=}15$.
Tukey HSD test indicated that BPE-300 significantly outperformed AE ($p{<}.01$) and NoEncoding ($p{<}.05$) on $L_{win}{=}150$ and both of them at $p{<}.05$ when $L_{win}{=}75$.


In Section~\ref{sec:proficiency}, we report that participants using touchpads and traditional mice show different proficiencies. Therefore, we analysed if such differences affected task
recognition.
We separately performed 5-fold cross-validation based on the two groups.
Since 24 participants used traditional mice while only 15 used touchpads, we randomly selected 15 traditional mouse users to reduce the influence of data amount on performance.
Because BPE-300 on the longest window achieved the best results on mouse data (Fig.~\ref{fig:TR-Ours}a), we used the same setting and did a Mann-Whitney U test on F1 scores achieved from two groups.
To mitigate the randomisation introduced by participant selection, we repeated the above procedure five times.
None of the five tests found a significant difference in performance.
The reason may be that our method does not explicitly encode time information, thus ignoring the speed difference in moving the cursor~\cite{hertzum2013effect}.

\paragraph{\textbf{Results on Buffalo}}
\label{sec:TR-UB}
On mouse data (Fig.~\ref{fig:TR-UB}a), BPE-300 performed the best %
and got the highest F1 score of 0.547. %
One-way ANOVA showed that differences between methods were significant ($p{<}.001$) with $F{=}20.345$ for $L_{win}{=}200$, $F{=}18.609$ for $L_{win}{=}100$ and $F{=}5.589$ for $L_{win}{=}20$).
Post-hoc Tukey HSD tests showed that BPE-300 significantly outperformed NoEncoding ($p{<}.05$) and AE-1 ($p{<}.001$) when $L_{win}{=}200$.
On keyboard data (Fig.~\ref{fig:TR-UB}b), BPE-900 and BPE-600 outperformed other methods. %
Differences between methods are significant with $F{=}30.218$ for $L_{win}{=}100$, $F{=}5.884$ for $L_{win}{=}50$ (both $p{<}.001$) and $F{=}4.791$, $p{<}.01$ for $L_{win}{=}10$.
According to post-hoc Tukey HSD tests, BPE-900 significantly outperformed AE-1 ($p{<}.01$) and NoEncoding ($p{<}.05$) when $L_{win}{=}100$, and BPE-600 significantly outperformed AE-1 ($p{<}.001$) when $L_{win}{=}50$.
On joint modalities (Fig.~\ref{fig:TR-UB}c), BPE resulted in similar %
yet higher F1 scores than baselines.
The best result was achieved by BPE-600 on the longest window %
of 0.701. %
Differences between methods were again significant ($p{<}.001$): %
$F{=}10.733$ for $L_{win}{=}150$; $F{=}11.151$ for $L_{win}{=}75$; and $F{=}7.397$ for $L_{win}{=}15$.
Tukey HSD test showed that BPE-600 significantly outperformed AE-2 ($p{<}.01$) and NoEncoding ($p{<}.05$) on $L_{win}{=}150$; BPE-300 outperformed AE-1 ($p{<}.01$) and NoEncoding ($p{<}.05$) on $L_{win}{=}75$; and BPE-600 outperformed AE-3 ($p{<}.05$) on $L_{win}{=}15$.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{bar_TR_UB.pdf}
    \caption{F1 scores of recognising two interactive tasks on Buffalo from (a) mouse, (b) keyboard and (c) both modalities, segmented by different windows.
    Error bars represent the standard deviation from a 5-fold cross-validation.}
    \label{fig:TR-UB}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{UB_TR_mouse_check_39.pdf}
    \caption{Distribution of the average Euclidean distances between mouse trajectories from different interactive tasks on the two datasets. Smaller distances mean that trajectories from different tasks are more similar.}
    \label{fig:distributions}
\end{figure}

It is noticeable that results obtained from Buffalo mouse data slightly exceeded the chance level and were much worse than those from keyboard data.
A possible reason is that the mouse behaviour on the Buffalo dataset was similar across different tasks.
To verify this, we calculated the average distances between mouse trajectories in different interactive tasks, following~\cite{wulff2019mouse}:
(1) all the mouse actions generated in one trial by one participant were considered one trajectory, on which 101 points were sampled uniformly;
(2) the distance between two trials was defined as the average Euclidean distance between each pair of points on two trajectories;
(3) the distance between two tasks was computed as the average distance between each trial from task 1 and each from task 2.
Fig.~\ref{fig:distributions} shows that the distance between tasks from Buffalo is smaller than EMAKI, suggesting that mouse behaviour generated from the two tasks from Buffalo is similar, consistent with the statistics of BPE vocabulary (Section~\ref{sec:experiment-vocabulary}).
Therefore, it is more difficult to classify tasks based on Buffalo mouse data.