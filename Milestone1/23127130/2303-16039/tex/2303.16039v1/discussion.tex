\section{Discussion}
\label{sec:discussion}

\subsection{Modelling Interactive Behaviour from a Natural Language Perspective}
\label{sec:discussion-language}
Our work is among the first to explore the similarity between interactive behaviour and natural language, given that both have a sequential and hierarchical structure.
Towards this goal, we applied BPE, which has been commonly used in state-of-the-art large language models to encode mouse and keyboard behaviour.
At the lowest level, input actions were considered as %
characters since they are atomic and inseparable.
For higher levels, BPE learned ``subwords'' from interactive behaviour, which were interactive activities, i.e., action sequences driven by underlying interaction goals.
The analysis of the learnt vocabulary showed that following the same way of learning the semantic hierarchy of language, BPE was able to %
capture meaningful activities %
such as mouse drags, keyboard shortcuts and precisely adjusting the mouse to click on a UI element (Section~\ref{sec:experiment-vocabulary}).
Despite representing just a first exploration, the insights from our analysis underline the similarity between interactive behaviour and natural language, %
and indicate the possibility of applying more powerful NLP methods like BERT~\cite{jawahar2019does,liu2019roberta} to encode interactive behaviour.
Besides the state-of-the-art performances achieved, such LLMs also have noticeable advantages of generalisability and reusability.
They can be pretrained on one dataset and re-used to encode other datasets to solve various downstream tasks with fine-tuning, which is more cost-effective than dedicating a specific large model towards each dataset or task~\cite{sun2019fine,zhang2020revisiting}.
Future HCI research can follow such NLP methods to build reusable pretrained interactive behaviour models for better generalisability and cost-effectiveness.

\subsection{NLP Encoding for Interactive Task Recognition}
\label{sec:discussion-compareMK}
Interactive task recognition is one of the key requirements of intelligent interactive systems to understand and adapt to interactive behaviour and interaction goals~\cite{pasqual2014mouse,fu2017your,gajos2004supple,joachims2002optimizing}.
On this recognition task, encoding with BPE significantly outperformed baselines on both datasets, all the modalities and windows. %
Specifically, on our out-of-the-lab, newly collected EMAKI dataset. %
Encoding with BPE obtained the highest F1 score of 0.703 recognising three tasks
(Fig.~\ref{fig:TR-Ours}).
On average, BPE improved the F1 score by 0.087 on keyboard data, 0.051 on mouse, and 0.044 on the joint modalities.
On the Buffalo dataset, %
BPE achieved the highest F1 score of 0.865 (Fig.~\ref{fig:TR-UB}) recognising two tasks.
On average, BPE improved the F1 score by 0.080 on joint modalities, 0.053 on keyboard data and 0.035 on mouse data.
These results, from a practical perspective, further reveal the promising effectiveness of modelling interactive behaviour as natural language.

We observed that methods generally achieved better results on longer windows, which may be due to that more actions may uncover richer characteristics of the tasks.
However, increasing the window size yields fewer training samples and makes the recognition model wait longer for a complete window of actions to provide a prediction.
In our experiments, windows that led to the best performance on mouse, keyboard and joint modalities had 200, 100 and 150 actions, respectively.
These values can be a reference for future mouse and keyboard behaviour modelling methods.

In addition, on both datasets, using BPE on keyboard behaviour improved the F1 score more than on mouse behaviour, indicating its better ability of handling keyboard than mouse behaviour.
This finding is expected, as typing on a keyboard is directly linked to expressing natural language. %
A second reason might be that discretising mouse data caused a loss of information.
On the joint modalities, we observed a general performance improvement from individual modalities on EMAKI, but not on Buffalo.
As shown in Section~\ref{sec:TR-UB}, Buffalo lacks the diversity in mouse behaviour and thus %
performance achieved by combining mouse and keyboard is in-between that of individual modality.

\subsection{EMAKI Dataset}
Most publicly available mouse and keyboard datasets were collected in constrained laboratory settings, such as the Buffalo dataset.
In contrast, our EMAKI is a step towards fully unconstrained settings to allow more natural interactive behaviour.
Our study did not control where, when, or how long participants joined the study.
In addition, participants used their own devices, which contributes to ecological validity.
Consequently, our participant pool is more diverse given that participants are from different countries, and used different input devices and screen resolutions.
All of Buffalo's participants were university students between 20-30 years old, while ours were between 18-54 and covered non-student participants.
Moreover, %
our participants spent various time on tasks as they were freer to pause and resume (as shown in Section~\ref{sec:Buffalo} and \ref{sec:Ourdataset}).
Buffalo primarily uses typing-focused tasks%
, while EMAKI has complementary characteristics and tasks -- like image editing and questionnaires -- encouraging diverse mouse behaviour, as confirmed by our analysis in Section \ref{sec:experiment-vocabulary}.
Furthermore, higher diversity in behaviour can lead to better task recognition performance (Section \ref{sec:task-recognition}).
We also verified that the amount of data in EMAKI is sufficient for training the method for task recognition (see Appendix).
Besides serving as a benchmark for task recognition, the questionnaires included in EMAKI also encourage future research on the interplay between multimodal behaviour and personality traits~\cite{zhao2020reading}.

\subsection{Limitations and Future Work}
Our user study %
covered %
diverse but predefined tasks and did not allow multitasking. %
In the future, we will keep moving towards fully uncontrolled settings. %
Time information may further improve behaviour modelling~\cite{azcarraga2011use,freihaut2021does} and will be explicitly encoded in future work. %
Finally, we note that even used on both modalities jointly, BPE learned activities composed of single modalities. 
A possible reason is that the behaviour of switching between mouse and keyboard is diverse, which BPE could not capture.
Future work could explore %
the use of other %
NLP methods to better capture the interplay between mouse and keyboard behaviour~\cite{liu2019roberta,raffel2020exploring,radford2019language}.
Moreover, we intend to study other interactive modalities, such as screen touch and mid-air gesture, as well as other HCI downstream tasks such as personality recognition.