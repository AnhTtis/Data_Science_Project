% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{fancybox}
% Remove the "review" option to generate the final version.
%\usepackage[review]{acl}
\usepackage[]{acl}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{ragged2e}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}



\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
%\usepackage{emoji}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{pgfplots.groupplots}
\usepackage{amssymb} % for checkmark
\usetikzlibrary{arrows,automata,calc,shapes, positioning,shadows,shadows.blur,shapes.geometric,decorations.pathmorphing,patterns,matrix}
\usepackage{pgfplotstable}
\usepgfplotslibrary{groupplots}
\definecolor{c1}{HTML}{ef476f}
\definecolor{c2}{HTML}{f78c6b}
\definecolor{c3}{HTML}{ffd166}
\definecolor{c4}{HTML}{06d6a0}
\definecolor{c5}{HTML}{118ab2}
\definecolor{c6}{HTML}{073b4c}



%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.9,0.98,0.95}


% If the title and author information does not fit in the area allocated, uncomment the following
% \setlength\titlebox{5cm}
% and set <dim> to something 5cm or larger.

\newcommand{\draftonly}[1]{#1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Uncomment for submission
\renewcommand{\draftonly}[1]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\wammar}[1]{{\color{blue}[#1]$_{wa}$}}
\newcommand{\faiz}[1]{{\color{green}[#1]$_{fs}$}}
\newcommand{\rahul}[1]{{\color{red}[#1]$_{rahul}$}}
\newcommand{\msano}[1]{{\color{violet}[#1]$_{msano}$}}
\newcommand{\sid}[1]{{\color{teal}[#1]$_{sid}$}}
\newcommand{\ag}[1]{{\color{red}[#1]$_{ag}$}}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\topfraction}{.75} 
% \newcommand{\wammar}[1]{}
% \newcommand{\faiz}[1]{}
% \newcommand{\rahul}[1]{}
% \newcommand{\msano}[1]{}
% \newcommand{\sid}[1]{}
% \newcommand{\ag}[1]{}
% To display xmark
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}%

% package for adding dash lines
\usepackage{arydshln}  
% \usepackage{arabtex}
% \usepackage{utf8}
% \setcode{utf8}
%%% define the dashline
%\setlength\dashlinedash{0.2pt}
%\setlength\dashlinegap{1.5pt}
%\setlength\arrayrulewidth{0.3pt}

\newcommand{\presto}[0]{\textsc{PRESTO}}

\title{\presto{}: \textbf{A Multilingual Dataset for \\
Parsing Realistic Task-Oriented Dialogs} \\ \quad}

\author{
\parbox{\linewidth}{\centering
% first authors
Rahul Goel\thanks{\quad Equal contribution.}~,~~
Waleed Ammar\footnotemark[1]~,~~
Aditya Gupta\footnotemark[1]~,~~
Siddharth Vashishtha\footnotemark[1]~\hspace{0.5mm}\thanks{\quad Work done during an internship at Google.}\hspace{1.2mm}$^{ \clubsuit}$,~~\\
Motoki Sano\footnotemark[1]~,~
Faiz Surani\footnotemark[1]~\hspace{0.5mm}\footnotemark[2]\hspace{1.2mm}$^\diamondsuit$,~
% others
Max Chang,~
HyunJeong Choe,~
David Greene,~
Kyle He,~\\
Rattima Nitisaroj,~
Anna Trukhina,~
Shachi Paul,~
Pararth Shah,~
Rushin Shah,~
Zhou Yu$^\spadesuit$
}
\\ Google Inc.\\ $^\clubsuit$ University of Rochester~~ $^\diamondsuit$ University of  California, Santa Barbara ~~ $^\spadesuit$Columbia University
\\ \texttt{presto.dataset@google.com} 
}

\date{}

\bibliographystyle{acl_natbib}

%Code listing style named "mystyle"
% \lstdefinestyle{mystyle}{
%   backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
%   keywordstyle=\color{magenta},
%   numberstyle=\tiny\color{codegray},
%   stringstyle=\color{codepurple},
%   basicstyle=\ttfamily\footnotesize,
%   breakatwhitespace=false,         
%   breaklines=true,                 
%   captionpos=none,                    
%   keepspaces=true,                 
%   numbersep=5pt,                  
%   showspaces=false,                
%   showstringspaces=false,
%   showtabs=false,                  
%   tabsize=0
% }

%"mystyle" code listing set
%\lstset{style=mystyle}

\begin{document}
\maketitle

\begin{abstract}
Research interest in task-oriented dialogs has increased as systems such as Google Assistant, Alexa and Siri have become ubiquitous in everyday life.
However, the impact of academic research in this area has been limited by the lack of datasets that realistically capture the wide array of user pain points.
To enable research on some of the more challenging aspects of parsing realistic conversations, we introduce \presto\footnote{\url{https://github.com/google-research-datasets/presto}}, a public dataset of over 550K contextual multilingual conversations between humans and virtual assistants. \presto{} contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions. It is the only large-scale human generated conversational parsing dataset that provides structured context such as a user's contacts and lists for each example. 
Our mT5 model based baselines demonstrate that the conversational  phenomenon present in \presto{} are challenging to model, which is further pronounced in a low-resource setup.    
\end{abstract}



\section{Introduction}
Virtual dialog agents (a.k.a. ``\emph{assistants}'') are increasingly becoming a part of our everyday life.
From setting up alarms to controlling one's home environment, users now converse with their devices to accomplish many tasks previously done manually.
Parsing task-oriented dialogs is the problem of understanding the user's intent and any associated arguments so that the assistant can fulfill the requested task.

Given the prominence of data-driven methods in NLP, public availability of relevant datasets dictates which problems can be effectively studied by the research community at large.
Datasets such as MultiWoz~\cite{multiwoz,eric2019multiwoz}, TOP~\cite{top}, MTOP~\cite{mtop}, and SMCalFlow~\cite{smcalflow} have enabled researchers to study numerous developments in conversational semantic parsing, e.g.,~\newcite{Budzianowski2019HelloIG}, ~\newcite{Pasupat2021ControllableSP}.
However, many challenging aspects of parsing, such as disfluencies, code-switching, and the use of the structured context which grounds conversations, have been missing in these datasets, stifling meaningful research progress in those areas.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{main.pdf}
    \caption{Examples of user revision dialog sessions from \presto. \presto{} includes annotated dialogs in 6 languages (\emph{de, en, es, fr, hi, ja}) with various characteristics such as corrections (\texttt{correct-argument, correct-action}), cancellations (\texttt{cancel-action}) etc. Each example consists of 
\underline{Input}: a user's virtual state (context), one or more user utterances and the corresponding virtual assistant responses (dialog).
\underline{Output}: the semantic parse of the last user utterance in the dialog (parse).}
    \label{fig:example}
\end{figure*}


With that in mind, we present \presto{}, a dataset that better represents real conversations users have with their virtual assistants. \presto{} consists of:
\begin{itemize}
    \item Challenging conversational phenomena such as disfluencies, code-switching and user revisions, addressing some of the aforementioned limitations of current virtual assistants and enabling researchers to test new methods of addressing these challenges. 
    \item Realistic multilingual dialogues. Unlike in other datasets, conversations in each language were not obtained by translating English dialog, but instead contributed by native speakers of each language.
    \item Human and synthetic structured context with the conversations. The data contributors were instructed to look at and optionally reference relevant context in the user's virtual environment. For example, contributors could reference the user's contact list while initiating a phone call or sending a message using the virtual assistant.
\end{itemize}


When users speak naturally to their virtual assistants, we observe a variety of conversational phenomena: disfluent turns~\cite{gupta2021disfl}, user revisions (also known as conversational repair~\cite{cassell2000human}) and code-switching (also known as code-mixing~\cite{agarwal2022cst5}), to name a few.
Our dataset highlights several such linguistic phenomena, and provides over thousands (Table~\ref{tab:data_stats}) of examples of each phenomenon per language, each produced by a native speaker.
Overall, our dataset contains more than 550k annotated utterances across six languages.
Each user utterance is further verified by three native speakers for fluency and correctness and then annotated separately twice. Section \S\ref{sec:data_collection} discusses how the data was collected.


Section \S\ref{sec:data_summary} discusses context as well as other important phenomena which are well-represented in this dataset such as user revisions and disfluent utterances. In section \S\ref{sec:experiments}, we use mT5-based models \citep{Xue2020mT5AM} to present some baseline model performance metrics for this dataset, e.g., exact match accuracy on the test sets, the relative performance of monolingually- vs. multilingually-trained models, and data scaling effects with respect to various linguistic phenomena. 



\section{Dataset Characteristics}\label{sec:data_summary}
In this section, we discuss the characteristics of \presto{} dataset that set it apart from existing datasets: native conversations in multiple languages, code-switched utterances, user revisions, disfluencies, and structured context.

\paragraph{Native Speakers.}
The \presto{} dataset only includes utterances provided by native speakers of the language, with no translation. 
Table \ref{tab:data_stats} shows the number of examples in each language.

\begin{table*}[]
\small
\begin{tabular}{lcccccccc}
\toprule
Language & \# Intents / & Avg. Slots / & Avg. Tokens / & Avg. Prev.  & Total & Code & User & Spoken\\
 & \# Slots  & Utterance & Utterance & Utterances & Examples & Switching & Revisions & Disfluencies \\

\midrule
German   & 34 / 285 & 1.60 & 9.17 & 2.56 & 83,584             & 12,357                 & 22,129         & 16,781       \\
English & 34 / 303 &  1.57 & 9.03 & 2.49 & 95,671             & 5,918                  & 27,741         & 17,588       \\
Spanish & 34 / 299 & 1.72 & 10.70 & 2.57 & 96,164            & 12,570                 & 27,713         & 21,510       \\
French  & 34 / 303 & 1.63 & 10.73  & 2.63 & 95,870            & 12,939                 & 25,157         & 20,137       \\
Hindi   & 34 / 285 & 1.47 & 9.32 & 2.55 &72,107            & 16,517                 & 15,833        & 10,193       \\
Japanese & 34 / 292 & 1.73 & 11.23 & 2.58 &	109,528           & 15,200                 & 29,474         & 23,838       \\
\midrule
Total & -  - & 1.63  & 10.11 & 2.57 & 552,924           & 75,501                 & 148,047        & 110,047     \\
\bottomrule
\end{tabular}
\caption{Corpus and sentence level statistics of \presto, slices per language, including various linguistic phenomenon.  }
\label{tab:data_stats}
\end{table*}



Prior large multilingual datasets for conversational semantic parsing such as MTOP~\cite{mtop} and MASSIVE~\cite{massive} contain non-English conversations obtained by translating English conversations to other languages,\footnote{The translation was constrained in order to ensure the English source and the translation have the same semantic parse with argument values in English translated to the corresponding phrases in the other language.} resulting in \textit{unnatural} and synthetic utterances which are unlikely to be spoken by native speakers of the non-English language.
% For example, the Arabic translation in MASSIVE:
For example, an Arabic example in MASSIVE:\\
% \begin{RLtext}
% أحتاج إلى الحصول على خدمات الموقع هل يمكنك التحقق
% \end{RLtext}
\vspace{-2em}
\begin{figure}[h]
\centering
\includegraphics[width=7.5cm]{arabic.png}
\end{figure}

\vspace{-1em}
\noindent does not resemble native Arabic speech of any dialect. The corresponding English utterance in the MASSIVE dataset is: \textit{I need to have location services on can you check}

\begin{figure}[h]
\centering
\includegraphics[width=7.5cm]{cm.pdf}
\caption{Examples of Hindi-English (\textit{Send a screenshot along with the SMS.}), Spanish-English (\textit{Sparkpeople, record my meal from this morning.}), and German-English (\textit{I want to continue my workout on the IPhone.}) code-switched utterances from \presto{}.}
\label{fig:codeswitching}
\end{figure}
\paragraph{Code-Switched Utterances.}
Multilingual users often mix words from two languages in the same utterance, as shown in Fig.~\ref{fig:codeswitching}.
Recognizing the difficulty of parsing such utterances, we asked bilingual data contributors to provide mixed utterances, and marked these examples with a special tag `\texttt{code-switching}' to enable isolated research focused on addressing this phenomenon. 
Table \ref{tab:data_stats} shows the number of code-switched examples in each language.\footnote{We use a language ID classifier to determine which of the two languages is dominant in a given utterance.}


\paragraph{User Revisions.} 

The example in Fig.~\ref{fig:example} highlights another important aspect of realistic conversations: users often revise their request to the assistant. We have 4 such tags in our dataset which are all grouped under the broader user-revision linguistic phenomenon. The tags are: \texttt{correct-action}, \texttt{correct-argument}, \texttt{within-turn-correction}, and \texttt{cancel- action}.  
Sometimes the revision is necessary due to a mistake made by the virtual assistant, as in Fig.~\ref{fig:example} (\textit{correct-argument}). At other times, the user may simply change their mind about what they want the assistant to do which may happen in the same utterance, e.g., ``\textit{Add bread no no wait add wheat bread to my shopping list}'', or in a later utterance, e.g., ``\textit{Sorry add wheat bread instead}.''
Another common revision users make is to cancel their requests, e.g., ``\textit{Sorry don't add anything}''. Fig.~\ref{fig:example} shows examples of some of these revisions. Table~\ref{tab:data_stats} reflects the number of examples with user revisions in the last utterance for each language in \presto{}.



\paragraph{Disfluencies.}
Due to the spoken nature of most conversations with virtual assistants, user utterances frequently have disfluencies such as repeated phrases and filler words, as shown in Fig.~\ref{fig:disfluency}. Table~\ref{tab:data_stats} shows the number of examples with disfluencies for each language in \presto{}.

\begin{figure}[h]
\centering

\includegraphics[width=7.5cm]{disf.pdf}
\caption{Examples of utterances in English, Japanese (\textit{Gas bill, I want to pay the gas bill.}), and French (\textit{Order an Ub an Uber for the cinema}) with disfluencies (filler words or repetitions) from \presto{}.}
\label{fig:disfluency}
\end{figure}

\paragraph{Structured Context.}
A typical user interacts with virtual assistants within a virtual world (henceforth \emph{context}) which may consist of various structured objects, e.g., a list of contacts on the user's phone, a shopping list, a to-do list, notes created by the user and others sent from family and friends, a recurrent 5:45 am alarm to get out of bed every weekday, 4 smart bulbs, 12 smart plugs, a smart sprinkler in the backyard, etc.

Depending on the query, this context may or may not be needed to correctly interpret user utterances; it is often hard for semantic parsing models to determine which part of the context matters for a given utterance, if any. Moreover, the modeling solutions should have the ability to model (and ignore) such structured information. Fig.~\ref{fig:example} (\texttt{correct-argument}) shows an example in our dataset where the virtual assistant fails to relate the `\textit{Henry Moore}' to the user's contacts in the structured contact and incorrectly interprets the name as a reference to the ``\textit{Henry Moore Foundation}''. 
The gold parse provided in the dataset shows the correct interpretation for the last utterance, leveraging the structured context.

\begin{table*}[]
\resizebox{\textwidth}{!}{\begin{tabular}{lcccc}
\toprule
Dataset & \# Languages & Multi-turn & Explicit  & Labeled Conversational  \\
 & & & Context & Phenomena \\
\midrule
\textbf{\presto{}} & 6 &  \checkmark & \checkmark & Code-Switching, Disfluencies, \\
 & & & & User-Revisions, Coreferences$^*$\\
 \midrule
TreeDST \citep{treedst} & 1 & \checkmark & \xmark & Coreferences \\
SMCalFlow \citep{smcalflow} & 1 & \checkmark & \xmark & Coreferences, User-Revisions \\
MultiWOZ \citep{multiwoz}& 1 & \checkmark &  \xmark & Coreferences$^*$\\
% DSTC2 \citep{dstc2} & 1 &  \checkmark \\
DSTC10 \citep{dstc10} & 1 & \checkmark & \xmark & Disfluencies$^*$ \\
TOP \citep{top}& 1 & \xmark  & \xmark & - \\
MTOP \citep{mtop}& 6 & \xmark & \xmark & - \\
MASSIVE \citep{massive}& 51 & \xmark & \xmark & - \\
SNIPS \citep{snips} & 1 & \xmark & \xmark & -\\
MultiATIS++ \citep{multiatis} & 9 & \xmark & \xmark & -\\
\bottomrule
\end{tabular}
}
\caption{ \small Comparison of \presto{} with other recent datasets on or related to semantic parsing of task-oriented dialog. * denotes phenomena which is implicitly present in the dataset but not explicitly tagged.
% \ag{please add citations to all dataset or none.} \sid{done}
}
\label{tab:data_comparison}
\end{table*}


\paragraph{Other Statistics.}
\presto{} covers 34 intents, including 
the intent `\texttt{Other}' which is a catch-all intent used for all out-of-scope utterances and constitutes 11.3\% of all examples in the dataset.
The average number of the previous user turns is around 2.6, which is representative of the relatively short conversations users tend to have with virtual assistants today.
Table \ref{tab:data_stats} provides summary statistics for the number of tokens in labeled user utterances in each language, demonstrating the large variety in utterance lengths.


\paragraph{Related Datasets}
There have been multiple related datasets which are listed in Table \ref{tab:data_comparison}. The existing task-oriented dialog datasets can be split by capabilities and representations. The salient capabilities are -- single turn or conversational, monolingual or multilingual, presence of conversational phenomenon, and  single-domain vs multi-domain. In terms of representations, there are 2 popular paradigms, a direct semantic parse  typically consisting of intents and slots vs maintaining an explicit dialog state. With the popularity of pretrained text-to-text models, the natural modeling choice for all of these has converged to similar models. In fact, in terms of models, our context representation can be seen as a constant dialog state for that conversation. 



\section{Data Collection}\label{sec:data_collection}
The scope for data collection is based on the App Actions built-in intents, by Google, available for 3rd-party Android Developers, which cover a wide spectrum of domains including finance, health, fitness, food, transportation, social, communications, shopping among others.\footnote{\url{https://developer.android.com/reference/app-actions/built-in-intents}} Our dataset has over 34 unique intents and 370 arguments across 6 languages. Table~\ref{tab:intent_distribution} talks about the intent distribution in the dataset.  
We use two complementary approaches to collect contextual and non-contextual examples.

\paragraph{Contextual Examples.}
Data contributors were asked to have conversations with a virtual assistant simulator based on different sets of instructions for each language and for each type of targeted linguistic phenomenon (e.g., \texttt{code-switching}).
Each data collection request targeted a single intent and was initialized with potentially relevant structured context, e.g., contacts, notes or lists, which can optionally be referenced by data contributors in the utterances they produce.
The initial seed contacts, notes and lists are authored by native speakers and are shared for all examples in one request.
For example, when initiating a call, data contributors may use one of the names in the contacts (e.g., ``\textit{call mom}''), but they may also use the name of a business not in the contact list (e.g., ``\textit{call mcdonalds}'').

Each user utterance from the collected conversations is exported as a candidate example for semantic annotation by trained expert linguists who produce two types of annotations.\footnote{Due to limited annotation capacity, we were not able to annotate all candidate examples and we prioritized candidate examples which have at least one previous turn. Candidates which were not included were discarded and are not included in this  data release.}
First, linguists decide whether the example is in scope or not.
Out of scope examples include incoherent or nonsensical utterances, unsupported arguments and out-of-scope intents.
Then, they choose the intent and nested arguments as shown in Fig.~\ref{fig:example}. 
Finally, they added semantic tags which indicate which linguistic phenomena of interest are expressed in this example, such as, \texttt{correct-argument}, \texttt{cancel-action}, \texttt{code-switching}, etc.


\paragraph{Non-Contextual Examples.}
A key challenge in representing context in real world semantic parsing is that most examples can be parsed without using it, for example utterances like ``\textit{play music}'' are typically non-contextual but an utterance like ``\textit{play workout music}'' might be contextual if the user has a playlist called ``\textit{workout}''. This makes it a challenging modeling task; context ideally should only be used when it is relevant. On the one hand, exclusively using relevant contextual examples when training a conversational semantic parser results in models which are over-sensitive to irrelevant contextual features.
On the other hand, exclusively using non-contextual training examples where the user utterance is not related to previous turns or structured context teaches the model that the context does not matter, and may result in emphasizing arbitrary contextual features which happen to correlate with some prediction in the training data. 

A balanced approach is to include both types of examples in training, and use targeted evaluation sets to better understand the impact of different ways of representing context.
We obtain non-contextual examples by asking linguists to produce single-turn utterances along with their gold parses, then pair them with samples of the structured context and previous turns from the contextual conversations discussed earlier to make the representation uniform. The pairing method is described in the post-processing section. We denote such synthetic generated context in the data by the tag \texttt{context: synthetic}. 

\paragraph{Data Quality.} 
We adopt several mechanisms for boosting the data quality of collected data. All our data collection is done in 2 phases. Phase 1 is data collection, where annotators come up with queries pertaining to a scenario. Phase 2 is the actual semantic parse annotation for the query. 

For non-contextual examples, we use a crowd compute like platform with native speakers to generate and annotate the data. The process is described below:
\begin{enumerate} 
    \item \texttt{\textbf{[Phase 1.1]}} Annotator 1 authors a query (contextual or non-contextual). 
    \item \texttt{\textbf{[Phase 1.2]}} Annotators 2, 3, 4 independently validate the query (scope judgment). If all 2, 3, 4 agree that the query is valid, then the query goes to phase 2. Upon any disagreement, the example is discarded.
    \item \texttt{\textbf{[Phase 2.1]}} Annotators 5 and 6 annotate the query independently. If their annotations match, the query is considered resolved along with the annotation.  
    \item \texttt{\textbf{[Phase 2.2]}}  If 5, 6 disagree, the query is sent to annotator 7 who annotates the query. If 7 matches any of 5 or 6, then the query is resolved to the matching annotation else the example is discarded. 
\end{enumerate}
As can be seen, the overall query generation and annotation process is similar for contextual and non-contextual examples. However, contextual examples require more care in annotation therefore we additionally ask trained in-house linguists who also speak the language to verify the plausibility of each example and annotations and exclude ones that are out of scope or unclear as discussed earlier.

In order to verify the efficacy of our approach, we manually analyze a random sample of $6K$ examples and evaluate the correctness of the parse and the plausibility of the user utterance.
The three labels available for assessing the semantic parse quality are: `\texttt{Accuracy:GOOD}', `\texttt{Accuracy:BAD:INTENT}', `\texttt{Accuracy:BAD:SLOT}'.
The two labels available for assessing the utterance are: `\texttt{Acceptability:HIGH}' and `\texttt{Acceptability:LOW}'.

We use the \texttt{Accuracy:GOOD} label when the top-level intent and all slot values are accurate according to  the annotation guidelines.
\texttt{Accuracy:BAD:INTENT} is used when an intent is not accurate based on the annotation guidelines, while \texttt{Accuracy:BAD:SLOT} is used when the intent is accurate but one or more slot values are not accurate.
We use the \texttt{Acceptability:HIGH} label when the utterance is likely to be spoken or typed by Assistant users, and is grammatical and contextually correct, otherwise we use \texttt{Acceptability:BAD}.
Before post-processing, we observe that the annotators agree on utterance plausibility (acceptability) for 92.9\% of  examples,
and agree on full semantic parse annotation (accuracy) for 90.7\% of examples.  

The breakdown of intent and argument errors in contextual examples can be found in Table~\ref{tab:errors} for five of the six languages (Spanish was not included in this analysis due to capacity constraints). 

\begin{table}[h]
\centering
\begin{tabular}{lcc} 
\toprule
Language & Bad Intent & Bad Argument \\\midrule
German & 2.8\% & 3.7\% \\
English & 2.9\% & 8.5\% \\
French & 0.0\% & 2.5\% \\
Hindi & 1.9\% & 4.8\% \\
Japanese & 4.3\% & 2.1\%
\\\bottomrule
\end{tabular}
\caption{Type of accuracy errors in  examples by language from our human evaluation.}
\label{tab:errors}
\end{table}

\paragraph{Post-processing.}
One of the common annotation errors we found is that code-switched examples often had the wrong language ID (\texttt{LangID}) associated with them. To address this, we use a language ID classifier to determine the dominant language in code-switched examples.

Some arguments (due to fine-grained domain modeling) were often confused for other arguments which have similar (but distinct) semantics, e.g., \texttt{ListItem} vs. \texttt{ExistingListItem}, resulting in inconsistent annotations and contributing to the error rate discussed earlier.
To address this, we merged pairs of argument names where the distinction rarely matters in practice.


To make our data uniform, we add synthetic context and previous turns to non-contextual examples. Examples which are missing previous turns are augmented to reuse previous turns from other examples by first sampling the number of previous turns using a negative binomial distribution with $r=5, p=0.8$, then sampling one of the collected conversations which has this many turns. 
Examples which are missing a given type of structured context, e.g., contacts, are augmented by randomly sampling contact names from related argument values specified by our data contributors, which results in realistic and diverse contexts.


We do another quality check to measure Accuracy and Acceptability after post-processing by randomly sampling 500 queries from each language (except Spanish due to capacity constraints). We ask 3 independent annotators (same as the earlier quality check) to rate the queries. After post-processing, we find that linguists agree, defined by 2 out of 3 annotators independently agreeing, on utterance plausibility (acceptability) for \textbf{98.8\%} of  examples and agree on full semantic parse annotation (accuracy) for \textbf{93.24\%} of examples.  This shows our post-processing decreased the data noise significantly. 


\section{Experiments}\label{sec:experiments}
In this section, we demonstrate a few experimental setups which are enabled by \presto{}, provide baseline results for future work, and summarize our findings.

\subsection{Setup}\label{sec:setup}
Each example in the dataset is designated as train, development or test with respective probabilities 0.50, 0.15 and 0.35, which enables us to have large test sets even when doing focused evaluation on a particular phenomenon of interest as in \S\ref{sec:experiments_code_switching}.
All data splits are provided as part of the data release.
We use the \texttt{t5x} library \cite{Roberts2022ScalingUM} to fine-tune mT5's public checkpoint \cite{Xue2020mT5AM} on the train portion of the data (unless otherwise stated in k-shot experiments).

For all experiments, we report \textbf{exact match accuracy} of the predicted semantic parse, which gives the model one point for each example where the intent and all the arguments are correctly predicted and zero points otherwise.
All experiments are based on the mT5-Base model (580M parameters),\footnote{\url{https://github.com/google-research/t5x/blob/main/t5x/examples/t5/mt5/base.gin}}
except the scaling experiments in \S\ref{sec:experiments_model_scaling} which demonstrate the effect of model scaling to mT5-Large (1.2B parameters), mT5-XL (3.7B parameters), and mT5-XXL
(13B parameters).
All models (except the monolingual models discussed in \S\ref{sec:experiments_monolingual}) are fine-tuned on the union of training examples from all languages in \presto{}.
Few shot experiments for code switching (\S\ref{sec:experiments_code_switching}), user revisions (\S\ref{sec:experiments_revisions}) and disfluencies (\S\ref{sec:experiments_disfluencies}) all share the same training set, e.g., the 5-shot model across all three sections consist of 5 code-switching examples, 5 cancellations, 5 within-turn corrections, 5 intent cross-turn corrections, 5 argument cross-turn corrections and 5 disfluency examples, in addition to all examples which do not represent any of these phenomena.

\paragraph{Features.} 
\label{para:features}
The input sequence fed to the model consists of the last user utterance which needs to be parsed, followed by previous turns (both user and assistant turns) in reverse chronological order, followed by some representation of the structured context (more on this in \S\ref{sec:context}), with a separator token between consecutive fields. An example is shown in Figure~\ref{fig:data_serialization}.

\begin{figure}[]
   {\small \textbf{Input:} \\ 
    \texttt{No, Henry Moore in my contacts | Calling Henry Moore Foundation (Henry Moore Studios \& Gardens) | Place a call to Henry Moore [SEP] Lists: School Supply, Cleaning, ... [SEP] Contacts: Charlotte Taylor, Henry Moore, ... [SEP] Notes: Meeting tomorrow, ...}
    \\
    \\
    {\small \textbf{Output:}\\
    \texttt{Initiate\_call( callee = Personal\_contact( person = Henry Moore ) ) }
    }%
}

    \caption{Example serialization and the output of the features in the dataset. The top box shows the encoding for the first input in Fig.~\ref{fig:example}. The output sequence consists of a depth-first traversal of the semantic parse, is also shown.  }
    \label{fig:data_serialization}
\end{figure}


\paragraph{Hyper-parameters.}
We do a rough hyperparameter tuning and fine-tune the model for 20K steps. We use the Adafactor optimizer with 0.8 decay rate and 0 step offset. We have a max input length of 512 for training and 1024 for inference and a batch size of 128. 
\input{tables/k_shot.tex}
\subsection{Overall Results}
Before diving into focused evaluations on various phenomena of interest, it is instructive to examine the overall model performance on this dataset when trained on 100, 1K, 10K examples, as well as the full the training set.
The results are shown in Fig.~\ref{fig:data-scaling-main-fig}, and they demonstrate a linear increase in model performance as the number of training examples grows exponentially.



Next, we turn our attention to the different linguistic phenomena represented in the dataset and assess how difficult it is for mT5-based models to parse examples with these phenomena. 
Table \ref{tab:0shotphenomena} shows the exact match accuracy for a multilingual model trained on all dataset examples which are not marked for the phenomena discussed earlier.
Each column corresponds to a different test set where all test examples either have user revisions, disfluent utterances, code switching or none of the above.
Unsurprisingly, the results demonstrate that models tend to perform much worse on examples where these phenomena are represented, when the model is not exposed to such phenomena in training.
In contrast, when the model is exposed to a lot of training examples with these phenomena, as in Table \ref{tab:fullshotphenomena}, we do not observe a large gap in performance across the different test sets, except for code-switching where the average performance remains significantly lower despite the high representation of code-switched examples in the training set.

% TODO: Add size of each test set
\begin{table*}[h]
\centering
%\setlength{\tabcolsep}{5pt} % Default value: 6pt
\begin{tabular}{c c c c c c }
\toprule
Language     & No Phenomenon        & User Revisions      &  Disfluency    & Code-Switching   \\
\midrule
German & 81.76 & 23.56 & 64.91 & 56.81 \\ 
English & 85.16 & 22.45 & 63.78 & 67.43 \\ 
Spanish & 81.26 & 19.94 & 61.31 & 62.60 \\ 
Japanese & 83.41 & 22.95 & 54.31 & 71.71 \\ 
Hindi & 76.60 & 26.94 & 54.89 & 58.10 \\ 
French & 84.27 & 15.25 & 67.42 & 64.21 \\ 
\hdashline
Overall & 82.38 &21.51  & 61.26 & 63.16 \\
\bottomrule
\end{tabular}
\caption{Exact match accuracy results (\%) on the test set for the zero-shot multilingual mT5 model (i.e. trained on all examples with no marked phenomena).}
\label{tab:0shotphenomena}
\end{table*}


\subsection{Code Switching}\label{sec:experiments_code_switching}
How do we parse code-switched utterances? To start answering this question, we create a focused test set which only includes the test set examples marked for code-switching.
Specifically, we want to build an intuition for how well an LLM-based model parses code-switched examples in the test set, when fine-tuned on a large number of multilingual examples but only a limited number of code-switching examples.
To this effect, we train several mT5-base models on different training sets which vary in the number of code-switched examples included in training: 0, 5, 25, 125, 625, 3125, and 15,625 examples, and compute their exact match accuracy on the code-switching test set in different languages. 

As shown in Fig.~\ref{fig:data-scaling-main-fig}, when no code-switching examples are used in training, the model's exact match accuracy ranges between 56-72\%, performing best on Japanese and worst on Hindi.  
Surprisingly, even at 0 shot we see a relatively high performance for this phenomenon. We hypothesize that this is due to using a \texttt{LangID} to identify code-switching utterances which may cause code-switched utterances to leak into the training set. 

\subsection{User Revisions}
\label{sec:experiments_revisions}
Similar to the previous section, we ask how many examples with user revisions the model needs to see in training to perform well on user revisions.
We train several mT5-base models on different training sets where the number of examples per user revisions subtype is limited to 0, 5, 25, 125, 625, 3125, and 15,625 examples each (i.e., 5  examples with intent corrections + 5 with argument corrections + 5 with within-turn corrections + 5 with cancellations).
While the training data are dominated by examples that do not have user revisions, the test set used in this section only includes examples with user revisions.

As shown in Fig.~\ref{fig:data-scaling-main-fig}, zero-shot exact match accuracy results range between 18-28\%. Unlike code switching, adding only 25 examples of each type of user revisions notably improves the performance on user revisions. 
After the first few examples, the performance improves linearly as the number of examples with relevant phenomena grows exponentially from 25 to 125 to 625, before it slows down when the number of examples reaches 15K.


\subsection{Disfluencies}
Here, we turn to disfluent utterances and try to estimate how well our models fare on them with various number of disfluent examples in the training set.
Fig.~\ref{fig:data-scaling-main-fig} shows the results for the focused test set with disfluent utterances.
In the zero-shot setup, the performance starts at a higher accuracy range for all languages, compared to use revisions.
As the number of training examples grows exponentially, we observe a linear increase in exact match accuracy for all languages.
\label{sec:experiments_disfluencies}

\subsection{Structured Context}\label{sec:context}
\presto{} provides an opportunity to examine how structured context in the user's virtual environment may be used to improve conversational parsing models.
In this section, we use a focused test set which highlights context by only including contextual examples in the test set (i.e., the first data collection approach discussed in \S\ref{sec:data_collection}).

We linearize and prepend the context tokens to the T5 parser input with \texttt{[SEP]} tokens as shown in the example presented in \ref{para:features}. Initially we simply linearized the entire context and added that as a feature. 
We noticed that naively prepending all the context had too much noise and was not resulting in performance improvements. Hence to represent context in the input sequence, we first identified context notes, contact names and list names which had a trigram similarity greater than 0.6 with the last user utterance and only used the last two turns. We call this `\emph{filtered context representation}' since it filters out parts of the context which are unlikely to provide useful information for the parser.

First, we examine how the performance increases on the focused test set as we include more contextual examples to the training set. 
In particular, we fine-tune the mT5-Base model with filtered context representation on all the non-contextual training examples in addition to 10, 100, 1K or 10K contextual training examples. Table~\ref{tab:contextual} provides average performance for 5 languages with and without context. 
We notice very little improvement with addition of the context features. Our interpretation for this result is that, even though the data contributors were encouraged to use the structured context when conversing with the virtual assistant, most utterances can be understood without referencing the structured context. 
For example, it is easy for the model to parse the utterance ``add apples to my shopping list'', with no knowledge about what lists the user have.(See \S\ref{sec:data_collection} for more details on contextual vs. non-contextual examples.)

\begin{table}[h]
\centering
\resizebox{7cm}{!}{
\begin{tabular}{c c c}
\toprule
Number  & Non-Contextual & Contextual \\
of Examples     & Model    & Model \\
\midrule

10 & 21.23 & 23.94 \\
100 & 40.84 & 41.90 \\
1000 & 64.29 & 64.93 \\
10000 & 80.28 & 80.50 \\
\bottomrule
\end{tabular}
}
\caption{Average performance (exact match accuracy (\%)) on the contextual test set across all the languages for contextual vs non contextual models. It can be seen that in low resource setup contextual models outperform non-contextual model, i.e. modeling context helps.}
\label{tab:contextual}
\end{table}



\subsection{Monolingual vs. Multilingual Models}\label{sec:experiments_monolingual}
In previous sections, we train a single parser on training examples from all languages in \presto{}, as proposed in \newcite{Ammar2016ManyLO}.
A more traditional approach is to train several monolingual models.
Figure \ref{fig:mono-vs-multi} shows how the monolingual models compare to multilingual models with varying amounts of training data.
Our results show that in lower data regimes there is a clear gap between monolingual and multilingual models, but when using all training instances in the dataset, monolingual and multilingual models converge towards similar performance, in terms of overall accuracy. 
\input{tables/multi_v_mono.tex}



\section{Conclusion}
We introduce \presto{}, a over 550K-examples, six-language dataset for parsing realistic task-oriented dialogs. \presto{} is enriched with a high representation of contextual examples, code switching, user revisions and disfluencies.  Our initial results demonstrate that it is possible for our models to perform well on various linguistic phenomena, but only when the model is exposed to a very large number of training examples with the given phenomenon. We observe that feeding a simple representation of the structured context to the model does not yield large improvements on contextual examples. We also notice that multilingual models perform better than monolingual models in lower data regimes. 


\paragraph{Open Questions.}
By introducing a new dataset with unique characteristics, this work deliberately asks more questions than it answers. Some of the obvious modeling questions which this dataset can help answer are: 
How do we train models which perform well on conversational phenomenon like code-switching, user revisions and disfluencies, with a relatively small number of training examples that exhibit these phenomena?
How do we identify examples for which context is critical to understanding the user's utterance?
How can structured context be represented in our models most optimally?
How can we increase cross-lingual supervision in multilingual models to make them more data efficient?
We hope that \presto{} is useful in answering these and other questions about conversational semantic parsing. 

\bibliography{references}




\section{Appendix}
We present the results on the overall dataset as well as the model scaling results in this section. Table ~\ref{tab:fullshotphenomena} presents results on the testsets if we use all the available training data. 


\input{tables/model_scaling.tex}

\subsection{Model Scaling}\label{sec:experiments_model_scaling}



The experiments presented so far use mT5-Base since it incurs lower cost than larger models and provides reasonable performance on many NLP tasks.
In this section, we examine larger model sizes impact the results on this dataset.
As a case study, we focus on the 125-shot models for code switching, disfluencies and user revisions.
Fig.~\ref{fig:model-scaling-main-fig} demonstrates the impact of increased model size on the overall test results.
As expected, the larger models consistently give better results but are still lower than the best results we were able to achieve by using the full training set with mT5-Base (discussed earlier in Fig.~\ref{fig:data-scaling-main-fig}).
Fig.~\ref{fig:model-scaling-main-fig} demonstrates a similar pattern for the focused evaluation on user revisions, and we see a similar pattern for other phenomena.
These results confirm that mT5-Base strikes a good balance between model size and performance, and we recommend using mT5-Base for extensions of this work.



\begin{table*}[] 
\centering
\begin{tabular}{lcccccc}
\toprule
\textbf{Intent}           &  \textbf{German}  & \textbf{English}  & \textbf{Spanish}  & \textbf{French } & \textbf{Hindi}  & \textbf{Japanese}   \\\midrule
\textbf{Add\_contact}           & 2779  & 2931  & 3382  & 3148  & 2122  & 3527   \\
\textbf{Add\_item\_to\_list}    & 944   & 2959  & 2913  & 986   & 1958  & 2503   \\
\textbf{BuyEventTickets}        & 2550  & 2463  & 2883  & 2842  & 1697  & 3049   \\
\textbf{Cancel}                 & 1479  & 3029  & 2061  & 1775  & 2318  & 2525   \\
\textbf{Cancel\_ride}           & 2252  & 2353  & 3239  & 3047  & 2053  & 3098   \\
\textbf{Check\_order\_status}   & 2700  & 2877  & 3448  & 3455  & 2136  & 3934   \\
\textbf{Create\_list}           & 844   & 594   & 745   & 1063  & 778   & 1023   \\
\textbf{Create\_note}           & 1566  & 1700  & 1903  & 1819  & 1568  & 1998   \\
\textbf{Find\_parking}          & 4562  & 1210  & 4387  & 2345  & 3117  & 2692   \\
\textbf{GetGenericBusinessType} & 3130  & 2613  & 2782  & 2904  & 1676  & 3139   \\
\textbf{Get\_bill}              & 2249  & 2466  & 2922  & 2582  & 1857  & 3275   \\
\textbf{Get\_health\_stats}     & 2477  & 2512  & 2827  & 2847  & 1982  & 2879   \\
\textbf{Get\_list}              & 701   & 705   & 698   & 676   & 856   & 761    \\
\textbf{Get\_message\_content}  & 2102  & 2142  & 2371  & 2343  & 1320  & 2879   \\
\textbf{Get\_note}              & 1737  & 1905  & 2022  & 2277  & 1978  & 2138   \\
\textbf{Get\_product}           & 2341  & 2076  & 2483  & 2438  & 1574  & 3344   \\
\textbf{Get\_security\_price}   & 2175  & 2333  & 2707  & 2425  & 1752  & 3245   \\
\textbf{Initiate\_call}         & 1822  & 4292  & 2239  & 1965  & 1807  & 2250   \\
\textbf{Log\_exercise}          & 2602  & 2708  & 2050  & 2663  & 1962  & 3049   \\
\textbf{Log\_nutrition}         & 2219  & 2459  & 2667  & 2605  & 1786  & 3204   \\
\textbf{Open\_app}              & 2488  & 2497  & 3367  & 3076  & 2069  & 3643   \\
\textbf{Order\_menu\_item}      & 224   & 860   & 659   & 736   & 155   & 740    \\
\textbf{Order\_ride}            & 2175  & 2342  & 2857  & 2992  & 1809  & 2993   \\
\textbf{Other}                  & 8457  & 17499 & 5650  & 10937 & 9163  & 13737  \\
\textbf{Pause\_exercise}        & 2821  & 2995  & 3109  & 3149  & 1830  & 3495   \\
\textbf{Pay\_bill}              & 2280  & 2304  & 2837  & 2823  & 1834  & 3048   \\
\textbf{Play\_game}             & 2640  & 2667  & 2889  & 3048  & 1941  & 3074   \\
\textbf{Post\_message}          & 2597  & 2224  & 3065  & 3278  & 2049  & 3529   \\
\textbf{Record\_video}          & 2609  & 2295  & 3317  & 3369  & 1934  & 3864   \\
\textbf{Resume\_exercise}       & 2367  & 2060  & 3136  & 2929  & 1991  & 2896   \\
\textbf{Send\_digital\_object}  & 2307  & 3127  & 2804  & 2652  & 2016  & 2905   \\
\textbf{Start\_exercise}        & 2719  & 3030  & 3272  & 3182  & 2031  & 3311   \\
\textbf{Stop\_exercise}         & 2719  & 2814  & 3207  & 3127  & 2078  & 3432   \\
\textbf{Take\_photo}            & 3950  & 2630  & 5266  & 4367  & 4910  & 4349   \\ \hline
\textbf{Overall}                  & 83584 & 95671 & 96164 & 95870 & 72107 & 109528 \\
\bottomrule
\end{tabular}
\caption{Intent distribution by language for \presto{}.}
\label{tab:intent_distribution}
\end{table*}


\begin{table*}[]
\centering
%\setlength{\tabcolsep}{3pt} % Default value: 6pt
\begin{tabular}{c c c c c }
\toprule
Language     & No Phenomenon        & User Revisions      &  Disfluency    & Code-Switching   \\
\midrule
German & 82.41 & 81.33 & 84.19 & 72.50 \\ 
English & 86.18 & 85.36 & 86.20 & 75.99 \\ 
Spanish & 82.95 & 82.26 & 82.81 & 72.22 \\ 
Japanese & 84.91 & 82.22 & 80.46 & 84.41 \\ 
Hindi & 78.30 & 73.24 & 75.49 & 73.24 \\ 
French & 85.42 & 82.99 & 85.82 & 75.98 \\ 
\hdashline
Overall & 83.66     & 81.85    & 82.92       & 75.88  \\
\bottomrule
\end{tabular}
\caption{Exact match accuracy results (\%) for the multilingual mT5 model trained on the full training set of \presto{}.}
\label{tab:fullshotphenomena}
\end{table*}


\clearpage

\end{document}


