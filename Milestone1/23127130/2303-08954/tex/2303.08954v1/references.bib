@inproceedings{zhang-etal-2018-personalizing,
    title = {{Personalizing Dialogue Agents: {I} have a dog, do you have pets too?}},
    author = "Zhang, Saizheng  and
      Dinan, Emily  and
      Urbanek, Jack  and
      Szlam, Arthur  and
      Kiela, Douwe  and
      Weston, Jason",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1205",
    doi = "10.18653/v1/P18-1205",
    pages = "2204--2213",
    abstract = "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
}

@inproceedings{gupta2021disfl,
    title = "{{Disfl-{QA}: A Benchmark Dataset for Understanding Disfluencies in Question Answering}}",
    author = "Gupta, Aditya  and
      Xu, Jiacheng  and
      Upadhyay, Shyam  and
      Yang, Diyi  and
      Faruqui, Manaal",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.293",
    doi = "10.18653/v1/2021.findings-acl.293",
    pages = "3309--3319",
}

@misc{hogan2021knowledge,
      title={{Knowledge Graphs}}, 
      author={Aidan Hogan and Eva Blomqvist and Michael Cochez and Claudia d'Amato and Gerard de Melo and Claudio Gutierrez and José Emilio Labra Gayo and Sabrina Kirrane and Sebastian Neumaier and Axel Polleres and Roberto Navigli and Axel-Cyrille Ngonga Ngomo and Sabbir M. Rashid and Anisa Rula and Lukas Schmelzeisen and Juan Sequeda and Steffen Staab and Antoine Zimmermann},
      year={2021},
      eprint={2003.02320},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{cassell2000human,
  title={{Human Conversation as a System Framework: Designing Embodied Conversational Agents}},
  author={Cassell, Justine and Bickmore, Tim and Campbell, Lee and Vilhjalmsson, Hannes and Yan, Hao},
  journal={Embodied Conversational Agents},
  pages={29--63},
  year={2000}
}

@article{agarwal2022cst5,
 url = {https://arxiv.org/abs/2211.07514},
  title={{CST5: Data Augmentation for Code-Switched Semantic Parsing}},
  author={Agarwal, Anmol and Gupta, Jigar and Goel, Rahul and Upadhyay, Shyam and Joshi, Pankaj and Aravamudhan, Rengarajan},
  journal={arXiv preprint arXiv:2211.07514},
  year={2022}
}

@misc{mo2017personalizing,
      title={{Personalizing a Dialogue System with Transfer Reinforcement Learning}}, 
      author={Kaixiang Mo and Shuangyin Li and Yu Zhang and Jiajun Li and Qiang Yang},
      year={2017},
      eprint={1610.02891},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{majumder-etal-2020-like,
    title = "{{Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions}}",
    author = "Majumder, Bodhisattwa Prasad  and
      Jhamtani, Harsh  and
      Berg-Kirkpatrick, Taylor  and
      McAuley, Julian",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.739",
    doi = "10.18653/v1/2020.emnlp-main.739",
    pages = "9194--9206",
    abstract = "Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.",
}

@inproceedings{welleck-etal-2019-dialogue,
title = "{{Dialogue Natural Language Inference}}",
    author = "Welleck, Sean  and
      Weston, Jason  and
      Szlam, Arthur  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1363",
    doi = "10.18653/v1/P19-1363",
    pages = "3731--3741",
    abstract = "Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model{'}s consistency.",
}

@misc{peng2021soloist,
      title={{SOLOIST: Building Task Bots at Scale with Transfer Learning and Machine Teaching}}, 
      author={Baolin Peng and Chunyuan Li and Jinchao Li and Shahin Shayandeh and Lars Liden and Jianfeng Gao},
      year={2021},
      eprint={2005.05298},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{petroni-etal-2019-language,
title = "{{Language Models as Knowledge Bases?}}",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.",
}

@article{rastogi2019towards,
title={{Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset}},
  author={Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
  journal={arXiv preprint arXiv:1909.05855},
  year={2019}
}

@inproceedings{taskmaster,
title	= {Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset},
author	= {Bill Byrne and Karthik Krishnamoorthi and Chinnadhurai Sankar and Arvind Neelakantan and Daniel Duckworth and Semih Yavuz and Ben Goodrich and Amit Dubey and Kyu-Young Kim and Andy Cedilnik},
year	= {2019}
}


@InProceedings{shalyminov2020fast,
author = {Shalyminov, Igor and Sordoni, Alessandro and Atkinson, Adam and Schulz, Hannes},
title = {Fast Domain Adaptation For Goal-Oriented Dialogue Using A Hybrid Generative-Retrieval Transformer},
booktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
year = {2020},
month = {April},
abstract = {Goal-oriented dialogue systems are now widely adopted in industry, where practical aspects of using them becomes of key importance. As such, it is expected from such systems to fit into a rapid prototyping cycle for new products and domains. For data-driven dialogue systems (especially those based on deep learning) that amounts to maintaining production-level performance having been provided with a few ‘seed’ dialogue examples, normally referred to as data efficiency. With extremely data-dependent deep learning methods, the most promising way to achieve practical data efficiency is transfer learning—i.e., leveraging a greater, highly represented data source for training a base model, then fine-tuning it to available in-domain data. In this paper, we present a hybrid generative-retrieval model that can be trained using transfer learning. By using GPT-2 as the base model and fine-tuning it to the multidomain MetaLWOz dataset, we obtain a robust dialogue model able to perform both response generation and ranking 1 . Combining both, it outperforms several competitive generative-only and retrieval-only baselines, measured by language modeling quality on MetaLWOz as well as in goal- oriented metrics (Intent/Slot Fl-scores) on the MultiWoz corpus.},
url = {https://www.microsoft.com/en-us/research/publication/fast-domain-adaptation-for-goal-oriented-dialogue-using-a-hybrid-generative-retrieval-transformer/},
}

@inproceedings{zang2020multiwoz,
  title={MultiWOZ 2.2: A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines},
  author={Zang, Xiaoxue and Rastogi, Abhinav and Sunkara, Srinivas and Gupta, Raghav and Zhang, Jianguo and Chen, Jindong},
  booktitle={Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, ACL 2020},
  pages={109--117},
  year={2020}
}

@article{Malaviya_Bhagavatula_Bosselut_Choi_2020, title={Commonsense Knowledge Base Completion with Structural and Semantic Context}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5684}, DOI={10.1609/aaai.v34i03.5684}, abstractNote={&lt;p&gt;Automatic KB completion for &lt;em&gt;commonsense&lt;/em&gt; knowledge graphs (e.g., ATOMIC and ConceptNet) poses unique challenges compared to the much studied conventional knowledge bases (e.g., Freebase). Commonsense knowledge graphs use free-form text to represent nodes, resulting in orders of magnitude more nodes compared to conventional KBs ( ∼18x more nodes in ATOMIC compared to Freebase (FB15K-237)). Importantly, this implies significantly sparser graph structures — a major challenge for existing KB completion methods that assume densely connected graphs over a relatively smaller set of nodes.&lt;/p&gt;&lt;p&gt;In this paper, we present novel KB completion models that can address these challenges by exploiting the structural and semantic context of nodes. Specifically, we investigate two key ideas: (1) learning from local &lt;em&gt;graph structure&lt;/em&gt;, using graph convolutional networks and automatic graph densification and (2) &lt;em&gt;transfer learning&lt;/em&gt; from pre-trained language models to knowledge graphs for enhanced contextual representation of knowledge. We describe our method to incorporate information from both these sources in a joint model and provide the first empirical results for KB completion on ATOMIC and evaluation with ranking metrics on ConceptNet. Our results demonstrate the effectiveness of language model representations in boosting link prediction performance and the advantages of learning from local graph structure (+1.5 points in MRR for ConceptNet) when training on subgraphs for computational efficiency. Further analysis on model predictions shines light on the types of commonsense knowledge that language models capture well.&lt;/p&gt;}, number={03}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Malaviya, Chaitanya and Bhagavatula, Chandra and Bosselut, Antoine and Choi, Yejin}, year={2020}, month={Apr.}, pages={2925-2933} }

@inproceedings{
    pezeshkpour2020revisiting,
    title={Revisiting Evaluation of Knowledge Base Completion Models},
    author={Pouya Pezeshkpour and Yifan Tian and Sameer Singh},
    booktitle={Automated Knowledge Base Construction},
    year={2020},
    url={https://openreview.net/forum?id=1uufzxsxfL},
    doi={10.24432/C53S3W}
}

@inproceedings{wu-etal-2020-tod,
    title = "{TOD}-{BERT}: Pre-trained Natural Language Understanding for Task-Oriented Dialogue",
    author = "Wu, Chien-Sheng  and
      Hoi, Steven C.H.  and
      Socher, Richard  and
      Xiong, Caiming",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.66",
    doi = "10.18653/v1/2020.emnlp-main.66",
    pages = "917--929",
    abstract = "The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.",
}

@INPROCEEDINGS{7078578,  author={X. {Li} and G. {Tur} and D. {Hakkani-Tür} and Q. {Li}},  booktitle={2014 IEEE Spoken Language Technology Workshop (SLT)},   title={Personal knowledge graph population from user utterances in conversational understanding},   year={2014},  volume={},  number={},  pages={224-229},  doi={10.1109/SLT.2014.7078578}}

@inproceedings{zhang2017tacred,
  author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017)},
  title = {Position-aware Attention and Supervised Data Improve Slot Filling},
  url = {https://nlp.stanford.edu/pubs/zhang2017tacred.pdf},
  pages = {35--45},
  year = {2017}
}

@inproceedings{nivre-etal-2016-universal,
    title = "{U}niversal {D}ependencies v1: A Multilingual Treebank Collection",
    author = "Nivre, Joakim  and
      de Marneffe, Marie-Catherine  and
      Ginter, Filip  and
      Goldberg, Yoav  and
      Haji{\v{c}}, Jan  and
      Manning, Christopher D.  and
      McDonald, Ryan  and
      Petrov, Slav  and
      Pyysalo, Sampo  and
      Silveira, Natalia  and
      Tsarfaty, Reut  and
      Zeman, Daniel",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L16-1262",
    pages = "1659--1666",
    abstract = "Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.",
}


@inproceedings{pappu-rudnicky-2014-knowledge,
    title = "Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems",
    author = "Pappu, Aasish  and
      Rudnicky, Alexander",
    booktitle = "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL})",
    month = jun,
    year = "2014",
    address = "Philadelphia, PA, U.S.A.",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-4326",
    doi = "10.3115/v1/W14-4326",
    pages = "194--198",
}

@article{Freitag_2017,
   title={Beam Search Strategies for Neural Machine Translation},
   url={http://dx.doi.org/10.18653/v1/W17-3207},
   DOI={10.18653/v1/w17-3207},
   journal={Proceedings of the First Workshop on Neural Machine Translation},
   publisher={Association for Computational Linguistics},
   author={Freitag, Markus and Al-Onaizan, Yaser},
   year={2017}
}
s

@article{Chen_Lv_Wang_Zhu_Tan_Yu_2020, title={Schema-Guided Multi-Domain Dialogue State Tracking with Graph Attention Neural Networks}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6250}, DOI={10.1609/aaai.v34i05.6250}, abstractNote={&lt;p&gt;Dialogue state tracking (DST) aims at estimating the current dialogue state given all the preceding conversation. For multi-domain DST, the data sparsity problem is also a major obstacle due to the increased number of state candidates. Existing approaches generally predict the value for each slot independently and do not consider slot relations, which may aggravate the data sparsity problem. In this paper, we propose a &lt;strong&gt;S&lt;/strong&gt;chema-guided multi-domain dialogue &lt;strong&gt;S&lt;/strong&gt;tate &lt;strong&gt;T&lt;/strong&gt;racker with graph attention networks (&lt;strong&gt;SST&lt;/strong&gt;) that predicts dialogue states from dialogue utterances and schema graphs which contain slot relations in edges. We also introduce a graph attention matching network to fuse information from utterances and graphs, and a recurrent graph attention network to control state updating. Experiment results show that our approach obtains new state-of-the-art performance on both MultiWOZ 2.0 and MultiWOZ 2.1 benchmarks.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Lu and Lv, Boer and Wang, Chi and Zhu, Su and Tan, Bowen and Yu, Kai}, year={2020}, month={Apr.}, pages={7521-7528} }

@article{WuGetting2019,
        title={Getting To Know You: User Attribute Extraction from Dialogues},
        author={Wu, Chien-Sheng
          and Madotto, Andrea
          and Lin, Zhaojiang
          and Xu, Peng
          and Fung, Pascale},
        journal={arXiv preprint arXiv:1908.04621},
        year={2019}
}

@article{miller2017parlai,
  title={ParlAI: A Dialog Research Software Platform},
  author={{Miller}, A.~H. and {Feng}, W. and {Fisch}, A. and {Lu}, J. and {Batra}, D. and {Bordes}, A. and {Parikh}, D. and {Weston}, J.},
  journal={arXiv preprint arXiv:{1705.06476}},
  year={2017}
}

@inproceedings{koncel-kedziorski-etal-2019-text,
    title = "{T}ext {G}eneration from {K}nowledge {G}raphs with {G}raph {T}ransformers",
    author = "Koncel-Kedziorski, Rik  and
      Bekal, Dhanush  and
      Luan, Yi  and
      Lapata, Mirella  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1238",
    doi = "10.18653/v1/N19-1238",
    pages = "2284--2293",
    abstract = "Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.",
}

@inproceedings{moon-etal-2019-opendialkg,
    title = "{O}pen{D}ial{KG}: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
    author = "Moon, Seungwhan  and
      Shah, Pararth  and
      Kumar, Anuj  and
      Subba, Rajen",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1081",
    doi = "10.18653/v1/P19-1081",
    pages = "845--854",
    abstract = "We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog {\textless}-{\textgreater} KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning.",
}

@inproceedings{10.1145/3341981.3344241,
author = {Balog, Krisztian and Kenter, Tom},
title = {Personal Knowledge Graphs: A Research Agenda},
year = {2019},
isbn = {9781450368810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341981.3344241},
doi = {10.1145/3341981.3344241},
abstract = {Knowledge graphs, organizing structured information about entities, and their attributes and relationships, are ubiquitous today. Entities, in this context, are usually taken to be anyone or anything considered to be globally important. This, however, rules out many entities people interact with on a daily basis. In this position paper, we present the concept of personal knowledge graphs: resources of structured information about entities personally related to its user, including the ones that might not be globally important. We discuss key aspects that separate them for general knowledge graphs, identify the main challenges involved in constructing and using them, and define a research agenda.},
booktitle = {Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {217–220},
numpages = {4},
keywords = {personal knowledge graphs, personal information management, knowledge representation},
location = {Santa Clara, CA, USA},
series = {ICTIR '19}
}

@inproceedings{liu-etal-2019-knowledge-aware,
    title = "Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs",
    author = "Liu, Zhibin  and
      Niu, Zheng-Yu  and
      Wu, Hua  and
      Wang, Haifeng",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1187",
    doi = "10.18653/v1/D19-1187",
    pages = "1782--1792",
    abstract = "Two types of knowledge, triples from knowledge graphs and texts from documents, have been studied for knowledge aware open domain conversation generation, in which graph paths can narrow down vertex candidates for knowledge selection decision, and texts can provide rich information for response generation. Fusion of a knowledge graph and texts might yield mutually reinforcing advantages, but there is less study on that. To address this challenge, we propose a knowledge aware chatting machine with three components, an augmented knowledge graph with both triples and texts, knowledge selector, and knowledge aware response generator. For knowledge selection on the graph, we formulate it as a problem of multi-hop graph reasoning to effectively capture conversation flow, which is more explainable and flexible in comparison with previous works. To fully leverage long text information that differentiates our graph from others, we improve a state of the art reasoning algorithm with machine reading comprehension technology. We demonstrate the effectiveness of our system on two datasets in comparison with state-of-the-art models.",
}

@inproceedings{tuan-etal-2019-dykgchat,
    title = "{D}y{K}g{C}hat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs",
    author = "Tuan, Yi-Lin  and
      Chen, Yun-Nung  and
      Lee, Hung-yi",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1194",
    doi = "10.18653/v1/D19-1194",
    pages = "1855--1865",
    abstract = "Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions.",
}

@article{Sap_Le_Bras_Allaway_Bhagavatula_Lourie_Rashkin_Roof_Smith_Choi_2019, title={ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4160}, DOI={10.1609/aaai.v33i01.33013027}, abstractNote={&lt;p&gt;We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed &lt;em&gt;if-then&lt;/em&gt; relations with variables (e.g., “&lt;em&gt;if&lt;/em&gt; X pays Y a compliment, &lt;em&gt;then&lt;/em&gt; Y will likely return the compliment”). We propose nine &lt;em&gt;if-then&lt;/em&gt; relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of &lt;em&gt;if-then&lt;/em&gt; relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sap, Maarten and Le Bras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A. and Choi, Yejin}, year={2019}, month={Jul.}, pages={3027-3035} }

@inproceedings{
    Holtzman2020The,
    title={The Curious Case of Neural Text Degeneration},
    author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rygGQyrFvH}
}

@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@inproceedings{zhang-etal-2020-dialogpt,
    title = "{DIALOGPT} : Large-Scale Generative Pre-training for Conversational Response Generation",
    author = "Zhang, Yizhe  and
      Sun, Siqi  and
      Galley, Michel  and
      Chen, Yen-Chun  and
      Brockett, Chris  and
      Gao, Xiang  and
      Gao, Jianfeng  and
      Liu, Jingjing  and
      Dolan, Bill",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-demos.30",
    doi = "10.18653/v1/2020.acl-demos.30",
    pages = "270--278",
    abstract = "We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",
}

@inproceedings{wadden-etal-2019-entity,
    title = "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    author = "Wadden, David  and
      Wennberg, Ulme  and
      Luan, Yi  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1585",
    doi = "10.18653/v1/D19-1585",
    pages = "5784--5789",
    abstract = "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
}

@misc{tigunova2019listening,
      title={Listening between the Lines: Learning Personal Attributes from Conversations}, 
      author={Anna Tigunova and Andrew Yates and Paramita Mirza and Gerhard Weikum},
      year={2019},
      eprint={1904.10887},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{tigunova-etal-2020-charm,
    title = "{CHARM}: Inferring Personal Attributes from Conversations",
    author = "Tigunova, Anna  and
      Yates, Andrew  and
      Mirza, Paramita  and
      Weikum, Gerhard",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.434",
    doi = "10.18653/v1/2020.emnlp-main.434",
    pages = "5391--5404",
    abstract = "Personal knowledge about users{'} professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.",
}

@inproceedings{
    alt2019improving,
    title={Improving Relation Extraction by Pre-trained Language Representations},
    author={Christoph Alt and Marc H{\"u}bner and Leonhard Hennig},
    booktitle={Automated Knowledge Base Construction (AKBC)},
    year={2019},
    url={https://openreview.net/forum?id=BJgrxbqp67}
}

@misc{hosseiniasl2020simple,
      title={A Simple Language Model for Task-Oriented Dialogue}, 
      author={Ehsan Hosseini-Asl and Bryan McCann and Chien-Sheng Wu and Semih Yavuz and Richard Socher},
      year={2020},
      eprint={2005.00796},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yu-etal-2020-dialogue,
    title = "Dialogue-Based Relation Extraction",
    author = "Yu, Dian  and
      Sun, Kai  and
      Cardie, Claire  and
      Yu, Dong",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.444",
    doi = "10.18653/v1/2020.acl-main.444",
    pages = "4927--4940",
    abstract = "We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https://dataset.org/dialogre/.",
}


@inproceedings{10.5555/3298023.3298212,
author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
title = {ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
year = {2017},
publisher = {AAAI Press},
abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings.ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use.When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4444–4451},
numpages = {8},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{lu-etal-2019-goal,
    title = "Goal-Oriented End-to-End Conversational Models with Profile Features in a Real-World Setting",
    author = "Lu, Yichao  and
      Srivastava, Manisha  and
      Kramer, Jared  and
      Elfardy, Heba  and
      Kahn, Andrea  and
      Wang, Song  and
      Bhardwaj, Vikas",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-2007",
    doi = "10.18653/v1/N19-2007",
    pages = "48--55",
    abstract = "End-to-end neural models for goal-oriented conversational systems have become an increasingly active area of research, though results in real-world settings are few. We present real-world results for two issue types in the customer service domain. We train models on historical chat transcripts and test on live contacts using a human-in-the-loop research platform. Additionally, we incorporate customer profile features to assess their impact on model performance. We experiment with two approaches for response generation: (1) sequence-to-sequence generation and (2) template ranking. To test our models, a customer service agent handles live contacts and at each turn we present the top four model responses and allow the agent to select (and optionally edit) one of the suggestions or to type their own. We present results for turn acceptance rate, response coverage, and edit rate based on approximately 600 contacts, as well as qualitative analysis on patterns of turn rejection and edit behavior. Top-4 turn acceptance rate across all models ranges from 63{\%}-80{\%}. Our results suggest that these models are promising for an agent-support application.",
}

@article{Luo_Huang_Zeng_Nie_Sun_2019, title={Learning Personalized End-to-End Goal-Oriented Dialog}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4654}, DOI={10.1609/aaai.v33i01.33016794}, abstractNote={&lt;p&gt;Most existing works on dialog systems only consider conversation content while neglecting the personality of the user the bot is interacting with, which begets several unsolved issues. In this paper, we present a personalized end-to-end model in an attempt to leverage personalization in goal-oriented dialogs. We first introduce a PROFILE MODEL which encodes user profiles into distributed embeddings and refers to conversation history from other similar users. Then a PREFERENCE MODEL captures user preferences over knowledge base entities to handle the ambiguity in user requests. The two models are combined into the PERSONALIZED MEMN2N. Experiments show that the proposed model achieves qualitative performance improvements over state-of-the-art methods. As for human evaluation, it also outperforms other approaches in terms of task completion rate and user satisfaction.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Luo, Liangchen and Huang, Wenhao and Zeng, Qi and Nie, Zaiqing and Sun, Xu}, year={2019}, month={Jul.}, pages={6794-6801} }


@inproceedings{ijcai2018-0595,
  title     = {Assigning Personality/Profile to a Chatting Machine for Coherent Conversation Generation},
  author    = {Qiao Qian and Minlie Huang and Haizhou Zhao and Jingfang Xu and Xiaoyan Zhu},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {4279--4285},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/595},
  url       = {https://doi.org/10.24963/ijcai.2018/595},
}

@article{pei2021cooperative,
  title={A Cooperative Memory Network for Personalized Task-oriented Dialogue Systems with Incomplete User Profiles},
  author={Pei, Jiahuan and Ren, Pengjie and de Rijke, Maarten},
  journal={arXiv preprint arXiv:2102.08322},
  year={2021}
}


@inproceedings{mazare-etal-2018-training,
    title = "Training Millions of Personalized Dialogue Agents",
    author = "Mazar{\'e}, Pierre-Emmanuel  and
      Humeau, Samuel  and
      Raison, Martin  and
      Bordes, Antoine",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1298",
    doi = "10.18653/v1/D18-1298",
    pages = "2775--2779",
    abstract = "Current dialogue systems fail at being engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and only contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.",
}

@misc{joshi2017personalization,
      title={Personalization in Goal-Oriented Dialog}, 
      author={Chaitanya K. Joshi and Fei Mi and Boi Faltings},
      year={2017},
      eprint={1706.07503},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{li-etal-2016-persona,
    title = "A Persona-Based Neural Conversation Model",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Spithourakis, Georgios  and
      Gao, Jianfeng  and
      Dolan, Bill",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1094",
    doi = "10.18653/v1/P16-1094",
    pages = "994--1003",
}

@article{porter1980algorithm,
  added-at = {2015-08-24T21:55:54.000+0200},
  author = {Porter, Martin F},
  biburl = {https://www.bibsonomy.org/bibtex/29ed77bc4c2a58578b5f6a7766064a352/thoni},
  interhash = {5cc5000d0cbdf957fd74a27ecbc3c2f7},
  intrahash = {9ed77bc4c2a58578b5f6a7766064a352},
  journal = {Program},
  keywords = {porter snowball stemming},
  number = 3,
  pages = {130--137},
  publisher = {MCB UP Ltd},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {An algorithm for suffix stripping},
  volume = 14,
  year = 1980
}

@Book{wordnet,
    title = {WordNet: An Electronic Lexical Database},
    author = {Christiane Fellbaum},
    year = {1998},
    publisher = {Bradford Books},
}

@misc{wolf2019transfertransfo,
      title={TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents}, 
      author={Thomas Wolf and Victor Sanh and Julien Chaumond and Clement Delangue},
      year={2019},
      eprint={1901.08149},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{roller2020recipes,
      title={Recipes for building an open-domain chatbot}, 
      author={Stephen Roller and Emily Dinan and Naman Goyal and Da Ju and Mary Williamson and Yinhan Liu and Jing Xu and Myle Ott and Kurt Shuster and Eric M. Smith and Y-Lan Boureau and Jason Weston},
      year={2020},
      eprint={2004.13637},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Nayak_Ng_2020, title={Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6374}, DOI={10.1609/aaai.v34i05.6374}, abstractNote={&lt;p&gt;A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Nayak, Tapas and Ng, Hwee Tou}, year={2020}, month={Apr.}, pages={8528-8535} }

@inproceedings{Bosselut2019COMETCT,
 author = {Antoine Bosselut and Hannah Rashkin and Maarten Sap and Chaitanya Malaviya and Asli Çelikyilmaz and Yejin Choi},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
 title = {COMET: Commonsense Transformers for Automatic Knowledge Graph Construction},
 year = {2019}
}

@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}


@misc{decao2021autoregressive,
      title={Autoregressive Entity Retrieval}, 
      author={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},
      year={2021},
      eprint={2010.00904},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={A. Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{mostafazadeh-etal-2020-glucose,
    title = "{GLUCOSE}: {G}enera{L}ized and {CO}ntextualized Story Explanations",
    author = "Mostafazadeh, Nasrin  and
      Kalyanpur, Aditya  and
      Moon, Lori  and
      Buchanan, David  and
      Berkowitz, Lauren  and
      Biran, Or  and
      Chu-Carroll, Jennifer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.370",
    doi = "10.18653/v1/2020.emnlp-main.370",
    pages = "4569--4586",
    abstract = "When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of {\textasciitilde}670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE{'}s rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans{'} mental models.",
}

@inproceedings{10.1145/3366423.3380064,
author = {Rongali, Subendhu and Soldaini, Luca and Monti, Emilio and Hamza, Wael},
title = {Don’t Parse, Generate! A Sequence to Sequence Architecture for Task-Oriented Semantic Parsing},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380064},
doi = {10.1145/3366423.3380064},
abstract = {Virtual assistants such as Amazon Alexa, Apple Siri, and Google Assistant often rely on a semantic parsing component to understand which action(s) to execute for an utterance spoken by its users. Traditionally, rule-based or statistical slot-filling systems have been used to parse “simple” queries; that is, queries that contain a single action and can be decomposed into a set of non-overlapping entities. More recently, shift-reduce parsers have been proposed to process more complex utterances. These methods, while powerful, impose specific limitations on the type of queries that can be parsed; namely, they require a query to be representable as a parse tree. In this work, we propose a unified architecture based on Sequence to Sequence models and Pointer Generator Network to handle both simple and complex queries. Unlike other works, our approach does not impose any restriction on the semantic parse schema. Furthermore, experiments show that it achieves state of the art performance on three publicly available datasets (ATIS, SNIPS, Facebook TOP), relatively improving between 3.3% and 7.7% in exact match accuracy over previous systems. Finally, we show the effectiveness of our approach on two internal datasets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2962–2968},
numpages = {7},
keywords = {Natural Language Understanding, Voice Assistants, Sequence to Sequence models, Semantic Parsing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@misc{sui2020joint,
      title={Joint Entity and Relation Extraction with Set Prediction Networks}, 
      author={Dianbo Sui and Yubo Chen and Kang Liu and Jun Zhao and Xiangrong Zeng and Shengping Liu},
      year={2020},
      eprint={2011.01675},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ye2021contrastive,
      title={Contrastive Triple Extraction with Generative Transformer}, 
      author={Hongbin Ye and Ningyu Zhang and Shumin Deng and Mosha Chen and Chuanqi Tan and Fei Huang and Huajun Chen},
      year={2021},
      eprint={2009.06207},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Zhou_Zhang_Cui_Huang_2020, title={Evaluating Commonsense in Pre-Trained Language Models}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6523}, DOI={10.1609/aaai.v34i05.6523}, abstractNote={&lt;p&gt;Contextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models’ commonsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhou, Xuhui and Zhang, Yue and Cui, Leyang and Huang, Dandan}, year={2020}, month={Apr.}, pages={9733-9740} }

@misc{dinan2019second,
      title={The Second Conversational Intelligence Challenge (ConvAI2)}, 
      author={Emily Dinan and Varvara Logacheva and Valentin Malykh and Alexander Miller and Kurt Shuster and Jack Urbanek and Douwe Kiela and Arthur Szlam and Iulian Serban and Ryan Lowe and Shrimai Prabhumoye and Alan W Black and Alexander Rudnicky and Jason Williams and Joelle Pineau and Mikhail Burtsev and Jason Weston},
      year={2019},
      eprint={1902.00098},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{kim-etal-2020-will,
    title = "{Will {I} Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness}",
    author = "Kim, Hyunwoo  and
      Kim, Byeongchang  and
      Kim, Gunhee",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.65",
    doi = "10.18653/v1/2020.emnlp-main.65",
    pages = "904--916",
    abstract = "We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.",
}


@article{Zheng_Zhang_Huang_Mao_2020, title={A Pre-Training Based Personalized Dialogue Generation Model with Persona-Sparse Data}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6518}, DOI={10.1609/aaai.v34i05.6518}, abstractNote={&lt;p&gt;Endowing dialogue systems with personas is essential to deliver more human-like conversations. However, this problem is still far from well explored due to the difficulties of both embodying personalities in natural languages and the persona sparsity issue observed in most dialogue corpora. This paper proposes a pre-training based personalized dialogue model that can generate coherent responses using persona-sparse dialogue data. In this method, a pre-trained language model is used to initialize an encoder and decoder, and personal attribute embeddings are devised to model richer dialogue contexts by encoding speakers’ personas together with dialogue histories. Further, to incorporate the target persona in the decoding process and to balance its contribution, an &lt;em&gt;attention routing&lt;/em&gt; structure is devised in the decoder to merge features extracted from the target persona and dialogue contexts using dynamically predicted weights. Our model can utilize persona-sparse dialogues in a unified manner during the training process, and can also control the amount of persona-related features to exhibit during the inference process. Both automatic and manual evaluation demonstrates that the proposed model outperforms state-of-the-art methods for generating more coherent and persona consistent responses with persona-sparse data.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zheng, Yinhe and Zhang, Rongsheng and Huang, Minlie and Mao, Xiaoxi}, year={2020}, month={Apr.}, pages={9693-9700} }

@article{Li_Jiang_Feng_Sprague_Zhou_Hoey_2020, title={ALOHA: Artificial Learning of Human Attributes for Dialogue Agents}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6328}, DOI={10.1609/aaai.v34i05.6328}, abstractNote={&lt;p&gt;For conversational AI and virtual assistants to communicate with humans in a realistic way, they must exhibit human characteristics such as expression of emotion and personality. Current attempts toward constructing human-like dialogue agents have presented significant difficulties. We propose Human Level Attributes (HLAs) based on &lt;em&gt;tropes&lt;/em&gt; as the basis of a method for learning dialogue agents that can imitate the personalities of fictional characters. Tropes are characteristics of fictional personalities that are observed recurrently and determined by viewers’ impressions. By combining detailed HLA data with dialogue data for specific characters, we present a dataset, HLA-Chat, that models character profiles and gives dialogue agents the ability to learn characters’ language styles through their HLAs. We then introduce a three-component system, ALOHA (which stands for Artificial Learning of Human Attributes), that combines character space mapping, character community detection, and language style retrieval to build a character (or personality) specific language model. Our preliminary experiments demonstrate that two variations of ALOHA, combined with our proposed dataset, can outperform baseline models at identifying the correct dialogue responses of chosen target characters, and are stable regardless of the character’s identity, the genre of the show, and the context of the dialogue.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Aaron W. and Jiang, Veronica and Feng, Steven Y. and Sprague, Julia and Zhou, Wei and Hoey, Jesse}, year={2020}, month={Apr.}, pages={8155-8163} }

@misc{zheng2020personalized,
      title={Personalized Dialogue Generation with Diversified Traits}, 
      author={Yinhe Zheng and Guanyi Chen and Minlie Huang and Song Liu and Xuan Zhu},
      year={2020},
      eprint={1901.09672},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{multiwoz,
    title = "{{M}ulti{WOZ} - A Large-Scale Multi-Domain {W}izard-of-{O}z Dataset for Task-Oriented Dialogue Modelling}",
    author = "Budzianowski, Pawe{\l}  and
      Wen, Tsung-Hsien  and
      Tseng, Bo-Hsiang  and
      Casanueva, I{\~n}igo  and
      Ultes, Stefan  and
      Ramadan, Osman  and
      Ga{\v{s}}i{\'c}, Milica",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1547",
    doi = "10.18653/v1/D18-1547",
    pages = "5016--5026",
    abstract = "Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.",
}

@inproceedings{Budzianowski2019HelloIG,
    title = {{Hello, It{'}s {GPT}-2 - How Can {I} Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems}},
    author = "Budzianowski, Pawe{\l}  and
      Vuli{\'c}, Ivan",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5602",
    doi = "10.18653/v1/D19-5602",
    pages = "15--22",
    abstract = "Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision making, and language generation from absurdly small amounts of task-specific data. In this paper, we demonstrate that recent progress in language modeling pre-training and transfer learning shows promise to overcome this problem. We propose a task-oriented dialogue model that operates solely on text input: it effectively bypasses explicit policy and language generation modules. Building on top of the TransferTransfo framework (Wolf et al., 2019) and generative model pre-training (Radford et al., 2019), we validate the approach on complex multi-domain task-oriented dialogues from the MultiWOZ dataset. Our automatic and human evaluations show that the proposed model is on par with a strong task-specific neural baseline. In the long run, our approach holds promise to mitigate the data scarcity problem, and to support the construction of more engaging and more eloquent task-oriented conversational agents.",
}


@inproceedings{mtop,
    title = "{{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark}",
    author = "Li, Haoran  and
      Arora, Abhinav  and
      Chen, Shuohui  and
      Gupta, Anchit  and
      Gupta, Sonal  and
      Mehdad, Yashar",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.257",
    doi = "10.18653/v1/2021.eacl-main.257",
    pages = "2950--2962",
    abstract = "Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.",
}

@inproceedings{Pasupat2021ControllableSP,
    title = "{{Controllable Semantic Parsing via Retrieval Augmentation}}",
    author = "Pasupat, Panupong  and
      Zhang, Yuan  and
      Guu, Kelvin",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.607",
    doi = "10.18653/v1/2021.emnlp-main.607",
    pages = "7683--7698",
    abstract = "In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an input query, the parser retrieves related exemplars from a retrieval index, augments them to the query, and then applies a generative seq2seq model to produce an output parse. The exemplars act as a control mechanism over the generic generative model: by manipulating the retrieval index or how the augmented query is constructed, we can manipulate the behavior of the parser. On the MTOP dataset, in addition to achieving state-of-the-art on the standard setup, we show that CASPER can parse queries in a new domain, adapt the prediction toward the specified patterns, or adapt to new semantic schemas without having to further re-train the model.",
}

@misc{massive,
      title={{MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages}},
      author={Jack FitzGerald and Christopher Hench and Charith Peris and Scott Mackie and Kay Rottmann and Ana Sanchez and Aaron Nash and Liam Urbach and Vishesh Kakarala and Richa Singh and Swetha Ranganath and Laurie Crist and Misha Britan and Wouter Leeuwis and Gokhan Tur and Prem Natarajan},
      year={2022},
      eprint={2204.08582},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Xue2021mT5AM,
    title = "{{m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer}}",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}

@article{Roberts2022ScalingUM,
  url = {https://arxiv.org/abs/2203.17189},
  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},
  title = {{Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$}},
  journal={arXiv preprint arXiv:2203.17189},
  year = {2022},
}

@article{Ammar2016ManyLO,
    title = "{{Many Languages, One Parser}}",
    author = "Ammar, Waleed  and
      Mulcaire, George  and
      Ballesteros, Miguel  and
      Dyer, Chris  and
      Smith, Noah A.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1031",
    doi = "10.1162/tacl_a_00109",
    pages = "431--444",
    abstract = "We train one multilingual model for dependency parsing and use it to parse sentences in several languages. The parsing model uses (i) multilingual word clusters and embeddings; (ii) token-level language information; and (iii) language-specific features (fine-grained POS tags). This input representation enables the parser not only to parse effectively in multiple languages, but also to generalize across languages based on linguistic universals and typological similarities, making it more effective to learn from limited annotations. Our parser{'}s performance compares favorably to strong baselines in a range of data scenarios, including when the target language has a large treebank, a small treebank, or no treebank for training.",
}

@inproceedings{top,
    title = "{{Semantic Parsing for Task Oriented Dialog using Hierarchical Representations}}",
    author = "Gupta, Sonal  and
      Shah, Rushin  and
      Mohit, Mrinal  and
      Kumar, Anuj  and
      Lewis, Mike",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1300",
    doi = "10.18653/v1/D18-1300",
    pages = "2787--2792",
    abstract = "Task oriented dialog systems typically first parse user utterances to semantic frames comprised of intents and slots. Previous work on task oriented intent and slot-filling work has been restricted to one intent per query and one slot label per token, and thus cannot model complex compositional requests. Alternative semantic parsing systems have represented queries as logical forms, but these are challenging to annotate and parse. We propose a hierarchical annotation scheme for semantic parsing that allows the representation of compositional queries, and can be efficiently and accurately parsed by standard constituency parsing models. We release a dataset of 44k annotated queries (\url{http://fb.me/semanticparsingdialog}), and show that parsing models outperform sequence-to-sequence approaches on this dataset.",
}

@article{smcalflow,
    title = "{{Task-Oriented Dialogue as Dataflow Synthesis}}",
    author = "Andreas, Jacob  and
      Bufe, John  and
      Burkett, David  and
      Chen, Charles  and
      Clausman, Josh  and
      Crawford, Jean  and
      Crim, Kate  and
      DeLoach, Jordan  and
      Dorner, Leah  and
      Eisner, Jason  and
      Fang, Hao  and
      Guo, Alan  and
      Hall, David  and
      Hayes, Kristin  and
      Hill, Kellie  and
      Ho, Diana  and
      Iwaszuk, Wendy  and
      Jha, Smriti  and
      Klein, Dan  and
      Krishnamurthy, Jayant  and
      Lanman, Theo  and
      Liang, Percy  and
      Lin, Christopher H.  and
      Lintsbakh, Ilya  and
      McGovern, Andy  and
      Nisnevich, Aleksandr  and
      Pauls, Adam  and
      Petters, Dmitrij  and
      Read, Brent  and
      Roth, Dan  and
      Roy, Subhro  and
      Rusak, Jesse  and
      Short, Beth  and
      Slomin, Div  and
      Snyder, Ben  and
      Striplin, Stephon  and
      Su, Yu  and
      Tellman, Zachary  and
      Thomson, Sam  and
      Vorobev, Andrei  and
      Witoszko, Izabela  and
      Wolfe, Jason  and
      Wray, Abby  and
      Zhang, Yuchen  and
      Zotov, Alexander",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.36",
    doi = "10.1162/tacl_a_00333",
    pages = "556--571",
    abstract = "We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, SMCalFlow, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the MultiWOZ dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The SMCalFlow dataset, code for replicating experiments, and a public leaderboard are available at https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines.",
}


@inproceedings{eric2019multiwoz,
    title = "{{M}ulti{WOZ} 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines}",
    author = "Eric, Mihail  and
      Goel, Rahul  and
      Paul, Shachi  and
      Sethi, Abhishek  and
      Agarwal, Sanchit  and
      Gao, Shuyang  and
      Kumar, Adarsh  and
      Goyal, Anuj  and
      Ku, Peter  and
      Hakkani-Tur, Dilek",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.53",
    pages = "422--428",
    abstract = "MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there are substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original dataset with user dialogue acts. This leads to multiple co-existent versions of the same dataset with minor modifications. In this work we tackle the aforementioned issues by introducing MultiWOZ 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32{\%} of state annotations across 40{\%} of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the MultiWOZ 2.1 dataset and show the joint state tracking performance on the corrected state annotations. We are publicly releasing MultiWOZ 2.1 to the community, hoping that this dataset resource will allow for more effective models across various dialogue subproblems to be built in the future.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{treedst,
    title = "{{Conversational Semantic Parsing for Dialog State Tracking}}",
    author = "Cheng, Jianpeng  and
      Agrawal, Devang  and
      Mart{\'\i}nez Alonso, H{\'e}ctor  and
      Bhargava, Shruti  and
      Driesen, Joris  and
      Flego, Federico  and
      Kaplan, Dain  and
      Kartsaklis, Dimitri  and
      Li, Lin  and
      Piraviperumal, Dhivya  and
      Williams, Jason D.  and
      Yu, Hong  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Johannsen, Anders",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.651",
    doi = "10.18653/v1/2020.emnlp-main.651",
    pages = "8107--8117",
    abstract = "We consider a new perspective on dialog state tracking (DST), the task of estimating a user{'}s goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. We describe an encoder-decoder framework for DST with hierarchical representations, which leads to {\textasciitilde}20{\%} improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs.",
}

@inproceedings{dstc2,
    title = "{{The Second Dialog State Tracking Challenge}}",
    author = "Henderson, Matthew  and
      Thomson, Blaise  and
      Williams, Jason D.",
    booktitle = "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL})",
    month = jun,
    year = "2014",
    address = "Philadelphia, PA, U.S.A.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4337",
    doi = "10.3115/v1/W14-4337",
    pages = "263--272",
}

@inproceedings{multiatis,
title = "{End-to-End Slot Alignment and Recognition for Cross-Lingual {NLU}}",
    author = "Xu, Weijia  and
      Haider, Batool  and
      Mansour, Saab",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.410",
    doi = "10.18653/v1/2020.emnlp-main.410",
    pages = "5052--5063",
    abstract = "Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors. In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer. We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus. Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time. We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.",
}

@article{snips,
    url = {https://arxiv.org/abs/1805.10190},
  title={{Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces}},
  author={Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Th{\'e}odore and Caulier, Alexandre and Leroy, David and Doumouro, Cl{\'e}ment and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and others},
  journal={arXiv preprint arXiv:1805.10190},
  year={2018}
}

@Inproceedings{dstc10,
 author = {Seokhwan Kim and Yang Liu and Di Jin and Alexandros Papangelis and Behnam Hedayatnia and Karthik Gopalakrishnan and Dilek Hakkani-Tür},
title = {{Knowledge-grounded task-oriented dialogue modeling on spoken conversations track at DSTC10}},
 year = {2022},
 url = {https://www.amazon.science/publications/knowledge-grounded-task-oriented-dialogue-modeling-on-spoken-conversations-track-at-dstc10},
 booktitle = {AAAI 2022 Workshop on Dialog System Technology Challenge},
}

@inproceedings{Xue2020mT5AM,
    title = "{{m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer}}",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}