\section{Related work}
\label{sec:relatedWork}

\noindent\textbf{Neural implicit representation.} 
% In 3D computer vision, diverse representations are designed and proposed for different applications, each containing its own advantages and draw-
% backs. 
3D shapes can be represented either \textsl{explicitly} (\eg, point sets, voxels, meshes) or \textsl{implicitly} (\eg, signed-distance functions, indicator functions), each of them comes with its own advantages and drawbacks. 
Recently, there is an explosion of neural implicit representations~\cite{mescheder2019occupancy,park2019deepsdf,chen2019learning} that allow for generating detail-rich 3D shapes by predicting the underlying signed distance fields. 
%Neural implicit surface representation have gained immense popularity because of the ability to generate complex, high spatial resolution 3D shapes while using a small memory footprint during training. 
Thanks to the ability to learn priors over shapes, many deep implicit works have been proposed to solve various 3D tasks, such as shape representation and completion~\cite{sitzmann2019scene,Atzmon2020SAL,chibane2020implicit}, image-based 3D reconstruction~\cite{tulsiani2017multi,xu2019disn,yariv2020multiview}, shape abstraction~\cite{tulsiani2017learning,genova2019learning} and novel view synthesis~\cite{mildenhall2020nerf,dellaert2020neural}. 
Theoretically, any of the above shape representations can be used to represent sketches. However, primitive-based methods usually suppress the ability cap of shape representation. In this work, we choose to fit an implicit sketch  representation using a neural network, and show its superiority over other representations (\eg, BSP~\cite{chen2020bsp}) in the ablation study, see Sec.~\ref{sec:ablations}.

% \jw{Theoretically, any of the above shape representations can represent sketches. However, primitive-based methods suppress the ability cap of shape representation. Other lower-level representations adopted by [BSP] and [CAPRI] rely on CSG operations, which makes network training cumbersome. In order to avoid the above problems, we aim to fit the sketch's implicit representation with a neural network, which has been proven effective in [xxx].} 

% we chose to use functional representation of the output shape as a neural occupancy and 3D mapping function over UV
% domains. Functional representation is well-suited for het-
% erogeneous geometry with varied levels of detail, since it
% does not require choosing a fixed sampling rate. 


% Our method is in sharp contrast to previous work in that

\noindent\textbf{Reverse engineering CAD reconstruction.} 
Over the past decades, reverse engineering has been extensively studied; %in computer vision and graphics and computer vision communities; 
it aims at converting measured data (a surface mesh or a point cloud) into solid 3D models that can be further edited and manufactured by industries. %that can be used in CAD software for further operations. 
%Given a 3D shape represented as a surface mesh or a point cloud, 
Traditional approaches addressing this problem consist of the following tasks: (1) segmentation of the point clouds/meshes~\cite{benkHo2004segmentation,zhang2020blending,Shen2022framework}, (2) fitting of parametric primitives to segmented regions~\cite{schnabel2007efficient,cohen2004variational,yan2012variational}, (3) finishing operations for CAD modeling~\cite{benkHo2001algorithms,langbein2004choosing}. 
Important drawbacks of these conventional methods are the time-consuming process and the requirement of a skilled operator to guide the reconstruction~\cite{buonamici2018reverse}. %the imposition of a time-consuming framework and the requirement of a highly skilled user to guide the reconstruction~\cite{buonamici2018reverse}. 

\begin{figure*}[!t]
\centering
  \includegraphics[width=1.0\linewidth]{figs/network_v6_compress.pdf}   
  \caption{\textbf{Network architecture for SECAD-Net}: The embedding $\mathbf{z}$ encoded from the voxel input is first fed to the extrusion box head to predict extrusion boxes. It is also sent to the sketch head network to calculate the sketch SDF $\hat{\mathcal{S}}_{\mathsf{sk}}^{i}$ after concatenating with the linear transformed sampling point. $\hat{\mathcal{S}} _{\mathsf{cyl} }^i$ stands for the SDF of the cylinder, which is acquired by extruding $\hat{\mathcal{S}}_{\mathsf{sk}}^{i}$ with height $h_i$. Then we convert $\hat{\mathcal{S}} _{\mathsf{cyl} }^i$ to occupancy of cylinder $\hat{\mathcal{O}}_i$ and finally obtain the complete shape by union all the occupancies. %We detail our framework in Sec.~\ref{sec:method}.
  }
  \label{fig:overview}
\end{figure*}

%To overcome the overwhelming complexity of previous methods, 
With the release of several large-scale CAD datasets (\eg, ABC~\cite{koch2019abc}, Fusion 360~\cite{willis2021fusion}), SketchGraphs~\cite{seff2020sketchgraphs}), 
numerous approaches have explored deep learning to address primitive segmentation/detection~\cite{yan2021hpnet,le2021cpfn}, parametric curve or surface inference from point clouds~\cite{li2019supervised,sharma2020parsenet,paschalidou2019superquadrics,wang2020pie,guo2022complexgen} or B-rep models~\cite{lambourne2021brepnet,jayaraman2021uv}. However, by only outputting individual curves or surfaces, these methods lack the CAD modeling operations that are needed to build solid models. 
Focusing on CAD generation rather than reconstruction task as ours, some approaches propose deep generative models that predict sequences of CAD modeling operations to produce CAD designs~\cite{li2020sketch2cad,xu2021inferring,wu2021deepcad,willis2021fusion,xu2022skexgen}. 
Aiming at CAD reconstruction involving inverse CSG modeling~\cite{du2018inversecsg}, CSGNet~\cite{sharma2018csgnet} first develops a neural model that parses a shape into a sequence of CSG operations. More recent works follow the line of CSG parsing by advancing the inference without any supervision~\cite{kania2020ucsg}, or improving representation capability with a three-layer reformulation
of the classic CSG-tree~\cite{ren2021csg}, or handling richer geometric and topological variations by introducing quadric surface primitives~\cite{yu2022capri}. While achieving high-quality reconstruction, CSG tends to combine a large number of shape primitives that are not as flexible as the extrusions of 2D sketches and are also not easily user edited to control the final geometry. 

% Motivated by modern design tools, Point2Cyl~\cite{uy2022point2cyl} adopts the sketch-extrude procedural model and learns 2D sketches that can be extruded to obtain 3D shapes. Our work differs from Point2Cyl in two significant ways: (1) Point2Cyl relies on laborious ground truth labels, including segmentation, normal, and sketch. In sharp contrast to that, SECAD-Net is trained in a self-supervised manner; (2) To achieve per-extrusion cylinder fitting, Point2Cyl requires surface-level segmentation. In contrast, SECAD-Net determines sketch planes by adaptively learning extrusion boxes.
\rev{Motivated by modern design tools, supervised methods are proposed~\cite{uy2022point2cyl,lambourne2022reconstructing} utilizing the sketch-extrude procedural models and learning 2D sketches that can be extruded to 3D shapes. In contrast to their reliance on 2D labels, SECAD-Net is trained in a self-supervised manner.}
\rev{Most closely related to our work is ExtrudeNet~\cite{ren2022extrudenet}. 
SECAD-Net distinguishes itself from ExtrudeNet in several significant aspects:
\romannumeral1) Following the traditional reconstruction process, ExtrudeNet first predicts the parameters of Bézier curves and then converts them into SDFs. In contrast, we jumped out of this paradigm and directly used neural networks to predict the 2D implicit fields of the profiles.
\romannumeral2) ExtrudeNet adopts closed Bézier curves to avoid self-intersection in sketches. This makes ExtrudeNet can only predict star-shaped profiles, which limits the expressive power of their CAD shapes. Our method does not impose any restrictions on the shape of the profile, thus having greater flexibility in shape expression.
\romannumeral3) To pursue the reconstruction effect, ExtrudeNet relies on a larger number of primitives, while our method is able to predict more compact CAD shapes.}

%Our method is in sharp contrast to previous work in that

% These are all trained utilizing supervision from ground truth sequence data. As they do not incorporate a loss function which
% directly compares the generated geometry and the target shape,
% their ability to match target geometry is limited.

% One
% early work is CSGNet [Sharma et al. 2017], which trains a neural
% network to infer the sequence of Constructive Solid Geometry (CSG)
% operations based on visual input. More recent works along this line
% of research include [Chen et al. 2020; Ellis et al. 2019; Kania et al.
% 2020; Tian et al. 2019]. 

% A different approach to CAD reconstruction is to solve the inverse
% problems of procedural CAD models. For example, inverse CSG
% [Du et al. 2018; Kania et al. 2020; Ren et al. 2021; Sharma et al.
% 2018] searches for CSG boolean operations and solid primitives
% that combine into the target shape; [Ganin et al. 2021; Para et al.
% 2021; Seff et al. 2021; Willis et al. 2021b; Wu et al. 2021] assume
% a “sketch+extrude” procedural model and study the generation of
% 2D sketches that can be extruded to obtain 3D shapes, by training
% on datasets of such modeling sequences [Seff et al. 2020; Willis
% et al. 2021c]. 

% the CSGNet paper [Sharma
% et al. 2018] trains a neural network that takes as input a 2D or 3D
% shape and outputs a CSG program. Compared to their work, our
% method does not require a training dataset and we demonstrate our
% algorithm on 3D shapes of much higher complexity. Wu et al. [2018]
% reconstruct a CSG tree from raw point clouds by extracting the
% primitives and inferring CSG tree structures. When building the
% CSG tree, they divide the bounding box into voxels and label each
% voxel as inside or outside the point cloud. A CSG tree is then built
% in a bottom-up manner by solving an energy minimization problem
% based on the labels of each voxel. Our work shares a similar pipeline
% but does not require discretizing the inputs into voxels, which allows
% us to handle inputs with details at various levels.

%3. CAD Datasets





 
%4. deep CSG and sketch
% The methods that either di-
% rectly learn the B-Rep structure of a CAD model [13, 18,
% 12, 45, 6] or predict sketches and CAD operations [43, 26,
% 30, 8], are closely related to our work. The works in [26, 8]



% parametric representation is constructive solid geometry (CSG) as it
% is a well understood, widely accepted staple in modern CAD systems
% and is compact in its representation. CSG encodes geometries as
% trees that are constructed by recursively applying boolean operators
% to primitive shapes [Requicha and Rossignac 1992]. Theory for the
% automatic conversion of 3D models to CSG trees has been widely
% studied for the past 20 years.

% Existing methods for representing meshes, such as BSP-N ET [ 5 ] and C VX N ET [ 6 ], achieve remarkable accuracy on a reconstruction tasks. However, the process of generating the mesh from predicted planes requires an additional post-processing step. These methods also assume that any object can be
% decomposed into a union of convex primitives. While holding, it requires many such primitives to represent concave shapes. Consequently, the decoding process is difficult to explain and modified with some external expert knowledge. On the other hand, there are fully interpretable approaches,
% like CSG-N ET [ 7 , 8 ], that utilize CSG parse tree to represent 3D shape construction process. Such solutions require expensive supervision that assumes assigned CSG parse tree for each example given during training.

