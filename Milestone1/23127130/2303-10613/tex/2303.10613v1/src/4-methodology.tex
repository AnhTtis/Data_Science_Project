\section{Method}
\label{sec:method}
%\subsection{ Learning Sketch-Extrude Operations}
\subsection{Sketch-Extrude Inferring}

We apply a standard 3D CNN encoder to extract a shape code $\mathbf{z}$ with size 256 from the input voxel. The code is then passed to the proposed SECAD-Net to output the sketch and extrusion cylinder parameters. 
Below we introduce the main modules of SECAD-Net following the order of data transmission during prediction.

\noindent\textbf{Extrusion box prediction.} 
We first apply a fully connected layer to predict the parameters of the extrusion boxes. 
Taking the feature encoding $\mathbf{z}$ as input, a decoder, refereed to \emph{sketch box head}, outputs a set of sketch boxes $\mathcal{B}=\left\{\mathbf{s}_i, \mathbf{c}_i,\mathbf{r}_i \mid i \in N  \right\}$, where $\mathbf{s}_i \in \mathbb{R} ^3  $ describes the 2D size (\ie, length and width) of the box, $\mathbf{c}_i \in \mathbb{R} ^3 $ represents the predicted position of the box’s center, and $\mathbf{r}_i \in \mathbb{R}^4$ is the rotation quaternion. The positive z-axis of the extrusion box determines the axial direction $\mathbf{e}_i$ of the sketch plane, and the height of the extrusion box is twice the height of the extrude operation (see Fig.~\ref{fig:definition}).

\noindent\textbf{2D sketch inference.} 
The sketches in each sketch plane depict the shape contained within the sketch box. Inspired by the recent neural implicit shapes~\cite{Atzmon2020SAL,uy2022point2cyl}, we encode the shape of each sketch into a sketch latent space. 
To this end, we first project the 3D sampling points \emph{w.r.t} the corresponding occupancy value onto the sketch plane along the axis $\mathbf{e}_i$.
A \emph{sketch head network} (SK-head) then computes the signed distance from each sampling point to the sketch contour. The distance is negative for points in the sketch and positive for points outside. 
Each SK-head contains $N_{lay}$ layers of fully connected layers, with softplus activation functions used between layers, and we clamp the output distance to [-1,1] in the last layer.
Each 2D point is concatenated with the feature encoding $\mathbf{z}$ as a global condition before being fed into the SK-head. 
Regarding the $i$-th SK-head as an implicit function $f_{i}$, then formally we get: 
\begin{equation}
\hat{\mathcal{S}}_{\mathsf{sk}}^{i} = f_{i}(\mathbf{x}_i^t,\mathbf{z} ),
\end{equation}
where $\mathbf{x}_i^t$ is the result of a linear transformation of the sampling points contained in the $i$-th extrusion box, which can be expressed as $\mathbf{r} _i^{-1}(\mathbf{x}_i -\mathbf{c} _i)$. $\hat{\mathcal{S}}_{\mathsf{sk}}^{i}$ represents the signed distance field of the $i$-th sketch plane.

\noindent\textbf{Differentiable extrusion.}
Next, we calculate the SDF of a cylinder based on the 2D distance field and the extrusion height $h$.
% Define the area between the hyperplane $p^{top}$ where the top surface of cylinder laid and the hyperplane $p^{bottom}$ where the bottom surface laid as $\Omega$, and the area inside the infinite cylinder where the side of cylinder is laid as $\Psi$. 
We denote $\Omega$ as the volume between two hyperplanes $p^{u}$ and $p^{l}$, where $p^{u}$ and $p^{l}$ are the upper and lower surfaces on which the cylinder is located. Similarly, we define $\Psi$ as the volume inside the infinite cylinder where the side of the cylinder is located. 
The implicit field of the $i$-th cylinder, $\hat{\mathcal{S}} _{\mathsf{cyl} }^i$, is equal to one of the following cases: 
(1) the distance from a point $\mathbf{x}_{i}$ to $p^{u}$ or $p^{l}$, when $\mathbf{x}_{i} \in \Omega \cap  \Psi^\complement$, where superscript $\complement$ stands for complement; 
(2) $\hat{\mathcal{S}}_{\mathsf{sk} }^{i}$, when $\mathbf{x}_{i} \in \Omega^\complement \cap \Psi $; 
(3) the distance from $\mathbf{x}_{i}$ to the intersection curves of the cylinder and hyperplanes, when $\mathbf{x}_{i} \in \Omega^\complement \cap \Psi^\complement$; 
(4) the maximum distance between $\hat{\mathcal{S}}_{\mathsf{sk} }^{i}$ and the point to $p^{u}$ or $p^{l}$, when $\mathbf{x}_{i} \in \Omega^\complement \cap \Psi$. The sub-formulas for each case are as follows:
\begin{equation}
\hat{\mathcal{S}} _{\mathsf{cyl} }^i = \begin{cases}
max( \hat{\mathcal{S}}_{\mathsf{sk} }^{i} , |\mathbf{x}_{i_z}|-h_{i})  
&, (\hat{\mathcal{S}}_{\mathsf{sk} }^{i}\le 0) \wedge  (|\mathbf{x}_{i_z}|\le h_{i}) \\
|\mathbf{x}_{i_z}|-h _{i}  
&, (\hat{\mathcal{S}}_{\mathsf{sk} }^{i}\le 0) \wedge (|\mathbf{x}_{i_z}|>h_{i})\\
\hat{\mathcal{S}}_{\mathsf{sk} }^{i}
&, (\hat{\mathcal{S}}_{\mathsf{sk} }^{i}>0) \wedge (|\mathbf{x}_{i_z}|\le h_{i}) \\
\left \| \hat{\mathcal{S}}_{\mathsf{sk} }^{i} ,(|\mathbf{x}_{i_z}|-h_{i}) \right \| _2  
&, ( \hat{\mathcal{S}}_{\mathsf{sk} }^{i}>0) \wedge (|\mathbf{x}_{i_z}|>h_{i}) \\
\end{cases}
\end{equation}
Combining the above four sub-formulas with the $max$ and $min$ operations, the following result is obtained:
\begin{eqnarray}
\hat{\mathcal{S}} _{\mathsf{cyl} }^i & = &min(max(\hat{\mathcal{S}}_{\mathsf{sk} }^{i},|\mathbf{x}_{i_z}|-h_{i}),0)   \nonumber\\
  & +   & \left \| max({\hat{\mathcal{S}}_{\mathsf{sk} }^{i}},0),max(|\mathbf{x}_{i_z}|-h_{i},0) \right \| _2
\end{eqnarray}

\noindent\textbf{Occupancy conversion and assembly.} 
The occupancy function represents points inside the shape as 1 and points outside the shape as 0, which can be transformed by SDF. Following~\cite{deng2020cvxnet,ren2021csg}, we use the Sigmoid function to perform differentiable transformation operations: 
\begin{equation}
\label{eq:sigmoid}
\hat{\mathcal{O}}_i = Sigmoid(- \eta \cdot \hat{\mathcal{S}} _{\mathsf{cyl} }^i) \text{.}
\end{equation} 
We finally assemble the occupancy $\hat{\mathcal{O}}_i$ of each cylinder to obtain the reconstructed shape. In order to express complex shapes, many works use intersection, union, and difference operations in CSG in the assembly stage~\cite{chen2020bsp,kania2020ucsg,ren2021csg,yu2022capri}. In contrast to them, we only use the union operation, because the extrusion cylinders can naturally represent concave shapes. This helps us avoid designing intricate loss functions or employing multi-stage training strategies without losing the flexibility of reconstructing shape representations. We adopt the Softmax to compute the union operation as it is shown to be effective in avoiding vanishing gradients~\cite{ren2021csg}: 
\begin{equation}
\label{eq:softmax}
\hat{\mathcal{O}}_{total} =\sum_{i}^{N}  Softmax(\varphi \cdot \hat{\mathcal{O}}_{i})\cdot \hat{\mathcal{O}}_{i},
\end{equation} 
where $\varphi$ is the modulating coefficient and $\hat{\mathcal{O}}_{total}$ is the occupancy representation of the final reconstructed shape.


% We train CSG-Stump Net end-to-end in an un-
% supervised manner. CSG-Stump Net learns to predict a
% CSG-Stump with primitives and their connections without
% explicit ground truth. Instead, the supervision signal is
% quantified by the reconstruction loss between the predicted
% and ground truth occupancy. Specifically, we sample testing
% points X ∈ R N×3 from the shape bounding box and mea-
% sure the discrepancy between the ground truth occupancy
% O ∗ and the predicted occupancy
% ˆ
% O as follows:

\subsection{Loss Function}
We train SECAD-Net in a self-supervised fashion through the minimization of the sum of two objective terms. 
%We define two objective terms for CAD reconstruction task, the first one 
The supervision signal is mainly quantified by the reconstruction loss, which measures the mean squared error between the predicted shape occupancy $\hat{\mathcal{O}}_{total}$ and the ground truth $\mathcal{O}_{total}^*$:
\begin{equation}
\mathcal{L}_{\textit{recon}}=\mathbb{E}_{x\in \mathbf{X} } \left [ (\hat{\mathcal{O}}_{total} - \mathcal{O}_{total}^*)^2 \right ],
\end{equation} 
where $x$ is a randomly sampled point in the shape volume. %,  and $\mathcal{O}^*$ are the predicted and ground truth occupancy, respectively.

However, we find that applying only $\mathcal{L}_{\textit{recon}}$ makes the network always learn fragmented cylinders. 
To tackle this problem, we design a 2D sketch loss to facilitate the network to learn the axis of the sketch plane and the complete profile.  
% Specifically, each sketch plane and voxel form an occupancy cross-section $ \hat{\mathcal{O}}_{s_i}$. 
Specifically, each sketch plane cuts the voxel model to form an occupancy cross-section ${\mathcal{O}_{cs}^{i^*}}$. 
We project the 3D sampling points inside the $i$-th extrusion box $\mathcal{B}^i$ onto the sketch plane along the axial direction, and calculate the difference between the occupancy value of the projected points $\hat{\mathcal{O}}_{proj}$ and ground truth ${\mathcal{O}_{cs}^{i^*}}$ :
\begin{equation}
\mathcal{L}_{\textit{sketch}}=\sum_{i=1}^{N}\mathbb{E}_{x\in \mathcal{B} ^i}\left [ (\hat{\mathcal{O}}_{proj}^i - {\mathcal{O}_{cs}^{i^*}})^2  \right ].
\end{equation} 
The overall objective of SECAD-Net is defined as the combination of the above two terms: 
\begin{equation}
\label{eq:loss}
\mathcal{L}_{total}=\mathcal{L}_{\textit{recon}}+ \lambda \mathcal{L}_{\textit{sketch}},
\end{equation} 
where $\lambda$ is a balance factor.

\subsection{CAD Reconstruction}
\label{sec:reconstruction}
The output of SECAD-Net during the training phase is an implicit occupancy function of the 3D shape. In the prediction stage, we reconstruct CAD models by using sketch-extrude operations instead of the marching cubes method.

\noindent\textbf{Sketch and extrusion.} 
To convert a 2D implicit field (Fig.~\ref{fig:splines} (a)) in the sketch latent space into an editable sketch, we input uniform 2D sampling points to the SK-head, and attach the implicit value to the position of the sampling point to obtain an explicit image-like 2D profile (Fig.~\ref{fig:splines} (b)). 
We then use the Teh-Chin chain approximation~\cite{teh1989detection} to extract the contours of the profiles and the hierarchical relationships between them. We further apply Dierckx's fitting~\cite{dierckx1982algorithms} to convert the contours into closed B-splines (Fig.~\ref{fig:splines} (c)).

\begin{figure}[!t]
    \centerline{
    \includegraphics[width=1.0\linewidth]{figs/splines.pdf}
    }
    \caption{Illustration of converting 2D implicit sketches into the closed B-splines.}
    \label{fig:splines}
\end{figure}

After extruding each sketch to get the cylinder primitives according to half the height of $\mathcal{B}^i$, we assemble cylinder primitives into cylinders by alternately performing union or difference operations according to the hierarchical relationship between contours (primitive at hierarchy 0 \emph{difference} primitives at hierarchy 1 in the case of Fig.~\ref {fig:splines}). Finally, we take the union of all cylinders to obtain the CAD model.

\noindent\textbf{Post-processing.}
We take two post-processing operations to clean up overlapping and shredded shapes in the result. First, for any two cylinders, when their overlapping  coefficient is greater than 0.95, the smaller of them is discarded. Second, we delete all cylinders whose height is less than 0.01 in the reconstruction result. We demonstrate our final reconstructions in Fig.~\ref{fig:compare_ABC} and Fig.~\ref{fig:compare_fusion}.


\subsection{Implementation Details}
SECAD-Net is implemented in PyTorch and trained on a TITAN RTX GPU from NVIDIA\textsuperscript{\textregistered}. 
We train our model using an Adam optimizer~\cite{kingma2015adam} with learning rate $1 \times 10^{-4}$ and beta parameters (0.5, 0.99).
We set both the number of MLP layers in the sketch head network and the number of output cylinders to 4.
For hyper-parameters in Eq.~\ref{eq:sigmoid}, Eq.~\ref{eq:softmax} and Eq.~\ref{eq:loss}, we set $\eta= 150$, $\varphi= 25$ and $\lambda = 0.01$ in default, which generally works well in our experiments.
Employing a similar training strategy to ~\cite{yu2022capri}, we first pre-train SECAD-Net on the training datasets for 1,000 epochs using batch size 24, which takes about 8 hours, and fine-tuning on each test shape for 300 epochs, which takes about 3 minutes per shape.