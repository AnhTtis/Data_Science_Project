\section{Methodology}
\label{sec:methodology}

\begin{figure*}
  \centering
  \includegraphics[width=0.85\linewidth]{figs/architecture.pdf}
  \caption{\textbf{An overview of the proposed pipeline DiffusionAD.} The reconstruction and segmentation sub-networks constitute the entire pipeline. 
  The input image is perturbed by two distinct noise scales. Following, the noise is predicted by an inference within the diffusion model. Finally, the norm-guided one-step denoising paradigm is employed to predict an anomaly-free reconstruction.
  The segmentation sub-network predicts pixel-wise anomaly scores by comparing commonalities and inconsistencies between the input image and its reconstruction.
  }
  \label{fig:architecture}
  \vspace{-0.3cm}
\end{figure*}

\subsection{Architecture}
\label{subsec:architecture}


Our proposed novel generative model-based framework consists of two components: a reconstruction sub-network and a segmentation sub-network, as shown in \cref{fig:architecture}.
With the proposed anomaly generation strategies~(\cref{subsec:Anomaly Synthetic Strategy}), We define $x_0$ as the input image that is either normal ($y=0$) or anomalous ($y=1$). 

\vspace{0.05in}
\noindent\textbf{Reconstruction sub-network.\quad}We implement the reconstruction sub-network via a diffusion model, which reformulates the reconstruction process as a \emph{noise-to-norm} paradigm. 
First, we use the diffusion forward process proposed~\cite{ho2020ddpm} to corrupt the input image $x_0$ at a random time step $t$ to obtain $x_t$ via \cref{eq:xt}. 
The input image $x_0$ gradually loses its discriminative features and approaches an isotropic Gaussian distribution as the time step increases. 
Then $\epsilon_{\theta}\left(x_{t}, t\right)$ is a function approximator intended to predict noise $\epsilon$ from ${x}_{t}$ and $t$, which is implemented with a U-Net~\cite{ronneberger2015U-Net,dhariwal2021diffusionbeatgans}-like architecture based on PixelCNN~\cite{salimanspixelcnn++}, ResNet~\cite{he2016resnet-18}, and Transformer~\cite{vaswani2017attention}. 
It is worth noting that after being perturbed by Gaussian noise, the anomalous pixels lose their distinctive features and tend to be treated by the model as injected noise. Consequently, the anomaly-free reconstruction $\hat{x}_{0}$ can be obtained from \cref{eq:xt-1} in an iterative manner.

\vspace{0.05in}
\noindent\textbf{Segmentation sub-network.\quad}The segmentation sub-network employs a U-Net~\cite{ronneberger2015U-Net}-like architecture consisting of an encoder, a decoder, and skip connections. The input to the segmentation sub-network is a channel-wise concatenation of $x_{0}$ and $\hat{x}_{0}$. 
The segmentation sub-network learns to identify anomalies by exploiting the inconsistencies and commonalities between the input image $x_{0}$ and its anomaly-free approximation $\hat{x}_{0}$ to predict the pixel-wise anomaly score without post-processing. Remarkably, the learned inconsistency reduces false positives caused by slight pixel-wise differences between the normal region and its reconstruction and highlights significantly different regions.

\subsection{Norm-guided One-step Denoising}
Each iteration of the denoising process in the diffusion model corresponds to a round of network inference, requiring substantial computational resources and time. This presents a significant challenge for real-time inference.

\vspace{0.05in}
\noindent\textbf{One-step Denoising.\quad}To this end, we employ a direct reconstruction method, which we refer to as \emph{one-step denoising}, as an alternative to the iterative approach.
More specifically, at any time step $t$, after the diffusion model predicts the noise $\epsilon_{\theta}\left(x_{t}, t\right)$ of $x_t$ by a single inference (one-step), direct recovery is always valid~\cite{ho2020ddpm}, as indicated by the following:
\begin{equation}
\begin{aligned}
\hat{x}_{0}=\frac{1}{\sqrt{\bar{\alpha}_{t}}}\left(x_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{\theta}\left(x_{t}, t\right)\right).
\end{aligned}
\label{eq:xhat}
\end{equation}
where $\hat{x}_{0}$ refers to the anomaly-free reconstruction via \emph{one-step denoising}. This direct prediction is $t$ times faster than iterative prediction, resulting in significant savings of computational resources and inference time. 
However, we observe that repairing different types of anomalies requires different scales of noise injection. When the anomaly region is relatively small or absent, direct prediction from $x_{t_s}$ is more suitable, as shown in \cref{fig:onestep_and_multi_corrupt} (b). The anomaly-free restoration $\hat{x}_{0_s}$ demonstrates higher pixel quality, preserving fine-grained details and closely aligning with the iterative result. Conversely, predicting directly from $x_{t_b}$ introduces some distortions, and as the time step $t$ increases, these distortions gradually amplify.
In cases where the anomaly region is larger or exhibits semantic changes,  predicting from $x_{t_s}$ leaves some residual anomaly regions, as illustrated in \cref{fig:onestep_and_multi_corrupt} (b). Therefore, the injection of noise at larger scales becomes necessary to effectively perturb the anomalous regions in order to achieve higher quality, anomaly-free restoration $\hat{x}_{0_b}$ at the semantic level.



\vspace{0.05in}
\noindent\textbf{Norm-guided Denoising.\quad}In order to harness the advantages of both noise scale regimes, we propose a norm-guided denoising paradigm.
We divide the random range of $t\in \{0, 1, ... , T\}$ into two parts using $\tau$, where $S = \{0, 1, ... , \tau\}$ and $B= \{ \tau+1, \tau+2, ... , T\}$. 
For an input image $x_0$, we first perturb it using two randomly sampled time steps $t_s \in S $ and $t_b \in B $ to obtain $x_{t_s}$ and $x_{t_b}$ through \cref{eq:xt}.
As the diffusion model defaults to being conditioned on $t$, we abbreviate $\epsilon_\theta(x_t,t)$ as $\epsilon_\theta(x_t)$.
Subsequently, we first employ the diffusion model to individually predict the noise of $x_{t_s}$ and $x_{t_b}$, corresponding to $\epsilon_\theta(x_{t_s})$ and $\epsilon_\theta(x_{t_b})$, respectively. We then directly predict $\hat{x}_{0_b}$ using $\epsilon_\theta(x_{t_b})$ by \cref{eq:xhat}.  
Although $\hat{x}_{0_b}$ exhibits some distortions and appears low in mass, it consistently presents an anomaly-free appearance despite the diversity of anomalies in $x_0$. 
Hence, $\hat{x}_{0_b}$ is regarded as $\hat{x}_{0_b} \sim q(x_{y=0})$ and is rewritten as $n$.
After that, $n$ assumes the role of a conditional image to guide the prediction of $\hat{x}_{0_s}$ by \cref{eq:image guidance}. 
We define $sim(x_t,n_t) = -\frac{1}{2}~(n_{t_s} - x_{t_s})^2$ to measure the similarity between these two images. 
$n_{t_s}$ is obtained by perturbing $n$ with the predicted noise $\epsilon_\theta(x_{t_s})$ instead of random noise in order to make the guidance more dependent on the difference between the image contents themselves.
Thus, \cref{eq:image guidance} is reformulated as follows:
\begin{equation} 
\begin{aligned}
\epsilon_\theta(x_{t_s},n_{t_s})=\epsilon_\theta(x_{t_s})-\sqrt{1-\bar{\alpha}_{t_s}}\mathrm{~}w\mathrm{~}(n_{t_s} - x_{t_s})
\end{aligned}
\label{eq:norm guidance}
\end{equation}
where $\epsilon_\theta(x_{t_s},n_{t_s})$ denotes the modified noise with norm-guidance. 
Finally, bring $x_{t_s}$ and $\epsilon_\theta(x_{t_s},n_{t_s})$ into \cref{eq:xhat} to directly predict the norm-guided anomaly-free reconstruction as follows:
\begin{equation}
\begin{aligned}
\hat{x}_{0_g}=\frac{1}{\sqrt{\bar{\alpha}_{t_s}}}\left(x_{t_s}-\sqrt{1-\bar{\alpha}_{t_s}} \epsilon_{\theta}\left(x_{t_s},n_{t_s} \right)\right).
\end{aligned}
\label{eq:guided xhat}
\end{equation}
Equipped with the norm-guided denoising paradigm, DiffusionAD can handle different types of anomalies and predict superior reconstructions.



\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{figs/onestep_and_multi_corrupt.pdf}
  \caption{Two-scale corruptions and their different reconstructions.}%
  \label{fig:onestep_and_multi_corrupt}
  \vspace{-0.5cm}
\end{figure}

\subsection{Training \& Inference}
\label{subsec:training}

\textbf{Training stage.\quad}
We jointly train the denoising and segmentation sub-networks.
The diffusion model learns the entire distribution of normal samples ($y=0$) by minimizing the following loss function:

\begin{footnotesize}
\begin{equation}
\begin{aligned}
\mathcal{L}_{noise}=\frac{(1-y)(\|\epsilon_{t_s}-\epsilon_{\theta}(x_{t_s})\|^{2} + \|\epsilon_{t_b}-\epsilon_{\theta}(x_{t_b})\|^{2})}{2}.
\end{aligned}
\label{eq:noise loss}
\end{equation}
\end{footnotesize}
The segmentation sub-network exploits the commonalities and differences between $x_{0}$ and $\hat{x}_{0_g}$ to predict pixel-wise anomaly scores as close as possible to the ground truth mask. The segmentation loss is defined as:
\begin{small}
\begin{equation}
\begin{aligned}
\mathcal{L}_{mask}=\operatorname{Smooth}_{\mathcal{L} 1}\left(M, \hat{M}\right)+\gamma \mathcal{L}_{focal}\left(M, \hat{M}\right).
\end{aligned}
\label{eq:segmentation loss}
\end{equation}
\end{small}
Where $M$ is the ground truth mask of the input image and $\hat{M}$ is the output of the segmentation sub-network. Inspired by ~\cite{zhang2023prn}, smooth L1 loss~\cite{girshick2015fast-rcnn} and focal loss~\cite{lin2017focal} are applied simultaneously to reduce over-sensitivity to outliers and accurately segment hard anomalous examples. $\gamma\in \mathbb{R}^{+}$ is a hyperparameter that controls the importance of $\mathcal{L}_{focal}$. Thus, the total loss used in jointly training DiffusionAD is:
\begin{equation}
\begin{aligned}
\mathcal{L}_{total}=\mathcal{L}_{noise} + \mathcal{L}_{mask}.
\end{aligned}
\label{eq:total loss}
\end{equation}

\vspace{0.05in}
\noindent\textbf{Inference stage.\quad}Unlike other diffusion methods that reconstruct images in an iterative manner, we still perform a one-step norm-guided estimation in the inference stage, which is hundreds of times faster while maintaining comparable sampling quality. 
The robust decision boundary learned by the segmentation network will also effectively mitigate the effect of such sub-optimal sample quality. 
Moreover, in the inference phase, $t_s$ and $t_b$ are fixed values. 
After the segmentation model predicts the pixel-level anomaly score $\hat{M}$, we take the average of the top $K$ anomalous pixels in $\hat{M}$ as the image-level anomaly score~\cite{zhang2023prn}.



\subsection{Anomaly Synthetic Strategy}
\label{subsec:Anomaly Synthetic Strategy}
Since prior information about anomalies is not available for training, we synthesize pseudo-anomalies online for end-to-end training. The idea of our anomaly synthesis strategy is adding visually inconsistent appearances to the normal samples inspired by ~\cite{zhang2023prn,zavrtanik2021draem,yang2023memseg}, and these out-of-distribution regions are defined as the synthesized anomalous regions. 


\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\linewidth]{figs/anomaly_synthetic.pdf}
  \caption{Visually inconsistent appearances are added to the normal samples to obtain synthetic anomalies.}
  \label{fig:Anomaly synthetic strategy}
  \vspace{-0.3cm}
\end{figure} 

\cref{fig:Anomaly synthetic strategy} illustrates the overall process of transforming a normal sample (\cref{fig:Anomaly synthetic strategy}, $N$) into a synthetic anomalous sample (\cref{fig:Anomaly synthetic strategy}, $S$). Random and irregular anomalous regions (\cref{fig:Anomaly synthetic strategy}, $P$) are first obtained from the Perlin~\cite{perlin1985perlin} noise and then multiplied by the object foreground~\cite{qin2022foreground} (\cref{fig:Anomaly synthetic strategy}, $F$) of the normal sample to obtain the ground truth mask (\cref{fig:Anomaly synthetic strategy}, $M$). For textural datasets, the foreground is replaced by a random part of the whole image. The appearance of visual inconsistencies (\cref{fig:Anomaly synthetic strategy}, $A$) mainly stems from the self-augmentation of normal samples or Describing Textures Dataset (DTD)~\cite{cimpoi2014DTD}. The proposed synthetic anomaly (\cref{fig:Anomaly synthetic strategy}, $S$) is defined as: 
\begin{equation}
\begin{aligned}
S=\beta(M \odot N)+(1-\beta)(M \odot A)+\bar{M} \odot N
\end{aligned}
\label{eq:anomaly synthetic}
\end{equation}
where $\bar{M}$ is the pixel-wise inverse operation of $M$, $\odot$ is the element-wise multiplication operation, and $\beta$ serves as an opacity parameter designed to enhance the fusion of anomalous and normal regions.























