\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Details}
\label{subsec:Experimental Details}



\textbf{Datasets.\quad}To assess the efficacy and generalizability of our approach, we conduct experiments on four diverse unsupervised datasets including MVTec~\cite{bergmann2019mvtec}, VisA~\cite{zou2022spd}, DAGM~\cite{wieler2007dagm}, and MPDD~\cite{jezek2021mpdd}.
These datasets contain samples of many different types of surface defects, such as scratches, cracks, holes, and depressions. 

\vspace{0.05in}
\noindent\textbf{Evaluation Metrics.\quad}To measure the performance of anomaly detection (image level), we report the Area Under Receiver Operator Characteristic curve (Image AUROC), the most widely used metric.
In terms of anomaly localization (pixel level) performance, we report three metrics: Pixel AUROC, PRO, and Pixel AP.
Pixel AUROC may provide an inflated view of the performance~\cite{zou2022spd}, which may pose challenges in measuring the true capabilities of the model when using only this metric.
The false positive rate is dominated by the extremely high number of non-anomalous pixels and remains low despite the false positive prediction~\cite{tao2022al_survey}, which is caused by the fact that anomalous regions typically only occupy a tiny fraction of the entire image. 
Thus, to comprehensively evaluate the anomaly localization performance, Per Region Overlap (PRO) and pixel-level Average Precision (AP) also play the role of evaluation metrics.
Per Region Overlap (PRO) metric is more capable of assessing the ability of fine-grained anomaly localization, which treats anomaly regions of varying sizes equally and is widely employed by previous works~\cite{deng2022rd4ad,roth2022patchcore,zhang2023prn}.
AP is more appropriate for highly imbalanced classes, especially for industrial anomaly localization, where accuracy is critical.




\vspace{0.05in}
\noindent\textbf{Implementation Details.\quad}All images in the four datasets are resized to $256 \times 256$.
For the denoising sub-network, we adopt the UNet~\cite{ronneberger2015U-Net} architecture for estimating $\epsilon_{\theta}$. This architecture is mainly based on PixelCNN~\cite{salimanspixelcnn++} and Wide ResNet~\cite{he2016resnet-18}, with sinusoidal positional embedding~\cite{vaswani2017attention} to encode the time step $t$. $T$ is set to 1000 and is divided by $\tau = 300$ into two parts. The noise schedule is set to linear. We set the base channel to 128, the attention resolutions to 32, 16, 8, and the number of heads to 4. We do not use  EMA. For the segmentation sub-network, we employ a U-Net-like architecture consisting of an encoder, a decoder, and skip connections. $\gamma$ in the segmentation loss $\mathcal{L}_{seg}$ is set to 5. We train for 3000 epochs with a batch size of 16 consisting of 8 normal samples and 8 anomalous synthetic samples. We use Adam optimizer~\cite{kingma2014adam} for optimization, with an initial learning rate $10^{-4}$. The image-level anomaly score is obtained by taking the average of the top 50 anomalous pixels in pixel-wise anomaly score $\hat{M}$. We implement our model and experiments on NVIDIA A100 GPUs.





\begin{table*}[h]
\centering
\scalebox{0.8}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
                           & \multicolumn{4}{c}{Feature embedding-based} & \multicolumn{4}{c}{Generative model-based}                      \\ \cmidrule(l){2-5}  \cmidrule(l){6-9}
\multirow{-2}{*}{Category} & PatchCore~\cite{roth2022patchcore}   & RD4AD~\cite{deng2022rd4ad}      & RD++~\cite{tien2023rd++}       & SimpleNet~\cite{liu2023simplenet}  & DMAD~\cite{liu2023dmad}  & DRAEM~\cite{zavrtanik2021draem}     & FastFlow~\cite{yu2021fastflow}  & \textbf{Ours}      \\ \midrule
candle                     & 98.6/94.0     & 92.2/92.2  & 96.4/93.8   & \textbf{98.7}/89.0    & 92.7/90.6                   & 94.4/93.7 & 92.8/86.7 & \textbf{98.7/97.0}   \\
capsules                   & 81.6/85.5   & 90.1/56.9    & 92.1/95.8   & 89.9/91.4  & 88.0/88.4                     & 76.3/84.5 & 71.2/33.3 & \textbf{97.9/98.7} \\
cashew                     & 97.3/\textbf{94.5}   & \textbf{99.6}/79.0    & 97.8/91.2   & 97.5/82.8  & 95.0/88.8                     & 90.7/51.8 & 91.0/68.3   & 96.5/91.8 \\
chewinggum                 & 99.1/84.6   & 99.7/92.5    & 96.4/88.1   & 99.8/85.3  & 97.4/73.9                   & 94.2/60.4 & 91.4/74.8 & \textbf{99.9/92.2} \\
fryum                      & 96.2/85.3   & 96.6/81.0    & 95.8/90.0    & 98.1/87.8  & 98.0/92.2                     & 97.4/93.1 & 88.6/74.3 & \textbf{98.3/96.5} \\
macaroni1                  & 97.5/95.4   & 98.4/71.3    & 94.0/96.9    & 99.4/\textbf{98.9}  & 94.3/97.1                   & 95.0/96.7   & 98.3/77.7 & \textbf{99.5}/98.5 \\
macaroni2                  & 78.1/94.4   & 97.6/68.0    & 88.0/97.7    & 82.4/97.3  & 90.4/98.5                   & 96.2/\textbf{99.6} & 86.3/43.4 & \textbf{99.0/99.6}   \\
pcb1                       & 98.5/94.3   & 97.6/43.2    & 97.0/95.8    & 99.0/91.1    & 95.8/96.2                   & 54.8/24.8 & 77.4/59.9 & \textbf{99.2/96.9} \\
pcb2                       & 97.3/89.2   & 91.1/46.4    & 97.2/90.6  & \textbf{99.1}/91.0    & 96.9/89.3                   & 77.8/49.4 & 61.9/40.7 & \textbf{99.1/94.2} \\
pcb3                       & 97.9/90.9   & 95.5/80.3    & 96.8/93.1  & 98.5/93.0    & 98.3/93.6                   & 94.5/89.7 & 74.3/61.5 & \textbf{98.6/94.9} \\
pcb4                       & 99.6/90.1   & 96.5/72.2    & \textbf{99.8}/91.9  & 99.6/64.5  & 99.7/91.4                   & 93.4/64.3 & 80.9/58.8 & 98.9/\textbf{94.6} \\
pipe\_fryum                & \textbf{99.8}/95.7   & 97.0/68.3    & 99.6/95.6  & 99.7/91.7  & 99.0/95.3                     & 99.4/75.9 & 72.0/38.0     & \textbf{99.8/97.2} \\ \midrule
Average                    & 95.1/91.2   & 96.0/70.9    & 95.9/93.4  & 96.8/88.7  & 95.5/91.3                   & 88.7/73.7 & 82.2/59.8 & \textbf{98.8/96.0}   \\ \bottomrule
\end{tabular}}
\caption{Anomaly Detection and Localization on VisA~\cite{zou2022spd}. The best results of Image AUROC / Pixel PRO are highlighted in bold.}
\label{tab:visa image auroc and pixel pro}
\end{table*}


\subsection{Anomaly Detection and Localization Results}
We compare the anomaly detection and localization performance of DiffusionAD with four feature embedding-based methods, \ie, PatchCore~\cite{roth2022patchcore}, RD4AD~\cite{deng2022rd4ad}, RD++~\cite{tien2023rd++}, SimpleNet~\cite{liu2023simplenet} and three generative model-based methods, \ie, DMAD~\cite{liu2023dmad}, FastFlow~\cite{yu2021fastflow} and DRAEM~\cite{zavrtanik2021draem}.
\label{subsec:Anomaly Detection and Localization on four datasets}

\vspace{0.05in}
\noindent\textbf{Per-class performance on VisA.\quad}The results of anomaly detection (Image AUROC) and anomaly localization (PRO) on the VisA dataset are shown in \cref{tab:visa image auroc and pixel pro}. 
Our method achieves the highest image AUROC and the highest PRO in 10 out of 12 classes. 
The average image AUROC results show that our method outperforms feature embedding-based SOTA by 2.0\% and generative model-based SOTA by 3.3\%. 
Meanwhile, for PRO, our method outperforms feature embedding-based SOTA by 2.6\% and generative model-based SOTA by 4.7\%.
In some hard cases, such as capsules and macaroni2, DiffusionAD outperforms previous generative model-based methods by a large margin.




\begin{table*}[]
\centering
\scalebox{0.75}{
\begin{tabular}{@{}lcccccccccccccccccccc@{}}
\toprule
\multirow{2}{*}{} & \multicolumn{4}{c}{VisA~\cite{zou2022spd}}  & \multicolumn{4}{c}{MVTec~\cite{bergmann2019mvtec}} & \multicolumn{4}{c}{DAGM~\cite{wieler2007dagm}}  & \multicolumn{4}{c}{MPDD~\cite{jezek2021mpdd}}  & \multicolumn{4}{c}{\textbf{Average}}\\ \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17} \cmidrule(lr){18-21} 
                  & I $\uparrow$    & P $\uparrow$    & O $\uparrow$    & A $\uparrow$    & I $\uparrow$    & P $\uparrow$    & O $\uparrow$    & A $\uparrow$    & I $\uparrow$    & P $\uparrow$    & O $\uparrow$    & A $\uparrow$    & I $\uparrow$    & P $\uparrow$    & O $\uparrow$    & A $\uparrow$  & I $\uparrow$    & P $\uparrow$    & O $\uparrow$    & A $\uparrow$  \\ \midrule
PatchCore    & 95.1 & 98.8 & 91.2 & \multicolumn{1}{c|}{40.1} & 99.1 & 98.1 & 93.5 & \multicolumn{1}{c|}{56.1} & 93.6 & 96.7 & 89.3 & \multicolumn{1}{c|}{51.7} & 94.8 & \textbf{99.0}   & 93.9 & \multicolumn{1}{c|}{43.2} 
& 95.7 & 98.2   & 92.0 & 47.8\\ 
RD4AD        & 96.0 & 90.1 & 70.9 & \multicolumn{1}{c|}{27.7} & 98.5 & 97.8 & 93.9 & \multicolumn{1}{c|}{58.0} & 95.8 & \textbf{97.5} & 93.0 & \multicolumn{1}{c|}{53.4} & 92.7 & 98.7 & \textbf{95.3} & \multicolumn{1}{c|}{45.5}
& 95.8 & 96.0 & 88.3 & 46.2\\ 
RD++         & 95.9 & 98.7 & 93.4 & \multicolumn{1}{c|}{40.8} & 99.4 & 98.3 & 95.0 & \multicolumn{1}{c|}{60.8} & 98.5 & 97.4 & \textbf{93.8} & \multicolumn{1}{c|}{64.3} & 92.9 & 98.3 & 94.9 & \multicolumn{1}{c|}{43.7} 
&96.7	&98.2	&94.3	&52.4\\ 
SimpleNet    & 96.8 & 97.8 & 88.7 & \multicolumn{1}{c|}{36.3} & 99.6 & 97.6 & 90.0   & \multicolumn{1}{c|}{54.8} & 95.3 & 97.1 & 91.3 & \multicolumn{1}{c|}{48.1} & 96.1 & 97.6 & 91.2 & \multicolumn{1}{c|}{40.7}
&96.9	&97.5	&90.3	&45.0\\ \midrule 
DMAD         & 95.5 & 98.6 & 91.3 & \multicolumn{1}{c|}{41.0} & 99.6 & 98.2 & 90.6 & \multicolumn{1}{c|}{53.5} & 89.1 & 92.5 & 83.8 & \multicolumn{1}{c|}{46.0} & 91.7 & 98.0   & 92.6 & \multicolumn{1}{c|}{42.6}
&94.0	&96.8	&89.6	&45.8\\ 
FastFlow     & 82.2 & 88.2 & 59.8 & \multicolumn{1}{c|}{15.6} & 90.5 & 95.5 & 85.6 & \multicolumn{1}{c|}{39.8} & 87.4 & 91.1 & 79.9 & \multicolumn{1}{c|}{34.2} & 88.7 & 80.8 & 49.8 & \multicolumn{1}{c|}{11.5}
&87.2	&88.9	&68.8	&25.3\\ 
DRAEM        & 88.7 & 94.4 & 73.7 & \multicolumn{1}{c|}{30.5} & 98.0 & 97.3 & 92.1 & \multicolumn{1}{c|}{68.4} & 90.8 & 86.8 & 71.0 & \multicolumn{1}{c|}{30.6} & 94.1 & 91.8 & 78.1 & \multicolumn{1}{c|}{28.8}
&92.9	&92.6	&78.7	&39.6\\ 
\rowcolor[HTML]{e9e9e9} 
\textbf{Ours}  & \textbf{98.8} & \textbf{98.9} & \textbf{96.0}   & \multicolumn{1}{c|}{\textbf{45.8}} & \textbf{99.7} & \textbf{98.7} & \textbf{95.7} & \multicolumn{1}{c|}{\textbf{76.1}} & \textbf{99.6} & \textbf{97.5} & \textbf{93.8} & \multicolumn{1}{c|}{\textbf{65.1}} & \textbf{96.2} & 98.5 & \textbf{95.3} & \multicolumn{1}{c|}{\textbf{45.8}}
&\textbf{98.6}	&\textbf{98.4}	&\textbf{95.2}	&\textbf{58.2}\\ \bottomrule 
\end{tabular}}
\caption{Comparison of DiffusionAD with other approaches on four datasets. ``I'', ``P'', ``O'' and ``A'' respectively refer to the four metrics of image auroc, pixel auroc, pixel pro and pixel ap. Best results are highlighted in bold.}
\label{tab:results of four datasets}
\vspace{-0.2cm}
\end{table*}



\vspace{0.05in}
\noindent\textbf{Quantitative results across the four datasets.\quad}\cref{tab:results of four datasets} enumerates the performance of the aforementioned methods across the four datasets. We conduct a comprehensive comparison based on four metrics, which include image AUROC for anomaly detection and pixel AUROC, PRO, and pixel AP for anomaly localization. 
In addition to its performance on the VisA dataset, DiffusionAD achieves state-of-the-art results on another widely used dataset, MVTec, across four key metrics. 
Remarkably, DiffusionAD outperforms the previous feature embedding-based SOTA by 15.3\% and the previous generative model-based SOTA by 7.7\% in terms of average pixel AP metric. 
Our method also outperforms previous approaches on the DAGM and MPDD datasets, especially in terms of average image AUROC.
Finally, we evaluate the generalization ability of the method by measuring its average performance across the four datasets. 
As shown in \cref{tab:results of four datasets}, our approach significantly outperforms previous methods and in particular, outperforms previous generative models by a large margin.  More details will be provided in \cref{sec: appendix experiments}.


\begin{figure}[h]
\vspace{-0.2cm}
  \centering
  \includegraphics[width=0.85\linewidth]{figs/visualization.pdf}
  \caption{Qualitative results on MVTec~\cite{bergmann2019mvtec} and VisA~\cite{zou2022spd}. %
}
  \label{fig:visualization}
  \vspace{-0.3cm}
\end{figure}



\vspace{0.05in}
\noindent\textbf{Qualitatively evaluate on anomaly localization.\quad}According to the results in \cref{fig:visualization}, we qualitatively assess the performance of DiffusionAD on anomaly detection and localization compared to the previous feature embedding-based method PatchCore~\cite{roth2022patchcore} and generative model-based method DRAEM~\cite{zavrtanik2021draem}. 
These images are sub-datasets of MVTec~\cite{bergmann2019mvtec} and Visa~\cite{zou2022spd}, where the anomalous regions vary in shape, size and number, as shown in the first and second columns of \cref{fig:visualization}.
The third and fourth columns show the reconstruction and anomaly localization results of DiffusionAD, respectively. It can be observed that the reconstruction sub-network successfully repairs various types of anomalies while maintaining high image fidelity. After that, the segmentation sub-network accurately predicts the pixel-wise anomaly score using the input image and its anomaly-free reconstruction, leading to more accurate anomaly localization than previous methods such as DRAEM~\cite{zavrtanik2021draem} and PatchCore~\cite{roth2022patchcore}.







\subsection{Ablation Study}
\label{subsec:Ablation Study}


\textbf{The importance of architecture and denoising paradigm.\quad}\cref{tab:ablation on module} illustrates the impact of different architectures and denoising methods on performance and inference speed. We introduce FPS (Frames Per Second) to evaluate the inference speed. As indicated in the first row of \cref{tab:ablation on module}, the performance of anomaly detection and localization is limited due to the unsatisfactory reconstruction results of the Autoencoder (AE.). However, its inference speed is notably fast. 
When we adopt the proposed noise-to-norm paradigm, as seen in the second row, which utilizes higher-quality reconstructions from the diffusion model (DM.), the performance is significantly enhanced. Nonetheless, each denoising iteration (Ite.) corresponds to one network inference (in this case, 400 iterations in total), which falls significantly short of the real-world requirements for inference speed.
In the third row, when we employ the proposed one-step (OS.) denoising, the inference speed is approximately 300 times faster than the iterative approach. 
Simultaneously, there is an improvement in both anomaly detection and localization performance. 
We attribute this to the comparable quality of one-step reconstruction results and the consistent training and inference paradigm, where the segmentation sub-network always receives the one-step reconstructions during training.
In the fourth row, the norm-guided (NG.) paradigm combines the advantages of both one-step reconstructions from different noise scales, leading to further performance enhancements while maintaining comparable inference speed.

\begin{table}[]
\centering
\scalebox{0.75}{
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\multicolumn{3}{c}{Architecture} & \multicolumn{3}{c}{Denoising } & \multicolumn{5}{c}{Performance}  \\ \cmidrule(lr){1-3} \cmidrule(lr){4-6} \cmidrule(lr){7-11}
Seg.                & AE.               & DM.              & Ite.      & OS.        & NG.           & I $\uparrow$    & P $\uparrow$    & O $\uparrow$    & A $\uparrow$    & F $\uparrow$    \\ \midrule
$\checkmark$        & $\checkmark$      &                   &               &                     &               & 90.6 & 96.2 & 76.1 & 31.9 & \textbf{32.4} \\
$\checkmark$        &                   & $\checkmark$      & $\checkmark$                        &                 &     & 96.8 & 98.5 & 92.5   & 42.5 & 0.09 \\
$\checkmark$        &                   & $\checkmark$      &               & $\checkmark$        &                 & 98.0 & 98.6 & 94.7 & 44.2 & 26.7 \\
$\checkmark$        &                   & $\checkmark$      &               & $\checkmark$        & $\checkmark$   & \textbf{98.8} & \textbf{98.9} & \textbf{96.0}   & \textbf{45.8} & 23.5 \\ \bottomrule
\end{tabular}}
\caption{Impact of modules and denoising paradigms.}
\label{tab:ablation on module}
\vspace{-0.5cm}
\end{table}


\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/one_step_vs_iterative_recon.pdf}
  \caption{Impact of noise scale on anomaly-free reconstruction.}
  \label{fig:one step vs iterative recon}
  \vspace{-0.3cm}
\end{figure}
\vspace{0.05in}
\noindent\textbf{The effect of noise scale.\quad}We investigate how the added noise scale of the input individually affects the reconstruction results and anomaly detection performance. 
First, \cref{fig:one step vs iterative recon} illustrates the effect of the noise scale on the quality of the reconstruction.
The second column of \cref{fig:one step vs iterative recon}  shows the gradual denoising results from $t = 400$, which can be considered as the ideal recovery.
For one-step reconstruction, direct prediction becomes more challenging as the noise scale increases, resulting in lower pixel quality. However, repairing more pronounced anomalies often requires larger noise scales to perturb the anomalies, leading to reconstructions with higher semantic quality.
Thus, the norm-guided one-step approach combines the advantages of both scales, resulting in a reconstruction that encompasses both pixel and semantic quality, closely approximating the quality of the iterative one.


\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/one_step_vs_iterative_seg.pdf}
  \caption{Impact of noise scale on anomaly detection performance.}
  \label{fig:one step vs iterative seg}
  \vspace{-0.5cm}
\end{figure}

\cref{fig:one step vs iterative seg} further depicts the effect of different noise scales on the anomaly detection performance. In the iterative approach, the predictions of the segmentation network show little variation, as the reconstruction results are consistent. However, the inference time increases with $t$.
In the one-step denoising approach, the inference time remains constant, but there is a decrease in Image AUROC as $t$ becomes relatively large, mainly due to the reduced reconstruction quality.
It is worth noting that the proposed one-step denoising paradigm consistently achieves superior anomaly detection performance compared to the iterative one.


\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/guidance_scale_visual.pdf}
  \caption{Ablations of guidance scale.}
  \label{fig:guidance_scale_visual}
  \vspace{-0.4cm}
\end{figure}

\vspace{0.05in}
\noindent\textbf{Effects of guidance scale.\quad}As shown in \cref{fig:guidance_scale_visual}, the influence of conditional information becomes more significant as the guidance scale $w$ increases. In this paper, $w$ is set to 1 as an empirical choice.

\vspace{-0.2cm}