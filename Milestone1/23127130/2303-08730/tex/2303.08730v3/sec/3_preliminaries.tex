\section{Preliminaries}
\label{sec:preliminaries}
\textbf{Denoising diffusion models.\quad}A family of generative models called denoising diffusion models~\cite{ho2020ddpm,song2020ddim,rombach2022ldm,sohl2015nonequilibrium} are inspired by equilibrium thermodynamics~\cite{song2020scoregm,song2019gradientsgm}. In particular, a diffusion probabilistic model specifies a forward diffusion phase in which the input data are progressively perturbed by adding Gaussian noise over several steps and then learns to reverse the diffusion process to recover the desired noise-free data from the noisy data. 
The forward noise process is defined as:
 \begin{equation}
\begin{aligned}
    x_{t}=x_{0} \sqrt{\bar{\alpha}_{t}}+\epsilon_{t} \sqrt{1-\bar{\alpha}_{t}}, \quad \epsilon_{t} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
\end{aligned}
\label{eq:xt}
\end{equation}
which transforms a data sample $x_{0}$ into a noisy sample $x_{t}$. Here, $t$ is randomly sampled from $\{0, 1, ... , T\}$, $\bar{\alpha}_{t}=\prod_{i=0}^{t} \alpha_{i}=\prod_{i=0}^{t}(1-\beta_{i})$ and $\beta_{i} \in(0,1)$ represents the noise variance schedule. This can be defined as a small linear schedule~\cite{sohl2015nonequilibrium} from $\beta_{1}=10^{-4}$ to $\beta_{T}=10^{-2}$. During training, a U-Net~\cite{ronneberger2015U-Net}-like architectures $\epsilon_{\theta}\left(x_{t}, t\right)$ is trained to predict $\epsilon$ by minimizing the training objective:
\begin{equation}
\begin{aligned}
\mathcal{L}=\mathbb{E}_{t \sim[1-T], x_{0} \sim q\left(x_{0}\right), \epsilon \sim \mathcal{N}(0, \mathbf{I})}\left[\left\|\epsilon-\epsilon_{\theta}\left(x_{t}, t\right)\right\|^{2}\right].
\end{aligned}
\label{eq:diffusion loss}
\end{equation}
At inference stage, $x_{t-1}$ is reconstructed from noise $x_{t}$ with the model $\epsilon_{\theta}\left(x_{t}, t\right)$ according to:
\begin{equation}
\begin{aligned}
x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \epsilon_{\theta}\left(x_{t}, t\right)\right)+\tilde{\beta}_{t} z.
\end{aligned}
\label{eq:xt-1}
\end{equation}
where $z \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ and $\tilde{\beta}_{t}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \beta_{t}$. $x_{0}$ is reconstructed from $x_{t}$ in an iterative way, 
\ie, $ x_{t} \rightarrow x_{t-1} \rightarrow \ldots \rightarrow x_{0}$.


\vspace{0.05in}
\noindent\textbf{Classifier guidance and image guidance.\quad}To improve the quality and diversity of generated samples, guided diffusion~\cite{dhariwal2021diffusionbeatgans} uses a pre-trained classifier $p_\phi(y|x_t)$ to guide the diffusion sampling process, where $y$ is the class label. The guiding process is defined as modifying the noise prediction by a guidance scale $w$:
\begin{equation}
\begin{aligned}
\epsilon_\theta(x_t,t,y)=\epsilon_\theta(x_t,t)-\sqrt{1-\bar{\alpha}_t}\mathrm{~}w\nabla_{x_t}\log p_\phi(y|x_t)
\end{aligned}
\label{eq:classifier guidance}
\end{equation}
Thus, the impact of class $y$ on the generated results can be controlled by adjusting the parameter $w$.

To achieve a more diverse form of control, SDG~\cite{liu2023morecontrol} utilizes the reference image $r$ to guide the sampling process and reformulates \cref{eq:classifier guidance} as follows:
\begin{equation} 
\begin{aligned}
\epsilon_\theta({x_t,t,r_t)=\epsilon_\theta(x_t,t)-\sqrt{1-\bar{\alpha}_t}\mathrm{~}w\nabla_{x_t}sim(x_t,r_t)}
\end{aligned}
\label{eq:image guidance}
\end{equation}
where $r_t$ is obtained from \cref{eq:xt} by perturbing $r$ and $sim( \cdot, \cdot )$ is a measure of the similarity or correlation between two images.
