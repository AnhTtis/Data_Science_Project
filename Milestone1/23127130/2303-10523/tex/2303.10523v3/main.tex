\documentclass[journal]{IEEEtai}

\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{color,array}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{caption}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{changes}
\setcounter{page}{1}

\input{defs}

\begin{document}

\title{Unsupervised Interpretable Basis Extraction for Concept -- Based Visual Explanations} 

\author{Alexandros Doumanoglou, Stylianos Asteriadis, and Dimitrios Zarpalas
\thanks{A. Doumanoglou is with the Information Technologies Institute, Centre for Research and Technology HELLAS, Thessaloniki, Greece and the Department of Advanced Computing Sciences, University of Maastricht, Maastricht, Netherlands (e-mail: aldoum@iti.gr).}
\thanks{S. Asteriadis was with the Department of Advanced Computing Sciences, University of Maastricht, Maastricht, Netherlands (e-mail: stelios.asteriadis@maastrichtuniversity.nl). 
}
\thanks{D. Zarpalas is with the Information Technologies Institute, Centre for Research and Technology HELLAS, Thessaloniki, Greece (e-mail: zarpalas@iti.gr).}
}

\markboth{}
{A. Doumanoglou \MakeLowercase{\textit{et al.}}: UIBE}

\maketitle
\begin{abstract}

An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human-understandable concepts. Previous work supports that deep representations are linearly separable with respect to their concept label, implying that the feature space has directions where intermediate representations may be projected onto, to become more understandable. These directions are called interpretable, and when considered as a set, they may form an interpretable feature space basis. Compared to previous top-down probing approaches which use concept annotations to identify the interpretable directions one at a time, in this work, we take a bottom-up approach, identifying the directions from the structure of the feature space, collectively, without relying on supervision from concept labels. Instead, we learn the directions by optimizing for a sparsity property that holds for any interpretable basis. We experiment with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to existing basis interpretability metrics and show that intermediate layer representations become more interpretable when transformed with the extracted bases. Finally, we compare the bases extracted with our method with the bases derived with supervision and find that, in one aspect, unsupervised basis extraction has a strength that constitutes a limitation of learning the basis with supervision, and we provide potential directions for future research.


%An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human-understandable concepts. The success of linear probing followed in previous works indicates that deep representations of image patches are linearly separable based on their concept label. The weight vectors of the linear probes constitute directions which are called interpretable due to their applicability in detecting human-understandable concepts. A set of interpretable directions may form an interpretable feature space basis where feature representations may be projected onto, to become more understandable. Compared to previous top-down probing approaches which use concept annotations to identify the interpretable directions one at a time, in this work, we take a bottom-up approach, identifying the directions collectively as a set from the structure of the feature space, without relying on supervision from concept labels. Instead, our approach learns the directions set by optimizing for a sparsity property that every interpretable basis meets. We experiment with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics, we compare the bases extracted with our method with the bases derived with a supervised approach and find that, in one aspect, the proposed unsupervised approach has a strength that constitutes a limitation of the supervised one and give potential directions for future research.



%An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics, we compare the bases extracted with our method with the bases derived with a supervised approach and find that, in one aspect, the proposed unsupervised approach has a strength that constitutes a limitation of the supervised one and give potential directions for future research.
\end{abstract}

\begin{IEEEImpStatement}
CNN image classifiers have demonstrated outstanding performance in real-world tasks. They can be used in robotics, visual understanding, automatic risk assessment, and more. However, to a human expert, CNNs are often black-boxes and the reasoning behind their predictions can be unclear.  Recent advances in explainable and interpretable artificial intelligence (XAI and IAI) attempt to shed light on this process. In an attempt to understand intermediate layer representations, one can project them onto a feature space basis that quantifies the presence of different concepts in the representation. This basis is called interpretable because it can make representations more understandable. In the typical approach, constructing an interpretable basis requires access to annotations. This work proposes a novel unsupervised method to learn such a basis, without the need for explicit labels. This can ease the process of obtaining explanations, eliminate annotation costs, save time, and eventually help humans debug and trust deep models.

\end{IEEEImpStatement}

\begin{IEEEkeywords}
Explainable Artificial Intelligence (XAI), Interpretable Artificial Intelligence (IAI), Interpretable Basis, Unsupervised Learning.
\end{IEEEkeywords}

\vspace{5cm}
\section{Introduction}
\label{sec:introduction}
\input{sections/introduction}

\section{Background \& Related Work}
\label{sec:related_work}
\input{sections/related_work}

\section{Motivation}
\label{sec:motivation}
\input{sections/motivation}

\section{Proposed Method}
\label{sec:proposed_method}
\input{sections/proposed_method}

\section{Basis orthogonality}
\label{sec:orthogonality}
\input{sections/orthogonality}

\section{Evaluation Metrics}
\label{sec:evaluation_metrics}
\input{sections/evaluation_metrics}

\section{Experimental Results}
\label{sec:experimental_results}
\input{sections/experimental_results}
\vspace{-7pt}
\section{Conclusion}
\label{sec:conclsion}
\input{sections/conclusion}

\input{sections/qualitative}




\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,refs.bib}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/alex.jpg}}]{Alexandros Doumanoglou}{\space} received the Diploma in
electrical and computer engineering from Aristotle University of Thessaloniki, Thessaloniki, Greece, in 2009 and joined the Information Technologies Institute, in 2012. Currently, he is working toward
the Ph.D. degree in explainable artificial intelligence at the Department of Advanced Computing Sciences of Maastricht University, The Netherlands.
His current research focuses on unsupervised learning and explainable and interpretable methods for deep learning models.
\end{IEEEbiography}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/stelios.png}}]{Stylianos Asteriadis}{\space} received the diploma of Electrical and Computer Engineer from Aristotle University of Thessaloniki, Thessaloniki, Greece in 2004, the M.Sc. degree in digital media from the School of Informatics at the same university in
2006, and the Ph.D. in Electrical and Computer Engineering from the National Technical University of Athens, Athens, Greece, in 2011.
He was an Associate Professor at the Department of Advanced Computing Sciences at Maastricht University, Maastricht, The Netherlands, until the
final acceptance of this paper, where he coordinated the Cognitive Systems
Group. He is currently working at the European Commission.\footnote{The information and views set out in this article are those of the authors and do not necessarily reflect the official opinion of the Institution.} 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/dimitris.jpg}}]{Dimitrios Zarpalas}{\space} received the Diploma in electrical and computer engineering from Aristotle University of Thessaloniki (A.U.Th), Thessaloniki, Greece in 2003, the M.Sc. degree in electrical engineering from the Pennsylvania State University, Philadelphia, USA, in 2006, and the Ph.D. degree in medical informatics from the Department of Medicine, Health Science School, A.U.Th, in 2014.
He joined the Information Technologies Institute, Thessaloniki, Greece, in 2007, where he is currently a Researcher, grade B. His research interests include tele-immersion applications, 3-D computer vision, 3-D object recognition, and motion capturing.
\end{IEEEbiography}

\end{document}
