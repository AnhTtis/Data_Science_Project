\begin{figure}[t]
    \centering
    \includegraphics[width=3.2in]{figs/directions-orthogonality.png}
    \caption{ The natural basis of the feature space is given by $\mathbf{e_0}, \mathbf{e_1}$. \textbf{Left:} An interpretable direction is the direction of the feature space along which, the feature representations of a concept lie. \textbf{Middle:} A case where the \textcolor{purple}{hyperplane} normals of two concept detectors (\textit{car}, \textit{red}) are not orthogonal. In this case, the feature representation of a \textit{car} is also classified as \textit{red} and vice versa. Consequently, $\textit{car}$ and \textit{red} are positively correlated and not (linearly) disentangled. \textbf{Right:} For a pair of mutually-exclusive concepts, the \textcolor{purple}{hyperplane} normals of the two concept detectors may form an angle greater than $90^\circ$. However, in a large dimensional feature space with several detectors of mutually-exclusive concepts, the maximum angle between all pairs of hyperplane normals, is approximately $90^\circ$.}
    \label{fig:concept-directions}
\end{figure}
\begin{figure*}[t]
    \centering
    \includegraphics[width=7.1in]{figs/motivation.png}
    \caption{\textbf{Left:} In a standard convolution layer with $D$ filters, all the filters work together to transform each input patch to a feature vector of spatial dimensionality $1 \times 1$. Each spatial element $\p$ of the transformed representation, is assigned a feature vector $\xp \in \R^D$ which lies in the co-domain of the transformation function. Thus the dimensionality of the feature space equals the number of filters in the layer, and each spatial element of the transformed representation, constitutes a sample of this feature space. \textbf{Middle:} To find an interpretable basis for the aforementioned feature space in a supervised way, it means to train a set of linear classifiers (concept detectors), one for each interpretable concept, to separate feature vectors corresponding to image patches with and without the concept. \textbf{Right:} In case classifier training succeeds, the application of the classifier rule ($\w^T \xp - b > 0$) to each spatial element of the representation $\xp$, produces a binary mask which is active for pixels corresponding to image patches containing the concept. We observe, that in a successfully learned interpretable basis, a single pixel is classified positively by at most one classifier, among a group of classifiers that are trained to detect mutually-exclusive concepts.} 
    \label{fig:motivation}
\end{figure*}


\IEEEPARstart{D}{espite} the impressive performance of convolutional neural networks (CNNs) in computer vision image classification tasks \cite{NIPS2012_Krizhevsky_AlexNet,simonyan2014VGG,Szegedy_GoogleNet_2014,he2016ResNet}, the understanding of their inner workings still remains a challenge. In an attempt to shed light on the CNN ``black-box'', the scientific community tries to understand the properties of the intermediate layers' feature space. Early research \cite{szegedy2013intriguing} showed that any possible direction in this feature space may have a semantic meaning, i.e., feature vectors that maximally activate a direction correspond to image patches that share some sort of semantic concept. For instance, image patches of \textit{car doors}, \textit{cat heads} or \textit{people's faces} maximally activate  different directions of this feature space. Beyond this early result, more recently, rigorous experimentation showed that linear separability of features corresponding to different semantic concepts increases towards the top layer \cite{alain2016_Understanding_Linear_Classifiers}. The latter has been attributed to the top layer's linearity and the fact that intermediate layers are enforced to produce representations that are helpful to solve the task at hand.

The fact that linear separation of concept representations is possible (especially for layers near the top) \cite{szegedy2013intriguing}, has motivated attempts in finding feature space directions for specific concepts \cite{kim_TCAV} and constructing an interpretable feature space basis \cite{Zhou_Interpretable_Basis_Decomposition}. In an interpretable basis, each basis vector points towards the direction of a concept's representations. Projecting a representation onto a basis vector quantifies the presence of the respective concept in the representation. An interpretable basis can help to obtain possible explanations regarding the CNN and its predictions. When considering the basis vectors as concept embeddings, an interpretable basis can be used to explain the relationship between concepts and filters, similar to what was proposed in \cite{fong_Net2Vec}. Moreover, it can also be used to interpret predictions of individual examples \cite{Zhou_Interpretable_Basis_Decomposition}, or used to quantify the class sensitivity of the CNN with respect to a concept \cite{kim_TCAV, RTCAV}.

In the typical approach for computing an interpretable basis, the set of interpretable concepts needs to be defined in the form of an annotated concept dataset. Using this dataset, one may have access to labels for intermediate CNN representations. These can be subsequently used to find the orientation of the hyperplane that separates the representations of a concept with respect to representations of other concepts \cite{kim_TCAV}, \cite{Zhou_Interpretable_Basis_Decomposition}. The interpretable basis is constructed by directly using the hyperplane normals as basis vectors.  As with any other supervised approach, using an annotated concept dataset to construct an interpretable basis may increase the fidelity of explanations obtained via that basis. However, this comes at the cost of obtaining the annotations, which is even more prominent when annotations need to be dense (per-pixel) \cite{Zhou_Interpretable_Basis_Decomposition}. Additionally, annotated concept datasets are domain-specific, and thus, explaining CNN classifiers for different domains can become even more costly.

The motivation of this work is based on an observation of how an interpretable basis transforms representations. We explain by examples that projecting a representation onto an interpretable basis and hard thresholding results in a new, sparse, binary representation. Thus, we propose a method that is able to suggest a feature space basis which satisfies this property that holds for interpretable bases. In contrast to the typical approach, the proposed method learns the basis directly from the structure of feature space representations, without requiring access to semantic annotations. In that sense, our method can be considered to be unsupervised. However, without annotations, the final suggested basis vectors are not assigned an explicit concept name. In a real-world setting, the concept name associated with a basis vector could be identified by inspecting samples of image patches whose projected representations onto the vector are maximum. For evaluation purposes though, a procedure to label the basis is required, by assigning a concept label to each basis vector, as in \cite{bau_NetDissection_CVPR} or \cite{mu2020compositional}.

Our work's contributions can be summarized to the following: (i) We present a \textbf{post-hoc}, unsupervised method that suggests an interpretable basis for the feature space of a CNN's intermediate layer. Since post-hoc, the proposed method applies to pre-trained CNN architectures and does not require any form of retraining them. %
(ii) Inspired by related work, we propose simple extensions for two basis interpretability metrics. (iii) We provide a quantitative evaluation of our method on extracting an interpretable basis for the last layer of popular CNNs, demonstrating applicability to standard architectures. We show that our method is able to improve on the interpretability metrics compared to the interpretability of the natural basis \cite{bau_NetDissection_CVPR}, and also compare against a supervised approach \cite{Zhou_Interpretable_Basis_Decomposition} to set a baseline for future works and discuss interesting findings that may help future research.
