We presented an unsupervised, post-hoc method to extract an interpretable basis for the feature space of a CNN's intermediate layer. Based on current literature, we also proposed two metrics that can be used to measure a basis for its interpretability. We evaluated the effectiveness of the proposed method in standard CNN architectures and demonstrated that intermediate layer representations become more interpretable when projected onto bases extracted with our method. Finally, using the proposed metrics, we compared the outcomes of our method with the outcomes of a method that derives an interpretable basis using supervision. According to the  interpretability metrics, the bases extracted with the proposed method, in one aspect, show appreciable interpretability improvements over the bases extracted with the supervised approach. At the same time, in a second aspect, the bases derived with supervision were significantly more interpretable than the bases that were suggested by our method. This fact might seem peculiar at first. However, a possible explanation was provided and directions for future research for deeper understanding were suggested. We hope that the present work has contributed additional knowledge to interpretable basis extraction and motivates further research for understanding \textit{black-box} models.