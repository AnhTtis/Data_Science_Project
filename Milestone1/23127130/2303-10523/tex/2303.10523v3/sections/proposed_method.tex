\begin{figure}
    \centering
    \includegraphics[width=3.3in]{figs/concept.png}
    \caption{Overview of the proposed method. Without any form of annotation, our method solves for hyperplane normal directions and thresholds of potential binary and linear concept classifiers, driven by the objective that for a single pixel of an image's representation, only a fraction of the classifiers make positive predictions. 
    The application of the linear classification rules to each pixel in the intermediate representation is accomplished by $1\times 1$ convolution between the image representation $\X \in \R^{H\times W \times D}$ and the classifiers' hyperplane normal directions $\wi \in \R^D$, followed by bias subtraction and application of the sigmoid activation function.
    While solving for that objective, the name of the concept that each classifier detects is unknown. In case annotations exist, labeling the basis can be achieved, in a post-processing step, by using methods in related work. In absence of annotations, the concepts can be identified by inspecting samples that the classifiers classify positively.}
    \label{fig:concept}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=3.1in]{figs/sparsity_rotation.png}
    \caption{Consider two feature space basis vectors (first -  \textcolor{red}{red}, second - \textcolor{softgreen}{green}) located at the origin (black). Furthermore, consider each feature space basis vector to be accompanied by a bias threshold which together with the basis vector direction constitutes a linear classifier, with the basis vector pointing towards the direction of higher positive classification confidence. On the \textbf{left}, the classifers' separating hyperplanes have been
placed at the location of the bias and indicated by (\textcolor{darkred}{dark red} and \textcolor{darkgreen}{dark green}) dashed lines. Let a feature (\textcolor{purple}{purple}) lie in this space. The projection points of the feature vector on the basis vectors are marked by \textcolor{darkgray}{gray} circular markers. The bottom horizontal and left vertical axes correspond to the standard feature space x and y axis, respectively. The right (top) vertical axis reports the \textcolor{red}{first} (\textcolor{softgreen}{second}) classifier's confidence levels for each point projected in the basis vector's direction. The exact confidence of the classifier at each projected point on its direction, is given by the sigmoid activation function depicted with the dotted (\textcolor{darkred}{dark red}) (\textcolor{darkgreen}{dark green}) curve. On the \textbf{left}, the classifiers attribute the presence of two concepts in the feature, since the projection of the feature on both basis vectors, exceeds the classifiers' biases by a large margin. This is indicated by (\textcolor{darkred}{dark red}, \textcolor{darkgreen}{dark green}) shaded areas under the sigmoid curves. The figure in the \textbf{middle}, depicts the same situation under a rotation of the basis vectors. In that case, the \textcolor{red}{first} classifier makes a confident positive prediction ($\sigma(\cdot) \approx 1.0$) (\textcolor{darkred}{dark red} sigmoid shaded area) for the feature, whereas the \textcolor{softgreen}{second} one makes a confident negative prediction ($\sigma(\cdot) \approx 0.0$) (\textcolor{softgreen}{soft green} sigmoid shaded area). The figure on the \textbf{right} depicts rotation with perfect alignment, where only one of the classifiers classifies the feature positively with high confidence.}
\label{fig:sparsity_rotation}
\end{figure}


In a typical convolutional neural network (CNN) that is trained for image classification, the intermediate layer representations have a cuboid structure. For a convolutional layer, those representations are calculated by applying the same transformation function (a series of dot products equal in number to the number of filters in the layer) to cuboid patches sliced from the representation of the layer beneath. Thus, the dimensionality of the layer's feature space equals the co-domain dimensionality of this transformation function. In this case, this dimensionality is equal to the number of filters in the layer. Feature vectors at different spatial locations of the cuboid, correspond to different samples from this feature space (Fig. \ref{fig:motivation} - left). This treatment of the feature space has been also considered in \cite{bau_NetDissection_CVPR, Zhou_Interpretable_Basis_Decomposition,fong_Net2Vec, mu2020compositional}

Let $D \in \N^+$ denote the dimensionality of a layer's feature space, and $\xp \in \R^D$ an element in this space at the spatial location $\p=(x,y)$ (Fig. \ref{fig:motivation} - left). In a convolutional layer, $D$ equals the layer's total number of hidden units or, as otherwise mentioned, output channels. %
Let's consider a set of $I \in \N^+, I \leq D$ linear classifiers to form a (possibly) interpretable basis. The $i$-th classifier is characterized by its hyper-plane's normal direction $\w_i \in \R^D$ and bias $b_i \in \R$, $i\in \I ,\, \I = \{0,1,...,I-1\}$. Additionally, each one of those classifiers is responsible to quantify the presence of one concept in $\xp$. Last, for the reasons discussed in the Section \ref{sec:orthogonality}, we also consider $\wi^T \wi = 1 \, \forall i,\, \w_i^T\w_j = 0 \, \forall \, i,j : i \neq j$, i.e. $\{\w_i\}$ should form an orthonormal basis. We consider $I$ to be a hyper-parameter of the method and, without loss of generality, when $I<D$, the orthogonal basis can be trivially completed to dimensionality $D$ in order to represent a rotation of the feature space. The additional $D-I$ directions can be considered as a non-interpretable \textit{residual}.

The overall concept of our method is depicted in Fig. \ref{fig:concept}. First, we record CNN intermediate layer representations for images coming from an unlabeled dataset. Starting from the representation of an image $\X$, we project each spatial element $\xp$ onto all the vectors of the basis $\wi$ via $1\times 1$ convolution. This operation transforms each pixel of the image representation to the new basis. The result is a new, transformed, cuboid representation. In the new representation, the pixel $\p$ of the $i$-th feature map has a value equal to the projection of $\xp$ onto $\wi$. Subsequently, we threshold the projections with a learned bias $b_i$ and use a sparsity objective to enforce each pixel to have a sparse thresholded representation across feature maps.

To formalize all the previous discussion, consider the standard binary sigmoid classifier $\sig(\wi^T\xp-b_i)$ which, since $||\wi||=1$ and for full expressivity,  requires an additional parameter $M_i \in \R^+$, such that, $\ypi = \sig(\frac{1}{M_i}(\wi^T\xp-b_i))$. $M_i$ is controlling the margin between the abscissas corresponding to the extremas of the sigmoid and $\ypi \in (0,1)$ denotes the confidence of classifier $i$ to classify $\xp$ positively. Without loss of generality and for mathematical and implementation convenience, we standardize the feature space with batch normalization \cite{ioffe2015batch} and without affine parameters. We do so, just after projecting $\xp$ to $\wi$ and before subtracting the bias $b_i$ or dividing by $M_i$. As already mentioned, the projection of each $\xp$ to the new basis is accomplished via standard $1\times 1$ convolution with $D$ input and $I$ output channels. While searching for $\wi$, the standardization of the feature space allows treating the magnitude of projections $\xp^T\wi$, biases $b_i$, and margin coefficients $M_i$ in the same scale, respectively, regardless of $i$. Thus, this allows us to make a simplification to the parameter space of our model and consider $b_i = b$ and $M_i = M$ (i.e. equal biases and margins in the standardized space) for all $i$. Orthogonality of the extracted basis is enforced by using \cite{lezcano2019trivializations}. The learnable parameters of our model are simply $\wi, b$ and $M$, while $b_i$ and $M_i$ can be later recovered by inverting the standardization process. A graphical explanation of the principal idea in the 2D feature space is provided in Fig. \ref{fig:sparsity_rotation} and the pipeline of the proposed method is given in Fig. \ref{fig:method}.

In the rest of the section, we introduce the loss terms that we use to derive an interpretable basis. For notation convenience, we assume $\p$ to vary across the spatial dimensions of all image representations in the dataset. Moreover, a pixel $\p$ is considered to be assigned to the $i$-th concept detector, when for the given $\xp$, $\ypi \gg 0.5$. In that case, we also say that $\xp$ is classified positively by the same concept detector. In a similar analogy, we mention $\xp$ to be classified negatively by the $i$-th concept detector whenever $\ypi \ll 0.5$.

\noindent\textbf{Sparsity Loss (SL)} Let $\yp = [\ypk{0},\ypk{1},...,\ypk{I-1}]^T$ denote the vector of activations containing the classification results for $\xp, \forall i \in \I$. The criterion that guides our search for $\wi$ implies sparsity in this vector of activations. Under the sparsity criterion, each pixel $\p$ is classified positively only by a portion of the classifiers in the new basis. We use entropy as a sparsity measure and define the sparsity loss $\loss^{s}$ as:
\begin{equation}
\label{eq:sparsity_loss}
\loss^{s} = \averagep\Big[-\sum_{i\in \I}\qpi\logtwo(\qpi)\Big]
\end{equation}
with
\begin{equation}
\label{eq:relative_scaling}
\qpi = \frac{\ypi}{\sum_{i \ \in \I}\ypk{i}}
\end{equation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=7.1in]{figs/method.png}
    \caption{The basis learning pipeline of the proposed method. Learnable parameters are given in purple next to the operations that actually use them.}
    \label{fig:method}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=3.4in]{figs/sparsity_not_enough.png}
    \caption{An example why pixel activation sparsity, which is enforced through entropy, is not alone sufficient to provide a meaningful basis. Since entropy can be applied only on probability distributions, $\yp$ is L1 normalized to $\qp$ before enforcing sparsity. This may lead to a set of classifiers that satisfy the sparsity criteria on $\qp$ but actually none of the classifiers classifies $\xp$ positively (i.e. with high confidence $\gg 0.5$). Thus, $\xp$ has no concept assigned to it. If we exaggerate to many $\p$, this may lead to a basis that does not classify positively any of the pixels. In the figure, $\yp_2$ is derived by scaling $\yp_1$ by 0.35. While both $\p_1$, $\p_2$ have sparse activation in the probability scale (described by $\qp$), only $\p_1$ has a concept assigned to it with high confidence. To mitigate this, we introduce the maximum activation loss which enforces strong activation magnitudes from the most positively confident classifiers.}
    \label{fig:sparsity_not_enough}
\end{figure}

\noindent\textbf{Maximum Activation Loss (MAL)} The sparsity criterion alone is not sufficient to extract a meaningful basis. This is better understood when considering the fact that entropy is applied in a relative scaling of the activation magnitudes, due to 
(\ref{eq:relative_scaling}). Thus, when optimizing $\loss^{s}$
alone, a pixel's activations may be considered sparse by eq (\ref{eq:sparsity_loss}), but $\xp$ might still be classified negatively by all concept detectors in the basis, i.e. $\mathop{\max}_{i} \ypi < 0.5$. For a meaningful basis, we would like to have each pixel assigned to at least one concept detector. To this end, we add an additional loss term $\loss^{ma}$ that encourages the most confident concept detector to not only be the most confident in a relative scale (compared to other concept detectors) but also in an absolute scale, reporting high confidence levels towards $1$ (Fig. \ref{fig:sparsity_not_enough})
\begin{equation}
\label{eq:max_act_loss}
\loss^{ma} = \averagep\Big[-\sum_{i \in \I} \qpi \logtwo(\ypi)\Big]
\end{equation}
In (\ref{eq:max_act_loss}), $\logtwo$ is chosen for its strong guiding gradient when $\mathop{\max}_{i} \ypi \ll 0.5$. From another viewpoint, this loss in combination with the sparsity loss, imposes each pixel to be classified positively with high confidence from the most confident classifiers and negative with high confidence from the remaining classifiers.

\noindent\textbf{Inactive Classifier Loss (ICL)} The two previous losses while they encourage assigning each pixel to a basis vector, they do not encourage, in any way, diversity in the assignments. For instance, all pixels could be assigned to one concept detector, with the rest of the detectors having no pixels assigned to them. In that case, the classifiers associated with an empty pixel set (i.e. when no pixel is assigned to them), actually never classify a pixel positively and thus the sparsity criterion can be easier fulfilled. Besides, if all pixels in the dataset are classified negatively by a classifier, then this classifier does not convey any meaningful information, it cannot serve as a concept detector and is redundant. 

To moderate this issue we introduce the inactive classifier loss. We design a loss term that linearly penalizes basis vectors with a few number of pixel assignments. This number is defined as a percentage threshold over the total number of pixels in the dataset. Instead of specifying this threshold for each $i \in \I$ individually, we introduce a set of hyper-parameters to make this more manageable. Let $\am \in [0,1]$ denote a percentage coefficient with $\sum_{\mu} \am = 1$ and $\alpha_{0} \ge \alpha_{1} \ge ... \ge \alpha_{N-1}$, $N \in \N^+$, $\mu = \{0,1,..., N-1\}$. We split $\I$ in $N$ partitions with each partition having $n_{\mu} \in \N$ elements:
\begin{equation}
\nm =  
\begin{cases}
    \floor{\am I} + 1 & \text{$\mu >= I - R$} \\
    \floor{\am I} & \text{otherwise}
\end{cases}
\end{equation}
with $R = I - \sum_{\mu} \floor{\am I}$, and $\floor{\cdot}$ denoting the floor operation. The previous procedure ensures that $\sum_{\mu}\nm = I$ while $\nm$ remains integer. Let $\tau \in [0,1]$ denote a  percentage threshold over the total number of pixels in the dataset. We distribute $\tau$ across the concept detectors using a weighting scheme that utilizes the same weight $\omega_{\mu} \in \R^+$ for all detectors in the same partition. The $i$-th concept detector is penalized whenever the percentage of pixels assigned to it falls below the threshold $\nu_{i}$ given below:
\begin{equation}
    \label{eq:inactive_loss_target_thres}
    \nu_{i} = \frac{\omega_{\mu}\tau}{\sum_{\mu}\omega_{\mu}\nm}
\end{equation}
From (\ref{eq:inactive_loss_target_thres}) it becomes apparent that all concept detectors in the same partition share the same threshold. Finally, we define the \textit{inactive classifier} loss as
\begin{equation}
\loss^{ic} = \averagei\Big[\frac{1}{\nu_{i}}\text{ReLU}\big(\nu_{i}-\averagep[{\ypk{i}}^\gamma)]\big)\Big]
\end{equation}
where the factor $\frac{1}{\nu_{i}}$ before the rectified linear unit activation function ($\text{ReLU}$) \cite{DLBook} normalizes the loss to be 1 when all concept detectors classify negatively the whole pixel dataset. The exponent $\gamma \in \R^+, \gamma > 1$, acts as a sharpening operator on $\ypk{i}$, in order to attenuate non-confident predictions that lie around 0.5.




\noindent\textbf{Maximum Margin Loss (MML)}
Since $M$ controls the margin between the abscissas corresponding to the extremas of the sigmoid classifier, we add an additional loss term that encourages a large classification margin in a similar sense as in the Support Vector Machine \cite{SVM}. We enforce $M$ to be a positive scalar via the parameterization $M = 1/t^2$ and simply define the maximum margin loss as:
\begin{equation}
    \loss^{mm} = \frac{1}{M} = t^2
\end{equation}

Conclusively, we introduce four loss terms that guides search for an interpretable basis. First, the Sparsity Loss (SL) which enforces each pixel to be classified positively by only a fraction of the concept detectors in the basis. Second, the Maximum Activation Loss (MML) which in combination with the sparsity loss enforces the most confident predictions in the relative scale (as implied by $\qp$), to also be confident in an absolute scale (as given by $\yp$ and close to $1$). Third, the Inactive Classifier Loss (ICL), which penalizes classifiers that never classify any pixel positively and last, the Maximum Margin Loss (MML) which enforces large hyperplane separation margin (in the SVM sense) between the positive and negative predictions of the classifiers.