
To describe the motivation of our approach, let's assume that we have access to an interpretable basis of a CNN. Let's also assume that the basis was successfully learned, i.e., the CNN representations can be linearly separated based on their semantic label. The latter implies a) the accuracy of the concept detectors is high and b) the CNN has learned to linearly separate (i.e. disentangle \cite{higgins2018disentangledrepresentations}) the aforementioned concepts. In b), disentangled representations can be obtained by projecting representations onto the basis. Inversely, in case the CNN representations could not be linearly separated based on their semantic label, it would mean that the accuracy of concept detectors is low and thus concept disentanglement via a linear transformation is not possible. 

For example, let's consider a basis with five concept detectors, one for each element in the set \{\textit{car, plane, sky, red, white}\}. Consider the images of the red car and white plane of Fig \ref{fig:motivation} - right. If we apply the concept detectors to the intermediate representation of an image patch, we observe that among a group of classifiers detecting mutually exclusive concepts, only one concept detector classifies the patch positively (i.e. as a patch containing the respective concept). For instance, a patch belonging to the concept \textit{car} (such as the one located at second row - second column) is not a \textit{plane} or \textit{sky}, while it is also \textit{red} and not \textit{white}. In that case, \{\textit{car, plane, sky}\} is a group of mutually exclusive concepts, and \{\textit{red, white}\} another one. This simple fact can be summarized to the following observation: \textit{projecting a representation to an interpretable basis and hard thresholding, results into sparse binary representations}.

In this work, we take a non-standard approach to extract an interpretable basis for the feature space of a CNN. Let us consider a set of linear classifiers. In this case, the classification rule dictates projection on the classifier's hyperplane normal and hard-thresholding against the classifier's bias. Based on our previous observation, in case this set of classifiers forms an interpretable basis, applying all the classifiers' rules to a CNN's intermediate representation (with hard thresholding) shall result in a new, transformed representation, which is binary and sparse. By optimizing basis vectors and biases for this sparsity objective, the proposed method is able to suggest an interpretable basis without requiring an annotated concept dataset.