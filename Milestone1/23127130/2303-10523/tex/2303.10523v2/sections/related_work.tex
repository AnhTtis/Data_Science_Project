In this section we discuss background and related-work in five areas related to this paper. First, we briefly describe prior work on supervised interpretable basis extraction and establish the terminology that is used in this paper. Second, we discuss on supervised and unsupervised discovering of interpretable feature space directions. Third, we highlight how the proposed method differs from other works proposing sparsity as a measure to build inherent-interpretable models. Fourth, we explain the basis labeling problem and potential solutions, and finally, discuss interpretability metrics for assessing the quality of a basis.

\subsection{Supervised interpretable basis extraction and terminology}
As already mentioned in the introduction, each basis vector of an interpretable basis points towards the direction of a concept's representations. To construct an interpretable basis, Zhou et al. \cite{Zhou_Interpretable_Basis_Decomposition} trained a set of binary linear classifiers that separate the CNN's intermediate feature representations based on their semantic meaning. This is accomplished with an densely (per-pixel) annotated concept dataset and implicit use of CNN receptive fields, to assign labels to spatial representation elements of images. Each binary classifier can be considered as a concept detector, since it can separate representations of one concept from representations of other concepts. As already mentioned in Section \ref{sec:introduction}, i) the hyperplane normal directions of the linear classifiers can form a (not necessarily orthogonal, and potentially over/under-complete) basis of the feature space and ii)  projecting a representation onto a basis vector, quantifies the presence of the respective concept in the representation. When constructing the basis, each basis vector retains the concept label of it's respective concept detector. In a strict sense, the concept detector's bias, which is related to the position of the hyperplane in the feature space, is not part of the basis. For simplicity though, we will retain the association between biases and basis vectors, in such a manner that biases together with the basis vectors form the original concept detectors. For brevity, in this paper, we will use the terms \textit{basis}, \textit{concept detectors}, and \textit{classifiers} almost interchangeably.

\subsection{Discovering concept directions in the CNN feature space} Discovering interpretable directions \cite{szegedy2013intriguing} in the feature space of a CNN image classifier has been previously studied in the literature. In most cases though \cite{alain2016_Understanding_Linear_Classifiers, Zhou_Interpretable_Basis_Decomposition, kim_TCAV, RTCAV}, those directions are directly computed by solving a logistic regression problem that linearly separates CNN's representations based on their concept label. Thus, these methods rely on the existence of an annotated image dataset. To alleviate the need for concept annotations, Ghorbani et al, \cite{ghorbani_ACE} proposed a method to automatically group semantically similar image patches out of an unlabeled image dataset. The image patches of each group could then be treated as samples coming from the same concept. Subsequently, the concept samples may be assigned pseudo-labels and can be used as label-representation pairs to reveal each concept's direction in the feature space of the CNN. The latter may be accomplished via solving the respective logistic regression problem. In contrast to \cite{ghorbani_ACE}, our approach is fundamentally different. In the proposed work, the thresholded projections of CNN's representations on the learned directions, are sparse. Thus, our work directly tries to exploit existing structure in the CNN's feature representations, instead of using pseudo-labels to convert the problem to a supervised one. %
\subsection{Sparsity in Inherent Interpretable Models} The proposed work shares conceptual similarities with \cite{zhang2018interpretable, liang2020training} and \cite{Losch_Fritz_Schiele_2021}. All previous works are proposing CNN architectures that are inherently interpretable. During training, they enforce intermediate layer representations to be comprised of pixels with sparse activations across feature maps. While we share the same principal idea that sparse pixel activations can lead to more interpretable representations, the proposed method is post-hoc, and has the potential to be applied (possibly) in any pre-trained CNN. In other words, our method suggests a view of the feature space described by the derived basis, that shares similar sparsity properties that other methods enforce during network training. Essentially, and in a more abstract and less strict way, our method reveals the degree that this property is already present in CNNs that were trained without explicitly enforcing this objective.
\subsection{Labeling a feature space basis}
We define \textit{basis labeling} as the procedure of assigning a concept label name to each one of its vectors. When the basis vectors have been learned in a supervised way, the concept label to attribute to the each vector is actually known before learning the vector's direction. However, when the basis is learned without annotations (such as the current work) or if the natural feature space basis is considered (as in \cite{bau_NetDissection_CVPR} or \cite{mu2020compositional}), attributing meaning to each basis vector requires to put the vector under test. In the testing procedure, each basis vector is accompanied with a (possibly learned) bias (threshold) to form a linear classifier. Then, for all possible concepts, the suitability of the classifier to separate the representations of one concept (positive samples) with respect to the representations of other concepts (negative samples) is evaluated. Finally, each basis vector is assigned a concept label name based on the evaluation metrics of the aforementioned procedure. It is evident, that labeling a basis requires access to a dataset containing concept annotations, such as \cite{bau_NetDissection_CVPR, xiao2018brodenp}, or \cite{cordts2016cityscapes}. Bau et al. \cite{bau_NetDissection_CVPR} assigned one concept label to each vector of the natural feature space basis based on the Broden dataset (which was also introduced in the same work). Later, Mu et al. \cite{mu2020compositional} used the same dataset to label the natural basis with logical compositions of concepts (e.g. the concept of ``blue AND (NOT water)''). In this work, we use \cite{bau_NetDissection_CVPR} to label the bases extracted with our method, while \cite{mu2020compositional} or other potential future works could also be considered.
\subsection{Metrics to evaluate the interpretability of a feature space basis}
In basis evaluation literature \cite{Zhou_Interpretable_Basis_Decomposition}, \cite{bau_NetDissection_CVPR, mu2020compositional}, measuring the interpretability of a basis slightly varies, depending on whether the basis was learned in a supervised way or not. On one hand, in case the basis was learned with supervision, Zhou et al. \cite{Zhou_Interpretable_Basis_Decomposition} used mean average precision (mAP) considering all the classifiers associated with the basis. On the other hand, to assess the interpretability of the natural basis, Bau et al. \cite{bau_NetDissection_CVPR} considered the number of \textit{unique} concept labels that have been assigned to the basis vectors, provided that the performance of the respective classifiers exceeds a threshold. Those labels come from the basis labeling procedure. In \cite{Losch_Fritz_Schiele_2021}, Losch et al. considered Area Under inspectability Curve (AUiC) in order to propose a metric agnostic to a specific threshold. In this paper, we combine ideas from \cite{bau_NetDissection_CVPR} and \cite{Losch_Fritz_Schiele_2021} to propose two metrics that can be used to evaluate the interpretability of a basis.

