\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/evaluation.png}
    \caption{The pipeline for evaluating the interpretability of a basis. The basis labeling procedure is only required when the learned basis was derived in an unsupervised way or when considering the natural feature space basis. In the supervised case, the concept label for each basis vector is actually known before learning the respective concept detector.}
    \label{fig:evaluation}
\end{figure}
\textbf{Overall Evaluation Approach} 
To the best of our knowledge, the proposed method is the first unsupervised method to suggest an interpretable basis. In addition to this, and once again to the best of our knowledge, except from \cite{bau_NetDissection_CVPR} which performs this in a statistical manner, the proposed method is the first unsupervised method to also provide an estimate for the position of the hyperplane that separates each concept's representations from the representations of other concepts. Therefore, we quantitatively evaluate the interpretability of the bases extracted with the proposed method against the interpretability of the natural feature space basis (\textit{baseline}). Apart from this, we quantitatively evaluate the bases extracted with our method with the bases extracted via the supervised approach of \cite{Zhou_Interpretable_Basis_Decomposition} and thus setting a baseline for future unsupervised works.

An exhaustive search and ablation study over all hyper-parameters is difficult, due to the sheer number of parameters, combinations and computational resource constraints. Nevertheless, the results presented below show that by making simple and intuitive hyper-parameter choices, one may obtain a basis that is more interpretable than the natural. In \cite{bau_NetDissection_CVPR}, Bau et. al proved experimentally that the natural feature space basis is more interpretable than other random bases. In this work, we build on the previous findings of \cite{bau_NetDissection_CVPR} and show that the proposed method is able to suggest a basis which is more interpretable than the natural and consequently more interpretable than most other random bases. In short, the main advantage of the proposed method is that it can provide an improvement over the interpretability of the natural basis, and do so, without annotations. Moreover, future, more exhaustive work on fine-tuning strategies has the potential to further improve interpretability. 

In all of our experiments we used the Broden \cite{bau_NetDissection_CVPR} concept dataset to probe the networks and obtain intermediate layer feature representations. Except for comparison with the supervised approach (Section \ref{sec:experimental_supervised}), where we only used the \textit{object} and \textit{part} categories of the dataset, on all other experiments we used the complete set of concept categories, namely \{\textit{scene, object, part, texture, material, color}\}. In all experiments, we used post ReLU activations of the considered network's \textit{last-layer}. A network's \textit{last-layer} refers to the latest convolutional or max-pooling layer where the representation remains spatial, before the flattening to the latest fully-connected one. 

To learn an interpretable basis with the proposed method, we used the training split of the concept dataset. Next, we used the same training split to label the basis using \cite{bau_NetDissection_CVPR}, and finally, we calculated the basis interpretability scores (eq. (\ref{eq:score1}), (\ref{eq:score2})) using the validation split of the same dataset. Annotation labels were only used to label the bases and perform quantitative evaluation, and were not used in any way to learn the aforementioned bases. Regarding the evaluation of the natural basis (\textit{baseline}), we used $\w_i = \e_i$, $\e_i = [\underbrace{0,...,0}_{i \text{ times}},1,\underbrace{0,...,0}_{D-i-1 \, \text{times}}]^T$ and we chose the thresholds $b_i$ according to the top 0.005 -- quantile among the population of projected representations, as suggested by \cite{bau_NetDissection_CVPR}. The rest of the evaluation pipeline was the same as before. Finally, to establish comparisons, we also used the same interpretability score functions of Section \ref{sec:evaluation_metrics}, in order to evaluate the bases extracted with the supervised approach of \cite{Zhou_Interpretable_Basis_Decomposition}. In that case, the bases were learned in a supervised way using the training split of the concept dataset. Given the a-priory known concept labels of the basis vectors, evaluation was performed on the validation split of the dataset, ommiting the basis labeling procedure which is not required. The overall evaluation pipeline is depicted in Fig. \ref{fig:evaluation}. 

\textbf {Basis Learning Details} To learn each one of the basis, we used the Adam \cite{kingma2014adam} optimizer with the default beta parameters ($0.9, \, 0.999$) provided by the PyTorch \cite{Pytorch} implementation. We fixed the learning rate to $0.001$ and did not employ any form of learning rate scheduling. In all cases, basis learning lasted for $300$ epochs. Batch size was a variable that varied across our experiments and its value was based solely on the available GPU memory resources. The values we used, approximately lied in the interval $\approx [800 - 3600]$.

\textbf{Hyper-parameters} We kept most of the hyper-parameters of our method fixed to the same values across all the presented experiments, except for the parameters we wanted to ablate. We linearly combined the loss terms with the weights given in Table \ref{tab:weights}. Empirical evaluation showed that $\lambda^{ma}$ should have higher weight than $\lambda^{s}$ due to the fact that even if the entropy sparsity criterion is fulfilled, the basis may still be not meaningful (Fig. \ref{fig:sparsity_not_enough}). The choice for the rest of the weights was guided by intuition for the  relative importance across loss terms. In all of our experiments we used $I=D$, while extensive study for cases where $I<D$ is left for future work.

\textbf{Parameter Initialization} In all of our experiments we initialize the basis vectors with the vectors of the natural feature space basis (i.e $\wi=\e_i$). We also initialize $t$ and $b$ with $t=0.5$ and $b=0.5$.

\begin{table}[]
\centering
\caption{The loss weight coefficients that we used for learning all our bases. In case of ablation studies, the deviations from these values are given in the respective Section. The superscript of each weight follows the notation of the respective loss.}
\label{tab:weights}
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
$\lambda^s$ & $\lambda^{ma}$ & $\lambda^{ic}$ & $\lambda^{mm}$ \\ \hline
$2.0$ & 5.0 & 5.0 & 0.5 \\ \hline
\end{tabular}
}
\end{table}

\subsection{Ablation studies}
In this section, we present three ablation studies regarding Maximum Margin (\textit{MML}) and Inactive Classifier (\textit{ICL}) losses. To do so, we choose two different CNN architectures trained on two different datasets. In particular, we extract bases for the \textit{last layer} of ResNet18 \cite{he2016ResNet} and VGG16 \cite{simonyan2014VGG} with batch normalization blocks (VGG16BN). The ResNet18 that we used, was trained on Places365 \cite{zhou2017places}, while VGG16BN was trained on ImageNet \cite{deng2009imagenet}.

\textbf{Ablation of MML weight in absence of ICL}
In the first ablation, we set $\lambda^{ic} = 0$ (i.e. completely eliminated the Inactive Classifier Loss) and varied $\lambda^{mm}$ to take values from the set $\{0.5, 1.0, 1.5\}$. The basis interpretability scores are given in Fig. \ref{fig:resnet_ablation_margin} and \ref{fig:vgg16bn_ablation_margin}. For both networks, we observe that all the bases extracted with the proposed method score significantly lower in terms of $\Score1$ than the baseline. In absence of ICL, this fact was actually expected, for the reasons described in Section \ref{sec:proposed_method}. For ResNet18, the proposed method extracted bases that were slightly more interpretable than the baseline according to $\Score2$, while for VGG16BN and the same metric, none of the learned basis scored higher compared to the baseline. Overall, we could say that for those cases, the sensitivity of the method with respect to $\lambda^{mm}$ was rather small. This stems from the fact that, according to those metrics, the learned bases are approximately equally interpretable, even though they were learned using different $\lambda^{mm}$.

\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/resnet-ablation-margin.png}
    \caption{Ablation study for $\lambda^{mm}$. ICL is not used in these experiments. Without ICL, the interpretability of the extracted basis is significantly worse than the baseline in terms of $\Score1$, and slightly better than the baseline in terms of $\Score2$.}
    \label{fig:resnet_ablation_margin}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/vgg16bn-ablation-margin.png}
    \caption{Ablation study for $\lambda^{mm}$. ICL is not used in these experiments. Without ICL, the interpretability of the extracted basis is worse than the baseline in terms of both $\Score1$ and $\Score2$.}
    \label{fig:vgg16bn_ablation_margin}
\end{figure}

\textbf{Ablating $\tau$ with ICL}
In the second ablation, we make use of \textit{ICL} and set $\lambda^{mm} = 0.5$ and $\lambda^{ic}=5$. For comparisons, we vary $\tau$ to take values from the set $\{0.3, 0.5, 0.7, 0.9\}$. In this study we also use one partition ($N=1, \alpha_{0} = 1, \omega_{0} = 1$) and set $\gamma = 2.5$. Basis interpretability scores are given in Fig. \ref{fig:resnet_ablation_t0} and \ref{fig:vgg16bn_ablation_t0}. The first observation for $\Score1$ is that, in contrast to the previous ablation and for both networks, the extracted bases are significantly more interpretable compared to the baseline. This, experimentally demonstrates the importance of ICL to obtain a meaninful basis. For $\Score2$, a notable improvement over the baseline is provided for ResNet18, while for VGG16BN the interpretability of all the bases, regardless of $\tau$, are comparable to the baseline. Overall, regarding $\Score1$, the value of $\tau$ seemed to have larger impact on the bases learned for VGG16BN compared to the bases that were learned for ResNet18, with increasing values of $\tau$ resulting into larger interpretability scores. We think it is reasonable to believe, that this behaviour possibly indicates that the impact of $\tau$ on the basis interpretability results also depends on the network architecture, the dataset used to train it and its relation with the concept dataset that was used to learn the basis. 

\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/resnet-ablation-t0.png}
    \caption{Ablation with respect to $\tau$. With the addition of ICL the interpretability of the bases extracted with out method is improved compared to the baseline.}
    \label{fig:resnet_ablation_t0}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/vgg16bn-ablation-t0.png}
    \caption{Ablation with respect to $\tau$. With the addition of ICL the interpretability of the bases extracted with out method is improved compared to the baseline, at least for $\Score1$. For $\Score2$, the interpretability of the same bases are comparable to the interpretability of the baseline.}
    \label{fig:vgg16bn_ablation_t0}
\end{figure}

\textbf{Ablating partition count with ICL}
In the last ablation, we study the effect of partition count to the basis interpretability scores. In these experiments we considered two cases with different number of partitions (Section \ref{sec:proposed_method}). In both cases, given the number of partitions $N$, we used the following hyper-parameters: $\alpha_{\mu} = 1/N, \mu = \{0,1,...,N-1\}$, and $\omega_{\mu} = \mu+1$. In particular, for the first case we used two partitions ($N=2$) with $\alpha_{\mu} = 0.5, \, \mu\in \{0,1\}$, $\omega_0 = 1, \, \omega_1 = 2$ and in the second case we used four partitions ($N=4$ with $\alpha_{\mu} = 0.25, \mu \in \{0,1,2,3\}$ and $\omega_0 = 1, \, \omega_1 = 2, \, \omega_2 = 3, \, \omega_3 = 4$. In these experiments we used $\tau = 0.7$ and $\gamma = 2.5$. Interpretability results are provided in Fig. \ref{fig:resnet_ablation_partitions}, \ref{fig:vgg16bn_ablation_partitions}. Regarding ResNet18 (Fig. \ref{fig:resnet_ablation_partitions}) we observe that using a single partition ($N=1$) slightly improves the interpretability metrics among the bases that were learned with a larger number of partitions. For VGG16BN, the same slight improvement applies for the basis that was learned with $N=4$. Overall, we could say, that on those experiments and for the given interpretability metrics, the sensitivity of the method with respect to partition count is rather low.

\subsection{Results for more networks}
In this section we apply the proposed method for interpretable basis extraction to two more networks. We consider AlexNet \cite{NIPS2012_Krizhevsky_AlexNet} (trained on Places365) and GoogleNet \cite{Szegedy_GoogleNet_2014} (trained on ImageNet). Regarding hyper-parameters, we use the loss weight factors of Table \ref{tab:weights}, $\tau = 0.7$, $N=1, \, \alpha_{0} = 1.0, \omega_{0} = 1.0, \, \gamma = 2.5$. We provide basis interpretability results that show improvement over the baseline in Fig. \ref{fig:alexnet} and \ref{fig:googlenet}.

\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/resnet-ablation-partitions.png}
    \caption{Ablation with respect to the number of partitions $N$. For ResNet18, just a single partition resulted into the most interpretable basis. However, for other values of $N$ the results are comparable and an improvement is noted compared to the baseline.}
    \label{fig:resnet_ablation_partitions}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/vgg16bn-ablation-partitions.png}
    \caption{Ablation with respect to the number of partitions $N$. For VGG16BN, four partitions resulted into the most interpretable basis. However, for other values of $N$ the results are comparable and an improvement is noted compared to the baseline.}
    \label{fig:vgg16bn_ablation_partitions}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/alexnet.png}
    \caption{Interpretability comparison between the baseline and a basis extracted with the proposed method. The proposed method suggested a more interpretable basis than the baseline.}
    \label{fig:alexnet}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/googlenet.png}
    \caption{Interpretability comparison between the baseline and a basis extracted with the proposed method. The proposed method suggested a more interpretable basis than the baseline.}
    \label{fig:googlenet}
\end{figure}

\subsection{Comparison with a supervised approach}
\label{sec:experimental_supervised}
\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/renset18-ibd.png}
    \caption{Comparing basis interpretability of the proposed method (\textit{UIBE}) with the natural feature space basis (\textit{baseline}) } and a basis extracted with a supervised approach (\textit{IBD}) \cite{Zhou_Interpretable_Basis_Decomposition}.
    \label{fig:resnet18_ibd}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3.4in]{figs/experiments/vgg16bn-ibd.png}
    \caption{Comparing basis interpretability of the proposed method (\textit{UIBE}) with the natural feature space basis (\textit{baseline}) } and a basis extracted with a supervised approach (\textit{IBD}) \cite{Zhou_Interpretable_Basis_Decomposition}.
    \label{fig:vgg16bn_ibd}
\end{figure}


In this section we compare the interpretability of bases extracted with the proposed method against the \textit{baseline} and the supervised approach of \cite{Zhou_Interpretable_Basis_Decomposition}. Once again, we consider the \textit{last-layers} of ResNet18 (trained on Places365) and VGG16BN (trained on ImageNet). We followed the approach of Interpretable Basis Decomposition (IBD) \cite{Zhou_Interpretable_Basis_Decomposition} and learned a basis in a supervised way for the concepts categories of \textit{objects} and \textit{parts}. To learn the basis we used the training split of the concept dataset. The number of basis vectors that were learned from IBD was $I=660$ while the dimensionality of the feature space for both CNNs is $D=512$. Regarding the proposed method, for ResNet18, we re-considered the basis \textit{b13} (Fig. \ref{fig:resnet_ablation_t0}) which was learned from all images (regardless the category annotations) of the concept dataset's training split. This time though, we only considered the concept categories of \textit{objects} and \textit{parts} to label the basis. We did the same for the natural feature space basis as well. Finally we report $\Score1$ and $\Score2$ on the validation split of the same dataset. A similar approach was taken for VGG16BN, where we re-considered the basis \textit{b26}. The results for the two networks are given in Fig. \ref{fig:resnet18_ibd} and \ref{fig:vgg16bn_ibd}. 

From the previously mentioned figures, we first observe, that the bases learned with IBD have the same score on both metrics. This is actually expected, since all concept labels in a basis learned with IBD are unique. On the contrary, when labeling the natural feature space basis or a basis extracted with the proposed method, the same concept label may be attributed to more than one basis vectors. This also might be a possible explanation for why the proposed method showcases significantly better interpretability scores for $\Score1$ compared to IBD. In other words, IBD is limited to learn a single direction for each one of the concepts, while the bases extracted with the proposed method may cover more than one direction for the same concept. Additionally, the sparsity criterion which we use to learn the interpretable basis, ensures that the different basis vectors cover different parts of the concept dataset. Another factor to consider for the same matter is the basis labeling procedure, which in our case is \cite{bau_NetDissection_CVPR}. Other basis labeling strategies might suggest different labels which might also affect the interpretability scores. It is also noteworthy that the same possible explanation might be given regarding Fig. \ref{fig:vgg16bn_ibd} where even the natural feature space basis scores better than IBD in terms of $\Score1$. 

Regarding $\Score2$, the bases extracted with IBD may be considered significantly more interpretable than the bases extracted with the current work, with the latter being even more prominent in the case of ResNet18. We think that this fact is also linked to the previous argument. In particular, since a basis learned without supervision may have duplicate labels, the number of unique concept labels that can be attributed to the basis vectors (which is related to what $\Score2$ measures), is expected to be less than the number of vectors in the basis. However, for a basis learned in a supervised way, this number is exactly equal to the number of vectors in the basis ($I$). Moreover, in this case, IBD used a basis with a larger number of vectors ($I=660$) compared to the proposed method (which only uses $I=D=512$).

Overall, we find it difficult to strictly position the proposed method in relation to a supervised approach for this problem. We think that the current work reveals a possible limitation of the supervised approach which assumes that concept representations lie only on a single direction of the feature space. The proposed method has the potential to overcome this limitation. However, the presented experimental results also suggest that the previously mentioned strength of the proposed method is also its limitation. By devoting more than one basis direction to a single concept, inevitably limits the number of different unique concepts that can described by the basis. A possible direction towards improvement might be to consider an \textit{approximately} orthogonal and over-complete basis of the feature space (i.e. $I>D$), which we leave for future work.

\subsection{Qualitative comparisons}
In this section we provide qualitative results which highlight the interpretability improvement gains that are obtained when we transform image feature representations to a basis learned with the proposed method. Thus, Fig. \ref{fig:qualitative_resnet1} and \ref{fig:qualitative_resnet2} depict results for ResNet18, Fig. \ref{fig:qualitative_vgg16bn1} and \ref{fig:qualitative_vgg16bn2} for VGG16BN, Fig. \ref{fig:qualitative_alexnet1} and \ref{fig:qualitative_alexnet2} for AlexNet and Fig. \ref{fig:qualitative_googlenet1} and \ref{fig:qualitative_googlenet2} for GoogleNet. In those figures, we used \cite{bau_NetDissection_CVPR} to assign concept labels for the bases vectors extracted by our method, as well as to the vectors of the natural feature space bases. Among the group of common concepts that have been assigned to the concept detectors of the two bases, we considered the top-performing concept detector in each basis. The common name of the concept detectors is given on the (sub-)figure's top. The basis name that each concept detector comes from, is written on the left. For each selected concept detector, we present a row of images whose representations have a spatial element which is ranked among the top-4 activations over the validation split of the concept dataset ($\Kvl$). The reported IoU scores which are given below the set of images, corresponds to the IoU performance of the respective concept detector over the whole set of images in the validation set of the concept dataset. Each figure is meant to be read as a $2 \times 2$ grid of 4 concepts with each cell containing $2 \times 4$ images.

\subsection{Are the bases learned with a supervised approach orthogonal ?}

In this last section of experimental results, we experimentally seek to validate our hypothesis that an interpretable basis should be orthogonal. While our hypothesis is based on the assumption that the CNN has linearly disentangled concept representations, we still try to, at least partially, answer to what extend this is already happening when we use a supervised method to learn an interpretable basis. Building on our previous experimental results, we consider the bases that we learned with IBD \cite{Zhou_Interpretable_Basis_Decomposition} for the \textit{last-layers} of ResNet18 and VGG16BN. We provide statistical measurements for the distribution of angles between basis vectors that are met in those bases. The results are depicted in Table \ref{tab:ibd_basis_angles}. It is noteworthy to mention that those bases have $I=660$ and $D=512$, with the important relation that $I>D$. Based on our measurements, the bases could be considered approximately orthogonal since the mean angle between any pairs of basis vectors is around $86.5^\circ$ and the standard deviation of the distribution is less than $3.77^\circ$, in the worst case. This fact could further support our intuition that interpretable bases shall be orthogonal. While the present work considers only $I\leq D$, future extensions could study the case where $I \ge D$ with approximate orthogonality constraints.
 
\begin{table}
\centering
\caption{
Statistics of Pairwise Vector Angles for the bases that we learned with the supervised approach of IBD \cite{Zhou_Interpretable_Basis_Decomposition}.
}
\label{tab:ibd_basis_angles}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccc}
\cline{2-5}
\multicolumn{1}{c|}{}                  & \multicolumn{4}{c|}{\textbf{Pairwise Vector Angles (deg)}}                                                                                     \\ \hline
\multicolumn{1}{|c|}{\textbf{Network}} & \multicolumn{1}{c|}{\textbf{Mean}} & \multicolumn{1}{c|}{\textbf{Std}} & \multicolumn{1}{c|}{\textbf{Min}} & \multicolumn{1}{c|}{\textbf{Max}} \\ \hline
\multicolumn{1}{|c|}{ResNet18}         & \multicolumn{1}{c|}{85.67}         & \multicolumn{1}{c|}{3.77}         & \multicolumn{1}{c|}{45.5}         & \multicolumn{1}{c|}{99.5}         \\ \hline
\multicolumn{1}{|c|}{VGG16BN}          & \multicolumn{1}{c|}{88.23}         & \multicolumn{1}{c|}{2.84}         & \multicolumn{1}{c|}{57.71}        & \multicolumn{1}{c|}{100.0}        \\ \hline
\multicolumn{1}{l}{}                   & \multicolumn{1}{l}{}               & \multicolumn{1}{l}{}              & \multicolumn{1}{l}{}              & \multicolumn{1}{l}{}             
\end{tabular}
}
\end{table}