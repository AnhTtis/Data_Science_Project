{
    "arxiv_id": "2303.12766",
    "paper_title": "Spherical Transformer for LiDAR-based 3D Recognition",
    "authors": [
        "Xin Lai",
        "Yukang Chen",
        "Fanbin Lu",
        "Jianhui Liu",
        "Jiaya Jia"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2023-03-23"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "abstract": "LiDAR-based 3D point cloud recognition has benefited various applications. Without specially considering the LiDAR point distribution, most current methods suffer from information disconnection and limited receptive field, especially for the sparse distant points. In this work, we study the varying-sparsity distribution of LiDAR points and present SphereFormer to directly aggregate information from dense close points to the sparse distant ones. We design radial window self-attention that partitions the space into multiple non-overlapping narrow and long windows. It overcomes the disconnection issue and enlarges the receptive field smoothly and dramatically, which significantly boosts the performance of sparse distant points. Moreover, to fit the narrow and long windows, we propose exponential splitting to yield fine-grained position encoding and dynamic feature selection to increase model representation ability. Notably, our method ranks 1st on both nuScenes and SemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU, respectively. Also, we achieve the 3rd place on nuScenes object detection benchmark with 72.8% NDS and 68.5% mAP. Code is available at https://github.com/dvlab-research/SphereFormer.git.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12766v1"
    ],
    "publication_venue": "Accepted to CVPR 2023. Code is available at https://github.com/dvlab-research/SphereFormer.git"
}