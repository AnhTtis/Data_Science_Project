% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tablefootnote}
\usepackage[flushleft]{threeparttable}

\usepackage{lipsum}
\usepackage{bbding}
\usepackage{url}
\urlstyle{rm}

\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{color}
\usepackage{enumitem}
\definecolor{mypink3}{cmyk}{0, 0.7808, 0.4429, 0.1412}
\definecolor{myblue2}{cmyk}{0, 0., 0.4412, 0.4808}
\definecolor{myblue}{cmyk}{0, 0.7808, 0., 0.1412}
\newcommand{\xinlai}[1]{{\color{myblue}{\bf\sf [xinlai: #1]}}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2825} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Spherical Transformer for LiDAR-based 3D Recognition}

\author{Xin Lai$^{1}$\hspace{1.0cm}Yukang Chen$^{1}$\hspace{1.0cm}Fanbin Lu$^{1}$\hspace{1.0cm}Jianhui Liu$^{2}$\hspace{1.0cm}Jiaya Jia$^{1,3}$\\
% \vspace{0.5cm}
$^{1}$The Chinese University of Hong Kong~~~
$^{2}$The University of Hong Kong~~~
$^{3}$SmartMore~~~
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
%\texttt{\footnotesize
%\{xinlai,lwwang,leojia\}@cse.cuhk.edu.hk\quad sliu@smartmore.com\quad hengshuang.zhao@eng.ox.ac.uk
%}
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
LiDAR-based 3D point cloud recognition has benefited various applications. Without specially considering the LiDAR point distribution, most current methods suffer from information disconnection and limited receptive field, especially for the sparse distant points. In this work, we study the varying-sparsity distribution of LiDAR points and present \textbf{SphereFormer} to directly aggregate information from dense close points to the sparse distant ones. We design radial window self-attention that partitions the space into multiple non-overlapping narrow and long windows. It overcomes the disconnection issue and enlarges the receptive field smoothly and dramatically, which significantly boosts the performance of sparse distant points. Moreover, to fit the narrow and long windows, we propose exponential splitting to yield fine-grained position encoding and dynamic feature selection to increase model representation ability. Notably, our method ranks 1\textsuperscript{st} on both nuScenes and SemanticKITTI semantic segmentation benchmarks with $81.9\%$ and $74.8\%$ mIoU, respectively. Also, we achieve the 3\textsuperscript{rd} place on nuScenes object detection benchmark with $72.8\%$ NDS and $68.5\%$ mAP. Code is available at \url{https://github.com/dvlab-research/SphereFormer.git}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Nowadays, point clouds can be easily collected by LiDAR sensors. They are extensively used in various industrial applications, such as autonomous driving and robotics. In contrast to 2D images where pixels are arranged densely and regularly, LiDAR point clouds possess the varying-sparsity property --- points near the LiDAR are quite dense, while points far away from the sensor are much sparser, as shown in Fig.~\ref{fig:erf} (a).

However, most existing work~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet,SubmanifoldSparseConvNet,choy20194d,yan2021sparse,tang2020searching,cheng20212,xu2021rpvnet,yan20222dpass} does not specially consider the the varying-sparsity point distribution of outdoor LiDAR point clouds. They inherit from 2D CNNs or 3D indoor scenarios, and conduct local operators (\eg, SparseConv~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet,SubmanifoldSparseConvNet}) uniformly for all locations. This causes inferior results for the sparse distant points. As shown in Fig.~\ref{fig:histogram}, although decent performance is yielded for the dense close points, it is difficult for these methods to deal with the \textit{sparse distant points} optimally. 

% Currently, Sparse Convolution (SC)~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet,SubmanifoldSparseConvNet,choy20194d} is the dominant method to extract features from LiDAR points. To reduce the computational complexity, the variant Submanifold Sparse Convolution (SSC) is proposed. It first performs voxelization on the input scene and then conducts convolution with a fixed kernel size (\eg, usually set to 3) on the voxels. Also, only the non-empty locations in the input are activated in the output. The receptive field is enlarged by stacking multiple layers of SC (only for downsampling) and SSC.

\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/fig_histogram_crop.pdf}
\end{center}
\vspace{-0.4cm}
\caption{Semantic segmentation performance on nuScenes \textit{val} set for points at different distances.}
\label{fig:histogram}
\vspace{-0.3cm}
\end{figure}

\begin{figure*}
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/fig_erf_v9_crop.pdf}
\end{center}
\vspace{-0.6cm}
\caption{Effective Receptive Field (ERF) of SparseConv and ours. (a) LiDAR point cloud. (b) Radial window partition. Only a single radial window is shown. Points inside the window are marked in red. (c) Zoom-in sparse distant points. A sparse \textit{car} is circled in yellow. (d) ERF of SparseConv, given the point of interest (with yellow star). White and red denote high contribution. (e) ERF of ours.}
\label{fig:erf}
% \vspace{-0.2cm}
\end{figure*}

We note that the root cause lies in limited receptive field. For sparse distant points, there are few surrounding neighbors. This not only results in inconclusive features, but also hinders enlarging receptive field due to information disconnection. To verify this finding, we visualize the Effective Receptive Field (ERF)~\cite{luo2016understanding} of the given feature (shown with the yellow star) in Fig.~\ref{fig:erf} (d). The ERF cannot be expanded due to disconnection, which is caused by the extreme sparsity of the distant \textit{car}.

Although window self-attention~\cite{lai2022stratified,fan2022embracing}, dilated self-attention~\cite{mao2021voxel}, and large-kernel CNN~\cite{chen2022scaling} have been proposed to conquer the limited receptive field, these methods do not specially deal with LiDAR point distribution, and remain to enlarge receptive field by stacking local operators as before, leaving the information disconnection issue still unsolved. As shown in Fig.~\ref{fig:histogram}, the method of cubic self-attention brings a limited improvement.
%However, if we could aggregate the long-range information directly in a single operator, we do not rely on stacking local operators to yield large receptive field, thus avoiding the disconnection issue, as shown in the bottom of the figure. With the proposed method, the performance of sparse distant points can be improved significantly (\ie, 10\% mIoU) as shown in Fig.\xinlai{add figure}. 

% % Regarding the issue of limited receptive field,
% To conquer limited receptive field,  They indeed possess larger receptive field for the sparse distant points. Nevertheless, all of them enlarge receptive field still by stacking local operators, and the disconnection issue remains unsolved given the extreme sparsity. As shown in Fig.~\ref{fig:histogram}, the method of cubic self-attention brings a limited improvement.

In this paper, we take a new direction to {\it aggregate long-range information directly in a single operator} to suit the varying-sparsity point distribution. We propose the module of \textit{SphereFormer} to perceive useful information from points 50+ meters away and yield large receptive field for feature extraction. Specifically, we represent the 3D space using spherical coordinates $(r, \theta, \phi)$ with the sensor being the origin, and partition the scene into multiple non-overlapping windows. Unlike the cubic window shape, we design radial windows that are long and narrow. They are obtained by partitioning only along the $\theta$ and $\phi$ axis, as shown in Fig.~\ref{fig:erf} (b). It is noteworthy that we make it a plugin module to conveniently insert into existing mainstream backbones. %This facilitates the sparse distant points to aggregate the long-range information from the dense region that is often semantically rich. %Meanwhile, it maintains computational efficiency since the window is narrow and each query only has less than 20 keys on average. 

The proposed module does not rely on stacking local operators to expand receptive field, thus avoiding the disconnection issue, as shown in Fig.~\ref{fig:erf} (e). Also, it facilitates the sparse distant points to aggregate information from the dense-point region, which is often semantically rich. So, the performance of the distant points can be improved significantly (\ie, +17.1\% mIoU) as illustrated in Fig.~\ref{fig:histogram}. %Also, it serves as a plugin module and can be conveniently inserted into existing mainstream backbones.

Moreover, to fit the long and narrow radial windows, we propose \textit{exponential splitting} to obtain fine-grained relative position encoding. The radius $r$ of a radial window can be over 50 meters, which causes large splitting intervals. It thus results in coarse position encoding when converting relative positions into integer indices. Besides, to let points at varying locations treat local and global information differently, we propose \textit{dynamic feature selection} to make further improvements.

In total, our contribution is three-fold.
\begin{itemize}
    \item We propose SphereFormer to directly aggregate long-range information from dense-point region. It increases the receptive field smoothly and helps improve the performance of \textit{sparse distant points}.
    
    \item To accommodate the radial windows, we develop exponential splitting for relative position encoding. Our dynamic feature selection further boosts performance.
    
    \item Our method achieves new state-of-the-art results on multiple benchmarks of both semantic segmentation and object detection tasks.
\end{itemize}

% \begin{figure}
% \begin{center}

% 	\centering
%     \begin{minipage}  {1.0\linewidth}
%         \centering
%         \includegraphics [width=1\linewidth,height=0.3\linewidth]
%         {figures/fig_erf/snapshot_rgb.png}
%     \end{minipage}      
%     \begin{minipage}  {1.0\linewidth}
%         \centering
%         \includegraphics [width=1\linewidth,height=0.3\linewidth]
%         {figures/fig_erf/snapshot_erf1.png}
%     \end{minipage}      
%      \begin{minipage}  {1.0\linewidth}
%         \centering
%         \includegraphics [width=1\linewidth,height=0.3\linewidth]
%         {figures/fig_erf/snapshot_erf2.png}
%     \end{minipage} 
% 	 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      
	 
% % 	 \vspace{-0.2cm}
	 
%     % \begin{minipage}  {0.32\linewidth}
%     %     \centering
%     %     \includegraphics [width=1\linewidth,height=0.6\linewidth]
%     %     {figures/erf_grad/gt.png}\\\footnotesize Input / Ground Truth
%     % \end{minipage}      
%     % \begin{minipage}  {0.32\linewidth}
%     %     \centering
%     %     \includegraphics [width=1\linewidth,height=0.6\linewidth]
%     %     {figures/erf_grad/baseline_pred.png}\\\footnotesize w/o stratified
%     % \end{minipage}      
%     %  \begin{minipage}  {0.32\linewidth}
%     %     \centering
%     %     \includegraphics [width=1\linewidth,height=0.6\linewidth]
%     %     {figures/erf_grad/stratified_pred.png}\\\footnotesize w/ stratified
%     % \end{minipage} 
    
%     % \vspace{0.1cm}
     
%     % \begin{minipage}  {0.06\linewidth}
%     %     \centering
%     %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
%     %     {figures/colors/bed.png}
%     % \end{minipage}\footnotesize bed
%     % \begin{minipage}  {0.06\linewidth}
%     %     \centering
%     %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
%     %     {figures/colors/chair.png}
%     % \end{minipage}\footnotesize chair
%     % \begin{minipage}  {0.06\linewidth}
%     %     \centering
%     %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
%     %     {figures/colors/curtain.png}
%     % \end{minipage}\footnotesize curtain
%     % \begin{minipage}  {0.06\linewidth}
%     %     \centering
%     %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
%     %     {figures/colors/desk.png}
%     % \end{minipage}\footnotesize desk
%     % \begin{minipage}  {0.06\linewidth}
%     %     \centering
%     %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
%     %     {figures/colors/floor.png}
%     % \end{minipage}\footnotesize floor
%     % \begin{minipage}  {0.06\linewidth}
%     %     \centering
%     %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
%     %     {figures/colors/table.png}
%     % \end{minipage}\footnotesize table
%     % \begin{minipage}  {0.06\linewidth}
%     %     \centering
%     %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
%     %     {figures/colors/wall.png}
%     % \end{minipage}\footnotesize wall
% \end{center}
% \vspace{-0.5cm}
% \caption{Visualization of Effective Receptive Field (ERF)~\cite{luo2016understanding}, given the feature of interest (shown with green star) in the output layer. Red region corresponds to high contribution. Top: the input scene and its semantic segmentation label. Middle: the ERF of vanilla SparseConvNet. Bottom: the ERF of our method.}
% \label{fig:erf}
% \vspace{-0.4cm}
% \end{figure}

%-------------------------------------------------------------------------
\section{Related Work}

\subsection{LiDAR-based 3D Recognition}
\paragraph{Semantic Segmentation.}
Segmentation~\cite{ronneberger2015u,zhao2017pyramid,chen2018encoder,lai2021semi,lai2022decouplenet,tian2022adaptive,tian2023learning,tian2022generalized,chu2022twist,icm-3d,li2021simultaneous} is a fundamental task for vision perception. Approaches for LiDAR-based semantic segmentation can be roughly grouped into three categories, \ie, view-based, point-based, and voxel-based methods. View-based methods either transform the LiDAR point cloud into a range view~\cite{wu2019squeezesegv2,xu2020squeezesegv3,behley2019semantickitti,milioto2019rangenet++,razani2021lite}, or use a bird-eye view (BEV)~\cite{zhang2020polarnet} for a 2D network to perform feature extraction. 3D geometric information is simplified. 

Point-based methods~\cite{qi2017pointnet, qi2017pointnet++, thomas2019kpconv, yan2020pointasnl, hu2020randla, tatarchenko2018tangent, lai2022stratified} adopt the point features and positions as inputs, and design abundant operators to aggregate information from neighbors. Moreover, the voxel-based solutions~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet, SubmanifoldSparseConvNet, choy20194d} divide the 3D space into regular voxels and then apply sparse convolutions. Further, methods of \cite{yan2021sparse,tang2020searching,zhu2021cylindrical,cheng20212,liu2022less,jiang2021guided,cohen2018spherical} propose various structures for improved effectiveness. All of them focus on capturing local information. We follow this line of research, and propose to directly aggregate long-range information.

Recently, RPVNet~\cite{xu2021rpvnet} combines the three modalities by feature fusion. Furthermore, 2DPASS~\cite{yan20222dpass} incorporates 2D images during training, and \cite{Robert_2022_CVPR} fuses multi-modal features. Despite extra 2D information, the performance of these methods still lags behind compared to ours.

% \vspace{-0.2cm}
\paragraph{Object Detection.} 
3D object detection frameworks can be roughly categorized into single-stage~\cite{3dssd, sessd, sassd, cia-ssd, spatial-pruned-conv, chen2023voxenext} and two-stage~\cite{point-rcnn, voxel-rcnn, pvrcnn, pyramid-rcnn} methods. VoxelNet~\cite{voxelnet} extracts voxel features by PointNet~\cite{qi2017pointnet} and applies RPN~\cite{fasterrcnn} to obtain the proposals. SECOND~\cite{second} is efficient thanks to the accelerated sparse convolutions. VoTr~\cite{mao2021voxel} applies cubic window attention to voxels. LiDARMultiNet~\cite{ye2022lidarmultinet} unifies semantic segmentation, panoptic segmentation, and object detection into a single multi-task network with multiple types of supervision.  Our experiments are based on CenterPoint~\cite{yin2021center}, which is a widely used anchor-free framework. It is effective and efficient. We aim to enhance the features of sparse distant points, and our proposed module can be conveniently inserted into existing frameworks. 

% Following this line of research, different ways for feature aggregation are designed to learn high-level semantic features.
% PointNet and its variants~\cite{qi2017pointnet, qi2017pointnet++} use max pooling to aggregate features. PointConv~\cite{wu2019pointconv} and KPConv~\cite{thomas2019kpconv} try to use an MLP or discrete kernel points to mimic a continuous convolution kernel. 

\subsection{Vision Transformer}

Recently, Transformer~\cite{vaswani2017attention} become popular in various 2D image understanding tasks~\cite{dosovitskiy2020vit, pmlr-v139-touvron21a, touvron2021cait, wang2021pyramid, wang2021pvtv2, liu2021Swin, chu2021Twins, yang2021focal, dong2021cswin, vip, detr, zhu2020deformable, mao2021voxel, zhao2020san}. ViT~\cite{dosovitskiy2020vit} tokenizes every image patch and adopts a Transformer encoder to extract features. Further, PVT~\cite{wang2021pyramid} presents a hierarchical structure to obtain a feature pyramid for dense prediction. It also proposes Spatial Reduction Attention to save memory. Also, Swin Transformer~\cite{liu2021Swin} uses window-based attention and proposes the shifted window operation in the successive Transformer block. Moreover, methods of~\cite{chu2021Twins, yang2021focal, dong2021cswin} propose different designs to incorporate long-range dependencies. There are also methods~\cite{zhao2021point,mao2021voxel,lai2022stratified,fan2022embracing,sun2022swformer} that apply Transformer into 3D vision. Few of them consider the point distribution of LiDAR point cloud. In our work, we utilize the varying-sparsity property, and design radial window self-attention to capture long-range information, especially for the sparse distant points.

%-------------------------------------------------------------------------
\section{Our Method}

In this section, we first elaborate on radial window partition in Sec.~\ref{sec:radial}. Then, we propose the improved position encoding and dynamic feature selection in Sec.~\ref{sec:pe} and \ref{sec:dfs}.

% \subsection{Overview}

% \begin{figure*}
% \begin{center}
% \includegraphics[width=1.0\linewidth]{figures/fig_overview_crop.pdf}
% \end{center}
% \vspace{-0.5cm}
% \caption{Model Overview. Local Operator Module: }
% \label{fig:overview}
% \vspace{-0.3cm}
% \end{figure*}

% The overview of our method is shown in Fig.~\ref{fig:overview}. The backbone model comprises a total of 5 stages, where each stage is composed of a Local Operator Module (\eg, sparse convolution~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet,SubmanifoldSparseConvNet,choy20194d}, local self-attention~\cite{mao2021voxel,fan2022embracing,zhao2021point,lai2022stratified}) and a Radial Context Module. In particular, the Radial Context Module is designed to enlarge effective receptive field, supplying with rich geometric and contextual information, especially for sparse distant points. Besides, there is a downsample layer between two successive stages, which is not shown in the figure.

% Our model can be easily applied to downstream tasks, such as semantic segmentation and object detection. For semantic segmentation, we append a decoder on top of the backbone model, following the encoder-decoder structure as U-Net~\cite{ronneberger2015u}. For object detection, we use the backbone model for feature extraction.

%------------------------------------------------------------------------
\subsection{Spherical Transformer}
\label{sec:radial}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/fig_radial_v5_crop.pdf}
\end{center}
\vspace{-0.6cm}
\caption{Cubic vs. Radial window partition. The radial window can directly harvest information from the dense-point region, especially for the sparse distant points.}
\label{fig:radial}
\vspace{-0.2cm}
\end{figure}

To model the long-range dependency, we adopt the window-attention~\cite{liu2021Swin} paradigm. However, unlike the cubic window attention~\cite{mao2021voxel,fan2022embracing,lai2022stratified}, we take advantage of the varying-sparsity property of LiDAR point cloud and present the SphereFormer module, as shown in Fig.~\ref{fig:radial}.

\paragraph{Radial Window Partition.} Specifically, we represent LiDAR point clouds using the spherical coordinate system $(r, \theta, \phi)$ with the LiDAR sensor being the origin. We partition the 3D space along the $\theta$ and $\phi$ axis. We, thus, obtain a number of non-overlapping radial windows with a long and narrow 'pyramid' shape, as shown in Fig.~\ref{fig:radial}. We obtain the window index for the token at ($r_i$, $\theta_i$, $\phi_i$) as
\begin{equation}
\footnotesize
\label{eq:window_partition}
    win\_index_{i} = (\lfloor \frac{\theta_i}{\Delta \theta} \rfloor, \lfloor \frac{\phi_i}{\Delta \phi} \rfloor),
\end{equation}
where $\Delta \theta$ and $\Delta \phi$ denote the window size corresponding to the $\theta$ and $\phi$ dimension, respectively.

Tokens with the same window index would be assigned to the same window. The multi-head self-attention~\cite{vaswani2017attention} is conducted within each window independently as follows.
\begin{equation}
\footnotesize
\label{eq:qkv_proj}
\mathbf{\hat{q}} = \mathbf{f} \cdot \mathbf{W}_q, \quad \mathbf{\hat{k}} = \mathbf{f} \cdot \mathbf{W}_k, \quad \mathbf{\hat{v}} = \mathbf{f} \cdot \mathbf{W}_v,
% \vspace{0.2cm}
\end{equation}
where $\mathbf{f}\in \mathbb{R}^{n\times c}$ denotes the input features of a window, $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{c\times c}$ are the linear projection weights, and $\mathbf{\hat{q}}, \mathbf{\hat{k}}, \mathbf{\hat{v}} \in \mathbb{R}^{n\times c}$ are the projected features. Then, we split the projected features $\mathbf{\hat{q}}, \mathbf{\hat{k}}, \mathbf{\hat{v}}$ into $h$ heads (\ie, $\mathbb{R}^{n\times (h \times d)}$), and reshape them as $\mathbf{q}, \mathbf{k}, \mathbf{v} \in \mathbb{R}^{h\times n \times d}$. For each head, we perform dot product and weighted sum as
\begin{equation}
\footnotesize
\label{eq:attn_softmax}
\mathbf{attn}_k = \mathbf{softmax}(\mathbf{q}_k \cdot \mathbf{k}_k^T),
\vspace{-0.4cm}
\end{equation}
\begin{equation}
\footnotesize
\hat{\mathbf{z}}_k = \mathbf{attn}_k \cdot \mathbf{v}_{k},
\vspace{0.1cm}
\end{equation}
where $\mathbf{q}_k, \mathbf{k}_k, \mathbf{v}_k \in \mathbb{R}^{n\times d}$ denote the features of the $k$-th head, and $\mathbf{attn}_k \in \mathbb{R}^{n\times n}$ is the corresponding attention weight. Finally, we concatenate the features from all heads and apply the final linear projection with weight $\mathbf{W}_{proj} \in \mathbb{R}^{c\times c}$ to yield the output $\mathbf{z}\in \mathbb{R}^{n\times c}$ as
\begin{equation}
\footnotesize
\label{eq:concat}
\hat{\mathbf{z}} = \mathbf{concat}(\{\hat{\mathbf{z}}_0, \hat{\mathbf{z}}_1, ..., \hat{\mathbf{z}}_{h-1}\}).
\vspace{-0.3cm}
\end{equation}
\begin{equation}
\footnotesize
\mathbf{z} = \hat{\mathbf{z}} \cdot \mathbf{W}_{proj}.
\vspace{0.1cm}
\end{equation}

SphereFormer serves as a plugin module and can be conveniently inserted into existing mainstream models, \eg, SparseConvNet~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet,SubmanifoldSparseConvNet}, MinkowskiNet~\cite{choy20194d}, local window self-attention~\cite{mao2021voxel,lai2022stratified,fan2022embracing}. In this paper, we find that inserting it into the end of each stage works well, and the network structure is given in the supplementary material. The resulting model can be applied to various downstream tasks, such as semantic segmentation and object detection, with strong performance as produced in experiments.

% SphereFormer could significantly enhance the performance of the sparse distant points. Initially, they suffer from severe sparsity and thus insufficient local information, but SphereFormer supplies them with long-range information directly from the dense region that is often semantically rich. Therefore, the sparse distant points could overcome the disconnection issue, and enlarge the effective receptive field smoothly and fast. As a result, additional clues from the long-range information can be exploited for the sparse distant points to make correct predictions.
SphereFormer is effective for the sparse distant points to get long-range information from the dense-point region. Therefore, the sparse distant points overcome the disconnection issue, and increase the effective receptive field.

\paragraph{Comparison with Cylinder3D.}
% \label{sec:comp_cylinder3d}
Although both Cylinder3D~\cite{zhu2021cylindrical} and ours use polar or spherical coordinates to match LiDAR point distribution, there are two essential differences yet. First, Cylinder3D aims at a more balanced point distribution, while our target is to enlarge the receptive field smoothly and enable the sparse distant points to directly aggregate long-range information from the dense-point region. Second, what Cylinder3D does is replace the cubic voxel shape with the fan-shaped one. It remains to use local neighbors as before and still suffers from limited receptive field for the sparse distant points. Nevertheless, our method changes the way we find neighbors in a single operator (\ie, self-attention) and it is not limited to local neighbors. It thus avoids information separation between near and far objects and connects them in a natural way. %\xinlai{Add test result on Cylinder3D}

% Notably, by inserting the Radial Transformer Module, the sparse distant points could directly capture long-range information, which enlarges the effective receptive field smoothly and extremely fast. Unlike local operators such as sparse convolution or local self-attention, the enlargement of receptive field for the sparse distant points is no longer blocked up due to the sparsity and disconnection issues, and their segmentation performance is significantly enhanced accordingly.

\subsection{Position Encoding} 
\label{sec:pe}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/fig_splitting_v2_crop.pdf}
\end{center}
\vspace{-0.6cm}
\caption{Comparison between (a) uniform splitting and (b) {exponential splitting}. The $query$ is at the leftmost point.}
\label{fig:splitting}
\vspace{-0.1cm}
\end{figure}

For the 3D point cloud network, the input features have already incorporated the absolute $xyz$ position. Therefore, there is no need to apply absolute position encoding. Also, we notice that Stratified Transformer~\cite{lai2022stratified} develops the contextual relative position encoding. It splits a relative position into several discrete parts uniformly, which converts the continuous relative positions into integers to index the positional embedding tables. 

This method works well with local cubic windows. But in our case, the radial window is narrow and long, and its radius $r$ can take even more than 50 meters, which could cause large intervals during discretization and thus coarse-grained positional encoding. As shown in Fig.~\ref{fig:splitting} (a), because of the large interval, $key_1$ and $key_2$ correspond to the same index. But there is still a considerable distance between them.

\paragraph{Exponential Splitting.} Specifically, since the $r$ dimension covers long distances, we propose \textit{exponential splitting} for the $r$ dimension as shown in Fig.~\ref{fig:splitting} (b). The splitting interval grows exponentially when the index increases. In this way, the intervals near the $query$ are much smaller, and the $key_1$ and $key_2$ can be assigned to different position encodings. Meanwhile, we remain to adopt the \textit{uniform splitting} for the $\theta$ and $\phi$ dimensions. In notation, we have a query token $q_i$ and a key token $k_j$. Their relative position $(r_{ij}, \theta_{ij}, \phi_{ij})$ is converted into integer index $(\mathbf{idx}^r_{ij}, \mathbf{idx}^\theta_{ij}, \mathbf{idx}^\phi_{ij})$ as
\begin{equation}
\begin{footnotesize}
\begin{aligned}
\mathbf{idx}^r_{ij} = \left \{
\begin{array}{lcl}
    -\max(0, \lceil \log_2(\frac{-r_{ij}}{a}) \rceil) - 1 &  & r_{ij} < 0 \\
    0 & & r_{ij} = 0 \\
    \max(0, \lceil \log_2(\frac{r_{ij}}{a}) \rceil) &  & r_{ij} > 0 \\
\end{array} \right.,\nonumber
\end{aligned}
\end{footnotesize}
\end{equation}
\begin{equation}
\begin{footnotesize}
\begin{aligned}
\mathbf{idx}^\theta_{ij} = \lfloor \frac{\theta_{ij}}{\mathbf{inteval}_\theta} \rfloor, \quad \mathbf{idx}^\phi_{ij} = \lfloor \frac{\phi_{ij}}{\mathbf{inteval}_\phi} \rfloor,\nonumber
\end{aligned}
\end{footnotesize}
\end{equation}
\begin{equation}
\begin{footnotesize}
\begin{aligned}
    \mathbf{idx}^{x} = \mathbf{idx}^{x} + \frac{L}{2},\quad x \in \{r, \theta, \phi\},\nonumber
\end{aligned}
\end{footnotesize}
\end{equation}
where $a$ is a hyper-parameter to control the starting splitting interval, and $L$ is the length of the positional embedding tables. Note that we also add the indices with $\frac{L}{2}$ to make sure they are non-negative.

The above indices ($\mathbf{idx}^r_{ij}, \mathbf{idx}^\theta_{ij}, \mathbf{idx}^\phi_{ij}$) are then used to index their positional embedding tables $\mathbf{t}_r, \mathbf{t}_\theta, \mathbf{t}_\phi \in \mathbb{R}^{L\times (h \times d)}$ to find the corresponding position encoding $\mathbf{p}^r_{ij}, \mathbf{p}^\theta_{ij}, \mathbf{p}^\phi_{ij} \in \mathbb{R}^{h\times d}$, respectively. Then, we sum them up to yield the resultant positional encoding $\mathbf{p} \in \mathbb{R}^{h\times d}$, which then performs dot product with the features of $q_{i}$ and $k_{j}$, respectively. The original Eq.~(\ref{eq:attn_softmax}) is updated to
\begin{equation}
\begin{footnotesize}
\begin{aligned}
 \mathbf{p} & = \mathbf{p}^r_{ij} + \mathbf{p}^\theta_{ij} + \mathbf{p}^\phi_{ij}, \nonumber \\
 \mathbf{pos\_bias}_{k, i, j} & = \mathbf{q}_{k,i} \cdot \mathbf{p}_{k}^T + \mathbf{k}_{k,j} \cdot \mathbf{p}_{k}^T, \\
 \mathbf{attn}_k & = \mathbf{softmax}(\mathbf{q}_k \cdot \mathbf{k}_k^T + \mathbf{pos\_bias}_k),
\end{aligned}
\end{footnotesize}
\end{equation}
where $\mathbf{pos\_bias} \in \mathbb{R}^{h\times n \times n}$ is the positional bias to the attention weight, $\mathbf{q}_{k,i} \in \mathbb{R}^{d}$ means the the $k$-th head of the $i$-th query feature, and $\mathbf{p}_k \in \mathbb{R}^{d}$ is the $k$-th head of the position encoding $\mathbf{p}$.

The \textit{exponential splitting} strategy provides smaller splitting intervals for near token pairs and larger intervals for distant ones. This operation enables a fine-grained position representation between near token pairs, and still maintains the same number of intervals in the meanwhile. Even though the splitting intervals become larger for distant token pairs, this solution actually works well since distant token pairs require less fine-grained relative position.

%-------------------------------------------------------------------------
\subsection{Dynamic Feature Selection}
\label{sec:dfs}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/snapshot01_edit_crop_v2.jpeg}
\end{center}
\vspace{-0.5cm}
\caption{Varying-sparsity property of LiDAR point clouds. The dense close \textit{car} is marked with a green circle and the sparse distant \textit{bicycle} is marked with a red circle (best viewed in color).}
\label{fig:car_and_bicycle}
\vspace{-0.3cm}
\end{figure}

Point clouds scanned by LiDAR have the varying-sparsity property --- close points are dense and distant points are much sparser. This property makes points at different locations perceive different amounts of local information. For example, as shown in Fig.~\ref{fig:car_and_bicycle}, a point of the \textit{car} (circled in green) near the LiDAR is with rich local geometric information from its dense neighbors, which is already enough for the model to make a correct prediction -- incurring more global contexts might be contrarily detrimental. However, a point of \textit{bicycle} (circled in red) far away from the LiDAR lacks shape information due to the extreme sparsity and even occlusion. Then we should supply long-range contexts as a supplement. This example shows treating all the query points equally is not optimal. We thus propose to dynamically select local or global features to address this issue.

As shown in Fig.~\ref{fig:feat_concat}, for each token, we incorporate not only the radial contextual information, but also local neighbor communication. Specifically, input features are projected into query, key and value features as Eq.~(\ref{eq:qkv_proj}). Then, the first half of the heads are used for radial window self-attention, and the remaining ones are used for cubic window self-attention. After that, these two features are concatenated and then linearly projected to the final output $\mathbf{z}$ for feature fusion. It enables different points to dynamically select local or global features.  Formally, the Equations~(\ref{eq:attn_softmax}-\ref{eq:concat}) are updated to
% \begin{footnotesize}
\begin{equation}
\begin{footnotesize}
\begin{aligned}
    \mathbf{attn}^{radial}_k = \mathbf{softmax}(\mathbf{q}^{radial}_k \cdot {\mathbf{k}^{radial}_k}^T), \nonumber
% \vspace{-0.4cm}
\end{aligned}
\end{footnotesize}
\end{equation}
\begin{equation}
\begin{footnotesize}
\begin{aligned}
    \hat{\mathbf{z}}^{radial}_{k} = \mathbf{attn}^{radial}_k \cdot {\mathbf{v}^{radial}_k}, \nonumber
% \vspace{-0.4cm}
\end{aligned}
\end{footnotesize}
\end{equation}
\begin{equation}
\begin{footnotesize}
\begin{aligned}
\mathbf{attn}^{cubic}_k = \mathbf{softmax}(\mathbf{q}^{cubic}_k \cdot {\mathbf{k}^{cubic}_k}^T), \nonumber
% \vspace{-0.4cm}
\end{aligned}
\end{footnotesize}
\end{equation}
\begin{equation}
\begin{footnotesize}
\begin{aligned}
    \hat{\mathbf{z}}^{cubic}_{k} = \mathbf{attn}^{cubic}_k \cdot {\mathbf{v}^{cubic}_k}, \nonumber
% \vspace{-0.4cm}
\end{aligned}
\end{footnotesize}
\end{equation}
\begin{equation}
\begin{footnotesize}
\begin{aligned}
    \hat{\mathbf{z}} = \mathbf{concat}(\{\hat{\mathbf{z}}^{radial}_0, \hat{\mathbf{z}}^{radial}_1, ..., \hat{\mathbf{z}}^{radial}_{h/2-1}, \hat{z}^{cubic}_{h/2}, ..., \hat{\mathbf{z}}^{cubic}_{h-1}\}), \nonumber
% \vspace{-0.2cm}
\end{aligned}
\end{footnotesize}
\end{equation}
% \end{footnotesize}
\noindent where $\mathbf{q}^{cubic}_k, \mathbf{k}^{cubic}_k, \mathbf{v}^{cubic}_k \in \mathbb{R}^{n^{cubic} \times d}$ denote the query, key and value features for the $k$-th head with cubic window partition, and $\mathbf{attn}^{cubic}_k \in \mathbb{R}^{n^{cubic} \times n^{cubic}}$ denotes the cubic window attention weight for the $k$-th head.

% \paragraph{Prior Guided Window Partition.} Since global contexts might be detrimental for the dense nearby points, we propose to impose spatial prior and confine them with a limited range. Specifically, besides partitioning along the $\theta$ and $\phi$ axis uniformly as mentioned in eq.~(\ref{eq:window_partition}), we also partition the windows along the $r$ axis but in an exponential way, as shown in Fig.\xinlai{add fig.}. Formally, the index of the prior guided window partition is yielded by
% $$
% \mathbf{index}_r = \left \{
% \begin{array}{rcl}
%   0  &  & 0 \le r_{ij} < b \\
%   t+1  &  & 2^t \cdot b \le r_{ij} < 2^{(t+1)} \cdot b
% \end{array} \right.,
% $$
% \begin{equation}
%     \mathbf{index_{win}} = (\mathbf{index}_r, \lfloor \frac{\theta}{\Delta \theta} \rfloor, \lfloor \frac{\phi}{\Delta \phi} \rfloor).
% \end{equation}
% where $b$ is a hyperparameter to control the window size.

% With the prior guided window partition, the dense nearby points focus on local information. Meanwhile, since the radius $r$ of the window grows exponentially, the windows for sparse distant points are still sufficiently long to capture beneficial long-range information.

\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/fig_feat_concat_v5_crop.pdf}
\end{center}
\vspace{-0.5cm}
\caption{Dynamic feature selection. We split the heads to conduct radial and cubic window self-attention respectively.}
\label{fig:feat_concat}
\vspace{-0.5cm}
\end{figure}

%------------------------------------------------------------------------
\section{Experiments}
% In the following, we first elaborate on the experimental setting. Then, we show the experimental results and also the ablation study. Finally, we demonstrate the visual comparison with other methods. Our code and models will be released publicly.

In this section, we first introduce the experimental setting in Sec.~\ref{sec:exp_setting}. Then, we show the semantic segmentation and object detection results in Sec.~\ref{sec:semseg} and \ref{sec:det}. The ablation study and visual comparison are shown in Sec.~\ref{sec:ablation} and \ref{sec:vis_comp}. Our code and models will be made publicly available.

\subsection{Experimental Setting}
\label{sec:exp_setting}
\paragraph{Network Architecture.} For semantic segmentation, we adopt the encoder-decoder structure and follow U-Net~\cite{ronneberger2015u} to concatenate the fine-grained encoder features in the decoder. We follow \cite{zhu2021cylindrical} to use SparseConv~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet,SubmanifoldSparseConvNet} as our baseline model. There are a total of 5 stages whose channel numbers are $[32,64,128,256,256]$, and there are two residual blocks at each stage. Our proposed module is stacked at the end of each encoding stage. For object detection, we adopt CenterPoint~\cite{yin2021center} as our baseline model, where the backbone possesses 4 stages whose channel numbers are $[16,32,64,128]$. Our proposed module is stacked at the end of the second and third stages. Note that our proposed module incurs negligible extra parameters, and more details are given in the supplementary material.

\vspace{-0.2cm}
\paragraph{Datasets.} Following previous work, we evaluate methods on nuScenes~\cite{caesar2020nuscenes}, SemanticKITTI~\cite{behley2019semantickitti}, and Waymo Open Dataset~\cite{sun2020scalability} (WOD) for semantic segmentation. For object detection, we evaluate our methods on the nuScenes~\cite{caesar2020nuscenes} dataset. The details of the datasets are given in the supplementary material.

\begin{table*}[!htbp]
    %\footnotesize
    %\vspace{0.1cm}
    \centering
    \tabcolsep=0.12cm
    {
        \begin{footnotesize}
        \begin{tabular}{ l | c | c c c c c c c c c c c c c c c c c c c}
            \toprule
            Method & mIoU & \rotatebox{90}{road} & \rotatebox{90}{sidewalk} & \rotatebox{90}{parking} & \rotatebox{90}{other-gro.} & \rotatebox{90}{building} & \rotatebox{90}{car} & \rotatebox{90}{truck} & \rotatebox{90}{bicycle} & \rotatebox{90}{motorcycle} & \rotatebox{90}{other-veh.} & \rotatebox{90}{vegetation} & \rotatebox{90}{trunk} & \rotatebox{90}{terrain} & \rotatebox{90}{person} & \rotatebox{90}{bicyclist} & \rotatebox{90}{motorcyclist} & \rotatebox{90}{fence} & \rotatebox{90}{pole} & \rotatebox{90}{traffic sign} \\

            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            % SupOnly & & & & & & & 64.7 \\ 
            % ST & & & & & & & 66.3 \\ 
            
            % \specialrule{0em}{0pt}{1pt}
            % \hline
            % \specialrule{0em}{1pt}{0pt}
            SqueezeSegV2~\cite{wu2019squeezesegv2} & 39.7 & 88.6 & 67.6 & 45.8 & 17.7 & 73.7 & 81.8 & 13.4 & 18.5 & 17.9 & 14.0 & 71.8 & 35.8 & 60.2 & 20.1 & 25.1 & 3.9 & 41.1 & 20.2 & 26.3 \\
            
            DarkNet53Seg~\cite{behley2019semantickitti} & 49.9 & 91.8 & 74.6 & 64.8 & 27.9 & 84.1 & 86.4 & 25.5 & 24.5 & 32.7 & 22.6 & 78.3 & 50.1 & 64.0 & 36.2 & 33.6 & 4.7 & 55.0 & 38.9 & 52.2 \\
            
            RangeNet53++~\cite{milioto2019rangenet++} & 52.2 & 91.8 & 75.2 & 65.0 & 27.8 & 87.4 & 91.4 & 25.7 & 25.7 & 34.4 & 23.0 & 80.5 & 55.1 & 64.6 & 38.3 & 38.8 & 4.8 & 58.6 & 47.9 & 55.9 \\
            
            3D-MiniNet~\cite{alonso20203d} & 55.8 & 91.6 & 74.5 & 64.2 & 25.4 & 89.4 & 90.5 & 28.5 & 42.3 & 42.1 & 29.4 & 82.8 & 60.8 & 66.7 & 47.8 & 44.1 & 14.5 & 60.8 & 48.0 & 56.6 \\
            
            SqueezeSegV3~\cite{xu2020squeezesegv3} & 55.9 & 91.7 & 74.8 & 63.4 & 26.4 & 89.0 & 92.5 & 29.6 & 38.7 & 36.5 & 33.0 & 82.0 & 58.7 & 65.4 & 45.6 & 46.2 & 20.1 & 59.4 & 49.6 & 58.9 \\
            
            PointNet++~\cite{qi2017pointnet++} & 20.1 & 72.0 & 41.8 & 18.7 & 5.6 & 62.3 & 53.7 & 0.9 & 1.9 & 0.2 & 0.2 & 46.5 & 13.8 & 30.0 & 0.9 & 1.0 & 0.0 & 16.9 & 6.0 & 8.9 \\
            
            TangentConv~\cite{tatarchenko2018tangent} & 40.9 & 83.9 & 63.9 & 33.4 & 15.4 & 83.4 & 90.8 & 15.2 & 2.7 & 16.5 & 12.1 & 79.5 & 49.3 & 58.1 & 23.0 & 28.4 & 8.1 & 49.0 & 35.8 & 28.5 \\
            
            PointASNL~\cite{yan2020pointasnl} & 46.8 & 87.4 & 74.3 & 24.3 & 1.8 & 83.1 & 87.9 & 39.0 & 0.0 & 25.1 & 29.2 & 84.1 & 52.2 & 70.6 & 34.2 & 57.6 & 0.0 & 43.9 & 57.8 & 36.9 \\
            
            RandLA-Net~\cite{hu2020randla} & 55.9 & 90.5 & 74.0 & 61.8 & 24.5 & 89.7 & 94.2 & 43.9 & 29.8 & 32.2 & 39.1 & 83.8 & 63.6 & 68.6 & 48.4 & 47.4 & 9.4 & 60.4 & 51.0 & 50.7 \\
            
            KPConv~\cite{thomas2019kpconv} & 58.8 & 90.3 & 72.7 & 61.3 & 31.5 & 90.5 & 95.0 & 33.4 & 30.2 & 42.5 & 44.3 & 84.8 & 69.2 & 69.1 & 61.5 & 61.6 & 11.8 & 64.2 & 56.4 & 47.4 \\
            
            PolarNet~\cite{zhang2020polarnet} & 54.3 & 90.8 & 74.4 & 61.7 & 21.7 & 90.0 & 93.8 & 22.9 & 40.3 & 30.1 & 28.5 & 84.0 & 65.5 & 67.8 & 43.2 & 40.2 & 5.6 & 61.3 & 51.8 & 57.5 \\
            
            JS3C-Net~\cite{yan2021sparse} & 66.0 & 88.9 & 72.1 & 61.9 & 31.9 & 92.5 & 95.8 & 54.3 & 59.3 & 52.9 & 46.0 & 84.5 & 69.8 & 67.9 & 69.5 & 65.4 & 39.9 & 70.8 & 60.7 & 68.7 \\
            
            SPVNAS~\cite{tang2020searching} & 67.0 & 90.2 & 75.4 & 67.6 & 21.8 & 91.6 & 97.2 & 56.6 & 50.6 & 50.4 & 58.0 & 86.1 & 73.4 & 71.0 & 67.4 & 67.1 & 50.3 & 66.9 & 64.3 & 67.3 \\
            
            Cylinder3D~\cite{zhu2021cylindrical} & 68.9 & 92.2 & 77.0 & 65.0 & 32.3 & 90.7 & 97.1 & 50.8 & 67.6 & 63.8 & 58.5 & 85.6 & 72.5 & 69.8 & 73.7 & 69.2 & 48.0 & 66.5 & 62.4 & 66.2 \\
            
            RPVNet~\cite{xu2021rpvnet} & 70.3 & 93.4 & 80.7 & 70.3 & 33.3 & 93.5 & 97.6 & 44.2 & 68.4 & 68.7 & 61.1 & 86.5 & 75.1 & 71.7 & 75.9 & 74.4 & 43.4 & 72.1 & 64.8 & 61.4 \\
            
            (AF)\textsuperscript{2}-S3Net~\cite{cheng20212} & 70.8 & 92.0 & 76.2 & 66.8 & 45.8 & 92.5 & 94.3 & 40.2 & 63.0 & 81.4 & 40.0 & 78.6 & 68.0 & 63.1 & 76.4 & 81.7 & 77.7 & 69.6 & 64.0 & 73.3 \\
            
            PVKD~\cite{hou2022point} & 71.2 & 91.8 & 70.9 & 77.5 & 41.0 & 92.4 & 97.0 & 67.9 & 69.3 & 53.5 & 60.2 & 86.5 & 73.8 & 71.9 & 75.1 & 73.5 & 50.5 & 69.4 & 64.9 & 65.8\\
            
            2DPASS~\cite{yan20222dpass} & 72.9 & 89.7 & 74.7 & 67.4 & 40.0 & 93.5 & 97.0 & 61.1 & 63.6 & 63.4 & 61.5 & 86.2 & 73.9 & 71.0 & 77.9 & 81.3 & 74.1 & 72.9 & 65.0 & 70.4 \\
            
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            Ours & \textbf{74.8} & 91.8 & 78.2 & 69.7 & 41.3 & 93.8 & 97.5 & 59.6 & 70.1 & 70.5 & 67.7 & 86.7 & 75.1 & 72.4 & 79.0 & 80.4 & 75.3 & 72.8 & 66.8 & 72.9 \\
            
            \bottomrule                                   
        \end{tabular}
        \end{footnotesize}
    }
    \vspace{-0.3cm}
    \caption{Semantic segmentation results on SemanticKITTI \textit{test} set. Methods published before the submission deadline (11/11/2022) are listed.}
    \label{tab:exp_semantickitti}   
% \vspace{-0.2cm}
\end{table*}


\begin{table*}[!htbp]
    %\footnotesize
    %\vspace{0.1cm}
    \centering
    \tabcolsep=0.13cm
    {
        \begin{footnotesize}
        \begin{tabular}{ l | c | c c | c c c c c c c c c c c c c c c c}
            \toprule
            
            Method & Input & mIoU & FW mIoU & \rotatebox{90}{barrier} & \rotatebox{90}{bicycle} & \rotatebox{90}{bus} & \rotatebox{90}{car} & \rotatebox{90}{construction} & \rotatebox{90}{motorcycle} & \rotatebox{90}{pedestrian} & \rotatebox{90}{traffic cone} & \rotatebox{90}{trailer} & \rotatebox{90}{truck} & \rotatebox{90}{driveable} & \rotatebox{90}{other flat} & \rotatebox{90}{sidewalk} & \rotatebox{90}{terrain} & \rotatebox{90}{manmade} & \rotatebox{90}{vegetation} \\
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            PolarNet~\cite{zhang2020polarnet} & L & 69.4 & 87.4 & 72.2 & 16.8 & 77.0 & 86.5 & 51.1 & 69.7 & 64.8 & 54.1 & 69.7 & 63.5 & 96.6 & 67.1 & 77.7 & 72.1 & 87.1 & 84.5 \\
            
            JS3C-Net~\cite{yan2021sparse} & L & 73.6 & 88.1 & 80.1 & 26.2 & 87.8 & 84.5 & 55.2 & 72.6 & 71.3 & 66.3 & 76.8 & 71.2 & 96.8 & 64.5 & 76.9 & 74.1 & 87.5 & 86.1 \\
            
            Cylinder3D~\cite{zhu2021cylindrical} & L & 77.2 & 89.9 & 82.8 & 29.8 & 84.3 & 89.4 & 63.0 & 79.3 & 77.2 & 73.4 & 84.6 & 69.1 & 97.7 & 70.2 & 80.3 & 75.5 & 90.4 & 87.6 \\
            
            AMVNet~\cite{liong2020amvnet} & L & 77.3 & 90.1 & 80.6 & 32.0 & 81.7 & 88.9 & 67.1 & 84.3 & 76.1 & 73.5 & 84.9 & 67.3 & 97.5 & 67.4 & 79.4 & 75.5 & 91.5 & 88.7 \\
            
            SPVCNN~\cite{tang2020searching} & L & 77.4 & 89.7 & 80.0 & 30.0 & 91.9 & 90.8 & 64.7 & 79.0 & 75.6 & 70.9 & 81.0 & 74.6 & 97.4 & 69.2 & 80.0 & 76.1 & 89.3 & 87.1 \\
            
            (AF)\textsuperscript{2}-S3Net~\cite{cheng20212} & L & 78.3 & 88.5 & 78.9 & 52.2 & 89.9 & 84.2 & 77.4 & 74.3 & 77.3 & 72.0 & 83.9 & 73.8 & 97.1 & 66.5 & 77.5 & 74.0 & 87.7 & 86.8 \\
            
            PMF~\cite{zhuang2021perception} & L+C & 77.0 & 89.0 & 82.0 & 40.0 & 81.0 & 88.0 & 64.0 & 79.0 & 80.0 & 76.0 & 81.0 & 67.0 & 97.0 & 68.0 & 78.0 & 74.0 & 90.0 & 88.0 \\
            
            2D3DNet~\cite{genova2021learning} & L+C & 80.0 & 90.1 & 83.0 & 59.4 & 88.0 & 85.1 & 63.7 & 84.4 & 82.0 & 76.0 & 84.8 & 71.9 & 96.9 & 67.4 & 79.8 & 76.0 & 92.1 & 89.2 \\
            
            2DPASS~\cite{yan20222dpass} & L & 80.8 & 90.1 & 81.7 & 55.3 & 92.0 & 91.8 & 73.3 & 86.5 & 78.5 & 72.5 & 84.7 & 75.5 & 97.6 & 69.1 & 79.9 & 75.5 & 90.2 & 88.0 \\
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            Ours & L & \textbf{81.9} & \textbf{91.7} & 83.3 & 39.2 & 94.7 & 92.5 & 77.5 & 84.2 & 84.4 & 79.1 & 88.4 & 78.3 & 97.9 & 69.0 & 81.5 & 77.2 & 93.4 & 90.2 \\
            
            \bottomrule                                   
        \end{tabular}
        \end{footnotesize}
    }
    \vspace{-0.3cm}
    \caption{Semantic segmentation results on nuScenes \textit{test} set. Methods published before the submission deadline (11/11/2022) are listed.}
    \label{tab:exp_nuscenes}   
\vspace{-0.2cm}
\end{table*}


\vspace{-0.2cm}
\paragraph{Implementation Details.} For semantic segmentation, we use 4 GeForce RTX 3090 GPUs for training. We train the models for 50 epochs with AdamW~\cite{loshchilov2017decoupled} optimizer and `poly' scheduler where \textit{power} is set to 0.9. The learning rate and weight decay are set to $0.006$ and $0.01$, respectively. Batch size is set to 16 on nuScenes, and 8 on both SemanticKITTI and Waymo Open Dataset. The window size is set to $[120m,2^{\circ},2^{\circ}]$ for $(r, \theta, \phi)$ on both nuScenes and SemanticKITTI, and $[80m,1.5^{\circ},1.5^{\circ}]$ on Waymo Open Dataset. During data preprocessing, we confine the input scene to the range from $[-51.2m, -51.2m, -4m]$ to $[51.2m, 51.2m, 2.4m]$ on SemanticKITTI and $[-75.2m, -75.2m, -2m]$ to $[75.2m, 75.2m, 4m]$ on Waymo. Also, we set the voxel size to $0.1m$ on both nuScenes and Waymo, and $0.05m$ on SemanticKITTI.

For object detection, we adopt the OpenPCDet~\cite{openpcdet2020} codebase and follow the default CenterPoint~\cite{yin2021center} to set the training hyper-parameters. We set the window size to $[120m,1.5^{\circ},1.5^{\circ}]$.

\subsection{Semantic Segmentation Results}
\label{sec:semseg}
% We evaluate our method mainly on semantic segmentation. Also, to demonstrate the generalization ability, we show the object detection results on the nuScenes dataset.

% \subsubsection{Semantic Segmentation}

\begin{table*}[!htbp]
    %\footnotesize
    %\vspace{0.1cm}
    \centering
    \tabcolsep=0.19cm
    {
        % \begin{threeparttable}
        \begin{footnotesize}
        \begin{tabular}{ l | c | c c c c c c c c c c c c c c c c}
            \toprule
            
            Method & mIoU & \rotatebox{90}{barrier} & \rotatebox{90}{bicycle} & \rotatebox{90}{bus} & \rotatebox{90}{car} & \rotatebox{90}{construction} & \rotatebox{90}{motorcycle} & \rotatebox{90}{pedestrian} & \rotatebox{90}{traffic cone} & \rotatebox{90}{trailer} & \rotatebox{90}{truck} & \rotatebox{90}{driveable} & \rotatebox{90}{other flat} & \rotatebox{90}{sidewalk} & \rotatebox{90}{terrain} & \rotatebox{90}{manmade} & \rotatebox{90}{vegetation} \\
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            RangeNet53++~\cite{milioto2019rangenet++} & 65.5 & 66.0 & 21.3 & 77.2 & 80.9 & 30.2 & 66.8 & 69.6 & 52.1 & 54.2 & 72.3 & 94.1 & 66.6 & 63.5 & 70.1 & 83.1 & 79.8 \\
            
            PolarNet~\cite{zhang2020polarnet} & 71.0 & 74.7 & 28.2 & 85.3 & 90.9 & 35.1 & 77.5 & 71.3 & 58.8 & 57.4 & 76.1 & 96.5 & 71.1 & 74.7 & 74.0 & 87.3 & 85.7 \\
            
            Salsanext~\cite{cortinhal2020salsanext} & 72.2 & 74.8 & 34.1 & 85.9 & 88.4 & 42.2 & 72.4 & 72.2 & 63.1 & 61.3 & 76.5 & 96.0 & 70.8 & 71.2 & 71.5 & 86.7 & 84.4 \\
            
            AMVNet~\cite{liong2020amvnet} & 76.1 & 79.8 & 32.4 & 82.2 & 86.4 & 62.5 & 81.9 & 75.3 & 72.3 & 83.5 & 65.1 & 97.4 & 67.0 & 78.8 & 74.6 & 90.8 & 87.9 \\
            
            Cylinder3D~\cite{zhu2021cylindrical} & 76.1 & 76.4 & 40.3 & 91.2 & 93.8 & 51.3 & 78.0 & 78.9 & 64.9 & 62.1 & 84.4 & 96.8 & 71.6 & 76.4 & 75.4 & 90.5 & 87.4 \\
            
            PVKD~\cite{hou2022point} & 76.0 & 76.2 & 40.0 & 90.2 & 94.0 & 50.9 & 77.4 & 78.8 & 64.7 & 62.0 & 84.1 & 96.6 & 71.4 & 76.4 & 76.3 & 90.3 & 86.9 \\
            
            RPVNet~\cite{xu2021rpvnet} & 77.6 & 78.2 & 43.4 & 92.7 & 93.2 & 49.0 & 85.7 & 80.5 & 66.0 & 66.9 & 84.0 & 96.9 & 73.5 & 75.9 & 76.0 & 90.6 & 88.9 \\
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            Ours & 78.4 & 77.7 & 43.8 & 94.5 & 93.1 & 52.4 & 86.9 & 81.2 & 65.4 & 73.4 & 85.3 & 97.0 & 73.4 & 75.4 & 75.0 & 91.0 & 89.2 \\
            
            Ours$^\ddagger$ & \textbf{79.5} & 78.7 & 46.7 & 95.2 & 93.7 & 54.0 & 88.9 & 81.1 & 68.0 & 74.2 & 86.2 & 97.2 & 74.3 & 76.3 & 75.8 & 91.4 & 89.7 \\
            
            \bottomrule
        \end{tabular}
        \end{footnotesize}
        % \begin{tablenotes}
        %   \small
        %   \item $\;\;^{\ddagger}$ Rotation and translation testing-time augmentations.
        % \end{tablenotes}
        % \end{threeparttable}
    }
    \vspace{-0.3cm}
    \caption{Semantic segmentation results on nuScenes \textit{val set}. $\;\;^{\ddagger}$ denotes using rotation and translation testing-time augmentations.}
    \label{tab:exp_nuscenes_val}   
% \vspace{-0.1cm}
\end{table*}


\begin{table*}[!htbp]
    %\footnotesize
    %\vspace{0.1cm}
    \centering
    \tabcolsep=0.05cm
    {
        \begin{footnotesize}
        \begin{tabular}{ l | c | c c c | c c c c c c c c c c c c c c c c c c c c c c}
            \toprule
            
            Method & mIoU & close & med. & far & \rotatebox{90}{car} & \rotatebox{90}{truck} & \rotatebox{90}{bus} & \rotatebox{90}{other-veh.} & \rotatebox{90}{motorcyclist} & \rotatebox{90}{bicyclist} & \rotatebox{90}{pedestrian} & \rotatebox{90}{sign} & \rotatebox{90}{traffic-light} & \rotatebox{90}{pole} & \rotatebox{90}{con.cone} & \rotatebox{90}{bicycle} & \rotatebox{90}{motorcycle} & \rotatebox{90}{building} & \rotatebox{90}{vegetation} & \rotatebox{90}{tree-trunk} & \rotatebox{90}{curb} & \rotatebox{90}{road} & \rotatebox{90}{lane-marker} & \rotatebox{90}{other-gro.} & \rotatebox{90}{walkable} & \rotatebox{90}{sidewalk}\\
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            SparseConv~\cite{SubmanifoldSparseConvNet} & 66.6 & 67.8 & 64.1 & 52.6 & 94.4 & 59.8 & 85.1 & 37.8 & 2.2 & 69.1 & 89.3 & 73.4 & 40.4 & 74.8 & 57.3 & 66.6 & 75.2 & 95.5 & 91.3 & 67.0 & 68.1 & 92.3 & 41.7 & 30.1 & 79.0 & 75.6 \\
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            Ours & \textbf{69.9} & 70.3 & 68.6 & 61.9 & 94.5 & 61.6 & 87.7 & 40.2 & 0.9 & 69.7 & 90.2 & 73.9 & 41.8 & 77.2 & 65.4 & 71.9 & 83.7 & 95.9 & 91.7 & 68.4 & 69.8 & 93.3 & 53.9 & 47.9 & 80.8 & 77.2 \\
            
            \bottomrule                                   
        \end{tabular}
        \end{footnotesize}
    }
    \vspace{-0.2cm}
    \caption{Semantic segmentation results on Waymo Open Dataset \textit{val set}.}
    \label{tab:exp_waymo_val}   
\vspace{-0.3cm}
\end{table*}


The results on SemanticKITTI \textit{test} set are shown in Table~\ref{tab:exp_semantickitti}. Our method yields $74.8\%$ mIoU, a new state-of-the-art result. Compared to the methods based on range images~\cite{wu2019squeezesegv2,milioto2019rangenet++} and Bird-Eye-View (BEV)~\cite{zhang2020polarnet}, ours gives a result with over $20\%$ mIoU performance gain. Moreover, thanks to the capability of directly aggregating long-range information, our method significantly outperforms the models based on sparse convolution~\cite{yan2021sparse,zhu2021cylindrical,tang2020searching,cheng20212,xu2021rpvnet}. It is also notable that our method outperforms 2DPASS~\cite{yan20222dpass} that uses extra 2D images in training by $1.9\%$ mIoU.

In Tables~\ref{tab:exp_nuscenes} and \ref{tab:exp_nuscenes_val}, we also show the semantic segmentation results on nuScenes \textit{test} and \textit{val} set, respectively. Our method consistently outperforms others by a large margin, and achieves the 1\textsuperscript{st} place on the benchmark. It is intriguing to note that our method is purely based on LiDAR data, and it works even better than approaches of~\cite{zhuang2021perception,genova2021learning,yan20222dpass} that use additional 2D information.

Moreover, we demonstrate the semantic segmentation results on Waymo Open Dataset \textit{val} set in Table~\ref{tab:exp_waymo_val}. Our model outperforms the baseline model with a substantial gap of $3.3\%$ mIoU. Also, it is worth noting that our method achieves a $9.3\%$ mIoU performance gain for the \textit{far} points, \ie, the sparse distant points.

\subsection{Object Detection Results}
\label{sec:det}

\begin{table}[!t]
    %\footnotesize
    %\vspace{0.1cm}
    \centering
    \tabcolsep=0.07cm
    {
        \begin{footnotesize}
        \begin{tabular}{ c |  c  c  c | c c c | c c }
            \toprule
            ID & RadialWin & ExpSplit & Dynamic & close & medium & far & overall & $\Delta$ \\

            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            \uppercase\expandafter{\romannumeral1} & & & & 78.79 & 51.54 & 13.28 & 75.21 & 0.00 \\ 
            
            \uppercase\expandafter{\romannumeral2} & \Checkmark & & & 78.95 & 57.21 & 26.67 & 76.31 & +1.10 \\ 
            
            \uppercase\expandafter{\romannumeral3} & \Checkmark &  \Checkmark & & 79.92 & 61.09 & 31.10 & 77.60 & +2.39 \\ 
            
            \uppercase\expandafter{\romannumeral4} & \Checkmark &   & \Checkmark & 79.51 & 58.94 & 28.95 & 77.05 & +1.84\\ 
            
            \uppercase\expandafter{\romannumeral5} & \Checkmark &  \Checkmark & \Checkmark & 80.80 & 60.78 & 30.38 & \textbf{78.41} & +3.20\\
            
            \bottomrule                                   
        \end{tabular}
        \end{footnotesize}
    }    
    \vspace{-0.2cm}
    \caption{Ablation study. \textbf{RadialWin}: Radial window shape. \textbf{ExpSplit}: Exponential splitting. \textbf{Dynamic}: Dynamic Feature Selection. Metric: mIoU. }
    \label{tab:ablation}   
\vspace{-0.2cm}
\end{table}

\begin{table}[!t]
    \centering
    \tabcolsep=0.45cm
    {
        \begin{footnotesize}
        \begin{tabular}{ c | c c c | c }
            \toprule
            Method & close & medium & far & overall \\

            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            Cubic & 79.21 & 54.31 & 19.31 & 76.19 \\
            
            Radial & 80.80 & 60.78 & 30.38 & 78.41 \\
            
            \bottomrule                                   
        \end{tabular}
        \end{footnotesize}
    }    
    \vspace{-0.2cm}
    \caption{Comparison between radial and cubic window shapes.}
    \label{tab:comp_radial_cubic}   
\vspace{-0.3cm}
\end{table}

\begin{table}[!t]
    \centering
    \tabcolsep=0.45cm
    {
        \begin{footnotesize}
        \begin{tabular}{ c | c c c c }
            \toprule
            window size & $1.0^\circ$ & $1.5^\circ$ & $2.0^\circ$ & $2.5^\circ$ \\

            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            mIoU (\%) & 77.8 & 77.5 & 78.4 & 77.6 \\
            
            \bottomrule                                   
        \end{tabular}
        \end{footnotesize}
    }    
    \vspace{-0.2cm}
    \caption{Effect of window size for the $\theta$ and $\phi$ dimensions.}
    \label{tab:exp_window_size}   
\vspace{-0.5cm}
\end{table}

\begin{table*}[!htbp]
    %\footnotesize
    %\vspace{0.1cm}
    \centering
    \tabcolsep=0.35cm
    {
        \begin{threeparttable}
        \begin{footnotesize}
        \begin{tabular}{ l | c | c | c c c c c c c c c c }
            \toprule
            
            Method & NDS & mAP & Car & Truck & Bus & Trailer & C.V. & Ped. & Mot. & Byc. & T.C. & Bar. \\
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            PointPillars~\cite{lang2019pointpillars} & 45.3 & 30.5 & 68.4 & 23.0 & 28.2 & 23.4 & 4.1 & 59.7 & 27.4 & 1.1 & 30.8 & 38.9 \\
            
            3DSSD~\cite{yang20203dssd} & 56.4 & 42.6 & 81.2 & 47.2 & 61.4 & 30.5 & 12.6 & 70.2 & 36.0 & 8.6 & 31.1 & 47.9 \\
            
            CBSG~\cite{zhu2019class} & 63.3 & 52.8 & 81.1 & 48.5 & 54.9 & 42.9 & 10.5 & 80.1 & 51.5 & 22.3 & 70.9 & 65.7 \\
            
            CenterPoint~\cite{yin2021center} & 65.5 & 58.0 & 84.6 & 51.0 & 60.2 & 53.2 & 17.5 & 83.4 & 53.7 & 28.7 & 76.7 & 70.9\\
            
            HotSpotNet~\cite{chen2020object} & 66.0 & 59.3 & 83.1 & 50.9 & 56.4 & 53.3 & 23.0 & 81.3 & 63.5 & 36.6 & 73.0 & 71.6\\
            
            CVCNET~\cite{chen2020every} & 66.6 & 58.2 & 82.6 & 49.5 & 59.4 & 51.1 & 16.2 & 83.0 & 61.8 & 38.8 & 69.7 & 69.7\\
            
            TransFusion~\cite{bai2022transfusion} & 70.2 & 65.5 & 86.2 & 56.7 & 66.3 & 58.8 & 28.2 & 86.1 & 68.3 & 44.2 & 82.0 & 78.2\\
            
            Focals Conv~\cite{chen2022focal} & 70.0 & 63.8 & 86.7 & 56.3 & 67.7 & 59.5 & 23.8 & 87.5 & 64.5 & 36.3 & 81.4 & 74.1\\
            
            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{0pt}{1pt}
            
            Ours & 70.7 & 65.5 & 84.9 & 55.1 & 66.4 & 59.3 & 29.9 & 86.0 & 71.4 & 47.1 & 79.7 & 75.2 \\
            
            Ours$^{\ddagger}$ &	\textbf{72.8} & \textbf{68.5} & 85.3 & 57.9 & 67.0 & 59.9 & 33.7 & 88.6 & 76.3 & 56.4 & 82.2 & 78.2 \\
            
            \bottomrule                                   
        \end{tabular}
        \end{footnotesize}
        \begin{tablenotes}
          \small
          \item $\;\;^{\ddagger}$ Flipping and rotation testing-time augmentations.
        \end{tablenotes}
        \end{threeparttable}
    }
    \vspace{-0.2cm}
    \caption{Object detection results on nuScenes \textit{test set}. Methods published before the submission deadline (11/11/2022) are listed.}
    \label{tab:exp_nuscenes_det}   
% \vspace{-0.1cm}
\end{table*}

Our method also achieves strong performance in object detection. As shown in Table~\ref{tab:exp_nuscenes_det}, our method outperforms other published methods on nuScenes \textit{test set}, and ranks 3\textsuperscript{rd} on the LiDAR-only benchmark. It shows that directly aggregating long-range information is also beneficial for object detection. It also manifests the capability of our method to generalize to instance-level tasks.

\begin{figure*}[ht]
	\centering
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot24.png}
        {figures/snapshots/1/snapshot02.png}
    \end{minipage}      
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot24.png}
        {figures/snapshots/1/gt.png}
    \end{minipage}      
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot22.png}
        {figures/snapshots/1/baseline.png}
    \end{minipage} 
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot23.png}
        {figures/snapshots/1/ours.png}
    \end{minipage}      
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot20.png}
        {figures/snapshots/1/baseline_erf.png}
    \end{minipage} 
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot21.png}
        {figures/snapshots/1/ours_erf.png}
    \end{minipage} 
	 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      
	 
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        {figures/snapshots/2/snapshot07.png}
    \end{minipage}      
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot24.png}
        {figures/snapshots/2/gt.png}
    \end{minipage}      
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot22.png}
        {figures/snapshots/2/baseline.png}
    \end{minipage} 
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot23.png}
        {figures/snapshots/2/ours.png}
    \end{minipage}      
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot20.png}
        {figures/snapshots/2/baseline_erf.png}
    \end{minipage} 
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        % {figures/snapshots/snapshot21.png}
        {figures/snapshots/2/ours_erf.png}
    \end{minipage} 
	 %\vspace{0.1cm}
	 
	 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      
	 
	 %\vspace{0.1cm}
	 
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        {figures/snapshots/3/snapshot17.png}\\\footnotesize{Input}
    \end{minipage}      
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        {figures/snapshots/3/gt.png}\\\footnotesize{Ground Truth}
    \end{minipage}      
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        {figures/snapshots/3/baseline.png}\\\footnotesize{SparseConv}
    \end{minipage} 
    \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        {figures/snapshots/3/ours.png}\\\footnotesize{Ours}
    \end{minipage}      
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        {figures/snapshots/3/baseline_erf.png}\\\footnotesize{SparseConv}
    \end{minipage} 
     \begin{minipage}  {0.16\linewidth}
        \centering
        \includegraphics [width=1\linewidth,height=0.5\linewidth]
        {figures/snapshots/3/ours_erf.png}\\\footnotesize{Ours}
    \end{minipage} 
	 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
	 
	 \vspace{0.2cm}
	 
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/barrier.jpg}
    \end{minipage}\footnotesize barrier
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/bicycle.jpg}
    \end{minipage}\footnotesize bicycle
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/truck.jpg}
    \end{minipage}\footnotesize truck
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/car.jpg}
    \end{minipage}\footnotesize car
    % \begin{minipage}  {0.04\linewidth}
    %     \centering
    %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
    %     {figures/color_map_nusc/construction_vehicle.jpg}
    % \end{minipage}\footnotesize cons. veh.
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/driveable_surface.jpg}
    \end{minipage}\footnotesize driveable surface
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/manmade.jpg}
    \end{minipage}\footnotesize manmade
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/terrain.jpg}
    \end{minipage}\footnotesize terrain
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/sidewalk.jpg}
    \end{minipage}\footnotesize sidewalk
    \begin{minipage}  {0.04\linewidth}
        \centering
        \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
        {figures/color_map_nusc/vegetation.jpg}
    \end{minipage}\footnotesize vegetation
    % \begin{minipage}  {0.04\linewidth}
    %     \centering
    %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
    %     {figures/s3dis_colors/table.png}
    % \end{minipage}\footnotesize table
    % \begin{minipage}  {0.04\linewidth}
    %     \centering
    %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
    %     {figures/s3dis_colors/wall.png}
    % \end{minipage}\footnotesize wall
    % \begin{minipage}  {0.04\linewidth}
    %     \centering
    %     \includegraphics [width=0.5\linewidth,height=0.5\linewidth]
    %     {figures/s3dis_colors/window.png}
    % \end{minipage}\footnotesize window
	 
    \vspace{-0.1cm}     
    \caption{Visual comparison between vanilla SparseConv and ours (best viewed in color and by zoom-in). The brown box is the zoom-in of the cyan box. The last two columns are the difference maps with the ground truth. More examples are given in the supplementary material.}
    \label{fig:vis_comparison}
    \vspace{-0.2cm}
%\end{wrapfigure}
% \vspace{-0.4cm}
\end{figure*}

\subsection{Ablation Study}
\label{sec:ablation}
To testify the effectiveness of each component, we conduct an extensive ablation study and list the result in Table~\ref{tab:ablation}. The Experiment~\uppercase\expandafter{\romannumeral1} (Exp.~\uppercase\expandafter{\romannumeral1} for short) is our baseline model of SparseConv. Unless otherwise specified, we train the models on nuScenes \textit{train} set and make evaluations on nuScenes \textit{val} set for the ablation study. To comprehensively 
reveal the effect, we also report the performance at different distances, \ie, close ($\le 20m$), medium ($>20m$ \& $\le 50m$), far ($>50m$) distances.

% \vspace{-0.2cm}
\paragraph{Window Shape.}
By comparing Experiments~\uppercase\expandafter{\romannumeral1} and \uppercase\expandafter{\romannumeral2} in Table~\ref{tab:ablation}, we can conclude that the radial window shape is beneficial. Further, the improvement stems mainly from better handling the \textit{medium} and \textit{far} points, where we yield $5.67\%$ and $13.39\%$ mIoU performance gain, respectively. This result exactly verifies the benefit of aggregating long-range information with the radial window shape.

Moreover, we also compare the radial window shape with the cubic one proposed in \cite{fan2022embracing,mao2021voxel,lai2022stratified}. As shown in Table~\ref{tab:comp_radial_cubic}, the radial window shape considerably outperforms the cubic one.

Besides, we investigate the effect of window size as shown in Table~\ref{tab:exp_window_size}. Setting it too small may make it hard to capture meaningful information, while setting it too large may increase the optimization difficulty.

\vspace{-0.1cm}
\paragraph{Exponential Splitting.}
Compared to Exp.~\uppercase\expandafter{\romannumeral4}, Exp.~\uppercase\expandafter{\romannumeral5} improves with $1.36\%$ more mIoU, which shows the effectiveness. Moreover, the consistent conclusion could be drawn from Experiments~\uppercase\expandafter{\romannumeral2} and \uppercase\expandafter{\romannumeral3}, where we witness $3.88\%$ and $4.43\%$ more mIoU for the \textit{medium} and \textit{far} points, respectively. Also, we notice that with exponential splitting, all the \textit{close}, \textit{medium}, and \textit{far} points are better dealt with.

\vspace{-0.1cm}
\paragraph{Dynamic Feature Selection.}
From the comparison between Experiments \uppercase\expandafter{\romannumeral3} and \uppercase\expandafter{\romannumeral5}, we note that dynamic feature selection brings a $0.8\%$ mIoU performance gain. Interestingly, we further notice that the gain mainly comes from the \textit{close} points, which indicates that the \textit{close} points may not rely too much on global information, since the dense local information is already enough for correct predictions for the dense close points. It also reveals the fact that points at varying locations should be treated differently. Moreover, the comparison between Exp.~\uppercase\expandafter{\romannumeral2} and \uppercase\expandafter{\romannumeral4} leads to consistent conclusion. Although the performance of \textit{medium} and \textit{far} decreases a little, the \textit{overall} mIoU still increases, since their points number is much than that of the \textit{close} points.

% \paragraph{Prior guided window partition.}

\subsection{Visual Comparison}
\label{sec:vis_comp}
As shown in Fig.~\ref{fig:vis_comparison}, we visually compare the baseline model (\ie, SparseConv) and ours. It visually indicates that with our proposed module, more sparse distant objects are recognized, which are highlighted with cyan boxes. More examples are given in the supplementary material.

\section{Conclusion}

We have studied and dealt with varying-sparsity LiDAR point distribution. We proposed SphereFormer to enable the sparse distant points to directly aggregate information from the close ones. We designed radial window self-attention, which enlarges the receptive field for distant points to intervene with close dense ones. Also, we presented exponential splitting to yield more detailed position encoding. Dynamically selecting local or global features is also helpful. Our method demonstrates powerful performance, ranking 1\textsuperscript{st} on both nuScenes and SemanticKITTI semantic segmentation benchmarks and achieving the 3\textsuperscript{rd} on nuScenes object detection benchmark. It shows a new way to further enhance 3D visual understanding. Our limitations are discussed in the supplementary material.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
