\newpage
\appendix
% \onecolumn

\input{algos/fai.tex}

\section{Data Statistics}
\label{apd:data_statistics}
We evaluate our model on MuST-C V1 English-German (EnDe), English-Spanish (EnEs) and English-French (EnFr) datasets \citep{di-gangi-etal-2019-must}.
For training set, we follow \citet{dong-etal-2022-learning} to filter out short speech of less than 1000 frames (62.5ms) and long speech of more than 480,000 frames (30s).
The statistics of different language pairs are illustrated in Table \ref{tab:data_statistics}.
\input{tables/data_statistics}

\section{Additional Preliminary Analysis}
\label{apd:additional_analysis}

\subsection{Which part of streaming speech representation is worse?}
\label{sec:which_part}
To further verify that only the representation of the end position in streaming speech is poor,
we calculate the cosine similarity $s_{t,t^{\prime}}$ between the speech representation at the $t^{\prime}$-th ($t^{\prime} \leq t$) position in the $t$-th streaming audio input $\mathbf{\hat{x}}_t$ and the speech representation at the same position in the full encoding. 
Then we average the cosine similarities over the sentences in dataset $\mathcal{B}$ to obtain robust statistics. 
%
\begin{equation}
\begin{aligned}
\text{For $t^\prime \leq t$}, \ \Bar{s}_{t,t^{\prime}} &= \frac{1}{|\mathcal{B}_t|}\sum\limits_{\mathbf{x}\in\mathcal{B}_t} s_{t,t^\prime} (\mathbf{x}) \\
&= \frac{1}{|\mathcal{B}_t|}\sum\limits_{\mathbf{x}\in\mathcal{B}_t} \operatorname{cos}(\hat{a}_{t,t^{\prime}}, a_{t^{\prime}}), 
\label{eq:audio_cos1}
\end{aligned}
\end{equation} 
%
where $\mathcal{B}_t = \{\mathbf{x}: | \mathbf{x} | \geq t\}$ contains the audio inputs with length no shorter than $t$.

We empirically compare the averaged cosine similarity at the beginning, middle, and end positions of the speech representations. 
%The data set used for this purpose is the EnDe tst-COMMON set.
Figure \ref{fig:part} shows $\Bar{s}_{t,t^{\prime}}$ of the first three ($t^{\prime}=1,2,3$), middle three ($t^{\prime}=\lfloor \frac{1+t}{2} \rfloor-1, \lfloor \frac{1+t}{2} \rfloor, \lfloor \frac{1+t}{2} \rfloor + 1$), and last three ($t^{\prime}=t-2, t-1, t$) positions for each encoding step $t$. 
At the beginning and middle positions, the averaged cosine similarity $\Bar{s}_{t,t^{\prime}}$ is greater than 0.8 except $t^\prime=1$, indicating that the representations at such positions in the partial streaming input are close to those in the full speech. 
Note that $t^\prime=1$ with a slightly lower similarity won't hurt the performance much, because in practice it is almost impossible to apply \textit{wait}-1 policy (only read 20\emph{ms} speech input) in streaming ST. 
However, the $\Bar{s}_{t,t^{\prime}}$ declines significantly for the end positions, especially for the last one. 
In addition, we observe that as $t$ becomes larger, the streaming input will gradually approximate the full speech input, then the gap of the speech representation between the offline and the online input becomes smaller. 
We conclude that \textbf{the representations of the end position in the streaming speech are particularly inferior.}
\input{figs/fig_part}

\input{figs/degree}

\subsection{Does the poor representation at the last positions of streaming speech affect streaming ST performance?}
\label{apd:degree}



To answer this question, we only calculate the average cosine similarity in the last position for each sample. 
%
\begin{equation}
\forall \mathbf{x}, \quad \Bar{s}_{-1} (\mathbf{x})= \frac{1}{T}\sum\limits_{t=1}^{t=T} \operatorname{cos}(\hat{a}_{t,t}, a_{t}),
\label{eq:audio_cos3}
\end{equation}
%
$\Bar{s}_{-1}(\mathbf{x})$ reflects the degree of deterioration of the representation at the last position of the streaming speech. 
We sort the dataset by the value of the degree and divide them evenly into 5 groups to ensure enough samples in each group. The translation quality of each group is shown in Figure \ref{fig:degree}. 
The performance of streaming ST drops close to 10 points as the representation at the last position of the streaming speech becomes worse, while the full-sentence ST fluctuates less than 4 points. 
In addition, the performance gap between the streaming ST and the full-sentence ST becomes larger as the representation at the last position gets worse. In the worse group, the streaming ST is 12.41 points lower than the full-sentence ST. 
Therefore, we conclude that \textbf{the poor representation at the end position of the streaming speech has a strong effect on the translation quality.} 


\section{Details of Offline Training} 
\label{apd:details_of_training} 

We use an Adam optimizer with learning rate $1e^{-4}$ and warmup step $10k$.
We decay the learning rate with inverse square root schedule. 

The offline ST model is first trained by a multi-task learning, including ASR and ST tasks. 
A language identity tag is prepended to the target sentence for indicating which task is learned. 
In this stage, the CIF module which is used to detect the acoustic boundary is deactivated, in other words, the CIF module is not trained. 
The main purpose is to learn a better decoder, i.e., a well-trained language model. 
Then, we activate the CIF module such that its parameters are trainable, and continue to train for another several epochs. 
In this stage, only the ST task is learned. 


\section{Additional Experiments}

\subsection{Why we use AL rather than \textit{k}?}
\label{apd:al}

In our presented results, we plot the BLEU \emph{v.s.} AL rather than $k$. 
We argue that $k$ is not a fair metric to evaluate the latency. 
In text streaming translation, different tokenization (\emph{e.g.}, different number of BPE operations) will lead to different token boundaries for the same sentence. 
It indicates the $k$ tokens do not necessarily represent the same partial sentence for different BPE methods. 
This situation becomes even severer for speech streaming translation. 
As we have a source text token boundary detector in our model, the first $k$ detected text tokens will represent different lengths of audio frames for different input audios. 
To be precise, the wait-$k$ policy used in our streaming speech translation is actually wait-$k$ detected tokens policy. 
Therefore, we prefer to use AL rather than $k$ as the latency metric in our experiments. 



\subsection{How important of the Wav2Vec2.0?}
\label{apd:w2v2}

\input{figs/w2v2.tex}

As we mentioned in the main text, the special audio token ``mask" in Wav2Vec2.0 is pre-trained on the Librispeech dataset to reconstruct the corresponding feature conditional on unmasked context via the contrastive task. 
In our experiments, we didn't include contrastive learning as the auxiliary task in the downstream ST training. 
And in our FAI inference, we directly leverage the mask embeddings as the future context by appending them to the streaming input. 
However, we found the speech representations after ST training becomes even better. 
Particularly, we calculate the cosine similarity between every predicted future representation and full speech representations at the same position, and the results are illustrated in Figure \ref{fig:mask_acc}. 
On either the Librispeech or the MuST-C audio test set, the fine-tuned Wav2Vec2.0 can produce better speech representations from the masking inputs. 


\subsection{Why $m>10$?}
Based on the analysis in Section 3, we observed that the representations of the last 10 positions of the streaming speech are poorer. For example, the speech representations $\mathbf{\hat{a}}_{t-10:t}$ for streaming speech $\mathbf{c}_{1:t}$ of length $t$ are poor. Similarly, in FKD for a teacher's streaming speech input $\mathbf{c}_{1:t+m}$ of length $t+m$, the speech representations $\mathbf{\hat{a}}_{t+m-10:t+m}$ are always suboptimal. Hence, not all $t+m$ speech representations can be utilized as teachers, only the first $t$ speech representations are taken into account for loss calculation. If $m < 10$, $t+m-10$ will be smaller than $t$, and the representations $\mathbf{\hat{a}}_{t-10+m:t}$ will also be of inferior quality, making the representation $\mathbf{\hat{a}}_{t-10+m:t}$ a poor teacher. Thus, $m$ needs to be greater than 10 for high quality teachers.





\subsection{Why are all predicted features discarded?}
\label{apd:discard}
\input{figs/discard.tex}
In FAI strategy, all the output representations corresponding to the $m=50$ masking tokens will be discarded, because we have demonstrated that the representations at the ending positions are inferior. 
However, as shown in \ref{fig:mask_acc}, the first 10 predicted representations are not as bad as the next 40. 
Therefore, on the EnDE test set, we also conduct another streaming ST inference by appending different numbers of predicted context to the original speech representations. 
We use discard rate $p$ to measure the number of appending features. 
When $p=1.0$, all predicted features are discarded and it reduces to the standard FAI inference. 
In Figure \ref{fig:discard}, we compare the streaming speech translation quality between regular FAI and its variant. 
It is concluded that the predicted future context is too noisy and harmful to the performance.


\subsection{Additional Results on EnDe/Es and EnFr}
\label{apd:appendix_more_results}

In this section, we evaluate our methods with other latency metrics AP and DAL. 
The AP-BLEU and DAL-BLEU curves on the MuST-C EnDe, EnEs, and EnFr tst-COMMON sets are shown in Figure \ref{fig:expand_results}. 
For three language pairs, our methods can consistently improve the baseline by a large margin. 

\input{figs/expanded_results.tex}



\section{Numeric Results for the Figures}
\label{apd:appendix_numeric_results}

\input{tables/numeric_results.tex}

We also provide the numeric results for Figures \ref{fig:main_results} and \ref{fig:expand_results} in Tables \ref{tab:main_exp_numeric}, and for Figures \ref{fig:ablation} in Table \ref{tab:ablation}, and for Figures \ref{fig:main_results} in Table \ref{tab:baseline_numeric}, for Figure \ref{fig:length_bleu} in Table \ref{tab:future_length_numeric}.
