
\section{Preliminary Analysis}
\label{sec:analysis}

In this section, we examine the mismatch problem in Transformer-based \citep{vaswani2017attention} ST architecture between offline training and online decoding. 
In offline full-sentence ST, the speech representation of each frame is obtained by attending to all frames, including future frames, in the transformer encoder layers. 
Recently, a common approach in speech translation is to stack a pre-trained Wav2Vec2.0 \citep{NEURIPS2020_92d1e1eb} as the acoustic encoder with a semantic MT encoder-decoder, resulting in state-of-the-art performance in the ST task \citep{han-etal-2021-learning,dong-etal-2022-learning,fang-etal-2022-stemm,ye-etal-2022-cross}. This approach leverages the ability of Wav2Vec2.0 pre-training to learn better speech representations.


When applying an offline model to streaming inference, the lack of future frames causes an apparent mismatch problem, which can lead to a deterioration in the extracted speech representations. 
To quantify this effect, we examine three offline ST models trained on the MuST-C EnDe dataset using the Chimera \citep{han-etal-2021-learning}, STEMM \citep{fang-etal-2022-stemm}, and MoSST \citep{dong-etal-2022-learning} architectures, with a trainable acoustic encoder initialized from Wav2Vec2.0. 
We conduct analysis on the tst-COMMON set with a duration between 2s and 10s by removing outliers and noisy data, resulting 1829 examples.


\input{figs/fig_end}

For an input sequence of audio frames $\mathbf{x}=(x_1,\ldots, x_{T})$, the convolutional subsampler of Wav2Vec2.0 shrinks the length of the raw audio by a factor 320 and outputs the full speech representation sequence $\mathbf{a}$. 
For readability reasons, we uniformly use the notation $T$ to denote the sequence length of $\mathbf{a}=(a_1,\ldots, a_{T})$. 
This simplified notation does not undermine any of our conclusions while making the equations more readable.
For streaming input $\forall t\leq T, \mathbf{\hat{x}}_t=(x_1,\ldots, x_{t})$, Wav2Vec2.0 will output the representation $\mathbf{\hat{a}}_t=(\hat{a}_{t,1},\ldots, \hat{a}_{t,t})$.

To quantify the difference in speech representations between offline and online inputs, we compute the cosine similarity $s_{t,t^{\prime}}$ between the speech representation at the $t^{\prime}$-th ($t^{\prime} \leq t$) position in the streaming audio input $\mathbf{\hat{x}}_t$ and at the same position with full-sentence encoding. 
We then calculate the statistics $\Bar{s}_{-\tau}$ by averaging the cosine similarity over both the testset $\mathcal{B}$ and the time dimension with a reverse index $-\tau$ corresponding to a position $\tau-1$ frames before the end of the streaming input. 
%
\begin{align}
  &s_{t,t^\prime} (\mathbf{x}) = \operatorname{cos}(\hat{a}_{t,t^{\prime}}, a_{t^{\prime}}), \forall t^\prime\leq t, \label{eq:audio_cos} \\
  &\Bar{s}_{-\tau} = \frac{1}{|\mathcal{B}|}\sum\limits_{\mathbf{x} \in \mathcal{B}} \frac{1}{|\mathbf{x}| - \tau + 1} \sum_{t=\tau}^{|\mathbf{x}|} s_{t,t-\tau+1}(\mathbf{x}) \label{eq:audio_cos2}
\end{align}
%
Figure \ref{fig:analysis_on_lastpos} displays the $\Bar{s}_{-\tau}$ curve for the last 100 positions in  streaming inputs. 
For $\tau > 10$, the averaged cosine similarity $\Bar{s}_{-\tau}$ is greater than 0.8, indicating that the representations at those positions in a streaming input are similar to those with the full speech. 
However, the curve shows a sharp decline in the averaged cosine similarity $\Bar{s}_{-\tau}$ for the ending positions, particularly for the last one ($\tau=1$), suggesting that the mismatch problem can significantly affect the quality of speech representation for these positions. 
We provide additional analysis in Appendix \ref{apd:additional_analysis}.

