\newpage
\appendix
\onecolumn

\section{Additional Preliminary Analysis}
\label{apd:additional_analysis}

\input{algos/fine_mask.tex}

\input{figs/fig_part}
\subsection{Which part of streaming speech representation is worse?}
\label{sec:which_part}
To further verify that only the representation of the end position in streaming speech is poor,
we calculate the cosine similarity $s_{t,t^{\prime}}$ between the speech representation at the $t^{\prime}$-th ($t^{\prime} \leq t$) position in the $t$-th streaming audio input $\mathbf{\hat{x}}_t$ and the speech representation at the same position in the full encoding. 
Then we average the cosine similarities over the sentences in dataset $\mathcal{B}$ to obtain robust statistics. 
%
\begin{equation}
\begin{aligned}
\text{For $t^\prime \leq t$}, \quad \Bar{s}_{t,t^{\prime}} = \frac{1}{|\mathcal{B}_t|}\sum\limits_{\mathbf{x}\in\mathcal{B}_t} s_{t,t^\prime} (\mathbf{x}) = \frac{1}{|\mathcal{B}_t|}\sum\limits_{\mathbf{x}\in\mathcal{B}_t} \operatorname{cos}(\hat{a}_{t,t^{\prime}}, a_{t^{\prime}}), 
\label{eq:audio_cos1}
\end{aligned}
\end{equation} 
%
where $\mathcal{B}_t = \{\mathbf{x}: | \mathbf{x} | \geq t\}$ contains the audio inputs with length no shorter than $t$.

We empirically compare the averaged cosine similarity at the beginning, middle, and end positions of the speech representations. 
Figure \ref{fig:part} shows $\Bar{s}_{t,t^{\prime}}$ of the first three ($t^{\prime}=1,2,3$), middle three ($t^{\prime}=\lfloor \frac{1+t}{2} \rfloor-1, \lfloor \frac{1+t}{2} \rfloor, \lfloor \frac{1+t}{2} \rfloor + 1$), and last three ($t^{\prime}=t-2, t-1, t$) positions for each encoding step $t$. 
At the beginning and middle positions, the averaged cosine similarity $\Bar{s}_{t,t^{\prime}}$ is greater than 0.8 except $t^\prime=1$, indicating that the representations at such positions in the partial streaming input are close to those in the full speech. 
Note that $t^\prime=1$ with a slightly lower similarity won't hurt the performance much, because in practice it is almost impossible to apply \textit{wait}-1 policy (only read 20\emph{ms} speech input) in streaming ST. 
However, the $\Bar{s}_{t,t^{\prime}}$ declines significantly for the end positions, especially for the last one. 
In addition, we observe that as $t$ becomes larger, the streaming input will gradually approximate the full speech input, then the gap of the speech representation between the offline and the online input becomes smaller. 
We conclude that \textbf{the representations of the end position in the streaming speech are particularly inferior.}


\input{figs/degree}

\subsection{Does the poor representation at the last positions of streaming speech affect streaming ST performance?}
\label{apd:degree}



To answer this question, we only calculate the average cosine similarity in the last position for each sample. 
%
\begin{equation}
\forall \mathbf{x}, \quad \Bar{s}_{-1} (\mathbf{x})= \frac{1}{T}\sum\limits_{t=1}^{t=T} \operatorname{cos}(\hat{a}_{t,t}, a_{t}),
\label{eq:audio_cos3}
\end{equation}
%
$\Bar{s}_{-1}(\mathbf{x})$ reflects the degree of deterioration of the representation at the last position of the streaming speech. 
We sort the dataset by the value of the degree and divide them evenly into 5 groups to ensure enough samples in each group. The translation quality of each group is shown in Figure \ref{fig:degree}. 
The performance of streaming ST drops close to 10 points as the representation at the last position of the streaming speech becomes worse, while the full-sentence ST fluctuates less than 4 points. 
In addition, the performance gap between the streaming ST and the full-sentence ST becomes larger as the representation at the last position gets worse. In the worse group, the streaming ST is 12.41 points lower than the full-sentence ST. 
Therefore, we conclude that \textbf{the poor representation at the end position of the streaming speech has a strong effect on the translation quality.} 


\section{Numerical stable implementation of KL divergence between two bernoulli distributions}
\label{apd:KL}
% reference: https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits

Suppose $q = \sigma(x_q) \in (0,1)$, and $p \in (0,1)$ is the teacher distribution, where $\sigma(x)=\frac{1}{1+e^{-x}}$. 
Then the KL loss is 
%
\begin{align}
\text{KL}(p \| q) &= p \log\frac{p}{q} + (1-p) \log\frac{1-p}{1-q} \\
                  &= p\log p + (1-p)\log(1-p) + p\log \sigma(x_q) - (1-p)\log(1-\sigma(x_q)) \\
                  &= - H(p) - p\log\frac{1}{1+e^{-x_q}} - (1-p)\log\frac{e^{-x_q}}{1+e^{-x_q}} \\
                  &= - H(p) + p\log(1+e^{-x_q}) + (1-p)\log(1+e^{-x_q}) - (1-p)\log e^{-x_q} \\
                  &= - H(p) + \log(1+e^{-x_q}) + (1-p)x_q \\
                  &= x_q - px_q + \log(1+e^{-x_q}) - H(p)
\end{align}
%
where $H(p)$ is the the entropy of teacher distribution, and only involves $p$. So it can be removed from the loss.

If $x_q < 0$, we have
%
\begin{align}
     & x_q - px_q + \log(1+e^{-x_q}) \\
    =& \log e^{-x_q} - px_q + \log(1+e^{-x_q}) \\
    =& -px_q + \log(1+e^{x_q})
\end{align}
%
Therefore, we have a numerical stable KL loss.
%
\begin{equation}
    \text{KL}(p \| q) = \max(0, x_q) - px_q + \log(1+e^{-|x_q|})
\end{equation}
%
It can be easily implemented with the following PyTorch code.\\
\texttt{F.relu(x) + torch.log1p(torch.exp(-torch.abs(x))) - p.detach() * x} .


\section{Details of Offline Training} 
\label{apd:details_of_training}
The offline ST model is first trained by a multi-task learning, including ASR and ST tasks. 
A language identity tag is prepended to the target sentence for indicating which task is learned. 
In this stage, the CIF module which is used to detect the acoustic boundary is deactivated, in other words, the CIF module is not trained. 
The main purpose is to learn a better decoder, i.e., a well-trained language model. 
Then, we activate the CIF module such that its parameters are trainable, and continue to train for another several epochs. 
In this stage, only the ST task is learned. 


\section{Additional Experiments}

\subsection{Why we use AL rather than \textit{k}?}
\label{apd:al}

In our presented results, we plot the BLEU \emph{v.s.} AL rather than $k$. 
We argue that $k$ is not a fair metric to evaluate the latency. 
In text streaming translation, different tokenization (\emph{e.g.}, different number of BPE operations) will lead to different token boundaries for the same sentence. 
It indicates the $k$ tokens do not necessarily represent the same partial sentence for different BPE methods. 
This situation becomes even severer for speech streaming translation. 
As we have a source text token boundary detector in our model, the first $k$ detected text tokens will represent different lengths of audio frames for different input audios. 
To be precise, the wait-$k$ policy used in our streaming speech translation is actually wait-$k$ detected tokens policy. 
Therefore, we prefer to use AL rather than $k$ as the latency metric in our experiments. 


\subsection{What examples are improved by our strategies?}
\label{apd:monotonic}

For tst-COMMON on MuST-C EnDe, we use awesome-align\footnote{\url{https://github.com/neulab/awesome-align}} \cite{dou-neubig-2021-word} to identify the token-level alignment between source transcription and target translation following \citet{zhang-feng-2022-reducing}. 
First, we define the source-to-target alignment position shift as $\max\{0, i - j\}$, where the $i$th source token is aligned to the $j$th target token. 
If $i - j$ is large, it means in order to translate the $j$th target token, the model may need to read more until seeing the $i$th source token. 
Then we calculate the monotonic level of each example as the averaged alignment position shift over the number of aligned tokens, \emph{i.e.}, 
\begin{equation}
    \text{monotonic\_level} = \frac{1}{|\text{aligned\_pairs}|}\sum_{(i,j) \in \text{aligned\_pairs}} \max\{0, i - j\}
\end{equation} 
We evenly divide the test set into three groups according to different monotonic levels.
For each group, we evaluate different inference methods and report the results in Table \ref{tab:reorder}. 
As we explained in \ref{apd:al}, it is almost impossible to guarantee the same AL for different inference methods. 
For a fair comparison, we try our best to set the AL of different methods to be approximately equal. 
We can see our inference strategies show a significant advantage on the non-monotonic examples (medium and hard groups). 

\input{tables/reorder_level.tex}


\subsection{How important of the Wav2Vec2.0?}
\label{apd:w2v2}

\input{figs/w2v2.tex}

As we mentioned in the main text, the special audio token ``mask" in Wav2Vec2.0 is pre-trained on the Librispeech dataset to reconstruct the corresponding feature conditional on unmasked context via the contrastive task. 
In our experiments, we didn't include contrastive learning as the auxiliary task in the downstream ST training. 
And in our FAI inference, we directly leverage the mask embeddings as the future context by appending them to the streaming input. 
However, we found the speech representations after ST training becomes even better. 
Particularly, we calculate the cosine similarity between every predicted future representation and full speech representations at the same position, and the results are illustrated in Figure \ref{fig:mask_acc}. 
On either the Librispeech or the MuST-C audio test set, the fine-tuned Wav2Vec2.0 can produce better speech representations from the masking inputs. 


\subsection{Why are all predicted features discarded?}
\label{apd:discard}

\input{figs/discard.tex}
In FAI strategy, all the output representations corresponding to the $m=50$ masking tokens will be discarded, because we have demonstrated that the representations at the ending positions are inferior. 
However, as shown in \ref{fig:mask_acc}, the first 10 predicted representations are not as bad as the next 40. 
Therefore, on the EnDE test set, we also conduct another streaming ST inference by appending different numbers of predicted context to the original speech representations. 
We use discard rate $p$ to measure the number of appending features. 
When $p=1.0$, all predicted features are discarded and it reduces to the standard FAI inference. 
In Figure \ref{fig:discard}, we compare the streaming speech translation quality between regular FAI and its variant. 
It is concluded that the predicted future context is too noisy and harmful to the performance.

\input{figs/expanded_results.tex}

\subsection{Additional Results on EnDe/Es and EnFr}
\label{apd:appendix_more_results}


In this section, we evaluate our methods with other latency metrics AP and DAL. 
The AP-BLEU and DAL-BLEU curves on the MuST-C EnDe, EnEs, and EnFr tst-COMMON sets are shown in Figure \ref{fig:expand_results}. 
For three language pairs, our proposed  methods can consistently improve the baseline by a large margin. 


\section{Numeric Results for the Figures}
\label{apd:appendix_numeric_results}

\input{tables/numeric_results.tex}

We also provide the numeric results for Figures \ref{fig:main_results} and \ref{fig:expand_results} in Tables \ref{tab:main_exp_numeric}, and for Figures \ref{fig:ablation} in Table \ref{tab:ablation}, and for Figures \ref{fig:main_results} in Table \ref{tab:baseline_numeric}, for Figure \ref{fig:length_bleu} in Table \ref{tab:future_length_numeric}.
