\section{Background and Related Work}
 
Speech translation systems can be roughly categorized into non-streaming (offline) and streaming (online) depending on the
inference mode. 
Regardless of the inference mode, speech translation models typically employ the encoder-decoder architecture and are trained on an ST corpus
$\mathcal{D}=\{(\mathbf{x}, \mathbf{z}, \mathbf{y})\}$, where
$\mathbf{x}=(x_1,\ldots, x_{T})$ denotes an audio sequence,
$\mathbf{z}=(z_1,\ldots, z_{I})$ and $\mathbf{y}=(y_1,\ldots, y_{J})$
the corresponding source transcription and target translation
respectively.


\textbf{Non-Streaming Speech Translation} For the non-streaming ST task, the encoder maps the entire input audio $\mathbf{x}$ to the speech representations $\mathbf{h}$, and the decoder generates the $j$-th target token $y_j$ conditional on the full representations $\mathbf{h}$ and the previously generated tokens $y_{<j}$. 
The decoding process of non-streaming ST is defined as:
\begin{equation}
p(\mathbf{y} \mid \mathbf{x})=\prod_{j=1}^{J} p\left(y_{j} \mid \mathbf{x}, \mathbf{y}_{<j}\right). 
\label{eq:full}
\end{equation}

A significant amount of works have focused on non-streaming ST, including pre-training \citep{wang-etal-2020-curriculum,dong2021consecutive,tang-etal-2022-unified,ao-etal-2022-speecht5}, 
multi-task learning \citep{liu2020synchronous,indurthi2020end,indurthi2021task}, 
data augmentation \citep{pino-etal-2019-harnessing,di-gangi-etal-2019-data,mccarthy2020skinaugment}, 
knowledge distillation \citep{dong2021listen,zhao-etal-2021-mutual,du2022regularizing},
and cross-modality representation learning \citep{tang-etal-2021-improving,fang-etal-2022-stemm,ye-etal-2022-cross}.

\textbf{Streaming Speech Translation} A streaming ST model generates the $j$-th target token $y_j$ based on streaming audio prefix $\mathbf{x}_{\leq g(j)}$ and the previous tokens $y_{<j}$ , where $g(j)$ is a monotonic non-decreasing function representing the ending timestamp of the audio prefix that needs to be consumed to generate the $j$-th word. 
The decoding probability is calculated as:
%
\begin{equation}
p(\mathbf{y} \mid \mathbf{x})=\prod_{j=1}^{J} p\left(y_{j} \mid \mathbf{x}_{\leq g(j)}, \mathbf{y}_{<j}\right). 
\label{eq:simul}
\end{equation}
%

Thus, a streaming ST model requires a policy to determine whether to wait for more source speech or emit new target tokens. 
Recent studies \citep{ma-etal-2020-simulmt,ren-etal-2020-simulspeech,zeng-etal-2021-realtrans,dong-etal-2022-learning} on streaming ST make read/write decisions based on a variant of the \textit{wait-$k$} policy \citep{ma-etal-2019-stacl} that was initially proposed for streaming text translation, which alternates write and read operations after reading the first $k$ source tokens. 
Because there is no explicit word boundaries in a streaming audio, several works attempt to detect word boundaries in the audio sequence using methods such as fixed length \citep{ma-etal-2020-simulmt}, Connectionist Temporal Classification \citep{ren-etal-2020-simulspeech,zeng-etal-2021-realtrans,papi-etal-2022-simultaneous}, ASR outputs \citep{chen-etal-2021-direct}, and continuous-integrate-and fire \citep{dong-etal-2022-learning, chang22f_interspeech}. 
The \textit{wait-$k$} policy is applied based on detected words rather than audio frames. 
In other words, $g(j)$ in Eq.(\ref{eq:simul}) represents the length of audio segment corresponding to the first $j+k-1$ detected words in the streaming ST. 
Moreover, some studies \citep{arivazhagan-etal-2019-monotonic,Ma2020Monotonic,zhang-etal-2020-learning-adaptive,schneider-waibel-2020-towards,miao-etal-2021-generative,zhang-feng-2022-gaussian,zhang-feng-2022-modeling,zhang-etal-2022-learning,chang22f_interspeech,liu-etal-2021-cross,zhang-feng-2022-information} explore adaptive policies to dynamically decide when to read or write for streaming text and/or streaming speech translation. 
\citet{zhang-feng-2022-reducing} fill future source positions with positional encoding to introduce future information during training for simultaneous machine translation within the prefix-to-prefix framework. 
In this paper, we focus on a matter less attended to -- how to alleviate the mismatch between offline training and online inference.

\textbf{Knowledge Distillation for Streaming Translation}
Existing studies on streaming text and/or speech translation usually introduce future information by distilling sequence-level knowledge from offline MT \cite{ren-etal-2020-simulspeech,zhang2021future,liu-etal-2021-cross,zhu-etal-2022-aisp,deng2023mono4simt} and online MT \cite{Zaidi2021DecisionAR}.
Moreover, \citet{ren-etal-2020-simulspeech} leverage the knowledge from the multiplication of attention weights matrices of streaming ASR and NMT models to supervise the attention of the streaming ST model.
However, our FKD aims to reduce the representation gap between full speech and streaming speech.
