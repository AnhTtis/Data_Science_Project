{
    "arxiv_id": "2303.07914",
    "paper_title": "Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference",
    "authors": [
        "Biao Fu",
        "Kai Fan",
        "Minpeng Liao",
        "Zhongqiang Huang",
        "Boxing Chen",
        "Yidong Chen",
        "Xiaodong Shi"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [
        "2023-03-15"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL"
    ],
    "abstract": "A popular approach to streaming speech translation is to employ a single offline model with a \\textit{wait-$k$} policy to support different latency requirements, which is simpler than training multiple online models with different latency constraints. However, there is a mismatch problem in using a model trained with complete utterances for streaming inference with partial input. We demonstrate that speech representations extracted at the end of a streaming input are significantly different from those extracted from a complete utterance. To address this issue, we propose a new approach called Future-Aware Streaming Translation (FAST) that adapts an offline ST model for streaming input. FAST includes a Future-Aware Inference (FAI) strategy that incorporates future context through a trainable masked embedding, and a Future-Aware Distillation (FAD) framework that transfers future context from an approximation of full speech to streaming input. Our experiments on the MuST-C EnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs between translation quality and latency than strong baselines. Extensive analyses suggest that our methods effectively alleviate the aforementioned mismatch problem between offline training and online inference.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07914v1"
    ],
    "publication_venue": "work in progress"
}