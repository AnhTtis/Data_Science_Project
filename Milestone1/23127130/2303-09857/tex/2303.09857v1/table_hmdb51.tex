\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
        \begin{tabular}{lccc}
        \toprule
        Method \& Arch.   & {Classifier}  & {Params}   & {HMDB51}  \\
        \midrule
        \rowcolor{light_gray}
        Full-tuning w/ ViT-B/16~\cite{vit} & Lin.  &   86M  &   59.4   \\
        Linear w/ ViT-B/16 & Lin.  &  0.1M  &   61.2   \\
        VPT~\cite{vpt} w/ ViT-B/16 &   Trans.   &   7M    &   62.4        \\
        AdaptFormer~\cite{adaptformer} w/ ViT-B/16  &   Trans.    &   8M       &   63.7   \\
        Pro-tuning~\cite{protuning} w/ ViT-B/16 &   Trans. &   9M    &  63.3    \\
        VideoPrompt~\cite{videoprompt} w/ ViT-B/16  &   Trans.   &   6M    &  66.4    \\
        ST-Adapter${^*}$~\cite{st-adapter} w/ ViT-B/16    &   Lin. &   7M       &   65.9   \\
        \rowcolor{Light}
        \textbf{\method} w/ ViT-B/16 &   MLPs.     &    10M    &  75.6         \\
        \bottomrule
        \end{tabular}\vspace{-7pt}
    \caption{Performance comparisons for action recognition on the HMDB51~\cite{hmdb51} dataset with the CLIP pretrained ViT-B/16~\cite{clip}. We report the type of classifier and the number of learnable parameters for baselines and ours. \textbf{Lin.} and \textbf{Trans.} denote the linear classifier and temporal transformer, respectively. Our \method  uses two MLP layers as the classifier. Note that we reproduce ST-Adapter~\cite{st-adapter} for fair comparison in terms of the pretrained dataset (denoting with ${*}$).
    }\vspace{-10pt}\label{tab:hmdb51}
    \end{table}