% \vspace{-5pt}
\section{Preliminaries and Baselines}\label{sec:baseline}
\vspace{-3pt}
    \subsection{Vision transformers for video}\vspace{-5pt}
    We briefly describe how to apply vision transformers for video understanding below.
    Following \cite{transformer}, given a set of $T$ frames in a video, we split each frame into $N$ patches of size $(P \times P)$ and tokenize them using a linear projection, such that
    \begin{equation}\label{equ:tokenization}
        {\mathbf{X}}_t = [\bx_t\{\cls\}, {\bx}_{t}^{1}, {\bx}_{t}^{2}, \cdots, {\bx}_{t}^{N}] + \bp,
    \end{equation}
    where ${\mathbf{X}}_t$ is a set of tokens for the $t$-th frame, and $\bx_t\{\cls\}$ and $\bp$ denote a learnable class token and a learned positional embedding respectively.
    We feed $(N+1)$ tokens of each frame to a sequence of $L$ transformer blocks, and the output of the $l$-th block $\bh_{l, t}$ can be derived by the following equations:
    \begin{equation}
    \begin{aligned}
        \bz_{l, t} = \bh_{l-1, t} &+ \mha(\lnorm(\bh_{l-1, t})),  \\
        \bh_{l, t} = \bz_{l, t} &+ \mlp(\lnorm(\bz_{l, t})),
    \end{aligned}\label{equ:transformer}
    \end{equation}
    where $\lnorm, \mha$, and $\mlp$ denote a layer normalization~\cite{layernorm}, multi-head attention~\cite{transformer}, and a multilayer perceptron operation, respectively.
    We apply layer normalization to the learned $T$ class tokens from the final transformer block and treat them as a set of frame representations.

    To take minimal temporal modeling into account the following baselines~\cite{vpt, adaptformer, protuning}, we employ a temporal transformer block followed by a full-connected (FC) layer as a classifier for video action recognition, similar to \cite{videoprompt}.
    We add learnable temporal positional embeddings $\bp_\text{temp}$ to the frame representations (\ie, $\bx_t\{\texttt{[cls]}\} \leftarrow \bx_t\{\texttt{[cls]}\} + \bp_\text{temp}$) and feed them into the transformer classifier.
    For the ST-adapter~\cite{st-adapter}, we use a single FC layer as a classifier.

    \input{figure_tex/baselines}

    \subsection{Baselines}\vspace{-5pt}
    The objective of our work is to transfer the superiority of vision transformers pretrained on large-scale image datasets to the video domain through efficient finetuning with a small number of learnable parameters, while freezing the pretrained parameters.
    To compare with other methods, we generalize four recent PETL methods to the video domain only with the least possible transformation; (1) VPT~\cite{vpt} (2) AdaptFormer~\cite{adaptformer} (3) Pro-tuning~\cite{protuning} (4) ST-adapter~\cite{st-adapter}.
    The most of works have been originally proposed to adapt pretrained image models to downstream image tasks~\cite{vpt, adaptformer, protuning} and video models to video tasks~\cite{adaptformer}, by learning visual prompt tokens~\cite{vpt}, adapter blocks~\cite{adaptformer} and prompt prediction blocks~\cite{protuning}.
    Only the ST-adapter~\cite{st-adapter} has proposed image-to-video transfer learning.
    In this section, we describe baselines for image-to-video transfer learning in detail.
    For brevity, we leave out the subscripts in \Eq{transformer} and arouse them as needed.
    In addition, we represent the \learn{\textbf{learnable}} and \frozen{\textbf{frozen}} parameters in red and blue colors, respectively.

    \input{figure_tex/framework}

    
        
    \noindent\textbf{Visual prompt tuning (VPT)}~\cite{vpt}
    prepends $K$ trainable prompt tokens to the input space of every transformer block\footnote{While the original work also presented a shallow version (VPT-Shallow) that inserts prompt tokens to the first layer, we explore a deep version (VPT-Deep) only.} while keeping pretrained parameters frozen.
    The input tokens for each transformer block can be written as:
    \begin{equation}
        \tilde{\bh} = [\learn{\be} ;\bh],
    \end{equation}
    where $\learn{\be} \in \mathbb{R}^{K \times d}$ is a set of trainable visual prompt tokens and $d$ is a channel dimension of the original token.

    \noindent\textbf{AdaptFormer}~\cite{adaptformer}
    learns a trainable bottleneck module~\cite{adapter-nlp1, adapter-nlp2}. 
    The intermediate feature $\bz$ in \Eq{transformer} of each transformer block is fed into the AdapterMLP that consists of the original MLP layers and a bottleneck structure (parallel adapter in \figref{fig:2b}).
    The output of the AdaptFormer block can be formulated by:
    \begin{equation}
    \begin{aligned}
        % \tilde{\mathbf{h}}' &= \frozen{\texttt{LN}}(\mathbf{h}'),    \\
        \tilde{\bz} & = \sigma(\frozen{\lnorm}(\bz)\cdot \learn{\mathbf{W}_{\text{down}}}) \cdot \learn{\mathbf{W}_{\text{up}}},   \\
        \mathbf{h}& = \bz + \frozen{\mlp}(\frozen{\lnorm}(\bz)) + s \cdot \tilde{\bz},
    \end{aligned}\label{equ:adapter}
    \end{equation}
    where $\learn{\mathbf{W}_{\text{down}}}, \learn{\mathbf{W}_{\text{up}}}$ are trainable down- and up-projection matrices, $\sigma(\cdot)$ is an activation function, and $s$ is a scaling factor.

    \noindent\textbf{Pro-tuning}~\cite{protuning}
    predicts task-specific vision prompts $\bv$ from the output of each transformer block using consecutive 2D convolution layers.
    The output of each block is reshaped as $\mathbb{R}^{P\times P \times C}$ to apply 2D convolutions and the final representation is derived by adding $\bv$ and $\bh$:
    \begin{equation}
    \begin{aligned}
        \bv = \reshape(\sigma(&\learn{\conv}(\reshape(\bh)))),  \\        
        \tilde{\bh} &= \bh + \bv,   \\
    \end{aligned}
    \end{equation}
    where $\learn{\texttt{Conv2D}}$ consists of $1\times 1$ convolution layer followed by $5\times 5$ depth-wise convolution~\cite{dw-convolution} and $1\times 1$ convolution.

    \noindent\textbf{ST-adapter}~\cite{st-adapter}
    inserts a depth-wise 3D convolution layer between the down-projection layer and the activation function of the adapter.
    Different from the conventional adapters (\eg AdaptFormer~\cite{adaptformer}), the ST-adapter takes tokens for all frames to enable the model to capture temporality in videos.
    The output of the ST-adapter can be represented as:
    \begin{equation}
    \begin{aligned}
        \tilde{\bz}_t &= \frozen{\lnorm}(\bz_t),    \\
        \hat{\bz} = \sigma(\learn{\dwconv}([&\tilde{\bz}_1{\cdot} \learn{\mathbf{W}_{\text{down}}}, \cdots, \tilde{\bz}_T{\cdot} \learn{\mathbf{W}_{\text{down}}}])){\cdot}\learn{\mathbf{W}_{\text{up}}},  \\
        \bh_t = \bz_t +& \frozen{\mlp}(\tilde{\bz}_t) + s \cdot \hat{\bz}_t,        
    \end{aligned}
    \end{equation}
    where $\learn{\dwconv}$ denotes the depth-wise 3D convolution layer.
    Note that the same down-projection matrix $\learn{\mathbf{W}_{\text{down}}}$ is applied to all tokens regardless of the frame index $t$.

    We emphasize that most of the baselines~\cite{vpt, adaptformer, protuning} have \textit{not} concerned with temporal modeling. Even though ST-Adapter~\cite{st-adapter} has employed depth-wise 3D convolution layers between linear projections, it results in high computational cost. To entirely leverage a simple and efficient architecture of the adapter~\cite{adaptformer}, we incorporate the dual-path design into the pretrained image transformers.
