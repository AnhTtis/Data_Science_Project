\section{Method}  
\vspace{-5pt}
    The dual-path design (also called two-stream) is well-known architecture in CNN-based models for video recognition~\cite{i3d, two-stream, slowfast}.
    They have commonly used an optical flow~\cite{i3d, two-stream} or multiple frames with a high temporal resolution~\cite{slowfast} to capture rapidly changing motion.
    Despite the effectiveness of dual-path architecture, it has yet to be explored with the transformer due to high computational costs.
    In this work, we propose a novel PETL method, called \method, comprised of \textit{spatial} and \textit{temporal} path adaptation.
    To the best of our knowledge, our \method is the first attempt to explicitly build the two-stream architecture upon the transformer while maintaining the computational cost similar to the single-stream architecture.
    The overall framework is depicted in \figref{fig:main}.

    \vspace{-5pt}
    \subsection{Spatial adaptation}\label{sec:sa}\vspace{-5pt}
    Since the image foundation models have been trained on large amounts of web datasets, we can intuitively speculate that they might be powerful to encode the spatial context even in videos.
    In order to make the outstanding ability of spatial modeling to be more suitable for video understanding with a slight parameter tuning, we adopt two parallel adapters for $\mha$ and $\mlp$ in each transformer block.
    The parallel adapters allow the model to learn the spatial context for action recognition from the appearance of target video data while maintaining the original contexts for object recognition.
    
    Specifically, we sample $T_S$ frames from a video and tokenize each frame.
    Similar to \Eq{tokenization},
    the set of spatial tokens $\mathbf{X}^{\text{SP}}_t$ includes the learnable positional encodings $\learn{\bpsp}$ and the spatial class token $\learn{\spcls}$.
    The spatial adaptation in the $l$-th transformer block can be formulated by the following equations:
    \begin{equation}
    \small
        \begin{aligned}
            \bz_{l}^{\text{SP}} = \bh_{l-1}^{\text{SP}} &+ \frozen{\mha}(\frozen{\lnorm}(\bh_{l-1}^{\text{SP}})) + \learn{\texttt{Adapter}}(\frozen{\lnorm}(\bh_{l-1}^{\text{SP}})), \\
            \bh_{l}^{\text{SP}} = \bz_{l}^{\text{SP}} &+ \frozen{\mlp}(\frozen{\lnorm}(\bz_{l}^{\text{SP}})) + \learn{\texttt{Adapter}}(\frozen{\lnorm}(\bz_{l}^{\text{SP}})),
        \end{aligned}
    \end{equation}
    where $\mathbf{h}_{0}^{\text{SP}} = [\learn{\spcls}, \mathbf{X}_t^{\text{SP}}]+\learn{\bpsp}$.
    We average the set of the spatial $\cls$ tokens from the final transformer block to obtain a global spatial representation $\mathbf{y}^{\text{SP}}$, such that,
    \begin{equation}
        \mathbf{y}^{\text{SP}} = \frac{1}{T_S}\sum_{t=1}^{T_S} \spcls.
    \end{equation}
    Recent methods have discussed that a high frame rate only increases the computation volume and is unnecessary to understand the semantics of appearance~\cite{slowfast, revisiting}.
    We thus sparsely sample $T_S$ frames with a low frame rate (\eg 8 frames per clip).

    \subsection{Temporal adaptation}\label{sec:ta}\vspace{-5pt}
    While spatial adaptation enables the models to take the spatial contexts in video data, the image models are still incapable of modeling the temporal dynamics.
    The key component that allows video transformers to model the solid temporal context is to learn relationships between local patches across \textit{frames} in the video~\cite{space-time, vivit}.
    To make image models capable of effectively establishing this component, we suggest a novel \textit{grid-like frameset} transform technique that aggregates multiple frames into a unified \textit{grid-like frameset}.
    Our grid-like frameset design is inspired by recent visual prompting research~\cite{inpainting, prompting}.
    It is simple yet surprisingly effective in imitating temporal modeling as spatial modeling and certainly reduces the computation.
    In each transformer block, we adopt two additional serial adapters for $\mha$ and $\mlp$, respectively.

    More concretely, we sample $T$ frames from a video and scale them with factors of $w$ and $h$, such that the scaled frame size is $[W/w \times H/h \times 3]$.
    We stack $w\times h$ scaled frames according to temporal ordering and reshape a stacked frame to construct a set of frames in a grid form of the same size as the original frame (\ie, $[W \times H \times 3]$).
    Note that the total number of grid-like framesets is $T_G = T/wh$.
    The set of temporal tokens $\mathbf{X}_g^{\text{TP}}$ for the $g$-th frameset is obtained in the same way in \Eq{tokenization} and combined with the learnable temporal class token $\learn{\tpcls}$.
    Unlike the spatial adaptation, we use fixed 3D positional encodings~\cite{fixed-position}, $\bptp$, to the tokens to take the absolute temporal order and spatial positions of patches into account.
    The input transformation allows transformers to observe multiple frames at the same level.
    In experiments, we mainly construct a grid-like frameset from 16 original frames (\ie, scaling factors $w=h=4$) to take the computational efficiency and promising performance.
    
    Whereas the parallel adapter is used in the spatial path, we sequentially append adapters to the top of \texttt{MHA} and \texttt{MLP} layers in each transformer block.
    Formally, temporal adaptation in the $l$-th block can be described as:
    \begin{equation}
        \begin{aligned}
            \bz_{l}^{\text{TP}} = \mathbf{h}_{l-1}^{\text{TP}} &+ \learn{\texttt{Adapter}}(\frozen{\texttt{MHA}}(\frozen{\texttt{LN}}(\mathbf{h}_{l-1}^{\text{TP}}))), \\
            \mathbf{h}_{l}^{\text{TP}} = \bz_{l}^{\text{TP}} &+ \learn{\texttt{Adapter}}(\frozen{\texttt{MLP}}(\frozen{\texttt{LN}}(\bz_{l}^{\text{TP}}))),
        \end{aligned}
    \end{equation}
    where $\bh^{\text{TP}}_{0} = [\learn{\tpcls}, \mathbf{X}_g^{\text{TP}}] + \bptp$.
    Similar to spatial adaptation, a global temporal representation $\by^{\text{TP}}$ can be derived by averaging the set of the temporal $\cls$ tokens from the final transformer block, \ie,
    \begin{equation}
        \by^{\text{TP}} = \frac{1}{T_G} \sum_{g=1}^{T_G} \mathbf{x}_g^{\text{TP}}\{\cls\}.
    \end{equation}

    
    For the final prediction, we concatenate the global spatial and temporal representations and feed them into the classifier with GeLU activation~\cite{gelu} between two FC layers.
    
    \input{figure_tex/example}
\input{table_kinetics}

    \subsection{Does a grid-like frameset really help to encode temporal context?}\vspace{-5pt}
    This work presents a new standpoint to perform video action recognition with the pretrained image transformer by transforming multiple frames into a unified grid-like frameset.
    However, it is still questionable whether the temporal path of \method can really capture the temporal context of videos.
    In this section, we provide some qualitative examples of the attention map.
    To validate the ability of precise temporal modeling, we sample videos from the SSv2~\cite{ssv2} dataset.
    Following \cite{st-adapter}, we depict the attention map of $\tpcls$ from the final transformer block of the temporal path.
    As shown in \figref{fig:qual}, the model with the temporal adaptation (TA) of \method tends to concentrate on action-related regions, contrary to the model without TA that focuses on the irrelevant background.
    This result suggests that the temporal adaptation of \method strengthens the temporal modeling ability of the image model.
    More examples are shown in \figref{fig:qual_supp} of Appendix.