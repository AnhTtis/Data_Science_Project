\section{Experiment}
\vspace{-3pt}
\subsection{Evaluation setup} \vspace{-5pt}
\paragrapht{Datasets.} 
    We evaluate the proposed method on four standard action recognition datasets, including the Kinetics-400 (K400)~\cite{k400}, HMDB51~\cite{hmdb51}, Something-something-v2 (SSv2)~\cite{ssv2}, and Diving-48~\cite{diving}. \vspace{-5pt}
\begin{itemize}
    \item{\textbf{Kinetics-400 (K400)} contains about 240K training videos and 20K validation videos for 400 human action categories.
    Each video is trimmed to have a length of 10 seconds.
    While the K400 dataset provides a wide range of categories, they are known to be highly biased in spatial appearance~\cite{k400-bias}.}\vspace{-5pt}
    \item{\textbf{Somthing-something-v2 (SSv2)} is a more challenging dataset since they require strong temporal modeling~\cite{k400-bias}.
    They contain about 168.9K training videos and 24.7K validation videos for 174 classes.}\vspace{-5pt}
    \item{\textbf{HMDB51} is a small dataset that provides about 7K videos of 51 action categories.
    We use all three splits, each split of which consists of 3570 and 1530 videos for training and evaluation, respectively.
    We report the average accuracy for three splits.}\vspace{-5pt}
    \item{\textbf{Diving-48} is a fine-grained diving action dataset.
    We use train-test split v2 that contains about 15K training videos and 2K validation videos of 48 diving actions.
    Since the action can not be determined by only the static representations (\eg objects or background), stronger temporal modeling is required for this dataset.}\vspace{-5pt}
\end{itemize}
    
    \input{table_ssv2}
   

\paragrapht{Pretrained image backbone.}
    We employ CLIP pretrained ViT-B/16 and ViT-L/14 as backbones. The results with Swin-B~\cite{swin} are provided in \tabref{tab:add_results} of the Appendix.\vspace{-5pt}
    \begin{itemize}
        \item{\textbf{ViT-B/16}}~\cite{vit} consists of 12 transformer blocks with 86M parameters and takes patches of size $16\times 16$ as inputs.
        \vspace{-5pt}
        \item{\textbf{ViT-L/14}}~\cite{vit}, a larger model than ViT-B/16, contains 24 transformer blocks with 303M parameters. It takes $14\times 14$ patches as inputs.
        \vspace{-5pt}
    \end{itemize}

\paragrapht{Implementation details.}
    For the K400, HMDB51, and Diving-48 datasets, we uniformly sample 8 frames (\ie, $T_s = 8$) with the sampling interval 8 in the spatial path.
    In the temporal path, we uniformly sample 16, 32, and 48 frames with the sampling intervals 4, 2, and 1 to construct 1, 2, and 3 grid-like framesets (\ie, $T_g = 1, 2, 3$), respectively.
    For the SSv2 dataset, we sample the same number of frames as in other datasets, but with a dynamic sampling interval to cover the whole video.
    Note that the frames for the spatial path are the subset of the temporal path frames.
    Please refer to more implementation details in Appendix~\ref{sec:imple_detail}.
    

\subsection{Comparison with state-of-the-art} \vspace{-5pt}
    In this section, we compare the proposed method with baselines~\cite{adaptformer, vpt, protuning, st-adapter} in \secref{sec:baseline} and state-of-the-art video transformers~\cite{multiscale, uniformer, space-time, vivit, video-swin, multiscale-v2, multiview, tokenlearner, actionclip, x-clip, frozen-clip} to demonstrate the effectiveness of the proposed method on four video action recognition datasets.
    Note that the number of frames of the spatial adaptation path of \method is fixed to 8 for all experiments, \ie, $T_S=8$.
    
    \paragrapht{Results on Kinetics-400.}
    We report the results evaluated on K400~\cite{k400} in \tabref{tab:k400}.
    We first compare the proposed method with state-of-the-art video models that are pretrained on the large-scale image dataset and fully finetuned on K400.
    In terms of memory and computational efficiency, video models require a huge number of parameters ($\sim$450M~\cite{tokenlearner}) and computations ($\sim$48912 GFLOPs~\cite{tokenlearner}).
    On the other hand, we require only 10M trainable parameters which are newly stored, and 710 GFLOPs for inference using 32 frames with ViT-B/16~\cite{vit} backbone.
    Compared to X-CLIP-L/14~\cite{x-clip} which leverages the additional text branch, our \method achieves state-of-the-art performance with ViT-L/14 backbone.
    The comparisons with parameter-efficient tuning methods~\cite{frozen-clip, st-adapter} show that our \method achieves higher performance while requiring much lower burdens in computations under the same conditions.
    
    \paragrapht{Results on Something-Something-v2.}
    We present the performance comparisons on SSv2~\cite{ssv2} in \tabref{tab:ssv2}.
    The results show that our \method with ViT-B/16 achieves comparable or better performance than the prior supervised video models~\cite{space-time, multiview, multiscale, vivit}, requiring a much smaller number of trainable parameters and GFLOPs.
    Our \method with ViT-L/14 shows more competitive performance, outperforming most prior works~\cite{multiscale-v2, uniformer, omnivore}.
    The baselines~\cite{videoprompt, vpt, adaptformer, protuning}, which have relatively weak temporal modeling ability, show significantly poor performance, implying that strong temporal modeling is mandatory for the SSv2 dataset.
    The comparisons to the CLIP pretrained PET approaches~\cite{frozen-clip, st-adapter} with ViT-B/16 demonstrate the effectiveness and efficiency of \method, achieving higher performance (70.3 vs 69.5~\cite{st-adapter}) with significantly low computations (716 vs 9641~\cite{frozen-clip} GFLOPs) using 32 frames.
    Thanks to the extreme computational efficiency, our \method comprises more competitive performance using 48 frames ($T_G=3$) with low computation requirements.
    
    \input{table_hmdb51}
        
    \input{table_diving48}

    \paragrapht{Results on HMDB51.}
    In \tabref{tab:hmdb51}, we compare the results with baselines~\cite{vpt, adaptformer, protuning, videoprompt, st-adapter} on HMDB51~\cite{hmdb51} that dominantly requires strong spatial modeling for action recognition.
    Surprisingly, our \method significantly outperforms baselines by large margins.
    This result demonstrates \method fully capitalizes on the exceptional spatial modeling ability of the pretrained image model for action recognition.
    The comparison with VideoPrompt~\cite{videoprompt} that uses the additional text branch demonstrates the effectiveness of \method, improving 9.2\% performance improvement.
    
    \paragrapht{Results on Diving-48.}
    \tabref{tab:diving48} shows performance comparisons on Diving-48~\cite{diving} that requires fine-grained action recognition.
    Our \method consistently outperforms all video models with only 10M trainable parameters.
    Particularly, we obtain a better performance than ORViT~\cite{orvit} which utilizes the additional tracking model.
    It indicates the utility of \method in fine-grained action recognition and the superiority of temporal modeling of \method.

    \subsection{Components analysis}\label{sec:ablation}\vspace{-5pt}
    
    \paragrapht{Impact of dual-path.}
    In the top panel of \tabref{tab:ablation}, we train the model by ablating each path and evaluate the performance on SSv2.
    Without the temporal path (\method w/o TA), the performance is significantly degraded despite using a larger number of frames ($T_S\,$=$\,16$ vs $8$).
    Without the spatial path (\method w/o SA), we can obtain slightly better performance than the result without the temporal adaptation.
    Since the SSv2 requires strong temporal modeling, we speculate that two ablation studies show comparison results.
    However, it still shows a substantial performance gap compared to the full model of \method, demonstrating the effectiveness of the dual-path design.
    
    
    \paragrapht{Frame rates in spatial path.}
    In the middle panel of \tabref{tab:ablation}, we analyze the effect according to the number of frames used in the spatial path.
    The temporal path identically uses 16 frames to construct a grid-like frameset, and frames used in the spatial path are sampled from such 16 frames.
    A large number of $T_S$ slightly improves performance, however, requires more computational costs.
    Considering the performance improvement compared to the computation increase, we mainly set $T_S$ to 8.

    \paragrapht{Number of frames in grid.}
    We scale down original frames with scaling factors $w$ and $h$ to construct grid-like framesets.
    These factors thus determine the number of frames the model observes within one grid-like frameset.
    While a large value of factors increases the temporal resolution, the information of each frame is inevitably reduced.
    For example, the original frame is scaled down to the size of $28\times 28$ with $w\,$=$\,h\,$=$\,8$.
    Meanwhile, a small value of factors retains richer information from the original frame, however, makes the temporal resolution small.
    As shown in the bottom panel of \tabref{tab:ablation}, we attain the best performance with $w\,$=$\,h\,$=$\,4$.    
        
    \input{table_ablation}