@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(JMLR = {J. Mach. Learn. Research})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(ICML  = {ICML})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})
@String(ACL = {ACL})
@String(NAACL = {NAACL})
@String(WACV = {WACV})
@String(EMNLP = {EMNLP})

%%% Intro
@article{inductive-bias1,
author = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
title = {Relational inductive biases, deep learning, and graph networks},
journal = {arXiv preprint: arXiv:1806.01261},
year = 2018
}

@inproceedings{transformer,
author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
title = {Attention is all you need},
booktitle = NIPS,
year = 2017
}

@inproceedings{context-weight1,
author = {Ishita Dasgupta and Erin Grant and Thomas L. Griffiths},
title = {Distinguishing rule- and exemplar-based generalization in learning systems},
booktitle = ICML,
year = 2022
}

@article{context-weight2,
author = {Stephanie C.Y. Chan and Ishita Dasgupta and Junkyung Kim and Dharshan Kumaran and Andrew K. Lampinen and Felix Hill},
title = {Transformers generalize differently from information stored in context vs weights},
journal = {arXiv preprint: arXiv:2210.05675},
year = 2022
}

%%% Pretraining
@inproceedings{moco,
author = {Kaining He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
title = {Momentum contrast for unsupervised visual representation learning},
booktitle = CVPR,
year = 2020
}

@inproceedings{byol,
author = {Jean-Bastien Grill and Florian Strub and Florent Altch\'e and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and R\'emi Munos and Michal Valko},
title = {Bootstrap your own latent: A new approach to self-supervised learning},
booktitle = NIPS,
year = 2020
}

@inproceedings{simclr,
author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
title = {A simple framework for contrastive learning of visual representations},
booktitle = ICML,
year = 2020
}

@inproceedings{swav,
author = {Mathilde Caron and Ishan Misra and Julien Mairal and Priya Goyal and Piotr Bojanowski and Armand Joulin},
title = {Unsupervised learning of visual features by contrasting cluster assignments},
booktitle = NIPS,
year = 2020
}

@inproceedings{cmp,
author = {Yucheng Zhao and Guangting Wang and Chong Luo and Wenjun Zeng and Zheng-Jun Zha},
title = {Self-supervised visual representation learning by contrastive mask prediction},
booktitle = ICCV,
year = 2021
}

@inproceedings{prototype,
author = {Junnan Li and Pan Zhou and Caiming Xiong and Richard Socher and Steven CH Hoi},
title = {Prototypical contrastive learning of unsupervised representations},
booktitle = ICLR,
year = 2021
}

@inproceedings{mae,
author = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll\'{a}r and Ross Girshick},
title = {Masked autoencoders are scalable vision learners},
booktitle = CVPR,
year = 2022
}

@inproceedings{empirical,
author = {Xinlei Chen and Saining Xie and Kaiming He},
title = {An empirical study of training self-supervised vision transformers},
booktitle = ICCV,
year = 2021
}

@inproceedings{simmim,
author = {Zhenda Xie and Zheng Zhang and Yue Cao and Yutong Lin and Jianmin Bao and Zhuliang Yao and Qi Dai and Han Hu},
title = {SimMIM: A simple framework for masked image modeling},
booktitle = CVPR,
year = 2022
}

@article{mae-st,
author = {Christoph Feichtenhofer and Haoqi Fan and Yanghao Li and Kaiming He},
title = {Masked autoencoders as spatiotemporal learners},
journal = {arXiv preprint: arXiv:2205.09113},
year = 2022
}

@inproceedings{videomae,
author = {Zhan Tong and Yibing Song and Jue Wang and Limin Wang},
title = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
booktitle = NIPS,
year = 2022
}

@inproceedings{video-moco,
author = {Tian Pan and Yibing Song and Tianyu Yang and Wenhao Jiang and Wei Liu},
title = {VideoMoCo: contrastive video representation learning with temporally adversarial examples},
booktitle = CVPR,
year = 2021
}

@inproceedings{provico,
author = {Jungin Park and Jiyoung Lee and Ig-Jae Kim and Kwanghoon Sohn},
title = {Probabilistic representations for video contrastive learning},
booktitle = CVPR,
year = 2022
}

@article{florence,
author = {Lu Yuan and Dongdong Chen and Yi-Ling Chen and Noel Codella and Xiyang Dai and Jianfeng Gao and Houdong Hu and Xuedong Huang and Boxin Li and Chunyuan Li and Ce Liu and Mengchen Liu and Zicheng Liu and Yumao Lu and Yu Shi and Lijuan Wang and Jianfeng Wang and Bin Xiao and Zhen Xiao and Jianwei Yang and Michael Zeng and Luowei Zhou and Pengchuan Zhang},
title = {Florence: A New Foundation Model for Computer Vision},
journal = {arXiv preprint: arXiv:2111.11432},
year = 2021
}

@inproceedings{align,
author = {Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
title = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
booktitle = ICML,
year = 2021
}

@inproceedings{videoclip,
author = {Hu Xu and Gargi Ghosh and Po-Yao Huang and Dmytro Okhonko and Armen Aghajanyan and Florian Metze and Luke Zettlemoyer and Christoph Feichtenhofer},
title = {VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding},
booktitle = EMNLP,
year = 2021
}

@inproceedings{videobert,
author = {Chen Sun and Austin Myers and Carl Vondrick and Kevin Murphy and Cordelia Schmid},
title = {VideoBERT: A Joint Model for Video and Language Representation Learning
},
booktitle = ICCV,
year = 2019
}

@techreport{gpt-1,
author = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
title = {Improving Language Understanding by Generative Pre-Training},
institution  = {OpenAI},
year = 2018
}

@techreport{gpt-2,
author = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
title = {Language Models are Unsupervised Multitask Learners},
institution  = {OpenAI},
year = 2019
}

@inproceedings{gpt-3,
author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish Alec Radford and Ilya Sutskever and Dario Amodei},
title = {Language Models are Few-Shot Learners},
booktitle = NIPS,
year = 2020
}

@article{t5,
author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
journal = JMLR,
year = 2020
}


@article{beit-pretraining,
author = {Wenhui Wang and Hangbo Bao and Li Dong and Johan Bjorck and Zhiliang Peng and Qiang Liu and Kriti Aggarwal and Owais Khan Mohammed and Saksham Singhal and Subhojit Som and Furu Wei},
title = {Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks},
journal = {arXiv preprint: arXiv:2208.10442},
year = 2022
}

@inproceedings{swin,
author = {Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
booktitle = ICCV,
year = 2021
}

@inproceedings{swin-v2,
author = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},
title = {Swin Transformer V2: Scaling Up Capacity and Resolution},
booktitle = CVPR,
year = 2022
}

@inproceedings{video-swin,
author = {Ze Liu and Jia Ning and Yue Cao and Yixuan Wei and Zheng Zhang and Stephen Lin and Han Hu},
title = {Video Swin Transformer},
booktitle = CVPR,
year = 2022
}

@inproceedings{cvt,
author = {Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang},
title = {Cvt: Introducing convolutions to vision transformers},
booktitle = ICCV,
year = 2021
}

@inproceedings{cswin,
author = {Xiaoyi Dong and Jianmin Bao and Dongdong Chen and Weiming Zhang and Nenghai Yu and Lu Yuan and Dong Chen and Baining Guo},
title = {Cswin transformer: A general vision transformer backbone with cross-shaped windows},
booktitle = CVPR,
year = 2022
}

@inproceedings{tokens-to-tokens,
author = {Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Zi-Hang Jiang and Francis EH Tay and Jiashi Feng and Shuicheng Yan},
title = {Tokens-to-token vit: Training vision transformers from scratch on imagenet},
booktitle = ICCV,
year = 2021
}

@inproceedings{space-time,
author = {Gedas Bertasius and Heng Wang and Lorenzo Torresani},
title = {Is space-time attention all you need for video understanding?},
booktitle = ICML,
year = 2021
}

@inproceedings{pretraining1,
author = {Chen Sun and Abhinav Shrivastava and Saurabh Singh and Abhinav Gupta},
title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
booktitle = ICCV,
year = 2017
}

@inproceedings{pretraining2,
author = {Xiaohua Zhai and Alexander Kolesnikov and Neil Houlsby and Lucas Beyer},
title = {Scaling Vision Transformers},
booktitle = CVPR,
year = 2022
}

@inproceedings{revisiting,
author = {Shyamal Buch and Crist\'{o}bal Eyzaguirre and Adrien Gaidon and Jiajun Wu and Li Fei-Fei and Juan Carlos Niebles},
title = {Revisiting the ``videos" in video-language understanding},
booktitle = CVPR,
year = 2022
}

%%% Baselines
@inproceedings{vit,
author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
booktitle = ICML,
year = 2021
}

@inproceedings{vivit,
author = {Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lu\v{c}i\'{c} and Cordelia Schmid},
title = {ViViT: A Video Vision Transformer},
booktitle = ICCV,
year = 2021
}

@inproceedings{c3d,
author = {Shuiwang Ji and Wei Xu and Ming Yang and Kai Yu},
title = {3d convolutional neural networks for human action recognition},
booktitle = ICML,
year = 2010
}

@inproceedings{two-stream,
author = {Karen Simonyan and Andrew Zisserman},
title = {Two-stream
convolutional networks for action recognition in videos},
booktitle = NIPS,
year = 2014
}

@inproceedings{i3d,
author = {Jo{\~{a}}o Carreira and Andrew Zisserman},
title = {Quo Vadis, action recognition? A new model and the kinetics dataset},
booktitle = CVPR,
year = 2017
}

@inproceedings{s3d,
author = {Saining Xie and Chen Sun and Jonathan Huang and Zhuowen Tu and Kevin Murphy},
title = {Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification},
booktitle = ECCV,
year = 2018
}

@inproceedings{r(2+1)d,
author = {Du Tran and Heng Wang and Lorenzo Torresani and Jamie Ray and Yann LeCun and Manohar Paluri},
title = {A closer look at spatiotemporal convolutions for action recognition},
booktitle = CVPR,
year = 2018
}

@inproceedings{slowfast,
author = {Christoph Feichtenhofer and Haoqi Fan and Jitendra Malik and Kaiming He},
title = {Slowfast networks for video recognition},
booktitle = ICCV,
year = 2019
}

@inproceedings{tsm,
author = {Ji Lin and Chuang Gan and Song Han},
title = {TSM: Temporal shift module for efficient video understanding},
booktitle = ICCV,
year = 2019
}

@inproceedings{vidtr,
author = {Yanyi Zhang and Xinyu Li and Chunhui Liu, Bing Shuai and Yi Zhu and Biagio Brattoli and Hao Chen and Ivan Marsic and Joseph Tighe},
title = {Vidtr: Video transformer without convolutions},
booktitle = ICCV,
year = 2021
}

@inproceedings{multiscale,
author = {Haoqi Fan and Bo Xiong and Karttikeya Mangalam and Yanghao Li and Zhicheng Yan and Jitendra Malik and Christoph Feichtenhofer},
title = {Multiscale vision transformers},
booktitle = ICCV,
year = 2021
}

@inproceedings{multiscale-v2,
author = {Yanghao Li and Chao-Yuan Wu and Haoqi Fan and Karttikeya Mangalam and Bo Xiong and Jitendra Malik and Christoph Feichtenhofer},
title = {MViTv2: Improved Multiscale Vision Transformers for Classification and Detection},
booktitle = CVPR,
year = 2022
}

@inproceedings{multiview,
author = {Shen Yan and Xuehan Xiong and Anurag Arnab and Zhichao Lu and Mi Zhang and Chen Sun and Cordelia Schmid},
title = {Multiview transformers for video recognition},
booktitle = CVPR,
year = 2022
}

@inproceedings{tokenlearner,
author = {Michael S Ryoo and AJ Piergiovanni and Anurag Arnab and Mostafa Dehghani and Anelia Angelova},
title = {Tokenlearner: Adaptive space-time tokenization for videos},
booktitle = NIPS,
year = 2021
}

@inproceedings{omnivore,
author = {Rohit Girdhar and Mannat Singh and Nikhila Ravi and Laurens van der Maaten and Armand Joulin and Ishan Misra},
title = {Omnivore: A single model for many visual modalities},
booktitle = CVPR,
year = 2022
}

@inproceedings{clip,
author = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
title = {Learning Transferable Visual Models From Natural Language Supervision},
booktitle = ICLR,
year = 2021
}


@inproceedings{vpt,
author = {Menglin Jia and Luming Tang and Bor-Chun Chen and Claire Cardie1 and Serge Belongie and Bharath Hariharan and Ser-Nam Lim},
title = {Visual prompt tuning},
booktitle = ECCV,
year = 2022
}

@article{visual-prompt,
author = {Hyojin Bahng and Ali Jahanian and Swami Sankaranarayanan and Phillip Isola},
title = {Exploring Visual Prompts for Adapting Large-Scale Models},
journal = {arXiv preprint: arXiv:2203.17274},
year = 2022
}

@inproceedings{adaptformer,
author = {Shoufa Chen and Chongjian Ge and Zhan Tong and Jiangliu Wang and Yibing Song and Jue Wang and Ping Luo},
title = {AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition},
booktitle = NIPS,
year = 2022
}

@article{bypass,
author = {Shibo Jie and Zhi-Hong Deng},
title = {Convolutional Bypasses Are Better Vision Transformer Adapters},
journal = {arXiv preprint. arXiv:2207.07039},
year = 2022
}

@inproceedings{bitfit,
author = {Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg},
title = {BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
booktitle = ACL,
year = 2022
}

@inproceedings{adapter-nlp1,
author = {Neil Houlsby and Andrei Giurgiu and Stanis\l aw Jastrz\c{e}bski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
title = {Parameter-Efficient Transfer Learning for NLP},
booktitle = ICML,
year = 2019
}

@inproceedings{adapter-nlp2,
author = {Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
title = {Towards a Unified View of Parameter-Efficient Transfer Learning},
booktitle = ICLR,
year = 2022
}

@inproceedings{vl-adapter,
author = {Yi-Lin Sung and Jaemin Cho and Mohit Bansal},
title = {VL-Adapter: Parameter-efficient transfer learning for vision-language tasks},
booktitle = CVPR,
year = 2022
}

@inproceedings{prompt-nlp,
author = {Yusheng Su and Xiaozhi Wang and Yujia Qin and Chi-Min Chan and Yankai Lin and Huadong Wang and Kaiyue Wen and Zhiyuan Liu and Peng Li and Juanzi Li and Lei Hou and Maosong Sun and Jie Zhou},
title = {On Transferability of Prompt Tuning for Natural Language Processing},
booktitle = NAACL,
year = 2022
}

@inproceedings{diff,
author = {Demi Guo and Alexander M. Rush and Yoon Kim},
title = {Parameter-efficient transfer learning with diff pruning},
booktitle = ACL,
year = 2021
}

@inproceedings{prefix,
author = {Xiang Lisa Li and Percy Liang},
title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
booktitle = ACL,
year = 2021
}

@inproceedings{uniformer,
author = {Kunchang Li and YaliWang and Gao Peng and Guanglu Song and Yu Liu and Hongsheng Li and Yu Qiao},
title = {Uniformer: Unified transformer for efficient spatial-temporal representation learning},
booktitle = ICLR,
year = 2021
}

@article{actionclip,
author = {Mengmeng Wang and Jiazheng Xing and Yong Liu},
title = {ActionCLIP: A New Paradigm for Video Action Recognition},
journal = {arXiv preprint: arXiv:2109.08472},
year = 2021
}

@article{protuning,
author = {Xing Nie and Bolin Ni and Jianlong Chang and Gaomeng Meng and Chunlei Huo and Zhaoxiang Zhang and Shiming Xiang and Qi Tian and Chunhong Pan},
title = {Pro-tuning: Unified Prompt Tuning for Vision Tasks},
journal = {arXiv preprint: arXiv:2207.14381},
year = 2022
}

@inproceedings{videoprompt,
author = {Chen Ju and Tengda Han and Kunhao Zheng and Ya Zhang and Weidi Xie},
title = {Prompting Visual-Language Models for Efficient Video Understanding},
booktitle = ECCV,
year = 2022
}

@inproceedings{x-clip,
author = {Bolin Ni and Houwen Peng and Minghao Chen and Songyang Zhang},
title = {Expanding Language-Image Pretrained Models for General Video Recognition},
booktitle = ECCV,
year = 2022
}

@inproceedings{st-adapter,
author = {Junting Pan and Ziyi Lin and Xiatian Zhu and Jing Shao and Hongsheng Li},
title = {ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition},
booktitle = NIPS,
year = 2022
}

@inproceedings{frozen-clip,
author = {Ziyi Lin and Shijie Geng and Renrui Zhang and Peng Gao and Gerard de Melo and Xiaogang Wang and Jifeng Dai and Yu Qiao and Hongsheng Li},
title = {Frozen CLIP Models are Efficient Video Learners},
booktitle = ECCV,
year = 2022
}

@inproceedings{orvit,
author = {Roei Herzig and Elad Ben-Avraham and Karttikeya Mangalam and Amir Bar and Gal Chechik and Anna Rohrbach and Trevor Darrell and Amir Globerson},
title = {Object-Region Video Transformers
},
booktitle = CVPR,
year = 2022
}

@article{aim,
author = {},
title = {AIM: Adapting image models for efficient video understanding},
journal = {},
year = 2023
}

@inproceedings{lora,
author = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
title = {LoRA: Low-Rank Adaptation of Large Language Models},
booktitle = ICLR,
year = 2022
}

@article{inpainting,
author = {Amir Bar and Yossi Gandelsman and Trevor Darrell and Amir Globerson and Alexei A. Efros},
title = {Visual prompting via image inpainting},
journal = {arXiv preprint: arXiv:2209.00647},
year = 2022
}

@article{prompting,
author = {Hyojin Bahng and Ali Jahanian and Swami Sankaranarayanan and Phillip Isola},
title = {Exploring Visual Prompts for Adapting Large-Scale Models},
journal = {arXiv preprint: arXiv:2203.17274},
year = 2022
}


%%% Datasets

@inproceedings{imagenet,
author = {Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei},
title = {Imagenet: A large-scale hierarchical image database},
booktitle = CVPR,
year = 2009
}

@inproceedings{laion-400m,
author = {Christoph Schuhmann and Richard Vencu and Romain Beaumont and Robert Kaczmarczyk and Clayton Mullis and Aarush Katta and Theo Coombes and Jenia Jitsev and Aran Komatsuzaki},
title = {LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},
booktitle = {NeurIPS Workshop},
year = 2021
}

@inproceedings{laion-5b,
author = {Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
title = {LAION-5B: An open large-scale dataset for training next generation image-text models},
booktitle = NIPS,
year = 2022
}

@article{basic,
author = {Hieu Pham and Zihang Dai and Golnaz Ghiasi and Kenji Kawaguchi and Hanxiao Liu and Adams Wei Yu and Jiahui Yu and Yi-Ting Chen and Minh-Thang Luong and Yonghui Wu and Mingxing Tan and Quoc V. Le},
title = {Combined Scaling for Open-Vocabulary Image Classification},
journal = {arXiv preprint: arXiv:2111.10050},
year = 2021
}

@inproceedings{hmdb51,
author = {Hildegard Kuehne and Hueihan Jhuang and Est\'ibaliz Garrote and Tomaso Poggio and Thomas Serre},
title = {HMDB: A large video database for human motion recognition},
booktitle = ICCV,
year = 2011
}

@inproceedings{ssv2,
author = {Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzy≈Ñska and Susanne Westphal and Heuna Kim and Valentin Haenel and Ingo Fruend and Peter Yianilos and Moritz Mueller-Freitag and Florian Hoppe and Christian Thurau and Ingo Bax and Roland Memisevic
},
title = {The ``something something" video database for learning and evaluating visual common sense},
booktitle = ICCV,
year = 2017
}

@article{k400,
author = {Will Kay and Joao Carreira and Karen Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and Trevor Back and Paul Natsev and Mustafa Suleyman and Andrew Zisserman
},
title = {The kinetics human action video dataset},
journal = {arXiv preprint: arXiv:1705.06950},
year = 2017
}

@article{k700,
author = {Joao Carreira and Eric Noland and Chloe Hillier and Andrew Zisserman
},
title = {A short note on the kinetics-700 human action dataset},
journal = {arXiv preprint: arXiv:1907.06987},
year = 2019
}

@inproceedings{diving,
author = {Yingwei Li and Yi Li and Nuno Vasconcelos
},
title = {Resound: Towards action recognition without representation bias},
booktitle = ECCV,
year = 2018
}

@inproceedings{k400-bias,
author = {Laura Sevilla-Lara and Shengxin Zha and Zhicheng Yan and Vedanuj Goswami and Matt Feiszli and Lorenzo Torresani
},
title = {Only Time Can Tell: Discovering Temporal Data for Temporal Modeling},
booktitle = WACV,
year = 2021
}

%% Techniques
@article{layernorm,
author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
title = {Layer normalization},
journal = {arXiv preprint: arXiv:1607.06450},
year = 2016
}

@inproceedings{dw-convolution,
author = {Fran\c{c}ois Chollet},
title = {Xception: Deep Learning with Depthwise Separable Convolutions},
booktitle = CVPR,
year = 2017
}

@inproceedings{st-position,
author = {Brendan Duke and Abdalla Ahmed and Christian Wolf and Parham Aarabi and Graham W. Taylor},
title = {SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation},
booktitle = CVPR,
year = 2021
}

@inproceedings{fixed-position,
author = {Yuqing Wang and Zhaoliang Xu and Xinlong Wang and Chunhua Shen and Baoshan Cheng and Hao Shen and Huaxia Xia},
title = {End-to-End Video Instance Segmentation with Transformers},
booktitle = CVPR,
year = 2021
}

@article{gelu,
author = {Dan Hendrycks and Kevin Gimpel},
title = {Gaussian Error Linear Units (GELUs)},
journal = {arXiv preprint: arXiv:1606.08415},
year = 2016
}

@inproceedings{kaiming-init,
author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
title = {Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification},
booktitle = ICCV,
year = 2015
}

@inproceedings{adamw,
author = {Ilya Loshchilov and Frank Hutter},
title = {Decoupled Weight Decay Regularization},
booktitle = ICLR,
year = 2019
}

@inproceedings{cosine-sched,
author = {Ilya Loshchilov and Frank Hutter},
title = {SGDR: Stochastic Gradient Descent with Warm Restarts},
booktitle = ICLR,
year = 2017
}

@inproceedings{p2p,
author = {Ziyi Wang and XuminYu and YongmingRao and Jie Zhou JiwenLu},
title = {P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting},
booktitle = NIPS,
year = 2022
}

@inproceedings{randaugment,
author = {Ekin Dogus Cubuk and Barret Zoph and Jon Shlens and Quoc Le},
title = {RandAugment: Practical Automated Data Augmentation with a Reduced Search Space},
booktitle = NIPS,
year = 2020
}

@inproceedings{randerase,
author = {Zhun Zhong and Liang Zheng and Guoliang Kang and Shaozi Li and Yi Yang},
title = {Random Erasing Data Augmentation},
booktitle = AAAI,
year = 2020
}

@inproceedings{sifar,
author = {Quanfu Fan and Chun-Fu (Richard) Chen and Rameswar Panda},
title = {Can an image classifier suffice for action recognition?},
booktitle = ICLR,
year = 2022
}