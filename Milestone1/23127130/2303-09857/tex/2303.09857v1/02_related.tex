\section{Related Work}
\vspace{-5pt}
\paragrapht{Pretraining vision models.}
    To address the burdens of collecting large-scale labeled datasets for supervised learning~\cite{imagenet, pretraining1, pretraining2}, self-supervised learning methods~\cite{moco, simclr, byol, mae, empirical, simmim} have been introduced to learn general-purpose visual representations from unlabeled data.
    Similarly, self-supervised learning methods for videos have also been proposed with large-scale unlabeled video/video-language data~\cite{video-moco, provico, mae-st, videomae,videoclip, actionclip, videobert}.
    However, collecting even unlabeled video-language pairs is still quite costly compared to image-language pairs.
    In addition, pretraining video models require more computational power than images.
    We thus take advantage of the powerful pretrained image-based models for efficient video understanding.

\paragrapht{Video action recognition.}
    Action recognition is one of the most fundamental research topics for video understanding.
    Early works have been built upon convolution neural networks (CNNs)~\cite{i3d, s3d, slowfast, tsm, r(2+1)d} to effectively infer the spatiotemporal context for action recognition.
    Since Vision Transformer (ViT)~\cite{vit} has become a new paradigm in computer vision, transformers for video understanding have been actively studied by extending pretrained image models.
    The pretrained image transformers have been used to initialize the part of the video transformers~\cite{vivit, space-time, vidtr, multiview} or inflated to the video transformers~\cite{video-swin}.
    While transformers have demonstrated superior performance on video action recognition, they require full finetuning on video datasets, making the training inefficient.
    
    
    
    \paragrapht{Parameter-efficient transfer learning (PETL).}
    To address the memory and parameter inefficiency of full-/partial-finetuning, PETL has first introduced in natural language processing (NLP)~\cite{adapter-nlp1, adapter-nlp2, diff, lora, prompt-nlp, prefix, bitfit}.
    The main objective of PETL is to attain comparable or surpassing performance on downstream tasks by finetuning with only a small number of trainable parameters.
    Although PETL approaches~\cite{vpt, adaptformer, visual-prompt, bypass, protuning, vl-adapter, adaptformer} have recently been studied in computer vision, they are `blind' to other modalities such that image models are used for image tasks, and so are the other modalities.
    In contrast, we share the same objective as recent works for image-to-video transfer learning~\cite{frozen-clip, x-clip, st-adapter, videoprompt}, demonstrating the pretrained image models can be good video learners.
    However, they have several limitations in terms of parameter and computational efficiency.
    For example, \cite{frozen-clip} learned an extra decoder that contains 3D convolution layers and cross-frame attention, and \cite{st-adapter} inserted additional depth-wise 3D convolution layers between the down-/up-projection layers of the adapter to perform temporal reasoning, inducing computational inefficiency.
    The most recent works~\cite{videoprompt, x-clip} require an additional text encoder branch as a classifier.
    Moreover, they have computational efficiency proportional to the temporal resolution.
    Our \method accomplishes more efficient spatiotemporal modeling while achieving higher performance.
    