\vspace{-5pt}
\section{Introduction}
\label{sec:intro}
\vspace{-5pt}
    Recognizing \textit{when}, \textit{where}, and \textit{what} happened is a fundamental capability in the human cognition system to understand our natural world.
    The research for video understanding inspires such capability for machine intelligence to comprehend scenes over time flow.
    Over the last decade, the development of deep neural networks~\cite{c3d, i3d, s3d, r(2+1)d} has contributed towards advances in video understanding.
    

    Vision Transformer (ViT)~\cite{vit} has recently emerged, making an upheaval in the research field of computer vision.
    ViT and its variants~\cite{swin, cvt, tokens-to-tokens, cswin} have demonstrated remarkable generalizability and transferability of their representations with scaled-up foundation models~\cite{clip, align, florence, beit-pretraining, pretraining1, pretraining2} and large-scale web-collected image data (\eg JFT-3B~\cite{pretraining2}, LAION-5B~\cite{laion-5b}).
    To capitalize on well-trained visual foundation models, finetuning entire parameters of the pretrained models with task-specific objectives has been the most popular transfer technique.
    However, it requires high-quality training data and plenty of computational resources to update the whole parameters for each downstream task, making overwhelming efforts for training.
    While partial finetuning~\cite{moco}, which trains additional multilayer perceptron (MLP) layers to the top of the model, has also been widely used for affordable training costs, unsatisfactory performance has been pointed out as a problem.

    \begin{figure}[t]
	\centering
	\includegraphics[width=0.99 \linewidth]{figures/fig1.pdf}\\ \vspace{-7pt}
	\caption{Performance comparison on the Kinetics-400~\cite{k400} dataset. We depict the action recognition performance (vertical axis, \%) with respect to the number of trainable parameters (horizontal axis). The size of circles indicates GFLOPs for inference.}\vspace{-5pt}
	\label{fig:1}
    \end{figure}
    
    Most recently, parameter-efficient transfer learning (PETL) methods~\cite{adapter-nlp1, adapter-nlp2, prefix, prompt-nlp, diff, lora} have been proposed as an alternative to finetuning in the natural language processing area to adapt the large-scale language model, such as GPT series~\cite{gpt-1, gpt-2, gpt-3} and T5~\cite{t5}, for each task.
    They have successfully attained comparable or even surpassing performance to full-tuning parameters by learning a small number of extra trainable parameters only while keeping the original parameters of the pretrained model frozen.
    Thanks to their effectiveness and simplicity, they have been extended to vision models by applying prompt-based methods~\cite{vpt, visual-prompt} and adapter-based methods~\cite{adaptformer, protuning, vl-adapter}.
    They have efficiently adapted pretrained models to downstream tasks with significantly reduced tuning parameters, but most of these works mainly focus on transferring image models to image tasks~\cite{vpt, visual-prompt, adaptformer, protuning} and vision-language models to vision-language tasks~\cite{vl-adapter}.
    Inspired by the advances of the prior arts, we raise two conceivable questions: \textbf{(1)} Is it possible to transfer the parameters of the image foundation model to another video domain? \textbf{(2)} Is it also possible the transferred model performs comparably to the carefully designed video models that take the spatiotemporal nature of the video into account?

    While image models have demonstrated strong spatial context modeling capabilities~\cite{clip, align, pretraining1, pretraining2}, video transformer models~\cite{vivit,video-swin,tokenlearner,multiview} require a more complex architecture (\eg 539 vs 48912 GFLOPs~\cite{tokenlearner}) with a large number of parameters (\eg 84M vs 876M parameters~\cite{multiview}) than ViT for temporal context reasoning.
    Therefore, the challenge in transferring image models for video understanding is to encode the temporal context of videos while leveraging the discriminative spatial context of the pretrained image models.
    A naive solution is to finetune image models on a video dataset by directly applying previous prompt-/adapter-based approaches~\cite{vpt, visual-prompt, adaptformer, protuning}.
    However, these approaches inevitably ignore the temporal context in videos because they bridge only the spatial contexts between image and video data.

    In this paper, we propose a novel adapter-based dual-path parameter efficient tuning method for video understanding, namely \textbf{\method}, which consists of two distinct paths (\textit{spatial} path and \textit{temporal} path).
    For both paths, we freeze the pretrained image model and train only additional bottleneck adapters for tuning.
    The \textbf{spatial path} is designed to encode the spatial contexts that can be inferred from the appearance of individual frames with the minimum tuning of the pretrained image model.
    To reduce the computation burden, we sparsely use the frames with a low frame rate in the spatial path. 
    The \textbf{temporal path} corresponds to the temporal context that should be encoded by grasping the dynamic relationship over several frames sampled with a high frame rate.
    Especially for two reasons, we construct a grid-like frameset that consists of consecutive low-resolution frames as an input of the temporal path: (i) preventing computational efficiency loss caused by calculating multiple frames simultaneously; (ii) precisely imitating the ViT's ability for extrapolating global dependencies between input tokens.
    To compare our \method with existing methods broadly, we implement several baselines with a unified perspective on recent domain-specific PETL approaches~\cite{adaptformer, vpt, protuning}.
    Extensive experiments on several action recognition benchmarks~\cite{k400, hmdb51, ssv2, diving} demonstrate the effectiveness and high efficiency of our \method, achieving comparable and even better performance than the baselines and prior video models~\cite{multiscale, multiscale-v2, uniformer, space-time, vivit, video-swin, multiview, tokenlearner, actionclip, omnivore}.
    We achieve these results with extremely low computational costs for both training and inference, as demonstrated in \figref{fig:1}.
