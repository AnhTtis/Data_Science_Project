\vspace{-5pt}
\section{Conclusion and Future Work}
    \vspace{-5pt}
    In this paper, we have introduced the novel image-to-video transfer learning approach, \method.
    By incorporating a dual-path design into image transformers, \method adapts image models to the video task (\ie, action recognition) with a small number of trainable parameters.
    The spatial path adaptation strengthens the inherent spatial context modeling of the pretrained image transformers for video data.
    The temporal path adaptation transforms multiple frames into a unified grid-like frameset, enabling the image model to capture relationships between frames.
    We appropriately employ the bottlenecked adapters in each path to adapt the pretrained features to target video data.
    In addition, we present several baselines transforming recent PETL approaches~\cite{vpt, adaptformer, protuning} for image-to-video adaptation.
    Experimental results demonstrated the superiority of the dual-path design and the grid-like frameset prompting, outperforming several baselines and supervised video models.
    
    There are many possible directions for future work, encompassing cross-domain transfer learning.
    While we have explored image-to-video transfer learning, large foundation vision-language models are available.
    It would also be interesting to expand the superior pretrained 2D knowledge to 3D spatial modeling~\cite{p2p}.
    We hope our study will foster research and provide a foundation for cross-domain transfer learning.
    