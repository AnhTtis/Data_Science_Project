\section{Implementation Details}\label{sec:imple_detail}
    We add parallel adapters in the spatial path and serial adapters in the temporal path to every transformer block.
    In our adapter, the dimension of the bottlenecked embedding is 128.
    Following prior work~\cite{adaptformer}, $\mathbf{W}_\text{down}$ is initialized with Kaiming Normal~\cite{kaiming-init} and $\mathbf{W}_{\text{up}}$ with zero initialization.
    For the SSv2~\cite{ssv2} dataset, we additionally insert one adapter before the multi-head attention layer in the temporal path for more robust temporal modeling.
    The experimental configurations according to the datasets are presented in \tabref{tab:imple_detail}.