\section{Additional Results}\label{sec:swin}

\subsection{Results with Swin-B}
\input{Supp/tab_SSv2_Swin}
\input{Supp/tab_efficiency}
\input{Supp/tab_hmdb_resolution}
    Our \method can be applied to other transformer-based pretrained image models.
    We conduct additional experiments with Swin-B~\cite{swin, swin-v2} transformer pretrained on the ImageNet-21K~\cite{imagenet}.
    The Swin-B contains 24 Swin transformer blocks with 88M parameters, requiring fewer GFLOPs than ViT-B/16~\cite{vit}.
    Each block consists of window-based and shifted window-based self-attention layers.
    As in the ViT backbones, we add parallel adapters in the spatial path and serial adapters in the temporal path to every Swin transformer block.
    Note that adapters are attached to only window-based self-attention layers while not adapting shifted window-based self-attention layers.
    For the SSv2 dataset, we use an additional adapter before the multi-head attention layer of the temporal path similar to the ViT backbones.
    The dimension of the bottlenecked embedding is set to 128.
    
    \tabref{tab:add_results} provides the experimental results of \method with Swin-B~\cite{swin, swin-v2} on the SSv2~\cite{ssv2} and HMDB51~\cite{hmdb51} datasets.
    Although the comparisons between ViT-B/16 and Swin-B backbones show the significantly low computation requirement of the Swin-B model (642 vs 287 GFLOPs with \method), we attain a comparable performance to the CLIP pretrained ViT-B/16.
    Compared to ST-Adapter~\cite{st-adapter} with Swin-B, the results consistently demonstrate the effectiveness of \method over the backbone networks, showing a higher performance of 2.7\% with Swin-B on the SSv2 benchmark.

\subsection{Additional efficiency analysis}
    We additionally compare the methods with \cite{frozen-clip, uniformer} in terms of training step time, throughput, and inference latency, following \cite{frozen-clip}.
    For a fair comparison, we obtain all results using V100-32G with PyTorch-builtin mixed precision.
    The throughput is measured with the largest batch size before out-of-memory and the inference latency is measured with a batch size of 1.
    As shown in \tabref{tab:efficiency}, \method takes about half of the training GPU hours and achieves ×2.5 more throughput and ×6.6 faster inference than EVL~\cite{frozen-clip} under the same hardware condition.

    
\subsection{Resolution of grid-like frameset}
    The grid-like frameset comprises a stack of 16 \textit{scaled} frames to make the same size as the original frame ($224\times 224$).
    We investigate the effectiveness of the resolution of the grid-like frameset in this section.
    Note that the impact of scaling factors that determine the temporal resolution is demonstrated in \tabref{tab:ablation} of the main paper.
    
    Specifically, we set the scaling factors $w$ and $h$ to 1, 2, and 4 while maintaining the temporal resolution as 16 such that the resolution of the grid-like frameset is $896\times 896$, $448\times 448$, and $224\times 224$, respectively.
    The backbone (ViT-B/16) is identically used and uniformly sampled 8 frames are used in the spatial path.
    Following \cite{st-adapter}, we sample one clip cropped into three different spatial views on SSv2~\cite{ssv2} (\ie, total of 3 clips) at test time.
    For HMDB51~\cite{hmdb51}, two clips sampled from a video are respectively cropped into three spatial views (\ie, a total of 6 clips).
    Since a high-resolution frameset contains more detailed information about the original frames, the highest performance is obtained with the $896\times 896$ size of the frameset in \tabref{tab:resolution_hmdb51}.
    However, the computational cost quadratically increases as the resolution of the grid-like frameset increases.
    When we use 48 frames (\ie, $T_G=3$) with the $224 \times 224$ size of the frameset, competitive performance is achieved in both datasets.
    It supports the resolution choice of \method in terms of the trade-off between performance and computational cost.