% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage[accsupp]{axessibility}
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{xcolor}
\usepackage{ctable}
\usepackage{colortbl}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{soul}
\usepackage{xspace}\xspace
\usepackage{adjustbox}
\usepackage{array}
\usepackage{epsfig}
% \usepackage{hyperref}
\usepackage{bbding}
\usepackage{dblfloatfix}
\usepackage{transparent}
\usepackage{diagbox}
% \usepackage{algorithm}
\usepackage{listings}
\usepackage{nicefrac} 
\usepackage{times}
\usepackage{url}
\urlstyle{rm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsbsy}
\usepackage{cases}
\usepackage{enumitem}
% \usepackage{caption}
\usepackage{subcaption}
\usepackage[titletoc]{appendix}
\usepackage{grffile}
\usepackage{diagbox}
\usepackage{pifont}
\usepackage{multicol}
\usepackage{subcaption}
\usepackage{comment}
\definecolor{citecolor}{HTML}{0071bc}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\definecolor{Gray}{gray}{0.92}
\definecolor{darkgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{Highlight}{HTML}{39b54a} 
\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}
% \newcommand{\cgaphl}[2]{
% \fontsize{6pt}{1em}\selectfont{\textcolor{Highlight}{(${#1}$\textbf{#2})}}
% }
% \newcommand{\cgaphlgray}[2]{
% \fontsize{6pt}{1em}\selectfont{\textcolor{gray}{(${#1}$\textbf{#2})}}
\newcommand{\cgaphl}[2]{
\fontsize{6pt}{1em}\selectfont{\textcolor{Highlight}{(${#1}$\textbf{#2})}}
}

\newcommand{\cgaphlgray}[2]{
\fontsize{6pt}{1em}\selectfont{\textcolor{gray}{(${#1}$\textbf{#2})}}
}

\newcommand{\xkx}[1]{{\color{blue} #1}}
\newcommand{\yxq}[1]{{\color{purple} #1}}
\newcommand{\gs}[1]{{\color{magenta} #1}}
% \newcommand{\3d}{{$3$ }}

% \definecolor{bviolet}{rgb}{0.54, 0.17, 0.89}
% \PassOptionsToPackage{pdftex,dvipsnames}{xcolor}
% \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
% \newcommandx{\rewrite}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).



% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6678} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\newcommand{\ourMethod}{CAPE}
\newcommand{\highNDS}{$61.0\%$}
\newcommand{\highmAP}{$52.5\%$}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{CAPE: Camera View Position Embedding for Multi-View 3D Object Detection}

%\title{LVTR: Local View Position Embedding for Multi-View 3D Object Detection}

\author{
Kaixin Xiong\thanks{Equal contribution. ~~\textsuperscript{\dag}Corresponding author. This work is done when Kaixin Xiong is an intern at Baidu Inc. } $  ^{,1}$, \ \  
Shi Gong$^{*,2}$, \ \  
Xiaoqing Ye$^{*,2}$, \ \  
Xiao Tan$^{2}$, \ \  
Ji Wan$^{2}$, \\
Errui Ding$^{2}$, \ \
Jingdong Wang$^{\dag, 2}$, \ \
Xiang Bai$^{1}$\\
\textsuperscript{1}Huazhong University of Science and Technology, 
\textsuperscript{2}Baidu Inc.\\
% Institution1 address\\
{\tt\small kaixinxiong@hust.edu.cn,} \ \
{\tt\small \{gongshi, yexiaoqing\}@baidu.com} \ \
{\tt\small wangjingdong@outlook.com} \ \
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}

%  \blfootnote{*: Equal contribution. â€ : Corresponding author. This work is done when Kaixin Xiong is an intern at Baidu Inc.}
%combining version
In this paper, we address the problem of detecting 3D objects from multi-view images.
% firstly we introduce what is the position embedding and its function
% we focus on studying the position embeddings for multi-view camera-based $3$D object detection. 
Current query-based methods rely on global 3D position embeddings (PE) to learn the geometric correspondence between images and 3D space.
We claim that directly interacting 2D image features with global 3D PE 
% \gs{hinders the performance} 
could increase the difficulty of learning view transformation due to the variation of camera extrinsics.
% \xkx{In this paper, we focus on eliminating the inconsistency of view transformation brought by variation of camera extrinsics for multi-view camera-based 3D object detection. } 
Thus we propose a novel method based on \textbf{CA}mera view \textbf{P}osition \textbf{E}mbedding, called CAPE.
We form the 3D position embeddings under the local camera-view coordinate system instead of the global coordinate system, such that 3D position embedding is free of encoding camera extrinsic parameters. 
%However, it is infeasible to directly adopt the local PE since the output queries are in the global coordinate system.
% \xkx{Given that decoder embeddings are originally defined in global space and position embeddings are formed in the local camera system, we compute their attention weights independently in the decoder to avoid mixture, termed as bilateral attention mechanism.}
% To ease the spatial inconsistencies between local 3D position embeddings and  decoder embeddings that are originally defined in global space, we adopt the bilateral attention mechanism in the decoder. Specifically, we separate the local query position embeddings from the decoder embeddings, and compute their attention weights independently.
Furthermore, we extend our CAPE to temporal modeling by exploiting the object queries of previous frames and encoding the ego motion for boosting $3$D object detection. CAPE achieves the state-of-the-art performance (\highNDS{} NDS and \highmAP{} mAP) among all LiDAR-free methods on nuScenes dataset.
Codes and models are available.\footnote{Codes of \href{https://github.com/PaddlePaddle/Paddle3D}{Paddle3D} and \href{https://github.com/kaixinbear/CAPE}{PyTorch Implementation}.}
%We propose a novel method, called CAPE, for 3D object detection from multi-view images. We claim that the views variation in global positional encoding(PE) hinders the performance in current sparse query-based methods. Based on this premise, we form a kind of view-invariant $3$D PE under the local camera coordinate system. 
% Code will be available at \url{https://github.com/kaixinbear/CAPE}.
% Code and models are available\footnote{\href{https://github.com/PaddlePaddle/Paddle3D}{PaddlePaddle Implementation}}
% \footnote{\href{https://github.com/kaixinbear/CAPE}{Pytorch Implementation}}.


% \gs{In this paper, we propose a novel method, called CAPE, for 3D objects detection from multi-view images. We claim that the views variation in global positional encoding(PE) hinders the performance in current sparse query-based methods. Based on this premise, we form a kind of view-invariant $3$D PE under the local camera coordinate system. However, it is infeasible to directly adopt the local PE since the output queries are in the global coordinate system. To ease the spatial inconsistencies between local and output queries, we adopt the bilateral attention mechanism in the decoder. Specifically, we separate the local queries from the output global queries, and calculate their attention weights independently. Furthermore, we exploit the object queries of previous frames through encoding the ego pose embedding for boosting $3$D object detection. CAPE achieves the state-of-the-art performance (59.9\% NDS and 51.5\% mAP) among all LiDAR-free methods on standard nuScenes dataset.}

% we study the positional encoding for multi-view images.
% We form the $3$D positional embeddings
% under the local coordinate system
% instead of the global coordinate system,
% such that $3$D positional embedding 
% is free of encoding local camera extrinsic parameters.
% We adopt the bilateral attention mechanism in the decoder
% and consider two attention weights,
% we address the problem
% of detecting {\rm 3D} objects
% from multi-view images.
% We are interested in the positional encoding for 
% the multi-view detection transformer framework,
% a modification of the transformer encoder-decoder architecture,
% detection transformer (DETR).
% The modifications mainly lie in the encoder:
% multiple encoders are adopted
% to process multiple views,
% and the encoded multi-view embeddings are simply stacked together
% and fed into the decoder for forming the keys and values.
% We follow the recently-developed method, PETR~\cite{},
% and focus on studying the positional embeddings.
% We adopt the disentangled attention mechanism in the decoder
% and separate the positional embedding
% from the content embedding,
% empirically showing the benifit
% for the training.
% We form the $3$D positional embeddings
% under the local coordinate system
% instead of the global coordinate system,
% such that $3$D positional embedding 
% is free of encoding local camera extrinsic parameters.
% In addition,
% we exploit the object queries of previous frames though encoding the ego pose embedding
% for boosting $3$D object detection.

% We follow the recently-developed method, PETR~\cite{},
% and focus on studying the positional embeddings.
% We adopt the bilateral attention mechanism in the decoder
% and consider two attention weights,
% the geometric and content attention weights,
% separately, {\color{red} benefits?}.
% We form the $3$D positional embeddings
% for each view 
% under the local coordinate system
% instead of the global coordinte system,
% such that feature-guided $3$D positional embedding estimation~\cite{}
% is free of local camera extrinsic parameter estimation.
% In addition,
% we exploit the object queries of previous frames though encoding the ego pose embedding
% for boosting $3$D object detection.



% \yxq{Please wait for Dr. Wang}
%    The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
%    Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
%    The abstract is to be in 10-point, single-spaced type.
%    Leave two blank lines after the Abstract, then begin the main text.
%    Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{sections/1_intro}
\input{sections/2_relatedwork}
\input{sections/3_method}
\input{sections/4_experiments}
\input{sections/5_conclusion}



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
% \end{document}

% \input{sections/supplementary}

\end{document}
