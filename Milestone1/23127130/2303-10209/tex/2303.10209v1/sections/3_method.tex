% \section{Preliminaries}
% \noindent\textbf{DETR.}
% Detection transformer (DETR)~\cite{}
% is a transformer encoder-decoder architecture for object detection. 
% The encoder receives an image $\mathbf{I}$ as input
% and outputs the image embedding $\mathbf{X}$,
% \begin{align}
% \mathbf{X} = 
% \operatorname{Encoder}(\mathbf{I}).
% \end{align}
% The decoder receives object queries $\mathbf{O}$, the image embedding $\mathbf{X}$
% and the positional embedding $\mathbf{P}$ as input


% \vspace{2mm}
% \noindent\textbf{\ourMethod{}.}
% We adopt the multi-view DETR framework,
% a multi-view extension of DETR,
% described as follows.
% The multiple image views,
% $\{\mathbf{I}_1, \mathbf{I}_2,
% \dots, \mathbf{I}_N,\}$,
% are processed with the encoders,
% extracting the image embeddings,
% \begin{align}
% \mathbf{X}_n = 
% \operatorname{Encoder}(\mathbf{I}_n).
% \end{align}
% The $N$ image embeddings
% are simply concatenated together,
% $\mathbf{X} = [\mathbf{X}_1~\mathbf{X}_2~\dots~\mathbf{X}_N]$.
% The associated positional embeddings
% are also concatenated together,
% $\mathbf{P} = [\mathbf{P}_1~\mathbf{P}_2~\dots~\mathbf{P}_N]$.
% The decoder is similar to DETR,
% and uses $\mathbf{X}$
% and $\mathbf{P}$
% to form the key and the value of cross-attention.

% One major effort lies in how
% to construct the image positional embeddings
% $\mathbf{P} = [\mathbf{P}_1~\mathbf{P}_2~\dots~\mathbf{P}_N]$.
% PETR~\cite{} constructs the embeddings
% by transforming the $3$D coordinates to the global coordinate system.



\section{Our Approach}
We present a camera-view position embedding (CAPE) approach for multi-view 3D detection 
and construct the position embeddings
in each camera coordinate system.

\vspace{2mm}
\noindent\textbf{Architecture.}
We adopt the multi-view DETR framework,
a multi-view extension of DETR,
depicted in Figure~\ref{fig:overview}.
The input multi-view images,
$\{\mathbf{I}_1, \mathbf{I}_2,
\dots, \mathbf{I}_N\}$, 
are processed with the encoders to extract the image embeddings,
\begin{align}
\mathbf{X}_n = 
\operatorname{Encoder}(\mathbf{I}_n).
\end{align}
The $N$ image embeddings
are simply concatenated together,
$\mathbf{X}=[\mathbf{X}_1~\mathbf{X}_2~\dots~\mathbf{X}_N]$.
The associated position embeddings
are also concatenated together,
$\mathbf{P} = [\mathbf{P}_1~\mathbf{P}_2~\dots~\mathbf{P}_N]$. $\mathbf{X}_n, \mathbf{P}_n\in \mathbb{R}^{C\times I}$, where $I$ is the pixels number.

The decoder is similar to DETR decoder, with a stack of $L$ decoder layers
that is composed of
self-attention, cross-attention , and feed-forward network (FFN).
The $l$-th decoder layer is formulated as 
follows,
\begin{align}
\mathbf{O}_{l} = 
\operatorname{DecoderLayer}(\mathbf{O}_{l-1},
\mathbf{R},
\mathbf{X},
\mathbf{P}).
\end{align}
Here,
$\mathbf{O}_{l-1}$ and $\mathbf{O}_{l}$
are the output decoder embedding of the $(l-1)$th and the $l$th layer, respectively.
$\mathbf{R}$ are $3$D reference points following the design in \cite{zhu2020deformable,liu2022petr}.

Self-attention is the same as the normal DETR,
and 
takes the sum of $\mathbf{O}_{l-1}$
and the position embedding of the reference points $\mathbf{P}$ as input.
Our work lies in learning 3D position embeddings.
In contrast to PETR~\cite{liu2022petr} and PETRv2~\cite{liu2022petrv2}
that form the position embedding
in the global coordinate system,
we focus on learning
position embeddings
in the camera coordinate system
for cross-attention.

% The decoder consists of repeated blocks
% of self-attention, cross-attention,
% and FFN.
% Self-attention is conducted on 
% object queries $\mathbf{O}$,
% and cross-attention is conducted
% with the query $\mathbf{Q} = \mathbf{O}$,
% the key $\mathbf{K} =\mathbf{X} + \mathbf{P}$,
% the value
% $\mathbf{V}=\mathbf{X}$.


% One major effort lies in how
% to construct the image positional embeddings
% $\mathbf{P} = [\mathbf{P}_1~\mathbf{P}_2~\dots~\mathbf{P}_N]$.
% PETR~\cite{} constructs the embeddings
% by transforming the $3$D coordinates to the global coordinate system.
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/method_v2.pdf}
\caption{The cross-attention module of \ourMethod{}. (I): Feature-guided Key Position Embedding (K-FPE); (II): Feature-guided Query Position Encoder (Q-FPE). The multi-head setting and fully connected layers are omitted for the sake of simplicity.}
\centering
\label{fig:transformer}
\vspace{-15pt}
\end{figure}


\vspace{2mm}
\noindent\textbf{Key Position Embedding Construction.}
We take one camera view (one image)
as an example
and describe how to construct the position embeddings
for one view.
Each $2$D position in the image plane
corresponds to $D$ $3$D coordinates along the predefined depth bins in the frustum:
% \yxq{add predefined depth bins@gongshi}
$\{\mathbf{c}_1, \mathbf{c}_2,
\dots, \mathbf{c}_D\}$.
The $3$D coordinate in the image frustum is transformed 
to the camera coordinate system,
\begin{align}\mathbf{c}'_d = \mathbf{T}^{-1}_i\mathbf{c}_d,
\end{align}
where % ${\mathbf{c}}$ is the augmented coordinate  \yxq{how to understand the augmented coords.} and 
$\mathbf{T}_i$ is the intrinsic matrix for $i$-th camera.
The $D$ transformed $3$D coordinates 
$\{\mathbf{c}'_1, \mathbf{c}'_2,
\dots, \mathbf{c}'_D\}$
are mapped into the single embedding,
%  Yes, it is single
% \yxq{single ? or sinusoidal positional embedding?@gongshi},
\begin{align}
    \mathbf{p}
    = \phi(\mathbf{c}'). 
    \label{eqn:KeyPE}
\end{align}
Here, $\mathbf{c}'$ is a $(D\times 3)$-dimensional vector, with 
% $\mathbf{c}' = 
% [\mathbf{c}'_1^\top~\mathbf{c}'_2^\top~\dots~\mathbf{c}'_D^\top]$.
$\mathbf{c}' = 
[{\mathbf{c}'}_1^\top~{\mathbf{c}'}_2^\top~\dots~{\mathbf{c}'}_D^\top]$.
$\phi$ is instantiated by a multi-layer perceptron (MLP) of two layers.


\vspace{2mm}
\noindent\textbf{Query Position Embedding Construction.}
We use a set of learnable $3$D reference points 
$\mathbf{R} = \{\mathbf{r}_1, \mathbf{r}_2,
\dots, \mathbf{r}_M\}$ in the global space 
to form the object queries. We transform the $M$ $3$D points
into the camera coordinate system for each view,
\begin{align}
    \bar{\mathbf{r}}_{nm}
    = \mathbf{T}_{n}^{e}\mathbf{r}_m,
\end{align}
where $\mathbf{T}_{n}^{e}$ is the extrinsic parameter matrix
for the $n$-th camera denoting the coordinate transformation from the global (LiDAR) system to the camera-view system.
% \yxq{denoting the transformation from xxx to xxx @gongshi }.
The transformed $3$D coordinates
are then mapped into 
the query position embedding,
\begin{align}
    \mathbf{g}_{nm} = \psi(\bar{\mathbf{r}}_{nm}),
    \label{eqn:QPE}
\end{align}
where $\psi(\cdot)$ is a two-layer MLP.

Considering that
the decoder embedding $\mathbf{O}\in \mathbb{R}^{C\times M}$
and query position embeddings
are about different coordinate systems:
global coordinate system
and camera view coordinate system,
respectively,
we form the camera-dependent decoder queries
through concatenation(denote as $[\cdot]$):
\begin{align}
    \mathbf{Q}_n =[\mathbf{O}^\top~\mathbf{G}_n^\top]^\top,
\end{align}
where $\mathbf{G}_n = [\mathbf{g}_{n1} \mathbf{g}_{n2}\dots \mathbf{g}_{nM}]\in \mathbb{R}^{C\times M}$.
Accordingly,
the keys for cross-attention between queries and image features
are also formed through concatenation, 
$\mathbf{K}_n = [\mathbf{X}_{n}^\top~\mathbf{P}_{n}^\top]^\top$.
% Given the query $\mathbf{Q}_n$ and key $\mathbf{K}_{n}$, 
The $n-$view pre-normalized cross-attention weights $\mathbf{W}_n \in \mathbb{R}^{I\times M}$
is computed from:
\begin{align}
    \mathbf{W}_n = \mathbf{K}_n^\top \mathbf{Q}_n = \mathbf{X}_n^\top\mathbf{O} + \mathbf{P}_{n}^\top \mathbf{G}_{n}
\end{align}
The decoder embedding is updated by aggregating information from all views:
\begin{align}
\mathbf{O} \xleftarrow{}{} \sum_n \mathbf{X}_n\sigma(\mathbf{W}_n),
\end{align}
where $\sigma$ is the soft-max, note that projection layers are omitted for simplicity. More details can be seen in the appendix.
% \begin{align}
% \label{eq:overall}
% \{\mathbf{G}_{1}^\top\mathbf{P}_{1}
% + \mathbf{X}_1^\top\mathbf{O}, 
% % \mathbf{G}_{2}^\top\mathbf{P}_{2} 
% % + \mathbf{X}_2^\top\mathbf{O}, 
% \dots,
% \mathbf{G}_{N}^\top\mathbf{P}_{N}
% + \mathbf{X}_N^\top\mathbf{O}\}.
% \end{align}
% Then the attention weights of all N views are scaled and together normalized by the softmax.
% \vspace{2mm}
\noindent\textbf{Feature-guided Key and Query Position Embeddings.}
Similar to PETRv2~\cite{liu2022petrv2},
we make use of the image features to guide the key position embedding computation by learning the scaling weights
and update Eq.\ref{eqn:KeyPE} as:
\begin{align}
    \mathbf{p}
    = \phi(\mathbf{c}')\odot\xi(\mathbf{x}),
    \label{eqn:FGPC}
\end{align}
where $\xi(\cdot)$ is a two-layers MLP, $\odot$ denotes the element-wise multiplication and $\mathbf{x}$
is the image features at the corresponding position.
It is assumed to
provide some informative guidance
(e.g., depth). 

On the query side, inspired by conditional DETR~\cite{meng2021conditional}, we use the decoder embedding
to guide the query position embedding computation 
and update Eq.\ref{eqn:QPE} as:
\begin{align}
\mathbf{g}_{nm} = \psi(\bar{\mathbf{r}}_{nm})\odot \eta(\mathbf{o}_m, \mathbf{T}_{n}^{e}).
\end{align}
%  Explain the symbols
The extrinsic parameter matrix $\mathbf{T}_{n}^{e}$
is used to transform the spatial decoder embedding $\mathbf{o}_m$
to camera coordinate system,
for alignment with the reference point position embeddings.
Specifically, $\eta(\cdot)$ is instantiated as:
\begin{align}
\label{eq:cam_extrin_modulate}
\eta(\mathbf{o}_m, \mathbf{T}_{n}^{e}) = 
\eta_l(\mathbf{o}_m \odot \eta_g(\mathbf{T}_{n}^{e})).
\end{align}
Here $\eta_l(\cdot)$ and $\eta_g(\cdot)$ are two-layers MLP.



% \vspace{2mm}
% \noindent\textbf{PETR.}
% Positional embedding transformer (PETR)~\cite{} is based on the multi-view extension of DETR (\ourMethod{}).
% \xkx{
% PETR extends DETR-paradigm and proposes $3$D global positional embedding for multi-view $3$D detection task.
% }


% The main work of PETR
% includes using the $3$D object queries
% and 
% constructing the $3$D positional embedding $\mathbf{P}_n$
% for each view $\mathbf{I}_n$.
% $3$D positional embedding is constructed as follows.

% Each $2$D position in the image space
% corresponds 
% to $D$ $3$D coordinates 
% in the frustum:
% $\{\mathbf{c}_1, \mathbf{c}_2,
% \dots, \mathbf{c}_D\}$.
% The $3$D coordinate is transformed 
% to the global coordinate system,
% \begin{align}\bar{\mathbf{c}}'_d = \mathbf{T}^{-1}_n\bar{\mathbf{c}}_d,
% \end{align}
% where $\bar{\mathbf{c}}$ is the augmented coordinate
% and $\mathbf{T}_n$ is the extrinsic parameter matrix.
% The $D$ transformed $3$D coordinates 
% $\{\mathbf{c}'_1, \mathbf{c}'_2,
% \dots, \mathbf{c}'_D\}$
% is mapped into a single embedding,
% \begin{align}
%     \mathbf{p}
%     = \psi(\mathbf{c}'),
% \end{align}
% where $\mathbf{c}'$ is a $(D\times 3)$-dimensional vector 
% $\mathbf{c}' = 
% [\mathbf{c}'_1^\top~\mathbf{c}'_2^\top\dots\mathbf{c}'_D^\top]^$

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/temporal_fusion.pdf}
\caption{The pipeline of our  temporal modeling. The ego-motion embedding modulates the decoder embedding for spatial alignment. Then each decoder embedding is scaled by the attention weights $(w1, w2)$ that predicted from the aligned embeddings.
% \yxq{Add more explanation @gongshi}
}
\centering
\label{fig:temporal}
% \vspace{-10pt}
\end{figure}

% \vspace{2mm}
\noindent\textbf{Temporal modeling
with ego-motion embedding.}
We utilize the previous frame information
to boost the detection
for the current frame.
The reference points of the previous frame
are transformed 
from the reference points
of the current frame
using the ego-motion matrix $\mathbf{M}$,
\begin{align}
    \mathbf{R}_{t'}
    = \mathbf{M}\mathbf{R}_t.
\end{align}
Considering the same moving objects may have different 3D positions in two frames, we build the separated decoder embeddings $(\mathbf{O}_{t}, \mathbf{O}_{t'})$ that can represent different position information for each frame. The interaction between the decoder embeddings of two frames for $l$-th decoder layer is formulated as follows:
\begin{align}
    (\bar{\mathbf{O}}_{t}^L,
    \bar{\mathbf{O}}_{t'}^L)
    = f(\mathbf{O}_{t}^{L}, \mathbf{O}_{t'}^{L}, \mathbf{M}).
\end{align}

%  too many equations, so here use text description
% \gs{As depicted in the Figure \ref{fig:temporal}, since $\mathbf{O}_{t}$ and $\mathbf{O}_{t'}$ are not in the same ego coordinate system, we inject the ego-motion information into the decoder embedding $\mathbf{O}_{t'}$ for spatial alignment. 
% We exploit the temporal information by learning the scaling weights $(w_1, w_2)$ from the concatenation of the decoder embeddings of two frames. Although two-frame fusion is shown, our method can be extended to any number of frames.
% We show the superiority of the proposed temporal fusion in Section xx.}

We elaborate on the interaction between queries in two frames in Figure~\ref{fig:temporal}. Given that $\mathbf{O}_{t}$ and $\mathbf{O}_{t'}$ are not in the same ego coordinate system, we inject the ego-motion information into the decoder embedding $\mathbf{O}_{t'}$ for spatial alignment. Then we update decoder embeddings with channel attention weights generated from the concatenation of the decoder embeddings of two frames. Compared with using one set of queries learning objects in different frames, queries in our method have a stronger capability in positional learning.

% \vspace{2mm}
\noindent\textbf{Heads and Losses.} The detection heads consist of the classification branch that predicts the probability of object classes and the regression branch that regresses 3D bounding boxes. The regression branch predicts the relative offsets \textit{w.r.t.} the coordinates of 3D reference points in the global system. As for the loss function, we adopt focal loss~\cite{lin2017focal} for classification $L_{cls}$ and L1 loss for regression $L_{reg}$ following prior works~\cite{wang2022detr3d,liu2022petr}. The label assignment strategy here is the Hungarian algorithm~\cite{kuhn1955hungarian}. Suppose that $\sigma$ is the assignment function, the loss for 3D object detection for the model without temporal modeling can be summarized as:
\begin{equation}
\begin{gathered}
L_{cur}(\mathbf{y}, \mathbf{\hat{y}}) = \lambda_{cls} L_{cls}(\mathbf{c}, \sigma{(\mathbf{\hat{c})}}) + L_{reg}(\mathbf{b}, \sigma{(\mathbf{\hat{b}})}),
\end{gathered}
\end{equation}
where $\mathbf{y}=(\mathbf{c}, \mathbf{b})$ and $\hat{\mathbf{y}}=(\mathbf{\hat{c}}, \mathbf{\hat{b}})$ denote the set of ground truths and predictions respectively. $\lambda_{cls}$ is a hyper-parameter to balance losses.
As for the network with temporal modeling, different from other methods supervise predictions only on the current frame, we predict results and supervise them on previous frames as auxiliary losses to enhance the temporal consistency. We use the center location and velocity of the ground truth on the current frame to generate the ground truths on the previous frame.
\begin{equation}
\begin{gathered}
L_{all} = L_{cur} + \lambda L_{prev}.
\end{gathered}
\end{equation}
$L_{cur}$ and $L_{prev}$ denote the losses for the current and previous frame separately. $\lambda$ is a hyper-parameter to balance losses.
