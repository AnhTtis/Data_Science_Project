\section{Experiments}
\subsection{Dataset}
We evaluate \ourMethod{} on the large-scale  nuScenes~\cite{caesar2020nuscenes} dataset. This dataset is composed of 1000 scene videos, with 700/150/150 scenes for training, validation, and testing set, respectively. Each sample consists of RGB images from 6 cameras and has 360 ° horizontal FOV. There are 20s video frames for each scene and 3D annotations are provided with every 0.5s. We report nuScenes Detection Score (NDS), mean Average Precision (mAP), and five True Positive (TP) metrics: mean Average Translation Error (mATE), mean Average Scale Error (mASE), mean Average Orientation Error (mAOE), mean Average Velocity Error (mAVE), mean Average Attribute Error (mAAE).

\subsection{Implementation Details}
We follow the PETR~\cite{liu2022petr} to report the results. We stack six transformer layers and adopt eight heads in the multi-head attention. 
Following other methods\cite{liu2022petr,wang2022detr3d, huang2021bevdet}, \ourMethod{} is trained with the pre-trained model FCOS3D~\cite{wang2021fcos3d} on validation dataset and with DD3D~\cite{park2021pseudo} pre-trained model on the test dataset as initialization. We use regular cropping, resizing, and flipping as data augmentations. The total batch size is eight, with one sample per GPU. 
We set $\lambda$ = 0.1 to balance the loss weight between the current frame and the previous frame and set $\lambda_{cls}$ = 2.0 to balance the loss weight between classification and regression. 
For validation dataset setting, we train \ourMethod{} for 24 epochs on 8 A100 GPUs with a starting learning rate of 2$e^{-4}$ that decayed with cosine annealing policy. 
% \xkx{
For test dataset setting, we adopt denoise~\cite{zhang2022dino} for faster convergence. We train 24 epochs with CBGS on the single-frame setting. Then we load the single-frame model of CAPE as the pre-trained model for multi-frame training of CAPE-T and train 60 epochs without CBGS. 
% }
% We load the single-frame model of CAPE as the pre-trained model for multi-frame training of CAPE-T on the test dataset for faster convergence.

\begin{table*}[t]
\small
\setlength{\tabcolsep}{3mm}
\centering
\caption{Comparison of recent works on the nuScenes \emph{test} set. $\ddagger$ is test time augmentation.
% ``Extra Data'' means using available data sources.
Setting ``S'':  only using single-frame information, ``M'': using multi-frame (two) information. 
% for training and testing. %  means using  information for training and testing phases.
``L'': using extra LiDAR data source as depth supervision.}
\vspace{-10pt}
\label{tab:main_result}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l | c | c | c| c  c | c c c c c} 
 \hline
 Methods & Year & Backbone & Setting &  NDS$\uparrow$&mAP$\uparrow$&mATE$\downarrow$&mASE$\downarrow$&mAOE$\downarrow$&mAVE$\downarrow$& mAAE$\downarrow$ \\ [0.5ex] 
 \hline
BEVDepth$^\ddagger$~\cite{li2022bevdepth} & arXiv2022 & V2-99 & M, L & 0.600 & 0.503 & 0.445 & 0.245 & 0.378 & 0.320 & 0.126 \\
BEVStereo$^\ddagger$~\cite{li2022bevstereo} & arXiv2022 & V2-99 & M, L & 0.610 & 0.525 & 0.431 & 0.246 & 0.358 & 0.357 & 0.138 \\
 \hline
FCOS3D$^\ddagger$~\cite{wang2021fcos3d}& ICCV2021 &  Res-101 & S & 0.428 & 0.358 & 0.690 & 0.249 & 0.452 & 1.434 & 0.124  \\ 
PGD$^\ddagger$~\cite{wang2022probabilistic} & CoRL2022 & Res-101 & S & 0.448 & 0.386 & 0.626 & 0.245 & 0.451 & 1.509 & 0.127  \\ 
DETR3D~\cite{wang2022detr3d} & CoRL2022 & Res-101 & S & 0.479 & 0.412 & 0.641 & 0.255 & 0.394 & 0.845 & 0.133  \\ 
BEVDet$^\ddagger$~\cite{huang2021bevdet} & arXiv2022 & V2-99   & S & 0.488 & 0.424 & 0.524 & 0.242 & 0.373 & 0.950 & 0.148  \\ 
PETR~\cite{liu2022petr} & ECCV2022 & V2-99 & S & 0.504 & 0.441 & 0.593 & 0.249 & 0.383 & 0.808 & 0.132  \\ 
\cellcolor{Gray}\ourMethod{} & \cellcolor{Gray} - & \cellcolor{Gray}V2-99  & \cellcolor{Gray}S & 
\cellcolor{Gray}\textbf{0.520} & 
\cellcolor{Gray}\textbf{0.458} & 
\cellcolor{Gray}0.561 & 
\cellcolor{Gray}0.252 &
\cellcolor{Gray}0.389 & 
\cellcolor{Gray}0.758 &
\cellcolor{Gray}0.132  \\ 
 \hline
BEVFormer~\cite{li2022bevformer}& ECCV2022 & V2-99 & M & 0.569 & 0.481 & 0.582 & 0.256 & 0.375 & 0.378 & 0.126  \\
BEVDet4D$^\ddagger$~\cite{2022BEVDet4D} & arXiv2022 & Swin-B & M & 0.569 & 0.451 & 0.511 & 0.241 & 0.386 & 0.301 & 0.121  \\
PETRv2~\cite{liu2022petrv2} & arXiv2022 & V2-99  & M & 0.582 & 0.490 & 0.561 & 0.243 & 0.361 & 0.343 & 0.120  \\
\cellcolor{Gray}\ourMethod{}-T & \cellcolor{Gray} - & \cellcolor{Gray}V2-99 & \cellcolor{Gray}M & 
\cellcolor{Gray}\textbf{0.610} & 
\cellcolor{Gray}\textbf{0.525} & 
\cellcolor{Gray}0.503 & 
\cellcolor{Gray}0.242 & 
\cellcolor{Gray}0.361 & 
\cellcolor{Gray}0.306 & 
\cellcolor{Gray}0.114 \\ 
 \hline
\end{tabular}
}
\vspace{-0.1cm}
\end{table*}

\begin{table*}[t]
\small
\setlength{\tabcolsep}{3mm}
\centering
% \caption{Comparison of recent works on the nuScenes \emph{validation} set. The results of FCOS3D and PGD are fine-tuned and tested with test time augmentation. The DETR3D, BEVDet, and PETR are trained with CBGS. † is initialized from an FCOS3D backbone.}
\caption{Comparison on the nuScenes \emph{validation} set with large backbones. All listed methods are trained with 24 epochs without CBGS. }
\vspace{-10pt}
\label{tab:main_result_val}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l | c | c | c | c c | c c c c c} 
 \hline
 Methods & Backbone &  Resolution & Setting & NDS$\uparrow$&mAP$\uparrow$&mATE$\downarrow$&mASE$\downarrow$&mAOE$\downarrow$&mAVE$\downarrow$& mAAE$\downarrow$ \\ [0.5ex] 
 \hline
% FCOS3D~\cite{wang2021fcos3d} & Res-101 & $1600 \times 900$ & S & 0.415 & 0.343 & 0.725 & 0.263 & 0.422 & 1.292 & 0.153  \\ 
% PGD~\cite{wang2022probabilistic}    & Res-101 & $1600 \times 900$ & S & 0.428 & 0.369 & 0.683 & 0.260 & 0.439 & 1.268 & 0.185  \\ 
% DETR3D†~\cite{wang2022detr3d}& Res-101 & $1600 \times 900$ & S & 0.434 & 0.349 & 0.716 & 0.268 & 0.379 & 0.842 & 0.200  \\ 
% PETR†~\cite{liu2022petr}  & Res-101 & $1600 \times 900$ & S & 0.442 & 0.370 & 0.711 & 0.267 & 0.383 & 0.865 & 0.201  \\
BEVDet~\cite{huang2021bevdet} & Swin-B & $1408 \times 512$  & S & 0.417 & 0.349 & 0.637 & 0.269 & 0.490 & 0.914 & 0.268  \\
% BEVFormer-S &Res-101& $1600\times900$& 0.448 & 0.375 & 0.725 & 0.272 & 0.391 & 0.802 & 0.200  \\
PETR~\cite{liu2022petr}   & V2-99   & $1600 \times 900$ & S & 0.455 & 0.406 & 0.736 & 0.271 & 0.432 & 0.825 & 0.204  \\
\cellcolor{Gray}\ourMethod{}& \cellcolor{Gray}V2-99 & \cellcolor{Gray}$1600 \times 900$ & 
\cellcolor{Gray}S &
\cellcolor{Gray}\textbf{0.479} &
\cellcolor{Gray}\textbf{0.439} & 
\cellcolor{Gray}0.683 & 
\cellcolor{Gray}0.267 & 
\cellcolor{Gray}0.427 & 
\cellcolor{Gray}0.814 & 
\cellcolor{Gray}0.197  \\
 \hline
% BEVformer~\cite{li2022bevformer} & Res-101 & $1600 \times 900$ & M & 0.517& 0.416& 0.673 & 0.274 & 0.372 & 0.394 & 0.190 \\
BEVDet4D~\cite{2022BEVDet4D} & Swin-B & $1600 \times 900$ & M & 0.515 & 0.396& 0.619 & 0.260 & 0.361 & 0.399 & 0.189 \\
PETRv2~\cite{liu2022petrv2} & V2-99 & $800 \times 320$    & M & 0.503 & 0.410 & 0.723 & 0.269 & 0.453 & 0.389 & 0.193 \\
\cellcolor{Gray}\ourMethod{}-T& \cellcolor{Gray}V2-99 & \cellcolor{Gray}$800 \times 320$ & 
\cellcolor{Gray}M &
\cellcolor{Gray}\textbf{0.536} & 
\cellcolor{Gray}\textbf{0.440} & 
\cellcolor{Gray}0.675 &
\cellcolor{Gray}0.267 &
\cellcolor{Gray}0.396 &
\cellcolor{Gray}0.323 &
\cellcolor{Gray}0.185 \\
 
% BEVDet* & V2-99   & 1600 $\times$ 900 & 0.488 & 0.424 & 0.524 & 0.242 & 0.373 & 0.950 & 0.148  \\ 
% PETR*   & V2-99   & 1600 $\times$ 900 & 0.504 & 0.441 & 0.593 & 0.249 & 0.383 & 0.808 & 0.132  \\ 
%  \hline
% BEVFormer* & V2-99 & 1600 $\times$ 900& 0.569 & 0.481 & 0.582 & 0.256 & 0.375 & 0.378 & 0.126  \\
% BEVDet4D & Swin-B  & 1600 $\times$ 900& 0.569 & 0.451 & 0.511 & 0.241 & 0.386 & 0.301 & 0.121  \\
% PETRv2* & V2-99   & 1600 $\times$ 900 & 0.582 & 0.490 & 0.561 & 0.243 & 0.361 & 0.343 & 0.120  \\
 \hline
\end{tabular}
}
\vspace{-0.2cm}
\end{table*}


\begin{table}[!t]
\small
\setlength{\tabcolsep}{3mm}
\caption{Comparison on the nuScenes \emph{validation} set with ResNet backbone. “†”: the results are reproduced for a fair comparison with our method(trained with 24 epochs). 
}
\vspace{-10pt}
\label{tab:res50_101}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c c c| c c} 
 \hline
 Method & Backbone & Resolution & CBGS &  NDS$\uparrow$&mAP$\uparrow$ \\ [0.5ex] 
 \hline
PETR†~\cite{liu2022petr} & Res-50 &  $1408 \times 512$ & \XSolidBrush & 0.367 & 0.317 \\
\cellcolor{Gray}CAPE & \cellcolor{Gray}Res-50 & \cellcolor{Gray}$1408 \times 512$ & \cellcolor{Gray}\XSolidBrush & \cellcolor{Gray}\textbf{0.380} & \cellcolor{Gray}\textbf{0.337} \\
\hline
FCOS3D~\cite{wang2021fcos3d} & Res-101 & $1600 \times 900$ & \Checkmark & 0.415 & 0.343 \\ 
PGD~\cite{wang2022probabilistic} & Res-101 & $1600 \times 900$ & \Checkmark  & 0.428 & 0.369 \\ 
DETR3D~\cite{wang2022detr3d}& Res-101 & $1600 \times 900$ & \Checkmark & 0.434 & 0.349 \\ 
BEVDet~\cite{huang2021bevdet} & Res-101 & $1056 \times 384$ & \Checkmark & 0.396 & 0.330 \\
PETR~\cite{liu2022petr} & Res-101 & $1600 \times 900 $ & \Checkmark & 0.442 & 0.370\\
% BEVFormer-S~\cite{li2022bevformer} &Res-101& $1600\times900$& S & 0.448 & 0.375 & 0.725 & 0.272 & 0.391 & 0.802 & 0.200  \\
% \cellcolor{Gray}CAPE & \cellcolor{Gray}Res-101 & \cellcolor{Gray}$1600 \times 900 $ &  \cellcolor{Gray}S & \cellcolor{Gray}\textbf{0.447} & \cellcolor{Gray}\textbf{0.385} & \cellcolor{Gray}0.667 & \cellcolor{Gray}0.278 & \cellcolor{Gray}0.445 & \cellcolor{Gray}0.855 & \cellcolor{Gray}0.204  \\ %
% reprocuding add dn
\cellcolor{Gray}CAPE & \cellcolor{Gray}Res-101 & \cellcolor{Gray}$1600 \times 900 $ &  \cellcolor{Gray}\Checkmark & \cellcolor{Gray}\textbf{0.463} & \cellcolor{Gray}\textbf{0.388} \\ % fcos3d pretrain
 \hline
% BEVDepth & Res50 & $704 \times 256$ & 0.436 & 0.330 & 0.702 & 0.280 & 0.535 & 0.553 & 0.227 \\
BEVFormer~\cite{li2022bevformer} & Res-50 & $800 \times 450$& \XSolidBrush & 0.354 & 0.252\\ % from official github
PETRv2†~\cite{liu2022petrv2} & Res-50 &  $704 \times 256$ & \XSolidBrush & 0.402 & 0.293\\ % reproduced
% BEVFormer~\cite{li2022bevformer} & Res-50 & $800 \times 450$& \XSolidBrush & 0.354 & 0.252\\ % from official github
\cellcolor{Gray}CAPE-T & \cellcolor{Gray}Res-50 &  \cellcolor{Gray}$704 \times 256$ &\cellcolor{Gray}\XSolidBrush & \cellcolor{Gray}\textbf{0.442} & \cellcolor{Gray}\textbf{0.318} \\ % reproducing 
\hline
BEVFormer~\cite{li2022bevformer} & Res-101 & $1600 \times 640$& \Checkmark & 0.517 & 0.416\\ % paper
PETRv2~\cite{liu2022petrv2} & Res-101 & $1600 \times 640$& \Checkmark & 0.524 & 0.421 \\ % told from author
\cellcolor{Gray}CAPE-T & \cellcolor{Gray}Res-101 &  \cellcolor{Gray}$1600 \times 640$ & \cellcolor{Gray}\Checkmark & \cellcolor{Gray}\textbf{0.533} & \cellcolor{Gray}\textbf{0.431}\\ % reproducing 
 \hline
\end{tabular}
}
\vspace{-0.2cm}
\end{table}


\subsection{Comparison with State-of-the-art}
We show the performance comparison in the nuScenes test set in Tab.~\ref{tab:main_result}. We first compare the \ourMethod{} with state-of-the-art methods on the single-frame setting and then compare \ourMethod{}-T(the temporal version of \ourMethod{}) with methods that leverage temporal information. 
% As for the model on the single-frame setting, it shows that our \ourMethod{} with VOVNetV2 backbone surpasses the second place method PETR~\cite{liu2022petr} on NDS by $1.6\%$ for a fair comparison. 
% It could be seen that \ourMethod{} surpasses DETR3D~\cite{wang2022detr3d} on NDS by $4.1\%$ and surpasses BEVDet~\cite{huang2021bevdet} on NDS by $3.4\%$. 
% When compared with FCOS3D~\cite{wang2021fcos3d} with Res101 backbone, \ourMethod{} surpasses it by $10.0\%$ on mAP.
As for the model on the single-frame setting, as far as we know, \ourMethod{}(NDS=52.0) could achieve \textbf{the first place} on nuScenes benchmark compared with vision-based methods with the single-frame setting.
As for the model with temporal modeling, \ourMethod{}-T still outperforms all listed methods. \ourMethod{} achieves \highNDS{} on NDS and \highmAP{} on mAP. We point out that using LiDAR as supervision could largely improve the mATE metric, thus it's not fair to compare methods (w/wo LiDAR supervision) together. Nevertheless, even compared with contemporary methods leveraging LiDAR as supervision, \ourMethod{} outperforms BEVDepth~\cite{li2022bevdepth} $1.0\%$ on NDS and $2.2\%$ on mAP and achieves comparable results to BEVStereo~\cite{li2022bevstereo}. 
% \ourMethod{} shows superiority in velocity estimation (mAVE) owing to leveraging ego-motion explicitly in sparse-query paradigm.

We further show the performance comparison on the nuScenes validation set in Tab.~\ref{tab:main_result_val} and Tab.~\ref{tab:res50_101}. It could be seen that \ourMethod{} surpasses our baseline to a large margin and performs well compared with other methods. 
% more backbone exp should be displayed.

\subsection{Ablation Studies} In this section, we validate the effectiveness of our designed components in \ourMethod{}.
% We keep all the experiments the same settings except the input resolution.
 We use the $1600 \times 900$ resolution for all single-frame experiments and the $800 \times 320$ resolution for all multi-frames experiments.

% \begin{table}[t]
% \small
% \setlength{\tabcolsep}{2.5mm}
% \centering
% \caption{Ablation study on the nuScenes validation set. We display NDS, mAP, mATE and mAOE as the evaluation metric. "da." means decoupling appearance and geometric information in cross attention, which is inspired from ~\cite{meng2021conditional}. "lp." means the position embedding is in the local system. The default position embedding is in the global system.}
% \label{tab:ablation_view-invariant_decouple}
% \begin{tabular}{l | c c c c} 
%  \hline
%  Setting & NDS$\uparrow$&mAP$\uparrow$&mATE$\downarrow$&mAOE$\downarrow$ \\ [0.5ex] 
%  \hline
%  Baseline & 0.455 & 0.406 & 0.736 & 0.432  \\
%  Baseline+da. & 0.462 & 0.415 & 0.702 & 0.441 \\
% %  Baseline+da.+dw. & 0.465 & 0.415 & 0.708 & 0.420\\
% %  Baseline+da.+dw.+lp. & 0.479 & 0.439 & 0.683 & 0.427 \\
%  Baseline+da.+lp. & 0.479 & 0.439 & 0.683 & 0.427 \\
%  \hline
% \end{tabular}
% \end{table}

% \textbf{Effectiveness of view invariant design.} Our view invariant design contains two core parts: one is decoupling appearances and geometry in attention mechanism, the other is decoupling camera intrinsic and extrinsic parameters in position embedding. We validate the effectiveness of each part based on non-temporal setting. We simply adopt PETR as our baseline without any modification. Inspired by~\cite{meng2021conditional}, we generate the affinity map in attention from appearance and geometric affinity map for eliminating the meaningless items in cross attention. In addition, we use learnable weights to balance the weight factor of these two items. For position embedding, we only use the camera intrinsic parameters to generate key position embedding. And we use camera extrinsic parameters to convert the 3D reference points to each view and generate local query position embedding. With these two steps, our network could perform attention calculation in the local view while preserving global information.

% As shown in Tab~\ref{tab:ablation_view-invariant_decouple}, when we add decoupled attention to the baseline directly, it would improve $0.7\%$. on NDS while $0.9\%$ performance drop in mAOE. When we convert global position embedding to local position embedding, it could improve by $1.7\%$ and $2.4\%$ on NDS and mAP further. This indicates that the view-invariant design makes the network learn the 2D-3D transformation easier and thus locate objects more precisely. It can be seen that our proposed view invariant design could improve $2.4\%$ and $3.3\%$ on NDS and mAP. This shows the effectiveness of our view invariant design.
% We also note that the error of orientation would be slightly greater.
% Todo: explain why mAOE is worse?
% \textbf{Effectiveness of camera view position embedding.} To validate our view invariant attention design, we show the results of attention conducted in the local and global system. As shown in Tab~\ref{tab:ablation_view-invariant_decouple}, we simply adopt PETR with feature-guided position embedding as our baseline in  setting(a). When we change the position embeddings from global space to the local space, it could be seen that the NDS improves $1.6\%$ and mAP improves $2.4\%$, which validates the effectiveness of using local 3D positional embeddings in multi-view 3D object detection. With local view attention solely is not enough to predict objects' orientation in the global space, since 3D reference points don't contain orientation information. Thus we should preserve global attention here. When we simply add the global embeddings and local embeddings together, the model couldn't converge at all since the information mixes up. We adopt the bilateral attention inspired by ~\cite{meng2021conditional}. We could see that using bilateral attention containing global and local attention simultaneously achieves highest performances. Compared with baseline in setting (a), our method improves $2.4\%$ in NDS and $3.3\%$ in mAP.
\vspace{2mm}
\noindent\textbf{Effectiveness of camera view position embedding.} We validate the effectiveness of our proposed camera view position embedding in Tab~\ref{tab:ablation_view-invariant_decouple}. In Setting(a), we simply adopt PETR with feature-guided position embedding as our baseline. When we adopt the bilateral attention mechanism with 3D position embedding in the LiDAR system in Setting(b), the performance can be improved by $0.5\%$ on NDS. When using camera 3D position embedding without bilateral attention mechanism in Setting(c), the model could not converge and only get $2.5\%$ NDS. It indicates that the 3D position embedding in the camera system should be decoupled with output queries in the LiDAR system. The best performances could be achieved when we use camera view position embeddings along with the bilateral attention mechanism.
% The best performances could be achieved when we use bilateral  attention mechanism with camera view position embedding.
For fair comparison with 3D position embedding in the LiDAR system, camera view position embedding improves $1.4\%$ in NDS and $2.4\%$ in mAP (see Setting(b) and (d)).


\begin{table}[!t]
\small
\centering
\setlength{\tabcolsep}{1.5mm}
\caption{Ablation study of camera view position embedding and bilateral attention mechanism on the nuScenes validation set.
% We display NDS, mAP, mATE and mAOE as the evaluation metric.
``Cam.View'': 3D position embeddings are constructed in the camera system, otherwise 3D position embeddings are constructed in the global (LiDAR) system.``Bilateral'': bilateral attention mechanism is adopted in the decoder.}
\label{tab:ablation_view-invariant_decouple}
\vspace{-10pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c| c c| c c c c} 
 \hline
 Setting & Cam.View & Bilateral & NDS$\uparrow$&mAP$\uparrow$&mATE$\downarrow$&mAOE$\downarrow$ \\ [0.5ex] 
 \hline
%  (a) & $\checkmark$ & & & 0.455 & 0.406 & 0.736 & 0.432  \\
(a) & & & 0.460 & 0.409 & 0.734 & 0.424  \\ % PETR + FPE
(b) & & $\checkmark$ & 0.465 & 0.415 & 0.708 & 0.420 \\ % PETR + FPE + Bilateral
% (a) & $\checkmark$ & & & 0.460 & 0.409 & 0.734 & 0.424  \\
%  (c) & & $\checkmark$ & & 0.471 & 0.430 & 0.672 & 0.452 \\
 (c) & $\checkmark$ &  & 0.025 & 0.100 & 1.117 & 0.918 \\
%  Baseline+da. & 0.462 & 0.415 & 0.702 & 0.441 \\
%  Baseline+da.+dw. & 0.465 & 0.415 & 0.708 & 0.420\\
%  Baseline+da.+dw.+lp. & 0.479 & 0.439 & 0.683 & 0.427 \\
 (d) & $\checkmark$ & $\checkmark$ &  0.479 & 0.439 & 0.683 & 0.427 \\
 \hline
\end{tabular}
}
\vspace{-10pt}
\end{table}
\begin{table}[t]
\small
\setlength{\tabcolsep}{2.3mm}
\centering
\caption{Ablation study of feature guided position embedding in queries and keys on the nuScenes validation set. ``Q-FPE'': using feature-guided design in queries.``K-FPE'': using feature-guided design in keys. }
\label{tab:ablation_KPFE_and_QFPE}
\vspace{-10pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c | c c | c c c c} 
 \hline
 Setting & Q-FPE & K-FPE & NDS$\uparrow$&mAP$\uparrow$&mATE$\downarrow$&mAOE$\downarrow$ \\ [0.5ex] 
 \hline
(a) & & & 0.447 & 0.415 & 0.719 & 0.515 \\ 
(b) &$\checkmark$ & & 0.449 & 0.421 & 0.693 & 0.551 \\ 
(c) & &$\checkmark$ & 0.463 & 0.420 & 0.700 & 0.473 \\ 
(d) &$\checkmark$ &$\checkmark$ & 0.479 & 0.439 & 0.683 & 0.427 \\
 \hline
\end{tabular}
}
\vspace{-15pt}
\end{table}

\vspace{2mm}
\noindent\textbf{Effectiveness of feature-guided position embedding.}
Tab.~\ref{tab:ablation_KPFE_and_QFPE} shows the effect of feature-guided position embedding in both queries and keys. 
% QFPE means using object queries modulates geometric query embedding.
% Q-FPE means using decoder embeddings to modulate the query positional embedding.
% And K-FPE means encoding key positional embedding by image features. 
Different from the FPE in PETRv2\cite{paszke2019pytorch}, our K-FPE here is formed under the local camera-view coordinate system instead of the global coordinate system. Compared with Setting(a) and (b), we find the Q-FPE increases the location accuracy (see the improvement of mAP and mATE), but decreases the orientation performance in mAOE, mainly owing to the lack of image appearance information in the local view attention. % This deduction is supported by Setting (d) where Q-FPE improves the mAOE by 4.6\% when using K-FPE. 
Compared with Setting(a) and (c), using K-FPE could improve $1.6\%$ on NDS and $4.2\%$ on mAOE, which benifits from more precise depth and orientation information in image appearance features. Compared with Setting(c) and (d), adding Q-FPE could bring $1.6\%$ and $1.9\%$ gain on NDS and mAP further. The benefit brought by Q-FPE could be explained by 3D anchor points being refined by input queries in high-dimensional embedding space. It could see that using both Q-FPE and K-FPE achieves the best performances. 
% Compared with Setting(a) which uses none of them, the performance could improve $3.2\%$ and $2.4\%$ on NDS and mAP.
% Todo: No QFPE exp should be updated by no QR results.

\vspace{2mm}
\noindent\textbf{Effectiveness of the temporal modeling approach.} We show the necessity of using a set of queries for each frame in Tab.~\ref{tab:ablation_group_queries}. It could be observed that decomposing queries into different frames could improve $0.9\%$ on NDS and $0.8\%$ on mAP. With the multi-group design, one object query could correspond to one instance on each frame respectively, which 
is proper for DETR-based paradigms. Similar conclusion is also observed in 2D instance segmentation tasks~\cite{wu2021seqformer}. Since we use multi groups of queries, auxiliary supervision on previous frames can be added to better align object queries between frames. Meanwhile, the generated ground truth on the previous frame could be treated as a type of data augmentation to avoid overfitting. When we 
adopt the previous loss, the mAP increases $0.5\%$, which proves the validity of supervision on multi-frames. Compared with Setting(a) and (c), our temporal modeling approach could improve $1.4\%$ on NDS and $1.2\%$ on mAP on the validation dataset. 

\begin{table}[t]
\small
\setlength{\tabcolsep}{2.3mm}
\centering
\caption{Ablation studies of our temporal modeling approach with ego-motion embedding on the nuScenes validation set. We validate the necessity of using a group of queries for each frame and the effectiveness of conducting the supervision on previous frames. ``QT'': whether sharing \textbf{Q}ueries for each frame in \textbf{T}emporal modeling. ``LP'': using auxiliary \textbf{L}oss on \textbf{P}revious frames. 
% We display NDS, mAP, mAOE and mAVE as the evaluation metric.
}
\label{tab:ablation_group_queries}
\vspace{-10pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c c | c c c c c} 
 \hline
 Setting & QT & LP & NDS$\uparrow$&mAP$\uparrow$&mAOE$\downarrow$&mAVE$\downarrow$ \\ [0.5ex] 
 \hline
 (a) & Share &  & 0.522 & 0.428 & 0.454 & 0.341 \\ 
 (b) & Not Share & & 0.531 & 0.436 & 0.401 & 0.346 \\
 (c) & Not Share & $\checkmark$ & 0.536 & 0.440 & 0.396 & 0.323 \\
 \hline
\end{tabular}
}
\vspace{-0.2cm}
\end{table}

% \vspace{2mm}
\begin{table}[t]
\small
\setlength{\tabcolsep}{1.7mm}
\centering
\caption{Ablation study of our temporal fusion approach on nuScenes validation set. ``Ego'': using ego motion embedding in the fusion process; ``concat + ML'': simply concatenating queries in two frames and using a two-layer MLP to output fused queries; ``channel att'': learning channel attention weights for each query.}
\label{tab:ablation_fusion_methods}
\vspace{-10pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c c| c c c c} 
 \hline
 Setting & Fusion ways & Ego & NDS$\uparrow$&mAP$\uparrow$&mAOE$\downarrow$&mAVE$\downarrow$ \\ [0.5ex] 
 \hline
 (a) & concat + MLP & &  0.527 & 0.432 & 0.413 & 0.343 \\ 
 (b) & concat + MLP & $\checkmark$ & 0.531 & 0.439 & 0.432 & 0.318 \\
 (c) & channel att & & 0.530 & 0.432 & 0.400 & 0.333 \\
 (d) & channel att & $\checkmark$ & 0.536 & 0.440 & 0.396 & 0.323 \\
 
 \hline
\end{tabular}
}
\vspace{-15pt}
\end{table}

% \vspace{-0.2cm}


\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{figs/vis_attn_version2.pdf}
\vspace{-10pt}
\caption{Visualization of attention maps from an object query in the last decoder layer. Four heads out of eight are shown here. We only show a single view for simplicity, (a): the normalized $\mathbf{G}_{n}^\top\mathbf{P}_{n}$ (local view attention maps), (b): the normalized $\mathbf{X}_n^\top\mathbf{O}$ (global view attention maps), (c): the overall attention maps that are the normalized weights of the summation of the former two items.
% the summation of local view attention maps and global view attention maps). 
Note that we only visualize the attention weights that are greater than $1e^{-4}$ for better visualization.
}
\centering
\label{fig:vis_attn}
\vspace{-10pt}
\end{figure*}

\vspace{2mm}
\noindent\textbf{Effectiveness of different fusion approaches.}
The fusion module is used to fuse different frames for temporal modeling. We explore some common fusion approaches for temporal fusion in Tab.~\ref{tab:ablation_fusion_methods}. We first try a simple fusion approach ``concat with MLP'', and achieve $52.7\%$ on NDS, which has $0.5\%$ improvement compared with sharing queries. Considering queries on each frame have similar semantic information and different positional information, we propose the fusion model inspired by the channel attention. As is seen in Tab.~\ref{tab:ablation_fusion_methods}, our proposed fusion approach ``channel att'' achieves higher performance compared to simple concatenation operation. 
We claim that the performance gain is not from the increased parameters since only three fully-connected layers are added in our model.
Since queries are defined in each frame's system and the ego motion occurs between frames, we encode the ego-motion matrix as a high dimensional embedding to align queries in the current frame's system. With ego-motion embeddings, our fusion approach could further improve $0.6\%$ on NDS and $0.8\%$ on mAP.  
% With ego motion embeddings, our fusion approach could further improve $0.6\%$ on NDS and $0.8\%$ on mAP. 

\subsection{Visualization} 
We show the visualization of attention maps in Figure ~\ref{fig:vis_attn} from 4 heads out of 8 heads. We display attention maps after the soft-max normalized operation. From the up-bottom way in each row, there are local view attention maps, global view attention maps, and overall attention maps separately. We obverse and draw three conclusions from visualization results. Firstly, local view attention mainly tends to highlight the neighbor of objects, such as the front, middle, and bottom of the objects, while global view attention pays more attention to the whole of images, especially on the ground plane and the same type of objects, as shown in  Figure ~\ref{fig:vis_attn} (a). 
This phenomenon indicates that local view attention and global view attention are complementary to each other. Secondly, as shown in Figure ~\ref{fig:vis_attn} (a) and (c), it can be seen that overall attention maps are highly similar to local view attention maps, which means the local view attention maps play the dominant role compared to global view attention. Thirdly, we could obverse that the overall attention maps are further concentrating on the foreground objects in a fine-grained way, which implies superior localization accuracy.
% implies our proposed attention mechanism could achieve great performances.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figs/robustness.pdf}
\vspace{-10pt}
\caption{The performance drop of PETRv2 and \ourMethod{}-T under different camera extrinsics noise levels on nuScenes validation set.}
\centering
\label{fig:robustness}
\vspace{-10pt}
\end{figure}

\subsection{Robustness Analysis}
We evaluate the robustness of our method on camera extrinsic interference in this section. Camera extrinsic interference is an unavoidable dilemma caused by calibration errors, vehicle jittering, etc. We imitate extrinsics noises on the rotation with different noisy levels following PETRv2~\cite{liu2022petrv2} for a fair comparison. Specifically, we randomly sample an angle within the specific range and then multiply the generated noisy rotation matrix by the camera extrinsics in the inference. We present the performance drop of metric mAP on both PETRv2 and \ourMethod{}-T in Fig.\ref{fig:robustness}. We could see that our method has more robust performances on all noisy levels compared to PETRv2 ~\cite{liu2022petrv2} when facing extrinsics interference. For example, in the noisy level setting $R_{max} = 4$, \ourMethod{}-T drops 1.31\% while PETRv2 drops 2.39\%, which shows the superiority of camera-view position embeddings. 
% Although it is still affected by  noisy extrinsics,  our method is more robust in anti-interference to some degree.
% We will try to address these issues in our further works. 

% model latency & limitations ?

% \subsection{Limitation}
% \gs{
% As demonstrated in previous experiments, \ourMethod{} performs well in most cases. However, it still fails in several challenging cases, mainly including serious occlusion and far-away objects. Note that all these errors
% are common challenges for the other state-of-the-art methods. We will try to address these issues in our further works.
% }