\section{Introduction}
\label{sec:intro}
3D perception from multi-view cameras is a promising solution for autonomous driving due to its low cost and rich semantic knowledge. Given multiple sensors equipped on autonomous vehicles, how to perform end-to-end 3D perception integrating all features into a unified space is of critical importance. In contrast to traditional perspective-view perception that relies on post-processing to fuse the predictions from each monocular view~\cite{wang2021fcos3d, wang2022probabilistic} into the global 3D space, perception in the bird's-eye-view (BEV) is straightforward and thus arises increasing attention due to its unified representation for 3D location and scale, and easy adaptation for downstream tasks such as motion planning.
%  Multi-view 3D object detection from cameras is a critical task for autonomous driving and robotics due to its low cost and high resolution. This task evolves all-around area perception from single-view monocular 3D object detection with mounted multi-cameras in different poses. Previous methods~\cite{wang2021fcos3d, wang2022probabilistic} treat multi-view 3D detection task as the monocular 3D detection problem and convert the predicted 3D proposals in each cameraâ€™s respective into the global view in post-processing processes.
%  The advantage of these methods is that the transformation from 2D image space to the 3D camera space is relatively invariant, thus making the learning process easier. And the drawback of these methods is that the multi-view geometric information is ignored, thus leading to sub-optimal performances. Recently, the bird's-eye-view (BEV) representation has drawn much attention since it provides a unified scene representation for multi-view 3D detection and it's suitable for downstream tasks such as predicting and planning.

The camera-based BEV perception is to predict 3D geometric outputs given the 2D features and thus the vital challenge is to learn the view transformation relationship between 2D and 3D space.
According to whether the explicit dense BEV representation is constructed, 
existing BEV approaches could be divided into two categories: the explicit BEV representation methods and the implicit BEV representation methods. 
The former constructs an explicit BEV feature map by lifting the 2D perspective-view features to 3D space~\cite{li2022bevformer,philion2020lift, huang2021bevdet}. % remove details
% via estimated depth or by projecting the preset 3D grid points onto 2D space to sample the corresponding features
% simplified by Kaixin
The latter mainly follow DETR-based~\cite{carion2020end} approaches in an end-to-end manner. Without projection or lift operation, those methods \cite{zhou2022cross, liu2022petr,liu2022petrv2} implicitly encode the 3D global information into 3D position embedding (3D PE) to obtain 3D position-aware multi-view features, which is shown in Figure~\ref{fig:intro1}(a).

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/intro.pdf}
\caption{
Comparison of the network structure between PETRv2 and our proposed \ourMethod{}. (a) In PETRv2, position embedding of queries and keys are in the global system. (b) In \ourMethod{}, position embeddings of queries and keys are within the local system of each view. Bilateral cross-attention is adopted to compute attention weights in the local and global systems independently.}
\centering
\label{fig:intro1}
\vspace{-10pt}
\end{figure}

%combining Kaixin's with Xiaoqing's
Though learning the transformation from 2D images to 3D global space is straightforward, we reveal that the interaction
% interacting the query embeddings with 3D position-aware multi-view features
in the global space for the query embeddings and 3D position-aware multi-view features hinders performance. The reasons are two-fold. For one thing, defining each camera coordinate system as the 3D local space, we find that the view transformation couples the 2D image-to-local transformation and the local-to-global transformation together. Thus the network is forced to differentiate variant camera extrinsics in the high-dimensional embedding space for 3D predictions in the global system, while the local-to-global relationship is a simple rigid transformation. For another, we believe the view-invariant transformation paradigm from 2D image to 3D local space is easier to learn, compared to directly transforming into 3D global space.
%  For example, as depicted in Fig.~\ref{fig:intro2} (a), even the two vehicles in two views have similar appearance in 2D features, it is forced to predict totally different 3D locations in the global space. 
For example, though two vehicles in two views have similar appearances in image features, the network is forced to learn different view transformations, as depicted in Figure~\ref{fig:intro2}~(a).
%\yxq{Though 3D object queries can directly be updated by interacting with the 3D position-aware features, we reveal that the 3D position embedding is computed from the global (world) space whereas the multi-view 2D features are in their respective views, forcing the implicit view transformation to learn the 2D image to 3D global space. As depicted in Fig.~\ref{fig:intro2} (a), existing 3D global PE requires the network to learn 3D locations in the global coordinate and even the two vehicles in two views have similar appearance in 2D features, it is forced to predict totally different 3D locations in the global space. Define each camera coordinate system as the 3D local space, in fact, the view transformation has to learn the 2D image to local and the local-to-global relationship simultaneously, causing ambiguity.} 

%\xkx{Though learning the transformation from 2D image to 3D global space is straightforward, we reveal that interacting query embeddings with 3D position-aware multi-view features in the global space is sub-optimal. The reasons are in two folds: one is that the network is required to differentiate variant camera extrinsics in the high-dimensional embedding space for final predictions in the global system, while the local-to-global relationship is a simple rigid transformation; The other reason is that the network couldn't reuse the regular pattern of transformation from 2D image to 3D local space. For example, as depicted in Fig.~\ref{fig:intro2} (a), even the two vehicles in two views have similar appearance in 2D features, it is forced to predict totally different 3D locations in the global space. }

% \yxq{
% To solve the ambiguity, we propose a simple yet effective approach based on local view attention, termed LVTR, which performs 3D position embedding in the local system of each camera instead of the 3D global space. As is depicted in Fig.~\ref{fig:intro2} (b), our approach gets rid of encoding the camera extrinsics and only need to learn the view transformation from 2D features to local 3D space, which eliminate the variances of view transformation caused by different camera extrinsics.
% }

To ease the difficulty in view transformation from 2D image to global space, we propose a simple yet effective approach based on local view position embedding, called \ourMethod{}, which performs 3D position embedding in the local system of each camera instead of the 3D global space. As depicted in Figure~\ref{fig:intro2} (b), our approach learns the view transformation from 2D image to local 3D space, which eliminates the variances of view transformation caused by different camera extrinsics. 

% \yxq{ It's hard to understand the following part. }
% Since there is no orientation information contained in positional queries in the local system, we preserve the attention calculation in the global system and adopt the bilateral attention to avoid the mixture of coordinate system.
% With unified learning process from 2D images to local system, our method could eliminate the variances in different camera views and make the transformation learning easier.
% ------------------------------------------------------------------
%                 Why we need the spatial and content
%-------------------------------------------------------------------
% \gs{Inspired by Conditional-DETR\cite{chong2021monodistill}}
% \gs{The global positional query and transform them to the local coordinates 
% The query positional embedding is defined in the local space }
% \yxq{Given that we form the keys position embedding in the local space rather than the global space whereas the queries are originally defined in the global coordinate system, 
% we adopt the bilateral attention mechanism in the decoder to decompose the queries into local and global queries so that the spatial queries and keys are in the consistent local coordinate system. }

% \gs{}

% \begin{comment}
% Specially, 
% inspired by Conditional-DETR\cite{meng2021conditional}, we first convert the 3D reference points defined in the global space into local camera system to match with the local positional embedding, and then we . 
% \end{comment}

% We construct the 3D position embedding of keys and queries in the local space as follows. 
Specially,
as for key 3D PE, we transform camera frustum into 3D coordinates in the camera system using camera intrinsics only, then encoded by a simple MLP layer.
As for query 3D PE, we convert the 3D reference points defined in the global space into the local camera system with camera extrinsics only, then encoded by a simple MLP layer. 
% To make 3D PE relevant to the input, 
% With the inspiration of 
Inspired by ~\cite{meng2021conditional, liu2022petrv2}, we obtain the 3D PE with the guidance of image features and decoder embeddings, for keys and queries, respectively.
 % we use the features of keys and queries as guidance, termed K-FPE and Q-FPE respectively.
Given that 3D PE is in the local space whereas the output queries are defined in the global coordinate system, 
we adopt the bilateral attention mechanism
% inspired by Conditional-DETR\cite{meng2021conditional} 
to avoid the mixture of embeddings in different representation spaces, as shown in Figure~\ref{fig:intro1}(b).
% Specially, we compute local view attention maps and global view attention maps independently and then adding two attention maps together to get the overall attention maps, as shown in Figure ~\ref{fig:intro1}(b).

% \yxq{
% The devil is that the predicted offset is not the same though the appearance information in images are resembling, as shown in Fig.~\ref{fig:intro2}. 
% We claim that the process of transforming images to 3D space should be simple and reusable, and the differences in view transformation could hamper the performances.
% }

% \xkx{The camera-based BEV perception task is to predict 3D geometric outputs given the 2D features and thus the vital challenge is to perform favourable view transformation between 2D and 3D space. According to the whether to construct explicit dense BEV representation, existing BEV approaches could be roughly divided into two categories: the explicit BEV representation methods and the implicit BEV representation methods. The former requires to construct explicit BEV features by lifting the perspective view features to 3D by estimated depth or by projecting BEV grid points onto 2D feature maps to extract the corresponding features to obtain an explicit BEV feature map~\cite{li2022bevformer,philion2020lift, huang2021bevdet}. The latter mainly follows DETR-based~\cite{carion2020end} approaches and initializes sparse object queries in 3D space and interact with 2D features to directly perform 3D object detection. }

%The 2D-3D methods~\cite{philion2020lift, huang2021bevdet} explicitly lift the perspective view features into 3D voxels via the estimated depth distribution then collapse to the BEV representation, which relies heavily on the accuracy of estimated depth. 
%These methods require explicit depth estimation, making the performance of 3D object detection highly sensitive to the errors in depth estimation.
%The 3D-2D methods either project 3D sample points onto 2D feature maps to extract the corresponding features~\cite{li2022bevformer,wang2022detr3d} or implicitly adopt the transformer-based attention mechanism extended from DETR~\cite{carion2020end}(see Fig.~\ref{fig:intro1}(a)) to perform geometric projection. These methods initialize object queries in 3D space and interact with 2D features to perform 3D perception. To be free of the susceptible feature sampling procedure, PETR\cite{liu2022petr}  generates the 3D position-aware features by encoding the 3D position embedding (PE) into multi-view 2D features and PETRv2\cite{liu2022petrv2} further introduces the feature-guided 3D PE to provide data-dependent guidance for transformer decoder learning.



%In this paper, we propose a transformer-based method tailored for multi-view 3D object detection task, termed MV-DETR, which aims to eliminate the variances of view transformation caused by different camera extrinsics. While previous methods learn geometric relationships between 2D image plane and 3D global space from attention mechanism (see Fig.~\ref{fig:intro1}(c)), our method learns the transformation from images to the local 3D space instead. 
%The transformation from images to the local 3D space is rather stable since only camera intrinsics with small variance are involved in this process. 




% To ease the spatial inconsistencies between local and output queries, we adopt the bilateral attention mechanism in the decoder. Specifically, we separate the local queries from the output global queries, and calculate their attention weights independently. Furthermore, we exploit the object queries of previous frames through encoding the ego pose embedding for boosting $3$D object detection. LVTR achieves the state-of-the-art performance (59.9\% NDS and 51.5\% mAP) among all LiDAR-free methods on standard nuScenes dataset.


%We follow the Conditional DETR~\cite{meng2021conditional} that decouples the cross attention into content and spatial parts(see Fig.~\ref{fig:intro1}(b)). Specifically, we transform spatial queries and keys to the local 3D system for attention calculation. As for spatial queries, we convert 3D reference points in the global 3D space into each local view using extrinsic parameters and generate local spatial queries from converted reference points. As for spatial keys, we use camera intrinsics to project the camera frustum to points in local 3D space and generate the local 3D position embedding from a small MLP encoder. Spatial queries and spatial keys are interacted to generate geometric affinity maps for learning geometric relationships(see Fig.~\ref{fig:intro1}(d)). In this way, the transformation could be unified for all views and the learning process would be insensitive to the different poses of mounted sensors. 


\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/intro_2.pdf}
\caption{View transformation comparison. In previous methods, view transformation is learned from the image to the global 3D coordinate system directly. In our method, the view transformation is learned from image to local (camera) coordinate system.}
\centering
\label{fig:intro2}
\vspace{-10pt}
\end{figure}

We further extend \ourMethod{} to integrate multi-frame temporal information to boost the 3D object detection performance, named \ourMethod{}-T. Different from previous methods that either warp the explicit BEV features using ego-motion~\cite{li2022bevformer, 2022BEVDet4D} or encode the ego-motion into the position embedding~\cite{liu2022petrv2}, we adopt separated sets of object queries for each frame and encode the ego-motion to fuse the queries.
%Further, Considering temporal information is essential for velocity estimation, we incorporate temporal modeling into our proposed MV-DETR, which is called as MV-DETR-T. Previous methods either warp the explicit BEV features using ego-motion~\cite{li2022bevformer, 2022BEVDet4D} or encode the ego motion into the position embedding~\cite{liu2022petrv2}. The former would be affected by inevitable quantization error from feature warp and the latter is the indirect way to encode the ego motion. In MV-DETR-T, we incorporate the ego motion information in the process of projecting the reference points onto previous frames. Considering instances would have different position on the image plane in each frame due to the ego motion and the movement by themselves, it's hard to differentiate instances with different poses using only one group of queries. Thus, we conduct attention in each frame respectively, and interact the queries for each frame with our proposed Query Fusion Module.

We summarize our key contributions as follows: 
\begin{itemize}
%
\vspace*{-1mm}
\item
% We propose \ourMethod{}, based on camera-view position embedding for end-to-end multi-view imaged-based 3D detection. \ourMethod{} could eliminate the inconsistency of view transformation brought by variation of camera extrinsics for multi-view 3D object detection.
We propose a novel multi-view 3D detection method, called \ourMethod{}, based on camera-view position embedding, which eliminates the variances of view transformation caused by different camera extrinsics.
%
\vspace*{-2mm}
\item
% We extend \ourMethod{} to \ourMethod{}-T, the version that supports multi-frames as input. We inject the ego motion into the projection of reference points and conducts attention in each frame respectively for boosting 3D object detection. And a simple fusion model is proposed for fusing queries from each frame.
We further generalize our \ourMethod{} to temporal modeling, by exploiting the object queries of previous frames and leveraging the ego-motion explicitly for boosting 3D object detection and velocity estimation.

\vspace*{-2mm}
\item
Extensive experiments on the nuScenes dataset show the effectiveness of our proposed approach and we achieve the state-of-the-art among all LiDAR-free methods on the challenging nuScenes benchmark.
\vspace*{-1mm}
\end{itemize}
%However, it is non-trival to adopt the feature sampling due to the inaccurate of reference points and complicate practical application. To solve the problem, the 3D position encoding in the global view is proposed by~\cite{liu2022petr} and it conducts global cross attention from multi-view images. 

% However, in multi-view 3D object detection, due to the large variances in the poses of different cameras, the transformation from different 2D pixel spaces (different 2D source spaces) to the global 3D space (the same 3D target space) is different. The inconsistent reflection relationships increase the difficulty for learning the 2D-3D transformation. We reckon that the source space should be unified to make the reflection learning easier. We propose Multi-View DETR(MV-DETR), a transformer-based method that normalize the 3D position embedding into the local view and promises much smaller variances in the source spaces. 
