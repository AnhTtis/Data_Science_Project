
\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figs/overview.pdf}
\caption{The overview of \ourMethod{}. The multi-view images are fed into the backbone network to extract the 2D features including $N$ views. Key position embedding is formed by transforming camera-view frustum points to 3D coordinates in the camera system with camera intrinsics. Images features are used to guide the key position embedding in K-FPE; Query positional embedding is formed by converting the global 3D reference points into $N$ camera-view coordinates with camera extrinsics. Then we encode them under the guidance of decoder embeddings in Q-FPE. The decoder embeddings are updated via the interaction with image features in the decoder. The updated decoder embeddings are used to predict 3D bounding boxes and object classes.}
\vspace{-10pt}
\centering
\label{fig:overview}
\end{figure*}

\section{Related Work}
\subsection{DETR-based 2D Detection}
DETR ~\cite{carion2020end} is the pioneering work that successfully adopts transformers~\cite{vaswani2017attention} in the object detection task. It adopts a set of learnable queries to perform cross-attention and treat the matching process as a set prediction case. Many follow-up methods~\cite{zhu2020deformable, chen2022group, wang2022anchor, liu2022dab, li2022dn} focus on addressing the slow convergence problem in the training phase. 
For example, 
% Deformable DETR~\cite{zhu2020deformable} proposes deformable attention that only calculates attention with sampled points. 
Conditional DETR~\cite{meng2021conditional} decouples the items in attention into spatial and content items, which eliminates the noises in cross attention and leads to fast convergence.
% Other methods mainly concentrate on analyzing the function of queries in the decoder.
% DAB-DETR~\cite{liu2022dab} uses dynamic anchor boxes to generate queries.
% DN-DETR~\cite{li2022dn} proposes a denoising training method for solving the instability of bipartite matching. 
% Recently, DINO ~\cite{zhang2022dino} improves the above models by using several techniques such as mixed query selection.

\subsection{Monocular 3D Detection}
% The goal of monocular 3D object detection is to predict 3D bounding boxes of objects from a single-view RGB image~\cite{chen2016monocular}. 
% It's an ill-posed problem since irreversible depth information is lost in the projection process. 
Monocular 3D Detection task is highly related to multi-view 3D object detection since they both require restoring the depth information from images.  
The methods can be roughly grouped into two categories: pure image-based methods and depth-guided methods. Pure image-based methods mainly learn depth information from objects' apparent size and geometry constraints provided by eight keypoints or pin-hole model ~\cite{brazil2019m3d,mousavian20173d, li2020rtm3d,li2019gs3d,lian2021geometry,liu2022learning,wang2021depth,lu2021GUPNet}. 
% M3D-RPN~\cite{brazil2019m3d} proposes 2D-3D anchor and depth-wise convolution to detect 3D objects.
% GUPNet~\cite{lu2021GUPNet} proposes a geometry uncertainty projection method for better 3D localization. 
%Recently, DID-M3D~\cite{peng2022did} decouples the object's depth into visual depth and attribute depth. 
Depth-guided methods need extra data sources such as point clouds and depth images in the training phase~\cite{CaDDN, ye2020monocular, ma2020rethinking, qian2020end, chong2021monodistill}. Pseudo-LiDAR~\cite{wang2019pseudo} converts pixels to pseudo point clouds and then feeds them into a LiDAR-based detector~\cite{shi2019pointrcnn, liu2020tanet, ye2020monocular,deng2021voxel}. DD3D~\cite{park2021pseudo} claims that pre-training paradigms could replace the pseudo-lidar paradigm. The quality of depth estimation would have a large influence on those methods.
   
\subsection{Multi-View 3D Detection}
Multi-view 3D detection aims to predict 3D bounding boxes in the global system from multi-cameras. 
Previous methods~\cite{wang2021fcos3d, wang2022probabilistic, epropnp} mostly extend from monocular 3D object detection. 
% FCOS3D~\cite{wang2021fcos3d} extends FCOS~\cite{tian2019fcos} to predict 3D bounding boxes in the local view. PGD~\cite{wang2022probabilistic} constructs geometric relation graphs across predicted objects and uses the graph to facilitate depth estimation.
% Epro-PnP~\cite{epropnp} learns 2D-3D correspondences via differentiable Perspective-n-Points.
Those methods cannot leverage the geometric information in multi-view images. 
Recently, several methods~\cite{roddick2018orthographic, roddick2020predicting, philion2020lift, gong2022gitnet, li2022bevformer} attempt to percept objects in the global system using explicit bird's-eye view (BEV) maps. LSS~\cite{philion2020lift} conducts view transform via predicting depth distribution and lift images onto BEV.
% BEVDet~\cite{huang2021bevdet} follows LSS and constructs an exclusive data augmentation strategy.
BEVFormer\cite{li2022bevformer} exploits  spatial and temporal information through predefined grid-shaped BEV queries. 
% Gitnet\cite{gong2022gitnet} proposes a novel two-stage transformation from the perspective view to BEV.
BEVDepth~\cite{li2022bevdepth} leverages point clouds as the depth supervision and encodes camera parameters into the depth sub-network.

Some methods learn implicit BEV features following DETR~\cite{carion2020end} paradigm. These methods mainly initialize 3D sparse object queries and interact with 2D features by attention to directly perform 3D object detection. For example, DETR3D~\cite{wang2022detr3d} samples 2D features from the projected 3D reference points and then conducts local cross attention to update the queries. PETR~\cite{liu2022petr} proposes 3D position embedding in the global system and then conducts global cross attention to update the queries. 
PETRv2~\cite{liu2022petrv2} extends PETR with temporal modeling and incorporate ego-motion in the position embedding.
% These methods suffer from the different view transformation from image to global space. 
\ourMethod{} conducts the attention in image space and local 3D space to eliminate the variances in view transformation. \ourMethod{} could preserve the pros in single-view approaches and leverage the geometric information provided by multi-view images. 

\subsection{View Transformation}
% \xkx{
The view transformation from the global view to local view in 3D scenes is an effective approach to boost performances for detection tasks. This could be treated as a normalization by aligning all the views, which could facilitate the learning procedure greatly.
Several LiDAR-based 3D detectors~\cite{shi2019pointrcnn, qi2018frustum, shi2020points, meyer2019lasernet, fan2021rangedet} estimate local coordinates rather than global coordinates for instances in the second stage, which could fully extract ROI features. 
For example, PointRCNN~\cite{shi2019pointrcnn} proposes the canonical 3D box refinement in the canonical system for more precise regression. 
% and Frustum-PointNet~\cite{qi2018frustum} estimates objects in local frustum system for more precise regression. 
To reduce the data variability for point cloud, AziNorm~\cite{chen2022azinorm} proposes a general normalization in the data pre-process stage.
Different from these methods, our method conduct view transformation for eliminating the extrinsic variances brought by multi cameras with camera-view position embedding.
% }
%------------------------------------------------------------------------