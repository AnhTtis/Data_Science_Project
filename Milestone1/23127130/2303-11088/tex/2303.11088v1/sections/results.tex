
\section{Experimental Results}\label{sec:experimental-results}

In this section, we discuss and present the results of our benchmark executions.
\cref{sec:experimental-results:base} starts addressing \rqbase\ by running baseline experiments for each benchmark and framework.
It also provides first results regarding \rqbeam, which is further investigated in \cref{sec:experimental-results:beam-config}.
\cref{sec:experimental-results:windows} addresses \rqwindows\ and evaluates how different stream processing frameworks scale when increasing the duration of window aggregations.
\cref{sec:experimental-results:vertical} addresses \rqvertical\ by evaluating how stream processing frameworks scale on a single node.
\cref{sec:experimental-results:public-private} addresses \rqpublic\ and compares our baseline results in the private cloud with those of a public cloud.
Finally, \cref{sec:experimental-results:large-cluster} addresses \rqshift\ and repeats the same experiments for benchmark UC3 in Kubernetes clusters of different sizes.


\subsection{Baseline Comparison of Frameworks}\label{sec:experimental-results:base}

\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/base_uc1.pdf}
		\caption{UC1}
		\label{fig:eval:frameworks:base:uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/base_uc2.pdf}
		\caption{UC2}
		\label{fig:eval:frameworks:base:uc2}
	\end{subfigure}
	
	\vspace{1em}
	
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/base_uc3.pdf}
		\caption{UC3}
		\label{fig:eval:frameworks:base:uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/base_uc4.pdf}
		\caption{UC4}
		\label{fig:eval:frameworks:base:uc4}
	\end{subfigure}
	\caption{Scalability benchmark results according to our resource demand metric for the stream processing frameworks Apache Beam (with the Flink and Samza runners), Apache Flink, Hazelcast Jet, and Apache Kafka Streams.}
	\label{fig:eval:frameworks:base}
\end{figure*}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/base_beam_uc3.pdf}
		\caption{UC3}
		\label{fig:eval:frameworks:base-low:uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/base_beam_uc4.pdf}
		\caption{UC4}
		\label{fig:eval:frameworks:base-low:uc4}
	\end{subfigure}
	\caption{Repetition of scalability experiments shown in \crefrange{fig:eval:frameworks:base:uc3}{fig:eval:frameworks:base:uc4} with lower load intensities for Apache Beam with the Flink and the Samza runners.}
	\label{fig:eval:frameworks:base-low}
\end{figure}

For our baseline experiments, we use the private cloud environment (see \cref{tab:clouds}).
We benchmark load intensities between 100\,000 and 1\,000\,000 simulated sensors (and, thus, generated messages per second) for benchmark UC1 and UC2, 10\,000 and 100\,000 simulated sensors for UC3, and 5 to 9 nested groups (1\,024--262\,144 generated messages per second) for benchmark UC4.

\cref{fig:eval:frameworks:base} shows the resource demand results for all evaluated frameworks and benchmarks. The results for benchmark UC4 (in the following figures as well) are visualized with an exponential scale with base 4 at the horizontal axis since the  number of generated messages grows exponentially with a linear increase in the number of nested groups.
We can observe that in almost all experiments, Flink, Hazelcast Jet, and Kafka Streams, have a considerably lower resource demand than the Beam deployments. Only Hazelcast Jet in UC4 and the Beam Flink runner for low loads in UC1 are exceptions to this.
As in some cases, the generated load intensities were too high for the Beam deployments, we repeat the corresponding experiments with lower load intensities (see \cref{fig:eval:frameworks:base-low} and \cref{fig:eval:frameworks:beam-config-samza}).

Despite some outliers (see Beam/Flink in UC1 and Beam/Samza in UC4), all frameworks show linear scalability, yet with different rates. Whereas both SUTs based on Beam show the steepest increase in required resources, the results of Flink, Kafka Streams, and Hazelcast Jet vary depending on the benchmark.
In UC1 (see \cref{fig:eval:frameworks:base:uc1}), all frameworks behave similarly, with resource demands increasing slightly steeper for Hazelcast Jet compared to Kafka Streams and for Kafka Streams compared to Flink.
For UC2 (see \cref{fig:eval:frameworks:base:uc2}), we see a clear ranking with Hazelcast Jet showing the best results, followed by Kafka Streams and Flink.
For UC3 (see \cref{fig:eval:frameworks:base:uc3}), Hazelcast Jet appears to be even more superior.
A single Jet instance is sufficient for all evaluated load intensities. On the other hand, Flink requires up to 10~TaskManagers and Kafka Streams up to 18~instances. Overall, Kafka Streams' resource demand for UC3 increases at a steeper rate compared to Flink.
To further inspect the scalability of Hazelcast Jet for UC3, we repeat these experiments with an aggregation duration of 30~days in contrast to 3~days as used in the other experiments. \cref{fig:eval:frameworks:hazelcastjet-uc3:base} shows that Hazelcast Jet also scales linearly in this case. %
With UC4 (see \cref{fig:eval:frameworks:base:uc4}), we observe a comparable increase in resource demand for Kafka Streams and Flink. In contrast to the other benchmarks, Hazelcast Jet shows a significantly higher resource demand. Up to 30~instances are not able to handle load from more than 7 nested sensor groups.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/base_hazelcastjet_uc3.pdf}
		\caption{30 days aggregation period}
		\label{fig:eval:frameworks:hazelcastjet-uc3:base}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/parallel_windows_uc3_hazelcastjet.pdf}
		\caption{100\,000 simulated sensors}
		\label{fig:eval:frameworks:hazelcastjet-uc3:parallel-windows}
	\end{subfigure}
	\caption{Repetition of scalability experiments with Hazelcast Jet and benchmark UC3. \protect\subref{fig:eval:frameworks:hazelcastjet-uc3:base} evaluates scalability regarding the number of simulated sensors with a 30~days aggregation period in contrast to the 3~days period in \cref{fig:eval:frameworks:base:uc3}.
		\protect\subref{fig:eval:frameworks:hazelcastjet-uc3:parallel-windows} evaluates scalability regarding the aggregation period with a constant load of 100\,000 simulated sensors in contrast to 10\,000 simulated sensors in \cref{fig:eval:frameworks:windows:not-beam}.}
	\label{fig:eval:frameworks:hazelcastjet-uc3}
\end{figure}

For the frameworks used with Apache Beam, we observe a significantly steeper increase in resource demand of Samza compared to Flink in UC1 (cf. \cref{fig:eval:frameworks:base:uc1}) %
and UC2 (cf. \cref{fig:eval:frameworks:base:uc2}). %
For benchmark UC3 (see \cref{fig:eval:frameworks:base-low:uc3}), both frameworks scale at similar rates with Samza requiring slightly fewer instances.
For benchmark UC4 (see \cref{fig:eval:frameworks:base-low:uc4}), it appears that the resource demand of Flink increases at a steeper rate as well. %
For lower load intensities (less than 5~nested groups), Flink requires fewer instances than Samza. 

Worth mentioning is also the significant difference between native Apache Flink and the Flink runner of Apache Beam. In almost all experiments, the resource demand of Apache Beam with Flink is at least twice as high. For the more compute-intensive benchmarks UC3 and UC4, it is tremendously higher.
The performance overhead of using Apache Beam as an abstraction layer has also been observed in related research~\cite{Hesse2019}.

\begin{tcolorbox}
	\textbf{\rqbase:}
	All frameworks appear to be linearly scalable, however, with different resource usage. Depending on the task sample, Flink, Hazelcast Jet, or Kafka Streams have the smallest resource demand. In particular for the task samples of medium complexity, Hazelcast Jet's performance is outstanding.
	Apache Beam implementations, executed by Samza or Flink, are inferior, independent of the use case.
\end{tcolorbox}





\subsection{Impact of Apache Beam Configuration}\label{sec:experimental-results:beam-config}

As we have seen in \cref{sec:experimental-results:base},
Apache Flink and Apache Samza in combination with Apache Beam have a significantly higher resource demand compared to the other evaluated frameworks.
In this section, we take a closer look at the scalability of the Apache Beam SUTs and evaluate how scalability is affected by different configuration options.
Again, we use the private cloud environment (see \cref{tab:clouds}).
In the following, we first look at the Apache Flink runner and, afterward, at the Apache Samza runner.

\subsubsection{Apache Flink}

In their master's thesis, \citet{Spaeren2021} investigates possible reasons for the performance overhead of the Flink runner found by \citet{Hesse2019}.
They discovered unnecessary serialization and deserialization between operators and introduced the \textit{FasterCopy} option, which disables these copy operations. This option is integrated in Beam since version 2.26.
While the stream processing application must fulfill some requirements to run with the \textit{FasterCopy} option, \citet{Bensien2021} found that the Theodolite benchmarks fulfill these requirements.
As additionally this option might become the default in future releases, %
we decided to turn on this option in our benchmark implementations by default.
In this section, we evaluate how enabling and disabling \textit{FasterCopy} affects scalability.

Additionally, we observed that Beam's Kafka consumers generate a lot of log messages if not configured differently. This contrasts with the other frameworks.
As extensive logging can actually have an impact on performance (see the following \cref{sec:experimental-results:vertical}),
we evaluate whether disabling all logging results in lower resource demand.


\begin{figure}
	\centering
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/optimization_beam_flink_uc1.pdf}
		\caption{UC1}
		\label{fig:eval:frameworks:beam-config-flink:uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/optimization_beam_flink_uc2.pdf}
		\caption{UC2}
		\label{fig:eval:frameworks:beam-config-flink:uc2}
	\end{subfigure}
	
	\vspace{1em}
	
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/optimization_beam_flink_uc3.pdf}
		\caption{UC3}
		\label{fig:eval:frameworks:beam-config-flink:uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/optimization_beam_flink_uc4.pdf}
		\caption{UC4}
		\label{fig:eval:frameworks:beam-config-flink:uc4}
	\end{subfigure}
	\caption{Scalability benchmark results for different configurations of Apache Beam with the Apache Flink runner.}
	\label{fig:eval:frameworks:beam-config-flink}
\end{figure}

\cref{fig:eval:frameworks:beam-config-flink} shows our results of running the scalability benchmarks with the \textit{FasterCopy} option disabled and logging disabled compared to the experiments from \cref{sec:experimental-results:base}.
We can see that enabling \textit{FasterCopy} results in significantly lower resource demands for UC1--UC3 (see \crefrange{fig:eval:frameworks:beam-config-flink:uc1}{fig:eval:frameworks:beam-config-flink:uc3}). This is in line with the performance improvements reported by \citet{Spaeren2021}.
For benchmark UC4 (see \cref{fig:eval:frameworks:beam-config-flink:uc4}), enabling \textit{FasterCopy} seems not to have an effect on the resource demand.
A possible explanation is that the dataflow architecture of UC4 involves more data transfer among instances and, hence, actually requires serialization and deserialization between operators.

We can observe that disabling all logging has only a small impact on the resource demand of benchmark UC2--UC4 (see \crefrange{fig:eval:frameworks:beam-config-flink:uc2}{fig:eval:frameworks:beam-config-flink:uc4}), but significantly reduces the resource demand of benchmark UC1 (see \cref{fig:eval:frameworks:beam-config-flink:uc1}).
The latter is expected since benchmark UC1 logs each incoming message to simulate side effects such as writing records to a database.
We can conclude that the extensive logging of Beam's Kafka consumer contributes very little to the overhead introduced by Apache Beam.

\subsubsection{Apache Samza}


In a blog post, software engineers at LinkedIn \cite{Zhang2020} report how they tremendously improved the performance of Beam's Samza runner.
Primarily, this was achieved by exporting Beam metrics more efficiently.
Moreover, the authors observed that performance could further be improved when disabling the Beam metrics entirely.
Although this might not be an option in production \cite{Zhang2020}, we are still interested in how much performance could be further improved when disabling all Beam metrics.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/optimization_beam_samza_uc1.pdf}
		\caption{UC1}
		\label{fig:eval:frameworks:beam-config-samza:uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/optimization_beam_samza_uc2.pdf}
		\caption{UC2}
		\label{fig:eval:frameworks:beam-config-samza:uc2}
	\end{subfigure}
	
	\vspace{1em}
	
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/optimization_beam_samza_uc3.pdf}
		\caption{UC3}
		\label{fig:eval:frameworks:beam-config-samza:uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/optimization_beam_samza_uc4.pdf}
		\caption{UC4}
		\label{fig:eval:frameworks:beam-config-samza:uc4}
	\end{subfigure}
	\caption{Scalability benchmark results for different configurations of Apache Beam with the Apache Samza runner.}
	\label{fig:eval:frameworks:beam-config-samza}
\end{figure}

\cref{fig:eval:frameworks:beam-config-samza} shows our results for benchmarking scalability with Beam metrics enabled and disabled. We can observe that independent of the benchmark, disabling metrics results in a similar linear increase in resource demand, yet at a lower level.
However, also with metrics disabled, the resource demand of Apache Beam with the Samza runner is considerable higher compared to most other frameworks (with metrics not disabled).

Worth mentioning is also the bachelor's thesis of \citet{Bensien2021}, who benchmarked Beam with the Samza runner in version 2.22 using Theodolite's UC1 benchmark. This Beam version did not include the performance improvements by \citet{Zhang2020} (released in Beam version 2.27).
\citeauthor{Bensien2021} observed a resource demand more than twice as high with metrics enabled, compared to disabling them.

\begin{tcolorbox}
	\textbf{\rqbeam:}
	The previously discovered negative impact on performance of using Apache Beam as abstraction layer persists.
	Possible configuration options for improving performance (Flink's \textit{FasterCopy}, disabling metrics, and disabling logging) have no significant impact on our results. %
\end{tcolorbox}


\subsection{Scaling the Window Aggregation Duration}\label{sec:experimental-results:windows}

In \cref{sec:experimental-results:base}, we configure benchmark UC3 with a window duration of 3 days to compute an average daily course.
This is a trade-off to still benchmark generated data volume of reasonable size.
However, it is likely that in practice, larger time windows are required to obtain more reasonable results. Therefore, in this section, we evaluate, how different stream processing frameworks scale with increasing benchmark UC3's window duration.
We increase the window duration from 3~days to 30~days, while keeping the number of simulated sensors and, thus, the incoming message rate constant.
Again, we use the private cloud environment (see \cref{tab:clouds}).
This evaluation is an example of benchmarking scalability with respect to the work performed for each incoming message in contrast to scaling the load at the framework and, thus, addresses \rqwindows.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/parallel_windows_uc3.pdf}
		\caption{10\,000 simulated sensors}
		\label{fig:eval:frameworks:windows:not-beam}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/parallel_windows_uc3_low.pdf}
		\caption{2\,000 simulated sensors}
		\label{fig:eval:frameworks:windows:beam}
	\end{subfigure}
	\caption{Scalability benchmark results of all stream processing frameworks when increasing the duration of aggregation windows and, thus the number of windows maintained simultaneously.}
	\label{fig:eval:frameworks:windows}
\end{figure}

\cref{fig:eval:frameworks:windows} shows the results of these experiments. According to our previous results in \cref{sec:experimental-results:base}, we simulate 10\,000 sensors for our experiments with Flink, Hazelcast Jet, and Kafka Streams (see \cref{fig:eval:frameworks:windows:not-beam}) and 2\,000 sensors for the Beam SUTs (see \cref{fig:eval:frameworks:windows:beam}).
We can observe that again all frameworks scale approximately linearly.
Remarkable is again the performance of Hazelcast Jet, which only requires a single instance, independently of the window size. We repeat these experiments with a higher load of 100\,000 sensors. As shown in \cref{fig:eval:frameworks:hazelcastjet-uc3:parallel-windows}, Hazelcast Jet also scales approximately linearly in this case.
In contrast to scaling with the number of sensors (see \cref{fig:eval:frameworks:base:uc3}), Kafka Streams and Flink scale now with about the same rate of resource demand increase.
Similar to the results shown \cref{fig:eval:frameworks:base-low:uc3}, Samza's resource demand increases less steeply compared to Beam's Flink runner.

\begin{tcolorbox}
	\textbf{\rqwindows:}
	For the studied case, scaling the computational work performed inside the event-driven microservice shows similar results compared to scaling the load intensity the microservice is subject to.
\end{tcolorbox}

\subsection{Scaling on a Single Node}\label{sec:experimental-results:vertical}

\begin{figure*}[tb]
	\captionsetup[subfigure]{
		skip=4pt
	}
	\centering
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_beamflink_uc1.pdf}
		\caption{Beam/Flink UC1}
		\label{fig:eval:frameworks:vertical:beamflink-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_beamflink_uc2.pdf}
		\caption{Beam/Flink UC2}
		\label{fig:eval:frameworks:vertical:beamflink-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_beamflink_uc3.pdf}
		\caption{Beam/Flink UC3}
		\label{fig:eval:frameworks:vertical:beamflink-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_beamflink_uc4.pdf}
		\caption{Beam/Flink UC4}
		\label{fig:eval:frameworks:vertical:beamflink-uc4}
	\end{subfigure}
	\vspace{0.5em}
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_beamsamza_uc1.pdf}
		\caption{Beam/Samza UC1}
		\label{fig:eval:frameworks:vertical:beamsamza-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_beamsamza_uc2.pdf}
		\caption{Beam/Samza UC2}
		\label{fig:eval:frameworks:vertical:beamsamza-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_beamsamza_uc3.pdf}
		\caption{Beam/Samza UC3}
		\label{fig:eval:frameworks:vertical:beamsamza-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_beamsamza_uc4.pdf}
		\caption{Beam/Samza UC4}
		\label{fig:eval:frameworks:vertical:beamsamza-uc4}
	\end{subfigure}
	\vspace{0.5em}
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_flink_uc1.pdf}
		\caption{Flink UC1}
		\label{fig:eval:frameworks:vertical:flink-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_flink_uc2.pdf}
		\caption{Flink UC2}
		\label{fig:eval:frameworks:vertical:flink-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_flink_uc3.pdf}
		\caption{Flink UC3}
		\label{fig:eval:frameworks:vertical:flink-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_flink_uc4.pdf}
		\caption{Flink UC4}
		\label{fig:eval:frameworks:vertical:flink-uc4}
	\end{subfigure}
	\vspace{0.5em}
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_hazelcastjet_uc1.pdf}
		\caption{Hazelcast Jet UC1}
		\label{fig:eval:frameworks:vertical:hazelcastjet-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_hazelcastjet_uc2.pdf}
		\caption{Hazelcast Jet UC2}
		\label{fig:eval:frameworks:vertical:hazelcastjet-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_hazelcastjet_uc3.pdf}
		\caption{Hazelcast Jet UC3}
		\label{fig:eval:frameworks:vertical:hazelcastjet-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_hazelcastjet_uc4.pdf}
		\caption{Hazelcast Jet UC4}
		\label{fig:eval:frameworks:vertical:hazelcastjet-uc4}
	\end{subfigure}
	\vspace{0.5em}
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_kstreams_uc1.pdf}
		\caption{Kafka Streams UC1}
		\label{fig:eval:frameworks:vertical:kstreams-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_kstreams_uc2.pdf}
		\caption{Kafka Streams UC2}
		\label{fig:eval:frameworks:vertical:kstreams-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_kstreams_uc3.pdf}
		\caption{Kafka Streams UC3}
		\label{fig:eval:frameworks:vertical:kstreams-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/vertical_kstreams_uc4.pdf}
		\caption{Kafka Streams UC4}
		\label{fig:eval:frameworks:vertical:kstreams-uc4}
	\end{subfigure}
	\caption{Scalability of stream processing frameworks on a single node.}
	\label{fig:eval:frameworks:vertical}
\end{figure*}

In this section, we address \rqvertical\ and evaluate whether vertical scaling can be a viable alternative to horizontal scaling for stream processing frameworks. We, therefore, scale our SUTs on a single node with both the number of instances and the amount of resources provided for a single SUT pod.

For the experiments of this section, we slightly modify our experimental setup (see \cref{tab:clouds}). We only deploy 3~Kafka brokers, which run on three different Kubernetes nodes. Of the other two nodes, we dedicate one to run the load generators and one to run the SUT instances.
Although we run only 3~Kafka brokers and all load generator instances on the same node, we can confirm that the configured load is still successfully generated.

To not fully utilize the SUT node, which also runs some infrastructure and monitoring components, we deploy up to 20 instances with one CPU core each or up to 20~CPU cores for a single instance. For scaling with the number of instances, we keep the same configuration as in the previous experiments. For scaling with the resources per pod, we only refer to the number of CPU cores, but scale memory proportionally. (However, in all our experiments we never observed fully utilized pod memory.)
In order to utilize multiple CPU cores, most stream processing frameworks have to be configured accordingly. For Flink, we scale the number of task slots of the TaskManager equally to the number of CPU cores. In Kafka Streams, we equally scale the number of threads to the number of CPU cores. For Samza, the documentation is inconsistent regarding scaling a standalone application on a single node. We decided to equally scale the \textit{container thread pool} size to the number of CPU cores. Hazelcast Jet does not require additional configuration as an instance configures its cooperative thread pool automatically according to the number of CPU cores provided.

For most experiments, we generate the same load intensities as in the first experiment. However, we use the smaller load intensities from \cref{sec:experimental-results:beam-config} for the Beam experiments and the 30~days window for Hazelcast Jet with benchmark UC3 as introduced in \cref{sec:experimental-results:windows}. %


\cref{fig:eval:frameworks:vertical} shows the results of our experiments with a single node.
Almost all frameworks show approximately linear scalability when scaling the number of instances.
We observe that Beam with the Samza runner does not scale with increasing the CPU resources per pod. Hence, we assume that scaling the container thread pool is not the right option to increase capacity on a single node. Whether other configuration options exist remains unclear.
Further, we can observe that no framework is able to process load intensities higher than 200\,000 messages per second with a single instance for benchmark UC1.
The reason for this is that we simulate database writes by printing all incoming records to the standard output stream. Running a single instance of the frameworks causes all threads to write to the same stream, which is synchronized and becomes the bottleneck of our evaluation. 

Kafka Streams seems to be more efficient when scaling with the number of instances, compared to scaling with the number of cores.
For Flink and Hazelcast Jet, scaling with the number of cores is more efficient for the more complex dataflows in UC3 and UC4, while with benchmark UC2 both types of scaling yield similar results.
Remarkable are the results for scaling with the number of cores with Beam and the Flink runner. In contrast to native Flink, Beam with the Flink runner seems not to be scalable with respect to the number of cores.
Moreover, the Kafka Streams implementation of benchmark UC3 scales with neither increasing the number of instances nor with increasing the number of cores. As it scales linearly when running on multiple nodes (see \cref{fig:eval:frameworks:base-low:uc3}), our results suggest that underlying hardware resources become exhausted. However, from manually observing system-level metrics, we cannot observe anything conspicuous.






\begin{figure*}[tb]
	\captionsetup[subfigure]{
		skip=4pt
	}
	\centering
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_beamflink_uc1.pdf}
		\caption{Beam/Flink UC1}
		\label{fig:eval:frameworks:public-private:beamflink-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_beamflink_uc2.pdf}
		\caption{Beam/Flink UC2}
		\label{fig:eval:frameworks:public-private:beamflink-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_beamflink_uc3.pdf}
		\caption{Beam/Flink UC3}
		\label{fig:eval:frameworks:public-private:beamflink-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_beamflink_uc4.pdf}
		\caption{Beam/Flink UC4}
		\label{fig:eval:frameworks:public-private:beamflink-uc4}
	\end{subfigure}
	\vspace{0.5em}
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_beamsamza_uc1.pdf}
		\caption{Beam/Samza UC1}
		\label{fig:eval:frameworks:public-private:beamsamza-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_beamsamza_uc2.pdf}
		\caption{Beam/Samza UC2}
		\label{fig:eval:frameworks:public-private:beamsamza-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_beamsamza_uc3.pdf}
		\caption{Beam/Samza UC3}
		\label{fig:eval:frameworks:public-private:beamsamza-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_beamsamza_uc4.pdf}
		\caption{Beam/Samza UC4}
		\label{fig:eval:frameworks:public-private:beamsamza-uc4}
	\end{subfigure}
	\vspace{0.5em}
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_flink_uc1.pdf}
		\caption{Flink UC1}
		\label{fig:eval:frameworks:public-private:flink-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_flink_uc2.pdf}
		\caption{Flink UC2}
		\label{fig:eval:frameworks:public-private:flink-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_flink_uc3.pdf}
		\caption{Flink UC3}
		\label{fig:eval:frameworks:public-private:flink-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_flink_uc4.pdf}
		\caption{Flink UC4}
		\label{fig:eval:frameworks:public-private:flink-uc4}
	\end{subfigure}
	\vspace{0.5em}
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_hazelcastjet_uc1.pdf}
		\caption{Hazelcast Jet UC1}
		\label{fig:eval:frameworks:public-private:hazelcastjet-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_hazelcastjet_uc2.pdf}
		\caption{Hazelcast Jet UC2}
		\label{fig:eval:frameworks:public-private:hazelcastjet-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_hazelcastjet_uc3.pdf}
		\caption{Hazelcast Jet UC3}
		\label{fig:eval:frameworks:public-private:hazelcastjet-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_hazelcastjet_uc4.pdf}
		\caption{Hazelcast Jet UC4}
		\label{fig:eval:frameworks:public-private:hazelcastjet-uc4}
	\end{subfigure}
	\vspace{0.5em}
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_kstreams_uc1.pdf}
		\caption{Kafka Streams UC1}
		\label{fig:eval:frameworks:public-private:kstreams-uc1}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_kstreams_uc2.pdf}
		\caption{Kafka Streams UC2}
		\label{fig:eval:frameworks:public-private:kstreams-uc2}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_kstreams_uc3.pdf}
		\caption{Kafka Streams UC3}
		\label{fig:eval:frameworks:public-private:kstreams-uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.246\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/public_private_kstreams_uc4.pdf}
		\caption{Kafka Streams UC4}
		\label{fig:eval:frameworks:public-private:kstreams-uc4}
	\end{subfigure}
	\caption{Scalability of stream processing frameworks in our private cloud (SPEL) compared to the Google cloud (GCP).}
	\label{fig:eval:frameworks:public-private}
\end{figure*}

\begin{tcolorbox}
	\textbf{\rqvertical:}
	Despite some exceptions for specific task samples, stream processing frameworks allow scaling them on a single node by increasing the number of instances.
	For Flink and Hazelcast Jet, it might be more resource efficient to scale with the CPU cores per microservice instance instead of scaling the number of instances.
	However, caution should be exercised when accessing shared resources.
\end{tcolorbox}


\subsection{Comparing Scalability in Public and Private Clouds}\label{sec:experimental-results:public-private}

\cref{sec:experimental-results:base} presented our baseline results of benchmarking scalability in the private cloud Kubernetes cluster. We repeat these experiments in an identically configured Kubernetes cluster in the Google cloud to address \rqpublic\ (see \cref{tab:clouds}).
As in the previous section, %
we generate the same load intensities as in the first experiment, yet using the smaller load intensities from \cref{sec:experimental-results:beam-config} for the Beam experiments and the 30~days window for Hazelcast Jet with benchmark UC3.

\cref{fig:eval:frameworks:public-private} compares the scalability results in the private and in the public cloud for each benchmark and stream processing framework. In general, we can observe that the results for both clouds are similar and all frameworks show linear scalability, independent of the cloud platform. Despite some point outliers (e.g., Flink in UC2 for 300\,000 simulated sensors), the resource demand in the Google cloud increases at a slightly steeper rate. We expect that this is due to the fact that we used Google's general-purpose \textit{E2} virtual machines with potentially less powerful resources.
Our benchmark UC3 execution of Hazelcast stands out as its resource demand increases at a lower rate for the Google cloud. Further experiments would be required to investigate whether this is due to measurement outliers or whether this combination of framework and task sample is special in some way.

\begin{tcolorbox}
	\textbf{\rqpublic:}
	Our previous results apply independently of whether a public or private cloud environment is used.
	In our experiments, resource demand increases steeper in the public cloud, yet we expect this to be due to the specific machine types selected.
\end{tcolorbox}

\subsection{Scaling the Cluster Size}\label{sec:experimental-results:large-cluster}

In the previous experiments, we deployed up to 30~SUT instances since for larger numbers we observed interference of the SUT, load generation, messaging system, and infrastructure components causing unstable results. In this section, we address \rqshift\ and evaluate whether stream processing frameworks can be scaled further when also increasing the underlying computing resources.
To obtain a more stable infrastructure, we modify our experimental setup in this section (see \cref{tab:clouds}).
We compose our Kubernetes cluster of two node groups in Google Cloud:
The first consists of four 16-core machines, which run the load generators, four Kafka brokers, and additional benchmarking infrastructure.
The second node group only runs the SUTs. We evaluate two sizes of this node group, namely four and eight 16-core machines.
As we observed linear scalability for all frameworks and independent of the benchmark, we focus on benchmark UC3 in these experiments. We simulate up to 1\,000\,000 sensors for Flink, Hazelcast Jet, and Kafka Streams and up to 100\,000 sensors for the Beam SUTs.
For the cluster with four SUT nodes, we deploy up to 55~instances, while for the cluster with eight SUT nodes, we deploy up to 110~instances. For all frameworks, we increase the number of instances in steps of five.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/large_cluster_beamflink_uc3.pdf}
		\caption{Beam/Flink}
		\label{fig:experimental-results:large-cluster:beam_flink_uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/large_cluster_beamsamza_uc3.pdf}
		\caption{Beam/Samza}
		\label{fig:experimental-results:large-cluster:beam_samza_uc3}
	\end{subfigure}
	
	\vspace{1em}
	
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/large_cluster_flink_uc3.pdf}
		\caption{Flink}
		\label{fig:experimental-results:large-cluster:flink_uc3}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/large_cluster_hazelcastjet_uc3.pdf}
		\caption{Hazelcast Jet}
		\label{fig:experimental-results:large-cluster:hazelcastjet_uc3}
	\end{subfigure}
	
	\vspace{1em}
	
	\begin{subfigure}[b]{0.495\linewidth}%
		\centering
		\includegraphics[width=\textwidth]{img/large_cluster_kstreams_uc3.pdf}
		\caption{Kafka Streams}
		\label{fig:experimental-results:large-cluster:kstreams_uc3}
	\end{subfigure}
	\caption{Scalability benchmark results for benchmark UC3 in Google Cloud for different cluster sizes (machines$\times$cores).}
	\label{fig:experimental-results:large-cluster}
\end{figure}


\cref{fig:experimental-results:large-cluster} shows our scalability benchmark results for larger clusters. For Beam/Flink, Flink, and Kafka Streams, we can observe that the maximum processable load increases when using a cluster of eight nodes instead of four nodes. For Hazelcast Jet, already the four-node cluster is able to process the highest generated load. For Beam with the Samza runner, only 20\,000 messages per second can be processed, independent of the cluster size.
We observe frequent crashes with large Beam/Samza deployments, but due to the many issues observed in our Samza experiments, do not further investigate this.
An important observation is that for all frameworks, fewer SUT instances are required when using the larger Kubernetes cluster.

\begin{tcolorbox}
	\textbf{\rqshift:}
	Observed scalability limits seem to be caused by utilized hardware and not by the streaming frameworks themselves. Hence, limits can be shifted by using larger clusters.
\end{tcolorbox}


