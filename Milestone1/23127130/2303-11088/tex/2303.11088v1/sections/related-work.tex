
\section{Related Work}\label{sec:related-work}



Over the last few years, several benchmarks for stream processing frameworks have been proposed and stream processing benchmarking studies have been conducted. The differentiation between benchmarks and experimental studies applying them is sometimes blurry. Many publications that present benchmarks perform also an experimental study with them. On the other hand, many experimental studies utilize existing benchmarks, but modify them.
Nevertheless, we structure this section into two parts: First, we give an overview of stream processing benchmarks to justify our benchmark selection for this study. Second, we discuss related stream processing benchmarking studies.

\subsection{Related Work on Stream Processing Benchmarks}

Besides the Theodolite benchmarks for event-driven microservices used in this study, several other benchmarks for stream processing frameworks have been proposed.
\cref{tab:related-benchmarks} summarizes characteristics of the discussed benchmarks. 


\begin{table*}
	\begin{threeparttable}[b]
		\caption{Overview of the characteristics and implementations of stream processing benchmarks.}
		\label{tab:related-benchmarks}
		\footnotesize
		\newcommand{\cmark}{\ding{51}}%
		\newcommand{\xmark}{\ding{55}}%
		\newcommand{\qmark}{\makebox[0pt][l]{\textbf{\textit{?}}}\phantom{\cmark}}%
		
		\newcommand{\txnote}[1]{\makebox[0pt][l]{\tnote{#1}}}
		
		\newcommand\undefcolumntype[1]{\expandafter\let\csname NC@find@#1\endcsname\relax}
		\newcommand\forcenewcolumntype[1]{\undefcolumntype{#1}\newcolumntype{#1}}
		
		\newcommand*\rot{\rotatebox{90}}
		\newcolumntype{L}{>{\raggedright\arraybackslash}X}
		\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
		\newcolumntype{C}{>{\centering\arraybackslash}X}
		\newcolumntype{o}{p{0pt}}
		\renewcommand{\arraystretch}{1.2}
		\newcommand{\fnoptional}{a}
		\newcommand{\fnbeam}{b}
		\newcommand{\fnriottasksamples}{d}
		\newcommand{\fnbeamnexmark}{c}
		\begin{tabularx}{\textwidth}{ll o C o C o CCC o CCCCCCC o C o CCC}
			\toprule
			&&& && && \multicolumn{3}{c}{Messaging} && \multicolumn{7}{c}{Stream processing framework} && && \multicolumn{3}{c}{Cloud-native} \\
			\cmidrule{8-10} \cmidrule{12-18} \cmidrule{22-24}
			Benchmark & Published && \rot{Task samples} && \rot{Open source} && \rot{Kafka} & \rot{Others} & \rot{None} && \rot{Flink} & \rot{Spark} & \rot{Storm} & \rot{Samza} & \rot{Kafka Streams} & \rot{Hazelcast Jet} & \rot{Others} && \rot{Database} && \rot{Containers} & \rot{Kubernetes} & \rot{Others} \\
			\midrule
			Theodolite \cite{BDR2021} & \citeyear{BDR2021}
			& %
			& 4
			& %
			& \cmark %
			& %
			& \cmark %
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& \cmark\txnote{\fnbeam} %
			& \cmark %
			& \cmark %
			& \cmark\txnote{\fnbeam} %
			& %
			& \phantom{\cmark}\txnote{\fnoptional} %
			& %
			& \cmark %
			& \cmark %
			& %
			\\
			Beam Nexmark \cite{BeamNexmark2022} & \citeyear{BeamNexmark2022}\txnote{\fnbeamnexmark}
			& %
			& 13
			& %
			& \cmark %
			& %
			& \cmark %
			& \cmark %
			& %
			& %
			& \cmark\txnote{\fnbeam} %
			& \cmark\txnote{\fnbeam} %
			& %
			& \qmark\txnote{\fnbeam} %
			& %
			& \qmark\txnote{\fnbeam} %
			& \cmark\txnote{\fnbeam} %
			& %
			& %
			& %
			& %
			& %
			& %
			\\
			ESPBench \cite{Hesse2021} & \citeyear{Hesse2021}
			& %
			& 5
			& %
			& \cmark %
			& %
			& \cmark %
			& %
			& %
			& %
			& \cmark\txnote{\fnbeam} %
			& \cmark\txnote{\fnbeam} %
			& %
			& \qmark\txnote{\fnbeam} %
			& %
			& \cmark\txnote{\fnbeam} %
			& \qmark\txnote{\fnbeam} %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			\\
			OSPBench \cite{vanDongen2020} & \citeyear{vanDongen2020}
			& %
			& 5
			& %
			& \cmark %
			& %
			& \cmark %
			& %
			& %
			& %
			& \cmark %
			& \cmark %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& \cmark %
			& %
			& \cmark %
			\\
			DSPBench \cite{Bordin2020} & \citeyear{Bordin2020}
			& %
			& 5
			& %
			& \cmark %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			& \cmark %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			\\
			\citet{Shahverdi2019} & \citeyear{Shahverdi2019}
			& %
			& 1
			& %
			& \cmark %
			& %
			& \cmark %
			& %
			& %
			& %
			& \cmark %
			& \cmark %
			& \cmark %
			& %
			& \cmark %
			& \cmark %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			\\
			\citet{Karimov2018} & \citeyear{Karimov2018}
			& %
			& 2
			& %
			& %
			& %
			& %
			& %
			& \cmark %
			& %
			& \cmark %
			& \cmark %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			\\
			RIoTBench \cite{Shukla2017} & \citeyear{Shukla2017}
			& %
			& 4\txnote{\fnriottasksamples} %
			& %
			& \cmark %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			\\
			YSB \cite{Chintapalli2016} & \citeyear{Chintapalli2016}
			& %
			& 1
			& %
			& \cmark %
			& %
			& \cmark %
			& %
			& %
			& %
			& \cmark %
			& \cmark %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			\\
			SparkBench \cite{Li2015} & \citeyear{Li2015}
			& %
			& 10
			& %
			& \cmark %
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			\\
			StreamBench \cite{Lu2014} & \citeyear{Lu2014}
			& %
			& 7
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			& \cmark %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			\\
			Linear Road \cite{Arasu2004} & \citeyear{Arasu2004}
			& %
			& 5
			& %
			& %
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& %
			& %
			& \cmark %
			& %
			& %
			& %
			& %
			& %
			& %
			\\
			\bottomrule
		\end{tabularx}
		\begin{tablenotes}\footnotesize
			\item[\fnoptional] optional
			\item[\fnbeam] using Apache Beam
			\item[\fnbeamnexmark] the Beam Nexmark benchmarks are based on the Nexmark paper \cite{Tucker2010} published in \citeyear{Tucker2010}
			\item[\fnriottasksamples] RIoTBench's 4 application benchmarks are composed of 27 microbenchmarks
		\end{tablenotes}
	\end{threeparttable}
\end{table*}



StreamBench~\cite{Lu2014} is one of the earliest benchmarks for modern stream processing frameworks. While originally only implemented for Spark and Storm, it has later been used to benchmark Apache Apex, Beam, Flink, and Samza as well \cite{Hesse2019, Qian2016}.
As its name suggests, SparkBench~\cite{Li2015} is a benchmark tailored to Apache Spark.
The Yahoo Streaming Benchmark (YSB) \cite{Chintapalli2016} is frequently used and adapted in research \cite{Lopez2016, Yang2017, Karakaya2017, Nasiri2019, Zeuch2019, Chu2020, vanDongen2020}.
Worth highlighting is the work of \citet{Shahverdi2019}, who extend YSB with implementations for the frameworks Kafka Streams and Hazelcast Jet. As discussed in \cref{sec:frameworks}, these frameworks are particularly suited for building event-driven microservices.
RIoTBench \cite{Shukla2017} provides four application benchmarks for Storm composed of 27~small task samples. \citet{Nasiri2019} adopt RIoTBench for Flink and Spark.
\citet{Karimov2018} present a benchmark with two task samples, derived from a real industrial context, yet without providing open-source implementations.

More recently, DSPBench \cite{Bordin2020}, OSPBench~\cite{vanDongen2020}, and ESPBench \cite{Hesse2021} have been proposed.
DSPBench contains 15~benchmarks, which resample typical stream processing applications, derived from reviewing the literature.
OSPBench provides benchmarks for analyzing traffic sensor data. Besides evaluations of latency, throughput, and resource usage, \citeauthor{vanDongen2020} used OSPBench to also evaluate scalability~\cite{vanDongen2021b} and fault recovery~\cite{vanDongen2021a}.
In contrast to most other benchmarks, OSPBench provides implementations for the rather new framework Kafka Streams, which is also evaluated in this study.
The Enterprise Stream Processing Benchmark (ESPBench) builds upon the Senska benchmark \cite{Hesse2018}.
It is special in the sense that it integrates a relational database management system.
In contrast to most other benchmarks, ESPBench's task samples are implemented with Apache Beam. While \citet{Hesse2021} only perform evaluations with Spark, Flink, and Hazelcast Jet, we expect that also other Beam runners can be used to run the benchmark.

The Nexmark benchmark \cite{Tucker2010} has originally been proposed as the \textit{Niagara Extension to the XMark benchmark} addressed to first-generation stream processing systems (see the survey of \citet{Fragkoulis2023} for a discussion of first and second-generation stream processing systems).
The Apache Beam community adapted and extended Nexmark with implementations for Beam to benchmark the performance of different runners~\cite{BeamNexmark2022}.
Documentation and benchmark results are provided for the direct runner as well as for the Flink, the Spark, and the Google Cloud Dataflow runners.
However, running the benchmark with other runners should be possible as well.
Recently, there seems to be an effort to implement the Nexmark task samples with other frameworks in an open-source project.\footnote{\url{https://github.com/nexmark/nexmark}}
However, currently this project only provides implementations for Apache Flink.
Moreover, \citet{Gencer2021} implemented the Nexmark benchmark for their performance evaluation of Hazelcast Jet.

Worth mentioning is also the Linear Road benchmark presented by \citet{Arasu2004}. Although published years before all modern stream processing frameworks considered in this work have been released, it is still used in research \cite{Zhang2017,Zeuch2019,Sax2020} and compared to newer benchmarks \cite{Bordin2020,Hesse2021}.
\citet{Pagliari2020} and \citet{Garcia2022a, Garcia2022b} present approaches to generate benchmarks.






From \cref{tab:related-benchmarks}, we can see that a lot of open-source benchmarks have been proposed. Apart from the Theodolite benchmarks, none of these benchmarks is particularly addressed to scalability.
Often originating in data management research, many benchmarks are defined as ``queries'' over data streams~\cite{Tucker2010,Karimov2018,Hesse2021}.
Most benchmarks include a messaging system as a middleware component between workload generation and stream processing framework. In the vast majority of cases, this is Apache Kafka.
\citet{Karimov2018} exclude such a system to not let it become the benchmark's bottleneck. Our Theodolite benchmarks purposely include Kafka to represent more realistic event-driven microservice deployments~\cite{BDR2021}.
Flink, Spark, and Storm are by far the most supported frameworks. Only a few benchmarks exist for Samza, Kafka Streams, and Hazelcast Jet, which are frameworks particularly suited for implementing event-driven microservice. Our Theodolite benchmarks are the only ones providing implementations for all of them.
While some benchmarks include an interaction with a database in their setup, others do not.
With the Theodolite benchmarks, a database can optionally be used as we did in a previous study~\cite{IC2E2022FaaSStreaming}.
Besides our Theodolite benchmarks, there is only one other benchmark (OSPBench) that is provided as container images to be used in a cloud-native setting. No other benchmark provides Kubernetes manifests.





\subsection{Related Work on Stream Processing Benchmarking}


\begin{table*}
	\begin{threeparttable}[b]
		\caption{Overview of employed benchmarks, frameworks, and experimental setup of stream processing benchmarking studies.}
		\label{tab:related-experiments}
		\footnotesize
		\newcommand{\cmark}{\ding{51}}%
		\newcommand{\xmark}{\ding{55}}%
		\newcommand{\qmark}{\makebox[0pt][l]{\textbf{\textit{?}}}\phantom{\cmark}}%
		
		\newcommand{\txnote}[1]{\makebox[0pt][l]{\tnote{#1}}}
		
		\newcommand\undefcolumntype[1]{\expandafter\let\csname NC@find@#1\endcsname\relax}
		\newcommand\forcenewcolumntype[1]{\undefcolumntype{#1}\newcolumntype{#1}}
		
		\newcommand*\rot{\rotatebox{90}}
		\newcolumntype{L}{>{\raggedright\arraybackslash}X}
		\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
		\newcolumntype{C}{>{\centering\arraybackslash}X}
		\newcolumntype{o}{p{0pt}}
		\renewcommand{\arraystretch}{1.2}
		\newcommand{\fnvandenpoel}{a}
		\newcommand{\fnbeam}{b}
		\begin{tabularx}{\textwidth}{ll o CCCCCCCCCCCCC o CCCCCCC o CCCCCC}
			\toprule
			&&& \multicolumn{13}{c}{Benchmark} && \multicolumn{7}{c}{Framework} && \multicolumn{6}{c}{Execution} \\
			\cmidrule{4-16} \cmidrule{18-24} \cmidrule{26-31}
			Publication & Year &&
			\rot{Theodolite \cite{BDR2021}} &
			\rot{Beam Nexmark \cite{BeamNexmark2022}} &
			\rot{ESPBench \cite{Hesse2021}} &
			\rot{OSPBench \cite{vanDongen2020}} &
			\rot{DSPBench \cite{Bordin2020}} &
			\rot{\citet{Shahverdi2019}} &
			\rot{\citet{Karimov2018}} &
			\rot{RIoTBench \cite{Shukla2017}} &
			\rot{YSB \cite{Chintapalli2016}} &
			\rot{SparkBench \cite{Li2015}} &
			\rot{StreamBench \cite{Lu2014}} &
			\rot{Linear Road \cite{Arasu2004}} &
			\rot{Others}
			&&
			\rot{Flink} &
			\rot{Spark} &
			\rot{Storm} &
			\rot{Samza} &
			\rot{Kafka Streams} &
			\rot{Hazelcast Jet} &
			\rot{Others}
			&&
			\rot{Cloud environment} &
			\rot{Distributed} &
			\rot{Different resource amounts} &
			\rot{\dots in isolated experiments} &
			\rot{Different load intensities} &
			\rot{\dots in isolated experiments}
			\\
			\midrule
			This work &
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark\txnote{\fnbeam} %
				& \cmark %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
			\\
			\citet{IC2E2022FaaSStreaming} & \citeyear{IC2E2022FaaSStreaming}
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark\txnote{\fnbeam} %
				& %
				& %
				& \cmark\txnote{\fnbeam} %
				& %
				& %
				& \cmark\txnote{\fnbeam} %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
			\\
			\citet{Hesse2021} & \citeyear{Hesse2021}
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark\txnote{\fnbeam} %
				& \cmark\txnote{\fnbeam} %
				& %
				& %
				& %
				& \cmark\txnote{\fnbeam} %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
			\\
			van Dongen\tnote{\fnvandenpoel} \cite{vanDongen2021b} & \citeyear{vanDongen2021b}
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& \cmark %
				& \cmark %
			\\
			van Dongen\tnote{\fnvandenpoel} \cite{vanDongen2021a} & \citeyear{vanDongen2021a}
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& \cmark %
				& %
			\\
			\citet{Bordin2020} & \citeyear{Bordin2020}
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
			\\
			\citet{Chu2020} & \citeyear{Chu2020}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& \cmark %
				& %
				& \cmark %
				& %
				& \cmark %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
			\\
			\citet{Vikash2020} & \citeyear{Vikash2020}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
			\\
			van Dongen\tnote{\fnvandenpoel} \cite{vanDongen2020} & \citeyear{vanDongen2020}
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
			\\
			\citet{Nasiri2019} & \citeyear{Nasiri2019}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
			\\
			\citet{Shahverdi2019} & \citeyear{Shahverdi2019}
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
			\\
			\citet{Zeuch2019} & \citeyear{Zeuch2019}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
			\\
			\citet{Karimov2018} & \citeyear{Karimov2018}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& \cmark %
				& \cmark %
			\\
			\citet{Truong2018} & \citeyear{Truong2018}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
			\\
			\citet{Karakaya2017} & \citeyear{Karakaya2017}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
			\\
			\citet{Shukla2017} & \citeyear{Shukla2017}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
			\\
			\citet{Yang2017} & \citeyear{Yang2017}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& \cmark %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
			\\
			\citet{Chintapalli2016} & \citeyear{Chintapalli2016}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
			\\
			\citet{Lopez2016} & \citeyear{Lopez2016}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& \cmark %
				& \cmark %
			\\
			\citet{Qian2016} & \citeyear{Qian2016}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
			\\
			\citet{Lu2014} & \citeyear{Lu2014}
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& %
				& %
				& %
				& %
				& %
				& %
				& \cmark %
				& \cmark %
				& \cmark %
				& %
				& %
			\\
			\bottomrule
		\end{tabularx}
		\begin{tablenotes}\footnotesize
			\item[\fnvandenpoel] and van den Poel
			\item[\fnbeam] using Apache Beam
		\end{tablenotes}
	\end{threeparttable}
\end{table*}

\cref{tab:related-experiments} provides an overview of experimental performance evaluation and benchmarking studies. It indicates the applied benchmark, the evaluated stream processing, and information regarding the experiment setup and method. The latter includes whether the respective study was performed in a cloud environment, in a distributed fashion with multiple instances of the framework deployed. Moreover, it shows whether the benchmarks have been executed with different resource amounts and different load intensities and whether different resource amounts and load intensities are evaluated in isolated experiments. In previous work, we argued that scalability should be evaluated with isolated experiments for different combinations of load and resources~\cite{LTB2021,EMSE2022}.

We can observe that there is no established stream processing benchmark. Only YSB is used in several studies. However, YSB can be considered a micro-benchmark~\cite{Bermbach2017} and, hence, is less suited to benchmark entire microservices.
Except for the preliminary evaluation of our Theodolite benchmarks~\cite{BDR2021}, there is no benchmarking study addressed to stream processing frameworks employed within microservice architectures.

Flink, Spark, and Strom are by far the most frequently benchmarked frameworks. Kafka Stream, Hazelcast Jet, and Samza, which are particularly suited for implementing event-driven microservices, are only benchmarked in a few studies and there is no study benchmarking all of them.

9 out of 20 studies report on experiments in public or private clouds.
Except for this and our previous study~\cite{IC2E2022FaaSStreaming}, there are no evaluations in Kubernetes.
Likewise, there are no further studies evaluating scalability with a systematic approach as we do in this study. \citet{Vikash2020}, \citet{Nasiri2019}, \citet{Karakaya2017}, and \citet{vanDongen2021b} explicitly evaluate scalability, however, without testing different load intensities against different resource amounts in isolated experiments. \citet{Nasiri2019} conduct independent evaluations of scaling load and computing resources and, thus, address another aspect than our study.
Our previous study~\cite{IC2E2022FaaSStreaming} applies our Theodolite method as well, but benchmarks scalability with respect to costs and is addressed to comparing stream processing deployments against Function-as-a-Service offerings.


