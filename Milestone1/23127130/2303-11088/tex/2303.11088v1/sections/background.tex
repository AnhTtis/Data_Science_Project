
\section{Background}\label{sec:background}




In the following, we first discuss the concepts of event-driven microservice architectures before outlining our Theodolite scalability benchmarking method used for this research.

\subsection{Event-driven Microservice Architectures}

At the time of this research, the topic of event-driven microservices architectures is covered only superficially in scientific literature.
While there are a couple of case studies reporting on event-driven microservices, research is still lacking a systematic evaluation of this new architectural style.
On the other hand, some textbooks for practitioners~\cite{Bellemare2020, Stopford2018} have recently been published, which also serve as references for this work.
Despite the lack of systematic studies, event-driven microservices are named an emerging trend~\cite{Davoudian2020,Fragkoulis2023,KarabeyAksakalli2021} and the need for further research on this topic is recognized~\cite{Katsifodimos2019,Laigner2021}.
While there is no established definition of what constitutes event-driven microservices, the following attributes are common in such architectures and particularly relevant to this study.




\paragraph{Self-contained and loosely coupled}
In microservice architectures, a software system is composed of multiple small services that are built around business capabilities~\cite{Hasselbring2018}. Individual microservices run in their own processes, may use different technology stacks, and communicate via lightweight, fault-tolerant mechanisms over the network.
For event-driven microservices employing distributed stream processing techniques, this means each service can use its own stream processing framework.
In contrast to traditional big data stream processing systems running on top of resource management systems (typically from the Hadoop ecosystem), this also leads to smaller jobs and a single job per stream processing cluster.



\paragraph{Independently scalable}

In contrast to monolithic systems, individual microservices can be scaled independently due to their loose coupling. %
In fact, scalability has been reported as one of the most important drivers for and benefits of adopting microservice architectures in several systematic literature reviews \cite{Pahl2016, Li2021, Kratzke2017, Soldani2018, Laigner2021}, interview studies \cite{Taibi2017, Fritzsch2019, Knoche2019, Laigner2021, Zhou2023}, and experience reports \cite{Balalaie2016b, Hasselbring2017, Bucchiarone2018}.
Typically, horizontal duplication, data partitioning, and function decomposition are distinguished as methods for scaling microservice architectures. 
From an operation's perspective, individual event-driven microservices can be scaled by horizontal duplication. Internally, however, duplicating services leads to data partitioning, meaning that each instance of a service handles only events with certain \textit{keys}. Unless configured explicitly, modern stream processing frameworks internally manage the assignment of keys to instances.



\paragraph{Cloud-native deployment}
Microservice architectures are a pattern, particularly suited for building cloud-native applications~\cite{Balalaie2016,Gannon2017,Pahl2018}.
They are mostly deployed as containers in public or private cloud environments, with Kubernetes~\cite{Burns2016} being the de-facto standard orchestration tool for cloud-native applications~\cite{CNCF2022}.
With Kubernetes, microservice deployments including, for example, resource restrictions or numbers of replicas, are defined purely declaratively.
Scaling event-driven microservices in Kubernetes is therefore quite simple. %


\paragraph{Asynchronous communication via messaging system}

Event-driven microservices employ log-based messaging systems for their communication.
To eventually reach consistency among individual event-driven microservices, the log must be durable, append-only, fault-tolerant, partitioned, and it must support sequential reads \cite{Kleppmann2019}.
Probably the most prominent messaging system fulfilling these properties is Apache Kafka~\cite{Kreps2011,Wang2015}, which is intensively used in industry.\footnote{\url{https://kafka.apache.org/powered-by}}

\paragraph{Adoption of distributed stream processing techniques}





Modern stream processing frameworks are designed to run in a distributed fashion on commodity hardware in order to scale with massive amounts of data~\cite{Fragkoulis2023}. Besides high throughput, these systems focus on low latency, fault tolerance, and coping with out-of-order streams.
Modern stream processing frameworks process data in jobs, where a job is defined as a dataflow graph of processing operators. They can be started with multiple instances (e.g., on different computing nodes, containers, or with multiple threads). For each job, each instance processes only a portion of the data. Whereas isolated processing of data records is not affected by the assignment of data portions to instances, processing that relies on previous data records (e.g., aggregations over time windows) requires the management of state. Similar to the MapReduce programming model, keys are assigned to records and the stream processing frameworks guarantee that all records with the same key are processed by the same instance. Hence, no state synchronization among instances is required. If a processing operator changes the record key and a subsequent operator performs a stateful operation, the stream processing framework splits the dataflow graph into subgraphs, which can be processed independently by different instances.
We refer to the recent surveys of \citet{Fragkoulis2023} and \citet{Margara2022} for detailed information on state-of-the-art stream processing models and patterns.



\subsection{Scalability Benchmarking with Theodolite}

Our study builds upon our Theodolite scalability benchmarking method for cloud-native applications~\cite{EMSE2022}.
According to the ACM SIGSOFT Empirical Standard for benchmarking as a software engineering research method~\cite{Ralph2021,PROPSER2021}, we briefly summarize the quality, metric, measurement method, and task samples used in this study.\footnote{\url{https://acmsigsoft.github.io/EmpiricalStandards/docs/?standard=Benchmarking}}

\paragraph{Quality}
The quality evaluated in this study is scalability. Scalability is defined as ``the ability of [a] system to sustain increasing workloads by making use of additional resources''~\cite{Herbst2013}.
It should not be confused with elasticity, which describes how fast or how precise a system (automatically) adapts to varying workloads~\cite{Herbst2013,Lehrig2015}.

\paragraph{Metric}
The Theodolite scalability benchmarking method provides two alternative scalability metrics, the \textit{resource demand} metric and the \textit{load capacity} metric. In this study, we focus on the \textit{resource demand} metric.
It is a function, mapping the load intensity on a system under test (SUT) to the minimal amount of resources that must be provisioned for the SUT such that the SUT fulfills all specified service-level objectives (SLOs).
In \cref{sec:experimental-setup:method}, we describe how load, resources, and SLOs are defined in this study.

\paragraph{Measurement method}
To measure scalability according to the metric, we chose discrete subsets of the load and resource domains and run isolated performance experiments for different load and resource combinations to assess whether the specified SLOs can be fulfilled.
Experiment duration, warm-up periods, and the number of repetitions are configurable and have to be adjusted to the context. By using appropriate search strategies, not all combinations of load and resources have to be executed.

\paragraph{Task samples}
Our Theodolite benchmarking method is not restricted to specific task samples, but allows also using existing benchmarks for cloud-native applications.
Based on real-world use cases for Industrial Internet of Things analytics~\cite{DIMA2021}, we proposed four task samples of different complexity for stream processing frameworks in previous work~\cite{BDR2021}.
As we discuss in \cref{sec:related-work}, these are the only benchmarks focusing on modern stream processing frameworks deployed as cloud-native, event-driven microservices.
In \cref{sec:experimental-setup:task-samples},
we describe how we configure these task samples for our evaluation.
