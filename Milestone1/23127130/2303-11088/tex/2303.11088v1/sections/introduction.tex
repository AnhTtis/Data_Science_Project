
\section{Introduction}

Over the last decade, microservices became a frequently adopted software architecture pattern for building scalable, cloud-native software systems~\cite{Hasselbring2017, Soldani2018, Fritzsch2019, Knoche2019}.
More recently, a shift toward combining event-driven architectures with microservices can be observed~\cite{Laigner2021}. In such systems, different microservices communicate with each other through asynchronous events, which are sent via scalable messaging systems such as Apache Kafka~\cite{Bellemare2020}. Especially data-intensive applications and big data analytics systems are increasingly designed as event-driven microservices building upon frameworks that process continuous data streams in a scalable manner~\cite{Davoudian2020}.

Combining microservice architectures and large-scale stream processing leads to a new way of operating stream processing frameworks. %
Whereas traditional big data systems rely on a single, heavyweight stream processing platform, %
which runs several data analysis jobs, event-driven microservices run stream processing frameworks embedded as a library.
This allows for choosing a suitable, usually more lightweight stream processing framework for each microservice. From an operational perspective, event-driven microservices are deployed and scaled using established microservice orchestration tooling such as Kubernetes.
Internally, the stream processing frameworks perform the necessary coordination among microservice instances to (re)partition data streams for state locality and, thus, enable scalability.



While there are several open-source stream processing frameworks promoting scalability as a core feature, there is only little empirical research evaluating and comparing their scalability.
However, the need for systematic scalability evaluations has been recognized \cite{vanDongen2020, Hesse2021}.
Benchmarking is a well-established method in software engineering to assess and compare the quality of software systems and services~\cite{PROPSER2021}. As such, it is used both in research and engineering~\cite{Kounev2020} to choose among competing software solutions, to evaluate the quality of new ones, or to assure quality levels over time.

In previous work, we presented and empirically evaluated a scalability benchmarking method for cloud-native applications in general \cite{EMSE2022} and stream processing frameworks in particular \cite{LTB2021}. Further, we presented specific benchmarks %
for stream processing frameworks \cite{BDR2021}.
In this work, we use the presented benchmarking method and the benchmarks to experimentally evaluate the scalability of stream processing frameworks, particularly suited for building event-driven microservices.
Specifically, we address the following research questions:


\begin{description}
	\item[\rqbase] How do different stream processing frameworks compete regarding their scalability?
	\item[\rqbeam] Are the previously discovered performance limitations of Apache Beam's abstraction layer \cite{Hesse2019} still present with more recent framework versions?
	\item[\rqwindows] How do stream processing frameworks scale with increasing computational work?
	\item[\rqvertical] Can vertical scaling be a viable alternative to horizontal scaling?
	\item[\rqpublic] Do scalability results differ between public and private clouds?
	\item[\rqshift] Can scalability limits be shifted by using larger clusters?
\end{description}

We address these research questions by conducting over 460 hours of scalability experiments on Kubernetes clusters in Google Cloud and in a private cloud environment.
We benchmark the frameworks Apache Flink, Apache Kafka Streams, Hazelcast Jet, and Apache Beam with the Flink and the Samza runners, for which we deploy up to 110 simultaneously running instances, which process up to one million messages per second.
We provide a replication package and the collected data of all experiments as supplemental material \cite{ReplicationPackage}, allowing other researchers to repeat and extend our work.



\paragraph{Outline}
The remainder of this work starts by discussing the foundations of event-driven microservices and scalability benchmarking in \cref{sec:background}.
\Cref{sec:frameworks} introduces the stream processing frameworks benchmarked in this study and \cref{sec:clouds} gives an overview of the cloud environments for our experiments.
\Cref{sec:experimental-setup} describes our experimental setup and \cref{sec:experimental-results} presents and discusses the results of our experiments.
Afterward, \cref{sec:threats-to-validity} discusses threats to validity, followed by a discussion of related work in \cref{sec:related-work}.
Finally, \cref{sec:conclusions} concludes this works.