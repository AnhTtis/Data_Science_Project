
\section{Introduction}

Over the last decade, microservices became a frequently adopted software architecture pattern for building scalable, cloud-native software systems~\cite{Hasselbring2017, Soldani2018, Fritzsch2019, Knoche2019}.
More recently, a shift toward adopting distributed stream processing techniques in microservice architectures can be observed~\cite{Laigner2021}. In such systems, different microservices communicate with each other through asynchronous messages, which are sent via scalable messaging systems such as Apache Kafka~\cite{Bellemare2020}. Especially data-intensive applications and big data analytics systems are increasingly designed as microservices building upon frameworks that process continuous data streams in a scalable manner~\cite{Davoudian2020}.

Combining microservice architectures and large-scale stream processing leads to a new way of operating stream processing frameworks. %
Whereas traditional big data systems rely on a single, heavyweight stream processing platform, %
which runs several data analysis jobs, microservice architectures allow running stream processing frameworks inside individual microservices, embedded as a library.
This allows for choosing a suitable, usually more lightweight stream processing framework for each microservice.
From an operational perspective, such microservices are deployed and scaled using established microservice orchestration tooling such as Kubernetes.
Internally, the stream processing frameworks perform the necessary coordination among microservice instances to (re)partition data streams for state locality and, thus, enable scalability.



While there are several open-source stream processing frameworks promoting scalability as a core feature, there is only little empirical research evaluating and comparing their scalability.
However, the need for systematic scalability evaluations has been recognized \cite{vanDongen2020, Hesse2021}.
Benchmarking is a well-established method in software engineering to assess and compare the quality of software systems and services~\cite{PROPSER2021}. As such, it is used both in research and engineering~\cite{Kounev2020} to choose among competing software solutions, to evaluate the quality of new ones, or to assure quality levels over time.

In previous work, we presented and empirically evaluated a scalability benchmarking method for cloud-native applications in general \cite{EMSE2022} and stream processing frameworks in particular \cite{LTB2021}. Further, we presented specific benchmarks %
for stream processing frameworks \cite{BDR2021}.
In this work, we use the presented benchmarking method and the benchmarks to experimentally evaluate the scalability of stream processing frameworks, particularly suited to be used within microservices.
Specifically, we address the following research questions:



\begin{description}
	\item[\rqbase:] \textbf{How do different stream processing frameworks deployed as microservices compete regarding their scalability?}
	Scalability is a main driver for adopting microservices and stream processing-based architectures.
	The choice of a stream processing framework is thus likely to have a crucial impact on the scalability of the overall system.
	\item[\rqbeam:] \textbf{Can the observed performance limitations of Apache Beam's abstraction layer be overcome with recently proposed performance optimization configurations?}
	Our results show that Apache Beam scales with significantly higher resource demands, which has also been reported in related work for an older version of the framework~\cite{Hesse2019}. Since a couple of configuration settings have been proposed for improved performance, we evaluate whether such configurations can improve scalability.
	\item[\rqwindows:] \textbf{How do stream processing frameworks scale with increasing computational work performed inside the microservice?}
	The most common way to assess scalability is by evaluating how a system can handle increasing external load as addressed by \rqbase.
	Likewise, however, it can be important to assess how a framework scales with increasing the complexity of the performed computation, for example, to improve the quality of its results.
	\item[\rqvertical:] \textbf{Can vertical scaling be a complementary measure to achieve scalability of stream processing frameworks?}
	A common way to scale microservices is to increase the number of instances deployed, which may require additional underlying cluster nodes (horizontal scaling). Within the bounds of the individual cluster nodes, microservices can also be scaled by increasing their provided resources, such as CPU cores or memory (vertical scaling). We benchmark both alternatives regarding their scalability on a single large node.
	\item[\rqpublic:] \textbf{Do scalability results differ between public and private clouds?}
	Public clouds and private clouds are both representative environments for operating microservice-based systems. We evaluate whether our scalability benchmark results differ between both environments.
	\item[\rqshift:] \textbf{Can scalability limits be raised by using larger clusters?}
	To investigate whether observed scalability limits are due to the frameworks or just due to high utilization of the underlying cluster resources, we repeat selected experiments with clusters of different sizes.
\end{description}

We address these research questions by conducting over 740~hours of scalability experiments on Kubernetes clusters in Google Cloud and in a private cloud environment.
We benchmark the frameworks Apache Flink, Apache Kafka Streams, Hazelcast Jet, and Apache Beam with the Flink and the Samza runners, for which we deploy up to 110 simultaneously running instances, which process up to one million messages per second.
We provide a replication package and the collected data of all experiments as supplemental material \cite{ReplicationPackage}, allowing other researchers to repeat and extend our work.

\paragraph{Outline}
The remainder of this work starts by discussing the relation of microservices and stream processing as well as the foundations of scalability benchmarking in \cref{sec:background}.
\Cref{sec:frameworks} introduces the stream processing frameworks benchmarked in this study and \cref{sec:clouds} gives an overview of the cloud environments for our experiments.
\Cref{sec:experimental-setup} describes our experimental setup and \cref{sec:experimental-results} presents and discusses the results of our experiments.
Afterward, \cref{sec:threats-to-validity} discusses threats to validity, followed by a discussion of related work in \cref{sec:related-work}.
Finally, \cref{sec:conclusions} concludes this works.
