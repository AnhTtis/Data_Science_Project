
\section{Experimental Setup}\label{sec:experimental-setup}

We run our experiments with our Theodolite scalability benchmarking framework\footnote{\url{https://www.theodolite.rocks/}} (version 0.9).
Theodolite is installed as a Kubernetes Operator inside the Kubernetes clusters and controls the execution of benchmarks according to our scalability benchmarking method~\cite{IC2E2022Demo}.
Unless stated differently, we run 5~Kafka brokers, one on each node, with Kafka version 3.2 and 100~partitions per Kafka topic.
All implementations of the benchmarks and the benchmarking tool itself are available as open-source software.\footnote{\url{https://github.com/cau-se/theodolite}}
In our replication package \cite{ReplicationPackage}, we provide the declarative Theodolite files used for executing the benchmarks and the collected data of all experiments along with analysis scripts, allowing other researchers to repeat and extend our work.
In the following, we summarize the configuration of the benchmarked stream processing frameworks, the selected benchmark task samples, and the benchmarking method.
\Cref{fig:experimental-setup:benchmark-architecture} illustrates our benchmark deployment.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/benchmark-architecture.pdf}
	\caption{Benchmark deployment illustrated for the case of benchmark UC3. Our Theodolite load generator sends messages with a constant, but configurable frequency to the input stream topic in Kafka. From there, our SUT microservice consumes the messages, aggregates them and sends the results to the output stream topic in Kafka. The actual processing inside the SUT microservice is implemented with a stream processing framework. For each evaluated stream processing framework, we implemented dedicated microservice. The implementation of Apache Flink requires an additional coordinating instance running the Flink JobManager. The deployment for the other task samples look similar, however, with different dataflow architectures~\cite{BDR2021} and potentially different input and output streams.}
	\label{fig:experimental-setup:benchmark-architecture}
\end{figure}

\subsection{Configuration of Stream Processing Frameworks}\label{sec:experimental-setup:frameworks}


We benchmark the stream processing frameworks Apache Beam (version 2.35) with the Flink (version 1.13) and the Samza runner (version 1.5), Apache Flink (version 1.13), Hazelcast Jet (version 4.5), and Apache Kafka Streams (version 3.1). For a fair comparison, we evaluate all frameworks with mostly their default configuration.

We enable committing read offsets to Kafka in all frameworks. This allows us to monitor the consumer lag via Kafka metrics, which is required to evaluate our lag trend SLO. %
Enabling offset committing is also often done in production deployments to increase observability.
We set the commit interval to 5~seconds for all frameworks, which is the default configuration of Kafka consumers once offset committing is enabled.
Kafka Streams has a default commit interval of 30~seconds as in Kafka Streams, the commit interval also controls fault tolerance (comparable to the checkpointing interval in other frameworks).
For our experiments with Apache Beam and the Flink runner, we enable the \textit{FasterCopy} option as we further discuss and evaluate in \cref{sec:experimental-results:beam-config}.
Each microservice instance runs as a container in its own Kubernetes Pod with an additional sidecar container, exposing monitoring data specific to the stream processing framework.
Per default, we configure 1~CPU and 4\,GB of memory for each Pod, which is a common ratio of CPU and memory of cloud VMs.
For Flink, we always have one additional Pod running the JobManager (see \cref{fig:experimental-setup:benchmark-architecture}).

\subsection{Configuration of Task Samples}\label{sec:experimental-setup:task-samples}

As benchmark task samples, we use dataflow architectures representing typical use cases for analyzing IIoT sensor data streams.
The task samples consume messages representing simulated power consumption measurements and produce messages with aggregation results.
Our previous publication~\cite{BDR2021} describes our four benchmark task samples, named UC1--UC4, in detail.
Unless otherwise stated, we use the following configuration of our benchmark dataflow architectures.
\begin{itemize}
	\item Benchmark UC1 is configured to write each incoming message as a log statement to the standard output stream to simulate a database write operation (i.e., simulating a side effect in the dataflow architecture).
	\item Benchmark UC2 aggregates incoming messages over tumbling windows of one minute. Any out-of-order records arriving after the window has been closed are discarded. %
	\item Benchmark UC3 aggregates records by their hour of day attribute over a time window of three days with a slide period of one day. That means each incoming record belongs to three time windows. Early results (i.e., before the end of the time window has passed) are emitted every 5~seconds.
	For Kafka Streams, such emission cannot explicitly be configured. However, Kafka Streams continuously forwards aggregation results based on the configured \textit{commit interval} (which is 5~seconds as well).
	\item Benchmark UC4 aggregates incoming messages in nested groups of sensors. In contrast to previous work, we benchmark a simplified version of benchmark UC4, which omits the feedback loop. This allows for better predictability of the message volume and, hence, more comparable results.
\end{itemize}

\subsection{Configuration of the Benchmarking Method}\label{sec:experimental-setup:method}

Our Theodolite benchmarking method assesses scalability regarding a configurable load type, a resource type, and SLOs.

\paragraph{Load type}
If not stated differently, we evaluate scalability in regards to increasing the number of simulated sensors as load type. In benchmark UC4, this is indirectly controlled by increasing the number of nested groups, with each group containing $4$ sub-groups or sensors. This means for $n$ nested groups, we simulate $4^n$ sensors. Each simulated sensor generates one measurement per second.
When addressing \rqwindows, we evaluate scalability in regards to increasing the size of time windows for which data is aggregated.

\paragraph{Resource type}
Unless otherwise stated, we use the number of instances as resource type.
When addressing \rqvertical, we additionally use the CPU and memory resources of a single instance as resource type.

\paragraph{SLOs}
In all benchmark executions, we use an SLO based on our consumer lag trend metric~\cite{BDR2021}. The consumer lag trend describes how many messages are queued in the messaging system, which have not been processed. Our consumer lag trend metric describes the average increase (or decrease)
of the lag per second. It can be measured by monitoring the lag and computing a trend line using linear regression. The slope of this line is the lag trend.
\Cref{fig:lag-trend-example} illustrates the concept of the lag trend.
For our experiments, we consider the SLO to be fulfilled if the lag trend does not increase by more than 1\% of the generated message volume.
For Apache Samza, we set this threshold to 5\% of the generated message volume since we observe a permanent slightly increase independent of the provided resources.

\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.33\linewidth}%
		\centering%
		\includegraphics[width=\textwidth]{img/lag-example_6.pdf}%
		\caption{6 SUT instances}%
	\end{subfigure}%
	\hfill%
	\begin{subfigure}[b]{0.33\linewidth}%
		\centering%
		\includegraphics[width=\textwidth]{img/lag-example_7.pdf}%
		\caption{7 SUT instances}%
	\end{subfigure}%
	\hfill%
	\begin{subfigure}[b]{0.33\linewidth}%
		\centering%
		\includegraphics[width=\textwidth]{img/lag-example_8.pdf}%
		\caption{8 SUT instances}%
	\end{subfigure}
	\caption{Illustration of the consumer lag trend metric for an exemplary benchmark execution (Theodolite's UC3 benchmark implemented with Kafka Streams and 50\,000 messages/second) with different numbers of SUT instances.
		Independent of the number of instances, we can observe a variable lag.
		However, computing a trend line (without considering the measurements from an initial warmup period), reveals that for 6 instances, the number of queued messages will steadily increase over time.
		Providing 7 instances leads to a decrease in queued messages after the warmup period, while 8 instances yields an almost constant trend line.}
	\label{fig:lag-trend-example}
\end{figure*}

In certain cases, we observed that under high load the consumer lag does not substantially increase, but records were discarded due to lateness. In most stream processing frameworks, operations on time windows still accept out-of-order records for a configurable amount of time. If this time has elapsed, records are discarded and not further processed. Thus, records are still consumed from the messaging system, not causing a consumer lag increase, but results become incorrect.
To detect these cases, our benchmarks UC2 and UC4 (which aggregate data in short windows) contain a second SLO, which requires that no more than 1\% of the generated messages are discarded.
For the Apache Beam implementation with the Samza runner, no metrics concerning the number of dropped records are provided. With Beam's Flink runner, these metrics are only unreliably available.\footnote{We asked a corresponding question regarding the metrics of both runners at Beam's mailing list, but did not receive an answer.}
This means that for these two SUTs, we cannot definitely be sure whether the determined resource demand for UC2 and UC4 is sufficient to process all records successfully, but still indicates a lower bound.


\paragraph{Additional configuration}
According to the previous experimental evaluations of our benchmarking method~\cite{EMSE2022},
we run our experiments with benchmark UC1--UC3 for a duration of 5~minutes while considering the first 2~minutes as warm-up period. As benchmark UC4 shows a higher variability in the results, we run its experiments for 10~minutes including a 4-minute warm-up period.
We repeat all our experiments 3~times, which we experimentally observed to be a good trade-off between overall execution time (or costs in the public cloud) and statistical grounding~\cite{EMSE2022}.
We provide a replication package~\cite{ReplicationPackage} allowing to further repeat our experiments.
We quantify scalability with the resource demand metric and use the \textit{linear search} strategy in combination with the \textit{lower bound restriction}~\cite{EMSE2022}.
