
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{rao2018lstm,
  title={LSTM with sentence representations for document-level sentiment classification},
  author={Rao, Guozheng and Huang, Weihang and Feng, Zhiyong and Cong, Qiong},
  journal={Neurocomputing},
  volume={308},
  pages={49--57},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{bujel2021zero,
  title={Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers},
  author={Bujel, Kamil and Yannakoudakis, Helen and Rei, Marek},
  booktitle={Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)},
  pages={195--205},
  year={2021}
}

@inproceedings{byrd2019effect,
  title={What is the effect of importance weighting in deep learning?},
  author={Byrd, Jonathon and Lipton, Zachary},
  booktitle={International Conference on Machine Learning},
  pages={872--881},
  year={2019},
  organization={PMLR}
}

@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT Press}
}

@inproceedings{gers2000recurrent,
  title={Recurrent nets that time and count},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
  volume={3},
  pages={189--194},
  year={2000},
  organization={IEEE}
}

@article{huang2015bidirectional,
  title={Bidirectional LSTM-CRF models for sequence tagging},
  author={Huang, Zhiheng and Xu, Wei and Yu, Kai},
  journal={arXiv preprint arXiv:1508.01991},
  year={2015}
}

@inproceedings{rei2017semi,
    title = "Semi-supervised Multitask Learning for Sequence Labeling",
    author = "Rei, Marek",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1194",
    doi = "10.18653/v1/P17-1194",
    pages = "2121--2130",
    abstract = "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.",
}

@article{zhong2019fine,
  title={Fine-grained sentiment analysis with faithful attention},
  author={Zhong, Ruiqi and Shao, Steven and McKeown, Kathleen},
  journal={arXiv preprint arXiv:1908.06870},
  year={2019}
}

@inproceedings{pruthi2019learning,
    title = "Learning to Deceive with Attention-Based Explanations",
    author = "Pruthi, Danish  and
      Gupta, Mansi  and
      Dhingra, Bhuwan  and
      Neubig, Graham  and
      Lipton, Zachary C.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.432",
    doi = "10.18653/v1/2020.acl-main.432",
    pages = "4782--4793",
    abstract = "Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention{'}s reliability as a tool for auditing algorithms in the context of fairness and accountability.",
}


@article{wachter2017right,
  title={Why a right to explanation of automated decision-making does not exist in the general data protection regulation},
  author={Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
  journal={International Data Privacy Law},
  volume={7},
  number={2},
  pages={76--99},
  year={2017},
  publisher={Oxford University Press}
}

@techreport{bray2014javascript,
  title={The javascript object notation (json) data interchange format},
  author={Bray, Tim},
  year={2014}
}

@inproceedings{pruthi2020weakly,
  title={Weakly-and Semi-supervised Evidence Extraction},
  author={Pruthi, Danish and Dhingra, Bhuwan and Neubig, Graham and Lipton, Zachary C},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3965--3970},
  year={2020}
}

@inproceedings{mosbach2020stability,
  title={On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines},
  author={Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{zaidan2007using,
  title={Using “annotator rationales” to improve machine learning for text categorization},
  author={Zaidan, Omar and Eisner, Jason and Piatko, Christine},
  booktitle={Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics; proceedings of the main conference},
  pages={260--267},
  year={2007}
}

@inproceedings{rei2016compositional,
  title={Compositional Sequence Labeling Models for Error Detection in Learner Writing},
  author={Rei, Marek and Yannakoudakis, Helen},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1181--1191},
  year={2016}
}

@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

@inproceedings{fomicheva2021translation,
  title={Translation Error Detection as Rationale Extraction},
  author={Fomicheva, Marina and Specia, Lucia and Aletras, Nikolaos},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={4148--4159},
  year={2022}
}


@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={China national conference on Chinese computational linguistics},
  pages={194--206},
  year={2019},
  organization={Springer}
}

@inproceedings{loshchilov2017decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{vanrossum1995python,
  title={Python reference manual},
  author={vanRossum, Guido},
  journal={Department of Computer Science [CS]},
  number={R 9525},
  year={1995},
  publisher={CWI}
}

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{hoerl1970ridge,
  title={Ridge regression: Biased estimation for nonorthogonal problems},
  author={Hoerl, Arthur E and Kennard, Robert W},
  journal={Technometrics},
  volume={12},
  number={1},
  pages={55--67},
  year={1970},
  publisher={Taylor \& Francis}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{zhu2021long,
  title={Long-short transformer: Efficient transformers for language and vision},
  author={Zhu, Chen and Ping, Wei and Xiao, Chaowei and Shoeybi, Mohammad and Goldstein, Tom and Anandkumar, Anima and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{yannakoudakis2018developing,
  title={Developing an automated writing placement system for ESL learners},
  author={Yannakoudakis, Helen and Andersen, {\O}istein E and Geranpayeh, Ardeshir and Briscoe, Ted and Nicholls, Diane},
  journal={Applied Measurement in Education},
  volume={31},
  number={3},
  pages={251--267},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{hao2019training,
  title={Training a single AI model can emit as much carbon as five cars in their lifetimes},
  author={Hao, Karen},
  journal={MIT Technology Review},
  year={2019}
}

@misc{gpt2,
  author = {Alec Radford},
  title = {{Better Language Models
and Their Implications}},
  howpublished = "\url{https://openai.com/blog/better-language-models/}",
  year = {2019}, 
  note = "[Online; accessed 27-January-2022]"
}

@article{regulation2016regulation,
  title={Regulation EU 2016/679 of the European Parliament and of the Council of 27 April 2016},
  author={Regulation, General Data Protection},
  journal={Official Journal of the European Union},
  year={2016}
}

@inproceedings{pappagari2019hierarchical,
  title={Hierarchical transformers for long document classification},
  author={Pappagari, Raghavendra and Zelasko, Piotr and Villalba, Jes{\'u}s and Carmiel, Yishay and Dehak, Najim},
  booktitle={2019 IEEE automatic speech recognition and understanding workshop (ASRU)},
  pages={838--844},
  year={2019},
  organization={IEEE}
}

@inproceedings{flachs2020grammatical,
  title={Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses},
  author={Flachs, Simon and Lacroix, Oph{\'e}lie and Yannakoudakis, Helen and Rei, Marek and S{\o}gaard, Anders},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={8467--8478},
  year={2020}
}

@inproceedings{wiegreffe2019attention,
  title={Attention is not not Explanation},
  author={Wiegreffe, Sarah and Pinter, Yuval},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={11--20},
  year={2019}
}

@article{lipton2018mythos,
  title={The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{rudin2018please,
  title={Please stop explaining black box models for high stakes decisions},
  author={Rudin, Cynthia},
  journal={stat},
  volume={1050},
  pages={26},
  year={2018}
}

@article{kindermans2016investigating,
  title={Investigating the influence of noise and distractors on the interpretation of neural networks},
  author={Kindermans, Pieter-Jan and Sch{\"u}tt, Kristof and M{\"u}ller, Klaus-Robert and D{\"a}hne, Sven},
  journal={arXiv preprint arXiv:1611.07270},
  year={2016}
}

@inproceedings{jain2019attention,
  title={Attention is not Explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={3543--3556},
  year={2019}
}

@article{linzen2016assessing,
  title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={521--535},
  year={2016},
  publisher={MIT Press}
}

@inproceedings{aubakirova2016interpreting,
  title={Interpreting Neural Networks to Improve Politeness Comprehension},
  author={Aubakirova, Malika and Bansal, Mohit},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2035--2041},
  year={2016}
}

@article{li2016understanding,
  title={Understanding neural networks through representation erasure},
  author={Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1612.08220},
  year={2016}
}

@inproceedings{feng2018pathologies,
  title={Pathologies of Neural Models Make Interpretations Difficult},
  author={Feng, Shi and Wallace, Eric and Grissom II, Alvin and Iyyer, Mohit and Rodriguez, Pedro and Boyd-Graber, Jordan},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={3719--3728},
  year={2018}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{thorne2019generating,
  title={Generating Token-Level Explanations for Natural Language Inference},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={963--969},
  year={2019}
}

@inproceedings{yannakoudakis2011new,
  title={A new dataset and method for automatically grading ESOL texts},
  author={Yannakoudakis, Helen and Briscoe, Ted and Medlock, Ben},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies},
  pages={180--189},
  year={2011}
}

@book{granger2014computer,
  title={The computer learner corpus: a versatile new source of data for SLA research},
  author={Granger, Sylviane},
  year={2014},
  publisher={Routledge}
}

@article{jaszczur2021sparse,
  title={Sparse is Enough in Scaling Transformers},
  author={Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, {\L}ukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{nawrot2021hierarchical,
  title={Hierarchical Transformers Are More Efficient Language Models},
  author={Nawrot, Piotr and Tworkowski, Szymon and Tyrolski, Micha{\l} and Kaiser, {\L}ukasz and Wu, Yuhuai and Szegedy, Christian and Michalewski, Henryk},
  journal={arXiv preprint arXiv:2110.13711},
  year={2021}
}



@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2018}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{koroteev2021bert,
  title={BERT: A Review of Applications in Natural Language Processing and Understanding},
  author={Koroteev, MV},
  journal={arXiv preprint arXiv:2103.11943},
  year={2021}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Icml},
  year={2010}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}

@inproceedings{ji-etal-2017-nested,
    title = "A Nested Attention Neural Hybrid Model for Grammatical Error Correction",
    author = "Ji, Jianshu  and
      Wang, Qinlong  and
      Toutanova, Kristina  and
      Gong, Yongen  and
      Truong, Steven  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1070",
    doi = "10.18653/v1/P17-1070",
    pages = "753--762",
    abstract = "Grammatical error correction (GEC) systems strive to correct both global errors inword order and usage, and local errors inspelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC.Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset.Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective incorrecting local errors that involve small edits in orthography.",
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{rush2015neural,
  title={A Neural Attention Model for Abstractive Sentence Summarization},
  author={Rush, Alexander M and Chopra, Sumit and Weston, Jason},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages={379--389},
  year={2015}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008}
}

@article{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1693--1701},
  year={2015}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  number={3},
  pages={1045--1048},
  year={2010},
  organization={Makuhari}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@article{graves2005framewise,
  title={Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  author={Graves, Alex and Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={18},
  number={5-6},
  pages={602--610},
  year={2005},
  publisher={Elsevier}
}

@article{greff2016lstm,
  title={LSTM: A search space odyssey},
  author={Greff, Klaus and Srivastava, Rupesh K and Koutn{\'\i}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"u}rgen},
  journal={IEEE transactions on neural networks and learning systems},
  volume={28},
  number={10},
  pages={2222--2232},
  year={2016},
  publisher={IEEE}
}

@inproceedings{rei2018zero,
  title={Zero-Shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens},
  author={Rei, Marek and S{\o}gaard, Anders},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={293--302},
  year={2018}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{atanasova2021diagnostics,
  title={Diagnostics-Guided Explanation Generation},
  author={Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  journal={arXiv preprint arXiv:2109.03756},
  year={2021}
}

@inproceedings{meister2021sparse,
  title={Is Sparse Attention more Interpretable?},
  author={Meister, Clara and Lazov, Stefan and Augenstein, Isabelle and Cotterell, Ryan},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  pages={122--129},
  year={2021}
}


@inproceedings{feucht2021description,
  title={Description-based Label Attention Classifier for Explainable ICD-9 Classification},
  author={Feucht, Malte and Wu, Zhiliang and Althammer, Sophia and Tresp, Volker},
  booktitle={Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)},
  pages={62--66},
  year={2021}
}

@inproceedings{atanasova2020diagnostic,
  title={A Diagnostic Study of Explainability Techniques for Text Classification},
  author={Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={3256--3274},
  year={2020}
}

@article{shi2021corpus,
  title={Corpus-level and concept-based explanations for interpretable document classification},
  author={Shi, Tian and Zhang, Xuchao and Wang, Ping and Reddy, Chandan K},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={16},
  number={3},
  pages={1--17},
  year={2021},
  publisher={ACM New York, NY}
}

@techreport{rnn,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

@inproceedings{aly2021feverous,
  title={FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information},
  author={Aly, Rami and Guo, Zhijiang and Schlichtkrull, Michael Sejr and Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Cocarascu, Oana and Mittal, Arpit},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year={2021}
}

@inproceedings{wiegreffe2021measuring,
  title={Measuring Association Between Labels and Free-Text Rationales},
  author={Wiegreffe, Sarah and Marasovi{\'c}, Ana and Smith, Noah A},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={10266--10284},
  year={2021}
}

@inproceedings{clark2019does,
  title={What Does BERT Look at? An Analysis of BERT’s Attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={276--286},
  year={2019}
}

@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@article{liu2019roberta,
  title={Ro{BERT}a: A robustly optimized {BERT} pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{zhang2019hibert,
  title={HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization},
  author={Zhang, Xingxing and Wei, Furu and Zhou, Ming},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5059--5069},
  year={2019}
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press}
}


@inproceedings{bell2019context,
  title={Context is Key: Grammatical Error Detection with Contextual Word Representations},
  author={Bell, Samuel and Yannakoudakis, Helen and Rei, Marek},
  booktitle={Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={103--115},
  year={2019}
}

@inproceedings{bigbird,
  title={Big Bird: Transformers for Longer Sequences.},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}