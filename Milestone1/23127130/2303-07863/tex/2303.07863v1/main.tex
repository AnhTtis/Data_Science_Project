\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb,tipa}
\usepackage{amsmath, amssymb, mathtools}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{cite}
\usepackage{color}
\usepackage{bm}
 \usepackage{bbding}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{825} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{You Can Ground Earlier than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos}
%A Strong Pipeline for Temporal Sentence Grounding in Compressed Videos
%Query-Aware Motion-Aided Memory-Augmented
\author{
Xiang Fang\textsuperscript{1*} \ \
Daizong Liu\textsuperscript{2*} \ \
Pan Zhou\textsuperscript{1$\dagger$} \ \
Guoshun Nan\textsuperscript{3} \ \
\\
\textsuperscript{1}Hubei Key Laboratory of Distributed System Security, Hubei Engineering Research Center on Big Data \\Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology \\
\textsuperscript{2}Peking University \ \
\textsuperscript{3}Beijing University of Posts and Telecommunications \\
{\tt\small xfang9508@gmail.com  \quad dzliu@stu.pku.edu.cn \quad panzhou@hust.edu.cn \quad nanguo2021@bupt.edu.cn}
}

\maketitle

\begin{abstract}
Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a target moment semantically according to a sentence query.
% An excellent model need to deal with a massive number of videos and queries with state-of-the-art grounding performance.
% In real-word computer vision tasks, we often collect the raw video bit-stream with multiple Group of Pictures (GOP). Each of GOP contains the first intra-frame (I-frame) followed by multiple  predicted frames (P-frames), represented by motion vectors and residuals.
Although previous respectable  works have made decent success, they only focus on high-level visual features extracted from the consecutive decoded frames and fail to handle the compressed videos for query modelling, suffering from insufficient representation capability and significant computational complexity during training and testing.
In this paper, we pose a new setting, compressed-domain TSG, which directly utilizes compressed videos rather than fully-decompressed frames as the visual input.
%To this end, in this paper, we propose a brand-new task, \textit{i.e.}, compressed-domain TSG, which directly utilizes compressed videos rather than fully-decompressed frames as the visual input.
% In this paper, we make the first attempt for the compressed-domain TSG task, and propose a novel plug-and-play Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) pipeline,
%Multi-level Compressed-domain Language-aware Network (MCLN).
To handle the raw video bit-stream input, we propose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) framework,
which extracts and aggregates three kinds of low-level visual features (I-frame, motion vector  and residual features) for effective and efficient grounding.
Particularly, instead of encoding the whole decoded frames like previous works, we capture the appearance representation by only learning the I-frame feature to reduce delay or latency.
Besides, we explore the motion information not only by learning the motion vector feature, but also by exploring the relations of neighboring frames via the residual feature.
In this way, a three-branch spatial-temporal attention layer with an adaptive motion-appearance fusion module is further designed to extract and aggregate both appearance and motion information
% . At last, a query-guided multi-modal integration module is utilized to integrate the visual and textual features
for the final grounding.
% Our proposed MCLN is the first attempt to introduce the video compression technology to the TSG task to improve the effectiveness and efficiency.
Experiments on three challenging datasets
% (ActivityNet Caption, Charades-STA and TACoS)
shows that our TCSF achieves better performance than other state-of-the-art methods with lower complexity.
% Besides, Extensive ablation study demonstrate that our MCLN is model-agnostic and can be directly integrated into other TSG methods without additional computation to improve their performance.
%deal with the target visual dataset based on complete frames, which will spend much time and limit their applications. Also, these methods complete the TSG task at high-level setting (based on the full frame). However, most of our collected videos are encoded for video transmission and decoding these videos still requires extra latency and extensive storage, which further deteriorates computational complexity for feature extraction. In this paper, we directly use encoded videos to obtain their I-, P-, B-frames to learn the visual features. Especially, we propose a Multi-level Compressed-domain Language-aware Network (MCLN) to deal with encoded videos. Particularly, we utilize a pretrained C3D to
\end{abstract}
\vspace{-36pt}

\blfootnote{
\textsuperscript{$*$}Equal contributions. ~~~~\textsuperscript{$\dagger$}Corresponding author.}

\section{Introduction}
\begin{figure}[t!]
\centering
\includegraphics[width=0.48\textwidth]{intro}
\vspace{-14pt}
\caption{(a) Example of the temporal sentence grounding (TSG). (b) Comparison between previous supervised TSG models and our compressed-domain TSG model. Previous models first decode the video into consecutive frames and then feed them into their networks, while our compressed-domain model directly leverages the compressed video as the visual input.}
\vspace{-14pt}
\label{fig:intro}
%\vspace{-10pt}
\end{figure}
As a significant yet challenging computer vision task, temporal sentence grounding (TSG) has drawn increasing attention due to its various applications, such as video understanding \cite{yang2022tubedetr,jiang2022semi,fang2021unbalanced,fang2020v,yang2021generalized,fang2021animc,yang2021semantically,fang2022multi,fang2022hierarchical,yang2020webly,fang2020double,yang2018enhancing,liu2021spatiotemporal,xu2019mhp} and temporal action localization \cite{wang2022negative,su2021stvgbert}. Given a long untrimmed video, the TSG task aims to locate the specific start and end timestamps of a video segment with an activity that semantically corresponds to a given sentence query. As shown in Figure \ref{fig:intro}(a), most of video contents are query-irrelevant, where only a short video segment matches the query. It is substantially more challenging since a well-designed method needs to not only model the complex multi-modal interaction among video and query, but also capture complicated context information for cross-modal semantics alignment.

By treating a video as a sequence of independent frames, most TSG methods \cite{zeng2021multi,tang2021frame,liu2022exploring1,zhang2021multi,liu2021adaptive,liu2021progressively,liu2022memory,cao2020strong,Liu_2021_CVPR,lei2020tvr,zhao2021cascaded,liu2020jointly,liu2022few,fang2022hierarchical,liu2022reducing,liu2022skimming} refer to the fully-supervised setting, where each frame is firstly fully decompressed from a video bit-stream and then manually annotated as query-relevant or query-irrelevant. Despite the decent progress on the grounding performance, these data-hungry methods severely rely on the fully decompression and numerous annotations, which are significantly labor-intensive and time-consuming to obtain from real-word applications. To alleviate this dense reliance to a certain extent, some weakly-supervised works \cite{ZhangLZZH20,2021LoGAN,ChenMLW19,DuanHGW0H18,MithunPR19,MaYKLKY20,LinZZWL20,song2020weakly,zhang2020counterfactual,liu2023hypotheses,fang2022multi} are proposed to only leverage the coarse-grained video-query annotations instead of the fine-grained frame-query annotations. Unfortunately, this weak supervision still requires the fully-decompressed video for visual feature extraction.

%In most of real-world computer vision tasks, the collected videos are compressed.
Based on the above observation, in this paper, we make the first attempt to explore if an effective and efficient TSG model can be learned without the limitation of the fully decompressed video input.
Considering that the real-world video always stored and transmitted in a compressed data format, we explore a more practical but challenging task: compressed-domain TSG, which directly leverages the compressed video instead of obtaining consecutive decoded frames as visual input for grounding.
As shown in the Figure \ref{fig:intro}(b), a compressed video is generally parsed by a stream of Group of successive Pictures (GOPs) and each GOP starts with one intra-frame (I-frame) followed by a variable number of predictive frames (P-frames) \cite{xu2022accelerating,li2022end}. Specifically, the I-frame contains complete RGB information of a video frame, while each P-frame contains a motion vector and a residual. The motion vectors store 2D displacements between I-frame and its neighbor frames, and the residuals store the RGB differences between I-frame and its reconstructed frame calculated by Motion Vectors in the P-frames after motion compensation.
The I-frame can be decoded itself, while these P-frames only store the changes from the previous I-frame by motion vectors and residuals.

Given the compressed video, our main challenge is how to effectively and efficiently extract contextual visual features from the above three low-level visual information for query alignment. Existing TSG works \cite{zeng2021multi,tang2021frame,liu2022exploring1,zhang2021multi,cao2020strong,Liu_2021_CVPR,lei2020tvr,zhao2021cascaded} cannot be applied directly to the compressed video because their video features (\emph{e.g.,} C3D and I3D) can only be extracted if all complete video frames are available after decompression. Moreover, decompressing all the frames will significantly increase computational complexity for feature extraction, leading to extra latency and extensive storage.

%Previous supervised methods cannot be directly utilized to the novel compressed-domain TSG task due to the following limitations:  existing methods first decompress the full video, and encode each frame to extract the visual features, shown in Figure \ref{fig:intro}. Therefore, compressed-domain TSG is still unexplored and more challenging than fully-supervised and weakly-supervised TSG.

To address this challenging task, we propose the first and novel approach for compressed-domain TSG, called Three-branch Compressed-domain Spatial-temporal
Fusion (TCSF).
Given a group of successive picture (GOP) in a compressed video, we first extract the visual features from each I-frame to represent the appearance at its timestamp, and then extract the features of its P-frames to capture the motion information near the I-frame.
In this way, we can model the activity content with above simple I-frame and P-frames instead of using their corresponding consecutive decoded frames.
Specifically, we design  a spatial attention and a temporal attention to integrate the appearance and motion features for activity modelling.
% the channel attention focuses on the fast-motion cases, while the spatial attention is more effective in the slow-motion situations.
To adaptively handle different fast-motion (P-frame guided) or slow-motion (I-frame guided) cases, we further design an adaptive appearance and motion fusion module to integrate the appearance and motion information by learning a balanced weight through a residual module. Finally, a query-guided multi-modal fusion is exploited to integrate the visual and textual features for final grounding.

Our contributions are summarized as follows:
\vspace{-2pt}
\begin{itemize}
    \item We propose a brand-new and challenging task: compressed-domain TSG, which aims to directly leverage the compressed video for TSG. To our best knowledge, we make the first attempt to locate the target segment in the compressed video.
    \item We present a novel pipeline for compressed-domain TSG, which can efficiently and effectively  integrate both appearance and motion information from the low-level visual information in the compressed video.
    \item Extensive experiments on three challenging datasets (ActivityNet Captions, Charades-STA and TACoS) validate the effectiveness and efficiency of our TCSF.
\end{itemize}

\section{Related Works}
\noindent \textbf{Temporal sentence grounding.} Most existing TSG methods are under the fully-supervised setting, where all video-query pairs and precise segment boundaries are manually annotated based on the fully-decompressed video.
% With adequate annotations, their main challenge is how to align multi-modal features to predict the accurate segment boundary.
These methods can be divided into two categories:
1) Proposal-based methods \cite{anne2017localizing,chen2018temporally,zhang2019cross,yuan2019semantic,zhang2019learning,liu2023jointly}: They first pre-define multiple segment proposals and then align these proposals with the query for cross-modal semantic matching based on the similarity. Finally, the best proposal with the highest similarity score is selected as the predicted segment.
Although achieving decent results, these proposal-based methods severely rely on the quality of the segment proposals and are time-consuming.
2) Proposal-free methods \cite{chenrethinking,yuan2019find,mun2020local,zhang2020span,liu2022unsupervised}: They directly regress the start and end boundary frames of the target segment or predict boundary probabilities frame-wisely.
% The predicted segment is obtained through post-processing steps that group or aggregate all frame-wise predictions.
Compared with the proposal-based methods, proposal-free methods are more efficient.
To alleviate the reliance to a certain extent, some state-of-the-art turn to the weakly-supervised setting \cite{ZhangLZZH20,2021LoGAN,ChenMLW19,DuanHGW0H18,MithunPR19,MaYKLKY20,LinZZWL20,song2020weakly,zhang2020counterfactual}, where only video-query pairs are annotated without precise segment boundaries in the fully-decompressed video.

In real-world computer vision tasks, we always collect the compressed video, rather than decompressed consecutive frames. In this paper, we present a brand-new practical yet challenging setting for TSG task, called compressed-domain TSL, with merely compressed video rather than a decompressed frame sequence.

\noindent \textbf{Video compression.} As a fundamental computer vision task, video compression \cite{xu2021detection,lu2022learning,lauzon1996performance,lin2009versatile,lee2006adaptive,wiegand2003overview,schwarz2007overview} divides a video into a group of pictures (GOP), where each frame is coded as an I-, P-, and B- frame. An I-frame is the first frame of the GOP to maintain full RGB pixels as an anchor. The subsequent P-and B-frames are then coded using a block-based motion vector with temporal prediction. The prediction is conducted by searching the closest matching block of a previously coded frame as a reference frame. A vector of the current block to the reference block is determined as a motion vector. Since the current block and the matching block are often different, the transformed residual is used to denote the difference.
%reconstructs a frame sequence using only a few intra-coded frames (I-frames) with complete RGB data and several ingredients for prediction such as residualand a motion vector (MV). In fact, most video content is compressed in advance

%Since I-frame does not contain the temporal prediction information, I-frame is independently decoded. In contrast, after all the reference frames are available,  P- and B-frames can be fully reconstructed. In the worst case, an inter-coded frame can start decoding after all the other frames in the same GOP are fully reconstructed. However, when the MV and residualare partially decoded, they can be collected immediately. Since MV is used to illustrate the difference between the current and the reference block, it can reflect the locally temporal change of a foreground object. The residualcan represent the abrupt changes in RGB values. Thus, in boundaries of fast-moving objects and scene changes, the amount of residualtends to be larger.

Compared with other deep features (\textit{e.g.,} optical flow \cite{ilg2017flownet}) widely used in the TSG task, the compressed-domain features (MVs and residual) have the following advantages:
1) Lower computational costs. The compressed-domain features can be obtained during decoding, while other deep features need to decompress  the compressed video and encode the video by a pretrained heavy-weight model (C3D \cite{tran2015learning} or I3D \cite{carreira2017quo}). The compressed-domain features only
even require partial-frame reconstruction by entropy decoding \cite{zhu2007pop}, inverse transform and quantization \cite{lee2016compressed}, and motion-compensation \cite{divakaran2000video}. In entropy decoding, the most time-consuming process is skipping the motion-compensation \cite{shen2005submacroblock}, whose computational complexity is much smaller than that of other deep features. 2) No delay or dependency. The compressed-domain features can be instantly obtained. When we large-scale datasets, the advantages are more obvious.
%\cite{shen2005submacroblock,shen1999adaptive,shen1998dct}
%When receiving a video package, we can use a ready-made codec method H.264 to decode it into I-frame (intra-coded frame), P-frame (predictive frame), and B-frame (bi-directional frame). We specify these frames into a Group of Pictures (GoP). Concretely, I-frame is decoded as RGB frame, P-frame indicates the previous frame but only retains the differences. B-frame is a special P-frame that refers to the context. The differences stored in the P-frame is represented by motion vector, i.e. $16\times 16$ macroblocks from the source frame to the target frame. We calculate the differences between the source and target frames and generate the residual. Both the I-frame and residual are 3-channel images with the same size as the original video, and the motion vector is the 2-channel horizontal and vertical displacement that has a smaller resolution. Compared to optical flow which depicts the pixel-level motion, motion vector conveys the block-level motion.

%To capture the complementary cues from three compressed modalities, we also exploit the relation between the I-frame and P-frame to decouple the input. Because motion vector and residual are referred to as the previous P-frame or I-frame, they cannot provide full information of the P-frame.

%\noindent \textbf{Memory network.}

\section{Proposed Method}
\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{pipeline.pdf}
\vspace{-20pt}
\caption{Overview of the proposed architecture. Firstly, we  leverage the entropy decoding approach to obtain the compressed video, \textit{i.e.}, I-frames and P-frames (containing motion vectors and residuals). Then, we enrich their information with pseudo features, and develop a three-branch spatial-temporal attention to model the query-related activity content. After that, we fuse the appearance and motion contexts, and integrate them with the query features for learning the joint multi-modal representations. At last, we feed the multi-modal features into the grounding head to predict the segment.}
\vspace{-10pt}
\label{fig:pipeline}
\end{figure*}
\subsection{Overview}
\noindent \textbf{Problem statement.}
Given a video bit-stream $\mathcal{V}$ with $T$ frames, the temporal sentence grounding (TSG) task aims to localize the precise boundary $(\tau_s, \tau_e)$ of a specific segment semantically corresponding to  a given query $\mathcal{Q}=\{q_j\}_{j=1}^M$, where $q_j$ denotes the $j$-th word, $M$ denotes the word number, $\tau_s$ and $\tau_e$ denote the start and end timestamps of the specific segment.
%, $v_t'$ denotes the $t$-th frame, $T$ denotes the frame number.
In our compressed-domain TSG setting, we do not feed the decompressed frames video as input. Instead, we partially decode the video bit-stream at a low cost to extract the compressed video, which includes $N$  group of pictures (GoPs). Each GoP $G_i$ contains one reference I-frame $I_i\in \mathbb{R}^{\mathcal{H}\times\mathcal{W}\times 3}$ followed by $L$ number of P-frames $\{P_i^l\}_{l=1}^L$.
Each $P_i^l$ consists of a motion vector $M_i^l\in \mathbb{R}^{\mathcal{H}\times \mathcal{W} \times 2}$ and a residual $R_i^l\in \mathbb{R}^{\mathcal{H}\times \mathcal{W} \times 3}$, which can be extracted nearly cost-free from $\mathcal{V}$.
For convenience, we assume that all GOPs contain  the same number of P-frames. Thus, $T=N\times(L+1)$.
The video bit-stream can be represented as $\mathcal{V}=\{I_i, P_i^1, P_i^2, \cdots, P_i^{L}\}_{i=1}^N$,
where $i$ denotes the $i$-th GOP.
% $\mathcal{H}$ and $\mathcal{W}$ are the height and width of the video frame, and $P_i^t$ denotes the $t$-th P-frame of the $i$-th GOP. The P-frame $P_i^t$ in the $i$-th GOP is formed by the motion vectors $M_i^h\in \mathbb{R}^{\mathcal{H}\times \mathcal{W} \times 2}$ and residual$R_i^h\in \mathbb{R}^{\mathcal{H}\times \mathcal{W} \times 3}$, which can be extracted nearly cost-free from the video bit-stream $\mathcal{V}$.
% We assume that the compressed video consists of $N$ GOPs, $G_1, G_2, \cdot, G_N$ where each $G_i$ contains one I-frame $I_i\in \mathbb{R}^{\mathcal{H}\times \mathcal{w} \times 3}$ followed by 15 P-frame, where $\mathcal{H}$ and $\mathcal{w}$ are the height and width of the video frame. Based on each P-frame $P_i^h$, we can easily obtain the motion vectors $M_i^h\in \mathbb{R}^{\mathcal{H}\times \mathcal{w} \times 2}$ and residuals $R_i^h\in \mathbb{R}^{\mathcal{H}\times \mathcal{w} \times 3}$, $h \in \{1, 2, \cdot, 15\}$.
Here, the I-frame contains complete RGB information of a video frame and can be decoded itself, while these P-frames only store the changes from the previous I-frame by motion vectors and residuals. The motion vectors store 2D displacements of the most similar patches between I-frame and the target frame, and the residuals store pixel-wise differences to correct motion compensation errors.
We use above three low-level information contained in compressed videos as our visual input.

\noindent \textbf{Pipeline.}
Our pipeline is summarized in Figure~\ref{fig:pipeline}.
Given a video bit-stream, we first utilize the entropy decoding approach \cite{wu2018compressed,wang2019fast} to generate a group of successive pictures (GOP), which consists of several I-frames with their related P-frames.
Then, we extract the visual appearance features
% Given a group of successive picture (GOP) in a compressed video, it extract the full visual appearance features
from I-frames by a pre-trained ResNet-50 network, while a light-weight ResNet-18 network is used to extract the motion vector and residual features from P-frames.
After that, we enrich these partial appearance and motion information with pseudo features to make the complete comprehension of the full video.
A spatial-temporal attention module is further introduced to better model the activity content based on the motion-appearance contexts.
% In particular, the channel attention focuses on the fast-motion cases, while the spatial attention is more effective in the slow-motion situations.
Next, we design an adaptive appearance and motion fusion module to selectively integrate the attentive appearance and motion information guided by the residual information. Finally, we design a query-guided multi-modal fusion module to integrate the visual and textual features for final grounding.

\subsection{Multi-Modal Encoding}
\noindent \textbf{Query encoder.}  Following \cite{gao2017tall}, we first employ the Glove network \cite{pennington2014glove} to embed each word into a dense vector.
Then, a Bi-GRU network \cite{chung2014empirical} and a multi-head self-attention module \cite{vaswani2017attention} are used to further integrate the sequential textual representations. Thus, final word-level features is denote as $Q=\{q_j\}_{j=1}^M\in \mathbb{R}^{M \times d}$, where $d$ is the feature dimension. By concatenating the outputs of the last hidden unit in Bi-GRU with a further linear projection, we can obtain the sentence-level feature as $q_{global} \in \mathbb{R}^{d}$.

% Following previous works \cite{gao2017tall,liu2020jointly,liu2021context}, we first employ the Glove model \cite{pennington2014glove} to embed each word of the given sentence query into a dense vector.
% Then, we use multi-head self-attention \cite{vaswani2017attention} and Bi-GRU \cite{chung2014empirical} modules to encode its sequential information. We denote the final word-level features as $\bm{Q}=\{\bm{q}_n\}_{n=1}^N \in \mathbb{R}^{N \times D}$.
% By concatenating the outputs of the last hidden unit in Bi-GRU with a further linear projection, we can obtain the sentence-level feature as $\bm{q}_{global} \in \mathbb{R}^{D}$.

\noindent \textbf{I-frame encoder.} Following \cite{marpe2010video,li2020efficient},
if the $\{t\}_{t=1^T}$-th frame is I-frame,
% These I-frames are RGB images, which contains the appearance features. These MVs denote the motion information from the source positions in previous frames to the destination positions in current frames, which contains the motion features. Then,
we use a pretrained ResNet-50 model \cite{he2016deep} to extract its appearance feature $a^t\in \mathbb{R}^{H\times W \times C}$, where $H$, $W$ and $C$ denotes dimensions of height, width, and channel.

\noindent \textbf{P-frame encoder.}  Following \cite{shou2019dmc, wu2018compressed},
if the $\{t\}_{t=1^T}$-th frame is P-frame containing a motion vector $M^t$ and a residual $R^t$, we utilize a ResNet-18 network \cite{he2016deep,yangopenood,yang2020webly,yang2022panoptic,yang2016sedmid,yang2022full} to extract the motion vector feature $m^t\in \mathbb{R}^{H\times W \times C}$ and the residual feature $r^t\in \mathbb{R}^{H\times W \times C}$.

%Then, we try to design a time-efficient  network using multi-level compressed-domain video features to improve performance at lower computational complexity.
% \subsection{Enhancing the Visual Features.}
% In real-world applications, the motion vectors often contain much motion information of scenes, and the residualalways record the compensation information. But both of them do not provide the context information
% of the scene appearance, which is only contained in the I-frame. Therefore, we aim to enhance these visual features.

% \noindent \textbf{Updating appearance features.}
% At time $t$, we first concatenate appearance feature $a_i$, motion vector feature $\{m_i^l\}^t$ and motion vector $\{M_i^l\}^t$ together in the channel dimension $v_c=[a_i;\{m_i^l\}^t;\{M_i^l\}^t]$, where $[ ; ]$ is the concatenation operation. Then, a  lightweight RAFT network \cite{teed2020raft} is utilized to compute the following attention weight $A_I^t$ for each feature map of $a_i$:
% \small
% \begin{align}\label{A_I^t}
%     A_I^t= \sigma(W_0 \cdot ReLU(W_1\cdot \text{AvgPool}(\text{RAFT}(v_c))+b_1)+b_0),
% \end{align}\normalsize
% where AvgPool is the average pooling, $\sigma$ is the sigmoid function, $W_0, W_1, b_0,  b_1$ are the learnable weights.
% Thus, the appearance feature $a_i$ can be updated as:
% \small
% \begin{align}\label{hat_a_i^t}
%     \hat{a}_i^t=A_I^t\otimes a_i,
% \end{align}\normalsize
% where $\otimes$ is the channel-wise multiplication. In Equitation \ref{hat_a_i^t}, we can leverage the guidance of the motion vectors to refine the appearance feature in channel dimension.

% \noindent \textbf{Updating motion and residualfeatures.} Similarly, given the appearance feature $a_i$, motion feature $\{m_i^l\}^t$ and motion vector $\{M_i^l\}^t$, we compute the 2D weight map $A_M^t$ as follows:
% \small
% \begin{align}\label{A_M^t}
%     A_M^t= \text{Softmax}(\text{2D-Conv}(\text{RAFT}(v_c))),
% \end{align}\normalsize
% where $A_M^t \in \mathbb{R}^{H \times W}$ is the spatial weight map. Then, we utilize $\hat{a}_i^t$  to update the motion feature as follows:
% \small
% \begin{align}\label{A_M^t}
%     \{\hat{m}_i^l\}^t=\text{AvgPool}(\{M_i^l\}^t)+\sum_{k=1}^{HW}\hat{a}_i^tA_M^t,
% \end{align}\normalsize
% where $k$ enumerates all spatial positions of $\hat{a}_i^tA_M^t$. Similar to the process of updating motion feature $\{m_i^l\}^t$, we can obtain the refined residualfeature $\{\hat{r}_i^l\}^t$.




% At time $t$, we
% Firstly, we introduce the attention weight for each feature map of ${\bf x}_I$ based on the information of the P-frame ${\bf x}_{M}^t$. Specifically, we concatenate I-frame feature ${\bf x}_I$, motion vector feature ${\bf x}_{M}^t$ and motion vectors $M^t$ together in the channel dimension to compute the channel weight $W_\text{cha}^t$ using a lightweight RAFT network \cite{teed2020raft}, \ie,




\noindent \textbf{Pseudo feature generation.}
% In real-world applications, the motion vectors often contain much motion information of scenes, and the residualalways record the compensation information. But both of them do not provide the information of the scene appearance, while the I-frame only contains appearance information without any motion and compensation information.
Since our compressed-domain TSG needs to locate the specific start and end frames of the target segment, we need to obtain the precise motion, compensation and appearance information of each frame for more accurate grounding.
However, in the compressed video, we only have partially $N$-number I-frames of appearance and $(N\times L)$-number P-frames of motion and compensation, lacking enough full-frames (\textit{i.e.}, $T$-number frames) knowledge of the complete appearance-motion information.
Thus, we tend to generate complementary pseudo features for the unseen frames of the video. For example, to warp the appearance feature from the current I-frame,
we can use $M^t$ to estimate the pseudo appearance feature $a^{t+1}$ in its adjacent frame (its next frame).
We can find that the pseudo feature generation approach exempts reconstructing each adjacent frame for feature extraction individually.
%
We assume that the $t$-frame is I-frame.
For constructing the pseudo appearance features of its $n$-th adjacent P-frame, we utilize a block-based motion estimation as:
\begin{align}\label{appear_generate}
a^{n+t}(s)=a^{n+t-1}(\delta M^{n+t-1}(s\delta)+s),
\end{align}
where $a^{n+t}$ denotes the appearance feature of the $n+t$-th P-frame, $s$ is a spatial coordinate of features, and $\delta$ is used as a scaling factor.  By Eq. \eqref{appear_generate}, we can obtain the appearance information of each P-frame based on off-the-shelf I-frames.
%This approach can keep the dimension of the appearance feature, which simplifies the next feature
%the calculation of attention to the given query and avoids repeated feature extraction.

Similarly, we will generate the motion information of each I-frame based on P-frames. Following \cite{feichtenhofer2019slowfast},  we  combine the temporal movement information of appearance features in these adjacent frames.
In the channel axis, we concatenate consecutive $n$ frames $[a^t;\cdots;a^{n+t}]$ as $V^t\in \mathbb{R}^{H \times W\times C\times n}$.
Setting $V_*^t=conv_{1\times 1}(V^t)$, we can get
\begin{align}\label{motion_generate}
m^t=ReLU(V_*^t),
\end{align}
where $m^t$ is the motion feature of $t$-th frame, ReLU is the ReLU function,  and $conv_{1\times 1}$ means $1\times 1$ convolution layer with stride 1, producing a channel dimension of feature $C\times n$ to $C$. Thus, for the $t$-th frame,
 its appearance and motion features are $a^t$ and $m^t$, respectively.
% Although $m_i^t$ does not go through the temporal down-sampling before the mixture in the last convolution layer of Eq.~\eqref{appear_adjacent}, we still can slight modify $m_i^t$ to using more spatial channels based a pre-trained  ResNet-18 network on each $n$ temporal features.
% Since motion vectors and residuals contain less visual information than I-frame, we utilize more light-weight ResNet-18 to extract the corresponding motion feature $m_{i,h}^t$ and the residualfeature $r_{i,h}^t$.

\subsection{Three-branch Spatial-temporal Attention}
In the TSG task, most of regions within a frame are query-irrelevant, where only a few regions are query-relevant.
To automatically learn the discriminative regions relevant to the query, we need to obtain the fine-grained local spatial context.
Besides, the temporal context is also important since we can correlate the region-attentive spatial information in time series for precisely modelling the activity.
%Temporal-spatial information of target segment is crucial for the TSG task.
% However, in our compressed-domain setting, we do not have any decompressed frame to help us understand the temporal-spatial information.
Therefore, we exploit previous-encoded three low-level features (appearance, motion and residual features) to obtain such query-relevant temporal-spatial information by designing a three-branch temporal and spatial attention.

\noindent \textbf{Spatial attention.} We propose the spatial attention to guide the model put more focus on the query-related region of the low-level features. Specifically, in the TSG task, most spatial visual information is noun-relevant. We first utilize the NLP tool spaCy \cite{honnibal2017natural} to parse nouns from the given query. Then, we exploit these nouns to enhance three visual features (appearance, motion and residual features) via an attention mechanism for helping the model learn to pay more attention on the spatial information precisely.
The details of spatial attention is shown in Figure \ref{fig:pipeline}, where we leverage the combination of two 2D convolutional layers with kernel size of $3\times 3$, two RelUs and a 2D convolutional layers with kernel size of $1\times 1$ to obtain the spatial attention map.
Therefore, we can enhance the region-attentive appearance features $a^t$ into $a_*^t$. Similarly, we can also obtain the region-attentive motion feature $m_*^t$ and region-attentive residual features $r_*^t$.
% As shown in Figure \ref{fig:pipeline}, we leverage the combination of two 2D convolutional layers with kernel size of $3\times 3$, two RelUs and a 2D convolutional layers with kernel size of $1\times 1$  to obtain the spatial attention map.
% Finally, the map is normalize by a Sigmoid function. We name the spatial attention module as ``SA''.
% \small
% \begin{align}
% w_s^t(a)=SA(\tilde{a}^t), w_s^t(m)=SA(\tilde{m}^t), w_s^t(r)=SA(\tilde{r}^t).
% \label{w_s}
% \end{align}\normalsize

%where $conv_{3\times 3}^{(1)}$ and $conv_{3\times 3}^{(2)}$ denote two 3×3 convolutional layers, and $V_s$ is the output of Eq.\eqref{channel_out}. For the appearance branch, $V_s=[a^0';a^1';\cdots a^L']$.
%where $W_{3\times 3}^{(1)}\in\mathbb{R}^{H\times H\times 3\times 3}$ and $W_{3\times 3}^{(2)}\in\mathbb{R}^{H\times H\times 3\times 3}$ are the weights of two 3×3 convolutional layers; $b_{3\times 3}^{(1)}\in\mathbb{R}^H$ and $q_{3\times 3}^{(2)}\in\mathbb{R}^H$ are their corresponding biases; the weight $W_{1\times 1}\in\mathbb{R}^{1\times H\times 1\times 1}$ and the bias $b_{1\times 1}\in\mathbb{R}^1$ belong to the $1\times 1$ convolutional layer.

%To further aim emphasis on the informative features and discard the useless ones, we leverage a max pooling layer as follows:
% Based on Eq.\eqref{w_s}, the final feature output is
% \small
% \begin{align}
% a^t'=\tilde{a}^t\odot w_s^t(a), m^t'=\tilde{m}^t\odot w_s^t(m), r^t'=\tilde{r}^t\odot w_s^t(r).
% \label{spatial_out}
% \end{align}\normalsize
%Similarly, we can obtain the final motion output $f_{\hat{m}}^t$ and the final residual output $f_{\hat{r}}^t$.
%evaluate the relevance.

\noindent \textbf{Temporal attention.}
After learning the region-aware spatial information, we further learn to capture their temporal relation to better model the query-relevant activity.
Specifically, we choose $K$ consecutive frames (starting at the $t$-th frame) for extracting their temporal information via a newly proposed temporal attention.
% Since these consecutive features contain much temporal information, we can obtain the temporal information by the temporal attention module.
% Considering that three temporal attention modules are similar,
Here, we take the temporal attention on appearance features for example.
For the appearance features, we first concatenate them as $\mathcal{A}=[a_*^t;\cdots;a'^{t+K-1}]$. To yield the temporal weights $w=[w^1,w^2,\cdots,w^K]\in\mathbb{R}^{K}$ on these consecutive frames, we first leverage a global average pooling along three dimensions $H\times W \times C$ to generate a temporal-wise statistics $S=[s^1,s^2,\cdots,s^K]\in\mathbb{R}^{K}$, where $s^t$ represents the whole temporal information of $w^t$. Then, we utilize the temporal attention module shown in Figure~\ref{fig:pipeline} to generate the temporal weights $w^t$ as:
\begin{align}
w^t=\sigma(W_{F1} \circ ReLU(W_{F2}\circ S+b_2)+b_1),
\label{w_c}
\end{align}
where $W_{FC_1}$ and $W_{FC_2}$ are the weights of two FC layers;  $b_1\in\mathbb{R}^{K}$ and $b_2\in\mathbb{R}^{K}$ are the biases of two FC layers; $\circ$ denotes the convolution operation. Therefore, the final output of the appearance branch is:
\begin{align}\label{channel_out}
f_{\hat{a}}^t=w_ca_*^t.
\end{align}
Similarly, we can obtain the final outputs of the MV and residual branches as: $f_{\hat{m}}^t$ and $f_{\hat{r}}^t$.

% Finally, a Sigmoid activation is applied to generate the temporal attention weights $w$ in which the value of each element is normalized to [0, 1]. The process can be written as:
% \small
% \begin{align}
% w_c=\sigma(W_{FC_1} \circ ReLU(W_{FC_2}\circ S+b_2)+b_1),
% \label{w_c}
% \end{align}\normalsize
% where $W_{FC_1}$ and $W_{FC_2}$ are the weights of two FC layers;  $b_1\in\mathbb{R}^{L+1}$ and $b_2\in\mathbb{R}^{L+1}$ are the biases of two FC layers; $\circ$ denotes the convolution operation. The final output for each channel can be computed by:
% \small
% \begin{align}
% \hat{a}'_i^t=\hat{a}_i^t\odot w_c, \{\hat{m}'_i^l\}^t=\{\hat{m}_i^l\}^t\odot w_c, \{\hat{r}'_i^l\}^t=\{\hat{r}_i^l\}^t\odot w_c,
% \label{channel_out}
% \end{align}\normalsize
% where $\odot$ is the element-wise multiplication. Based on the channel attention, we can extract the temporal information of each channel (I-frame, motion vector and residual).


% For $P_i^l$, we first project the channel dimension of appearance feature $\hat{a}_i^t$ into the same channel dimension of $\{\hat{m}_i^l\}^t$  and $\{\hat{r}_i^l\}^t$. To keep the motion vector information and residualinformation, we concatenate the appearance feature $\hat{a}_i^t$, motion  feature $\{\hat{m}_i^l\}^t$ and residualfeature $\{\hat{r}_i^l\}^t$. The concatenated feature is denoted as $z_i^t=[\hat{a}_i^t; \{\{\hat{m}_i^l\}^t; \{\hat{r}_i^l\}^t]$. Then, we utilize a four-layer dense-connected convolutional network to fully exchange and fuse these three channels:
% \small
% \begin{align}
% z_i^t=[\hat{a}_i^t; \{\{\{\hat{m}_i^l\}^t; \{\hat{r}_i^l\}^t\}_{l=1}^L],
% \label{z_channel}
% \end{align}\normalsize
% Finally, a Sigmoid activation is applied to generate the temporal attention weights $w$ in which the value of each element is normalized to [0, 1]. The process can be written as:
% \small
% \begin{align}
% w_c=\sigma(W_{F1} \circ ReLU(W_{F2}\circ S+b_2)+b_1),
% \label{w_c}
% \end{align}\normalsize
% where $W_{F1}$ and $W_{F2}$ are the weights of two FC layers;  $b_1\in\mathbb{R}^{3+5L}$ and $b_2\in\mathbb{R}^{3+5L}$ are the biases of two FC layers; $\circ$ denotes the convolution operation. The final output for each channel can be computed by:
% \small
% \begin{align}
% \hat{a}'_i^t=\hat{a}_i^t\odot w_c, \{\hat{m}'_i^l\}^t=\{\hat{m}_i^l\}^t\odot w_c, \{\hat{r}'_i^l\}^t=\{\hat{r}_i^l\}^t\odot w_c,
% \label{channel_out}
% \end{align}\normalsize
% where $\odot$ is the element-wise multiplication. Based on the channel attention, we can extract the temporal information of each channel (I-frame, motion vector and residual).

% \noindent \textbf{Spatial attention.} To extract the local spatial information of each channel, we firstly down-sample the temporal dimension by using the combination of a 3D convolutional layer with kernel size of $(L+1)\times 1 \times 1$ and a ReLU module  to discard the temporal-wise dependencies of re-scaled features in Eq.\eqref{channel_out}. Then, we leverage the combination of two 2D convolutional layers with kernel size of $3\times 3$, two RelUs and a 2D convolutional layers with kernel size of $1\times 1$  to obtain the spatial attention map. Finally, the map is normalize by a sigmoid function as follows:
% \small
% \begin{align}
% w_s=\sigma(conv_{1\times 1}(ReLU(conv_{3\times 3}^{(1)}(ReLU(conv_{3\times 3}^{(2)}(V_s)))))),
% \label{w_s}
% \end{align}\normalsize
% where $conv_{3\times 3}^{(1)}$ and $conv_{3\times 3}^{(2)}$ denote two 3×3 convolutional layers, and $V_s$ is the output of Eq.\eqref{channel_out}. For the appearance branch, $V_s=[a^0';a^1';\cdots a^L']$.
% %where $W_{3\times 3}^{(1)}\in\mathbb{R}^{H\times H\times 3\times 3}$ and $W_{3\times 3}^{(2)}\in\mathbb{R}^{H\times H\times 3\times 3}$ are the weights of two 3×3 convolutional layers; $b_{3\times 3}^{(1)}\in\mathbb{R}^H$ and $q_{3\times 3}^{(2)}\in\mathbb{R}^H$ are their corresponding biases; the weight $W_{1\times 1}\in\mathbb{R}^{1\times H\times 1\times 1}$ and the bias $b_{1\times 1}\in\mathbb{R}^1$ belong to the $1\times 1$ convolutional layer.

% %To further aim emphasis on the informative features and discard the useless ones, we leverage a max pooling layer as follows:
% Similar to the channel attention, the final appearance output is
% \small
% \begin{align}
% f_{\hat{a}}^t=a^t'\odot w_s.
% \label{spatial_out}
% \end{align}\normalsize
% Similarly, we can obtain the final motion output $f_{\hat{m}}^t$ and the final residual output $f_{\hat{r}}^t$.
% a simple convolutional layer to get the attention map $\overline{Att}_{{\rm spa}, IM}^k$ on spatial dimension between I-frame and Motion Vector. Each element in the attention map measuring the impact of I-frame spatial positions on Residual spatial position:

% Fig.~\ref{} shows our designed visual memory construct with $s$ memory slots $M=\{m_1,m_2,\ldots,m_s\}$ and three hidden states $H=\{h^a,h^m,h^v\}$. Based on the memory construct, we leverage $h^a$ and $h^m$ to refine the appearance and motion features respectively, which will be written into the visual memory. For our textual memory, we use $s$ memory slots $M=\{m_1,m_2,\ldots,m_s\}$ and a hidden state $h^q$

% To store the global visual context, we utilize the separate hidden state $h^v$ to integrate these appearance and motion features. We denote the sigmoid function as $\sigma$. For convenience, we combine superscript $a$ and $m$ for identical operations on both appearance and motion features, and denote superscript $q$ as the query feature.

% \noindent \textbf{Memory writing.} For the time $t$, to write into the memory as the non-linear mappings from input and previous hidden state, we define the following content $c_t$:
% \begin{align}
% c_t=\sigma(f_c(t-1)+f_c(t)+b_c),
% \end{align}
% where $f_c(t-1)$ and $f_c(t)$ mean the state function at time $t-1$ and time $t$, respectively. For the appearance/motion/query features $c_t^{a/m/q}$, we have $f_c(t-1)^{a/m/q}=W_{i\rightarrow h}^{a/m/q}h_{t-1}^{a/m/q}$, $f_c(t)^{a/m/q}=W_{i\rightarrow c}^{a/m/q}f_t^{a/m/q}$, and $b_c=b_c^{a/m/q}$.
% Then, we define $\varpi_t=\{w_{1,t},\ldots,w_{s,t}\}$ as the  weights of $c_t$ in each memory slot during memory writing:
% \begin{align}
% \varpi_t=Softmax(v_w^Ttanh(f_w(t-1)+f_w(t)+b_w)),
% \end{align}
% where $f_w(t-1)=W_{h\rightarrow w}^{a/m/q}h_{t-1}^{a/m/q}$, $f_w(t)=W_{c\rightarrow w}^{a/m/q}c_t^{a/m/q}$, and $b_w=b_w^{a/m/q}$. The write weight $\varpi_t$ is used to balance the importance of each memory slot.
% To integrate the appearance content $c_t^a$ and the motion feature $c_t^m$ for a unified visual features, we write them into the current memory $M_t$. To balance them, we design a learnable modality weight $z_t=\{z_{1,t},z_{2,t},z_{3,t}\}$  as follows:
% \begin{align}
% z_t=Softmax(v_w^Ttanh(f_z(t-1)+f_z(t)+b_z)),
% \end{align}
% where $f_z(t-1)=W_{h\rightarrow z}h_{t-1}^v$, $f_z(t)=W_{a\rightarrow z}c_t^a+W_{m\rightarrow z}c_t^m$. Therefore, we can write the visual memory $M_t$ at time $t$ by
% \begin{align}
% M_t=z_{1,t}w_t^ac_t^a+z_{2,t}w_t^mc_t^m+z_{3,t}M_{t-1},
% \end{align}
% where the modality weight $z_t$ is used to balance the writing of appearance and motion features. Thanks to our designed memory-write approach, we can integrate appearance and motion features for a learnable co-attention. Besides, we can memorize different spatio-temporal patterns of the given compressed video in a synchronized and global context.

% As for the textual memory, we can write the textual memory $M_t^q$ at time $t$ by
% \begin{align}
% M_t^q=w_t^qc_t^q+(1-w_t^q)M_{t-1}^q,
% \end{align}
% \noindent \textbf{Memory reading.}
% When reading each memory, we need to obtain the relevant memory contents in $s$ memory slots. Firstly, we define the reading weights $r_t=\{r_{1,t},\ldots,r_{s,t}\}$ by
% \begin{align}
% r_t=Softmax(v_r^Ttanh(f_r(t-1)+f_r(t)+b_r)).
% \end{align}
% For the visual memory, $f_r(t-1)=W_{h\rightarrow r}h_{t-1}^v$ and $f_r(t)=W_{a\rightarrow r}c_t^a+W_{m\rightarrow r}c_t^m$. As for the textual memory, $f_r(t-1)=W_{h\rightarrow r}h_{t-1}^q$ and $f_r(t)=W_{q\rightarrow r}c_t^q$. The reading content $R_t$ from memory is the weighted sum of each memory slot $R_t=\sum_{i=1}^sr_tm_i$. Thus, based on the visual memory, we can integrate both appearance and motion features during the memory reading.

%\noindent \textbf{Memory updating.} In this step, we want to update the memory by




\begin{figure}[t!]
\centering
\includegraphics[width=0.4\textwidth]{res.pdf}
\vspace{-4pt}
\caption{Framework of adaptive motion-appearance fusion.}
\label{fig:res}
\vspace{-12pt}
\end{figure}
\subsection{Adaptive Motion-Appearance Fusion}
% In different computer vision applications, we need to deal with different video datasets. Some videos contain many abrupt temporal changes, which often shows more motion information.
% Other videos consist of few abrupt temporal changes, illustrating more appearance information.
After obtaining the attentive motion and appearance information, we tend to aggregate them to infer the activity content.
Considering different videos may contain different abrupt temporal changes, we cannot equally fuse both the motion and appearance.
Specifically,
in the TSG task, a video with more abrupt temporal changes often corresponds to a related word. For example, a video corresponding to ``run'' often have more temporal changes than another video corresponding to ``walk''.
Therefore, we propose an adaptive strategy to fuse motion and appearance reasonably.
Specifically, we first enhance the appearance and motion features based on the query features. Then, we leverage the residual information to balance the enhanced appearance features and the enhanced motion features adaptively.

\noindent \textbf{Query-guided feature enhancement.}
We first utilize an attention mechanism to aggregate the word-level query features $\{q_j\}_{j=1}^M$ for each appearance feature $f_{\hat{a}}^t$ as:
\begin{equation}\label{attention_appearance}
X^t=W^{\top} \text{tanh}(\bm{W}_1 f_{\hat{a}}^t + \bm{W}_2 \bm{q}_j + \bm{b}_0),\\
    A_a^t = \frac{X^t}{\sum_sX^t(s)},
\end{equation}
where $A_a^t$ is the attention,
$\bm{W}_1$ and $\bm{W}_2$ are projection matrices, $\bm{b}_0$ is the bias vector, and the $W^{\top}$ is the row vector as in \cite{zhang2019cross}. Based on Eq. \eqref{attention_appearance}, we can obtain the query-enhanced appearance feature $f_a^t=f_{\hat{a}}^t\odot  A_a^t$, where $\odot$ denotes the operation of element-wise product. Similarly, we can obtain the query-enhanced motion feature $f_m^t$, which also semantically corresponds to the query.

\noindent \textbf{Residual-guided feature fusion.}
The residual features not only represent the temporal changes (\textit{i.e.}, motion context) among adjacent frames, but also denote the changes occur in RGB pixels (\textit{i.e.}, appearance context).
Thus, we utilize the residual feature as guidance to synchronize motion and appearance features by
% In video compression, larger amounts of residualoften denotes more temporal changes occur in RGB pixels, where the motion feature is more important for video grounding. To closely synchronize motion and appearance features, we use a learnable parameter to balance the motion and appearance information in the visual features.
% %If an object suddenly disappears or the scene changes on a frame, direct expressing motion based on its adjacent frames will reduce the motion’s expression accuracy.
% To closely synchronize motion and appearance features, we use a learnable parameter to balance the motion and appearance information in the visual features. In video compression, larger amounts of residualoften denotes more temporal changes occur in RGB pixels.
a learnable Block (shown Figure \ref{fig:res}):
% As shown in Figure \ref{fig:res}, we leverage a block named Ext as following:
\begin{align}\label{beta}
\beta^t=Block(f_{\hat{r}}^t),
\end{align}
where $\beta^t\in[0,1]$ is a learnable balance, the block contains an average pooling, two fully-connected layers and a RelU network. If there are many abrupt temporal changes between different scenes, $\beta^t$ will approach 1. On the contrary, when there are few abrupt temporal changes, $\beta^t$ goes nearly to 0. At last, we fuse the motion and appearance information with this balanced weight as:
\begin{align}\label{f_fuse}
f_{v}^t=\beta W_3f_a^t+(1-\beta)W_4f_m^t,
\end{align}
where matrices $W_3$ and $W_4$ are learnable parameters.
% , $A_a^t$ and $A_m^t$ are word-aware attentions to aggregate the word-level query features $Q=\{q_j\}_{j=1}^M$ for $f_{\hat{a}_i}^t$ and $f_{\hat{m}_i^l}^t$, which are defined as follows:
% \small
% \begin{align}\label{word_aware}
% f_{joint}^t=\beta W_4(A_a^t\odot f_{\hat{a}_i}^t)+(1-\beta)W_5(A_m^t\odot f_{\hat{m}_i^l}^t)
% \end{align}\normalsize
% We first utilize an attention mechanism to aggregate the word-level query features $\{q_j\}_{j=1}^M$ for each object $\bm{f}^a_{t,k}$ as:
% \small
% \begin{align}
% X^t&=W^{\top} \text{tanh}(\bm{W}_5 f_{\hat{a}_i}^t + \bm{W}_6 \bm{q}_j + \bm{b}_6),\\
%     A_a^t &=
%     \sum_{j=1}^M \text{softmax}(X^t),
% \end{align}\normalsize
% where $\bm{W}_5$ and $\bm{W}_6$ are projection matrices, $\bm{b}_6$ is the bias vector and the $W^{\top}$ is the row vector as in \cite{zhang2019cross}.
% Similarly, we can obtain $A_a^t$.
% To ensure consistency of visual and textual dimensions, we reshape the fused visual features by
% \small
% \begin{align}
% f_v^l=FC(ReLU(Conv_{1\times 1}(f_{joint}^l))),
% \end{align}\normalsize
% where $f_v^l\in\mathbb{R}^d$.

\subsection{Multi-modal Fusion and Grounding Head}
After obtaining the motion-appearance enhanced visual feature, we further integrate it with the textual features as:
\begin{align}
\label{eq:o_feature}
o=W_5\sum_{t=1}^{T}f_v^t+W_6\sum_{j=1}^Mq_j+W_7q_{global}.
\end{align}
where $o$ is the fused feature, and $W_5$, $W_6$ and $W_7$ are learnable weight matrices.
% To integrate the visual and textual features, we
% To recognize the relevant regions and movements in the current frame and its adjacent frames, we first compute the attention between visual and textual features by $A_v^t=exp(M_v^t)/\sum_{u=1}^K exp(M_u^t)$, where $M_v^t=W_1tanh(W_2f_v^t+c_1)+c_2$ and  $W_1\in \mathbb{R}^{d_s \times d_c}$ and $W_2\in \mathbb{R}^{d_c \times d_s}$ are learnable transform matrices; $c_1$ is a learnable offset.
% % Given a language query, we capture more relevant objects in a frame
% % \begin{align}
% % E_q=\sum_{i=1}^{N_w}W_0(v_w^i)^T+\sum_{i=1}^{N_w}W_8f_{global_q}^i,
% % \end{align}
% % where $E_q\in \mathbb{R}^{d_h \times d_w \times 1}$ and
% % $W_0\in \mathbb{R}^{d_h \times d_w \times d_r}$.

% Then, we want to fuse visual and textual information by $v_{local}=W_6\sum_{t=1}^{N_v}(f_v^t\odot A_v^t)+W_7\sum_{w=1}^{N_q}f_q^w$, where $W_6$ and $W_7$ are learnable parameters, $N_v$ is the frame number of the video, and $N_q$ is the word number of the given query.
% Finally, we fuse the local and global cross-modal features as follows:
% \small
% \begin{align}
% o=W_9f_q+W_{10}f_v^t
% \end{align}\normalsize
% % Finally, we fuse the local and global cross-modal features as follows:
% % \begin{align}
% % o=o_{local}+\lambda o_{global},
% % \end{align}
% where $W_9$ and $W_{10}$ are learnable weight matrices.

% % \begin{align}
% % A_s^t=\frac{exp(M_s^t)}{\sum_{u=1}^K exp(M_u^t)}
% % \end{align}
% % where
% design a grounding head module to predict the target video moment semantically corresponding to the given query.
% % improved triplet ranking loss
% % \begin{align}
% % \mathcal{L}_{intra}=\max(0, \alpha+sim(v,m)-sim(v,a))
% % \end{align}
% Then, we summarize the information while highlighting important segment features based on temporal attention. Finally, we
Based on the multi-modal features $o$,
we utilize two Multilayer Perceptron (MLP) layers to predict the start and end scores ($p_s^t$, $p_e^t$) on each video clip as
% we predict the time interval ($\tau_s$, $\tau_e$) using the Multilayer Perceptron (MLP):
\begin{align}
\gamma = \text{softmax} (\text{MLP}_{1}(o)), \
(p_s^t, p_e^t) = \text{MLP}_{\text{reg}}(\gamma^t o^t),
\label{eq:regression}
\end{align}
%
where $\gamma \in \mathbb{R}^{T}$ is the attention weights for segments.
% Note that $\text{MLP}_{1}$ and $\text{MLP}_{\text{reg}}$ have $d/{2}$- and $d$-dimensional hidden layers, respectively.
Following \cite{liu2022reducing,zhang2020span}, we introduce
the regression loss $\mathcal{L}_{\text{reg}}$ to learn the timestamp scoring prediction as follows:
\begin{align}
\mathcal{L}_{\text{reg}}= \frac{1}{2T} \sum_{t=1}^T [\mathcal{S}(\hat{p}_s^t-p_s^t) + \mathcal{S}(\hat{p}_e^t-p_e^t)],
\label{eq:reg_loss}
\end{align}
where $(\hat{p}_s^t, \hat{p}_e^t) \in [0,1]$ are the ground-truth labels, $\mathcal{S}(x)$ is the cross-entropy loss.
% regression loss
% In Eq.\eqref{eq:regression}, we directly regress the boundary based on the temporal attention. Thus,
We also introduce a confident loss $\mathcal{L}_{\text{guide}}$ to guide timestamp prediction:
\begin{align}
\mathcal{L}_{\text{guide}} = -\frac{\sum_{t=1}^{{T}} \hat{\gamma}^t \log(\gamma^t)}{\sum_{t=1}^{{T}} \hat{\gamma}^t},
\label{eq:guide_loss}
\end{align}
%
where $\hat{\gamma}^t=1$ if the $t$-th segment is located within the ground-truth boundary and $\hat{\gamma}^t=0$ otherwise.
By Eq. \eqref{eq:guide_loss}, we can obtain higher attention weights for the segments semantically relevant to the text query.

% Following \cite{yuan2019semantic,zhang2019learning,liu2022memory}, we leverage a proposal-based grounding head method to determine the target moment by ranking pre-defined moment proposals. Specifically, we first define some moment proposals with various sizes on each frame $t$. Then, multiple 1d-convolutional layers are used to process frame-wise features $f_v^t$ to compute the  temporal offsets and confidence scores  of these moment proposals. For the given the given video, if we obtain $P$ proposals, and  the start and end timestamp of each moment proposal is $(\tau_s,\tau_e)$.
% The corresponding confidence score and boundary offsets are denoted as $c$ and $(\delta_s,\delta_e)$, where $s,e$ means the start and end. Especially, if the confidence score $c$ of a proposal is larger than a threshold value $\lambda^k$, we define this proposal as a positive proposal and denote the positive proposal number as $P_{pos}$.
% For each proposal, its predicted moment can be denoted as $(\tau_s+\delta_s,\tau_e+\delta_e)$. When training, we compute the Intersection over Union (IoU) score $s^{gt}$ between each pre-defined moment proposal and the ground truth, and leverage the following loss  to supervise the confidence score as:
% \small
% \begin{align}
% \mathcal{L}_{iou}=- \frac{1}{P} \sum s^{gt} \text{log}(s)+(1-c^{gt})\text{log}(1-s).
% \end{align}\normalsize
% Since these pre-defined moment proposals only can be used to coarsely predict the moment, we need fine-tune boundary offsets of target moments for precise start and end timestamps by the following boundary loss:
% \small
% \begin{align}
%     \mathcal{L}_{boundary} = \frac{1}{P_{pos}} \sum ||\delta_s - \delta_s^{gt}||_1 + ||\delta_e - \delta_e^{gt}||_1,
% \end{align}
% \normalsize
Therefore, the final loss function is formulated as:
\begin{align}
\mathcal{L}_{final}=\mathcal{L}_{\text{reg}}+\alpha\mathcal{L}_{\text{guide}},
\end{align}
where $\alpha$ is a hyper-parameter.

\noindent \textbf{Inference.}  Given a video bit-stream and a language query, we first feed them into our TCSF to obtain the fused cross-modal feature $o$ in Eq. \eqref{eq:o_feature}. Then, we predict the start and end boundary scores ($p_s^t$, $p_e^t$) by $o$ in Eq. \eqref{eq:regression} and the confidence score in Eq. \eqref{eq:guide_loss}.  Based on the predicted scores of the start/end timestamps and confidence scores, we generate several candidate moments, ``Top-n (R@n)'' candidates will be selected with non-maximum suppression.
\section{Experiment}
\begin{table}[t!]
\small
    \centering
    \caption{Effectiveness comparison for temporal sentence grounding on ActivityNet Captions dataset under official train/test splits.}
     \vspace{-8pt}
    \setlength{\tabcolsep}{1.5mm}{\begin{tabular}{c|c|cccc}
    \hline
%    \multicolumn{6}{c}{ActivityNet Captions}\\ \hline
    \multirow{2}*{Method} & \multirow{2}*{Type} & R@1, & R@1, & R@5, & R@5, \\
    ~ & ~ & IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5\\ \hline
% VSA-RNN & FS & 39.28 & 24.43 & 70.84 & 55.52\\
% VSA-STV & FS & 41.71 & 24.01 & 71.05 & 56.62\\
CTRL \cite{gao2017tall}& FS & - & 29.01 & - & 59.17\\
%TGN & FS & 43.81 & 27.93 & 54.56 & 44.20 \\
%LGI \cite{mun2020local}&FS&58.52&41.51&-&-\\
2D-TAN \cite{zhang2019learning}& FS & 59.45 & 44.51 & 85.53 & 77.13 \\
 DRN \cite{zeng2020dense}& FS & - & 45.45 & - & 77.97\\
%FVMR&FS&60.63&45.05&86.11&77.42\\
RaNet \cite{gao2021relation}&FS&-&45.59&-&75.93\\
 MIGCN \cite{zhang2021multi}& FS&-&48.02&-&78.02\\
%IVG-DCL & FS & 63.22 & 43.84 & - & - \\
MMN \cite{wang2022negative}&FS&65.05&48.59&87.25&79.50\\
\hline
%CTF & WS & 44.30 & 23.60 & - & -\\
ICVC \cite{chen2022explore}& WS & 46.62 & 29.52 & 80.92 & 66.61 \\
%MARN & WS & 47.01 & 29.95 & 72.02 & 57.49\\
%SCN & WS & 47.23 & 29.22 & 71.45 & 55.69\\
LCNet  \cite{yang2021local}& WS & 48.49 & 26.33 & 82.51 & 62.66\\
%CCL & WS & 50.12 & 31.07 & 77.36 & 61.29 \\
VCA \cite{wang2021visual}& WS & 50.45 & 31.00 & 71.79 & 53.83 \\
WSTAN \cite{wang2021weakly}& WS & 52.45 & 30.01 & 79.38 & 63.42\\
%CRM & WS & 55.26 & 32.19 & - & - \\
CNM \cite{zheng2022weakly}&WS&55.68&33.33&-&-\\
%CPL \cite{zheng2022weakly}& WS&55.73 &31.37& 63.05& 43.13\\
\hline\hline
% 2D-TAN* & OS \\
% WSTAN* & OS \\
\textbf{Our TCSF} & \textbf{CD} & \textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24}  \\\hline
 \end{tabular}}
  \vspace{-10pt}
    \label{tab:ActivityNet}
\end{table}
\subsection{Datasets}
\noindent \textbf{ActivityNet Captions.}
Built from ActivityNet v1.3 dataset \cite{caba2015activitynet} for the dense video captioning task, ActivityNet Captions contains 20k YouTube videos and 100k language queries. On average, a video are 2 minutes and a query has about 13.5 words.  Following the public split \cite{gao2017tall}, we use 37421, 17505, and 17031 video-query pairs for training, validation and testing.




\noindent \textbf{Charades-STA.}
Built upon the Charades  dataset \cite{sigurdsson2016hollywood,gao2017tall}, Charades-STA contains 16128 video-sentence pairs. Folowing \cite{gao2017tall}, we utilize 12408 pairs  for training and the others  for testing. The average video length is 0.5 minutes. The language annotations are generated by sentence decomposition and keyword matching with manual check.

\noindent \textbf{TACoS.}
Collected from the cooking scene by \cite{regneri2013grounding}, TACoS is employed for the video grounding and dense video captioning tasks. The dataset consists of 127 videos, whose average length is 4.8 minutes. Following the same split of \cite{gao2017tall}, we leverage 10146, 4589, and 4083 video-query pairs for training, validation, and testing respectively.


\subsection{Experimental Settings}
\noindent \textbf{Evaluation metric.}
Following \cite{gao2017tall,liu2018attentive,zhang2020span}, we evaluate the grounding performance by ``R@n, IoU=m'', which means the percentage of  queries having at least one result whose Intersection over Union (IoU) with ground truth is larger than m. In our experiments, we use $n \in \{1,5\}$ for all datasets, $m \in \{0.5,0.7\}$ for ActivityNet Captions and Charades-STA, $m \in \{0.3,0.5\}$ for TACoS.

\begin{table}[t!]
\small
    \centering
    \caption{Performance comparison for temporal sentence grounding on Charades-STA dataset under  official train/test splits.}
     \vspace{-8pt}
  \setlength{\tabcolsep}{1.5mm}{\begin{tabular}{c|c|cccc}
    \hline
     \multirow{2}*{Method} & \multirow{2}*{Type} & R@1, & R@1, & R@5, & R@5, \\
~ & ~ & IoU=0.5 & IoU=0.7 & IoU=0.5 & IoU=0.7\\
\hline
% VSA-RNN & FS & 10.50 & 4.32 & 48.43 & 20.21 \\
% VSA-STV & FS & 16.91 & 5.81 & 53.89 & 23.58 \\
CTRL \cite{gao2017tall} & FS & 23.62 & 8.89 & 58.92 & 29.52\\
MMN \cite{wang2022negative}&FS&47.31& 27.28& 83.74& 58.41\\
2D-TAN \cite{zhang2019learning}& FS & 39.81 & 23.25 & 79.33 & 52.15\\
RaNet \cite{gao2021relation}&FS&43.87& 26.83& 86.67& 54.22\\
 DRN \cite{zeng2020dense} & FS & 45.40 & 26.40 & 88.01 & 55.38 \\
 %LGI \cite{mun2020local}&FS&\\
%IVG-DCL & FS & 50.24 & 32.88 & - & - \\
\hline
%SCN & WS & 23.58 & 9.97 & 71.80 & 38.87 \\
%CTF & WS & 27.30 & 12.90 & - & - \\
WSTAN \cite{wang2021weakly}& WS & 29.35 & 12.28 & 76.13 & 41.53 \\
ICVC \cite{chen2022explore}& WS & 31.02 & 16.53 & 77.53 & 41.91 \\
%MARN & WS & 31.94 & 14.18 & 70.00 & 37.40 \\
%CCL & WS & 33.21 & 15.68 & 73.50 & 41.87 \\
%CRM & WS & 34.76 & 16.37 & - & - \\
CNM \cite{zheng2022weakly}&WS&35.15&14.95&-&-\\
VCA \cite{wang2021visual}& WS & 38.13 & 19.57 & 78.75 & 37.75 \\
LCNet  \cite{yang2021local}& WS & 39.19 & 18.17 & 80.56 & 45.24 \\
    \hline \hline
    \textbf{Our TCSF} & \textbf{CD}& \textbf{53.85}&\textbf{37.20}&\textbf{90.86}&\textbf{58.95} \\ \hline
    \end{tabular}}
    \vspace{-4pt}
    \label{tab:Charades}
\end{table}
\begin{table}[t!]
\small
    \centering
    \caption{Performance comparison for temporal sentence grounding on  TACoS dataset under official train/test splits.}
     \vspace{-8pt}
    \setlength{\tabcolsep}{1.5mm}{\begin{tabular}{c|c|cccc}
    \hline
\multirow{2}*{Method} & \multirow{2}*{Type} & R@1, & R@1, & R@5, & R@5, \\
    ~ & ~ & IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5  \\ \hline
%2D-TAN &FS& 37.29& 25.32&  57.81& 45.04\\
%ABLR &FS &   19.50& 9.40& - &-\\
CTRL \cite{gao2017tall}&FS&18.32& 13.30& 36.69& 25.42\\
ACRN \cite{liu2018attentive}&FS &   19.52& 14.62&  34.97& 24.88\\
%TGN &FS &   21.77& 18.90&  39.06& 31.02\\
CMIN \cite{zhang2019cross}&FS &   24.64& 18.05&  38.46& 27.02\\
%QSPN \cite{Xu0PSSS19} & Full& 25.31& 20.15& 15.23& 53.21& 36.72& 25.30\\
%GDP &FS  &   24.14& 13.50& - &-\\
SCDM \cite{yuan2019semantic}&FS & 26.11& 21.17& 40.16& 32.18\\
 DRN \cite{zeng2020dense}&FS&-&23.17&-&33.36\\
%CBP &FS  &27.31& 24.79&  43.64& 37.40\\
%CSMGAN &FS & 33.90& 27.09&  53.98& 41.22\\
2D-TAN \cite{zhang2019learning}&FS&37.29&25.32&57.81&45.04\\
MMN \cite{wang2022negative}&FS&39.24&26.17&62.03&47.39\\
FVMR \cite{gao2021fast}&FS& 41.48&29.12&64.53&50.00\\
RaNet \cite{gao2021relation}&FS&43.34& 33.54& 67.33& 55.09\\
MIGCN \cite{zhang2021multi}&FS&48.79& 37.57& 67.63& 57.91\\
%VSLBase \cite{ZhangSJZ20}&Full& 23.59& 20.40& 16.65& -&-&-\\
%VSLNet &FS &29.61& 24.27& -&-\\
    \hline \hline
    \textbf{Our TCSF} & \textbf{CD}& \textbf{49.82}&\textbf{38.53}&\textbf{68.60}&\textbf{59.89}\\ \hline
    \end{tabular}}
    \vspace{-10pt}
    \label{tab:TACoS}
\end{table}

\noindent \textbf{Implementation details.}
All the experiments are implemented by PyTorch with an NVIDIA Quadro RTX 6000.
%For a fair comparison with previous works, we use the pre-trained C3D \cite{tran2015learning} model to extract visual features and utilize the Glove model \cite{pennington2014glove} to obtain word embedding.
For entropy decoding, following \cite{wu2018compressed,wang2019fast}, we use an  MPEG-4 decoder \cite{sikora1997mpeg} to decompress video bit-stream  for obtaining I-frame and P-frame.
% Considering that some videos are overlong, we uniformly downsample visual feature sequences 128 for Charades-STA and 256 for ActivityNet Captions and TACoS, respectively.
As for query encoding, we embed each word to 300-dimension features by the Glove model \cite{pennington2014glove}. Besides, we set the head size of multi-head self-attention to 8, and the hidden dimension of Bi-GRU to 512, respectively.
During training, we optimize parameter by  Adam optimizer with learning rate  $4\times 10^{-4}$ and linear learning rate decay of 10 for each 40 epochs. The batch size is 16 and the maximum training epoch is  100. We set $\alpha=0.8$ and $K=7$ in this paper.







\begin{table}[t!]
\small
    \centering
    \caption{Time complexity (s) of 100 videos on ActivityNet Captions dataset. The total time $T_{total}$ comprises the measurement time of decompressing video frames ($T_{dec}$), extracting the corresponding features ($T_{ext}$), and executing the network models ($T_{exe}$), where ``Other'' means the feature encoder (\textit{e.g.,} C3D/I3D).}
      \vspace{-8pt}
      \scalebox{0.95}{
    \setlength{\tabcolsep}{0.8mm}{
    \begin{tabular}{c|c|cccc|c|ccccccccccccc}
    \hline
    \multirow{2}*{Model} & \multirow{2}*{$T_{dec}$}&\multicolumn{4}{|c|}{$T_{ext}$} &\multirow{2}*{$T_{exe}$}& \multirow{2}*{$T_{total}$} \\ \cline{3-6}
    ~ & ~&I-frame&MV&Residual& Other&~&~\\ \hline
        CTRL \cite{gao2017tall}&50.72&-&-&-&30.36&372.74&453.82\\
        RaNet \cite{gao2021relation}&50.72&-&-&-&30.36&406.30&487.38\\
    2D-TAN \cite{zhang2019learning}&50.72&-&-&-&30.36&434.91&515.99\\
%      PLPNet \cite{li2022phrase}&50.72&-&-&-&30.36&496.09&577.17\\
%        CSMGAN&50.72&-&-&-&30.36&512.74&593.82\\
        MIGCN \cite{zhang2021multi}&50.72&-&-&-&30.36&529.27&610.35\\
        MMN \cite{wang2022negative}&50.72&-&-&-&30.36&556.43&637.51\\
%        LGI \cite{mun2020local}&50.72&-&-&-&30.36&575.91&656.99\\
        DRN \cite{zeng2020dense}&50.72&-&-&-&30.36&585.72&666.80\\\hline
    TAG \cite{mithun2019weakly}&50.72&-&-&-&30.36&162.28&243.36\\
    WSTAN \cite{wang2021weakly}&50.72&-&-&-&30.36&183.86&264.94\\
    CNM \cite{zheng2022weakly}&50.72&-&-&-&43.86&175.37&269.95\\\hline \hline
%    WSSTG&50.72&-&-&-&\\
    \textbf{Our TCSF}&\textbf{12.67}&\textbf{1.84}&\textbf{0.61}&\textbf{0.28}&-&\textbf{30.76} &\textbf{46.16}\\\hline
    \end{tabular}}}
    \vspace{-10pt}
    % \vspace{-6pt}
    \label{tab:efficiency}
\end{table}

\subsection{Comparison with State-of-the-Arts}
%\noindent \textbf{Compared methods.}
We conduct performance comparison on three datasets. To evaluate efficiency, we only choose the open-source compared methods  that are grouped into two categories: (i) Fully-supervised (FS) setting  \cite{gao2017tall,liu2018attentive,yuan2019semantic,zhang2019cross,zhang2019learning,zeng2020dense,gao2021fast,zhang2021multi,gao2021relation,wang2022negative};
(ii) Weakly-supervised (WS) setting
\cite{chen2022explore,yang2021local,zhang2020counterfactual,wang2021visual,wang2021weakly,zheng2022weakly}.
%, WSSTG \cite{chen2019weakly}
For convenience, we denote ``compressed-domain setting'' as ``CD''.
Following \cite{zhang2021natural,munro2020multi}, we directly cite the results of compared methods from corresponding works.  Note that no weakly-supervised  method reports its results on TACoS. The best results are \textbf{bold}.
From Tables~\ref{tab:ActivityNet}, \ref{tab:Charades} and \ref{tab:TACoS}, we can find that our  TCSF outperforms all compared methods by a large margin. It demonstrates that our model can achieve effective performance in more challenging compressed-domain setting.


%\noindent \textbf{Effectiveness comparison.}
%Tables~\ref{performance_result} shows the comparison results.
%between our TCSF and existing works on three datasets.
%  From the tables, we have the following findings:
% \begin{itemize}
%     \item Compared to the weakly-supervised methods, our method outperforms them by a large margin. Specifically, as shown in Table \ref{performance_result}, compared to the best method CRM, we bring the improvement of 9.61\% and 14.19\% in  R@1, IoU=0.3 and R@1, IoU=0.5 on ActivityNet Captions dataset. We also outperform the best method LCNet by 11.66\%, 17.03\%,  9.30\% and 11.71 on all metrics.
% %    On Charades-STA dataset, we also outperform the best method LCNet by 10.43, 16.31, 8.73 and 12.26 on all metrics. \textit{Note that}, our one-shot setting costs similar human labors as the weakly-supervised setting since the latter also requests the annotators to determine the matched video-query pair with at least one query-relevant frame. However, our performance is much better than the weakly-supervised one, demonstrating the effectiveness of the proposed tree-structure framework.
%     \item Compared to the fully-supervised methods, our method also achieves competitive performances, beating most methods. Specifically,  we outperform to the best method DRN by 0.93\% in R@1, IoU=0.5 on ActivityNet Captions dataset. Compared to the best method IVG-DCL, we bring improvement of 2.29\% in R@1, IoU=0.7 on Charades-STA dataset. Compared with the best method 2D-TAN, we improve the performance by 1.85\% in R@5, IoU=0.5 on TACoS dataset. Although we only utilize the compressed video and  much less annotations than these fully-supervised methods, our carefully-designed framework still can fuse  the motion and appearance information for video grounding.
%     %This demonstrates that our framework is robust to the weak annotations. Although we have much less annotations than the fully-supervised works, our well-designed tree-structure framework can group the query- and labeled frame-related adjacent frames into the same segment under the supervision of the proposed effective self-supervised losses.
%     % \item Thirdly, it shows that the one-shot setting is worth being investigated since it has less annotations than fully setting while costing similar labeling labors as weakly one.
% \end{itemize}
% The state-of-the-art performance shows the effectiveness of our TCSF.


\noindent \textbf{Efficiency comparison.}
To fairly evaluate the efficiency of our TCSF, we conduct comparison on ActivityNet Captions dataset with some state-of-the-art methods whose source codes are available.
% We further evaluate the efficiency of our TCSF, by fairly comparing its efficiency with existing methods on ActivityNet Captions dataset.
% For the comparisons, we choose the fully-supervised method 2D-TAN and the weakly-supervised method WSTAN because their source codes are available and the time complexity can be measured in the same platform.
Table~\ref{tab:efficiency} reports the results, and we consider the decompressing time $T_{dec}$, the feature extracting time $T_{ext}$, the network executing time $T_{exe}$, where the time is measured via an average on the whole videos. As depicted in Table~\ref{tab:efficiency}, we have the following observations: (i) Our model takes 12.67s to decompress GOPs in each video bit-streams and 1.84s, 0.61s, 0.28s to extract their three features, which is much efficient than previous works. The main reason is that previous works need to decompress full frames of the video and rely on the heavy-weight 3D encoder like C3D/I3D to extract the features.
Instead, we need less frame-level context with much light-weight encoder.
(ii) Our network executing $T_{exe}$ also has less parameters to learn than previous work, thus achieving faster speed.
% Compared with these compared methods, the main time-consuming gap is extract the motion feature. Our TCSF takes 0.27s to retrieve motion vectors from the bit-stream and 0.34s to generate motion features, which takes 0.61s for extracting the motion features. Similarly, TCSF takes 0.09s to retrieve the residuals and 0.19s to generate residualfeatures.
% In contrast, all the compared methods require  more time due to the heavy-light C3D/CLIP network. Thus, it requires more decompression time approximately at 50.72s. For these compared methods, CNM utilizes CLIP network for feature extraction, the others use the C3D network. Using C3D and CLIP for visual feature extraction requires 30.36s and 43.83s, respectively.
% Our TCSF is free from the above complex and time-consuming operations, thus achieving faster speed than all the compared methods. Specially, our total time $T_{total}$ is smaller than the decompression time $T_{dec}$ of compared methods.
Overall, experimental results demonstrate the time-efficiency of our method.

\begin{table}[t!]
\small
\caption{Main ablation study on ActivityNet Captions dataset, where we remove each key individual component to investigate its effectiveness. ``PFG'' denotes ``pseudo feature generation'', ``TTA'' denotes ``Three-branch spatial-temporal attention'', ``AMF'' denotes ``adaptive motion-appearance fusion''.}
\vspace{-8pt}
\scalebox{1.0}{
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{ccc|cccccccccccccccccc}
\hline
\multirow{2}*{PFG}&\multirow{2}*{TTA}&\multirow{2}*{AMF}& R@1 & R@1 & R@5 & R@5\\
~&~&~  & IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5\\\hline
\XSolidBrush & \XSolidBrush & \XSolidBrush &  50.59&32.84&76.12&68.33\\
\CheckmarkBold & \XSolidBrush & \XSolidBrush &  60.25&41.82&79.10&72.08\\
\XSolidBrush & \CheckmarkBold & \XSolidBrush & 62.79&45.87&79.45&76.13 \\
\XSolidBrush & \XSolidBrush & \CheckmarkBold & 63.74&45.39&80.16&76.05  \\
\CheckmarkBold & \CheckmarkBold & \XSolidBrush & 64.19&47.56&83.77&76.90 \\\hline
\CheckmarkBold &\CheckmarkBold &\CheckmarkBold &\textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24}
\\ \hline
\end{tabular}}}
\vspace{-4pt}
\label{tab:ablation1}
\end{table}

\begin{table}[t!]
\small
\caption{Ablation study on pseudo feature generation.}
\vspace{-8pt}
\scalebox{1.0}{
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{cc|ccccccccccccccccccc}
\hline
Appearance&Motion& R@1 & R@1 & R@5 & R@5\\
feature&feature& IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5\\\hline
\XSolidBrush &\CheckmarkBold &64.73&47.51&88.09&78.10 \\
\CheckmarkBold & \XSolidBrush & 65.85&48.02&87.80&79.03\\\hline
\CheckmarkBold &\CheckmarkBold  &\textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24}
\\ \hline
\end{tabular}}}
\vspace{-4pt}
\label{tab:ablation_rfg}
\end{table}

\begin{table}[t!]
\small
\caption{Ablation study on three-branch spatial-temporal attention.}
\vspace{-8pt}
\scalebox{1.0}{
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{cc|ccccccccccccccccccc}
\hline
Spatial&Temporal& R@1 & R@1 & R@5 & R@5\\
attention&attention& IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5\\\hline
\XSolidBrush &\CheckmarkBold &64.56&43.82&84.13&77.50\\
\CheckmarkBold & \XSolidBrush & 65.31& 43.20&83.72&76.81\\\hline
\CheckmarkBold &\CheckmarkBold  &\textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24}
\\ \hline
\end{tabular}}}
\vspace{-4pt}
\label{tab:ablation_tta}
\end{table}

\begin{table}[t!]
\small
\caption{Ablation study on adaptive motion-appearance fusion.}
\vspace{-8pt}
\scalebox{0.9}{
\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{cc|ccccccccccccccccccc}
\hline
Query-guided&Residual-guided& R@1 & R@1 & R@5 & R@5\\
enhancement&fusion& IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5\\\hline
\XSolidBrush &\CheckmarkBold &65.94&47.46&86.52&78.88\\
\CheckmarkBold & \XSolidBrush & 65.80&47.92&87.63&79.15\\\hline
\CheckmarkBold &\CheckmarkBold  &\textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24}
\\ \hline
\end{tabular}}}
   \vspace{-10pt}
\label{tab:ablation_amf}
\end{table}

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{qualitative.pdf}
\vspace{-18pt}
\caption{Qualitative prediction examples, where we complete the prediction at the \textcolor{red}{red} time. We find that our TCSF can ground earlier than the ground-truth start timestamp, while other methods ground later than the end timestamp.}
\label{fig:qualitative}
\vspace{-8pt}
\end{figure*}

\subsection{Ablation study}
To validate the effectiveness of each component in our TCSF, we conduct extensive ablation studies on the most challenging  ActivityNet Captions dataset.

\noindent \textbf{Main ablation studies.}
To analyze how each component contributes to the challenging task, we perform main ablation study as shown in Table~\ref{tab:ablation1}. Firstly, we set a baseline model that does not utilize pseudo feature, three-branch spatial-temporal attention module and adaptive motion-appearance fusion strategy to address the compressed-domain TSG. Similar to previous supervised methods, the baseline model directly generates multiple coarse segment proposals and then utilizes the rank loss for training. We can find that this baseline performs worse than most state-of-the-art methods in Table \ref{tab:ActivityNet}. Secondly, by designing the pseudo feature generation (PFG) module, we can effectively improve the performance since it enriches the full-frame context of the video. Table \ref{tab:ablation_rfg} further analyzes the effective of both pseudo appearance and motion features.
% Generating both appearance and motion features can achieve the apparent improvement, which shows the effectiveness of our pseudo feature generation module.
Thirdly, applying three-branch spatial-temporal attention (TTA) module also brings the large improvement since our well-designed spatial-temporal attention extracts the more fine-grained region-attentive temporal-spatial information for modelling more accurate activity content. As shown in Table \ref{tab:ablation_tta}, we further illustrate the effectiveness of spatial and temporal attentions separately.
Besides, the adaptive motion-appearance fusion (AMF) strategy also boost the performance a lot because it can balance the importance between appearance and motion features. Table \ref{tab:ablation_amf} illustrates the contributions of the query-guided feature enhancement and residual-guided fusion in AMF module. Overall, each component brings the performance improvement, and the full TCSF achieves the best results.

% \noindent \textbf{Effect of our compressed-domain pipeline.} To fairly compare with the existing fully- and weakly-supervised methods, we re-implement some works under our compressed-domain setting as shown in Table~\ref{tab:ablation2}. We choose the fully-supervised method 2D-TAN and the weakly-supervised method WSTAN because their source codes are available. We first utilize the partial decoding to extract I-frame, motion vector and residual from compressed video. Then, we leverage the three-branch channel-spatial attention and adaptive motion-appearance fusion to obtain the fused visual features. Finally, we treat our fused features as their final visual features of 2D-TAN and WSTAN. By treating our model as a plug-and-play module, both 2D-TAN and WSTAN achieve better performance and lower computational cost.
% Thus, our compressed-domain setting is worth being investigated, and our proposed method is effective and efficient.

\begin{table}[t!]
\small
   \centering
    \caption{Effect of different low-level features.}
    \vspace{-8pt}
    \setlength{\tabcolsep}{1.1mm}{
    \begin{tabular}{ccc|cccccccccccc}
\hline
\multirow{2}*{I-frame}& \multirow{2}*{MV}& \multirow{2}*{Residual}& R@1,   & R@1,   & R@5, & R@5, \\
&&&IoU=0.3&IoU=0.5&IoU=0.3&IoU=0.5\\\hline
\CheckmarkBold & \XSolidBrush & \XSolidBrush & 64.27&46.83&85.75&76.54\\
\CheckmarkBold & \XSolidBrush & \CheckmarkBold & 65.66& 47.82&86.36&78.59\\
\CheckmarkBold & \CheckmarkBold & \XSolidBrush & 66.03&47.94&88.03&79.28\\\hline
\CheckmarkBold &\CheckmarkBold &\CheckmarkBold &\textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24}\\
\hline
   \end{tabular}}
   \vspace{-4pt}
    \label{tab:channel}
%    \vspace{-8pt}
\end{table}
\noindent \textbf{Effect of different  low-level features.} To analyze the contribution of different low-level features, we conduct the ablation study as shown in Table \ref{tab:channel}. Both MV and residual can significantly improve the performance. The improvement shows the effectiveness of MV and residual.

\begin{table}[t!]
\small
    \centering
    \caption{Effect of the nouns-formed query in spatial attention.}
    \vspace{-8pt}
    \setlength{\tabcolsep}{2.8mm}{
    \begin{tabular}{c|ccccc}
    \hline
 \multirow{2}*{Changes} & R@1, & R@1, & R@5, & R@5,  \\
~ & IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5 \\ \hline
      w/o  query & 64.26&47.82&88.17&77.54 \\
 w/  query &\textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24}  \\ \hline
    \end{tabular}}
       \vspace{-10pt}
    \label{tab:noun}
\end{table}
\noindent \textbf{Effect of the nouns-formed query.}  In our spatial attention module, we utilize the noun feature to help us extract the spatial information. As shown in Table \ref{tab:noun}, we analyze the effect of the specific nouns-formed query. Based on the query, our TCSF improves the performance by 2.61\% in ``R@1, IoU=0.3''. This is because the nouns-formed query can locate the specific region for each frame, which reduces the  distraction of background information in the video.

\noindent \textbf{Analysis on the hyper-parameters.}
Moreover, we investigate the robustness of the proposed model to different hyper-parameters in Table~\ref{tab:ablation4}. In the temporal attention module, we  choose consecutive $K$ frame to extract the temporal information.
% hen $K$ is too small, we cannot obtain enough temporal information for video grounding; if $K$ is very large, the  temporal information we extract will contains much query-irrelevant information.
We find we can obtain the best performance when $K=7$. In the grounding head module, we leverage $\alpha$ to balance the two losses. When $\alpha=0.8$, our TCSF obtains the best performance.

\subsection{Qualitative Results}
As shown in Figure~\ref{fig:qualitative}, we report the representative visualization of the grounding performance. Our  TCSF can  ground more accurate query-related segment boundaries than 2D-TAN and WSTAN with faster grounding.











% \begin{table}[t!]
% \centering
% \small
% \caption{Our proposed method serves as a plug-and-play module for several state-of-the-art models on different datasets.}
%  \vspace{-10pt}
% % \vspace{-0.38cm}
% \setlength{\tabcolsep}{0.8mm}{
% \begin{tabular}{c|c|cc|c||cc|cccccccccccccc}
% \hline
% \multirow{3}*{Method}&\multirow{3}*{Type} & \multicolumn{3}{c||}{ActivityNet Captions}& \multicolumn{3}{c}{Charades-STA}  \\\cline{3-8}
% ~&~ & R@1,  & R@1, &\multirow{2}*{$T_{total}$}& R@1,  & R@1,&\multirow{2}*{$T_{total}$}  \\
% ~&~   &  IoU=0.3 & IoU=0.5 &~ & IoU=0.5 & IoU=0.7&~ \\\hline
% \multirow{2}*{2D-TAN}&FS&59.45& 44.51&~&39.81& 23.25\\
% ~&\textbf{CD} &53.86&38.76&~&34.71&19.14 \\\hline
% \multirow{2}*{WSTAN}&WS&52.45& 30.01&~&29.35& 12.28\\
% ~&\textbf{CD}  &54.86&33.71&~&34.95&16.54\\\hline
% Our TCSF&\textbf{CD}&64.87& 46.38&~&50.85& 35.20 \\\hline
% \end{tabular}
% }
% \label{shiyanjieguo_jichajiyong}
% \end{table}

% \begin{table}[t!]
%   \centering
%     \caption{Effect on visual feature extractor network on  ActivityNet Captions and Charades-STA datasets in R@1.}
%   \vspace{-10pt}
%     \setlength{\tabcolsep}{1mm}{
%     \begin{tabular}{c|cc|cccccccc}
% \hline
% \multirow{2}*{Option}  & \multicolumn{2}{c||}{ActivityNet Captions}& \multicolumn{2}{c}{Charades-STA}  \\\cline{2-5}
% ~& IoU=0.5 &$T_{total}$ & IoU=0.5& $T_{total}$ \\\hline
% 2D-TAN(C3D) & \\
% WSTAN(C3D)& \\
% \textbf{TCSF(C3D)} &  \\\hline
% 2D-TAN(Resnet-50) & \\
% WSTAN(ResNet-50)&  \\
% \textbf{TCSF(ResNet-50)}  & \\\hline
%   \end{tabular}}
%     \label{tab:resnet}
% %    \vspace{-8pt}
% \end{table}


% \begin{table}[t!]
%     \centering
%     \caption{Ablation study on the query encoder on ActivityNet Captions dataset.}
%     \vspace{-10pt}
%     \setlength{\tabcolsep}{1.0mm}{
%     \begin{tabular}{c|ccccc}
%     \hline \hline
%  \multirow{2}*{Changes} & R@1, & R@1, & R@5, & R@5,  \\
% ~ & IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5 \\ \hline
%       w/o self-attention & 64.26&47.82&88.17&77.54 \\
%  w/ self-attention &\textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24}  \\ \hline
% w/o Bi-GRU & 64.44&47.20&87.72&78.03 \\
% w/ Bi-GRU & \textbf{66.87}&\textbf{48.38}&\textbf{88.75}&\textbf{80.24} \\\hline
%     \end{tabular}}
%     \label{tab:ablation2}
% \end{table}




\begin{table}[t!]
\small
\caption{Effect of different hyper-parameters.}
\vspace{-8pt}
\scalebox{1.0}{
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{cc|cccccc}
\hline
\multirow{2}*{Module} &\multirow{2}*{Changes} & R@1 & R@1 & R@5 & R@5\\
&~ & IoU=0.3 & IoU=0.5 & IoU=0.3 & IoU=0.5\\
\hline
\multirow{3}*{\tabincell{c}{Temporal \\ attention}}&$K=6$& 66.28&47.95&87.34&80.17\\
~&\textbf{$K=7$}& \textbf{66.87}&48.38&\textbf{88.75}&\textbf{80.24}\\
~&$K=8$&65.92&\textbf{48.53}&86.11&79.30\\\hline
\multirow{3}*{\tabincell{c}{Grounding \\ head}} &$\alpha=0.7$ & 66.02 & 46.98 & 87.94 & \textbf{80.31} \\
~&\textbf{$\alpha=0.8$} &\textbf{66.87}&\textbf{48.38}&\textbf{88.75}&80.24 \\
~&$\alpha=0.9$ & 65.93&47.06&87.29&79.23
\\ \hline
\end{tabular}}}
   \vspace{-10pt}
\label{tab:ablation4}
\end{table}

\section{Conclusion}
In this paper, we introduce a brand-new compressed-domain setting into the temporal sentence grounding task to directly utilize the compressed video rather than decompressed frames.  To handle the challenging setting, we propose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) framework to extract and aggregate three kinds of low-level visual features for  grounding.
%To our best knowledge, we make the first attempt to locate the target segment in the compressed video.
Experimental results on three challenging datasets (ActivityNet Captions, Charades-STA and TACoS) demonstrate that our TCSF significantly outperforms existing fully- and weakly-supervised methods.
%Experimental results demonstrate that our TCSF significantly outperforms existing state-of-the-art methods.
%Experiments on three datasets show the effectiveness and efficiency of TCSF.
%(I-frame feature, motion vector feature and residualfeature) for  grounding.

% Experimental results on three challenging datasets demonstrate that our TCSF significantly outperforms existing fully- and weakly-supervised methods.
\noindent \textbf{Acknowledgements.}
This work is supported by National Natural Science Foundation of China (NSFC) under Grant No. 61972448.


\bibliographystyle{ieee_fullname}

% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper
\bibliography{egbib}


\end{document}
