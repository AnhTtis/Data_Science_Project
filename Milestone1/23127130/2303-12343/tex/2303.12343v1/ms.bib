@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}


@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@inproceedings{hu2016segmentation,
  title={Segmentation from natural language expressions},
  author={Hu, Ronghang and Rohrbach, Marcus and Darrell, Trevor},
  booktitle={European Conference on Computer Vision},
  pages={108--124},
  year={2016},
  organization={Springer}
}

@inproceedings{liu2017recurrent,
  title={Recurrent multimodal interaction for referring image segmentation},
  author={Liu, Chenxi and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Yuille, Alan},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1271--1280},
  year={2017}
}

@inproceedings{li2018referring,
  title={Referring image segmentation via recurrent refinement networks},
  author={Li, Ruiyu and Li, Kaican and Kuo, Yi-Chun and Shu, Michelle and Qi, Xiaojuan and Shen, Xiaoyong and Jia, Jiaya},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5745--5753},
  year={2018}
}

@inproceedings{margffoy2018dynamic,
  title={Dynamic multimodal instance segmentation guided by natural language queries},
  author={Margffoy-Tuay, Edgar and P{\'e}rez, Juan C and Botero, Emilio and Arbel{\'a}ez, Pablo},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={630--645},
  year={2018}
}

@inproceedings{yu2018mattnet,
  title={Mattnet: Modular attention network for referring expression comprehension},
  author={Yu, Licheng and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Bansal, Mohit and Berg, Tamara L},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1307--1315},
  year={2018}
}

@article{hertz2022prompt,
  title={Prompt-to-prompt image editing with cross attention control},
  author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2208.01626},
  year={2022}
}

@inproceedings{sun2021semantic,
  title={Semantic image matting},
  author={Sun, Yanan and Tang, Chi-Keung and Tai, Yu-Wing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11120--11129},
  year={2021}
}

@inproceedings{xu2017deep,
  title={Deep image matting},
  author={Xu, Ning and Price, Brian and Cohen, Scott and Huang, Thomas},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2970--2979},
  year={2017}
}

@article{brooks2022instructpix2pix,
  title={Instructpix2pix: Learning to follow image editing instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  journal={arXiv preprint arXiv:2211.09800},
  year={2022}
}

@inproceedings{shi2018key,
  title={Key-word-aware network for referring expression image segmentation},
  author={Shi, Hengcan and Li, Hongliang and Meng, Fanman and Wu, Qingbo},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={38--54},
  year={2018}
}

@inproceedings{ye2019cross,
  title={Cross-modal self-attention network for referring image segmentation},
  author={Ye, Linwei and Rochan, Mrigank and Liu, Zhi and Wang, Yang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10502--10511},
  year={2019}
}

@inproceedings{wang2022cris,
  title={Cris: Clip-driven referring image segmentation},
  author={Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11686--11695},
  year={2022}
}

@inproceedings{yang2022lavt,
  title={LAVT: Language-Aware Vision Transformer for Referring Image Segmentation},
  author={Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and Zhao, Hengshuang and Torr, Philip HS},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18155--18165},
  year={2022}
}

@inproceedings{luddecke2022image,
  title={Image Segmentation Using Text and Image Prompts},
  author={L{\"u}ddecke, Timo and Ecker, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7086--7096},
  year={2022}
}

@inproceedings{nagaraja2016modeling,
  title={Modeling context between objects for referring expression understanding},
  author={Nagaraja, Varun K and Morariu, Vlad I and Davis, Larry S},
  booktitle={European Conference on Computer Vision},
  pages={792--807},
  year={2016},
  organization={Springer}
}


@inproceedings{yu2016modeling,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={European Conference on Computer Vision},
  pages={69--85},
  year={2016},
  organization={Springer}
}

@inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={11--20},
  year={2016}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10684--10695},
  year={2022}
}


@article{turk1991eigenfaces,
  title={Eigenfaces for recognition},
  author={Turk, Matthew and Pentland, Alex},
  journal={Journal of cognitive neuroscience},
  volume={3},
  number={1},
  pages={71--86},
  year={1991},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{ke2004pca,
  title={PCA-SIFT: A more distinctive representation for local image descriptors},
  author={Ke, Yan and Sukthankar, Rahul},
  booktitle={Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
  volume={2},
  pages={II--II},
  year={2004},
  organization={IEEE}
}

@article{saharia2022photorealistic,
  title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  journal={arXiv preprint arXiv:2205.11487},
  year={2022}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@inproceedings{de2001robust,
  title={Robust principal component analysis for computer vision},
  author={De la Torre, Fernando and Black, Michael J},
  booktitle={Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},
  volume={1},
  pages={362--369},
  year={2001},
  organization={IEEE}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}

@inproceedings{karras2020analyzing,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8110--8119},
  year={2020}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@inproceedings{xu2018attngan,
  title={Attngan: Fine-grained text to image generation with attentional generative adversarial networks},
  author={Xu, Tao and Zhang, Pengchuan and Huang, Qiuyuan and Zhang, Han and Gan, Zhe and Huang, Xiaolei and He, Xiaodong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1316--1324},
  year={2018}
}

@inproceedings{zhang2021cross,
  title={Cross-modal contrastive learning for text-to-image generation},
  author={Zhang, Han and Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={833--842},
  year={2021}
}

@inproceedings{ramesh2021zero,
  title={},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@article{gafni2022make,
  title={Make-a-scene: Scene-based text-to-image generation with human priors},
  author={Gafni, Oran and Polyak, Adam and Ashual, Oron and Sheynin, Shelly and Parikh, Devi and Taigman, Yaniv},
  journal={arXiv preprint arXiv:2203.13131},
  year={2022}
}

@inproceedings{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{bucher2019zero,
  title={Zero-shot semantic segmentation},
  author={Bucher, Maxime and Vu, Tuan-Hung and Cord, Matthieu and P{\'e}rez, Patrick},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{li2020consistent,
  title={Consistent structural relation learning for zero-shot segmentation},
  author={Li, Peike and Wei, Yunchao and Yang, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10317--10327},
  year={2020}
}

@inproceedings{gu2020context,
  title={Context-aware feature generation for zero-shot semantic segmentation},
  author={Gu, Zhangxuan and Zhou, Siyuan and Niu, Li and Zhao, Zihan and Zhang, Liqing},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={1921--1929},
  year={2020}
}

@article{gu2022pixel,
  title={From pixel to patch: Synthesize context-aware features for zero-shot semantic segmentation},
  author={Gu, Zhangxuan and Zhou, Siyuan and Niu, Li and Zhao, Zihan and Zhang, Liqing},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@inproceedings{baek2021exploiting,
  title={Exploiting a joint embedding space for generalized zero-shot semantic segmentation},
  author={Baek, Donghyeon and Oh, Youngmin and Ham, Bumsub},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9536--9545},
  year={2021}
}

@article{xu2021simple,
  title={A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model},
  author={Xu, Mengde and Zhang, Zheng and Wei, Fangyun and Lin, Yutong and Cao, Yue and Hu, Han and Bai, Xiang},
  journal={arXiv preprint arXiv:2112.14757},
  year={2021}
}

@article{hu2020uncertainty,
  title={Uncertainty-aware learning for zero-shot semantic segmentation},
  author={Hu, Ping and Sclaroff, Stan and Saenko, Kate},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21713--21724},
  year={2020}
}

@inproceedings{song2018transductive,
  title={Transductive unbiased embedding for zero-shot learning},
  author={Song, Jie and Shen, Chengchao and Yang, Yezhou and Liu, Yang and Song, Mingli},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1024--1033},
  year={2018}
}

@article{lv2020learning,
  title={Learning unbiased zero-shot semantic segmentation networks via transductive transfer},
  author={Lv, Fengmao and Liu, Haiyang and Wang, Yichen and Zhao, Jiayi and Yang, Guowu},
  journal={IEEE Signal Processing Letters},
  volume={27},
  pages={1640--1644},
  year={2020},
  publisher={IEEE}
}

@inproceedings{zhang2021prototypical,
  title={Prototypical matching and open set rejection for zero-shot semantic segmentation},
  author={Zhang, Hui and Ding, Henghui},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6974--6983},
  year={2021}
}

@inproceedings{xian2019semantic,
  title={Semantic projection network for zero-and few-label semantic segmentation},
  author={Xian, Yongqin and Choudhury, Subhabrata and He, Yang and Schiele, Bernt and Akata, Zeynep},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8256--8265},
  year={2019}
}

@inproceedings{wu2020phrasecut,
  title={Phrasecut: Language-based image segmentation in the wild},
  author={Wu, Chenyun and Lin, Zhe and Cohen, Scott and Bui, Trung and Maji, Subhransu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10216--10225},
  year={2020}
}

@inproceedings{kamath2021mdetr,
  title={MDETR-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1780--1790},
  year={2021}
}

@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

@inproceedings{isola2017image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

@article{dosovitskiy2016generating,
  title={Generating images with perceptual similarity metrics based on deep networks},
  author={Dosovitskiy, Alexey and Brox, Thomas},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{yu2021vector,
  title={Vector-quantized image modeling with improved vqgan},
  author={Yu, Jiahui and Li, Xin and Koh, Jing Yu and Zhang, Han and Pang, Ruoming and Qin, James and Ku, Alexander and Xu, Yuanzhong and Baldridge, Jason and Wu, Yonghui},
  journal={arXiv preprint arXiv:2110.04627},
  year={2021}
}

@inproceedings{voynov2020unsupervised,
  title={Unsupervised discovery of interpretable directions in the gan latent space},
  author={Voynov, Andrey and Babenko, Artem},
  booktitle={International conference on machine learning},
  pages={9786--9796},
  year={2020},
  organization={PMLR}
}

@inproceedings{voynov2021object,
  title={Object segmentation without labels with large-scale generative models},
  author={Voynov, Andrey and Morozov, Stanislav and Babenko, Artem},
  booktitle={International Conference on Machine Learning},
  pages={10596--10606},
  year={2021},
  organization={PMLR}
}

@article{melas2021finding,
  title={Finding an unsupervised image segmenter in each of your deep generative models},
  author={Melas-Kyriazi, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},
  journal={arXiv preprint arXiv:2105.08127},
  year={2021}
}

@inproceedings{zhang2021datasetgan,
  title={Datasetgan: Efficient labeled data factory with minimal human effort},
  author={Zhang, Yuxuan and Ling, Huan and Gao, Jun and Yin, Kangxue and Lafleche, Jean-Francois and Barriuso, Adela and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10145--10155},
  year={2021}
}

@inproceedings{tritrong2021repurposing,
  title={Repurposing gans for one-shot semantic part segmentation},
  author={Tritrong, Nontawat and Rewatbowornwong, Pitchaporn and Suwajanakorn, Supasorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4475--4485},
  year={2021}
}

@article{baranchuk2021label,
  title={Label-efficient semantic segmentation with diffusion models},
  author={Baranchuk, Dmitry and Rubachev, Ivan and Voynov, Andrey and Khrulkov, Valentin and Babenko, Artem},
  journal={arXiv preprint arXiv:2112.03126},
  year={2021}
}

@InProceedings{Wang_2022_CVPR,
    author    = {Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
    title     = {CRIS: CLIP-Driven Referring Image Segmentation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11686-11695}
}


@InProceedings{pmlr-v37-sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@inproceedings{
schuhmann2022laionb,
title={LAION-5B: An open large-scale dataset for training next generation image-text models},
author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade W Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa R Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=M3Y74vmsMcY}
}

@inproceedings{nagaraja16refexp,
  title={Modeling Context Between Objects for Referring Expression Understanding},
  author={Varun K. Nagaraja and Vlad I. Morariu and Larry S. Davis},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{kazemzadeh2014referitgame,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={787--798},
  year={2014}
}

@InProceedings{Xu_2018_CVPR,
author = {Xu, Tao and Zhang, Pengchuan and Huang, Qiuyuan and Zhang, Han and Gan, Zhe and Huang, Xiaolei and He, Xiaodong},
title = {AttnGAN: Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@InProceedings{Zhu_2019_CVPR,
author = {Zhu, Minfeng and Pan, Pingbo and Chen, Wei and Yang, Yi},
title = {DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-To-Image Synthesis},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}
@inproceedings{tao2022df,
  title={DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis},
  author={Tao, Ming and Tang, Hao and Wu, Fei and Jing, Xiao-Yuan and Bao, Bing-Kun and Xu, Changsheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16515--16525},
  year={2022}
}
@article{ye2021improving,
  title={Improving Text-to-Image Synthesis Using Contrastive Learning},
  author={Ye, Hui and Yang, Xiulong and Takac, Martin and Sunderraman, Rajshekhar and Ji, Shihao},
  journal={The 32nd British Machine Vision Conference (BMVC)},
  year={2021}
}
@inproceedings{zhou2022towards,
  title={Towards Language-Free Training for Text-to-Image Generation},
  author={Zhou, Yufan and Zhang, Ruiyi and Chen, Changyou and Li, Chunyuan and Tensmeyer, Chris and Yu, Tong and Gu, Jiuxiang and Xu, Jinhui and Sun, Tong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17907--17917},
  year={2022}
}
@article{nichol2021glide,
  title={Glide: Towards photorealistic image generation and editing with text-guided diffusion models},
  author={Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  journal={arXiv preprint arXiv:2112.10741},
  year={2021}
}
@inproceedings{gu2022vector,
  title={Vector quantized diffusion model for text-to-image synthesis},
  author={Gu, Shuyang and Chen, Dong and Bao, Jianmin and Wen, Fang and Zhang, Bo and Chen, Dongdong and Yuan, Lu and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10696--10706},
  year={2022}
}
@article{tang2022improved,
  title={Improved Vector Quantized Diffusion Models},
  author={Tang, Zhicong and Gu, Shuyang and Bao, Jianmin and Chen, Dong and Wen, Fang},
  journal={arXiv preprint arXiv:2205.16007},
  year={2022}
}
@inproceedings{NEURIPS2021_a4d92e2c,
 author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and Tang, Jie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {19822--19835},
 publisher = {Curran Associates, Inc.},
 title = {CogView: Mastering Text-to-Image Generation via Transformers},
 url = {https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf},
 volume = {34},
 year = {2021}
}
@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{razavi2019generating,
  title={Generating diverse high-fidelity images with vq-vae-2},
  author={Razavi, Ali and Van den Oord, Aaron and Vinyals, Oriol},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{NEURIPS2021_49ad23d1,
 author = {Dhariwal, Prafulla and Nichol, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8780--8794},
 publisher = {Curran Associates, Inc.},
 title = {Diffusion Models Beat GANs on Image Synthesis},
 url = {https://proceedings.neurips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{nichol2021improved,
  title={Improved denoising diffusion probabilistic models},
  author={Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle={International Conference on Machine Learning},
  pages={8162--8171},
  year={2021},
  organization={PMLR}
}
@misc{UNet,
  doi = {10.48550/ARXIV.1505.04597},
  
  url = {https://arxiv.org/abs/1505.04597},
  
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{semanticGAN, 
title={Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization}, 
booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, 
author={Li, Daiqing and Yang, Junlin and Kreis, Karsten and Torralba, Antonio and Fidler, Sanja}, 
year={2021}, 
}

@InProceedings{pmlr-v119-chen20j,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

@inproceedings{deng2009imagenet, 
  title={Imagenet: A large-scale hierarchical image database}, 
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li}, 
  booktitle={2009 IEEE conference on computer vision and pattern recognition}, 
  pages={248--255}, 
  year={2009}, 
  organization={Ieee} 
}

@article{yu15lsun,
    Author = {Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong},
    Title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
    Journal = {arXiv preprint arXiv:1506.03365},
    Year = {2015}
}
@article{fei2006one,
  title={One-shot learning of object categories},
  author={Fei-Fei, Li and Fergus, Robert and Perona, Pietro},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={28},
  number={4},
  pages={594--611},
  year={2006},
  publisher={IEEE}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{li2022grounded,
  title={Grounded language-image pre-training},
  author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10965--10975},
  year={2022}
}

@inproceedings{zhang2022glipv2,
  title={Glipv2: Unifying localization and vision-language understanding},
  author={Zhang, Haotian and Zhang, Pengchuan and Hu, Xiaowei and Chen, Yen-Chun and Li, Liunian Harold and Dai, Xiyang and Wang, Lijuan and Yuan, Lu and Hwang, Jenq-Neng and Gao, Jianfeng},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{couairon2022diffedit,
  title={Diffedit: Diffusion-based semantic image editing with mask guidance},
  author={Couairon, Guillaume and Verbeek, Jakob and Schwenk, Holger and Cord, Matthieu},
  journal={arXiv preprint arXiv:2210.11427},
  year={2022}
}

@InProceedings{Lugmayr_2022_CVPR,
    author    = {Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
    title     = {RePaint: Inpainting Using Denoising Diffusion Probabilistic Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11461-11471}
}

@article{xie2022smartbrush,
  title={SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model},
  author={Xie, Shaoan and Zhang, Zhifei and Lin, Zhe and Hinz, Tobias and Zhang, Kun},
  journal={arXiv preprint arXiv:2212.05034},
  year={2022}
}

@inproceedings{chen2018language,
  title={Language-based image editing with recurrent attentive models},
  author={Chen, Jianbo and Shen, Yelong and Gao, Jianfeng and Liu, Jingjing and Liu, Xiaodong},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8721--8729},
  year={2018}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13},
  pages={818--833},
  year={2014},
  organization={Springer}
}