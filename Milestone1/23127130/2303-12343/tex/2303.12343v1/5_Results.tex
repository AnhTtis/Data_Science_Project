\section{Results}
\label{results}

\subsection{Image Segmentation Using Text Prompts}
\begin{table}[t]
    \centering
    \begin{adjustbox}{width=0.9\columnwidth}
    \begin{tabular}{|c||c|c|c|c|}
    \hline
    Method & mIoU & $IoU_{FG}$ & AP \\
    \hline
        \hline
        MDETR \cite{kamath2021mdetr} & 53.7 & - & - \\
        GLIPv2-T \cite{zhang2022glipv2} & 59.4 & - & - \\
        \hline\hline
        RMI \cite{wu2020phrasecut} & 21.1 & 42.5 & - \\
        Mask-RCNN Top \cite{wu2020phrasecut} & 39.4 & 47.4 & -\\
        HulaNet \cite{wu2020phrasecut} & 41.3 & 50.8 & - \\
        CLIPSeg (PC+)  \cite{luddecke2022image}& 43.4 & 54.7 & 76.7\\
        CLIPSeg (PC, D=128) \cite{luddecke2022image} & 48.2 & 56.5 & 78.2\\
        \hline
        RGBNet & 46.7 & 56.2 & 77.2 \\
        \rowcolor{lightgray} ZNet (Ours) & 51.3 & 59.0 & 78.7 \\
        \rowcolor{lightgray} LD-ZNet (Ours) & \textbf{52.7} & \textbf{60.0} & \textbf{78.9} \\
        % \rowcolor{lightgray} CLIP Image features only &  & 49.04 & 57.76 & 77.51 \\
        % \rowcolor{lightgray} Z + CLIP &  & 49.8 & 58.8 & 79.8 \\
        % \rowcolor{lightgray} Z + CLIP Img + LDM features&  &  &  &  \\
    \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Text-based image segmentation performance on the PhraseCut testset. The performance of ZNet and LD-ZNet is highlighted in gray. Both these models outperform the baseline RGBNet on all the metrics.}
    \vspace{-0.5em}
    \label{tab:ris_results}
\end{table}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.19\columnwidth}
        \centering
        % \includegraphics[width=\linewidth]{Images/visual/00047_47_a_bad_photo_of_a_white_stand._image.png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/00695_16_a_photo_of_the_oblong_pastry._image.png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/01364_0_a_bad_photo_of_a_license_plate._image.png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/04736_43_a_photograph_of_a_riding_person._image.png}
        \includegraphics[width=\linewidth]{Images/visual/00893_36_hanging_clock._image.png}
        \includegraphics[width=\linewidth]{Images/visual/03209_0_a_photo_of_a_castle._image.png}
        \caption{Input}
        % \label{fig:architecture}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}[t]{0.19\columnwidth}
        \centering
        % \includegraphics[width=\linewidth]{Images/visual/00047_47_a_bad_photo_of_a_white_stand._gt.png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/00695_16_a_photo_of_the_oblong_pastry._gt.png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/01364_0_a_bad_photo_of_a_license_plate._gt.png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/04736_43_a_photograph_of_a_riding_person._gt.png}
        \includegraphics[width=\linewidth]{Images/visual/00893_36_hanging_clock._gt.png}
        \includegraphics[width=\linewidth]{Images/visual/03209_0_a_photo_of_a_castle._gt.png}
        \caption{GT mask}
        % \label{fig:architecture}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}[t]{0.19\columnwidth}
        \centering
        % \includegraphics[width=\linewidth]{Images/visual/00047_47_a_bad_photo_of_a_white_stand..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/00695_16_a_photo_of_the_oblong_pastry..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/01364_0_a_bad_photo_of_a_license_plate..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/04736_43_a_photograph_of_a_riding_person..png}
        \includegraphics[width=\linewidth]{Images/visual/00893_36_hanging_clock..png}
        \includegraphics[width=\linewidth]{Images/visual/03209_0_a_photo_of_a_castle..png}
        \caption{RGBNet}
        % \label{fig:architecture}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}[t]{0.19\columnwidth}
        \centering
        % \includegraphics[width=\linewidth]{Images/visual/00047_64_a_photo_of_a_white_stand..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/00695_49_an_image_of_a_oblong_pastry..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/01364_22_an_image_of_a_license_plate..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/04736_67_riding_person..png}
        \includegraphics[width=\linewidth]{Images/visual/00893_47_an_image_of_a_hanging_clock..png}
        \includegraphics[width=\linewidth]{Images/visual/03209_66_a_bad_photo_of_a_castle..png}
        \caption{ZNet}
        % \label{fig:architecture}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}[t]{0.19\columnwidth}
        \centering
        % \includegraphics[width=\linewidth]{Images/visual/00047_85_a_photo_of_one_white_stand..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/00695_96_a_good_photo_of_a_oblong_pastry..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/01364_77_a_good_photo_of_a_license_plate..png}
        % \includegraphics[width=0.95\linewidth]{Images/visual/04736_85_a_good_photo_of_a_riding_person..png}
        \includegraphics[width=\linewidth]{Images/visual/00893_89_an_image_of_a_hanging_clock..png}
        \includegraphics[width=\linewidth]{Images/visual/03209_86_a_good_photo_of_a_castle..png}
        \caption{LD-ZNet}
        % \label{fig:architecture}
    \end{subfigure}
    
    \caption{Qualitative comparison on the PhraseCut test set. Each row contains an input image with a text prompt as an input, with the goal being to segment the image regions corresponding to the reference text. The text prompts are \emph{``hanging clock"} and \emph{``castle"} for the top and bottom rows. We show improvements using ZNet and LD-ZNet compared to the RGBNet.}
    \vspace{-1em}
    \label{fig:visual_results}
\end{figure}

% \textcolor{red}{define each approach z baseline, RGB baseline etc. separately, so that the text reads smoother}


On the PhraseCut dataset, we compare the performance of previous approaches with our ZNet and LD-ZNet for the text-based image segmentation task (Table \ref{tab:ris_results}). In order to showcase the performance improvement of our proposed networks, we create a baseline named RGBNet with the same architecture as ZNet except we use the original images as the input instead of its latent space $z$. For RGBNet, we use additional learnable convolutional layers to map the original image to match the input resolution of ZNet. From Table \ref{tab:ris_results}, we observe that our ZNet and LD-ZNet significantly outperform RGBNet. Specifically, the performance improvement from using the latent representation $z$ over the original images is clear (i.e. ZNet vs RGBNet baseline). Performance further improves upon incorporating the LDM visual-linguistic representations (LD-ZNet) - by 6\% overall on the $mIoU$ metric compared to RGBNet. We also highlight this qualitatively in Figure~\ref{fig:visual_results}. In the figure, we show the original image and the GT mask along with outputs from the RGBNet baseline followed by ZNet and LD-ZNet, where both ZNet and LD-ZNet help improve results consistently. For example in the top row, RGBNet detects light fixtures for the ``hanging clock" prompt, and although ZNet does not have as strong activations for these incorrect detections, it is LD-ZNet that correctly segments the ``clock". Similarly in the bottom row, while RGBNet completely got the ``castle" wrong, ZNet correctly has activations on the right buildings, but with lower confidence. However, LD-ZNet improves it further. 

We outperform in all the metrics when compared to previous works, other than MDETR \cite{kamath2021mdetr} and GLIPv2 \cite{zhang2022glipv2}. Notably, these works are pre-trained on detection and phrase grounding for predicting bounding boxes on huge corpus of text-image pairs across various publicly available datasets with bounding box annotations and are later fine-tuned on the Phrasecut dataset for the segmentation task. However, our work is orthogonally focused towards exploring and utilizing LDMs and its internal features for improving the text-based segmentation performance. Note that object detection datasets  have a good overlap with the visual content in PhraseCut, however, they are not representative of the diversity in images available on the internet. For example, while they could learn common concepts like sky, ocean, chair, table and their synonyms, methods like MDETR would not understand concepts like Mikey Mouse, Pikachu etc., which we will show in Section~\ref{discussion}.

\subsection{Generalization to AI Generated Images}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\columnwidth]{Images/AI_generated_stats}
%     \caption{Generalization of the proposed LD-ZNet on our AI-generated dataset when compared with other state-of-the-art text-based segmentation methods - CLIPSeg (PC+) and MDETR.}
%     \vspace{-1em}
%     \label{fig:ai_generated_chart}
% \end{figure} 

\begin{table}[t]
    \centering
    \begin{tabular}{|c||c|c|}
    \hline
         Method & mIoU & AP\\
         \hline
         MDETR \cite{kamath2021mdetr} & 53.4 & 63.8 \\
         CLIPSeg (PC+) \cite{luddecke2022image} & 56.4 & 79.0 \\
         \hline
         RGBNet & 63.4 & 84.1\\
         \rowcolor{lightgray} ZNet (Ours) & 68.4 & 85.0\\
         \rowcolor{lightgray} LD-ZNet (Ours) & \textbf{74.1} & \textbf{89.6}\\
    \end{tabular}
    \caption{Generalization of the proposed LD-ZNet on our AIGI dataset when compared with other state-of-the-art text-based segmentation methods.}
    \vspace{-1em}
    \label{tab:ai_generated_chart}
\end{table}

\begin{figure} [!t]
    \centering
    \begin{subfigure}[t]{0.24\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/AI_gen_final/Input/Mickey_mouse_as_an_evil_dictator.jpg}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/Input/medieval_goblin_eating_cakes_painted_by_hieronymus.jpg}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/Input/cute_cartoon_cat_eating_ramen_wearing_sunglasses.jpg}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/Input/voxel_art_of_animals_in_a_forest.jpg}
        \caption{Input}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}[t]{0.24\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/AI_gen_final/MDETR/Mickey_mouse_as_an_evil_dictator_mickey_mouse.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/MDETR/medieval_goblin_eating_cakes_painted_by_hieronymus_goblin.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/MDETR/cute_cartoon_cat_eating_ramen_wearing_sunglasses_ramen.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/MDETR/voxel_art_of_animals_in_a_forest_animals.png}
        \caption{MDETR \cite{kamath2021mdetr}}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}[t]{0.24\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/AI_gen_final/CLIPSeg/Mickey_mouse_as_an_evil_dictator_mickey_mouse.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/CLIPSeg/medieval_goblin_eating_cakes_painted_by_hieronymus_goblin.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/CLIPSeg/cute_cartoon_cat_eating_ramen_wearing_sunglasses_ramen.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/CLIPSeg/voxel_art_of_animals_in_a_forest_animals.png}
        \caption{CLIPSeg \cite{luddecke2022image}}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}[t]{0.24\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/AI_gen_final/LD-ZNet/Mickey_mouse_as_an_evil_dictator_mickey_mouse.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/LD-ZNet/medieval_goblin_eating_cakes_painted_by_hieronymus_goblin.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/LD-ZNet/cute_cartoon_cat_eating_ramen_wearing_sunglasses_ramen.png}
        \includegraphics[width=\linewidth]{Images/AI_gen_final/LD-ZNet/voxel_art_of_animals_in_a_forest_animals.png}
        \caption{LD-ZNet}
    \end{subfigure}
    
    \caption{Qualitative comparison on the AI-generated images for text-based segmentation. The text prompts are \emph{``Mickey mouse"}, \emph{``Goblin"}, \emph{``Ramen"} and \emph{``animals"} respectively.}
    \vspace{-1em}
    \label{fig:ai_generated_qualitative}
\end{figure}

% We also study the generalization ability of our proposed method to AI-generated images
With the growing popularity of AI generated images, text-based image segmentation is extensively being used by content creators in their daily workflows. Many public libraries \footnote{\href{https://github.com/brycedrennan/imaginAIry}{imaginAIry}, \href{https://github.com/AUTOMATIC1111/stable-diffusion-webui}{stable-diffusion-webui}} widely employ methods such as CLIPSeg \cite{luddecke2022image} for performing segmentation in AI-generated images. So we study the generalization ability of our proposed segmentation approach on AI-generated images. To this extent, we first prepare a dataset of 100 AI-generated images from lexica.art and manually annotate them using 214 text-prompts. We name this dataset AIGI and plan to release it for future research. Next, we evaluate our approaches ZNet and LD-ZNet along with our RGBNet baseline and other text-based segmentation methods - CLIPSeg (PC+) \cite{luddecke2022image}, MDETR \cite{kamath2021mdetr}. Glipv2 was not publicly available for us to evaluate at the time of this submission. All these methods are trained on the Phrasecut dataset and we measure the IoU metric as shown in Table \ref{tab:ai_generated_chart}. It can be seen that RGBNet outperforms CLIPSeg and MDETR because its built on the UNet architecture initialized from the LDM weights that contains semantic information for good generalization. Our methods ZNet and LD-ZNet further improve the generalization to these AI-generated images by more than 20\% compared to MDETR. This is largely due to the robust $z$-space of the LDM that resulted from a VQGAN pre-training on a variety of domains like art, cartoons, illustrations \etc. Furthermore, the latent diffusion features that contain useful semantic information for the synthesis task, also help in segmenting the AI-generated images. We show the qualitative comparison of these methods in Figure \ref{fig:ai_generated_qualitative} for four AI-generated images from our dataset. While CLIPSeg can estimate most distinctive regions such as face of the \emph{Mickey mouse} or rough locations of \emph{Goblin}, \emph{Ramen} and \emph{animals}, MDETR incorrectly segments them because these concepts are unknown to it and because of the domain gap between Phrasecut and AIGI images respectively. In both such cases, our proposed LD-ZNet estimates accurate segmentation. More qualitative results for LD-ZNet on images from the AIGI dataset are shown in Figure \ref{fig:ldznet_ai_generated}.


%\begin{figure}
%\centering
%\includegraphics[width=\linewidth]%{Images/Categories_chart_latest}
%\caption{Category-level comparison of RGBNet and LD-ZNet on the text-based image segmentation task on the 25 most frequent classes of the PhraseCut testset. LD-ZNet outperforms RBGNet for all ``object" classes and most ``stuff" classes.}%, based on the mIoU metric.}
%\vspace{-1em}
%\label{fig:category}
%\end{figure}



% \begin{figure}[!t]
%     \captionsetup[subfigure]{labelformat=empty}
%     \centering
%     \begin{subfigure}{0.3\columnwidth}
%         \centering
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/seattle_vangough_a photo of one Ice Cap Mountains._image.png}
%         \end{subfigure}
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/astronaut_Horse._image.png}
%         \end{subfigure}
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/umbrella_art_a photo of one woman._image.png}
%         \end{subfigure}
%         % \caption{Input image}
%         % \label{fig:architecture}
%         \vspace{-1em}
%     \end{subfigure}%
%     \hspace{0.05cm}%
%     \begin{subfigure}{0.3\columnwidth}
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{``Space Needle"}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/seattle_vangough_a photo of one space needle._mask.png}
%         \end{subfigure}
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{``Horse"}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/astronaut_Horse._mask.png}
%         \end{subfigure}
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{``Umbrella"}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/umbrella_art_a photo of one Umbrella._mask.png}
%         \end{subfigure}
%         \vspace{-1em}
%     \end{subfigure}%
%     \hspace{0.05cm}%
%     \begin{subfigure}{0.3\columnwidth}
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{``Ice cap mountains"}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/seattle_vangough_a photo of one Ice Cap Mountains._mask.png}
%         \end{subfigure}
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{``Astronaut"}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/astronaut_Astronaut._mask.png}
%         \end{subfigure}
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \caption{``Woman"}
%             \includegraphics[width=\columnwidth]{Images/visual/zseg_SD_features/umbrella_art_a photo of one woman._mask.png}
%         \end{subfigure}
%         \vspace{-1em}
%     \end{subfigure}
%     \caption{Qualitative results on a diverse variety of images show that LD-ZNet can correctly segment multiple text prompts given the same image. Images used from LDM generations at \href{https://lexica.art/}{lexica.art} (top two rows) and google (bottom row).}
%     \vspace{-1em}
%     \label{fig:visual_results2}
% \end{figure}

\begin{figure*}[!t]
    \captionsetup[subfigure]{labelformat=empty,font=small,labelfont={bf,sf},skip=0pt}
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \begin{subfigure}{0.32\textwidth}
            \centering
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/indoor_an_image_of_a_Books._image.png}
                % \smallskip
            \end{subfigure}
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Books"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/indoor_an_image_of_a_Books._mask.png}
            \end{subfigure}
        \end{subfigure}%
        \hspace{0.05cm}%
        \begin{subfigure}{0.32\textwidth}
            \centering
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Flowers"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/indoor_an_image_of_a_Flowers._mask.png}
            \end{subfigure}
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Sofa"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/indoor_an_image_of_a_sofa._mask.png}
            \end{subfigure}
        \end{subfigure}%
        \hspace{0.05cm}%
        \begin{subfigure}{0.32\textwidth}
            \centering
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Table"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/indoor_an_image_of_a_Table._mask.png}
            \end{subfigure}
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Trees"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/indoor_an_image_of_a_Trees._mask.png}
            \end{subfigure}
        \end{subfigure}
    \end{subfigure}%
    \rulesep
    % \noindent\rule{\textwidth}{0.4pt}
    \begin{subfigure}{\columnwidth}
        \centering
        \begin{subfigure}{0.32\textwidth}
            \centering
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/camping_an_image_of_a_Chair._image.png}
            \end{subfigure}
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Chair"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/camping_an_image_of_a_Chair._mask.png}
            \end{subfigure}
        \end{subfigure}%
        \hspace{0.05cm}%
        \begin{subfigure}{0.32\textwidth}
            \centering
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Clouds"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/camping_an_image_of_a_Clouds._mask.png}
            \end{subfigure}
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Grass"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/camping_an_image_of_a_Grass._mask.png}
            \end{subfigure}
        \end{subfigure}%
        \hspace{0.05cm}%
        \begin{subfigure}{0.32\textwidth}
            \centering
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``Mountains"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/camping_an_image_of_a_Mountains._mask.png}
            \end{subfigure}
            \begin{subfigure}{\columnwidth}
                \centering
                \caption{``River"}
                \includegraphics[width=\columnwidth]{Supp_Images/visual/zseg_SD_features/More_analysis/camping_an_image_of_a_River._mask.png}
            \end{subfigure}
        \end{subfigure}
    \end{subfigure}
    \caption{LD-ZNet text-based image segmentation results for a real image and a cartoon on diverse set of things and stuff classes. High quality segmentation across multiple classes suggests that LD-ZNet has a good understanding of the overall scene.}
    % Images used from google and \href{https://www.freepik.com}{freepik}.}
    \label{fig:scene_understanding}
\end{figure*}


\begin{table}[t]
    \centering
    \begin{adjustbox}{width=\columnwidth}
    \begin{tabular}{|c||c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Method} & \multicolumn{2}{|c|}{RefCOCO} & \multicolumn{2}{|c|}{RefCOCO+} & \multicolumn{2}{|c|}{G-Ref}\\
    \cline{2-7}
    & IoU & AP & IoU & AP & IoU & AP\\
    \hline
            CLIPSeg (PC+) \cite{luddecke2022image}& 30.1 & 14.1 & 30.3 & 15.5 & 33.8 & 23.7 \\
            \hline\hline
            RGBNet & 36.3 & 15.7 & 37.1 & 16.7 & 41.9 & 27.8 \\
            \rowcolor{lightgray}ZNet (Ours) & 40.1 & 16.8 & 40.9 & 17.8 & 47.1 & 29.2 \\
            \rowcolor{lightgray}LD-ZNet (Ours) & \textbf{41.0} & \textbf{17.2} & \textbf{42.5} & \textbf{18.6} & \textbf{47.8} & \textbf{30.8} \\
    \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Generalization of our proposed approaches to different types of expressions from other datasets. Z-Net and LD-ZNet outperform both the RGBNet baseline and CLIPSeg on the generalization across all datasets.}
    \vspace{-0.5em}
    \label{tab:ris_generalization}
\end{table}


\begin{table}
    \centering
    \begin{adjustbox}{width=0.95\columnwidth}
    \begin{tabular}{|c||c|c|c|c|}
    \hline
    Diffusion features via & mIoU & $IoU_{FG}$ & AP \\
    \hline
        LD-ZNet with concatenation & 50.2 & 59.0 & 78.1 \\
        LD-ZNet with cross-attention & \textbf{52.7} & \textbf{60.0} & \textbf{78.9} \\
    \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Incorporating LDM features into ZNet via cross-attention (LD-ZNet) leverages the visual-linguistic information present in them, compared to concatenation, leading to better performance on the text-based image segmentation task.}
    \label{tab:ris_concat_vs_crossattn}
\end{table}

\subsection{Generalization to Referring Expressions}
Reference expression segmentation task is aimed for robot-localization kind of applications, where segmenting at instance-level is performed through distinctive referring expression. Many works such as \cite{yang2022lavt, wang2022cris} also train the text encoder to learn the complex positional references in the text. However, we are focused on generic text-based segmentation that has support for stuff categories as well as for multiple instances. We study the generalization ability of the proposed approach - using LDM features, to this complex task. Specifically, we use the models trained on the PhraseCut dataset and evaluate them on the RefCOCO \cite{kazemzadeh2014referitgame}, RefCOCO+ \cite{kazemzadeh2014referitgame} and G-Ref \cite{nagaraja16refexp} datasets whose complex referring expressions are for single-instance localization and segmentation. We also evaluated the generalization of CLIPSeg (PC+) \cite{luddecke2022image} model that was trained on an extended version of the PhraseCut dataset (PC+), to further demonstrate the generalization capability of our methods. Table \ref{tab:ris_generalization} summarizes the performance for our models along with the RGBNet baseline. We observe a similar trend in performance improvements across RGBNet $< Z$Net $<$ LD-ZNet. These experiments demonstrate that the LDM features enhance the generalization power of the LD-ZNet model even on complex referring expressions.



\begin{figure}[!t]
    \captionsetup[subfigure]{labelformat=empty,font=small,labelfont={bf,sf},skip=0pt}
    \centering
    \begin{subfigure}{0.24\columnwidth}
        \centering
        \begin{subfigure}{\columnwidth}
            \centering
            \caption{``Hoodie"}
            \includegraphics[width=\columnwidth]{Images/AI_gen_final/LD-ZNet/plush_rabbit_coding_late_and_sunglasses_hoodie.png}
        % \vspace{0.005cm}
        \end{subfigure}
        \begin{subfigure}{\columnwidth}
            \centering
            \caption{``Spiderman"}
            \includegraphics[width=\columnwidth]{Images/AI_gen_final/LD-ZNet/spider-man_as_a_robot_serving_pizza_spider-man.png}
        \end{subfigure}
    \end{subfigure}
    \hspace{0.05cm}%
    \begin{subfigure}{0.24\columnwidth}
        \centering
        \begin{subfigure}{\columnwidth}
            \centering
            \caption{``Owl"}
            \includegraphics[width=\columnwidth]{Images/AI_gen_final/LD-ZNet/highly_detailed_owl.png}
        % \vspace{0.005cm}
        \end{subfigure}
        \begin{subfigure}{\columnwidth}
            \centering
            \caption{``Trump"}
            \includegraphics[width=\columnwidth]{Images/AI_gen_final/LD-ZNet/Polaroid_portrait_of_Barack_Obama_shaking_hands_wi_Donald_Trump.png}
        \end{subfigure}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}{0.24\columnwidth}
        \centering
        \begin{subfigure}{\columnwidth}
            \centering
            \caption{``Pikachu"}
            \includegraphics[width=\columnwidth]{Images/AI_gen_final/LD-ZNet/pikachu._comfy_pajamas_pikachu.png}
        % \vspace{0.005cm}
        \end{subfigure}
        \begin{subfigure}{\columnwidth}
            \centering
            \caption{``Joker"}
            \includegraphics[width=\columnwidth]{Images/AI_gen_final/LD-ZNet/the_joker_walking_through_streets_of_new_york_joker.png}
        \end{subfigure}
    \end{subfigure}%
    \hspace{0.05cm}%
    \begin{subfigure}{0.24\columnwidth}
        \centering
        \begin{subfigure}{\columnwidth}
            \centering
            \caption{``Godzilla"}
            \includegraphics[width=\columnwidth]{Images/AI_gen_final/LD-ZNet/godzilla_in_mexico_godzilla.png}
        % \vspace{0.005cm}
        \end{subfigure}
        \begin{subfigure}{\columnwidth}
            \centering
            \caption{``Eiffel"}
            \includegraphics[width=\columnwidth]{Images/AI_gen_final/LD-ZNet/france_en_super_vilain_eiffel_tower.png}
        \end{subfigure}
    \end{subfigure}
    \caption{More qualitative results of LD-ZNet from AIGI dataset.}
    \label{fig:ldznet_ai_generated}
    \vspace{-1em}
\end{figure}


% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{Images/Categories_chart}
% \caption{Category level comparison of RBGNet and LD-ZNet on the referring image segmentation task on the 25 most frequent classes of the PhraseCut test dataset. LD-ZNet outperforms RBGNet for all ``object" classes and most ``stuff" classes, based on the mIoU metric.}
% \vspace{-1em}
% \label{fig:category}
% \end{figure}

% \subsection{Category level improvements from LDM}
% We study the class-wise performance on the test dataset of PhraseCut for a more fine grained analysis of the quantitative improvement we see in Table~\ref{tab:ris_results}. 
% %We show the advantage of using LDM features by analyzing the category level improvements over predictions from the test dataset of PhraseCut in Figure \ref{fig:category}. 
% This dataset consists of 1180 categories. In order to draw meaningful conclusions, we discard the categories with less than 100 samples, leaving us with 25 categories. We compute the mIoU metric for the predictions averaged over each category and compare the RGBNet with our LD-ZNet model. The results of our analysis is in Figure \ref{fig:category}. 

% Note that the categories on the x-axis are listed in the decreasing order of relative performance gain upon using LD-ZNet. We observe that LD-ZNet has larger performance gains for object classes such as fence, person, sign, table, chair \etc compared to stuff classes such as floor, sky, street \etc. We believe this is because the large scale LDMs are typically good at generating salient objects which are predominantly in the foreground.


% \subsection{Generalization to referring object detection}
% \review{
% We show the use of LDM features for referring object detection (ROD), an instance level visual understanding task, in Table \ref{tab:rod_results_maskrcnn}. Specifically, we use the RGBNet, ZNet and LD-ZNet as the backbones and trained three Mask-RCNN object detection heads and report mAP on the handheld validation set of Phrasecut. We see a significant improvement going from RGBNet $ < Z$Net $<$ LD-ZNet confirming the usefulness of LDM features.
% }
% \begin{table}[!h]
%     \centering
%     \begin{adjustbox}{scale=0.7}
%     \begin{tabular}{|c||c|c|c|c|c|c|}
%     \hline
%     \multirow{Method} & \multicolumn{3}{|c|}{Object Detection} & \multicolumn{3}{|c|}{Instance Segmentation}\\
%     \cline{2-7}
%     & mAP & mAP50 & mAP75 & mAP & mAP50 & mAP75\\
%     \hline
%             RGBNet & 23.9 & 52.3 & 18.9 & 21.2 & 47.7 & 15.8 \\
%             \rowcolor{lightgray}ZNet (Ours) & 25.9 & 55.0 & 21.4 & 23.2 & 51.4 & 18.1 \\
%             \rowcolor{lightgray}$CA$-ZNet (Ours) & \textbf{27.9} & \textbf{58.6} & \textbf{23.0} & \textbf{25.6} & \textbf{55.2} & \textbf{20.7} \\
%     \hline
%     \end{tabular}
%     \end{adjustbox}
%     \caption{ROD performance on the PhraseCut validation dataset.}
%     \vspace{-1em}
%     \label{tab:rod_results_maskrcnn}
% \end{table}



% \begin{figure*}
% \centering
% \includegraphics[width=\linewidth]{Images/Categories_chart}
% \caption{Category level comparison between RBG baseline and our technique of using the LDM latent space intermediate features (Z +LDM features). The metric used for comparison is mIoU and the classes are from the PhraseCut test dataset. The performance of our technique is better for all ``object" classes and most ``stuff" classes.}
% \label{fig:category}
% \end{figure*}

% \subsection{Query segmentation for novel concepts}
% \subsection{Finer masks for Image editing}
%------------------------------------------------------------------------
