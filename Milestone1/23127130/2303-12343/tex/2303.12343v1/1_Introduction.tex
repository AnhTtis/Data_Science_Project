\section{Introduction}
\label{sec:intro}
%first talk about how is language based image segmentation changing the world

%The use of language-based image segmentation has exploded in the past year due to its ability to provide fine-grained control to a user for image synthesis and editing tasks \cite{}. For example, given an image, the algorithm extracts a fine-grained mask associated to a user-provided text prompt. This mask is then used as input in several downstream editing applications like inpainting \cite{}, image to image translation \cite{}, background replacement \cite{} etc. Unlike the semantic and instance segmentation tasks that limit predictions to a predefined set of categories, language-based segmentation can segment regions in the image in an open-world setting and can generalize well to semantically similar concepts not encountered during training. %

Text-based image segmentation is useful for several image editing applications such as inpainting \cite{couairon2022diffedit, Lugmayr_2022_CVPR, xie2022smartbrush}, image matting \cite{sun2021semantic, xu2017deep}, language-guided editing \cite{chen2018language} etc. Recently, it has also gained traction inside several text-based image synthesis workflows \footnote{\href{https://github.com/brycedrennan/imaginAIry}{imaginAIry}, \href{https://github.com/AUTOMATIC1111/stable-diffusion-webui}{stable-diffusion-webui}}, which often require fine-grained control over the synthesis pipeline. %
However, teaching networks to accurately find the boundaries of objects is hard and annotation of boundaries at internet scale is impractical. Also, most self-supervised or weakly supervised problems do not incentivize learning boundaries. For example, training on classification or captioning allows models to learn the most discriminative parts of the image without focusing on boundaries \cite{selvaraju2017grad, zeiler2014visualizing}. Our insight is that Latent Diffusion Models (LDMs) \cite{rombach2022high}, which can be trained without object level supervision at internet scale, must attend to object boundaries, and so we hypothesize that they can learn features which would be useful for text-based image segmentation.  We support this hypothesis by showing that LDMs can improve performance on this task by up to 6\%, compared to standard baselines and these gains are further amplified when LDM based segmentation models are applied on AI generated images.

% Segmenting images based on text provides a means to move towards open-world segmentation, whereas standard semantic and instance segmentation tasks are restricted to a predefined set of categories. Conditioning on the text modality naturally enables good generalization to semantically similar concepts not encountered during training. However, existing text-based segmentation methods mostly rely on text prompts from curated datasets with annotations for finite categories, which leads to a limited generalization to internet-scale data. At the same time, obtaining annotations for objects on an internet-scale is also implausible. To tackle this conundrum, we propose a novel approach to leverage the recently proposed text-to-image latent diffusion models (LDMs) \cite{rombach2022high} for the task of text-based segmentation. These LDMs are trained on large scale text-image pairs from the internet and are shown to generate images that are photo-realistic and semantically meaningful with the conditional text. Such a kind of pretraining can hold object-level knowledge from internet and offers another step towards open-world segmentation. To this end, we present novel architectures in the form of ZNet and LD-ZNet, which leverage the text-to-image LDMs that are internet-scale pretrained, for the task of text-based image segmentation. 

%Text-based image segmentation is useful for several image editing applications such as inpainting \cite{couairon2022diffedit, Lugmayr_2022_CVPR, xie2022smartbrush}, and language-guided editing \cite{chen2018language}. It has recently gained traction inside several text-based image synthesis workflows, \footnote{\href{https://github.com/brycedrennan/imaginAIry}{imaginAIry}, \href{https://github.com/AUTOMATIC1111/stable-diffusion-webui}{stable-diffusion-webui}} which often require fine-grained control over the synthesis pipeline. Interestingly, the models used in these image synthesis workflows \cite{rombach2022high} are trained on text-image pairs from the entire internet, therefore it is natural to think that their internal representations would also contain object-level semantic information, like detailed object boundaries. This is usually difficult to learn from other internet scale image-level pretraining tasks like image classification, image captioning etc \cite{} as these models only have an incentive to learn the most discriminative parts instead of the entire extent of an object. Alternately, object-level pretraining techniques such as MDETR \cite{kamath2021mdetr}, GLIP (v2) \cite{li2022grounded, zhang2022glipv2} learn semantic boundaries but do not generalize to internet scale due to a lack of detailed annotations. To this end, we present novel architectures in the form of ZNet and LD-ZNet, which leverage internet-scale text-to-image latent diffusion models (LDMs) \cite{rombach2022high}, for solving text-based image segmentation. %imagenet, JFT, BLIP, CLIP etc. To this end, we propose a novel approach by leveraging the internet-scale text-to-image LDMs for language-based image segmentation.

%The use of text-based image segmentation has exploded in the past year due to its ability to provide fine-grained control to a user for image synthesis and editing tasks \cite{}. For example, given an image, the algorithm extracts a fine-grained mask associated to a user-provided text prompt. This mask is then used as input in several downstream editing applications like inpainting \cite{}, image to image translation \cite{}, background replacement \cite{} etc. Unlike the semantic and instance segmentation tasks that limit predictions to a predefined set of categories, text-based segmentation can segment regions in the image in an open-world setting and can generalize well to semantically similar concepts not encountered during training

% Recently, language-based image synthesis became popular with the success of latent diffusion models (LDMs) \cite{} due to their ability to generate highly photo-realistic images from large scale text-image pairs from the internet. In this work, we investigate the usefulness of these text-to-image latent diffusion models for open-world image segmentation.

\begin{figure}[!t]
    \centering
        \centering
        \includegraphics[width=\linewidth]{Images/Motivation_new1}
        % \caption{Coarse segmentation from a pretrained LDM for two disctinct images, suggesting its internal features encode fine-grained object-level semantic information.}
        \caption{Coarse segmentation results from an LDM for two distinct images, demonstrating the encoding of fine-grained object-level semantic information within the model's internal features.}
        % {Given an image on the left, we compute the difference between the unconditional noise estimate from a pretrained LDM vs one conditioned on the text prompt. This difference represents a coarse segmentation of the astronaut being queried. These noise estimates are from an LDM which was not trained for the task of segmentation. The quality of the segmentation suggests that internal features of an LDM encodes fine-grained semantic information, that can be used for language-based segmentation.}
        \vspace{-1em}
        \label{fig:motivation1}
\end{figure}

\begin{figure*}[!t]
    \centering
        \centering
        \includegraphics[width=\linewidth]{Images/LDZNet-Summary.pdf}
        \caption{Overview of the proposed ZNet and LD-ZNet architectures. We propose to use the compressed latent representation $z$ as input for our segmentation network ZNet. Next, we propose LD-ZNet, which incorporates the latent diffusion features at various intermediate blocks from the LDM's denoising UNet, into ZNet.}
        \vspace{-1em}
        \label{fig:ldznet_summary}
\end{figure*}


To test the aforementioned hypothesis about the presence of object-level semantic information inside a pretrained LDM, we conduct a simple experiment. We compute the pixel-wise norm between the unconditional and text-conditional noise estimates from a pretrained LDM as part of the reverse diffusion process. This computation identifies the spatial locations that need to be modified for the noised input to align better with the corresponding text condition. Hence, the magnitude of the pixel-wise norm depicts regions that identify the text prompt. As shown in the Figure~\ref{fig:motivation1}, the pixel-wise norm represents a coarse segmentation of the subject although the LDM is not trained on this task. This clearly demonstrates that these large scale LDMs can not only generate visually pleasing images, but their internal representations encode fine-grained semantic information, that can be useful for tasks like segmentation. 

The success of LDMs for high-fidelity, photo-realistic and semantically meaningful text-to-image synthesis is largely attributed to their reliance on an efficiently compressed latent space $z$, extracted by a VQGAN \cite{esser2021taming}. In our experiments, we observe this $z$ to be a semantics-preserving compressed latent representation that acts as a better visual input to the segmentation network compared to the original image. Since the $z$ space is trained on several domains like art, cartoons, illustrations and real photographs, it is also a more robust input representation for text-based segmentation on AI-generated images. Furthermore, the internal layers of the LDM are responsible for generating the structure of the image and hence contain rich semantic information about objects. Soft masks from these layers have also been used as a latent input in recent work on image editing \cite{hertz2022prompt, brooks2022instructpix2pix}. Since this information is already present while generating the image, we propose an architecture in the form of LD-ZNet (shown in Figure~\ref{fig:ldznet_summary}) to decode it for obtaining the semantic boundaries of objects generated in the scene. Not only does our architecture benefit segmentation of objects in AI generated images, but it also improves performance over natural images. Overall our contributions are as follows: %
%1) We comprehensively evaluate the effectiveness of LDMs pretrained on large scale internet data for image segmentation based on text prompts. %
% \begin{itemize}
%     \item We propose a text-based segmentation architecture, ZNet that operates on the compressed latent space of the LDM ($z$).
%     \item Next, we study the internal representations at different stages of pretrained LDMs and show that they are useful for text-based image segmentation.
%     \item Finally, we propose a novel approach named LD-ZNet to incorporate the visual-linguistic latent diffusion features from a pretrained LDM and show improvements across several metrics and domains for text-based image segmentation.
% \end{itemize}
    \begin{itemize}
    \item We propose a text-based segmentation architecture, ZNet that operates on the compressed latent space of the LDM ($z$).
    \item Next, we study the internal representations at different stages of pretrained LDMs and show that they are useful for text-based image segmentation.
    \item Finally, we propose a novel approach named LD-ZNet to incorporate the visual-linguistic latent diffusion features from a pretrained LDM and show improvements across several metrics and domains for text-based image segmentation.
    \end{itemize}

%Segmenting images based on text prompts requires a fine-grained understanding of language and the spatial location of various objects in complex images. While many previous works \cite{} proposed ways to encode visual and linguistic information together, some of the latest works \cite{} proposed vision-language pretraining on other recognition tasks (object detection and phrase grounding) for later segmentation finetuning. However, these approaches are limited by the lack of object-level annotations (bounding boxes, masks \etc) at internet scale, making it unclear whether they can generalize to novel concepts like \emph{``Scarlett Johansson"}, \emph{``Taj Mahal"} \etc. However, the text-to-image LDMs, having been trained on large scale internet data, can not only synthesize images with such novel concepts, but also exhibit object-level understanding for them. Therefore, their internal features could benefit segmentation of novel concepts such as celebrity identities, famous landmarks, relative attributes of objects. For example, they can segment \emph{``Scarlett Johansson"} among a bunch of people in an image. Consequently, leveraging LDMs for segmentation algorithms additionally holds the potential to facilitate generalization to new concepts that surpass the standard categories present in the existing curated datasets.

% we explore the language-based segmentation task using the pretrained LDMs for their object-level understanding at internet-scale.

% Segmenting images based on text prompts requires a fine-grained understanding of language and the spatial location of various objects in complex images. To achieve this, many previous works \cite{} proposed different ways to encode the visual and linguistic information together such as []. Alternately, some of the latest works \cite{kamath2021mdetr, zhang2022glipv2} proposed vision-language pretraining on other recognition tasks such as object detection and phrase grounding for later finetuning on segmentation. Orthogonal to these approaches, we explore the recently proposed latent diffusion models (LDMs) \cite{rombach2022high} pretrained on large scale text-image paired data, for the task of language based image segmentation. These generative models were shown to generate the kind of photo-realistic images from text prompts that were unimaginable even a year ago due to their ability to learn from billions of text-image pairs \cite{schuhmann2022laionb} from the internet. This requires pixel-level photo-realism corresponding to several objects in the scene based on text. An interesting question thus arises - do these models also understand the underlying visual semantics of the input words and sentences?

% Unlike the image-level foundation models such as CLIP \cite{radford2021learning}, where the internal features activate on the most discriminative regions of the image such as face of a dog, the object-level foundation models such as MDETR \cite{kamath2021mdetr} or GLIP (v2) \cite{li2022grounded, zhang2022glipv2} can learn activations around the finer-boundaries of objects too, such as the boundary of the entire dog, which is crucial for visual recognition. However, these object-level models rely on object-level annotations such as bounding boxes or segmentation masks that are not available at internet scale, making it unclear whether they can generalize to novel concepts like \emph{``Scarlett Johansson"}, \emph{``Taj Mahal"} \etc. On the other hand, the LDMs, during synthesis, capture the underlying visual semantics of generalized text based prompts from large scale internet data. Therefore, their internal features could benefit segmentation of novel concepts such as celebrity identities, famous landmarks, relative attributes of objects. For example, they can segment \emph{``Scarlett Johansson"} among a bunch of people in an image. Consequently, leveraging LDMs for segmentation algorithms additionally holds the potential to facilitate generalization to new concepts that surpass the standard categories present in the existing curated datasets.

%This task is different from the referring expression segmentation task, which is typically useful for robot localization, where the goal is to spatially localize a {\em unique object} with a distinctive referring expression (that can moreover contain complex positional references requiring dedicated training of the language encoder). This is necessary because for image editing applications, the algorithm should be able to localize ``stuff" categories like clouds/ocean/beach etc. and also be able to generate masks for multiple objects, if they are applicable to the text prompt. %
%
 %
% by exploring the utility of the recently proposed latent diffusion models for this task.


% Segmenting images based on text prompts requires a fine-grained understanding of language and the spatial location of various objects in complex images. To achieve this, many previous works \cite{} proposed different ways to encode the visual and linguistic information together such as []. Alternately, some of the latest works \cite{kamath2021mdetr, zhang2022glipv2} proposed vision-language pretraining on other recognition tasks such as object detection and phrase grounding for later finetuning on segmentation. Orthogonal to these approaches, we explore the recently proposed latent diffusion models (LDMs) \cite{ramesh2021zero,saharia2022photorealistic,rombach2022high} pretrained on large scale text-image paired data, for the task of language based image segmentation. These generative models were shown to generate the kind of photo-realistic images from text prompts that were unimaginable even a year ago due to their ability to learn from billions of text-image pairs \cite{schuhmann2022laionb} from the internet. This requires pixel-level photo-realism corresponding to several objects in the scene based on text. An interesting question thus arises - do these models also understand the underlying visual semantics of the input words and sentences?

% In order to answer this question, we conduct a simple experiment. We compute the pixel-wise norm between the unconditional noise estimate from an LDM and the noise estimate from the same LDM conditioned on a piece of text relevant to the image. These noise estimates were obtained from the reverse diffusion process at a certain fixed timestep during inference. \review{This computation identifies the spatial locations that need to be modified for the noised input to align better with the corresponding text condition, as determined by the LDM. Hence, the magnitude of the pixel-wise norm depicts regions that identify the conditional text.} As shown in the \cref{fig:motivation1,fig:motivation2}, the pixel-wise norm represents a coarse segmentation of the subject referenced by the text prompt, despite the LDM not having been explicitly trained for the task of segmentation. This clearly demonstrates that these large scale LDMs can not only generate visually pleasing images, but their internal representations encode fine-grained semantic information, that can be useful for tasks such as segmentation. 

% \begin{figure}[!t]
%     \captionsetup[subfigure]{labelformat=empty}
%     \centering
%     \begin{subfigure}{0.5\linewidth}
%         \centering
%         \includegraphics[width=.9\linewidth]{Images/stonehedge_resized.png}
%         \includegraphics[width=.9\linewidth]{Images/golden-gate_resized.png}
%         \caption*{}
%         \vspace{-1.5em}
%         \label{fig:panda}
%     \end{subfigure}%
%     \begin{subfigure}{0.5\linewidth}
%         \centering
%         \includegraphics[width=.9\linewidth]{Images/stonehedge_0400_attention_overlay.png}
%         \includegraphics[width=.9\linewidth]{Images/golden-gate_0400_attention_overlay.png}
%         \caption*{}
%         \vspace{-1.5em}
%         \label{fig:panda_mask}
%     \end{subfigure}
%     \caption{Mask extractions on diverse images using a LDM through the same process described in Figure 1. The queries used to obtain these masks are \emph{``A picture of the stonehenge."} and \emph{``A picture of the golden gate bridge."} respectively.}
%     \vspace{-1.5em}
%     \label{fig:motivation2}
% \end{figure}



% have to synthesize photorealistic pixels corresponding to several objects in a scene based on text, therefore, it is intuitive to think that for visual recognition tasks which require understanding of fine-boundaries of objects, like segmentation, or other localization tasks like language based object detection, large scale LDMs may be more appropriate. 


% However, these approaches are still constrained to the concepts that are semantically similar to the categories from the training datasets and do not generalize to novel concepts at internet-scale. For example, the standard language based segmentation models fail to segment ``Donald Trump" from a bunch of people in an image. We focus on using the recently proposed latent diffusion model, pretrained on large scale dataset for the task of text-to-image. At the same time, getting mask annotations for internet-scale images is an extremely hard problem because of potentially non-finite plausible novel concepts. 

%talk more about z-net and LD-z-net instead of analysis
