\section{Discussion}
\label{discussion}



%Our paper approaches the text-based image segmentation task by exploring the utility of a text-%to-image LDM, pretrained on large scale internet data. Specifically, the latent space ($z$) of the first-stage is found to be a useful compressed representation over an original image. Also, we analyze the intermediate features of the second-stage for understanding the visual-linguistic semantics of an image, given a relevant text prompt. We demonstrate that the features from the latent space of an LDM are very useful not only for the text-based segmentation but also for the generalization to referring expressions. Figure~\ref{fig:motivation1}, contain motivating qualitative examples, which show that the LDM features contain sufficient semantic information to coarsely segment these images without the need for retraining or using additional decoder layers. Section~\ref{results} gives quantitative results comparing ZNet and LD-ZNet with the RGBNet and existing methods for text-based segmentation along with its superior generalization capability to the challenging referring expression segmentation task.
%Table~\ref{tab:ris_results} compares ZNet and LD-ZNet with the state of the art techniques on the referring image segmentation. Using our proposed method, LD-ZNet, we outperform all of these techniques except MDETR~\cite{kamath2021mdetr}, which is \review{pretrained for detection and phrase grounding and later finetuned for RIS}. Similarly, Table \ref{tab:ris_generalization} demonstrate the generalization capability of our methods to different reference texts. Additionally these two tables also show %effects of each of the individual contributions listed in our paper, i.e. 1)using the z-space instead of the image space along with 2)using intermediate LDM latent space features. 
%that while the contributions of the latent space $z$ and the internal features of LDM are effective in their own rights, the best results are obtained when applied together. We also analyzed the LDM features from different blocks and timesteps of the reverse diffusion process to determine which features contain the maximum information for referring image segmentation (Figure~\ref{fig:ldm_analysis}). Based on the results, the information contained in the features peaks at the $400^{th}$ time step and it drops off towards both extremes. Similarly the middle blocks {6, 7} in the UNet gives the best performance.



%Our paper analyzes the utility of the latent space ($z$) of large scale LDM's  and their intermediate features for understanding the semantics of an image. We use the referring image segmentation and the zero-shot referring image segmentation tasks to showcase our findings and demonstrate that the features from the latent space of an LDM are very useful both these tasks. Figures~\ref{fig:motivation1}\&~\ref{fig:motivation2}, contain motivating qualitative examples, which show that the LDM features contain sufficient semantic information to coarsely segment these images without the need for retraining or using additional decoder layers. Table~\ref{tab:ris_results} \& table~\ref{tab:ris_generalization} compares ZNet with CA-ZNet with the state of the art techniques on the referring image segmentation and the zero-shot referring image segmentation tasks. Using our proposed method, CA-ZNet, we outperform all of these techniques except MDETR~\cite{kamath2021mdetr}, which uses additional datasets and other losses/tasks for multi-modal training. Additionally these 2 tables also show %effects of each of the individual contributions listed in our paper, i.e. 1)using the z-space instead of the image space along with 2)using intermediate LDM latent space features. 
%that while ZNet and CA-ZNet contributions are effective in their own rights but give the best results when applied together. %Similarly table~\ref{tab:ris_generalization} displays the effectiveness of LDM intermediate features in Z domain for zero-shot referring image segmentation task. 

In this section we present more qualitative results to demonstrate several interesting aspects of our proposed technique when applied towards downstream segmentation tasks. %Figure~\ref{fig:visual_results} shows the original image and the GT mask along with outputs from the RGBNet baseline followed by ZNet and LD-ZNet, where both ZNet and LD-ZNet help improve results consistently. For example in the top row, RGBNet detects light fixtures for the ``hanging clock" prompt, and although ZNet does not have as strong activations for these incorrect detections, it is LD-ZNet that correctly segments the ``clock". Similarly in the bottom row, while RGBNet completely got the ``castle" wrong, ZNet correctly has activations on the right buildings, but with lower confidence. However, LD-ZNet improves it further. %In Figure~\ref{fig:category} we compare the RGBNet baseline with LD-ZNet for different categories in the PhraseCut test dataset and we see a consistent improvement in almost all categories except ``sky" and ``street". This suggests that LDM contain more information for foreground ``objects" vs background ``stuff". 
%
In \cref{fig:ai_generated_qualitative,fig:ldznet_ai_generated,fig:scene_understanding}, we visualize results of text-based image segmentation on a diverse set of images, which include AI generated images, illustrations and generic photographs.
%Specifically in Figure~\ref{fig:ldznet_ai_generated}, we show segmentation results on animated images and illustrations and that LD-ZNet, by virtue of using LDM features is able to perform better than RGBNet which fails due to the domain gap.  %\review{talk about figures 12 and 13}. 
%
 In Figure~\ref{fig:scene_understanding}, we show that when LD-ZNet is applied on the same image with various text prompts, and it is able to correctly segment the object and stuff classes being referred to in both examples. This capability is crucial for open-world segmentation and overall understanding of the scene. The results also highlights that the algorithm works remarkably well on other domains like cartoons/illustrations. It is noteworthy that LD-ZNet  can perform accurate segmentation for text prompts which include cartoons (Pikachu, Godzilla), celebrities (Donald Trump, Spiderman), famous landmarks (Eiffel Tower), as seen in Figure~\ref{fig:ldznet_ai_generated}.
%
% Finally Figure~\ref{fig:visual_results_supp} shows that the LDM features help LD-ZNet to correctly recognize color, relative height, action pose etc.
