\section{Experiments}
\label{experiments}

%\subsection{Implementation details}
\textbf{Implementation details:}
In this paper, we use the stable-diffusion v1.4 checkpoint as our LDM that internally uses the frozen ViT-L/14 CLIP text encoder \cite{radford2021learning}. We implement the above described $Z$Net and LD-$Z$Net in pytorch inside the stable-diffusion library.  We also initialize our networks with the weights from the LDM wherever possible, while initializing the remaining parameters from a normal distribution. We train $Z$Net and LD-$Z$Net on 8 NVIDIA A100 gpus with a batch size of 4 using the Adam optimizer and a base learning rate of 5$e^{-7}$ per mini-batch sample, per gpu. For all our experiments, we keep the text encoder frozen and use an image resolution of 384 for a fair comparison with the previous works.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{Images/Attentionpool.pdf}
    \caption{We propose to incorporate the visual-linguistic representations from LDM obtained at the spatial-attention modules via a cross-attention mechanism into the corresponding spatial-attention modules of the ZNet through an \emph{attention pool} layer.}
    \vspace{-1em}
    \label{fig:spatial-attention}
\end{figure}

%\subsection{Datasets}
\textbf{Datasets:}
We use Phrasecut \cite{wu2020phrasecut}, which is currently the largest dataset for the \textit{text-based image segmentation} task, with nearly 340K phrases along with corresponding segmentation masks that not only permit annotations for stuff classes but also accommodate multiple instances. Following \cite{radford2021learning}, we randomly augment the phrases from a fixed set of prefixes. For the images, we randomly crop a square around the object of interest with maximum area, ensuring that the object remains at least partially visible. We avoid negative samples to remove ambiguity in the LDM features for non-existent objects.

We create a dataset consisting of AI-generated images which we name \textbf{AIGI} dataset, to showcase the usefulness of our approach for text-based segmentation on a different domain. We use 100 AI-generated images from {\em lexica.art} and manually annotated multiple regions for 214 text-prompts relevant to these images. 

We also use the popular referring expression segmentation datasets namely RefCOCO \cite{kazemzadeh2014referitgame}, RefCOCO+ \cite{kazemzadeh2014referitgame} and G-Ref \cite{nagaraja16refexp} to demonstrate the generalization abilities of ZNet and LD-ZNet. In RefCOCO, each image contains two or more objects and each expression has an average length of 3.6 words. RefCOCO+ is derived from RefCOCO by excluding certain absolute-location words and focuses on purely appearance based descriptions. For example it uses ``the man in the yellow polka-dotted shirt‚Äù rather than ``the second man from the left" which makes it more challenging. Unlike RefCOCO and RefCOCO+, the average length of sentences in G-Ref is 8.4 words, which have more words about locations and appearances. While we adopt the UNC partition for RefCOCO and RefCOCO+ in this paper, we use the UMD partition for G-Ref.

%\subsection{Metrics} 
\textbf{Metrics:}
We follow the evaluation methodology of \cite{luddecke2022image} and report best foreground IoU ($IoU_{FG}$) for the foreground pixels, the best mean IoU of all pixels (mIoU), and the Average Precision (AP).