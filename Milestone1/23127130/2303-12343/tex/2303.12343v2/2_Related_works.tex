\section{Related work}
\label{relatedworks}
\subsection{Text-based image segmentation}
Text-based image segmentation
is the general task of segmenting specific regions in an image, based on a text prompt. This is different from the referring expression segmentation (RES) task, which aims to extract instance-level segmentation of different objects through distinctive referring expressions. While RES helps applications in robotics that require localization of a {\em single} object in an image, text-based segmentation benefits image editing applications by being able to also segment 1) ``stuff" categories (clouds/ocean/beach \etc) and 2) multiple instances of an object category applicable to the text prompt. However, both these tasks have some shared literature in terms of approaches. Preliminary works \cite{hu2016segmentation, liu2017recurrent, shi2018key, li2018referring, ye2019cross} focused on the multi-modal feature fusion between the language and visual representations obtained from recurrent networks (such as LSTM) and CNNs respectively. The subsequent set of works \cite{margffoy2018dynamic, yu2018mattnet, wang2022cris, yang2022lavt} included variations of multi-modal training, attention and cross-attention networks etc. Recently, \cite{wang2022cris, luddecke2022image} used CLIP \cite{radford2021learning} to extract visual linguistic features of the image and the reference text separately. These features were then combined using a transformer based decoder to predict a binary mask. Alternately, \cite{kamath2021mdetr, zhang2022glipv2}, proposed vision-language pretraining on other text-based visual recognition tasks (object detection and phrase grounding) and later finetuned for the segmentation task. \camready{The concurrent works segment-anything (SAM) \cite{kirillov2023segment} and segment-everything-everywhere-all-at-once (SEEM) \cite{zou2023segment} allow interactive segmentation via point clicks, bounding boxes and text inputs \etc. demonstrating good zero-shot performance.} Different from all these works, we show the significance of using the latent space and the internal features from a pretrained latent diffusion model \cite{rombach2022high} for improving the more generic text-based image segmentation task.

%This task is different from the referring expression segmentation task, which is typically useful for robot localization, where the goal is to spatially localize a {\em unique object} with a distinctive referring expression (that can moreover contain complex positional references requiring dedicated training of the language encoder). This is necessary because for image editing applications, the algorithm should be able to localize ``stuff" categories like clouds/ocean/beach etc. and also be able to generate masks for multiple objects, if they are applicable to the text prompt.
\subsection{Text-to-Image synthesis}
Text-to-Image synthesis has initially been explored using GANs \cite{Xu_2018_CVPR, Zhu_2019_CVPR, tao2022df, zhang2021cross, ye2021improving, zhou2022towards} on publicly available image captioning datasets. Another line of work is by using autoregressive models \cite{ramesh2021zero, NEURIPS2021_a4d92e2c, gafni2022make} via a two stage approach. The first stage is a vector quantized autoencoder such as a VQVAE \cite{van2017neural, razavi2019generating} or a VQGAN \cite{esser2021taming} with an image reconstruction objective to convert an image into a shorter sequence of discrete tokens. This low dimensional latent space enables the training of compute intensive autoregressive models even for high resolution text-to-image synthesis. With the recent advancements in Diffusion Models (DM) \cite{nichol2021improved,NEURIPS2021_49ad23d1}, both in unconditional and class conditional settings, they have started gaining more traction compared to GANs. Their success in the text-to-image tasks \cite{saharia2022photorealistic,ramesh2022hierarchical} made them even more popular. However, the prior diffusion models worked in the high-dimensional image space that made training and inference computationally intensive. Subsequently, latent space representations \cite{nichol2021glide, gu2022vector, tang2022improved, rombach2022high} were proposed for high resolution text-to-image synthesis to reduce the heavy compute demands. More specifically, the latent diffusion model (LDM) \cite{rombach2022high} mitigates this problem by relying on a perceptually compressed latent space produced by a powerful autoencoder from the first stage. Moreover, they employ a convolutional backed UNet \cite{UNet} as the denoising architecture, allowing for different sized latent spaces as input. %
%
Recently this architecture is trained on large scale text-image data \cite{schuhmann2022laionb} from the internet and released as Stable-diffusion\footnote{\href{https://github.com/CompVis/stable-diffusion}{https://github.com/CompVis/stable-diffusion}}, which exhibited photo-realistic image generations. Subsequently, several language guided image editing applications such as inpainting \cite{couairon2022diffedit, Lugmayr_2022_CVPR, xie2022smartbrush}, text-guided image editing \cite{chen2018language, brooks2022instructpix2pix} became more popular and the usage for text-based image segmentation has surged, especially for AI generated images. We propose a solution for text-based image segmentation by leveraging the features which are already present as part of the synthesis process.

\begin{figure}[t]
    \centering
        \centering
        \includegraphics[width=\linewidth]{Images/ZSEG-VQGAN}
        \caption{Reconstructions from the first stage of the LDM. Given an input image, the latent representation $z$ generated by the encoder, can be used to reconstruct images that are perceptually indistinguishable from the inputs. The high quality of these reconstructions suggests that the latent representation $z$, preserves most of the semantic information present in the input images.}
        \vspace{-1em}
        \label{fig:vqgan}
\end{figure}

\subsection{Semantics in generative models}
Semantics in generative models
such as GANs have been studied for binary segmentation~\cite{voynov2021object,melas2021finding} as well as multi-class segmentation~\cite{zhang2021datasetgan, tritrong2021repurposing, pakhomov2021segmentation} where the intermediate features have been shown to contain semantic information for these tasks. Moreover, \cite{semanticGAN} highlighted the practical advantages of these representations, such as out-of-distribution robustness. However, prior generative models (GANs \etc) as representation learners have received less attention compared to alternative unsupervised methods \cite{pmlr-v119-chen20j}, because of the training difficulties on complex, diverse and large scale datasets. Diffusion models \cite{nichol2021improved}, on the other hand are another class of powerful generative models that recently outperformed GANs on image synthesis \cite{NEURIPS2021_49ad23d1} and are able to train on large datasets such as Imagenet \cite{deng2009imagenet} or LAION \cite{schuhmann2022laionb}. In \cite{baranchuk2021label}, the authors demonstrated that the internal features of a pre-trained diffusion model were effective at the semantic segmentation task. However, this type of analysis \cite{zhang2021datasetgan, baranchuk2021label} has mostly been done in limited settings like few shot learning \cite{fei2006one} or limited domains like faces \cite{karras2019style}, horses \cite{yu15lsun} or cars \cite{yu15lsun}. Different from these works, we analyze the visual-linguistic semantic information present in the internal features of a text-to-image LDM \cite{rombach2022high} for text based image segmentation, which is an open world visual recognition task. %
%
Furthermore, we leverage these LDM features and show performance improvements when training with full datasets instead of few-shot settings.
