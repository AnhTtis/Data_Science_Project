\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{xcolor,colortbl}
% \usepackage[table,x11names]{xcolor}
\usepackage{adjustbox}
\usepackage{multirow}

% \usepackage{caption}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage{dirtytalk}
% Support for easy cross-referencing

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{xspace}
\usepackage{mwe}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{12703} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\newcommand{\camready}[1]{\textcolor{black}{#1}}
\newcommand{\links}[1]{\textcolor{magenta}{#1}}
\newcommand{\rulesep}{\unskip\ \vrule\ }

% \begin{figure*}[!t]
%     \centering
%     \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images/Motivation}
%         \caption{}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.65\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images/LDZNet-Summary}
%         \caption{}
%     \end{subfigure}
%         \caption{}
%         \vspace{-1.5em}
%         \label{fig:ldznet_summary}
% \end{figure*}

\def\@fnsymbol#1{\ensuremath{\ifcase#1\or *\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\newcommand{\ssymbol}[1]{^{\@fnsymbol{#1}}}

\begin{document}

%%%%%%%%% TITLE
% \title{Leveraging Text-To-Image Models for Text-Based Image Segmentation}
\title{LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation}
\author{Koutilya PNVR$\ssymbol{2}$\thanks{This work was done when Koutilya and Bharat were at Amazon.}
\and
Bharat Singh$\ssymbol{3}$$^\ast$
\and
Pallabi Ghosh$\ssymbol{4}$
\and
Behjat Siddiquie$\ssymbol{4}$
\and
David Jacobs$\ssymbol{2}$
\and
University of Maryland College Park$\ssymbol{2}\qquad$\hspace{0.3cm}Vchar.ai$\ssymbol{3}\qquad\qquad$\hspace{1.6cm}Amazon$\ssymbol{4}\qquad\qquad$\hspace{0.2cm}\\
{\tt\small $\{$koutilya, djacobs$\}$@umiacs.umd.edu$\qquad$bharat@vchar.ai$\qquad\{$gpallabi, behjats$\}$@amazon.com}
% \and
% \url{https://koutilya-pnvr.github.io/LD-ZNet/}
}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal
\thispagestyle{empty}
\fi


\begin{abstract}
% Lala \url{www.example.com}.

% \urlstyle{sf}
% Lala \url{www.example.com}.

% \renewcommand\UrlFont{\color{red}\rmfamily\itshape}
% Lala \url{www.example.com}.
\camready{Large-scale pre-training tasks like image classification, captioning, or self-supervised techniques do not incentivize learning the semantic boundaries of objects. However, recent generative foundation models built using text-based latent diffusion techniques may learn semantic boundaries. This is because they have to synthesize intricate details about all objects in an image based on a text description. Therefore, }we present a technique for segmenting real and AI-generated images using latent diffusion models (LDMs) trained on internet-scale datasets. First, we show that the latent space of LDMs (z-space) is a better input representation compared to other feature representations like RGB images or CLIP encodings for text-based image segmentation. By training the segmentation models on the latent z-space, which creates a compressed representation across several domains like different forms of art, cartoons, illustrations, and photographs, we are also able to bridge the domain gap between real and AI-generated images. We show that the internal features of LDMs contain rich semantic information and present a technique in the form of LD-ZNet to further boost the performance of text-based segmentation. Overall, we show up to 6\% improvement over standard baselines for text-to-image segmentation on natural images. For AI-generated imagery, we show close to 20\% improvement compared to state-of-the-art techniques. \camready{The project is available at \href{https://koutilya-pnvr.github.io/LD-ZNet/}{https://koutilya-pnvr.github.io/LD-ZNet/}.}
% \href{}{https://koutilya-pnvr.github.io/LD-ZNet/}{}.}

%We present a technique for text-based image segmentation using the recently proposed text-to-image latent diffusion models (LDMs). In our experiments, we find that the compressed latent space of the LDM ($z$) is a better visual representation for this task compared to the commonly used inputs such as RGB images or CLIP features. Based on this finding, we propose ZNet, a segmentation approach based on text with the latent space as the visual input. Next, we study the internal features of the LDM in controlled settings and based on this analysis, we present a technique in the form of LD-ZNet to utilize these latent diffusion features for further performance gain. Experiments on several datasets show that LD-ZNet, which combines the usage of the compressed latent space and the latent diffusion features, can improve the performance of text-based segmentation by as much as 6\% compared to standard features. We are the first to show the utility of an LDM for text-based segmentation on various domains including AI-generated images.

% While there are several studies which leverage better fusion architectures for visual-linguistic representations or pretraining on other recognition tasks for object-level features; for the first time, we show that our network ZNet, which is trained on the compressed latent space (VQGAN encoding) of the LDM obtains better results compared to commonly used inputs like RGB images or CLIP features, for text-based image segmentation. Furthermore, we study the internal features of the LDM in controlled settings and based on this analysis, we present a technique in the form of LD-ZNet to utilize these latent diffusion features for further performance gain. Experiments on several datasets show that LD-ZNet, which combines the usage of the compressed latent space and the latent diffusion features can improve the performance of text-based image segmentation tasks by as much as 6\%.
\end{abstract}

%%%%%%%%% ABSTRACT
% \begin{abstract}
%     Being able to segment regions of an image using text prompts paves the way towards open-world recognition. This avoids limiting segmentation to predetermined set of categories. Previous works approached this problem by either proposing better architectures for feature fusion between language and visual features or relying on a pretraining mechanism using other recognition tasks for learning object-level features. Different from these, we explore the recently proposed text-to-image latent diffusion models (LDMs) for the task of language based image segmentation. The success of LDMs for high-fidelity, photo-realistic and semantically meaningful text-to-image synthesis is largely attributed to their reliance on an efficiently compressed latent space. %
%     However, the information encoded in the internal representations of the LDM is not very well understood. 
%     % However, the internal representations of LDMs, like the input on which they operate or the information encoded inside them is not very well understood. 
%     In this work we shed light on these representations by analyzing the semantic information present in the internal visual-linguistic features of a LDM, pretrained on large scale data. We show that these features can be used to improve the performance of a language based image segmentation task by as much as 6\% when using state-of-the-art transformer based architectures. To the best of our knowledge, we are the first to show the utility of the features inside an LDM on an open-world segmentation task.
% \end{abstract}
%\vspace{-1.5em}

%------------------------------------------------------------------------
% Introduction
\input{1_Introduction.tex}

%------------------------------------------------------------------------
% RELATED WORKS
\input{2_Related_works.tex}

%------------------------------------------------------------------------
% Method
\input{3_Method.tex}

%------------------------------------------------------------------------
% Experiments
\input{4_Experiments.tex}

%------------------------------------------------------------------------
% Results
\input{5_Results.tex}

%------------------------------------------------------------------------
% Ablation studies
\input{6_Ablation_studies.tex}

% Discussion
\input{7_Discussion.tex}

\section{Conclusion}
% In this paper we have explored the utilization of a text-to-image LDM, pretrained on large scale internet data for the task of langugage-based image segmentation. We show significant quantitative performance gain (as much as 6\%) using both the latent $z$ space as well as the LDM intermediate features. Also, we quantitatively demonstrated the utility of LDM features for generalization to the challenging task of referring expression segmentation. Additionally, we depicted qualitative results using our technique on a diverse real, animation, art, and celebrity images. To the best of our knowledge we are the first work to analyze the use of a text-to-image LDM for an open-world language based segmentation task. Based on our findings, LDMs have enough semantic information to improve the language-based segmentation tasks. We are also hopeful that the proposed technique can open more research efforts towards generalizing to the internet-scale novel concepts without requiring annotations.  

%This paper presents a novel approach for text-based image segmentation using a text-to-image latent diffusion model, pretrained on large-scale internet data. Through extensive experimentation, we demonstrated significant quantitative performance improvements (up to 6\%) by leveraging both the latent $z$ space and the LDM intermediate features. Moreover, our results showcase the potential of LDM features for generalizing to the challenging task of referring expression segmentation, and we have presented qualitative results on diverse real-world images, including art, animation, and celebrity images. To the best of our knowledge, our study represents the first analysis of the use of a text-to-image LDM for open-world text-based segmentation tasks, and our findings suggest that LDMs contain sufficient semantic information to improve such tasks. Furthermore, our proposed technique has the potential to spur additional research efforts towards generalizing to internet-scale novel concepts without requiring annotations.

We presented a novel approach for text-based image segmentation using large scale latent diffusion models. By training the segmentation models on the latent z-space, we were able to improve the generalization of segmentation models to new domains, like AI generated images. We also showed that this z-space is a better representation for text-to-image tasks in natural images. By utilizing the internal features of the LDM at appropriate time-steps, we were able to tap into the semantic information hidden inside the image synthesis pipeline using a cross-attention mechanism, which further improved the segmentation performance both on natural and AI generated images. This was experimentally validated on several publicly available datasets and on a new dataset of AI generated images, which we will make publicly available. 

\label{conclusion}

\section{Acknowledgments}
Koutilya PNVR and David Jacobs were supported in part by the National Science Foundation under grant number IIS-1910132 and IIS-2213335.
% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit
% your finished paper. We MUST have this form before your paper can be
% published in the proceedings.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{ms}
}

% %%%%%%%%%% Merge with supplemental materials %%%%%%%%%%
% Supplementary
% \twocolumngrid
\clearpage
% \twocolumngrid
% \begin{center}
% \large\textbf{Supplementary Material}
% \end{center}
\twocolumn[{%
 \centering
 \LARGE \textbf{Supplementary Material}\\[1em]
 % \large Author: Anton van der Vegt\\[1em]
}]
%%%%%%%%%% Merge with supplemental materials %%%%%%%%%%
%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%

\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\setcounter{section}{0}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
% \renewcommand{\bibnumfmt}[1]{[S#1]}
% \renewcommand{\citenumfont}[1]{S#1}
\renewcommand{\thesection}{S\arabic{section}}
%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%

\input{Supplementary}

\end{document}