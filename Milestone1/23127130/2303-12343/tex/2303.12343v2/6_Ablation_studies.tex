\subsection{Cross-attention vs Concat for LDM features}
\label{ablations}


% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{Images/Categories_chart_latest}
% \caption{Category-level comparison of RGBNet and LD-ZNet on the text-based image segmentation task on the 25 most frequent classes of the PhraseCut testset. LD-ZNet outperforms RBGNet for all ``object" classes and most ``stuff" classes.}%, based on the mIoU metric.}
% \vspace{-1em}
% \label{fig:category}
% \end{figure}

%\textbf{Category level improvements with LDM features}
%We study the class-wise performance on the testset of PhraseCut for a more fine grained analysis of the quantitative improvement we see in Table~\ref{tab:ris_results}. 
%We show the advantage of using LDM features by analyzing the category level improvements over predictions from the test dataset of PhraseCut in Figure \ref{fig:category}. 
%This dataset consists of 1180 categories. In order to draw meaningful conclusions, we discard the categories with less than 100 samples, leaving us with 25 categories. We compute the mIoU metric for the predictions averaged over each category and compare the RGBNet baseline with our LD-ZNet model. The results of our analysis is in Figure \ref{fig:category}. 
%Note that the categories on the x-axis are listed in the decreasing order of relative performance gain upon using LD-ZNet. We observe that LD-ZNet has larger performance gains for \emph{object classes} such as fence, person, sign, table, chair \etc compared to \emph{stuff classes} such as floor, sky, street \etc. We believe this is because the large scale LDMs are typically good at generating salient objects which are predominantly in the foreground.

% We showcase the importance of each of the components in the proposed ZSEG: Latent diffusion features, CLIP Image features.
%\subsection{Z space vs RGB Image space}

%\subsection{Limitations of LDM on negative samples}
%In continuation to the proposed models from Table \ref{tab:ris_results}, we also trained two separate models that additionally includes negative samples constructed from the PhraseCut dataset. We do this following \cite{luddecke2022image} with a probability of $q_{neg}=0.2$ where no object from the image matches the prompt and the corresponding ground-truth masks are all zeros. Specifically, the sampleâ€™s phrase is replaced by a different phrase randomly chosen from the dataset. We report the test performance of these models on PhraseCut test dataset at Table \ref{tab:ris_negative_samples} and we observe that the gain in incorporating LDM features is not as much as we observed in Table \ref{tab:ris_results}. We believe this is because LDM was trained on positive examples only in the first place, as is the case with the training of any diffusion model. Thus when a negative sample appears in the training data, there is little help from the LDM features, yielding diminished gains overall.
%\begin{table}
%    \centering
%    \begin{adjustbox}{width=\columnwidth}
%    \begin{tabular}{|c||c|c|c|c|}
%    \hline
%    Method & mIoU & $IoU_{FG}$ & AP \\
%    \hline
%        Z (+ neg)& 50.24 & 58.54 & 79.11 \\
%        Z + LDM features (+ neg)& \textbf{51.47} & \textbf{59.29} & \textbf{79.6} \\
%    \hline
%    \end{tabular}
%    \end{adjustbox}
%    \caption{Having negative samples in the dataset results in diminished gains with LDM features}
    %\label{tab:ris_negative_samples}
%\end{table}

In LD-ZNet, we inject LDM features into the ZNet model using cross-attention (Figure \ref{fig:spatial-attention}). In order to understand the importance of the cross-attention layer, we also train and evaluate another model where the LDM features are concatenated with the features of the ZNet right before the spatial-attention layer. The results are summarized in Table \ref{tab:ris_concat_vs_crossattn} and it shows that concatenating the LDM features yields inferior results compared to the proposed method. This is because of the \emph{attention pool} layer which serves as a learnable layer and also encodes positional information into the LDM features for setting up the cross-attention. Moreover, the cross-attention layer learns how feature pixels from the ZNet attend to feature pixels from the LDM, thereby leveraging context and correlations from the entire image. With concatenation however, we only fuse the corresponding features of LDM and ZNet which is sub-optimal. 

% \begin{table}
%     \centering
%     \begin{adjustbox}{width=\columnwidth}
%     \begin{tabular}{|c||c|c|c|c|}
%     \hline
%     Diffusion features via & mIoU & $IoU_{FG}$ & AP \\
%     \hline
%         Concatenation & 50.22 & 58.04 & 78.19 \\
%         Cross-attention & \textbf{52.7} & \textbf{59.12} & \textbf{78.93} \\
%     \hline
%     \end{tabular}
%     \end{adjustbox}
%     \caption{Cross-attention of the diffusion features into the ZNet results in better usage of visual-linguistic information compared to concatenation}
%     \label{tab:ris_concat_vs_crossattn}
% \end{table}

%------------------------------------------------------------------------
