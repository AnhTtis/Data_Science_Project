\section{LDMs for Text-Based Segmentation}
\label{ldm analysis}
%reminder about how LDMs are awesome, where are they trained, brief overview of their architecture. Which part is GAN and which part is diffusion model.

%The latent diffusion architecture introduced in \cite{rombach2022high} is a diffusion model applied in the latent space of images generated using a powerful auto-encoder trained on a large dataset. These models are capable of generating photo-realistic images when conditioned on a variety of modalities such as text, class labels, semantic maps and for tasks such as inpainting and super-resolution. The latent diffusion architecture consists of two stages: \
The text-to-image latent diffusion architecture introduced in \cite{rombach2022high} consists of two stages:
%\vspace{-0.5em}
%\begin{enumerate} 
%\item 
1) An auto-encoder based VQGAN \cite{esser2021taming} that extracts a compressed latent representation ($z$) for a given image 
%\vspace{-0.5em}
%\item 
2) A diffusion UNet that is trained to denoise the noisy $z$ created in the forward diffusion process, conditioned on the text features. These text features are obtained from a pretrained frozen CLIP text encoder \cite{radford2021learning} and is conditioned at multiple layers of the UNet via cross-attention. 
%\end{enumerate} 
%\vspace{-0.5em}
% {\color{blue} We experiment with the output of both these stages as described in sections~\ref{sec:znet} and~\ref{sec:caznet} respectively.} These models are capable of generating photo-realistic images when conditioned on a variety of modalities such as text, class labels, semantic maps and for tasks such as inpainting and super-resolution.
%For text-to-image synthesis, the text features obtained through a pre-trained CLIP network are used as an input at multiple layers of the UNet via cross-attention, thereby guiding the image synthesis process. Recently this architecture was trained on large scale datasets (laion-2B-en and laion-aesthetics v2.5+ \cite{schuhmann2022laionb}), and released as the  Stable-diffusion\footnote{\href{https://github.com/CompVis/stable-diffusion}{https://github.com/CompVis/stable-diffusion}} model. In this paper, we analyze the intermediate visual-linguistic representations of this model and show performance improvements on a text-based visual recognition task upon using these features.
% During test time, given a text prompt, a latent feature is eventually recovered by the diffusion UNet over multiple timesteps, starting with a random noise. This synthesized latent feature is then finally decoded back into the image space using the decoder of the trained VQGAN from the first stage. 
% In this paper, we analyze the compressed latent space from the first-stage and the internal visual-linguistic representations from the second-stage of the stable-diffusion model and show performance improvements on a language-based image segmentation task upon using these features.

In this paper, we show performance improvements on the text-based segmentation task in two steps. Firstly, we analyze the compressed latent space ($z$) from the first-stage and propose an approach named ZNet that uses $z$ as the visual input to estimate segmentation mask when conditioned on a text prompt. Secondly, we study the internal representations from the second stage of the stable-diffusion LDM for visual-linguistic semantic information and propose a way to utilize them inside ZNet for further improvements in the segmentation task. We name this approach as LD-ZNet.

% This stable diffusion model was trained for text-to-image synthesis on large scale datasets (laion-2B-en and laion-aesthetics v2.5+ \cite{schuhmann2022laionb}) using the text features obtained through a pre-trained CLIP network as an input at multiple layers of the UNet via cross-attention.

\subsection{ZNet: Leveraging Latent Space Features}
\label{sec:znet}

%Mention how you extract z features from transformer based VQ-GAN. Describe briefly the architecture of the GAN, what is the size of these features etc. which get extracted. Where was it trained? How do reverse decodings look like? Show that it is able to capture visual information in a compressed format by showing reconstructions of a few images. Hence this is a good feature
%The first stage of the LDM extracts a latent representation ($z$) of the image using an auto-encoder. 

We observe that the latent space ($z$) from the first-stage of the LDM is a compressed representation of the image that preserves semantic information, as depicted in Figure \ref{fig:vqgan}. The VQGAN in the first-stage achieves such semantic-preserving compression with the help of large scale training data as well as a combination of losses - perceptual loss \cite{zhang2018unreasonable}, a patch-based \cite{isola2017image} adversarial objective \cite{dosovitskiy2016generating, esser2021taming, yu2021vector}, and a KL-regularization loss.

% {\color{blue} For our ZNet experiments we use the auto-encoder layers of the LDM without the diffusion layers.}
% This auto-encoder consists of a sequence of ResNet blocks and strided-convolutions, generating a compressed feature representation $z$, which is used as an input for the next stage. This feature representation $z$, is then transformed back into the image domain using a decoder that consists of another sequence of ResNet blocks and deconvolutions. This latent space is obtained through perceptual loss based compression \cite{zhang2018unreasonable}, a patch-based \cite{isola2017image} adversarial objective \cite{dosovitskiy2016generating, esser2021taming, yu2021vector}, and a KL-regularization loss. %Using a combination of these losses, LDMs are able to generate high quality images without compromising on the finer-details, while also having relatively low compute requirements. 
% When trained on a large-scale dataset in conjunction with the aforementioned losses, the auto-encoder, despite having a high-compression rate, retains most of the information present in the input image as shown in Figure \ref{fig:vqgan} {\color{blue} while having relatively low compute}. %We first evaluate the effectiveness of the z-space instead of RGB images as an input for visual recognition tasks. %We first propose a baseline with the z-space as input for the referring image segmentation task and show improvements on incorporating the LDM features from the second stage. 

In our experiments, we observe that this compressed latent representation $z$ is more robust compared to the original image in terms of their association with the text prompts. We believe this is because $z$ is a $\frac{H}{8} \times  \frac{W}{8} \times 4$ dimensional feature with 48 $\times$ fewer elements compared to the original image, while preserving the semantic information. Several prior works \cite{turk1991eigenfaces,ke2004pca,de2001robust}, show that compression techniques like PCA, which create information preserving lower dimensional representations generalize better. Therefore, we propose using the $z$ representation along with the frozen CLIP text features \cite{radford2021learning} as an input to our segmentation network. Furthermore, because the VQGAN is trained across several domains like art, cartoons, illustrations, portraits, etc., it learns a robust and compact representation which generalizes better across domains, as can be seen in our experiments on AI generated images. We call this approach ZNet. The architecture of ZNet is shown in the bottom box of Figure \ref{fig:ldznet_summary}, and is the same as the denoising UNet module of the LDM. We therefore initialize it with pretrained weights of the second-stage of the LDM.

%A primary motivation for using this $Z$Net architecture is drawn from our initial understanding of pretrained LDMs containing visual-linguistic information as presented in \cref{fig:motivation1,fig:motivation2}. More details about the architecture of $Z$Net are provided in the supplementary material.

%We show that the latent $z$ space is a better representation of the image compared to the RGB space, as it is compact and also preserves the semantic characteristics of the image. This is in line with several prior works \cite{turk1991eigenfaces,ke2004pca,de2001robust} in the literature which point to a similar direction that compression techniques like PCA, which create information preserving lower dimensional representations generalize better. As will be seen in our experiments, the latent $z$ space representation also leads to better performance and improved generalization across domains. %Our experiments show an increased robustness using $z$ space over the original image space.  

% for text-to-image, the encoder downsamples the image spatial dimensions 8 times and encodes into 4 channels (\eg  $512\times512\times3 \;\textit{I} \rightarrow 64\times64\times4 \; \textit{z}$), . Similarly, the decoder upsamples the latent representation 8 times and decodes back into the 3 channel RGB domain (\eg  $64\times64\times4 \; \textit{z} \rightarrow 512\times512\times3 \;\textit{I}$)



% \begin{figure}
%     \centering
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \includegraphics[width=\linewidth]{Images/ZSEG-UNet_encoder}
%             \caption{}
%             \label{fig:encoder}
%         \end{subfigure}
        
%         \begin{subfigure}{\columnwidth}
%             \centering
%             \includegraphics[width=\linewidth]{Images/ZSEG-UNet_decoder}
%             \caption{}
%             \label{fig:decoder}
%         \end{subfigure}
    
%     \caption{Elements of the LDM UNet architecture. (a, b) A typical encoder/decoder block has a residual layer followed by a spatial-attention module that internally employs self-attention and then cross-attention with the text features, (b) The decoder block takes features from the encoder branch via skip-connection as additional input for decoding. We use the features right after the spatial-attention layers in both encoder and decoder for the segmentation network.}
%     %\vspace{-1em}
%     \label{fig:unet}
% \end{figure}

%\subsection{CA-ZNet: Leveraging Diffusion Space features}

\subsection{LD-ZNet: Leveraging Diffusion Features}
\label{sec:caznet}
%Describe briefly the architecture of LDMs, where and how was it trained? How does language play a role in generating these features? What is the size of the features which get extracted. What are the timesteps which were used to extract the features? 

Given a text prompt and a timestep $t$, the second-stage of the LDM is trained to denoise $z_t$ - a noisy version of the latent representation $z$ obtained via forward diffusion process for $t$ timesteps. A UNet architecture is used whose encoder/decoder elements are shown in Figure \ref{fig:ldznet_summary} (top right). A typical encoder/decoder block contains a residual layer followed by a spatial-attention module that internally has self-attention and then cross-attention with the text features. We analyze the semantic information in the internal visual-linguistic representations developed at different blocks of encoder and decoder right after these spatial-attention modules. We also propose a way to utilize these latent diffusion features using cross-attention into the ZNet segmentation network and we call the final model as LD-ZNet.

% For LD-ZNet we use second stage of the LDM which is a diffusion model applied in the latent space of the first stage auto-encoder described in section~\ref{sec:znet}. Diffusion models \cite{pmlr-v37-sohl-dickstein15} learn a data distribution by gradually denoising a normally distributed variable. This variable is typically obtained from the forward diffusion process (fixed markov chain) over specific number of timesteps, where based on the timestep parameter, a certain amount of noise is added to the latent representation $z$. This noisy $z$ is input into the reverse diffusion UNet along with the timestep and the conditional text information. Using these inputs, the UNet predicts a noise estimate that mimics the ground truth noise added to $z$ in the forward diffusion process. The typical encoder and decoder elements of the UNet contain ResNet followed by spatial-attention layers which internally employ self-attention and cross-attention mechanisms with the text features as shown in Figure \ref{fig:unet}. We analyze the semantic information present in the internal visual-linguistic representations developed at different blocks right after these spatial-attention layers. We also propose a way to utilize these features using cross-attention for downstream visual recognition and we call the final model as LD-$Z$Net.

        
\subsubsection{Visual-Linguistic Information in LDM Features}
\label{ldm analysis}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Images/ZSEG_chart}
    \caption{Semantic information present in the LDM features at various blocks and timesteps for the referring image segmentation task. AP is measured on a small validation subset of the PhraseCut dataset.}
    \vspace{-1em}
    \label{fig:ldm_analysis}
\end{figure}

% In order to decide which features from the UNet to use, we first 
We evaluate the semantic information present in the pretrained LDM at various blocks and timesteps for the text-based image segmentation task. In this experiment, we consider the latent diffusion features right after the spatial-attention layers 1-16 spanning across all the encoder and decoder blocks present in the UNet. At each block, we analyze the features for every $100^{th}$ timestep in the range $[100, 1000]$. We use a small subset of the training and validation sets from the Phrasecut dataset and train a simple decoder on top of these features to predict the associated binary mask. Specifically, given an image $I$ and timestep $t$, we first extract its latent representation $z$ from the first stage of LDM and add noise from the forward diffusion to obtain $z_t$ for a timestep $t$. Next we extract the frozen CLIP text features for the text prompt and input both of them into the denoising UNet of the LDM to extract the internal visual-linguistic features at all the blocks for that timestep. We use these representations to train the corresponding decoders until convergence. Finally, we evaluate the AP metric on a small subset of the validation dataset. The performance of features from different blocks and timesteps is shown in Figure \ref{fig:ldm_analysis}. 

Similar to \cite{baranchuk2021label}, we observe that the middle blocks \{6,7,8,9,10\} of the UNet contain more semantic information compared to either the early blocks of the encoder or the later blocks of the decoder. We also observe that the timesteps 300-500 contain the maximum visual-linguistic semantic information compared to other timesteps, for these middle blocks. This is in contrast to the findings of \cite{baranchuk2021label} that report the timesteps \{50, 150, 250\} to contain the most useful information when evaluated on an unconditional DDPM model for the few shot semantic segmentation task for horses \cite{yu15lsun} and faces\cite{karras2019style}. We believe that the reason for this difference is because, in our case, the image synthesis is guided by text, leading to the emergence of semantic information earlier in the reverse diffusion process (t=1000$\rightarrow$0), in contrast to unconditional image synthesis.

%\section{Leveraging LDMs for text-based Visual Recognition}
%\label{sec:method}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{Images/ZSEG}
%     \caption{Overview of the LD-$Z$Net to utilize the visual-linguistic features from LDM (top box) into ZNet (bottom box), for referring image segmentation.}
%     \vspace{-1em}
%     \label{fig:architectural_details}
% \end{figure}

%\vspace{-2em}
\subsubsection{LD-ZNet Architecture}
%describe how we combine large scale LDMs with these existing architectures. Concat vs cross attention? Where to combine and how?
% \begin{figure*}
%     \centering
%     \begin{subfigure}{0.6\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images/ZSEG}
%         \caption{}
%         \label{fig:architectural_details}
%     \end{subfigure}%
%     \begin{subfigure}{0.4\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images/AttentionPool.pdf}
%         \caption{}
%         \label{fig:spatial-attention}
%     \end{subfigure}
%     \caption{(a) Overview of the proposed method to utilize the visual-linguistic features from LDM (top box) into ZNet (bottom box), for referring image segmentation. (b) We propose to incorporate the visual-linguistic representations from LDM obtained at the spatial-attention layers via a cross-attention mechanism into the corresponding spatial-attention layers of the CA-ZNet decoder through an Attention Pool layer.}
%     \label{fig:CA-ZNET}
% \end{figure*}

We propose using the aforementioned visual-linguistic representations at multiple spatial-attention modules of the pretrained LDM into the ZNet as shown in Figure \ref{fig:ldznet_summary}. These latent diffusion features are injected into the ZNet via a cross-attention mechanism at the corresponding spatial-attention modules as shown in Figure \ref{fig:spatial-attention}. This allows for an interaction between the visual-linguistic representations from the ZNet and the LDM. Specifically, we pass the latent diffusion features through an \emph{attention pool} layer that not only acts as a learnable layer to match the range of the features participating in the cross-attention, but also adds a positional encoding to the pixels in the LDM representations. The outputs from the attention pool are now positional-encoded visual-linguistic representations that enable the proposed cross-attention mechanism to attend to the corresponding pixels from the ZNet features. ZNet when augmented with these latent diffusion features from the LDM (through cross-attention) is referred to as LD-ZNet.

Following the semantic analysis of latent diffusion features (Sec. \ref{ldm analysis}), we incorporate the internal features from blocks \{6,7,8,9,10\} of the LDM into the corresponding blocks of ZNet, in order to make use of the maximum semantic and diverse visual-linguistic information from the LDM. For AI generated images, these blocks are anyways responsible to generate the final image and using LD-ZNet, we are able to tap into this information which can be used for segmenting objects in the scene. 

% Specifically, we extract the representations from blocks [6,7,8,9,10] of the LDM at a random timestep between 300-500 and feed them into the corresponding block of the decoder UNet.
