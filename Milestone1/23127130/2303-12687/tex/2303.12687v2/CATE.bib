Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Mendelson2010,
abstract = {Under mild assumptions on the kernel, we obtain the best known error rates in a regularized learning scenario taking place in the corresponding reproducing kernel Hilbert space (RKHS). The main novelty in the analysis is a proof that one can use a regularization term that grows significantly slower than the standard quadratic growth in the RKHS norm. {\textcopyright} 2010 Institute of Mathematical Statistics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1001.2094v1},
author = {Mendelson, Shahar and Neeman, Joseph},
doi = {10.1214/09-AOS728},
eprint = {arXiv:1001.2094v1},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Mendelson2010.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Least-squares,Model selection,Regression,Regulation,Reproducing kernel Hilbert space},
number = {1},
pages = {526--565},
title = {{Regularization in kernel learning}},
volume = {38},
year = {2010}
}

@article{Bibaut2021,
abstract = {We consider adaptive designs for a trial involving N individuals that we follow along T time steps. We allow for the variables of one individual to depend on its past and on the past of other individuals. Our goal is to learn a mean outcome, averaged across the N individuals, that we would observe, if we started from some given initial state, and we carried out a given sequence of counterfactual interventions for {\$}\backslashtau{\$} time steps. We show how to identify a statistical parameter that equals this mean counterfactual outcome, and how to perform inference for this parameter, while adaptively learning an oracle design defined as a parameter of the true data generating distribution. Oracle designs of interest include the design that maximizes the efficiency for a statistical parameter of interest, or designs that mix the optimal treatment rule with a certain exploration distribution. We also show how to design adaptive stopping rules for sequential hypothesis testing. This setting presents unique technical challenges. Unlike in usual statistical settings where the data consists of several independent observations, here, due to network and temporal dependence, the data reduces to one single observation with dependent components. In particular, this precludes the use of sample splitting techniques. We therefore had to develop a new equicontinuity result and guarantees for estimators fitted on dependent data. We were motivated to work on this problem by the following two questions. (1) In the context of a sequential adaptive trial with K treatment arms, how to design a procedure to identify in as few rounds as possible the treatment arm with best final outcome? (2) In the context of sequential randomized disease testing at the scale of a city, how to estimate and infer the value of an optimal testing and isolation strategy?},
archivePrefix = {arXiv},
arxivId = {2101.07380},
author = {Bibaut, Aurelien and Petersen, Maya and Vlassis, Nikos and Dimakopoulou, Maria and van der Laan, Mark},
eprint = {2101.07380},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Bibaut2021.pdf:pdf},
journal = {arXiv preprint arXiv:2101.07380},
title = {{Sequential causal inference in a single world of connected units}},
url = {http://arxiv.org/abs/2101.07380},
year = {2021}
}

@article{VandeGeer2014,
abstract = {Uniform convergence of empirical norms - empirical measures of squared functions - is a topic which has received considerable attention in the literature on empirical processes. The results are relevant as empirical norms occur due to symmetrization. They also play a prominent role in statistical applications. The contraction inequality has been a main tool but recently other approaches have shown to lead to better results in important cases. We present an overview including the linear (anisotropic) case, and give new results for inner products of functions. Our main application will be the estimation of the parental structure in a directed acyclic graph. As intermediate result we establish convergence of the least squares estimator when the model is wrong.},
author = {van de Geer, Sara},
doi = {10.1214/14-EJS894},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/vandeGeer2014.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Additive model,Causal inference,Empirical measure,Uniform convergence},
number = {1},
pages = {543--574},
title = {{On the uniform convergence of empirical norms and inner products, with application to causal inference}},
volume = {8},
year = {2014}
}

@article{Fisher2023,
archivePrefix = {arXiv},
arxivId = {arXiv:2307.09700v2},
author = {Fisher, Aaron},
eprint = {arXiv:2307.09700v2},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/R-Learner/Fisher2024.pdf:pdf},
journal = {arXiv preprint arXiv:2307.09700},
title = {{The Connection Between R-Learning and Inverse-Variance Weighting for Estimation of Heterogeneous Treatment Effects}},
year = {2023}
}

@article{Chernozhukov2024,
abstract = {An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.},
author = {Chernozhukov, Victor and Hansen, Christian and Kallus, Nathan and Spindler, Martin and Vasilis, Syrgkanis},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/R-Learner/CausalML{\_}book.pdf:pdf},
title = {{Applied Causal Inference Powered by ML and AI}},
year = {2024}
}

@article{LarsVanderLaan2023,
abstract = {Debiased machine learning estimators for nonparametric inference of smooth functionals of the data-generating distribution can suffer from excessive variability and instability. For this reason, practitioners may resort to simpler models based on parametric or semiparametric assumptions. However, such simplifying assumptions may fail to hold, and estimates may then be biased due to model misspecification. To address this problem, we propose Adaptive Debiased Machine Learning (ADML), a nonparametric framework that combines data-driven model selection and debiased machine learning techniques to construct asymptotically linear, adaptive, and superefficient estimators for pathwise differentiable functionals. By learning model structure directly from data, ADML avoids the bias introduced by model misspecification and remains free from the restrictions of parametric and semiparametric models. While they may exhibit irregular behavior for the target parameter in a nonparametric statistical model, we demonstrate that ADML estimators provides regular and locally uniformly valid inference for a projection-based oracle parameter. Importantly, this oracle parameter agrees with the original target parameter for distributions within an unknown but correctly specified oracle statistical submodel that is learned from the data. This finding implies that there is no penalty, in a local asymptotic sense, for conducting data-driven model selection compared to having prior knowledge of the oracle submodel and oracle parameter. To demonstrate the practical applicability of our theory, we provide a broad class of ADML estimators for estimating the average treatment effect in adaptive partially linear regression models.},
journal = {arXiv preprint arXiv:2307.12544},
archivePrefix = {arXiv},
arxivId = {2307.12544},
author = {van der Laan, Lars and Carone, Marco and Luedtke, Alex and van der Laan, Mark},
eprint = {2307.12544},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/R-Learner/2307.12544v1.pdf:pdf},
keywords = {adaptive debiased machine learning,causal inference,model selection,se-,superefficient},
title = {{Adaptive debiased machine learning using data-driven model selection techniques}},
url = {http://arxiv.org/abs/2307.12544},
year = {2023}
}



@article{van2024combining,
  title={Combining T-learning and DR-learning: a framework for oracle-efficient estimation of causal contrasts},
  author={van der Laan, Lars and Carone, Marco and Luedtke, Alex},
  journal={arXiv preprint arXiv:2402.01972},
  year={2024}
}

@article{vansteelandt2023orthogonal,
  title={Orthogonal prediction of counterfactual outcomes},
  author={Vansteelandt, Stijn and Morzywo{\l}ek, Pawe{\l}},
  journal={arXiv preprint arXiv:2311.09423},
  year={2023}
}

@article{Foster2023,
abstract = {We provide nonasymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker assumptions than in previous works and accommodate settings in which the target parameter belongs to a complex nonparametric class. We provide conditions on the metric entropy of the nuisance and target classes such that oracle rates of the same order, as if we knew the nuisance parameter, are achieved.},
archivePrefix = {arXiv},
arxivId = {1901.09036},
author = {Foster, Dylan and Syrgkanis, Vasilis},
eprint = {1901.09036},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/23-AOS2258-1.pdf:pdf},
issn = {21688966},
journal = {Annals of Statistics},
keywords = {Neyman orthogonality,Statistical learning,double machine learning,local Rademacher complexity,policy learning,treatment effects},
number = {3},
pages = {879--908},
title = {{Orthogonal Statistical Learning}},
volume = {51},
year = {2023}
}

@article{Kennedy2023,
abstract = {Heterogeneous effect estimation is crucial in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been pro-posed, but there are gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several ways. First, we study a two-stage doubly robust CATE estimator and give a generic error bound, which yields rates faster than much of the literature. We ap-ply the bound to derive error rates in smooth nonparametric models, and give sufficient conditions for oracle efficiency. Along the way we give a general error bound for regression with estimated outcomes; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker condi-tions, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations.},
archivePrefix = {arXiv},
arxivId = {2004.14497},
author = {Kennedy, Edward},
eprint = {2004.14497},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/23-EJS2157.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Conditional effects,influence function,minimax rate,nonparametric regression},
number = {2},
pages = {3008--3049},
title = {{Towards optimal doubly robust estimation of heterogeneous causal effects}},
volume = {17},
year = {2023}
}

@article{Dahabreh2020a,
abstract = {When treatment effect modifiers influence the decision to participate in a randomized trial, the average treatment effect in the population represented by the randomized individuals will differ from the effect in other populations. In this tutorial, we consider methods for extending causal inferences about time-fixed treatments from a trial to a new target population of nonparticipants, using data from a completed randomized trial and baseline covariate data from a sample from the target population. We examine methods based on modeling the expectation of the outcome, the probability of participation, or both (doubly robust). We compare the methods in a simulation study and show how they can be implemented in software. We apply the methods to a randomized trial nested within a cohort of trial-eligible patients to compare coronary artery surgery plus medical therapy versus medical therapy alone for patients with chronic coronary artery disease. We conclude by discussing issues that arise when using the methods in applied analyses.},
archivePrefix = {arXiv},
arxivId = {1805.00550},
author = {Dahabreh, Issa and Robertson, Sarah and Steingrimsson, Jon and Stuart, Elizabeth and Hern{\'{a}}n, Miguel},
eprint = {1805.00550},
file = {:Users/pawel/Desktop/Transportability/Statistics in Medicine - 2020 - Dahabreh - Extending inferences from a randomized trial to a new target population-1.pdf:pdf},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {double robustness,generalizability,observational analyses,randomized trials,transportability},
number = {14},
pages = {1999--2014},
pmid = {32253789},
title = {{Extending inferences from a randomized trial to a new target population}},
volume = {39},
year = {2020}
}

@article{McGrath2022,
abstract = {We consider the problem of constructing minimax rate-optimal estimators for a doubly robust nonparametric functional that has witnessed applications across the causal inference and conditional independence testing literature. Minimax rate-optimal estimators for such functionals are typically constructed through higher-order bias corrections of plug-in and one-step type estimators and, in turn, depend on estimators of nuisance functions. In this paper, we consider a parallel question of interest regarding the optimality and/or sub-optimality of plug-in and one-step bias-corrected estimators for the specific doubly robust functional of interest. Specifically, we verify that by using undersmoothing and sample splitting techniques when constructing nuisance function estimators, one can achieve minimax rates of convergence in all H$\backslash$"older smoothness classes of the nuisance functions (i.e. the propensity score and outcome regression) provided that the marginal density of the covariates is sufficiently regular. Additionally, by demonstrating suitable lower bounds on these classes of estimators, we demonstrate the necessity to undersmooth the nuisance function estimators to obtain minimax optimal rates of convergence.},
archivePrefix = {arXiv},
journal={arXiv preprint arXiv:2212.14857},
arxivId = {2212.14857},
author = {McGrath, Sean and Mukherjee, Rajarshi},
eprint = {2212.14857},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/DR{\_}vs{\_}NW/Review/Papers/McGrathMukherjee2022.pdf:pdf},
title = {{On Undersmoothing and Sample Splitting for Estimating a Doubly Robust Functional}},
url = {http://arxiv.org/abs/2212.14857},
year = {2022}
}

@article{Gill1990,
author = {Gill, Richard and Johansen, Soren},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Gill1990.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {1501--1555},
title = {{A Survey of Product-Integration with a View Toward Application in Survival Analysis}},
volume = {18},
year = {1990}
}

@article{Westling2023,
abstract = {In the absence of data from a randomized trial, researchers may aim to use observational data to draw causal inference about the effect of a treatment on a time-to-event outcome. In this context, interest often focuses on the treatment-specific survival curves, that is, the survival curves were the population under study to be assigned to receive the treatment or not. Under certain conditions, including that all confounders of the treatment-outcome relationship are observed, the treatment-specific survival curve can be identified with a covariate-adjusted survival curve. In this article, we propose a novel cross-fitted doubly-robust estimator that incorporates data-adaptive (e.g., machine learning) estimators of the conditional survival functions. We establish conditions on the nuisance estimators under which our estimator is consistent and asymptotically linear, both pointwise and uniformly in time. We also propose a novel ensemble learner for combining multiple candidate estimators of the conditional survival estimators. Notably, our methods and results accommodate events occurring in discrete or continuous time, or an arbitrary mix of the two. We investigate the practical performance of our methods using numerical studies and an application to the effect of a surgical treatment to prevent metastases of parotid carcinoma on mortality. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {2106.06602},
author = {Westling, Ted and Luedtke, Alex and Gilbert, Peter and Carone, Marco},
eprint = {2106.06602},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Inference for Treatment-Specific Survival Curves Using Machine Learning.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Causal inference,Confidence band,Cross-fitting,G-computation,Right-censored data},
number = {0},
pages = {1--26},
publisher = {Taylor {\&} Francis},
title = {{Inference for Treatment-Specific Survival Curves Using Machine Learning}},
url = {https://doi.org/10.1080/01621459.2023.2205060},
volume = {0},
year = {2023}
}

@article{Rotnitzky2021,
abstract = {We study a class of parameters with the so-called mixed bias property. For parameters with this property, the bias of the semiparametric efficient one-step estimator is equal to the mean of the product of the estimation errors of two nuisance functions. In nonparametric models, parameters with the mixed bias property admit so-called rate doubly robust estimators, i.e., estimators that are consistent and asymptotically normal when one succeeds in estimating both nuisance functions at sufficiently fast rates, with the possibility of trading off slower rates of convergence for the estimator of one of the nuisance functions against faster rates for the estimator of the other nuisance function. We show that the class of parameters with the mixed bias property strictly includes two recently studied classes of parameters which, in turn, include many parameters of interest in causal inference. We characterize the form of parameters with the mixed bias property and of their influence functions. Furthermore, we derive two functional loss functions, each being minimized at one of the two nuisance functions. These loss functions can be used to derive loss-based penalized estimators of the nuisance functions.},
archivePrefix = {arXiv},
arxivId = {1904.03725},
author = {Rotnitzky, A. and Smucler, E. and Robins, J.},
eprint = {1904.03725},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Rotnitzky2021.pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {Average treatment effect,Causal inference,Doubly robust estimation,Influence function,One-step estimator},
number = {1},
pages = {231--238},
title = {{Characterization of parameters with a mixed bias property}},
volume = {108},
year = {2021}
}

@article{Westreich2017,
abstract = {Increasingly, the statistical and epidemiologic literature is focusing beyond issues of internal validity and turning its attention to questions of external validity. Here, we discuss some of the challenges of transporting a causal effect from a randomized trial to a specific target population. We present an inverse odds weighting approach that can easily operationalize transportability. We derive these weights in closed form and illustrate their use with a simple numerical example. We discuss how the conditions required for the identification of internally valid causal effects are translated to apply to the identification of externally valid causal effects. Estimating effects in target populations is an important goal, especially for policy or clinical decisions. Researchers and policy-makers should therefore consider use of statistical techniques such as inverse odds of sampling weights, which under careful assumptions can transport effect estimates from study samples to target populations.},
author = {Westreich, Daniel and Edwards, Jessie and Lesko, Catherine and Stuart, Elizabeth and Cole, Stephen},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Westreich2017.pdf:pdf},
issn = {14766256},
journal = {American Journal of Epidemiology},
keywords = {Causal inference,Epidemiologic methods,External validity,Generalizability,Transportability},
number = {8},
pages = {1010--1014},
pmid = {28535275},
title = {{Transportability of Trial Results Using Inverse Odds of Sampling Weights}},
volume = {186},
year = {2017}
}

@article{Colnet2024,
abstract = {With increasing data availability, causal effects can be evaluated across different data sets, both randomized controlled trials (RCTs) and observational studies. RCTs isolate the effect of the treatment from that of unwanted (confounding) co-occurring effects but they may suffer from unrepresentativeness, and thus lack external validity. On the other hand, large observational samples are often more representative of the target population but can conflate confounding effects with the treatment of interest. In this paper, we review the growing literature on methods for causal inference on combined RCTs and observational studies, striving for the best of both worlds. We first discuss identification and estimation methods that improve generalizability of RCTs using the representativeness of observational data. Classical estimators include weighting, difference between conditional outcome models, and doubly robust estimators. We then discuss methods that combine RCTs and observational data to either ensure uncounfoundedness of the observational analysis or to improve (conditional) average treatment effect estimation. We also connect and contrast works developed in both the potential outcomes literature and the structural causal model literature. Finally, we compare the main methods using a simulation study and real world data to analyze the effect of tranexamic acid on the mortality rate in major trauma patients. A review of available codes and new implementations is also provided.},
archivePrefix = {arXiv},
arxivId = {2011.08047},
author = {Colnet, B{\'{e}}n{\'{e}}dicte and Mayer, Imke and Chen, Guanhua and Dieng, Awa and Li, Ruohong and Varoquaux, Ga{\"{e}}l and Vert, Jean-Philippe and Josse, Julie and Yang, Shu},
eprint = {2011.08047},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Colnet2024.pdf:pdf},
issn = {21688745},
journal = {Statistical Science},
keywords = {Causal effect generalization, transportability, double robustness, data integration, heterogeneous data, S-admissibility,and phrases,ble robustness,causal effect generalization,data integration,dou-,heterogeneous data,s-admissibility,transportability},
number = {1},
pages = {165--191},
title = {{Causal Inference Methods for Combining Randomized Trials and Observational Studies: A Review}},
volume = {39},
year = {2024}
}

@article{Morzywolek2022a,
abstract = {Background and objectives: Defining the optimal moment to start renal replacement therapy (RRT) in acute kidney injury (AKI) remains challenging. Multiple randomized controlled trials (RCTs) addressed this question whilst using absolute criteria such as pH or serum potassium. However, there is a need for identification of the most optimal cut-offs of these criteria. We conducted a causal analysis on routinely collected data (RCD) to compare the impact of different pre-specified dynamic treatment regimes (DTRs) for RRT initiation based on time-updated levels of potassium, pH, and urinary output on 30-day ICU mortality. Design, setting, participants, and measurements: Patients in the ICU of Ghent University Hospital were included at the time they met KDIGO-AKI-stage ≥ 2. We applied inverse-probability-of-censoring-weighted Aalen–Johansen estimators to evaluate 30-day survival under 81 DTRs prescribing RRT initiation under different thresholds of potassium, pH, or persisting oliguria. Results: Out of 13,403 eligible patients (60.8 ± 16.8 years, SOFA 7.0 ± 4.1), 5622 (63.4 ± 15.3 years, SOFA 8.2 ± 4.2) met KDIGO-AKI-stage ≥ 2. The DTR that delayed RRT until potassium ≥ 7 mmol/l, persisting oliguria for 24–36 h, and/or pH {\textless} 7.0 (non-oliguric) or {\textless} 7.2 (oliguric) despite maximal conservative treatment resulted in a reduced 30-day ICU mortality (from 12.7{\%} [95{\%} CI 11.9–13.6{\%}] under current standard of care to 10.5{\%} [95{\%} CI 9.5–11.7{\%}]; risk difference 2.2{\%} [95{\%} CI 1.3–3.8{\%}]) with no increase in patients starting RRT (from 471 [95{\%} CI 430–511] to 475 [95{\%} CI 342–572]). The fivefold cross-validation benchmark for the optimal DTR resulted in 30-day ICU mortality of 10.7{\%}. Conclusions: Our causal analysis of RCD to compare RRT initiation at different thresholds of refractory low pH, high potassium, and persisting oliguria identified a DTR that resulted in a decrease in 30-day ICU mortality without increase in number of RRTs. Our results suggest that the current criteria to start RRT as implemented in most RCTs may be suboptimal. However, as our analysis is hypothesis generating, this optimal DTR should ideally be validated in a multicentric RCT.},
author = {Morzywo{\l}ek, Pawe{\l} and Steen, Johan and Vansteelandt, Stijn and Decruyenaere, Johan and Sterckx, Sigrid and {Van Biesen}, Wim},
file = {:Users/pawel/Documents/UGent/MyPapers/Clinical paper/eb882efa-4b36-4588-89f9-46d8ece5e6a0.pdf:pdf},
issn = {1466609X},
journal = {Critical Care},
number = {1},
pages = {1--11},
pmid = {36443861},
publisher = {BioMed Central},
title = {{Timing of dialysis in acute kidney injury using routinely collected data and dynamic treatment regimes}},
url = {https://doi.org/10.1186/s13054-022-04252-1},
volume = {26},
year = {2022}
}

@article{VanderWeele2013,
abstract = { Abstract : In this article, we discuss causal inference when there are multiple versions of treatment. The potential outcomes framework, as articulated by Rubin, makes an assumption of no multiple versions of treatment, and here we discuss an extension of this potential outcomes framework to accommodate causal inference under violations of this assumption. A variety of examples are discussed in which the assumption may be violated. Identification results are provided for the overall treatment effect and the effect of treatment on the treated when multiple versions of treatment are present and also for the causal effect comparing a version of one treatment to some other version of the same or a different treatment. Further identification and interpretative results are given for cases in which the version precedes the treatment as when an underlying treatment variable is coarsened or dichotomized to create a new treatment variable for which there are effectively “multiple versions”. Results are also given for effects defined by setting the version of treatment to a prespecified distribution. Some of the identification results bear resemblance to identification results in the literature on direct and indirect effects. We describe some settings in which ignoring multiple versions of treatment, even when present, will not lead to incorrect inferences. },
author = {VanderWeele, Tyler and Hernan, Miguel},
file = {:Users/pawel/Documents/UGent/Papers/10.1515{\_}jci-2012-0002.pdf:pdf},
issn = {2193-3677},
journal = {Journal of Causal Inference},
keywords = {biostatistics,causal inference,consistency,corresponding author,departments of epidemiology and,e-mail,edu,harvard,harvard school of public,health,hernan,hsph,massachusetts,miguel a,multiple versions of treatment,potential outcomes,sutva,tvanderw,tyler j,united,united states,vanderweele},
number = {1},
pages = {1--20},
title = {{Causal inference under multiple versions of treatment}},
volume = {1},
year = {2013}
}

@article{Curth2020,
abstract = {We aim to construct a class of learning algorithms that are of practical value to applied researchers in fields such as biostatistics, epidemiology and econometrics, where the need to learn from incompletely observed information is ubiquitous. We propose a new framework for statistical machine learning of target functions arising as identifiable functionals from statistical models, which we call `IF-learning' due to its reliance on influence functions (IFs). This framework is problem- and model-agnostic and can be used to estimate a broad variety of target parameters of interest in applied statistics: we can consider any target function for which an IF of a population-averaged version exists in analytic form. Throughout, we put particular focus on so-called coarsening at random/doubly robust problems with partially unobserved information. This includes problems such as treatment effect estimation and inference in the presence of missing outcome data. Within this framework, we propose two general learning algorithms that build on the idea of nonparametric plug-in bias removal via IFs: the 'IF-learner' which uses pseudo-outcomes motivated by uncentered IFs for regression in large samples and outputs entire target functions without confidence bands, and the 'Group-IF-learner', which outputs only approximations to a function but can give confidence estimates if sufficient information on coarsening mechanisms is available. We apply both in a simulation study on inferring treatment effects.},
archivePrefix = {arXiv},
arxivId = {2008.06461},
author = {Curth, Alicia and Alaa, Ahmed and van der Schaar, Mihaela},
eprint = {2008.06461},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Curth2020.pdf:pdf},
keywords = {causal inference,counterfactual inference,double robustness,efficient estimation,missing outcomes,nonparametric,regression,treatment effects},
journal = {arXiv preprint arXiv:2008.06461},
title = {{Estimating Structural Target Functions using Machine Learning and Influence Functions}},
url = {http://arxiv.org/abs/2008.06461},
year = {2020}
}

@article{Hines2022,
abstract = {Evaluation of treatment effects and more general estimands is typically achieved via parametric modeling, which is unsatisfactory since model misspecification is likely. Data-adaptive model building (e.g., statistical/machine learning) is commonly employed to reduce the risk of misspecification. Na{\"{i}}ve use of such methods, however, delivers estimators whose bias may shrink too slowly with sample size for inferential methods to perform well, including those based on the bootstrap. Bias arises because standard data-adaptive methods are tuned toward minimal prediction error as opposed to, for example, minimal MSE in the estimator. This may cause excess variability that is difficult to acknowledge, due to the complexity of such strategies. Building on results from nonparametric statistics, targeted learning and debiased machine learning overcome these problems by constructing estimators using the estimand's efficient influence function under the nonparametric model. These increasingly popular methodologies typically assume that the efficient influence function is given, or that the reader is familiar with its derivation. In this article, we focus on derivation of the efficient influence function and explain how it may be used to construct statistical/machine-learning-based estimators. We discuss the requisite conditions for these estimators to perform well and use diverse examples to convey the broad applicability of the theory.},
archivePrefix = {arXiv},
arxivId = {2107.00681},
author = {Hines, Oliver and Dukes, Oliver and Diaz-Ordaz, Karla and Vansteelandt, Stijn},
eprint = {2107.00681},
file = {:Users/pawel/Documents/UGent/Papers/Demystifying Statistical Learning Based on Efficient Influence Functions-1.pdf:pdf},
issn = {15372731},
journal = {American Statistician},
keywords = {Data-adaptive estimation,Double machine learning,Nonparametric methods,Post-selection inference,Targeted learning},
number = {3},
pages = {292--304},
publisher = {Taylor {\&} Francis},
title = {{Demystifying Statistical Learning Based on Efficient Influence Functions}},
url = {https://doi.org/10.1080/00031305.2021.2021984},
volume = {76},
year = {2022}
}

@article{Bach2022,
abstract = {DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org.},
archivePrefix = {arXiv},
arxivId = {2104.03220},
author = {Bach, Philipp and Chernozhukov, Victor and Kurz, Malte and Spindler, Martin},
eprint = {2104.03220},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2103.09603.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Causal inference,Causal machine learning,Machine learning,Python},
pages = {1--45},
title = {{DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python}},
volume = {23},
year = {2022}
}

@article{Belloni2017,
abstract = {In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets.},
archivePrefix = {arXiv},
arxivId = {1311.2645},
author = {Belloni, A. and Chernozhukov, V. and Fernandez-Val, I. and Hansen, C.},
eprint = {1311.2645},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Econometrica - 2017 - Belloni - Program Evaluation and Causal Inference With High‐Dimensional Data.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {1},
pages = {233--298},
title = {{Program Evaluation and Causal Inference With High-Dimensional Data}},
volume = {85},
year = {2017}
}

@article{Kallus2021,
abstract = {Policy learning can be used to extract individualized treatment regimes from observational data in healthcare, civics, e-commerce, and beyond. One big hurdle to policy learning is a commonplace lack of overlap in the data for different actions, which can lead to unwieldy policy evaluation and poorly performing learned policies. We study a solution to this problem based on retargeting, that is, changing the population on which policies are optimized. We first argue that at the population level, retargeting may induce little to no bias. We then characterize the optimal reference policy and retargeting weights in both binary-action and multi-action settings. We do this in terms of the asymptotic efficient estimation variance of the new learning objective. We further consider weights that additionally control for potential bias due to retargeting. Extensive empirical results in a simulation study and a case study of personalized job counseling demonstrate that retargeting is a fairly easy way to significantly improve any policy learning procedure applied to observational data. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1906.08611},
author = {Kallus, Nathan},
eprint = {1906.08611},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/More Efficient Policy Learning via Optimal Retargeting.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Efficient policy learning,Individualized treatment regimes,Optimization,Overlap},
number = {534},
pages = {646--658},
publisher = {Taylor {\&} Francis},
title = {{More Efficient Policy Learning via Optimal Retargeting}},
url = {https://doi.org/10.1080/01621459.2020.1788948},
volume = {116},
year = {2021}
}

@techreport{vanderLaan2014,
 title = "Targeted Learning of an Optimal Dynamic Treatment, and Statistical Inference for its Mean Outcome",
 author = "van der Laan, Mark and Luedtke, Alexander",
 type = "Working Paper",
 institution = "U.C. Berkeley Division of Biostatistics Working Paper Series",
 number = "332",
 year = "2014",
 month = "December"
}

@techreport{Crump2006,
 title = "Moving the Goalposts: Addressing Limited Overlap in the Estimation of Average Treatment Effects by Changing the Estimand",
 author = "Crump, Richard and Hotz, Joseph and Imbens, Guido and Mitnik, Oscar",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Technical Working Paper Series",
 number = "330",
 year = "2006",
 month = "October",
 URL = "http://www.nber.org/papers/t0330",
 abstract = {Estimation of average treatment effects under unconfoundedness or exogenous treatment assignment is often hampered by lack of overlap in the covariate distributions. This lack of overlap can lead to imprecise estimates and can make commonly used estimators sensitive to the choice of specification. In such cases researchers have often used informal methods for trimming the sample. In this paper we develop a systematic approach to addressing such lack of overlap. We characterize optimal subsamples for which the average treatment effect can be estimated most precisely, as well as optimally weighted average treatment effects. Under some conditions the optimal selection rules depend solely on the propensity score. For a wide range of distributions a good approximation to the optimal rule is provided by the simple selection rule to drop all units with estimated propensity scores outside the range [0.1,0.9].},
}

@inproceedings{Coston2020,
 author = {Coston, Amanda and Kennedy, Edward and Chouldechova, Alexandra},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {4150--4162},
 title = {Counterfactual Predictions under Runtime Confounding},
 url = {https://proceedings.neurips.cc/paper/2020/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Hernan2019,
abstract = {Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term "data science" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.},
author = {Hern{\'{a}}n, Miguel and Hsu, John and Healy, Brian},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/hernan{\_}chance19.pdf:pdf},
issn = {0933-2480},
journal = {Chance},
number = {1},
pages = {42--49},
title = {{A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks}},
volume = {32},
year = {2019}
}

@article{VanGeloven2020,
abstract = {In this paper we study approaches for dealing with treatment when developing a clinical prediction model. Analogous to the estimand framework recently proposed by the European Medicines Agency for clinical trials, we propose a ‘predictimand' framework of different questions that may be of interest when predicting risk in relation to treatment started after baseline. We provide a formal definition of the estimands matching these questions, give examples of settings in which each is useful and discuss appropriate estimators including their assumptions. We illustrate the impact of the predictimand choice in a dataset of patients with end-stage kidney disease. We argue that clearly defining the estimand is equally important in prediction research as in causal inference.},
archivePrefix = {arXiv},
arxivId = {2004.06998},
author = {van Geloven, Nan and Swanson, Sonja and Ramspek, Chava and Luijken, Kim and van Diepen, Merel and Morris, Tim and Groenwold, Rolf and van Houwelingen, Hans and Putter, Hein and le Cessie, Saskia},
eprint = {2004.06998},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/s10654-020-00636-1.pdf:pdf},
isbn = {0123456789},
issn = {15737284},
journal = {European Journal of Epidemiology},
keywords = {Censoring,Clinical prediction model,Estimands,Predictimands,Treatment},
number = {7},
pages = {619--630},
pmid = {32445007},
publisher = {Springer Netherlands},
title = {{Prediction meets causal inference: the role of treatment in clinical prediction models}},
url = {https://doi.org/10.1007/s10654-020-00636-1},
volume = {35},
year = {2020}
}

@article{Li2018,
abstract = {Covariate balance is crucial for unconfounded descriptive or causal comparisons. However, lack of balance is common in observational studies. This article considers weighting strategies for balancing covariates. We define a general class of weights—the balancing weights—that balance the weighted distributions of the covariates between treatment groups. These weights incorporate the propensity score to weight each group to an analyst-selected target population. This class unifies existing weighting methods, including commonly used weights such as inverse-probability weights as special cases. General large-sample results on nonparametric estimation based on these weights are derived. We further propose a new weighting scheme, the overlap weights, in which each unit's weight is proportional to the probability of that unit being assigned to the opposite group. The overlap weights are bounded, and minimize the asymptotic variance of the weighted average treatment effect among the class of balancing weights. The overlap weights also possess a desirable small-sample exact balance property, based on which we propose a new method that achieves exact balance for means of any selected set of covariates. Two applications illustrate these methods and compare them with other approaches.},
archivePrefix = {arXiv},
arxivId = {1404.1785},
author = {Li, Fan and Morgan, Kari and Zaslavsky, Alan},
eprint = {1404.1785},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Balancing Covariates via Propensity Score Weighting.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Balancing weights,Causal inference,Clinical equipoise,Confounding,Exact balance,Overlap weights},
number = {521},
pages = {390--400},
publisher = {Taylor {\&} Francis},
title = {{Balancing Covariates via Propensity Score Weighting}},
url = {https://doi.org/10.1080/01621459.2016.1260466},
volume = {113},
year = {2018}
}

@article{Imbens2009,
abstract = {Many empirical questions in economics and other social sciences depend on causal effects of programs or policies. In the last two decades, much research has been done on the econometric and statistical analysis of such causal effects. This recent theoretical literature has built on, and combined features of earlier work in both the statistics and econometrics literatures. It has by now reached a level of maturity that makes it an important tool in many areas of empirical research in economics, including labor economics, public finance, development economics, industrial organization, and other areas of empirical microeconomics. In this review, we discuss some of the recent developments. We focus primarily on practical issues for empirical researchers, as well as provide a historical overview of the area and give references to more technical research.},
author = {Imbens, Guido and Wooldridge, Jeffrey},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/jel.47.1.5.pdf:pdf},
issn = {00220515},
journal = {Journal of Economic Literature},
number = {1},
pages = {5--86},
title = {{Recent developments in the econometrics of program evaluation}},
volume = {47},
year = {2009}
}

@article{Vansteelandt2009,
abstract = {Intensive care unit (ICU) patients are highly susceptible to hospital-acquired infections due to their poor health and many invasive therapeutic treatments. The effect on mortality of acquiring such infections is, however, poorly understood. Our goal is to quantify this using data from the National Surveillance Study of Nosocomial Infections in ICUs (Belgium). This is challenging because of the presence of time-dependent confounders, such as mechanical ventilation, which lie on the causal path from infection to mortality. Standard statistical analyses may be severely misleading in such settings and have shown contradictory results. Inverse probability weighting for marginal structural models may instead be used but is not directly applicable because these models parameterize the effect of acquiring infection on a given day in ICU, versus "never" acquiring infection in ICU, and this is ill-defined when ICU discharge precedes that day. Additional complications arise from the informative censoring of the survival time by hospital discharge and the instability of the inverse weighting estimation procedure. We accommodate this by introducing a new class of marginal structural models for so-called partial exposure regimes. These describe the effect on the hazard of death of acquiring infection on a given day s, versus not acquiring infection "up to that day," had patients stayed in the ICU for at least s days.},
author = {Vansteelandt, Stijn and Mertens, Karl and Suetens, Carl and Goetghebeur, Els},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/kxn012.pdf:pdf},
issn = {14654644},
journal = {Biostatistics},
keywords = {Causal inference,Direct effect,ICU,Intermediate variables,Marginal structural models,Nosocomial infection,Time-dependent confounding},
number = {1},
pages = {46--59},
pmid = {18503036},
title = {{Marginal structural models for partial exposure regimes}},
volume = {10},
year = {2009}
}

@article{VanDerLaan2007,
author = {{van der Laan}, Mark and Polley, E. and Hubbard, A.},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {Causal effect,Causal graph,Censored data,Collaborative double robust,Cross-validation,Double robust,Dynamic treatment regimens,Efficient influence curve,Estimating function,Estimator selection,Locally efficient,Loss function},
number = {25},
title = {{Super Learner}},
volume = {6},
year = {2007}
}

@article{Dahabreh2020,
archivePrefix = {arXiv},
arxivId = {1805.00550},
author = {Dahabreh, Issa and Robertson, Sarah and Steingrimsson, Jon nd Stuart, Elizabeth and Hern{\'{a}}n, Miguel},
eprint = {1805.00550},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Statistics in Medicine - 2020 - Dahabreh - Extending inferences from a randomized trial to a new target population.pdf:pdf},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {double robustness,generalizability,observational analyses,randomized trials,transportability},
number = {14},
pages = {1999--2014},
pmid = {32253789},
title = {{Extending inferences from a randomized trial to a new target population}},
volume = {39},
year = {2020}
}

@book{Pfanzagl1990,
author = {Pfanzagl, Johann},
publisher = {Springer},
title = {{Estimation in Semiparametric Models}},
year = {1990}
}

@book{Bickel1998,
author = {Bickel, Peter and Klaassen, Chris and Ritov, Ya'acov and Wellner, Jon},
publisher = {Springer},
title = {{Efficient and Adaptive Estimation for Semiparametric Models}},
year = {1998}
}

@book{vanderLaan2003,
author = {van der Laan, Mark and Robins, James},
publisher = {Springer},
title = {{Unified Methods for Censored Longitudinal Data and Causality}},
year = {2003}
}

@book{vanderLaan2011,
author = {van der Laan, Mark and Rose, Sherri},
publisher = {Springer},
title = {{Targeted Learning. Causal Inference for Observational and Experimental Data}},
year = {2011}
}

@book{Kosorok2008,
author = {Kosorok, Michael},
publisher = {Springer},
title = {{Introduction to Empirical Processes and Semiparametric Inference}},
year = {2008}
}

@article{KDIGO2012,
file = {:Users/pawel/Documents/UGent/Papers/KDIGO-2012-AKI-Guideline-English.pdf:pdf},
issn = {21548331},
author = {KDIGO},
journal = {Kidney International Supplements},
number = {1},
pages = {1--138},
pmid = {24566591},
title = {{Kidney Disease: Improving Global Outcomes (KDIGO) Acute Kidney Injury Work Group. KDIGO Clinical Practice Guideline for Acute Kidney Injury.}},
volume = {2},
year = {2012}
}

@article{Kennedy2017,
abstract = {Continuous treatments (e.g. doses) arise often in practice, but many available causal effect estimators are limited by either requiring parametric models for the effect curve, or by not allowing doubly robust covariate adjustment. We develop a novel kernel smoothing approach that requires only mild smoothness assumptions on the effect curve and still allows for misspecification of either the treatment density or outcome regression. We derive asymptotic properties and give a procedure for data-driven bandwidth selection. The methods are illustrated via simulation and in a study of the effect of nurse staffing on hospital readmissions penalties.},
archivePrefix = {arXiv},
arxivId = {1507.00747},
author = {Kennedy, Edward and Ma, Zongming and McHugh, Matthew and Small, Dylan},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Journal of the Royal Statistical Society Series B Statistical Methodology - 2016 - Kennedy - Non‐parametric methods for.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Causal inference,Dose–response,Efficient influence function,Kernel smoothing,Semiparametric estimation},
number = {4},
pages = {1229--1245},
title = {{Non-parametric methods for doubly robust estimation of continuous treatment effects}},
volume = {79},
year = {2017}
}

@book{Tsybakov2009,
author = {Tsybakov, Alexandre},
file = {:Users/pawel/Documents/UGent/Books/Nonparametric estimation/20121209191850{\_}7.pdf:pdf},
publisher = {Springer},
title = {{Introduction to Nonparametric Estimation}},
year = {2009}
}

@article{Sun1994,
author = {Sun, Jiayang and Loader, Clive},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/1176325631.pdf:pdf},
journal = {The Annals of Statistics},
pages = {1328--1345},
title = {{Simultaneous Confidence Bands for Linear Regression and Smoothing}},
volume = {22},
year = {1994}
}

@article{Faraway1995,
author = {Faraway, Julian and Sun, Jiayang},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2291347.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Scheff{\'{e}} confidence interval,Tube formula},
number = {431},
pages = {1094--1098},
title = {{Simultaneous confidence bands for linear regression with heteroscedastic errors}},
volume = {90},
year = {1995}
}

@book{Wasserman2006,
author = {Wasserman, Larry},
booktitle = {All of Nonparametric Statistics},
file = {:Users/pawel/Documents/UGent/Books/Nonparametric estimation/book2(2).pdf:pdf},
publisher = {Springer},
title = {{All of Nonparametric Statistics}},
url = {http://books.google.com/books?id=9tv0taI8l6YC},
year = {2006}
}

@article{Rubin1974,
abstract = {A discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation is presented. The objective is to specify the benefits of randomization in estimating causal effects of treatments. The basic conclusion is that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and nec-essary procedure in many cases.},
author = {Rubin, Donald},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/420{\_}paper{\_}Rubin74.pdf:pdf},
journal = {Journal of Educational Psychology},
number = {5},
pages = {688--701},
title = {{Estimating causal effects of treatment in randomized and nonrandomized studies}},
url = {http://www.fsb.muohio.edu/lij14/420{\_}paper{\_}Rubin74.pdf},
volume = {66},
year = {1974}
}
@article{Rosenbaum1983,
abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.},
author = {Rosenbaum, Paul and Rubin, Donald},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/70-1-41.pdf:pdf},
isbn = {9780511810725},
journal = {Biometrika},
volume = {70},
number = {1},
pages = {41--55},
title = {{The central role of the propensity score in observational studies for causal effects}},
year = {1983}
}
@article{Athey2015,
abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without "sparsity" assumptions. We propose an "honest" approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the "ground truth" for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90{\%} confidence intervals, whereas coverage ranges between 74{\%} and 84{\%} for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7-22{\%}.},
archivePrefix = {arXiv},
arxivId = {1504.01132},
author = {Athey, Susan and Imbens, Guido},
eprint = {1504.01132},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Machine{\_}Learning{\_}Methods{\_}for{\_}Estimating{\_}Heterogene.pdf:pdf},
issn = {10916490},
keywords = {Causal inference,Cross-validation,Heterogeneous treatment effects,Potential outcomes,Supervised machine learning},
pmid = {27382149},
title = {{Machine Learning Methods for Estimating Heterogeneous Causal Effects}},
year = {2015}
}

@article{Robinson2010,
author = {Robinson, P},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/3{\_}20140520034711{\_}Root-N-consistent semiparametric regression models.pdf:pdf},
journal = {Econometrica},
number = {4},
pages = {931--954},
title = {{Root-N-Consistent Semiparametric Regression}},
volume = {56},
year = {1988}
}
@article{Newey1994,
author = {Newey, Whitney},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2951752.pdf:pdf},
journal = {Econometrica},
number = {6},
pages = {1349--1382},
title = {{The Asymptotic Variance of Semiparametric Estimators}},
volume = {62},
year = {1994}
}
@book{Tsiatis2006,
author = {Tsiatis, Anastasios},
file = {:Users/pawel/Documents/UGent/Books/(Springer Series in Statistics) Anastasios Tsiatis-Semiparametric Theory and Missing Data -Springer (2006).pdf:pdf},
isbn = {9780387775005},
issn = {01727397},
pmid = {15772297},
publisher = {Springer},
title = {{Semiparametric Theory and Missing Data}},
year = {2006}
}
@article{Semenova2017,
abstract = {This paper provides estimation and inference methods for a large number of heterogeneous treatment effects in the presence of an even larger number of controls and unobserved unit heterogeneity. In our main example, the vector of heterogeneous treatments is generated by interacting the base treatment variable with a subset of controls. We first estimate the unit-specific expectation functions of the outcome and each treatment interaction conditional on controls and take the residuals. Second, we report the Lasso (L1-regularized least squares) estimate of the heterogeneous treatment effect parameter, regressing the outcome residual on the vector of treatment ones. We debias the Lasso estimator to conduct simultaneous inference on the target parameter by Gaussian bootstrap. We account for the unobserved unit heterogeneity by projecting it onto the time-invariant covariates, following the correlated random effects approach of Mundlak (1978) and Chamberlain (1982). We demonstrate our method by estimating price elasticities of groceries based on scanner data.},
archivePrefix = {arXiv},
arxivId = {1712.09988},
author = {Semenova, Vira and Goldman, Matt and Chernozhukov, Victor and Taddy, Matt},
eprint = {1712.09988},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/1712.09988.pdf:pdf},
issn = {2331-8422},
number = {2016},
title = {{Estimation and Inference on Heterogeneous Treatment Effects in High-Dimensional Dynamic Panels}},
url = {http://arxiv.org/abs/1712.09988},
year = {2017}
}
@article{Nie2021,
abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines.},
archivePrefix = {arXiv},
arxivId = {1712.04912},
author = {Nie, X and Wager, S},
eprint = {1712.04912},
file = {:Users/pawel/Documents/UGent/Papers/asaa076(1).pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {2},
pages = {299--319},
title = {{Quasi-oracle estimation of heterogeneous treatment effects}},
volume = {108},
year = {2021}
}
@incollection{Robins2004,
abstract = {I describe two new methods for estimating the optimal treatment regime (equivalently, protocol, plan or strategy) from very high dimesional observational and experimental data: (i) g-estimation of an optimal double-regime structural nested mean model (drSNMM) and (ii) g-estimation of a standard single regime SNMM combined with sequential dynamic-programming (DP) regression. These methods are compared to certain regression methods found in the sequential decision and reinforcement learning literatures and to the regret modelling methods of Murphy (2003). I consider both Bayesian and frequentist inference. In particular, I propose a novel ``Bayes-frequentist compromise'' that combines honest subjective non- or semiparametric Bayesian inference with good frequentist behavior, even in cases where the model is so large and the likelihood function so complex that standard (uncompromised) Bayes procedures have poor frequentist performance.},
author = {Robins, James},
booktitle = {Proceedings of the Second Seattle Symposium in Biostatistics: Analysis of Correlated Data},
chapter = {Optimal St},
editor = {{Lin, D. Y. and Heagerty}, P. J.},
file = {:Users/pawel/Documents/UGent/Papers/seattlemay04final.pdf:pdf},
pages = {189--326},
publisher = {Springer New York},
title = {{Optimal Structural Nested Models for Optimal Sequential Decisions}},
year = {2004}
}
@article{Neyman1979,
author = {Neyman, Jerzy},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/25050174.pdf:pdf},
journal = {Sankhya},
number = {1},
pages = {1--21},
title = {{C($\alpha$) Tests and Their Use}},
volume = {41},
year = {1979}
}
@article{Chernozhukov2016,
abstract = {Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a "double ML" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods.},
archivePrefix = {arXiv},
arxivId = {1608.00060},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
eprint = {1608.00060},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/1608.00060.pdf:pdf},
title = {{Double/Debiased Machine Learning for Treatment and Causal Parameters}},
url = {http://arxiv.org/abs/1608.00060},
year = {2016}
}

@article{vanderLaan2013,
author = {van der Laan, Mark J},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Targeted Learning of an Optima(1).pdf:pdf},
journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
title = {{Targeted Learning of an Optimal Dynamic Treatment, and Statistical Inference for its Mean Outcome}},
year = {2013}
}

@article{Chernozhukov2018a,
abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter $\theta$0 in the presence of high-dimensional nuisance parameters $\eta$0. We depart from the classical setting by allowing for $\eta$0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate $\eta$0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating $\eta$0 cause a heavy bias in estimators of $\theta$0 that are obtained by naively plugging ML estimators of $\eta$0 into estimating equations for $\theta$0. This bias results in the naive estimator failing to be N-1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest $\theta$0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate $\theta$0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N-1 -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/ectj00c1.pdf:pdf},
issn = {1368423X},
journal = {Econometrics Journal},
number = {1},
pages = {C1--C68},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Nie2021b,
abstract = {Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. We develop an “advantage doubly robust” estimator for learning such dynamic treatment rules using observational data under the assumption of sequential ignorability. We prove welfare regret bounds that generalize results for doubly robust learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not need any structural (e.g., Markovian) assumptions. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1905.09751},
author = {Nie, Xinkun and Brunskill, Emma and Wager, Stefan},
eprint = {1905.09751},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Learning When to Treat Policies.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Causal inference,Minimax regret,Reinforcement learning,Sequential ignorability},
number = {533},
pages = {392--409},
publisher = {Taylor {\&} Francis},
title = {{Learning When-to-Treat Policies}},
url = {https://doi.org/10.1080/01621459.2020.1831925},
volume = {116},
year = {2021}
}
@article{Athey2017,
author = {Athey, Susan},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/science.aal4321.pdf:pdf},
journal = {Science},
number = {February},
pages = {483--485},
title = {{Beyond prediction: Using big data for policy problems}},
volume = {355},
year = {2017}
}
@article{Nekipelov2021,
abstract = {This paper proposes a Lasso-type estimator for a high-dimensional sparse parameter identified by a single index conditional moment restriction (CMR). In addition to this parameter, the moment function can also depend on a nuisance function, such as the propensity score or the conditional choice probability, which we estimate by modern machine learning tools. We first adjust the moment function so that the gradient of the future loss function is insensitive (formally, Neyman orthogonal) with respect to the first-stage regularisation bias, preserving the single index property. We then take the loss function to be an indefinite integral of the adjusted moment function with respect to the single index. The proposed Lasso estimator converges at the oracle rate, where the oracle knows the nuisance function and solves only the parametric problem. We demonstrate our method by estimating the short-term heterogeneous impact of Connecticut's Jobs First welfare reform experiment on women's welfare participation decision.},
archivePrefix = {arXiv},
arxivId = {1806.04823},
author = {Nekipelov, Denis and Semenova, Vira and Syrgkanis, Vasilis},
eprint = {1806.04823},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Plug-in{\_}Regularized{\_}Estimation{\_}of{\_}High-Dimensional.pdf:pdf},
issn = {1368-4221},
journal = {The Econometrics Journal},
number = {June 2018},
title = {{Regularised orthogonal machine learning for nonlinear semiparametric models}},
year = {2021}
}
@article{Kunzel2019,
abstract = {There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
archivePrefix = {arXiv},
arxivId = {1706.03461},
author = {K{\"{u}}nzel, S{\"{o}}ren and Sekhon, Jasjeet and Bickel, Peter and Yu, Bin},
eprint = {1706.03461},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/4156.full.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Observational studies,conditional average treatment effect,heterogeneous treatment effects,minimax optimality,randomized controlled trials},
number = {10},
pages = {4156--4165},
pmid = {30770453},
title = {{Metalearners for estimating heterogeneous treatment effects using machine learning}},
volume = {116},
year = {2019}
}
@article{Luedtke2016a,
abstract = {We consider the estimation of an optimal dynamic two time-point treatment rule defined as the rule that maximizes the mean outcome under the dynamic treatment, where the candidate rules are restricted to depend only on a user-supplied subset of the baseline and intermediate covariates. This estimation problem is addressed in a statistical model for the data distribution that is nonparametric, beyond possible knowledge about the treatment and censoring mechanisms. We propose data adaptive estimators of this optimal dynamic regime which are defined by sequential loss-based learning under both the blip function and weighted classification frameworks. Rather than a priori selecting an estimation framework and algorithm, we propose combining estimators from both frameworks using a super-learning based cross-validation selector that seeks to minimize an appropriate cross-validated risk. The resulting selector is guaranteed to asymptotically perform as well as the best convex combination of candidate algorithms in terms of loss-based dissimilarity under conditions. We offer simulation results to support our theoretical findings.},
author = {Luedtke, Alexander and van der Laan, Mark},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Super-Learning{\_}of{\_}an{\_}Optimal{\_}Dynamic{\_}Treatment{\_}Rul(1).pdf:pdf},
issn = {15574679},
journal = {International Journal of Biostatistics},
keywords = {Causal inference,cross-validation,dynamic treatment,loss function,oracle inequality},
number = {1},
pages = {305--332},
pmid = {27227726},
title = {{Super-Learning of an Optimal Dynamic Treatment Rule}},
volume = {12},
year = {2016}
}
@article{Imai2013,
abstract = {When evaluating the efficacy of social programs and medical treatments using randomized experiments, the estimated overall average causal effect alone is often of limited value and the researchers must investigate when the treatments do and do not work. Indeed, the estimation of treatment effect heterogeneity plays an essential role in (1) selecting the most effective treatment from a large number of available treatments, (2) ascertaining subpopulations for which a treatment is effective or harmful, (3) designing individualized optimal treatment regimes, (4) testing for the existence or lack of heterogeneous treatment effects, and (5) generalizing causal effect estimates obtained from an experimental sample to a target population. In this paper, we formulate the estimation of heterogeneous treatment effects as a variable selection problem. We propose a method that adapts the Support Vector Machine classifier by placing separate sparsity constraints over the pre-treatment parameters and causal heterogeneity parameters of interest. The proposed method is motivated by and applied to two well-known randomized evaluation studies in the social sciences. Our method selects the most effective voter mobilization strategies from a large number of alternative strategies, and it also identifies the characteristics of workers who greatly benefit from (or are negatively affected by) a job training program. In our simulation studies, we find that the proposed method often outperforms some commonly used alternatives. {\textcopyright} 2013 Institute of Mathematical Statistics.},
author = {Imai, Kosuke and Ratkovic, Marc},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/svm.pdf:pdf},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Causal inference,Individualized treatment rules,LASSO,Moderation,Variable selection},
number = {1},
pages = {443--470},
title = {{Estimating treatment effect heterogeneity in randomized program evaluation}},
volume = {7},
year = {2013}
}

@article{Robins1986,
abstract = {In his published paper the author developed a formal theory of causal inference in epidemiologic studies. in which causal questions that could be asked in English concerning the study population become mathematical conjectures about the population causal parameters of a causally interpreted structured tree graph (CISTG). It was shown that for randomized CISTGs a subset of the population causal parameters, i. e. the G-causal parameters, could be empirically estimated. It is shown here that under certain assumptions, the G-causal parameters of certain CISTGs that are not R CISTGs can also be empirically estimated. Other extensions to his previous paper are given.},
author = {Robins, James},
file = {:Users/pawel/Documents/UGent/Papers/1-s2.0-0270025586900886-main.pdf:pdf},
issn = {00974943},
journal = {Mathematical Modelling},
number = {9-12},
pages = {923--945},
title = {{A New Approach To Causal Inference in Mortality Studies With a Sustained Exposure Period - Application To Control of the Healthy Worker Survivor Effect}},
volume = {14},
year = {1986}
}
@article{Hirano2003,
abstract = {We are interested in estimating the average effect of a binary treatment on a scalar outcome. If assignment to the treatment is exogenous or unconfounded, that is, independent of the potential outcomes given covariates, biases associated with simple treatment-control average comparisons can be removed by adjusting for differences in the covariates. Rosenbaum and Rubin (1983) show that adjusting solely for differences between treated and control units in the propensity score removes all biases associated with differences in covariates. Although adjusting for differences in the propensity score removes all the bias, this can come at the expense of efficiency, as shown by Hahn (1998), Heckman, Ichimura, and Todd (1998), and Robins, Mark, and Newey (1992). We show that weighting by the inverse of a nonparametric estimate of the propensity score, rather than the true propensity score, leads to an efficient estimate of the average treatment effect. We provide intuition for this result by showing that this estimator can be interpreted as an empirical likelihood estimator that efficiently incorporates the information about the propensity score.},
author = {Hirano, Keisuke and Imbens, Guido and Ridder, Geert},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/efficient{\_}estimation{\_}of{\_}average{\_}treatment{\_}effects{\_}using{\_}the{\_}estimated{\_}propensity{\_}score.pdf:pdf},
issn = {00129682},
journal = {Econometrica},
keywords = {Propensity score,Semiparametric efficiency,Sieve estimator,Treatment effects},
number = {4},
pages = {1161--1189},
title = {{Efficient estimation of average treatment effects using the estimated propensity score}},
volume = {71},
year = {2003}
}

@article{Chernozhukov2018,
abstract = {We provide adaptive inference methods, based on {\$}\backslashell{\_}1{\$} regularization, for regular (semiparametric) and non-regular (nonparametric) linear functionals of the conditional expectation function. Examples of regular functionals include average treatment effects, policy effects, and derivatives. Examples of non-regular functionals include average treatment effects, policy effects, and derivatives conditional on a covariate subvector fixed at a point. We construct a Neyman orthogonal equation for the target parameter that is approximately invariant to small perturbations of the nuisance parameters. To achieve this property, we include the Riesz representer for the functional as an additional nuisance parameter. Our analysis yields weak "double sparsity robustness": either the approximation to the regression or the approximation to the representer can be "completely dense" as long as the other is sufficiently "sparse". Our main results are non-asymptotic and imply asymptotic uniform validity over large classes of models, translating into honest confidence bands for both global and local parameters.},
archivePrefix = {arXiv},
arxivId = {1802.08667},
author = {Chernozhukov, Victor and Newey, Whitney and Singh, Rahul},
eprint = {1802.08667},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/1802.08667v5.pdf:pdf},
issn = {2331-8422},
keywords = {gaussian approximation,neyman orthogonality,sparsity},
number = {1959},
pages = {1--49},
title = {{De-Biased Machine Learning of Global and Local Parameters Using Regularized Riesz Representers}},
url = {http://arxiv.org/abs/1802.08667},
year = {2018}
}
@article{Athey2016,
abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without "sparsity" assumptions. We propose an "honest" approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the "ground truth" for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90{\%} confidence intervals, whereas coverage ranges between 74{\%} and 84{\%} for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7-22{\%}.},
archivePrefix = {arXiv},
arxivId = {1504.01132},
author = {Athey, Susan and Imbens, Guido},
eprint = {1504.01132},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/7353.full.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Causal inference,Cross-validation,Heterogeneous treatment effects,Potential outcomes,Supervised machine learning},
number = {27},
pages = {7353--7360},
pmid = {27382149},
title = {{Recursive partitioning for heterogeneous causal effects}},
volume = {113},
year = {2016}
}
@article{Neyman1923,
author = {Neyman, Jerzy},
file = {:Users/pawel/Documents/UGent/Papers/euclid.ss.1177012031.pdf:pdf},
journal = {Roczniki Nauk Rolniczych Tom X},
pages = {1--51},
title = {{On the Application of Probability Theory to Agricultural Experiments}},
year = {1923}
}
@article{Hahn1998,
author = {Hahn, Jinyong},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2998560.pdf:pdf},
journal = {Econometrica},
number = {2},
pages = {315--331},
title = {{On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects Author ( s ): Jinyong Hahn Published by : The Econometric Society Stable URL : http://www.jstor.org/stable/2998560 REFERENCES Linked references are ava}},
volume = {66},
year = {1998}
}
@article{Robins1995,
abstract = {We consider the efficiency bound for the estimation of the parameters of semiparametric models defined solely by restrictions on the means of a vector of correlated outcomes, Y, when the data on Y are missing at random. We show that the semiparametric variance bound is the asymptotic variance of the optimal estimator in a class of inverse probability of censoring weighted estimators and that this bound is unchanged if the data are missing completely at random. For this case we study the asymptotic performance of the generalized estimating equations (GEE) estimators of mean parameters and show that the optimal GEE estimator is inefficient except for special cases. The optimal weighted estimator depends on unknown population quantities. But for monotone missing data, we propose an adaptive estimator whose asymptotic variance can achieve the bound. {\textcopyright} 1995 Taylor {\&} Francis Group, LLC.},
author = {Robins, James and Rotnitzky, Andrea},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2291135(1).pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Correlated outcomes,Generalized estimating equations,Generalized least squares,Longitudinal studies,Missing at random},
number = {429},
pages = {122--129},
title = {{Semiparametric efficiency in multivariate regression models with missing data}},
volume = {90},
year = {1995}
}
@article{Kitagawa2018,
abstract = {One of the main objectives of empirical analysis of experiments and quasi-experiments is to inform policy decisions that determine how treatments are allocated to individuals with different observable covariates. We propose the Empirical Welfare Maximization (EWM) method, which estimates a treatment assignment policy by maximizing the sample analog of average social welfare over a class of candidate treatment policies. We show that, when propensity score is known, the average social welfare attained by EWM rules converges at least at n{\{}{\^{}}{\}}{\{}{\{}{\}}-1/2{\{}{\}}{\}} rate to the maximum obtainable welfare. This holds uniformly over a minimally constrained class of data distributions, and this uniform convergence rate is minimax optimal. In comparison with this benchmark rate, we examine how the uniform convergence rate of the average welfare improves or deteriorates depending on the richness of the class of candidate decision rules, on the distribution of conditional treatment effects, on the lack of knowledge for the propensity score, and on additional smoothness assumptions for the regression functions or propensity scores. We also discuss practically implementable computation for the EWM rule. As an empirical application, we derive an EWM rule for the a training program using the experiment data analyzed in La Londe (1986).},
author = {Kitagawa, Toru and Tetenov, Aleksey},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/kitagawa{\_}tetenov{\_}ecta2018.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {2},
pages = {591--616},
title = {{Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice}},
volume = {86},
year = {2018}
}

@book{Hernan2020,
abstract = {Causal Inference is an admittedly pretentious title for a book. Causal inference is a complex scientific task that relies on triangulating evidence from multiple sources and on the application of a variety of methodological approaches. No book can possibly provide a comprehensive description of methodologies for causal inference across the sciences. The authors of any Causal Inference book will have to choose which aspects of causal inference methodology they want to emphasize. The title of this introduction reflects our own choices: a book that helps scientists–especially health and social scientists–generate and analyze data to make causal inferences that are explicit about both the causal question and the assumptions underlying the data analysis. Unfortunately, the scientific literature is plagued by studies in which the causal question is not explicitly stated and the investigators' unverifiable assumptions are not declared. This casual attitude towards causal inference has led to a great deal of confusion. For example, it is not uncommon to find studies in which the effect estimates are hard to interpret because the data analysis methods cannot appropriately answer the causal question (were it explicitly stated) under the investigators' assumptions (were they declared).},
author = {Hern{\'{a}}n, Miguel and Robins, James},
booktitle = {Boca Raton: Chapman {\&} Hall/CRC},
file = {:Users/pawel/Desktop/ciwhatif{\_}hernanrobins{\_}31jan21.pdf:pdf},
publisher = {Boca Raton: Chapman {\&} Hall/CRC},
title = {{Causal Inference - what if}},
year = {2020}
}
@article{Kosorok2019,
author = {Kosorok, Michael and Laber, Eric},
file = {:Users/pawel/Documents/UGent/Papers/nihms-983058.pdf:pdf},
issn = {15445208},
journal = {Annual Review of Statistics and Its Application},
number = {5},
pages = {263--286},
pmid = {29733714},
title = {{Precision medicine}},
volume = {6},
year = {2019}
}
