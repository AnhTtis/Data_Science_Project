\documentclass[12pt]{article}
\textwidth16cm
\textheight21cm
\oddsidemargin0cm
\topmargin-15mm

\usepackage{t1enc}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{apalike}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{epsfig}
\usepackage{textcomp}
\usepackage{alltt}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{dcolumn}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{amssymb}
%\usepackage{xypic,t1enc}
\usepackage{bbm, dsfont}
\usepackage[svgnames]{xcolor}
\usepackage{mathtools}
\usepackage{listings}

\usepackage{pdflscape}

\lstset{language=R,
    basicstyle=\small\ttfamily,
    %stringstyle=\color{DarkGreen},
    otherkeywords={0,1,2,3,4,5,6,7,8,9},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    %keywordstyle=\color{blue},
    commentstyle=\color{DarkGreen},
}

\usepackage{tikz}
\tikzstyle{ov}=[shape=rectangle,
                draw=black!50,
                thick,
                minimum width=0.7cm,
                minimum height=0.7cm]

\tikzstyle{av}=[shape=rectangle,
                draw=black!50,
                fill=black!10,
                thick,
                minimum width=0.7cm,
                minimum height=0.7cm]

\tikzstyle{lv}=[shape=circle,draw=black!50,thick]

\usetikzlibrary{shapes,calendar,matrix,backgrounds,folding,snakes}
\allowdisplaybreaks

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\tr}{tr}

\begin{document}
\newcommand{\cip}{\perp\!\!\!\!\perp}
%\newcommand{}{\begin{itemize}}
%\newcommand{}{\end{itemize}}
\newcommand{\nothere}[1]{}
\newcommand{\noi}{\noindent}
\newcommand{\mbf}[1]{\mbox{\boldmath $#1$}}
\newcommand{\cond}{\, |\,}
\newcommand{\hO}[2]{{\cal O}_{#1}^{#2}}
\newcommand{\hF}[2]{{\cal F}_{#1}^{#2}}
\newcommand{\tl}[1]{\tilde{\lambda}_{#1}^T}
\newcommand{\la}[2]{\lambda_{#1}^T(Z^{#2})}
\newcommand{\I}[1]{1_{(#1)}}
\newcommand{\cd}{\mbox{$\stackrel{\mbox{\tiny{\cal D}}}{\rightarrow}$}}
\newcommand{\cp}{\mbox{$\stackrel{\mbox{\tiny{p}}}{\rightarrow}$}}
\newcommand{\cas}{\mbox{$\stackrel{\mbox{\tiny{a.s.}}}{\rightarrow}$}}
\newcommand{\ld}{\mbox{$\; \stackrel{\mbox{\tiny{def}}}{=3D} \; $}}
\newcommand{\nk}{\mbox{$n \rightarrow \infty$}}
\newcommand{\con}{\mbox{$\rightarrow $}}
\newcommand{\dprime}{\mbox{$\prime \vspace{-1 mm} \prime$}}
\newcommand{\Borel}{\mbox{${\cal B}$}}
\newcommand{\bevis}{\mbox{$\underline{\em{Proof}}$}}
\newcommand{\Rd}[1]{\mbox{${\Re^{#1}}$}}
\newcommand{\il}[1]{{\int_{0}^{#1}}}
\newcommand{\pl}[1]{\mbox{\bf {\LARGE #1}}}
\newcommand{\expit}{\text{expit}}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\blind}{1}
\newcommand{\pr}{\text{pr}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\Bin}{\text{Bin}}
\newcommand{\Exp}{\text{Exp}}
\newcommand{\unif}{\text{unif}}
\newcommand{\logit}{\text{logit}}
\newcommand{\sign}{\text{sign}}
\newcommand{\support}{\text{support}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\diag}{\text{diag}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem*{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{condition}{Condition}

\parindent12pt

\begin{center}{\Large{On a General Class of Orthogonal Learners for the Estimation of Heterogeneous Treatment Effects}}
 \end{center}
 
{ 
\begin{center}
Pawe\l{} Morzywo\l{}ek$^{1}$, Johan Decruyenaere$^{2}$, Stijn Vansteelandt$^{1}$ \\
\bigskip
\scriptsize{$^1$Department of Applied Mathematics, Computer Science and Statistics, Ghent University} \\
\scriptsize{$^2$Department of Intensive Care Medicine, Ghent University Hospital} 
\end{center}
}
\smallskip
\begin{center}
\today
\end{center}
\smallskip

\setlength{\parindent}{0.3in} \setlength{\baselineskip}{24pt}
\begin{abstract}
Motivated by applications in personalized medicine and individualized policy making, there is a growing interest in techniques for quantifying treatment effect heterogeneity in terms of the conditional average treatment effect (CATE). Some of the most prominent methods for CATE estimation developed in recent years are T-Learner, DR-Learner and R-Learner. The latter two were designed to improve on the former by being Neyman-orthogonal. However, the relations between them remain unclear, and likewise does the literature remain vague on whether these learners converge to a useful quantity or (functional) estimand when the underlying optimization procedure is restricted to a class of functions that does not include the CATE. In this article, we provide insight into these questions by discussing DR-learner and R-learner as special cases of a general class of Neyman-orthogonal learners for the CATE, for which we moreover derive oracle bounds. Our results shed light on how one may construct Neyman-orthogonal learners with desirable properties, on when DR-learner may be preferred over R-learner (and vice versa), and on novel learners that may sometimes be preferable to either of these. Theoretical findings are confirmed using results from simulation studies on synthetic data, as well as an application in critical care medicine.
\end{abstract}
\noi
{\it Causal Inference, Conditional Average Treatment Effect, Orthogonal Statistical Learning, Precision Medicine}


\section{Introduction} \label{Section1}

Data-driven decision support systems hold a promise to revolutionize decision making in various areas ranging from medicine to policy making. With the increasing amount of available data it becomes clear that the old paradigm "one-size-fits-all" is no longer satisfactory and modern decision-support systems need to recognize heterogeneity in the data to allow personalized decision-making \citep{Athey2017, Kosorok2019}.  Recognition of this emerging field has led causal inference and computer science research communities to develop techniques for heterogeneous treatment effects (or conditional average treatment effect - CATE) estimation, which has been applied in medicine and the social sciences \citep{vanderLaan2014, Imai2013, Athey2016, Luedtke2016a, Kunzel2019, Foster2019, Kennedy2020a, Nie2021}. 
\\
\indent
The vast majority of the causal inference literature focuses on estimation of the average treatment effect, averaged across the whole population.  In contrast, the CATE expresses the treatment effect as a function of a set of observable features. This has a number of advantages. For example, it allows to identify sub-populations in which the intervention is particularly beneficial or harmful. Furthermore, it is often believed to be better transportable between studies \citep{Dahabreh2020}. However, CATE estimation is more ambitious than the classical problem of estimating the average treatment effect in the whole population. Whereas the latter summarizes the treatment effect into a single number, the former is an infinite-dimensional response curve in function of covariate values. 
\\
\indent
Some of the most prominent methods for CATE estimation developed in recent years are T-Learner \citep{Kunzel2019}, DR-Learner \citep{Kennedy2020a} and R-Learner \citep{Nie2021}. These different learners have been developed from very different perspectives, making the literature opaque on what are the precise relations between these learners. While they all intend to deliver estimates of the CATE, it is for instance unclear what they deliver when these distinct learners are optimized over a function class that does not contain the CATE.  
\\
\indent
In this work, we accommodate this by developing a class of CATE learners obtained by minimization of well-chosen loss functions that quantify squared bias in the CATE learner, and which differ in how this squared bias is averaged over the study population. This reproduces popular examples from the literature on heterogeneous treatment effects, e.g. DR-Learner and R-Learner. Viewing different methods through the lens of a single framework provides insight into their differences, but also leads to novel learners based on Neyman-orthogonal loss functions \citep{Chernozhukov2018a, Foster2019}. This Neyman-orthogonality is essential as it insulates the learners against small perturbations of the nuisance parameters that index the loss function on which they are based. This renders the resulting estimators for the CATE, obtained upon substituting these nuisance parameters by (sufficiently fast converging) consistent plug-in estimators, asymptotically equivalent to the oracle estimator, which uses the known nuisance parameters. This allows to mitigate the regularization bias resulting from using flexible modelling techniques for estimation of nuisance parameters.
\\
\indent
The paper is structured as follows. In Section \ref{Section2}, we describe the problem setting and introduce key concepts related to heterogeneous treatment effects estimation. In Section \ref{Section3}, we describe a general approach to construct Neyman-orthogonal loss functions for the CATE, building on the literature of semiparametric and nonparametric inference \citep{Pfanzagl1990, Bickel1998, vanderLaan2003, Tsiatis2006, Kosorok2008, vanderLaan2011, Chernozhukov2018a, Foster2019}.
This gives rise to a general class of orthogonal learners for heterogeneous treatment effects estimation.  In Section \ref{Section4}, we present an empirical example from critical care medicine. In Section \ref{Section5}, we present a simulation study comparing the different methods. We conclude with a discussion of the results in Section \ref{Section6}.

\section{Problem Setting and Notation} \label{Section2}

We consider $n$ independent and identically distributed observations $Z_i \equiv \left( Y_i, A_i, X_i \right)$, for $i = 1, \dots,n$, where $X_i \in \mathcal{X} \subseteq \mathbb{R}^{d_X}$ is sufficient for confounding adjustment of a binary treatment $A_i \in \left\lbrace 0, 1 \right\rbrace$ on outcome $Y_i \in \mathbb{R}$. 
Based on this, our aim is to estimate the conditional average treatment effect (CATE) given by 
\begin{equation*} 
\tau \left( v \right) \equiv \mathbb{E} \left( Y^1 - Y^0 \cond V = v \right).
\end{equation*}
Here $Y^a$ is the potential outcome under treatment $A = a$, for $a = 0,1$ \citep{Neyman1923, Rubin1974,Robins1986, Hernan2020} and $V \in \mathcal{V} \subseteq \mathbb{R}^{d_V}$ is a subset of the features $X$, i.e. $d_V \leq d_X$.  The CATE is identifiable under assumptions usually invoked in the causal inference literature:
\begin{assumption} \label{Assump1}
Conditional exchangeability, i.e. $Y_i^a \indep A_i \cond X_i$ for $a = 0,1$.
\end{assumption}
Conditional exchangeability, sometimes referred to as the no unmeasured confounding assumption, states that the set of collected features is sufficient for confounding adjustment. 
\begin{assumption} \label{Assump2}
Positivity, i.e. $0 < \pi \left( X_i \right) < 1$, where $\pi \left( X_i \right) \equiv \mathbb{P} \left( A_i = 1 \cond X_i\right)$ is a propensity score. 
\end{assumption}
The positivity assumption guarantees that for each covariate stratum $X = x$ we have a positive probability of observing patients receiving each of the treatment options. This is necessary because to infer the effect of a treatment we need to compare outcome of patients with similar characteristics but different treatment assignment. The assumptions of conditional exchangeability and positivity are sometimes jointly referred to as strong ignorability \citep{Rosenbaum1983}. 
\begin{assumption} \label{Assump3}
Consistency, i.e. $Y_i^a = Y_i$ for $A_i = a$.
\end{assumption}
The consistency assumption states that versions of the interventions observed in the data correspond to well-defined interventions. 
\\
\indent
To estimate the conditional average treatment effect we set out to find a function $g \in \mathcal{G} \equiv \left\lbrace h \cond h \colon \mathcal{V} \rightarrow \mathbb{R} \right\rbrace$ that is the best approximation of CATE in terms of the mean-squared error. For instance, we may consider minimizing the following population risk function
\begin{align} \label{eqn1}
\mathcal{L} \left( g \right) &= \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right] \\
\nonumber
&= \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - \mathbb{E} \left( Y^1 - Y^0 \cond V \right) + \mathbb{E} \left( Y^1 - Y^0 \cond V \right) - g \left( V \right) \right\rbrace^2 \right] \\
\nonumber
&=  \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - \mathbb{E} \left( Y^1 - Y^0 \cond V \right) \right\rbrace^2 \right] + \mathbb{E} \left[ \left\lbrace \mathbb{E} \left( Y^1 - Y^0 \cond V \right) - g \left( V \right) \right\rbrace^2 \right] \\
\nonumber
&+ 2 \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - \mathbb{E} \left( Y^1 - Y^0 \cond V \right) \right\rbrace \left\lbrace \mathbb{E} \left( Y^1 - Y^0 \cond V \right) - g \left( V \right) \right\rbrace  \right] \\
\nonumber
&=  \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - \mathbb{E} \left( Y^1 - Y^0 \cond V \right) \right\rbrace^2 \right] + \mathbb{E} \left[ \left\lbrace \mathbb{E} \left( Y^1 - Y^0 \cond V \right) - g \left( V \right) \right\rbrace^2 \right],
\end{align}
where the last equality follows using the law of iterated expectations. We denote the minimizer of the population risk function (\ref{eqn1}) by
\begin{align*}
g^* \equiv \argmin_{g \in \mathcal{G}} \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right].
\end{align*}
Note that if $\mathbb{E} \left( Y^1 - Y^0 \cond V \right) \in \mathcal{G}$, then it follows from the above calculation, upon invoking the projection theorem, that $g^* \left( V \right) = \mathbb{E} \left( Y^1 - Y^0 \cond V \right)$. 
\\
\indent
The population risk (\ref{eqn1}) cannot be directly used for estimation as it relies on knowledge of both potential outcomes $Y^0$ and $Y^1$, which are not available to the data analyst who observes only one of the potential outcomes for each individual. To circumvent this problem one usually invokes the identifiability assumptions of conditional exchangeability, positivity and consistency to express the population risk (\ref{eqn1}) in terms of the observed data. This leads to a population risk function $\mathcal{L} \left( g, \eta \right)$, which additionally to the parameter of interest $g$ depends on so-called nuisance parameters $\eta$ (e.g., the infinite-dimensional propensity score and/or conditional mean outcome). We illustrate this in the following sections. 

\subsection{Nonparametric outcome regression} \label{subsect4}

When $V$ is sufficient for the confounding adjustment, i.e. $V = X$, under the above stated identifiability assumptions of conditional exchangeability, positivity and consistency, we can express the CATE as $\tau \left( x \right) = Q^{\left( 1 \right)} \left( x \right) - Q^{\left( 0 \right)} \left( x \right)$, where $Q^{\left( a \right)} \left( x \right) = \mathbb{E} \left( Y \cond X = x, A = a \right)$. Note that, if $\mathbb{E} \left( Y^1 - Y^0 \cond X \right) \in \mathcal{G}$, then $g^* \left( x \right) = Q^{\left( 1 \right)} \left( x \right) - Q^{\left( 0 \right)} \left( x \right)$. This suggests the following approach to CATE estimation, which treats it as a classical prediction problem. First, construct estimators $\hat{Q}^{\left( a \right)} \left( x \right)$, for $a = 0, 1$, through fitting nonparametric outcome prediction models separately in the treated and untreated populations. Once we have $\hat{Q}^{\left( 0 \right)} \left( x \right)$ and $\hat{Q}^{\left( 1 \right)} \left( x \right)$ we subtract them to obtain a CATE estimator
\begin{align*}
\hat{\tau} \left( x \right) = \hat{Q}^{\left( 1 \right)} \left( x \right) - \hat{Q}^{\left( 0 \right)}\left( x \right).
\end{align*}
Following \cite{Kunzel2019} we refer to this approach as the T-Learner. While simple, it has several limitations. 
\\
\indent
Firstly, note that through minimization of the population risk function (\ref{eqn1}) we aim to balance the bias-variance trade-off for CATE estimation, which is often achieved through regularization.  However, T-learner does not target the CATE directly, but instead consists of two separate minimization problems resulting in two, possibly regularized, outcome predictions. Each of them aims to balance the bias-variance trade-off in a single outcome regression problem. However, once we subtract these predictions to estimate the CATE we no longer have control over the bias-variance trade-off for the CATE estimator, which may result in a solution with too high variance, i.e. undersmoothing, or too much bias, i.e. oversmoothing. Moreover, T-Learner is unable to exploit a priori knowledge that for instance the CATE is an element of a "simple" class $\mathcal{G}$. Even if we had such knowledge (as indeed we may often believe treatment effects to lack heterogeneity w.r.t. certain features), then the use of regularization separately on each outcome regression merely helps to produce "simple" functions $\hat{Q}^{\left( 0 \right)} \left( x \right)$ and $\hat{Q}^{\left( 1 \right)} \left( x \right)$; the difference $\hat{Q}^{\left( 1 \right)} \left( x \right) - \hat{Q}^{\left( 0 \right)} \left( x \right)$ of such "simple" functions is nonetheless often much more "complex".  
\\
\indent
Secondly note that the outcome regression $\hat{Q}^{\left( 0 \right)} \left( x \right)$ is fitted in the untreated population and $\hat{Q}^{\left( 1 \right)} \left( x \right)$ is fitted in the treated population. However, to have a well-performing T-Learner one needs the outcome regressions $\hat{Q}^{\left( 0 \right)} \left( x \right)$ and $\hat{Q}^{\left( 1 \right)} \left( x \right)$ to model the response surfaces well in the whole population, i.e. over the whole covariate space. It is often the case that exposed and unexposed groups of individuals have a different distribution of the covariates $X$, i.e. they are concentrated in different regions of the covariate space, which is referred to as covariate shift.  In this case, the outcome regression $\hat{Q}^{\left( 0 \right)} \left( x \right)$, by relying strongly on the parts of the covariates space with many unexposed individuals, may over-smooth and extrapolate in the parts of covariate space with treated individuals. Similarly, the outcome regression $\hat{Q}^{\left( 1 \right)} \left( x \right)$ may over-smooth and extrapolate in the parts of covariate space where untreated individuals are concentrated.  This may lead to severe bias, resulting in poor CATE estimators. Moreover, as described by \cite{Kunzel2019}, in the situation of unbalanced data T-Learner might over-smooth the outcome regression in the less represented group leading to spurious heterogeneity resulting from the fitted models. 
\\
\indent
Finally, T-Learner does not optimize a useful or well-understood quantity when $\tau \left( X \right) \notin \mathcal{G}$,  i.e. when CATE does not belong to the class of functions over which we are minimizing the population risk, as is for instance the case when we wish to restrict $\mathcal{G}$ to functions of only the subset of features $V$, which is sometimes referred to as "runtime confounding" \citep{Coston2020}.  

\subsection{Inverse-probability-weighting} 

In view of the aforementioned problems with the T-learner, we may instead consider replacing the treatment effect $Y^1 - Y^0$ in (\ref{eqn1}) by the pseudo-outcome
\begin{equation*}
\frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)},
\end{equation*}
where $\pi_0 \left( X \right) \equiv \mathbb{P} \left( A = 1 \cond X \right)$ is the propensity score. In this way we obtain the following population risk function
\begin{align} \label{eqn2}
\mathcal{L} \left( g, \pi_0 \right) = \mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - g \left( V \right) \right\rbrace^2 \right].
\end{align}
The inverse-probability-weighted population risk function (\ref{eqn2}) is an example of a population risk function with nuisance parameter given by the propensity score.  
\begin{lemma} \label{lemma1}
Population risk functions (\ref{eqn1}) and (\ref{eqn2}) lead to the same minimizer over the function space $\mathcal{G}$.
\end{lemma}
\begin{proof}
See Appendix \ref{AppA}.
\end{proof}
Lemma \ref{lemma1} shows that this IPW-learner addresses several of the concerns raised for the T-learner. First, minimization of population risk function (\ref{eqn2}) delivers the CATE when $\mathbb{E} \left( Y^1 - Y^0 \cond V \right) \in \mathcal{G}$. Second, when the CATE does not belong to $\mathcal{G}$, minimization of (\ref{eqn2}) still retrieves the function that is closest to the CATE in terms of mean-squared error, or from (\ref{eqn1}), the function that best predicts the individual treatment effect. Third, by directly modelling the CATE via (\ref{eqn2}), we can also directly incorporate any a priori knowledge that we may have (e.g., that the function class $\mathcal{G}$ should contain constant functions to allow for the possibility of no treatment effect heterogeneity).
\\
\indent
In practice, unless known by design, the propensity score $\pi_0 \left( X \right)$ is unknown and needs to be estimated. Therefore, one actually minimizes the sample analog of the risk function of the form
\begin{align} \label{eqn3}
\hat{\mathcal{L}} \left( g, \hat{\pi} \right) = \frac{1}{n} \sum_{i=1}^n \left\lbrace \frac{A_iY_i}{\hat{\pi} \left( X_i \right)} - \frac{\left( 1-A_i \right) Y_i}{1- \hat{\pi} \left( X_i \right)} - g \left( V_i \right) \right\rbrace^2 + \Lambda \left\lbrace g \left( \cdot \right) \right\rbrace.
\end{align}
Here $\Lambda \left\lbrace g \left( \cdot \right) \right\rbrace$ is a penalization term that usually will be present when employing a machine learning algorithm to avoid that the resulting CATE estimate is an overly "complex" function and $\hat{\pi} \left( X \right)$ is a nonparametric or data-adaptive estimator (e.g. spline estimator or random forest) of the propensity score. This is sometimes referred to as plug-in empirical risk minimization \citep{Foster2019}. One hopes that the minimizer $\hat{g}$ for the parameter of interest $g$ obtained through minimizing the sample risk function evaluated at the estimated value of the propensity score, i.e. sample risk (\ref{eqn3}), is close to the minimizer $g^*$ obtained through minimizing the population risk evaluated at the true value of the propensity score, i.e. the population risk function (\ref{eqn2}). Unfortunately, in general this is not the case for inverse-probability-weighting.
In particular, when the dimension of $V$ is much smaller than that of $X$, or when the function class $\mathcal{G}$ is relatively small, then the estimation errors in $\hat{\pi} \left( X \right)$ may dominate those in the minimizer of (\ref{eqn3}) with known propensity score, thereby causing the minimizer of (\ref{eqn3}) to have drastically different behaviour. 
\\
\indent
Throughout, we will therefore aim to find estimation procedures that are at most minimally affected by the estimation of nuisance parameters, in the sense that small errors in the estimated nuisance parameters have only small impact on the estimation of the target parameter (as we will formalize later). In line with the recent literature, we will achieve this by utilizing loss functions with a property called Neyman-orthogonality \citep{Neyman1979, Chernozhukov2018a, Foster2019}.

\subsection{Retargeting for heterogeneous treatment effects estimation} 

Before discussing Neyman-orthogonality in Section \ref{Section3}, we would like to alert the reader to the fact that minimization of the mean-squared error (\ref{eqn1}) may not be relevant in populations where treatment or no-treatment are not feasible options for some. For instance, in the empirical example presented in the Section \ref{Section4} that inspired this work, we are interested in estimates of the effect of initiating renal replacement therapy (RRT) in acute kidney injury (AKI) patients at the intensive care unit (ICU), conditional on patient characteristics. Such estimates could then be used by clinicians to decide whether or not to initiate treatment for particular patients. However, for many AKI patients at the ICU clinicians would never consider initiating RRT, based on their available characteristics. It is therefore not of much interest to minimize, as in (\ref{eqn1}), average mean squared error in the individual treatment effect $Y^1-Y^0$, averaged over the whole population of AKI patients at the ICU. Instead, one may rather wish to focus on the patient population with the highest uncertainty about the optimal treatment decision, as this is arguably exactly the patient population for which clinicians are in most need for treatment decision support. 
\\
\indent
One way to address this problem is through the use of weighted estimands \citep{Hirano2003, Crump2006} and retargeting, also used by \cite{Kallus2021} in the context of policy learning. The aim is to change the population on which we optimize the policy by reweighting individuals by a function of the covariates $X$ to generate a larger population in which all treatment options are plausible, so that treatment effects become easier to infer in this retargeted population. This also prioritizes the population of individuals for whom decisions need to be made. 
\\
\indent
As we show below this reweighting of the loss function does not change the minimizer, provided that $V = X$ and the function space $\mathcal{G}$ contains the CATE. In particular, we consider the following population risk based on a weighted loss function
\begin{align} \label{eqn4}
\mathcal{L} \left( g; \omega \right) = \mathbb{E} \left[ \omega \left( X \right) \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right]
\end{align}
over the function space $\mathcal{G}$ for a weight function $\omega \left( \cdot \right)$ of the covariate data $X$. We have the following result.
\begin{lemma} \label{lemma2}
Assume that $V = X$ and $\mathbb{E} \left( Y^1 - Y^0 \cond V \right) \in \mathcal{G}$. Then CATE is the minimizer of the population risk function (\ref{eqn4}) for each choice of the weight function $\omega \left( \cdot \right)$. 
\end{lemma}
\begin{proof}
See Appendix \ref{AppA}.
\end{proof}

It follows from Lemma \ref{lemma2} that population risk function (\ref{eqn4}) forms the basis for an entire class of CATE learners. These all deliver the CATE when $V = X$ and $\mathbb{E} \left( Y^1 - Y^0 \cond V \right) \in \mathcal{G}$, but may have drastically different behaviour otherwise. For instance, in the runtime confounding setting, (\ref{eqn4}) has a different minimizer 
\begin{equation} \label{eqn9}
\frac{ \mathbb{E} \left\lbrace \omega \left( X \right) \left( Y^1 - Y^0 \right) \cond V \right\rbrace }{ \mathbb{E} \left\lbrace \omega \left( X \right) \cond V \right\rbrace }.
\end{equation}
depending on the choice of weight function.
We will study this in more detail in the subsequent sections, where we will also address the fact that population risk function (\ref{eqn4}) cannot be used directly in a data analysis as it involves both potential outcomes $Y^0$ and $Y^1$. In particular, in the next section, we will construct Neyman-orthogonal learners whose population risk functions approximate (\ref{eqn4}) under the earlier identifiability assumptions. We will moreover find these learners to be differently affected by the estimation of nuisance parameters, even when $V = X$ and $\mathbb{E} \left( Y^1 - Y^0 \cond V \right) \in \mathcal{G}$. 


\section{Orthogonal statistical learning for heterogeneous treatment effects} \label{Section3}

Following the methodology of orthogonal statistical learning \citep{Foster2019} we consider population risk functions of the form $\mathcal{L} \left( g, \eta \right)$, where $g \in \mathcal{G}$ is called the target parameter (or the parameter of interest) belonging to a function space $\mathcal{G}$ and $\eta \in \mathcal{H}$ is a nuisance parameter belonging to an infinite-dimensional nuisance space $\mathcal{H}$. The aim is to estimate $g^*$, which is a minimizer of the population risk function $\mathcal{L} \left( g, \eta \right)$, i.e.
\begin{equation*}
g^* \equiv \argmin_{g \in \mathcal{G}} \mathcal{L} \left( g, \eta_0 \right),
\end{equation*}
evaluated at the true value of the nuisance parameter $\eta_0$. As $\eta_0$ is unknown, we consider minimizing the population risk function $\mathcal{L} \left( g, \hat{\eta} \right)$ with plugged-in estimated value $\hat{\eta}$ instead.  This will lead to a different minimizer $\hat{g}$ than $g^*$. The quality of the nuisance parameter estimation may then have a significant impact on the quality of $\hat{g}$. 
\\
\indent
Ideally, we wish that the estimator for the parameter of interest $g$ obtained through minimizing the risk function evaluated at the estimated value of the nuisance parameter led us close to the minimizer that would be obtained through minimizing the risk function evaluated at the true value of the nuisance parameter. Risk functions with such a property are called Neyman-orthogonal \citep{Neyman1979, Chernozhukov2018a, Foster2019}. Before we can precisely define Neyman-orthogonality we need to define the notion of a directional derivative, which allows us to consider differentiability in function spaces. 
\begin{definition}[Directional derivative \citep{Foster2019}] Let $\mathcal{F}$ be a vector space of functions. For a functional $F \colon \mathcal{F} \rightarrow \mathbb{R}$, we define the derivative operator $D_f F \left( f \right) \left[ h \right] = \frac{d}{dt} F \left( f + t h \right) \Bigr|_{t=0} $ for a pair of functions $f,h \in \mathcal{F}$. Likewise, we define $D_f^k F \left( f \right) \left[ h_1, \dots, h_k \right] = \frac{\partial^k}{\partial t_1 \dots \partial t_k} F \left( f + t_1 h_1 + \dots + t_k h_k \right)  \Bigr|_{t_1 = \dots = t_k = 0}$.
\end{definition}
Now we can define Neyman-orthogonality:
\begin{definition}[Neyman-orthogonal loss function \citep{Foster2019}] The population risk $\mathcal{L}$ is Neyman-orthogonal, if
\begin{align*}
D_\eta D_g \mathcal{L} \left( g^*, \eta_0 \right) \left[ g - g^*, \eta - \eta_0 \right] = 0 \text{ } \forall g \in \mathcal{G}, \forall \eta \in \mathcal{H}.
\end{align*}
\end{definition}
Note that the directional derivative with respect to the target parameter can be viewed as an estimating function, analogous to the score function in maximum likelihood estimation. Therefore, Neyman-orthogonality of a population risk function states that the estimating function obtained as a directional derivative with respect to the target parameter is locally insensitive to small perturbations of the nuisance parameter around its true value. 


\subsection{Orthogonal loss function based on the efficient influence function}  \label{subsect1}

To construct an observed data loss function that approximates  (\ref{eqn4}) and is Neyman-orthogonal, we leverage results on Neyman-orthogonality for scalar estimands from the literature on semiparametric and nonparametric inference \citep{Pfanzagl1990, Bickel1998, vanderLaan2003, Tsiatis2006, Kosorok2008, vanderLaan2011, Chernozhukov2018a}. As a first step we choose a finite-dimensional estimand, defined as the minimizer to (\ref{eqn4}) with $g(V)$ set to a constant $g$. This delivers the weighted average treatment effect (WATE) \citep{Hirano2003, Crump2006}
\begin{equation} \label{eqn10}
g \equiv \frac{ \mathbb{E} \left\lbrace \omega \left( X \right) \left( Y^1 - Y^0 \right) \right\rbrace }{ \mathbb{E} \left\lbrace \omega \left( X \right)  \right\rbrace },
\end{equation}
where $\omega \left( X \right) \equiv \lambda \left\lbrace \pi \left( X \right) \right\rbrace$ with $\lambda \colon \left[ 0, 1 \right] \rightarrow \mathbb{R}$ known function and the propensity score unknown.  Note that the average treatment effect is a special case with $\lambda \left( \cdot \right) \equiv 1$. More generally, we thus obtain a class of candidate estimands indexed by weight function $\omega \left( \cdot \right) = \lambda \left\lbrace \pi \left( \cdot \right) \right\rbrace $. 
\\
\indent
The key building block in the construction of Neyman-orthogonal loss functions for CATE estimation is the efficient influence function (EIF) under the nonparametric model for the chosen finite-dimensional estimand. The efficient influence function under the nonparametric model for the WATE \citep{Hirano2003, Crump2006} is given by:
\begin{align} \label{eqn11}
\nonumber
\phi \left( Z; \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) &= \frac{ \lambda \left\lbrace \pi \left( X \right) \right\rbrace }{ \mathbb{E} \left[ \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] } \left( \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right. \\
&\left. + \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \frac{ \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace }{\lambda \left\lbrace \pi \left( X \right) \right\rbrace} + 1 \right] \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - g \right\rbrace \right)
\end{align}
where $\eta \equiv \left( \pi \left( X \right), Q^{\left( 0 \right)} \left( X \right), Q^{\left( 1 \right)} \left( X \right) \right)$ is the nuisance parameter. If $\left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \neq 0$, then we can rewrite (\ref{eqn11}) as follows: 
\begin{align*}
&\phi \left( Z; \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) = \frac{ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace}{ \mathbb{E} \left[ \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] } \left\lbrace \varphi \left( Z; \eta,  \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) - g \right\rbrace
\end{align*}
where
\begin{align} \label{eqn24}
&\varphi \left( Z; \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) \equiv \frac{\lambda \left\lbrace \pi \left( X \right) \right\rbrace}{ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace} \\
\nonumber
&\times \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right] + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right). 
\end{align} 
We consider an example of a weight function $\lambda \left\lbrace \pi \left( \cdot \right) \right\rbrace$, such that $\left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace = 0$ in Section \ref{subsect2}.
\\
\indent
As known from semiparametric theory \citep{Pfanzagl1990, Bickel1998, vanderLaan2003, Tsiatis2006, Kosorok2008, vanderLaan2011, Chernozhukov2018a}, the EIF is Neyman-orthogonal in the sense that its directional derivatives along paths that perturb the nuisance parameters have mean zero. This makes it an attractive candidate estimating function for $g$. In particular, a population risk function whose derivative with respect to $g$ equals the EIF - and which is therefore itself Neyman-orthogonal - is given by
\begin{align*}
&\mathcal{L} \left( g, \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) = \\
&\frac{1}{\mathbb{E} \left[ \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right]} \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - g \right\rbrace^2 \right).
\end{align*}
We can now generalize this to a population risk function for CATE estimation by allowing $g$ to depend on $V$:
\begin{align} \label{eqn5}
\nonumber
&\mathcal{L} \left( g, \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) = \\
&\frac{1}{\mathbb{E} \left[ \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right]} \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - g \left( V \right) \right\rbrace^2 \right).
\end{align}
This leads to the following result:
\begin{lemma}  \label{lemma4} 
Population risk function (\ref{eqn5}) is Neyman-orthogonal for each choice of weight function $\lambda \left( \cdot \right)$.
\end{lemma}
Importantly, the above construction of a Neyman-orthogonal risk function did not change the minimization problem, in the following sense.
\begin{lemma} \label{lemma3}
Population risk function (\ref{eqn5}) has the same minimizer $g^*$ over the function space $\mathcal{G}$ as the population risk function
\begin{align} \label{eqn12}
\mathcal{L} \left( g, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) = \mathbb{E} \left[ \lambda \left\lbrace \pi \left( X \right) \right\rbrace \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right].
\end{align}
\end{lemma}
Combining Lemma \ref{lemma2} and Lemma \ref{lemma3} we obtain the following result:
\begin{corollary} \label{corollary1}
Assume that $V = X$ and $\mathbb{E} \left( Y^1 - Y^0 \cond X \right) \in \mathcal{G}$. Then CATE is the minimizer of population risk function (\ref{eqn5}) for each choice of the weight function $\lambda \left( \cdot \right)$. 
\end{corollary}
Proofs of Lemma \ref{lemma4} and Lemma \ref{lemma3} can be found in the Appendix \ref{AppA}. 

\subsection{Examples of orthogonal CATE learners}

Below we present some of the recently developed methods for CATE estimation, e.g. DR-Learner \citep{Kennedy2020a} and R-Learner \citep{Nie2021}, in light of the framework described above, and we introduce a novel learner. 

\subsubsection{DR-Learner}

The constant weight function $\lambda \left\lbrace \pi \left( X \right) \right\rbrace \equiv 1$ gives rise to the following population risk function:
\begin{align} \label{eqn6}
\nonumber
&\mathcal{L} \left( g, \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) = \mathbb{E} \left[ \left\lbrace \varphi \left( Z; \eta, 1 \right) - g \left( V \right) \right\rbrace^2 \right] \\
&= \mathbb{E} \left( \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - g \left( V \right) \right]^2 \right).
\end{align}
The CATE estimation approach aiming to minimize the population risk function (\ref{eqn6}) is known as DR-Learner \citep{Kennedy2020a}. From Lemma \ref{lemma3} we can see that minimizing the population risk function (\ref{eqn6}) leads to the same minimizer as obtained through minimization of the population risk function
\begin{equation*}
\mathcal{L} \left( g, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) = \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right].
\end{equation*}
This means that DR-Learner finds a function $g \in \mathcal{G}$ that is the closest to the treatment effect $Y^1 - Y^0$ in the whole population in terms of mean-squared error. 

\subsubsection{Propensity-score-weighted DR-Learner} \label{subsect2}

The weight function $\lambda \left\lbrace \pi \left( X \right) \right\rbrace = \pi \left( X \right)$ yields $\left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace = A$, which is non-zero in the treated only (i.e. individuals with $A = 1$). In this case, population risk function (\ref{eqn5}) equals
\begin{align} \label{eqn13}
&\mathcal{L} \left( g, \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) = \frac{1}{\mathbb{E} \left\lbrace \pi \left( X \right) \right\rbrace} \mathbb{E} \left[ A \left\lbrace \varphi \left( Z; \eta, \pi \left( X \right) \right) - g \left( V \right) \right\rbrace^2 \right] \\
\nonumber
&= \frac{1}{\mathbb{E} \left\lbrace \pi \left( X \right) \right\rbrace} \mathbb{E} \left\lbrace A \left(\frac{ \pi \left( X \right) }{  A } \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right] \right.\right. \\
\nonumber
&\left.\left. + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - g \left( V \right) \right)^2 \right\rbrace \\
\nonumber
&= \frac{1}{\mathbb{E} \left\lbrace \pi \left( X \right) \right\rbrace} \mathbb{E} \left( A \left[ \frac{A - \pi \left( X \right) }{ \left\lbrace 1 - \pi \left( X \right) \right\rbrace A} \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace - g \left( V \right) \right]^2 \right).
\end{align}
which restricts minimization to the population of treated \citep{Hirano2003}. We refer to the approach aiming to minimize the population risk function as propensity-score-weighted DR-Learner. When minimization is over an unrestricted function class, then it targets the conditional average treatment effect in the treated:
\begin{align*}
\frac{\mathbb{E} \left\lbrace \pi \left( X \right) \left( Y^1 - Y^0 \right) \cond V \right\rbrace}{\mathbb{E} \left\lbrace \pi \left( X \right) \cond V \right\rbrace} &= \frac{ \mathbb{E} \left[ \mathbb{E} \left\lbrace \pi \left( X \right) \left( Y^1 - Y^0 \right) \cond X \right\rbrace \cond V \right]}{\mathbb{E} \left\lbrace \pi \left( X \right) \cond V \right\rbrace} \\
&= \frac{ \mathbb{E} \left\lbrace \mathbb{E} \left( A \cond X \right) \mathbb{E} \left( Y^1 - Y^0 \cond X \right) \cond V \right\rbrace}{\mathbb{E} \left\lbrace \mathbb{E} \left( A \cond X \right) \cond V \right\rbrace} \\
&= \frac{ \mathbb{E} \left[ \mathbb{E} \left\lbrace A \left( Y^1 - Y^0 \right) \cond X \right\rbrace \cond V \right]}{ \mathbb{E} \left( A \cond V \right) } \\
&= \frac{ \mathbb{E} \left\lbrace A \left( Y^1 - Y^0 \right) \cond V \right\rbrace}{\mathbb{E} \left( A \cond V \right)} \\
&= \frac{ \mathbb{E} \left( Y^1 - Y^0 \cond V, A = 1 \right) \mathbb{P} \left( A = 1 \cond V \right)}{\mathbb{P} \left( A = 1 \cond V \right)} \\
&= \mathbb{E} \left( Y^1 - Y^0 \cond V, A = 1 \right).
\end{align*} 
From Lemma \ref{lemma3}, we moreover see that minimizing the population risk function (\ref{eqn13}) leads to the same minimizer as obtained through minimization of population risk function
\begin{align*}
\mathcal{L} \left( g, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) &= \mathbb{E} \left[ \pi \left( X \right) \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right] \\
&= \mathbb{E} \left[ A \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right].
\end{align*}
This means that the propensity-score-weighted DR-Learner finds a function $g \in \mathcal{G}$ that is closest to the treatment effect $Y^1 - Y^0$ in the treated population in terms of mean-squared error. 
\\
\indent
By aiming to minimize prediction error in the treated, the propensity-score-weighted DR-Learner is less relevant for providing treatment decision support in prospective settings where treatment decisions must be made for new patients. It is primarily indicated in retrospective settings, where one wishes to quantify treatment effect heterogeneity in settings where some patients are ineligible for treatment, as in the previously discussed critical care example, or in exposure prevention studies. For instance, studies on the impact of hospital-acquired infections are primarily interested in the impact of preventing infection in those who acquired it \citep{Vansteelandt2009}. 

\subsubsection{R-Learner}

For the weights $\lambda \left\lbrace \pi \left( X \right) \right\rbrace \equiv \pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace$, population risk function (\ref{eqn5}) specializes to 
\begin{align} \label{eqn14} 
\nonumber
&\mathcal{L} \left( g, \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) \\
\nonumber
&= \frac{1}{\mathbb{E} \left[ \pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace \right]} \mathbb{E} \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace^2 \left\lbrace \varphi \left( Z; \eta, \pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace \right) - g \left( V \right) \right\rbrace^2 \right] \\
\nonumber
&= \frac{1}{\mathbb{E} \left\lbrace \pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace \right]} \mathbb{E} \left\lbrace \left\lbrace A - \pi \left( X \right) \right\rbrace^2 \left( \frac{\pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace}{ \left\lbrace A - \pi \left( X \right) \right\rbrace^2} \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace \right.\right.\right. \\
\nonumber
&\left.\left.\left. - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right] + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - g \left( V \right) \right)^2 \right\rbrace \\
\nonumber
&= \frac{1}{\mathbb{E} \left\lbrace \pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace \right]} \times \\
\nonumber
&\mathbb{E} \left\lbrace \left\lbrace A - \pi \left( X \right) \right\rbrace^2 \left( \frac{1}{A - \pi \left( X \right)} \left[ Y - \pi \left( X \right) Q^{\left( 1 \right)} \left( X \right) - \left\lbrace 1 - \pi \left( X \right) \right\rbrace Q^{\left( 0 \right)} \left( X \right) \right] - g \left( V \right) \right)^2 \right\rbrace \\
\nonumber
&= \frac{1}{\mathbb{E} \left\lbrace \pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace \right]} \times \\
\nonumber
&\mathbb{E} \left\lbrace \left( \left[ Y - \pi \left( X \right) Q^{\left( 1 \right)} \left( X \right) - \left\lbrace 1 - \pi \left( X \right) \right\rbrace Q^{\left( 0 \right)} \left( X \right) \right] -  \left\lbrace A - \pi \left( X \right) \right\rbrace g \left( V \right) \right)^2 \right\rbrace \\
&= \frac{1}{\mathbb{E} \left\lbrace \pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace \right]} \mathbb{E} \left( \left[ \left\lbrace Y - Q \left( X \right) \right\rbrace -  \left\lbrace A - \pi \left( X \right) \right\rbrace g \left( V \right) \right]^2 \right),
\end{align}
where $Q \left( X \right) \equiv \mathbb{E} \left( Y \cond X \right)$ is the outcome prediction and in the last step we have used the identity $Q \left( X \right) = \pi \left( X \right) Q^{\left( 1 \right)} \left( X \right) + \left\lbrace 1 - \pi \left( X \right) \right\rbrace Q^{\left( 0 \right)} \left( X \right)$. This approach is known as R-Learner \citep{Nie2021}. It follows from the above that R-Learner can be viewed as a propensity-overlap-weighted DR-Learner, and thus that DR-Learner and R-Learner are special cases of the retargeting framework presented above.
\\
\indent
From Lemma \ref{lemma3} we can see that minimizing the population risk function (\ref{eqn14}) leads to the same minimizer as obtained through minimization of the population risk function
\begin{equation*}
\mathcal{L} \left( g,  \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) = \mathbb{E} \left[ \pi \left( X \right) \left\lbrace 1 - \pi \left( X \right) \right\rbrace \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right].
\end{equation*}
It follows that in cases when $\tau \left( V \right) \notin \mathcal{G}$, i.e. when CATE does not belong to the class of functions over which we are minimizing the risk function, R-Learner does not target the CATE, but the propensity-overlap-weighted CATE instead. It thus targets the optimization to the patient population with the most uncertainty about the treatment decision. Arguably, this is also patient population for which clinicians would be the most interested in the treatment effect estimates to guide their decisions whether or not to initiate the treatment. As argued in \cite{Li2018} the propensity-overlap-weighted population represents a population of patients who could receive either of the treatment options with substantial probability and as such remain in clinical equipoise. For such patients the clinical consensus regarding the treatment option had not been met yet, and therefore arguably such a population is of particular interest from the research point of view. 

\subsection{Estimation of the CATE} \label{subsect3}

Based on the obtained population risk function (\ref{eqn5}) we estimate CATE using the following two-step procedure relying on sample-splitting into two parts: A and B \citep{Kennedy2020a, Nie2021, Foster2019}. Divide the data evenly into $K$ folds. Commonly, $K$ is equal $2$, $5$ or $10$. Combine $K-1$ folds into "part A" of the data and the remaining fold will be "part B" of the data. In the first step, we estimate the nuisance parameters $\eta \equiv \left( \pi \left( X \right), Q^{\left( 0 \right)} \left( X \right), Q^{\left( 1 \right)} \left( X \right) \right)$ on the part A of the data using flexible data-adaptive (e.g. machine learning) methods. In the second step we regress the pseudo-outcome (\ref{eqn24}) with plug-in estimates for the nuisance parameters obtained in the first step on the covariates $V$ again using data-adaptive methods with weights $\left\lbrace A_i - \hat{\pi} \left( X_i \right) \right\rbrace \lambda^\prime \left\lbrace \hat{\pi} \left( X_i \right) \right\rbrace + \lambda \left\lbrace \hat{\pi} \left( X_i \right) \right\rbrace$ on the part B of the data. This corresponds to the following plug-in empirical risk minimization
\begin{align} \label{eqn26}
&\frac{ \sum_{i=1}^n \left[ \left\lbrace A_i - \hat{\pi} \left( X_i \right) \right\rbrace \lambda^\prime \left\lbrace \hat{\pi} \left( X_i \right) \right\rbrace + \lambda \left\lbrace \hat{\pi} \left( X_i \right) \right\rbrace \right] \left\lbrace \varphi \left( Z_i; \hat{\eta}, \lambda \left( \hat{\pi} \right) \right) - g \left( V_i \right) \right\rbrace^2}{\sum_{i=1}^n \lambda \left\lbrace \hat{\pi} \left( X_i \right) \right\rbrace} + \Lambda \left\lbrace g \left( \cdot \right) \right\rbrace.
\end{align}
Here, similarly as in (\ref{eqn3}), $\Lambda \left\lbrace g \left( \cdot \right) \right\rbrace$ is a penalization term that usually will be present when employing a machine learning algorithm to avoid that the resulting CATE estimate is an overly "complex" function. 
\\
\indent
Sample-splitting may lead to loss of efficiency as now we are using only one part of the data (part A) to estimate nuisance parameters and part of the data (part B) to estimate the parameter of interest, i.e. CATE. For the purpose of making the most efficient use of the available data one may apply the idea of cross-fitting \citep{Chernozhukov2018a, Kennedy2020a}. This means that one performs the procedure described above $K$ times, permuting the roles of the folds, i.e. each time different fold will be treated as "part B" of the data.

\subsection{Error bounds} 

In this section, we will derive error bounds for the previously discussed learners. Our main result relies on the following definition. 
\begin{definition}[Star hull \citep{Foster2019}]
Let $\mathcal{X}$ be a vector space. For $x \in \mathcal{X}$, the star hull is defined as $\text{star} \left( \mathcal{X}, x \right) \equiv \left\lbrace t x + \left( 1 - t\right) x^\prime \cond x^\prime \in \mathcal{X}, t \in \left[ 0, 1 \right] \right\rbrace$.
\end{definition}
By Neyman-orthogonality we have the following favourable error bounds for the CATE estimator $\hat{g}$ obtained via minimization of the weighted sample risk function $\hat{\mathcal{L}} \left( g, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right)$ and $g^*$ obtained via minimization of the weighted population risk function $\mathcal{L} \left( g, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right)$ evaluated at the true values of the nuisance parameters.
\begin{theorem} \label{theorem1}
Let $R_g \equiv \mathcal{L} \left( \hat{g}, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) - \mathcal{L} \left( g^*,  \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right)$. For the estimator $\hat{g}$ obtained through minimization of the weighted sample risk function $\hat{\mathcal{L}} \left( g, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right)$ and $g^*$ being a minimizer of the weighted population risk function $\mathcal{L} \left( g, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right)$ we have the following error bound:
\begin{align} \label{eqn7}
\norm{\hat{g} - g^*}_\mathcal{G}^2 &\leq \frac{1}{\alpha/2 - \delta_1 - \delta_2 - \delta_3} \left( R_g + \frac{1}{\delta_1} \mathbb{E} \left[ C_1 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^4 \right] \right.  \\
\nonumber
&+ \frac{1}{\delta_2}  \mathbb{E} \left[ C_2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace^2 \right] \\
\nonumber
&\left. + \frac{1}{\delta_3} \mathbb{E} \left[ C_3 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace^2 \right] \right)
\end{align}
where $\norm{ g }_\mathcal{G} \equiv \left[ \mathbb{E} \left\lbrace \lvert g \left( V \right) \rvert^2 \right\rbrace \right]^{1/2}$ is the $\mathcal{L}^2$-norm, $\delta_i > 0$, for $i \in \left\lbrace 1,2,3 \right\rbrace$, such that $\delta_1 + \delta_2 + \delta_3 < \alpha/2$. Furthermore, $\alpha \geq 0$ is such that for all $g \in \mathcal{G}$ and $\eta = \left( \pi \left( X \right), Q^{\left( 0 \right)} \left( X \right), Q^{\left( 1 \right)} \left( X \right) \right) \in \mathcal{H}$:
\begin{equation} \label{eqn8}
\frac{\mathbb{E} \left( \left[\left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace^2 \right)}{ \mathbb{E} \left[ \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace^2 \right]} \geq \frac{\alpha}{2}
\end{equation}
and $C_1$, $C_2$ and $C_3$ are given by:
\begin{align*}
C_1 &\equiv \left( \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi}^3 \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{ 1-A }{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^3} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace \right] \right. \\
\nonumber
&- \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi}^2 \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace + \frac{1-A}{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^2} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace \right] \\
\nonumber
&+ \frac{1}{2} \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi} \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1-A}{1-\overline{\pi} \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace \right.\\
&\left.- \overline{Q}^{\left( 1 \right)} \left( X \right) + \overline{Q}^{\left( 0 \right)} \left( X \right) + g^* \left( V \right)\right] \\
\nonumber
&+\left. \frac{1}{2} \lambda^{\prime\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \left\lbrace \overline{Q}^{\left( 1 \right)} \left( X \right) - \overline{Q}^{\left( 0 \right)} \left( X \right) - g^* \left( V \right) \right\rbrace \right)^2
\end{align*}
\begin{align*}
C_2 \equiv \left[ \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{A}{\overline{\pi}^2 \left( X \right)} - \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{A}{\overline{\pi} \left( X \right)} + \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right]^2
\end{align*}
\begin{align*}
C_3 \equiv \left[ \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{ 1-A }{ \left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^2 } + \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{1-A}{1-\overline{\pi} \left( X \right)} - \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right]^2,
\end{align*}
where $\overline{\eta} = \left( \overline{\pi} \left( X \right),  \overline{Q}^{\left( 0 \right)} \left( X \right), \overline{Q}^{\left( 1 \right)} \left( X \right) \right) \in \text{star} \left( \mathcal{H}, \eta_0 \right)$.
\end{theorem}
The proof follows the ideas outlined in \cite{Foster2019} and uses the fact that population risk function (\ref{eqn6}) is Neyman-orthogonal, see Appendix \ref{AppA} for details. The result of Theorem \ref{theorem1} tells us that the error due to nuisance parameter estimation is of higher-order, therefore we are able to recover quasi-oracle error bound for the target parameter.  
\begin{remark}
Sample-splitting is a crucial step in our procedure of estimating CATE and has been previously commonly used in the CATE estimation literature \citep{Kennedy2020a, Nie2021, Foster2019}. Sample-splitting is needed to be able to invoke the generalization error bounds for the machine learning algorithm used for the plug-in empirical risk minimization (\ref{eqn26}), since it requires the data to be independent and identically distributed (i.i.d.). Fitting the nuisance parameters on the same sample as used for minimizing (\ref{eqn26}) would result in minimizing (\ref{eqn26}) over a correlated sample (through the plug-in estimates of the nuisance parameters) and hence would lead to violation of the i.i.d. assumption.
\end{remark}

\subsubsection{DR-Learner}

In this case, the constants $C_1$, $C_2$ and $C_3$ in the error bound (\ref{eqn7}) from Theorem \ref{theorem1} specialize to
\begin{align*}
C_1 &= \left[ \frac{A}{\overline{\pi}^3 \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{ \left(1-A \right)}{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^3} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace \right]^2 \\
C_2 &= \frac{A}{\overline{\pi}^4 \left( X \right)} \\
C_3 &= \frac{A}{\overline{\pi}^4 \left( X \right)} 
\end{align*}
and condition (\ref{eqn8}) specializes to $\alpha \leq 2$ for all $g \in \mathcal{G}$. Importantly, the error bound for DR-Learner depends on the propensity score via the denominator. In the cases when the propensity score is equal $0$ or $1$ the estimation of the nuisance parameter may therefore strongly impact the performance of the DR-Learner. For instance, in the empirical example from Section \ref{Section4} we consider RRT initiation in the AKI population at ICU. However, for some of the patients, namely those with propensity score close to $0$, clinicians would not even consider RRT initiation. In this case, the error bound for the DR-Learner is large or even infinite. This can be remedied through the use of different weight functions (see below).  
\\
\indent
To gain further insight into the oracle bound, let the rate of convergence of $\hat{g}$ to $g_0$ in the root-mean-squared error sense, i.e. $R_g$, be $n^{-\varphi_g}$ with $\varphi_g \leq 1$. Likewise, let $n^{-\varphi_\pi}$, $n^{-\varphi_{Q^{\left( 0 \right)}}}$ and $n^{-\varphi_{Q^{\left( 1 \right)}}}$ be the rates of convergence for the nuisance parameters $\pi$, $Q^{\left( 0 \right)}$ and $Q^{\left( 1 \right)}$, respectively. In order to have oracle behaviour of the learner, the estimation errors for the nuisance parameters must be of higher order than the estimation order for the target parameter. In that case, $n^{-\varphi_g} = \max \left\lbrace n^{-\varphi_g},  n^{-\varphi_1}, n^{-\varphi_2}, n^{-\varphi_3} \right\rbrace$, where $n^{-\varphi_i}$ is upper-bounding the rate of convergence of the $C_i$-term in the error bound (\ref{eqn7}) for $i \in \left\lbrace 1, 2, 3 \right\rbrace$.  Furthermore, assuming that $\overline{\pi} \left( X \right)$ is bounded away from $0$ and $1$, we have the following upper-bounds for each of the terms: $n^{-4 \varphi_\pi}$ for the first term containing $C_1$, $n^{- 2 \varphi_\pi - 2 \varphi_{Q^{\left( 1 \right)}}}$ for the second term containing $C_2$ and $n^{- 2 \varphi_\pi - 2 \varphi_{Q^{\left( 0 \right)}}}$ for the third term containing $C_3$. Therefore, the oracle behaviour condition translates for the DR-Learner to 
\begin{equation} \label{eqn25}
n^{-\varphi_g} = \max \left\lbrace n^{-\varphi_g},  n^{-4 \varphi_\pi}, n^{- 2 \varphi_\pi - 2 \varphi_{Q^{\left( 1 \right)}}}, n^{- 2 \varphi_\pi - 2 \varphi_{Q^{\left( 0 \right)}}} \right\rbrace.
\end{equation}
In particular, when restricting the class of functions $\mathcal{G}$ to parametric functions, we may expect a parametric rate for the target parameter $g$, i.e. $n^{-\varphi_g} = n^{-1}$, when the nuisance parameters are known. To attain oracle behaviour, we then need the rate of convergence of the propensity score to equal $\varphi_\pi \geq 1/4$.  Additionally, the rates of convergence for the outcome prediction models can be slower than $n^{-1/4}$ as long as $\varphi_\pi + \varphi_{Q^{\left( i \right)}} \geq 1/2$ for $i \in \left\lbrace 0, 1 \right\rbrace$. When $g$ is instead obtained using non-parametric or data-adaptive methods, and hence its rate of convergence is slower than parametric, then also the propensity score and the conditional mean outcomes can have slower rates of convergence as long as (\ref{eqn25}) is satisfied.

\subsubsection{Propensity-score-weighted DR-Learner}

For the propensity-score-weighted DR-Learner, the constants $C_1$, $C_2$ and $C_3$ in the error bound (\ref{eqn7}) from Theorem \ref{theorem1} specialize to 
\begin{align*}
    C_1 &= \left[ \frac{ \left(1-A \right) \left\lbrace 1-2\overline{\pi} \left( X \right) \right\rbrace \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace }{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^3}  \right]^2 \\
    C_2 &= 0 \\
    C_3 &= \frac{1-A}{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^4}
\end{align*}
and condition (\ref{eqn8}) specializes to 
\begin{equation*}
\frac{\mathbb{E} \left[ A \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace^2 \right]}{ \mathbb{E} \left[ \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace^2 \right]} \geq \frac{\alpha}{2} 
\end{equation*}
for all $g \in \mathcal{G}$. The error bound does not depend on the estimation of $Q^{\left( 1 \right)} \left( X \right)$.  This is because the population risk function (\ref{eqn5}) for $\omega \left( X \right) = \pi \left( X \right)$ does not depend on $Q^{\left( 1 \right)} \left( X \right)$. Furthermore, the error bound for the propensity-score-weighted DR-Learner does not depend on $\pi \left( X \right)$ via the denominator, but only on $1-\pi \left( X \right)$.  In this case individuals with propensity score equal $0$ are not problematic and only the ones with propensity score equal $1$ would lead to an infinite error bound. More generally, the oracle behaviour condition now reduces to 
\begin{equation*} 
n^{-\varphi_g} = \max \left\lbrace n^{-\varphi_g},  n^{-4 \varphi_\pi}, n^{- 2 \varphi_\pi - 2 \varphi_{Q^{\left( 0 \right)}}} \right\rbrace.
\end{equation*}

\subsubsection{R-Learner}

In this case the constants $C_1$, $C_2$ and $C_3$ in the error bound (\ref{eqn7}) from Theorem \ref{theorem1} specialize to 
\begin{align*}
    C_1 &= \left\lbrace \overline{Q}^{\left( 1 \right)} \left( X \right) - \overline{Q}^{\left( 0 \right)} \left( X \right) - g^* \left( V \right) \right\rbrace^2 \\
    C_2 &= \left\lbrace A - 2\overline{\pi} \left( X \right) \right\rbrace^2 \\
    C_3 &= \left\lbrace 1 - 2 \overline{\pi} \left( X \right) + A \right\rbrace^2.
\end{align*}
and condition (\ref{eqn8}) specializes to 
\begin{equation*}
\frac{\mathbb{E} \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace^2 \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace^2 \right]}{ \mathbb{E} \left[ \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace^2 \right]} \geq \frac{\alpha}{2} 
\end{equation*}
for all $g \in \mathcal{G}$. Importantly, the error bound does not depend on the propensity score via the denominator, therefore values of the propensity score equal to $0$ and $1$ do not lead to an infinite error bound for the R-Learner. The oracle behaviour condition for the R-Learner now equals
\begin{equation*} 
n^{-\varphi_g} = \max \left\lbrace n^{-\varphi_g},  n^{-4 \varphi_\pi}, n^{- 2 \varphi_\pi - 2 \varphi_{Q^{\left( 1 \right)}}}, n^{- 2 \varphi_\pi - 2 \varphi_{Q^{\left( 0 \right)}}} \right\rbrace.
\end{equation*}

\section{Empirical Example} \label{Section4}

\subsection{Data}

The ICIS (Intensive Care Information System) of the Ghent University Hospital ICUs is a database containing routinely collected medical information from all adult patients admitted to the intensive care unit since $2013$. We have focused our analysis on $3728$ adult stage $2$ and $3$ AKI patients (based on the KDIGO -- Kidney Disease: Improving Global Outcomes -- criteria \citep{KDIGO2012}) admitted to the ICU between $1/1/2013$ and $31/12/2017$, who had no recorded RRT history and no RRT restrictions by the time of the inclusion at stage $2$ AKI diagnosis. 
\\
\indent
We have information on several patient characteristic, i.e. ICU admission time, ICU discharge time, vital status at discharge, timestamps of all dialysis sessions during each ICU episode, baseline covariates (e.g. age, weight, gender, admission category \{"No surgery", "Planned surgery", "Emergency surgery"\}, indicator whether patient has received dialysis prior to current ICU admission, indicator whether patient had chronic kidney disease diagnosis prior to current ICU admission) and longitudinal measurements (e.g. SOFA scores, indicator whether KDIGO AKI (stage $1/2/3$) creatinine condition has been reached during ICU episode, indicator whether KDIGO AKI (stage $1/2/3$) oliguric condition has been reached during ICU episode, indicator whether patient has received diuretics during ICU episode, cumulative total fluid intake, cumulative total fluid output, arterial pH values, serum potassium values (in mmol/L), serum ureum values (in mg/dL), serum magnesium values (in mmol/L), fraction of inspired oxygen (FiO$_2$), peripheral oxygen saturation (SpO$_2$), arterial oxygen concentration (PaO$_2$), ratio of arterial oxygen concentration to the fraction of inspired oxygen (P/F ratio), DNR ("Do Not Resuscitate") code) and their timestamps. Table \ref{table1} presents an overview of the overall observed patient population, observed treated patient population, observed patient population weighted by the inverse-probability weights and observed patient population weighted by the propensity-overlap weights. In the observed population weighted by the inverse-probability weights each individual obtains a weight given by
\begin{equation*}
    \frac{A_i}{\pi \left( X_i \right)} + \frac{1 - A_i}{1 - \pi \left( X_i \right)},
\end{equation*}
whereas in the observed population weighted by the propensity-overlap weights each individual receives a weight
\begin{equation*}
    \pi \left( X_i \right) \left\lbrace 1 - \pi \left( X_i\right) \right\rbrace \left\lbrace \frac{A_i}{\pi \left( X_i \right)} + \frac{1 - A_i}{1 - \pi \left( X_i \right)} \right\rbrace = A_i \left\lbrace 1 - \pi \left( X_i\right) \right\rbrace + \left( 1 - A_i \right) \pi \left( X_i \right).
\end{equation*}

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|c|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
 & \shortstack{\textbf{Observed} \\ \textbf{population}} & \shortstack{\textbf{Observed} \\ \textbf{treated} \\ \textbf{population}} & \shortstack{\textbf{IPW} \\ \textbf{population}} & \shortstack{\textbf{POW} \\ \textbf{population}} \\
 \hline
 \hline
\textbf{Age on admission} & $64.5 \pm 14.9$ & $63.4 \pm 13.5$ & $65.6 \pm 13.4$ & $64.1 \pm 14.5$  \\
\hline
\textbf{Weight} & $80.6 \pm 17.1$ & $77.1 \pm 13.9$ & $78.4 \pm 15.9$ & $77.1 \pm 15.6$ \\
\hline
\textbf{Gender} & \shortstack{$31.9 \%$ \\ of females} & \shortstack{$26.9 \%$ \\ of females} & \shortstack{$29.1 \%$ \\ of females} & \shortstack{$29.0 \%$ \\ of females} \\
\hline
\textbf{SOFA score} & $10.9 \pm 4.0$ & $13.7 \pm 4.7$ & $11.8 \pm 4.2$ & $13.2 \pm 4.4$ \\
\hline
\shortstack{\textbf{Time from ICU} \\ \textbf{admission to AKI} \\ \textbf{stage $2$ diagnosis (in h)}} & $33.4 \pm 68.3$ & $18.4 \pm 44.3$ & $32.5 \pm 69.2$ & $27.5 \pm 71.9$ \\
\hline
\hline
\end{tabular}
\end{center}
\caption{Overview of the patient population in the train set (mean and standard deviation). "IPW population" is the observed population weighted with the inverse-probability weights. "POW population" is the "IPW population" weighted with the propensity-overlap weights. } \label{table1}
\end{table}

\subsection{Results}

In our analysis we are interested in the effect on the $7$-day ICU mortality of initiating RRT within $24$h from the stage $2$ AKI diagnosis in the stage $\geq2$ AKI patient population. For this purpose, we apply several learners described in the previous sections, in particular the IPW-Learner,  the DR-Learner and the R-Learner. Furthermore, when focusing on the treatment effect in the population of the AKI patients that had received the RRT, we additionally compute the propensity-score-weighted DR-Learner.  
\\
\indent
As outlined in the Section \ref{subsect3}, implementation of each of the learners has been performed in two steps. In the first step, one estimates the nuisance parameters, i.e. the propensity score model and the conditional mean outcomes models. In the second step, one computes the pseudo-outcome given by (\ref{eqn24}) and regresses the pseudo-outcome on the subset of covariates $V$ using weighted regression, which corresponds to minimizing the empirical risk function (\ref{eqn26}) with plug-in estimates for nuisance parmeters. In our analysis, we are interested in estimating the effect of initiating RRT conditional on few relevant patient characteristics. In our case these are values of serum potassium, arterial pH, cumulative total fluid intake and cumulative total fluid output.
\\
\indent
To perform our analysis, we split the data into three equally sized parts: training data set A, training data set B and test data set. The test data set is put aside and used for computation of the final output, i.e. effect on risk difference scale of initiating RRT within 24h from the stage $2$ AKI diagnosis in the stage $\geq2$ AKI patient population on $7$-day ICU mortality conditional on patient's values of serum potassium, arterial pH, cumulative total fluid intake and cumulative total fluid output. To make the most efficient use of the available data we apply the idea of cross-fitting as described in the Section \ref{subsect3}. In the final step, given the model for the CATE estimation obtained via cross-fitting on the training data sets A and B, we use the test data set to obtain the final output, i.e. effect on risk difference scale of initiating RRT within 24h from the stage $2$ AKI diagnosis in the stage $\geq2$ AKI patient population on $7$-day ICU mortality. We evaluate the performance on a separate test set to avoid possible overoptimism in the reported performance, which could arise once evaluating the performance of different methods on the same data that has been used for training the models. Note, that for computing propensity-score-weighted DR-Learner we perform the above procedure restricting the data set to the population of treated patients to guarantee that the corresponding loss function is well-defined. All the models, i.e. propensity score model, outcome prediction model, and weighted regressions for all pseudo-outcomes have been computed using \texttt{SuperLearner} \citep{VanDerLaan2007} with the following list of wrappers: glm, glmnet, random forest (\texttt{ranger}) and xgboost.
\\
\indent
Figure \ref{figure1} presents the histogram of the effect on the risk difference scale of initiating RRT within 24h from the stage $2$ AKI diagnosis in the stage $\geq2$ AKI patient population on $7$-day ICU mortality. Panel A presents the histogram of the treatment effect conditional on the values of serum potassium, arterial pH, fluid intake and fluid output computed in the whole patient population using IPW-Learner ("IPW"), DR-Learner ("DR") and R-Learner ("R"). Dashed vertical lines indicate the average treatment effect. The mean average treatment effect together with the standard deviation were equal $0.16 \pm 0.01$ for the IPW-Learner, $0.21 \pm 0.13$ for the DR-Learner and $0.23 \pm 0.1$ for the R-Learner. Note, that negative values represent a beneficial effect of RRT initiation, i.e. lowering $7-$day ICU mortality, for a particular patient. The IPW-Learner estimates a harmful effect of RRT initiation at considered timepoint for $100\%$ of patients in the test set, DR-Learner for $95.3\%$ and R-Learner for $98.2\%$. This is comparable with the observed proportion of patients receiving RRT at that timepoint, which was $2.9\%$.
\\
\indent
Panel B presents a histogram of the effect on the risk difference scale of initiating RRT within $24$h from the stage $2$ AKI diagnosis in the stage $\geq2$ AKI patient population that received the RRT on $7$-day ICU mortality. Similarly as in the Panel A, the presented treatment effects are conditional on the values of serum potassium, arterial pH, fluid intake and fluid output. For the treatment effect in the treated, additionally to IPW-Learner, DR-Learner and R-Learner, we also applied the propensity-score-weighted DR-Learner ("psDR"). The mean average treatment effect in treated together with the standard deviation were equal $0.16 \pm 0.01$ for the IPW-Learner, $0.16 \pm 0.16$ for the DR-Learner, $0.15 \pm 0.1$ for the R-Learner and $0.15 \pm 0.18$ for the propensity-score-weighted DR-Learner. Note, that since in the empirical example we are interested in estimation of CATE conditional on a subset of available covariates we do not present results of T-Learner as it targets a different estimand, i.e. CATE conditional on all available covariates. 
\\
\indent
Figure \ref{figure2} presents the relationship between the treatment effects as presented in the Figure \ref{figure1} and the propensity scores for each patient from the test set. As in Figure \ref{figure1}, we present the results for the whole patient population in Panel A and for the treated patient population in Panel B. Dashed horizontal lines indicate the average treatment effect for each of the methods.
\\
\indent
It is important to note that IPW-Learner and DR-Learner are more prone to the occurrence of extreme weights than other methods, which may lead to instability of the results. In particular, in the case of the propensity scores being close to $0$ or $1$ it would be impossible to compute these learners. See Figure \ref{figure5} and Figure \ref{figure6} in the Appendix \ref{subsect5} for comparison of the inverse-probability and propensity-overlap weights for the empirical example.
\begin{landscape}
\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.97]{figure_hist.png}
\end{center}
\caption{Panel A presents the histogram of the effect on the risk difference scale of initiating RRT within $24$h from the stage $2$ AKI diagnosis in the stage $\geq2$ AKI patient population on $7$-day ICU mortality conditional on the values of serum potassium, arterial pH, fluid intake and fluid output computed in the whole patient population using IPW-Learner ("IPW"), DR-Learner ("DR") and R-Learner ("R"). Panel B presents a histogram of the effect on the risk difference scale of initiating RRT within $24$h from the stage $2$ AKI diagnosis in the stage $\geq2$ AKI patient population that received the RRT on $7$-day ICU mortality. Additionally to IPW-Learner, DR-Learner and R-Learner, we also present propensity-score-weighted DR-Learner ("psDR"). Dashed vertical lines indicate the average treatment effect for each of the methods.} \label{figure1}
\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.97]{figure_scatterplot.png}
\end{center}
\caption{Relationship between the effect on the risk difference scale of initiating RRT within 24h from the stage $2$ AKI diagnosis in the stage $\geq2$ AKI patient population on $7$-day ICU mortality and the propensity score of different patients. Similarly as in the the Figure \ref{figure1} in Panel A we present the results in the whole patient population and in Panel B we present the results in the treated patient population. Dashed horizontal lines indicate the average treatment effect for each of the methods.} \label{figure2}
\end{figure}
\end{landscape}

\section{Simulation Study} \label{Section5}

We perform a simulation study to assess the performance of the different learners described in the previous sections. We consider two simulation set-ups inspired by the simulations in \cite{Nie2021} and a simulation set-up from \cite{Belloni2017}. To perform our simulation study, similarly as in the case of the empirical example, we split the available data into three parts: training data set A, training data set B and a test data set, each of sample size $500$. A test data set is put aside and used for computation of the final output.  The results are computed using $1000$ simulations. For the purpose of making the most efficient use of the available data we apply the idea of cross-fitting. In what follows we consider three different performance metrics computed on the test set, i.e. mean-squared error, mean-squared error in the treated population and propensity-overlap-weighted mean-squared error, which is defined as
\begin{align*}
    MSE_{pow} = \frac{\sum_{i=1}^n \pi \left( X_i \right) \left\lbrace 1 - \pi \left( X_i \right) \right\rbrace \left\lbrace \hat{\tau} \left( X_i \right) - \tau \left( X_i \right) \right\rbrace^2}{\sum_{i=1}^n \pi \left( X_i \right) \left\lbrace 1 - \pi \left( X_i \right) \right\rbrace}.
\end{align*}
Note, that for computing propensity-score-weighted DR-Learner we perform the above procedure restricting the data set to the population of treated patients to guarantee that the corresponding loss function is well-defined. Furthermore, note that in the simulations we compute CATE conditional on the complete set of available covariates $X$. All the models, i.e. propensity score model, outcome prediction model, and weighted regressions for all pseudo-outcomes have been computed using \texttt{SuperLearner} with the following list of models: glm, glmnet, random forest (\texttt{ranger}) and xgboost. 
\\
\indent
Simulation set-ups $1$ and $2$ are based on the following data generating mechanism from \cite{Nie2021}: $x_i \sim P_d$, $a_i \vert x_i \sim \text{Bern} \left\lbrace \pi_0 \left( x_i \right) \right\rbrace$, $\delta_i \vert x_i \sim \mathcal{N} \left( 0, 1 \right)$, $y_i = b \left( x_i \right) + \left( a_i - 0.5 \right) \tau \left( x_i \right) + \sigma \delta_i$, where $b \left( \cdot \right)$ is baseline main effect and $P_d$ is a distribution of the covariates $x$ indexed by the dimension $d = 20$ and the noise level $\sigma = 0.5$. 
\\
\indent
The simulation set-up $1$ is inspired by the simulation set-up C in \cite{Nie2021}.  We use $b \left( x_i \right) = 2 \log \left\lbrace 1 + \exp \left( x_{i1} + x_{i2} + x_{i3} + x_{i4} + x_{i5} \right) \right\rbrace$, $x_i \sim \mathcal{N} \left( 0,  I_{d \times d} \right)$, where $I_{d \times d}$ is a $d-$dimensional identity matrix, $\pi \left( x_i \right) = 1 / \exp \left(1 + x_{i2} + X_{i3} + x_{i4} + x_{i5} \right)$ and $\tau \left( x_i \right) = 1$. In this set-up  the CATE is a simple function. The results are presented in Figure~\ref{figure3}. Panels A and B show the mean-squared error and the propensity-overlap-weighted mean-squared error computed on the test set using T-Learner, IPW-Learner, DR-Learner and R-Learner. Panel C presents the mean-squared error in the treated computed using the propensity-score-weighted DR-Learner, additionally to previously mentioned methods. Panels D, E and F present the same results as the corresponding plots above, however with restricted y-axis for a better interpretability of the results. The IPW-Learner performs much worse than other methods, whereas R-Learner is the best performing among considered methods. Furthermore, DR-Learner and R-Learner outperform T-Learner. This could be expected since CATE is a simple function in this set-up, therefore the methods that directly aim at CATE estimation have advantage over T-Learner that models two outcomes separately (as discussed in Section \ref{subsect4}). Furthermore, the outcome generating process is fairly complex in this set-up, which may negatively impact performance of T-Learner. The propensity-score-weighted DR-Learner performs worse than the T-Learner, DR-Learner and R-Learner, which may be related to the fact that it is trained on a much smaller population of treated patients. 
\\
\indent
The simulation set-up $2$ is inspired by the simulation set-up A in \cite{Nie2021}.  We use $b \left( x_i \right) = \sin \left( \pi x_{i1} x_{i2} \right) + 2 \left( x_{i3} - 0.5 \right)^2 + x_{i4} + 0.5 x_{i5} + x_{i6}$, $x_i \sim \mathcal{U} \left( 0,1 \right)^d$, $\pi \left( x_i \right) = \max \left[ \alpha,  \min \left\lbrace \sin \left( \pi x_{i1} x_{i2} x_{i3} x_{i4} \right), 1 - \alpha \right\rbrace \right]$ with $\alpha = 0.1$ and $\tau \left( x_i \right) = \left( x_{i1} + x_{i2} + x_{i3} \right)/2$. The results are presented in Figure~\ref{figure4}. Description of the figure is analogous to that of Figure~\ref{figure3}. Also in this set-up, the IPW-Learner performs much worse than other methods. The remaining methods perform similarly with R-Learner performing slightly better than other methods for each of the considered metrics. Interestingly, DR-Learner performs worse than T-Learner in this set-up and has very variable performance. This shows that DR-Learner suffers a lot, when the propensity score is difficult to estimate as in this simulation set-up.  
\\
\indent
The simulation set-up $3$ uses the data-generating mechanism from \cite{Belloni2017}. It is defined as 
\begin{align*}
    a_i &= \mathbbm{1} \left\lbrace \frac{\exp \left( c_d x_i^\prime \beta\right)}{1 + \exp \left( c_d x_i^\prime \beta\right)} > v_i \right\rbrace, \text{ } v_i \sim \mathcal{U} \left( 0,1 \right) \\
    y_i &= \theta a_i + c_y x_i^\prime \beta a_i + \zeta_i, \text{ } \zeta_i \sim \mathcal{N} \left( 0,1 \right),
\end{align*}
such that $x_i \sim \mathcal{N} \left( 0, \Sigma \right)$ where $\Sigma$ is a matrix with entries $\Sigma_{kj} = 0.5^{\lvert k-j \rvert}$. Furthermore, $\beta$ is a $d$-dimensional vector with entries $\beta_j = \frac{1}{j^2}$ and constants $c_y$ and $c_d$ are given by $c_y =\sqrt{\frac{R_y^2}{\left( 1-R_y^2 \right) \beta^\prime \Sigma \beta}}$ and $c_d =\sqrt{\frac{\left( \pi^2/3 \right) R_d^2}{\left( 1-R_d^2 \right) \beta^\prime \Sigma \beta}}$ with $R_y^2 = 0.5$ and $R_d^2 = 0.5$. The results are presented in Figure~\ref{figure4b}. Description of the figure is analogous to that of Figure~\ref{figure3}. In this set-up, T-Learner performs the worst and R-Learner performs the best among the considered methods. Interestingly, DR-Learner has even slightly worse performance than IPW-Learner in this setting. 
\\
\indent
Figure \ref{figure4a} presents the inverse-probability weights from three considered simulation set-ups.

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=0.57]{C_1000sim_n500_1000_500.png}
\end{center}
\caption{Results of the simulation set-up $1$. Panels A and B show the mean-squared error and the propensity-overlap-weighted mean-squared error computed on the test set in the second simulation setting using T-Learner ("T"), IPW-Learner ("IPW"), DR-Learner ("DR") and R-Learner ("R"). Panel C presents the mean-squared error in the treated computed using the propensity-score-weighted DR-Learner ("psDR"), additionally to previously mentioned methods. Panels D, E and F present the same results as the corresponding plots above, however with a restricted y-axis for a better interpretability of the results. \label{figure3}}
\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=0.57]{A_1000sim_n500_1000_500.png}
\end{center}
\caption{Results of the simulation set-up $2$. Description of the figure is analogous to that of Figure \ref{figure3}.}  \label{figure4}
\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=0.57]{DoubleML_1000sim_n500_1000_300.png}
\end{center}
\caption{Results of the simulation set-up $3$. Description of the figure is analogous to that of Figure \ref{figure3}.}  \label{figure4b}
\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[scale=0.57]{figure_IPW_sim.png}
\end{center}
\caption{Panels A, B and C show inverse-probability weights in the simulation set-ups $1$, $2$ and $3$, respectively. Panels C, D and E show corresponding inverse-probability weights on the log scale.} 
\label{figure4a}
\end{figure}
\end{landscape}

\section{Discussion} \label{Section6}

In this paper we have presented a unified framework for estimation of heterogeneous treatment effects leading to a class of weighted loss functions with nuisance parameters. Some of the recently developed approaches for CATE estimation, e.g. DR-Learner and R-Learner, can be viewed as special cases resulting from the presented framework for particular choices of the weights. Viewing different learners in the light of a general class of learners allows to gain insights into differences and similarities between them. Furthermore, relying on the Neyman-orthogonality of the derived loss functions and leveraging the recently developed literature on the orthogonal statistical learning \citep{Foster2019} we provided error bounds for CATE estimates resulting from minimization of the considered weighted loss functions.  
\\
\indent
We consider the problem of CATE estimation in two distinct cases, namely when the conditioning set of covariates $V$ is sufficient for confounding adjustment, i.e. $V = X$ and the case $V \subsetneq X$, i.e. when the conditioning set $V$ is not sufficient for confounding adjustment. In the first case all weighted loss functions target the same estimand, i.e. CATE, as long as CATE belongs to the class of functions over which we are minimizing the loss function. The situation is different when the assumed simple form of the CATE is not met or when $V \subsetneq X$. In this case the choice of weights defines the estimand that is being targeted. Therefore, different weighted loss functions aim to estimate different quantities. 
\\
\indent
Additionally to allowing comparisons of existing methods we also proposed a new learner, which is based on the weight function being the propensity-score and which corresponds to DR-Learner restricted to the population of treated patients. Such a learner can be of particular interest in retrospective studies, where the focus is on the treatment effects in treated. Furthermore, one could consider other choices of the weight functions leading to new learners, e.g. $\omega \left( x \right) = 1 - \pi \left( x \right)$, which gives rise to a DR-Learner restricted to the control group \citep{Li2018}. Furthermore, \cite{Crump2006} consider weight functions given by an indicator function $\omega \left( x \right) = \mathbbm{1} \left\lbrace x \in \mathcal{A} \right\rbrace$, where $\mathcal{A}$ is a closed subset of the covariate space $\mathcal{X}$. In particular, one could set $\mathcal{A} = \left\lbrace x \in \mathcal{X} \vert \alpha \leq \pi \left( x \right) \leq 1 - \alpha \right\rbrace$, which leads to restricting the analysis to patients having the propensity score within the range $\left[ \alpha, 1 - \alpha \right]$ for a chosen level of $\alpha$. To avoid problems with differentiability in  the construction of an orthogonal learner, one could instead approximate the indicator function by a well-chosen sigmoid function.
\\
\indent
One of the limitations of our approach focusing on estimating heterogeneous treatment effects is the fact that it may still provide too limited information to a decision-maker, not solely because of the difficulty in obtaining confidence intervals on the proposed estimates of the CATE. Imagine a situation, where a clinician needs to make a decision whether to initiate an invasive treatment for a particular patient (e.g. whether to initiate RRT for an AKI patient). It could potentially happen that for a given patient there is a slight benefit from applying the treatment, hence we would obtain a positive CATE estimate. However, when the patient's outcome is already satisfactory without treatment initiation, then the risks and costs of applying the treatment could potentially outweigh the benefit of the treatment. Therefore, ideally one would wish to provide to the clinician a prediction for patient's outcome under each hypothetical intervention, which is also referred to in the literature as counterfactual or causal prediction \citep{Hernan2019, VanGeloven2020, Coston2020}. Our proposed framework likewise suggests a class of weighted learners for counterfactual prediction. Detailed study of these goes beyond the scope of this work.

\section*{Acknowledgments}

The authors would like to thank Vasilis Syrgkanis for a helpful discussion and Bram Gadeyne, Veerle Brams, Christian Danneels and Johan Steen for technical support. The authors were supported by the Flemish Research Council (FWO Research Project 3G068619  Grant FWO.OPR.2019.0045.01).

\section*{Data Availability Statement}
According to GDPR rules, and in line with the informed consent and the approval of the ethics committee of the Ghent University Hospital, full, non-aggregated data can only be made available by the authors after permission of the ethics committee. Interested parties can address the authors to discuss an eventual project proposal to be submitted to the ethics committee.

\section*{Supporting information}

Additional supporting information, including proofs of the main results, may be found in the Appendix. The \texttt{R} code to reproduce our simulations is available at \texttt{https://github.com \\ /pmorzywolek/CATEsimulations}.


\bibliography{CATE}
\bibliographystyle{apalike}

\newpage

\appendix

\section{Proofs} \label{AppA}

\begin{proof}[Proof of Lemma \ref{lemma1}]
We have the following:
\begin{align*}
&\mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - g \left( V \right) \right\rbrace^2 \right] \\
=&\mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - \left( Y^1 - Y^0 \right) + \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right] \\
=&\mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - \left( Y^1 - Y^0 \right) \right\rbrace^2 \right] + \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right] \\
+& 2\mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - \left( Y^1 - Y^0 \right) \right\rbrace \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace \right].
\end{align*}
Furthermore, we can show that:
\begin{align*}
&\mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - \left( Y^1 - Y^0 \right) \right\rbrace  g \left( V \right) \right] \\
&= \mathbb{E} \left( \mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - \left( Y^1 - Y^0 \right) \right\rbrace  g \left( V \right) \cond V \right] \right) \\
&= \mathbb{E} \left\lbrace \mathbb{E} \left( \mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - \left( Y^1 - Y^0 \right) \right\rbrace  g \left( V \right) \cond X \right] \cond V \right) \right\rbrace \\
&= \mathbb{E} \left[ g \left( V \right) \left\lbrace \mathbb{E} \left( \mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} \right\rbrace \cond X \right] \cond V \right) - \mathbb{E} \left( Y^1 - Y^0 \cond V \right) \right\rbrace \right] \\
&= \mathbb{E} \left( g \left( V \right) \left[ \mathbb{E} \left\lbrace \mathbb{E} \left( Y^1 - Y^0 \cond X \right) \cond V \right\rbrace - \mathbb{E} \left( Y^1 - Y^0 \cond V \right) \right] \right) \\
&= \mathbb{E} \left[ g \left( V \right) \left\lbrace \mathbb{E} \left( Y^1 - Y^0 \cond V \right) - \mathbb{E} \left( Y^1 - Y^0 \cond V \right) \right\rbrace \right] = 0.
\end{align*}
Therefore, we have shown that 
\begin{align*}
\mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - g \left( V \right) \right\rbrace^2 \right] &= \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right] \\ 
&+ \mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - \left( Y^1 - Y^0 \right) \right\rbrace^2 \right] \\
&+ 2\mathbb{E} \left[ \left\lbrace \frac{AY}{\pi_0 \left( X \right)} - \frac{\left( 1-A \right) Y}{1- \pi_0 \left( X \right)} - \left( Y^1 - Y^0 \right) \right\rbrace \left( Y^1 - Y^0 \right) \right].
\end{align*}
Note that the second and third term do not depend on $g$, hence do not have impact on the minimization problem.  Therefore, it follows that the loss functions (\ref{eqn1}) and (\ref{eqn2}) have the same minimizer. 
\end{proof}
 
\begin{proof}[Proof of Lemma \ref{lemma2}]
Since $V = X$, we have the following:
\begin{align*}
&\mathbb{E} \left[ \omega \left( X \right) \left\lbrace \left( Y^1 - Y^0 \right) - g \left( X \right) \right\rbrace^2 \right] \\
&= \mathbb{E} \left( \mathbb{E} \left[ \omega \left( X \right) \left\lbrace \left( Y^1 - Y^0 \right) - g \left( X \right) \right\rbrace^2 \cond X \right] \right) \\
&= \mathbb{E} \left( \omega \left( X \right) \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - \tau \left( X \right) + \tau \left( X \right) - g \left( X \right) \right\rbrace^2 \cond X \right] \right) \\
&= \mathbb{E} \left\lbrace \omega \left( X \right) \left( \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - \tau \left( X \right) \right\rbrace^2 \cond X \right] + \mathbb{E} \left[ \left\lbrace \tau \left( X \right) - g \left( X \right) \right\rbrace^2 \cond X \right] \right. \right. \\
&\left. \left. + 2 \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - \tau \left( X \right) \right\rbrace \left\lbrace \tau \left( X \right) - g \left( X \right) \right\rbrace \cond X \right] \right) \right\rbrace \\
&= \mathbb{E} \left( \omega \left( X \right) \mathbb{E} \left[ \left\lbrace \left( Y^1 - Y^0 \right) - \tau \left( X \right) \right\rbrace^2 \cond X \right] \right) + \mathbb{E} \left[ \omega \left( X \right) \left\lbrace \tau \left( X \right) - g \left( X \right) \right\rbrace^2 \right],
\end{align*}
where the last equality follows using the law of iterated expectations. The first term does not contain $g$, therefore is not relevant for the minimization. The second term, and therefore the whole expression, is minimized for any choice of weight function $\omega \left( \cdot \right)$ at $g \left( X \right) = \tau \left( X \right)$.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lemma4}]
Calculate the pathwise derivative with respect to the first argument (target parameter):
\begin{align*} 
&D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ g - g^* \right] = \\
&\frac{d}{dt} \left[ \mathbb{E} \left\lbrace \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \left( \varphi_0 \left( Z; \eta, \lambda \left( \pi \right) \right) - \left[ g^* \left( V \right) + t \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right] \right)^2 \right\rbrace \right] \Bigr|_{t=0} \\
&= -2 \mathbb{E} \left\lbrace \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \left( \varphi_0 \left( Z; \eta, \lambda \left( \pi \right) \right) - \left[ g^* \left( V \right) + t \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right] \right) \right.\\
&\left. \times \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right\rbrace \Bigr|_{t=0} \\
&= -2 \mathbb{E} \left( \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \left\lbrace \varphi_0 \left( Z; \eta, \lambda \left( \pi \right) \right) - g^* \left( V \right) \right\rbrace \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right) \\
&= -2 \mathbb{E} \left\lbrace \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \left( \frac{\lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace}{ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace } \right.\right.\\
&\left.  \times \left[ \frac{A}{\pi_0 \left( X \right)} \left\lbrace Y - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace - \frac{1-A}{1-\pi_0 \left( X \right)} \left\lbrace Y - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right] + Q^{\left( 1 \right)}_0 \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) - g^* \left( V \right) \right) \\
&\left. \times \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right\rbrace \\
&= -2 \mathbb{E} \left\lbrace \left( \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \left[ \frac{A}{\pi_0 \left( X \right)} \left\lbrace Y - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace - \frac{1-A}{1-\pi_0 \left( X \right)} \left\lbrace Y - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right] \right. \right. \\
&\left. \left. +  \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \left\lbrace Q^{\left( 1 \right)}_0 \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) - g^* \left( V \right) \right\rbrace \right) \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right\rbrace.
\end{align*}
Calculate pathwise derivative with respect to $Q^{\left( 1 \right)}$:
\begin{align*}
&D_{Q^{\left( 1 \right)}} D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left( \pi_0 \left( X \right) \right) \right) \left[ g - g^*,  Q^{\left( 1 \right)} - Q^{\left( 1 \right)}_0 \right] \\
&= -2 \frac{d}{dt} \mathbb{E} \left( \left[ \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \left\lbrace \frac{A}{\pi_0 \left( X \right)} \left( Y - \left[ Q^{\left( 1 \right)}_0 \left( X \right) + t \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace \right] \right) \right. \right. \right. \\
&\left. - \frac{1-A}{1-\pi_0 \left( X \right)} \left\lbrace Y - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right\rbrace + \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \\
&\left.\left. \times \left( \left[ Q^{\left( 1 \right)}_0 \left( X \right) + t \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace \right] - Q^{\left( 0 \right)}_0 \left( X \right) - g^* \left( V \right) \right) \right] \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right) \Bigr|_{t=0} \\
&= 2 \mathbb{E} \left\lbrace \left( \frac{A \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace }{ \pi_0 \left( X \right) } -  \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \right) \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace \right. \\
&\left. \times \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right\rbrace \\
&= 0.
\end{align*}
Calculate pathwise derivative with respect to $Q^{\left( 0 \right)}$:
\begin{align*}
&D_{Q^{\left( 0 \right)}} D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left( \pi_0 \right) \right) \left[ g - g^*,  Q^{\left( 0 \right)} - Q^{\left( 0 \right)}_0 \right] = -2 \frac{d}{dt} \mathbb{E} \left( \left[ \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \left\lbrace \frac{A}{\pi_0} \left\lbrace Y - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace \right.\right.\right. \\
&\left.- \frac{1-A}{1-\pi_0 \left( X \right)} \left( Y - \left[ Q^{\left( 0 \right)}_0 \left( X \right) + t \left\lbrace Q^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right] \right) \right\rbrace \\
&\left. \left. + \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \left( Q^{\left( 1 \right)}_0 \left( X \right) - \left[ Q^{\left( 0 \right)}_0 \left( X \right) + t \left\lbrace Q^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right]  - g^* \left( V \right) \right) \right] \right.\\
&\left. \times \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right) \Bigr|_{t=0} \\
&= 2 \mathbb{E} \left\lbrace \left( \frac{ \left( 1 - A \right) \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace }{ 1 - \pi_0 \left( X \right) } - \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace + \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right] \right) \left\lbrace Q^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right.\\
&\left. \times \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right\rbrace \\
&= 0.
\end{align*}
Calculate pathwise derivative with respect to $\pi$:
\begin{align*}
&D_{\pi} D_g \mathcal{L} \left( g^*, \eta_0,  \lambda \left( \pi_0 \right) \right) \left[ g - g^*,  \pi - \pi_0 \right] = -2 \frac{d}{dt} \mathbb{E} \left( \left[ \lambda \left[ \pi_0 \left( X \right) + t \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \right] \right.\right.\\
&\times \left[ \frac{A}{\pi_0 \left( X \right) + t \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace} \left\lbrace Y - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace - \frac{1-A}{1- \left[ \pi_0 \left( X \right) + t \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \right]} \left\lbrace Y - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right] \\
&+  \left\lbrace \left( A - \left[ \pi_0 \left( X \right) + t \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \right] \right) \lambda^\prime \left[ \pi_0 \left( X \right) + t \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \right] \right. \\
&\left.\left.\left.+ \lambda \left[ \pi_0 \left( X \right) + t \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \right] \right\rbrace \left\lbrace Q^{\left( 1 \right)}_0 \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) - g^* \left( V \right) \right\rbrace \right] \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right) \Bigr|_{t=0} \\
&= -2  \mathbb{E} \left[ \lambda^\prime \left\lbrace \pi_0 \left( X \right) \right\rbrace \left[ \frac{A}{\pi_0 \left( X \right)} \left\lbrace Y - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace - \frac{1-A}{1- \pi_0 \left( X \right)} \left\lbrace Y - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right] \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \right. \\
&\left. \times \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
&+ 2 \mathbb{E} \left[ \lambda \left\lbrace \pi_0 \left( X \right)  \right\rbrace \left[ \frac{A}{\pi_0^2 \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace + \frac{ 1-A }{\left\lbrace 1 - \pi_0 \left( X \right)  \right\rbrace^2 } \left\lbrace Y - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \right] \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \right. \\
&\left. \times \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
&- 2 \mathbb{E} \left[ \left\lbrace A - \pi_0 \left( X \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \pi_0 \left( X \right) \right\rbrace \left\lbrace Q^{\left( 1 \right)}_0 \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) - g^* \left( V \right) \right\rbrace \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
&= 0.
\end{align*}
Therefore, the loss function (\ref{eqn5}) is an orthogonal loss function.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lemma3}]
We have the following:
\begin{align*}
&\mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - g \left( V \right) \right\rbrace^2 \right) \\
&= \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - \left( Y^1 - Y^0 \right) + \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right) \\
&= \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - \left( Y^1 - Y^0 \right) \right\rbrace^2 \right) \\
&+ \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right) \\
&+ 2 \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - \left( Y^1 - Y^0 \right) \right\rbrace \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace \right).
\end{align*}
Furthermore, we can show that:
\begin{align*}
&\mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - \left( Y^1 - Y^0 \right) \right\rbrace g \left( V \right) \right) \\
&= \mathbb{E} \left\lbrace \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left( \frac{\lambda \left\lbrace \pi \left( X \right) \right\rbrace}{ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace} \right.\right.\\
&\left.\left. \times \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right] + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - \left( Y^1 - Y^0 \right) \right) g \left( V \right) \right\rbrace \\
&= \mathbb{E} \left( \lambda \left\lbrace \pi \left( X \right) \right\rbrace \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right.\right. \\
&\left.\left. + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - \left( Y^1 - Y^0 \right) \right] g \left( V \right) \right) \\
&+ \mathbb{E} \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) \right\rbrace g \left( V \right) \right] \\
&- \mathbb{E} \left[ A \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace \left( Y^1 - Y^0 \right) g \left( V \right) \right] \\
&+ \mathbb{E} \left[ \pi \left( X \right) \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace \left( Y^1 - Y^0 \right) g \left( V \right) \right].
\end{align*}
Let's consider each of the terms. The first term:
\begin{align*}
&\mathbb{E} \left( \lambda \left\lbrace \pi \left( X \right) \right\rbrace \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right.\right. \\
&\left.\left. + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - \left( Y^1 - Y^0 \right) \right] g \left( V \right) \right) \\
&= \mathbb{E} \left\lbrace \mathbb{E} \left( \lambda \left\lbrace \pi \left( X \right) \right\rbrace \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right.\right. \right. \\
&\left.\left. \left. + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - \left( Y^1 - Y^0 \right) \right] g \left( V \right) \cond X \right) \right\rbrace \\
&= \mathbb{E} \left( \lambda \left\lbrace \pi \left( X \right) \right\rbrace g \left( V \right) \mathbb{E} \left[ \frac{A}{\pi \left( X \right) } \left\lbrace Y - Q^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{1 - A}{1 - \pi \left( X \right) } \left\lbrace Y - Q^{\left( 0 \right)} \left( X \right) \right\rbrace \right.\right. \\
&\left.\left. + Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) - \left( Y^1 - Y^0 \right) \cond X \right] \right) \\
&= \mathbb{E} \left[ \lambda \left\lbrace \pi \left( X \right) \right\rbrace g \left( V \right) \left\lbrace \mathbb{E} \left( Y^1 - Y^0 \cond X \right) - \mathbb{E} \left( Y^1 - Y^0 \cond X \right) \right\rbrace \right] = 0.
\end{align*}
The second term:
\begin{align*}
&\mathbb{E} \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) \right\rbrace g \left( V \right) \right] \\
&= \mathbb{E} \left( \mathbb{E} \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 0 \right)} \left( X \right) \right\rbrace g \left( V \right) \cond X \right] \right) = 0.
\end{align*}
The third term:
\begin{align*}
& \mathbb{E} \left[ A \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace \left( Y^1 - Y^0 \right) g \left( V \right) \right] \\
&= \mathbb{E} \left( \mathbb{E} \left[ A \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace \left( Y^1 - Y^0 \right) g \left( V \right) \cond X\right] \right) \\
&= \mathbb{E} \left[ \pi \left( X \right) \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace \left( Y^1 - Y^0 \right) g \left( V \right) \right],
\end{align*}
which equals the forth term, hence they cancel each other out. Therefore, we have shown that:
\begin{align*}
&\mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - \left( Y^1 - Y^0 \right) \right\rbrace g \left( V \right) \right) =0.
\end{align*}
Furthermore, we have 
\begin{align*}
&\mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right) \\
&= \mathbb{E} \left\lbrace \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \cond X \right) \right\rbrace \\
&= \mathbb{E} \left\lbrace \mathbb{E} \left( \left[ \left\lbrace \pi \left( X \right) - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \cond X \right) \right\rbrace \\
&= \mathbb{E} \left[ \lambda \left\lbrace \pi \left( X \right) \right\rbrace \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right],
\end{align*}
where the second equality follows by the conditional exchangeability (i.e. Assumption \ref{Assump1}). \\
Therefore, we have shown that 
\begin{align*}
&\mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - g \left( V \right) \right\rbrace^2 \right) \\
&= \mathbb{E} \left[ \lambda \left\lbrace \pi \left( X \right) \right\rbrace \left\lbrace \left( Y^1 - Y^0 \right) - g \left( V \right) \right\rbrace^2 \right] \\
&+ \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - \left( Y^1 - Y^0 \right) \right\rbrace^2 \right) \\
&+ 2 \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - \left( Y^1 - Y^0 \right) \right\rbrace \left( Y^1 - Y^0 \right) \right).
\end{align*}
Note that the second and third term do not depend on $g$, hence do not have impact on the minimization problem.  Therefore, it follows that the loss functions (\ref{eqn5}) and (\ref{eqn12}) have the same minimizer. 
\end{proof}

\subsection{Proof of Theorem \ref{theorem1}}
\begin{proof}
Using second-order Taylor expansion on the risk at $\hat{\eta}$, there exists $\overline{g} \in \text{star} \left( \mathcal{G}, g^* \right)$ such that
\begin{align*} 
\mathcal{L} \left( \hat{g}, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) &= \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) + D_g \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \\
&+ \frac{1}{2} D^2_g \mathcal{L} \left( \bar{g}, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{g} - g^*  \right].
\end{align*} 
Therefore, we obtain
\begin{align} \label{eqn16}
\frac{1}{2} D^2_g \mathcal{L} \left( \bar{g}, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{g} - g^*  \right] &= \mathcal{L} \left( \hat{g}, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) - \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \\
\nonumber
&- D_g \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right].
\end{align} 
Furthermore, we have:
\begin{align*} 
&D_g^2 \mathcal{L} \left( \overline{g}, \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right) \left[ g - g^*, g - g^* \right] \\
&=  \frac{\partial^2}{\partial t_1 \partial t_2} \mathcal{L} \left[ \overline{g} \left( V \right)  + t_1 \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace + t_2 \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace , \eta, \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \Bigr|_{t_1=t_2=0} \\
&= \frac{\partial^2}{\partial t_1 \partial t_2} \left[ \mathbb{E} \left\lbrace \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \right.\right.\\
&\left.\left. \times \left( \varphi \left( Z; \eta, \lambda \left( \pi \right) \right) - \left[ \overline{g} \left( V \right) + t_1 \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace + t_2 \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right] \right)^2 \right\rbrace \right] \Bigr|_{t_1=t_2=0} \\
&= 2 \mathbb{E} \left( \left[ \left\lbrace A - \pi \left( X \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( X \right) \right\rbrace + \lambda \left\lbrace \pi \left( X \right) \right\rbrace \right] \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace^2 \right).
\end{align*}
Therefore, using condition (\ref{eqn8}) we have
\begin{align*} 
D^2_g \mathcal{L} \left( \bar{g}, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{g} - g^*  \right] \geq \alpha \norm{\hat{g} - g^*}_\mathcal{G}^2.
\end{align*} 
Therefore, we obtain from (\ref{eqn16}) the following
\begin{align} \label{eqn17}
\nonumber
\frac{\alpha}{2} \norm{\hat{g} - g^*}_\mathcal{G}^2 &\leq \mathcal{L} \left( \hat{g}, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) - \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) - D_g \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \\
&= R_g - D_g \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right].
\end{align} 
Furthermore, we apply a second-order Taylor-expansion, which implies there exists $\overline{\eta} \in \text{star} \left(\mathcal{H}, \eta_0 \right)$ such that
\begin{align*} 
- D_g \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] &= - D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \\
&- D_\eta D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{\eta} - \eta_0 \right] \\
&- \frac{1}{2} D^2_\eta D_g \mathcal{L} \left( g^*, \overline{\eta}, \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{\eta} - \eta_0, \hat{\eta} - \eta_0 \right].
\end{align*} 
Using Neyman-orthogonality of the loss function (\ref{eqn5}) (see Lemma \ref{lemma4}) we have
\begin{align*}
D_\eta D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{\eta} - \eta_0 \right] = 0.
\end{align*} 
Therefore, we obtain 
\begin{align*} 
- D_g \mathcal{L} \left( g^*, \hat{\eta}, \lambda \left\lbrace \hat{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] &= - D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \\
&- \frac{1}{2} D^2_\eta D_g \mathcal{L} \left( g^*, \overline{\eta}, \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{\eta} - \eta_0, \hat{\eta} - \eta_0 \right].
\end{align*} 
Combining the above results we obtain from (\ref{eqn17}) the following:
\begin{align} \label{eqn18}
\frac{\alpha}{2} \norm{\hat{g} - g^*}_\mathcal{G}^2 & \leq R_g  - D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \\
\nonumber
&- \frac{1}{2} D^2_\eta D_g \mathcal{L} \left( g^*, \overline{\eta}, \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{\eta} - \eta_0, \hat{\eta} - \eta_0 \right]. 
\end{align} 
Now, we need to calculate the second-order derivative with respect to the nuisance parameters. The gradient calculations can be found in Appendix \ref{AppB}. Here, $\nabla_g$ denotes the derivative with respect to the target parameter $g$ and $\nabla_\eta$ denotes the derivative with respect to the nuisance parameter $\eta$.
\begin{align} \label{eqn19}
\nonumber
& D_{\eta}^2 D_g \mathcal{L} \left( g^*, \overline{\eta}, \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right) \left[ g - g^*,  \eta - \eta_0, \eta - \eta_0 \right]  \\
\nonumber
&= \mathbb{E} \left[ \left( \eta \left( X \right) - \eta_0 \left( X \right) \right)^\intercal \nabla_{\eta \eta}^2 \nabla_g l \left( g^*, \overline{\eta}, \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right) \left( \eta \left( X \right) - \eta_0 \left( X \right) \right) \left( g \left( V \right) - g^* \left( V \right) \right) \right] \\
\nonumber
&= \mathbb{E} \left( \begin{bmatrix}
\pi \left( X \right) - \pi_0 \left( X \right) & Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) & Q^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right)
\end{bmatrix} 
\nabla_{\eta \eta}^2 \nabla_g l \left( g^*, \overline{\eta}, \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right) \right. \\
\nonumber
&\left. \times
\begin{bmatrix}
\pi \left( X \right) - \pi_0 \left( X \right) \\
Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \\
Q^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right)
\end{bmatrix} 
\left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right) \\
\nonumber
&= - 4 \mathbb{E} \left\lbrace \left( \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi}^3 \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{ 1-A }{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^3 } \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace \right]  \right. \right.  \\
\nonumber
&- \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi}^2 \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace + \frac{1-A}{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^2} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace \right] \\
\nonumber
&+ \frac{1}{2} \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi} \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace - \overline{Q}^{\left( 1 \right)} \left( X \right) - \frac{1-A}{1-\overline{\pi} \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace + \overline{Q}^{\left( 0 \right)} \left( X \right) + g^* \left( V \right) \right] \\
\nonumber
&\left.\left. + \frac{1}{2} \lambda^{\prime\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace  \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \left\lbrace \overline{Q}^{\left( 1 \right)} \left( X \right) - \overline{Q}^{\left( 0 \right)} \left( X \right) - g^* \left( V \right) \right\rbrace \right) \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right\rbrace \\
\nonumber
&- 4 \mathbb{E} \left( \left[ \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{A}{\overline{\pi}^2 \left( X \right)} - \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{A}{\overline{\pi} \left( X \right)} + \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right] \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \right. \\
\nonumber
&\left. \times \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right) \\
\nonumber
&- 4 \mathbb{E} \left( \left[ \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace  \frac{ 1-A }{ \left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^2} + \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{1-A}{1-\overline{\pi} \left( X \right)} - \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right] \right.\\
\nonumber
&\left. \times \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \left\lbrace Q^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right) \\
&= - 4 \mathbb{E} \left[ C_1 \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
\nonumber
&- 4 \mathbb{E} \left[ C_2 \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \left\lbrace Q^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
\nonumber
&- 4 \mathbb{E} \left[ C_3 \left\lbrace \pi \left( X \right) - \pi_0 \left( X \right) \right\rbrace \left\lbrace Q^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \left\lbrace g \left( V \right) - g^* \left( V \right) \right\rbrace \right]
\end{align}
where
\begin{align} \label{eqn20}
&C_1 \coloneqq  \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi}^3 \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace - \frac{ 1-A }{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^3} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace \right]  \\
\nonumber
&- \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi}^2 \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace + \frac{1-A}{\left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^2} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace \right] \\
\nonumber
&+ \frac{1}{2} \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \left[ \frac{A}{\overline{\pi} \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 1 \right)} \left( X \right) \right\rbrace - \overline{Q}^{\left( 1 \right)} \left( X \right) - \frac{1-A}{1-\overline{\pi} \left( X \right)} \left\lbrace Y - \overline{Q}^{\left( 0 \right)} \left( X \right) \right\rbrace + \overline{Q}^{\left( 0 \right)} \left( X \right) + g^* \left( V \right) \right] \\
\nonumber
&+ \frac{1}{2} \lambda^{\prime\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace  \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \left\lbrace \overline{Q}^{\left( 1 \right)} \left( X \right) - \overline{Q}^{\left( 0 \right)} \left( X \right) - g^* \left( X \right) \right\rbrace
\end{align}
\begin{align} \label{eqn21}
C_2 \coloneqq  \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{A}{\overline{\pi}^2 \left( X \right)} - \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{A}{\overline{\pi} \left( X \right)} + \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace
\end{align}
\begin{align} \label{eqn22}
C_3 \coloneqq \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace  \frac{ 1-A }{ \left\lbrace 1-\overline{\pi} \left( X \right) \right\rbrace^2} + \lambda^\prime \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \frac{1-A}{1-\overline{\pi} \left( X \right)} - \left\lbrace A - \overline{\pi} \left( X \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \overline{\pi} \left( X \right) \right\rbrace.
\end{align}
Based on the above calculation (\ref{eqn19}) we have the following:
\begin{align*} 
&- \frac{1}{2} D^2_\eta D_g \mathcal{L} \left( g^*, \overline{\eta}, \lambda \left\lbrace \overline{\pi} \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^*, \hat{\eta} - \eta_0, \hat{\eta} - \eta_0 \right] \\
&= 2 \mathbb{E} \left[ C_1 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{g} \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
&+ 2 \mathbb{E} \left[ C_2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace \left\lbrace \hat{Q}^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace \left\lbrace \hat{g} \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
&+ 2 \mathbb{E} \left[ C_3 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace \left\lbrace \hat{Q}^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \left\lbrace \hat{g} \left( V \right) - g^* \left( V \right) \right\rbrace \right].
\end{align*} 
Combining the above expressions we obtain from (\ref{eqn18}) the following:
\begin{align*} 
\frac{\alpha}{2} \norm{\hat{g} - g^*}_\mathcal{G}^2 &\leq R_g - D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \\
&+ 2 \mathbb{E} \left[ C_1 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{g} \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
&+ 2 \mathbb{E} \left[ C_2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace \left\lbrace \hat{Q}^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace \left\lbrace \hat{g} \left( V \right) - g^* \left( V \right) \right\rbrace \right] \\
&+ 2 \mathbb{E} \left[ C_3 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace \left\lbrace \hat{Q}^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace \left\lbrace \hat{g} \left( V \right) - g^* \left( V \right) \right\rbrace \right].
\end{align*} 
Applying Cauchy-Schwarz inequality to last three terms we get:
\begin{align*} 
\frac{\alpha}{2} \norm{\hat{g} - g^*}_\mathcal{G}^2 &\leq R_g - D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \\
&+ 2 \sqrt{ \mathbb{E} \left[ C_1^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^4 \right] } \norm{\hat{g} - g^*}_\mathcal{G} \\
&+ 2 \sqrt{ \mathbb{E} \left[ C_2^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace^2 \right] } \norm{\hat{g} - g^*}_\mathcal{G} \\
&+ 2 \sqrt{ \mathbb{E} \left[ C_3^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace^2 \right] } \norm{\hat{g} - g^*}_\mathcal{G}.
\end{align*} 
Furthermore, using the AM-GM inequality for the last two terms, for any constants $\delta_1>0$, $\delta_2>0$ and $\delta_3>0$ we have:
\begin{align*} 
\frac{\alpha}{2} \norm{\hat{g} - g^*}_\mathcal{G}^2 &\leq R_g - D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \\
&+ \frac{1}{\delta_1} \mathbb{E} \left[ C_1^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^4 \right] \\
&+ \frac{1}{\delta_2}  \mathbb{E} \left[ C_2^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace^2 \right] \\
&+ \frac{1}{\delta_3} \mathbb{E} \left[ C_3^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace^2 \right]  \\
&+ \left( \delta_1 + \delta_2 + \delta_3 \right) \norm{\hat{g} - g^*}_\mathcal{G}^2.
\end{align*} 
Therefore, we obtain (for $\delta_1 + \delta_2 + \delta_3 < \frac{\alpha}{2}$):
\begin{align} \label{eqn23}
\norm{\hat{g} - g^*}_\mathcal{G}^2 \leq \frac{1}{\alpha/2 - \delta_1 - \delta_2 - \delta_3} &\left( R_g - D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \right. \\
\nonumber
&+ \frac{1}{\delta_1} \mathbb{E} \left[ C_1^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^4 \right] \\
\nonumber
&+ \frac{1}{\delta_2}  \mathbb{E} \left[ C_2^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace^2 \right] \\
\nonumber
&\left. + \frac{1}{\delta_3} \mathbb{E} \left[ C_3^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace^2 \right] \right).
\end{align} 

Furthermore, since we have $D_g \mathcal{L} \left( g^*, \eta_0, \lambda \left\lbrace \pi_0 \left( X \right) \right\rbrace \right) \left[ \hat{g} - g^* \right] \geq 0$, hence we obtain from (\ref{eqn23}) the following:
\begin{align*}
\norm{\hat{g} - g^*}_\mathcal{G}^2 \leq \frac{1}{\alpha/2 - \delta_1 - \delta_2 - \delta_3} &\left( R_g + \frac{1}{\delta_1} \mathbb{E} \left[ C_1^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^4 \right]  \right. \\
&+ \frac{1}{\delta_2}  \mathbb{E} \left[ C_2^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 1 \right)} \left( X \right) - Q^{\left( 1 \right)}_0 \left( X \right) \right\rbrace^2 \right] \\
&\left. + \frac{1}{\delta_3} \mathbb{E} \left[ C_3^2 \left\lbrace \hat{\pi} \left( X \right) - \pi_0 \left( X \right) \right\rbrace^2 \left\lbrace \hat{Q}^{\left( 0 \right)} \left( X \right) - Q^{\left( 0 \right)}_0 \left( X \right) \right\rbrace^2 \right] \right).
\end{align*}
where $C_1$, $C_2$ and $C_3$ are given by (\ref{eqn20}), (\ref{eqn21}) and (\ref{eqn22}), respectively.
\end{proof}

\newpage

\section{Gradients calculations for the weighted DR-Learner loss function} \label{AppB}

\begin{align*} 
&l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) =  \left[ \left\lbrace a - \pi \left( x \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace + \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right] \left( \frac{\lambda \left\lbrace \pi \left( x \right) \right\rbrace }{ \left\lbrace a - \pi \left( x \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace + \lambda \left\lbrace \pi \left( x \right) \right\rbrace } \right. \\
&\left. \times \left[ \frac{a}{\pi \left( x \right)} \left\lbrace y - Q^{\left( 1 \right)} \left( x \right) \right\rbrace - \frac{1-a}{1-\pi \left( x \right)} \left\lbrace y - Q^{\left( 0 \right)} \left( x \right) \right\rbrace \right] + Q^{\left( 1 \right)} \left( x \right) - Q^{\left( 0 \right)} \left( x \right) - g \left( v \right) \right)^2 
\end{align*}
\begin{align*} 
\nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) &= -2 \left( \lambda \left\lbrace \pi \left( x \right) \right\rbrace \left[ \frac{a}{\pi \left( x \right)} \left\lbrace y - Q^{\left( 1 \right)} \left( x \right) \right\rbrace - \frac{1-a}{1-\pi \left( x \right)} \left\lbrace y - Q^{\left( 0 \right)} \left( x \right) \right\rbrace \right] \right. \\
&\left. + \left[ \left\lbrace a - \pi \left( x \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace + \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right] \left\lbrace Q^{\left( 1 \right)} \left( x \right) - Q^{\left( 0 \right)} \left( x \right) - g \left( v \right) \right\rbrace \right)
\end{align*}
\begin{align*} 
\nabla_{Q^{\left( 1 \right)}} \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) &= -2 \left[ \lambda \left\lbrace \pi \left( x \right) \right\rbrace \left\lbrace 1- \frac{a}{\pi \left( x \right)} \right\rbrace +  \left\lbrace a - \pi  \left( x \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace \right]
\end{align*}
\begin{align*} 
\nabla_{Q^{\left( 0 \right)}} \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) &= 2 \left[ \lambda \left\lbrace \pi \left( x \right) \right\rbrace \left\lbrace 1 - \frac{1-a}{1-\pi \left( x \right)} \right\rbrace + \left\lbrace a - \pi \left( x \right) \right\rbrace \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace \right]
\end{align*}
\begin{align*} 
\nabla_\pi \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) &= 2 \lambda \left\lbrace \pi \left( x \right) \right\rbrace \left[ \frac{a}{\pi^2 \left( x \right)} \left\lbrace y - Q^{\left( 1 \right)} \left( x \right) \right\rbrace + \frac{1-a}{\left\lbrace 1-\pi \left( x \right) \right\rbrace^2} \left\lbrace y - Q^{\left( 0 \right)} \left( x \right) \right\rbrace \right] \\
&-2 \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace \left[ \frac{a}{\pi \left( x \right)} \left\lbrace y - Q^{\left( 1 \right)} \left( x \right) \right\rbrace - \frac{1-a}{1-\pi \left( x \right)} \left\lbrace y - Q^{\left( 0 \right)} \left( x \right) \right\rbrace \right] \\
&-2 \lambda^{\prime\prime} \left\lbrace \pi \left( x \right) \right\rbrace \left\lbrace a - \pi \left( x \right) \right\rbrace \left\lbrace Q^{\left( 1 \right)} \left( x \right) - Q^{\left( 0 \right)} \left( x \right) - g \left( v \right) \right\rbrace 
\end{align*}
\begin{align*} 
\nabla_{Q^{\left( 1 \right)}Q^{\left( 1 \right)}}^2 \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) = 0
\end{align*}
\begin{align*} 
\nabla_{Q^{\left( 0 \right)}} \nabla_{Q^{\left( 1 \right)}} \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) = 0
\end{align*}
\begin{align*} 
\nabla_{\pi} \nabla_{Q^{\left( 1 \right)}} \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) = - 2 \left[ \lambda \left\lbrace \pi \left( x \right) \right\rbrace \frac{a}{\pi^2 \left( x \right)} - \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace \frac{a}{\pi \left( x \right)} + \left\lbrace a - \pi \left( x \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \pi \left( x \right) \right\rbrace \right]
\end{align*}
\begin{align*} 
\nabla_{Q^{\left( 1 \right)}} \nabla_{Q^{\left( 0 \right)}} \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) = 0
\end{align*}
\begin{align*} 
\nabla_{Q^{\left( 0 \right)}Q^{\left( 0 \right)}}^2 \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) = 0
\end{align*}
\begin{align*} 
\nabla_{\pi} \nabla_{Q^{\left( 0 \right)}} \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) = -2  \left[ \lambda \left\lbrace \pi \left( x \right) \right\rbrace \frac{ 1-a }{\left\lbrace 1-\pi \left( x \right) \right\rbrace^2}  + \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace \frac{1-a}{1-\pi \left( x \right)} - \left\lbrace a - \pi \left( x \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \pi \left( x \right) \right\rbrace \right]
\end{align*}
\begin{align*} 
\nabla_{Q^{\left( 1 \right)}} \nabla_{\pi} \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) = - 2 \lambda \left\lbrace \pi \left( x \right) \right\rbrace \frac{a}{\pi^2 \left( x \right)} + 2  \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace \frac{a}{\pi \left( x \right)} - 2 \left\lbrace a - \pi \left( x \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \pi \left( x \right) \right\rbrace
\end{align*}
\begin{align*} 
\nabla_{Q^{\left( 0 \right)}} \nabla_{\pi} \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) &= - 2 \lambda \left\lbrace \pi \left( x \right) \right\rbrace  \frac{ 1-a }{ \left\lbrace 1-\pi \left( x \right) \right\rbrace^2} - 2 \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace \frac{1-a}{1-\pi \left( x \right)} \\
&+ 2 \left\lbrace a - \pi \left( x \right) \right\rbrace \lambda^{\prime\prime} \left\lbrace \pi \left( x \right) \right\rbrace
\end{align*}
\begin{align*} 
&\nabla_{\pi \pi}^2 \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) = -4 \lambda \left\lbrace \pi \left( x \right) \right\rbrace \left[ \frac{a}{\pi^3 \left( x \right)} \left\lbrace y - Q^{\left( 1 \right)} \left( x \right) \right\rbrace - \frac{ 1-a }{\left\lbrace 1-\pi \left( x \right)\right\rbrace^3} \left\lbrace y - Q^{\left( 0 \right)} \left( x \right) \right\rbrace \right] \\ 
&+ 4 \lambda^\prime \left\lbrace \pi \left( x \right) \right\rbrace \left[ \frac{a}{\pi^2 \left( x \right)} \left\lbrace y - Q^{\left( 1 \right)} \left( x \right) \right\rbrace + \frac{1-a}{\left\lbrace 1-\pi \left( x \right) \right\rbrace^2} \left\lbrace y - Q^{\left( 0 \right)} \left( x \right) \right\rbrace \right] \\
&-2 \lambda^{\prime\prime} \left\lbrace \pi \left( x \right) \right\rbrace \left[ \frac{a}{\pi \left( x \right)} \left\lbrace y - Q^{\left( 1 \right)} \left( x \right) \right\rbrace - Q^{\left( 1 \right)} \left( x \right) - \frac{1-a}{1-\pi \left( x \right)} \left\lbrace y - Q^{\left( 0 \right)} \left( x \right) \right\rbrace + Q^{\left( 0 \right)} \left( x \right) + g \left( v \right) \right] \\
&-2 \lambda^{\prime\prime\prime} \left\lbrace \pi \left( x \right) \right\rbrace \left\lbrace a - \pi \left( x \right) \right\rbrace \left\lbrace Q^{\left( 1 \right)} \left( x \right) - Q^{\left( 0 \right)} \left( x \right) - g \left( v \right) \right\rbrace 
\end{align*}
\begin{align*} 
&\nabla_{\eta \eta}^2 \nabla_g  l \left( g, \eta,  \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) \\
& =
\begin{bmatrix}
\nabla_{\pi \pi}^2 \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right)  & \nabla_{\pi} \nabla_{Q^{\left( 1 \right)}} \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right)  & \nabla_{\pi} \nabla_{Q^{\left( 0 \right)}} \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) \\
\nabla_{Q^{\left( 1 \right)}} \nabla_{\pi} \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right)   & \nabla_{Q^{\left( 1 \right)} Q^{\left( 1 \right)}}^2 \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) & \nabla_{Q^{\left( 1 \right)}} \nabla_{Q^{\left( 0 \right)}} \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) \\
\nabla_{Q^{\left( 0 \right)}} \nabla_{\pi} \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right)   & \nabla_{Q^{\left( 0 \right)}} \nabla_{Q^{\left( 1 \right)}} \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) & \nabla_{Q^{\left( 0 \right)} Q^{\left( 0 \right)}}^2 \nabla_g  l \left( g, \eta, \lambda \left\lbrace \pi \left( x \right) \right\rbrace \right) 
\end{bmatrix}
\end{align*}

\newpage

\section{Empirical Example - Additional information}

\subsection{Overview of the weights} \label{subsect5}

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.8]{figure_IPW.png}
\end{center}
\caption{Inverse-probability weights in both treatment groups. Treatment level "$1$" denotes "RRT initiation" and treatment level "$0$" denotes "no RRT initiation".} \label{figure5}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.8]{figure_POW.png}
\end{center}
\caption{Propensity-overlap weights in both treatment groups. Treatment level "$1$" denotes "RRT initiation" and treatment level "$0$" denotes "no RRT initiation".} \label{figure6}
\end{figure}

\end{document}


