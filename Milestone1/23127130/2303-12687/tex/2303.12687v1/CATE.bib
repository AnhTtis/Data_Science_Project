Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Belloni2017,
abstract = {In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets.},
archivePrefix = {arXiv},
arxivId = {1311.2645},
author = {Belloni, A. and Chernozhukov, V. and Fernandez-Val, I. and Hansen, C.},
doi = {10.3982/ecta12723},
eprint = {1311.2645},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Econometrica - 2017 - Belloni - Program Evaluation and Causal Inference With High‐Dimensional Data.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {1},
pages = {233--298},
title = {{Program Evaluation and Causal Inference With High-Dimensional Data}},
volume = {85},
year = {2017}
}

@article{Kallus2021,
abstract = {Policy learning can be used to extract individualized treatment regimes from observational data in healthcare, civics, e-commerce, and beyond. One big hurdle to policy learning is a commonplace lack of overlap in the data for different actions, which can lead to unwieldy policy evaluation and poorly performing learned policies. We study a solution to this problem based on retargeting, that is, changing the population on which policies are optimized. We first argue that at the population level, retargeting may induce little to no bias. We then characterize the optimal reference policy and retargeting weights in both binary-action and multi-action settings. We do this in terms of the asymptotic efficient estimation variance of the new learning objective. We further consider weights that additionally control for potential bias due to retargeting. Extensive empirical results in a simulation study and a case study of personalized job counseling demonstrate that retargeting is a fairly easy way to significantly improve any policy learning procedure applied to observational data. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1906.08611},
author = {Kallus, Nathan},
doi = {10.1080/01621459.2020.1788948},
eprint = {1906.08611},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/More Efficient Policy Learning via Optimal Retargeting.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Efficient policy learning,Individualized treatment regimes,Optimization,Overlap},
number = {534},
pages = {646--658},
publisher = {Taylor {\&} Francis},
title = {{More Efficient Policy Learning via Optimal Retargeting}},
url = {https://doi.org/10.1080/01621459.2020.1788948},
volume = {116},
year = {2021}
}

@techreport{vanderLaan2014,
 title = "Targeted Learning of an Optimal Dynamic Treatment, and Statistical Inference for its Mean Outcome",
 author = "van der Laan, Mark J and Luedtke, Alexander R",
 type = "Working Paper",
 institution = "U.C. Berkeley Division of Biostatistics Working Paper Series",
 number = "332",
 year = "2014",
 month = "December"
}

@techreport{Crump2006,
 title = "Moving the Goalposts: Addressing Limited Overlap in the Estimation of Average Treatment Effects by Changing the Estimand",
 author = "Crump, Richard K and Hotz, V. Joseph and Imbens, Guido W and Mitnik, Oscar A",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Technical Working Paper Series",
 number = "330",
 year = "2006",
 month = "October",
 doi = {10.3386/t0330},
 URL = "http://www.nber.org/papers/t0330",
 abstract = {Estimation of average treatment effects under unconfoundedness or exogenous treatment assignment is often hampered by lack of overlap in the covariate distributions. This lack of overlap can lead to imprecise estimates and can make commonly used estimators sensitive to the choice of specification. In such cases researchers have often used informal methods for trimming the sample. In this paper we develop a systematic approach to addressing such lack of overlap. We characterize optimal subsamples for which the average treatment effect can be estimated most precisely, as well as optimally weighted average treatment effects. Under some conditions the optimal selection rules depend solely on the propensity score. For a wide range of distributions a good approximation to the optimal rule is provided by the simple selection rule to drop all units with estimated propensity scores outside the range [0.1,0.9].},
}

@inproceedings{Coston2020,
 author = {Coston, Amanda and Kennedy, Edward and Chouldechova, Alexandra},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {4150--4162},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Predictions under Runtime Confounding},
 url = {https://proceedings.neurips.cc/paper/2020/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Hernan2019,
abstract = {Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term "data science" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.},
author = {Hern{\'{a}}n, Miguel A. and Hsu, John and Healy, Brian},
doi = {10.1080/09332480.2019.1579578},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/hernan{\_}chance19.pdf:pdf},
issn = {0933-2480},
journal = {Chance},
number = {1},
pages = {42--49},
title = {{A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks}},
volume = {32},
year = {2019}
}

@article{VanGeloven2020,
abstract = {In this paper we study approaches for dealing with treatment when developing a clinical prediction model. Analogous to the estimand framework recently proposed by the European Medicines Agency for clinical trials, we propose a ‘predictimand' framework of different questions that may be of interest when predicting risk in relation to treatment started after baseline. We provide a formal definition of the estimands matching these questions, give examples of settings in which each is useful and discuss appropriate estimators including their assumptions. We illustrate the impact of the predictimand choice in a dataset of patients with end-stage kidney disease. We argue that clearly defining the estimand is equally important in prediction research as in causal inference.},
archivePrefix = {arXiv},
arxivId = {2004.06998},
author = {van Geloven, Nan and Swanson, Sonja A. and Ramspek, Chava L. and Luijken, Kim and van Diepen, Merel and Morris, Tim P. and Groenwold, Rolf H.H. and van Houwelingen, Hans C. and Putter, Hein and le Cessie, Saskia},
doi = {10.1007/s10654-020-00636-1},
eprint = {2004.06998},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/s10654-020-00636-1.pdf:pdf},
isbn = {0123456789},
issn = {15737284},
journal = {European Journal of Epidemiology},
keywords = {Censoring,Clinical prediction model,Estimands,Predictimands,Treatment},
number = {7},
pages = {619--630},
pmid = {32445007},
publisher = {Springer Netherlands},
title = {{Prediction meets causal inference: the role of treatment in clinical prediction models}},
url = {https://doi.org/10.1007/s10654-020-00636-1},
volume = {35},
year = {2020}
}

@article{Li2018,
abstract = {Covariate balance is crucial for unconfounded descriptive or causal comparisons. However, lack of balance is common in observational studies. This article considers weighting strategies for balancing covariates. We define a general class of weights—the balancing weights—that balance the weighted distributions of the covariates between treatment groups. These weights incorporate the propensity score to weight each group to an analyst-selected target population. This class unifies existing weighting methods, including commonly used weights such as inverse-probability weights as special cases. General large-sample results on nonparametric estimation based on these weights are derived. We further propose a new weighting scheme, the overlap weights, in which each unit's weight is proportional to the probability of that unit being assigned to the opposite group. The overlap weights are bounded, and minimize the asymptotic variance of the weighted average treatment effect among the class of balancing weights. The overlap weights also possess a desirable small-sample exact balance property, based on which we propose a new method that achieves exact balance for means of any selected set of covariates. Two applications illustrate these methods and compare them with other approaches.},
archivePrefix = {arXiv},
arxivId = {1404.1785},
author = {Li, Fan and Morgan, Kari Lock and Zaslavsky, Alan M.},
doi = {10.1080/01621459.2016.1260466},
eprint = {1404.1785},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Balancing Covariates via Propensity Score Weighting.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Balancing weights,Causal inference,Clinical equipoise,Confounding,Exact balance,Overlap weights},
number = {521},
pages = {390--400},
publisher = {Taylor {\&} Francis},
title = {{Balancing Covariates via Propensity Score Weighting}},
url = {https://doi.org/10.1080/01621459.2016.1260466},
volume = {113},
year = {2018}
}

@article{Imbens2009,
abstract = {Many empirical questions in economics and other social sciences depend on causal effects of programs or policies. In the last two decades, much research has been done on the econometric and statistical analysis of such causal effects. This recent theoretical literature has built on, and combined features of earlier work in both the statistics and econometrics literatures. It has by now reached a level of maturity that makes it an important tool in many areas of empirical research in economics, including labor economics, public finance, development economics, industrial organization, and other areas of empirical microeconomics. In this review, we discuss some of the recent developments. We focus primarily on practical issues for empirical researchers, as well as provide a historical overview of the area and give references to more technical research.},
author = {Imbens, Guido W. and Wooldridge, Jeffrey M.},
doi = {10.1257/jel.47.1.5},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/jel.47.1.5.pdf:pdf},
issn = {00220515},
journal = {Journal of Economic Literature},
number = {1},
pages = {5--86},
title = {{Recent developments in the econometrics of program evaluation}},
volume = {47},
year = {2009}
}

@article{Vansteelandt2009,
abstract = {Intensive care unit (ICU) patients are highly susceptible to hospital-acquired infections due to their poor health and many invasive therapeutic treatments. The effect on mortality of acquiring such infections is, however, poorly understood. Our goal is to quantify this using data from the National Surveillance Study of Nosocomial Infections in ICUs (Belgium). This is challenging because of the presence of time-dependent confounders, such as mechanical ventilation, which lie on the causal path from infection to mortality. Standard statistical analyses may be severely misleading in such settings and have shown contradictory results. Inverse probability weighting for marginal structural models may instead be used but is not directly applicable because these models parameterize the effect of acquiring infection on a given day in ICU, versus "never" acquiring infection in ICU, and this is ill-defined when ICU discharge precedes that day. Additional complications arise from the informative censoring of the survival time by hospital discharge and the instability of the inverse weighting estimation procedure. We accommodate this by introducing a new class of marginal structural models for so-called partial exposure regimes. These describe the effect on the hazard of death of acquiring infection on a given day s, versus not acquiring infection "up to that day," had patients stayed in the ICU for at least s days.},
author = {Vansteelandt, Stijn and Mertens, Karl and Suetens, Carl and Goetghebeur, Els},
doi = {10.1093/biostatistics/kxn012},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/kxn012.pdf:pdf},
issn = {14654644},
journal = {Biostatistics},
keywords = {Causal inference,Direct effect,ICU,Intermediate variables,Marginal structural models,Nosocomial infection,Time-dependent confounding},
number = {1},
pages = {46--59},
pmid = {18503036},
title = {{Marginal structural models for partial exposure regimes}},
volume = {10},
year = {2009}
}

@article{VanDerLaan2007,
author = {{van der Laan}, Mark J. and Polley, E. and Hubbard, A.},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {Causal effect,Causal graph,Censored data,Collaborative double robust,Cross-validation,Double robust,Dynamic treatment regimens,Efficient influence curve,Estimating function,Estimator selection,Locally efficient,Loss function},
number = {25},
title = {{Super Learner}},
volume = {6},
year = {2007}
}

@article{Dahabreh2020,
archivePrefix = {arXiv},
arxivId = {1805.00550},
author = {Dahabreh, Issa J. and Robertson, Sarah E. and Steingrimsson, Jon A. and Stuart, Elizabeth A. and Hern{\'{a}}n, Miguel A.},
doi = {10.1002/sim.8426},
eprint = {1805.00550},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Statistics in Medicine - 2020 - Dahabreh - Extending inferences from a randomized trial to a new target population.pdf:pdf},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {double robustness,generalizability,observational analyses,randomized trials,transportability},
number = {14},
pages = {1999--2014},
pmid = {32253789},
title = {{Extending inferences from a randomized trial to a new target population}},
volume = {39},
year = {2020}
}

@book{Pfanzagl1990,
author = {Pfanzagl, Johann},
publisher = {Springer},
title = {{Estimation in Semiparametric Models}},
year = {1990}
}

@book{Bickel1998,
author = {Bickel, Peter and Klaassen, Chris and Ritov, Ya'acov and Wellner, Jon},
publisher = {Springer},
title = {{Efficient and Adaptive Estimation for Semiparametric Models}},
year = {1998}
}

@book{vanderLaan2003,
author = {van der Laan, Mark J and Robins, James},
publisher = {Springer},
title = {{Unified Methods for Censored Longitudinal Data and Causality}},
year = {2003}
}

@book{vanderLaan2011,
author = {van der Laan, Mark J and Rose, Sherri},
publisher = {Springer},
title = {{Targeted Learning. Causal Inference for Observational and Experimental Data}},
year = {2011}
}

@book{Kosorok2008,
author = {Kosorok, Michael R.},
publisher = {Springer},
title = {{Introduction to Empirical Processes and Semiparametric Inference}},
year = {2008}
}

@article{KDIGO2012,
file = {:Users/pawel/Documents/UGent/Papers/KDIGO-2012-AKI-Guideline-English.pdf:pdf},
issn = {21548331},
author = {KDIGO},
journal = {Kidney International Supplements},
number = {1},
pages = {1--138},
pmid = {24566591},
title = {{Kidney Disease: Improving Global Outcomes (KDIGO) Acute Kidney Injury Work Group. KDIGO Clinical Practice Guideline for Acute Kidney Injury.}},
volume = {2},
year = {2012}
}

@article{Kennedy2017,
abstract = {Continuous treatments (e.g. doses) arise often in practice, but many available causal effect estimators are limited by either requiring parametric models for the effect curve, or by not allowing doubly robust covariate adjustment. We develop a novel kernel smoothing approach that requires only mild smoothness assumptions on the effect curve and still allows for misspecification of either the treatment density or outcome regression. We derive asymptotic properties and give a procedure for data-driven bandwidth selection. The methods are illustrated via simulation and in a study of the effect of nurse staffing on hospital readmissions penalties.},
archivePrefix = {arXiv},
arxivId = {1507.00747},
author = {Kennedy, Edward H. and Ma, Zongming and McHugh, Matthew D. and Small, Dylan S.},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Journal of the Royal Statistical Society Series B Statistical Methodology - 2016 - Kennedy - Non‐parametric methods for.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Causal inference,Dose–response,Efficient influence function,Kernel smoothing,Semiparametric estimation},
number = {4},
pages = {1229--1245},
title = {{Non-parametric methods for doubly robust estimation of continuous treatment effects}},
volume = {79},
year = {2017}
}

@book{Tsybakov2009,
author = {Tsybakov, Alexandre B.},
file = {:Users/pawel/Documents/UGent/Books/Nonparametric estimation/20121209191850{\_}7.pdf:pdf},
publisher = {Springer},
title = {{Introduction to Nonparametric Estimation}},
year = {2009}
}

@article{Sun1994,
author = {Sun, Jiayang and Loader, Clive},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/1176325631.pdf:pdf},
journal = {The Annals of Statistics},
pages = {1328--1345},
title = {{Simultaneous Confidence Bands for Linear Regression and Smoothing}},
volume = {22},
year = {1994}
}

@article{Faraway1995,
author = {Faraway, Julian J. and Sun, Jiayang},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2291347.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Scheff{\'{e}} confidence interval,Tube formula},
number = {431},
pages = {1094--1098},
title = {{Simultaneous confidence bands for linear regression with heteroscedastic errors}},
volume = {90},
year = {1995}
}

@book{Wasserman2006,
author = {Wasserman, Larry},
booktitle = {All of Nonparametric Statistics},
file = {:Users/pawel/Documents/UGent/Books/Nonparametric estimation/book2(2).pdf:pdf},
publisher = {Springer},
title = {{All of Nonparametric Statistics}},
url = {http://books.google.com/books?id=9tv0taI8l6YC},
year = {2006}
}

@article{Rubin1974,
abstract = {A discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation is presented. The objective is to specify the benefits of randomization in estimating causal effects of treatments. The basic conclusion is that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and nec-essary procedure in many cases.},
author = {Rubin, Donald B.},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/420{\_}paper{\_}Rubin74.pdf:pdf},
journal = {Journal of Educational Psychology},
number = {5},
pages = {688--701},
title = {{Estimating causal effects of treatment in randomized and nonrandomized studies}},
url = {http://www.fsb.muohio.edu/lij14/420{\_}paper{\_}Rubin74.pdf},
volume = {66},
year = {1974}
}

@article{Rosenbaum1983,
abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.},
author = {Rosenbaum, Paul R. and Rubin, Donald B.},
doi = {10.1017/CBO9780511810725.016},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/70-1-41.pdf:pdf},
isbn = {9780511810725},
journal = {Biometrika},
volume = {70},
number = {1},
pages = {41--55},
title = {{The central role of the propensity score in observational studies for causal effects}},
year = {1983}
}

@article{Athey2015,
abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without "sparsity" assumptions. We propose an "honest" approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the "ground truth" for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90{\%} confidence intervals, whereas coverage ranges between 74{\%} and 84{\%} for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7-22{\%}.},
archivePrefix = {arXiv},
arxivId = {1504.01132},
author = {Athey, Susan and Imbens, Guido},
doi = {10.1073/pnas.1510489113},
eprint = {1504.01132},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Machine{\_}Learning{\_}Methods{\_}for{\_}Estimating{\_}Heterogene.pdf:pdf},
issn = {10916490},
keywords = {Causal inference,Cross-validation,Heterogeneous treatment effects,Potential outcomes,Supervised machine learning},
pmid = {27382149},
title = {{Machine Learning Methods for Estimating Heterogeneous Causal Effects}},
year = {2015}
}

@article{Robinson2010,
author = {Robinson, P M},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/3{\_}20140520034711{\_}Root-N-consistent semiparametric regression models.pdf:pdf},
journal = {Econometrica},
number = {4},
pages = {931--954},
title = {{Root-N-Consistent Semiparametric Regression}},
volume = {56},
year = {1988}
}
@article{Newey1994,
author = {Newey, Whitney K.},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2951752.pdf:pdf},
journal = {Econometrica},
number = {6},
pages = {1349--1382},
title = {{The Asymptotic Variance of Semiparametric Estimators}},
volume = {62},
year = {1994}
}
@book{Tsiatis2006,
author = {Tsiatis, Anastasios A.},
doi = {10.1007/978-0-387-98135-2},
file = {:Users/pawel/Documents/UGent/Books/(Springer Series in Statistics) Anastasios Tsiatis-Semiparametric Theory and Missing Data -Springer (2006).pdf:pdf},
isbn = {9780387775005},
issn = {01727397},
pmid = {15772297},
publisher = {Springer New York},
title = {{Semiparametric Theory and Missing Data}},
year = {2006}
}
@article{Semenova2017,
abstract = {This paper provides estimation and inference methods for a large number of heterogeneous treatment effects in the presence of an even larger number of controls and unobserved unit heterogeneity. In our main example, the vector of heterogeneous treatments is generated by interacting the base treatment variable with a subset of controls. We first estimate the unit-specific expectation functions of the outcome and each treatment interaction conditional on controls and take the residuals. Second, we report the Lasso (L1-regularized least squares) estimate of the heterogeneous treatment effect parameter, regressing the outcome residual on the vector of treatment ones. We debias the Lasso estimator to conduct simultaneous inference on the target parameter by Gaussian bootstrap. We account for the unobserved unit heterogeneity by projecting it onto the time-invariant covariates, following the correlated random effects approach of Mundlak (1978) and Chamberlain (1982). We demonstrate our method by estimating price elasticities of groceries based on scanner data.},
archivePrefix = {arXiv},
arxivId = {1712.09988},
author = {Semenova, Vira and Goldman, Matt and Chernozhukov, Victor and Taddy, Matt},
eprint = {1712.09988},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/1712.09988.pdf:pdf},
issn = {2331-8422},
number = {2016},
title = {{Estimation and Inference on Heterogeneous Treatment Effects in High-Dimensional Dynamic Panels}},
url = {http://arxiv.org/abs/1712.09988},
year = {2017}
}
@article{Nie2021,
abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines.},
archivePrefix = {arXiv},
arxivId = {1712.04912},
author = {Nie, X and Wager, S},
doi = {10.1093/biomet/asaa076},
eprint = {1712.04912},
file = {:Users/pawel/Documents/UGent/Papers/asaa076(1).pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {2},
pages = {299--319},
title = {{Quasi-oracle estimation of heterogeneous treatment effects}},
volume = {108},
year = {2021}
}
@incollection{Robins2004,
abstract = {I describe two new methods for estimating the optimal treatment regime (equivalently, protocol, plan or strategy) from very high dimesional observational and experimental data: (i) g-estimation of an optimal double-regime structural nested mean model (drSNMM) and (ii) g-estimation of a standard single regime SNMM combined with sequential dynamic-programming (DP) regression. These methods are compared to certain regression methods found in the sequential decision and reinforcement learning literatures and to the regret modelling methods of Murphy (2003). I consider both Bayesian and frequentist inference. In particular, I propose a novel ``Bayes-frequentist compromise'' that combines honest subjective non- or semiparametric Bayesian inference with good frequentist behavior, even in cases where the model is so large and the likelihood function so complex that standard (uncompromised) Bayes procedures have poor frequentist performance.},
author = {Robins, James M.},
booktitle = {Proceedings of the Second Seattle Symposium in Biostatistics: Analysis of Correlated Data},
chapter = {Optimal St},
doi = {10.1007/978-1-4419-9076-1_11},
editor = {{Lin, D. Y. and Heagerty}, P. J.},
file = {:Users/pawel/Documents/UGent/Papers/seattlemay04final.pdf:pdf},
pages = {189--326},
publisher = {Springer New York},
title = {{Optimal Structural Nested Models for Optimal Sequential Decisions}},
year = {2004}
}
@article{Neyman1979,
author = {Neyman, Jerzy},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/25050174.pdf:pdf},
journal = {Sankhya},
number = {1},
pages = {1--21},
title = {{C($\alpha$) Tests and Their Use}},
volume = {41},
year = {1979}
}
@article{Chernozhukov2016,
abstract = {Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a "double ML" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods.},
archivePrefix = {arXiv},
arxivId = {1608.00060},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
eprint = {1608.00060},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/1608.00060.pdf:pdf},
title = {{Double/Debiased Machine Learning for Treatment and Causal Parameters}},
url = {http://arxiv.org/abs/1608.00060},
year = {2016}
}

@article{Kennedy2020a,
abstract = {Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations.},
archivePrefix = {arXiv},
arxivId = {2004.14497},
author = {Kennedy, Edward H.},
eprint = {2004.14497},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2004.14497.pdf:pdf},
journal = {arXiv},
pages = {1--37},
title = {{Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects}},
year = {2020}
}

@article{Foster2019,
abstract = {We provide excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target model depends on an unknown model that must be to be estimated from data (a $\backslash$nuisance model"). We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target model and one for the nuisance model. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning literature to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate the case where the target parameter belongs to a complex nonparametric class. We characterize conditions on the metric entropy such that oracle rates-rates of the same order as if we knew the nuisance model-are achieved. We also analyze the rates achieved by specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance in the literature: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data.},
archivePrefix = {arXiv},
arxivId = {1901.09036},
author = {Foster, Dylan J. and Syrgkanis, Vasilis},
eprint = {1901.09036},
file = {:Users/pawel/Library/Application Support/Mendeley Desktop/Downloaded/Foster, Syrgkanis - 2019 - Orthogonal statistical learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--86},
title = {{Orthogonal statistical learning}},
year = {2019}
}
@article{vanderLaan2013,
author = {van der Laan, Mark J},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Targeted Learning of an Optima(1).pdf:pdf},
journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
title = {{Targeted Learning of an Optimal Dynamic Treatment, and Statistical Inference for its Mean Outcome}},
year = {2013}
}

@article{Chernozhukov2018a,
abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter $\theta$0 in the presence of high-dimensional nuisance parameters $\eta$0. We depart from the classical setting by allowing for $\eta$0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate $\eta$0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating $\eta$0 cause a heavy bias in estimators of $\theta$0 that are obtained by naively plugging ML estimators of $\eta$0 into estimating equations for $\theta$0. This bias results in the naive estimator failing to be N-1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest $\theta$0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate $\theta$0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N-1 -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1111/ectj.12097},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/ectj00c1.pdf:pdf},
issn = {1368423X},
journal = {Econometrics Journal},
number = {1},
pages = {C1--C68},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Nie2021b,
abstract = {Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. We develop an “advantage doubly robust” estimator for learning such dynamic treatment rules using observational data under the assumption of sequential ignorability. We prove welfare regret bounds that generalize results for doubly robust learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not need any structural (e.g., Markovian) assumptions. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1905.09751},
author = {Nie, Xinkun and Brunskill, Emma and Wager, Stefan},
doi = {10.1080/01621459.2020.1831925},
eprint = {1905.09751},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Learning When to Treat Policies.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Causal inference,Minimax regret,Reinforcement learning,Sequential ignorability},
number = {533},
pages = {392--409},
publisher = {Taylor {\&} Francis},
title = {{Learning When-to-Treat Policies}},
url = {https://doi.org/10.1080/01621459.2020.1831925},
volume = {116},
year = {2021}
}
@article{Athey2017,
author = {Athey, Susan},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/science.aal4321.pdf:pdf},
journal = {Science},
number = {February},
pages = {483--485},
title = {{Beyond prediction: Using big data for policy problems}},
volume = {355},
year = {2017}
}
@article{Nekipelov2021,
abstract = {This paper proposes a Lasso-type estimator for a high-dimensional sparse parameter identified by a single index conditional moment restriction (CMR). In addition to this parameter, the moment function can also depend on a nuisance function, such as the propensity score or the conditional choice probability, which we estimate by modern machine learning tools. We first adjust the moment function so that the gradient of the future loss function is insensitive (formally, Neyman orthogonal) with respect to the first-stage regularisation bias, preserving the single index property. We then take the loss function to be an indefinite integral of the adjusted moment function with respect to the single index. The proposed Lasso estimator converges at the oracle rate, where the oracle knows the nuisance function and solves only the parametric problem. We demonstrate our method by estimating the short-term heterogeneous impact of Connecticut's Jobs First welfare reform experiment on women's welfare participation decision.},
archivePrefix = {arXiv},
arxivId = {1806.04823},
author = {Nekipelov, Denis and Semenova, Vira and Syrgkanis, Vasilis},
doi = {10.1093/ectj/utab022},
eprint = {1806.04823},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Plug-in{\_}Regularized{\_}Estimation{\_}of{\_}High-Dimensional.pdf:pdf},
issn = {1368-4221},
journal = {The Econometrics Journal},
number = {June 2018},
title = {{Regularised orthogonal machine learning for nonlinear semiparametric models}},
year = {2021}
}
@article{Kunzel2019,
abstract = {There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
archivePrefix = {arXiv},
arxivId = {1706.03461},
author = {K{\"{u}}nzel, S{\"{o}}ren R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
doi = {10.1073/pnas.1804597116},
eprint = {1706.03461},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/4156.full.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Observational studies,conditional average treatment effect,heterogeneous treatment effects,minimax optimality,randomized controlled trials},
number = {10},
pages = {4156--4165},
pmid = {30770453},
title = {{Metalearners for estimating heterogeneous treatment effects using machine learning}},
volume = {116},
year = {2019}
}
@article{Luedtke2016a,
abstract = {We consider the estimation of an optimal dynamic two time-point treatment rule defined as the rule that maximizes the mean outcome under the dynamic treatment, where the candidate rules are restricted to depend only on a user-supplied subset of the baseline and intermediate covariates. This estimation problem is addressed in a statistical model for the data distribution that is nonparametric, beyond possible knowledge about the treatment and censoring mechanisms. We propose data adaptive estimators of this optimal dynamic regime which are defined by sequential loss-based learning under both the blip function and weighted classification frameworks. Rather than a priori selecting an estimation framework and algorithm, we propose combining estimators from both frameworks using a super-learning based cross-validation selector that seeks to minimize an appropriate cross-validated risk. The resulting selector is guaranteed to asymptotically perform as well as the best convex combination of candidate algorithms in terms of loss-based dissimilarity under conditions. We offer simulation results to support our theoretical findings.},
author = {Luedtke, Alexander R. and van der Laan, Mark J.},
doi = {10.1515/ijb-2015-0052},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/Super-Learning{\_}of{\_}an{\_}Optimal{\_}Dynamic{\_}Treatment{\_}Rul(1).pdf:pdf},
issn = {15574679},
journal = {International Journal of Biostatistics},
keywords = {Causal inference,cross-validation,dynamic treatment,loss function,oracle inequality},
number = {1},
pages = {305--332},
pmid = {27227726},
title = {{Super-Learning of an Optimal Dynamic Treatment Rule}},
volume = {12},
year = {2016}
}
@article{Imai2013,
abstract = {When evaluating the efficacy of social programs and medical treatments using randomized experiments, the estimated overall average causal effect alone is often of limited value and the researchers must investigate when the treatments do and do not work. Indeed, the estimation of treatment effect heterogeneity plays an essential role in (1) selecting the most effective treatment from a large number of available treatments, (2) ascertaining subpopulations for which a treatment is effective or harmful, (3) designing individualized optimal treatment regimes, (4) testing for the existence or lack of heterogeneous treatment effects, and (5) generalizing causal effect estimates obtained from an experimental sample to a target population. In this paper, we formulate the estimation of heterogeneous treatment effects as a variable selection problem. We propose a method that adapts the Support Vector Machine classifier by placing separate sparsity constraints over the pre-treatment parameters and causal heterogeneity parameters of interest. The proposed method is motivated by and applied to two well-known randomized evaluation studies in the social sciences. Our method selects the most effective voter mobilization strategies from a large number of alternative strategies, and it also identifies the characteristics of workers who greatly benefit from (or are negatively affected by) a job training program. In our simulation studies, we find that the proposed method often outperforms some commonly used alternatives. {\textcopyright} 2013 Institute of Mathematical Statistics.},
author = {Imai, Kosuke and Ratkovic, Marc},
doi = {10.1214/12-AOAS593},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/svm.pdf:pdf},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Causal inference,Individualized treatment rules,LASSO,Moderation,Variable selection},
number = {1},
pages = {443--470},
title = {{Estimating treatment effect heterogeneity in randomized program evaluation}},
volume = {7},
year = {2013}
}

@article{Robins1986,
abstract = {In his published paper the author developed a formal theory of causal inference in epidemiologic studies. in which causal questions that could be asked in English concerning the study population become mathematical conjectures about the population causal parameters of a causally interpreted structured tree graph (CISTG). It was shown that for randomized CISTGs a subset of the population causal parameters, i. e. the G-causal parameters, could be empirically estimated. It is shown here that under certain assumptions, the G-causal parameters of certain CISTGs that are not R CISTGs can also be empirically estimated. Other extensions to his previous paper are given.},
author = {Robins, James M.},
doi = {10.1016/0898-1221(87)90238-0},
file = {:Users/pawel/Documents/UGent/Papers/1-s2.0-0270025586900886-main.pdf:pdf},
issn = {00974943},
journal = {Mathematical Modelling},
number = {9-12},
pages = {923--945},
title = {{A New Approach To Causal Inference in Mortality Studies With a Sustained Exposure Period - Application To Control of the Healthy Worker Survivor Effect}},
volume = {14},
year = {1986}
}
@article{Hirano2003,
abstract = {We are interested in estimating the average effect of a binary treatment on a scalar outcome. If assignment to the treatment is exogenous or unconfounded, that is, independent of the potential outcomes given covariates, biases associated with simple treatment-control average comparisons can be removed by adjusting for differences in the covariates. Rosenbaum and Rubin (1983) show that adjusting solely for differences between treated and control units in the propensity score removes all biases associated with differences in covariates. Although adjusting for differences in the propensity score removes all the bias, this can come at the expense of efficiency, as shown by Hahn (1998), Heckman, Ichimura, and Todd (1998), and Robins, Mark, and Newey (1992). We show that weighting by the inverse of a nonparametric estimate of the propensity score, rather than the true propensity score, leads to an efficient estimate of the average treatment effect. We provide intuition for this result by showing that this estimator can be interpreted as an empirical likelihood estimator that efficiently incorporates the information about the propensity score.},
author = {Hirano, Keisuke and Imbens, Guido W. and Ridder, Geert},
doi = {10.1111/1468-0262.00442},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/efficient{\_}estimation{\_}of{\_}average{\_}treatment{\_}effects{\_}using{\_}the{\_}estimated{\_}propensity{\_}score.pdf:pdf},
issn = {00129682},
journal = {Econometrica},
keywords = {Propensity score,Semiparametric efficiency,Sieve estimator,Treatment effects},
number = {4},
pages = {1161--1189},
title = {{Efficient estimation of average treatment effects using the estimated propensity score}},
volume = {71},
year = {2003}
}
@article{Chernozhukov2018,
abstract = {We provide adaptive inference methods, based on {\$}\backslashell{\_}1{\$} regularization, for regular (semiparametric) and non-regular (nonparametric) linear functionals of the conditional expectation function. Examples of regular functionals include average treatment effects, policy effects, and derivatives. Examples of non-regular functionals include average treatment effects, policy effects, and derivatives conditional on a covariate subvector fixed at a point. We construct a Neyman orthogonal equation for the target parameter that is approximately invariant to small perturbations of the nuisance parameters. To achieve this property, we include the Riesz representer for the functional as an additional nuisance parameter. Our analysis yields weak "double sparsity robustness": either the approximation to the regression or the approximation to the representer can be "completely dense" as long as the other is sufficiently "sparse". Our main results are non-asymptotic and imply asymptotic uniform validity over large classes of models, translating into honest confidence bands for both global and local parameters.},
archivePrefix = {arXiv},
arxivId = {1802.08667},
author = {Chernozhukov, Victor and Newey, Whitney and Singh, Rahul},
eprint = {1802.08667},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/1802.08667v5.pdf:pdf},
issn = {2331-8422},
keywords = {gaussian approximation,neyman orthogonality,sparsity},
number = {1959},
pages = {1--49},
title = {{De-Biased Machine Learning of Global and Local Parameters Using Regularized Riesz Representers}},
url = {http://arxiv.org/abs/1802.08667},
year = {2018}
}
@article{Athey2016,
abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without "sparsity" assumptions. We propose an "honest" approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the "ground truth" for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90{\%} confidence intervals, whereas coverage ranges between 74{\%} and 84{\%} for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7-22{\%}.},
archivePrefix = {arXiv},
arxivId = {1504.01132},
author = {Athey, Susan and Imbens, Guido},
doi = {10.1073/pnas.1510489113},
eprint = {1504.01132},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/7353.full.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Causal inference,Cross-validation,Heterogeneous treatment effects,Potential outcomes,Supervised machine learning},
number = {27},
pages = {7353--7360},
pmid = {27382149},
title = {{Recursive partitioning for heterogeneous causal effects}},
volume = {113},
year = {2016}
}
@article{Neyman1923,
author = {Neyman, Jerzy},
file = {:Users/pawel/Documents/UGent/Papers/euclid.ss.1177012031.pdf:pdf},
journal = {Roczniki Nauk Rolniczych Tom X},
pages = {1--51},
title = {{On the Application of Probability Theory to Agricultural Experiments}},
year = {1923}
}
@article{Hahn1998,
author = {Hahn, Jinyong},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2998560.pdf:pdf},
journal = {Econometrica},
number = {2},
pages = {315--331},
title = {{On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects Author ( s ): Jinyong Hahn Published by : The Econometric Society Stable URL : http://www.jstor.org/stable/2998560 REFERENCES Linked references are ava}},
volume = {66},
year = {1998}
}
@article{Robins1995,
abstract = {We consider the efficiency bound for the estimation of the parameters of semiparametric models defined solely by restrictions on the means of a vector of correlated outcomes, Y, when the data on Y are missing at random. We show that the semiparametric variance bound is the asymptotic variance of the optimal estimator in a class of inverse probability of censoring weighted estimators and that this bound is unchanged if the data are missing completely at random. For this case we study the asymptotic performance of the generalized estimating equations (GEE) estimators of mean parameters and show that the optimal GEE estimator is inefficient except for special cases. The optimal weighted estimator depends on unknown population quantities. But for monotone missing data, we propose an adaptive estimator whose asymptotic variance can achieve the bound. {\textcopyright} 1995 Taylor {\&} Francis Group, LLC.},
author = {Robins, James M. and Rotnitzky, Andrea},
doi = {10.1080/01621459.1995.10476494},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/2291135(1).pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Correlated outcomes,Generalized estimating equations,Generalized least squares,Longitudinal studies,Missing at random},
number = {429},
pages = {122--129},
title = {{Semiparametric efficiency in multivariate regression models with missing data}},
volume = {90},
year = {1995}
}
@article{Kitagawa2018,
abstract = {One of the main objectives of empirical analysis of experiments and quasi-experiments is to inform policy decisions that determine how treatments are allocated to individuals with different observable covariates. We propose the Empirical Welfare Maximization (EWM) method, which estimates a treatment assignment policy by maximizing the sample analog of average social welfare over a class of candidate treatment policies. We show that, when propensity score is known, the average social welfare attained by EWM rules converges at least at n{\{}{\^{}}{\}}{\{}{\{}{\}}-1/2{\{}{\}}{\}} rate to the maximum obtainable welfare. This holds uniformly over a minimally constrained class of data distributions, and this uniform convergence rate is minimax optimal. In comparison with this benchmark rate, we examine how the uniform convergence rate of the average welfare improves or deteriorates depending on the richness of the class of candidate decision rules, on the distribution of conditional treatment effects, on the lack of knowledge for the propensity score, and on additional smoothness assumptions for the regression functions or propensity scores. We also discuss practically implementable computation for the EWM rule. As an empirical application, we derive an EWM rule for the a training program using the experiment data analyzed in La Londe (1986).},
author = {Kitagawa, Toru and Tetenov, Aleksey},
doi = {10.3982/ecta13288},
file = {:Users/pawel/Documents/UGent/MyPapers/Counterfactual{\_}prediction/Papers/CATE/kitagawa{\_}tetenov{\_}ecta2018.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {2},
pages = {591--616},
title = {{Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice}},
volume = {86},
year = {2018}
}

@book{Hernan2020,
abstract = {Causal Inference is an admittedly pretentious title for a book. Causal inference is a complex scientific task that relies on triangulating evidence from multiple sources and on the application of a variety of methodological approaches. No book can possibly provide a comprehensive description of methodologies for causal inference across the sciences. The authors of any Causal Inference book will have to choose which aspects of causal inference methodology they want to emphasize. The title of this introduction reflects our own choices: a book that helps scientists–especially health and social scientists–generate and analyze data to make causal inferences that are explicit about both the causal question and the assumptions underlying the data analysis. Unfortunately, the scientific literature is plagued by studies in which the causal question is not explicitly stated and the investigators' unverifiable assumptions are not declared. This casual attitude towards causal inference has led to a great deal of confusion. For example, it is not uncommon to find studies in which the effect estimates are hard to interpret because the data analysis methods cannot appropriately answer the causal question (were it explicitly stated) under the investigators' assumptions (were they declared).},
author = {Hern{\'{a}}n, Miguel A. and Robins, James M.},
booktitle = {Boca Raton: Chapman {\&} Hall/CRC},
file = {:Users/pawel/Desktop/ciwhatif{\_}hernanrobins{\_}31jan21.pdf:pdf},
publisher = {Boca Raton: Chapman {\&} Hall/CRC},
title = {{Causal Inference - what if}},
year = {2020}
}
@article{Kosorok2019,
author = {Kosorok, Michael and Laber, Eric B},
doi = {10.1377/hlthaff.2018.0520},
file = {:Users/pawel/Documents/UGent/Papers/nihms-983058.pdf:pdf},
issn = {15445208},
journal = {Annual Review of Statistics and Its Application},
number = {5},
pages = {263--286},
pmid = {29733714},
title = {{Precision medicine}},
volume = {6},
year = {2019}
}
