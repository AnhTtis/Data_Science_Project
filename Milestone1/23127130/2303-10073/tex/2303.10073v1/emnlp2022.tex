% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2022}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{booktabs}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{DialogPaint: A Dialog-based Image Editing Model}
\author{
  Jingxuan Wei\textsuperscript{2,3\textdagger} , Shiyu Wu\textsuperscript{2,4\textdagger},
  Xin Jiang\textsuperscript{1} , Yequan Wang\textsuperscript{1}\thanks{Corresponding author} \\
  \textsuperscript{1}Beijing Academy of Artificial Intelligence, Beijing, China \\
  \textsuperscript{2}University of Chinese Academy of Sciences, Beijing, China \\
  \textsuperscript{3}Shenyang Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China \\
  \textsuperscript{4}Institute of Automation, Chinese Academy of Sciences, Beijing, China \\
  \texttt{weijingxuan20@mails.ucas.edu.cn, wushiyu2022@ia.ac.cn} \\
  \texttt{jiangxin@baai.ac.cn, tshwangyequan@gmail.com}
}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\begin{document}
\maketitle
\begin{abstract}

  We present DialogPaint, an innovative framework that employs an interactive conversational approach for image editing.
  %We propose the dialogue-based image editing task, which can be a common scenario in human-computer interaction. 
  The framework comprises a pretrained dialogue model (Blenderbot) and a diffusion model (Stable Diffusion).
  The dialogue model engages in conversation with users to understand their requirements and generates concise instructions based on the dialogue. Subsequently, the Stable Diffusion model employs these instructions, along with the input image, to produce the desired output.
  Due to the difficulty of acquiring fine-tuning data for such models, we leverage multiple large-scale models to generate simulated dialogues and corresponding image pairs. After fine-tuning our framework with the synthesized data, we evaluate its performance in real application scenes. The results demonstrate that DialogPaint excels in both objective and subjective evaluation metrics effectively handling ambiguous instructions and performing tasks such as object replacement, style transfer, color modification. Moreover, our framework supports multi-round editing, allowing for the completion of complicated editing tasks.
  
\end{abstract}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Equal contribution.}
\renewcommand{\thefootnote}{\arabic{footnote}}

\section{Introduction}
Recently, great progress has been achieved in the field of image generation through the use of diffusion models \cite{RN3, RN4, RN5, RN8, RN9}. These large-scale text-to-image models have enabled the synthesis of high-quality, diverse images using concise textual prompts. Owing to their vivid output and stable training performance, diffusion models have surpassed generative adversarial networks (GAN)\cite{RN6} in popularity. Consequently, an increasing number of individuals are utilizing various diffusion models for a wide array of tasks, engaging in the creation of personalized images.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{img_bird.pdf}
\end{center}
\caption{An example of interactive editing}
\label{fig:img_bird}
\end{figure}
Although image generation has become increasingly accessible to the general public, other tasks, such as image editing, remain considerably challenging. 
% According to our investigation, people prefer making changes to existing image rather than generating another new image of same semantics. 
% 上面这个结论看起来有点武断。

This preference can be explained by the continuity of thought, as altering a portion of a scene aligns more closely with human cognition. Utilizing natural language instructions to edit images has proven to be both helpful and appealing to users. Current methods for semantic image editing using generative models are abundant and diverse \cite{RN7}. However, these methods often prove unsuitable for human-computer interaction or rely heavily on personal adjustments. 
Based on an in-depth analysis of this issue, we identified two primary limitations that significantly impact the editing performance of these models. First, most text-to-image models struggle to process human instructions, a phenomenon we term "instruction unfriendly." This arises because these models are predominantly trained on declarative sentences, making imperative sentences, such as instructions, unfamiliar to them. Second, individuals frequently provide models with ambiguous instructions, which can lead to confusion regarding sources and targets in the absence of a given image. Some users simply employ vague phrases like "something else" to refer to the target, which existing models find difficult to interpret. To address these challenges, we propose using dialogues to clarify instructions. By engaging in conversation with large language models, precise commands can be extracted and subsequently employed to guide image generation.

In this study, we introduce a user-friendly approach to image editing through natural language conversation. Upon providing our model with an input image, users can engage in a dialogue with the model to convey their editing requirements. Our model is capable of discerning user needs and formulating a concise instruction to edit the image accordingly. We employ two models, a dialogue model \cite{RN11} and a image generation model \cite{RN9} to complete this task. The dialogue model is used to converse with the user and, if the user's input instruction is deemed ambiguous, it seeks clarification through additional questions. Finally, the language model generates a clear instruction, which the image generation model utilizes, along with the input image, to produce a new, edited image. 
Since there are no suitable existing datasets for fine-tuning these two large models, we adopted the approach of self-instruct \cite{RN29} and generate simulated dialogues and image pairs for fine-tuning purposes. Results demonstrate that our model achieves zero-shot generalization in real-world application scenarios. Our model is capable of performing various edits, including object replacement, style transfer, and color alteration. A demo of our project is made publically available, with an interactive editing example presented in Figure \ref{fig:img_bird}. 
%In practical applications, we replaced command phrases like "forget it" with an interactive interface that allows users to freely select any image from the dialogue process for editing. 
%Command summaries, such as "change the seasons to winter", will not be directly displayed to the user, but rather responded to with a sentence like "Here it is."

% Our contributions are summarized as follows:
% \begin{itemize}
%     \item We propose a new task, dialogue-based image editing, which effectively combines dialogue understanding with image editing to achieve precise modifications through natural language instructions.

% \itemWe construct a dataset containing both dialogue and image editing samples, which enables the fine-tuning of our integrated model to handle diverse image editing tasks.

% \itemWe conduct extensive experiments that demonstrate the strong performance of our model in both objective and subjective evaluation metrics, highlighting its applicability and potential in various domains.

% \end{itemize}

\section{Related Work}

\subsection{Large Language Models}
Large language models \cite{RN13, RN11, RN14, RN15} is able to chat with humans fluently, which have been widely studied in recent years. These models, such as GPT-3 \cite{RN15}, have the ability to generate simulated data according to given samples, which is a convenient way to gather language data in a specific format and fine-tune other language models. Furthermore, conversation-oriented language models have also received attention.

DialoGPT \cite{DBLP:conf/acl/ZhangSGCBGGLD20} is an open-domain conversation model that generates high-quality human-like responses in conversations. It uses large-scale generation models to achieve this. Meena \cite{DBLP:journals/corr/abs-2001-09977} is a chatbot developed by Google that aims to train more conversations and empathy in human interactions by utilizing large-scale conversation models. BlenderBot \cite{DBLP:conf/eacl/RollerDGJWLXOSB21} is a conversation agent trained on different conversation datasets from social media platforms, and it can converse on a wide range of topics. ChatGPT (\url{https://openai.com/blog/chatgpt}) is a large-scale language model developed by OpenAI for generating high-quality text in a conversational context. It has shown exceptional performance on various conversational tasks.

Currently, BlenderBot and its subsequent open-source versions are popular in the field of conversation, while ChatGPT only has an API port available for now.
% Nowadays, large language models \cite{RN13, RN11, RN14, RN15} is able to chat with humans fluently. Current models such as GPT-3 \cite{RN15}
% can generate simulated data according to the given samples. This can be a pretty convenient way to gather data of languages
% in special format and finetune other language models. 
% Furthermore, various conversation-oriented language models are gradually receiving attention. DialoGPT is a large-scale generation model that can generate high-quality human-like responses in open-domain conversations \cite{DBLP:conf/acl/ZhangSGCBGGLD20}. Meena is a chatbot developed by Google that aims to train more conversations and empathy in human interactions by utilizing large-scale conversation models \cite{DBLP:journals/corr/abs-2001-09977}. BlenderBot is a conversation agent trained on different conversation datasets from social media platforms, with the goal of being able to converse on a wide range of topics \cite{DBLP:conf/eacl/RollerDGJWLXOSB21}. ChatGPT is a large-scale language model developed by OpenAI for generating high-quality text in a conversational context, and has shown exceptional performance on various conversational tasks (\url{https://openai.com/blog/chatgpt}). Currently, BlenderBot and its subsequent open-source versions are popular in the field of conversation, while ChatGPT only has an API port available for now.

\subsection{Diffusion Models}

Diffusion model \cite{RN3, RN4, RN5, RN8, RN9} is a new kind of generative models which generate images from Gaussian noise by progressively denoising it. 
The model gradually adds noise to the input image by a preseted noise adding method which is named as forward process. And 
then it uses a deep neural network to restore the original image, which is called sampling process. As the dimensions of 
latent space in diffusion models can be really high, the output images can be very fantastic with high quality and diversity. 
Usually, a diffusion model is trained on the following variant of the variational bound:
\begin{equation}
    L_{simple} = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{c}, \boldsymbol{\epsilon}, t} \left( || \boldsymbol{\epsilon} - 
    \boldsymbol{\epsilon}_\theta (\mathbf{x}_0, t, \boldsymbol{c}) ||^2_2 \right)
\end{equation}
where $\mathbf{x}_0$ and $\boldsymbol{c}$ are input images and optional conditions, $t \sim \mathcal{U} (0, 1)$ are time 
steps and $\boldsymbol{\epsilon} \in \mathcal{N} (0, 1)$ are added gaussian noise in forward process. $\boldsymbol{\epsilon}_
\theta$ is a learnable neural network which predicts the noise added on the image of previous moment. Usually a UNet \cite{RN16} is used 
to quickly and efficiently do this job. conditions $\boldsymbol{c}$ are the semantic embeddings of the input sentences processed 
by CLIP \cite{RN17}. With sampling methods such as DDIM \cite{RN5} and DPM-Solver \cite{RN18, RN19} that can speed up the sampling process Conspicuously, diffusion models 
is able to synthesize an image in $15 \sim 25$ steps. The widly used stable diffusion applies an AutoEncoder, which encodes the
input image $\mathbf{x}$ into a latent space first and decode the sampling output to an real image. In this case, diffusion models 
will not deal with high-frequency details and can synthesis images with higher quality. 

\subsection{Text-driven Image Editing}

Text-driven editing with GAN \cite{RN20, RN21, RN22, RN31, RN32} has been carefully studied in recent years. Early works targeted at single task like style transferring. 
They trained the model with special image pairs to complete special editing tasks, which is based on domain transferring. With the 
advent of CLIP, now people can guide image editing with texts as input conditions. As for diffusion models, some of them \cite{RN23, RN30} natively 
have the ability for editing images due to the strong capabilities of text features extraction by CLIP\cite{RN17}. Models with mask guidance \cite{RN28} can 
make the edit more accurately. Another editing way is using textual inversion \cite{RN25, RN24}. Models will learn a speical word in textual embedding 
space and bind it with the specific subject in the given image. After training, with a sentence that contains the special word, the 
diffusion model can generate images of the specific subject in different scenes described by the sentence. 

\section{Methodology}

\begin{figure*}[h]
\begin{center}
\includegraphics[width=\linewidth]{img_all_data.pdf}
\end{center}
\caption{The processing of dialogue and image editing dataset construction}
\label{fig:all_ataset}
\end{figure*}
We propose a new task: dialogue-based image editing. Our system receives images and editing instructions from users in the form of dialogue. The system actively solicits clarifications and provides feedback and summaries to complete the dialogue-based image editing. For this task, we first introduce a multi-turn dialogue-based image editing dataset (Sec. \ref{3.1}). Then, based on the generated dataset, we construct the dialogue-based image editing model (Sec. \ref{3.2}).

\subsection{Construction of Dialogue and Image Editing Datasets}
\label{3.1}

\subsubsection{Building Dialogue Dataset}
\label{3.1.1}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{image_dialogue.pdf}
\end{center}
\caption{Example of Dialogue Dataset Construction }
\label{fig:Dialogue_Dataset}
\end{figure}
The construction of the dialogue and image editing datasets is shown in Figure \ref{fig:all_ataset}, which includes Building Dialogue Dataset and Building Image Editing Dataset. For Building Dialogue Dataset, we randomly selected image captions from CUB-200-2011 \citep{DBLP:journals/tcsv/HeP20}, Microsoft COCO \citep{DBLP:conf/eccv/LinMBHPRDZ14}, DeepFashion \citep{DBLP:conf/cvpr/LiuLQWT16}, and Flickr-Faces-HQ (FFHQ) \citep{DBLP:journals/pami/KarrasLA21} datasets, and combined them with prompt instructions to generate the necessary dialogue data. For Building Image Editing Dataset, we randomly selected image-text pairs from the same datasets, and input 
the texts into text-davinci-003 to generate editing instructions. These datasets were chosen due to their diversity and prevalence in the computer vision community. 
Regarding the construction of dialogue datasets, we randomly selected 10,000 image captions from four different datasets: CUB-200-2011, Microsoft COCO, DeepFashion, and FFHQ. Using self-instruct \citep{RN29}, we combined these image captions with prompt instructions and input them into text-davinci-003 to generate the necessary dialogue data. The overall process of constructing the dialogue dataset is shown in Step 1 of Figure \ref{fig:all_ataset}.

As shown in the example in Figure \ref{fig:Dialogue_Dataset}, we first defined a Prompt Head to describe the dialogue generation task. Here, we instructed text-davinci-003 to "generate a dialog about a user ordering the system to edit data of the image based on the given image caption." Then, we randomly selected 20 manually written dialogue examples from a sample library containing 500 dialogue examples, as shown in the Example section of Figure \ref{fig:Dialogue_Dataset}. The Example section consists of two parts: the "Caption" which is the input image caption, and the "Dialog" which is the desired dialogue that includes simulating multiple rounds of conversation for modifying the image's colors, scenes, and other changes, as well as correcting fuzzy instructions. Finally, we concatenated an image caption after the Examples section and input the whole string into text-davinci-003. The response generated by text-davinci-003 was used to create the desired dialogue data.

Using the dialogue dataset construction method described above, we obtained a total of 10,000 dialogue data samples that meet the needs of open-domain dialogue image editing, including various modifications to people, objects, backgrounds, etc. mentioned in the image captions.


\subsubsection{Building Image Editing Dataset}

The overall process for building the image editing dataset is shown in Step 2 and Stage 2 in Figure \ref{fig:all_ataset}. The process is divided into two parts: Step 2 generates image editing instructions, and Stage 2 uses existing text-to-image editing models to generate edited images based on the text data generated in Step 2. In Step 2, we randomly selected 10,000 image-text pairs from the CUB-200-2011, Microsoft COCO, DeepFashion, and FFHQ datasets. Here, the text refers to the image caption, and we used self-instruct\citep{RN29} to input the prompt instructions and image captions together into the text-davinci-003 model to generate the desired text editing instructions. The generated data was then fed into the text-to-image editing model in Stage 2 to obtain the edited images.

In Step 2, similar to Figure \ref{fig:Dialogue_Dataset}, we first defined a Prompt Head to describe the task of generating the dialogues. We told the text-davinci-003 model, "The following is the automatic generation of modification instructions based on caption, as well as the generation of new sentences to add modification instructions. Modification instructions are not limited to human editing, but can be any object, anything...". We then randomly selected 20 human-written editing instructions from a sample library containing 500 editing instruction examples. An example of such an instruction is shown in Figure \ref{fig:Image_Editing_Dataset} as "Example", which is divided into three parts: "Image Caption" as the input image caption, "Modify Instructions" as the editing instructions for modifying anything, and "Edited Captions" as the output generated by using the editing instructions as prompt to modify the caption. For example, given the image caption "A cat laying on top of a wooden bench.", the Modify Instructions "Change the cat to a yellow labrador.", the Edited Captions would be "A yellow Labrador laying on top of a wooden bench." Finally, we concatenated an image caption with the Examples, inputted it into the text-davinci-003 model, and obtained the desired Modify Instructions and Edited Captions data as the response.

Note that we generated some datasets here for partial transformation and object isolation of images to be used for fine-tuning the model in Section \ref{3.2.2}.

In Stage 2, we organize the editing instruction data generated in Step 2 and use multiple pre-trained models with Image Captions, Modify Instructions, Edited Captions to generate edited images. Inspired by previous works, we use four text-to-image editing models, including Prompt-to-Prompt \citep{RN26}, DE-Net \citep{DBLP:journals/corr/abs-2206-01160}, Text2Human \citep{DBLP:journals/tog/JiangYQWLL22}, and StyleMC \citep{DBLP:conf/wacv/KocasariDTY22}. As shown in Figure \ref{fig:all_ataset} Stage 2, Prompt-to-Prompt takes Image Captions and Edited Captions as input to generate the original and edited images simultaneously, as it cannot directly modify the editing instruction. DE-Net, Text2Human, and StyleMC take Image Captions, Modify Instructions, and the original image as input to generate the edited image. After generating images using these four models, we filter them using the CLIP-based metric that measures the similarity change between the original and edited images. This approach maximizes the reduction of noise in the dataset and ensures data quality.

Finally, after filtering, we obtain a total of 6468 pairs of original image-text and edited image-text pairs, which satisfies the requirements of common open-domain image editing tasks.

\label{3.1.2}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{img_edit.pdf}
\end{center}
\caption{Example of Image Editing Dataset}
\label{fig:Image_Editing_Dataset}
\end{figure}

\subsection{Construction of Dialogue and Image Editing Models}
\label{3.2}
The Construction of Dialogue and Image Editing Models module consists of two parts: Dialogue Model Construction and Image Editing Model Construction. We fine-tuned the BlenderBot and Stable Diffusion models, respectively, on our newly generated Dialogue Editing dataset. The BlenderBot model was used for generating context-aware dialogue responses, while the Stable Diffusion model was used for image editing tasks based on explicit textual instructions. The module aims to facilitate the generation of more realistic and coherent dialogues, as well as the generation of high-quality edited images based on natural language instructions. This module addresses the challenges of dialogue and image editing tasks and provides a practical solution for generating high-quality content. 
\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{img_model.pdf}
\end{center}
\caption{Model Architecture for Dialogue-Based Image Editing}
\label{fig:img_model}
\end{figure*}

\subsubsection{Dialogue Model Construction}
\label{3.2.1}
Our task is to generate open-domain conversations for image editing, where the model takes in a natural language prompt describing the image and an editing task, and generates a response dialogue that leads to the final edited image. This is achieved through a single or multiple rounds of dialogue, resulting in a clear instruction that guides the image editing module.

To accomplish this task, we fine-tuned the Blender dialogue model \citep{DBLP:conf/eacl/RollerDGJWLXOSB21}. The response generation is formulated as maximizing the probability of generating a response given the prompt, which can be expressed as:
\begin{equation}
P(\text{response}|\text{prompt})=\prod_{i=1}^{n} P(w_i|w_{<i},\text{prompt})
\end{equation}

where $w_i$ denotes the $i$-th word in the response, $w_{<i}$ denotes the words before $w_i$, and $n$ is the length of the response.

During training, we minimize the negative log-likelihood loss:
\begin{equation}
\mathcal{L}=-\log P(\text{response}|\text{prompt})
\end{equation}

To fine-tune our model for the task, we utilized our own dialogue dataset consisting of multi-turn dialogues for image editing. The objective of fine-tuning is to generate high-quality responses given a dialogue history $x$, with the aim of producing explicit editing instructions. Response generation is formulated as maximizing the probability of generating a response $y$ given a dialogue history $x$, which can be expressed as:

\begin{equation}
\arg\max_{y} P(y|x) = \arg\max_{y} \prod_{t=1}^{T} P(y_t|y_{<t},x)
\end{equation}

where $T$ is the length of the generated response, and $y_{<t}$ denotes the previously generated tokens.

\subsubsection{Image Editing Model Construction}
\label{3.2.2}
Our image editing model is fine-tuned based on the Stable Diffusion model \cite{RN26} to perform image editing according to explicit instructions from a dialogue model. We use the diffusion model to fine-tunes a pre-trained stable diffusion model on an image editing dataset to learn a network that predicts the noise to be added to the latent image based on text instructions. Specifically, we minimize the following diffusion target:
% \begin{equation}
% \begin{split}
% \left.L=\mathbb{E}_{\mathcal{E}(x), \mathcal{E}\left(c_I\right), c_T, \epsilon \sim \mathcal{N}(0,1), t}\left[\| \epsilon-\epsilon_\theta\left(z_t, t, \mathcal{E}\left(c_I\right), c_T\right)\right) \|_2^2\right]
% \end{split}
% \end{equation}
\begin{equation}
\resizebox{0.9\linewidth}{!}{$
\begin{aligned}
\left.L=\mathbb{E}_{\mathcal{E}(x), \mathcal{E}\left(c_I\right), c_T, \epsilon \sim \mathcal{N}(0,1), t}\left[\| \epsilon-\epsilon_\theta\left(z_t, t, \mathcal{E}\left(c_I\right), c_T\right)\right) \|_2^2\right]
\end{aligned}
$}
\end{equation}

Here, $E$ and $D$ are the encoder and decoder of a pre-trained variational autoencoder in the stable diffusion model, $x$ is the input image, $c_{I}$ is the image adjustment, and $c_{T}$ is the text instruction adjustment. The diffusion process injects noise $z_t = E(x)$ into the encoded latent image, producing a noise latent image $z_t$ with increasing noise levels over time steps $t \in T$. Given the conditions $c_I$ and $c_T$, the network $\theta$ predicts the noise added to the noise latent potential $z_t$.

In addition, As with InstructPix2Pix, we added an extra input channel in the first convolutional layer, connecting $z_t$ and $E(c_I)$ to support image adjustment. Unconditional diffusion guidance was also introduced, using two guidance scales $s_I$ and $s_T$ to adjust the trade-off between the correspondence with the input image and the editing instruction. The modified score estimation is given as follows.
\begin{equation}
\resizebox{0.9\linewidth}{!}{$
\begin{aligned}
\tilde{e_\theta}\left(z_t, c_I, c_T\right)= & e_\theta\left(z_t, \varnothing, \varnothing\right) \\
& +s_I \cdot\left(e_\theta\left(z_t, c_I, \varnothing\right)-e_\theta\left(z_t, \varnothing, \varnothing\right)\right) \\
& +s_T \cdot\left(e_\theta\left(z_t, c_I, c_T\right)-e_\theta\left(z_t, c_I, \varnothing\right)\right)
\end{aligned}
$}
\end{equation}

\section{Experiments}

\subsection{Experimental Setup}
We applied our model on two sets of data: 10,000 dialogue samples and 6,468 filtered image editing samples. For the dialogue model, we used 9,000 samples for training, 500 for validation, and 500 for testing. For the image editing model, we used 5,868 samples for training, 300 for validation, and 300 for testing. We fine-tuned BlenderBot using the version described in \cite{RN11}. on the dialogue data, and fine-tuned our Stable Diffusion using the model provided by \url{https://huggingface.co/timbrooks/instruct-pix2pix/tree/main} on the image editing data. Both fine-tuning processes were performed on 8 Nvidia Tesla A100 40G GPUs.

When fine-tuning BlenderBot, we set the batch size to 128, embedding size to 2560, ffn size to 10240, n-heads to 32, n-positions to 128, n-encoder-layers to 2, n-decoder-layers to 24, dropout to 0.1, used the Adam optimizer with a learning rate of 7e-06, and implemented early stopping if there was no improvement for 10 consecutive iterations. For Stable Diffusion, we set the batch size to 32, input image size to 256, and kept the diffusion model configuration and no-classifier guidance algorithm the same as in the original paper. We fine-tuned the model for 125 epochs.

Finally, we fixed the parameters of the fine-tuned dialogue and image editing models and connected them according to the architecture shown in Figure \ref{fig:img_model}, resulting in the implementation of our dialogue-based image editing system.

\subsection{Qualitative Analysis of Experimental Cases}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{img_comp.pdf}
\end{center}
\caption{Comparison of Performance between DialogPaint and InstructPix2Pix under Same Precise Editing Instructions}
\label{fig:img_comp}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Dialogue-based Fashion Transformation 1]{
\includegraphics[width=\linewidth]{img_woman1.pdf}
\label{fig:image1}
}\hfill
\subfloat[Dialogue-based Fashion Transformation 2]{
\includegraphics[width=\linewidth]{img_fashion_woman2.pdf}
\label{fig:image2}
}\hfill
\subfloat[Dialogue-based Fashion Transformation 3]{
\includegraphics[width=\linewidth]{img_3.pdf}
\label{fig:image3}
}\hfill
\caption{Fashion Transformation using Dialogue-guided Image Editing Model.In each round of the example shown, we demonstrate various image editing instructions using clear and specific commands.}
\label{fig:overall}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[Animal Transformation using Dialogue-based Image Editing Model]{
\includegraphics[width=\linewidth]{img_animal_5.pdf}
\label{fig:image5}
}\hfill
\subfloat[Scene Transformation using Dialogue-based Image Editing Model]{
\includegraphics[width=\linewidth]{img_6.pdf}
\label{fig:image6}
}\hfill
\subfloat[Fruit Transformation using Dialogue-based Image Editing Model]{
\includegraphics[width=\linewidth]{img_8.pdf}
\label{fig:image7}
}\hfill
\caption{Multi-modal Image Editing using Dialogue-based Approach. Here, various editing operations are demonstrated using explicit editing instructions.}
\label{fig:overall2}
\end{figure}

In order to evaluate the performance of our proposed dialog-based image editing model, we compared it to InstructPix2Pix, which is the baseline model in this paper, under the same precise single-turn instructions, and compared the results of the two models.  The comparison results are shown in Figure \ref{fig:img_comp}.  It can be observed that for InstructPix2Pix, when the input image undergoes significant transformations, the model exhibits overfitting problems, resulting in the loss of fine-grained background information from the original scene when changing the scene.  For DialogPaint, the model can retain fine-grained background information while completing the image transformation.  In addition, for InstructPix2Pix, the model has difficulty isolating the specified object.  For example, when given the instruction "Just turn eyes blue" the model cannot isolate the specified object, resulting in not only the eyes of the person but also a corner of their clothing becoming blue.  When using DialogPaint, better object isolation can be achieved.

We attribute the model's ability to the facts that we avoid over-fitting and isolate specified objects in the dataset we provided. Our dataset contained more fine-grained images, and the descriptions were clearer, making it easier for the model to learn fine-grained knowledge and achieve precise transformation.

To further verify the ability of our proposed dialogue-based image editing model, we conducted multiple rounds of dialogue-based testing, as shown in Figure \ref{fig:overall}. We conducted over 15 rounds of image transformations, including continuous transformations and mid-image replacement according to the given instructions. In Figure \ref{fig:image1}, we started with a fashion image and changed the color of different parts of the person's body and clothing, and added items to the image, achieving continuous editing. In Figure \ref{fig:image2}, we continued the previous dialogue and made more fine-grained changes, such as changing the style and color of the person's clothing. In Figure \ref{fig:image3}, we continued the previous dialogue and tried to delete and modify objects in the original image, fully demonstrating the precise editing capability of our model through dialogue.

Furthermore, to further verify the dialogue-based editing capability of our model, we conducted multi-domain and multi-round testing in different fields, such as animals (Figure \ref{fig:image5}), scenes (Figure \ref{fig:image6}), and fruits (Figure \ref{fig:image7}). The results showed that our model can achieve precise image editing through dialogue in different domains, meeting user expectations.

\subsection{Quantitative Analysis of Evaluation Metrics}
\begin{table}[h]
  \centering
  \caption{Evaluation Metrics for the Dialogue-based Image Editing Model}
    \begin{tabular}{lc}
    \toprule
    Evaluation Metric & Score \\
    \midrule
    Perplexity (ppl) & 1.578 \\
    Fréchet Inception Distance (FID) & 1.52 \\
    Precision-Recall Distance (PRD) & 1.56 \\
    Overall Satisfaction & 4.22 \\
    Mean Opinion Score (MOS) & 4.32 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%
Based on the evaluation of our proposed dialogue-based image editing model, we have used a combination of objective and subjective metrics to assess its performance. The results are shown in Table \ref{tab:addlabel}.

For objective metrics, we have used perplexity (ppl), Fréchet Inception Distance (FID), and Precision and Recall Distance (PRD). Perplexity measures the model's ability to predict the next word in a dialogue sequence, with lower values indicating better performance. FID compares the generated images to real images based on their statistics, with lower values indicating better image quality. PRD compares the distributions of generated and real images in a feature space, with lower values indicating a better match between the distributions.

For subjective metrics, we asked 100 participants to use the dialogue editing model and rate their overall satisfaction on a scale of 1 to 5. The average rating for overall satisfaction was 4.22, indicating a high level of satisfaction with the model's performance. Additionally, we asked participants to rate the quality of the generated images on a Mean Opinion Score (MOS) scale of 1 to 5, and the average MOS was 4.32, indicating a high level of image quality.

Overall, our dialogue-guided image editing model demonstrates its high performance in terms of both objective and subjective evaluation metrics, which indicates its potential for real-world applications.

\subsection{Model Limitations}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{fail_img.pdf}
\end{center}
\caption{Qualitative and Quantitative Comparison of Performance Improvement through Image Editing before and after Fine-tuning}
\label{fig:fail_img}
\end{figure}
We introduced the task of dialog-based image editing and demonstrated the capability of our proposed dialog-based image editing model. Although our method can achieve image editing through dialog by providing explicit instructions, removing ambiguous instructions, summarizing context, and making stylistic, color, and other changes to the image, there are still some limitations.

Due to the limited number of dialog samples and image editing operations in the current dataset, our model exhibits some limitations when dealing with complex dialog-based image editing tasks. For example, when explicit instructions are given through dialog, the image editing effect may be unsatisfactory due to the complexity of the original image content, as shown in the failed examples in Figure \ref{fig:fail_img}.

In Figure \ref{fig:fail_img}, we present some examples of insufficient precision in fine-grained changes, such as in the top left corner, where the instruction is to change the color of the upper body's tank top to black, but the color of the entire body is changed instead. This indicates that local color processing is still not sophisticated enough.

We will continue to improve the precision of the dialogue-based image editing instructions and the image editing module to gradually overcome these limitations.

\section{Conclusion}

In this paper, we present a dialogue-based image editing model that enables image modification through explicit instructions in a conversation. To facilitate this, we constructed a dataset containing both dialogue and image editing, and conducted fine-tuning using dialogue and image generation models. Our experimental results indicate that the proposed model exhibits strong performance in both objective and subjective evaluation metrics, showcasing its dialogue-based image editing capabilities across various domains. However, due to the limited number of dialog samples and image editing operations present in the dataset, the model currently faces challenges in handling complex editing tasks. Moving forward, we plan to enhance the performance of both the instruction and image editing components in order to progressively mitigate these limitations.

In future research, we will further explore the potential of dialogue-based image editing models and attempt to apply them to a wider range of fields, such as smart homes and facial recognition. We will also endeavor to refine our dataset and collect more dialog samples with more diverse operations to enhance our model's performance. 
% Furthermore, we will attempt to apply our model to practical scenarios, such as online image editing applications or product design and user interaction. 
% 这个有必要说吗？
Furthermore, we will investigate other ways to combine our model with other models or technologies to achieve more efficient image editing and processing.

\nocite{*}

% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
\bibliographystyle{acl_natbib}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is a section in the appendix.

\end{document}
