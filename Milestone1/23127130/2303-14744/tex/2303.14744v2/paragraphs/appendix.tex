\input{tables/dataset_comparison}

\textbf{Analysis of the dataset used for pre-training.} 
Table~\ref{tb:compare_dataset} describes the performance of ResNet50 models pre-trained with different datasets or data augmentation, where we apply weight regularization to train these models. Generally, pre-training on diverse data makes the model generalize well on OOD datasets. Also, improvements over the FT baseline by the regularization get more significant in the pre-training. In other words, pre-trained on diverse data, the model will likely forget the learned representations with standard training.


\input{figures/lamda.tex}
\textbf{ID-OOD Trade-off by the coefficient.}
Fig.~\ref{fig:trade-off} shows a trade-off in ID-and-OOD performance controlled by the regularization coefficient, $\lambda$ in Eq. ~\ref{eq:objective}, which is measured using Pascal with ResNet50-Instagram. Increasing $\lambda$ puts more regularization to keep the parameter near the initial model, reflected as the performance decrease in ID (Pascal). Clipart and Comic increase the performance with more regularization, while Watercolor peaks near 0.1. The peak of OOD performance should differ by the similarity with ID dataset.