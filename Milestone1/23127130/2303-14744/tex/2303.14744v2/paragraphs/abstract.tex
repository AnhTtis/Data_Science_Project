%%%%%%%%% ABSTRACT

\begin{abstract}
Building object detectors that are robust to domain shifts is critical for real-world applications. Prior approaches fine-tune a pre-trained backbone and risk overfitting it to in-distribution (ID) data and distorting  features useful for out-of-distribution (OOD) generalization. We propose to use Relative Gradient Norm (RGN) as a way to measure the vulnerability of a backbone to feature distortion, and show that high RGN is indeed correlated with lower OOD performance. Our analysis of RGN yields interesting findings: some backbones lose OOD robustness during fine-tuning, but others gain robustness because their architecture prevents the parameters from changing too much from the initial model. 
Given these findings, we present recipes to boost OOD robustness for both types of backbones. Specifically, we investigate regularization and architectural choices for minimizing gradient updates so as to prevent the tuned backbone from losing generalizable features. Our proposed techniques complement each other and show substantial improvements over baselines on diverse architectures and datasets.
\end{abstract}

\iffalse
\begin{abstract}
Building object detectors robust to various domain shifts is critical to real-world applications. In object detection, prior fine-tuning approaches risk overfitting to in-distribution (ID) data and distort the pre-trained backbone. We study how fine-tuning affects robustness regarding the backbone's distortion by analyzing the expected distortion using its gradient norm, called Relative Gradient Norm (RGN), which indicates how aggressively each parameter will be updated during fine-tuning. The analysis of RGN on diverse backbones reveals interesting findings, \eg, i) The magnitude of RGN varies significantly across different backbones, ii) Backbones with large RGN likely distort the model and lose OOD robustness by fine-tuning, iii) some backbones can significantly increase robustness by fine-tuning since they
have the architecture that can reduce RGN, e.g., Squeeze and Excitation block (\seblock). 
Given these findings, we study two complementary techniques to preserve the generalizable representations of the large-scale pre-trained model. 
First, we investigate the regularization by the distance between the adapted and the initial backbone in the weight space, which prevents the adapted backbone from losing generalizable features. Second, pre-training a decoder, i.e., components except for the backbone, leading to low RGN is the key to boosting OOD performance. Our fine-tuning recipes, which combine this weight regularization and decoder-level regularization, boost the generalization of diverse backbones such as ResNet, EfficientNet, ConvNext, and SwinTransformer.

\end{abstract}
\fi

\iffalse
\begin{abstract}
Building object detectors robust to various domain shifts is critical to real-world applications. Fine-tuning a robust pre-trained model should be a promising solution, yet is not investigated well. In image classification, naive fine-tuning often improves accuracy on a given training distribution but reduces robustness to out-of-distribution (OOD) by distorting the model's parameters. We study how fine-tuning affects robustness in object detection and find that the improvements in OOD depend a lot on the architecture of the pre-trained model. 
We analyze the expected model's distortion using its gradient norm, called Relative Gradient Norm (RGN), which denotes how aggressively each parameter will be updated by fine-tuning. The analysis of RGN on diverse models shows interesting findings, e.g., i) values of RGN vary a lot by models, ii) models with large RGN are updated aggressively and overfitted to ID data, iii) squeeze and excitation block (\seblock) seems to reduce gradient in backward operation. Given these findings, we study two complementary techniques to preserve the generalizable representations. First, we investigate the regularization by the distance between the adapted and the initial model in the weight space, which prevents the model's parameters from being far away from the initial ones. Second, pre-training a detector leading to low RGN is the key to boosting OOD performance. Our recipes, which combine this weight regularization and decoder-level regularization, boost the generalization of diverse backbones such as ResNet, EfficientNet, ConvNext, and SwinTransformer.
\end{abstract}
\fi

\iffalse
\begin{abstract}
Building object detectors robust to various domain shifts is critical to real-world applications. 
Though transferring the robustness of a pre-trained model to the detectors should be crucial, robust fine-tuning for object detection is not explored well. In this paper, we first investigate how to transfer the robustness of a pre-trained model to an object detector.
In image classification, naive fine-tuning distorts features of a pre-trained model, substantially degrading the out-of-domain performance (OOD). One remedy is to train a linear head first before fine-tuning the backbone. 
We study the two-step training in object detection, i.e., training the remaining detector components (decoder) before fine-tuning backbones. Our first novel finding is that the improvement is architecture-specific. To analyze the behavior, we measure the expected model's distortion using its gradient norm, called Relative Gradient Norm (RGN), which denotes how aggressively each parameter will be updated by fine-tuning. The analysis of RGN on diverse models shows interesting findings, e.g., i) values of RGN differ a lot by models, ii) models with large RGN are updated aggressively and overfitted to ID data, iii) squeeze and excitation block (\seblock) seems to reduce gradient in backward operation. Given these findings, we study two complementary techniques to preserve the generalizable representations. First, we investigate the regularization by the distance between the adapted and the initial model in weight space, which prevents the model's parameters from being far away from the initial ones. Second, training a decoder leading to low RGN is the key to boosting OOD performance. Our recipes, which combine this weight regularization and decoder-level regularization, boost the generalization of diverse architectures such as ResNet, EfficientNet, ConvNext, and SwinTransformer.
\end{abstract}
\fi

\iffalse
\begin{abstract}
Building object detectors robust to various domain shifts is critical to real-world applications. 
%This paper studies a recipe for generalizable fine-tuning in object detection. 
%Recent studies on image classification show that 
In image classification, naive fine-tuning distorts features of a pre-trained model, substantially degrading the out-of-domain performance (OOD). One remedy is to train a linear head first while freezing the backbone before fine-tuning the backbone. 
But, it is unclear whether this idea helps in object detection due to the substantial disparities between classification and detection tasks. In this paper, we answer this question by studying the two-step training in object detection, i.e., training the remaining detector components (decoder) before fine-tuning backbones. We find that the improvement is architecture-specific. 
We measure the distortion by the ratio of gradient norm to parameter norm, i.e., Relative Gradient Norm (RGN), computed on the training (in-distribution; ID) data. Intuitively, the ratio denotes how aggressively each parameter will be updated by fine-tuning. We analyze RGN on diverse pre-trained models and observe interesting findings, e.g., i) models with large RGN are updated aggressively and overfitted to ID data, ii) squeeze and excitation block (\seblock) seems to reduce gradient in backward operation.
Given these findings, we study two complementary techniques to preserve the generalizable representations. First, we investigate the regularization by the distance between the adapted and the initial model in weight space, which prevents the model's parameters from being far away from the initial ones.
Second, we find that training a decoder that can lead to low RGN is the key to boosting OOD performance. Our recipes boost the generalization of diverse architectures such as ResNet, EfficientNet, ConvNext, and SwinTransformer.
\end{abstract}
\fi


\iffalse
\begin{abstract}
Building object detectors robust to various domain shifts is critical to real-world applications. 
This paper studies a recipe for generalizable fine-tuning in object detection. Recent studies on image classification show that naive fine-tuning distorts features of a pre-trained model, substantially reducing the out-of-domain performance (OOD). One way to address the distortion is to train a linear head first while freezing the backbone before fine-tuning all layers. We study this two-step training in object detection, \ie, training the decoder first before fine-tuning backbones, and find that the improvement is architecture-specific. 
We measure the amount of distortion in each model by the ratio of gradient norm to parameter norm, i.e., Relative Gradient Norm (RGN), computed on the training (in-distribution; ID) data. Intuitively, the ratio denotes how aggressively each parameter will be updated by fine-tuning. 
We analyze RGN on diverse pre-trained models and observe interesting findings, \eg, i) models with large RGN are updated aggressively and overfitted to ID data, ii) squeeze and excitation block (SEblock) seems to reduce gradient in backward operation.
Given these findings, we study two techniques to preserve the generalizable representations. First, we investigate the regularization by the distance between the adapted and the initial model in weight space, which prevents the model's parameters from being far away from the initial ones.
Second, we introduce the SEblock into the two-step training of the detector, which adds only a small amount of parameters, yet effectively reduce gradients to maintain the knowledge of the pre-trained model.
Our approach is effective for diverse architectures such as ResNet, EfficientNet, and ConvNext on diverse domain shifts.

\end{abstract}
\fi
