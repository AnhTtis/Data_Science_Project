\section{Experiments}
\vspace{-1mm}
In this section, we first study the effect of weight regularization and \seblock. After that, we analyze the combination of the two techniques and other regularization, such as data augmentation. We follow the experimental protocol presented in Sec.~\ref{sec:analysis}. We set $\lambda$ in Eq.~\ref{eq:objective} as 0.1 based on the performance on Pascal to Watercolor using ResNet50, and apply the same value to all other settings with weight regularization. We analyze the sensitivity to $\lambda$ in the appendix.

\noindent\textbf{Effect of weight regularization.}
Table~\ref{tb:main} shows results with three pre-trained models. The results show the effectiveness of using weight regularization for all architectures. This means that even for the architecture with a robust structure, \eg, EfficientNet, adapting to downstream tasks without losing generalization is not trivial, thus, regularization is useful. Large $\lambda$ values in Eq.~\ref{eq:objective} impose strong regularization on the training, that is, larger $\lambda$ should improve OOD generalization while limiting the improvements on ID performance. We obtain empirical results consistent with this intuition in the appendix.  

\input{tables/seblock_pascal}
\noindent\textbf{Inserting \seblock into the backbone.}
Table~\ref{tb:seblock} investigates the effectiveness of inserting \seblock into the pre-trained model following Sec.~\ref{sec:decoder}. Although it does not necessarily outperform the DP model in OOD, it consistently improves FT and DP-FT, showing that \seblock improves generalization. Additionally, combining the technique with weight regularization (see +WR) significantly improves performance, showing the compatibility of the two techniques.
Table~\ref{tb:seblock_rgn_analysis} compares DP and DP-SE models in terms of RGN and the detection loss computed on the checkpoints. 
The decrease in loss can reduce the RGN value, but even when the loss value increases a little (see ConvNeXt), DP-SE substantially reduces RGN value. This indicates that \seblock is important in preventing large gradients from passing to lower layers.

\noindent\textbf{Distance from the pre-trained model.}
Table~\ref{tb:weight_distance} shows the distance between fine-tuned (DP-FT) models and an initial model. Since weight regularization uses the distance as a penalty, the fine-tuned model gets closer to the initial one. Also, \seblock, the architecture level regularization, reduces the distance. Although the plain training renders the model far from the initial one, its performance on ID is worse than in some regularized models. Overfitting can cause performance degradation even ID. 

\input{tables/seblock_rgn_analysis}
\input{tables/distance_from_pretrained}
%\textbf{SE-pretrained model vs SE-plugged-in model.} 

\input{figures/analysis_decoder}

\input{tables/compare_baselines.tex}

\noindent\textbf{Decoder producing small RGN helps OOD generalization.} 
Fig.~\ref{fig:long_train} compares DP, DP-FT, and DP-FT plus weight regularization by increasing the number of training iterations in the DP stage. We conduct training on Pascal with the ResNet50 model and report the performance averaged over the three OOD domains. 
We have two findings; (1) longer training in DP reduces RGN and improves OOD generalization in all approaches, yet, (2) the backbone still loses generalization by fine-tuning (DP vs. DP-FT), and (3) weight regularization is helpful to maintain the generalization. Fig.~\ref{fig:nas_fpn} further investigates the capacity of the decoder model using NAS-FPN~\cite{ghiasi2019fpn}, by increasing the stack of feature pyramid modules. Stacking more modules increases the generalization on OOD (NAS-FPN DP), but fine-tuning from such decoders significantly reduces generalization (NAS-FPN DP-FT). The decoder with more FPN stacks has a larger RGN, reducing generalization by fine-tuning. These models underperform a plain decoder with DP-FT trained with weight regularization (from Fig. ~\ref{fig:long_train}), showing that a stronger decoder does not necessarily produce a generalizable detector. We also try longer DP training for this strong decoder but do not see improvements in OOD performance because the strong decoder easily overfits the ID data. These observations suggest that training a lightweight decoder longer in the DP stage and applying weight regularization during fine-tuning is the best recipe to achieve an OOD robust detector. 


\noindent\textbf{Compatibility with other DG approaches.}
Since the prior work on single-domain generalization for object detection~\cite{chen2021robust}, comparable to our setting, does not publish training code, we develop several DG approaches inspired by DG image classification to see the compatibility with our techniques in Table~\ref{tb:compare_baselines}.
First, to see the compatibility with data augmentation, we study the Augmix data augmentation~\cite{hendrycks2019augmix}. Comparing the second row (FT) and the last row (WR+SE), our approach and data augmentation have an orthogonal effect; data augmentation expands the training domain while our approach retains the knowledge of the pre-trained model. From this result, we conjecture that our approach is compatible with other augmentation, such as adversarial augmentation~\cite{chen2021robust}. 
Second, to ensure consistency with a pre-trained model, we introduce a model regularized with feature consistency loss. Specifically, given the DP model, this approach minimizes detection loss and contrastive loss between region-wise features from the DP and the tuned model, wherein Augmix~\cite{hendrycks2019augmix} is employed as data augmentation. Although the contrastive loss to ensure consistency with the pre-trained model is explored in image classification~\cite{chen2021contrastive}, our work is the first to test in object detection. The regularization approach, CR, outperforms FT trained with Augmix. Note that, thanks to the simplicity of weight regularization, it is easy to plug in, computationally efficient, and consistently improves the performance of CR (CR+WR). These results indicate that our proposed recipes are compatible with other generalization techniques.  

\input{tables/swin_pascal}
\noindent\textbf{Transformer.} 
We further investigate the effectiveness of the regularization in Swin-Transformer~\cite{liu2021swin}. We employ Swin-B pre-trained with ImageNet-21K in Table~\ref{tb:swin_base}. While both FT and DP-FT decrease performance on OOD or the improvements are marginal, adding the weight regularization remarkably improves OOD generalization with a small decrease in ID performance. The advantage of using \seblock is not evident in this experiment. Further exploration of gradient-preserving layers in transformers is part of future work.

\input{tables/coco}
\noindent\textbf{Experiments on COCO.} 
We evaluate our approach using COCO~\cite{coco} and COCO-C~\cite{michaelis2019benchmarking} containing images with diverse types and severity of image corruptions. Table~\ref{tb:coco} shows that regularization boosts robustness to corruptions by more than five points over the plain fine-tuning (FT). 

\input{tables/lvis}
\textbf{Analysis on the long-tailed instance segmentation.}
Although we focus on the input-level distribution shift in evaluation, one of the important shifts is in label distribution. Especially, long-tailed recognition aims to train a model which generalizes well on diverse categories from data with imbalanced label distribution.  
We hypothesize that the large-scale pre-trained backbone has representations effective at recognizing diverse categories, but it can lose the representations if trained on the imbalanced data; thus, the regularization on a backbone can be effective. Then, we conduct experiments on Lvis v1.0~\cite{gupta2019lvis} in Table~\ref{tb:lvis}.
We see that DP-FT degrades performance on rare categories (APr) compared to DP, while weight regularization improves the performance on all types of categories. This analysis indicates the effectiveness of a large-scale pre-trained model for long-tailed recognition.





