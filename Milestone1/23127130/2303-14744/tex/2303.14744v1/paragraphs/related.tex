\section{Related Work}
\vspace{-2mm}
\noindent \textbf{Domain Generalization in Image classification.}
%Domain generalization aims to train a model generalizable to unseen target domains. 
%Diverse approaches have been proposed for domain generalization in image classification. 
Robust fine-tuning, namely, adapting a pre-trained model to a downstream task without losing generalization to unseen domains, has been popular in recent work. Gulrajani \etal \cite{gulrajani2020search} find that simple ERM, \ie fine-tuning on source with well-tuned hyperparameters, shows strong generalization. Kim \etal~\cite{kim2022broad} and Angarano \etal~\cite{angarano2022back} show that larger models have stronger OOD performance. Kumar \etal propose warm-up with a linear head~\cite{kumar2022fine}, while Wortsman \etal propose to interpolate the weights of a fine-tuned and a zero-shot model to produce a single robust model~\cite{wortsman2022robust}. This technique is extended to ensemble diverse models trained from the same initialization~\cite{wortsman2022model,rame2022diverse}, and to ensemble several checkpoints from one training trajectory~\cite{cha2021swad}. Regularizing the change w.r.t the pre-trained model's features is another promising way to tackle the problem~\cite{chen2021contrastive, cha2022domain}. 
Surgical fine-tuning~\cite{lee2022surgical} changes only a subset of layers to adapt a model to a downstream task. Their goal is to improve the performance on the downstream task given a small number of training data. They find that tuning a block with a large RGN achieves high performance on the downstream task. In contrast, our findings suggest that large RGN can indicate the risk of overfitting the training data. 

\input{tables/dp-ft_compare.tex}

\noindent \textbf{Data Augmentation.}
Data augmentation effectively expands the training domain~\cite{zhou2021domain,xu2021fourier} \eg, Augmix~\cite{hendrycks2019augmix} exploits diverse synthetic data augmentation, and augmentation with style transfer can further diversify training images~\cite{zhong2022adversarial}. Our experiments show that the backbone can lose generalization even with augmentation, needing regularization to maintain OOD robustness. 

\noindent \textbf{Domain Generalization in Object detection.}
Wang~\etal employ temporal frames to ensure the consistency of features~\cite{wang2021robust}. Normalization perturbation (NP)~\cite{fan2022normalization}, concurrent with our work, uses style randomization in feature space. Det-AdvProp~\cite{chen2021robust} combines adversarial and clean examples to train a model. The settings of NP and Det-AdvProp are comparable to ours, but we do not compare against their approaches since they do not publish code to train models. Instead, we show the compatibility of our approach with data augmentation. This indicates the potential compatibility with their methods, which use data augmentation for training. 
%These approaches do not focus on robust fine-tuning of an existing pre-trained backbone. 
There exist multiple source generalizable object detection~\cite{lin2021domain, wang2019towards}, but we tackle a single-source scenario with no domain labels. Vasconcelos~\etal~\cite{vasconcelos2022proper} demonstrate that training only a strong decoder on top of a frozen backbone can improve detection models. They focus on evaluating the model on ID, but we inspect the effectiveness of OOD generalization and fine-tuning the backbone. 

\noindent \textbf{Continual Learning.}
Our study connects with early work on continual learning~\cite{kirkpatrick2016overcoming} in considering the distance from a pre-trained model in weight space. They find that elastic weight consolidation (EWC), which accounts for the importance of each weight on the older task, is useful for maintaining performance on older tasks. Mirzadeh \etal~\cite{mirzadeh2022wide} show that wider neural networks forget less and have sparser gradient updates. 
Xuhong~\etal~\cite{xuhong2018explicit} inspect fine-tuning for retaining the features learned on the source task, compare several regularization methods including EWC, and observe that a simple L2 penalty on the pre-trained model mitigates overfitting well, which we also observe in experiments (see appendix for more details).
Wortsman~\etal~\cite{wortsman2022robust} also use the L2 penalty as a baseline in an image classification task but the penalty is not explored in object detection. We highlight that we are the first to show that weight regularization is an overlooked, yet powerful tool for keeping OOD performance in object detection. 