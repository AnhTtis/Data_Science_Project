\input{figures/per_layer_rgn.tex}
\vspace{-3mm}
\section{Analysis of Feature Distortion}~\label{sec:analysis}
%Our goal is to improve OOD robustness of object detectors by preserving generalizable features during the fine-tuning of the backbone. 
%First, we aim to analyze how fine-tuning can distort the features of robust pre-trained models and the factors involved in the distortion. 
In this section, we aim to answer three questions; (1) how the model's distortion can be quantified, (2) the correlation between OOD performance and the distortion, and (3) whether the distortion depends on the backbone architecture. 
We start by analyzing three tuning strategies; (1) a decoder-probing (DP) model, which freezes the feature extractor while training the remaining detector modules, \ie, decoder, (2) fine-tuning (FT) model, which tunes both the extractor and detector, (3) a decoder-probing before fine-tuning (DP-FT), which applies DP training before FT. 
%In summary, we have three findings; (1) the improvement in OOD, from DP to FT, depends on the architecture of the backbone, (2) the improvement has an inverse correlation with the norm of gradient, (3) some modules such as squeeze and excitation block can filter gradient, thus improving OOD performance by fine-tuning. 

\noindent \textbf{Set-up.} 
We use the Faster R-CNN model~\cite{faster} with a feature pyramid network~\cite{lin2017feature} as our detector since it is the typical detector architecture. We regard the pyramid modules and detector head as the decoder, wherein only the decoder is trained for a DP model. We follow the protocol of detectron2~\cite{wu2019detectron2} for hyper-parameter selection, \eg, the shallow CNN layers (stages 1 and 2) and all BatchNorm parameters are frozen by the default setting of the library. DP and FT train for the same number of iterations, DP-FT initializes the decoder with the DP model and trains both decoder and backbone as in FT training. 
Backbones are chosen mainly from models pre-trained with large-scale data such as Instagram~\cite{mahajan2018exploring}, JFT~\cite{sun2017revisiting}, and ImageNet21K~\cite{deng2009imagenet} (IN21K) since our primary goal is the investigation of robust models.
%We limit our scope to fine-tuning the downstream task with relatively small number of training data considering the cost of collecting the dataset.
We utilize Pascal~\cite{everingham2010pascal} (16,551 training images) or Cityscapes~\cite{cordts2016cityscapes} (2,965 training images) as training data. We employ watercolor, comic, and clipart~\cite{inoue2018cross} as OOD in Pascal, FoggyCityscapes~\cite{sakaridis2018semantic} and BDD~\cite{yu2020bdd100k} as OOD in Cityscapes. 
We focus on the setting where the training data is not very large as this is the case in many applications. 
We employ the mean Average Precision (mAP) used in COCO~\cite{coco} as the evaluation metric.
\input{figures/rgn_vs_imp_tmp}

\noindent \textbf{The performance improvement by DP-FT depends on the architecture.}
First, we conduct experiments to see the effect of DP-FT in Table~\ref{tb:dp-ft}. 
Interestingly, the performance improvement over DP depends on the pre-trained models. EfficientNet~\cite{tan2019efficientnet,xie2020self} improves both OOD and ID performance by fine-tuning feature extractor, but ResNet50~\cite{mahajan2018exploring} significantly degrades OOD performance. Also, DP-FT improves performance compared to FT but still underperforms DP in OOD for ResNet50. Overall, ConvNeXt improves OOD performance, but the gain is marginal, considering its high performance on ID. Prior work on image classification~\cite{kumar2022fine} only reports that naive fine-tuning tends to decrease OOD generalization. Our novel finding is that the performance decrease or increase depends on the architecture of the pre-trained models. 

\noindent \textbf{Relative Gradient Norm differs by architectures.}
The analysis above motivates us to study the factors that improve OOD performance with simple fine-tuning. 
From the insight on linear-warm-up training for image classification~\cite{kumar2022fine}, if the amount of updates, \ie, model distortion, by fine-tuning is large, the model will likely lose generalization on OOD. To quantify the model distortion, we compute the variant of the ratio of gradient norm to parameter norm (RGN)~\cite{lee2022surgical}. 

Let $\mathbf{W} \in \mathbb{R}^{C_{i} \times C_{o} \times F} $ denote a weight parameter in a convolution layer, where $C_{i}, C_{o}, F$ denote input, output channel size, and the kernel size of filters. 
Since we are interested in the scale of updates per each filter, we take summation within each filter to compute RGN in layer $L$, $\mathbf{RGN}^{L}_{ij} = \frac{\sum_{k} |\nabla{\mathbf{W}_{ijk}}|}{\sum_{k} |\mathbf{W}_{ijk}|}$. Then, $\mathbf{RGN}^{L} = \frac{1}{C_{i}C_{o}}\sum_{i, j} \mathbf{RGN}^{L}_{ij}$. To get the RGN value for one model, we take the average over layers, $L$.
We utilize the DP model to compute the gradient of detection loss; thus, RGN indicates how aggressively $W$ will be updated by fine-tuning. RGN is averaged over training images. Note that since we do not update the DP model during this computation. RGN shows the expected update at the starting point of DP-FT. 

Fig.~\ref{fig:rgn_depth} plots RGN values for the three models on Pascal, with different layers, used in Table~\ref{tb:dp-ft}. RGN differs significantly by model; EfficientNet has a small RGN in most layers, ConvNeXt has a large RGN in the first few layers, and ResNet50 has large RGN  across all layers. Also, note that ResNet50 loses OOD generalization by fine-tuning while the other two networks improve it in Table~\ref{tb:dp-ft}. The RGN averaged over all layers are 1.34 (ResNet50), 0.13 (ConvNeXt), and 0.06 (EfficientNet), respectively.
This observation is consistent with previous findings, \ie, the performance on OOD will be high if a pre-trained model does not have to aggressively update parameters to fit a downstream task~\cite{kumar2022fine}. 
However, our novel finding is that the model distortion is specific to each pre-trained model. Next, we conduct a more extensive study to investigate whether the difference stems from architecture or the dataset used to train the model.

\noindent \textbf{Relative Gradient Norm (RGN) has a negative correlation with performance improvement in OOD.}
In Fig.~\ref{fig:rgn_vs_improvement}, we investigate the relationship between RGN and the ratio of improvement on OOD to improvement on ID, \ie $\frac{(\rm{OOD_{FT}}-OOD\rm{_{DP}})}{(\rm{ID_{FT}}-ID_{DP})}$, where $\rm{OOD}$ and $\rm{ID}$ denote the mAP on OOD and ID respectively. We employ 14 models with diverse architectures and different pre-training datasets, \eg, ResNet~\cite{he2016deep}, SeNet~\cite{hu2018squeeze}, EfficientNet~\cite{tan2019efficientnet}, ConvNeXt~\cite{liu2022convnet}, MobileNetV2~\cite{sandler2018mobilenetv2}.
We observe that RGN and relative improvement has a negative correlation, indicating that RGN is a good measurement of the performance improvement on OOD gained by fine-tuning on ID. Intuitively, the target domain with low mAP is far from the ID. Then, the correlation becomes more evident in these plots as OOD gets farther away from ID. For example, the correlation is more evident in Comic (mAP: 10.3) while it is not evident in Watercolor (map: 22.0). 

\input{tables/rgn_dataset_architecture.tex}
\noindent \textbf{Pre-training dataset and SE-Block affect RGN.}
In Table~\ref{tb:rgn_dataset_arch}, we pick ResNet50 trained with different datasets and SeNet50 to show their RGN. First, surprisingly, using more data tends to increase RGN, as shown in the results of ResNet50. If a model is trained on diverse data, it may need to lose much representation to be specific to the downstream task. Second, SeNet has a much smaller RGN than ResNet50 does. The only difference between SeNet and ResNet50 is the existence of \seblock. Therefore, we conjecture that one of the keys to reducing RGN is in \seblock and investigate it in the next paragraph. 

\input{figures/seblock_histogram.tex}
\noindent \textbf{Analysis of \seblock.}
Squeeze-and-excitation block performs channel-wise attention in the residual block as follows:
\begin{equation}
     \hat{\mathbf{X}} = \mathbf{X} + \sigma(S(R(\mathbf{X}))) \circ R(\mathbf{X}), 
\end{equation}
where $\sigma$, $S$, $R$, and $\circ$ denote sigmoid activation, multi-layer perceptron after average pooling, convolution layers, and channel-wise attention, respectively. This block applies sigmoid activation to scale outputs from 0 to 1. Therefore, the block masks several outputs from $R(X)$ resulting in a sparse activation pattern. In Fig.~\ref{fig:se_histogram}, we visualize the histogram of the activation from four SE-Blocks, computed on the SeNet~\cite{hu2018squeeze} pre-trained on ImageNet~\cite{deng2009imagenet}. We observe that many units are close to either zero or one.

For simplicity, let $M(\mathbf{X})$ denote $S(R(\mathbf{X}))$. Then, we can calculate the derivative as follows:
\begin{equation}\label{eq:se_derivative}
    \diffp{\hat{\mathbf{X}}}{\mathbf{X}}= I + \diffp{\sigma(M(\mathbf{X}))}{\mathbf{X}} \circ R(\mathbf{X}) + \sigma(M(\mathbf{X}))\circ \diffp{R(\mathbf{X})}{\mathbf{X}}.
\end{equation}

Note that the second term includes a derivative of the sigmoid activation, which is nearly zero if the activation is close to one or zero:
\begin{equation}
   \diffp{\sigma(M(\mathbf{X}))}{\mathbf{X}}= (I - \sigma(M(\mathbf{X})))\sigma(M(\mathbf{X})) \diffp{M(\mathbf{X})}{\mathbf{X}}.
\end{equation}
Also, $\sigma(M(\mathbf{X})))$ in the third term of Eq. ~\ref{eq:se_derivative} can mask the derivative from $R(\mathbf{X})$. From this analysis and empirical findings, we hypothesize that \seblock is masking gradient from upper layers and promotes robust fine-tuning. 

In summary, in this analysis, we introduce RGN to measure the distortion of the model by fine-tuning, revealing that the large distortion is correlated with lower OOD performance.  Also, some backbones lose OOD robustness during fine-tuning, but others gain robustness because their architecture prevents distortion. 

\section{Regularization for Robust Fine-tuning}
Given the findings in the previous section, we aim to find a fine-tuning recipe for achieving a robust object detector.
In the previous section, we have seen that large RGN distorts features of the pre-trained model, and \seblock can effectively reduce the amount of gradient. In addition, DP-FT boosts OOD performance compared to FT. 
We build two techniques inspired by these observations on top of DP-FT. 
\input{figures/training_figure.tex}

\input{tables/main_results.tex}
\subsection{Weight Regularization}
Our analysis shows that RGN is a key factor in losing generalizability. To reduce RGN and incorporate an explicit inductive bias of a pre-trained model, we investigate the effect of introducing the distance from the initial value as a regularizer, which we denote $\Omega(w)$, and optimize it with detection loss as follows:

\begin{equation}\label{eq:objective}
    L(w) = L_{det}(w) + \lambda\Omega(w),
\end{equation}
where $L_{det}(w)$ denotes the loss of object detection. Note that the decoder modules are trained without regularization. 
There are several options for $\Omega(w)$ as indicated by previous work on transfer learning~\cite{kirkpatrick2016overcoming,xuhong2018explicit}. %Our goal is not to propose the best regularization but to investigate the importance of ensuring consistency with a pre-trained model in weight space.

\noindent \textbf{$L^{2}$ penalty.}
One simple option for the distance function is the L2 distance,
\begin{equation}
\Omega(w) = \sum_{i}\norm{w_{i}-{w_{i}}^{pre}}^{2}.
\end{equation}
The distance looks naive, yet it empirically works well to preserve robustness to OOD. 

\noindent \textbf{$L^{2}$ penalty weighted by RGN.}
We also study weighting the $L^{2}$ penalty by RGN:
\begin{equation}
\Omega(w) =  \sum_{i}{{\mathbf{RGN}}_{i}} \norm{w_{i}-{w_{i}}^{pre}}^{2}, 
\end{equation}
where $\mathbf{RGN}_{i}$ denotes RGN value for the parameter $w_{i}$. The RGN value needs to be computed after DP training. 

\noindent \textbf{Elastic weight consolidation (EWC).}
EWC~\cite{kirkpatrick2016overcoming} utilizes a fisher-information matrix to weight the regularizer, 
\begin{equation}
\Omega(w) =  \sum_{i}{{F}_{i}} \norm{w_{i}-{w_{i}}^{pre}}^{2}, 
\end{equation}
where $F$ denotes the fisher matrix. Since Kirkpatrick~\etal~\cite{kirkpatrick2016overcoming} employ diagonal approximation to compute the Fisher matrix, the biggest difference between the penalty with RGN and this penalty is in the normalization by the norm of the parameter. 
Empirically, we find that these techniques are almost equally effective in boosting the performance of OOD, that is, by choosing proper $\lambda$, they show comparable performance in both OOD and ID. 
But, the regularization with RGN excels at transferring $\lambda$ across different pre-trained models probably because RGN considers the scale of gradient and weights, making regularization's strength consistent across different architectures. In experiments, we employ the regularization with RGN and leave the comparison among regularizers in the appendix.


\subsection{Regularization by Decoder Design}\label{sec:decoder}
\noindent\textbf{Decoder-Probing with \seblock.}
Although the \seblock effectively avoids distorting features, not all pre-trained models are built with it. Thus, we study inserting  SE-Blocks into several layers of pre-trained models and perform decoder-probing fine-tuning. By default, ResNet-like architectures split the network into four stages. We insert the block at the end of each stage, as shown in Fig.~\ref{fig:seblock_figure}. We tune the decoder and \seblock during decoder-probing training, then tune all modules during fine-tuning. The block is expected to filter the gradient from the upper layers, thus leading to better OOD performance. Additionally, since the block is a lightweight plug-in module, the parameter and running time increase during inference is very small. 

\noindent\textbf{Stronger decoder.}
One simple way to reduce the gradient updates to the backbone model is to use a strong decoder in decoder-probing training, then fine-tune the whole network. If the strong decoder decreases the loss, the backbone will not be significantly updated. In experiments, we investigate increasing training iterations in decoder-probing and decoder architecture. %NAS-FPN~\cite{ghiasi2019fpn, vasconcelos2022proper} architecture.





