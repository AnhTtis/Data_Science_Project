\section{Experimental Details}
\noindent \textbf{Datasets.}
The number of test images used for evaluation is listed in Table~\ref{tb:dataset_summary}. For experiments on COCO, we use the validation split for evaluation. 

\input{tables/dataset_summary}
\noindent \textbf{Training Details.}
We set up the hyper-parameters following the instructions provided by detectron2~\cite{wu2019detectron2}, \eg, we train the model for 80,000 iterations with a batch size of 2 in Pascal and 24,000 iterations with a batch size of 8 in Cityscape. In Cityscape, MaskRCNN is trained and evaluation is done on the instance segmentation task. 
For other architectural choices \eg, the architecture of the feature pyramid and detector head, we employ the default configuration. 
One exception is in the training of ConvNeXt~\cite{liu2022convnet}, where we use the group normalization~\cite{wu2018group} in the feature pyramid module to stabilize the training. 
For pre-trained models, we employ weights available in PyTorch Image Models~\cite{rw2019timm}.
We will publish the code used for training, including each configuration, upon acceptance. 

\section{Additional Results}
\input{tables/cityscape}

\textbf{Results on Cityscape.}
Table~\ref{tb:cityscape} shows the full ablation study on Cityscape using ConvNeXt. Overall, combining all modules performs well in many domains.


\input{tables/dataset_comparison}
\textbf{Analysis of the dataset used for pre-training.} 
Table~\ref{tb:compare_dataset} describes the performance of ResNet50 models pre-trained with different datasets or data augmentation, where we apply weight regularization to train these models. Generally, pre-training on diverse data makes the model generalize well on OOD datasets. Also, improvements over the FT baseline by the regularization get more significant in the pre-training. In other words, pre-trained on diverse data, the model will likely forget the learned representations with standard training.


\input{figures/lamda.tex}
\input{figures/compare_regularization}
\input{figures/nas_fpn_analysis}


\textbf{ID-OOD Trade-off by the coefficient.}
Fig.~\ref{fig:trade-off} shows a trade-off in ID-and-OOD performance controlled by the regularization coefficient, $\lambda$ in Eq. 4, which is measured using Pascal with ResNet50-Instagram. Increasing $\lambda$ adds more regularization to keep the parameter near the initial model, reflected as the performance decrease in ID (Pascal). Clipart and Comic increase the performance with more regularization, while Watercolor peaks near 0.1. The peak of OOD performance should differ by the similarity with ID dataset.

\input{tables/compare_regularization}


\textbf{Comparison among different regularizations.}
Table~\ref{tb:compare_regularization} compares different regularization, \ie, weight regularization with L2-distance, EWC, and RGN weighted regularization, using DP-FT. 
We train models using Pascal and pick $\lambda$ based on the Watercolor domain and show the average over three domains as OOD. We do not see a significant difference among the three regularizations, in particular, in OOD performance. We further investigate the strength of the regularization on each $\lambda$ in Fig.~\ref{fig:compare_reg}. The Y-axis shows the MAP on Pascal with each coefficient over the MAP of the DP-FT model. With more regularization, the value decreases. Therefore, the value implies the strength of the regularization. Then, we see that the strength of the regularizer is similar across different architectures by RGN while the other two show different strengths. This indicates that RGN can add similar strength of the regularization on different architectures by the same $\lambda$. This is because the regularization is weighted by the relative gradient norm, which should adjust the strength of the regularization depending on the norm of the gradient and parameter. In short, L2-distance and EWC may require additional hyper-parameter $\lambda$ tuning when the architecture is changed while RGN weighted regularization is less sensitive to the changes in the architecture.
\input{tables/robustness}

\textbf{Analysis on NAS-FPN.}
Fig.~\ref{fig:nas_fpn_dp_long} shows the effect of training iterations for the DP model of NAS-FPN and FPN, where we set the number of stacks in NAS-FPN as 3. The NAS-FPN shows strong performance on ID by longer training but reduces the performance on OOD. By contrast, FPN improves the performance on both types of distribution through longer training. A strong decoder can easily overfit ID data; thus, OOD performance can decrease with longer training. 

\input{figures/robustness}
\textbf{Analysis on the robustness to image corruptions.}
Table~\ref{tb:robustness_details} shows the results on the robustness to image corruptions using COCO. Using our regularization, the trained detector improves the robustness to all types of corruption. Interestingly, the use of \seblock decreases the robustness to the high-frequecy noise, \eg, gaussian noise, while improving the robustness to the other types of corruptions. The architectural difference seems to cause this change. 
Fig.~\ref{fig:severity} shows the performance on each severity of the corruptions. Trained with regularization (WR, SE), the detector performs better on the diverse level of severities than the model without regularization. 

%\input{tables/lvis}
%\textbf{Analysis on the long-tailed instance segmentation.}
%Although we focus on the input-level distribution shift in evaluation, one of the important shifts is in label distribution. Especially, long-tailed recognition aims to train a model which generalizes well on diverse categories from data with imbalanced label distribution.  
%We hypothesize that the large-scale pre-trained backbone has representations effective at recognizing diverse categories, but it can lose the representations if trained on the imbalanced data; thus, the regularization on a backbone can be effective. Table~\ref{tb:lvis} shows the results. We see that DP-FT degrades performance on rare categories (APr) compared to DP, while weight regularization improves the performance on them. 


\input{figures/ex_pascal}
\input{figures/ex_cityscape}
\textbf{Qualitative Results.}
Fig.~\ref{fig:ex_pascal} and ~\ref{fig:ex_cityscape} show qualitative results on Pascal and Cityscape. Note that, using our regularization, more objects are detected, \eg, middle in Fig.~\ref{fig:ex_pascal} and top right in Fig.~\ref{fig:ex_cityscape}, or correctly classified, \eg, top left in Fig.~\ref{fig:ex_pascal}. Several objects are not localized by both models, which indicates the difficulty of this task. 

