\section{Introduction}
Fine-tuning a large-scale pre-trained vision model has become the defacto transfer learning paradigm that significantly improves performance on diverse downstream tasks compared to training from scratch~\cite{girshick2014rich,long2015fully,chu2016best,yosinski2014transferable}. Although naive fine-tuning performs well on the downstream task's training distribution (in-distribution; ID), it has subpar performance on data unseen during training (out-of-distribution; OOD)~\cite{kumar2022fine}. 
Kumar \etal~\cite{kumar2022fine} suggest that fine-tuning without unnecessarily distorting the pre-trained backbone is the key to achieving high performance in both ID and OOD. They show that ``linear warm-up'' training, \ie training a linear head first while freezing the backbone before fine-tuning all layers, is effective for robust image classification. %model ensembling in the weight space~\cite{wortsman2022model}, and temporal weight space ensembling ~\cite{cha2021swad} are effective for the image classification task. 
\input{figures/teaser.tex}
Inspired by this, we explore the effect of backbone distortion during fine-tuning in object detection. Although there exist generalizable detection approaches that use data augmentation~\cite{chen2021robust}, feature augmentation~\cite{fan2022normalization}, or advanced objectives~\cite{wang2021robust}, our work is the first to ask whether biasing toward the initial model is effective for this task.

We study an equivalent of ``linear warm-up'' training for object detection. Specifically, we first train the decoder (all detector modules except for the backbone) while keeping the backbone frozen, which we refer to as \textit{Decoder-Probing (DP)}. After DP, we perform full fine-tuning of the detector (including the backbone), which we call \textit{Decoder-Probing Fine-Tuning (DP-FT)}. Surprisingly, we observe that the improvement in OOD performance of DP-FT over DP depends on the architecture of the backbone.
To analyze this phenomenon, we investigate the relationship between performance improvement from fine-tuning and the amount of gradient in each parameter. Specifically, two detectors are compared for each backbone to compute the performance change after tuning the backbone: (1) a DP model and (2) fine-tuning all modules (FT). We then compute the ratio of gradient norm to parameter norm, i.e., \textit{Relative Gradient Norm (RGN)}~\cite{lee2022surgical}, calculated on the training (in-distribution) data using DP. Intuitively, RGN indicates how much updating feature extractors require to fit ID data. We measure RGN's correlation with the performance improvement on OOD relative to that on ID.  
Our novel findings are three-fold: (1) a model with small RGN is more likely to improve performance on both OOD and ID data after fine-tuning, (2)  RGN depends on the network architecture, (3) in particular, some modules such as squeeze excitation (SE) blocks~\cite{hu2018squeeze} can reduce the gradient, thus lowering RGN and improving both OOD and ID performance after fine-tuning. 

Based on these findings, we explore two techniques to preserve the generalizability of a diverse set of pre-trained models (see Fig.~\ref{fig:teaser}).
%We study two recipes to reduce the RGN by explicitly adding weight regularization to the loss and incorporating the decoder designed to preserve the knowledge of a diverse set of pre-trained models   (see Fig.~\ref{fig:teaser}).
The first is a regularizer that minimizes the distance from the initial model in the parameter space to prevent feature distortion. The backbone is constrained to find a point performing well on ID, yet not too far away from the initial model. This simple regularization effectively boosts OOD performance for diverse architectures, yet is overlooked in generalizable object detection. Second, we study the effect of the decoder's architecture and training iterations on decreasing the RGN of the backbone. A decoder producing a lower RGN tends to train a detector more generalizable to OOD. Specifically, inserting an \seblock into the backbone significantly decreases RGN. 

Our contributions can be summarized as follows:
\vspace{-2mm}
\begin{itemize}
    \item We reveal a novel finding that some backbones lose more OOD robustness after fine-tuning than others. At the same time, some backbones can significantly increase robustness due to an architecture that prevents the parameters from being too far away from the initial model, such as having an \seblock.
    \vspace{-1mm}
    \item We present methods to boost OOD robustness for both types of backbones. Specifically,  weight regularization and  decoder design are investigated, and the two techniques are shown to complement each other.  
    \vspace{-1mm}
    \item Our methods show substantial improvements  over baselines when evaluating on diverse architectures and object detection datasets. 
\end{itemize}



% \section{Introduction}
% Pre-training a vision model on a large-scale dataset before transferring to a downstream task significantly improves robustness over training from scratch.  
% However, under the fine-tuning setting, naive fine-tuning performs well on the downstream task's training distribution (in-distribution; ID) but has subpar performance on data distribution unseen during training (out-of-distribution; OOD)~\cite{kumar2022fine}. Overfitting to ID reduces generalization on OOD while underfitting to ID degrades performance near ID. Kumar \etal~\cite{kumar2022fine} suggest that fine-tuning without distorting the pre-trained feature extractor is the key to achieving high performance in both ID and OOD.
% To train a model with a good trade-off between ID and OOD performance, linear-layer warm-up training~\cite{kumar2022fine}, model ensembling in the weight space~\cite{wortsman2022model}, and temporal weight space ensembling ~\cite{cha2021swad} are effective for the image classification task. 
% \input{figures/teaser.tex}
% However, generalizable fine-tuning on object detection is not well explored regardless of its importance, considering the vast amount of annotation cost. There should exist a significant difference between pre-training tasks, \eg, image classification and object detection that requires understanding the fine details of the image. The feature extractor needs to largely update its parameters while retaining generalizable representations, making the task very challenging.

% Unlike existing work on generalizable object detection (GOD), which attempts to solve the problem with data augmentation ~\cite{chen2021robust}, feature augmentation~\cite{fan2022normalization}, and advanced objective~\cite{wang2021robust}, we tackle the task from the aspect of feature distortion of a pre-trained model. One way to address the feature distortion in image classification is to train a linear head first while freezing the backbone before fine-tuning all layers. We study this two-step training in object detection, i.e., training the detector module first, \textit{Detector-Probing (DP)}, before fine-tuning backbones, which we call \textit{Detector-Probing Fine-Tuning (DP-FT)}, and, surprisingly, observe that the performance improvement in OOD, compared to DP, is architecture-specific. 
% To analyze the issue, we investigate the relationship between the performance improvement by fine-tuning and the amount of expected updates of each parameter. Specifically, two detectors are compared on each model; i) freezing the feature extractor while training only detector modules (detector-probing: DP) and ii) fine-tuning all modules (fine-tuning: FT). We compute the ratio of gradient norm to parameter norm, i.e., Relative Gradient Norm (RGN), calculated on the training (in-distribution; ID) data using DP. Intuitively, RGN indicates how much update feature extractors require to fit ID data. We measure RGN's correlation with the ratio of the performance improvement on OOD to that on ID, i.e., $\frac{(OOD_{FT}-OOD_{DP})}{(ID_{FT}-ID_{DP})}$. 

% Our finds are three-fold; i) a model with small RGN is more likely to improve performance on both OOD and ID data by fine-tuning, ii) the RGN differs by the network architecture, iii) some modules such as squeeze excitation~\cite{hu2018squeeze} blocks can filter the gradient, thus reducing RGN and improve both OOD and ID performance by fine-tuning. 

% Given the findings, we introduce two techniques to preserve the generalizable representations.
% First, to penalize the feature distortion, we propose to minimize the distance from the initial model in the parameter space. Given the penalty, the detector is constrained to find an optimal which performs well on ID, yet is not too far away from the intial model. This regularization is simple and effective for diverse architectures to boost the performance on OOD, yet overlooked in generalizable object detection. 
% Next, we introduce the SEblock into DP-FT, where SEblock is inserted in the pre-trained model and optimized in detector probing before fine-tuning all modules.
% Our approach is effective for diverse architectures such as ResNet, EfficientNet, ConvNext on various domain shifts.

% Our contributions are summarized as follows, 

% \begin{itemize}
%     \item we investigate the relationship between OOD robustness and RGN,
%     \item we propose to apply the simple weight regularization during the training of object detector and find effectiveness on diverse architectures. 
%     \item the regularization is compatible with advanced data augmentation techniques. 
%     \item we evaluate our approach on several benchmark datasets and show substantial improvements over baselines. 
% \end{itemize}





