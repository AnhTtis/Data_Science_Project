\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{image_files/teaser_v4.pdf}
    \vspace{-4mm}
    \caption{Top: The standard fine-tuning approach in object detection can cause overfitting to ID and  hurt the performance on OOD data (\textcolor{orange}{Orange} dot with \textbf{Plain Tuning}) or achieve minimal improvements (\textcolor{blue}{Blue} dot with \textbf{Plain Tuning}) depending on the network architecture. Based on a thorough analysis, we propose a new fine-tuning recipe that can achieve good performance on both ID and OOD performance for diverse pre-trained backbones (\textcolor{blue}{Blue} and \textcolor{orange}{Orange} dots with \textbf{Our Tuning}). Results are on Pascal~\cite{everingham2010pascal}, see Table~\ref{tb:seblock}. Bottom: We visualize detected results in Foggy Cityscape.
    } 
    % \caption{We find that, in object detection, whether the fine-tuning on a pre-trained backbone can improve performance on OOD, compared to a detector with the frozen backbone, depends on architectures. Based on the intuition obtained from this analysis, we propose a new fine-tuning recipe that can achieve good trade-off between ID and OOD performance for diverse pre-trained backbones.}
    \label{fig:teaser}
\end{figure}