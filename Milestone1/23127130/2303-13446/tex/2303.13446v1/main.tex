\documentclass[conference]{IEEEtran}
\usepackage{times}
\usepackage{amsmath,amsfonts,amssymb}
% numbers option provides compact numerical references in the text. 
\usepackage[numbers,compress]{natbib}
\usepackage{notoccite}
\usepackage{amsfonts} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage[bookmarks=true,hidelinks]{hyperref}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[font=small,skip=2pt]{caption}
\usepackage{float}
\usepackage{diagbox}
\usepackage{bm}
\usepackage{changepage}
% \usepackage[most,breakable]{tcolorbox}


\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% \title{KODex: Learning Koopman Operators for \\ Dexterous Manipulation }
% \title{KODex: Using Koopman Operator Theory to Learn Dexterous Manipulation Skills from Demonstrations}
% \title{KODex: Encoding Dexterous Manipulation Skills using Learned Koopman Operators}
% \title{Is Lifted Linearization All You Need for Learning Dexterous Manipulation Skills?}
\title{On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills}
% You will get a Paper-ID when submitting a pdf file to the conference system
\author{\authorblockN{Yunhai Han\authorrefmark{1}, Mandy Xie\authorrefmark{1}, Ye Zhao\authorrefmark{1}, and Harish Ravichandar\authorrefmark{1}}
\thanks{$^{1}$The authors are with the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA 30332, USA (email: \{yhan389, yzhao301, harish.ravichandar\}@gatech.edu).
}
\authorblockA{\authorrefmark{1}Institute for Robotics and Intelligent Machines\\
Georgia Institute of Technology,
Atlanta, Georgia 30332--0250\\ Email: \{yhan389, manxie, yezhao,  harish.ravichandar\}@gatech.edu}
}

% \author{Author Names Omitted for Anonymous Review. Paper-ID [294]}

% \author{\authorblockN{Michael Shell}
% \authorblockA{School of Electrical and\\Computer Engineering\\
% Georgia Institute of Technology\\
% Atlanta, Georgia 30332--0250\\
% Email: mshell@ece.gatech.edu}
% \and
% \authorblockN{Homer Simpson}
% \authorblockA{Twentieth Century Fox\\
% Springfield, USA\\
% Email: homer@thesimpsons.com}
% \and
% \authorblockN{James Kirk\\ and Montgomery Scott}
% \authorblockA{Starfleet Academy\\
% San Francisco, California 96678-2391\\
% Telephone: (800) 555--1212\\
% Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
% \author{\authorblockN{Michael Shell\authorrefmark{1},
% Homer Simpson\authorrefmark{2},
% James Kirk\authorrefmark{3}, 
% Montgomery Scott\authorrefmark{3} and
% Eldon Tyrell\authorrefmark{4}}
% \authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
% Georgia Institute of Technology,
% Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
% \authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
% Email: homer@thesimpsons.com}
% \authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
% Telephone: (800) 555--1212, Fax: (888) 555--1212}
% \authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}

% Using \citet for citation, and include \href for the title of each referenced item
\maketitle

\begin{abstract}
% This paper study on utility of Koopman operator theory for high-dimensional dexterous manipulation tasks. We conduct extensive experiments on KODex 
Recent advances in learning-based approaches have led to impressive dexterous manipulation capabilities. Yet, we haven't witnessed widespread adoption of these capabilities beyond 
% a few well-resourced laboratories. 
the laboratory. This is likely due to practical limitations,
% However, these advances are only accessible to those with significant resources due to a number  of practical barriers, 
such as significant computational burden, inscrutable policy architectures, sensitivity to parameter initializations, and the considerable technical expertise required for implementation. 
In this work, we investigate the utility of Koopman operator theory in alleviating these limitations. Koopman operators are simple yet powerful control-theoretic structures that help represent complex nonlinear dynamics as \textit{linear} systems in higher-dimensional spaces. 
Motivated by the fact that complex nonlinear dynamics underlie dexterous manipulation, we develop an imitation learning framework that leverages Koopman operators to simultaneously learn the desired behavior of both robot and object states. 
We demonstrate that a Koopman operator-based framework is surprisingly effective for dexterous manipulation and offers a number of unique benefits. 
First, the learning process is \textit{analytical}, eliminating the  sensitivity to parameter initializations
 and painstaking hyperparameter optimization.
Second, the learned reference dynamics can be combined with a \textit{task-agnostic} tracking controller such that task changes and variations can be handled with ease.
Third, a Koopman operator-based approach can perform comparably to state-of-the-art imitation learning algorithms in terms of task success rate and imitation error, while being \textit{an order of magnitude} more computationally efficient. 
In addition, we discuss a number of avenues for future research made available by this work. 
\end{abstract}

\IEEEpeerreviewmaketitle
% Koopman learning for high-dimensional robotic system, including hybrid systems
% Koopman linear evolution is good for optimal control or contraction analysis with performance guarantee
% Koopman states space embed the object states (task-related, like policy-based RL method), combined the underlying the data-driven dynamics system with the policy mapping(state -> action)
% We consider it as an autonomous system without control input
% Demonstrate on new robot hand

\section{Introduction}
\begin{figure*}[t]
\centering
\includegraphics[scale=0.45]{images/KODex_block_diagram1.pdf}
% \includesvg{images/KODex_block_diagram.svg}
\caption{KODex simultaneously encodes complex nonlinear dynamics of the desired motion of both the robot state ($\mathrm{x}_r$) and the object state ($\mathrm{x}_o$) as a \textit{linear} dynamical system in a higher-dimensional space by learning a Koopman operator $\bm{K}$ directly from demonstrations. Further, KODex learns an inverse dynamics controller to track the robot reference trajectory ($\{\hat{\mathrm{x}}_r(t)\}_{t=1}^T$) generated by the lifted linear system.}
\label{fig:framework}
\end{figure*}
%%%%%%%%%%%%%%%%%%%% Start HR draft %%%%%%%%%%%%%%%%%%%%
% Dexterous manipulation is an important and useful problem
Autonomous dexterous manipulation is a necessary skill for robots operating in a physical world built by and for humans. However, achieving reliable and generalizable dexterous manipulation skills has been a long-standing challenge~\cite{okamura2000overview} due to numerous factors, such as complex nonlinear dynamics, high-dimensional action spaces, and difficulties in designing bespoke controllers that do not generalize to task variations.

% Recent learning-based approaches show a lot of promise
Over the past decade, learning-based solutions are beginning to emerge as a promising solution that can address the challenges in acquiring dexterous manipulation skills. Indeed, these methods have been shown to be capable of impressive feats, such as solving Rubik's cubes~\cite{OpenAI2018Cube}, manipulation baoding balls~\cite{nagabandi2020deep}, and retrieving tool trays~\cite{xieneural}. 

% Existing methods are computationally expensive, hard to implement, and require painstaking hyper parameter tuning
However, existing learning approaches suffer from practical limitations that hinder their widespread adoption. First, implementing existing algorithms requires significant technical expertise and modern machine learning infrastructure. Second, training policies consume significant computational resources. Third, existing approaches demand painstaking effort in tuning hyperparameters to achieve acceptable performance. Fourth, performance tends to be highly sensitive to parameter initialization. See Section~\ref{sec:related_work} for a detailed discussion of related work.

% In this work, we explore the use of Koopman operator theory - How Koopman captures the underlying nonlinear dynanmics as linear dynamics in lifted spaces (kernal trick for dynamical system)
In this work, we investigate the utility of Koopman operator theory in alleviating the limitations of existing learning-based approaches as identified above. The Koopman operator theory helps represent arbitrary nonlinear dynamics in finite dimensional spaces as linear dynamics in an infinite-dimensional Hilbert space~\cite{Koopman1931Koopman}. Indeed, while this equivalence is exact and fascinating from a theoretical standpoint, it is not directly applicable in practice. However, recent advances have resulted in Koopman operator-based approaches that approximate this equivalence in high, but finite, dimensional spaces by learning the operator directly from data. 

% Why Koopman? Desired robot trajectory *and* object trajectory can be seen solutions to underlying dynamics.
We develop a novel imitation learning algorithm, dubbed as \textit{Koopman Operator-based Dexterous Manipulation (KODex)}, to evaluate the effectiveness and benefits of using Koopman operator theory for dexterous manipulation (see Fig. \ref{fig:framework} for a block diagram). Specifically, we model desired behaviors as solutions to \textit{nonlinear} dynamical systems and learn Koopman operators that define approximately-equivalent \textit{linear} dynamics in higher-dimensional spaces. While we use polynomial functions to lift the state space in this work, KODex is agnostic to the specific lifting function. A unique aspect of dexterous manipulation is that it is object-centered~\cite{okamura2000overview}, and the desired motion of the robot has to depend on the object state. As such, KODex \textit{simultaneously} learns the desired motions of both the robot and the object from demonstrations. KODex then relies on a learned inverse dynamics tracking controller to generate actions that can track the reference trajectory generated by the learned reference dynamics. Note that the tracking controller is \textit{task-agnostic}, and as such, can be preserved even if we wish to learn an entirely new task on the same robot platform.
% and can be learned via self supervision.

% Benefits of Koopman include analytical solution, low computational cost, low variance/predictablity/easy to inspect. 
A number of reasons motivate our exploration of Koopman operator theory within the context of dexterous manipulation. A significant benefit of learning Koopman operators from data is that it lends itself to an \textit{analytical} solution. As such, KODex is simple to implement and does not require expertise and familiarity with state-of-the-art (SOTA) machine learning infrastructure. More importantly, KODex incurs significantly lower computational costs compared to existing approaches, and does not rely on hyperparameters that have to be painstakingly tuned by an expert. Further, KODex offers consistent and predictable performance since the learning process is analytical and thus not sensitive to parameter initializations. Finally, given that KODex learns a \textit{linear} dynamical system, it enables an easy inspection of the learned dynamics via a wide array of control theoretic tools.

% Our results demonstrate that ...
We carry out extensive evaluations of KODex within the context of four dexterous manipulation skills on the simulated Adroit hand, an established experimental platform for dexterous manipulation~\cite{Rajeswaran2018DAPG}. Further, we compare KODex against SOTA imitation learning approaches in terms of general efficacy, training efficiency, sample efficiency, and robustness to changes to physical properties that could result from sim-to-real gap or task variations. 
% task success rate, imitation error, training time, and sample efficiency. 
Our results demonstrate that KODex is more computationally efficient than SOTA approaches by \textit{an order of magnitude} or more, while achieving comparable sample efficiency and task success rate. These results suggest that Koopman operator theory has the potential to be an effective, computationally-efficient, and reliable tool to learn dexterous manipulation skills, and to reduce the barriers to adoption.


%%%%%%%%%%%%%%%%%%%% End HR draft %%%%%%%%%%%%%%%%%%%%

% This demo file is intended to serve as a ``starter file" for the
% Robotics: Science and Systems conference papers produced under \LaTeX\
% using IEEEtran.cls version 1.7a and later.  
 % For traditional robot control tasks, people attempted to first design the analytical model and then build the controller for each specific task. However, any model uncertainties or external disturbances can lead to poor performance using this model-based approach, especially for the complex tasks. As a result, data-driven methods \cite{Solomatine2008Data} have attracted more attention as the system dynamics and environment interactions can be accurately modelled with different representative structures from the data. Among these structures, Koopman operator \cite{Koopman1931Koopman} has been successfully applied to various robotic systems, such as Sphero SPRK robot \cite{abraham2017model}, soft robot arm \cite{bruder2019modeling} \cite{Bruder2021Soft}, and robotic excavator \cite{Sotiropoulos2022Bucket}, because of its magic power of linearly representing the complex dynamical systems via infinite number of nonlinear scalar-valued lifting functions. In practice, Koopman operator can be approximated on a finite-dimensional subspace without losing much accuracy. Besides, it is a solely data-driven method, which only requires to actuate the robotic system for data collection. However, the question of if Koopman operator can be applied on high-dimensional robotic systems
%  ,for example, a dexterous robot hand with 24 DoF,
 % is still an open question as all of the previous work deals with the systems with relatively low dimensions, e.g., two-dimensional position and velocity ($\mathbb{R}^4$) for a Sphero SPRK robot or three-dimensional positions of three pneumatically actuated bending sections ($\mathbb{R}^9$) for a soft robot arm.
 
% A dexterous robot hand is a typical high-dimensional robotic system, where multiple fingers simultaneously manipulate in-hand objects and each finger has 3 or 4 joints \cite{okamura2000overview}. For example, the Shadow Dexterous Hand \cite{sharma2014shadow} is designed for human-level dexterity, which is actuated with 20 motors (active joints) with 4 under-actuated joints. In addition to the daunting difficulty in controlling such high DoF systems, another challenge is to reason about making and breaking contacts. The switching contacts between objects and fingers render it harder to predict the behaviour using model-based methods. As a consequence, within the class of data-driven methods, the deep reinforcement learning (DRL) has become the primary choice for learning the hand controller given the current states of the joints and objects. For instance, in \cite{OpenAI2018Cube} \cite{chen2021a} \cite{Huang2021Geometry}, they used DRL to learn the object reorientation policies from scratch. However, these approaches often take several hours or even days for training, which severely limits their applications only in simulation and makes them undesirable for hyperparameter tuning. 
% \begin{figure}[t]
% % {.48\textwidth}
% \setlength{\belowcaptionskip}{-0.65cm}
% \centering
% \includegraphics[width=1\linewidth]{images/task.pdf}
% \caption{{\color{red}This image is directly copied from DAPG paper to serve as a placeholder.}
% }
% \label{fig:cover}
% \end{figure}
% Motivated by the recent applications of Koopman operator and the powerful linear representation of Koopman operator for hybrid systems \cite{Gerlach2020Hybrid} \cite{Bakker2020Hybrid} \cite{Jerry2022Hybrid} (contact-switching systems in this work), we utilize the Koopman operator to learn the dexterous manipulation skills via expert demonstration data. However, unlike other robotic applications which mainly focus on the robot states, a distinguishing characteristic of dexterous manipulation is that it is object-centered \cite{okamura2000overview}. That is, how the objects are manipulated is more important. In DRL training settings, this is reflected as the object states are part of the environment observations and used to compute the rewards. To tackle this problem for Koopman-based learning, we describe the linear evolution of the lifted dynamical system along the joint trajectories of the robot hand and the in-hand objects. Therefore, in addition to the nonlinear dynamical model of the hand, the linear evolution in the lifted state space also embeds the task-specific interactions between the hand and object. While there are several ways to represent the hand motion, we utilize the dynamical system-based approach \cite{Harish2017Dynamical}, which considers the hand motion as an autonomous system driven by an underlying dynamical system. In other words, the learned Koopman dynamics serves as a trajectory/motion generator. To ultimately execute the predicted motions, we design another joint-level controller separately, which can be a fine-tuned PID controller or an inverse dynamics controller \cite{hanna2017grounded} \cite{bahl2020neural} learned from demonstration data. Training a control policy via reinforcement learning is also feasible \cite{peng2020learning}.

% mention about the autonomous system, how to collect data, how to compute the koopman matrix and how about the performance.
% In order to collect the demonstration data, we first train a DRL agent from \cite{Rajeswaran2018DAPG} for each different tasks: in-hand reorientation, object relocation, tool use, and door opening. Each piece of demonstration data contains the full trajectories of hand joints and target objects, and the task-dependant initial or goal conditions. Given the choice of nonlinear scalar-valued lifting functions, Koopman operator is approximated via linear regression using all the consecutive state pairs in the demonstration data. In Experiments, we show that with only hundred pieces of demonstration data and \textsl{one minute CPU-only training}, a Koopman-based agent can obtain comparable results as the expert DRL agent across all tasks. In addition, unlike the deep neural networks, which are in essence nonlinear nonconvex function approximators of the dynamical systems, the linear evolution held by Koopman operator makes it easier to integrate with optimal control techniques, such as LQR or MPC, for dexterous manipulations. Besides, recent work \cite{Bowen2021Contraction} on the equivalence of Koopman operator and contraction analysis makes it possible to provide with performance guarantees while for all deep learning based methods, we can only expect for the good performance. In this work, the optimal control or contraction are not the focuses, but our approach builds the first step towards the future dexterous manipulation researches along these directions.

% The contributions of this paper is summarized as follows:
% Do we have to list contributions, 
% \begin{itemize}
    % \item We extend the application of Koopman operator to a high-dimensional robotic system for dexterous manipulations, and prove this approach to be extremely training-efficient compared with Deep reinforcement training approaches.
    % \item We conduct extensive experiments to compare KODex against other state-of-the-art policy architectures. Based on the results, we conclude that in spite of relatively simple structure, KODex can achieve comparable performances. Besides, unlike neural networks, KODex does not require much hyper-parameter tuning and is not sensitive to parameter initialization (random seeds).
    % % \item We explicitly incorporate the object states into the original state space, which is essential to capture the underlying intentional hand-object interactions from the expert agent. 
% \end{itemize}

\section{Related Work}\label{sec:related_work}
In this section, we discuss related work  from different sub-fields of robotics and their connection to our work.

\subsection{Learning Manipulation Skills as Dynamical Systems}
\label{sec:rw_learning_DS}
% talk about dynamical systems for 7DoF arm , not high-dimsional
% to learn dynamical system, instead of learning in action space
Our work falls into the category of dynamical-system-based imitation learning methods for manipulation (see \citet{Harish2022Survey} for a detailed discussion). Made popular by the seminal Dynamics Movement Primitives (DMPs)~\cite{ijspeert2013dynamical}, these methods model robot motions as solutions to an underlying dynamical system which can be learned from demonstrations. The past decade saw a plethora of approaches that have built upon the same principle (e.g., \cite{khansari2011learning,neumann2015learning,Harish2017Dynamical,rana2020learning, rana2020euclideanizing,figueroa2022locally}), creating increasingly capable LfD tools for manipulation. Robustness to perturbations, high sample efficiency, and provable convergence are all but a few examples of the many advantages of dynamical-system-based approaches. These approaches are highly structured and leverage control-theoretic and topological tools to learn complex desired motions from demonstrations in a highly sample efficient manner. In fact, most of these approaches have demonstrated that goal-directed motions can be learned from as few as six demonstrations. More recently, \citet{bahl2020neural} developed Neural Dynamic Policy (NDP) which embeds the dynamical systems structure into deep neural network-based policies to enable end-to-end learning.
However, a common limitation of these approaches is that they were designed to capture end-effector skills for serial-link manipulators. As such, they deal with low-dimensional systems with limited degrees-of-freedom (DOFs). We discuss a few recent exceptions to this limitation in Section~\ref{sec:rw_learning_DM}. 
In contrast, our work investigates the utility of Koopman operators in learning dexterous manipulation skills on high-DOF platforms.
% a dynamical-system-based approach, encode the desired motion with a dynamical system containing a linear proportional derivative (PD)-type term coupled with a nonlinear term. However, DMPs learn one dynamical model for each robot dimension separately, thereby neglecting the coupling effect across robot dimensions. 
% In \cite{Harish2017Dynamical}, the motion dynamics were approximated using a Gaussian mixture model (GMM), and its parameters were learned from demonstrations subject to constraints derived from partial contraction analysis. It has been shown that the learned systems can accurately reproduce the demonstrations on a 7-DoF robot arm. Nonetheless, for higher dimensional systems, it is computationally intractable to learn such motion parameters under these constraints. 
% In \cite{bahl2020neural}, a Neural Dynamic Policy (NDP) was proposed to embed structures of a dynamical systems into deep neural network-based policies. Specifically, the parameters of the dynamical system were predicted as outputs of neural networks. 
Further, existing dynamical-system approaches rely on expertly-designed hyperparameters and their efficacy is sensitive to parameter initializations.
On the contrary, Koopman operators can be learned \textit{analytically} from demonstrations, alleviating the dependence on painstaking hyperparameter tuning and unreliable numerical optimization.
% However, this method tends to be sensitive to parameter tuning. 
% In addition to these structures, Koopman operator theory, the fundamental component of KODex, excels at modelling complex dynamics in a linear representation.
% they are NN-based methods, involving lots of hyperparater tuning and sensitive to parameter initialization.

\subsection{Learning Dexterous Manipulation Skills}
\label{sec:rw_learning_DM}
% For high dimensional robotic tasks, people use RL/IL to learn the policy, pure black box, no one knows what happen, super time consuming

% some policies are directly defined in action space, 
% For instance, \citet{OpenAI2018Cube} used DRL to learn a Rubik's cube's reorientation policy in simulation and deployed it on a physical robot hand via sim-to-real techniques; \citet{chen2021a} proposed teacher-student networks 
Deep Reinforcement Learning (RL) has been dominating the field of dexterous manipulation recently, enabling a variety of impressive autonomous dexterous manipulation skills. One of the most popular approaches, developed by \citet{OpenAI2018Cube}, demonstrated that a high-DOF multi-finger hand can learn to solve the Rubik's cube. More recently, \citet{chen2021a} developed a model-free RL framework that was capable of reorienting over 2000 geometrically different objects and showed strong zero-shot transfer to new objects. Along similar lines, several recent approaches have demonstrated the effectiveness of RL for dexterous manipulation~\cite{nagabandi2020deep, qi2022hand, zhu2019dexterous}. Despite their impressive successes, RL-based algorithms suffer from poor sample efficiency and their policies are notoriously difficult to to train. 
In contrast, Imitation learning (IL) aims to improve sample efficiency by leveraging expert demonstrations~\cite{osa2018algorithmic,Harish2022Survey}. However, most existing IL-based methods (including those discussed in Sec.~\ref{sec:rw_learning_DM}) focus on lower-DOF manipulators and do not scale well to high-dimensional systems. Indeed, there are at least two recent notable exceptions to this limitation. \citet{xieneural} developed a highly structured IL method to learn dexterous manipulation skills from demonstrations in the form of a dynamical system. \citet{arunachalam2022holo} introduced a mixed-reality framework that can collect high-quality demonstrations and learn dexterous manipulation skills by leveraging visual representations and motion retargeting. 
Recent work has also attempted to combine IL with RL in efforts to get the best of both approaches and has been able to achieve impressive performance (e.g.,~\cite{kumar2016learning,Rajeswaran2018DAPG}).  
A common attribute of existing learning methods to dexterous manipulation, irrespective of whether they leverage RL, IL, or both, is that they rely on large neural networks that have to be trained via numerical optimization. As a result, training typically requires considerable computational resources and relies on significant user expertise for implementation, and hyperparameter tuning.
% there are much fewer IL-based approaches focused on dexterous manipulation (\cite{xieneural, arunachalam2022holo}), perhaps due to the prohibitive cost involved in collecting a large number of demonstrations. 
Furthermore, the effectiveness of their approaches is highly sensitive to parameter initialization \cite{henderson2018deep}. 
In stark contrast, KODex encodes complex skills as linear dynamical systems that can be learned analytically from demonstrations. As such, KODex demands significantly fewer computational resources and is not sensitive to parameter initialization. Further, unlike opaque deep neural networks, KODex learns a linear dynamical system that can be inspected via a wide array of control-theoretic tools.

\subsection{Koopman Operator for Robotics}
% talk about the Koopman robotics work, not high-dimensional
Recently, Koopman operator theory has been applied to various robotic systems. It has been shown that learning Koopman operators can enable model-based control of differential drive mobile robots~\cite{abraham2017model}, and data-driven control of spherical and serial-link manipulator robots~\cite{Abraham2019ActiveKoopman}. Recent approaches have also shown that it is possible to model and control a soft robot manipulator using Koopman operator \cite{bruder2019modeling,Bruder2021Soft}. However, the robotic systems investigated in these works are low-dimensional and have limited DOFs. In contrast, our work is focused on evaluating the effectiveness of Koopman operators in learning skills for a high-dimensional robotic system with more complex dynamics (e.g. a multi-finger dexterous hand). Further, prior works have not presented rigorous comparisons of their Koopman-based methods against SOTA learning methods built upon neural network architectures. As such, we do not yet fully understand the benefits of leveraging Koopman operator when comparing to SOTA learning approaches, and the circumstances under which these benefits hold.
% This leaves us a question of whether or under what tasks/metrics Koopman operator can better model complex system dynamics than neural networks. Therefore, 
In our work, we thoroughly evaluate KODex against SOTA imitation learning methods within the context of four dexterous manipulation tasks in terms of various performance metrics.
% acknowledge that our method belongs to learning dynamics from demonstrations and there are lots of cool work along this direction

%
\section{Preliminary: Koopman Operator Theory}
We begin by providing a brief introduction to Koopman operator theory -- a control-theoretic approach used to represent nonlinear dynamical systems as a linear system of equations~\cite{Koopman1931Koopman}.
% We first describe the infinite-dimensional Koopman operator representation of the autonomous dynamical system in Section.~\ref{sec:koopman}.
% Then, we present a practical system identification method to extract the matrix approximation of the Koopman operator on the finite-dimensional lifted state space via linear regression in Section.~\ref{sec:approximation}.
\subsection{Koopman Operator Representation}
\label{sec:koopman}
Consider a discrete-time autonomous nonlinear dynamical system whose state evolution is defined by
\begin{equation}
\label{eqn:system}
\mathrm{x}(t+1) = F(\mathrm{x}(t)), 
\end{equation}
where $\mathrm{x}(t) \in \mathcal{X} \subset \mathbb{R}^n$ represents the system state at time $t$, and $F(\cdot)$ is an arbitrary nonlinear mapping: $\mathbb{R}^n \rightarrow 
\mathbb{R}^n$. 

To represent the the nonlinear dynamical system in (\ref{eqn:system}) as a linear system, we can begin by introducing a set of \textit{observables} using the so-called \textit{lifting function} $g:\mathcal{X} \rightarrow \mathcal{O}$,
where $\mathcal{O}$ is the space of observables. We can now define the \textit{Koopman Operator} $\mathcal{K}$, an infinite-dimensional operator on the lifting function $g(\cdot)$ as follows
\begin{equation}
\label{eqn:KG}
[\mathcal{K}g] = g \circ F(\mathrm{x}(t))
\end{equation}
where $\circ$ is the compositional operator. For the discrete time system defined in (\ref{eqn:system}), the above equation can be rewritten as
\begin{equation}
\label{eqn:KG_2}
[\mathcal{K}g] = g(F(\mathrm{x}(t))) = g(\mathrm{x}(t+1))
\end{equation}
If the observables belong to a vector space, the Operator $\mathcal{K}$ can be seen as an infinite-dimensional linear map that describes the evolution of the observables as follows
\begin{equation}
\label{eqn:observable_system}
g(\mathrm{x}(t+1)) = \mathcal{K}g(\mathrm{x}(t))
\end{equation}
% Now, let's define the lifting function as , which maps original states from $X$ to lifted states $\mathbb{C} \in \mathbb{R}^p$. Put it in other words, $g(\mathrm{x}_t)$ is a vector of $p$ scalar-valued functions $\phi(\mathrm{x}_t)$:
% \begin{equation}
% \label{eqn:gx}
% g(\mathrm{x}_t) = [\phi_1(\mathrm{x}_t), \phi_2(\mathrm{x}_t), \ldots, \phi_p(\mathrm{x}_t)]^\top,
% \end{equation}
% If $p = \infty$, the Koopman operator $\mathcal{K}$ is an infinite dimensional operator that can directly act on the element of $\mathbb{C}$ to describe the system evolution in the lifted state space
% As long as $g(\mathrm{x}_t)$ is defined in vector space as illustrated in Eqn.~\ref{eqn:gx}, $\mathcal{K}$ is a linear operator, so Eqn.~\ref{eqn:KG} can be rewritten as
% \begin{equation}
% \label{eqn:newKG}
% % [\mathcal{K}g](\mathrm{x}_t) = 
% g(\mathrm{x}_{t+1}) = \mathbf{K}g(\mathrm{x}_t),
% \end{equation}
% where $\mathbf{K} \in \mathbb{R}^{p \times p}$. 
Therefore, the Koopman operator $\mathcal{K}$ linearly propagates forward the infinite-dimensional lifted states (i.e., observables). 

% The system defined in Eqn.~\ref{eqn:system} only cares about the robot states, for example, hand joint positions. However, for dexterous manipulation tasks, robots need to \textbf{understand the object status and make decisions accordingly}. As a result, the effects of object status can implicitly alter the underlying autonomous system dynamics, which is represented by $F$ in Eqn.~\ref{eqn:system}. Hence, in order to take these effects into consideration, we incorporate the object states $\mathrm{y}_t$ into the autonomous dynamical system, where $\mathrm{y}_t \in Y \subset \mathbb{R}^m$ and $Y$ is also a compact set. For simplicity, we introduce another symbol $\hat{\mathrm{x}}_t = [\mathrm{x}_t, \mathrm{y}_t]$ and $\hat{\mathrm{x}}_t \subset \hat{X} \in \mathbb{R}^{n+m}$. Intuitively, we also specify that the input to the scalar-valued functions in Eqn.~\ref{eqn:gx} are either the robot states or object states. Therefore, Eqn.~\ref{eqn:system}, \ref{eqn:gx}, \ref{eqn:newKG} are modified as follow
% \begin{equation}
% \label{eqn:newsystem}
% \hat{\mathrm{x}}_{t+1} = \hat{F}(\hat{\mathrm{x}}_t),
% \end{equation}
% \begin{equation}
% \hat{g}(\hat{\mathrm{x}}_t) = [\phi_1(\mathrm{x}_t), \ldots, \phi_r(\mathrm{x}_t), \psi_1(\mathrm{y}_t), \ldots,\psi_{p-r}(\mathrm{y}_t)]^\top,
% \end{equation}
% \begin{equation}
% \label{eqn:newRelation}
% [\mathcal{K}\hat{g}](\hat{\mathrm{x}}_t) = \hat{g}(\hat{\mathrm{x}}_{t+1}) =  \mathbf{K}\hat{g}(\hat{\mathrm{x}}_t),
% \end{equation}
% Where $ r \in \mathbb{R}^{+} < p$, $\hat{F}$ is  another nonlinear mapping: $\mathbb{R}^{n+m} \rightarrow \mathbb{R}^{n+m}$, $\hat{g}$ maps the states from $\hat{X}$ to $\mathbb{C}$ and $\psi_i(\mathrm{y}_t), i=1,\ldots,p-r$ are another set of scalar-valued functions with input as object states. It is clear that $\hat{g}(\hat{\mathrm{x}}_t)$ is still in vector space containing $p$ elements, so the Koopman operator remains linear.

% Under this setting, when we collect demonstration data for Koopman operator approximation, the evolution of robot states and object states are assumed to be the solutions of Eqn~.\ref{eqn:newsystem}. 

%after describing the dynamics, we mention that all the trajectories are the solution of such autonomous system.

In practice, we do not benefit from this representation since it is infinite-dimensional. However, we can approximate $\mathcal{K}$ using a matrix $\mathbf{K}\in\mathbb{R}^{p \times p}$ and define a finite set of observables $\phi_t = g(x(t)) \in \mathbb{R}^p $. Thus, we can rewrite the relationship in (\ref{eqn:observable_system}) as
\begin{equation}
\label{eqn:approximate_KG}
 g(\mathrm{x}(t+1)) =  \mathbf{K}g(\mathrm{x}(t)) + r(\mathrm{x}(t)),
\end{equation}
where $r(\mathrm{x}(t)) \in \mathbb{R}^p$ is the residual error caused by the finite dimensional approximation, which can be arbitrarily reduced based on the choice of $p$. 

\subsection{Learning Koopman Operator from Data}
\label{subec:learning_koopman}
Given a choice of observables, it is possible to learn the matrix operator $\mathbf{K}$ directly from a dataset $D = [\mathrm{x}(1), \mathrm{x}(2), \cdots, \mathrm{x}(T)]$, which contains the solution to the system in (\ref{eqn:system}) for $T$ time steps.
% Now, suppose we have $N$ pieces of demonstration data and each piece is the full trajectory of system evolution, which is assumed to be an exact solution of Eqn.~\ref{eqn:newsystem}. Each piece of demonstration data consists of $T$ datapoints, where $T$ represents the task-specific time horizon. One datapoint contains the robot states and object states at one time step $t$. The demonstration data $D$ can be defined as
% \begin{equation}
% D = [\{\hat{\mathrm{x}}_t^{1}\}^{t=T}_{t=1}, \{\hat{\mathrm{x}}_t^2\}^{t=T}_{t=1}, \cdots, \{\hat{\mathrm{x}}_t^N\}^{t=T}_{t=1}],
% \end{equation}
% It should be noted that a recorded $n$th trajectory with missing datapoints is also acceptable, as long as all the state pairs $(\hat{\mathrm{x}}_t^n, \hat{\mathrm{x}}_{t+1}^n)$ are temporally consecutive. Additionally, we also record the actuated torque $\mathrm{\tau}_t$ in each datapoint for the controller design discussed in Section.~\ref{sec:control}. 

Given the choice of $g(\cdot)$, the finite dimensional Koopman matrix $\mathbf{K}$ can now be determined by minimizing the approximation error defined in (\ref{eqn:approximate_KG}). Specifically, we can obtain $\mathbf{K}$ from $D$ by minimizing the cost function $\mathbf{J}(\mathbf{K})$ given below
\begin{equation}
\label{eqn:goal}
\begin{split}
\mathbf{J}(\mathbf{K}) & = \frac{1}{2}\sum_{t=1}^{t=T-1} \Vert r(\mathrm{x}(t)) \Vert ^2 \\
& =\frac{1}{2}\sum_{t=1}^{t=T-1} \Vert g(\mathrm{x}(t+1)) - \mathbf{K}g(\mathrm{x}(t)) \Vert ^2
\end{split}
\end{equation}
% via minimizing $\mathcal{J}$, which is the $L_2$ sum of residual error $r(\mathrm{x}_t)$ on all datapoints in $D$

Note minimizing $\mathbf{J}(\mathbf{K})$ amounts to solving a least-square problem, whose solution is given by~\cite{williams2015data}
\begin{equation}
\label{eqn:k}
\mathbf{K} = \mathbf{A}\mathbf{G}^\dagger
\end{equation}
where $\mathbf{G}^\dagger$ denotes the Mooreâ€“Penrose inverse of $\mathbf{G}$, and 
\begin{equation}
\label{eqn:k_comp}
\begin{split}
\mathbf{A} = \frac{1}{T-1}\sum_{t=1}^{t=T-1}g(\mathrm{x}(t+1)) \otimes g(\mathrm{x}(t))  \\ 
\mathbf{G} = \frac{1}{T-1}\sum_{t=1}^{t=T-1}g(\mathrm{x}(t)) \otimes g(\mathrm{x}(t))
\end{split}    
\end{equation}
where $\otimes$ denotes the outer product between two vectors. 

% Thus, we can utilize the computed Koopman matrix $\mathbf{K}$ to make predictions in lifted state space via linear evolution (Eqn.~\ref{eqn:approximate_KG}) starting from $g(\mathrm{x}_1)$.

% In order to capture more underlying dynamics of the original system, the higher dimension of the lifted states is preferred. Furthermore, the data distribution in $D$ has a dominant effect on the generalizability of computed Koopman matrix $\mathbf{K}$. Therefore, there is a trade-off between the number of demonstration data and the cost of date collection, especially when data is expensive, e.g. collecting data on a physical robot. To grab a hint on how much demonstration data is "enough" to obtain desirable results, we first initialize an empty data set $\bar{D}$ and another empty error list $E$ in experiments. During each iteration ($N$ in total), we add one piece of demonstration data $\{\hat{\mathrm{x}}_t^{n}\}^{t=T}_{t=1}$ selected from $D$ into $\bar{D}$ and adaptively obtain the Koopman matrix $\mathbf{K}_{\bar{D}}$ using Eqn.~\ref{eqn:k} \ref{eqn:k_comp}. Then, we compute the $L_2$ sum of residual error $e_n$ on full demonstration data $D$ using $\mathbf{K}_{\bar{D}}$ and add $e_n$ into $E$. At the last iteration, we will have $\mathbf{K}_{D} = \mathbf{K}_{\bar{D}}$.


\section{Learning Koopman Operators for Dexterous Manipulation}
In this section, we introduce our learning framework KODex. 
We begin by introducing our framework to model dexterous manipulation skills as nonlinear dynamics and discuss the importance of incorporating object states into the system (Section~\ref{sec:modeling}). Next, we describe how KODex learns the reference dynamics for a given skill from demonstrations (Section~\ref{sec:demons}). Finally, we discuss how to learn a low-level controller, also from demonstrations, in order to faithfully track the reference trajectories generated by KODex (Section~\ref{sec:control}).
% we formulate the learning framework for dexterous manipulation using Koopman operator theory. First, in Section.~\ref{sec:incorp} and \ref{sec:demons}, we demonstrate the modified components of Koopman operator, which are necessary to the complex dexterous manipulation tasks.
% Therefore, the unknown nonlinear dynamical model of the dexterous manipulation system is completely constructed by the linear model in the lifted space. Besides, the hand states can be easily retrieved from the lifted states, which serve as the referenced hand trajectories. 
% Next, 
% in order to execute the trajectories on a dexterous hand, 
% we show how to separately learn an inverse dynamics controller from demonstration data in Section~\ref{sec:control}, which takes the input as the consecutive pair of robot states and outputs the actuation torques. Finally, we describe how to execute the learned Koopman operator in Section~\ref{sec:implementation}. The framework overview can be found in 
Further, an overall pseudo-code for KODex can be found in Algorithm~\ref{alg: overview}.
\subsection{Modeling Dexterous Manipulation Skills}
\label{sec:modeling}
A central principle behind KODex is that the desired behavior of a robot can be represented using a dynamical system. 
% To that end, we begin by defining the state of the dynamical system. 
Note that, unlike other kinds of manipulation skills (e.g., end-effector skills of multi-link manipulators), dexterous manipulation is explicitly concerned with how an object moves as a result of the robot's motion~\cite{okamura2000overview}. As such, KODex captures the desired motion of the robot along with that of the object. To this end, we define the state at time $t$ as $\mathrm{x}(t) = [{\mathrm{x}_r(t)}^\top, {\mathrm{x}_o(t)}^\top]^\top$, where ${\mathrm{x}_r(t)} \in \mathcal{X}_r \subset \mathbb{R}^n$ and ${\mathrm{x}_o(t)} \in \mathcal{X}_o \subset \mathbb{R}^m$ represent the state of the robot and the object, respectively, at time $t$. As such, the dynamical system we wish to capture is
\begin{equation}
\label{eqn:F*}
    \mathrm{x}(t+1) = F^*(\mathrm{x}(t))
\end{equation}
where $F^*(\cdot):\mathcal{X}_r \times \mathcal{X}_o \rightarrow \mathcal{X}_r \times \mathcal{X}_o $ denotes the true dynamics that govern the \textit{interdependent} motions of the robot and the object. Note that the dynamical system above is time-invariant. Indeed, it has been demonstrated that time-invariant dynamical systems provide a natural way to capture manipulation skills that is more robust to intermittent perturbations than those that explicitly depend on time~\cite{Harish2022Survey}.

A key challenge in learning the dynamical system in (\ref{eqn:F*}) is that it can be arbitrarily complex and highly nonlinear, depending on the particular skill of interest. KODex leverages Koopman operator theory to learn a \textit{linear} dynamical system that can effectively approximate such complex dynamics. To this end, we first define a set of observables through the lifting function $g(\mathrm{x})$ as follows
\begin{equation}
\label{eqn:gx}
g(\mathrm{x}(t)) = [{\mathrm{x}_r(t)}^\top, \psi_r({\mathrm{x}_r(t)}), {\mathrm{x}_o(t)}^\top, \psi_o({\mathrm{x}_o(t)})]^\top, \forall t
\end{equation}
where $\psi_r:\mathbb{R}^n \rightarrow \mathbb{R}^{n\prime}$ and $\psi_o:\mathbb{R}^m \rightarrow \mathbb{R}^{m\prime}$ are vector-valued lifting functions that transform the robot and object state respectively. Now, the linear dynamical system in the space of observables will take the same form as in (\ref{eqn:approximate_KG}).

In our implementation, we use polynomial functions up to a finite degree in our lifting function since polynomial functions allow for flexible definition of complex functions. However, it is important to note that KODex is agnostic to the specific choice of observables. 
Further, we do not assume that we know the ideal set of observables for any given skill. Instead, as we demonstrate in Section \ref{sec:results}, KODex can learn different skills on the same space of observables. 
% Instead, KODex leverages polynomial functions up to a certain degree as basis functions, and the learned Koopman operator $\mathbf{K}$ determines their relative influence on the system.

% The system defined in Eqn.~\ref{eqn:system} only cares about the robot states, for example, hand joint positions. However, for 
% dexterous manipulation tasks, robots need to \textbf{understand the object status and make decisions accordingly}. As a result, the effects of object status can implicitly alter the underlying autonomous system dynamics, which is represented by $F$ in Eqn.~\ref{eqn:system}. Hence, in order to take these effects into consideration, we need to incorporate the object states $\mathrm{y}_t$ into the autonomous dynamical system, where $\mathrm{y}_t \in Y \subset \mathbb{R}^m$ and $Y$ is also a compact set. From another perspective, the operation over $Y$ serves to \textbf{remove the time dependency} across robot trajectories $X$, rendering it possible to model time invarient tasks.
% For simplicity, we introduce another symbol $\hat{\mathrm{x}}_t = [\mathrm{x}_t, \mathrm{y}_t]$ and $\hat{\mathrm{x}}_t \subset \hat{X} \in \mathbb{R}^{n+m}$. Intuitively, we also specify that the input to the scalar-valued functions in Eqn.~\ref{eqn:gx} are either the robot states or object states. Therefore, Eqn.~\ref{eqn:system}, \ref{eqn:gx}, \ref{eqn:newKG} are modified as follow
% \begin{equation}
% \label{eqn:newsystem}
% \hat{\mathrm{x}}_{t+1} = \hat{F}(\hat{\mathrm{x}}_t),
% \end{equation}
% \begin{equation}
% \label{eqn: new_g}
% \hat{g}(\hat{\mathrm{x}}_t) = [\phi_1(\mathrm{x}_t), \ldots, \phi_r(\mathrm{x}_t), \psi_1(\mathrm{y}_t), \ldots,\psi_{p-r}(\mathrm{y}_t)]^\top,
% \end{equation}
% \begin{equation}
% \label{eqn:newRelation}
% % [\mathcal{K}\hat{g}](\hat{\mathrm{x}}_t) = 
% \hat{g}(\hat{\mathrm{x}}_{t+1}) =  \mathbf{K}\hat{g}(\hat{\mathrm{x}}_t),
% \end{equation}
% Where $ r \in \mathbb{R}^{+} < p$, $\hat{F}$ is  another nonlinear mapping: $\mathbb{R}^{n+m} \rightarrow \mathbb{R}^{n+m}$, $\hat{g}$ maps the states from $\hat{X}$ to $\mathbb{C}$ and $\psi_i(\mathrm{y}_t), i=1,\ldots,p-r$ is another set of scalar-valued functions with input as object states. It is clear that $\hat{g}(\hat{\mathrm{x}}_t)$ is still in vector space, so the Koopman operator remains linear. As a result, the Koopman matrix $\mathbf{K}$ can be computed similarly as shown in Section.~\ref{sec:approximation}.

\subsection{Learning Reference Dynamics}
\label{sec:demons}
% For complex tasks,
% (e.g. dexterous manipulation in this work)
% learning a robust policy requires a significant amount of system trajectories. Therefore, we deploy a well-trained RL agent \cite{Rajeswaran2018DAPG} in simulation and record its rollouts as demonstration data.

We now turn to the challenge of learning the Koopman operator $\mathbf{K}$ from demonstrations. Let 
% suppose we have $N$ demonstrated trajectories and each trajectory has the full system evolution, which is assumed to be an exact solution of (\ref{eqn:F*}). 
% Each trajectory has $T$ time steps, and $T$ depends on specific skills. At each time step $t$, we record the robot states and object states, which give us
$
D = [\{{\mathrm{x}}^{(1)}(t),\mathrm{\tau}^{(1)}(t)\}^{t=T^{(1)}}_{t=1}, \cdots, \{{\mathrm{x}}^{(N)}(t),\mathrm{\tau}^{(N)}(t)\}^{t=T^{(N)}}_{t=1}]$
denote a set of $N$ demonstrations containing trajectories of state-torque pairs.
% denote a set of $N$ demonstrations, where $\{\mathrm{x}^{(n)}(t)\}^{t=T^{(n)}}_{t=1}$ is the $n$th demonstration containing the evolution of both the robot and the object states for $T^{(n)}$ time steps.
Now, we can compute the Koopman matrix as $\mathbf{K} = \mathbf{A}\mathbf{G}^\dagger$, where $\mathbf{A}$ and $\mathbf{G}$ can be computed by modifying (\ref{eqn:k_comp}) as follows
\begin{equation}
\label{eqn:k_comp_new}
\begin{split}
\mathbf{A} =\sum_{n=1}^{n=N}\sum_{t=1}^{t=T^{(n)}-1}  \frac{g(\mathrm{x}^{n}(t+1)) \otimes g(\mathrm{x}^n(t))}{N(T^{(n)}-1)} \  \\ 
\mathbf{G} = \sum_{n=1}^{n=N}\sum_{t=1}^{t=T^{(n)}-1} \frac{g(\mathrm{x}^{n}(t)) \otimes g(\mathrm{x}^n(t))}{N(T^{(n)}-1)}
\end{split}    
\end{equation}
It is worth noting that KODex can also leverage partial trajectories that do not complete the task, as long as all the state pairs $(\mathrm{x}^n(t), \mathrm{x}^n(t+1))$ are temporally consecutive. Additionally, we also record the actuated torque $\mathrm{\tau}(t)$ at each time step for the controller design discussed in Section.~\ref{sec:control}. 

Once the reference dynamics are learned, we will able to integrate it to generate a rollout in the observables space. However, in order to command the robot, we need to obtain the rollouts in the original robot states. Recall from Eq. (\ref{eqn:gx}) that we designed $g(\mathrm{x}(t))$ such that both robot state $\mathrm{x}_r(t)$ and object state $\mathrm{x}_o(t)$ are part of the observables. As such, we can retrieve the desired robot state trajectory $\{\hat{\mathrm{x}}_r(t)\}$ by selecting the corresponding elements in $g(\mathrm{x}(t))$. 
% Note that in practice, we are only interested in the predicted robot states $\mathrm{x}_r(t+1)$ retrieved from $g(\mathrm{x}(t+1))$.

% In order to capture more underlying dynamics of the original system, the higher dimension of the lifted states ($\hat{g}(\hat{\mathrm{x}}_t)$) is preferred.

Indeed, the data distribution in $D$ has a considerable effect on the generalizability of the computed Koopman matrix $\mathbf{K}$. Therefore, there is an inevitable trade-off between the number of demonstrations and the cost of data collection - a challenge shared by most imitation learning algorithms.

% To grab a hint on how much demonstration data is "enough" to obtain desirable results, in experiments, we first initialize an empty data set $\bar{D}$ and another empty error list $E$. During each iteration ($N$ in total), we add one trajectory $\{\hat{\mathrm{x}}_t^{n}\}^{t=T}_{t=1}$ selected from $D$ into $\bar{D}$ and adaptively obtain the Koopman matrix $\mathbf{K}_{\bar{D}}$. Then, we compute the $L_2$ sum of residual error $e_n$ on full demonstration data $D$ using $\mathbf{K}_{\bar{D}}$ and add $e_n$ into $E$. At the last iteration, we will have $\mathbf{K}_{\bar{D}} = \mathbf{K}_{D}$ as $\bar{D} = D$.

\begin{algorithm}[t] 
\SetAlgoLined
\textbf{Demonstration Data Collection} \\
Initialize $D = \varnothing$; \\
\For{$n\in \{1, ..., N\}$}{
Generate a $T^{(n)}$-horizon trajectory of states and torques $\{[\mathrm{x}^{n}(t), \mathrm{\tau}^{n}(t)]\}^{t=T^{(n)}}_{t=1}$;\\
Add $\{[\mathrm{x}^{n}(t), \mathrm{\tau}^{n}(t)]\}^{t=T^{(n)}}_{t=1}$ to $D$;\\
}
\textbf{Koopman Operator Approximation} \\
Determine lifting function $g(\mathrm{x}(t))$; \\
% Initialize $\bar{D} = \varnothing$; \\
% Initialize $E = [\,]$; \\
% \For{$n\in \{1, ..., N\}$}{
% Select $\{\hat{\mathrm{x}}_t^{n}\}^{t=T}_{t=1}$ from $D$ and add it to $\bar{D}$; \\
% Compute $\mathbf{K}_{\bar{D}}$ on $\bar{D}$ with $\hat{g}(\hat{\mathrm{x}}_t)$ via Eqn.~\ref{eqn:k} \ref{eqn:k_comp}; \\
% Compute the $L_2$ sum of residual error $e_n$ on $D$ using $\mathbf{K}_{\bar{D}}$; \\
% Add $e_n$ to $E$; \\
% }
Compute $\mathbf{K}$ on $D$ (\ref{eqn:k}, \ref{eqn:k_comp_new}); \\
\textbf{Controller Design} \\
Build a controller $C$ as a neural network with inputs as $(\mathrm{x}_r(t), \mathrm{x}_r(t+1))$ and output as $\mathrm{\tau}(t)$;\\
Train $C$ using state-torque pairs $(\mathrm{x}_r^n(t), \mathrm{x}_r^n(t+1),\mathrm{\tau}^n(t))$ in $D$ (\ref{eqn: controller});\\
\textbf{Implementation} \\
Specify the initial states $\mathrm{x}(1)$; \\
\For{$t\in \{1, ..., T-1\}$}{
Predict the next robot states $\hat{\mathrm{x}}_r(t+1)$ using $\mathbf{K}$ (\ref{eqn:observable_system} \ref{eqn:gx}); \\ 
Read the current robot states $\mathrm{x}_r(t)$;\\
Generate the torque $\mathrm{\tau}(t)$ using $C$ on $(\mathrm{x}_r(t), \hat{\mathrm{x}}_r(t+1))$ and execute it;
}
\caption{KODex}
\label{alg: overview}
\end{algorithm} 

\subsection{Learning the Controller}
\label{sec:control}
% also, mention the lifting functions include the original states themselves, from the first n columns.
% To execute the learned skill on the robot, we first need to retrieve the robot states from the lifted states.
% For convenience, we define
% \begin{equation}
% \label{eqn:gx}
% g(\mathrm{x}(t)) = [{\mathrm{x}_r(t)}^\top, \psi_r({\mathrm{x}_r(t)}), {\mathrm{x}_o(t)}^\top, \psi_o({\mathrm{x}_o(t)})]^\top,
% \end{equation}
% where $\psi_r$ and $\psi_o$ are both set of vector-valued lifting functions.

To track the desired trajectories generated from the learned reference dynamics, we design an inverse dynamics controller $C$ \cite{hanna2017grounded} \cite{bahl2020neural}, which takes as input the current robot state $\mathrm{x}_r(t)$ and the desired next state from the reference trajectory  $\hat{\mathrm{x}}_r(t+1))$, and generates the torque $\mathrm{\tau}(t)$ required for actuation. The robot state can be read from simulators or robot sensors. 

We use a multi-layer perception (MLP) to parameterize the tracking controller and train it with supervision using the recorded state-torque pairs $(\mathrm{x}_r^n(t), \mathrm{x}_r^n(t+1),\mathrm{\tau^n}(t))$ from the demonstrations. In other words, we minimize:
\begin{equation}
\label{eqn: controller}
\mathcal{L}_{\text{control}} =\sum_{n=1}^{n=N}\sum_{t=1}^{t=T^{(n)}-1} \frac{|C(\mathrm{x}_r^n(t), \mathrm{x}_r^n(t+1)) - \mathrm{\tau}^n(t)|^2}{N(T^{(n)}-1)}
\end{equation}

% The alternative is to fine-tune a PID controller, which requires extra manual work but does not need access to the demonstrated torques. Thus, during data collection, we only record the robot and object states and no control information is needed.
Note that the controller can also be trained without the demonstrations by collecting self-play data. 
% It should be noted that the PID controller should run at a higher frequency than the trajectory generation.
Indeed, it has been shown that it is possible to separately train a controller via reinforcement learning to track the reference trajectories~\cite{peng2020learning}. Note that KODex does not require torque information to learn the reference dynamics. As such, if the controller is training from self-play data, KODex can learn skills from state-only observations~\cite{torabi2019recent}.

\subsection{Execution}
\label{sec:implementation}
% From the previous sections, we already have the lifting function $g(\cdot)$, the computed Koopman operator $\mathbf{K}$, and the controller $C$. 
With the reference dynamics and the tracking control learned, we detail how we execute the policy in this section.
Now, suppose $\mathrm{x}(1) = (\mathrm{x}_r(1), \mathrm{x}_o(1))$ is the given initial state, we can then generate the reference trajectory ($\{\hat{\mathrm{x}}_r(t)\}_{t=1}^T$) by integrating the learned reference dynamics. Further, at time step $t$, we pass the current robot state $\mathrm{x}_r(t)$ and the desired next robot state $\hat{\mathrm{x}}_{r}(t+1)$ to the controller $C$ to compute the required torque $\mathrm{\tau}(t)$.

\section{Experimental Design}

% {\color{red} I think the experiments section should include:
% \begin{itemize}
%     \item Using manually designed observables and NN-based controller, report the best performance of each task and make comparisons with DRL agents(The expert agent that is trained for hours). Also, report training details, e.g. training tine. Make several hand joint tracking figures.
%     \item Test if we can replace the NN from demo with NN from self-play
%     \item Using the same demonstration data, train a BC agent from scratch, input: current hand state + object states, output: next hand states, using the same controller for simulation. This could give us an insight of how good Koopman-based imitation learning is. Three agent for comparison: koopman agent, expert DRL agent, and BC agent. The reason of adding BC is to test how well Koopman-based IL can beat the NN-based methods. Koopman has two clear benefits: linear representation and potential extension to contradiction analysis.
%     \item Show the results of pen task using PID controller (just this one task I think, to sell the idea of learning from observation, as this is getting hotter)
%     \item To see if we can demonstrate the system on the PYSONIC hand in Mujoco: 1). build a simulator, 2). train it via RL, and 3). apply domain randomization to see if it can be used in real world. (this would take lots of time)
%     \item TO see if we can directly learn the Koopman policy from the human demonstration provided by DAPG project
%     \item Compare the results of proprioceptive experiments, which does not incorporate the object trajectories.  For pen task.
%     \item Missing pairs in the trajectories.
%     \item Hammer tasks, OOD cases perform worse. Include the OOD cases in Demo and see how it goes. Expect to be much better, as IL suffers most from OOD cases. Covariate shift for IL is a common issue, can be solved using other techniques. 
%     \item Train the control policy via RL, unlike the RSS2020 paper where for different gaits, the number of actuation joints are the same. For our cases, different tasks have different actuators.  While it may be possible to set some unused ones to be zeros. But even for the RSS paper, each walking gait they train a separate control policy(MLP). Probably this would give best performance.
% \end{itemize}}
To evaluate KODex and compare it to existing approaches, we designed and carried out a series of experiments. We designed our experiments on KODex in terms of its i) general efficacy (Sec. \ref{sec:general_efficacy}), ii) sample efficiency (Sec.~\ref{sec:sample_efficiency}), iii) Out-of-distribution generalization (Sec.~\ref{sec:ood}), and iv) robustness to changes in physical properties (Sec.~\ref{sec:robustness}).

% i) How KODex contributes to the state-of-the-art in imitation learning for dexterous manipulation? (see Section \ref{sec:IL_result}), and ii) Are the reference dynamics learned by KODex robust to variations in physical properties (see Section \ref{sec:robustness}). 
% Next, we will describe the robotic system, baselines, tasks, and metrics.

% The source code to reproduce our results can be found at: ??.

\subsection{Evaluation Platform}
We conducted all our experiments on the widely-used ADROIT Hand~\cite{Rajeswaran2018DAPG} -- a 30-DoF simulated system (24-DoF hand + 6-DoF floating wrist base) built with MuJoCo~\cite{Todorov2012MUJuCO}.

% The goal is to efficiently learn the Koopman operators and inverse dynamics controllers from the demonstration data. 

\subsection{Baselines}
We compared KODex against the following baselines:
\begin{itemize}
    \item \textit{NN}: A fully-connected Neural Network policy
    \item \textit{LSTM}: A recurrent neural network policy with Long Short-Term Memory units
    \item \textit{NDP}: An Neural Dynamic policy \cite{bahl2020neural}
    % \item \textit{RMP}: A Riemannian Motion policy \cite{mukadam2020riemannian}
    \item \textit{NGF}: An Neural Geometric Fabrics policy \cite{xieneural}
    % \item \textit{NPG}: A Natural Policy Gradient policy \cite{kakade2001natural}
    % \item \textit{DAPG}: A Demo Augmented Policy Gradient policy \cite{Rajeswaran2018DAPG}
\end{itemize}
Note that the NN and LSTM represent unstructured baselines that serve to investigate the need for structured policies. NDP and NGF are highly structured policies that serve as SOTA imitation learning baselines for manipulation.
% We evaluated each policy's performance in modeling complex reference dynamics associated with dexterous manipulation skills. 

We undertook several precautions to ensure a fair comparison. First, we designed the robot and object state space for all baselines and KODex to be identical. Second, we carefully designed the baselines policies and tuned their hyper-parameters for each baseline method as details in Appendices~\ref{sec:policy_designs} and \ref{appendix:layer_size}. Third, we trained each baseline method over five random seeds for each task to control for initialization effects. For all tasks, we saved the baseline policies that performed the best on a validation set of 50 held-out demonstrations. Note that KODex utilizes an analytical solution and thus does not require parameter initialization or hyper parameter optimization.

\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.8\textwidth}
        \centering
    \includegraphics[width=1\linewidth, height = 3.2cm]{images/all_task.pdf}
     \end{subfigure}
     \caption{We evaluate KODex on four tasks from \cite{Rajeswaran2018DAPG}. \textit{Left to right}: Tool Use, Door Opening, Object Relocation, and In-hand Reorientation.}
      \label{fig:tasks}
\end{figure*}

% \vspace{3pt}

\subsection{Tasks}
We evaluated KODex and the baselines on the same set of four tasks originally proposed in \cite{Rajeswaran2018DAPG} (see Fig. \ref{fig:tasks}).
% \begin{figure}[t!]
%      \centering
%      \begin{subfigure}[b]{0.5\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth, height = 2cm]{images/pen.pdf}
%         \caption{In-hand reorientation - Reorient the blue pen to the goal orientation represented by the green pen.}
%         \label{fig:pen}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.5\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth, height = 2cm]{images/relocate.pdf}
%         \caption{Object relocation - Move the blue ball to the green target.}
%         \label{fig:relocate}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.5\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth, height = 2cm]{images/door.pdf}
%         \caption{Door opening - Undo the latch and drag the door open.}
%         \label{fig:door}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.5\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth, height = 2cm]{images/hammer.pdf}
%         \caption{Tool use - Pick up the hammer and use it to forcefully drive the nail into the board.}
%         \label{fig:tool}
%      \end{subfigure}
%      \caption{An illustration of different tasks adopted from \cite{Rajeswaran2018DAPG}. Each cases shown above are considered successful and the ADROIT Hand is actuated using our approach as detailed in Alg.~\ref{alg: overview}.}
% \end{figure}
\begin{itemize}
    \item \textit{Tool use}: Pick up the hammer to drive the nail into the board placed at a randomized height.
    % The initial poses of the hand and the hammer are fixed. Given a randomized nail position, the task is considered successful if the entire nail is inside the bored. The time horizon $T$ for this task is 100.
    \item \textit{Door opening}:
    Given a randomized door position, undo the latch and drag the door open.
    % the task is considered successful if the door opening angle is larger than a threshold. The time horizon $T$ for this task is 70.
    \item \textit{Object relocation}: Move the blue ball to a randomized target location indicated by the green sphere. 
    % (second right scene in Fig.~\ref{fig:tasks}). 
    % The initial position of the blue ball is fixed. Given a randomized target, the task is considered successful if the position difference is within tolerance. The time horizon $T$ for this task is 100.
    \item \textit{In-hand reorientation}:  Given a randomized goal orientation represented by the green pen, reorient the blue pen to the goal orientation.
    % (right-most scene in Fig.~\ref{fig:tasks}). 
    % The task is considered successful if the orientation difference is within tolerance. The time horizon $T$ for this task is 100.
\end{itemize}
See Appendix~\ref{Appendix:state_design} for the state space design of all tasks.

% For each task, we consider an execution to be successful if the following criteria are met within the time horizon. 
% \textit{In-hand Reorientation}: the robot manipulates the object to the goal orientation; \textit{Object Relocation}: the robot grasps the object from the table and moves it to the target position; \textit{Door Opening}: the robot opens the door; \textit{Tool Use}: the robot grasps the hammer and uses it to drive the nail into the board. The quantitative criteria for each task used in programming are detailed in Appendix.~\ref{Task_Success}.
\subsection{Metrics}\label{subsec:metrics}
We use the following metrics to quantify the performance of KODex and that of the baseline methods:

\begin{itemize}
\item \textit{Training time}: The number of seconds taken by each method to train a policy.
\item \textit{Imitation error}: The difference (L1-norm) between robot joint trajectories generated by the learned policy and the demonstrations when initialized at the same configuration.
\item \textit{Task success rate}: The percentage of trials in which the task was successfully performed by the learned policy (see Appendix \ref{appendix:Task_Success_criteria} for the specific success criteria).
\end{itemize}

Further, we investigate each method's \textit{sample efficiency} by reporting how the above metrics vary as a function of the number of demonstrations.
% For baseline methods, we report the mean and mean $\pm$ standard deviation of all three metrics across 5 random seeds.

\subsection{Expert Policy}
For each task, we trained an expert RL agent using DAPG as described in~\cite{Rajeswaran2018DAPG} and collected 250 successful rollouts as expert demonstrations (200 for training and 50 for validation). See Appendix.~\ref{Appendix:distribution} for a more detailed description of the sampling procedure.

\subsection{Inverse Dynamics Controller}
To standardize controller performance across methods, we first trained a common inverse dynamics controller for each task using the $250$ demonstrations (see Appendix.~\ref{appendix:controller}).


% Imitation Learning: The performance is quantified by \textit{imitation error}, \textit{task success rate} and \textit{training efficiency}. 
% Imitation error captures the difference in trajectories generated by the learned policy and demonstrated trajectories in $D$. Task success rate is computed as the division of number of successful executions over number of total executions. Training efficiency is measured by the duration of training. In this work, we considered \textit{CPU-only}\footnote{CPU model: 11th Gen Intel(R) Core(TM) i9-11900K @ 3.50GHz} training, which means all policies can be potentially trained more efficiently if we introduce GPU parallel computing. For example, in order to parallelize the Koopman training, we can conduct the outer product of different vector pairs in Eqn.~\ref{eqn:k_comp_new} simultaneously. 
% In addition, we measured the task success rate when training each policy on sets of demonstration with various size to evaluate the \textit{sample efficiency}.
% \item Reinforcement Learning: The performance is quantified by \textit{number of samples} and \textit{task success rate}. Number of sample counts how many samples are generated to train the policy under RL settings. Task success rate is the same as used in IL settings.


% \subsection{Data}

% For our comparisons against IL methods, we use the trained RL agents from \cite{Rajeswaran2018DAPG} as expert policies to collect demonstrations for each task. Specifically, we collect $N = 250$ demonstrated trajectories with varying case conditions for each task. 


% \subsection{Experimental Procedure}
% \begin{itemize}
%     \item Imitation Learning: For each task, the inverse dynamics controller first learned using all demonstrations (Appendix.~\ref{appendix:controller}). Then, we trained each policy on sets of demonstrations with sizes [50, 100, 150, 200] (5 random data splits for each size) and recorded the training time. Then, we evaluated each trained policy's performance on the imitation error and task success rate across all demonstrations ($N=250$). We report the mean values and mean $\pm$ standard deviation of the training time, imitation error, and the task success rate for each policy across 5 random splits. Lastly, we train each policy on all demonstrations and test their ability to generalize to another two sets of 200 unseen cases ($D_1$ and $D_2$) via task success rate. These two test sets are generated randomly within and out of the training distribution, respectively. Specifically, they correspond to cases of new goal orientations (in-hand reorientation), new target positions (object relocation), new door positions (door opening) and new nail positions (tool use), respectively.
%     \item Reinforcement Learning:
% \end{itemize}

\section{Results and Discussion}
\label{sec:results}
% Use more powerful representation structure, such as NN
% Improve the performance on OOD cases
In this section, we report the results from the experiments that were setup as discussed in the previous section. We evaluate KODex on its i) general efficacy, ii) sample efficiency, iii) out-of-distribution generalization, and iv) robustness to changes in physical properties of the robot and the object.

% For each size, we sampled five sets of demonstrations and trained a policy on each set in order to report the mean and standard deviation of all metrics across all five policies. 
% For instance, we sampled five sets of 50 demonstrations of the Tool Use task to train five policies using one method.
% We also train each method on all 250 demonstrations and evaluate generalization to unseen settings.

\begin{figure*}[t]
     \centering
     \begin{subfigure}[t]{0.96\textwidth}
     \centering
        \raisebox{-\height}{\includegraphics[width=0.48\textwidth]{images/legend_200.pdf}}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%first row
        \begin{subfigure}[t]{0.30\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/time.pdf}}
    \end{subfigure}
        \begin{subfigure}[t]{0.30\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/error.pdf}}
    \end{subfigure}
    \begin{subfigure}[t]{0.30\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/success.pdf}}
    \end{subfigure}
    \caption{Policy performances on all tasks based on three metrics: Training Time (left figure), Imitation Error (middle figure), and Task Success Rate (right figure). Each method was trained on 200 demonstrations and evaluated on the test set. For baseline methods, the error bars show the standard deviation over five random seeds. Note that we did not plot NN's imitation errors as they were out of scale.}
\label{fig:performance_on_200}
\end{figure*}

\begin{figure*}[t]
     \centering
     \begin{subfigure}[t]{0.96\textwidth}
     \centering
        \raisebox{-\height}{\includegraphics[width=0.5\textwidth]{images/legend.pdf}}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%first row
        \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/hammer_training.pdf}}
    \end{subfigure}
        \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/door_training.pdf}}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/relocation_training.pdf}}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/pen_training.pdf}}
    \end{subfigure}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%second row
        \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/hammer_joint.pdf}}
    \end{subfigure}
        \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/door_joint.pdf}}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/relocation_joint.pdf}}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/pen_joint.pdf}}
    \end{subfigure}
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%third row
        \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/hammer_success.pdf}}
        \caption{Tool Use (Hammer)} 
    \end{subfigure}
        \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/door_success.pdf}}
        \caption{Door Opening} 
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/relocation_success.pdf}}
        \caption{Object Relocation} 
    \end{subfigure}
    \begin{subfigure}[t]{0.24\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth]{images/pen_success.pdf}}
        \caption{In-hand Reorientation} 
    \end{subfigure}
    \caption{The effects of the number of demonstrations on training time (the top row), the imitation error (the middle row), and the task success rate (the bottom row). Solid lines are the mean values and the shaded areas show mean $\pm$ standard deviation, over five random seeds for each demonstration size. Note that we did not plot NN's imitation errors as they were out of scale.}
\label{fig:imitation_learning}
\end{figure*}
\vspace{3pt}

\subsection{General Efficacy}
\label{sec:general_efficacy}
We generated 200 new rollouts with the expert RL policy (within the same distribution as the demonstrations) as the test set.
In Fig. \ref{fig:performance_on_200}, we report the training time, imitation error, and task success rate for each method on each task, when trained on 200 demonstrations and tested on the new 200 testing instances.

\textit{Training time}: As can be seen, KODex takes a significantly (by an order of magnitude) shorter time to train a policy compared to both unstructured baselines (NN, LSTM) and SOTA IL methods (NDP, NGF). Further, this trend holds across all the tasks. This is to be expected since KODex analytically computes the Koopman operator unlike all the baselines, which rely on gradient descent and numerical optimization.

\textit{Imitation error}: We have excluded the significantly larger imitation errors generated by NN from the plot in order to preserve necessary resolution to distinguish between the performance of other methods. In general, all methods (except NN) achieve low imitation error for the Tool Use task with a minimal difference across methods. In the three remaining tasks, we see that all structured methods (NDP, NGF, KODex) considerably outperform the unstructured baseline (LSTM). These results reinforce the effectiveness of structured methods in imitating demonstrations. Importantly, these results suggest that KODex, despite its simplicity, is able to achieve imitation performance comparable to the SOTA IL methods, while remaining an order of magnitude more computationally efficient. 

\textit{Task success rate}: 
% Following the trends we have observed in other metrics, KODex considerably outperforms or matches all baselines in terms of task success rate. Specifically, 
As one would expect, the naive NN policy performs significantly worse than all other methods.  On the other hand, LSTM achieves impressive task success rates, even outperforming NDP in two of the four tasks. This is in stark contrast to its high imitation error. While counter-intuitive at first, this observation is not surprising given the recent finding that a low imitation error does not necessarily translate to a high task success rate~\cite{mandlekar2021matters}.
We observe that KODex and NGF perform comparably, with one achieving a higher task success rate than the other in two of the four tasks. 
% KODex is also able to consistently outperform NN, LSTM, and NDP policies and match NGF policy at 200 demonstrations, as shown in Fig.~\ref{fig:performance_on_200}.
Importantly, KODex results in the most consistent and predictable performance due to its analytical nature and free of sensitivity to initialization. 
% As we would expect, the naive NN baseline results in the lowest performance, while LSTM is able to occasionally match the performance of other structured methods. We also see that imitation error and task success rate do not necessarily correlate, matching the findings from a recent review paper~\cite{mandlekar2021matters}.

% These results suggest that \textbf{(i).} KODex achieves the highest training efficiency, which is not surprising as KODex analytically computes the optimal solution instead of running gradient descent for hundreds of iterations. \textbf{(ii).} KODex consistently offers the best performance against all baselines over all tasks, indicating the benefits of introducing Koopman operator for dexterous manipulation tasks. We believe that the observables' strong power of modelling the nonlinearity of each dynamical system is the main key of the magic performances. While other baseline models are less effective at capturing the system nonlinearity, e.g. using activation functions (NN, LSTM) or predefined dynamics structures (NDP), resulting in the worse and inconsistent performance across all tasks. \textbf{(iii).} KODex is the most sample-efficient as it results in the lowest mean imitation error and largest task success rate (and both with smallest variance) across almost all sizes of demonstrations. Take Door opening and Tool use as examples, with only 50 demonstrations, KODex can almost achieve 100\% success rate. 

% \vspace{3pt}
% \noindent \textbf{Sample Efficiency}: 
\subsection{Sample Efficiency}
\label{sec:sample_efficiency}
To investigate sample efficiency, we trained policies on a varying number of demonstrations ([10, 25, 50, 100, 150, 200]). In Fig.~\ref{fig:imitation_learning}, we report the training time, imitation error, and task success rate for each method as a function of the number of demonstrations when tested on the same 200 test instances used to evaluate general efficacy. 

\textit{Training time}: A key observation from these plots is that, KODex's training time increases at a significantly lower rate compared to the baselines as we increase the number of demonstrations, suggesting that KODex scales much better than the baselines. 

\textit{Imitation error and Task success rate}: We find that unstructured models (NN and LSTM) fail to demonstrate a consistent monotonic decrease (increase) in imitation error (task success rate) as we increase the number of demonstrations. In stark contrast, structured methods (NDP, NGF, and KODex) are able to leverage additional demonstrations to consistently drive down imitation errors and improve task success rates. KODex almost consistently achieves the lowest imitation error and highest task success rate with the fewest number of demonstrations, which is closely followed by NGF. These observations suggest that KODex tends to be more sample efficient than the baselines. This superior performance is likely due to the rich structure induced by the Koopman operator theory and the resulting effectiveness in capturing nonlinear dynamics. The only exception to this trend is observed for the Object Relocation task, in which KODex requires 150 demonstrations to perform comparably well to the other structured methods (NDP, NGF). We conjecture this is due to the fact that the hand base needs to move across a large task space for this task and thus the trajectories exhibit high-variance in the demonstrations. 
% These results suggest that KODex can more accurately imitate the expert with fewer demonstrations. 

 % achieves the highest success rate for both Tool Use and Door Opening tasks starting at 50 demonstrations and maintains this advantages as the number of demonstrations increase to 100, 150, and 200. 

% \vspace{3pt}
% \noindent \textbf{Generalization}:
\subsection{Out-of-Distribution Generalization}
\label{sec:ood}
We generated a new set of 200 out-of-distribution samples to evaluate how the policies that were trained on 200 demonstrations generalize to unseen samples (see Appendix.~\ref{Appendix:distribution} for more details). In Table~\ref{tab:All_outof_range}, we report the task success rates of each method trained on 200 demonstrations and tested on 200 out-of-distribution samples. In addition, we also report the task success rate of the expert policy on the same 200 out-of-distribution samples to establish a baseline. Perhaps unsurprisingly, none of the methods are able to consistently outperform the expert policy in most tasks. We notice that KODex is able to outperform the baselines in Tool Use and Door Opening tasks, while NGF performs better in Object Relocation and In-Hand Reorientation tasks. 
% We believe that the reason for KODex's poor performance in the Object Relocation and In-Hand Reorientation tasks is due to its limited representation power. Note that this is not an inherent limitation of Koopman-operator-based method. But rather of the polynomial lifting functions. Indeed, recent work in other domains have demonstrated that it is possible to learn the lifting functions in conjunction with the Koopman Operator directly from data~\cite{??}

% In Table~\ref{tab:All_outof_range}, we report the task success rate of each method tested on the 200 unseen out-of-distributional samples. It should note that the expert policy was trained on the same distribution as the demonstrations, so expert's performance also become a 
%  \begin{table}[t]
%     \caption{Task success rate of KODex across all demonstrations}
%     \centering
%     \begin{tabular}{c|c|c|c|c}
%     \hline
%       &  Reorientation & Relocation & Door & Tool  \\
%         \hline
%     Task Success Rate & 66.0\%  & 96.0\% & 94.0\% & 100.0\% \\
%         \hline
%     \end{tabular}
%     \label{tab:KODex_on_demo}
% \end{table}

% Finally, we report the task success rate when each policy are trained using all demonstrations and tested on $D_1$ (within training distribution) in Table.~\ref{tab:All_with_range} and $D_2$ (out of training distribution) in Table.~\ref{tab:All_outof_range}.
%  \begin{table}[t]
%   \scriptsize
%     \caption{Within-distribution task success rates}
%     \centering
%     \begin{tabular}{c|c|c|c|c|c}
%     \hline
%         \multirow{3}{*}{\diagbox[width=11em,height=3\line]{Task}{Success Rate}{Policy}} & \multirow{3}{*}{KODex} & \multirow{3}{*}{NN} & \multirow{3}{*}{LSTM} & \multirow{3}{*}{NDP} &  \multirow{3}{*}{{\color{gray}Expert}}\\ 
%             &  & &  & & \\
%             &  & &  & & \\
%         \hline
%     In-hand Reorientation & 63.5\%  & 30.5\% & 60.0\% & \textbf{76.0\%} & {\color{gray}93.5\%} \\
%     \hline
%     Object Relocation & \textbf{92.0\%}  & 1.5\% & 8.5\% & 90.5\% & {\color{gray}100.0\%} \\
%     \hline
%     Door Opening & \textbf{93.5\%}  & 0.0\% & 77.5\% & 63.5\% & {\color{gray}88.0\%} \\
%     \hline
%     Tool Use (Hammer) & \textbf{100.0\%}  & 26.5\% & 65.0\% & 96.5\% & {\color{gray}100.0\%} \\
%     \hline
%     \end{tabular}
%     \label{tab:All_with_range}
% \end{table}
\setlength{\tabcolsep}{4pt} % Default value: 6pt
 \begin{table}[t]
 \scriptsize
    \caption{Out-of-distribution task success rates}
    \centering
    \begin{tabular}{c|c|c|c|c}
    \hline
        \multirow{3}{*}{\diagbox[width=11em,height=3\line]{Policy}{Success Rate}{Task}} & \multirow{3}{*}{Tool} & \multirow{3}{*}{Door} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Reorientation} \\ 
            (\%) &   &  &  & \\
            &  &  &  & \\
        \hline
    NN & 0.0($\pm$0.0)  & 0.0($\pm$0.0) & 0.0($\pm$0.0) & 14.2($\pm$9.5) \\
    \hline
    LSTM & 24.6($\pm$16.0)  & 20.9($\pm$12.1) & 2.6($\pm$5.0) & 48.3($\pm$2.2) \\
    \hline
    NDP & 35.8($\pm$4.2)  & 11.3($\pm$1.7) & 73.4($\pm$14.2) & 53.9($\pm$3.3) \\
    \hline
    NGF & 42.7($\pm$10.9)  & 29.8($\pm$3.9) & \textbf{97.7($\pm$3.4)} & \textbf{63.6($\pm$7.6)} \\
    \hline
    KODex & \textbf{61.5}  & \textbf{30.5} & 66.0 & 33.0 \\
    \hline
    {\color{gray} Expert} & {\color{gray} 99.0}  & {\color{gray} 31.5} & {\color{gray} 100.0} & {\color{gray} 51.5} \\
    \hline
    \end{tabular}
    \label{tab:All_outof_range}
\end{table}

% From the above results, we can observe that for the specific in-hand reorientation task, NDP policy can achieve the best performance, and KODex and LSTM policy show the similar performance. However, across all the tasks, KODex provides with the most consistently desirable performances. Besides, KODex does not require large amount of efforts on hyper-parameter tuning, which is mostly likely to be a painful process. We also note one interesting finding that for In-hand Reorientation task, NDP policy even outperforms the expert when testing on OOD case. One conjecture is that NDP better captures the trajectory distribution for this specific task. Overall speaking, KODex is able to offer desirable performances across all tasks with limited demonstrations. 

% \subsection{How does KODex compare with deep RL methods?}
% \label{sec:RL_result}
% We then compare KODex against our RL-based baselines
% (NPG, DAPG).

% \vspace{3pt}
% \noindent \textit{Training}: Instead of training two baseline models from scratch, 
% We compared the performance of KODex against DAPG and NPG directly on their performances shown in Table I reported from \cite{Rajeswaran2018DAPG} as all of the three policies were trained on samples generated from the same distribution. In addition, we believe that the authors of \cite{Rajeswaran2018DAPG} selected the most appropriate hyperparameters for both methods and reported the best results. For KODex, we used the results obtained in Section.~\ref{sec:IL_result}.

% \vspace{3pt}
% \noindent \textit{Testing}: To study sample efficiency, we evaluate the three policies on how many demonstrations or RL iterations are needed in order to achieve 90\% task success rate. Note that KODex needs demonstrations, while NPG policy was trained from scratch in RL setting. In between, DAPG policy combined demonstrations with RL training. 
%  \begin{table}[h]
%  \small
%     \caption{Sample Efficiency of KODex compared to DAPG and NPG from scratch with shared rewards. $N_\text{Demo}$ and $N_\text{RL}$ are the number of demonstrations and number of RL iterations that are used to achieve 90\% task success rate, respectively.}
%     \centering
% \begin{tabular}{l|ll|ll|ll}
% \hline Method & \multicolumn{2}{c|}{KODex} & \multicolumn{2}{c|}{DAPG} &  \multicolumn{2}{c}{NPG}   \\
% \hline 
% Task & $N_\text{Demo}$ & $N_\text{RL}$ & $N_\text{Demo}$ & $N_\text{RL}$ & $N_\text{Demo}$ & $N_\text{RL}$ \\
% \hline
% Tool & 10 & 0 & 25 & 55 & 0 & 448 \\
% Door & 10 & 0 & 25 & 42 & 0 & 146 \\
% Relocation & $\approx$200 & 0 & 25 & 52 & 0 & 880 \\
% Reorientation & $>$200 & 0 & 25 & 30 & 0 & 864 \\
% \hline
% \end{tabular}
% \label{table:compare_RL}
% \end{table}

% \vspace{3pt}
% \noindent \textbf{Summary}: In Table~\ref{table:compare_RL}, we can clearly tell that for both Tool Use and Door Opening tasks, KODex significantly outperforms both RL-based baselines as it only needs 10 demonstrations without any further RL training. For Object Relocation task, KODex needs more demonstrations (around 200) to obtain desirable performance due to higher variances in the hand joint trajectories. However, regarding the In-hand Reorientation task, the most difficult task among the four, KODex is not able to achieve 90\% task success rate. Besides, partially indicated from the right-most column in Fig.~\ref{fig:imitation_learning}, KODex does not benefit much from using more demonstrations for this task, which infers that KODex might have a representational limit with a given choice of lifting functions. Thus, for relatively complex dynamical tasks, KODex would need more hyperparameter tuning to select the best choice of lifting functions.

\subsection{Robustness to changes in physical properties}
\label{sec:robustness}
% We also evaluated the robustness of the reference dynamics KODex to variations in the environment. 
This experiment is motivated by the fact that sim-to-real transfer often involves changes in physical properties. Further consistent use of robotic hardware could result in changes to physical properties. We evaluate the robustness of the reference dynamics learned by each method to changes in hand mass or object mass for the Object Relocation task. Specifically, we consider three variations: i) \textit{Heavy Object}: 0.18 (default) $\rightarrow$ 1.88 (new), ii) \textit{Light Hand}: 4.0 (default) $\rightarrow$ 3.0 (new), and iii) \textit{Heavy Hand}: 4.0 (default) $\rightarrow$ 5.0 (new). We are not including a variation involving a lighter object as we found that this variation did not impact the performance of the default controller.

It is important to note we held the reference dynamics learned by each method constant for this experiment, irrespective of the changes to the hand or the object. Instead, we relearned two tracking controllers, each using 200 rollouts from either the expert agent or a noisy expert agent, following the procedure detailed in Section.~\ref{sec:control}. 
% To achieve this, we collected another 200 rollouts from the expert agent deployed in the new environment to relearn the inverse dynamics controller through the procedures shown in Section.~\ref{sec:control}. 
% In order to evaluate the influence of demonstration quality to controller learning, we used noiseless and noisy expert agents to collect demonstrations and trained the inverse dynamics controller respectively.

% \vspace{3pt}
% \noindent \textbf{Analysis}:
In Table.~\ref{tab:Variation_test}, we report the task success rate of KODex, NDP, and NGF policies (all trained on 200 demonstrations) before and after relearning the controller. We also report the task success rate of both noiseless and noisy expert agents to establish baselines. It is no surprise that all methods suffer a drop in performance when using the original controller. 
For the Light Hand variation, the original controller already shows good performance, so the relearned controllers have little to offer in terms of task success rate. 
In contrast, all methods clearly benefit from relearning the controller in the Heavy Object and Heavy Hand variations, evidenced by the increased task success rate.
% since expert's decision during the grasping phase has shorter duration compared to that of relocation phase, 
We also see that all methods struggle to regain the similar task success rate in the Heavy Object variation compared to Heavy Hand variation, perhaps due to the complexity of learning to grasp a considerably heavier object. Though these methods with the controllers tuned by the noisy expert perform worse than the ones trained by the expert, they are still able to  outperform the noisy expert itself. 
% For NDP and NGF policies, we can observe the similar trends and KODex is able to perform comparably to them.
\setlength{\tabcolsep}{1.5pt} % Default value: 6pt
 \begin{table}[t]
 \scriptsize
    \caption{Robustness to variations in the physical properties}
    \centering
    \begin{tabular}{c|c|c|c}
    \hline
        \multirow{3}{*}{\diagbox[width=18em,height=3\line]{Controller}{Success Rate}{Variation}} &  \multirow{3}{*}{Heavy Object} & \multirow{3}{*}{Light Hand} & \multirow{3}{*}{Heavy Hand} \\ 
            (\%) &    &  & \\
            &  &   & \\
        \hline
    Expert agent & 77.0  & 100.0 & 100.0 \\
    \hline
    Noisy expert agent & 0.0  & 47.0 & 56.5  \\
    \hline
    KODex + Original controller &  19.5 & 82.5 & 21.5 \\
    KODex + Expert-tuned controller &  34.0 & 85.0 & 89.0  \\
    KODex + Noisy-expert-tuned controller & 24.5  & 77.5 & 63.5  \\
    \hline
    NDP + Original controller &  13.5($\pm$5.0) & 72.1($\pm$9.6) & 31.6($\pm$10.0) \\
    NDP + Expert-tuned controller &  19.9($\pm$5.8) & 63.2($\pm$15.0) & 92.4($\pm$1.2)  \\
    NDP + Noisy-expert-tuned controller & 23.6($\pm$8.1)  & 66.8($\pm$8.1) & 42.5($\pm$18.0)  \\
    \hline
    NGF + Original controller &  25.8($\pm$4.9) & 96.6($\pm$0.97) & 19.3($\pm$3.8) \\
    NGF + Expert-tuned controller &  52.6($\pm$3.9) & 95.6($\pm$2.2) & 94.5($\pm$0.9)  \\
    NGF + Noisy-expert-tuned controller & 32.8($\pm$5.5)  & 82.4($\pm$2.0) & 57.3($\pm$12.2)  \\
    \hline
    \end{tabular}
    \label{tab:Variation_test}
\end{table}

These results demonstrate that changes to the robot/system dynamics (e.g., due to the sim-to-real gap) can be handled in a task-agnostic way by fine tuning the tracking controller, without the need for relearning the reference dynamics. Further, once gain, we observe that KODex is able to perform comparably to SOTA approaches despite its simplicity.

% \vspace{3pt}
% \noindent \textbf{Summary}: Taken together, the above results show that KODex consistently either outperforms or performs comparably to baselines in terms of sample efficiency (as measured by task success rate and imitation error) and scalability (as measured by training time). Importantly, these results come at an order of magnitude lower computational burden. When sufficient demonstrations are available, KODex manages to perform comparably to SOTA baselines despite  learning a \textbf{linear} dynamical system, thanks to the Koopman operator's effectiveness in capturing nonlinear dynamics. It is important to note that, unlike all baselines, KODex does not require painstaking optimization of hyper-parameters to learn different tasks. These observations suggest that KODex contributes to the state-of-the-art in imitation learning methods that encode reference dynamics for dexterous manipulation. Further, the linear dynamical system learned by KODex enables rigorous inspection via a wide array of control-theoretic tools.

\section{Conclusions and Limitations} 
\label{sec:conclusion}
In this work, we investigate the utility of Koopman operator theory in learning dexterous manipulation skills. To this end, we contribute a new imitation learning framework dubbed as KODex that can learn complex dexterous manipulation skills as linear dynamical systems in higher-dimensional spaces. Our experimental results conclusively demonstrate that Koopman operator theory offers a unique set of advantages over existing learning-based approaches. Specifically, we show that KODex can i) analytically learn dexterous manipulation skills, eliminating the sensitivity to parameter initialization and reducing the need for significant user expertise, ii) match or outperform SOTA imitation learning approaches on various dexterous manipulation tasks, while being an order of magnitude more computationally efficient.

% \section{Limitation and Future Work} 
While our work offers considerable promise for the utility of Koopman-operator theory in dexterous manipulation, it reveals numerous avenues for improvement and future research.
First, KODex has limited capacity for out-of-distribution generalization in its current form. This necessitates the need for additional training data collected offline or online using approaches like DAGGER~\cite{ross2011reduction}. Future work can also investigate if out-of-distribution generalization can be improved by swapping the polynomial lifting functions with learned lifting functions~\cite{weissenbacher2022koopman,lusch2018deep,Li2020Learning}, allowing for end-to-end policy learning.
% the In addition, a more robust controller may be required if deploying KODex in real world.
Second, we did not deploy KODex on physical hardware. Although our results on robustness to changes in physical properties show promise, we plan to deploy KODex on different physical platforms to evaluate if it can learn to control physical robots.
% In the future, we plan to conduct hardware experiments on a physical dexterous robot hand, PYSONIC Ability Hand \cite{akhtar2020touch}. 
% Meanwhile, we would like to explore the usage of Koopman operator as the policy structure in neural networks  
Third, Koopman operators can be used to learn system dynamics via self play to enable model-based reinforcement learning. 
Fourth, policies learned using KODex can be subject to rigorous theoretical analysis, thanks to its linear dynamical system. For instance, a recently-discovered connection between contraction analysis and Koopman operator theory~\cite{Bowen2021Contraction} can help derive convergence and stability guarantees on learned policies.
\label{sec:conclusion}

\bibliographystyle{unsrtnat}
% \bibliographystyle{plainnat} % the index is not correct
\bibliography{references}

\newpage

% \section*{Acknowledgments}
\section*{Appendices}
\subsection{State Design}
\label{Appendix:state_design}
% The settings are the same as DAPG project. Do we have to mention this in paper? Or reviews or readers may be confused why the DOF of hand for each task varies.
% I think for our own hand, we can set whatever it is as we want, but for the ADROIT Hand, we need to follow exactly the settings before. We can claim this in context.
In this section, we show the state design for each task in detail. It should be noted that the motion capability of the hand for each task were suggested from the work \cite{Rajeswaran2018DAPG} that originally introduced these tasks. For a decent implementation, we employed the same setting.
\newline
\textbf{Tool use}
For this task, the floating wrist base can only rotate along the $x$ and $y$ axis, so we have $\mathrm{x}_r(t) \in \mathcal{X}_r \subset \mathbb{R}^{26}$. Regarding the object states, unlike the other tasks, where the objects of interest are directly manipulated by the hand, this task requires to modify the environment itself. As a result, except for the hammer positions, orientations and their corresponding velocities $\mathrm{p}^{\text{tool}}_t, \mathrm{o}^{\text{tool}}_t, \dot{\mathrm{p}}^{\text{tool}}_t, \dot{\mathrm{o}}^{\text{tool}}_t$ ($\mathbb{R}^{3}$), we also define the nail goal position $\mathrm{p}^{\text{nail}}$
 ($\mathbb{R}^{3}$).
 % which functions similarly as $\mathrm{p}^{\text{door}}$ in the task of door opening.
 Finally, we have $\mathrm{x}_o(t) = [\mathrm{p}^{\text{tool}}_t, \mathrm{o}^{\text{tool}}_t, \dot{\mathrm{p}}^{\text{tool}}_t, \dot{\mathrm{o}}^{\text{tool}}_t, \mathrm{p}^{\text{nail}}] \in \mathcal{X}_o \subset \mathbb{R}^{15}$. As a result, $\mathrm{x}(t)$ includes 41 states in total and we use $T = 100$.
 \newline
\textbf{Door opening} For this task, the floating wrist base can only move along the direction that is perpendicular to the door plane but rotate freely, so we have $\mathrm{x}_r(t) \in \mathcal{X}_r \subset \mathbb{R}^{28}$. Regarding the object states, we define the fixed door position $\mathrm{p}^{\text{door}}$, which can provide with case-specific information (similar to $\mathrm{p}^{\text{nail}}$ in Tool Use), and the handle positions $\mathrm{p}^{\text{handle}}_t$ (both $\mathbb{R}^{3}$). In order to take into consideration the status of door being opened, we include the angular velocity of the opening angle $v_t$$
 (\mathbb{R}^{1})$. Finally, we have $\mathrm{x}_o(t) = [\mathrm{p}^{\text{handle}}_t, v_t, \mathrm{p}^{\text{door}}] \in \mathcal{X}_o \subset \mathbb{R}^{7}$. As a result, $\mathrm{x}(t)$ includes 35 states in total and we use $T = 70$.
 \newline
\textbf{Object relocation} For this task, the ADROIT hand is fully actuated, so we have $\mathrm{x}_r(t) \in \mathcal{X}^{r} \subset \mathbb{R}^{30}$ (24-DoF hand + 6-DoF floating wrist base). Regarding the object states, we define $\mathrm{p}^{\text{target}}$ and $\mathrm{p}^{\text{ball}}_t$ as the target and current positions. Then, we compute $\bar{\mathrm{p}}^{\text{ball}}_t = \mathrm{p}^{\text{ball}}_t - \mathrm{p}^{\text{target}}$, which is the component of $\mathrm{p}^{\text{ball}}_t$ in a new coordinate frame that is constructed by $\mathrm{p}^{\text{target}}$ being the origin. 
 % Similar to the in-hand reorientation task, 
 We additional include  the ball orientation $\mathrm{o}^{\text{ball}}_t$ and their corresponding velocities $\dot{\mathrm{p}}^{\text{ball}}_t$,  $\dot{\mathrm{o}}^{\text{ball}}_t$ (all $\mathbb{R}^{3}$).
Finally, we have $\mathrm{x}_o(t) = [\bar{\mathrm{p}}^{\text{ball}}_t, \mathrm{o}^{\text{ball}}_t, \dot{\mathrm{p}}^{\text{ball}}_t, \dot{\mathrm{o}}^{\text{ball}}_t] \in \mathcal{X}_o \subset \mathbb{R}^{12}$. As a result, $\mathrm{x}(t)$ includes 42 states in total and we use $T = 100$.
\newline
\textbf{In-hand reorientation} For this task, the floating wrist base is fixed, so we only consider the 24-DoF hand joints. Therefore, we have $\mathrm{x}_r(t) \in \mathcal{X}_{r} \subset \mathbb{R}^{24}$. Regarding the object states, we define $\mathrm{o}^{\text{goal}}$ and $\mathrm{o}^{\text{pen}}_t$ as the goal and current pen orientations, which are both unit direction vectors. Then, we transform $\mathrm{o}^{\text{pen}}_t$ to a new rotated coordinate frame that is constructed by $\mathrm{o}^{\text{goal}}$ being $x$ axis ([1,0,0]). Note that the vector $\bar{\mathrm{o}}^{\text{pen}}_t$ after transformation is also a unit vector and it converges to x axis if the pen is perfectly manipulated to goal orientation $\mathrm{o}^{\text{goal}}$. In addition, we also include the center of mass position $\mathrm{p}^{\text{pen}}_t$ and their corresponding velocities $\dot{\mathrm{p}}^{\text{pen}}_t$,  $\dot{\mathrm{o}}^{\text{pen}}_t$ (all  $\mathbb{R}^{3}$). Finally, we have $\mathrm{x}_o(t) = [\mathrm{p}^{\text{pen}}_t, \bar{\mathrm{o}}^{\text{pen}}_t, \dot{\mathrm{p}}^{\text{pen}}_t, \dot{\mathrm{o}}^{\text{pen}}_t] \in \mathcal{X}_o \subset \mathbb{R}^{12}$. As a result, $\mathrm{x}(t)$ includes 36 states in total and we use $T = 100$.

 In this work, we only included the joint positions as the robot states (with the only exception of NGF's second-order policy) for the following reasons: 1) Given that these tasks are not repetitive, we found that joint position information was sufficient to disambiguate the robot's next action, 2) even when ambiguity arises for a given joint position, object state information can help with disambiguation. Further, the impressive performance achieved by KODex in our experiments support this design choice. Indeed, KODex is agnostic to this specific state design. One can incorporate velocity information into the robot state space without the need of any changes to the training procedure.

% {\color{red} Should we mention that the dynamical effects are handled by the robust controller? I think that as long as there is no velocity ami, we can always make good traj predictions (Because we assume the demonstrator has a good internal sense of dynamical effects, so the demonstrated traj already embed the dynamical effects even we only have positions, so that the controller design should be easier, as the underlying physical dynamics is embedded in traj, which agrees with natural dynamics). In that case, if we have a good controller to track the traj, the dynamical effect is natural handled by the controller. so this controller is not necessarily the inverse dynamics controller, a perfect PID controller should also work. But the IDC is the simplest one to obtain in our work. So we uses this one.
% and we may need to have velocity for real-world tasks as controller is not perfect, so we need to the additional velocity to constrain the controller output.}

% However, it is possible that on the real robot hand, we also need to predict the joint velocities for the controller design.

% Also, our state designs are mainly chosen upon intuitions, so it is possible that others come up with better state representations by exploiting more domain knowledge of each task.
% {\color{red} Note from Yunhai: Explicitly explain why we can ignore velocity in the state space when predicting traj in this work (for dexterous manipulation), as it contradicts to common dynamical system. For the real-world applications or the cases that we can not access the demonstrated torques for controller learning, the policy should also predict velocities for the controller tuning \cite{abraham2017model}. Q: should we add an experiment for Koopman with velocity?}
% \label{State_Design}

\subsection{Task Success Criteria}
\label{appendix:Task_Success_criteria}
The quantitative criteria of task successes are listed below. We used the same settings as in the paper \cite{Rajeswaran2018DAPG}.
\newline
\textbf{Tool Use:} The task is considered successful if at last time step $T$, the Euclidean distance between the final nail position and the goal nail position is smaller than 0.01.
\newline
\textbf{Door Opening:} The task is considered successful if at last time step $T$, the door opening angle is larger than 1.35 rad.
\newline
\textbf{Object Relocation:} At each time step $t$, if $\sqrt{|\mathrm{p}^{\text{target}} - \mathrm{p}^{\text{ball}}_t|^2} < 0.10$, then we have $\rho(t) = 1$. The task is considered successful if $\sum_{t=1}^T \rho(t) > 10$.
\newline
\textbf{In-hand Reorientation:} At each time step $t$, if $\mathrm{o}^{\text{goal}} \cdot \mathrm{o}^{\text{pen}}_t > 0.90$ ($\mathrm{o}^{\text{goal}} \cdot \mathrm{o}^{\text{pen}}_t$ measures orientation similarity), then we have $\rho(t) = 1$. The task is considered successful if $\sum_{t=1}^T \rho(t) > 10$.

\subsection{Sampling Procedure}
\label{Appendix:distribution}
We describe the sampling procedure in this section. The sample distributions used for RL training and demo collection were identical, which was also suggested in \cite{Rajeswaran2018DAPG}. For out-of-distribution data, we generated them to evaluate the generalizability of each policy.
\newline
\textbf{Tool Use:} We randomly sampled the nail heights ($h$) from a uniform distributions. Within distribution: we used $h \in \mathcal{H} \sim \mathcal{U}(0.1,0.25)$; Out of distribution: we used $h \in \mathcal{H} \sim \mathcal{U}(0.05,0.1) \cup \mathcal{U}(0.25,0.3)$.
\newline
\textbf{Door Opening:} We randomly sampled the door positions ($xyz$) from uniform distributions. Within distribution: we used $x \in \mathcal{X} \sim \mathcal{U}(-0.3,0)$, $y \in \mathcal{Y} \sim \mathcal{U}(0.2,0.35)$, and $z \in \mathcal{Z} \sim \mathcal{U}(0.252,0.402)$; Out of distribution: we used $y \in \mathcal{Y} \sim \mathcal{U}(0.1,0.2)$ ($x, z$ remained unchanged).
\newline
\textbf{Object Relocation:} We randomly sampled the target positions ($xyz$) from uniform distributions. Within distribution: we used $x \in \mathcal{X} \sim \mathcal{U}(-0.25,0.25)$, $y \in \mathcal{Y} \sim \mathcal{U}(-0.25,0.25)$, and $z \in \mathcal{Z} \sim \mathcal{U}(0.15,0.35)$; Out of distribution: we used $z \in \mathcal{Z} \sim \mathcal{U}(0.35,0.40)$ ($x, y$ remained unchanged).
\newline
\textbf{In-hand Reorientation:} We randomly sampled the pitch ($\alpha$) and yaw ($\beta$) angles of the goal orientation from uniform distributions. Within distribution: we used $\alpha \in \mathcal{A} \sim \mathcal{U}(-1,1)$ and $\beta \in \mathcal{B} \sim \mathcal{U}(-1,1)$; Out of distribution: we used 
$\alpha \in \mathcal{A} \sim \mathcal{U}(-1.2,-1) \cup \mathcal{U}(1,1.2)$ and $\beta \in \mathcal{B} \sim \mathcal{U}(-1.2,-1) \cup \mathcal{U}(1,1.2)$.

\subsection{Policy Design}
\label{sec:policy_designs}
We show the detailed policy design in this section. All the baseline policies were trained to minimize the trajectory reproduction error.
\newline
\noindent \textbf{KODex:}
The representation of the system is given as: $\mathrm{x}_r = [x_r^1, x_r^2, \cdots, x_r^n]$ and $\mathrm{x}_o = [x_o^1, x_o^2, \cdots, x_o^m]$ and superscript is used to index states. The details of the state design for each task is provided in Appendix.~\ref{Appendix:state_design}. In experiments, the vector-valued lifting functions $\psi_r$ and $\psi_o$ in (\ref{eqn:gx}) were polynomial basis function defined as
\begin{equation}
\label{eqn:lift}
\begin{split}
\psi_r =& \{x_r^ix_r^j\} \cup \{(x_r^i)^3\} \text{ for }i,j=1,\cdots, n\\
\psi_o =& \{x_o^ix_o^j\} \cup \{(x_o^i)^2(x_o^j)\}  \text{ for } i,j=1,\cdots, m
\end{split}
\end{equation}
Note that $x_r^ix_r^j$/$x_r^jx_r^i$ only appears once in lifting functions (similar to $x_o^ix_o^j$/$x_o^jx_o^i$), and we ignore $t$ as the lifting functions are the same across the time horizon. 

The choice of lifting functions can be viewed as the hyper-parameter of KODex. We make this choice as inspired from 
\cite{abraham2017model} and experimental results also indicate its effectiveness. Through all the experiments, we sticked with the same set of lifting functions, which helped to relieve us from extensive efforts of tuning the hyper-parameters, e.g. network layer size, that were necessary for baseline policies as shown in Appendix.~\ref{appendix:layer_size}.

\noindent \textbf{Full-connected Neural Network (NN):} The first baseline is a feedforward network that ingests the states $\mathrm{x}(1)$ and iteratively produces the predictions $\mathrm{x}(t), t=2,\cdots, T$ via the rollout of a Multilayer Perceptron (MLP).  The reference joint trajectories ($\mathrm{x}_r(t)$) are then used to execute the robot with the learned controller $C$. The significance of this baseline is to evaluate a policy that produces a high-dimensional motion without any additional structure.  
\newline
\textbf{Long Short-Term Memory (LSTM):} We create an LSTM-based policy under the same input-output flow as the NN policy. We also apply two fully-connected layers between the task input/output and the input/hidden state of the LSTM network. Similarly, the same controller $C$ is deployed to track the reference joint trajectory.
LSTM networks are known to be beneficial to imitation learning \cite{mandlekar2021matters} and suitable for sequential processing \cite{greff2016lstm}, e.g, motion generation. Therefore, we expect to evaluate the performance of the recurrent structures in these tasks.
\newline
\textbf{Neural Dynamic Policy (NDP):}
The Neural Dynamic Policy \cite{bahl2020neural} embeds desired dynamical structure as a layer in neural networks. Specifically, the parameters of the second order Dynamics Motion Primitives (DMP) are predicted as outputs of the preceding layers (MLP in \cite{bahl2020neural}). As a result, it allows the overall policy easily reason in the space of trajectories and can be utilized for learning from demonstration. We train an NDP policy following the imitation learning pipeline described in \cite{bahl2020neural}. For each task, given $\mathrm{x}(1)$, the neural network components in NDP generate the parameters of DMPs (radial basis functions (RBFs) in \cite{bahl2020neural}), which are forward integrated to produce the reference joint trajectories for tracking.  
% \newline
% \textbf{Riemannian Motion policy (RMP):}
\newline
\textbf{Neural Geometric Fabrics policy (NGF):} The Neural Geometric Fabrics~\cite{xieneural}, a structured policy class, that enables efficient skill learning for dexterous manipulation from demonstrations by leveraging structures induced by Geometric Fabrics~\cite{van2022geometric}. Geometric Fabrics is a stable class of the Riemannian Motion Policy (RMP)~\cite{cheng2020RmpflowJournalArxiv}. It has been demonstrated that NGF outperforms RMP in policy learning for dexterous manipulation task in~\cite{xieneural}. The NGF policy is defined in the configuration space of the robot, which is composed of a geometric policy, a potential policy and a damping term. More specifically, the NGF policy is constructed as follows: (1) define a geometric policy pair $[\mathbf{M}, \pi]$ and a potential policy pair $[\mathbf{M}_f, \pi_f]$ in the configuration space $\mathbf{q}$, (2) energize the geometric policy (project orthogonal to the direction of motion with $\mathbf{p}_e$) to create a collection of energy-preserving paths (the Geometric Fabric), and (3) force the Geometric Fabric with a potential defined by $[\mathbf{M}_f, \pi_f]$ and damp via $b$ applied along $\dot{\mathbf{q}}$, which ensures convergence to the potential's minima. The potential policy $\pi_f$ is the gradient of a function of position only. Note that we parameterize the geometric policy pair $[\mathbf{M}, \pi]$, the potential policy pair $[\mathbf{M}_f, \pi_f]$, and the damping scalar $b$ with MLP networks and learn them from demonstration data.
% \newline
% \textbf{Natural Policy Gradient (NPG) \& Demo Augmented Policy Gradient (DAPG):} For both methods, the policy design details can be found in \cite{Rajeswaran2018DAPG}. 
\subsection{Optimizing baseline model size}
\label{appendix:layer_size}
As described in Appendix.~\ref{sec:policy_designs}, we sticked with the same set of lifting functions for KODex and report the task success rate when we trained KODex on training set and tested it on validation set in Table.~\ref{tab:KODEX_all_demo}. However, for baselines, the hyper-parameters were selected through a set of ablation experiments for each task using the training set over three choices of model size, including small size, median size and large size. We generated five random seeds for parameter initialization per model size, per baseline, and per task, as all learning based baseline models are sensitive to parameter initialization \cite{henderson2018deep}. For each baseline policy, we report the mean and standard deviation of the task success rate on the validation set over five random seeds in Table.~\ref{tab:NN_size} \ref{tab:LSTM_size} \ref{tab:NDp_size} \ref{tab:NGF_size}.

Based on these results, we selectd the model size that offers the best performance in terms of task success rate. In addition, these results indicate that, unlike KODex, extensive hyper-parameter tuning and various trials on parameter initialization for baseline models are necessary. Note that we use $l$ to denote dim$(\mathrm{x}(t))$.
\label{Hyper_Parameters}
 \begin{table}[h]
    \caption{Task success rate on validation set (KODex)}
    \centering
    \begin{tabular}{c|c|c|c}
    \hline
  Tool  &  Door & Relocation & Reorientation \\
        \hline
    % 62.0\% & 88.0\%  & 96.0\% & 100.0\% \\
    100.0\% & 96.0\% & 88.0\% & 62.0\%\\
        \hline
    \end{tabular}
    \label{tab:KODEX_all_demo}
\end{table}

% \begin{table}[h]
% \scriptsize
%     \caption{Hyper-parameters on NN Network Sizes}
%     \centering
%     \begin{tabular}{c|c|c|c|c}
%     \hline
%         \multirow{3}{*}{\diagbox[width=10em,height=3\line]{Model Size}{Success Rate}{Task}} & \multirow{3}{*}{Reorientation} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Door} & \multirow{3}{*}{Tool}\\ 
%             (\%) &  & &  & \\
%             &  & &  & \\
%         \hline
%     MLP: (32, 64, 32)    &  6.8($\pm$3.9) & 0.4($\pm$0.8) & 0.0($\pm$0.0) & \textbf{0.4($\pm$0.8)} \\
%         \hline
%     MLP: (64, 128, 64) & \textbf{10.4($\pm$6.6)}  & \textbf{1.2($\pm$2.4)} & \textbf{0.4($\pm$0.8)}  & 0.0($\pm$0.0) \\
%         \hline
%    MLP: (128, 256, 128)    &  6.0($\pm$1.5)  & 0.8($\pm$1.6)  &  0.0($\pm$0.0)  & 0.0($\pm$0.0) \\
%     \hline
%     \end{tabular}
%     \label{tab:NN_size}
% \end{table}
\begin{table}[h]
\scriptsize
    \caption{Hyper-parameters on NN Network Sizes}
    \centering
    \begin{tabular}{c|c|c|c|c}
    \hline
        \multirow{3}{*}{\diagbox[width=10em,height=3\line]{Model Size}{Success Rate}{Task}} & \multirow{3}{*}{Tool} & \multirow{3}{*}{Door} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Reorientation}\\ 
            (\%) &  & &  & \\
            &  & &  & \\
        \hline
    % MLP: (32, 64, 32)    &  6.8($\pm$3.9) & 0.4($\pm$0.8) & 0.0($\pm$0.0) & \textbf{0.4($\pm$0.8)} \\
    MLP: (32, 64, 32)    &  \textbf{0.4($\pm$0.8)} & 0.0($\pm$0.0) & 0.4($\pm$0.8) & 6.8($\pm$3.9) \\
        \hline
    % MLP: (64, 128, 64) & \textbf{10.4($\pm$6.6)}  & \textbf{1.2($\pm$2.4)} & \textbf{0.4($\pm$0.8)}  & 0.0($\pm$0.0) \\
    MLP: (64, 128, 64) & 0.0($\pm$0.0)  & \textbf{0.4($\pm$0.8)} & \textbf{1.2($\pm$2.4)}  & \textbf{10.4($\pm$6.6)} \\
        \hline
   % MLP: (128, 256, 128)    &  6.0($\pm$1.5)  & 0.8($\pm$1.6)  &  0.0($\pm$0.0)  & 0.0($\pm$0.0)
      MLP: (128, 256, 128)    &  0.0($\pm$0.0)  & 0.0($\pm$0.0)  &  0.8($\pm$1.6)  & 6.0($\pm$1.5)
   \\
    \hline
    \end{tabular}
    \label{tab:NN_size}
\end{table}


% \begin{table}[h]
% \scriptsize
%     \caption{Hyper-parameters on LSTM Network Sizes}
%     \centering
%     \begin{tabular}{c|c|c|c|c}
%     \hline
%         \multirow{3}{*}{\diagbox[width=10em,height=3\line]{Model Size}{Success Rate}{Task}} & \multirow{3}{*}{Reorientation} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Door} & \multirow{3}{*}{Tool}\\ 
%            (\%) &  & &  & \\
%             &  & &  & \\
%         \hline
%     LSTM: 200 &  \multirow{2}{*}{\textbf{56.4($\pm$7.4)}} & \multirow{2}{*}{7.6($\pm$5.9)} & \multirow{2}{*}{\textbf{87.6($\pm$10.3)}} &  \multirow{2}{*}{28.8($\pm$25.0)}\\
%     fc: ($l$, 100), (200, $l$) & & & & \\
%         \hline
%     LSTM: 250 &  \multirow{2}{*}{48.0($\pm$17.0)} & \multirow{2}{*}{7.6($\pm$7.5)} & \multirow{2}{*}{80.8($\pm$24.5)} &  \multirow{2}{*}{\textbf{60.8($\pm$36.6)}}\\
%     fc: ($l$, 175), (250, $l$) & & & & \\
%         \hline
%        LSTM: 300 &  \multirow{2}{*}{54.0($\pm$11.0)} & \multirow{2}{*}{\textbf{16.4($\pm$14.5)}} & \multirow{2}{*}{82.0($\pm$13.9)}  & \multirow{2}{*}{44.8($\pm$31.8)}\\
%     fc: ($l$, 250), (300, $l$) & & & & \\
%     \hline
%     \end{tabular}
%     \label{tab:LSTM_size}
% \end{table}

\begin{table}[!th]
\scriptsize
    \caption{Hyper-parameters on LSTM Network Sizes}
    \centering
    \begin{tabular}{c|c|c|c|c}
    \hline
        \multirow{3}{*}{\diagbox[width=10em,height=3\line]{Model Size}{Success Rate}{Task}} & \multirow{3}{*}{Tool} & \multirow{3}{*}{Door} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Reorientation}\\ 
           (\%) &  & &  & \\
            &  & &  & \\
        \hline
    % LSTM: 200 &  \multirow{2}{*}{\textbf{56.4($\pm$7.4)}} & \multirow{2}{*}{7.6($\pm$5.9)} & \multirow{2}{*}{\textbf{87.6($\pm$10.3)}} &  \multirow{2}{*}{28.8($\pm$25.0)}\\
    % fc: ($l$, 100), (200, $l$) & & & & \\
    LSTM: 200 &  \multirow{2}{*}{28.8($\pm$25.0)} & \multirow{2}{*}{\textbf{87.6($\pm$10.3)}} & \multirow{2}{*}{7.6($\pm$5.9)} &  \multirow{2}{*}{\textbf{56.4($\pm$7.4)}}\\
    fc: ($l$, 100), (200, $l$) & & & & \\
        \hline
    % LSTM: 250 &  \multirow{2}{*}{48.0($\pm$17.0)} & \multirow{2}{*}{7.6($\pm$7.5)} & \multirow{2}{*}{80.8($\pm$24.5)} &  \multirow{2}{*}{\textbf{60.8($\pm$36.6)}}\\
    % fc: ($l$, 175), (250, $l$) & & & & \\
LSTM: 250 &  \multirow{2}{*}{\textbf{60.8($\pm$36.6)}} & \multirow{2}{*}{80.8($\pm$24.5)} & \multirow{2}{*}{7.6($\pm$7.5)} &  \multirow{2}{*}{48.0($\pm$17.0)}\\
    fc: ($l$, 175), (250, $l$) & & & & \\
        \hline
       % LSTM: 300 &  \multirow{2}{*}{54.0($\pm$11.0)} & \multirow{2}{*}{\textbf{16.4($\pm$14.5)}} & \multirow{2}{*}{82.0($\pm$13.9)}  & \multirow{2}{*}{44.8($\pm$31.8)}\\
       LSTM: 300 &  \multirow{2}{*}{44.8($\pm$31.8)} & \multirow{2}{*}{82.0($\pm$13.9)} & \multirow{2}{*}{\textbf{16.4($\pm$14.5)}}  & \multirow{2}{*}{54.0($\pm$11.0)}\\
    fc: ($l$, 250), (300, $l$) & & & & \\
    \hline
    \end{tabular}
    \label{tab:LSTM_size}
\end{table}

% \begin{table}[h]
% \scriptsize
%     \caption{Hyper-parameters on NDP Network Sizes}
%     \centering
%     \begin{tabular}{c|c|c|c|c}
%     \hline
%         \multirow{3}{*}{\diagbox[width=10em,height=3\line]{Model Size}{Success Rate}{Task}} & \multirow{3}{*}{Reorientation} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Door} & \multirow{3}{*}{Tool}\\ 
%           (\%)  &  & &  & \\
%             &  & &  & \\
%         \hline
%     MLP: (32, 64, 32) &  \multirow{2}{*}{57.2($\pm$8.6)} & \multirow{2}{*}{30.0($\pm$9.3)} & \multirow{2}{*}{8.0($\pm$2.5)} & \multirow{2}{*}{0.0($\pm$0.0)}\\
%     10 RBFs & & & & \\
%         \hline
%     MLP: (64, 128, 64) &  \multirow{2}{*}{59.2($\pm$6.5)} & \multirow{2}{*}{74.0($\pm$4.9)} & \multirow{2}{*}{40.8($\pm$8.1)} & \multirow{2}{*}{16.8($\pm$29.8)}\\
%     20 RBFs & & & & \\
%         \hline
%     MLP: (128, 256, 128) &  \multirow{2}{*}{\textbf{62.4($\pm$7.8)}} & \multirow{2}{*}{\textbf{79.2($\pm$7.7)}} & \multirow{2}{*}{\textbf{66.0($\pm$5.2)}} & \multirow{2}{*}{\textbf{18.4($\pm$31.9)}}\\
%     30 RBFs & & & & \\
%     \hline
%     \end{tabular}
%     \label{tab:NDp_size}
% \end{table}

\begin{table}[h]
\scriptsize
    \caption{Hyper-parameters on NDP Network Sizes}
    \centering
    \begin{tabular}{c|c|c|c|c}
    \hline
        \multirow{3}{*}{\diagbox[width=10em,height=3\line]{Model Size}{Success Rate}{Task}} & \multirow{3}{*}{Tool} & \multirow{3}{*}{Door} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Reorientation}\\ 
          (\%)  &  & &  & \\
            &  & &  & \\
        \hline
    % MLP: (32, 64, 32) &  \multirow{2}{*}{57.2($\pm$8.6)} & \multirow{2}{*}{30.0($\pm$9.3)} & \multirow{2}{*}{8.0($\pm$2.5)} & \multirow{2}{*}{0.0($\pm$0.0)}\\
    MLP: (32, 64, 32) &  \multirow{2}{*}{0.0($\pm$0.0)} & \multirow{2}{*}{8.0($\pm$2.5)} & \multirow{2}{*}{30.0($\pm$9.3)} & \multirow{2}{*}{57.2($\pm$8.6)}\\
    10 RBFs & & & & \\
        \hline
    % MLP: (64, 128, 64) &  \multirow{2}{*}{59.2($\pm$6.5)} & \multirow{2}{*}{74.0($\pm$4.9)} & \multirow{2}{*}{40.8($\pm$8.1)} & \multirow{2}{*}{16.8($\pm$29.8)}\\
    MLP: (64, 128, 64) &  \multirow{2}{*}{16.8($\pm$29.8)} & \multirow{2}{*}{40.8($\pm$8.1)} & \multirow{2}{*}{74.0($\pm$4.9)} & \multirow{2}{*}{59.2($\pm$6.5)}\\
    20 RBFs & & & & \\
        \hline
    % MLP: (128, 256, 128) &  \multirow{2}{*}{\textbf{62.4($\pm$7.8)}} & \multirow{2}{*}{\textbf{79.2($\pm$7.7)}} & \multirow{2}{*}{\textbf{66.0($\pm$5.2)}} & \multirow{2}{*}{\textbf{18.4($\pm$31.9)}}\\
    MLP: (128, 256, 128) &  \multirow{2}{*}{\textbf{18.4($\pm$31.9)}} & \multirow{2}{*}{\textbf{66.0($\pm$5.2)}} & \multirow{2}{*}{\textbf{79.2($\pm$7.7)}} & \multirow{2}{*}{\textbf{62.4($\pm$7.8)}}\\
    30 RBFs & & & & \\
    \hline
    \end{tabular}
    \label{tab:NDp_size}
\end{table}

% \begin{table}[h]
%     \caption{Hyper-parameters on NGF Network Sizes}
%     \centering
%     \begin{tabular}{c|c|c|c|c}
%     \hline
%         \multirow{3}{*}{\diagbox[width=10em,height=3\line]{Model Size}{Success Rate}{Task}} & \multirow{3}{*}{Reorientation} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Door} & \multirow{3}{*}{Tool}\\ 
%            (\%) &  & &  & \\
%             &  & &  & \\
%         \hline
%     MLP: (64, 32)    &  77.6($\pm$2.3) & 87.6($\pm$8.5) & 87.2($\pm$12.0) & 99.2($\pm$1.6) \\
%         \hline
%     MLP: (128, 64) & 72.4($\pm$4.5)  & 94.4($\pm$3.2) & 90.0($\pm$5.9)  & \textbf{100.0($\pm$0.0)} \\
%         \hline
%    MLP: (256, 128)    &  \textbf{78.4($\pm$3.4)}  & \textbf{95.2($\pm$1.6)}  &  \textbf{90.8($\pm$4.3)}  & 83.6($\pm$20.1) \\
%     \hline
%     \end{tabular}
%     \label{tab:NGF_size}
% \end{table}

\begin{table}[h]
\scriptsize
    \caption{Hyper-parameters on NGF Network Sizes}
    \centering
    \begin{tabular}{c|c|c|c|c}
    \hline
        \multirow{3}{*}{\diagbox[width=10em,height=3\line]{Model Size}{Success Rate}{Task}} & \multirow{3}{*}{Tool} & \multirow{3}{*}{Door} & \multirow{3}{*}{Relocation} & \multirow{3}{*}{Reorientation}\\ 
           (\%) &  & &  & \\
            &  & &  & \\
        \hline
    % MLP: (64, 32)    &  77.6($\pm$2.3) & 87.6($\pm$8.5) & 87.2($\pm$12.0) & 99.2($\pm$1.6) \\
    MLP: (64, 32)    &  99.2($\pm$1.6) & 87.2($\pm$12.0) & 87.6($\pm$8.5) & 77.6($\pm$2.3) \\
        \hline
    % MLP: (128, 64) & 72.4($\pm$4.5)  & 94.4($\pm$3.2) & 90.0($\pm$5.9)  & \textbf{100.0($\pm$0.0)} \\
    MLP: (128, 64) & \textbf{100.0($\pm$0.0)}  & 90.0($\pm$5.9) & 94.4($\pm$3.2)  & 72.4($\pm$4.5) \\
        \hline
   % MLP: (256, 128)    &  \textbf{78.4($\pm$3.4)}  & \textbf{95.2($\pm$1.6)}  &  \textbf{90.8($\pm$4.3)}  & 83.6($\pm$20.1) \\
   MLP: (256, 128)    &  83.6($\pm$20.1)  & \textbf{90.8($\pm$4.3)}  &  \textbf{95.2($\pm$1.6)}  & \textbf{78.4($\pm$3.4)} \\
    \hline
    \end{tabular}
    \label{tab:NGF_size}
\end{table}

\subsection{Hyper-parameters for controller learning}
\label{appendix:controller}
The hyper-parameters we used to learn the inverse dynamics controller $C$ for each task were the same as listed in Table.~\ref{tab:controller}. Note 
 that we use $l_r$ to denote dim$(\mathrm{x}_r(t))$.
 \begin{table}[h]
    \caption{Hyper-parameters on controller learning}
    \centering
    \begin{tabular}{c|c|c|c}
    \hline
   Hidden Layer   &  Activation & Learning Rate & Iteration \\
        \hline
    $(4l_r, 4l_r, 2l_r)$ & ReLU  & 0.0001 & 300 \\
        \hline
    \end{tabular}
    \label{tab:controller}
\end{table}

% \subsection{Baseline Training details}
% \label{Train_detail}
% We provide with additional training details for all the baseline models, namely batch size, training iteration, learning rate.
% \begin{table}[h]
%     \caption{Hyper-parameters on baseline training}
%     \centering
%     \begin{tabular}{c|c|c|c}
%     \hline
%      &Batch Size& Training Iteration& Learning Rate  \\
%         \hline
%    NN   &  8 & 200 & 0.001  \\
%         \hline
%     LSTM & 4  & 200 & 0.001 \\
%         \hline
%    NDP  &  4  & 200 & 0.001   \\
%     \hline
%     \end{tabular}
%     \label{tab:NN_size}
% \end{table}

%% Use plainnat to work nicely with natbib. 
% plainnat may cause some wrong orderings

\end{document}

% Please make sure to include \verb!natbib.sty! and to use the
% \verb!plainnat.bst! bibliography style. \verb!natbib! provides additional
% citation commands, most usefully \verb!\citet!. For example, rather than the
% awkward construction 

% {\small
% \begin{verbatim}
% \cite{kalman1960new} demonstrated...
% \end{verbatim}
% }

% \noindent
% rendered as ``\cite{kalman1960new} demonstrated...,''
% or the
% inconvenient 

% {\small
% \begin{verbatim}
% Kalman \cite{kalman1960new} 
% demonstrated...
% \end{verbatim}
% }

% \noindent
% rendered as 
% ``Kalman \cite{kalman1960new} demonstrated...'', 
% one can
% write 

% {\small
% \begin{verbatim}
% \citet{kalman1960new} demonstrated... 
% \end{verbatim}
% }
% \noindent
% which renders as ``\citet{9561177} demonstrated...'' and is 
% both easy to write and much easier to read.

% This year, we would like to use the ability of PDF viewers to interpret
% hyperlinks, specifically to allow each reference in the bibliography to be a
% link to an online version of the reference. 
% As an example, if you were to cite ``Passive Dynamic Walking''
% \cite{McGeer01041990}, the entry in the bibtex would read:

% {\small
% \begin{verbatim}
% @article{McGeer01041990,
%   author = {McGeer, Tad}, 
%   title = {\href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic Walking}}, 
%   volume = {9}, 
%   number = {2}, 
%   pages = {62-82}, 
%   year = {1990}, 
%   doi = {10.1177/027836499000900206}, 
%   URL = {http://ijr.sagepub.com/content/9/2/62.abstract}, 
%   eprint = {http://ijr.sagepub.com/content/9/2/62.full.pdf+html}, 
%   journal = {The International Journal of Robotics Research}
% }
% \end{verbatim}
% }
% \noindent
% and the entry in the compiled PDF would look like:

% \def\tmplabel#1{[#1]}

% \begin{enumerate}
% \item[\tmplabel{1}] Tad McGeer. \href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic
% Walking}. {\em The International Journal of Robotics Research}, 9(2):62--82,
% 1990.
% \end{enumerate}
% %
% where the title of the article is a link that takes you to the article on IJRR's website. 


% Linking cited articles will not always be possible, especially for
% older articles. There are also often several versions of papers
% online: authors are free to decide what to use as the link destination
% yet we strongly encourage to link to archival or publisher sites
% (such as IEEE Xplore or Sage Journals).  We encourage all authors to use this feature to
% the extent possible.
