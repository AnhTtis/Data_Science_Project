
\section{Introduction}

\blfootnote{$^\star$ Indicates equal contribution.\\\new{$^\dagger$ part of this work was done during an internship at MetaAI.}}

Diffusion models have rapidly emerged as formidable generative models for images, replacing others (\eg, VAEs, GANs) for a range of applications, including image colorization~\cite{palette:sigg:22}, image editing~\cite{meng2021sdedit}, and image synthesis~\cite{ho2022cascaded,dhariwal2021diffusion}.
These models explicitly optimize the likelihood of the training samples, can be trained on millions if not billions of images, and have been shown to capture the underlying model distribution better~\cite{dhariwal2021diffusion} than previous alternatives.

A natural next step is to bring diffusion models to 3D data.  
Compared to 2D images, 3D models facilitate direct manipulation of the generated content, result in perfect view consistency across different cameras, and allow object placement using direct handles.
However, learning 3D diffusion models is hindered by the lack of a sufficient volume of 3D data for training.
A further question is the choice of representation for the 3D data itself (\eg, voxels, point clouds, meshes, occupancy grids, etc.).
Researchers have proposed 3D-aware diffusion models for point clouds~\cite{pointCloudDiffusion:21}, volumetric shape data using wavelet features~\cite{waveletDiffusion:22} and novel view synthesis~\cite{watson20223dim}.
They have also proposed to distill a pretrained 2D diffusion model to generate neural radiance fields of 3D objects~\cite{poole2022dreamfusion, lin2022magic3d}.
However, a diffusion-based 3D generator model trained using only 2D image for supervision is not available yet.

In this paper, we contribute \name, the first unconditional 3D diffusion model that can be trained with \textit{only} real posed 2D images.
By posed, we mean different views of the same object with known cameras, for example, obtained by means of structure from motion~\cite{schoenberger2016sfm}.

We make two main technical contributions:
(i) We propose a new 3D model that uses a hybrid explicit-implicit feature grid.
The grid can be rendered to produce images from any desired viewpoint and, since the features are defined in 3D space, the rendered images are consistent across different viewpoints.
Compared to utilizing an explicit density grid, the feature representation allows for a lower resolution grid.
The latter leads to an easier estimation of the probability density due to a smaller number of variables.
Furthermore, the resolution of the grid can be decoupled from the resolution of the rendered images.
(ii)
We design a new diffusion method that can learn a distribution over such 3D feature grids while only using 2D images for supervision.
Specifically, we first generate intermediate 3D-aware features conditioned only on the input posed images. 
Then, following the standard diffusion model learning, we add noise to this intermediate representation and train a denoising 3D UNet to remove the noise.
\new{We} apply the denoising loss as photometric error between the rendered images and the Ground-Truth training images.
The key advantage of this approach is that it enables training of the 3D diffusion model from 2D images, which are abundant, sidestepping the difficult problem of procuring a huge dataset of 3D models for training.

We train and evaluate our method on the Co3Dv2~\cite{reizenstein21common} dataset where \name outperforms existing alternatives both qualitatively and quantitatively.

