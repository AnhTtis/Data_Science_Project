\setcounter{figure}{0} \renewcommand{\thefigure}{\Roman{figure}}
\setcounter{table}{0} \renewcommand{\thetable}{\Roman{table}}

\begin{strip}%
 \centering
 \Large
 \textbf{%
{
\name: Training a 3D Diffusion Model using 2D Images
}
\\
 \vspace{0.3cm} 
 \textit{Supplementary material}
 }\\
\vspace{0.5cm}
\end{strip}

\section{Views2Voxel-grid Unprojection Mechanism}
Given a training video $s$ containing frames $I_j$, we generate a grid $\bar V \in \real^{d^V \times S \times S \times S}$
of auxiliary features $\bar V_{:mno} \in [-1, 1]^{d_V}$ by using the following procedure.
We first project the 3D coordinate $\bx^V_{mno}$ of each grid element $(m,n,o)$  to every video frame $I_j$ and sample corresponding 2D image features. The 2D image features $f_{mno}^{j}$ are obtained using a
ResNet32~\cite{he15deep} encoder $E(I_j)$. We use bilinear interpolation for sampling continuous values and use zero-features for projected points that lie outside the Image. Thus, we obtain $N_\text{frames}$ features (corresponding to each frame in the video) for each grid element of the voxel-grid. We accumulate these features using the Accumulator MLP $\mathcal{A}_{acc}$. The accumulator $\mathcal{A}_{acc}$ takes as input $[f_{mno}^{j}; v^j]$, where $[;]$ denotes concatenation and $v^j$ corresponds to the viewing direction corresponding to the camera center of $j^{\text{th}}$ frame, and outputs $[\sigma^j_{mno}; {f'}_{mno}^{j}]$. Finally, we compute the feature at each of the voxel grid centers as a weighted sum of the newly mapped features:
\begin{equation}
    F_{mno} = \sum_{j} \sigma^j_{mno} {f'}_{mno}^{j}.
\end{equation}

\section{Implementation Details}
In this section, we provide more details related to implementing our proposed method.

\subsection{Network Architectures}
Our proposed pipeline (Fig 2. of main paper) contains three neural components: The Encoder, Diffusion UNet and Renderer. The Encoder network is a ResNet32 model \cite{he15deep}.
For the main diffusion network, we use a 3D variant of the UNet used by Dhariwal and Nichol \cite{dhariwal2021diffusion}. The model comprises residual blocks containing downsampling, upsampling, and self-attention blocks (with additive residual connections).

\subsection{Renderer}
In order to decode the generated voxel-grid of features into density and radiance fields, we use a NeRF-like \cite{mildenhall2020nerf} MLP (Multi-layer perceptron). The MLP contains 4 layers of 256 hidden units with a skip-connection on the 3rd hidden layer. The skip connection concatenates the input features with the intermediate hidden layer features. Similar to NeRF, and for the reasons described in Zhang et al. \cite{zhang2020nerf++}, we also input the view-directions at a latter layer in the MLP. The input features are not encoded, but we apply sinusoidal encodings \cite{mildenhall2020nerf, vaswani17attention} to the input viewing directions with max frequency level $L=4$. The activation functions used are: \texttt{LeakyReLU} for the hidden layers, \texttt{Softplus} for the density output head, and the \texttt{Sigmoid} for the radiance output head. All trainable weights are initialized using the Xavier uniform initialization \cite{glorot2010understanding}. Figure \ref{fig:render_mlp} shows the detailed architecture of the \texttt{RenderMLP}. 

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{figures/supplementary/suppl_renderMLP.png}
\caption{Architecture of the \texttt{RenderMLP} used for decoding the features of the generated voxel grids into density and radiance fields.}
\label{fig:render_mlp}
\end{figure}

\subsection{Training Details}
We train the full \name pipeline for $1000$ epochs over the dataset containing the object-centric videos. During training, we randomly sample 11 source views for unprojecting into the initial voxel-grid, and 1 target (reserved) novel view for computing loss. The latter enforces 3D structure in the generated samples. We use $L2$ distance between the rendered views and the G.T. views as the photometric-consistency loss. In terms of hardware, we train all our models on 4-8 32GB-V100 GPUs, with a \texttt{batch-size} equal to the number of GPUs in use, i.e., each GPU processes one voxel-grid during training.
We use \textit{Adam} \cite{kingma15adam:} optimizer with a learning rate ($\alpha$) of $0.00005$ and default values of $\beta_1$, $\beta_2$, and $\epsilon$ for all the trainable networks during training.

\subsection{Diffusion Details}
We use the DDPM \cite{ho2020denoising} diffusion-formulation for our bootstrap-latent-diffusion module as described in section 4.2 of the main paper. We use the default $t = 1000$ time-steps and the default $\beta_t$ schedule in our experiments: wherein we set $\beta_0 = 0.0001; \beta_{999} = 0.02$. Rest of the $\beta_t$ values are obtained by linearly interpolating between the $\beta_0$ and $\beta_{999}$. Finally, to improve the input conditioning of our diffusion module, we apply $tanh$ to the voxel features to constrain their values in the range of [-1, 1], as proposed in Karras et al. \cite{karras2022elucidating}. This allows us to apply [-1, 1] clipping during sampling.














