\section{Related Work}%
\label{s:realted}




\subsection{Image-conditioned 3D Reconstruction}

\paragraph{Neural and differentiable rendering.}

Neural rendering~\cite{tewari2020state} is a class of algorithms that partially or entirely use neural networks to approximate the light transport equation. 

The 2D versions of neural rendering include variants of pix2pix~\cite{isola2017image}, deferred neural rendering~\cite{thies2019deferred}, and their follow-up works.
The common theme in all these methods is that a post-processor neural network (usually a CNN) maps neural feature images into photorealistic RGB images.

The 3D versions of neural rendering have recently been popularized by NeRF~\cite{mildenhall2020nerf}, which uses a Multi Layer Perceptron (MLP) to model the parameters of the 3D scene (radiance and occupancy) and a physically-based rendering procedure (Emission-Absorption raymarching).
NeRF solves the inverse rendering problem where, given many 2D images of a scene, the aim is to recover its 3D shape and appearance.
The success of NeRF gave rise to many follow-up works \cite{mildenhall2020nerf, barron2021mip, zhang2020nerf++, barron2022mip, verbin2022ref}.
While the latter uses MLPs to represent the occupancy and radiance of the underlying scene, different representations were explored in~\cite{lombardi2019neural, chen2022tensorf, yu2021plenoxels, sun2022direct, muller2022instant, wang2018pixel2mesh, groueix2018papier, karnewar2022relu}.

 
\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/pipeline_small.png}
\caption{
\textbf{Method overview. }
Our \name takes as input video frames for category-specific videos $\{s^i\}$ and trains a diffusion-based generative model $\mathcal{D}_\theta$.
The model is trained with only posed image supervision $\{(I_j^i, P_j^i)\}$, without access to 3D ground-truth.
Once trained, the model can generate view-consistent results from novel camera locations.
Please refer to \cref{sec:method} for details.
}%
\label{fig:pipeline}
\end{figure*}

\paragraph{Few-view reconstruction.}

In many cases, dense image supervision is unavailable, and one has to condition the reconstruction on a small number of scene views instead.
Since 3D reconstruction from few-views is ambiguous, recent methods aid the reconstruction with 3D priors learned by observing many images of an object category. %
Works like CMR~\cite{kanazawa18learning}, C3DM~\cite{novotny20canonical}, and UMR~\cite{li2020self} learn to predict the parameters of a mesh by observing images of individual examples of the object category.
DOVE~\cite{wu21dove:} also predicts meshes, but additionally leverages stronger constraints provided by videos of the deformable objects.

Others~\cite{yu20pixelnerf:,henzler2021unsupervised} aimed at learning to fit NeRF given only a small number of views;
they do so by sampling image features at the 2D projections of the 3D ray samples.
Later works~\cite{reizenstein21common,wang2021ibrnet} have improved this formulation by using transformers to process the sampled features.
Finally, ViewFormer~\cite{kulhanek2022viewformer} drops the rendering model and learns a fully implicit transformer-based new-view synthesizer.
Recently, BANMo~\cite{yang2022banmo} reconstructed deformable objects with a signed distance function.

\subsection{3D Generative Models}

\paragraph{3D Generative advesarial networks.}
Early 3D generative models leveraged adversarial learning~\cite{goodfellow2020gan} as the primary form of supervision.
PlatonicGAN~\cite{henzler2019platonic-gan} learns to generate colored 3D shapes \new{from} an unstructured corpus of images belonging to the same category, by rendering a voxel grid from random viewpoints so that an adversarial discriminator cannot distinguish between the renders and natural images sampled from a large uncurated database.
PrGAN~\cite{gadelha163d-shape} differs from PlatonicGAN by focusing only on rendering untextured 3D shapes.
To deal with the large memory footprint of voxels, HoloGAN~\cite{nguyen-phuoc19hologan:} adjusts PlatonicGAN by rendering a low-resolution 2D feature image of a coarse voxel grid, followed by a 2D convolutional decoder mapping the feature render to the final RGB image.
The results are, however, not consistent with camera motion. 

Inspired by the success of NeRF~\cite{mildenhall20nerf:}, GRAF~\cite{schwarz20graf:} also trains in a data setting similar to PlatonicGAN~\cite{henzler2019platonic-gan} but, differently from PlatonicGAN, represents each generated scene with a neural radiance field.
The GRAF pipeline was subsequently improved by PiGAN~\cite{chan2021pi}, leveraging a SIREN-based~\cite{sitzmann20implicit} architecture.
Similar to HoloGAN, StyleNerf~\cite{gu2021stylenerf} first renders a radiance feature field followed by a convolutional super-resolution network.
EG3D~\cite{chan2022efficient} further improves the pipeline by initially decoding a randomly sampled latent vector to a tri-plane representation followed by a NeRF-style rendering of a radiance field supported by the tri-plane.
EpiGRAF~\cite{skorokhodov2022epigraf} further improves upon the triplane-based 3D generation.
GAUDI~\cite{bautista2022gaudi} also uses the tri-plane while building upon DeVries et. al.~\cite{devries2021unconstrained} which used a single plane representing the floor map of the indoor room scenes being generated.

Besides radiance fields, other shape representations have also been explored.
While VoxGRAF~\cite{schwarz2022voxgraf} replaces the radiance field of GRAF with a sparse voxel grid, StyleSDF~\cite{or2022stylesdf} employs signed distance fields, and Neural Volumes~\cite{lombardi2019neural} propose a novel trilinearly-warped voxel grid.
Wu et al.~\cite{wu2020unsupervised} differentiably render meshes and aid the adversarial learning with a set of constraints exploiting symmetry properties of the reconstructed categories.
Recently, GET3D~\cite{gao2022get3d} differentiably converts an initial tri-plane representation to colored mesh, which is finally rendered.

The aforementioned approaches are trained solely by observing uncurated category-centric image collections \textit{without} the need for any explicit 3D supervision in form of the ground truth 3D shape or camera pose.
However, since the rendering function is non-smooth under camera motion, these methods can either successfully reconstruct image databases with a very small variation in camera poses (\eg, fronto-parallel scenes such as portrait photos of cat or human faces) or datasets with a well-defined distribution of camera intrinsics end extrinsics (\eg, synthetic datasets).
We tackle the pose estimation problem by leveraging a dataset of category-centric videos each containing multiple views of the same object.
Observing each object from a moving vantage point allows for estimating accurate scene-consistent camera poses that provide strong constraints.

While most approaches focus on generating shapes of isolated instances of object categories, GIRAFFE~\cite{niemeyer21giraffe:} and BlockGAN~\cite{nguyen-phuoc20blockgan:} extend GRAF and HoloGAN to reconstruct compositions of objects and their background.
Alternative approaches focus on text-conditioned 3D shape generation~\cite{bautista2022gaudi,jain2022zero,poole2022dreamfusion}, or learn~\cite{karnewar_3InGan_3dv_22} a generative model by observing a single self-similar scene.

\paragraph{3D diffusion models.}

Diffusion models for 3D shape learning have been explored only very recently.
Luo~\etal~\cite{luo2021diffusion} use full 3D supervision to learn a generative diffusion model of point clouds.
In a concurrent effort, Watson~\etal~\cite{watson2022novel} learns a new-view synthesis function which, conditioned on a posed image of a scene, generates a new view of the scene from a specified target viewpoint.
Differently from us, \cite{watson2022novel} do not employ an explicit image formation model which may lead to geometrical inconsistencies between generated viewpoints.


 
