\section{Experiments}

\begin{figure*}
\centering%
\includegraphics[width=1.02\linewidth]{figures/mega_qualitative_figure_camera_ready.pdf}
\caption{
\textbf{Comparisons. }
Samples generated by our \name compared to those by pi-GAN, EG3D, and GET3D. 
\label{fig:mega_qualitative}
\label{fig:qualitative}
}
\end{figure*}

In this section, we evaluate our method.
\new{First} we perform the \textit{quantitative} evaluation and then follow it by visualizing samples for assessing the \textit{quality} of generations.

\paragraph{Datasets and baselines.}

For our experiments, we use CO3Dv2~\cite{reizenstein21common}, which is currently the largest available dataset of fly-around real-life videos of object categories.
The dataset contains videos of different object categories and each video makes a complete circle around the object, showing all sides of it.
Furthermore, camera poses and object foreground masks are provided with the dataset (they were obtained by the authors by running off-the-shelf Structure-from-Motion and instance segmentation software, respectively).

\new{We} consider the four categories \texttt{Apple}, \texttt{Hydrant}, \texttt{TeddyBear} and \texttt{Donut} for our experiments. For each of the categories we train a single model on the 500 ``train'' videos (\new{i.e.} approx. $500 \times 100$ frames in total) with the highest camera cloud quality score, as defined in the CO3Dv2 annotations, in order to ensure clean ground-truth camera pose information. \new{We note} that all trainings were done on $2$-to-$8$ V100 32GB GPUs for 2 weeks. 

\new{We consider} the prior works pi-GAN~\cite{chan2021pi}, EG3D~\cite{chan2022efficient}, and GET3D~\cite{gao2022get3d} as baselines for comparison.
Pi-GAN generates radiance fields represented by MLPs and is trained using an adversarial objective.
Similar to our setting, they only use 2D image supervision for training. 
\new{EG3D~\cite{chan2022efficient}} uses the feature triplane, decoded by an MLP as the underlying representation, while needing both the images and the camera poses as input to the training procedure. \new{GET3D~\cite{gao2022get3d}} is another GAN-based baseline, which also requires the images and camera poses for training. Apart from this, GET3D also requires the fg/bg masks for training; which we supply in form of the masks available in CO3Dv2. Since GET3D applies a Deformable Marching Tetrahedra step in the pipeline, the samples generated by them are in the form of textured meshes.

\paragraph{\new{Quantitative evaluation}.}
\input{___tab_quant}
\begin{figure*}%
\centering%
\includegraphics[width=0.95\linewidth]{figures/time_change.pdf}\\
\includegraphics[width=1.02\linewidth]{figures/fig_explosion.pdf}%
\caption{%
\textbf{Sampling across time. }
Rendering of \name's \new{iterative sampling process} for a hydrant and a teddy bear.
The diffusion time decreases from left ($t=T=1000$) to the right ($t=0$).
\label{fig:explosion}
}
\end{figure*}
We report Frechet Inception Distance (FID)~\cite{heusel2017gan-stability}, and Kernel Inception Distance (KID)~\cite{binkowski2018demystifying} for assessing the generative quality of our results. 
As shown in \Cref{tab:base_qunat}, our \name produces better scores than EG3D and GET3D.
Although pi-GAN gets better scores than ours on some categories, we note that the 3D-agnostic training procedure of pi-GAN cannot recover the proper 3D structure of the unaligned shapes of CO3Dv2. Thus, without the 3D-view consistency, the 3D neural fields (MLPs) produced by pi-GAN essentially mimic a 2D image GAN.

\paragraph{\new{Qualitative evaluation}.}
\Cref{fig:qualitative} depicts random samples generated from all the methods under comparison. \name produces the most appealing, consistent and realistic samples among all.
\Cref{fig:qualitative_vp} further analyzes the viewpoint consistency of pi-GAN compared to ours.
It is evident that, although individual views of pi-GAN samples look realistic, their appearance is inconsistent with the change of viewpoint. Please refer to the project webpage
for more examples and videos of the generated samples. %










