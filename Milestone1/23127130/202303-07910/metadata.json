{
    "arxiv_id": "2303.07910",
    "paper_title": "Revisit Parameter-Efficient Transfer Learning: A Two-Stage Paradigm",
    "authors": [
        "Hengyuan Zhao",
        "Hao Luo",
        "Yuyang Zhao",
        "Pichao Wang",
        "Fan Wang",
        "Mike Zheng Shou"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Parameter-Efficient Transfer Learning (PETL) aims at efficiently adapting\nlarge models pre-trained on massive data to downstream tasks with limited\ntask-specific data. In view of the practicality of PETL, previous works focus\non tuning a small set of parameters for each downstream task in an end-to-end\nmanner while rarely considering the task distribution shift issue between the\npre-training task and the downstream task. This paper proposes a novel\ntwo-stage paradigm, where the pre-trained model is first aligned to the target\ndistribution. Then the task-relevant information is leveraged for effective\nadaptation. Specifically, the first stage narrows the task distribution shift\nby tuning the scale and shift in the LayerNorm layers. In the second stage, to\nefficiently learn the task-relevant information, we propose a Taylor\nexpansion-based importance score to identify task-relevant channels for the\ndownstream task and then only tune such a small portion of channels, making the\nadaptation to be parameter-efficient. Overall, we present a promising new\ndirection for PETL, and the proposed paradigm achieves state-of-the-art\nperformance on the average accuracy of 19 downstream tasks.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07910v1"
    ],
    "publication_venue": "11 pages"
}