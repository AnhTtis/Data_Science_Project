\section{Expert Interview}\label{sec:expert}

We conducted expert interviews to better understand whether the proposed system achieves its design goals, as well as its strengths and limitations. 
Based on the feedback from the pilot study where domain experts expressed their concerns in using existing tools that are limited by basic or surface-level group analyses, we see the evaluation process needs to be formulated in a way that demonstrates how users can gain the insights that are more complex (i.e., involving several pieces of data as evidence \textit{"in a synergistic way"} rather than simple individual data), relevant (i.e., \textit{"deeply embedded"} in the relevant domain), and deep (i.e., \textit{“accumulating and building on itself”}) while using our system. It is referred as the insight-based evaluation as termed from prior research in evaluating visualization \cite{northMeasuringVisualizationInsight2006, plaisantPromotingInsightBasedEvaluation2008}. For this purpose, instead of a quantitative evaluation we chose to conduct a more elaborate semi-structured interview to let the interviewees facilitate their thinking process enough to derive the context-specific and insightful findings simulating their workflows where they can test their own the hypothesis and find out quantitative evidence.

We invited three domain experts -- a political scientist, a social psychologist, and a machine learning expert specialized in natural language processing. All three experts had experience in working with social media data. Two of them had participated in our pilot interviews and their concerns and desired analytic support have been incorporated into our design guideline. In these open-ended interviews, we aimed to evaluate \name in a realistic group analysis workflow.

Each interview lasted about 90 minutes. The first was conducted in person, while the other two were via video conferencing. The system was running on a Chrome browser from both computers of the interviewer and interviewee. For each interview, we first provided a guided tutorial of the system and dataset, followed by a walkthrough of the system and a semi-structured interview. To emulate a realistic workflow, we asked the participants to think aloud. They were asked to consider: (1) a research question they would like to explore, or any hypothesis they may want to test or generate with the system, (2) how the system may facilitate the exploration of their question, and (3) the limitation or desired activities of current system. This section summarizes our findings from the three interviews.

\begin{figure}[!t]
    \vspace{-1.5em}
    \setlength\tabcolsep{2pt} % default value: 6pt
    \begin{tabular}{c}\\
    \includegraphics[width=0.95\linewidth]{figures/expert-interview-1.pdf}
    \end{tabular}  
    \vspace{-1em}
    \caption{\label{fig:expert-interview-1}
    \textbf{Expert interview 1}: Analyzing the within-group language variability of \care. (a) After observing the variance within three subgroups in \variscope, the expert retrieved the relevant language-level evidence to explore the aspect of issue polarization in \languagescope.}
    \vspace{-1em}
\end{figure}

\subsection{Interview 1: Exploring ways of political polarization}
Expert 1 is a political scientist interested in studying {\it the varying aspects of political polarization}. He would like to use \name to capture how social media users are polarized on gun-related issues. In particular, he wanted to explore whether the increasingly polarized online space is more of a reflection of {\it issue polarization} or {\it non-issue polarization}. He explained that in non-issue polarization, such as {\it affective} or {\it identity} polarization, the divide is driven by ideology, partisanship, or group identity, whereas in issue polarization, the group difference reflects different issue positions or policy attitudes. He hypothesized that in the case of non-issue polarization, {\it the language patterns will be more similar in one camp but mutually disjoint between camps; in contrast, users' languages will be more diverse in general if the concern is issue based}.

\paragraph{\bf Identify typical group behaviors (T1) and representative subgroups (T3)}
He started with the \scopecontroller and found the two camps were largely dissimilar in most of the sociolinguistic attributes. While confirming that the attributes of \blue were statistically significant from the \red in most of the dimensions (except for \puri) (Fig. \ref{fig:system-overview}b-i), he commented that the clear overall differences could be a sign for non-issue polarization. Next, the \variscope on \grouptrend caught his attention, ``{\it [it allows me to] take a closer look at each attribute and observe the subtle differences in each group.}'' He observed that the overall subgroup trends showed how the two groups were separated, and that the subgroups, 1 and 10, were quite ``representative'' of each camp, which represented how the attribute values of one camp were far from the other side. Having seen the varying patterns, he commented ``{\it [this could be] a useful tool for observing the partisan divide not just from political pundits but also from normal citizens.}''

\paragraph{\bf Refine initial hypotheses with language cues (T5) and help mitigate overgeneralization on group characterization}
Continuing on his exploration, he found that he learned more about ``{\it group variation rather than group coherence}'' in \variscope. For example, two \blue-dominant subgroups (2 and 3) had quite different values in the \care dimension (Fig. \ref{fig:expert-interview-1}i,ii), while \red-dominant subgroup (10) was likely to express with a tone contrary to \care (i.e., {\it harm}), which was similar to that of the \blue-dominant subgroup (3) (Fig. \ref{fig:expert-interview-1}iii). Uncertain about what such similarity means, he used \languagescope to retrieve relevant language cues (Fig. \ref{fig:expert-interview-1}b). The highlighted text from the subgroup 2, \tweet{thoughts and prayers are} clearly expressed \care; on the other hand, the texts \tweet{chicago has lost so many to gun violence} from the subgroup 3 and \tweet{gun law country because they cannot defend themseleves} from the subgroup 10 both concerned the harm but there was a difference in what was responsible for the harm (gun violence vs. gun law). ``{\it [These languages differences] did show the varying aspect of concerns [on this gun issue]},'' but after observing the language cues (Fig. \ref{fig:expert-interview-1}b), he felt he needed to be more cautious in interpreting the ``similar'' language patterns. While he found more evidence for the issue polarization hypothesis, he felt his original set up through comparing the language similarity was insufficient and can be misleading if not inspecting the subtleness of how the languages are used in the issue contexts. He concluded, ``{\it [this tool] offers enough depth and information to allow me to learn from the complexity of messages}.'' 

\begin{figure}[!t]
    \vspace{-1.5em}
    \setlength\tabcolsep{2pt} % default value: 6pt
    \begin{tabular}{c}\\
    \includegraphics[width=0.95\linewidth]{figures/expert-interview-2.pdf}
    \end{tabular}
    \vspace{-1em}
    \caption{\label{fig:expert-interview-2}
    \textbf{Expert interview 2}: Summary of representative language cues for \fair and \domi from \blue-dominated and \red-dominated subgroups. The language cues from the subgroups represent various aspects of online campaigns and debates about guns and gun control policies. }
    \vspace{-1em}
\end{figure}

\subsection{Interview 2: Language insights for online activism and campaigns}

Expert 2 is a social psychologist interested in studying language use and narratives in online movements and campaigns. Having learned about the gun-issue dataset, she was eager to use \name to see how the two political camps differ in psycholinguistic dimensions, particularly in \fair and \domi. She hypothesized the two camps would show different patterns in \domi because she perceived a gradual shift in  public opinions (with recent polls showing increasing support for gun regularization policies), and ``{\it in this backdrop, conservatives may express a lower level of feeling in control}.'' Her hypothesis of the difference in \fair came from her understanding of the central argument on both sides: liberals view the gun regulation as a justified means to fairly guard the public safety ({\it fair}), whereas conservatives view the restriction on gun ownership and rights as putting people in danger ({\it unjust}). She was curious about how her hypothesized differences may reflect in the language used in tweets.

\paragraph{\bf Glance over the group patterns (T1), focus on specific attributes and nuance subgroup patterns (T3)}
Her attention was first drawn to \grouptrend (T1), ``{\it so nice…you can see the overall patterns for the two major group of tweets only at a few glances}.'' She further used the \scopecontroller to select the two focal dimensions (by unchecking others). After looking closely, she confirmed that the differences between the two camps were aligned with her initial hypotheses, and meanwhile, she noticed that the distinction in \fair seems to be greater than that in \domi. Observing this, she was now interested in adding more attributes in \grouptrend to examine whether other dimensions may have better distinguishing power than the two she originally focused on, ``{\it [this makes it] easier to inspect which [additional] dimensions could be more useful in differentiating the groups}.'' She noticed that the \blue tweets tend to cluster more closely around higher values in most dimensions, whereas the \red lines spread wider in all the dimensions, which suggests that some tweets from \red camps might be similar to those from \blue. ``{\it [This shows] a more complex picture [of the \red camp]}.'' Observing this, she concluded that the impressions based on the overall patterns may be too overgeneralized. To examine the complexity, she praised \variscope--subgroup for not displaying a simple, dichotomous picture of the two camps but capturing the varying patterns across subgroups, ranging from the most \blue-dominant group, to in-between purple ones, and to the most \red-dominant group.

\paragraph{\bf Establish test validity with language cues, generate a new hypothesis (T5) and help mitigate overgeneralization on group characterization} 
To examine the patterns beyond the dichotomy and to test her hypotheses, she decided to pick subgroups with distinct colors and compared their languages by the \languagescope. ``{\it [It is] so convenient [that it allows for] a quick check on the language sequences from the subgroups.}'' She mentioned that it was usually a complicated and even a tedious process to check the test validity from the natural language signals, and ``{\it a system like yours really facilitates people to navigate more qualitative, complicated messages beyond numbers.}'' From the \languagescope, she found several tweets supported her original hypotheses. For example (as shown in Fig. \ref{fig:expert-interview-2}), texts from \blue-dominant groups (Fig. \ref{fig:expert-interview-2}a-c) mentioning \tweet{to demand vote,} \tweet{join filibuster,} and \tweet{shout with on voice,} expressing a strong \fair tone about righteousness in advocating legal means to make a change, and texts from \red tweets (Fig. \ref{fig:expert-interview-2}d-e) like \tweet{failure of strict gun laws} and \tweet{laws not affect criminals} expressing a low level of \fair (unfair or unjust) tone. After looking at the language cues more closely, she pointed out that the languages within the \red-dominant subgroup were less coherent and direct, which matched the previous observations that the distribution of the scores varied more widely among \red tweets. For example, one \red tweet was actually in favor of gun regulation, ``\textit{vote on gun violence prevention legislation.}'' More, after reviewing more closely to the \red tweets at the lower end, she concluded, ``{\it this gives [new] insights too! ... makes me think of a new hypothesis that for \textit{Dominance}, at the lower end, the languages used in \red tweets may be less coherent. They shared less common narratives.}'' Our design -- which offers non-dichotomous exploration, together with the chance to inspect the language patterns and their variations -- enables her to engage in the kind of sense-making regarding within-group variations against over-generalized conclusions.


%may not have shared narratives in terms of gun issues.}''
\begin{figure}[!t]
    \vspace{-2em}
    \setlength\tabcolsep{2pt} % default value: 6pt
    \begin{tabular}{c}\\
    \includegraphics[width=0.95\linewidth]{figures/expert-interview-3.pdf}
    \end{tabular}  
    \vspace{-1em}
    \caption{\label{fig:expert-interview-3}
    \textbf{Expert interview 3}: Four edge cases from the subgroup 5 identified through the dual-sided histogram chart in \evalscope. The user selected a set of instances to conduct contrastive analysis: (a) comparing true cases (true \blue vs. true \red), and (b) comparing positive cases (true \red vs. false \blue).
    }
    \vspace{-1.6em}
\end{figure}

\subsection{Interview 3: Interpretable machine learning for discovering common ground and edge cases}

Expert 3 is a natural language processing (NLP) researcher who wishes to %understanding how interpretable ML helps with reasoning about the classification results behind the groups' psycholinguistic differences. 
better understanding the relationship between the interpretable ML's predictions and the groups' psycholinguistic differences.  
While walking through the system, she was particularly interested in examining the subgroup 5, which is a borderline subgroup, having roughly similar portions of members from both camps. She hoped this subgroup might reveal ``{\it what is the psycholinguistic common ground between the two sides?}'' She found \evalscope useful as ``{\it [it] provides an overview of the predictive quality and lets [her] inspect the false and true predictions more closely.}'' She noticed that the subgroup 5 had more edge cases (more false and true predictions with the posterior probabilities close to 0.5), which she thought would bring the interesting finding in understanding the classifier.

\paragraph{\bf Identify edge cases (T2) and examine the inference variability (T3)}
From \grouptrend, she found this particular subgroup had many \blue and \red lines in the middle-ranged values across all dimensions (Fig. \ref{fig:expert-interview-3}). She used \evalscope to find four sets of edge cases and clicked to select each set to see the attribute values across different sociolinguistic dimensions. She found that, when comparing the two true cases (true \blue and true \red on the right), the system gave correct prediction because the differences between the two edge cases, while small, were consistent with the major differences between the two camps -- in terms of \domi and \fair (Fig. \ref{fig:expert-interview-3}a-i and a-ii). When comparing with the two positive cases (false \blue and true \red at the bottom), she found the two sets of instances had similar values of \vale and \domi, but the overall \grouptrend showed that the highlight \blue lines had bigger variance in the two dimensions (Fig. \ref{fig:expert-interview-3}b-i). The lack of coherence in these dimensions from the \blue camp ``{\it would make it trickier for the classifier to do correct prediction.}'' She elaborated that the common ground would be likely to appear from the true edge cases rather than from the incorrect prediction resulted from the noisier basis on either side, and considered the system's ability to tell apart the edge cases very valuable. 

\paragraph{\bf Retrieve similar language patterns (T3) and compare the prediction rationales (T6)}
Noticing some tweets in the two camps had similar sociolinguistic attribute values, she was curious about how the system would explain the differences. She picked two tweets (that have very similar attribute values) and used \rationalescope to check why one tweet was classified as \blue and the other as \red (by selecting the o-mode in the contrastive explanation, as shown in Fig. \ref{fig:teaser}f). She was satisfied when the system returned a rationale indicating \fair as the most discriminative feature for the predictions. She concluded that such level of interpretability that {\it directly links the plain text to sociolinguistic features to group prediction} could be useful to help to determine whether the results were from machine behaviors or human behaviors. 

While using the system, she felt that system helped her gain a better understanding of the interactions between the data and the underlying ML model. She commented that ``{\it the suite of tools allowed [her] to both keep a global view about the data while drilling down to the more interesting subgroups.}'' She was particularly positive about \rationalescope for its contrastive explanation: ``{\it it is useful to be given not just the most discriminative feature but also two contrastive samples; even if I might not personally agree with a particular characterization (say if I don't think this tweet strongly expresses \fair) I at least get a sense for the range that the system is operating under by comparing it against the contrastive tweet.}'' As a suggestion for further development, she anticipate the future system might allow users to construct more directed queries beyond semantic similarity so that a user might dynamically create new data subsets and test new hypotheses.\\

\textbf{Mitigate the overgeneralization on group characteristics (T3-T6).} After exploring functions of \name, she compared it with other relevant tools she had experience with, such as the machine learning tool Weka \cite{witten2016weka}, or the visualization NLPReViz~\cite{trivedi2018nlpreviz} designed for similar purpose. She appreciated the ``analytic engagement'' in the current design -- not simply offering a model concluding what properties may be attributed to a group, but also analytical tools for
users to engage conversations with ``\textit{machine learned classifiers that do over-generalize by its nature and the predictions may not be always correct in general.}'' She explained, ``\textit{I feel that, in the typical group analysis, the burden is on the user’s side. It is usually user’s role to make sure not to jump to conclusion}.'' But, in \name, ``\textit{the overall framework gives the analysts more tools to visualize and study those predictions.}'' She elaborated on the point in details mentioning her experience that, ``{\it Some tools available out there like Weka, for example, offers statistics such as contingency table or uncertainty estimates, which is in the high-level but conveys just one-sided explanations. It does not help break down the reasoning behind it or back up evidences enough to provide the details on identified group characteristics.}'' She particularly highlighted our tool's capability to provide a variety of mechanisms and opportunities to inspect across subgroups at multiple levels of granularity including actual tweet instances and language patterns, so that ``\textit{the analysts could investigate the prediction outcome, the model's underlying rationales, and make informed judgment about whether the hypothesis holds.}'' She added that many tools such as NLPReViz only offered two views, either globally or at individual instances. Finally, in terms of what particular users may benefit from our design, she pointed out,  ``\textit{the contrasting explanation greatly helped non-content experts [who lack prior knowledge of the sociopsychological dimensions] ... it helped them make sense of why the system predicted certain labels}.''
