\section{Appendix}

\subsection{Psycholinguistic Attributes}\label{sec:attr-def}

Drawing upon literature~\cite{graham2009liberals, shepherd2018guns,mendez2017neurology}, we identify seven most relevant sociolinguistic attributes that could potentially predict how two ideological groups talk differently on the gun issues.

Two {\it affect} dimensions:
\begin{itemize}
\item \vale: emotions can range from positive (e.g., pleasant, happy, hopeful) to negative (e.g., unhappy, annoyed, despairing)
\item \domi: emotions can range from the most dominant (e.g., feeling-in-control, influential, autonomous) to the least dominant (e.g, weak, submissive, and guided)
\end{itemize}

Five {\it moral} foundations:
\begin{itemize}
\item \care: the virtue of caring, nurturing, and protecting the vulnerable
\item \fair: the virtue of reciprocal altruism, including justice, rights, and welfare
\item \auth: the virtue of respect for authority
\item \loya: the virtue of being loyal to your identified groups
\item \puri: the virtue of seeing the human bodies as holly temples that should not be contaminated
\end{itemize}

\subsection{Human Annotated Attribute Values}\label{sec:annotation}

The human annotation included two phases: (1) creating reliable coding rules, and (2) coding. In the first phase, a major task is to the create the inclusion criteria for human annotators to identify the language signals that correspond to the theorized attributes in tweets. To do so, we sampled a subset of tweets (10-40\%) for each attribute from the total of 3100 relevant tweets. Through an iterative process, one of our authors who is in the field of social psychology began with open-coding to evaluate how the theoretical constructs and categories can be manifested in tweets' language use. She identified the discourse features and themes, and then built, tested, and refining the rules with a graduate research assistant. The inclusion criteria were created with 100\% agreement between the two criteria developers. Once the criteria were set up, each of these tweets was then coded by two independent annotators by a group of four research assistants who did not participate in the the criteria development stage but were trained to follow the coding schemes. These research assistants were chosen because they had been trained prior to this project and developed skills to analyze social media discussions that involve complex politics and social contexts. For each tweet, the annotators determined whether the tweet texts involved each of the seven attributes as a set of binary outcomes. The coding in this phase resulted in fair to substantial agreements between the annotators, with inter-rater reliability in terms of the Cohen's kappa ranging from 0.32 to 0.88 across all attributes. Any disagreement was reconciled after discussion and the coding criteria and procedure were formulated through the process. In the second phase, every tweet (from the 3100 relevant set) was annotated. For {\it moral} attribute values (e.g., \fair, \auth), we followed the coding schemes developed from the first phase. The annotation generated categorical values for each of the moral attribute. For {\it affect} (e.g., \vale), we determined to adopt the Best-Worse Scaling used by Mohammad et al.~\cite{mohammad2018obtaining} after testing it in the first phase. This annotation scheme employed comparative annotation method, which can be used to generate continuous rating for an attribute. We adopted this method and implemented the coding through crowdsourcing on Amazon Mturk. In the crowdsourcing annotation, three annotations are required for each of the $2N$ tweet-tuples (where each tuple contains 4 randomly-grouped tweets, and $N=3100$ in our case) in order generate reliable annotation results. Finally, the annotated scores for affect attributes are normalized to range from -1 to 1.

%The human annotation of the present sociolinguistic attributes involved: (1) an open coding phase, and (2) a categorical coding phase. First, the open coding phase is to generate the coding criteria and procedure for coding a tweet on each attribute based on the theoretical definitions in literature \cite{XX}. To do so, we sampled a subset of tweets (10-40\%) from the total of 3100 relevant tweets. Each of these tweets was coded by two independent annotators who were familiar with the theoretical definitions. For each tweet, the annotators determined whether the tweet texts involved each of the seven attributes as a set of binary outcomes. The coding in this phase resulted in fair to substantial agreements between the coders, with inter-rater reliability in terms of the Cohen's kappa ranging from 0.32 to 0.88 across all attributes. Any disagreement was reconciled after discussion and the coding criteria and procedure were formulated through the process. In the second phase, every tweet (from the 3100 relevant set) was annotated. For {\it moral} attribute values (e.g., \fair, \auth), we followed the coding schemes developed from the first phase. The annotation generated categorical values for each of the moral attribute. For {\it affect} (e.g., \vale), we determined to adopt the Best-Worse Scaling used by Mohammad et al.~\cite{mohammad2018obtaining} after testing it in the first phase. This annotation scheme employed comparative annotation method, which can be used to generate continuous rating for an attribute. We adopted this method and implemented the coding through crowdsourcing on Amazon Mturk. In the crowdsourcing annotation, three annotations are required for each of the $2N$ tweet-tuples (where each tuple contains 4 randomly-grouped tweets, and $N=3100$ in our case) in order generate reliable annotation results. Finally, the annotated scores for affect attributes are normalized to range from -1 to 1.
%\yrl{WT: please check and fix issues; also could you provide a short/brief layman description about each attribute in the bullet points below?}

%\begin{itemize}
%\item \vale:
%\item \domi:
%\item \care:
%\item \fair:
%\item \puri:
%\item \auth:
%\item \loya:
%\end{itemize}

% \wtc{The annotation involved two phases. The first was to develop the operational definitions and coding schemes of all the attributes. A subset of tweets (10-40\% of the total of 3100 tweets) were randomly sampled, used to generate operational definitions, and each tweet was coded by two independent annotators for obtaining reliable results. For each tweet, the annotators determined whether the tweet texts involved the seven attributes, yes or no, as a binary code.  Reliability testing indicated fair to substantial agreements, with the values of Cohen's kappa ranging from .32 to .88. In the second phase, every tweet was annotated. For moral values, we followed the coding schemes developed in phase 1. For affect, we further adapt the Best-Worse Scaling ~\cite{mohammad2018obtaining}, an annotation scheme that employs comparative annotation, which can generate continuous values instead of binary codes, for each dimension, that allows the direct comparisons among tweets. Prior empirical studies  ~\cite{mohammad2018obtaining} have developed a reliable procedure to annotate emotions from words, which suggested to use three annotations to annotate 2N 4-randomly-grouped-tweets (N is the numbers of data item; in our study, the total number of tweets, 3100) is sufficient to have reliable scores. We followed the recommendations and implemented through crowd-sourcing on Mturk. The scores are normalized so range from -1 to 1.}. 


\subsection{Evaluation for the multi-task prediction}\label{sec:holdout}

\input{tables/tab_clf_perf}


We evaluate the multi-task prediction models using a hold-out experiment on the 3100 relevant tweets, where 50\% samples are used for training, 15\% samples for validating, and the remaining samples for testing. 
We consider two types of baseline models: (a) group prediction baseline 1: to predict group labels with all the annotated attributes on sample tweets using standard machine learning method; (b) group prediction baseline 2: to predict group labels with tweet texts on sample tweets using single-task neural network architecture; and (c) attribute prediction baseline: to predict a single attribute value with tweet texts on sample tweets using single-task neural network architecture. 
The group prediction task is evaluated using {\it accuracy} as our dataset is balanced. 
The attribute prediction tasks are evaluated in terms of the {\it Pearson correlation coefficient} for continuous attributes, and by {\it accuracy} for categorical attributes. 
Table~\ref{tb:pred_performance} report performances of all models and baselines.
The performance gain (or loss) of multi-task models compared to the baselines are reported in the parentheses. 
We highlight key observations from the results: (1) For group prediction, our best model achieves accuracy 0.814, outperforming the two group prediction baselines by up to 30\%. (2) For attribute prediction, the performance measures of our models range 0.768--0.804 in terms of \textit{Pearson} correlation (for the two continuous attributes) and 0.623--0.975 in terms of accuracy (for the five categorical attributes), which is very close to the attribute baseline (with only 0.3\% differences on average). 
Such results suggest that our multi-task models can significantly improve group prediction without sacrificing the performance for attribute prediction.
