\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/multi-model.pdf}
    \caption{
    The neural network architecture for generating language cues. An input of word sequence in a tweet is transformed to word embedding and encoded by a Bi-LSTM layer, with predictive weights learned through an attention mechanism. Weighted latent vectors are taken into the dense layers to jointly predict the group and attribute value. Informative language cues are generated from a function of the learned attention weights. 
    }
    \label{fig:multi-model}
\end{figure}