\documentclass[journal]{IEEEtran}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amstext}
\usepackage{color,xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[switch]{lineno}
% \usepackage{url} 
% \usepackage{footnote} 
%
\usepackage{xcolor}
\definecolor{citecolor}{HTML}{0071bc}
% \usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=citecolor, bookmarks=false]{hyperref}
\usepackage[colorlinks, linkcolor=red,  anchorcolor=blue, citecolor=citecolor]{hyperref} 
\usepackage{cite}
\usepackage{xcolor}
\definecolor{SeaGreen4}{RGB}{0,205,102} 
\definecolor{SlateBlue}{RGB}{106,90,205} 
\definecolor{DarkRed}{RGB}{178,34,34} 



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\raggedbottom


\begin{document}
%\linenumbers
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
%\title{Tiny Object Tracking with Multilevel Knowledge Distillation Network and A Large-scale Benchmark}
%\title{Absorption Transformer Learning for RGBT Tracking}
\title{RGBT Tracking via Progressive Fusion Transformer with Dynamically Guided Learning}

% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs

\author{Yabin Zhu, Chenglong Li, Xiao Wang, \emph{Member, IEEE}, Jin Tang, Zhixiang Huang, \emph{Senior Member, IEEE}
%~~Liang Wang,~\IEEEmembership{Fellow,~IEEE}
%        John~Doe,~\IEEEmembership{Fellow,~OSA,}
%        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}
\thanks{Y. Zhu and Z. Huang are with Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, Key Laboratory of Electromagnetic Environmental Sensing of Anhui Higher Education Institutes, School of Electronic and Information Engineering, Anhui University, Hefei 230601, China. (email: zhuyabin0726@foxmail.com, zxhuang@ahu.edu.cn)

C. Li is with Information Materials and Intelligent Sensing Laboratory  of Anhui Province, Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Artificial Intelligence, Anhui University, Hefei 230601, China. (email: lcl1314@foxmail.com)

X. Wang and J. Tang are with Information Materials and Intelligent Sensing Laboratory  of Anhui Province, Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei 230601, China. (email: xiaowang@ahu.edu.cn; tangjin@ahu.edu.cn)}
}



% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
\markboth{IEEE Transactions manuscript, March~2023}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Existing Transformer-based RGBT tracking methods either use cross-attention to fuse the two modalities, or use self-attention and cross-attention to model both modality-specific and modality-sharing information. However, the significant appearance gap between modalities limits the feature representation ability of certain modalities during the fusion process. To address this problem, we propose a novel Progressive Fusion Transformer called ProFormer, which progressively integrates single-modality information into the multimodal representation for robust RGBT tracking. In particular, ProFormer first uses a self-attention module to collaboratively extract the multimodal representation, and then uses two cross-attention modules to interact it with the features of the dual modalities respectively. In this way, the modality-specific information can well be activated in the multimodal representation. Finally, a feed-forward network is used to fuse two interacted multimodal representations for the further enhancement of the final multimodal representation. 
In addition, existing learning methods of RGBT trackers either fuse multimodal features into one for final classification, or exploit the relationship between unimodal branches and fused branch through a competitive learning strategy. However, they either ignore the learning of single-modality branches or result in one branch failing to be well optimized. To solve these problems, we propose a dynamically guided learning algorithm that adaptively uses well-performing branches to guide the learning of other branches, for enhancing the representation ability of each branch. Extensive experiments demonstrate that our proposed ProFormer sets a new state-of-the-art performance on RGBT210, RGBT234, LasHeR, and VTUAV datasets. In particular, the improvements of ProFormer on the LasHeR dataset are up to $+6.2\%/+4.1\%$ in PR/SR compared with existing best RGBT tracker. 
\end{abstract}

\begin{comment}
\begin{abstract}
The fundamental reason why RGBT trackers perform better than single-modal trackers is that RGBT data provides rich multimodal information. Existing work attempts to model rich multimodal information through early, mid, and late feature fusion strategies as well as modal-weighted feature fusion, but there are three adverse effects due to factors such as noise and modality differences. 1) for fusion models that directly use addition or concatenation, they are easily affected negatively by poorer quality modalities; 2) for adaptive weighted fusion models, it is difficult to effectively learn the weight of each modality; 3) existing RGBT trackers typically use convolution to extract local features for modality fusion, resulting in a lack of global representation ability. These reasons cause each single-modal information in the fused feature to be more or less submerged, leading to a decrease in tracker performance. Therefore, how to model and reactivate multimodal information in the fused feature becomes crucial. To address these issues, we propose a novel Multi-Modal Feature Activation Network (MMFAN) for RGBT tracking. This network can address the above problems in two ways: 1) we propose a Transformer-based feature activation module to effectively model and activate multimodal information in the fused feature. Specifically, we use two encoders and a self-attention network to encode the specific features of each modality and the global contextual information of the fused modality, respectively. Then, we use two cross-attention modules to interact the fused feature and single-modal features to reactivate each modality information in the fused feature. Finally, we use a feed-forward network to effectively aggregate two fused features containing RGB and Thermal information to obtain the final feature representation. 2) To further integrate single-modal information into the fusion branch, we design a cooperative learning algorithm that can guide the fusion branch and single-modal branch to mutually learn from each other, thereby enhancing the representation ability of the single-modal branch and fusion branch. Extensive experiments demonstrate that our proposed method sets a new state-of-the-art performance on RGBT210, RGBT234, LasHeR and VTUAV datasets. In particular, compared with the best method, the improvements in PR/NPR/SR on the LasHeR dataset are up to $18.8\%, 19.2\%, 15.1\%$, respectively. 
%% 
Both the source code and pre-trained models will be released for free academic usage.    
\end{abstract}
\end{comment}
\begin{comment}

\begin{abstract}
  How to effectively fuse RGB and thermal infrared information is still an outstanding problem for RGBT tracking. In this paper, we propose a Transformer fusion-enhanced network for robust RGBT tracking. The proposed network addresses the fusion problem from two aspects. 1) We design a Transformer fusion-enhanced module, which aims to integrate modality-specific information as much as possible into the fused feature.  Specifically, we use two encoders and a self-attention network to encode modality-specific features and fused modality features, respectively. Then, we use two cross-attention modules to interact the fused features with modality-specific features to better preserve modality-specific information in fused features. Finally, we use a feed forward network to effectively aggregate two fused features that contain modality-specific information into a final feature representation. 
  2) To further integrate modality-specific information into the fusion branch, we design a collaborative learning algorithm that enables the fusion branch and modality-specific branch to mutually guide learning and enhance their representation abilities.  
Extensive experiments demonstrate that our proposed method sets a new state-of-the-art performance on RGBT210, RGBT234, LasHeR and VTUAV datasets. In particular, compared with the best method, the improvements in PR/NPR/SR on the LasHeR dataset are up to $18.8\%, 19.2\%, 15.1\%$, respectively. 
%% 
Both the source code and pre-trained models will be released for free academic usage. 
\end{abstract}
\end{comment}
\begin{comment}
   \begin{abstract}
%The fundamental reason why RGBT trackers outperform single-modality trackers is the rich multi-modal information provided by RGBT data. It is crucial to preserve modality-specific information in the fused features to fully exploit the rich information of multimodal data.
Retaining modality-specific information in fused features is crucial for mining rich information in multimodal data. %Moreover, the fundamental reason why RGBT trackers outperform unimodal trackers is that the amount of information provided by multiple modalities is far greater than that provided by a single modality. 
Previous RGBT tracking works attempt to model modality-specific information in fused features, but it is difficult to effectively preserve modality-specific information due to noise and other factors. To address this problem, we propose a modality-specific information preservation network for RGBT tracking. The network can address this issue in two ways:
1)We propose a Transformer fusion module to effectively integrate modality-specific information in fused features. Specifically, we use two encoders and a self-attention network to encode modality-specific features and fused modality features, respectively. Then, we use two cross-attention modules to interact the fused features with modality-specific features to better preserve modality-specific information in fused features. Finally, we use a feed forward network to effectively aggregate two fused features that contain modality-specific information into a final feature representation.
2)To further mine modality-specific information in fused features and enhance the representation ability of modality-specific and fusion branches, we design a collaborative learning algorithm that enables the fusion branch and modality-specific branch to mutually guide learning.
Extensive experiments demonstrate that our proposed method sets a new state-of-the-art performance on RGBT210, RGBT234, LasHeR and VTUAV datasets. In particular, compared with the best method, the improvements in PR/NPR/SR on the LasHeR dataset are up to $18.8\%, 19.2\%, 15.1\%$, respectively. 
%% 
Both the source code and pre-trained models will be released for free academic usage. 
\end{abstract} 
\end{comment}

\begin{comment}
\begin{abstract}
Transformer is successfully applied to visual tracking, but rarely explored in the filed of RGBT tracking. In this work, we propose a novel absorption Transformer learning framework termed AFormer, to solve three key problems in RGBT tracking. 
First, visible and thermal modalities have significant appearance differences, and the heterogeneous information would be lost if the cross-attention is directly applied to the fusion of two modalities.
To handle this problem, we design an absorption Transformer module to perform effective information absorption from the fused and modality-specific features and thus preserve modality-shared and modality-specific information as much as possible.
Second, the relations between the fused and modality-specific features are very important but ignored in the learning of discriminative multi-modal representations.
To this end, we design a novel collaborative learning algorithm to adaptively utilize the well-performing branch as a guidance to teach remaining branches. 
Finally, existing RGBT trackers usually use convolutions to extract local features for modality fusion, and thus lack global representation ability. Inheriting from the peculiarity of Transformer, our AFormer is able to generate the global contextual representation for robust RGBT tracking. 
Extensive experiments demonstrate that our proposed AFormer sets a new state-of-the-art performance on RGBT210, RGBT234, LasHeR and VTUAV datasets. In particular, compared with the best method, the improvements in PR/NPR/SR on the LasHeR dataset are up to $18.8\%, 19.2\%, 15.1\%$, respectively. 
%% 
Both the source code and pre-trained models will be released for free academic usage. 
%However, it has not able to achieve good performance improvement in RGBT tracking task.
\end{abstract} 
\end{comment}


% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
RGBT Tracking, Progressive Fusion Transformer, Dynamically Guided Learning.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle



\section{Introduction}\label{sec:intro}

\begin{figure}[htb]
\centering
% Requires \usepackage{graphicx}
\includegraphics[width=0.5\textwidth]{fig/Transformer-fusionv2} \\
\caption{Transformer fusion and learning structure. The $\rm{CL\_Loss}$, $\rm{Cls\_Loss}$, and $\rm{Reg\_Loss}$ denote the collaborative learning loss, classification loss, and regression loss respectively. Here, the Transformer layer is the modules of Transformer such as multi-head self-attention, multi-head cross-attention, feed-forward network, etc.}
\label{fig::structure}
\end{figure}

The aggregation of RGB and thermal cues enables visual trackers to achieve accurate and robust performance in challenging scenarios, such as \emph{illumination variation}, \emph{background clutter}, and \emph{bad weather}.
Therefore, RGBT tracking draws more and more attention in recent years in the visual tracking community. 
Boosted by deep learning techniques~\cite{he2016deep, vaswani2017attention} and large-scale benchmark datasets~\cite{li2018paralleleye, li2022features, lu2021rgbt, li2021lasher}, this research field is developing rapidly. 



%With this question in mind, we carefully review existing RGBT trackers to dissect and look for possible causes. 
%We find that previous researchers mainly exploit the concatenation, attention mechanism and dynamic filter prediction for RGBT tracking. 
%To be specific, some existing works \cite{zhu2020quality, zhang2019multi, zhang2020dsiammft} use simple addition and concatenation operations to directly fuse two modality features. There are also some works \cite{zhu2020quality, lu2022duality, tang2022temporal} that adopt attention mechanisms to learn the contribution of each modality for adaptive fusion of dual modalities. Dynamic convolution based RGBT trackers \cite{wang2021mfgnet, peng2021dynamic} design dynamic modality-aware filter generation network to boost message communication between RGB and thermal data by adaptively adjusting convolutional kernels. In addition, some works \cite{tang2022temporal, zhang2022visible} explore several fusion strategies (including pixel-level, feature-level and decision-level) to boost tracking performance. 
% In order to further mine the features of two modalities, \cite{zhang2020object} and  \cite{zhu2021rgbt} try to use a multi-branches classification network to integrate modal heterogeneous information. 
%These works might be limited by the following issues: 
%1) They might obtain sub-optimal results
%since only local features are well-learned by convolution and/or attention schemes.
%2) They cannot effectively employ the relations between the fused and separated features, which is very important for the learning of discriminative multi-modal representations.
Some existing works \cite{zhang2019multi, zhang2020dsiammft} use simple addition and concatenation operations to directly fuse features from two modalities. There are also some works \cite{zhu2020quality, lu2022duality, tang2022temporal} that adopt attention mechanisms to learn the contribution of each modality for adaptive fusion of dual modalities. Dynamic convolution-based RGBT trackers \cite{wang2021mfgnet, peng2021dynamic} design dynamic modality-aware filter generation network to boost message communication between RGB and thermal data by adaptively adjusting convolutional kernels. In addition, some works \cite{tang2022temporal, zhang2022visible, yang2021rgbtcmmp} explore several fusion strategies (including pixel-level, feature-level and decision-level) to boost tracking performance. However, these methods lack global context modeling capability, which leads to limited performance of RGBT tracking.





\begin{figure}
\centering
% Requires \usepackage{graphicx}
\includegraphics[width=0.49\textwidth]{fig/CA-our-feature} \\
\caption{Comparison of the feature maps between the naive Cross-Attention based fusion ($3^{rd}$ column) and our proposed ProFormer ($4^{th}$ column).}\label{fig::CA-our-feature}
\end{figure}


In recent years, Transformer has achieved remarkable success in the field of computer vision and is introduced into RGBT tracking. %Feng \emph{et al.} \cite{feng2022learning} and Hou \emph{et al.} \cite{hou2022mirnet} 
Some methods \cite{feng2022learning, hou2022mirnet} directly use two or more cross-attention networks to fuse the features of the two modalities. Mei \emph{et al.} \cite{mei2023differential} try to model the specific and shared information of modalities by using self-attention and cross-attention networks. Xiao \emph{et al.} \cite{APFNet} propose an attribute-based progressive fusion network, which can enhance the modality-specific information in the fused attribute features. 


Although the above methods achieve some performance improvement, they are not as brilliant as Transformers applied to other visual fields. The main reason is that the significant differences (i.e., the inherent difference between the dual modalities and the difference caused by the quality of different modalities) between modalities suppress the feature representation of modalities during the fusion process. Specifically, the differences between various modalities have damaged existing Transformer-based tracking due to the following reasons:  
1) We find that the heterogeneous information would be lost if the cross-attention is directly applied to the fusion of two modalities. As shown in Fig.~\ref{fig::CA-our-feature}, when the two modalities have a significant difference, the widely used cross-attention scheme fails to fuse the dual features well. This may be caused by the fact that cross-attention calculates the similarity between two modalities and tends to extract shared information. The experimental results in Table~\ref{tb::TFS} also validated this assumption. 
2) The features of two modalities are difficult to be represented collaboratively, which makes the modal complementary information cannot be fully exploited. In particular, the modality-specific and modality-sharing information explicitly modeled by the above approach is not integrated into an effective multimodal representation.

%2)The feed-forward network of Transformer is removed or unreasonably applied resulting in two modalities not being modeled well together in a Transformer module, which makes the modal complementary information cannot be fully exploited.

%Although Li \emph{et al.} \cite{APFNet} tries to model global contextual information by using Transformer, which can enhance the modality-specific information in the fused features, but it does not effectively integrate the complementary information between dual modalities. 


%The vast majority of RGBT tracking algorithms \cite{zhu2020quality, zhang2019multi, zhang2020dsiammft, zhu2020quality, lu2022duality, tang2022temporal,wang2021mfgnet, peng2021dynamic, tang2022temporal,zhang2022visible} are based on local features, which lack the global contextual information essential for robust RGBT tracking.
%but it is difficult for these methods to simultaneously take into account the problems of modality-specific information modeling, inter-modal complementary information mining, and inter-modal domain differences elimination.
%For example, as shown in Figure \ref{fig::structure}(a, b), \cite{sun2019videobert,lin2020interbert} directly fuse multimodal information and feed Transformer layer, which may ignore modality-specific information. In \cite{li2021ai},
%although modality-specific global information is modeled, there is no explicit interaction between the two modalities, and the interaction between modalities is very important to eliminate inter-domain differences, shown in Figure \ref{fig::structure}(c). Another example is in Figure \ref{fig::structure}(d, e), \cite{lu2019vilbert} and \cite{hasan2021humor} directly interact the two modalities by using cross-attention, which will cause the network to focus more on the shared information (similar part) between the modalities and ignore heterogeneous information.




In order to solve the aforementioned problems, we propose a novel progressive fusion Transformer framework that progressively integrates single-modality information into fused features for robust RGBT tracking. Specifically, ProFormer is divided into three fusion stages: multimodal self-attention fusion, modality-specific cross-attention fusion and multimodal enhancement fusion. These stages can gradually integrate the rich features of the two modalities into the fused branch.
Firstly, ProFormer uses a self-attention module to collaboratively extract the multimodal representation. Then, two cross-attention modules are used to interact fused features in the previous stage and the features of the dual modalities, respectively. In this way, the modality-specific information can well be activated in the multimodal representation. Finally, a feed-forward network is used to fuse two interacted multimodal representations for further enhancement of the final multimodal representation. It is worth noting that all three stages are modeling the information of the two modalities collaboratively, so as to solve the problem that existing Transformer-based RGBT tracker cannot represent both modalities well in a collaborative manner. In short, ProFormer can both take good care of activating modality-specific in the fused features and represent the information of dual modalities collaboratively.


 
%we design a simple but highly effective progressive fusion Transformer module, which well solves the problem of modal information suppression caused by modal differences in the fusion process. This progressive fusion Transformer module first uses a self-attention module to collaboratively extract the features of the fused branch. Then, we use two cross-attention modules to interact the fused features with the features of the dual modalities modeled by the encoder, which can activate the modality-specific information in the fused features. Finally, a feed-forward network is used to co-model two fused features containing mode-specific information as the final feature representation. 
%This effectively solves the fusion problem caused by modal differences. The specific benefits are as follows： 1) It effectively avoids the problem of over-focusing on modal shared information ignoring modal specific information caused by the direct use of cross attention interaction between two modalities. 2)All three stages are modeling the information of the two modalities collaboratively, which can well integrate the complementary information between the modalities into the fusion features.



%The encoder mainly consists of multi-head attention module and feed forward network. The decoder contains two cross-attention modules, which can be used to integrate the modality-specific information in the fused features and make use of the rich multimodal information as much as possible. After cross-attention operations, a feed forward network is used to aggregate the modality-specific information in the fused features for the final feature representation. 
%In order to further mine the complementary information of two modalities, Zhang \emph{et al.} \cite{zhang2020object} and Zhu \emph{et al.} \cite{zhu2021rgbt} try to use a multi-branch classification network to integrate modal heterogeneous information. 
How to use the relations of the fused and modality-specific branches is also a key point of RGBT tracking. Most of the existing RGBT tracking methods \cite{kristan2019seventh, lu2022duality, lu2021rgbt, zhang2021learning, zhu2020quality, feng2022learning} directly fuse the features of two modalities for the final classification, but these methods may ignore the learning of single modality branches. Some methods \cite{tang2022exploring, feng2020learning} directly use the decision-level fusion strategy to weight the classification scores of the two modalities adaptively, however, they ignore the collaborative learning between modalities. 
 Zhang \emph{et al.} \cite{zhang2020object} try to use the relationship between unimodal branches and fused branch by a competitive learning strategy. However, the competitive learning strategy may cause a branch to fail in the competition and prevent it from being well-optimized. To handle these problems, we design a dynamically guided learning algorithm to adaptively utilize the better-performing branch as guidance for teaching other branches. 
In the training phase, we retain both the classification heads of the fused branch and the modality-specific branch, and then determine which branch performs better by comparing their classification losses. Finally, we use the dynamically guided learning loss to make that the better-performing branch (i.e., small classification loss) guides the learning of other poorly performing branches (i.e., larger classification losses). Such a dynamically guided learning algorithm can effectively improve each branch’s representation ability and thus deliver robust RGBT tracking with powerful multimodal representations. 


Our contributions can be summarized as follows:

\begin{itemize}
    \item We propose an effective multimodal fusion framework based on a novel progressive fusion Transformer called ProFormer, which progressively integrates single-modal representations into a robust multimodal representation. This framework handles the limited feature representation problem caused by the significant modality gap in RGBT tracking effectively.  
    
    \item We design a new dynamically guided learning algorithm, which adaptively utilizes the better-performing branch as guidance for teaching other branches, to well optimize both modality-fused and modality-specific branches. 
    
    \item Extensive results show that our proposed method achieves new state-of-the-art (SOTA) performance on four public RGBT tracking datasets, including RGBT210, RGBT234, LasHeR, and VTUAV. In particular, our method improves the PR/SR scores by $+6.2\%/+4.1\%$ over the SOTA methods on the testing set of LasHeR dataset.      
\end{itemize}


%Note that compared to unimodal branches that directly guide aggregation branch learning, this collaborative mutual learning approach can simultaneously enhance the representation capability of unimodal branches, and improving the representation capability of unimodal branches is more conducive to integrating the complementary information between to modalities into aggregation branches.
%-------------------------------------------------------------------------



\section{Related Work}

In this section, we give a brief introduction to RGBT Tracking and Multimodal fusion with Transformer. For more related works on these topics, please refer to the following survey papers~\cite{zhang2020multimodaltrackSurvey, han2022transformersurvey, wang2023MMPTMSurvey}.  

\subsection{RGBT Tracking} 
The fusion and representation of visible and thermal infrared are very important for robust RGBT tracking and are a current research hot spot in the field of tracking. Zhu \emph{et al.} \cite{zhu2019dense} propose a recursive aggregation method, which can fuse all complementary features of two modalities. Some works \cite{zhu2020quality, tang2022temporal} use an adaptive weighting fusion strategy to fuse the features of two modalities for more robust RGBT tracking. Tang \emph{et al.} \cite{tang2022temporal} explore several fusion strategies (including pixel-level, feature-level and decision-level), which attempt to further boost tracking performance. Zhang \emph{et al.} \cite{zhang2022visible} unifies various multimodal fusion strategies (including pixel-level, feature-level, and decision-level) into a hierarchical fusion framework.
Furthermore, some other methods \cite{zhang2020object, tang2022exploring, feng2020learning} use a multi-branch classification network to integrate the learning of modal heterogeneous information. 
However, these algorithms might obtain sub-optimal results since only local features are well-learned by convolution schemes.
 In recent years, Transformers-based RGBT tracking methods have become more and more popular and have achieved competitive performance.
Some works \cite{feng2022learning, hou2022mirnet} directly use two or more cross-attention module to fuse the features of dual modalities.
Mei \emph{et al.} \cite{mei2023differential} adopt self-attention and cross-attention module to model both specific and shared information from two modalities.
Xiao \emph{et al.} \cite{APFNet} try to enhance modality-specific information within fused attribute features using an attribute-based fusion network.
However, significant differences between modalities (i.e., the inherent difference between the dual modalities and the difference caused by the quality of different modalities) may limit feature representation of modalities during the fusion process.
Moreover, these methods are unable to effectively model relationships between fused and modality-specific branches for high-quality discriminative representation learning.


\subsection{Multimodal Fusion with Transformer } 
Transformer plays an important role in the field of multimodal learning because of its strong feature representation abilities and flexible network architecture~\cite{sun2019videobert, lin2020interbert, li2021ai, lu2019vilbert, hasan2021humor, wang2021visevent, tang2022coesot}. To be specific, as shown in Fig. \ref{fig::structure} (a), Sun \emph{et al.}~\cite{sun2019videobert} use early fusion to aggregate multimodal information which may ignore modal-specific information. By following this work, Lin \emph{et al.} propose a hierarchical attention method~\cite{lin2020interbert} which adopts an encoder to fuse two modalities and two encoders to decouple modality-specific information, as shown in Fig. \ref{fig::structure} (b). In Fig. \ref{fig::structure} (c), Li \emph{et al.}~\cite{li2021ai} first encode modal-specific information separately and then directly fuse modal feature information with an encoder. However, no explicit interactions between the two modalities are considered which is actually very important to eliminate inter-domain differences. As shown in Fig. \ref{fig::structure} (d), some works~\cite{lu2019vilbert, hasan2021humor} directly interact two modalities with the cross-attention module, which may learn shared information well, but fail to capture modality-specific information.
%%%% 
In this paper, we propose novel progressive Transformer fusion framework, which progressively integrates single-modal representations into a robust multimodal representation. Moreover, we also design a novel dynamically guided learning algorithm to adaptively utilize well-performing branches as guidance for teaching remaining branches.
 


% This method is difficult to obtain modality-specific information only through implicit coding decoupling and will ignore the complementary information between modalities. 
% However, there is no explicit interaction between the two modalities, and the interaction between modalities is very important to eliminate inter-domain differences. 

% directly interact two modalities with cross attention module, 

% which may make the network more inclined to learn the shared information between the modalities and ignore 








\section{Methodology} 
In this section, we will first give an overview of our proposed ProFormer. Then, we will dive into the details of each module, including the representations of dual inputs, the network architecture, and the optimization strategy. Finally, we introduce the detailed tracking procedure by incorporating our newly proposed ProFormer for high-performance RGBT tracking. 

\begin{figure*}[htb]
\centering
\includegraphics[width=\textwidth]{fig/TFFCL_new} 
\caption{An illustration of our proposed progressive fusion Transformer framework for RGBT Tracking. The ResNet50 is adopted as the backbone network for feature embedding of RGB and thermal input, respectively. Then, the proposed progressive fusion Transformer module is used to fuse the features of two modalities. 
A dynamically guided learning algorithm is proposed to help optimize the whole network in a more discriminative way.} 
\label{fig::framework}
\end{figure*} 


\subsection{Overview} 
As shown in Fig.~\ref{fig::framework}, our proposed ProFormer mainly contains the Feature Extraction module, progressive fusion Transformer module, and Tracking Head. To be specific, the ResNet50~\cite{he2016deep} is adopted as the backbone network for both modalities. 
After the RGB and Thermal features are extracted, the proposed progressive fusion Transformer module is used to fuse them. 
This progressive fusion Transformer module is divided into three stages to gradually integrate the rich features of the two modalties into the fused branch. 
In the first stage, 
we add the features of two modalities into one unified feature representation and combine them with positional encoding as the token embeddings. The multi-head self-attention module is introduced to encode the added features for collaborative representation of features of two modalities. Meanwhile, we feed the RGB/Thermal features into the Trans-Encoder network for obtaining modality-specific global features. In the second stage, we use two cross-attention modules to interact the fused features with the features of the dual modalities to boost the modality-specific information in the fused features. Specifically, the fused features are treated as the query input, and the unimodal features are adopted as the key and value input of the Cross-Attention module.
In the third stage, two fused features containing modality-specific information are added and normalized and fed into feed-forward layers as the final feature representation. The RGB, thermal, and fused features will be fed into three classifiers in the training phase. In addition to the standard loss functions used for target object classification and regression, we also design a dynamically guided learning algorithm, which can adaptively use well-performing branches to guide the learning of other branches for improving the representation ability of each branch. 
%%%% 
By equipping our proposed framework into the template and search branch of baseline tracker ToMP~\cite{mayer2022transforming}, we can achieve high-performance RGBT tracking on multiple tracking benchmark datasets. 










\subsection{Progressive Fusion Transformer} 
In this work, we follow the Siamese framework for Visible-Thermal based tracking. Given the input visible and thermal image pairs $\{I_v, I_t\}$, we first crop and resize the template image $\{Z_v \in \mathbb{R}^{288\times288}, Z_t \in \mathbb{R}^{288\times288}\}$ and search image $\{X_v \in \mathbb{R}^{288\times288}, X_t \in \mathbb{R}^{288\times288}\}$ based on the initialized bounding box. The ResNet50 is adopted as the backbone network to extract their features and obtain $\{z_v \in \mathbb{R}^{18\times18}, z_t \in \mathbb{R}^{18\times18}, x_v \in \mathbb{R}^{18\times18}, x_t \in \mathbb{R}^{18\times18}\}$. 
 


As we all know, the previous CNN-based RGBT trackers usually focus on designing fusion modules to integrate the features of two modalities, such as modality attention-based fusion and dynamic convolution operations. However, the utilization of CNN layers only mines the local features well, therefore, sub-optimal tracking results can be obtained only. Transformers have strong global information capture capabilities and have been successfully used in many vision tasks~\cite{carion2020end, misra2021end, wang2021end, he2021transreid, meinhardt2022trackformer}. Although few, there are still some works based on Transformer networks to fuse bimodal information for tracking, however, limited tracking performance is achieved by these trackers due to the direct utilization of cross-attention or self-attention schemes. 


To address the issues caused by the significant differences between visible and thermal modalities (i.e., the inherent difference between the dual modalities and the difference caused by the quality of different modalities), in this work, we rethink the network designs for the Transformer based RGBT tracking. As shown in Fig.~\ref{fig::framework}, we propose a novel progressive fusion Transformer for RGBT tracking, termed ProFormer. It can be divided into three fusion stages, namely, multimodal self-attention fusion, modality-specific cross-attention fusion, and multimodal enhancement fusion, which gradually integrate the rich features of the two modalities into the fused branch. 

 




% However, significant differences (including feature difference and  modal quality difference) between modalities suppress the feature representation of certain modalities during the fusion process. 
% Specifically, the differences between different modalities have adverse effects on the existing Transformer-based tracking as follows:
% 1) The heterogeneous information would be lost if the cross-attention is directly applied to fuse the feature of two modalities. Heterogeneous information loss problem 
 
% 2) The features of two modalities are difficult to represent collaboratively, which makes the modal complementary information unable to be fully exploited. 


\begin{figure*}[htbp]
\centering
% Requires \usepackage{graphicx}
\includegraphics[width=1\textwidth]{fig/grad_cam} 
\caption{Visualization of features grad class activation maps for our method and baseline}\label{fig::fm}
\end{figure*}

\begin{figure*}[htbp]
\centering
% Requires \usepackage{graphicx}
\includegraphics[width=1\textwidth]{fig/fusion-process} 
\caption{Visualization of features grad class activation maps for our fusion process}\label{fig::fp}
\end{figure*}




\textbf{Multimodal Self-attention Fusion.} In the first stage, a fused branch is proposed to aggregate the two modalities by first adding them to form a unified representation ($x_f = x_v + x_t$) and then enhancing the fused features using the multi-head attention mechanism. 
Before feeding $x_f$ into the MHA module, the positional encoding is also added by following the ViT~\cite{dosovitskiy2020VIT}. Formally, we have: 
\begin{equation}
    \label{fusionMHA} 
    \bar{x}_f = LN(x_f + MHA(x_f)), 
\end{equation}
where the $LN$ denotes the $\mathrm{LayerNorm}$ operation.
%%%% 
Meanwhile, two encoders are introduced to learn unimodal global information ($\bar{x}_v, \bar{x}_t$).   

\textbf{Modality-specific Cross-attention Fusion.} In the second stage, to reactivate the mode-specific information submerged in the fused features, we introduce two cross-attention module to interact the fused features with the features of the dual modalities. More in detail, the enhanced multimodal features $\bar{x}_f$ are used as the query input $q$, and the unimodal features $\bar{x}_v$ or $\bar{x}_t$ enhanced using Trans-Encoder are treated as the key and value inputs $k, v$. For RGB branch,
\begin{align}
    \label{crossAttVIS}  
    &\hat{x}_v = CrossAttention(q, k, v) \\ 
    &~~~~~= CrossAttention(\bar{x}_f+p, \bar{x}_v+p, \bar{x}_v)
\end{align}
The $p$ is position encoding.
Similarly, we have the cross-attention operation on the thermal branch, i.e., 
\begin{align}
    \label{crossAttINF}  
    &\hat{x}_v = CrossAttention(q, k, v) \\ 
    &~~~~~= CrossAttention(\bar{x}_f+p, \bar{x}_t+p, \bar{x}_t)
\end{align} 
Introducing fused multimodal features into the cross-attention module brings us the following two benefits: 
1) avoiding direct interaction between RGB and Thermal features, thus preserving modality-specific information; 
2) enriching fused feature information by maintaining modality-specific information.
% Note that the features of the fusion branch will be directly used for online tracking, therefore the mining of the information richness of the fused features is crucial. 



\textbf{Multimodal Enhancement Fusion.} In the third stage, two fused features containing modality-specific information will be added and fed into the $\mathrm{LayerNorm}$, the feed-forward network consists of two Fully Connected ($FC$) layers, two $\mathrm{Dropout}$, and an activation function $\mathrm{ReLU}$. The output dimensions of the first and second FC layers are 2048 and 256, respectively. It is worth noting that the three fusion stages cooperate to model the complementary information of two modalities and effectively avoid the suppression of some modal information during the fusion process.

As shown in Fig.~\ref{fig::fm}, compared with the features of single modality and the directly fused features, the generated features of the progressive fusion Transformer are more discriminative under challenges such as similar appearance and low resolution. In addition, as shown in Fig.~\ref{fig::fp}, we also show some feature maps of the progressive fusion process. Due to the huge difference in modal quality, it is difficult to effectively aggregate useful features between modalities for the first stage of fusion. For example, in the first row of Fig.~\ref{fig::fp}, the thermal crossover effect causes the thermal infrared sensor to fail to successfully capture the target object, resulting in serious interference with this fused feature. But these problems can be effectively solved by our second stage (Cross-RGB Feature and Cross-T Feature) and third stage (Final Feature) of progressive fusion strategy.



% After obtaining two fused features containing modality-specific information, the feed-forward is used to encode the fused features. As with a normal Transformer, the residual operation and normalization function are used after each submodule. In this paper, the normalization function is . In our method, 
% The dropout rate is set to 0.1, 
% and 
% The activation function is . 


% The operation of feed forward network can be described as follows:
% \begin{equation}
% \begin{aligned}
% &\mathrm{FFN}(F_{\text {Agg}})=\mathrm{Drop}(\mathrm{FC}(\mathrm{Drop}(\mathrm{ReLU}(\mathrm{FC}\left(\mathrm{F}_{\text {Agg}}\right)))))\\
% \end{aligned}
% \end{equation}

% The main operations of Transformer feature fusion module can be described as follows,

% The encoder part,
% \begin{equation}
% \begin{aligned}
% &\mathrm{F}_{\text {V}}^{e}=\mathrm{Encoder}\left(\mathrm{F}_{\text {V}}\right)\\
% &\mathrm{F}_{\text {T}}^{e}=\mathrm{Encoder}\left(\mathrm{F}_{\text {T}}\right)
% \end{aligned}
% \end{equation}

% The decoder part,
% \begin{equation}
% \begin{aligned}
% &\mathrm{F}_{\text{Agg}}^{ve}=\mathrm{MHA}\left(\mathrm{F}_{\text {V}}\right)\\
% &\mathrm{F}_{\text {Agg}}^{te}=\mathrm{MHA}\left(\mathrm{F}_{\text {T}}\right)\\
% &\mathrm{F}_{\text {Agg}}=\mathrm{CA}(\mathrm{F}_{\text {Agg}}^{ve},\mathrm{F}_{\text {V}}^{e})+\mathrm{CA}(\mathrm{F}_{\text {Agg}}^{te},\mathrm{F}_{\text {T}}^{e})\\
% &\mathrm{F}_{\text {Agg}}=\mathrm{FFN}(F_{\text {Agg}})\\
% \end{aligned}
% \end{equation}
% %2)  It can integrate and retain the modality-specific information in the features of the fusion branch. T 
% where $\mathrm{MHA}$ indicates multi-head attention operation, $CA$ indicates cross attention operation.

%It is worth emphasizing that the effective exploitation of the rich information between multiple modalities is crucial for the improvement of tracking performance. Therefore, we follow a principle in designing the fusion module that extracts as much rich information between modalities as possible, even if there is information redundancy. 




\subsection{Dynamically Guided Learning Algorithm} 

How to use the relations of the fused and modality-specific is also a key point of RGBT tracking. Tang \emph{et al.}~\cite{tang2022exploring} and Feng \emph{et al.} \cite{feng2020learning} use directly adaptively weight the classification scores of the two modalities through decision-level fusion strategy, but they completely ignore the collaborative learning between modalities. Zhang \emph{et al.} \cite{zhang2020object} design a competitive learning strategy to model the relationship between unimodal branches and fused branch. However, the competitive learning strategy may cause a branch to fail in the competition and not be well optimized. Therefore, these methods are difficult for mining the complementary information of dual modalities. 


To this end, we design a dynamically guided learning algorithm, which can adaptively utilize the well-performing branch as a guide to teaching the remaining branches. Specifically, we retain both the classification heads of the fused branch and the unimodal branch in the training phase. The loss values of the fused and unimodal branch can be used as a signal to determine the teacher branch. Finally, we use dynamically guided learning loss to make the well-performing branch guide the learning of poorly performing branches. Let's take the fused branch learning guided by a visible branch as an example, the formula for related operations can be written as: 
\begin{equation}
\begin{aligned}
\mathrm{L}_{dgl\_{v}}=\frac{1}{\mathrm{~N}} \sum_{\mathrm{i}=1}^{\mathrm{N}}\mathrm{W}_{v}^{i}\left(\mathrm{S}_{f}^{i}-\mathrm{S}_{v}^{i}\right)^2
\end{aligned}
\end{equation}
where $\mathrm{L}_{dgl\_{v}}$ is the dynamically guided learning loss of the fused branch guided by the visible branch.
$\mathrm{S}_{f}^{i}$ and $\mathrm{S}_{v}^{i}$ represent the classification score of the fused branch and visible branch of the $i^{th}$ sample. $\mathrm{N}$ is the number of samples. The $\mathrm{W}_{v}^{i}$ is the adaptive selection weight.

\begin{equation}
\begin{aligned}
\mathrm{W}_{v}^{i}= \begin{cases}1, & \mathrm{L}_{cls-f}^{i}-\mathrm{L}_{cls-v}^{i}<0 \\ 0, & \mathrm{L}_{cls-f}^{i}-\mathrm{L}_{cls-v}^{i} \geq 0\end{cases}
\end{aligned}
\end{equation}
where $\mathrm{L}_{cls-f}^{i}$ and $\mathrm{L}_{cls-v}^{i}$ represent the classification loss of fused branch and visible branch of the $i^{th}$ sample. 
Our classification loss is exactly following DiMP~\cite{bhat2019learning}, and the specific classification loss function is as follows.
\begin{equation}
\mathrm{L}_{cls} = \sum_{(x, c) \in I_{\text {search}}}\left\|l\left(x * w, z_{c}\right)\right\|^{2},
\label{cls}
\end{equation}
where the label $z_{c}$ is set to a Gaussian function centered as the target $c$, $x$ and $w$ are the output feature and target model weight of network. $I_{search}$ is the training examples of search frame, and $l(s, z)$ is a hinge-like residual function. 



The total loss of the fused branch is described as follows,
\begin{equation}
\begin{aligned}
\mathrm{L}_{f}= \alpha \mathrm{L}_{cls-f} +\beta\mathrm{L}_{iou-f}+ \gamma(\mathrm{L}_{dgl\_{v}}+\mathrm{L}_{dgl\_{t}})
\end{aligned}
\label{eq:la}
\end{equation}
The $\mathrm{L}_{dgl\_{t}}$ is the dynamically guided learning loss of the fused branch guided by the thermal infrared branch. The $\mathrm{L}_{iou-f}$ represents the GIoU loss~\cite{rezatofighi2019generalized} of the fused branch. The $\alpha$, $\beta$, and $\gamma$ are set to 100, 1, and 100000 respectively.

Similarly, the total loss of the visible and thermal infrared branches can be described as follows: 
\begin{equation}
\begin{aligned}
&\mathrm{L}_{v}=  \alpha\mathrm{L}_{cls-v} +\beta\mathrm{L}_{iou-v}+ \gamma\mathrm{L}_{dgl\_{fv}},\\
&\mathrm{L}_{t}= \alpha\mathrm{L}_{cls-t} +\beta\mathrm{L}_{iou-t}+ \gamma\mathrm{L}_{dgl\_{ft}}.\\
\end{aligned}
\end{equation}
where $\mathrm{L}_{dgl\_{fv}}$ and $\mathrm{L}_{dgl\_{ft}}$ are the dynamically guided learning loss of the visible branch and thermal branch guided by the fused branch, respectively. Here, the settings of $\alpha$, $\beta$ and $\gamma$ are the same as in Equation \ref{eq:la}.
$\mathrm{L}_{iou-v}$ and $\mathrm{L}_{iou-t}$ are the GIoU loss~\cite{rezatofighi2019generalized} of the visible branch and thermal branch. Note that, we use an iterative training strategy to avoid mutual interference among branches, i.e., the gradients of other branches will be truncated when optimizing a particular branch. A similar training strategy is also used for the optimization of GAN network~\cite{goodfellow2020gan, wang2021ganTrack}. 



\begin{figure}
\centering
% Requires \usepackage{graphicx}
\includegraphics[width=0.48\textwidth]{fig/test-network} 
\caption{An overview of the tracking phase of our proposed ProFormer-based RGBT tracking.} \label{fig::test-network}
\end{figure}

  


\subsection{Online Tracking} 

In order to improve the efficiency of tracking, we only keep the fused branch and discard the single modal branches during the tracking phase.  As shown in Fig.~\ref{fig::test-network}, we first take the input visible and thermal image pairs ${I_v, I_t}$ of the initial frame, crop them with a region that is five times larger than the target bounding box, and resize them to ${288\times288}$ as the template image pair ${Z_v \in \mathbb{R}^{3\times288\times288}, Z_t \in \mathbb{R}^{3\times288\times288}}$. Similarly, we use the image of the next frame as the search frame, and obtain the image of the search region ${\bar{X}_v \in \mathbb{R}^{3\times288\times288}, \bar{X}_t \in \mathbb{R}^{3\times288\times288}}$ by cropping and scaling it. We adopt ResNet50 as the backbone network to extract their features and obtain $\{z_v \in \mathbb{R}^{1024\times18\times18}, z_t \in \mathbb{R}^{1024\times18\times18}, x_v \in \mathbb{R}^{1024\times18\times18}, x_t \in \mathbb{R}^{1024\times18\times18}\}$. To reduce the computational complexity of subsequent operations, we use a convolution of ${1\times1\times256}$ to lower the dimensions of these features. Then, we design a ProFormer module to fuse these features from two modalities for obtaining $z_f, x_f$. We use the embedding $e_{\mathrm{fg}} \in \mathbb{R}^{1\times 256}$ to represent target foreground, and a Gaussian $y \in \mathbb{R}^{18\times18\times1}$ centered at the target location is introduced. The target encoding function is defined as follows:
\begin{equation}
\psi\left(y, e_{\mathrm{fg}}\right)=y \cdot e_{\mathrm{fg}}
\end{equation}
where $\cdot$ is point-wise multiplication with broadcasting. Next, the $z_f$ and target encoding are combined as follows:
\begin{equation}
z_t = z_f + \psi\left(y, e_{\mathrm{fg}}\right)
\end{equation}
where $z_t \in \mathbb{R}^{256\times18\times18}$ is the features of template frame that contain the encoded target state information. Similarly, we combine the $x_f$ and test encoding $e_{\mathrm{s}}$ as follows:
\begin{equation}
x_s = x_f +\mu\left(e_{\mathrm{s}}\right),
\end{equation}
where $\mu(\cdot)$ denotes the token $e_{\mathrm{s}}$ for each patch of $x_f$.

To obtain our target model, we first apply a Transformer encoder~\cite{vaswani2017attention} module to process the features from the search frame and template frame. Specifically, we concatenate $z_t$ and $x_s$ and jointly process them in a Transformer Encoder as follows:
\begin{equation}
\left[\hat{z}_t, \hat{x}_s\right]=T_{\text {enc }}\left(\left[z_t,x_s\right]\right)
\end{equation}
Then, the Transformer decoder~\cite{vaswani2017attention} module is adopted to predict the target model weight $w$. Specifically, we feed the foreground embedding $e_{\mathrm{fg}}$ as query, and the concatenated features($\left[\hat{z}_t, \hat{x}_s\right]$) as key and value into the Transformer decoder module as follows:
\begin{equation}
w=T_{\mathrm{dec}}\left(\left[\hat{z}_t, \hat{x}_s\right], e_{\mathrm{fg}}\right)
\end{equation}
After getting the target model weight $w \in \mathbb{R}^{1\times256}$, we use two $1\times256$ $FC$ layers to get the classification weight $w_{cls}$ and the bounding box regression weight $w_{bbreg}$, respectively. The target classification scores can be computed as follows:
\begin{equation}
h\left(w_{cls}, \hat{x}_s\right)=w_{cls} * \hat{x}_s
\end{equation}
where $*$ represents convolution operation, the $w_{cls} \in \mathbb{R}^{1\times256}$ is the weights of convolution filter.
%And the target classification scores is calculated twice for predicting a more accurate score.
Similarly, the $w_{bbreg} \in \mathbb{R}^{1\times256}$ is used to compute a attention map, and the  attention map are then multiplied point-wise with the search features $x_s$ before feeding them into a Convolutional Neural Network (CNN). The specific operation formula is as follows:
\begin{equation}
x_{bbreg}=j\left(w_{bbreg}, \hat{x}_s\right)x_s=(w_{bbreg} * \hat{x}_s)x_s
\end{equation}
Finally, four sets of $3\times3\times256$ convolutions and a $3\times3\times4$ convolution are used to predict the offset of bounding box.

Note that our template frames include the initial frame and the previously tracked frame. If the classification score threshold of the previously tracked frame is lower than 0.9, replace it with the latest frame with a higher threshold.

\section{Experiments}

\begin{table*}[htb]
	\centering
    \renewcommand\arraystretch{1.5}	
	\caption{PR, NPR, and SR scores (\%) of our traker on RGBT210, RGBT234, the testing set of LasHeR and the short testing set of VTUAV against other trackers. The best and second results are in $\color{red} red$ and $\color{blue} blue$ colors, respectively. * indicates the tracker is re-trained.}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{ccccccccccccc}
		\hline
		Methods &Pub. Info. &Framework& \multicolumn{2}{c}{RGBT210} & \multicolumn{2}{c}{RGBT234} & \multicolumn{3}{c}{LasHeR} & \multicolumn{2}{c}{VTUAV} & {FPS} \\
		&& & MPR$\uparrow$ & MSR$\uparrow$ & MPR$\uparrow$ & MSR$\uparrow$ & PR$\uparrow$ & NPR$\uparrow$ & SR$\uparrow$ & MPR$\uparrow$ & MSR$\uparrow$ & $\uparrow$\\
		\hline
		% SGT~\cite{li2017weighted} & ACM MM2017 & 85.1 & 62.8 & 67.5 & 43.0 & 72.0 & 47.2 & 32.7 & 28.3 & 23.2 \\
		% CMR~\cite{li2018cross} & ECCV2018 & 82.7 & 64.3 & - & - & 71.7 & 48.6 & 35.2 & 30.1 & 25.7 \\
		% MANet~\cite{li2019multi} & ICCVW2019 & 89.4 & 72.4 & - & - & 77.7 & 53.9 & 45.5 & 38.3 & 32.6 \\
		DAPNet~\cite{zhu2019dense} & ACM MM 2019 &CNN& - & - & 76.6 & 53.7 & 43.1 & 38.3 & 31.4 &-&-& 2\\
		DAFNet~\cite{gao2019deep} & ICCVW 2019 &CNN& - & - & 79.6 & 54.4 & 44.8 & 39.0 & 31.1 &62.0&45.8& 20\\
		mfDiMP~\cite{zhang2019multi} & ICCVW 2019 &CNN& 78.6 & 55.5 & - & - & 44.7 & 39.5 & 34.3 &67.3&55.4& 10.3 \\
		CMPP~\cite{wang2020cross} & CVPR 2020  &CNN& - & - & 82.3 & 57.5 & - & - & - &-&-&1.3 \\
		MaCNet~\cite{zhang2020object} & Sensors 2020  &CNN& - & - & 79.0 & 55.4 & 48.2 & 42.0 & 35.0 &-&-& 0.8\\
		CAT~\cite{li2020challenge} & ECCV 2020  &CNN& 79.2 & 53.3 & 80.4 & 56.1 & 45.0 & 39.5 & 31.4 &-&-& 20\\
		FANet~\cite{zhu2020quality} & TIV 2021  &CNN& - & - & 78.7 & 55.3 & 44.1 & 38.4 & 30.9 &-&-& 19\\
		M5L~\cite{tu2021m} & TIP 2021  &CNN& - & - & 79.5 & 54.2 & - & - & - &-&-& 9.7\\
		ADRNet~\cite{zhang2021learning} & IJCV 2021 &CNN& - & - & 80.7 & 57.0 & - & - & - &62.2&46.6& 25\\
		JMMAC~\cite{zhang2021jointly} & TIP 2021 &CNN & - & - & 79.0 & 57.3 & - & - & - &-&-& 4\\
		MANet++~\cite{lu2021rgbt} & TIP 2021  &CNN& - & - & 80.0 & 55.4 & 46.7 & 40.4 & 31.4 &-&-& 25.4 \\
		DMCNet~\cite{lu2022duality} & TNNLS 2022 &CNN& 79.7 & 55.5 & 83.9 & 59.3 & 49.0 & 43.1 & 35.5 &-&-&2.3 \\
		FSRPN~\cite{kristan2019seventh} & ICCVW 2019 &CNN& 68.9 & 49.6 & 71.9 & 52.5 &- &- &-&65.3&54.4& 29 \\
		TFNet~\cite{zhu2021rgbt} &TCSVT 2022 &CNN & 77.7 & 52.9 &80.6 & 56.0 &- &- &-&-&-& 17 \\		
		DiMP~\cite{bhat2019learning}-RGBT & -&CNN & 78.1 & 54.5 & 79.2 & 56.5 & 52.9 & 47.9 & 39.2&-&-& 29 \\
		DiMP~\cite{bhat2019learning}-RGBT* & - &CNN& 83.9 & 59.5 & 85.7 & 62.2 & 60.0 & 54.9 & 46.2 &-&-&29 \\
		HMFT~\cite{zhang2022visible}& CVPR 2022 &CNN& 78.6 & 53.5 & 78.8 & 56.8 & - & - & - &75.8&62.7& \color{blue}30.2 \\
		\hline
  		APFNet~\cite{APFNet} & AAAI 2022 &CNN+Trans& - & - & 82.7 & 57.9 & 50.0 & 43.9 & 36.2 &-&-& 1.3\\
		LRMW~\cite{feng2022learning} & KBS 2022 &CNN+Trans& 80.6 & 59.2 & 82.5 & 61.6 & \color{blue}78.0 & - & \color{blue}62.6 &-&-&24.6 \\ 
		DRGCNet~\cite{mei2023differential} & IEEE SENS J 2023 &CNN+Trans& - & - & 82.5 & 58.1 & 48.3 & 42.3 & 33.8 &-&-&4.9 \\   
		MIRNET~\cite{hou2022mirnet} & ICME 2023 &CNN+Trans& - & - & 81.6 & 58.9 & -& - & - &-&-&30 \\    
  		\hline    
		ToMP50~\cite{mayer2022transforming}-RGBT & - &CNN+Trans&82.1 & 59.3 & 81.1 & 60.2 & 55.1 & 50.4 & 43.3 &77.5&66.0& \color{red}34\\
		ToMP50~\cite{mayer2022transforming}-RGBT* & - &CNN+Trans& \color{blue}84.1 & \color{blue}61.8 & \color{blue}86.8 & \color{blue}65.1 & 65.4 & \color{blue}61.1 & 51.6 &81.4&69.3& \color{red}34\\
		\hline		
		\bf ProFormer & -&CNN+Trans& \color{red}86.8 & \color{red}62.2 & \color{red}89.9 & \color{red}65.7 & \color{red}84.2 & \color{red}80.3 & \color{red}66.7 &\color{red}84.6 &\color{red}71.0 & 22\\
		\hline
	\end{tabular}}
	\label{result}
\end{table*}




% Our tracker is implemented in PyTorch 1.10.2, NVIDIA RTX A6000 GPU, and pytorch 1.8, Nvidia RTX 2080Ti GPU. On a single Nvidia RTX 2080Ti GPU, our tracker can achieve 22 FPS. In this part, we will introduce the details of the datasets, the evaluation metrics, quantitative comparison, and qualitative comparison.



\subsection{Datasets and Evaluation Metrics}
We evaluate our tracker on four benchmarks, including RGBT210, RGBT234, the testing split of LasHeR and the short-term testing split of VTUAV. 
These datasets are only four large-scale public RGBT tracking datasets. RGBT210 is the first large-scale RGBT dataset, which contains 210 video sequence pairs, 210K frames and 12 tracking challenge attributes. RGBT234 is an extension dataset of the RGBT210, which contains 234 video sequence pairs, 234K frames and 12 tracking challenge attributes. LasHeR is the large-scale RGBT dataset with the largest number of sequences, which contains 1224 video sequence pairs with 730K image pairs. In order to cater to the training based on deep learning trackers, it divides the dataset into a training set and a testing set, where the training set has 979 video sequence pairs and the testing set has 245 video sequence pairs. Unlike the above datasets, VTUAV is specific to the UAV scene and it is the RGBT dataset with the most image pairs. Specifically, VTUAV contains 500 video sequence pairs having 1.7M video frame pairs with $1920 \times 1080$. In addition, it divides the dataset into long-term and short-term sequences, and also divided it into the training set and testing set. 
To mitigate small alignment errors, these benchmarks use Maximum Precision Rate (MPR) instead of PR. Specifically, for each frame, we compute the above Euclidean distance on both RGB and thermal modalities and adopt the smaller distance to compute the precision. Similarly, the Maximum Success Rate (MSR) replaces SR as a measure of success rate. In particular, LasHeR does a better alignment, therefore it directly uses the PR, and SR metrics, and it adds an additional Normalized Precision Rate (NPR) metric. The above large-scale datasets are enough to fairly and comprehensively evaluate our tracker.


\subsection{Implementation Details} It is worth noting that our training process is divided into two stages. There is a lack of sufficient RGBT training data, therefore, to provide a good initialization parameter for our network, we first train our method on single modal data (including the training splits of the LaSOT~\cite{fan2019lasot}, GOT10k~\cite{huang2019got}, TrackingNet~\cite{muller2018trackingnet}, and MS-COCO~\cite{lin2014microsoft} datasets). In the first stage, the inputs of both modalities are visible data. In the second stage, we load the trained model from the first stage and then train our tracker on the RGBT234 dataset and the training sets of LasHeR~\cite{li2021lasher} and VTUAV~\cite{zhang2022visible} for online tracking on the testing of LasHeR and VTUAV. In addition, we train our tracker on the training set of LasHeR for testing RGBT234~\cite{li2019rgb} and RGBT210~\cite{li2017weighted}. Throughout the training phase, we train for 100 epochs on three Nvidia RTX 2080Ti GPUs.
We use AdamW~\cite{ilya2019decoupled} with different learning rates for the Transformer feature fusion module (0.0001) and other modules (0.00001), and decay them by 0.2 after 50 epochs. 
%The specific tracking details are the same as ToMP~\cite{mayer2022transforming}.













\begin{table*}[htp]	
\setlength{\tabcolsep}{0.65cm}
\centering
\renewcommand\arraystretch{2.0}	
\caption{The tracking results (PR/SR) under each attribute on RGBT234 dataset. (The top two results are highlighted in \textcolor{red}{red} and \textcolor{blue}{blue}, respectively). } 
\begin{tabular}{c|ccccc|c}
\hline 
\hline 
\textbf{Attribute}   &\textbf{TFNet}  &\textbf{APFNet} &\textbf{CMPP}&\textbf{DMCNet}   &\textbf{ToMP50-RGBT*}     &\textbf{\bf ProFormer} \\
\hline 
\textbf{NO} &93.1/67.3   &94.8/68.0 &\textcolor{blue}{95.6}/67.8    &92.3/67.1  &94.7/\textcolor{blue}{72.4} &\color{red}97.1/73.0 \\
% 
\textbf{PO}  &83.6/57.8   &86.3/60.6 &85.5/60.1    &89.5/\textcolor{blue}{63.1}  &\textcolor{blue}{88.3}/\textcolor{red}{66.7} &\color{red}91.1/66.7 \\
% 
\textbf{HO} &72.1/49.1   &73.8/50.7 &73.2/50.3    &74.5/52.1  &\textcolor{blue}{81.8}/\textcolor{blue}{60.2} &\textcolor{red}{85.5}/\textcolor{red}{61.3} \\
%
\textbf{LI} &80.5/54.1   &84.3/56.9 &86.2/58.4    &\textcolor{blue}{85.3}/58.7  &82.9/\textcolor{blue}{60.5}  &\textcolor{red}{90.9}/\textcolor{red}{65.7}\\
%
\textbf{LR} &83.7/54.4   &84.4/56.5 &\textcolor{blue}{86.5}/57.1    &85.4/57.9  &80.4/\textcolor{blue}{58.2} &\textcolor{red}{88.1}/\textcolor{red}{61.6} \\
%
\textbf{TC} &80.9/57.7   &82.2/58.1 &83.5/58.3    &87.2/61.2  &\textcolor{blue}{90.0}/\textcolor{red}{68.1} &\textcolor{red}{90.4}/\textcolor{blue}{65.7} \\
%
\textbf{DEF} &76.5/54.3   &78.5/56.4 &75.0/54.1    &77.9/56.5  &\textcolor{blue}{83.8}/\textcolor{blue}{64.3} &\textcolor{red}{87.0}/\textcolor{red}{64.9} \\
%
\textbf{FM} &78.2/49.0   &79.1/51.1 &78.6/50.8    &80.0/52.4  &\textcolor{blue}{82.9}/\textcolor{red}{61.4} &\textcolor{red}{86.4}/\textcolor{blue}{60.6} \\
%
\textbf{SV} &80.3/56.8   &83.1/57.9 &81.5/57.2    &84.6/59.8  &\textcolor{blue}{89.7}/\textcolor{blue}{67.8} &\textcolor{red}{90.9}/\textcolor{red}{68.3} \\
%
\textbf{MB} &70.2/50.6   &74.5/54.5 &75.4/54.1    &77.3/55.9  &\textcolor{blue}{87.0}/\textcolor{blue}{66.2} &\textcolor{red}{90.5}/\textcolor{red}{67.3} \\
%
\textbf{CM} &75.0/53.4   &77.9/56.3 &75.6/54.1   &80.1/57.6  &\textcolor{blue}{87.5}/\textcolor{blue}{66.4} &\textcolor{red}{91.6}/\textcolor{red}{67.5} \\
%
\textbf{BC} &81.3/52.5   &81.3/54.5 &83.2/53.8    &\textcolor{blue}{83.8}/55.9  &80.2/\textcolor{blue}{56.5} &\textcolor{red}{85.7}/\textcolor{red}{59.0} \\
\hline 
%
\textbf{ALL} &80.6/55.9   &82.7/57.9 &82.3/57.5    &83.9/59.3  &\textcolor{blue}{86.8}/\textcolor{blue}{65.1} &\textcolor{red}{89.9}/\textcolor{red}{65.7} \\
\hline 
\end{tabular}
\label{tb::CA}
\end{table*}  


\subsection{Quantitative Comparison}
We evaluate our proposed  method on four popular RGBT tracking benchmarks and compare performance with some state-of-the-art trackers, including	DAPNet~\cite{zhu2019dense}, DAFNet~\cite{gao2019deep}, mfDiMP~\cite{zhang2019multi}, CMPP~\cite{wang2020cross}, MaCNet~\cite{zhang2020object}, CAT~\cite{li2020challenge}, FANet~\cite{zhu2020quality}, M5L~\cite{tu2021m}, ADRNet~\cite{zhang2021learning}, JMMAC~\cite{zhang2021jointly}, MANet++~\cite{lu2021rgbt}, APFNet~\cite{APFNet}, DMCNet~\cite{lu2022duality}, FSRPN~\cite{kristan2019seventh},  HMFT~\cite{zhang2022visible}, LRMW~\cite{feng2022learning}, DRGCNet~\cite{mei2023differential},and MIRNET~\cite{hou2022mirnet}. In addition, we also evaluate two RGBT tracking baselines by extending two RGB trackers (DiMP50~\cite{bhat2019learning} and ToMP50~\cite{mayer2022transforming}) with direct feature addition of both modalities.

\begin{figure}[htb]
\centering
% Requires \usepackage{graphicx}
\includegraphics[width=0.5\textwidth]{fig/RGBT210-VIS} \\
\caption{Some visual cases of tracking result on RGBT210.}\label{fig::sc}
\end{figure}

\subsubsection{Evaluation on RGBT210 dataset}
As shown in Table \ref{result}, we can see that the performance of our tracker is clearly superior to the state-of-the-art RGBT methods in all metrics. In particular, the PR/SR score of our method is 7.1\%/6.7\% higher than that of the best RGBT tracker DMCNet. Compared with our baseline method(ToMP-RGBT), the MPR/MSR scores of our tracker are improved by 2.7\%/0.4\%, which is sufficient to prove the effectiveness and superiority of our method on RGBT210. We have visually observed the tracking results of the baseline, and the results show that the baseline method only fail to track the target in extremely challenging scenarios. For example, in Fig.~\ref{fig::sc} (a), the target object is interfered by both strong illumination and similar object, causing the baseline method to fail in tracking process. Another example in Fig.~\ref{fig::sc} (c), the baseline tracker cannot handle the joint challenge problems consisting of occlusion, similar targets, and illumination changes at the same time.
As shown in Fig.~\ref{fig::sc}, compared with the baseline method, our algorithm shows stronger robustness in extreme challenge scenarios. 

\subsubsection{Evaluation on RGBT234 dataset}
RGBT234 is the most widely used dataset in the field of RGBT tracking, and almost every RGBT algorithm is evaluated on this dataset or its subsets. By observing Table \ref{result}, it is clear that our algorithm performs far better than other RGBT tracking algorithms. In particular, compared with the best Transformer-based RGBT tracker (LRMW~\cite{feng2022learning}), our proposed method improves the MPR/MSR scores up to +7.4\%, +4.1\%, which demonstrates that our method can avoid the existing Transformer-based tracker drawbacks and thus efficiently integrate the complementary information of the two modalities.
In addition, the MPR/MSR performance of our algorithm improves by 3.1\%/0.6\% over baseline method. 

To evaluate the performance of our algorithm on various challenge attributes, in Table \ref{tb::CA}, we show the results of our tracker against other state-of-the-art RGBT trackers, including DMCNet, APFNet, CMPP, TFNet, and ToMP50-RGBT*. The attributes include no occlusion (NO), partial occlusion (PO), heavy occlusion (HO), low illumination (LI), low resolution (LR), thermal crossover (TC), deformation (DEF), fast motion (FM), scale variation (SV), motion blur (MB), camera moving (CM) and background clutter (BC). As shown in Table \ref{tb::CA}, the results show that our method performs the best in terms of most challenges, which demonstrates the robustness of our tracker in handling the most adverse conditions. In particular, our algorithm achieves a substantial improvement compared to the baseline under challenges of low illumination and low resolution. The quality of visible image decreases dramatically under these challenges, which shows that our algorithm can indeed solve the fusion problems caused by modal quality differences to cope with similar challenges. In addition, our approach can perform well in classical difficult challenge scenarios such as occlusion and background clutter.


\begin{table}[]\footnotesize
\renewcommand\arraystretch{2}
\caption{Validation on RGBT234 and LasHeR testing set}
\centering
\setlength{\tabcolsep}{3.2mm}{
\begin{tabular}{c|c c| c c c}
	\hline
Datasets    &\multicolumn{2}{c|}{RGBT234}  &\multicolumn{3}{c}{LasHeR testing set} \\
\hline
Metric    &MPR &MSR & PR &NPR &SR \\
\hline
TOMP-RGBT* &94.1 &73.8 &76.0 &71.8 &60.1 \\
\bf ProFormer  &\bf 98.3 &\bf 79.3 &\bf 88.4 &\bf 84.9 &\bf 70.8 \\\hline
\end{tabular}}
\label{tb::Validation}
\end{table}


\begin{figure}[htb]
\centering
% Requires \usepackage{graphicx}
\includegraphics[width=0.5\textwidth]{fig/CA-cut3} \\
\caption{Analysis of main components on LasHeR testing set}\label{fig::AS}
\end{figure}
\subsubsection{Evaluation on LasHeR dataset}
For existing trackers, LasHeR is the most difficult RGBT tracking dataset, which contains a large number of video sequences with challenges such as partial occlusion, similar appearance, fast motion, etc. Table \ref{result} shows that our method achieves impressive tracking results. Compared with the best RGBT trackers, our proposed method improves the PR/SR scores up to +6.2\%, +4.1\%. Compared to the baseline method trained in the same way, the PR/NPR/SR scores of our method are improved by 18.8\%/19.2\%15.1\%. The results show that our algorithm gets better results every time. There are two possible reasons for achieving such good performance: 
%1) The distribution of LasHeR training set and testing set is consistent, and training on LasHeR training set helps to improve the performance of testing set. 
1) LasHeR has high-precision annotation and alignment, which is more suitable for the optimization of our dynamically guided learning algorithm. 
2) Our method can also achieve significant performance improvement compared with the trained baseline, indicating that our model has a stronger learning ability. To verify our conjecture, we conduct a set of experiments where our method and baseline method are trained on the entire LasHeR dataset (both training and testing sets) and RGBT234 and then tested on the LasHeR testing set and RGBT234. As shown in Table \ref{tb::Validation}, our method achieves very high performance compared with the trained baseline. This is difficult for general RGBT tracking algorithms to do, which demonstrates that our tracker really has a strong learning ability and a high upper limit.

\begin{figure*}[htb]
\centering
% Requires \usepackage{graphicx}
\includegraphics[width=1\textwidth]{fig/results-vis} \\
\caption{Visual comparison of our tracker versus three state-of-the-art trackers on four video sequences.}\label{fig::qc}
\end{figure*}

\subsubsection{Evaluation on VTUAV dataset}
As shown in Table~\ref{result}, our algorithm achieves the best performance on the short tracking testing set of VTUAV. In particular, the MPR/MSR scores of our algorithm are 8.8\%/8.3\% higher than the advanced UAV RGBT tracking algorithm HMFT, and 3.2\%/1.7\% higher than baseline. VTUAV is a RGBT tracking dataset created for UAV scenarios. Our tracker achieves good performance on VTUAV as well, which shows that our algorithm has good generalization ability in general and specific scenarios. It is worth noting that our dynamically guided learning algorithm requires well-aligned data, otherwise ambiguity arises. However, VTUAV is a dataset that is not well aligned, therefore to avoid ambiguity, we remove the dynamically guided learning loss to train our network for testing VTUAV.

\begin{comment}
\begin{table}[htp]\footnotesize
\renewcommand\arraystretch{1.4}
\caption{Analysis of main components}
\centering
\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{c| c c| c c c}
	\hline
Datasets    &RGBT234&  &&LasHeR testing set& \\
\hline
Metric    &MPR &MSR & PR &NPR &SR \\
\hline
TOMP-RGBT* &86.8 &65.1 &65.4 &64.1 &51.6 \\
Our-only-fusion &xx &xx &78.7 &74.8 &62.1 \\
Our  &\bf 89.9 &\bf 65.7 &\bf 84.2 &\bf 80.3 &\bf 66.7 \\\hline
\end{tabular}}
\label{tb::AS}
\end{table}
\end{comment}

\subsubsection{Ablation Studies}
In order to verify the effectiveness of each component, we conduct some necessary ablation experiments. The $\text{ProFormer-PFT}$ indicates that our tracker only uses the progressive fusion Transformer module. As shown in Fig. \ref{fig::AS}, compared with baseline ($\text{ToMP50-RGBT*}$), the $\text{ProFormer-PFT}$ achieves 13.3\%, 13.7\%, and 10.5\% improvement in PR, NPR, and SR, which demonstrates that our progressive fusion Transformer module can effectively solve the fusion problems caused by significant modal differences and mine the complementary features between dual modalities. Combining the progressive fusion Transformer module and dynamically guided learning can achieve a greater performance improvement, which also proves that using dynamically guided learning can effectively improve the performance of our network. In addition, compared with the dynamically guided learning algorithm, the progressive fusion Transformer has a greater performance improvement for our network. Moreover, by observing the performance of the Transformer-based method, we can also find that compared with other RGBT tracking methods, the performance of RGBT trackers with Transformer can be greatly improved. Therefore, in future research, how to reasonably use Transformer to improve the performance of multimodal tracking is an interesting and important research topic. 

\begin{table}[]\footnotesize
\renewcommand\arraystretch{2}
\centering
\caption{Results compared with other Transformer fusion structures.}
\setlength{\tabcolsep}{2.8mm}{
\begin{tabular}{c|c c| c c c}
	\hline
Datasets    &\multicolumn{2}{c|}{VTUAV}  &\multicolumn{3}{c}{LasHeR testing set} \\
\hline
Metric    &MPR &MSR & PR &NPR &SR \\
\hline
TOMP-RGBT* &81.4 &69.3 &65.4 &61.1 &51.6 \\\hline
ProFormer-Cross-RGB &81.2 &68.9 &66.3 &62.0 &52.4 \\
ProFormer-Cross-T &77.5 &65.7 &64.1 &59.9 &50.4 \\\hline
Trans-CA &81.2 &68.5 &66.0 &62.2 &52.4 \\
Trans-SA &82.7 &70.1 &66.0 &61.6&52.1\\
Trans-SA-CA &79.8 &68.2 &59.4 &55.1 &46.9 \\
Trans-SA-SA&82.3 &69.7 &63.4 &59.3 &50.1 \\\hline
\bf ProFormer-PFT  &\bf 84.6 &\bf 71.0 &\bf78.7 &\bf 74.8 &\bf 62.1 \\\hline
\end{tabular}}
\label{tb::TFS}
\end{table}


\subsubsection{Comparison with Other Transformer Fusion Structures}
In order to further verify the superiority of our proposed progressive fusion Transformer, we compare six common Transformer-based fusion strategies on the short testing set of VTUAV and the testing set of LasHeR, as shown in Table~\ref{tb::TFS}. A brief introduction to these algorithms is given below. 

$\bullet$ \textbf{ProFormer-Cross-RGB} and \textbf{ProFormer-Cross-T} indicate the tracker that only visible or thermal infrared interact during our second stage of fusion. 

$\bullet$ \textbf{Trans-CA} indicates that two multi-head cross-attention modules are used to interact with two modal features respectively, and then a feed-forward network is used to enhance the fusion features, as illustrated in Fig.~\ref{fig::structure} (d). 

$\bullet$ \textbf{Trans-SA} means that two modal features are added directly and then enhanced with a Trans-Encoder, as shown in Fig.~\ref{fig::structure} (a). 

$\bullet$ \textbf{Trans-SA-CA} indicates that the features of the two modalities are first encoded with two Trans-Encoders. Then the output features of dual modalities are interacted with a multi-head cross-attention module and finally enhanced directly with the feed-forward network. 

$\bullet$ \textbf{Trans-SA-SA} indicates that the features of the two modalities are first encoded with two Trans-Encoders, then the output features of dual modalities are added and finally enhanced directly with additional Trans-Encoders, as shown in Fig.~\ref{fig::structure} (c). 

$\bullet$ \textbf{ProFormer-PFT} indicates that our tracker only uses the progressive fusion Transformer module. 


To achieve a fair comparison, the training strategy is kept the same for all the above comparison experiments. From the tracking results reported in Table~\ref{tb::TFS}, the ProFormer-Cross-RGB and ProFormer-Cross-T show significant performance degradation compared to our method, which indicates that the effective use of the complementary information of the two modalities is crucial for robust RGBT tracking. 
The Trans-CA, Trans-SA, Trans-SA-CA, and Trans-SA-SA are compared with ProFormer-Cross-RGB and baseline method, which show that these fusion methods are affected by the significant differences between modalities resulting in not effectively mining the complementary information of inter-modal. Compared with the above methods, our fusion method has achieved the best experimental results, which proves that our fusion module can address the fusion problems caused by modal significant differences well. 


\subsection{Qualitative Comparison}
We present a qualitative comparison of our algorithm with three state-of-the-art RGBT trackers on some video sequences shown in Fig. \ref{fig::qc}. These state-of-the-art RGBT trackers include ToMP-RGBT*, DMCNet, and APFNet. The four groups of figures contain eight types of challenge attributes, including similar appearance, scale variation, low resolution, fast motion, motion blur, high illumination, partial occlusion, and thermal crossover. As shown in Fig. \ref{fig::qc}, our method is more robust than other algorithms under complex challenge scenarios. For example, the sequence in Fig. \ref{fig::qc} (a) contains similar appearance, low resolution, and thermal crossover challenges, our algorithm can still run well compared with other algorithms. For another example, the high illumination interference in the sequence of Fig. \ref{fig::qc} (d) causes the target in visible modality to not be able to be effectively captured, and the sequence video frames are accompanied by occlusion and other challenges. In such a difficult challenge scenario, our algorithm can still track the target well, which shows that our algorithm can truly solve the fusion problems caused by modal significant differences and mine the complementary features between dual modalities for more robust RGBT tracking.



\subsection{Limitation Analysis} 
Our proposed ProFormer can effectively solve the fusion problem caused by modal differences, but its inference speed does not reach real-time. This may affect the efficiency and feasibility of the algorithm in practical applications. The main reasons may be twofold: 1) RGBT tracking requires the extraction of bimodal features, which increases the computational effort compared to unimodal tracking; 2) the existing fusion models are complex and time-consuming, especially the Transformer-based trackers are less efficient due to the large number of matrix multiplication operations. For the problem of existing fusion models being complex and time-consuming, future prospects can be: 1) simplifying and optimizing the structure and parameters of fusion models, reducing the computation and memory consumption; 2) introducing novel Transformer's acceleration methods \cite{xu2022evo, wu2022tinyvit} to improve the fusion effect and speed.





\section{Conclusion}
In this paper, we propose a novel progressive fusion Transformer to perform an effective fusion of different modalities using a Transformer in RGBT tracking. In particular, ProFormer can progressively integrate single-modality information into the multimodal representation for robust RGBT tracking. In addition, we design a new dynamically guided learning algorithm to model the relations between the fused and modality-specific branches. It adaptively utilizes the well-performing branch to guide the learning of other branches for enhancing the representation ability of each branch. The experimental results show that our proposed method achieves new state-of-the-art performance on four public RGBT tracking datasets, including RGBT210, RGBT234, LasHeR, and VTUAV datasets. In particular, the PR/NPR/SR scores of our method improve by $+6.2 \% +4.1\%$ on the testing set of LasHeR over other state-of-the-art methods. 




\bibliographystyle{IEEEtran}
\bibliography{Aformer-bib}

% \ifCLASSOPTIONcaptionsoff
%   \newpage
% \fi

% that's all folks
\end{document}


