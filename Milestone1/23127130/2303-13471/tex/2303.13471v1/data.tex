
\section{The Epic Sounding Object Dataset}
\label{sec:data}

Existing sound source visual localization evaluation datasets, such as SoundNet-Flickr~\cite{senocak2018learning}, VGG-Sound Source~\cite{chen2021localizing}, only contain \textit{third-person} recordings. To the best of our knowledge, there is no existing dataset that is suitable for evaluating our model.
%
% This has inhibited sounding object visual localization in the egocentric domain, and it remains essentially unexplored. 
%
Thus, we introduce an Epic Sounding Object Dataset for egocentric audio-visual sounding object localization. 
Built upon the well-known Epic-Kitchens~\cite{damen2020rescaling} dataset, we collect sounding object annotations on its action recognition test set.
%
% As Epic-Kitchens contains diverse kitchen visual scenes and sound events, it is challenging to localize sounding objects in videos. 
%
% Next, we will briefly summarize the annotation pipeline and show some interesting studies on it.
% \anurag{maybe add one/two line summary. Something like - it is a two stage process. in the first stage we obtain bounding boxes of objects. In the second stage we define annotation rules based on challenges of egocentric sound source localization and then manually annotate based on those rules.}
% 1. Amazon turk
% 2. design question
% 3. filtering data
% 4. 
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.3\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/duration.png}
%     \caption{The statistics of video duration in Epic-Kitchen test set. 
%     %
% %    \cxu{Add text explanations about \textit{why this is a sounding object and why labeling this region makes sense?} for each example.}
% }
%     \label{fig:duration}
%     % \vspace{-1.5em}
% \end{figure}
\\
\textbf{Data preparation.} 
%
We select 13k test videos from the Epic-Kitchens action recognition benchmark as our source data. 
%
Since these videos are not originally collected for audio-visual analysis, they vary in length, and not all of them contain meaningful sounds.
%
% For instance, the sound of ``stewing food'' might be too low to recognize, and some videos may not even contain audible sounds.
%
% These videos make no contribution to our task.
%
To verify the videos for annotations, we conduct a two-step process:
%
We first determine if a video is silent by checking its sound-level in decibels relative to full scale. Consequently, silent videos are filtered out to provide a meaningful data source.
% 
Second, we bin the videos by their duration and show the statistics in \cref{fig:dataset}~(b).
The majority last less than 2 seconds, and hence we choose to trim the center 1-second clip from each video.
%

After pre-processing, we obtain 5,089 videos in total for annotation.
%
For each video, we uniformly select three frames and annotate sounding objects in the frames. 
%
% This can cover the egomotion in a short time window.
We follow previous works~\cite{tian2021cyclic,chen2021localizing} to use bounding boxes to annotate the objects that emit sounds.
%
%
% Since humans unconsciously associate sounds with specific visual footage, it's meaningful if machines can capture such sound-objects correlation to perform video understanding. 
%
% we follow previous work~\cite{tian2021cyclic} to use bounding boxes to annotate the objects that emit sounds.
%
To obtain proposals of potential sounding objects automatically, we follow Epic-Kitchens~\cite{damen2020rescaling} to use a Mask R-CNN object detector~\cite{he2017mask} trained on MS-COCO~\cite{lin2014microsoft} and a hand-objects detector~\cite{shan2020understanding} that is pretrained with 42K egocentric images~\cite{damen2020rescaling,li2015delving,sigurdsson2018charades}.
%
% \begin{table}[ht]
% \setlength{\tabcolsep}{2mm}
%     \centering
%     \begin{tabular}{cc|cc}
%         \toprule
%          Videos (total) &   Frames (total) &  Out-of-view & Frames  \\
%         \midrule
%         3172 & 9196 & 540 & 1539 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Statistics of the \textit{Epic Sounding Object} dataset.}
%     \label{tab:statistics}
% \end{table}
\begin{table}[t]
\centering
\begin{tabular}{p{1.5cm}l|ccc}
\toprule
\multicolumn{2}{c|}{Before voting}                 & \multicolumn{3}{c}{After voting}                                                \\ \midrule
Video                & \multicolumn{1}{c|}{Frames} & Video                & \multicolumn{1}{c}{Frames} & \multicolumn{1}{c}{Classes} \\
\multicolumn{1}{p{1.5cm}}{5,089} &      15,267                      & \multicolumn{1}{c}{3,172 } &      9,196                       &        30                  \\
\bottomrule
\end{tabular}
    \caption{Statistics of the \textit{Epic Sounding Object} dataset.}
    \label{tab:statistics}
    \vspace{-7mm}
\end{table}
\\
\textbf{Annotation collection.} 
Given the pre-processed data, we then annotate the sounding objects manually.
% the annotators are required to preview the video first. 
Unlike third-person view videos, the object-sound associations in the egocentric domain are more complicated.
%
There are two main challenges: 
(i) Numerous egocentric videos record wearer-environment interaction (\eg, a human places a dish on the table). The object-sound associations could be dynamic, and sometimes it is hard to determine what objects are emitting sounds;
and (ii) the objects in egocentric videos are often missing from the screen, resulting in variations in scale (see Fig.~\ref{fig:dataset}~(c)).
%
We address the above issues by taking advantage of human commonsense knowledge.
%
We ask three or more annotators from the Amazon Mechanic Turk to annotate the same video (frames).
%
Concretely, they do this by first watching the 1-second video with three annotated frames to confirm what objects make sounds in the video.
%
During the annotation course, they are asked to answer two questions:
 (1) Does the video contain out-of-view sounds?
 (2) Which bounding boxes correspond to the sounding objects?
 %(don't select it if the bounding box is too big or too small)? 
%
We collect the annotations in multiple rounds until each video has at least three or more valid annotations from the Amazon annotators.
%
Finally, we conduct an annotation verification by voting on all videos. 
%
% If not, the video is dropped.
%
% Sounding objects could sometimes be ambiguous.
% 
% Different people may choose different objects as the sounding ones.
%
% To alleviate this issue, we ask three different annotators from Amazon Mechanic Turk to annotate the same video simultaneously. 
%
% We then conduct a voting process to get the correct sounding object.
%
If at least two annotators agree on the same answer, it will be considered as a correct annotation;
If not, we will simply omit the video.
The annotation statistics after voting are shown in \cref{tab:statistics}. We obtain 30 classes of sounds by counting the noun (object) classes. The annotations are evenly split into two sets for validation and testing.
%
% Statics of the annotation is summarized in Table~\ref{tab:dataset}.
%
The examples and statistics in Fig.~\ref{fig:dataset} illustrate the diverse and complicated nature of egocentric audio-visual scenes.
%


% \\
% \textbf{Manual frame-level annotation.}
% Given a video clip, we uniformly select three frames and annotate the sounding objects by watching the video.
% %
% In egocentric videos, the distinct viewpoints and characteristics of unique motion patterns bring out several challenges in this process;
% %
% (i) usually, the camera is mounted on a recorder's head or chest. Thus, the body movement sometimes causes ``out-of-view'' sound or partially occluded sounding objects, which might be difficult to localize in the scene;
% %
% (ii) the sound sources are ambiguous, either because there are multiple instances of the same object class in a frame (e.g., frying multiple pans at the same time), or the actual sound source is hidden (e.g., oven in the cupboard is heating food); 
% %
% and (iii), the determination of the actual sounding object can be difficult to define, for example in cases where the sounds can either originate from a single object or human-objects interactions. 
% % Hence, how to define the actual sounding objects should be determined. 

% Our annotation pipeline attempts to address the above-mentioned challenges. 
% %
% First, for cases falling in (i) and (ii) above, if the sounding objects are difficult or perhaps impossible to verify, we remove the frame from the annotation procedure. 
% %
% Second, we define the following rules for annotating frames with  different number of sounding objects:
% \vspace{-0.3em}
% \begin{itemize}
%     \setlength\itemsep{0.1em}
%     \item If the sound is generated by object itself (``A pan is frying food''), we annotate the object as sounding; in the case that there are multiple objects emitting sounds separately, we annotate each one of them.
%     \item If the sound is produced by interactions, e.g., the spatula hits the edge of the pan, it's hard to define how large the sounding region is. But at the object level, humans can associate the sound with both spatula and pan. Therefore, in this case, we annotate all of them as sounding objects.
% \end{itemize}
% \vspace{-0.3em}

%\cxu{I guess the reviewers might question you how this was labeled, i.e., in-house or crowd-sourcing, how many responses were collected for each example, and how do you handle conflicts in labeling.}

% \begin{table}[!h]
%     \centering
%     \begin{tabular}{cccc}
%     \toprule
%          Split & \# Videos  & \# Frames & \# Bbox\\
%          \midrule
%          Val & 428 & 1026 & 1221 \\
%          Test & 429 & 1030 & 1194\\
%     \bottomrule
%     \end{tabular}
%     \caption{The statics summarization of annotation, including the number of videos, frames and sounding objects.}
%     \label{tab:dataset}
% \end{table}


% \section{Egocentric Audio-Visual Object Localization}
% \label{sec:formulation}
% We define the \textit{egocentric audio-visual object localization} as a task to associate audio with dynamic visual scenes and localize sounding objects in egocentric videos. Concretely, given an egocentric video clip $V$ containing $T$ frames $I = \{I_i\}^T_{i=1}$ and its synchronized sound stream $s = \sum_{n=1}^{N}s_n$, $\mathcal{O} = \{O_i\}^T_{i=1}$ are sounding objectness maps that indicate locations of audible objects in the video frames. Here, $s_n$ is the $n$-th sound source in the audio track. Note that there could be multiple sound sources mixed together ($N\geq1$) and not all of them associate with visual objects.
% Our task aims to predict each map $O_i$ from input frames: $I$ and audio $s$ to capture audio associated visual objects and mitigate potential audio noises from audio-visual irrelevant sound sources. Since egocentric videos have a limited FoV, out-of-screen sound sources and egomotion originated from dramatic view changes are ubiquitous. The essential challenges in egocentric audio and visual data make our problem very challenging. 

% % Since egocentric vision has a limited FoV, 


% % find the spatial regions that are responsible for the objects that emit sound in egocentric videos.
% %
% % Concretely, given a video clip and its synchronized audio stream, we extract both audio and visual features and calculate the audio-visual association to spatially localize sounding objects with high responses.
% %

% In this work, we solve it in a self-supervised manner.
% During training, ground-truth sounding object annotations are not available. Instead, we take the audio-visual temporal synchronization as the supervision signal and use audio-guided cross-modal attention to learn the map $O_i$. The network can be optimized in terms of computed maps from sampled positive and negative audio-visual pairs with Contrastive learning~\cite{le2020contrastive}.
% %
% To leverage the temporal contexts in $I$, we consider it as a Multiple Instance Learning (MIL) problem to reduce the temporal uncertainty. 
% %
% % A typical way is solved by contrastive learning~\cite{ots2018,afouras2020self}, 
% %
% During inference, the predicted audio-visual association map $O_i$ can ground sounding objects in the corresponding visual frame. 




% %
% The key of this task is to learn expressive audio-visual associations.
% %
% Compared to third-person view audio-visual localization~\cite{senocak2018learning,chen2021localizing}, the audio-visual association might be affected by: (i) audio noise. Due to the wearer's limited FoV and egomotion, a previously on screen sounding object can move out-of-view shortly. Thus, the captured audio will contain background noise and damage the association learning; 
% %
% (ii) egomotion; The object appearance may undergo various kinds of deformation and occlusion due to the viewpoint changes, makes it harder to learn precise object-sound associations.
% %
% To solve the egocentric audio-visual localization task, the above-mentioned issues should be addressed.

% However, such a \textit{third-person} point of view object detector may not fully perceive the potential egocentric sounding regions.
%
% To make the extracted bounding boxes cover as much as the likely sounding regions, we additionally incorporate a hand-objects interactions detector~\cite{shan2020understanding} that pretrained with 42K egocentric images~\cite{damen2020rescaling,li2015delving,sigurdsson2018charades}.
%
% We use the detected hand-interacted objects as additional bounding box sources.

