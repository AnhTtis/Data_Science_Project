\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{images/framework.png}
    \caption{An overview of our egocentric audio-visual object localization framework. In the beginning, our model extracts deep features from the video and audio streams. Then, the audio and visual features are fed into the cascaded feature enhancement module to inject localization cues for both branches. Such a module is additionally trained with ``mix-and-separation" strategy. Next, our geometric-aware temporal modeling block leverages the relative geometric information between visual frames and performs temporal context aggregation to get the final visual features for localization.}
    \label{fig:framework}
    \vspace{-4mm}
\end{figure*}

\section{Related Work}
\label{sec:background}

% In this section, we discuss some related work on audio-visual learning and scene understanding in the third-person as well as egocentric view videos. 

% \vspace{2mm}
\noindent \textbf{Audio-visual learning in third-person view videos.}
Taking the natural audio-visual synchronization in videos, a large number of studies in the past few years have proposed to jointly learn from both auditory and visual modalities. 
% %
% Moreover, the problem of scene understanding is benefited considerably from major efforts in establishing new audio-visual benchmarks.
% %
% To name a few, SoundNet-Flicker~\cite{senocak2018learning} and VGG-Sound Source (VGG-SS) for evaluating sound localization performance, Audio-Visual Event(AVE)~\cite{tian2018audio} and LLP~\cite{tian2020unified} datasets with dense audio and video event labels for temporal localization, and other general datasets (\emph{e.g.}, MIT MUSIC~\cite{zhao2018sound}) for sound separation.
%
We have seen a spectrum of new audio-visual problems and applications, including visually guided sound source separation~\cite{ephrat2018looking,zhao2018sound,tian2021cyclic,gao2019co,gan2020music,gao2018learning,gao20192,zhou2020sep, rouditchenko2019self}, audio-visual representation learning~\cite{arandjelovic2017look,aytar2016soundnet,hu2019deep,korbar2018cooperative,owens2018audio,owens2016ambient,afouras2020self}, audio-visual event localization~\cite{tian2018audio,tian2019audio,wu2019dual,lin2019dual}, audio-visual video parsing~\cite{tian2020unified,wu2021exploring}, and sounding object visual localization~\cite{senocak2018learning,ots2018,chen2021localizing,qian2020multiple,li2021space,hu2022mix,song2022self,mo2022localizing,mo2022closer,hu2020discriminative}.
%
Most previous approaches learn audio-visual correlations from third-person videos, while the distinct challenges of audio-visual learning in egocentric videos are underexplored.
%
% In this work, we propose to learn egocentric audio-visual association and design an geometry-aware approach to explicitly handle egomotion and mitigate audio noises, which widely exists in egocentric videos.
%
Different from existing works, we propose an audio-visual learning framework to explicitly solve egomotion and out-of-view audio issues in egocentric videos.

% \ytian{Discuss the key differences between our work and existing audio-visual video understanding especially sounding object localization. Not just from a method perspective. It should be more on problem itself.}

% we design a geometry-aware temporal aggregation approach to alleviate the problem brought by egomotion. 
% %
% Compared to previous audio-visual localization methods~\cite{chen2021localizing,senocak2018learning}, our framework also explore improving audio and visual features progressively before predicting the localization results.
%

\noindent \textbf{Egocentric video understanding.}
In the last decade, video scene understanding techniques thrived because of the well-defined third-person video datasets~\cite{carreira2017quo,soomro2012ucf101,caba2015activitynet,miech2019howto100m}.
%
Nevertheless, most of the algorithms are developed to tackle videos curated by human photographers.
%
The natural characteristics of egocentric video data, \eg, view changes, large motions,  and visual deformation, are not well-explored. 
%
To bridge this gap, multiple egocentric datasets~\cite{sigurdsson2018charades,damen2018scaling,su2016detecting,lee2012discovering,damen2020rescaling,grauman2021ego4d} have been collected. These datasets have significantly advanced investigations on egocentric video understanding problems, including activity recognition~\cite{kazakos2019epic,li2021ego,zhou2015temporal}, human(hand)-object interaction~\cite{cai2016understanding,nagarajan2019grounded,damen2016you,shan2020understanding}, anticipation~\cite{abu2018will,furnari2020rolling,liu2020forecasting,singh2016krishnacam}, and human body pose inferring~\cite{jiang2017seeing,ng2020you2me}.
%
However, only a handful of audio-visual works~\cite{cartas2019much,xiao2020audiovisual,kazakos2019epic, mittal2022learning} is presented for egocentric video understanding.
%
Among those, Kazakos~\etal~\cite{kazakos2019epic} proposed an audio-visual fusion network for action recognition, while Mittal~\etal~\cite{mittal2022learning} used audible interactions as cues to learn state-aware visual representations in egocentric videos.
%
There are limited studies in explicit egomotion mitigation and fine-grained audio-visual association learning in egocentric videos. 
% %
% An early attempt Epic-fusion~\cite{kazakos2019epic} only utilized audio as additional information for action recognition.
%
Unlike past works, we tackle challenges in egocentric audio-visual data and propose a robust sounding object localization framework. To enable the research, we propose \textit{Epic Sounding Object} dataset based on Epic-Kitchens~\cite{damen2018scaling,damen2020rescaling}.
%
% Our model attempts to discover the fine-grained audio-visual association in egocentric videos and therefore understand the surrounding scenes from the first-person view.

% \noindent
% \textbf{Geometry modeling.}


