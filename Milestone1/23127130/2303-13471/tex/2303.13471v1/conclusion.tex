\section{ Discussions and Conclusions}
\label{sec:conclusion}
% \vspace{-2mm}
In this work, we tackle a fundamental task: egocentric audio-visual localization to promote the field of study in egocentric audio-visual video understanding.
%
The uniqueness of egocentric videos, such as egomotions and out-of-view sounds pose significant challenges to learning fine-grained audio-visual associations.
%
To address these problems, we propose a new framework with a cascaded feature enhancement module to disentangle visually indicated audio representations and a geometry-aware temporal modeling module to mitigate egomotion.
%
% Moreover, we leverage the rich geometry information included in egocentric videos and design a geometry-aware temporal modeling approach to effectively aggregate features under different viewpoints.
%
Extensive experiments on our annotated \textit{Epic Sounding Object} dataset underpin the findings that explicitly mitigating out-of-view sounds and egomotion can boost localization performance and learn the better audio-visual association for egocentric videos.

\noindent
\textbf{Limitations.} The proposed geometry-aware temporal modeling approach requires geometric transformation computation. 
For certain visual scenes with severe illumination changes or drastic motions, the homography estimation may fail. Then, our GATM will degrade to a vanilla temporal modeling approach. To mitigate the issue, we can consider designing a more robust geometric estimation approach.

\noindent
\textbf{Potential Applications.} Our work offers potential for several applications: (a) Audio-visual episodic memory. As egocentric video records what and where of an individualâ€™s daily life experience, it would be interesting to build an intelligent AR assistant to localize the object (``\textit{where did I use it?}'') by processing an audio query, \eg, an audio clip of ``vacuum cleaner''; (b) Audio-visual object state recognition. In egocentric research, it is important to know the state of objects that human is interacting with, while the human-object interaction often makes a sound. Therefore, localizing objects by sounds provides a new angle in recognizing an object state; (c) Audio-visual future anticipation: following the audio-visual object state recognition task, it's natural to predict the trajectory of a sounding object by analyzing the most recent audio-visual clips.

% \noindent
% \textbf{Acknowledgements:} This work was supported by Meta Research. The article solely reflects the opinions and conclusions of its authors but not the funding agents.