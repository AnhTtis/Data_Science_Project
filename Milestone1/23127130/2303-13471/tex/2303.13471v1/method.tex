% \vspace{-2mm}
\section{Method}
\label{sec:method}
% \vspace{-2mm}


% Given an unlabeled egocentric video clip with its synchronized audio stream, the goal of our model is to find the spatial locations related to the sound by computing the correlation between visual and audio features.
% %
% For localization, we adopt a similar way~\cite{afouras2020self,ots2018,chen2021localizing} that use cosine similarity to estimate the audio-visual correlations at different spatial locations.
% %
% By using temporal synchronization as supervision signal, our model can be trained in a self-supervised manner without any label. 
%
% A common way to train such pipeline is to treat the synchronized audio-visual pairs as positive samples, while obtain negative signal by associating the sound with different videos.
%
% However, this strategy tend to ignore the dynamic nature in egocentric videos, as previous methods usually select one frame to represent the underlying scene.
%
% Our goal is to visually localize sounding objects in egocentric videos, 
% and we formulate our egocentric audio-visual object localization task in \cref{subsec:formulation}. 
% Regarding our method, we first introduce the feature extraction process in \cref{subsec:extraction}. In \cref{subsec:cfe}, we describe the two-stage cascaded feature enhancement pipeline and the strategy to drive its training.
% %
% The details on how to perform geometry-aware temporal aggregation are described in \cref{subsec:gatm}.
% %
% Finally, we summarize the overall training objective in \cref{subsec:learning}.

Our goal is to localize sounding objects in egocentric videos visually. We start by formulating our egocentric audio-visual object localization task in \cref{subsec:formulation}. 
Our proposed method includes a feature extraction process (described in \cref{subsec:extraction}), a two-stage cascaded feature enhancement pipeline (in \cref{subsec:cfe}), and a geometry-aware temporal aggregation module (explained in \cref{subsec:gatm}).
%
Finally, we summarize the overall training objective in \cref{subsec:learning}.

%
\subsection{Problem Formulation and Method Overview}
\label{subsec:formulation}
% In this work, we formulate the \textit{egocentric audio-visual object localization} as a task to associate audio with dynamic visual scenes and localize sounding objects in egocentric videos. 
%
Given an egocentric video clip $V = \{I_i\}^T_{i=1}$ in $T$ frames and its corresponding sound stream $s$, sounding object visual localization aims at predicting location maps $\mathcal{O} = \{O_i\}^T_{i=1}$ that represent sounding objects in the egocentric video. 
Specifically, $O_i(x,y) \in \{0, 1\}$  and positive visual regions indicate locations of sounding objects.
In real-world scenarios, the captured sound can be a mixture of multiple sound sources $s = \sum_{n=1}^{N}s_n$, 
%$\mathcal{O} = \{O_i\}^T_{i=1}$ are sounding objectness maps that indicate locations of audible objects in the video frames. 
where $s_n$ is the $n$-th sound source and it could be out of view. 
%
For the visual input, the video frames may be captured from different viewpoints.
To design a robust and effective egocentric audio-visual sounding object localization system, we should consider the above issues in egocentric audio and visual data and answer two key questions: (Q1) how to associate visual content with audio representations while out-of-view sounds may exist;
(Q2) how to persistently associate  audio features with visual content that are captured under different viewpoints.


% Note that there could be multiple sound sources mixed together ($N\geq1$) and not all of them associate with visual objects.
% The task aims to predict each map $O_i$ from the input frames: $I$, and the audio $s$. 

% The motivation is to capture audio associated visual objects and mitigate potential audio noises from audio-visual irrelevant sound sources. Since egocentric videos have a limited FoV, out-of-screen sound sources and egomotion originating from dramatic view changes are ubiquitous. These characteristics of egocentric video data make the problem very challenging. 

% Since egocentric vision has a limited FoV, 


% find the spatial regions that are responsible for the objects that emit sound in egocentric videos.
%
% Concretely, given a video clip and its synchronized audio stream, we extract both audio and visual features and calculate the audio-visual association to spatially localize sounding objects with high responses.
%

% Concretely, we solve this problem in a self-supervised manner.
% During training, ground-truth sounding object annotations are not available. Instead, we take the audio-visual temporal synchronization as the supervision signal and use audio-guided cross-modal attention to learn the map $O_i$. The network can be optimized in terms of computed maps from sampled positive and negative audio-visual pairs with Contrastive learning~\cite{le2020contrastive}.
% %
% To leverage the temporal contexts in $I$, we also consider it as a Multiple Instance Learning (MIL) problem to reduce the temporal uncertainty. 
%
% A typical way is solved by contrastive learning~\cite{ots2018,afouras2020self}, 
%
% During inference, the predicted audio-visual association map $O_i$ can ground sounding objects in the corresponding visual frame. 

%The issues in egocentric audio and visual data 
% To solve this task, some key audio-visual correspondence problems in highly dynamic videos need to be solved:
% (i) how to associate visual contents with audio representations while out-of-view sounds may exist;
% (ii) and how to associate audio features with visual contents that are captured under different viewpoints.

%
Due to the dynamic nature of egocentric videos, it is difficult and costly to annotate sounding objects for supervised training.
%
To bypass the tedious labeling, we solve the egocentric audio-visual object localization task in a self-supervised manner. The proposed framework is shown in Fig.~\ref{fig:framework}.
%
Our model first extracts representations from the audio $s$ and video clip $V$. 
%and then performs cascaded feature enhancement on both audio and visual branches.
In order to handle Q1, we develop a cascaded feature enhancement module to disentangle visually indicated sound sources 
and attend to visual regions that correspond to the visible sound sources.
%
To enable the disentanglement, we use on-screen sound separation task as the proxy and adopt a multi-task learning objective to train our model where the localization task is solved along with a sound-separation task. 
% We found the target of the disentanglement module can be beneficial for both sounding object visual localization and visible sound separation tasks.
% %
% Therefore, we adopt a multi-task learning objective to drive the training.
%
% The training of audio enhancement network is driven by mix-and-separate strategy~\cite{gao2019co,tian2021cyclic,zhao2018sound}, targeting to disentangle visually indicated sound sources 
% and the output audio features provide more clear localization cues.
%
% Benefiting from the enhanced audio features, the sounding-irrelevant visual features can be reweighted by means of audio-guided cross-modal attention.
%
To deal with the egomotion in egocentric videos (Q2), we design a geometry-aware temporal modeling approach to mitigate the feature distortion brought by viewpoint changes and aggregate the visual features temporally.
% our model estimates the homography transformation between the frames, and then applies it to align frame-level features and aggregate temporal contexts.
%
% solve this problem in a self-supervised manner.
We take the audio-visual temporal synchronization as the supervision signal and estimate the localization map $\Tilde{O_i}$.
% The network is optimized in terms of sampled positive and negative audio-visual pairs with Contrastive learning~\cite{le2020contrastive}.
%
% To leverage the temporal contexts in $I$ and operate without any fine-grained labels, we formulate it as a Multiple Instance Learning (MIL) problem. 
%

% A typical way is solved by contrastive learning~\cite{ots2018,afouras2020self}, 

% During inference, the predicted audio-visual association map $O_i$ can ground sounding objects in the corresponding visual frame. 
% The model is trained with all frame-audio pairs by solving MIL.

% As illustrated in section~\ref{sec:intro}, it is insufficient to capture the fine-grained audio-visual correlations since be out of the scene sometimes.
% %
% To this end, we define the egocentric audio-visual localization as a MIL problem to reduce the temporal uncertainty.
%
% \vspace{-10mm}
\subsection{Feature Extraction}
\label{subsec:extraction}
% Given a short video clip $V$, we select $T$ frames from it to construct visual inputs $\{I_i\}^T_{i=1}$, where $I$ refers to the visual frame. 
%
\noindent
% And the sound signal $s$ of the entire video is input to the audio branch.
\textbf{Visual representation.} 
We use a visual encoder network $E_v$ to extract visual feature maps from each input frame $I_i$.
%
In our implementation, a pre-trained Dilated ResNet~\cite{yu2017dilated} model is adopted by removing the final fully-connected layer.
%
We can subsequently obtain a group of feature maps $v_i = E_v(I_i)$, where $v_i \in \mathbb{R}^{c \times h_v \times w_v}$.
%
Here $c$ is the number of channels, and $h_v \times w_v$ denotes the spatial size.
% 

\noindent
\textbf{Audio representation.}
To extract audio representations from the input raw waveform, we first transform audio stream $s$ into a magnitude spectrogram $X$ with the short-time Fourier transform (STFT). 
%
Then, we extract audio features $a = E_a(X), a \in \mathbb{R}^{c \times h_a \times w_a}$ by means of a CNN encoder $E_a$ in the Time-Frequency (T-F) space. 
%
% The resultant audio features $a = E_a(s), a \in \mathbb{R}^{c \times h_a \times w_a}$.
%



\subsection{Cascaded Feature Enhancement}
\label{subsec:cfe}

As discussed in Sec.~\ref{subsec:formulation}, a sound source $s_n$ in the mixture $s$ could be out of view due to constant view changes in egocentric videos and the limited FoV.
%
This poses challenges in visually localizing sound sources and performance can degrade when the audio-visual associations are not precise. 
% For instance, \emph{frying sound} and \emph{human speech} may simultaneously be captured, while only the visual object of frying sound is presented in the scene.
%
% In this case, these additional sound sources are essentially noise and can corrupt subsequent audio representations.
%
%Specifically, the localization performance will degrade as the audio-visual association can not be precisely described.
%
% Therefore, it might introduce unwanted contents into the localization process.
% %
% For instance, the input audio may simultaneously contain the "frying" sound and human speech, while the visual counterpart of the latter is not presented to the scene.
%
% The human speech is essentially noise here which should be removed from the input.
%
%To address this, we propose to disentangle visually guided audio representations from the mixture features.
%
To address this, we update the features in a cascaded fashion. We first force the network to learn disentangled audio representations from the mixture using visual guidance. Then we utilize the disentangled audio representations to inject the visual features with more localization cues.

\noindent
%\textbf{Audio feature disentanglement.}
\textbf{Disentanglement through sound source separation.}
% Our goal here is to disentangle the visually related audio components from the mixture.
%
%To learn the disentanglement, the sound source localization objective can provide an implicit guidance since the disentangled audio features will contribute to learning a precise localization map.
Sound source localization objective can implicitly guide the system to learn disentangled audio features as the network will try to precisely localize the sound, and in turn, the on-screen sound will get disentangled from the rest. 
% \ZL{I don't understand this sentence. What do you mean by ``appropriate''?}
%
%However, as our task is formulated in a self-supervised manner, it is difficult for the network to learn discriminative features.
%
However, we formulate our problem in an unsupervised setting where labels for such localization objective are not available. 


%Audio-visual sound separation task~\cite{gao2019co,zhao2018sound} can use visual information as a guidance to learn to separate individual sound sources from the mixture. Given the visual guidance it is expected that the learned representations mainly encode information from visually indicated sound sources. Based on the observation, we propose a multi-task learning approach. It jointly learns audio-visual sounding object location and sound separation tasks to disentangle visible audio representations from the mixture.  
Audio-visual sound separation task~\cite{gao2019co,zhao2018sound} uses visual information as guidance to learn to separate individual sounds from a mixture. Given the visual guidance, it is expected that the learned representations primarily encode information from visually indicated sound sources. Hence we argue for a multi-task learning approach to solve our primary task. Along with the audio-visual sounding object localization task, the network also learns to disentangle visible audio representations from the mixture through a source separation task. 

% Interestingly, we found that the visual sound separation task~\cite{gao2019co,zhao2018sound,tian2021cyclic} shares the same backbone to fuse the visual and audio features to obtain visually related audio representation.
%
% Inspired by this, we adopt multi-task learning to offer complete guidance to train the disentanglement.

\begin{itemize}
    \vspace{-1mm}
    \item \textbf{Training.} We adopt the commonly used ``mix-and-separate" strategy~\cite{gao2019co,zhao2018sound} for audio-visual sound separation.
    %
    Given the current audio $s^{(1)}$, we randomly sample another audio stream $s^{(2)}$ from a different video and mix them together to generate input audio mixture  $\tilde{s} = s^{(1)} + s^{(2)}$. 
    % \ZL{these are very bad notations. Use $\tilde{s}$ or $s_\text{mix}$ to denote audio mixture.}
    % %
    % By doing so, the input audio stream $s$ is modified as $s = s^{(1)} + s^{(2)}$.
    %
    We then obtain magnitude spectrograms $\tilde{X}$, $X^{(1)}$, $X^{(2)}$ for $\tilde{s}$, $s^{(1)}$ and $s^{(2)}$ respectively. 
    The audio features is then modified as $a = E_a(\tilde{X})$.
    %

    \vspace{-1mm}
    % \ZL{Similarly, use $\tilde{X}$ or $X_\text{mix}$ to denote the magnitude spectrograms for the audio mixture.}
    \item \textbf{Inference.} During inference, we take the original audio stream as input: $s = s^{(1)}$ and $X = X^{(1)}$ to extract visually correlated audio representations. Note that the audio features is $a = E_a(X)$.
    % as we have no prior information about whether the input sound mixture has out-of-view components, we will simply take the original audio stream as input: $s = s^{(1)}$ and $X = X^{(1)}$.
    %
    % We can then obtain a mixed spectrogram $X$ and the other two original magnitude spectrograms $X^{(1)}$ and $X^{(2)}$, respectively.
    \vspace{-1mm}
\end{itemize}

%
We define the audio disentanglement network as a network $f(\cdot)$, which produces the disentangled audio features $\hat{a} \in \mathbb{R}^{c \times h_a \times w_a}$.
%\Anurag{ and produces disentagled features as outputs ? Add that output here as well. It adds clarity to Eq 1.} \Chao{Added.}
%
In this network, we want to associate the visual content with the audio representations to perform disentanglement in the embedding space.
%
% In other words, the output of $f$ should be disentangled from audio noise.
%
Concretely, we first apply spatial average pooling on each $v_i$ and temporal max pooling along the time axis to obtain a visual feature vector $g_v \in \mathbb{R}^c$. 
%
Then we replicate the visual feature vector $h_a \times w_a$ times and tile them to match the size of $a$.
We concatenate the visual and audio feature maps along the channel dimension and feed them into the network.
%
Therefore, the audio feature disentanglement can be formulated as:
%
\begin{equation}
    \hat{a} = f(\,\textsc{Concat}[\, a, \; \textsc{Tile}(g_v)\,] \,).
\end{equation}
%
In practice, we implement the disentanglement network $f$ using two 1x1 convolution layers. The audio feature $\hat{a}$ will be used for both separation mask and sounding object localization map generation. 

%\Anurag{TO add to Yapeng's point, So there is a 3D feature map inputs to $f()$, $c\,X\,h_a\,X\,w_a$ ? and the output dimension of $\hat{a}$ is ?? Also how are these 3D feature maps processed by MLP.}\Chao{the output dimension of $\hat{a}$ is also $c\,X\,h_a\,X\,w_a$. I have revised the MLP to 1x1 conv to make it more clear. Revise the definition of $f()$}
%\yt{It is not fully clear how the function is implemented. Drawing a clear figure is always the best way to illustrate network. At least do it in your appendix.}
%
% Since the additive audio signal $s^{(2)}$ is known, it is natural to supervise the training of $f$ by solving a spectrogram mask generation task.
%
To separate visible sounds, we add an audio decoder $D_a$ following the disentanglement network to output a binary mask $M_{pred} = D_a(\hat{a})$ (at the bottom of Fig.~\ref{fig:framework}).
%
U-Net architectures~\cite{gao2019co} are used in the audio encoder $E_a$ and decoder $D_a$. 
%
We implement the $E_a$ and $D_a$ in five convolution and up-convolution layers, respectively.
%
Details of the network architectures are provided in the Appendix.
%
The ground truth separation mask $M_{gt}$ can be calculated by determining whether the original input sound is dominant at locations $(u,v)$ in the T-F space: 
% \ZL{I easily forgot that X is a mixture here. That is the reason why the notation is bad.}
\begin{align}
    M_{gt}(u,v) = [X^{(1)}(u,v) \geq \Tilde{X}(u,v)].
\end{align}
% \yt{Provide details (e.g., network structures and hyperparameters) of the encoder and decoder in appendix}
% %
% \begin{equation}
%     M_{gt}: M_{gt}[u,v] &= [X^{(1)}[u,v] \geq X[u,v]].
%     % M_{pred} &= D_a(\hat{a}), 
%     % M_{gt}: M_{gt}[u,v] &= [X^{(1)}[u,v] \geq X[u,v]].
% \end{equation}
%

To train the sound separator, we minimize the $\ell_2$ distance between the predicted and ground-truth masks as the disentanglement learning objective:
\begin{equation}
    \mathcal{L}_{dis} = ||M_{pred} - M_{gt}||_2^2 .
\end{equation}
%
% During training, the model accepts mixed audios as inputs and disentangles the visually indicated audio representations $\hat{a}$ from the mixed audios.
% %
% During inference, the audio enhancement module directly takes the original audio $s$ as input and generates the enhanced audio features $\hat{a}$, where unrelated audio noise should be removed.
% %
% Experimental analysis of the audio noise is provided in Sec.~\ref{subsec:results}.

\noindent
\textbf{Soft localization.}
%
% Given the disentangled audio features $\hat{a}$ from $f(\cdot)$, we utilize it to 
%
Similar to the out-of-view sounds, the visual frames may contain sound-irrelevant regions.
%
In order to learn more precise audio-visual associations, we propose to highlight the spatial regions that are more likely to be correlated with the on-screen sounds by computing audio-visual attention.
%
The attention map will indicate the correlation between audio and visual representations at different spatial locations.
%
Given the output $\hat{a}$ from disentanglement network $f(\cdot)$, we apply max pooling on its time and frequency dimensions, obtaining an audio feature vector $g_{\hat{a}}$.
%
Then at each spatial position (x,y) of visual feature $v_i$, we compute the cosine similarity between audio and visual feature vectors:
%
\begin{equation}
    S_{i}: S_{i}(x,y) = \textsc{CosineSim}(v_i(x,y), g_{\hat{a}}).
\label{equ:avattn}
\end{equation}
%
%
\textsc{Softmax} is then used on $S_{i}$ to generate a soft mask that represents the audio-visual correspondence. 
%
Hence, each $v_i$ can be attended with the calculated weights:
\begin{align}
    \hat{v}_i = \textsc{Softmax}(S_{i}) \cdot v_i.
\end{align}

% \Anurag{A few things are not clear, apart from what Yapend already mentioned.
% (a) why are we calling 3.3 cascaded feature enhancement - it is not explained at all and nor is it obviously clear to me. (b) } \Chao{For (a), because the features first go through the disentanglement module and then soft localization, it's like a cascaded fashion.}
% \Anurag{You should explain the part about cascade then.The soft localization is missing a bit of context in any case. So put it there that the output of f() is cascaded to learn produce soft localization.}



\subsection{Geometry-Aware Temporal Modeling}
\label{subsec:gatm}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{images/geometry.png}
    \caption{
    %\textcolor{red}{make the figure more compact.} 
    Overview of our proposed geometry-aware modeling approach. The visual features $\hat{v}_j$ are warped to viewpoint $i$ by homography transformation. 
    % \yt{delete the warping at the bottom.}
    }
    \label{fig:gatm}
    \vspace{-4mm}
\end{figure}

%Since videos are continuous signals over time, the sounding objects usually do not change instantaneously in the captured video frames.
% Since sounds are naturally temporal phenomena and the audio-visual associations are expected to persist for some duration, we incorporate temporal information from neighboring frames to learn sounding object features.
%
Given the temporal nature of sounds and the persistence of audio-visual associations, we incorporate temporal information from neighboring frames to learn sounding object features.
%
%Based on this assumption, it is natural to utilize temporal information from neighboring frames to learn discriminative sounding object features. 
%
However, temporal modeling is a challenging problem for egocentric videos due to widespread egomotion and object-appearance deformations.
%If a sounding object suffers from severe deformation or occlusion, visual features may undergo distortions resulting in an inferior temporal aggregation.
% %
%  Egomotion and object appearance deformation widely exist in egocentric videos, which  bring significant challenges to the egocentric audio-visual sounding object localization task.
% %
% If a sounding object suffers from severe deformation or occlusion, visual features may undergo distortions resulting in inferior localization performance.
%

% Since the video is continuous signals over time rather than isolated frames, the sounding objects usually do not change instantaneously in the captured egocentric videos.
% %
% Based on this assumption, it is natural to utilize temporal information from neighboring frames to learn discriminative sounding object features.
% %
% However, it would become harder to effectively capture and aggregate temporal contexts due to the changing appearance and position of sounding objects.
%
Although visual objects are dynamically changing, the surrounding physical environment is persistent.
% Indeed, as the underlying 3D physical environment is persistent, the sequence of frames can summarize the content of the surroundings.
%
Hence, temporal variations in egocentric videos reveal rich 3D geometric cues that can recover the surrounding scene from changing viewpoints.  
%
Prior works have shown that given a sequence of frames, one can reconstruct the underlying 3D scene from the 2D observations~\cite{schonberger2016structure,vijayanarasimhan2017sfm}.
%Given a sequence of frames, the ``structure from motion" works~\cite{schonberger2016structure,vijayanarasimhan2017sfm} can reconstruct the underlying 3D scene from these 2D observations.  
%
In our work, rather than reconstructing the 3D structures,  we estimate the relative geometric transformation between frames to alleviate egomotion. Specifically, we apply the transformation at the feature level to perform geometry-aware temporal aggregation.
%
Given $\{I_i\}^T_{i=1}$ and their features ${\{
\hat{v}_i\}^T_{i=1}}$, we take $\hat{v}_i$ as a query at a time and use the other features from neighboring frames as support features to aggregate temporal contexts.
%
For clarity, we decompose the geometry-aware temporal aggregation into two parts: geometry modeling and temporal aggregation.
%

% \vspace{2mm}
\noindent
\textbf{Geometry modeling.}
%
This step aims to compute the geometric transformation that represents the egomotion between frames (see Fig.~\ref{fig:gatm}).
%
We found that homography estimation, which can align images taken from different perspectives, can serve as a way to measure geometric transformation.
%
We adopt SIFT~\cite{lowe2004distinctive} + RANSAC~\cite{fischler1981random} to solve homography.
%
To be specific, a homography is a $3\times3$ matrix that consists of 8 degree of freedom (DOF) for scale, translation, rotation, and perspective respectively.
%
Given the query frame $I_i$ and a supporting frame $I_j$, we use $h(\cdot)$ to denote the computation process:
%
\begin{equation}
    \mathcal{H}_{ji} = h(I_j, I_i)_{j\xrightarrow{} i} ,
\end{equation}
%
where $\mathcal{H}_{ji}$ represents the homography transformation from frame $I_j$ to $I_i$.
%
With the computed homography transformation, we can then apply it at the feature level to transform visual features $\hat{v}_j$ to $\hat{v}_{ji}$. The $\hat{v}_{ji}$ is egomotion-free under the viewpoint of $I_i$. 
%
Since the resolution of feature maps is scaled down compared to the raw frame size, the homography matrix $\mathcal{H}$ should also be downsampled using the same scaling factor.
%
The feature transformation can be written as:
%
\begin{equation}
     \hat{v}_{ji} = \mathcal{H}_{ji} \otimes \hat{v}_{j},
\end{equation}
%
where $\otimes$ represents the warping operation.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{images/temporal.png}
    \caption{Illustration on the temporal context aggregation process for query feature $\hat{v}_{i}$ and neighboring frame features $\hat{v}_{i-1}$ and $\hat{v}_{i+1}$, which is performed independently at each spatial location. 
    % \yt{add symbols (e.g., $\hat{v}_i$) to the figure as what you did for Fig. 3.}
    }
    \label{fig:temporal}
    \vspace{-1em}
\end{figure}

% \vspace{2mm}
\noindent
\textbf{Temporal aggregation.}
%
For the query feature $\hat{v}_{i}$, we end up with set of aligned features $\{\hat{v}_{ji}\}^{T}_{j=1}$ corresponding to each frame viewpoint. 
%
To aggregate the temporal contexts, we propose to compute the correlation between features from different frames at the same locations (see Fig.~\ref{fig:temporal}).
%
The aggregation process can be formulated as:
%
\begin{equation}
    z_i(x,y) = \hat{v}_i(x,y) + \textsc{Softmax}(\frac{\hat{v}_i(x,y) {\boldsymbol{\hat{v}}(x,y)}^T}{\sqrt{d}})\boldsymbol{\hat{v}}(x,y),
\end{equation}
where $\boldsymbol{\hat{v}} = [\hat{v}_{1i};...;\hat{v}_{Ti}]$ is the concatenation of frame features;
%
the scaling factor $d$ is equal to the feature dimension; and $(\cdot)^T$ represents the transpose operation~\cite{vaswani2017attention}.
%
The aggregation operation is applied at all spatial locations $(x,y)$ to generate the updated visual features $z_i$.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.98\textwidth]{images/dataset.png}
    \vspace{-2mm}
    \caption{\textbf{Illustration of the \textit{Epic Sounding Object} dataset statistics}. \textbf{(a)} Example of our video frames and sounding object annotations. Class diversity (squeeze packaging, close trash can, put the pot, etc.) \textbf{(b)}: The distribution of untrimmed video duration.  \textbf{(c)}: The number of videos that are annotated as containing out-of-view sounds. \textbf{(d)}: Distribution of bounding box areas in Epic Sounding Object dataset, the majority of boxes cover less than 20\% of the image area, demonstrating the difficulty of this task. 
}
    \label{fig:dataset}
    % \vspace{-2mm}
\end{figure*}

\subsection{Training Objective}
\label{subsec:learning}

We take audio-visual synchronization as the ``free" supervision and solve the task in a self-supervised manner using contrastive learning~\cite{afouras2020self,ots2018,senocak2018learning,chen2021localizing}.
%

With the audio feature vector $g_{\hat{a}}$ and the visual features $\{z_{i}\}^{T}_{i=1}$, we can compute an audio-visual attention map $S_i$ in Eq.~\ref{equ:avattn} for each frame $I_i$.
%
The training objective should optimize the network such that only the sounding regions have a high response in $S_i$. 
%
Since the ground-truth sounding map is unknown, we apply differential thresholding on $S_i$ to predict sounding objectness map ${O}_i = sigmoid((S_i - \epsilon)/\tau)$~\cite{chen2021localizing}, where $\epsilon$ is the threshold, and $\tau$ denotes the temperature that controls the sharpness. 
%

In an egocentric video clip, a visual scene is usually temporally dynamic. Sometimes a single audio-visual pair ($I_i$,$s$) may not be audio-visually correlated.
%
To this end, we solve the localization task in the Multiple-Instance Learning (MIL)~\cite{maron1997framework} setting to improve robustness.
%
Concretely, we use a soft MIL pooling function to aggregate the concatenated attention maps $\boldsymbol{S} = [S_1;...;S_T]$ by assigning different weights to $S_t$ at different time steps:
\begin{gather}
   \overline{S} = \sum_{t=1}^T (W_t \cdot \boldsymbol{S})[:,:,t],
\end{gather}
%
where $W_t[x,y,:] = \textsc{Softmax}(\boldsymbol{S}[x,y,:])$, $x$ and $y$ are the indices on spatial dimensions.
Subsequently, an aggregated sounding objectness map $\overline{O}$ is calculated from $\overline{S}$.
%
In this way, for each video clip $V$ in the batch, we can define its positive and negative training signals as:
\begin{gather}
    P = \frac {1}{|\overline{O}|} \langle \overline{O} , \overline{S}\rangle,
    \quad
    N = \frac {1}{hw} \langle \mathbf{1} , S_ {neg}\rangle,
\end{gather}
%
where $ \langle \cdot , \cdot \rangle$ is the Frobenius inner product. 

% \yt{what is the $\overline{m}$?}
% \Anurag{These above paragraphs are not very clearly describing that Loss formulation. }
%
We obtain negative audio-visual attention maps $S_{neg}$ by associating the current visual inputs $I$ with audio from other video clips.
%
$\mathbf{1}$ denotes an all ones tensor with shape $h \times w$.
%
Therefore, the localization optimization objective is:
\begin{gather}
        \mathcal{L}_{loc} = -\frac{1}{N} \sum_{k=1}^N [\log \frac{\exp(P_k)}{\exp(P_k) + \exp(N_k)}],
\end{gather}
%
where $k$ is the video sample index in a training batch.
%
The overall objective is $\mathcal{L} = \mathcal{L}_{loc} + \lambda \mathcal{L}_{dis}$, where we empirically set $\lambda=5$ in our experiments.