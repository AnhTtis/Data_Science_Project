% \section{Discussion}
% \label{sec:discussion}

% % \Chao{Todo}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.47\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/discussion.png}
%     \caption{
%     % Potential audio-visual object localization in egocentric videos. (a) \textit{Past}: Given an audio query, this task requires localizing where the answer can be seen within the user’s past video; (b) 
%     % \yt{the piano-book example is not good. the book does not make sound.} 
%     }
%     \label{fig:discussion}
% \end{figure}

% In this work, we investigate the egocentric audio-visual localization task, and the results show that our method can localize the objects that emit sounds. Capturing such a fine-grained audio-visual association is potentially beneficial to more egocentric applications, as discussed below: (a) Audio-visual episodic memory. As egocentric video records the what and where of an individual’s daily life experience, it's interesting for an AR assistant to localize the object (``where did I use it?'') by processing an audio query, e.g., an audio clip of ``vacuum cleaner''; (b) Audio-visual object state recognition: In egocentric research, it's important to know the state of objects that human is interacting with, while the human-object interaction is likely to make a sound. Therefore, localizing objects by sounds provides a new angle in recognizing an object state; (c) Audio-visual future anticipation: following the audio-visual object state recognition task, it's natural to predict what's the trajectory of a sounding object by analyzing the most recent image-audio clips.

% \textbf{Limitations.} Our proposed geometry-aware temporal modeling approach may be affected by the geometric transformation computation. 
% For scenes where visual frames undergo severe illumination change or drastic motion, the homography estimation may fail, and then our GATM will degrade to a vanilla temporal modeling approach.