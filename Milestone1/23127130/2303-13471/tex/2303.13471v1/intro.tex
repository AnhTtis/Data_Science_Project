\section{Introduction}
\label{sec:intro}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{images/motivation.png}
    \caption{\textbf{Sounding object localization in egocentric videos.} 
    % An example of sounding objects inside egocentric videos. 
    Due to the wearer's egomotion, the viewpoint changes continuously across time.
    % The sounding pot in the above video clip first occurs but then is partially occluded due to the ego-motion. Later, it appears again as the wearer's attention return to the pot. 
    Consequently, audio-visual relations are dynamically changing in egocentric videos.  
    %For instance, human speech sound happens out-of-view during some moments.
    % Therefore, it's captured as audio noise.
    Our approach tackles challenges in the egocentric audio-visual sounding object task and learns audio-visual associations from first-person videos.
    } 
    \label{fig:motivation}
    \vspace{-5mm}
\end{figure}

% logic: why this task?
% What's the observation?
% Motivation of method and some details
% para1: audio-visual is important in human perception, and great success at this stage
% para2: lack of an essential property of human center, important in robotics and AR
% para3: observation in ego-centric video
% para4: Our method part 1: MIL training and Geometry
% para5: Enhancement
% para6: Experiment 


% human perception system require audio and visual
% current work excel
% v1:
% Auditory and visual sensory are two indispensable components of the human perception system.
% %
% Utilizing both modalities can achieve robust scene understandings especially in scenarios where one modality is "blind," e.g., an object emits sound is outside of the field-of-view.
% %
% % Current community manage to build effective audio-visual systems to understand the surrounding scenes in a more human-like way.
% %
% This intrigues the community in developing perception systems that rely on audio and visual modality.
% %
% Tremendous progress have been made in a series of downstream tasks, e.g., sounding object visual localization~\cite{ots2018,qian2020multiple,senocak2018learning,tian2021cyclic}, visually guided sound sources separation~\cite{zhao2018sound,gao2018learning,gao2019co}, audio-visual event localization~\cite{tian2018audio,chen2020vggsound}, and audio-visual parsing~\cite{tian2020unified,wu2021exploring}.
%
%
%v2: importance of ego centric videos
The emergence of wearable devices has drawn the attention of the research community to egocentric videos, the significance of which can be seen from egocentric research in a variety of applications such as robotics~\cite{martin2021jrdb,kim2019eyes,kawamura2002toward}, augmented/virtual reality~\cite{jones2008effects,poupyrev1998egocentric,swan2007egocentric}, and healthcare~\cite{serino2015detecting,morganti2013allo}.
% % ~\cite{martin2021jrdb}
% , augmented/virtual reality
% % ~\cite{xxx}
% , and healthcare
% % ~\cite{xxx}
% . 
%With the emergence of portable wearable devices, research on egocentric videos have attracted a lot of attention including in areas such as robotics~\cite{xxx}, augmented/virtual reality~\cite{xxx}, and healthcare~\cite{xxx}. \ytian{Find some refs.}
%
In recent years, the computer vision community has made substantial efforts to build benchmarks~\cite{damen2018scaling,damen2020rescaling,northcutt2020egocom,donley2021easycom,sigurdsson2018charades,li2018eye}, establish new tasks~\cite{fathi2011learning,li2013pixel,li2013learning,park2016egocentric,lee2012discovering}, and develop frameworks~\cite{li2021ego,kazakos2019epic,munro2020multi,wang2021interactive} for egocentric video understanding.

% \ytian{Try to cover most representative works. It is not proper just cite works from one group. I think you can find a lot of relevant papers from Ego4D authors or certain GitHub paper list repos}.
% \anurag{perhaps also refer to other datasets such as Egocom, Easycom}. 
%
% Limitation
%v1:
% While current field of study keeps achieving better performance in audio-visual learning, it leaves out a gap to how human actually perceive the world -- where we receive the signal from the "\textit{ego-centric}" point of view. 
% %
% Existing algorithms target at learning audio-visual correlations from third-person view videos, which are intentionally collected and curated by human.
% %
% In contrast, egocentric videos are usually long and fluid video streams, where the visual perception is driven by wearer's attention that resulting in frequent viewpoint changes, object occlusion and \textit{etc}.
%
%
%v2: 

While existing works achieve promising results in the egocentric domain, it still remains an interesting but challenging topic to perform fine-grained egocentric video understanding.
%
For instance, understanding which object is emitting sound in a first-person recording is difficult for machines. As shown in Fig.~\ref{fig:motivation}, the wearer moves his/her head to put down the bottle. The frying pot which emits sound subsequently suffers deformation and occlusion due to the wearer's egomotion. 
%
Human speech outside the wearer's view also affects the machine's understanding of the current scene.
%
This example reveals two significant challenges for designing powerful and robust egocentric video understanding systems:
% %
First, people with wearable devices usually record videos in naturalistic surroundings, where a variety of illumination conditions, object appearance, and motion patterns are shown. 
%
The dynamic visual variations introduce difficulties in accurate visual perception.
% %
Second, egocentric scenes are often perceived within a limited field of view (FoV). The common body and head movements cause frequent view changes (see Fig.~\ref{fig:motivation}), which brings object deformation and creates dynamic out-of-view content.

% Drop
% % However, the majority of work focuses on visual scene analysis, and it presents 
% % It is challenging to understand video scenes captured in a first-person view. Since the egocentric visual signals are often perceived within a limited field of view (FoV), the user's attention might be driven by out-of-view contents and leading to frequent view changes (see Figure~\ref{fig:motivation}).
% \ZL{My biggest concern is that there is no task described. The paper directly jumps to the challenges in egocentric video.}
% The majority of the works focus on visual scene analysis.
% % %
% It presents significant challenges for designing powerful and robust egocentric video understanding systems. 
% % %
% First, people with wearable devices usually record videos in naturalistic surroundings, where a variety of illumination conditions, object appearance, and motion patterns are shown. 
% The dynamic visual variations introduce difficulties in achieving accurate visual perception. \ZL{I do not quite understand why this is the first challenge. Wouldn't non-egocentric videos have the same issues of visual variations? Are you suggesting that, for example, the carrot has large appearance variations over time? In that case, you can use an example to elaborate on this challenge.}
% % %
% Second, egocentric scenes are often perceived within a limited field of view (FoV). The common body and head movements of users cause frequent view changes (see Fig.~\ref{fig:motivation}), which brings large motions \ZL{Movement brings ``motions'' is obvious. Are you suggesting motion blur?} and object deformation and creates dynamic out-of-view content.
% % % An user's attention might be driven by out-of-view content and leading to frequent view changes (see Figure~\ref{fig:motivation}).
% % %
% % In this case, modern vision systems may not be able to fully decode the surrounding information from egocentric videos.
% % \ZL{``Therefore'' here seems far-fetched. You can move this sentence to the beginning of the next paragraph by saying ``while visual-only'' system suffer from the challenges, audio can ...}
% % Therefore, it is difficult for a visual-only system to fully decode the surrounding information and perceive dynamic scenes in egocentric videos.


%
% v1
% Given the dynamic nature of visual perception in egocentric videos, we hypothesize that the current audio-visual learning paradigm will be challenged by the lack of knowledge about egocentric temporal variations.
% %
% Prior works~\cite{qian2020multiple,chen2021localizing,tian2018audio} assume that the relation sound and visual appearance are usually unchanged in the selected video clips.
% %
% Thus, the algorithm can learn good representations by solving audio-visual correspondence.
% %
% However, in egocentric videos, the relations between sound and its visual counterpart can become more dynamic since a sounding object may occur and disappear back and forth even in a very short time (see Figure~\ref{fig:motivation}).
% %
% The rapid change of sounding object status (either visible or out-of-view) in a short video clip posts new challenges on selecting effective audio-visual pairs to guide the learning.
% %
% Also, due to the viewpoint changes, the visual appearance of sounding objects suffer from various kinds of deformation, which makes localizing sounding objects consistently even harder. 
%
%v2:            
Although a visual-only system may struggle to fully decode the surrounding information and perceive scenes in egocentric videos, audio provides stable and persistent signals associated with the depicted events. 
%\ZL{Let me first read the method section and go back to the introduction for a better version of motivation.}
Instead of purely visual perception, numerous psychological and cognitive studies~\cite{shams2008benefits,jacobs2019can,bulkin2006seeing,spence2003multisensory} show that integration of auditory and visual signals is significant in human perception.
%
% In layman's term, we human utilize sight and sound to understand our surroundings from the first-person point of view.
%
Audio, as an essential but less focused modality, often provides synchronized and complementary information with the video stream.
%
In contrast to the variability of first-person visual footage, sound describes the underlying scenes consistently.
% provides stable and persistent signals associated with the depicted events. \ZL{I copied this sentence to the first sentence of the paragraph. Chao: you need to rewrite this sentence.}
These natural characteristics make audio another indispensable ingredient for egocentric video understanding.
%
% For example, audio can be an essential ingredient for understanding the ``cooking'' activity
% for egocentric video understanding in Figure~\ref{fig:motivation}. 
% egocentric audio-visual scene understanding is promoted to enable novel applications with first-person videos.

%v1:
% To explore the field of ego-centric audio-visual scene understanding,
% we study the problem of ego-centric sounding objecting visual localization. 
% %
% Being aware of the temporal variations in first-person view videos, we propose a novel framework to address the above problems by explicitly leveraging the rich geometry information and temporal contexts.
% %
% Concretely, rather just selecting a single audio-visual pair from the video clip, we formulate it as a Multiple Instance Learning (MIL) problem to reduce the temporal uncertainty brought by temporal variation.
% %
% It is more likely that a sounding object occurs in a time window than just a random timestamp.
% %
% Furthermore, although the viewpoint changes make it more difficult to localize sounding objects, it simultaneously provides us rich geometric information about the underlying 3D physical surroundings.
% %
% Thus, we propose a Geometry-Aware Temporal Modeling (GTAM) block to effectively align and aggregate the temporal contexts by explicitly extracting the geometric transformation across a sequence of frames.
% %
% This block helps the model to consistently capture discriminative temporal features for localization.
%
%v2
To effectively leverage audio and visual information in egocentric videos, a pivotal problem is to analyze the fine-grained audio-visual association, 
specifically identifying which objects are emitting sounds in the scene.
% \emph{i.e.}, discovering what objects are emitting sounds in the scene. 
%
In this paper, we explore a novel egocentric audio-visual object localization task, which aims to associate audio with dynamic visual scenes and localize sounding objects in egocentric videos.
%
Given the dynamic nature of egocentric videos, it is exceedingly challenging to link visual content from various viewpoints with audio captured from the entire space.
% For instance, the wearer could constantly shift their attention back and forth, which makes audio-object relations dynamically change over time (Fig.~\ref{fig:motivation}).
%
% To capture audio-visual association and tackle the challenges in egocentric videos, in this paper, we explore an egocentric audio-visual object localization task, which aims to associate audio with dynamic visual scenes and localize sounding objects in egocentric videos. 
Hence, we develop a new framework to model the distinct characteristics of egocentric videos by integrating audio.
%
In the framework, we propose a geometry-aware temporal module to handle egomotion explicitly. 
% The egomotion in egocentric videos leads to large motions and object deformations, making it difficult to localize the sounding objects consistently.
Our approach mitigates the impact of egomotion by performing geometric transformations in the embedding space and aligning visual features from different frames. 
% estimating the temporal geometry transformation and exploiting it to update visual representations.
%
% Despite the downside, we found that the egomotion also provides rich geometry information about the underlying scene and gives hints on the relative geometric transformation between frames. 
% %
% Motivated by this, we use the predicted geometric transformation to mitigate the object deformation in the embedding space and align the visual features.
%
We further use the aligned features to leverage temporal contexts across frames to learn discriminative cues for localization.  
Additionally, we introduce a cascaded feature enhancement module to handle out-of-view sounds.
The module helps mitigate audio noises and improves cross-modal localization robustness.

% A series of related audio-visual tasks have been studied in third-person videos, including sounding object visual localization~\cite{ots2018,qian2020multiple,senocak2018learning,tian2021cyclic}, visually guided sound sources separation~\cite{zhao2018sound,gao2018learning,gao2019co}, audio-visual event localization~\cite{tian2018audio,chen2020vggsound}, and audio-visual parsing~\cite{tian2020unified,wu2021exploring}.
% %
% But such works ignore the uniqueness of learning from egocentric videos.
% %
% For instance, the wearer shift their attention back and forth and lead to dynamic relations between the audio and the sounding objects (see Figure~\ref{fig:motivation}).
%

% \ytian{YT: I've made some modifications. But, I think we can do better for this paragragh. Do not try to provide all details. Just deliver the key methods to solve the challenges and your key findings. }

% % need rewrite 
% To capture audio-visual association and tackle the challenges in egocentric videos, in this paper, we explore an egocentric audio-visual object localization task, which aims to associate audio with dynamic visual scenes and localize sounding objects in egocentric videos. We develop a new framework to explicitly model the distinct characteristics of egocentric videos by integrating audio.
% %
% Specifically, in the framework, we propose a geometry-aware temporal module to handle egomotion explicitly. The egomotion in egocentric videos leads to large motions and object deformations, making it difficult to consistently localize the sounding objects. Our approach mitigates the effect of egomotion by performing geometric transformations in the embedding space and aligning visual features from different frames. 
% % estimating the temporal geometry transformation and exploiting it to update visual representations.
% %
% % Despite the downside, we found that the egomotion also provides rich geometry information about the underlying scene and gives hints on the relative geometric transformation between frames. 
% % %
% % Motivated by this, we use the predicted geometric transformation to mitigate the object deformation in the embedding space and align the visual features.
% %
% Based on the aligned features, we further leverage the temporal contexts across frames to learn discriminative cues for localization.  
% In addition, we propose a cascaded feature enhancement module to handle out-of-view sounds. The module helps mitigate audio noises and improves cross-modal localization robustness.
% % %
% % Our localization model aims to find precise audio-visual associations.
% % %
% % However, the results might be affected by out-of-view sounds or unrelated visual contents, \emph{e.g.}, the silent objects. 
% % %
% % To this end, we propose a cascaded feature enhancement module to mitigate the audio noise and improve cross-modal localization robustness.
% % %
% % At the first stage, our model learns to disentangle the audio representations by integrating visual cues. 
% % %
% % In this manner, the refined audio features only keep the components that are related to the current scene;
% % %
% % (ii) at the second stage, the refined audio features are fed into the visual branch to compute the sound response at different locations.
% % %
% % Then, the visual features are enhanced with the localization cues.
%

Due to the dynamic nature of egocentric videos, it is hard and costly to label sounding objects for supervised training.
%
% Indeed, the rich viewpoint changes provide various kinds of potential audio-visual correspondence as supervision.
%
To avoid tedious labeling, we formulate this task in a self-supervised manner, and our framework is trained with audio-visual temporal synchronization. 
%
Since there are no publicly available egocentric sounding object localization datasets, we annotate an \textit{Epic Sounding} dataset to facilitate research in this field. 
%
Experimental results demonstrate that modeling egomotion and mitigating out-of-view sound can improve egocentric audio-visual localization performance.
% , which may also be beneficial to future egocentric audio-visual learning. 
%

%
%\yt{Results and findings - you should mention both. Recall that you are solving two problems in the paper} \Chao{Added.}



% As the above GTAM only operates on the visual branch, we design a cascaded feature enhancement module to enable better interactions between audio and visual branches and inject localization cues for both audio and visual features.
% %
% The main advantages of this module is two-fold: 
% (i) The input audio is not always clean. With audio enhancement module, the model can first filter out some unrelated sound components, e.g., background noise; 
% (ii) as the audio features are enhanced with localization cues at the first stage, we further incorporate a visual feature enhancement module by emphasizing the potential sounding regions with soft localization masks.
% %
% In this pipeline, both audio and visual features will be injected with localization cues to improve the final results.
% %
% Additionally, we adopt the mix-and-separate design to provide auxiliary supervision for training the module.
% %
% To facilitate our research, we collect a \textit{Epic Sounding Object} dataset with diverse kitchen scenes. 
% %
% Bounding box annotation on the sounding objects is applied to obtain a test set for evaluation.

In summary, our contributions are: 
(1) the first systematical study on egocentric audio-visual sounding object localization;
(2) an effective geometry-aware temporal aggregation approach to deal with unique egomotion;
(3) a novel cascaded feature enhancement module to progressively inject localization cues; and
(4) an \textit{Epic Sounding Object} dataset with sounding object annotations to benchmark the localization performance in egocentric videos.
% Our dataset, source code, and pre-trained models will be released. 

% \vspace{-5mm}
%\Anurag{I think self-supervision may need to emphasized more.} \Chao{I have added more in the second last paragraph above.}