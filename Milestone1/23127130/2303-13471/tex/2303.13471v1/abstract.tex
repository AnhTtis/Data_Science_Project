\begin{abstract}
    % Audio, acting as an essential cue for the human perception, promotes solving audio-visual scene understanding tasks in an egocentric manner. However, prior works mainly focus on parsing audio-visual scenes from a third-person perspective, ignoring the unique characteristics in ego-centric videos, e.g., frequent viewpoint changes. 
    % To bridge this gap, we study the ego-centric sounding object visual localization problem and propose a novel framework to explicitly leverage the ego-centric information for localizing sounds. 
    % Specifically, we formulate this problem as learning from multiple synchronized audio-visual pairs to reduce temporal uncertainty. We design a cascaded feature enhancement module to associate the audio and visual features with localization cues. 
    % Then, we propose a Geometry-Aware Temporal Modeling block to measure the geometric variation across frames and aggregate the feature more discriminatively. 
    % To facilitate research in this field, we annotate an \textit{Epic Sounding Object} dataset to benchmark our method and also conduct experiments on the \textit{Ego4D} dataset to show the generalization ability.
    
Humans naturally perceive surrounding scenes by unifying sound and sight from a first-person view.
% This underpins the importance of pushing machines to approach human intelligence that can learn with multisensory inputs from an egocentric perspective.
Likewise, machines are advanced to approach human intelligence by learning with multisensory inputs from an egocentric perspective.
In this paper, we explore the challenging egocentric audio-visual object localization task and observe that 1) egomotion commonly exists in first-person recordings, even within a short duration; 2) The out-of-view sound components can be created when wearers shift their attention. 
% \ZL{Shouldn't the two problems are the same in essence? The out-of-view sound is due to the egomotion.}
%Since we humans naturally perceive surrounding scenes by unifying sound and sight in a first-person view, such a problem is important for pushing machines to approach human intelligence that can learn with multisensory inputs in an egocentric perspective. 
%We take temporal synchronization as the ``free'' supervision and solve the task in a self-supervised manner. 
To address the first problem, we propose a geometry-aware temporal aggregation module that handles the egomotion explicitly.
The effect of egomotion is mitigated by estimating the temporal geometry transformation and exploiting it to update visual representations.
% A cascaded feature enhancement module is developed to associate the audio and visual features with localization cues. 
% Moreover, we mitigate audio noises and improve cross-modal localization robustness by disentangling visually-indicated audio representation.
Moreover, we propose a cascaded feature enhancement module to overcome the second issue. It improves cross-modal localization robustness by disentangling visually-indicated audio representation.
% We take advantage of the naturally available audio-visual temporal synchronization as the ``free'' self-supervision to avoid costly labeling of the training data.
During training, we take advantage of the naturally occurring audio-visual temporal synchronization as the ``free'' self-supervision to avoid costly labeling.
We also annotate and create the \textit{Epic Sounding Object} dataset for evaluation purposes. 
Extensive experiments show that our method achieves state-of-the-art localization performance in egocentric videos and can be generalized to diverse audio-visual scenes. 
% Code is available at \href{https://github.com/WikiChao/Ego-AV-Loc/}{this link}.
Code is available at \url{https://github.com/WikiChao/Ego-AV-Loc}.

% It effectively alleviates geometric variations across video frames and captures superior audio-visual associations while reducing audio noise. 
% Thanks to its universality, our model can be generalized to capture diverse audio-visual scenes in the new Ego4D dataset.

% In this paper, we introduce an egocentric audio-visual object localization task, which aims to associate audio with dynamic visual scenes and localize sounding objects in egocentric videos. Since we humans naturally perceive surrounding scenes by unifying sound and sight in a first-person view, such a problem is important for pushing machines to approach human intelligence that can learn with multisensory inputs in an egocentric perspective. 
% %We take temporal synchronization as the ``free'' supervision and solve the task in a self-supervised manner. 
% The naturally available audio-visual temporal synchronization is used as the ``free'' supervision in our self-supervised learning framework.
% Concretely, we propose a geometry-aware temporal aggregation approach to explicitly handle the challenging egocentric videos. We develop a cascaded feature enhancement module to associate the audio and visual features with localization cues. Moreover, we mitigate audio noises and improve cross-modal localization robustness by disentangling visually-indicated audio representation. To enable our investigation, we annotate parts of the Epic-Kitchens Dataset to produce \textit{Epic Sounding Object} dataset. Experimental results show that our method can achieve state-of-the-art localization performance. It effectively alleviates geometric variations across video frames and captures superior audio-visual associations while reducing audio noise. Furthermore, thanks to its universality, our model can be generalized to capture diverse audio-visual scenes in the new Ego4D dataset.

% point 1: temporal variation due to ego motion
% point 2: dual enhancement 
% point 3: Epic sounding dataset and ego4d

\end{abstract}