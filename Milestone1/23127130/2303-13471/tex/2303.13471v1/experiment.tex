\section{Experiment}

% \begin{table*}
% \parbox[t]{.58\linewidth}{
% \setlength{\tabcolsep}{1mm}
% \small
% \centering
% \begin{tabular}{l||cccc}
% \toprule
% \multirow{2}{*}{}             & \multicolumn{3}{c|}{CIoU}                 &   \multicolumn{1}{c}{\multirow{2}{*}{AUC}}    \\ \cline{2-4}
%                                                & @0.2  & @0.3  & \multicolumn{1}{c|}{@0.4} &    \\ \midrule
% \multicolumn{1}{l||}{Attention~\cite{senocak2018learning}}                & 7.12  (+443\%)     & -      &     -                      & 6.42 (+186\%)     \\
% \multicolumn{1}{l||}{STM~\cite{li2021space}}                                          & 12.10 (+220\%)     & 7.64 (+154\%)     & 4.01 (+162\%)         & 8.87 (+107\%)      \\
% \multicolumn{1}{l||}{Hardway~\cite{chen2021localizing}}                                   & 24.51 (+57.9\%) & 13.55 (+43.3\%) & 6.10  (+43.3\%)        & 13.38 (+37.4\%) \\
% \multicolumn{1}{l||}{SSPL~\cite{song2022self}}                                            & 13.62  (+184\%)    & 8.10 (+140\%)     &  4.45 (+72.3\%)            &  9.56  (+92.3\%)   \\
% \multicolumn{1}{l||}{Mix~\cite{hu2022mix}}                                         & 26.01(+48.8\%)      &  15.25 (+27.3\%)    &  9.90  (+6.16\%)             &  15.39  (+19.4\%)   \\
% \hline
% \multicolumn{1}{l||}{Our}                                       &  \textbf{38.71}     &\textbf{ 19.42}      & \textbf{10.51  }                        & \textbf{18.38}     \\
% \bottomrule
% \end{tabular}
% \caption{Quantitative comparison of localization results on \textit{Epic Sounding Object} dataset. The results of metrics CIoU@\{0.2, 0.3, 0.4\} and AUC are reported. The top-1 results are highlighted. Also, the relative improvements of our methods over the compared method under different metrics are shown.}
% \label{tab:major}
% }
% \hfill
% \parbox[t]{.35\linewidth}{
% \small
% \setlength{\tabcolsep}{0.8mm}
% \centering
% \begin{tabular}[!t]{cccc||ccc}
%     \toprule
%     Model & GATM & SL & $L_{dis}$  & CIoU@0.2 & AUC  \\
%     \midrule
%      a & & &  & 27.41 & 15.36 \\
%      b & \checkmark& &  & 37.38 & 16.59 \\
%     c &  \checkmark&\checkmark &  & 38.21 & 17.92 \\
%     d &  \checkmark&\checkmark & \checkmark   & \textbf{38.71} & \textbf{18.38} \\
%     \bottomrule
% \end{tabular}
% \caption{Ablations study on GATM, Soft Localization (SL), and audio disentanglement module. The top-1 result in each column is highlighted.}
% \label{tab:ablation}
% }
% \vspace{-5mm}
% \end{table*}

% \subsection{Experimental Setting}
% \label{subsec:setting}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{images/viz_resized.png}
    \vspace{-2mm}
    \caption{Qualitative comparison on \textit{Epic Sounding Object} dataset.
    We show diverse sounding objects in the kitchen scenes in the first column, sounding objects are annotated in red boxes.
    % The corresponding sounds from left to right are: 
    Our method outperforms all the compared works. 
    % \yt{method names are not consistent with Tab.2}
    }
    \vspace{-4mm}
    \label{fig:viz}
\end{figure}
\noindent
\textbf{Datasets.} 
In our experiments, we use two egocentric datasets. 
%
% We use Epic-Kitchen for the major quantitative and qualitative comparisons while employing Ego4D to validate the generalization ability of our method.
%
(1) Epic-Kitchens~\cite{damen2020rescaling}: The dataset consists of 100 hours of egocentric recordings from 45 kitchen scenes. Thus, diverse kitchen-relevant events and sounds are in the dataset. We follow the same data split released in their action recognition benchmark and select 62,413 training videos.
%
We then filter out the silent videos by measuring the Decibels relative to full scale. 
%
This results in 47,214 training videos in total.
%
For evaluation, we use our annotated \textit{Epic Sounding Object} dataset to report the results;
%
(2) Ego4D~\cite{grauman2021ego4d}: Ego4D is the most recent large-scale egocentric video dataset.
%
Besides kitchen scenes, it includes diverse daily life scenarios.
%
Specifically, it consists of different subsets serving different benchmarks.
%
We select the ``Bristol" subset as it contains diverse scenarios (\eg, entertainment, sports, commuting, and more) to test our method.
%
We randomly sample and trim 50,000 1-second videos from this subset and use 90\%/10\% as the train/test split.
%
A similar filtering strategy is applied to obtain 26,858 videos for training.
%
We conduct experiments in Sec.~\ref{subsubsec:generalization} on this dataset to showcase the generalization ability of our method.
%
% since it is newly released, we select its ``Bristol" subset since it contains diverse scenarios such as entertainment, sports, commuting, and more.
%
% The sounds are also more diverse and complicated in this dataset. 
%
% Therefore, it's challenging to train on this dataset.
%
% We provide a generalization study in Sec.~\ref{subsubsec:generalization} to showcase the effectiveness of our localization network.
%
% However, randomly cutting video segments directly from the raw video is inferior, as it is noisy and tend to cover silent contents.
% %
% To mitigate this, we utilize the action temporal boundary label to extract video clips that contain human actions.
% %
% As shown in Section~\ref{subsec:motivation}, human-objects interaction is very likely to associate with sounds.
% %
% Thus, such training data is more clean and reliable for learning audio-visual correlations.
%
% We extract these video clips from the original action recognition training split.
%
% This results in 62413 training videos in total.
\\
\textbf{Evaluation metric.}
% To quantitatively evaluate the localization performance of our method, we employ both box-level and pixel-level metrics. 
%
% For the box-level measurement, we generate bounding boxes of sounding objects by applying Otsu's threshold and contour detection algorithm to the audio-visual attention map $S$.
%
% Then we compare the Intersection over Union (IoU) between the predicted and ground truth boxes and treat each box as an accurate prediction if IoU($pred$, $gt$)$>\theta$.
%
% Different from objects in third-person view videos, the objects in egocentric videos may be partially occluded or captured from usual viewpoints.
%
% Thus the scale of objects could be smaller than its real size.
%
% To give a more expressive quantitative evaluation, we set $\theta=0.3$ in our experiments.
%
% We then report the result on F-score@0.3 by considering both the precision and recall of box predictions.
%
% For pixel-level evaluation, we follow the prior works~\cite{qian2020multiple,chen2021localizing,li2021space} to adopt Consensus Intersection over Union (cIoU) and Area Under Curve (AUC).
%
% In practice, we report cIoU@0.3 and AUC as well.
%
We follow the prior works~\cite{hu2022mix,chen2021localizing,li2021space} and adopt the pixel-level measurement for evaluating localization performance.  
%
Given the ground truth sounding object bounding boxes, we compute the Consensus Intersection over Union (CIoU) and Area Under Curve (AUC) between the predicted localization map and ground truth boxes. %
%
We report CIoU over a range of thresholds to expose the finer aspects of comparison.
%
% Additionally, we also use pixel-wise average precision (AP)~\cite{choe2020evaluating,hu2022mix} to provide threshold-invariant judgment on the methods.
\\
\textbf{Implementation details.}
To facilitate the training, we cut a 1-second long video around the center of each raw video.
% 
We select the middle frame from the video clip and its four neighboring frames with an interval of 2 between frames.
%
Consequently, we get $T=5$ frames as visual input.
%
During training, the frames are first resized to 256$\times$256 and then randomly cropped to $224\times224$. 
%
During inference, all the frames are directly resized to the desired size without cropping.
%
For the audio stream, we extract the corresponding 1-second audio clip to create the audio-visual pairs.
%
The audio waveform is sub-sampled at 11kHz and transformed into a spectrogram with a Hann window of size 254 and a hop length of 64.
%
The obtained spectrogram is subsequently resampled to $128\times128$ to feed into the audio network. We set the number of audio and visual feature channels as 512 and choose $\epsilon=0.5$ and $\tau=0.03$. 
%
All models are trained with the Adam optimizer, with a learning rate of $10^{-4}$ on the visual encoder and temporal network, while using a learning rate of $10^{-2}$ for updating the audio encoder. 

% \begin{table}[t]
% \setlength{\tabcolsep}{2mm}
%     \centering
%     \begin{tabular}{l|ccc}
%         \toprule
%         Method & F-score@0.3 & cIoU@0.3 & AUC  \\
%         \midrule
%         Senocak \emph{et al.}~\cite{senocak2018learning} & 0.052 & 0.036 & 0.091 \\
%         Li~\emph{et al.}~\cite{li2021space} & 0.109 & 0.140 & 0.144 \\
%         Afouras~\emph{et al.}~\cite{afouras2020self} & 0.131 & 0.159 & 0.151 \\
%         Chen~\emph{et al.}~\cite{chen2021localizing} & 0.136 & 0.179 & 0.169 \\
%         Hu~\emph{et al.}~\cite{hu2022mix} & - & - & - \\
%         Song~\emph{et al.}~\cite{song2022self} & - & - & - \\
%         Ours & \textbf{0.294} & \textbf{0.337} & \textbf{0.235} \\
%         \bottomrule
%     \end{tabular}
%     \caption{Quantitative comparison of localization results on \textit{Epic Sounding Object} dataset. All methods are trained on Epic-Kitchen. The evaluation on F-score@0.3, cIoU@0.3 and AUC are reported. The top-1 results are highlighted.}
%     \label{tab:major}
% \end{table}

% \begin{table}[t]
% \centering
% \scalebox{0.92}{
% \begin{tabular}{l||ccccc}
% \toprule
% \multirow{2}{*}{}            & \multicolumn{1}{|c|}{\multirow{2}{*}{AP}} & \multicolumn{3}{c|}{CIoU}                 &   \multicolumn{1}{c}{\multirow{2}{*}{AUC}}    \\ \cline{3-5}
%                              & \multicolumn{1}{|c|}{}                    & @0.2  & @0.3  & \multicolumn{1}{c|}{@0.4} &    \\ \midrule
% \multicolumn{1}{l||}{Senocak~\emph{et al.}~\cite{senocak2018learning}} &  6.23                                        & 7.12      & -      &     -                      & 6.42      \\
% \multicolumn{1}{l||}{STM~\cite{li2021space}}      &  8.31                                        & 12.10      & 7.64      & 4.01                       & 8.87      \\
% \multicolumn{1}{l||}{Hardway~\cite{chen2021localizing}} & 13.41                                    & 24.51 & 13.55 & 6.10                      & 13.38 \\
% \multicolumn{1}{l||}{SSPL~\cite{song2022self}}    & 11.47                                         & 13.62      & 8.10      &  4.45                         &  9.56     \\
% \multicolumn{1}{l||}{Mix~\cite{hu2022mix}}      & 14.10                                    & 26.01      &  15.25     &  9.90                         &  15.39     \\
% \hline
% \multicolumn{1}{l||}{Our}     &  \textbf{15.02}                                   &  \textbf{38.71}     &\textbf{ 19.42}      & \textbf{10.51  }                        & \textbf{18.38}     \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-2mm}
% \caption{Quantitative comparison of localization results on \textit{Epic Sounding Object} dataset. All methods are trained on Epic-Kitchen. The results of metrics AP, CIoU@\{0.2, 0.3, 0.4\} and AUC are reported. The top-1 results are highlighted.}
% \vspace{-5mm}
% \label{tab:major}
% \end{table}

% \begin{table}[t]
% \centering
% \small
% \scalebox{0.92}{
% \begin{tabular}{l||cccc}
% \toprule
% \multirow{2}{*}{}             & \multicolumn{3}{c|}{CIoU}                 &   \multicolumn{1}{c}{\multirow{2}{*}{AUC}}    \\ \cline{2-4}
%                                                & @0.2  & @0.3  & \multicolumn{1}{c|}{@0.4} &    \\ \midrule
% \multicolumn{1}{l||}{Senocak~\emph{et al.}~\cite{senocak2018learning}}                & 7.12  (+443\%)     & -      &     -                      & 6.42 (+186\%)     \\
% \multicolumn{1}{l||}{STM~\cite{li2021space}}                                          & 12.10 (+220\%)     & 7.64 (+154\%)     & 4.01 (+162\%)         & 8.87 (+107\%)      \\
% \multicolumn{1}{l||}{Hardway~\cite{chen2021localizing}}                                   & 24.51 (+57.9\%) & 13.55 (+43.3\%) & 6.10  (+43.3\%)        & 13.38 (+37.4\%) \\
% \multicolumn{1}{l||}{SSPL~\cite{song2022self}}                                            & 13.62  (+184\%)    & 8.10 (+140\%)     &  4.45 (+72.3\%)            &  9.56  (+92.3\%)   \\
% \multicolumn{1}{l||}{Mix~\cite{hu2022mix}}                                         & 26.01(+48.8\%)      &  15.25 (+27.3\%)    &  9.90  (+6.16\%)             &  15.39  (+19.4\%)   \\
% \hline
% \multicolumn{1}{l||}{Our}                                       &  \textbf{38.71}     &\textbf{ 19.42}      & \textbf{10.51  }                        & \textbf{18.38}     \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-2mm}
% \caption{Quantitative comparison of localization results on \textit{Epic Sounding Object} dataset. All methods are re-trained on Epic-Kitchen. The results of metrics CIoU@\{0.2, 0.3, 0.4\} and AUC are reported. The top-1 results are highlighted.}
% \vspace{-7mm}
% \label{tab:major}
% \end{table}


\begin{table}[t]
\centering
% \small
% \large
\scalebox{1.}{
\begin{tabular}{l||ccc|c}
\toprule
\multirow{2}{*}{}             & \multicolumn{3}{c|}{CIoU}                 &   \multicolumn{1}{c}{\multirow{2}{*}{AUC}}    \\ \cline{2-4}
                                               & @0.2  & @0.3  & \multicolumn{1}{c|}{@0.4} &    \\ \midrule
\multicolumn{1}{l||}{Attention~\cite{senocak2018learning}}                & 7.12      & -      &     -                      & 6.42     \\
\multicolumn{1}{l||}{STM~\cite{li2021space}}                                          & 12.10      & 7.64      & 4.01        & 8.87       \\
\multicolumn{1}{l||}{Hardway~\cite{chen2021localizing}}                                   & 24.51  & 13.55  & 6.10        & 13.38  \\
\multicolumn{1}{l||}{SSPL~\cite{song2022self}}                                            & 13.62      & 8.10      &  4.45            &  9.56     \\
\multicolumn{1}{l||}{Mix~\cite{hu2022mix}}                                         & 26.01     &  15.25    &  9.90              &  15.39     \\
\hline
\multicolumn{1}{l||}{Our}                                       &  \textbf{38.71}     &\textbf{ 19.42}      & \textbf{10.51  }                        & \textbf{18.38}     \\
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\caption{Quantitative comparison of localization results on \textit{Epic Sounding Object} dataset. All methods are re-trained on Epic-Kitchen. The results of metrics CIoU@\{0.2, 0.3, 0.4\} and AUC are reported. The top-1 results are highlighted.}
\vspace{-4mm}
\label{tab:major}
\end{table}


\subsection{Results}
\label{subsec:results}
% In Sec.~\ref{sec:expcomp}, we first compare with recent approaches on our \textit{Epic Sounding Object} dataset. %
% Then, we provide ablation analysis in Sec.~\ref{sec:ablation}.
% %
% Finally, we validate the generalization capability of our model in Sec.~\ref{subsubsec:generalization}.

\subsubsection{Experimental Comparison}
\label{sec:expcomp}
%
To validate the effectiveness of our framework, we compare it with recent audio-visual localization methods: Attention~\cite{senocak2018learning}, STM~\cite{li2021space}, Hardway~\cite{chen2021localizing}, SSPL~\cite{song2022self} and Mix~\cite{hu2022mix}.
%
Among all the comparative methods, STM~\cite{li2021space} utilizes weak labels for the training, while the other methods are trained with self-supervision.
We hence adjust STM~\cite{li2021space} with our self-supervised localization loss.
%
As all the methods are developed for third-person view videos, we retrain their methods on our training data for a fair comparison.
%%
The quantitative results are shown in Tab.~\ref{tab:major}.
%
We can find that our method outperforms all the compared approaches by a large margin in all metrics, indicating the benefits of mitigating out-of-view sounds and explicitly modeling egomotion in learning egocentric audio-visual localization.
%
Moreover, we provide a qualitative comparison to visually showcase our localization results.
%
In Fig.~\ref{fig:viz}, we can see that our model produces localization results that are tight around the ground truth sounding objects.
%

\begin{table}[!t]
\setlength{\tabcolsep}{1.1mm}
    \centering
    \begin{tabular}{cccccc||cc}
        \toprule
        \multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{3}{c}{TM} & \multicolumn{1}{c}{\multirow{2}{*}{SL}} & \multicolumn{1}{c||}{\multirow{2}{*}{$L_{dis}$}} & \multicolumn{1}{c}{\multirow{2}{*}{CIoU@0.2}} & \multicolumn{1}{c}{\multirow{2}{*}{AUC}}  \\ \cline{2-4}
        \multicolumn{1}{c}{} & Avg  & Max  & \multicolumn{1}{c}{GA} & & \multicolumn{1}{c||}{}   \\ 
        % Model & AvgTM & MaxTM & GATM & SL & $L_{dis}$  & CIoU@0.2 & AUC  \\
        \midrule
         a & & & & &  & 27.41 & 15.36 \\
         b & \checkmark& &&&  & 31.84 & 15.79 \\
         c & &\checkmark &&&  & 33.29 & 16.10 \\
         d & & &\checkmark&&  & 37.38 & 16.59 \\
        f &&&  \checkmark&\checkmark &  & 38.21 & 17.92 \\
        g &&&  \checkmark&\checkmark & \checkmark   & \textbf{38.71} & \textbf{18.38} \\
        \bottomrule
    \end{tabular}
    \vspace{-2mm}
    \caption{Ablations on GATM, SL, and audio disentanglement module. The top-1 result in each column is highlighted. 
    % \yt{remove AP results?}
    }
    \vspace{-1em}
    \label{tab:ablation}
\end{table}

\vspace{-2mm}
\subsubsection{Ablation Study}
\label{sec:ablation}
\vspace{-2mm}
We conduct an ablation study to illustrate how each module affects localization performance. As shown in Tab.~\ref{tab:ablation}, we compare our full model with different baselines --- \textbf{model a}: we remove all the modules and only use the features from the visual and audio encoders to compute the localization map; \textbf{model b-d}: we insert \textit{Average}, \textit{Max}, and \textit{Geometry-Aware} Temporal modeling approaches separately in the framework; in \textbf{model f}, we incorporate the soft localization (SL) into the pipeline; and in \textbf{model g}, we employ the audio disentanglement module and train the model with $L_{dis}$.
% \yt{adding another baseline without GATM.}
%
By comparing \textbf{a} and \textbf{b-c}, we found that it's crucial to aggregate temporal context, while \textbf{d} emphasizes the importance of GATM in mitigating the egomotion in egocentric videos.
\textbf{model d} vs. \textbf{f} shows that SL slightly enhances the performance since some of the unrelated visual content can be reweighted. 
%
The comparison between \textbf{f} and \textbf{g} demonstrates that by incorporating audio feature disentanglement, the localization performance can be further boosted because it can handle the out-of-view sounds in videos. 
%

\noindent
\textbf{Naive baseline.} To assess the difficulty of the task, we provide a center box method that predicts a gaussian heatmap around the center. This results in a naive baseline of \textbf{16.51} compared to \textbf{27.41 (model a)} from \cref{tab:ablation}, showing that there are various challenging scenarios apart from the object being in the center that this naive baseline cannot capture. 

% \begin{table}[!t]
% \setlength{\tabcolsep}{1.5mm}
%     \centering
%     \begin{tabular}{cccc|ccc}
%         \toprule
%         Model & GATM & SL & $L_{dis}$ & AP & CIoU@0.2 & AUC  \\
%         \midrule
%          a & & & & 14.51 & 27.41 & 15.36 \\
%          b & \checkmark& & & 14.66 & 37.38 & 16.59 \\
%         c &  \checkmark&\checkmark & & 14.78 & 38.21 & 17.92 \\
%         d &  \checkmark&\checkmark & \checkmark & 15.02  & 38.71 & 18.38 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Ablations study on GATM, Soft Localization (SL), and audio disentanglement module. The top-1 result in each column is highlighted. \yt{remove AP results?}}
%     \label{tab:ablation}
% \end{table}



% \begin{table}[!t]
% \centering
% \setlength{\tabcolsep}{1.5mm}
% \begin{tabular}{l|ccc}
% \toprule
% \multicolumn{1}{l|}{model} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{CIoU@0.2} & \multicolumn{1}{c}{AUC} \\ \hline
% Baseline                   &    14.51                    &    27.41            &    15.36                     \\ \hline
% + GATM *                   &    14.22                    &    33.29            &    16.59                    \\
% + GATM                     &    14.66                    &      37.38                   &      17.88                    \\ \hline
% + SL            &          14.78              &      38.21                    &   17.92                      \\ \hline
% + $L_{dis}$          &      15.23                 &    -                  &           -             \\

% \bottomrule
% \end{tabular}
%     \caption{Ablations study on soft localization and audio feature enhancement module. The top-1 result in each column is highlighted.}
%     \label{tab:ablation}
% \end{table}

% \subsubsection{Analysis on audio disentanglement.}
% \yt{Move this subsection to appendix since the improvements are relatively small.}
% \begin{table}[t]
% \setlength{\tabcolsep}{2mm}
%     \centering
%     \begin{tabular}{c|c|ccc}
%         \toprule
%         Model & Input  & @0.2 & @0.3 & @0.4   \\
%         \midrule
%         w/o $L_{dis}$ & $s^{(1)}+s^{(2)}$ & 38.15 & 18.60 & 10.42 \\
%         w/ $L_{dis}$ & $s^{(1)}+s^{(2)}$ & 38.70 & 19.42 & 10.51 \\
%         w/o $L_{dis}$ & $s^{(1)}$ & 38.21 & 18.67 & 10.42 \\
%         w/ $L_{dis}$ & $s^{(1)}$ & 38.71 & 19.42 & 10.51 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Analysis of our model's performance by controlling the out-of-view sound. We compare two models (trained w/ and w/o $L_{dis}$) and report results over CIoU@\{0.2, 0.3, 0.4\}. 
%     % \yt{show \% results.}
%     }
%     \label{tab:noise}
% \end{table}

% To further investigate the issue of out-of-view sounds in egocentric videos, we experimented with analyzing the performance of our model when dealing with mixed audio.
% %
% We compare two different models, one is trained with disentanglement loss $\mathcal{L}_{ids}$ while the other does not.
% %
% During inference, we generate the audio input by mixing the original audio $s^{(1)}$ with an audio clip $s^{(2)}$ from another video in the dataset.
% %
% The corresponding results are shown in Table~\ref{tab:noise}.
% %
% We found that with audio feature disentanglement, our model can consistently yield good performance for both mixed and original audio inputs.
% %
% A more detailed study will be presented in the Appendix.
% %


\vspace{-2mm}
\subsubsection{Generalization to More Scenarios}
\vspace{-2mm}
\label{subsubsec:generalization}
The experiments on \textit{Epic Sounding Obejct} dataset demonstrate the effectiveness of our method in localizing sounding objects in the egocentric videos.
%
To further validate the generalization ability of our method, we train our audio-visual sounding object localization network on Ego4D~\cite{grauman2021ego4d} and qualitatively showcase the localization results in Fig.~\ref{fig:ego4d}.
%
The examples are all selected from the Ego4D test set.
%
We can see that our model can learn audio-visual associations and localize the sounding objects in diverse scenes.
% \noindent
% \ytian{I think it would be helpful if we can provides visual results on consecutive video sequences. Try to pick some examples with unseen sounds and temporally asynchronous audio-visual content.}

\begin{figure}[!hbpt]
    \centering
    \includegraphics[width=0.47\textwidth]{images/ego4d_resized.png}
    % \vspace{-4mm}
    \caption{Localization results on diverse scenarios in Ego4D~\cite{grauman2021ego4d}.
    % including both indoor and outdoor activities, such as playing musical instruments, gardening, and walking.
    Ref.: Sounding objects; Pred.: predicted  localization results. 
    % \yt{the piano-book example is not good. the book does not make sound.} 
    }
    \label{fig:ego4d}
    % \vspace{-5mm}
\end{figure}