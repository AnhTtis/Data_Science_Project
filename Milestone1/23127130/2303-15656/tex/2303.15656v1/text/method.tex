\section{Methodology}

In this section, we first formulate the problem. We then describe the single-task learning baseline (i.e., Neural Network) for predicting each adverse neonatal outcome. Finally, we present the proposed multi-task learning framework for multiple outcome predictions.


\subsection{Problem Formulation}
We formulate the prediction of adverse neonatal outcomes as a multi-task learning problem. The input feature ${\bx}_i \in \mathbb{R}^{1\times D}$ consists of information about the infant, its parents,  delivery room, nutrition summary, etc. $D$ denotes the number of input features, and $x_i$ denotes the $i\text{-th}$ sample in the dataset. $\by = \{\by^{(j)}\}_{j=1}^{M}$ refers to the ground truth of different adverse neonatal outcomes, where $M$ indicates the total number of adverse neonatal outcomes and $\by^{(j)}$ is the $j\text{-th}$ outcome. For the MTL framework, we define the shared hidden layers as $f(\cdot; \theta)$ transferring input features $\bx$ into latent features $f(\bx; \theta)$, where $\theta$ denotes the learnable parameters. Following the shared hidden layers, $M$ task-specific branches $\{g^{(j)}(\cdot; \theta^{(j)})\}_{j=1}^{M}$ transfer the shared latent features into the final predictions $\hat{\by}_i$ for different neonatal outcomes:
\begin{equation}
    \hat{\by}_i=\{g^{(j)}(f(\bx_i; \theta); \theta^{(j)})\}_{j=1}^{M},
\end{equation}
where $\hat{\by}_i$ denotes the prediction of the $i$-th sample with $M$ outcomes, and $\theta^{(j)}$ represents the parameters of the $j$-th task-specific branch.

\subsection{Single-Task Learning Baseline}
As shown in Fig.~\ref{fig:single}, we employ a Neural Network with multiple hidden layers as our backbone. In particular, the neural network $\text{NN}(\cdot; \theta)$ converts an input sample $\bx_i$ with $D$ input attributes into a prediction $\hat{\by}_i^{(j)}$ for the $j$-th neonatal outcome. 


For the classification task, we obtain the prediction scores $\hat{\by}^{(j)}$ using a softmax function. After that, we use the cross-entropy loss to optimize the classifier:
 \vspace{-1mm}
\begin{equation}
    \label{eq:cls}\mathcal{L}_\mathtt{cls}^{(j)} = -\frac{1}{N} \sum^{N}_{i=1} \by_i^{(j)} \cdot \log(\hat{\by}_i^{(j)}),
\end{equation}
 \vspace{-2mm}

where $\by_i^{(j)}$ denotes the ground truth of the $i\text{-th}$ training sample for the $j\text{-th}$ task, and $N$ denotes the batch size.

For the regression task, we use the mean squared error (MSE) loss to optimize the regression model:
 \vspace{-1mm}
\begin{equation}
    \label{eq:reg}
    \mathcal{L}_\mathtt{reg}^{(j)} = \frac{1}{N}\sum^{N}_{i=1} \Vert \by_i^{(j)} - \hat{\by}_i^{(j)} \Vert^2_2.
\end{equation}
 \vspace{-2mm}



\input{tables/mtl_exp}

\subsection{Multi-Task Learning Framework}

Although previous ML-based works have shown the effectiveness of the single-task learning framework, such a framework might ignore the potential correlations between different outcomes, leading to suboptimal results. Moreover, single-task learning models are more likely to suffer from overfitting issues, especially in cases with limited training data. 
As shown in Fig.~\ref{fig:multi}, we propose a novel multi-task learning framework to leverage the potential correlation between various adverse neonatal outcomes and avoid the overfitting problem. Technically, the proposed multi-task learning framework consists of shared hidden layers and multiple task-specific branches. Each task-specific branch contains several hidden layers and a prediction layer. The overall objective $\mathcal{L}_\mathtt{mtl}$ of the MTL framework is computed by combining the weighted losses of multiple tasks:
\begin{equation}
    \mathcal{L}_\mathtt{mtl} = \sum^{M}_{j=1} \lambda^{(j)} \mathcal{L}_\mathtt{stl}^{(j)},
\end{equation}
where $\mathcal{L}_\mathtt{stl}^{(j)}$ denotes the $j$-th task-specific objective with the loss weight $\lambda^{(j)}$, and $M$ is the number of tasks.

Overall, the proposed MTL framework can exploit the correlations among neonatal outcomes since by using multi-task learning, the framework is better equipped to capture the general patterns that are relevant to all tasks. These patterns would promote the generalization ability and prevent ML-based methods from overfitting~\cite{zhang2021survey,ruder2017overview}. Meanwhile, the MTL framework can enhance the performance of a specific primary task by using related tasks as auxiliary tasks~\cite{liebel2018auxiliary}, which provide auxiliary information for the primary task, resulting in a more efficient and effective learning process.
