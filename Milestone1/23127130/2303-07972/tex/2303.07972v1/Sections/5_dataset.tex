\section{Dataset}\label{sec:dataset}

Training \methodname{} requires a dataset of successful grasps where the approach direction is labeled according to the discrete bins of $\mathrm{SO}(3)$. However, due to the discretization of $\mathrm{SO}(3)$ using the Euler angles, our dataset depends on which coordinate system we express the grasps in. Therefore, in this section, we first discuss the choice of the coordinate system and then introduce the dataset.    

\subsection{Camera-Centric Coordinate System}

Prior \ac{dl}-based grasping works has generally not discussed the coordinate system used for expressing their training data. The reason is that there is no need for such a discussion as the training data can seamlessly be transformed from one coordinate system to another without influencing the grasp labels. Unfortunately, this is not the case in our work, as the labels C are coordinate-specific.

Two commonly used coordinate systems for expressing the grasp poses are the global and camera-centric coordinate systems. We choose to express the grasp poses in the camera-centric coordinate system because of two benefits. Firstly, in a camera-centric coordinate system, the object \pc{} is always aligned with the camera axes. Secondly, the camera-centric coordinate system enables a human-like way of communicating the grasp approach direction. For instance, with the camera-centric coordinate system, we can express grasp approach directions similar to a human from the left, right, above, below, behind, or in front of the object. Neither of these benefits applies when using a global coordinate frame unless the global coordinate system coincides with the camera coordinate system, which is rarely the case. 

\subsection{The Approach-Constrained Grasping Dataset}

To train \methodname{}, we need a large-scale grasping dataset containing object \pcs{} $\matr{O}$ and successful grasps $\matr{G}$ where each grasp $\mathbf{g} \in \matr{G}$ has a specific constraint label C depending on its yaw and pitch angle. To date, no such grasping dataset exists. Still, instead of generating and labeling a completely new grasping dataset, we convert the grasps in the already established large-scale Acronym dataset~\cite{eppner2021acronym} that consists of 17.7 million simulated parallel-jaw grasps on 8872 objects from ShapeNet~\cite{chang2015shapenet} to fit our needs.

The steps for curating our dataset from the Acronym data are similar to the one presented in \cite{lundell2023constrained}, but with some key differences stemming from the need to discretize the space of rotations and label grasps accordingly. In detail, the steps we follow to create the dataset used in this work are:

\begin{enumerate}[label=(\roman*)]
    \item Discretize the space of rotations $\mathrm{SO}(3)$ into B bins using \eqref{eq:descritization}.
    \item For each object, randomize 100 camera poses pointing toward the object and render a \pc{} $\matr{O} \in \mathbb{R}^{\text{N}\times 3}$ from each pose.
    \item Transform 1000 random grasps $\matr{G}$ on each object from Acronym to the camera-centric coordinate system and then label them according to one of the B bins using \eqref{eq:vec_to_euler}.
\end{enumerate}
The above process was carried out on all the 8872 Acronym objects. From the final dataset, 112 random objects were held out for evaluation while the rest were used for training \methodname{}.

