\section{Method}\label{sec:method}

As formalized in the previous section, two \acp{dnn} are required: a generative grasp sampler and a grasp discriminator. Next, we describe, in detail, \methodname{} our novel generative approach-constrained grasp sampling network and the automatic \ac{pc}-based bin selection process. Finally, we introduce the grasp discriminator from \cite{mousavian20196} for completeness but refer the reader to the original work in \cite{mousavian20196} for specific details.

% \begin{figure*}
% \centering
% \begin{minipage}{.48\textwidth}
%   \centering
%   \captionsetup{margin=0.25cm}
%         \includegraphics[width=0.55\linewidth]{figures/yaw_pitch_roll_obj.png}
%     \captionof{figure}{The camera-centric grasp orientation representation. Here, $\Vec{a}$ is the gasp approach direction, $\alpha$ the roll angle, $\beta$ the pitch angle, and $\gamma$ the yaw angle, all represented in the camera coordinate system. 
%     }
%   \label{fig:yaw-pitch-cam}
% \end{minipage}%
% \begin{minipage}{.48\textwidth}
%   \centering
%   \captionsetup{margin=0.25cm}
%         \includegraphics[width=1\linewidth]{figures/yaw_pitch_roll_cam_dist.png}
%         % \includesvg[width=1\linewidth]{figures/yaw_pitch_roll_cam_dist.svg}
%     \captionof{figure}{An example of discretizing $\mathrm{SO}(3)$ into 8 bins ($\text{N}_\beta=2$, $\text{N}_\gamma=4$). Yaw and pitch labels are generated for $\gamma$ and $\beta$ in \figref{fig:yaw-pitch-cam}.}
%     \label{fig:yaw-pitch-discretize}
% \end{minipage}
% \end{figure*}

\subsection{\methodname{}}\label{sec:generator}

\methodname{} is an approach-constrained deep generative grasp sampling network $\mathcal{Q}_{\boldsymbol{\theta}}(\matr{G}|\matr{O}, \text{C})$ that approximates $\prob{(\matr{G}|\matr{O}, \text{C})}$, where $\matr{O} \in \mathbb{R}^{\text{N}\times3}$ is the object \pc{} and $\text{C}\subset \mathrm{SO}(3)$ is a subset of the rotation group $\mathrm{SO}(3)$. As discussed in the previous section, the idea behind conditioning the network on $\text{C}$ is to constrain the approach vector $\Vec{a}$ or the pitch and yaw Euler angles to lie within some subset of $\mathrm{SO}(3)$.  

Unfortunately, designing \acp{dnn} that operates on sets is non-trivial. To alleviate this problem, we propose to discretize $\mathrm{SO}(3)$ into $\text{B}$ bins using the yaw and pitch of the Euler angles where each bin $\text{b}_{\text{i}} \in \text{B}$ is represented by a two-integer label $\text{b}_{\text{i}} = [\text{c}^{\text{i}}_{\beta},~\text{c}^{\text{i}}_{\gamma}] \in \mathbb{Z}_+^2$. The constraint $\text{C}$ then becomes one of the bins $\text{b}_\text{i} \in \text{B}$. Mathematically, the discretization is realized by first choosing a range of values for each of the two Euler angles pitch ($\beta \in [0,~\pi]$) and yaw ($\gamma \in [0,~2\pi]$). Next, these ranges are divided into equally spaced intervals of size $\text{N}_\beta$ and $\text{N}_\gamma$, respectively. Finally, a query Euler angle $\beta_\text{i}$ and $\gamma_\text{i}$ is mapped into the correct label $\text{b}_\text{i}=[\text{c}_\beta^{\text{i}},~\text{c}_\gamma^{\text{i}}]$ by:

\begin{equation}\label{eq:descritization}
\text{c}_{\beta}^{\text{i}} = \lfloor \frac{\text{N}_{\beta}\beta_{\text{i} }}{\pi} \rfloor,~\text{c}_{\gamma}^{\text{i}} = \lfloor \frac{\text{N}_{\gamma}\gamma_{\text{i}}}{2\pi} \rfloor.
\end{equation}
An example discretization using the above process is shown in \figref{fig:yaw-pitch-discretize}.


\begin{figure}[t]
\centering
  \centering
  \captionsetup{margin=0.25cm}
        \includegraphics[width=0.55\linewidth]{figures/yaw_pitch_roll_obj.png}
    \captionof{figure}{The camera-centric grasp orientation representation. Here, $\Vec{a}$ is the gasp approach direction, $\alpha$ the roll angle, $\beta$ the pitch angle, and $\gamma$ the yaw angle, all represented in the camera coordinate system. 
    }
    \vspace{-9pt}
  \label{fig:yaw-pitch-cam}
\end{figure}

Using the two-integer constraint $\text{C}$, we design \methodname{} similarly to \cite{mousavian20196,lundell2023constrained} as a \ac{cvae}~\cite{sohnLearningStructuredOutput2015} but with both $\matr{O}$ \textit{and} \text{C} as the conditional variables. The \ac{cvae} consists of an encoder $\mathsf{q}_{\boldsymbol{\zeta}}(\mathbf{z}\mid\matr{O}, \text{C}, \matr{G})$ and a decoder $\mathsf{p}_{\boldsymbol{\chi}}(\matr{G}\mid\matr{O},\text{C},\mathbf{z})$, where $\mathbf{z}\in \mathbb{R}^{\text{L}}$ is a latent vector of size $\text{L}$. As we want to operate directly on \pcs{}, we choose \pointnet{} as the backbone of \methodname{}. The input \pc{} $\matr{X}\in \mathbb{R}^{\text{N}\times (3+\text{K})}$ to the encoder consist of the object \pc{} $\matr{O} \in \mathbb{R}^{\text{N}\times 3 }$ with the constraint $\text{C}$ and the grasp pose $\matr{G}$ as the K additional point-wise features. The decoder uses the same input \pc{} as the encoder but with $\mathbf{z}$ instead of $\matr{G}$ as additional point-wise features. 

For optimizing the parameters of \methodname{} we use the standard \ac{vae} loss:

\begin{align}
\label{eq:vae_loss}
    \mathcal{L}_{\text{VAE}} = \mathcal{L}(\matr{G}^*,\hat{\matr{G}}) + \eta \mathcal{D}_{\text{KL}}[\mathsf{q}_{\zeta} (\mathbf{z}\mid\matr{O},\text{C},\matr{G}^*),~\mathcal{N}(\matr{0},\matr{I})],
\end{align}
where $\eta$ is a scalar, $\matr{G}^*$ is a ground truth stable grasp, and $\mathcal{D}_{\text{KL}}$ is the KL-divergence. Similarly to \cite{mousavian20196,lundell2023constrained} we define the reconstruction loss $\mathcal{L}(\matr{G}^*,\hat{\matr{G}})$ as
\begin{align}
    \mathcal{L}(\matr{G}^*,\hat{\matr{G}}) = \norm{\text{h}(\matr{G}^*)-\text{h}(\mathbf{\hat{\matr{G}}})}_1,
\end{align}
where $\matr{\hat{G}}$ is a generated grasp from the decoder $\mathsf{p}_{\chi}$, and $\text{h}: \mathbb{R}^{7} \rightarrow  \mathbb{R}^{6\times 3}$ is a function that maps a 7-dimensional grasp pose into a \pc{} representation of the gripper $\matr{P} \in \mathbb{R}^{6\times 3}$. The benefit of a point cloud representation for the gripper pose is to combine orientation and translation into one loss function.
% The gripper pose is mapped to a \pc{} because it combines the orientation and translation into one loss function.

Although both the encoder and the decoder are used for training \methodname{}, only the decoder is used for sampling grasps on an unknown \pc{}. Specifically, to generate M grasps on an unknown object \pc{} $\matr{O}$ with the approach direction constrained to a yaw and pitch angle of $\beta_{\text{j}}$ and $\gamma_{\text{j}}$ the first step is to sample M iid latent vectors from the zero mean Gaussian $\mathbf{z}_{0,\dots,\text{M}}\sim \mathcal{N}(\matr{0},\matr{I})$.  Next, the yaw and pitch angles are mapped into the two-integer bin value $\text{b}_j$ using \eqref{eq:descritization}. Finally, M copies of the object \pc{} are created, where each \pc{} copy $\matr{O}_{\text{i}}$ is concatenated with a unique latent vector $\mathbf{z}_{\text{i}}$ and the constraint labels $\text{b}_{\text{j}}$, and passed through the decoder to produce the M grasps. 

\subsection{\ac{pc}-based Bin Selection}\label{sec:pc_selection}
As described above, the main downside of \methodname{} is the need to a-priori specify the approach direction constraint $\text{C}$. Two options for specifying $\text{C}$ are to use a human or to learn it. Unfortunately, both of these options are expensive to realize. Therefore, we propose selecting bins that align with some easy-to-estimate latent features of the object's geometry. 

\begin{figure}[t]
  \centering
  \captionsetup{margin=0.25cm}
        \includegraphics[width=1\linewidth]{figures/yaw_pitch_roll_cam_dist.png}
        % \includesvg[width=1\linewidth]{figures/yaw_pitch_roll_cam_dist.svg}
    \captionof{figure}{An example of discretizing $\mathrm{SO}(3)$ into 8 bins ($\text{N}_\beta=2$, $\text{N}_\gamma=4$). Yaw and pitch labels are generated for $\gamma$ and $\beta$ in \figref{fig:yaw-pitch-cam}.}
    \vspace{-6pt}
    \label{fig:yaw-pitch-discretize}
\end{figure}

The features we use in this work are the \acp{pc} of the object's point cloud as these have been used in similar non-learning-based grasping works discussed in \secref{sec:geometrically_constrained_grasping}. To calculate the \acp{pc}, we ultilize the \ac{pca} using the covariance matrix $\Sigma_{\matr{O}} = \frac{1}{N}(\matr{O}-\bar{\matr{O}})^{T}(\matr{O}-\bar{\matr{O}})$ of the object point cloud $\matr{O}$ with the barycenter $\bar{\matr{O}}$. This results in three ordered eigenvalues $\lambda_1$, $\lambda_2$, $\lambda_3$ and corresponding eigenvectors, $\hat{v}_1$, $\hat{v}_2$, $\hat{v}_3$, where $\lambda_1 \geq \lambda_2 \geq \lambda_3$. The eigenvectors are also known as the \acs{pc}. Following prior work \cite{balasubramanian2012physical}, we select the second largest \ac{pc} $\hat{v}_2$ to constrain the approach direction. 

Due to the symmetry of the \acp{pc}, we need to find the bins that intersect with $\pm \hat{v}_2$. For that, we first calculate the roll, pitch, and yaw separately for $\pm \hat{v}_2$ using

\begin{equation}\label{eq:vec_to_euler}
\begin{split}
&\alpha \in [0, 2\pi], \\
&\gamma = \arctan (\hat{v}_{2,y}, \hat{v}_{2,x}) \in [0, 2\pi], \\
&\beta = \arccos (\hat{v}_{2,z}) \in [0,\pi], \\
\end{split}
\end{equation}
where $\hat{v}_{2,x}$, $\hat{v}_{2,y}$, and $\hat{v}_{2,z}$ represent the x-, y-, and z-component of the vector $\hat{v}_{2}$ respectively.
Then, for each $\pm \hat{v}_2$, the pitch ($\beta$) and yaw ($\gamma$) angles are mapped into the corresponding two-integer label using \eqref{eq:descritization}. These two two-integer labels are then used one after the other to generate grasps with the approach vector constrained to $\pm \hat{v}_2$.    

\subsection{Grasp Discriminator}

\methodname{} can generate unsuccessful grasps between modes because it is only trained on distributions of successful grasps \cite{mousavian20196}. To avoid executing poor grasps, we used the grasp discriminator introduced in \cite{mousavian20196} to score how likely the sampled grasps were to succeed.

Mathematically, the  grasp discriminator $\mathcal{D}_{\boldsymbol{\psi}}(\text{S}=1 | \matr{G}, \matr{O})$ is optimized to approximates $\prob{(\text{S}=1 | \matr{G}, \matr{O})}$. It is based on the same \pointnet{} architecture as \methodname{}. However, the input \pc{} $\matr{Y} \in \mathbb{R}^{(\text{N}+6)\times (3+1)}$ to the evaluator differs significantly from the generator in that the object \pc{} $\matr{O} \in \mathbb{R}^{N \times 3}$ is concatenated with a grasp \pc{} $\matr{K} \in \mathbb{R}^{6\times 3}$ and an additional one-dimensional binary feature is added to the input \pc{} $\matr{Y}$ to distinguish between the two \pcs{} $\matr{O}$ and $\matr{K}$. The grasp discriminator is optimized to distinguish between successful and unsuccessful grasps using the binary cross-entropy loss:
\begin{align}
\mathcal{L}_{E}=(-\text{S}^*\log(\text{S})+(1-\text{S}^*)\log(1-\text{S})),    
\end{align}
where $\text{S}^*$ is the ground-truth success of a grasp and $\text{S}$ is the predicted success. %\zehang{The generated grasps from the grasp sampler will be fed into the discriminator network, and ranked by their scores for execution.}

% To address orientation-constrained grasp synthesis, we propose a sampler model called OCVGS, derived from the vanilla 6-Dof GraspNet, which generates grasp poses for objects in 3D point clouds. The proposed model is based on a conditional variational autoencoder (CVAE), consisting of an encoder and a decoder module. Given an observed point cloud $X$, with an explicit orientation constraint $C_{ori}$, the decoder module acts as a grasp sampler for grasp generation. Besides, the evaluator from the vanilla GraspNet is adopted, for ranking the sampled grasps such that grasps with higher ranking scores will be chosen for execution.

%In this section, we present a novel approach for generating grasp poses for objects represented in 3D point clouds, which addresses the challenge of incorporating orientation constraints. Our method, referred to as OCVGS, defines a camera-centric orientation representation and utilizes a conditional variational autoencoder (CVAE) architecture, which is derived from the 6-DoF GraspNet model. Furthermore, a simple geometric-based algorithm based on principal component is introduced for the automatic orientation selection, which enables the generation of high-quality grasps.

%[concept figure here?]

% The CVAE architecture consists of an encoder and a decoder module, which are used to generate grasp poses. Given an observed point cloud $X$ and an explicit orientation constraint $C_{ori}$, the decoder module acts as a grasp sampler. Additionally, the evaluator from the vanilla GraspNet is utilized to rank the generated grasp poses, with those possessing the highest ranking scores being selected for execution.

% $(X, g, O)$

% $$P(G|X, O) = \int P(G|X,O,z;\theta)P(z)dz$$

% $$v_{a}'=T_{cam}(v_{a})$$
% $$X_{e} = [X, g, O], g \in \mathbf{R}^{7}, X\in \mathbf{R}^{N\times 3}, O \in \mathbf{Z}^{+2}$$


% $$Q  , P$$
% Z
% P(G | X, z; Î˜)P(z)dz

%\subsection{Camera-centric Grasp orientation Representation}




%A well-defined grasp orientation representation is necessary for constructing the constraint variable $C_{ori}$ in both the learning and inferring stages, which is not yet discussed for the data-driven method in the previous work. Traditional grasp orientation representation of a specific grasp is defined by its approaching vector $v_{a}$ and hand orientation angle $\phi$ in the world space which is not easy to transfer across different robotic systems and unseen objects. Instead, we choose to represent the grasp orientation in the camera space, which is called camera-centric grasp orientation representation in our method.

%The approaching vector $v_{a}$ is transformed into the local camera space and normalized as a unit directional vector. The grasp orientation is defined as a 3D $ZYX$ Euler angle vector $r_{g}=[r_{yaw}, r_{pitch}, r_{roll}]\in \mathbb{R}^{3}$ as follows:

%\begin{equation}\label{eq:cam-repre}
%\begin{split}
%&r_{yaw} = atan2 (v_{a,y}', v_{a,x}') \in [0, 2\pi] \\
%&r_{pitch} = arccos (v_{a,z}') \in [0,\pi] \\
%&r_{roll} = \phi \in [0, 2\pi] \\
%&v_{a}' = [v_{a,x}', v_{a,y}', v_{a,z}'] = \frac{T_{cam}(v_{a})}{||v_{a}||} \in \mathbb{R}^{3} 
%\end{split}
%\end{equation}

% $$r_{yaw} = atan2 (v_{a,y}', v_{a,x}') \in [0, 2\pi]$$
% $$r_{pitch} = arccos (v_{a,z}') \in [0,\pi]$$
% $$r_{roll} = \phi \in [0, 2\pi]$$
% $$v_{a}' = [v_{a,x}', v_{a,y}', v_{a,z}'] = \frac{T_{cam}(v_{a})}{||v_{a}||} \in \mathbb{R}^{3} $$

%where $T_{cam}$ is the camera transformation matrix.

%To embed the conditional vector for the GraspNet-like framework, we discretize $r_{g}$ in each dimension with pre-define resolution hyperparameters. The discretized constraint vector $C_{ori}$ is defined by three orientation labels:

%\begin{equation}\label{eq:descritization}
%\begin{split}
%C_{ori} &= [c_{yaw}, c_{pitch}, c_{roll}] \in Z^{+3} \\
%c_{yaw} = \lfloor \frac{N_{yaw}r_{yaw} }{2\pi} \rfloor&, c_{pitch} = \lfloor \frac{N_{pitch}r_{pitch} }{\pi} \rfloor, c_{roll} = \lfloor \frac{N_{roll}r_{roll} }{2\pi} \rfloor
%\end{split}
%\end{equation}

% $$C_{ori} = [c_{yaw}, c_{pitch}, c_{roll}] \in Z^{+3}$$
% $$c_{yaw} = \lfloor \frac{N_{yaw}r_{yaw} }{2\pi} \rfloor, c_{pitch} = \lfloor \frac{N_{pitch}r_{pitch} }{\pi} \rfloor, c_{roll} = \lfloor \frac{N_{roll}r_{roll} }{2\pi} \rfloor$$

%where $N_{yaw}$, $N_{pitch}$, $N_{roll}$ are resolutions for yaw, pitch, roll dimensions respectively. In the work, we give the network flexibility to decide the roll angle itself. By doing this, we only constrain how the gripper approaches the target object, such that $C_{ori}=[c_{yaw}, c_{pitch}] \in Z^{+2}$. \textcolor{blue}{To be mentioned, evenly splitting the yaw and pitch ranges into a fixed number of discrete values will not result in an equal distribution of surface area on a sphere. This is because the surface area of a sphere is not evenly distributed across all possible orientations.} 

%\subsection{Network Archietecture}

% We derive the sampler and evaluator model from the vanilla GraspNet structure with PointNet++ as the backbone. 
%We model the CVAE, evaluator network based on PointNet++. To utilize this framework with orientation constraint, we construct a feature point cloud based on the observed point cloud $O\in \mathbb{R}^{N\times 3}$, a successful grasp configuration $g\in \mathbb{R}^{7}$ and its associated constraint $C_{ori}$. We compute $O_{cam}$ and $g_{cam}$ by transforming $O,g$ into the camera space using $T_{cam}$, subtracting the point cloud center.

% $O$ and $g$ are transformed into the camera space using $T_{cam}$, and normalized by the point cloud center.

% $$O_{cam} = Centered(T_{cam}(O)) \in \mathbb{R}^{N\times 3}$$
% $$g_{cam} = Centered(T_{cam}(g)) \in \mathbb{R}^{7}$$

%Then, $g_{cam}$ and $C_{ori} \in Z^{+2}$ are stacked on each point in $O_{cam}$ to form a 12-D point cloud $X_{e}$ as the encoder input. The encoder $Q(z | X_{e})$ maps $X_{e}$ to a compact latent space with size $L$ so that we can sample $z \in \mathbb{R}^{L}$ from Q.

%Similarly, for the decoder, we combine the sampled latent vector $z$, the orientation constraint $C_{ori}$ and $O_{cam}$ to form a (5+L)-D point cloud $X_{d}$ as input. The decoder takes $X_{d}$ as input and generates a 7D grasp $g^{x} \in \mathbb{R}^{7}$. OCVGS is trained on a VAE loss by minimizing the KL divergence and the L1 grasp reconstructed error in the euclidean space. Please refer to the vanilla GraspNet paper for more details. [One small difference is that they use 16-D grasp representation for the encoder input, while 7-D representation for the decoder output. But here, we use 7-D representation for both networks]
%$$L_{VAE} = \lVert \Tau(g^{*}) - \Tau(g)) \rVert_{1} + \alpha D_{KL}(Q(z|X_e), N(0,I))$$

%where $\alpha$ is a scalar weight, $D_{KL}$ computes the KL divergence, and $\Tau$ maps the grasp into a point cloud representation with 6 grasp surface points.

%In this work, a grasp evaluator is used, which was first introduced in the vanilla GraspNet. It is based on the PointNet++ and takes a point cloud $X_{eval}\in \mathbb{R}^{(N+6)\times (3+1)}$ as input. Unlike the input point cloud used in CVAE, $X_{eval}$ is composed of the observed point cloud $O_{cam}$ combined with a grasp point cloud $P_{g} \in \mathbb{R}^{6 \times 3}$, and an additional binary feature to distinguish between the two point cloud items. The evaluator is optimized on the cross-entropy loss:

%$$L_{evaluator} = -(y\log (s) + (1-y)\log(1-s))$$

%where y is the binary ground-truth success label of the examined grasp. and s is the predicted probability of success from the evaluator network. During the inference stage, we feed the grasps from the CVAE decoder to the grasp evaluator and only execute grasps with higher success probability.

%\subsection{PCA-2 OCVGS Strategy}

%As previously mentioned, OCVGS allows for explicit orientation constraint labels to be inputted through human selection, making it useful in situations involving human-robot interaction. But in some cases, it is desirable for OCVGS to be able to generate high-quality grasps without humans in the loop. To address this, we extract geometric information from the observed point cloud which can guides generating orientation labels.

%As we aim for object-agnostic grasp synthesis, it is impossible to extract accurate shape and pose information from the self-occluded point cloud from a single camera view, which makes it difficult for the purpose of selecting orientations. Inspired by [the paper on analyzing human grasping strategy], we notice that grasp success is highly correlated to the principal axes. We propose a strategy called PCA-2 OCVGS such that the we generate orientation constraint according to the second principal component.

%This strategy aligns well with human intuition in real-world scenarios. When observing an object for grasping, it is typical to analyze the longest axis of the global object shape. The hand is then usually aligned such that the antipodal points lie in a plane perpendicular to this axis, with the second principal component preserving the largest amount of information and serving as the first principal component in this plane. This orientation selection offers two additional benefits from both a physics and geometry perspective. Firstly, the grasp perpendicular to the first principal axis results in low torque, thereby improving grasp stability. Secondly, it increases the chance of grasping the thin part of the object.

%Once we have obtained $O_{cam}$, we calculate the covariance matrix $\Sigma$ and derive the point cloud's abstract representation using the ordered principal component vectors $b_1$, $b_2$, $b_3$ and the corresponding eigenvalues $e_1$, $e_2$, $e_3$, such that $e_1>e_2>e_3$. For a given eigenvalue $e_{2}$, both $b_{2}$ and $-b_{2}$ comply with the rule of eigen-decomposition.

%$$\Sigma(\pm b_2) = e_2 (\pm b_2)$$

%[need a figure here to show principal components of new object point cloud]

%We utilize both $b_2$ and $-b_{2}$ to generate labels denoted as $C_{ori}^{+}$ and $C_{ori}^{-}$ respectively. Specifically, we compute the yaw and pitch angles of $b_2$ and $-b_{2}$ and their labels by \eqref{eq:descritization}. OCVGS will sample grasps in both approaching directions, the grasps are then ranked together by the evaluator quality score for execution. In the experiment section, we present details why we choose the second component instead of the other two.