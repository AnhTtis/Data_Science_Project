\section{Dataset}\label{sec:dataset}

Training \methodname{} requires a dataset of successful grasps where the approach direction is labeled according to the discrete bins of $\mathrm{SO}(3)$. However, due to the discretization of $\mathrm{SO}(3)$ using the Euler angles, our dataset depends on which coordinate system we express the grasps in. Therefore, in this section, we first discuss the choice of the coordinate system and then introduce the dataset.    

\subsection{Camera-Centric Coordinate System}

Prior \ac{dl}-based grasping works have generally not discussed the coordinate system used for expressing their training data. The reason is that there is no need for such a discussion, as the training data can seamlessly be transformed from one coordinate system to another without influencing the grasp labels. Unfortunately, this is not the case in our work, as the labels C are coordinate-specific.

Two commonly used coordinate systems for expressing the grasp poses are the global and camera-centric coordinate systems. We choose to express the grasp poses in the camera-centric coordinate system because of two benefits. Firstly, in a camera-centric coordinate system, the object \pc{} is always aligned with the camera axes. Secondly, the camera-centric coordinate system enables a human-like way of communicating the grasp approach direction. For instance, with the camera-centric coordinate system, we can express grasp approach directions similar to a human from the left, right, above, below, behind, or in front of the object. Neither of these benefits applies when using a global coordinate frame unless the global coordinate system coincides with the camera coordinate system, which is rarely the case. 

%Two commonly used coordinate systems for expressing grasp poses are the global and camera-centric coordinate systems. We choose to express the approach constrained grasps in the camera-centric coordinate system because of two benefits. Firstly, in the case of expressing the point clouds, this choice enables the learning-based framework to implicitly take advantage of the perspective information provided by the camera while the visible regions of point clouds always face the camera. Additionally, camera coordinate system using a frame of reference that is relative to the viewpoint or sensor position can make it easier to communicate orientations naturally. This is similar to the frame of reference used by humans to describe grasp orientations (e.g. left, right, top, down, front and back), which is based on the position of their own bodies and eyes. In contrast, expressing point clouds and grasp orientation in the global world coordinate system can be more complex and less intuitive, as this frame of reference is defined based on a fixed set of axes and a point of origin that is not relative to any particular viewpoint or sensor position.

% A well-defined grasp orientation representation is necessary for constructing the constraint variable $C_{ori}$ in both the learning and inferring stages, which is not yet discussed for the data-driven method in the previous work. Traditional grasp orientation representation of a specific grasp is defined by its approaching vector $v_{a}$ and hand orientation angle $\phi$ in the world space which is not easy to transfer across different robotic systems and unseen objects. Instead, we choose to represent the grasp orientation in the camera space, which is called camera-centric grasp orientation representation in our method.

% The approaching vector $v_{a}$ is transformed into the local camera space and normalized as a unit directional vector. The grasp orientation is defined as a 3D $ZYX$ Euler angle vector $r_{g}=[r_{yaw}, r_{pitch}, r_{roll}]\in \mathbb{R}^{3}$ as follows:

\subsection{The Approach-Constrained Grasping Dataset}

To train \methodname{}, we need a large-scale grasping dataset containing object \pcs{} $\matr{O}$ and successful grasps $\matr{G}$ where each grasp $\mathbf{g} \in \matr{G}$ has a specific constraint label C depending on its yaw and pitch angle. To date, no such grasping dataset exists. Still, instead of generating and labeling a completely new grasping dataset, we convert the grasps in the already established large-scale Acronym dataset~\cite{eppner2021acronym} that consists of 17.7 million simulated parallel-jaw grasps on 8872 objects from ShapeNet~\cite{chang2015shapenet} to fit our needs.

The steps for curating our dataset from the Acronym data are similar to the one presented in \cite{lundell2023constrained}, but with some key differences stemming from the need to discretize the space of rotations and label grasps accordingly. In detail, the steps we follow to create the dataset used in this work are:

\begin{enumerate}[label=(\roman*)]
    \item Discretize the space of rotations $\mathrm{SO}(3)$ into B bins using \eqref{eq:descritization}.
    \item For each object, randomize 100 camera poses pointing toward the object and render a \pc{} $\matr{O} \in \mathbb{R}^{\text{N}\times 3}$ from each pose as done in \cite{lundell2023constrained}.
    \item Transform 1000 random grasps $\matr{G}$ on each object from Acronym to the camera-centric coordinate system and then label them according to one of the B bins using \eqref{eq:vec_to_euler}.
\end{enumerate}
The above process was carried out on all the 8872 Acronym objects. From the final dataset, 112 random objects were held out for evaluation while the rest were used for training \methodname{}.

%from the Acronym dataset not seen during training will be selected and initialized as the free-floating state in the simulation environment for grasping.
%During the learning phase, we generate partially observed point clouds and downsample them to a resolution of 1024 for objects viewed from up to 100 distinct camera perspectives. Furthermore, we select 1000 Acronym successful grasps for each object and transform them into the camera-centric coordinate system, following by labeling them according to equation ~\eqref{eq:cam-repre}~\eqref{eq:descritization}

% them into camera-centric representations to compute the yaw and pitch labels as constraints, according to equation ~\eqref{eq:cam-repre}~\eqref{eq:descritization}.

