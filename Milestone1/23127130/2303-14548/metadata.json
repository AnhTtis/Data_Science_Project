{
    "arxiv_id": "2303.14548",
    "paper_title": "Viewpoint Equivariance for Multi-View 3D Object Detection",
    "authors": [
        "Dian Chen",
        "Jie Li",
        "Vitor Guizilini",
        "Rares Ambrus",
        "Adrien Gaidon"
    ],
    "submission_date": "2023-03-25",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
    ],
    "abstract": "3D object detection from visual sensors is a cornerstone capability of robotic systems. State-of-the-art methods focus on reasoning and decoding object bounding boxes from multi-view camera input. In this work we gain intuition from the integral role of multi-view consistency in 3D scene understanding and geometric learning. To this end, we introduce VEDet, a novel 3D object detection framework that exploits 3D multi-view geometry to improve localization through viewpoint awareness and equivariance. VEDet leverages a query-based transformer architecture and encodes the 3D scene by augmenting image features with positional encodings from their 3D perspective geometry. We design view-conditioned queries at the output level, which enables the generation of multiple virtual frames during training to learn viewpoint equivariance by enforcing multi-view consistency. The multi-view geometry injected at the input level as positional encodings and regularized at the loss level provides rich geometric cues for 3D object detection, leading to state-of-the-art performance on the nuScenes benchmark. The code and model are made available at https://github.com/TRI-ML/VEDet.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14548v1"
    ],
    "publication_venue": "11 pages, 4 figures; accepted to CVPR 2023"
}