
\input{tables/tab1-nusc_val.tex}

\input{tables/tab2-nusc_test.tex}

\input{tables/tab3_ablations.tex}

\subsection{Experimental setup}
\mypar{Dataset and metrics} We evaluate our method on the large-scale benchmark NuScenes. NuScenes has 1000 scenes split into 700/150/150 as train/val/test subsets. Each scene contains 20-second videos collected by 6 surround-view cameras at 10Hz, with synchronized 3D box annotations at 2Hz. We report on metrics defined by NuScenes: mean Average Precision (mAP), mean Average Translation Error (mATE), mean Average Scale Error (mASE), mean Average Orientation Error, mean Average Velocity Error (mAVE), mean Average Attribution Error (mAAE), and NuScenes Detection Score (NDS) which is a comprehensive score that aggregates the above five sub-metrics.


\mypar{Implementation details} We adopt ResNet \cite{he2016deep} and VoVNetV2 \cite{lee2020centermask} with FPN \cite{lin2017feature} as the backbone network, and use the P4 features (1/16 image size) from the pyramid for all our experiments. We use the AdamW optimizer \cite{loshchilov2017decoupled} with cosine annealing \cite{loshchilov2016sgdr} to train \Acronym with learning rate starting at $2\times10^{-4}$ and ending at $2\times10^{-7}$, on 8 Tesla A100 GPUs with a total batch size 8. For image data augmentation, we apply random resize, horizontal flip, and crop; and for the 3D space we apply random scaling and rotation following CenterPoint \cite{yin2021center}. Importantly, when flipping the input image, we flip the 3D box annotations and camera extrinsics accordingly. The algorithm-specific hyper-parameters are ablated in \cref{sec:ablations} and fixed for all experiments. Please see the supplemental material's full list of hyper-parameters and more training details. All experiments are trained for 24 epochs except for test submission, which is trained for 96 epochs without CBGS \cite{zhu2019class}.


\subsection{Comparison to state of the art}

Our \Acronym achieves state-of-the-art detection performance on NuScenes val set across a range of model setups as shown in \cref{tab:1_nusc_val}, compared to previous works and some concurrent preprints~\cite{liu2022petrv2,jiang2022polarformer}. We use ImageNet-pretrained models for setups with ResNet-50/101 backbones to process image resolutions of $384\times1056$ and $512\times1408$, and outperform existing baselines. As the full-fledged setup, we adopt a V2-99 backbone initialized with depth-pretrained weights \cite{park2021pseudo}, operating on $640\times1600$ images. In this high-performance regime we compare two closely related works, PETR \cite{liu2022petr}, and PETRv2 \cite{liu2022petrv2}, as shown in the third group. \Acronym surpasses PETRv2 by 2.0$\%$ mAP and 1.0\% NDS, excelling at 3 sub-metrics. Our \textit{single-frame} version \Acronym-SF also achieves substantial gain over the single-frame baseline PETR, by 2.9\% mAP and 3.9\% NDS; it even outperforms the two-frame PETRv2 by 0.2\% mAP and at 3 sub-metrics. The noticeably lower mATE scores of \Acronym and \Acronym-SF further verify the strong localization capability.
For the test submission we adopt the depth-pretrained V2-99 backbone from ~\cite{park2021pseudo} with $640\times1600$ images. Without using the more advantageous data sampling strategy CBGS \cite{zhu2019class} as all the other baselines do, \Acronym still outperforms PETRv2 and achieves state-of-the-art performance with 50.5\% mAP and 58.5\% NDS.



\subsection{Ablation studies}
\label{sec:ablations}


 We first ablate the choices of some important hyper-parameters used in \Acronym shown in \cref{tab:ablate_freq,tab:ablate_bands,tab:ablate_views,tab:ablate_weights}, and then analyze the most critical components proposed in \Acronym by adding one component at a time as shown in \cref{tab:ablate_component_add} or making variations of specific components shown in \cref{tab:ablate_component_var}.

\mypar{Hyper-parameter selection.} We ablate the maximum Fourier frequency $f_{\mathrm{max}}$ \cref{tab:ablate_freq} and number of Fourier bands $k$ \cref{tab:ablate_bands} used in \cref{eq:input_fourier,eq:queries}, the number of virtual query views $V$ \cref{tab:ablate_views} and their weighting $\lambda_{\mathrm{v}}$ \cref{tab:ablate_weights} used in box regression loss. The table shows that \Acronym is robust across a wide range of hyper-parameter choices with competitive performance. Importantly, in \cref{tab:ablate_views} starting from not using any virtual query views ($V=0$), adding more views gradually improves performance until too many views might have caused the optimization to be more difficult. We choose $f_{\mathrm{max}}=8,k=64,V=2,\lambda_{\mathrm{v}}=0.2$ according to the best results and fix them for all experiments.

\mypar{Geometric positional encodings and object queries.} In \cref{tab:ablate_component_add}, we note that \#1 is effectively PETR \cite{liu2022petr}. The ``Fourier+MLP GPE'' column means we switch the position embeddings and queries in PETR with ours introduced in \cref{sec:encoding,sec:decoding}. From \#1 to \#2 we can see  significant improvements in mAP by 1.6\% and NDS by 1.7\%, which demonstrates that \textit{our proposed implicit geometric mapping better captures the 3D geometries} thanks to its Fourier component and the use of geometric attributes $[\mathbf{r}, \Quat,\mathbf{t}]$.

\mypar{Virtual query views.} In \cref{tab:ablate_component_add}, we then add $V=2$ virtual views during training on top of adopting our proposed geometric position embeddings and object queries, for both single-frame and 2-frame (``2-frame'' column) settings. From \#2 to \#3 , we see further jumps in mAP by 1.4\% and NDS by 2.4\%; from \#4 to \#5, mAP increases by 1.9\% and NDS by 3.2\%. This shows \textit{our proposed multi-view consistency loss applied on virtual view decoding effectively guides the model to improve 3D detection}. 

\mypar{Mutual benefit between \Acronym and multi-sweep.} Based on the above two comparisons, another observation is that our proposed \Acronym benefits in the 2-frame setting more than the single-frame setting, indicating that more geometric cues can be exploited when the input images contain richer multi-view context. Similarly, when looking at \#2 to \#4 (+1.2\% mAP, +3.1\% NDS) and \#3 to \#5 (+1.7\% mAP, +3.9\% NDS), we can see that adding more frames becomes more helpful after we add in the viewpoint equivariance on $V=2$ views. These two observations further consolidate the effectiveness of our geometric position embeddings, object queries, and virtual views.

\mypar{Fourier encoding.} In \cref{tab:ablate_component_var} ``no Fourier'' we show the importance of the Fourier encoding before the MLP by simply dropping it for both position embeddings and object queries, such that the MLPs map the geometric primitives directly to the 256-dim vectors. This leads to a drastic decrease in the detection performance (-6.9\% mAP and -5.1\% NDS), showing the critical role of the Fourier encoding to capture fine-grained changes in the geometries, which can be considered as high-frequency signals \cite{tancik2020fourier}.

\mypar{Partial camera geometry.} In \cref{tab:ablate_component_var} ``no $\mathbf{\bar{q}}$'' and ``no $\mathbf{t}$'' we show that removing partial information from the input cameras' poses leads to noticeably degraded performance. Specifically, removing the rotation information leads to drops in mAP by 1.5\% and NDS by 1.2\%, since the rotation indicates how the perspective projection plane is facing, which the rays are insufficient to describe; removing the translation leads to catastrophic drops in mAP by 9.2\% and NDS by 7.4\%, justifying the importance of translation information which is too difficult to infer from data implicitly if missing.

\mypar{Multi-view consistency.} In \cref{sec:mtv}, we constrain the different views of a query point to be considered simultaneously by concatenating the box predictions for matching and calculating loss. Instead, in \cref{tab:ablate_component_var} ``no joint match'' we treat them as individual objects. This leads to the possibility that a query point with different query views can be matched to different boxes, which leads to $2.6\%$ drop in mAP and 3.9\% drop in NDS. This demonstrates the multi-view consistency of the \textit{same} query point is meaningful, whereas simply augmenting the queries with views and treating all queries individually does not exploit the geometric signals enough. Without multi-view consistency, the excessive number of queries might be even harmful to the optimization, as indicated by the lower performance (42.5\% mAP, 48.3\% NDS) than the ``$V=0$'' VEDet (43.2\% mAP, 49.5\% NDS) in \cref{tab:ablate_views}.


\subsection{More analysis on viewpoint equivariance}

\input{figures/fig4-one-to-many.tex}

Given our design of view-conditioned queries and utilizing multiple virtual views during training, a natural question arises: does the performance gain come from the viewpoint equivariance regularization, or simply because more queries participate in the training? To show the effectiveness of the viewpoint equivariance, we compare to an intuitive baseline described as follows. We start from a plain version where we do not apply virtual views in queries (i.e. setting $V=0$). Under this setting, we add an additional set of $V\times M$ query points during training while duplicating the box targets by $V$ more times, and only use the original $M$ queries during inference. This setting \textit{matches the number of participating queries and box targets as \Acronym but does not contain any viewpoint equivariance regularization}, denoted by ``+2$M$ qry, no VE..'' in \cref{fig:one-to-many}, for NuScenes val set performances.

As shown in \cref{fig:one-to-many}, the extra queries and box targets generate more learning signals at the early stage as reflected by the superior mAP and NDS (\textcolor{orange}{orange curves}) compared to our ``$V=0$'' VEDet (\textcolor{blue}{blue curves}). However, their effects diminish as the curves plateau when reaching the end, and eventually ``+2$M$ qry, no VE.'' underperforms ``VEDet-0 views'', as shown in both plots of \cref{fig:one-to-many}. In contrast, Our counterpart VEDet with 2 virtual views during training, denoted by ``VEDet, $V=2$'' (\textcolor{purple}{purple curve}), outperforms ``VEDet, $V=0$'' consistently throughout the training and noticeably boosts the performance by +1.9\% mAP and +3.2\% NDS to 45.1\% mAP to 52.7\% NDS. This justifies the viewpoint equivariance regularization is more than just increasing the number of queries, and that it brings richer geometric learning signals for the model.

