\input{figures/fig1-teaser.tex}
Camera-based 3D object detection is a critical research topic, with important applications in areas such as autonomous driving and robotics due to the semantic-rich input and low cost compared to range sensors.
In the past few years, monocular 3D detection has seen significant progress, from relying on predicting pseudo point clouds as intermediate representation \cite{ wang2019pseudo,weng2019monocular,qian2020end} to end-to-end learning~\cite{simonelli2019disentangling,park2021pseudo,wang2021fcos3d}. 
However, monocular 3D detectors are inherently ambiguous in terms of depth, which motivated some recent exploration in multi-view and multi-sweep 3D object detection~\cite{li2022bevformer,wang2022detr3d,liu2022petr,liu2022petrv2}. 

In a conventional monocular setting, given multiple cameras on a sensor rig, single-view detections are merged to the global frame through rule-based processing such as Non-Maximum Suppression (NMS). Recent advances in multi-view camera-based 3D algorithms~\cite{wang2022detr3d, liu2022petr} proposed to jointly aggregate multi-view information at the feature level, and directly predict a single set of detections in the global frame. These algorithms demonstrate a giant leap in 3D detection performance on multi-camera datasets (E.g., Nuscenes~\cite{caesar2020nuscenes}).
To aggregate information from different views, one line of query-based detectors adopt transformers to query image features \cite{wang2022detr3d,liu2022petr,liu2022petrv2} or bird's-eye-view (BEV) features \cite{li2022bevformer,jiang2022polarformer} via an attention mechanism. In contrast, another line of works ``lift-splat-shoot'' \cite{philion2020lift} image features from each view into the shared BEV features to be processed by convolutional detection heads \cite{li2022bevdepth}.

To further mitigate the depth ambiguity, some concurrent works have started extending multi-view to ``multi-sweep'' across timestamps and observe a promising performance boost~\cite{li2022bevformer,liu2022petrv2}.

While the works mentioned above demonstrate a strong potential for multi-view 3D detection, progress has concentrated on input aggregation and information interplay across frames and less on learning objectives. We argue that the learning objective can play a crucial role in ingesting the core knowledge in a multi-view setting: 3D geometry. 

This paper proposes to encourage 3D geometry learning for multi-view 3D detection models through viewpoint awareness and equivariance. 
We obtain our intuition from traditional structure-from-motion works~\cite{build-rome}, where multi-view geometry is modeled through multi-view consistency.
To this end, we propose viewpoint-awareness on the object queries, as well as a multi-view consistency learning objective as a 3D regularizer that enforces the model to reason about geometry.
Compared to existing methods that make 3D predictions in the default egocentric view, our proposed multi-view predictions and viewpoint equivariance effectively bring stronger geometric signals conducive to the 3D reasoning.
More specifically, in our query-based framework, the geometry information of image features and object queries is injected completely via implicit geometric encodings, and the transformer decoder is expected to learn better correspondence and 3D localization under the viewpoint equivariance objective.
We demonstrate that our proposed framework can make the best of available geometry information with extensive experiments and establish the new state-of-the-art in multi-view 3D object detection.
In summary, our contributions are:
\begin{itemize}
    \item  We propose a novel \textbf{Viewpoint Equivariance (VE)} learning objective that encourages multi-view consistency in 3D detection models, leading to improved 3D object detection performance.
    \item We propose a new multi-view 3D object detection framework, \textbf{\Acronym}, which employs a query-based transformer architecture with perspective geometry and viewpoint awareness injected both at the encoding and decoding stages. \Acronym fully enables our proposed VE learning objective, facilitating geometry learning with implicit inductive biases.
    \item \Acronym achieves \textbf{state-of-the-art on large-scale benchmark}, reaching \textbf{45.1\%mAP on NuScenes val set and 50.5\% mAP on test set}. We provide a comprehensive analysis of our components, and share insights based on empirical observations.
\end{itemize}


