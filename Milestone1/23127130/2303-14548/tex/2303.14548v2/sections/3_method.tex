
The multi-view 3D object detection task aims at detecting 3D bounding boxes in the scene with class labels, given a set of images from $N$ cameras with poses and intrinsics $\{\mathbf{I}_i \in \mathbb{R}^{3\times H\times W},\mathbf{T}_i \in \mathit{SE}(3), \mathbf{K}_i \in \mathbb{R}^{3\times 3},i=1,2,\dots,N\}$. In this section, we will first introduce the overall \Acronym framework in \cref{sec:overall_framework}. \cref{sec:encoding} describes how we use geometric positional encoding to inject geometry information for image features and object queries implicitly. In \cref{sec:decoding} we propose making object queries view-conditioned so that 3D boxes are predicted in the specified view. Lastly, we present the novel viewpoint equivariance learning objective in \cref{sec:mtv}, which exploits the viewpoint-awareness of object queries and produces stronger geometric signals to improve 3D detection.

\subsection{Overall framework}
\label{sec:overall_framework}

The workflow of our proposed \Acronym builds upon a transformer-based architecture, as depicted in \cref{fig:overallarch}.
We first employ a backbone network that extracts image features $\{\mathbf{F}_i\in\mathbb{R}^{C\times H'\times W'}\}$ from multi-view images.
For each 2D location on the feature map grid, we calculate a geometric positional encoding that jointly considers pixel location, camera pose, and intrinsics. 

The image features, with their associated positional encoding, are flattened and processed by a transformer decoder \cite{carion2020end} with a set of object queries $\{\mathbf{q}_j\}$. The queries are constructed from a set of learnable 3D \textit{query points} $\{\mathbf{c}_j\}$ combined with a given \textit{query view} $\{\mathbf{T}^v\}$.

A series of self- and cross-attention layers then aggregate and update the 3D scene information into the queries, after which a feed-forward detection head maps the updated queries to box predictions $\{\mathbf{\hat{b}}_j\}$.
The box predictions are conditioned and expressed in the query views $\mathbf{T}^v_j$ associated with the queries, as detailed in \cref{sec:decoding}. 
Finally, we optimize the network by applying a viewpoint equivariance (VE) loss on the view-conditioned box predictions, as detailed in Sec~\ref{sec:mtv}.


\input{figures/fig2-overall_architecture.tex}

\subsection{Geometric positional encoding}
\label{sec:encoding}

Positional encodings provide location information of feature embeddings in Transformer architectures \cite{vaswani2017attention}. In this work, we encode the 3D geometric attributes associated with the image features as well as the object queries, when they are processed by the decoder. Inspired by~\cite{yifan2022input,liu2022petr}, for the image features we propose to encode the camera pose and the 3D inverse projection ray that combines pixel position and camera perspective geometry; for object queries the learnable 3D query point and the selected query view are encoded (more in \cref{sec:decoding}).

Specifically, given the extracted image features $\{\mathbf{F}_i\}$, we construct a triplet of geometric attributes, $[\mathbf{r}_{(u_i,v_i)}, \Quat_i,\mathbf{t}_i]$ for \textit{each feature location} $(u_i,v_i)$. 
$[\Quat_i,\textbf{t}_i]$ denote the quaternion vector and translation of the camera pose, and 
$\mathbf{r}_{(u_i,v_i)}$ denotes a unit-length inverse perspective projection ray originating from the pixel location given by:

\begin{equation}
\label{eq:rays}
    \mathbf{r}_{(u_i,v_i)}'=(\mathbf{K}_i\mathbf{R}_i^T)^{-1}[\alpha u_i,\alpha v_i,1]^T, \mathbf{r}=\frac{\mathbf{r}'}{||\mathbf{r}'||_2},
\end{equation}
where $\alpha$ is the downsample factor of $\mathbf{F}_i$ compared to image $\mathbf{I}_i$, $\mathbf{K}_i$ and $\mathbf{R}_i$ are instrinsic and rotation matrix of camera $i$.
The triplet $[\mathbf{r}_{(u_i,v_i)}, \Quat_i,\mathbf{t}_i]$ fully describes the perspective geometry for a given image feature $\mathbf{F}_i(u_i,v_i)$. Compared to PETR \cite{liu2022petr,liu2022petrv2} which model the positional information of image features by manually sampling a set of 3D point locations along the ray at pre-defined depth frustums, \Acronym employs a simpler design\footnote{PETR also combines a few other components with the 3D PE, namely 2D grid PE and view number PE, which we do not use.} and chooses not to assume the discretized depth prior, as we believe $[\mathbf{r}_{(u_i,v_i)}, \Quat_i,\mathbf{t}_i]$ keeps the full geometry information with which the model can learn 3D localization better.

\mypar{Learnable geometry mapping} 

We encode the geometric attributes into high-dimensional embeddings via Fourier transform followed by a learnable mapping.
Inspired by advances in NeRF \cite{mildenhall2021nerf,tancik2020fourier}, we first apply a Fourier transform to capture the fine-grained changes in the geometric attributes.
\begin{equation}
\label{eq:input_fourier}
   \gamma(x|[f_1,\dots,f_k]) = [\sin{(f_1\pi x)},\cos{(f_1\pi x)},\dots]
\end{equation}
The $k$ frequencies $[f_1,\dots,f_k]$ are sampled evenly between $[0, f_{\mathrm{max}}]$. Afterward, an MLP is used to project the output to dimension $C$ as our final geometric positional encoding:
\begin{equation}
\label{eq:input_embed}
    \mathbf{p}_{(u_i,v_i)}^e = \mathrm{MLP}_{\mathrm{enc}}(\gamma([\mathbf{r}_{(u_i,v_i)}, \Quat_i,\mathbf{t}_i])
\end{equation}

As a result, even without explicitly projecting the image features $\{\mathbf{F}_i\}$ back to 3D space, they become 3D geometry-aware when augmented with the 3D geometric positional encodings $\{\mathbf{P}_i^e \in \mathbb{R}^{C\times H'\times W'}\}$. Hence, we implicitly encode the multi-view perception of the scene at an input level, which will work jointly with our proposed VE learning objective to enforce 3D geometric modeling. 

\mypar{Temporal modeling} In the context of a multi-sweep setting, we follow \cite{liu2022petrv2} and transform the camera pose from previous frames into the current global frame via ego-motion compensation. The multi-sweep features are concatenated at the token dimension. 




\subsection{View-conditioned query}
\label{sec:decoding}

\Acronym adopts a DETR-style \cite{carion2020end} decoder that consists of $L$ transformer layers, as shown in \cref{fig:overallarch}. Each layer performs self-attention among a set of $M$ queries $\{\mathbf{q}_j \in \mathbb{R}^C, j=1,2,\dots,M\}$, and cross-attention between the queries and the 3D geometry-aware image features $\{(\mathbf{F}_i, \mathbf{P}_i^e)\}$. The updated queries $\{\mathbf{q}_j\}$ will serve as input to the next layer:
\begin{equation}
\label{eq:decoding}
\resizebox{.9\hsize}{!}{$
    \{\mathbf{q}_j\}_{l} = \psi_{l-1}(\{\mathbf{F}_i\}, \{\mathbf{P}_i^e\}, \{\mathbf{q}_j\}_{l-1}), l=1,2,\dots,L$,}
\end{equation}
where $L$ is the number of attention layers. 
A classification and regression MLP heads map the queries from each layer into class logits and bounding box predictions, respectively.

\begin{equation}
\label{eq:output_heads}
    \mathbf{\hat{s}}_j = \mathrm{MLP}_{\mathrm{cls}}(\mathbf{q}_j), \mathbf{\hat{b}}_j = \mathrm{MLP}_{\mathrm{reg}}(\mathbf{q}_j)
\end{equation}



We propose to ground the queries with multi-view geometry. Concretely, a query $\mathbf{q}_j$ is constructed from two parts: a \textit{3D query point} and a \textit{query view}. We initialize a set of $M$ learnable 3D query points $\{\mathbf{c}_j\in\mathbb{R}^3, j=1,2,\dots,M\}$ in the \textit{global frame} $\mathbf{T}^0$, similarly to PETR~\cite{liu2022petr}, which is optimized during training.

\mypar{Query views} For the second part of query geometry, a query view $\mathbf{T}^v=[\Quat^v,\mathbf{t}^v]$ is selected relative to the global frame.
To construct the query, the 3D query points are first transformed into the query view via $\mathbf{c}_j^v=(\mathbf{T}^v)^{-1}\mathbf{c}_j$, and together with the query view $[\mathbf{c}_j^v,\Quat^v,\mathbf{t}^v]$ compose the query geometries. As described in \cref{sec:encoding}, the query geometries are similarly mapped by a Fourier transform followed by an MLP, into \textbf{view-conditioned queries}:

\begin{equation}
\label{eq:queries}
    \mathbf{q}_j^v=\mathrm{MLP}_{\mathrm{dec}}(\gamma([\mathbf{c}_j^v,\Quat^v,\mathbf{t}^v])).
\end{equation}

For query views, we refer the \textit{global frame} $\mathbf{T}^0=[[1,0,0,0],\mathbf{0}]$ as a default query view.\footnote{
This view is also the egocentric coordinate frame for box prediction and evaluation as done in other multi-view detection works~\cite{wang2022detr3d,liu2022petr,liu2022petrv2}.}
%
Additionally, we generate $V$ \textit{virtual query views} to provide variation to the decoding views and encourage viewpoint awareness in the model. Concretely, we randomly sample Euler angles $\Theta^v\in\mathbb{R}^3$ and translation $\mathbf{t}^v\in\mathbb{R}^3$ from uniform distributions $[\Theta_{\mathrm{min}}, \Theta_{\mathrm{max}}]$ and $[\mathbf{t}_{\mathrm{min}}, \mathbf{t}_{\mathrm{max}}]$, after which the Euler angles will be converted to the equivalent quaternion $\Quat^v\in SO(3)$, giving $\{\mathbf{T}^v=[\Quat^v,\mathbf{t}^v]\}$. In total, there are $V+1$ query views consisting of the default view and $V$ virtual views $\{\mathbf{T}^v, v=0,1,\dots,V\}$. Therefore, given the $M$ 3D query points $\{\mathbf{c}_j\}$ and $V+1$ query views $\{\mathbf{T}^v\}$, we construct object queries from $\{\mathbf{c}_j\}\times \{\mathbf{T}^v\}$, resulting in total $M\times(V+1)$ individual object queries.

\mypar{View-conditioned predictions} The query view specifies the coordinate system in which boxes (groundtruth, predicted) are defined. Specifically, given a \textit{view-conditioned} query $\mathbf{q}_j^v$, the box predictions $\mathbf{\hat{b}}_j^v$ are local to the underlying query view $\mathbf{T}^v$, parameterized as:
\begin{equation}
     \mathbf{\hat{b}}_j^v=[\Delta\mathbf{\hat{c}}_j^v, \mathbf{\hat{d}}_j, \cos(\phi), \sin(\phi), \mathbf{\hat{v}}_j^v] ,  
\end{equation}
where $\Delta\mathbf{\hat{c}}_j^v\in\mathbb{R}^3$ is the offset from the 3D query point $\mathbf{c}_j^v$ to the bounding box center, $\mathbf{\hat{d}}_j\in\mathbb{R}^3$ is the box dimensions, $\phi$ is the yaw angle of the box, and $\mathbf{\hat{v}}_j^v\in\mathbb{R}^3$ is the box velocity. 
As for the object classification score, we simply decode one from the global frame for each query point $\textbf{c}_j$, as it is simple and decoding from virtual views did not show advantage in our experiments. We predict a binary score for each class normalized by a sigmoid function.

The view-conditioned queries and their local predictions serve as a form of data augmentation during training and, more importantly, enable viewpoint equivariance regularization as discussed in \cref{sec:mtv}. More design choices are also ablated in \cref{sec:ablations}.
We only use the global frame $\mathbf{T}^0$ as the query view at inference time.


\subsection{Viewpoint equivariance loss}
\label{sec:mtv}

\input{figures/fig3-multivew.tex}

As described in \cref{sec:decoding}, given $V+1$ query views, there are $V+1$ versions of bounding box predictions $\{\mathbf{\hat{b}}_j^v\}$ coming from a single query point $\mathbf{c}_j$. 
The $V+1$ bounding boxes are expressed in different coordinate frames but of the same underlying ground truth object.
According to multi-view geometry, the observations of the \textit{same} object from different frames should be geometrically consistent and only differ by the relative transformation as shown in \cref{fig:mtvtarget}. 
 Therefore, we propose a viewpoint equivariance objective that considers the multi-view predictions coming from the same query point $c_j$ and box target from all query views \textit{jointly}. 
 
Concretely, we first ensure that the $V+1$ versions predictions from query point $\mathbf{c}_j$ are assigned to the same ground truth object. 
To achieve this goal, we create a \textbf{super box} by concatenating the predictions from different query views:
\begin{equation}
    \mathbf{\hat{B}}_j=[\mathbf{\hat{b}}_j^0,\mathbf{\hat{b}}_j^1,\dots,\mathbf{\hat{b}}_j^V].
\end{equation}
Similarly, we extend all the ground truth bounding boxes into super boxes:
\begin{equation}
    \mathbf{{B}}_m=[\mathbf{{g}}_m^0,\mathbf{{g}}_m^1,\dots,\mathbf{{g}}_m^V],
\end{equation}
where $\mathbf{g}^v_m$ is the ground truth bounding box in the expressed in the query view $\{\mathbf{T}^v\}$.
 
 Next, we perform Hungarian matching \cite{kuhn1955hungarian} to decide the optimal assignment between $\{\mathbf{\hat{B}}_j\}$ and $\{\mathbf{B}_m\}$, using the following cost function similar to DETR \cite{carion2020end}:

 \begin{equation}
\label{eq:loss}
\resizebox{.87\hsize}{!}{
  $  {\sigma} = -\mathds{1}_{\{c_m\neq \emptyset\}}\log (\textbf{s}_j(c_m)) + \mathds{1}_{\{c_m\neq \emptyset\}}L_{reg}(\mathbf{B}_m, \mathbf{\hat{B}}_j)$,}
\end{equation}
where $c_m$ is the ground truth class label and $L_{reg}()$ is a weighted L1 loss, given by:
\begin{align}
\label{eq:loss_reg}
\resizebox{.87\hsize}{!}{$
    L_{reg}(\mathbf{B}_m, \mathbf{\hat{B}}_j)=||\mathbf{\hat{b}}^0_j-\mathbf{g}^0_m||_1 + \Sigma_1^V\lambda_v ||\mathbf{\hat{b}}^v_j-\mathbf{g}^v_m||_1$.}
\end{align}
We use $\lambda_v$ to weigh the virtual views. Once we identify the optimal assignment, we calculate the loss on the super boxes:
\begin{equation}
\label{eq:loss_ve}
    L_{VE} = \lambda_{cls}L_{cls}(\textbf{s},c) + \lambda_{reg}L_{reg}(\mathbf{B}, \mathbf{\hat{B}}),
\end{equation}
for each paired prediction and ground truth. We adopt focal loss \cite{lin2017focal} for classification loss $L_{cls}$, and the same form of regression loss $L_{reg}$ as in matching. $\lambda_{cls}$ and $\lambda_{reg}$ are loss weights. 
For each 3D query point, by considering $V+1$ versions of predictions \textit{jointly} during both matching and optimization, the model learns viewpoint equivariance through multi-view consistency, leading to better 3D detection.