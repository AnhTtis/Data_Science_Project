\subsection{Monocular 3D object detection}
Early works tackled camera-based 3D object detection in a monocular setting by adopting a two-stage pseudo-LiDAR paradigm \cite{wang2019pseudo,weng2019monocular,qian2020end,you2019pseudo} or directly building upon 2D detection frameworks to predict extra 3D properties \cite{mousavian20173d,xu2018multi,brazil2019m3d,simonelli2019disentangling,park2021pseudo,wang2021fcos3d,chen2022epro}. Due to the inherent scale ambiguity in depth estimation, one standard approach was to lift from 2D to 3D by aligning 3D properties and their 2D projections on the image plane \cite{barabanau2019monocular,he2019mono3d++,li2019gs3d,ku2019monocular}, while others leveraged additional object or scene priors such as shapes \cite{he2019mono3d++}, CAD models \cite{chabot2017deep}, or ground planes \cite{ansari2018earth}. In the line of representation learning, DD3D \cite{park2021pseudo} exploited large-scale pre-training to learn a depth-aware representation that can universally benefit 3D detection algorithms. However, all these methods still struggle with the two major drawbacks in monocular 3D detection: inherent depth ambiguity and insufficient context to infer objects across images. As a multi-view method, our work addresses both of these issues by leveraging the ample 3D geometric cues in multi-camera setups. 

\subsection{Multi-view 3D object detection}
Recent advances in camera-based 3D object detection have started to leverage multi-view context, which can improve the detection of objects that appear in more than one image. One line of works extends the DETR framework \cite{carion2020end}, which decodes 3D bounding boxes with a set of queries \cite{wang2022detr3d,liu2022petr,liu2022petrv2,li2022bevformer,jiang2022polarformer,li2022unifying}. Using camera parameters, DETR3D \cite{wang2022detr3d} directly projects 3D queries to 2D image planes to update query features, while PETR \cite{liu2022petr} constructs 3D position embeddings from point frustums to implicitly guide query updates. BEVFormer \cite{li2022bevformer} and UVTR \cite{li2022unifying} first build an intermediate voxelized feature space around the vehicle/robot's ego coordinate frame, before feeding the features to a DETR-style decoder. Another line of work follows LSS \cite{philion2020lift} and constructs the voxelized feature space, before applying a detection head on the features to predict bounding boxes. Typically, a depth head is also trained to predict a depth bin distribution in order to lift-splat-shoot the features, as in BEVDepth \cite{li2022bevdepth}. Our work falls in the first line of research and exploits multi-view geometric consistency to improve bounding box localization in 3D space.

\subsection{Implicit geometric encoding}
The Transformer architecture \cite{vaswani2017attention,dosovitskiy2020image,han2022survey} introduced the use of positional encodings for input features. This brought upon a paradigm shift in how to model the relative position of elements, from \textit{explicitly}, i.e. by recurrent operations or convolutional filters, to \textit{implicitly}, i.e. learned from data. Inspired by this new paradigm, some works started to investigate how to use positional encodings constructed from geometric priors as input-level inductive biases \cite{yifan2022input,liu2022petr,liu2022petrv2}. ILIB \cite{yifan2022input} utilizes multi-view geometry, including camera and epipolar cues, to generate position encodings, to be processed by a generalist Perceiver IO~\cite{jaegle2021perceiver} architecture to produce a set of latent vectors. From this latent space, ILIB constructs queries from 3D viewing rays to decode depth maps. PETR \cite{liu2022petr,liu2022petrv2} similarly constructs position encodings from generated point frustums at pre-defined depth values, and decodes 3D bounding boxes using queries constructed from 3D anchor points \cite{wang2022anchor} in the ego vehicle space. Positional encoding has also been used extensively in the context of neural fields (i.e. coordinate-based multi-layer perceptron) to process the appearance, radiance, or occupancy of a scene~\cite{sitzmann2019siren,mildenhall2021nerf,xie2021neural}. Our work improves the design of 3D geometric position encoding and introduces a new optimization objective to guide the detection model toward learning better object localization.
