% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage[title]{appendix}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage[table]{xcolor}
\usepackage{lipsum}  
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{xcolor}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\Acronym}[0]{VEDet\xspace}
\newcommand{\Quat}[0]{\mathbf{\bar{q}}}
\newcommand\mypar[1]{\par\vspace{2.0mm}\noindent\textbf{#1}\;\;}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6940} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\newcommand{\jie}[1]{\textcolor{blue}{Jie: #1}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Viewpoint Equivariance for Multi-View 3D Object Detection}

\author{Dian Chen \quad Jie Li \quad Vitor Guizilini \quad Rareș Ambruș \quad Adrien Gaidon\\
Toyota Research Institute (TRI), Los Altos, CA\\
{\tt\small \{firstname.lastname\}@tri.global}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
\input{sections/0_abstract.tex}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\input{sections/1_introduction.tex}

%------------------------------------------------------------------------
\section{Related Work}
\label{sec:related}
\input{sections/2_related.tex}

%------------------------------------------------------------------------
\section{Viewpoint Equivariant 3D Detection}
\label{sec:method}
\input{sections/3_method.tex}

\section{Experiments}
\label{sec:experiments}
\input{sections/4_experiments.tex}
\vspace{-2mm}
\section{Limitations}
\input{sections/5_limitations.tex}
\label{sec: limitation}
\vspace{-2mm}
\section{Conclusion}
\label{sec:conclusion}
\input{sections/6_conclusion.tex}

\section*{Appendix}
\begin{appendix}
\section{Implementation details}
\mypar{VEDet model.} We use three different backbones to report performance on NuScenes: ResNet-50 and ResNet-101 \cite{he2016deep} are initialized from the ImageNet-pretrained weights hosted on OpenMMLab \cite{mmcv}; VoVNetV2-99 \cite{lee2020centermask} is initialized from the depth-pretrained weights released by \cite{park2021pseudo}. The image features and geometric positional encodings have dimension $C=256$, and are added element-wise as the keys to the transformer decoder, which has $L=6$ transformer layers. In the transformer layers, we use multi-head attention with $8$ heads, dropout rate $0.1$ on the residual connection, and $2048$ hidden dimensions in the feed-forward network. To predict the classification scores, we use a single linear projection from $256$-dim queries to $10$-dim class scores; for predicting the 3D box attributes, we use a $2$-layer MLP with $[512, 512]$ hidden dimensions  interleaved with ReLU activations. The classification and regression heads are both shared across the $6$ transformer layers.

\mypar{Learnable geometry mapping.} For the MLP in the learnable geometry mapping, used to make both geometric positional encoding and object queries, we use $1$ hidden layer with $1920$ dimensions, followed by a ReLU activation and a final projection to $C=256$ dimensions. Therefore, given Fourier bands $k=64$, the dimensions go through the following changes: $d_0 \rightarrow_{\textrm{Fourier}} 1280 \rightarrow_{\textrm{hidden}} 1920 \rightarrow_{\textrm{proj}} 256$, where $d_0=10$ for both perspective geometry of an image feature $[\mathbf{r}_{(u_i,v_i)}, \Quat,\mathbf{t}]$ and query geometry $[\mathbf{c}_j^v,\Quat^v,\mathbf{t}^v]$.

\mypar{Query points.} We use $900$ learnable 3D query points in all experiments. We follow \cite{wang2022detr3d} to use object ranges $[-51.2m, -51.2m, -5.0m, 51.2m, 51.2m, 3.0m]$ in XYZ axes of the global BEV space around the vehicle. The query points are normalized to $[0, 1]$ by a sigmoid operation and scaled by their range. The predictions of box center offsets are added to the points before the sigmoid operation.

\mypar{Virtual view sampling.} During training, the range we use to uniformly sample the translation for the virtual query views is $[-0.6m, -1.0m, -0.3m, 0.6m, 1.0m, 0m]$ in XYZ axes. We uniformly sample the yaw angle to be between $[0, 2\pi]$.

\mypar{Temporal modeling.} In the full-version \Acronym we concatenate $2$ temporal frames at the token dimension. Following \cite{huang2022bevdet4d,liu2022petrv2}, we randomly sample one frame from the past $[3, 27]$ frames during training, and use the past $15$-th frame during inference. The time interval between consecutive frames is roughly $0.083$s.

\mypar{Optimization.} During training, the loss weights we use are $\lambda_{cls}=2.0$ and $\lambda_{reg}=0.25$ following \cite{wang2022detr3d,liu2022petr}. We use the AdamW optimizer \cite{loshchilov2017decoupled} with weight decay $0.01$. The learning rate is linearly warmed up in the first $500$ iterations from $6.77e^{-5}$ ($\frac{1}{3}$ of initial learning rate) to $2e^{-4}$. The learning rate of the pretrained backbone is multiplied by $0.1$ compared to all other components, that are trained from scratch. Checkpointing \cite{chen2016training} is adopted during training to save GPU memory, bringing the training time of the full-version \Acronym (2 frames, $640\times1600$ images, $V=2$) to 20 hours on 8 A100 GPUs, for 24 epochs on NuScenes.


\mypar{Data augmentation.} We use data augmentations following \cite{liu2022petr}, in the order shown below:

\begin{itemize}
    \item Resize. The original images are resized keeping the aspect ratio. The resize factor is sampled uniformly from $[0.564, 0.8]$ for $384\times1056$ images, $[0.79, 1.1]$ for $512\times1408$ images, and $[0.94, 1.25]$ for $640\times1600$ images.
    \item Crop. Given a crop size $H\times W$ and an intermediate image size $H'\times W'$ after the resizing, the top area $[0, H'-H]$ is cropped to meet the final height $H$. The left limit of the cropping box is uniformly sampled from $[0, W'-W]$.
    \item Horizontal flip. With a $50\%$ probability, we flip all $N$ images at the same time, alongside the 3D box annotations. The camera poses and intrinsics are transformed accordingly to reflect the flipping. Concretely, the X coordinate of the camera translation and yaw angle are flipped, while the principal point in the intrinsic matrix has the X-coordinate flipped.
    \item Global rotation. Without changing the images, the camera poses and 3D box annotations are rotated around the Z axis of the global BEV space. The angle is uniformly sampled from $[-22.5^{\circ}, 22.5^{\circ}]$.
    \item Global scaling. Without changing the images, the camera poses and 3D box annotations are scaled relative to the origin of the global BEV space. The scaling factor is uniformly sampled from $[0.95, 1.05]$.
\end{itemize}

During testing, no random augmentations are used. The images are resized to the final width while keeping the aspect ratio, and cropped at the bottom-center.

\end{appendix}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
