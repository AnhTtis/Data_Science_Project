\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}

\usepackage{graphicx}
\usepackage[font=footnotesize]{subcaption}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{amstext,amsmath,amssymb}
\usepackage{microtype}
\usepackage{wrapfig}
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{appendix}
\usepackage{algorithm}
%\usepackage{algorithmicx}
\usepackage{algorithmic}
%\usepackage{algpseudocode}


\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
        breaklinks=true
}

% Use the following line for the initial blind version submitted for review:


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


%\newcommand{\etal}{\emph{et al. }}
%\newcommand{\ie}{\emph{i.e., }}
%\newcommand{\eg}{\emph{e.g., }}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\mccow}{\includegraphics[scale=0.4]{images/cow.png}}
\newcommand{\mcsheep}{\includegraphics[scale=0.4]{images/sheep.png}}
\newcommand{\mcmilk}{\includegraphics[scale=0.4]{images/milk_bucket.png}}
\newcommand{\mcbucket}{\includegraphics[scale=0.4]{images/bucket.png}}
\newcommand{\mcshear}{\includegraphics[scale=0.4]{images/shears.png}}
\newcommand{\mcsunflower}{\includegraphics[scale=0.6]{images/sunflower.png}}
\newcommand{\mcshovel}{\includegraphics[scale=0.4]{images/diamond_shovel.png}}
\newcommand{\mctallgrass}{\includegraphics[scale=0.4]{images/tall_grass.png}}
\newcommand{\mcleaf}{\includegraphics[scale=0.4]{images/jungle_leaves.png}}
\newcommand{\mcwool}{\includegraphics[scale=0.4]{images/white_wool.png}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\iccvfinalcopy % *** Uncomment this line for the final submission

%\def\iccvPaperID{10658} % *** Enter the ICCV Paper ID here
%\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ificcvfinal\pagestyle{empty}\fi

%\usepackage{marvosym}
%\let\Cross\relax
%\usepackage[misc]{ifsym}



\begin{document}


%%%%%%%%% TITLE
\title{CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft}

\author{Ziluo Ding$^{12\ast}$ \ \ Hao Luo$^{12}$\thanks{These authors contribute equally to this work.}  \  \ \ Ke Li$^{2}$ \ \ Junpeng Yue$^{23}$ \ \ Tiejun Huang$^{12}$ \ \ Zongqing Lu$^{12}$\thanks{Correspondence to Zongqing Lu$<$zongqing.lu@pku.edu.cn$>$, Ziluo Ding $<$ziluo@pku.edu.cn$>$, Hao Luo $<$lh2000@pku.edu.cn$>$} \\  \\
$^{1}$PKU \qquad  $^{2}$BAAI    \qquad  $^{3}$TJU
}

\maketitle
% Remove page # from the first page of camera-ready.
%\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
One of the essential missions in the AI research community is to build an autonomous embodied agent that can attain high-level performance across a wide spectrum of tasks. However, acquiring reward/penalty in all open-ended tasks is unrealistic, making the Reinforcement Learning (RL) training procedure impossible. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn an RL-friendly vision-language model that serves as a reward function for open-ended tasks. Therefore, no further task-specific reward design is needed. Intuitively, it is more reasonable for the model to address the similarity between the video snippet and the language prompt at both the action and entity levels. To this end, a motion encoder is proposed to capture the motion embeddings across different intervals. The correlation scores are then used to construct the auxiliary reward signal for RL agents. Moreover, we construct a neat YouTube dataset based on the large-scale YouTube database provided by MineDojo. Specifically, two rounds of filtering operations guarantee that the dataset covers enough essential information and that the video-text pair is highly correlated. Empirically, we show that the proposed method achieves better performance on RL tasks compared with baselines.
\end{abstract}



\section{Introduction}

Recently, autonomous agents have had great success in Atari games \cite{oh2015action}, Starcraft \cite{samvelyan2019starcraft}, Dota2 \cite{berner2019dota}, and Go \cite{SilverSSAHGHBLB17}. However, these popular works have also been criticized for poor generalization, \ie, agents cannot generalize beyond a very specific set of tasks, unlike humans that continuously learn from open-ended tasks. Thus, building an autonomous embodied agent that can attain high-level performance across a wide spectrum of tasks has been one of the greatest challenges facing the AI research community.

One main challenge is that acquiring reward/penalty in all open-ended tasks is unrealistic, making the Reinforcement Learning (RL) training procedure impossible. To this end, MineDojo \cite{fan2022MineDojo} has proposed an internet-scale, multi-modal knowledge base of YouTube videos to facilitate learning in open-ended settings. With the advent of a such large-scale database, agents are possible to harvest practical knowledge encoded in large amounts of media like human beings. Moreover, a video-text contrastive framework, MineCLIP \cite{fan2022MineDojo}, is proposed to utilize the internet-scale domain knowledge. In more detail, the learned correlation score between the visual observation and the language prompt can be used effectively as an open-vocabulary, massively multi-task reward function for RL training. Therefore, no further task-specific reward design is needed for open-ended tasks.  

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{images/keyword.pdf}
\caption{Key entities in our YouTube dataset.}
  \label{keywords}
  \vspace{-0.2cm}
\end{figure}

However, the autonomous embodied agent requires the contrastive learning framework to provide a more instructive correlation score. Give the partial observations, \eg, video snippet, and the language prompt that describes the task, 
%Given the language prompt that describes the tasks and the partial observations, \eg, video snippet, 
the agent needs to figure out three non-trivial matters to better evaluate the current state. First, \textit{whether the target entities are present within its field of vision? }  MineCLIP has addressed this question. Second, \textit{whether the agent has made the right action toward the right target?} Unlike vision tasks, RL tasks concern how agents ought to take action. Namely, we should measure the similarity between the video snippet and the language prompt at the action level, not only at the entity level. Third, \textit{what is the relationship between each video snippet and the degree of completion of the task?} To illustrate, the procedure of completing one single task contains numerous video snippets. However, different video snippets represent different levels of completion of the task even though they may have similar correlation scores with the language prompt. The higher level of completion of the task, the closer the video snippets are to the target state. 
%Intuitively, the states that are closer to the target state should be more addressed. 
Thus, we argue it is important to incorporate the level of completion of the task into the reward function. At the current stage, we only attempt to address the second problem so that the contrastive learning framework can be more RL-friendly. 

In this paper, we propose an upgraded cross-modal contrastive learning framework, CLIP4MC, to provide a more RL-friendly reward function. Specifically, a motion encoder is proposed to extract the motion features of the video snippet. Intuitively, the atomic actions are captured in the difference between two adjacent frames. A sequence of such actions corresponds to the behavior described in the language prompt. Therefore, we stack the two frames with different intervals as atomic action representations of different amplitudes. After extracting motion features from those representations, we need to summarize a series of motion features at the same interval as the motion embedding and multi-interval motion embeddings as the final embedding. Motion embedding, together with video embedding, is used to measure the correlation score with the language prompt at both entity and action levels. Based on the score, we can render the auxiliary reward signal of the current state to the agent. The RL training is finally possible. 

Though a large-scale database is provided by MineDojo, it contains significant noise due to its nature as an online resource. Since MineDojo has not released the training dataset for MineCLIP, we attempt to construct a neat YouTube dataset to facilitate the learning of basic game concepts. In more detail, we have done two rounds of filtering operations to guarantee two things. One is that the dataset covers enough essential information, including key entities and basic semantic events. Another is that there is a stronger content correlation between the video and transcript clips. Note that our proposed method is trained on our YouTube dataset, and we evaluate our method on MineDojo Programmatic tasks, including Harvest and Finding. Results show that our method can provide a more friendly reward signal for the RL training procedure. In addition, we validate various design choices of CLIP4MC through extensive ablation studies.% Lastly, we verify our filtering opertaion can indeed help constructing a neater 

\vspace{2mm}
\noindent \textbf{Our main contributions, among others, are:} 
\begin{itemize}
%\vspace{-1mm}
\setlength\itemsep{1mm}
    \item We bring up three key issues we need to solve for learning a vision-language model as the universal reward function for open-ended tasks.
    \item We build and release a neat vision-language dataset for Minecraft using the YouTube videos from MineDojo.
    \item We propose an RL-friendly vision-language model, CLIP4MC, which aligns actions implicitly contained in the video and transcript clips in addition to entities.
\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{images/dataset_sample.pdf}
\vspace{-0.2cm}
\caption{Illustration of the YouTube video database. The screenshots of video clips are on the left and key entities are circled in red. The corresponding transcript clips are on the right and key entities are marked in red. We give examples of irrelevant, mismatched, and matched video content in the YouTube video database.}
  \label{illustration_dataset}
  %\vspace{-0.2cm}
\end{figure*}


\section{Related Work}

\noindent\textbf{Video-Text Retrieval.} Video-text retrieval plays an essential role in multi-modal research and has been widely used in many real-world web applications. Recently, the pre-trained models have dominated this line of research with noticeable results on both zero-shot and fine-tuned retrieval. Especially, BERTs \cite{DevlinCLT19}, ViTs \cite{dosovitskiy2021an}, and CLIP \cite{RadfordKHRGASAM21}, are used as the backbones to extract the text or video embedding. The cross-modal embeddings are then matched with specific fusion networks to find the correct video-text pair.

In more detail, CLIP4CLIP \cite{LuoJZCLDL22} proposes three different similarity modules to calculate the correlation between video and text embeddings. HiT \cite{Liu0QCDW21} performs hierarchical matching at two different levels, \ie semantic level and feature level. Note that semantic level and feature level features are from the transformer network's higher and lower feature layers, respectively. Frozen \cite{BainNVZ21} proposes a dual encoder architecture that utilizes the flexibility of a transformer visual encoder to train from images or video clips with text captions. Moreover, MDMMT \cite{DzabraevKKP21} adopts several pre-training models as encoders and it shows the CLIP-based model performs the best. Therefore, our model follows this line of research by using the pre-trained model, CLIP \cite{RadfordKHRGASAM21}, to extract the feature embeddings. 

\vspace{2mm}
\noindent\textbf{Minecraft for AI Research.} 
As an open-world video game with an egocentric vision, Minecraft is a splashy and important domain in RL due to the nature of the sparse reward, large exploration space, and long-term episodes. Since the release of the Malmo simulator \cite{johnson2016malmo} and later the MineDojo simulator \cite{fan2022MineDojo}, a series of methods \cite{tessler2017deep,shu2018hierarchical,guss2019neurips,lin2021juewu} have attempted to train an agent to complete tasks in Minecraft. Approaches such as hierarchical RL, goal-based RL, and reward shaping have been adopted to alleviate the sparse reward and exploration difficulty for the agent. Recently, DreamerV3 \cite{abs-2301-04104} succeeds in training agents in Minecraft with a learned world model. More recently, DEPS \cite{abs-2302-01560} introduces a pre-trained large language model for sub-goal planning, combined with goal-based bottom-level policy \cite{cai2023open} learned from behavior cloning of offline data to successfully complete a series of Minecraft tasks.

In addition, some works attempt to incorporate the human player experience. MineRL \cite{GussHTWCVS19} collected 60M player demonstrations with action labels, motivating some methods \cite{ShahWWMKGWWPMGF21,abs-2202-10583} based on behavior cloning. As well-labeled data is limited in content, some works instead attempt to use vast and diverse but unlabelled data from the Internet. MineDojo \cite{fan2022MineDojo} collects 730K+ narrated Minecraft videos from YouTube to learn a vision-language model (MineCLIP) providing auxiliary reward signals. VPT \cite{abs-2206-11795} uses action-labeled data to train an inverse dynamic model to label internet-scale data and then conduct behavior cloning.

Unlike these works (except MineCLIP) that incorporate the human experience and require a large number of demonstrations with action labels to train the agent, our work focuses on only using the data without action labels to assist agent learning, which is more friendly with data collection and has the potential to scale in the future.

\section{Background} \label{MineDojo}

\noindent\textbf{MineDojo tasks.}
MineDojo has provided thousands of benchmark tasks with its simulator APIs, which can be used to develop generally capable agents in Minecraft. Furthermore, the tasks can be divided into two categories, Programmatic and Creative tasks. The former has ground-truth simulator states to assess whether the task has been completed. The latter, however, do not have well-defined success criteria and tend to be more open-ended, but have to be evaluated by humans. 

We mainly focus on Programmatic tasks since they can be automatically assessed. In addition, MineDojo provides 4 categories of programmatic tasks, including Harvest, Combat, Survival, and Tech Tree, with 1,581 template-generated natural language goals to evaluate the agent’s different capabilities. Among these tasks, Survival and Tech Tree tasks are harder than Harvest and Combat. At the current stage, MineCLIP only expresses the promising potential in some Harvest and Combat tasks. Harvest means finding, obtaining, cultivating, or manufacturing hundreds of materials and objects. Combat means fighting various monsters and creatures that require fast reflexes and martial skills. 

\vspace{2mm}
\noindent\textbf{POMDP.}
We model the programmatic task as a partially observable Markov decision process (POMDP) \cite{kaelbling1998planning}. At each timestep $t$, the agent obtains the partial observation $o_t$ from the global state \(s_t\) and a language prompt \(G\), takes action \(a_{t}\) following its policy \(\pi(a_{t}|o_{t})\), and receives a reward \(r_{t} = \Phi(V_{t},G)\), where $V_t$ is the fixed-length sequence of observations till $t$ (thus a video snippet) and \(\Phi\) maps \(V_t\) and \(G\) to a scalar value. Then the environment transitions to the next state \(s_{t+1}\) given the current state and action according to transition probability function \(\mathcal{T}(s_{t+1}|s_t,a_t)\). The agent aims to maximize the expected return \(R=\mathbb{E}_\pi\sum_{t=1}^{T}\gamma^{t-1}r_{t}\), where \(\gamma\) is the discount factor and \(T\) is the episode time horizon. 

\section{YouTube Dataset}
\label{dataset}
MineDojo provides an extensive suite of APIs to interact with the Minecraft environment. However, it is still an arduous challenge for agents to complete long-term tasks with only online experiences, owing to the sparsity of rewards and the vast exploration space. Fortunately, the internet is awash with copious amounts of Minecraft-related data, which harbors a wealth of weak-labeled or even unlabelled Minecraft knowledge, including crucial entities, plausible motions, and common-sense event processes. In MineDojo \cite{fan2022MineDojo}, a total of 640K video clips (8$\sim$16 seconds/clip) sourced from 730K+ YouTube videos and their corresponding transcripts are leveraged to train a vision-language model (MineCLIP) that provides auxiliary reward signals for agents to learn task-specific strategies efficiently. Due to the limitation of computing resources, we cannot fully use 730K+ YouTube videos, which makes a gap between video clips and tasks. Thus, the quality of the vision-language model is highly sensitive to the quality of video-text pairs used for training. Since the specific video-text clip pairs used in MineDojo have not been released, we construct a neat video-text dataset on the basis of the internet-scale database. 

Our dataset construction is based on the YouTube video database collected by MineDojo, which comprises over 730K Minecraft videos with a combined duration of 33 years and 2.2B transcript words. The database is massive in scale and contains significant noise, due to its nature as an online resource. As illustrated in Figure \ref{illustration_dataset}, on one hand, some videos feature irrelevant game content that may not be conducive to learning basic game concepts. On the other hand, the alignment between the transcripts and videos may not always be precise, leading to temporal or content discrepancies that could hinder the learning of semantic events for the vision-language model. We aim to cover the essential information in the Minecraft environment, including key entities and basic semantic events, with a smaller amount of data. To achieve this, we employed a two-step filtering process to construct a neat video-text dataset.

% dataset composition and data clip process
\vspace{2mm}
\noindent\textbf{Content Filtering.}
To begin with, we employed transcript-based filtering to ensure that the video content in our dataset is relevant to the key entities in Minecraft, as shown in Figure \ref{keywords}, thus facilitating the learning of basic game concepts. As the fundamental elements of Minecraft, the key entities, \eg, stones, trees, and sheep, are shared between tasks in MineDojo and videos from YouTubers. Specifically, we identify entity keywords in the transcripts with a handmade keyword list and extract transcript clips of a fixed context length $L$ to encompass as many keywords as possible. The extracted transcript clips serve as the text part of our dataset and determine the location of corresponding video clips.

\vspace{2mm}
\noindent\textbf{Correlation Filtering.}
Secondly, we aim to find video clips that are highly correlated with the extracted transcript clips to ensure the alignment between the text and video modalities. As the reasonable start and end timestamps of the video clips do not necessarily coincide with the corresponding transcripts clips, we adopt a method of aligning the center points of the transcript and video clips for possible semantic overlap content and we extract video clips of a fixed duration $D$. However, even with this approach, we cannot guarantee consistency between the video content and the transcript content. Therefore, we utilize the pre-trained MineCLIP model to obtain the embeddings of selected video clips and transcript clips. Then, we calculate the cosine similarity distribution of their embeddings to represent their correlation. In more detail, we perform a second round of filtering, selecting from the top $k\%$ of video clips whose embedding is closest to the embedding of the corresponding transcript clips. We construct a dataset with a stronger content correlation between the video and transcript clips, as shown in Figure \ref{distribution_dataset}.

In terms of scale, we apply the aforementioned two-step approach to construct a training set of 640K video-text clip pairs and extract additional 4K pairs for validation of video-text retrieval. For the constants of the approach, we set $L$, $D$, and $k$ to $25$ words, $16$ seconds and $50$, respectively. This means that our dataset contains only 1 week's total duration of videos and 0.16B words, significantly smaller than the scale of the original database. \textit{Importantly, we will release our YouTube dataset by specifying transcript clips and the corresponding timestamps of the videos in the original database.}


\begin{figure}[t]
\centering
\vspace{-0.2cm}
\includegraphics[width=0.85\linewidth]{images/scores_all.pdf}
\vspace{-0.1cm}
\caption{Distribution of correlation score of video-text pairs with and without correlation filtering in YouTube Dataset.}
  \label{distribution_dataset}
  \vspace{-0.2cm}
\end{figure}


\section{CLIP4MC}

For the RL training procedure, we expect the learned vision-language model can provide a high-quality reward signal without any domain adaptation techniques. As it eliminates the need to manually engineer the reward function for each MineDojo task, the agent can continuously learn from open-ended tasks.

Given a video snippet \(V\) and a language prompt \(G\), the vision-language model outputs a correlation score, \(C\), that measures the similarity between the video snippet and the language prompt. Ideally, if the agent performs behaviors following the description of the language prompt, the vision-language model will generate a higher correlation score, leading to a higher reward. Otherwise, the agent will be given a lower reward.

\subsection{Overall Architecture}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{images/structure.pdf}
\caption{Overall architecture of CLIP4MC. The model consists of three encoders, \ie, video, motion, and text encoders. Note that video and motion encoders aim to extract the embeddings from the video snippet at the entity and action levels, respectively. Correlation scores are used to construct the auxiliary reward signal for RL training.}
  \label{arch}
\end{figure*}

%The video snippet can be regarded as the sequence of observations of the agent and is represented as a sequence of \(N\) consecutive frames as in previous work \cite{fan2022MineDojo}. 

Intuitively, only when the agent makes the right \textit{behavior} toward the right \textit{entity}, the agent shall be given a higher reward. CLIP4MC follows such a principle. Figure \ref{arch} depicts the overall architecture of CLIP4MC, which consists of a video encoder, a motion encoder, a text encoder, and a similarity calculator. We have two encoders to extract two different levels of video embeddings, \ie, entity level and motion level. 
%Intuitively, only when the agent makes the right \textit{behavior} toward the right \textit{entity}, the agent shall be given a higher reward. CLIP4MC follows such a principle.

\vspace{2mm}

\noindent\textbf{Video Encoder.} To generate entity-level video embedding, \(z_v\), we follow the same design as MineCLIP \cite{fan2022MineDojo}. All the video frames first go through the spatial transformer to obtain a sequence of frame features. The temporal transformer is then utilized to summarize the sequence of frame features into a single video embedding. CLIP Adapter further processes the video embedding for better features. Note that we initialize the weights of the spatial transformer from the checkpoint of CLIP \cite{RadfordKHRGASAM21} and only the last two layers are finetuned during training. Empirically, we found out the video encoder (essentially MineCLIP) can provide a bond between the entities and the language prompts and give a similar high reward as long as the target entities are in the video frames. However, such a reward is not instructive enough for RL tasks. 

\vspace{2mm}
\noindent\textbf{Motion Encoder.} Further, we explicitly pilot the temporal transformer to encode motion embeddings. Normally, two consecutive or spaced frames contain entity displacement, which implies the actual actions with different amplitudes. Motivated by this, we stack two frame features with different intervals as action representations. Note that the frame features are reused from the video encoder. The temporal transformer then processes all representations to obtain the motion features separately. %Directly differentiating two consecutive frames may also express the motion tendency \cite{fang2021clip2video}. However, this inevitably captures the movement of entities, \eg, cow. 
%the dataset only provides spaced \(N\) frames, not consecutive frames, since the input frames are uniformly sampled from longer video clips. 

For the motion features from the same interval, we again summarize them via the temporal transformer and obtain one motion embedding of this interval. The multi-interval motion embeddings, \(z_m^{1:j}\), capture richer information about actual actions. The final step is to compress the multi-interval motion embeddings into one embedding. Empirically, we found out that max or average pooling operation cannot summarize the final motion embedding well. Therefore, another fusion transformer is adopted to generate the final motion embedding, \(z_m\). Importantly, the temporal transformers are shared across different encoders, \ie, video and motion encoders, for fast convergence.  

\vspace{2mm}
\noindent\textbf{Text Encoder.}
We also directly utilize the architecture from the CLIP to extract the text embedding, \(z_t\). We initialized the weights from the checkpoint of CLIP and only the last two layers of the text encoder are finetuned during training. 

\vspace{2mm}
\noindent\textbf{Similarity Calculator.}
 After extracting the embeddings of input from different modalities, the final stage comes to the similarity calculation. The similarity function \(s(V, G)\) that measures the correlation score, \(C\), between the video snippet and the language prompt is defined as the cosine similarity:

 \begin{equation}
 \nonumber
     s(V,G) = \frac{z_v^{\top} z_t}{2 \lVert z_v \rVert \lVert z_t \rVert} + \frac{z_m^{\top} z_t}{2 \lVert z_m \rVert \lVert z_t \rVert}.
 \end{equation}

%However, the training language prompt contains lots of meaningless words as well as grammar mistakes since the text data is obtained from the Internet without a uniform template and data cleaning. 



%\subsection{Contrastive Learning}

%\subsection{Reward Generation}

\subsection{Training}

%\textbf{Date Preprocessing}


\noindent\textbf{Contrastive Learning.} For CLIP4MC training, we use a contrastive loss \cite{oord2018representation} to learn the correspondence between video snippet and language prompt. In more detail, we aim to minimize the sum of the multi-modal contrastive losses, including video-to-text, text-to-video, motion-to-text, and text-to-motion:
\begin{equation}
\begin{aligned}
\nonumber
\mathcal{L}  = & -\sum_{(z_v,z_m,z_t)\in \mathcal{B}}\Big(\log {\rm NCE}(z_v,z_t)+\log {\rm NCE}(z_t,z_v) \\& \qquad \quad + \lambda \log {\rm NCE}(z_m,z_t)+ \lambda \log {\rm NCE}(z_t,z_m)\Big),
\end{aligned}
\end{equation}
where \(\mathcal{B}\) is the batch containing sampled video-text pairs, \(\lambda\) is the weight coefficient, and \({\rm NCE}(\cdot,\cdot)\) is the contrastive loss that calculates the similarity of two inputs. To illustrate, the video-to-text contrastive loss is given by 
\begin{equation}
\nonumber
    {\rm NCE}(z_v,z_t) = \frac{\exp(z_v \cdot z_t^{+}/\tau)}{\sum_{z \in \{z_t^{+},z_t^{-}\}} \exp(z_v \cdot z/\tau)},
\end{equation}
where \(\tau\) is a temperature hyperparameter, \(z_t^{+} \)
is the positive text embedding matching with the video embedding \(z_v\), and \(\{z_t^{-}\}\) are negative text embeddings that are implicitly formed by other text clips in the training batch. Other contrastive losses, \ie, text-to-video, motion-to-text, and text-to-motion, are defined in the same way.

\vspace{2mm}
\noindent
\textbf{RL Training.} For RL training, the first step is reward generation. At timestep \(t\), we concatenate the agent’s latest 16 egocentric RGB frames in a temporal window to form a video snippet, \(V_t\). CLIP4MC outputs the probability \(P_{G,t} \) that calculates the similarity of \(V_t\) to the task prompt, \(G\), against all other negative prompts. To compute the reward, we further process the raw probability as previous work \cite{fan2022MineDojo} \(r_t = \max ( P_{G,t} -\frac{1}{N_t},0) \), where \(N_t\) is the number of prompts passed to CLIP4MC. Note that CLIP4MC can handle unseen language prompts without any further finetuning. 
 
The ultimate goal is to train a policy network that takes as input raw pixels and other structural data and outputs discrete actions to accomplish the task that is described by the language prompt, \(G\). We use Proximal Policy Optimization (PPO) \cite{schulman2017proximal} as our RL training backbone and the policy is trained on the CLIP4MC reward together with the sparse task-completion reward if any. The policy input contains several modality-specific components and more details can be found in Appendix. In addition, self-imitation learning (SI) \cite{oh2018self} is used to further improve the sample efficiency as MineCLIP \cite{fan2022MineDojo}, because computing the reward using a vision-language model in the loop makes the training more expensive. The training phases alternate between the PPO phase and the SI phase.

In the next section, we will show that CLIP4MC can provide a more reliable reward signal that accelerates the learning of RL tasks compared with other methods. Importantly, a model that can achieve better performance on video-text retrieval metrics, \eg, R@1, is not necessarily able to provide RL-friendly reward signals.

\section{Experiments}

In this section, we conduct a comprehensive evaluation and analysis of our proposed model, CLIP4MC, utilizing the open-ended platform, MineDojo. This platform features a benchmark suite comprised of thousands of diverse, open-ended Minecraft tasks designed for embodied agents. 

%Moreover, we systematically evaluate our proposed model through ablation studies. 
\vspace{2mm}
\noindent We compare CLIP4MC against the following baselines: 
\begin{enumerate}
\vspace{-1mm}
\setlength\itemsep{1mm}
\item \textbf{MineCLIP(pre-trained)} refers to the officially released MineCLIP model by MineDojo \cite{fan2022MineDojo}. It is a variant of the video-text retrieval method, CLIP4CLIP \cite{LuoJZCLDL22}.
\item \textbf{MineCLIP(scratch)} uses the same architecture as MineCLIP(pre-trained) but is trained from scratch on our YouTube dataset. It also serves as the ablation of CLIP4MC without the motion encoder.
\item \textbf{CLIP4MC-single} is another ablation of CLIP4MC, where the motion encoder only processes the motion representations with one interval.
\end{enumerate}

Note that CLIP4MC, MineCLIP(scratch), and CLIP4MC-single are trained on our YouTube dataset. Specifically, we trained these models for 20 epochs and select the models with the highest performance on RL tasks. Please refer to Appendix for more training details. All results are presented in terms of the mean and standard deviation of five runs with different random seeds.

%the officially released MineCLIP model by MineDojo as MineCLIP (pre-trained)
%Specifically, we conduct an ablative experiment in which we remove the motion encoder from our model structure, thus rendering it identical to MineCLIP's network structure. We train both the MineCLIPv2 and ablation model using our YouTube dataset for 20 epochs. Ultimately, we select the best-performing model on the test dataset to analyze and compare with MineCLIP. The results of ablation study shed light on the importance of the motion encoder in our approach and can inform future research in this area.

%To differentiate between the two models, we refer to the officially released MineCLIP model by MineDojo as MineCLIP (pre-trained), and the ablation model that we trained from scratch using our dataset as MineCLIP (from scratch). 

\begin{figure*}[t!]
	\vspace*{-0.2cm}
	\centering
	\hspace{-0.2cm}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\setlength{\abovecaptionskip}{3pt}
		\includegraphics[width=1\textwidth]{images/milk.pdf}
		\caption{Milk a Cow}
	\end{subfigure}
        \hspace{-2mm}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\setlength{\abovecaptionskip}{3pt}
		\includegraphics[width=1\textwidth]{images/wool.pdf}
		\caption{Sheer Wool}
	\end{subfigure}
        \hspace{-2mm}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\setlength{\abovecaptionskip}{3pt}
		\includegraphics[width=1\textwidth]{images/flower.pdf}
		\caption{Pick a Flower}
	\end{subfigure}

        \vspace{-1mm}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\setlength{\abovecaptionskip}{3pt}
		\includegraphics[width=1\textwidth]{images/leaves.pdf}
		\caption{Pick a Leaf}
	\end{subfigure}
        \hspace{-2mm}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\setlength{\abovecaptionskip}{3pt}
		\includegraphics[width=1.\textwidth]{images/fd_cow.pdf}
		\caption{Find a Cow}
	\end{subfigure}
        \hspace{-2mm}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\setlength{\abovecaptionskip}{3pt}
		\includegraphics[width=1.\textwidth]{images/fd_sheep.pdf}
		\caption{Find a Sheep}
	\end{subfigure}

	%\vspace*{-0.2cm}
	\caption{Learning curves of CLIP4MC and baselines in six Programmatic tasks of MineDojo. Note that CLIP4MC, CLIP4MC-single, and MineCLIP(scratch) are trained on our YouTube dataset. All results are presented in terms of the mean and standard deviation of five runs with different random seeds.} 
	\label{fig:learning_cruves}
\end{figure*}

\subsection{Environment Settings}

As mentioned in Section \ref{MineDojo}, there are four categories of Programmatic tasks, \ie Harvest, Combat, Survival, and Tech Tree. However, the goals of Survival and Tech Tree tasks are more complex and the corresponding language prompts are too abstract. Thus, it is hard for the agent to achieve the final goal without additional subgoal decomposition and planning. Surprisingly, all the methods, including MineCLIP(pre-trained), also fail in Combat tasks (nearly zero success rate). Similar results are also found in recent work \cite{abs-2301-10034}. This may be due to different experimental settings, such as episode length. A thorough investigation of Combat tasks is left as future work. 

To comprehensively evaluate our proposed model, we conduct experiments on six different RL tasks featuring diverse domain-specific entities (\eg, animals, resources, terrains, and tools). These tasks are either built-in tasks in the MineDojo benchmark suite, or designed by ourselves. Based on their semantic definitions, we group these six RL tasks into two categories: Harvest and Finding. To guarantee a fair comparison, we adopt the most hyperparameters of RL training from MineCLIP \cite{fan2022MineDojo} for all tasks and all methods. We only adjust the batch and buffer size to fit our computation resource.

\vspace{2mm}
\noindent
\textbf{Harvest Task.} There are four harvest tasks in total, each requiring the agent to obtain a specific item: one sunflower \mcsunflower, one leaf \mcleaf, one bucket of milk \mcmilk, and one wool \mcwool. The task to harvest one sunflower \mcsunflower \, was designed by ourselves, and the other three are built-in tasks in MineDojo. To successfully complete these tasks, the agent is initialized in various biomes with specific tools that are necessary to obtain the target item in Minecraft. A harvest task is regarded as successful when the target item is obtained by the agent with a specified quantity. In addition, a language prompt for the task of harvesting milk \mcmilk is, “\textit{obtain milk from a cow using an empty bucket}.”


\vspace{2mm}
\noindent
\textbf{Finding Task.} We also include two finding tasks, in which the agent is tasked with locating a specific target object, such as a cow \mccow \, or a sheep \mcsheep. These finding tasks are not from the MineDojo benchmark suite but are specially designed to assess the agent's ability to efficiently explore the environment. In Minecraft, each block is 1x1 square. To initialize our experiment, we randomly spawn a target object in a 60x60 square area, while the agent is positioned at the center of the area. A task is considered successful when two conditions are met: the target object is within the agent's field of view, and the Euclidean distance between the agent and the target object must be less than 3. We use the built-in lidar function to detect whether the target object is within the agent's field of view. A natural language prompt for the task of finding cow \mccow \, could be “\textit{find a cow in the plain and the cow is nearby}.” More details about the environment settings can be found in Appendix.



\subsection{Results}

\noindent
\textbf{RL Results.}
Figure \ref{fig:learning_cruves} shows the learning curves of all the methods regarding the success rate. CLIP4MC achieves better performance than MineCLIP(pre-trained) in all the tasks except for picking a leaf where they perform comparably. It is also shown that the RL training based on MineCLIP(pre-trained) is highly unstable. Note that these two models are trained on two different datasets. The comparison may not be fair since the difference in the key entity distribution of the dataset can affect the models' performance on different tasks. In addition, the difference in training details could also impact the final results.

Moreover, CLIP4MC outperforms MineCLIP(scratch) and CLIP4MC-simple by converging to the highest success rate in all six RL tasks. It demonstrates the benefits brought by the motion encoder. In more detail, it turns out that the usage of MineCLIP does not fully exploit the temporal relationship of video snippets. It is hard to learn motion features with a temporal transformer by only relying on video embedding. The improvement of CLIP4MC over CLIP4MC-simple is attributed to CLIP4MC's ability to capture more extensive and detailed motion features with different amplitudes. Intuitively, multi-interval action representations contain temporal correlations of short-term and long-term segments. Therefore, the motion encoder is more likely to obtain useful features that match the behaviors reflected in the language prompts. 



 \begin{figure*}[!t]
	\centering
    %\setlength{\abovecaptionskip}{5pt}
	\includegraphics[width=.93\textwidth]{images/reward_sample.pdf}
	\vspace*{-0.2cm}
	\caption{Illustration of learned reward function of CLIP4MC (\textit{upper panel}) and MineCLIP(pre-trained) (\textit{lower panel}). For each row, the first three screenshots are from Milk a Cow and the last three screenshots are from Sheer Wool. The key entities are boxed in yellow and the reward signal, \(r\), is shown in the white box.}
	\label{fig:interpre}
\end{figure*}

\vspace{2mm}
\noindent
\textbf{Video-Text Retrieval Results.}
Table \ref{tab:v2t_results} shows the results of text-to-video retrieval on the test set. Note that the \(*\) symbol means the model is trained on the YouTube dataset without correlation filtering. From the results, the models trained on the dataset without correlation filtering obtain worse performance than those trained on our YouTube dataset. In addition, they also fail in most RL tasks. The results demonstrate the superiority and necessity of our proposed YouTube dataset. 

Moreover, we found a model that can achieve better performance on video-text retrieval metrics is not necessarily able to provide RL-friendly reward signals. In more detail, CLIP4MC achieves the best performance on RL tasks, but CLIP4MC-simple obtained the best R@1 value. Through the model analysis in Section \ref{model_analysis}, we provide one of the possible answers.


\begin{table}[t]
\centering
\caption{Results of video-to-text retrieval on the test set. We train these models for 20 epochs and select the models with the highest R@1 value on the test set, respectively. The best results are shown in bold.}
\label{tab:v2t_results}
%\vspace{0.2cm}
%\footnotesize
\resizebox{1\linewidth}{!}{\begin{tabular}{cccccc}
\toprule
 Methods & R@1 \(\uparrow\) & R@5 \(\uparrow\) & R@10 \(\uparrow\) &MdR \(\downarrow\) & MnR \(\downarrow\) \\
\midrule
CLIP4MC & \(11.9\) & \(25.1\) & \(33.1\) &  \(40.0\) & \(\textbf{270.3}\)   \\
CLIP4MC-simple & \(\textbf{12.3}\) & \(25.1\)  & \(32.5\) &\(41.0\) & \(281.3\)\\
MineCLIP(scratch) & \(12.2\) & \(\textbf{26.0}\) & \(\textbf{33.4}\) &  \(\textbf{38.0}\) & \(278.1\) \\
CLIP4MC* & \(10.7\) & \(23.1\)  & \(30.5\) &\(46.0\) & \(281.8\)  \\
CLIP4MC-simple* & \(10.5\) & \(22.8\) & \(29.8\) &  \(49.0\) & \(291.6\) \\
MineCLIP(scratch)* & \(10.4\) & \(21.4\) & \(28.8\) &  \(49.0\) & \(301.5\) \\
\bottomrule
\end{tabular}}
%\vspace{-0.4cm}
\end{table}




\subsection{Model Analysis}
\label{model_analysis}
In this section, we conduct model analysis by extracting a series of screenshots in an episode. Figure \ref{fig:interpre} shows the learned reward function of CLIP4MC and MineCLIP(pre-trained) in two tasks. For MinCLIP(pre-trained), we can find that it provides a tight bond between the entities and the language prompts. However, if the agent can get a higher reward far away from the entity, it prefers to choose to stay, rather than approach the entity. Therefore, further exploration is relatively hard and this may explain its unstable learning procedure of RL tasks. 

On the contrary, though not always, CLIP4MC can learn a reward signal that gradually increases as the agent approaches the entity. Under such a reward function, agents are more likely to explore the right actions moving toward the targets. Note that we can observe a similar phenomenon in both tasks. 




\section{Conclusions}

In this paper, we first bring up three key issues we need to solve for learning a vision-language model as the universal reward function for open-ended RL tasks. To address the issue of whether the agent has made the right action toward the right target, we propose an RL-friendly vision-language model. It can align actions implicitly contained in the video and transcript clips in addition to entities. Moreover, we construct and release a neat vision-language dataset for Minecraft based on YouTube videos from MineDojo. Empirically, our method can provide a more friendly reward signal for the RL training procedure.

%\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\onecolumn

\appendix
%\onecolumn
%\section*{Appendix}
\setcounter{table}{0}
\setcounter{figure}{0}

\section{Environment Details}

\paragraph{Environment Initialization.}

Table \ref{setup-harvest} outlines how we set up and initialize the environment for each harvest task. Tabel \ref{setup-finding} outlines the environment configuration for each finding task.

\begin{table}[htbp]\centering 
\caption{Environment Setup for Harvest Tasks}
\label{setup-harvest}
\begin{tabular}{@{}cccll@{}} 
\toprule
\textbf{Harvest Item} & \textbf{Initialized Tool} & \textbf{Biome} \\ \midrule
milk \mcmilk                 & empty bucket \mcbucket             & plains          \\
wool \mcwool                 & shears \mcshear                   & plains          \\
leaf \mcleaf                & shears \mcshear                   & jungle         \\
sunflower \mcsunflower             & diamond shovel \mcshovel            & sunflower plains         \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}[htbp]\centering 
\caption{Environment Setup for Finding Tasks}
\label{setup-finding}
\begin{tabular}{@{}cccll@{}} 
\toprule
\textbf{Target Item} & \textbf{Initialized Tool} & \textbf{Biome} \\ \midrule
cow \mccow                 & barehand              & plains         \\
sheep \mcsheep            & barehand           & plains          \\ \bottomrule
\end{tabular}
\end{table}



\paragraph{Biomes.} As described in the task description, our model was tested in a simulation environment featuring three distinct terrains: jungle, plains, and sunflower plains. Each terrain presents unique challenges for our RL tasks. Specifically, while the plains offer a wider field of view, resources and targets are situated further away from the agent. Conversely, the jungle terrain features resources and targets that are closer to the agent, but the terrain is bumpier and presents additional obstacles. Figure \ref{fig:total-biome} shows the biomes of plains, sunflower plains, and jungle in Minecraft, respectively.

\begin{figure}[htbp]
   \centering
   \includegraphics[scale=0.22]{images/total-biome.png}
   \caption{Biomes of plains, sunflower plains, and jungle (from left to right).}
   \label{fig:total-biome}
\end{figure}


\paragraph{Observation Space.}
To enable the creation of multi-task and continually learning agents that can adapt to new scenarios and tasks, MineDojo provides unified observation and action spaces. The observation space mainly includes 9 parts. Table \ref{obs-space} provides detailed descriptions of the observation space. For more details, please refer to MineDojo's \cite{fan2022MineDojo} \hyperlink{https://docs.minedojo.org/sections/core_api/obs_space.html}{observation space}.

\begin{table}[htbp]
\centering
\caption{Observation Space of MineDojo Environment}
\label{obs-space}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Observation} & \textbf{Descriptions} \\ \midrule
\textbf{Egocentric RGB fram} &
  \begin{tabular}[c]{@{}l@{}}RGB frames provide an egocentric view of the running Minecraft client; \\ The shape height (H) and width (W) are specified by argument image\_size.\\ In our experiment, it is (160, 256).\end{tabular} \\
 &
   \\
\textbf{Equipment} &
  \begin{tabular}[c]{@{}l@{}}Equipment observation includes names, quantities, and durability of agent’s equipment.\\ They are flattened with the order of “main hand, foot, leg, body, head, off hand”.\end{tabular} \\
 &
   \\
\textbf{Inventory} &
  \begin{tabular}[c]{@{}l@{}}Inventory observation includes names, quantities, and durability of items in agent’s inventory. \\ There are 36 slots in the inventory, including 10 hotbar slots and 26 inventory slots.\end{tabular} \\
 &
   \\
\textbf{Inventory Change} &
  The features on inventory change can help the agent associate the inventory with its activities. \\
 &
   \\
\textbf{Voxels} &
  \begin{tabular}[c]{@{}l@{}}Voxels observation refers to the 3x3x3 surrounding blocks around the agent. \\ This type of observation is similar to how human players perceive their surrounding blocks. \\ It includes names and properties of blocks.\end{tabular} \\
 &
   \\
\textbf{Life Statistics} &
  \begin{tabular}[c]{@{}l@{}}Life statistics include agent’s health, armor, food saturation, etc. \\ It can be regarded as a vector representation of the heads-up display\end{tabular} \\
 &
   \\
\textbf{Location Statistics} &
  \begin{tabular}[c]{@{}l@{}}Location statistics include information about the terrain the agent currently occupies. \\ It also includes agent’s location and compass.\end{tabular} \\
 &
   \\
\textbf{Nearby Tools} &
  \begin{tabular}[c]{@{}l@{}}This observation indicates if a crafting table or a furnace are nearby. \\ Both are important tools for crafting/smelting new items.\end{tabular} \\
 &
   \\
\textbf{Damage Source} &
  \begin{tabular}[c]{@{}l@{}}Damage source tells information about damage taken by the agent. \\ It includes the damage amount and properties of damage.\end{tabular} 
  %\\&
   \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Action Space.}
The action space is an 8-dimensional multi-discrete space, including moving actions (forward, backward, camera actions, etc.) and functional actions (attack, use, craft, etc.).  At each step, the agent chooses one movement action and one optional functional action. Table \ref{act-space} summarizes the action space in the Minecraft environment. For more details, please refer to MineDojo's \cite{fan2022MineDojo} \hyperlink{https://docs.minedojo.org/sections/core_api/action_space.html}{action space}.

\begin{table}[htbp]
\centering
\caption{Action Space of MineDojo Environment}
\label{act-space}
\begin{tabular}{ccc}
\toprule
\textbf{Index} & \textbf{Descriptions}                        & \textbf{Num of Actions} \\ \midrule
\textbf{0}     & Forward and backward                         & 3                \\
\textbf{1}     & Move left and right                          & 3                 \\
\textbf{2}     & Jump, sneak, and sprint            & 4                       \\
\textbf{3}             & Camera delta pitch                           & 25       \\
\textbf{4}     & Camera delta yaw                             & 25                \\
\textbf{5}     & Functional actions                           & 8                  \\
\textbf{6}     & Argument for “craft”                         & 244                \\
\textbf{7}     & Argument for “equip”, “place”, and “destroy” & 36                 \\ \bottomrule
\end{tabular}
\end{table}

% 




\section{RL Training Details}

During the RL training stage, we adopt an algorithm pipeline similar to MineDojo \cite{fan2022MineDojo}. We apply both Proximal Policy Optimization (PPO) algorithm and Self-Imitation Learning (SIL) \cite{oh2018self} by storing the trajectories with high clip reward values in a buffer. We then alternated between PPO and SIL gradient steps during the training process. The hybrid method enables us to leverage their respective strengths and achieve better results than using either method alone. The training hyper-parameters for the RL task are listed in Table \ref{tab:hyper-rl}.


% \begin{algorithm}[htb] 
% \caption{ PPO-SIL Hybrid Training Pipeline} 
% \label{alg:Framwork} 
% \begin{algorithmic}[1]
% \STATE {\bf Input: } policy $\pi_\theta$, value function $V_\omega$, self-imitation buffer threshold $\lambda$, self-imitation frequency $q$ \\
% \STATE Initialize empty SI buffers $\textbf{\textit{D}}_{SI} \gets \emptyset$ \\
% \STATE Initialize an episode $counter \gets 0$\\
% %\STATE {\bf Output:} trained policy $\pi_\theta$, trained value function $V$
% \WHILE{$\textit{not done}$}
% \STATE Collect a set of trajectories $\bm{{\tau}}$ by running policy $\pi_\theta$ (in parallel environments)
% \FORALL{$\tau \in \bm{{\tau}} $ such that $\tau$ is \textit{successful} or $\tau$'s episode return $\geq$ $\mu_{return}(\textbf{\textit{D}}_{SI}) + \lambda \times \sigma_{return}(\textbf{\textit{D}}_{SI})$ }
% \STATE $\textbf{\textit{D}}_{SI} \gets \textbf{\textit{D}}_{SI} \cup \tau $
% \ENDFOR
% \STATE Increase $counter$ accordingly
% \STATE Update policy $\pi_\theta$ using PPO
% \STATE Fit $V_\omega$ by regression on mean-squared error
% \IF{$\mathbb I$ ($counter$ mod $q$ = 0)}
% \STATE Sample trajectories from each buffer $\textbf{\textit{D}}_{SI}$ in a prioritized manner
% \STATE Update $\pi_\theta$ on $\textbf{\textit{D}}_{SI}$ using SIL
% \ENDIF
% \ENDWHILE \\
% %\RETURN policy $\pi_\theta$, value function $V$
% \end{algorithmic}
% \end{algorithm}

  % We train each task for 1000 epochs, we measure the percentage of successful episodes based on predefined criteria. 


\begin{table}[htbp]\centering 
\caption{Training hyper-parameters for RL\&IL.}
~\\
\begin{tabular}{@{}cc@{}} 
\toprule
\textbf{Hyperparameter} & \textbf{Value}\\ \midrule
Optimizer & Adam \\
LR & 1e-4 \\
RL discount factor & 0.99 \\
Lambda for GAE & 0.95 \\
Steps per episode & 200 \\
Buffer size for IL & 500 \\
Imitate frequency & per 100 epochs \\
Number of workers (CPU) & 1 \\
Parallel GPUs & 1 \\
MineDojo image size & 160 × 256 \\
Clip ratio & 0.2 \\
Early stopping KL & 0.01 \\
\bottomrule
\end{tabular}

\label{tab:hyper-rl}
\end{table}



\section{Details on Dataset}

In this section, we provide more details about the dataset, including the construction process and a sample of the dataset format. The construction steps of the dataset are as follows:
\begin{enumerate}
    \item Obtain YouTube videos and corresponding transcripts from the MineDojo database.
    \item Manually construct a list of keywords related to Minecraft gameplay.
    \item For each video with a transcript, annotate all keywords (including different forms of keywords such as combined words, plural forms, etc.) appearing in the transcript.
    \item Slide a window of length $L$ words on the transcript until the first keyword in the window is about to leave the window. Use the midpoint between the first and last keyword in the window as the center to extract a transcript clip of length $L$ words.
    \item Extract all non-overlapping transcript clips from each video with a transcript following step 4.
    \item For each transcript clip, calculate the central timestamp corresponding to the clip based on the transcript timestamps. Use this central timestamp to extract a video clip of duration $D$ seconds from the video.
    \item From all the video-clip pairs extracted in the previous steps, extract $M$ pairs and encode these pairs using a pre-trained MineCLIP attention variant to calculate the cosine similarity.
    \item Select the top $k\%$ pairs with the highest cosine similarity as the training set from the $M$ pairs.
    \item Randomly select $M^\prime$ pairs in addition to the $M$ pairs as the validation set.
\end{enumerate}

The values of the parameters used in the above process are listed in Table~\ref{tab:parameters}. Specifically, steps 2-6 of the process constitute content filtering, while steps 7-8 are correlation filtering. Following this process, we construct a training set of size 640K and a test set of size 4096. Both of the filtering methods have been applied to construct the training set, while only content filtering is used for the test set to reflect the distribution of data in the database. we release our YouTube dataset by specifying the transcript
clips and the corresponding timestamps of the videos in the original database. The link is \hyperlink{https://drive.google.com/drive/folders/19vDy2jaooF74MDt3dLAsyLRpRcUFKVCY?usp=sharing}{https://drive.google.com/drive/folders/19vDy2jaooF74MDt3dLAsyLRpRcUFKVCY?usp=sharing}.

\begin{table}[htbp]\centering 
\caption{Values of parameters used in the dataset construction process.}
~\\
\begin{tabular}{@{}cc@{}} 
\toprule
\textbf{Parameter} & \textbf{Value}\\ \midrule
$L$ & 25 \\
$D$ & 16 \\
$M$ & 1,280,000  \\
$k$ & 50    \\
$M^\prime$ & 4,096\\ \bottomrule
\end{tabular}

\label{tab:parameters}
\end{table}

\section{CLIP4MC Training}

This section describes the training process of CLIP4MC. The training process for CLIP4MC was adapted from the training processes for CLIP4CLIP and MineCLIP. Practically, we trained all models on the 640K training set. For each video-text clip pair, we obtain 16 frames of RGB image through equidistant sampling and normalize each channel separately. During training, we use random resize crops for data augmentation. We use cosine learning rate annealing with 320 gradient steps of warming up. We only fine-tune the last two layers of pre-trained CLIP encoders, and we apply a module-wise learning rate decay (learning rate decays along with the modules) for better fine-tuning. Training is performed on 1 node of 4 × V100 GPUs with FP16 mixed precision via the PyTorch native \texttt{amp} module. All hyperparameters are listed in Table~\ref{tab:hyperparameters}.

\begin{table}[htbp]\centering 
\caption{Training hyperparameters for CLIP4MC.}
~\\
\begin{tabular}{@{}cc@{}} 
\toprule
\textbf{Hyperparameter} & \textbf{Value}\\ \midrule
LR schedule & Cosine with warmup \\
Warmup steps & 320 \\
LR & 1.5e-4 \\
Weight decay & 0.2 \\
Layerwise LR decay & 0.65 \\
Batch size per GPU & 100 \\
Parallel GPUs & 4 \\
Video resolution & 160 × 256 \\
Number of frames & 16 \\
Image encoder & ViT-B/16 \\
\bottomrule
\end{tabular}

\label{tab:hyperparameters}
\end{table}


\section{CLIP4MC Architecture}

\paragraph{Input.}
The length of each transcript clip is 25 words, while the length of the video is 16 seconds. The resolution of the video stream is 160 × 256, with 5 fps. In other words, the video stream is 80 frames. As for the video snippet, we further equidistantly sample it to 16 frames for fewer computing resources.

\paragraph{Text Encoder.}
Referring to the design of MineCLIP \cite{fan2022MineDojo}, the text encoder is a 12-layer 512-width GPT model, which has 8 attention heads. The textual input is tokenized via the tokenizer used in CLIP and is padded/truncated to 77 tokens. The initial weights of the model use the public checkpoint of CLIP and only finetune the last two layers during training.

\paragraph{Spatial Encoder.}
The Spatial encoder is a frame-wise image encoder referred to the design of MineCLIP \cite{fan2022MineDojo}, which uses the ViT-B/16 architecture to compute a 512-D embedding for each frame. The initial weights of the model use the public checkpoint of OpenAI CLIP, and only the last two layers are finetuned during training.

\paragraph{Temporal Transformer.}
The temporal Transformer is a shared module across video and motion encoders for fast convergence, which is a 2-depth 8-head Transformer module whose input dimension is 512 and maximum sequence length is 32.

\paragraph{Multi-interval Fusion Transformer.}
Multi-interval fusion transformer is a Transformer model similar to Temporal Transformer, which fuses motion information of different intervals. The architecture is exactly the same as the Temporal Transformer, except that the parameters are not shared.

\paragraph{Adapter.}
In order to get better embedding, we use an adapter to map video embedding, text embedding, and motion embedding. The adapter models are 2-layer MLP, except the text adapter which is an identity. 

%\paragraph{Video Clip Equidistant Sampling Details.}
%The video snippet is a 16-second, 80-frame video, and we equidistantly sample it to 16 frames in order to occupy less computing resources.


\section{Agent Architecture}

Like MineDojo \cite{fan2022MineDojo}, our policy framework is also composed of three components: an encoder for input features, a policy head, and a value function head. In order to deal with cross-modal observations, the feature extractor includes a variety of modality-specific components as described in Table \ref{policy-network}.
%\vspace{2mm}
\begin{itemize}
%\vspace{-1mm}
\setlength\itemsep{1mm}
    \item RGB frame: To optimize for computational efficiency and equip the agent with strong visual representations from scratch, we use the fixed frame-wise image encoder from CLIP4MC to process RGB frames.
    \item Yaw and Pitch: We first compute sin(·) and cos(·) features respectively, then pass them through CompassMLP.
    \item GPS: normalize and pass through GPSMLP.
    \item Voxel: To process the 3 × 3 × 3 voxels surrounding the agent, we embed discrete block names as dense vectors, flatten them, and pass them through VoxelEncoder.
    \item Previous action: Our agent relies on its immediate previous action, which is embedded and processed through PrevActionEmb, which is a conditioning factor.
    \item BiomeID: To perceive the discrepancy in different environments, we embed BiomeID as a vector through an MLP named BiomeIDEmb. 
\end{itemize}

The features from each modality are combined by concatenation, passed through an additional feature fusion network (FeatureFusion), and then directed into both the policy head and value function head. The policy head is modeled using an MLP (PolicyMLP), which transforms the input feature vectors into an action probability distribution. Similarly, ValueMLP is used to estimate the value function, conditioned on the same input features. Note that we do not use all the information of the observation space. Only partial observation information will be sent into the policy network.


\begin{table}[tbh]
\centering
\caption{Agent Networks}
\label{policy-network}
\renewcommand\arraystretch{.9}
\begin{tabular}{@{}cc@{}}
\toprule
\textbf{Network} & \textbf{Details} \\ \midrule
\textbf{Policy\&ValueMLP} &
  \begin{tabular}[c]{@{}c@{}}hidden\_dim: 256\\ hidden\_depth: 3\end{tabular}\\
 & \\
\textbf{CompassMLP} & \begin{tabular}[c]{@{}c@{}}hidden\_dim: 128\\ output\_dim: 128\\ hidden\_depth: 2\end{tabular} \\
 &
   \\
\textbf{GPSMLP} &
  \begin{tabular}[c]{@{}c@{}}hidden\_dim: 128\\ output\_dim: 128\\ hidden\_depth: 2\end{tabular}                 \\
 &
   \\
\textbf{VoxelEncoder} &
  \begin{tabular}[c]{@{}c@{}}embed\_dim: 8\\ hidden\_dim: 128\\ output\_dim: 128\\ hidden\_depth: 2\end{tabular} \\
 &
   \\
\textbf{BiomeIDEmb} &
  embed\_dim: 8                                                                                                  \\
 &
   \\
\textbf{PrevActionEmb} &
  embed\_dim: 8                                                                                                  \\
 &
   \\
\textbf{FeatureFusion} &
  \begin{tabular}[c]{@{}c@{}}output\_dim: 512\\ hidden\_depth: 0\end{tabular}
  %\\&
   \\ \bottomrule
\end{tabular}
\end{table}

\end{document}

