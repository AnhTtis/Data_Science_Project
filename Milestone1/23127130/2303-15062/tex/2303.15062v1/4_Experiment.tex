\section{Experiments}

\subsection{Datasets}
We evaluate our method on the COCO 2017 dataset~\cite{(coco)lin2014microsoft} that contains 118,287 training samples and 5,000 validation samples for 80 common object categories.
To validate our method under the WSSIS regime, we randomly sample subsets containing 1\%, 2\%, 5\%, 10\%, 20\%$\sim$50\% of the COCO training dataset.
COCO 10\% means using 10\% of the fully labeled data and the rest of 90\% of the point labeled data.
We use a centroid point of an instance mask label as a point label.
In addition, we conduct experiments on BDD100K dataset~\cite{(bdd100k)yu2020bdd100k}, which is a large-scale driving scene dataset with diverse scene types and 8 classes.
The BDD100K dataset contains 7k mask-labeled images and 67k box-labeled images, and we use the center of the box as the point label for this dataset.


\subsection{Implementation Details}
We adopt SOLOv2~\cite{(solov2)wang2020solov2} as the baseline instance segmentation network since it is a point-based and box-free straightforward method.
For both teacher and student networks, we use the same ResNet-101~\cite{(resnet)he2016deep} backbone network and follow the default training recipe and network setting as in \cite{(solov2)wang2020solov2}.
For the MaskRefineNet, we adopt the ResNet-101 FPN~\cite{(fpn)lin2017feature} architecture and produce the output only from the highest resolution pyramid level, P2.
We set the batch size of 16, the learning rate of 1e-4 with cosine decay scheduling, dice loss~\cite{(dice_loss)milletari2016v}, and input size of 256$\times$256 for training the MaskRefineNet.
After training the teacher network, the MaskRefineNet is trained by taking the rough mask outputs from the teacher network.
We implement the proposed method using Pytorch~\cite{(pytorch)paszke2019pytorch} and train on 8 V100 GPUs.

Following the labeling budget calculation in \cite{(what_point)bearman2016s, (budget_aware)bellver2019budget}, we estimate the labeling budget for the COCO trainset as follows: Full mask ($645.9s/img$), Bounding box ($127.5s/img$), Point ($87.9s/img$), Image-level ($80s/img$).
Detailed calculation method is described in our supplementary material.

\input{sources/fig_result_coco}

\subsection{Experimental Results}

We compare the proposed method against two baselines with the same network architecture and optimization strategy.
The first is training with only fully labeled data, and the second is training with fully labeled and unlabeled data, which is a semi-supervised setting.
For the second baseline, we generate pseudo instance masks for the unlabeled data from the teacher network without any weak labels.
As shown in Figure \ref{fig:result_coco}, our method achieves remarkable performances on all COCO subsets.
Especially, the performance gap between ours and the baselines is notably larger when we use smaller subsets with fully labeled images, $e.g.,$ COCO 1\% or 5\%.
Compared to the fully-supervised setting (COCO 100\%), our method with COCO 50\% shows a highly competitive result (38.8\% vs. 39.7\%).
Moreover, the qualitative results in Figure \ref{fig:abs_coco_percent} show that ours with COCO 5\% can properly segment instances of various sizes.
These results demonstrate that the cost-efficient point labels can be leveraged as an effective source for instance segmentation.

We also compare against other methods that use various weak labels based on the labeling budget in Table \ref{tab:comparison_budget}.
According to the type and the number of labels, we calculate the labeling budget following the aforementioned cost.
All methods use the same amount of total training images, so the labeling time of unlabeled data is treated as zero.
Compared to the state-of-the-art semi-supervised method, NB~\cite{(NB)wang2022noisy}, ours show superior performances when using the same amount of fully labeled data, especially when using 5\% of fully labeled data (33.7\% vs. 25.6\%).
We also achieve higher performance with a lower labeling budget (35.8\% with a budget of 196.7 vs. 35.5\% with a budget of 265.2).
In addition, compared to the state-of-the-art box-supervised method, BoxInst~\cite{(boxinst)tian2021boxinst}, our efficiency is better (33.7\% with a budget of 158.5 vs. 33.2\% with a budget of 174.5).
This result demonstrates the effectiveness of budget-friendly point labels with the proposed method.
Furthermore, we emphasize the potential of our method for performance improvement when using more labeling budget.

\input{sources/tab_result_coco_testdev}
\input{sources/tab_merge_three}
\input{sources/tab_effect_threshold}

Also, we conduct experiments on BDD100K dataset.
As we increase the amount of point labeled data with a fixed amount of 7k fully labeled data, the performance is gradually improved, as in Table \ref{tab:result_BDD100K}.
Especially, when leveraging all available point labels (67k), ours can achieve significant performance improvements compared to using only 7k fully labeled data (22.1\%{$\rightarrow$}27.9\%).

\subsection{Ablation Study}
We conduct an ablation study of our method on the COCO 10\% setting.
Unless otherwise specified, we measure the quality of pseudo labels generated by the teacher network using randomly sampled 5,000 images in the rest 90\% of COCO data, we name it COCO \textit{train5K}.

\noindent \textbf{Effect of Point Labels.} 
In Table \ref{tab:ablation_threshold}, we verify the effectiveness of each weak label candidate ($i.e.,$ unlabeled, image-level, and point label) in instance segmentation.
For this analysis, we measure the quality of pseudo labels and the performance of the student network on the COCO 2017 validation set.
When the unlabeled data is leveraged as a weak label, we should carefully tune the confidence threshold to balance between false-negative and false-positive proposals; the average recall ($AR_{100}$) and precision ($AP$) largely vary according to the confidence threshold.
It implies that human effort for tuning the threshold is required for target datasets, and this global threshold may not be optimal for every instance.
Leveraging the image-level label as a weak label can eliminate the misclassified proposals, boosting the performance from 25.9\% to 29.5\%.
However, the performance gap with the fully-supervised setting is still significant (29.5\% vs. 39.0\%).
When we leverage the point label as a weak label, we filter out the proposals to keep only true-positive proposals, deprecating the requirement of the confidence threshold.
It makes a more straightforward and effective pipeline, resulting in 32.2\%.
Compared to the annotation cost of the image-level label (80 $s/img$), the point label is still budget-friendly (87.9 $s/img$) and gives a noticeable performance improvement (32.2\% vs. 29.5\%).
Moreover, our point-guided MaskRefineNet further reduces the performance gap with the fully-supervised setting (35.5\% vs. 39.0\%).
This result demonstrates that our method can effectively leverage the point label for cost-efficient and high-performance instance segmentation.


Furthermore, we test the robustness of our method to the position of the point label.
We originally used the centroid point of each instance as our point label. 
For the analysis, we randomly choose one pixel in an instance mask as a point label five times and measure the average quality of the pseudo labels.
As shown in Table \ref{tab:robustness_point}, the performance gap between the center point and the random point is marginal.
The reason is that all pixels included in the instance region within the proposal branch are trained to generate instance proposals, as in \cite{(solov2)wang2020solov2}.
This result demonstrates the robustness of our method to the position of the point labels, which gives us more opportunity to reduce the annotation effort.

\input{sources/tab_result_BDD100K}
\input{sources/fig_visualize_coco_subsets}

\noindent \textbf{Effect of Adaptive Pyramid-Level Selection.} 
We quantitatively analyze the behavior of the FPN in Table \ref{tab:ablation_pyramid}.
When we produce pseudo instance masks from a single layer feature map, $i.e.,$ without FPN, we achieve an unsatisfactory pseudo label quality of 23.4\% as shown in the first row in Table \ref{tab:ablation_pyramid}.
When we generate pseudo masks from all pyramid feature maps (P2$\sim$P6), we achieve an inferior quality of 10.3\% because the outputs from unfit pyramid levels are pretty noisy, as shown in Figure \ref{fig:pyramid}.
Using our Adaptive Pyramid-Level Selection strategy, we choose one appropriate pyramid level based on the reliability of the network, achieving the improved quality of 28.6\%.
The result demonstrates that the proposed strategy is highly effective in leveraging the behavior of the FPN structure for generating high-quality pseudo labels.
Also, the result of 30.9\% when using ground-truth instance size information leaves us room for improvement of our method.

\noindent \textbf{Effect of MaskRefineNet.}
In Figure \ref{fig:result_coco}, we conduct experiments on various subsets without the MaskRefineNet.
When the teacher network has enough mask representation ability as in the COCO 50\% setting, the improvement of the MaskRefineNet is marginal (38.3\% vs. 38.8\%).
However, the MaskRefineNet yields a considerable performance improvement, especially in the limited number of fully labeled data settings, $e.g.,$ COCO 1\% (14.3\%{$\rightarrow$}24.0\%) and COCO 5\% (29.0\%{$\rightarrow$}33.7\%) settings. 
The result demonstrates that MaskRefineNet is a remarkably effective method to improve the quality of pseudo labels in the limited quantity of fully labeled data conditions.

In addition, we analyze the effect of input sources of the MaskRefineNet in Table~\ref{tab:ablation_maskrefinenet}.
Before applying the MaskRefineNet, the quality of pseudo labels is measured as 28.6\%.
When the MaskrefineNet only takes an image as input, the accuracy of the pseudo labels drastically reduces to 14.8\% since the network fails to converge due to the absence of prior knowledge.
When taking the rough mask as an input source, the quality of the pseudo labels improves from 14.8\% to 29.7\% because the MaskRefineNet takes the knowledge of the teacher network for fast and stable convergence.
However, the improvement is still minor compared to the model without MaskRefineNet.
When additionally taking the point information as an input source, the quality of pseudo labels dramatically improves to 39.1\%.
The reason is that the point information is used as a guidance seed for the target instance, helping a more accurate segment of occluded instances and refining the missing predictions in the rough mask, as shown in Figure \ref{fig:quality_of_maskrefinenet}.