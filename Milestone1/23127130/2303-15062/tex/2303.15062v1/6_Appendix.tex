\clearpage

\section*{Appendix: Additional Experimental Details}

\paragraph{Labeling Budget Calculation.}
Seminar works~\cite{(what_point)bearman2016s, (budget_aware)bellver2019budget} offered the annotation time of various labeling sources ($e.g.,$ full mask, bounding box, point, image-level labels) on Pascal VOC dataset~\cite{(voc)everingham2010pascal}.
Since the COCO dataset~\cite{(coco)lin2014microsoft} we used has more categories and instances per image than the VOC dataset, we estimate the labeling budget for the COCO dataset following their budget calculation method.
The COCO 2017 trainset has a total of 80 categories and contains 118,287 images and 860,001 instances.
Also, it has an average of 7.2 instances and 2.9 categories per image.
By considering this statistic of COCO dataset, we calculate the labeling budget as follows: 
\begin{itemize}
    \item \textbf{Full mask}: $77.1 classes/img \times 1s/class+7.2inst/img \times 79s/mask=\textbf{645.9s/img}$.
    \item \textbf{Bounding box}: $77.1 classes/img \times 1s/class+7.2inst/img \times 7s/bbox=\textbf{127.5s/img}$.
    \item \textbf{Point}: $77.1 classes/img \times 1s/class+2.9 classes/img \times 2.4s/point+(7.2inst/img-2.9 classes/img) \times 0.9s/point=\textbf{87.9s/img}$.
    \item \textbf{Image-level}: $80 classes/img \times 1s/class=\textbf{80s/img}$.
\end{itemize}

\paragraph{Input of MaskRefineNet.}
In this section, we further provide the details about the input sources for MaskRefineNet.
After training the teacher network using the fully labeled data, we generate instance mask outputs for the point-guided filtered proposals ($i.e.,$ true-positive proposals) using the trained teacher network.
We treat the mask outputs as rough masks to be used as the input source of the MaskRefineNet.
For each rough mask, we loosely crop each instance region in the input image, rough mask, and point heatmap.
Specifically, after obtaining the bounding box from the rough mask using the min-max operations, we re-scale the size of the box to double, and then we use this box region as the cropping region.
In addition, for the point heatmap, we encode each point to a 2-dimensional gaussian kernel with a sigma of 6, as done in ~\cite{(centermask)wang2020centermask,(centernet)zhou2019objects}.
We concatenate the three input sources ($i.e.,$ cropped input image $\mathcal{R}^{H{\times}W{\times}3}$, cropped rough mask $\mathcal{R}^{H{\times}W{\times}1}$, and cropped point heatmap $\mathcal{R}^{H{\times}W{\times}C}$) to be the input tensor $\mathcal{R}^{H{\times}W{\times}(3+1+C)}$ of the MaskRefineNet, where $C$ is the number of classes.


\section*{Appendix: Additional Analysis}

\paragraph{Effect of the input size of MaskRefineNet.}
We originally set the input size of MaskRefineNet to 256{$\times$}256. Here, we change the input size to verify its effect on the WSSIS result in table~\ref{tab:ablation_size_maskrefinenet}.
For this, we train the MaskRefineNet using the input size of 128{$\times$}128 or 384{$\times$}384.
We measure the AP result of the student network trained with the pseudo and full labels on the COCO 2017 validation set.
Consequently, the 256{$\times$}256 size yields the best AP score of 35.5\% but its performance gap with the 384{$\times$}384 size is marginal (35.5\% vs 35.4\%).

\input{sources/tab_input_size_maskrefinenet}
\input{sources/tab_iterative_training}
\input{sources/tab_result_coco_testdev_supp}

\paragraph{Effect of iterative training strategy.}
Some weakly-supervised methods~\cite{arun2020weakly, wang2018weakly,khoreva2017simple} utilize iterative training strategy; after training the target network, they generate pseudo labels using the target network, and then they newly train the target network using the pseudo labels.
This strategy could give additional performance improvement but demands a more complex training pipeline.
In this work, we suffer from the insufficient mask representation of the network when the amount of fully labeled data is extremely limited ($e.g.,$ COCO 1\%).
Although we can alleviate the problem with the proposed MaskRefineNet, we additionally try to adopt this strategy since we assume that the trained student network may have stronger mask representation ability than the teacher network.
For this, after training the student network, we newly generate pseudo instance masks for point labeled images.
Using both full labels and new pseudo labels, we train a new student network.
As the results in table~\ref{tab:abs_iterative_training}, the iterative training strategy yields meaningful improvements on tiny fully labeled data conditions (COCO 1\%: 23.9\%$\rightarrow$25.6\%).
However, there is no significant performance improvement for subsets above COCO 30\%.
This result demonstrates that (1) the iterative training strategy is helpful only when the amount of fully labeled data is extremely limited, (2) in more generous conditions such as COCO 30\% and 50\%, our MaskRefineNet is enough to replenish the mask representation of the network.

\noindent \textbf{Additional Comparison with weakly-supervised method}:
Point-sup~\cite{cheng2022pointly} introduced a new type of weak supervision source, multiple (10) points.
They achieved remarkable instance segmentation results with a highly reduced annotation cost.
To compare with them, we estimate the annotation time for 10-points according to the literature; they labeled 10-points in the bounding box region.
\begin{itemize}
    \item \textbf{10 Points}: $77.1 classes/img \times 1s/class+7.2inst/img \times (7s/bbox + 10 points \times 0.9s/point)=\textbf{192.3s/img}$.
\end{itemize}
In table~\ref{tab:comparison_budget_supp}, we provide the results for weakly-supervised methods and ours on COCO \textit{test-dev} in terms of accuracy and labeling budget.
Although Point-sup shows a slightly better efficiency than ours (37.7\% with a budget of 263.2 days vs. 37.1\% with a budget of 273.1 days), we argue that our training setting is more applicable for the current dataset conditions than them because they require newly annotating of 10-points.
Also, we show the possibility for more performance improvement up to 38.8\%, which is highly close to the result of the fully-supervised setting.
Furthermore, they give us a new future direction; incorporating 10-points and single-point without any mask labels.

\input{sources/tab_comparision_with_WSSOD}

\noindent \textbf{Comparison with weakly semi-supervised object detection methods}: 
In our main paper, we discussed the weakly semi-supervised object detection (WSSOD) methods~\cite{(point_detr)chen2021points,(group_rcnn)zhang2022group}, which used the box labels as strong labels and the point labels as weak labels.
Since the instance segmentation covers object detection, we measure our performance on the COCO \textit{test-dev} object detection benchmark.
For this, we use the min-max points from the instance mask output as our bounding box output.
Even though our strong label is different from theirs (full mask vs. bounding box), the results in table~\ref{tab:comparision_WSSOD} show that ours can surpass the state-of-the-art WSSOD performance.
We note that all methods use the same ResNet-50~\cite{(resnet)he2016deep} backbone network and the same amount of total strong and weak labels.

\noindent \textbf{Qualitative analysis for the effect of input sources of MaskRefineNet}: 
In Table~\ref{tab:ablation_pyramid} of our main paper, we provided the quantitative analysis of the effect of input sources of MaskRefineNet.
Here, we supplement our analysis with the qualitative results according to the input sources of the MaskRefineNet in Figure~\ref{fig:abs_of_maskrefinenet}.
When given all three informative input sources, the MaskRefineNet can produce high-quality refined masks by separating overlapping instances and removing noisy pixels.

\noindent \textbf{Qualitative comparison of baselines and our WSSIS method.} 
In Figure~\ref{fig:result_coco} of our main paper, we provided the AP evolution of two baselines and our WSSIS method according to the COCO subsets.
In Figure~\ref{fig:quality_various_supervision_sources_supp}, we provide the qualitative results of two baselines and our method under the COCO 10\% setting.
There are four types of methods: (a) training with fully labeled data only, (b) training with fully labeled data and unlabeled data, (c) training with fully labeled data and point labeled data, and (d) training with fully labeled data and point labeled data along with our point-guided MaskRefineNet.
The results demonstrate that the network trained with our method can be guided with higher-quality pseudo labels, resulting less false-positive and false-negative outputs.

\noindent \textbf{Additional qualitative results on COCO dataset.}
In Figure~\ref{fig:abs_coco_percent_supp}, we provide additional qualitative results of ours trained with 5\%, 20\%, and 50\% COCO subsets. 

\noindent \textbf{Qualitative results on BDD100K dataset.}
We qualitatively analyze the effect of leveraging point labels for the instance segmentation model using the BDD100K dataset~\cite{(bdd100k)yu2020bdd100k}.
There are two types of networks: the first is the network trained with only 7K fully labeled data, and the second is the network trained with 7K fully labeled data and 67K point labeled data.
As shown in Figure~\ref{fig:quality_bdd_supp}, due to our effective leveraging of the point labels, the second network is much more robust to large and small instances and occluded instances.

\input{sources/fig_maskrefinenet_input_sources}
\input{sources/fig_visualize_training_scheme}
\input{sources/fig_visualize_coco_subsets_appendix.tex}
\input{sources/fig_visualize_bdd}
