% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{CJKutf8}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4387} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{DeepVecFont-v2:\\ Exploiting Transformers to Synthesize Vector Fonts with Higher Quality}

\author{Yuqing Wang$^{1,2}$, Yizhi Wang$^1$, Longhui Yu$^2$, Yuesheng Zhu$^2$, Zhouhui Lian$^1$\thanks{Corresponding author. E-mail: lianzhouhui@pku.edu.cn}\\
$^1$Wangxuan Institute of Computer Technology, Peking University, China\\
$^2$School of Electronic and Computer Engineering, Peking University, China\\
%Institution1 address\\
%{\tt\small \{wyq,yulonghui\}@stu.pku.edu.cn, \{wangyizhi,zhuys,lianzhouhui\}@pku.edu.cn,}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
    
    Vector font synthesis is a challenging and ongoing problem in the fields of Computer Vision and Computer Graphics.
    The recently-proposed DeepVecFont~\cite{wang2021deepvecfont} achieved state-of-the-art performance by exploiting information of both the image and sequence modalities of vector fonts. However, it has limited capability for handling long sequence data and heavily relies on an image-guided outline refinement post-processing. Thus, vector glyphs synthesized by DeepVecFont still often contain some distortions and artifacts and cannot rival human-designed results. To address the above problems, this paper proposes an enhanced version of DeepVecFont mainly by making the following three novel technical contributions. First, we adopt Transformers instead of RNNs to process sequential data and design a relaxation representation for vector outlines, markedly improving the model's capability and stability of synthesizing long and complex outlines. Second, we propose to sample auxiliary points in addition to control points to precisely align the generated and target Bézier curves or lines. Finally, to alleviate error accumulation in the sequential generation process, we develop a context-based self-refinement module based on another Transformer-based decoder to remove artifacts in the initially synthesized glyphs. Both qualitative and quantitative results demonstrate that the proposed method effectively resolves those intrinsic problems of the original DeepVecFont and outperforms existing approaches in generating English and Chinese vector fonts with complicated structures and diverse styles. 
    
    
\end{abstract}
% Scalable Vector Graphics (SVG) is a commonly used representation in Computer Graphics, which comprises a series of drawing commands with numerical arguments. The key of high-quality vector font generation in SVG format lies in two aspect: 1) smooth track of individual drawing sequences; 2) smooth transitions between adjacent sequences. Previous methods the sequence are connected by shared endpoints

% rely on the shared endpoints between adjacent sequence, which causes accumulated biases from the original drawing track, resulting in suboptimal fonts with distortions and artifacts. To address this issue, we model each sequence as individual shape segment separately, and adopt transformer encoder-decoder architecture to explore each segment and connections between them. 



%is an external characteristic of text, 
%conveying rich emotions in the process of creating, reading and writing.
% needing to design the spatial layout of components and strokes with a consistent style
\section{Introduction}

%Designing high-quality vector fonts is a time-consuming task, requiring extensive experience and professional skills from designers.
%Automated font generation aims to simplify and facilitate the font designing process: learning font styles from a small set of user-provided glyphs and then generating the whole font libraries. However, automatic vector font generation still faces enormous challenges due to the variety of topology structures, sequential lengths, styles, together with the writing systems.

Vector fonts, in the format of Scalable Vector Graphics (SVGs), are widely used in displaying documents, arts, and media contents. However, designing high-quality vector fonts is time-consuming and costly, requiring extensive experience and professional skills from designers.
Automatic font generation aims to simplify and facilitate the font designing process: learning font styles from a small set of user-provided glyphs and then generating the complete font library. until now, there still exist enormous challenges due to the variety of topology structures, sequential lengths, and styles, especially for some writing systems such as Chinese.

%\vspace{-0.5cm}
\begin{figure}[t!]
   % \begin{center}
   %\hspace{-0.8cm}
    \includegraphics[width=\columnwidth]{page1-new.pdf}
   %\end{center}
   %\vspace{-0.25cm}
       \caption{Visualization of the vector glyphs synthesized by DeepVecFont and Ours, where different colors denote different drawing commands. (a) DeepVecFont w/o refinement suffers from location shift. (b) DeepVecFont w/ refinement has both over-smoothness (see green circles) and under-smoothness (see blue circles). (c) Our method can directly synthesize visually-pleasing results with compact and coordinated outlines. Zoom in for better inspection.}
    
    \label{page1}
    \end{figure}
%without the needing for differentiable rendering.     
%SVG-VAE developed an image variational autoencoder(VAE)\cite{2014Autoencoder} architecture to learn the class-independent latent code, and employed stacked LSTM\cite{1997Lstm} followed by a Mixture Density Network\cite{1994Mixture} to generate the vector sequence. DeepSVG\cite{carlier2020deepsvg} constructed a novel hierarchical generative network based on Transformer to model sequence for vector graphic generation. %due to the encoded single modality (either image or vector) could not provide sufficient information for better feature learning. 



Recent years have witnessed significant progress~\cite{svgvae, carlier2020deepsvg} made by deep learning-based methods for vector font generation. Nevertheless, vector fonts synthesized by these existing approaches often contain severe distortions and are typically far from satisfactory. More recently, Wang and Lian~\cite{wang2021deepvecfont} proposed DeepVecFont that utilizes a dual-modality learning architecture by exploiting the features of both raster images and vector outlines to synthesize visually-pleasing vector glyphs and achieve state-of-the-art performance.
However, DeepVecFont tends to bring location shift to the raw vector outputs (Fig. \ref{page1}(a)), which are then further refined according to the synthesized images. Specifically, DeepVecFont adopts a differentiable rasterizer~\cite{2020diffvg} to fine-tune the coordinates of the raw vector glyphs by aligning their rasterized results and the synthesized images. However, after the above process, the refined vector outputs tend to be over-fitted to the inherent noise in the synthesized images. Thus, there often exist suboptimal outlines (Fig. \ref{page1}(b)) with over-smoothed corners (green circles) or under-smoothed adjacent connections (blue circles) in the final synthesized vector fonts, making them unsuited to be directly used in real applications.

% 我们的方法：
To address the above-mentioned issues, we propose a new Transformer-based~\cite{2017Attentionisallyouneed} encoder-decoder architecture, named DeepVecFont-v2, to generate high-fidelity vector glyphs with compact and coordinated outlines. 
% 贡献1：relaxation Representation
Firstly, we observed that the commonly used SVG representation, which shares the same starting and ending points between adjacent drawing commands, is more suitable for the learning process of RNNs~\cite{1997Lstm} than Transformers~\cite{2017Attentionisallyouneed}.
RNNs simulate a recurrent drawing process, where the next movement is determined according to the current hidden state fed with the current drawing command. Therefore, the starting point of the following drawing command can be omitted (replaced by the ending point of the current drawing command).
On the contrary, Transformers make drawing prediction based on the self-attention operations performed on any two drawing commands, whether adjacent or not. Therefore, to make the attention operator receive the complete information of their positions, the starting point of each drawing command cannot be replaced by the ending point of the previous command. Based on the above observation, we propose a relaxation representation that models these two points separately and merges them via an extra constraint.
%  In the inference stage, this representation will certainly bring the location shift the drawing trajectory (Fig. \ref{ya}) if a wrong prediction is made. 

Secondly, although the control points of a Bézier curve contain all the primitives, we found that the neural networks still need more sampling points from the curve to perform a better data alignment.
Therefore, we sample auxiliary points distributed along the Bézier curves when computing the proposed Bézier curve alignment loss. 
% force the trajectory of each sequence precisely aligned with the target.

Thirdly, to alleviate the error accumulation in the sequential generation process, we design a self-refinement module that utilizes the context information to further remove artifacts in  the initially synthesized results.

Experiments conducted on both English and Chinese font datasets demonstrate the superiority of our method in generating complicated and diverse vector fonts and its capacity for synthesizing longer sequences compared to existing approaches. To summarize, the major contributions of this paper are as follows:

\begin{itemize} 
    \item[-] We develop a Transformer-based generative model, accompanied by a relaxation representation of vector outlines, to synthesize high-quality vector fonts with compact and coordinated outlines.
    % \item[-] We propose to sample auxiliary points in addition to control points to precisely align the generated and target Bézier curves or lines.
    % \item[-] We design a context-based self-refinement module to fully utilize the context information to remove artifacts in the initially synthesized glyphs. 
    \item[-] We propose to sample auxiliary points in addition to control points to precisely align the generated and target
outlines, and design a context-based self-refinement module to
fully utilize the context information to further remove artifacts.
    %bidirectional 
    %Finally, to overcome the accumulated errors in the sequential generation process, we develop a parallel refinement module based on another Transformer decoder to remove some artifacts in the initially synthesized results.
    \item[-] Extensive experiments have been conducted to verify that state-of-the-art performance can be achieved by our method in both English and Chinese vector font generation.
\end{itemize} 


\section{Related Work}

%Font generation methods aim to synthesize fonts typically consisting of raster images or vector glyphs.
Font generation methods can be roughly classified into approaches that aim to synthesize fonts consisting of raster images and vector glyphs, respectively.

%Glyph image generation methods aim to generate a raster bitmap of the characters. 
%Early glyph image generation methods, such as Rewrite~\cite{tian2016rewrite}, proposed to generate Chinese fonts by transferring source glyphs with the standard font style to target glyphs with desired font styles based on CNN architecture, but it could only be transferred to one target font style in a training process. 
In recent years, glyph image synthesis methods typically draw inspirations from recent advances in deep generative models such as VAEs~\cite{2014Autoencoder} and GANs~\cite{2014Generative}.
% and Jiang et al.~\cite{jiang2017dcfont} 
Tian et al.~\cite{tian2017zi2zi} and Lyu et al.~\cite{lyu2017auto} employed the framework of pix2pix~\cite{pix2pix} based on cGAN~\cite{mirza2014conditional} to transfer a template font to target fonts with desired styles. Zhang et al.~\cite{emd} proposed EMD to explicitly separate the style and content features of glyph images.
%, and then combine them to learn the mapping between standard fonts and stylized fonts based on an image-to-image translation architecture.
Mc-GAN~\cite{azadi2018multi} and AGIS-Net~\cite{gao2019agisnet} decomposed the pipeline of artistic font generation into glyph image synthesis and texture transfer.
Wang et al.~\cite{wang2020attribute2font} presented Attribute2font, a cGAN-based network to synthesize fonts according to user-specified attributes and their corresponding values. %,删了park2021multiple
For Chinese font generation, Park et al.~\cite{park2021few} and Kong et al.~\cite{kong2022look} proposed to fully exploit the component and layout information in Chinese glyphs.
Xie et al.~\cite{xie2021dg} proposed a deformable generative network which can synthesize target glyph images without direct supervision.
Tang et al.~\cite{tang2022fewshot} adopted a cross-attention mechanism for a fine-grained local style representation to generate target glyph images. \par
%However, these methods can only synthesize raster images which .
%Benefiting from the scale-invariant representation, vector glyphs can be rendered at arbitrary resolution without aliasing.
There is growing interest in the task of vector font synthesis, which can directly deliver results with scale-invariant representation.
Suveeranont and Igarash~\cite{suveeranont2010example} proposed to represent the user-defined character example as a weighted sum of the outlines and skeletons from the template fonts, and apply the weights to all characters to generate the new font. Campbell and Kautz~\cite{fontmf} aimed to learn a font manifold from existing fonts and create new font styles by interpolation and extrapolation from the manifold. SketchRNN~\cite{ha2017sketchrnn} employed a sequence VAE based on bi-directional RNNs %~\cite{1997BidirectionalRnn} 
to generate sketches. Easyfont~\cite{lian2018easyfont} decomposed the handwriting style into the shape and layout style of strokes to generate personal handwriting Chinese characters. SVG-VAE~\cite{svgvae} developed an image autoencoder architecture to learn style vectors of fonts, and then used LSTMs~\cite{1997Lstm} followed by a Mixture Density Network~\cite{1994Mixture} to generate the SVG drawing sequence. DeepSVG~\cite{carlier2020deepsvg} adopted a hierarchical generative network based on Transformers to generate vector icons with multiple paths. Im2Vec~\cite{reddy2021Im2Vec} can directly generate vector graphics from raster training images without explicit vector supervision. DeepVecFont~\cite{wang2021deepvecfont} exploited both the image and sequence modalities to facilitate the synthesis of vector glyphs. Liu et al.~\cite{implicit} proposed to represent glyphs implicitly as shape primitives enclosed by several quadratic curves, followed by a style transfer network to render glyph images at arbitrary resolutions. More recently, Aoki and Aizawa~\cite{svgforcn} introduced AdaIn \cite{adain} into the pipeline of DeepSVG to synthesize Chinese vector fonts. However, there still often exist non-negligible distortions and artifacts on the vector glyphs synthesized by the above-mentioned existing approaches. 
%by introducing for better style transfer. 
% Most of the above methods adopted the SVG representation from SVG-VAE \cite{svgvae} that shares the same starting/ending points between adjacent drawing commands, which may lead to location shift. In contrast, we emphasize that each sequence should be represented as individual primitives with their own starting and ending points, together with the proposed Bézier curve alignment loss and on-line refinement module to synthesize glyphs with compact and coordinated outlines. 

\begin{figure*}[t!]
\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}# ,angle=90
\includegraphics[width=\textwidth]{architecture.pdf}
\end{center}
%It receives a few reference glyphs as input, and outputs the synthesized glyph images and vector glyphs. Then the 
\vspace{-0.3cm}
   \caption{The pipeline of our DeepVecFont-v2. The inputs are reference glyphs in both raster images and vector outlines. (a) A dual-branch architecture based on Transformers and CNNs aims to synthesize the target vector glyph. (b) The self-refinement module is designed to remove artifacts in the initially synthesized vector glyphs. (c) In addition to control points, auxiliary points are sampled to align the synthesized glyph with the corresponding target via the Bézier curve alignment loss.
   }

\label{architecture}
\end{figure*}
    
\begin{figure}[t!]
    \begin{center}
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=\columnwidth]{ba-draw.pdf}
    \end{center}
   % \vspace{-0.3cm}
       \caption{An illustration of our data structure used to describe vector glyphs. The command type and coordinates are shown below each canvas. The origin of the axes is at the top left of each canvas and ``$\emptyset$" denotes the unused argument.}
    
    \label{data_struc_ilus}
    \end{figure}



\section{Method}
In this section, we present the details of our proposed DeepVecFont-v2. Specifically, we first introduce the data structure and the learned SVG embeddings of vector glyphs. Then, we describe the overview of our network architecture and the implementation details of each module.

\subsection{Data Structure and SVG Embeddings}
%\textbf{Relaxation Representation}
A vector font is a set of vector glyphs $\{G_1,...,G_{Nchar}\}$, where $N_{char}$ denotes the number of character categories. As shown in Fig.~\ref{data_struc_ilus}, a vector glyph $G_i$ can be viewed as a sequence of drawing commands, denoted as $G_i =[C_{i,1},..., C_{i,N_c}]$, where $N_c$ is the total length of commands. The drawing command $C_{i,j}$ is a tuple $(z_{i,j},p_{i,j})$, where $z_{i,j}$ and $p_{i,j}$ denote the command type and command coordinates, respectively.
The drawing sequence starts from the top-left command.
We consider 4 command types, i.e., $z_i \in \{MoveFromTo, LineFromTo, CurveFromTo, EOS\}$. $MoveFromTo$ means moving the drawing position to a new location, which is used for starting a new path. $LineFromTo$ and $CurveFromTo$ mean drawing a straight line and a three-order Bézier curve, respectively. $EOS$ means ending the drawing sequence.\par
Typically, 
$p_{i,j}$ is made up of $N_p$ pairs of coordinates: $p_{i,j}=[(x_{i,j}^1,y_{i,j}^1),...,(x_{i,j}^{N_p},y_{i,j}^{N_p})]$, known as the control points, where $N_p$ is determined by the order of Bézier curves. In the typical SVG representation, the starting point is omitted so that the number of curve order is equal to $N_p$, namely, $N_p$ = 3 for the three-order Bézier curve and $N_p$ = 1 for the line (equal to the one-order Bézier curve). \par
\textbf{Relaxation Representation} Different from SVG-VAE and DeepVecFont, we assign each drawing command with individual starting and ending points.
% different from the commonly used coupled relative positioning information from DeepVecFont \cite{wang2021deepvecfont} and SVG-VAE \cite{svgvae} that set $N_p=3$ for all kinds of drawing commands, corresponding to the first control point, the second control point, and the end point coordinates of the current command. The adjacent drawing command are coupled together which means the starting points of the current command is exactly the endpoints of the previous commands, which is prone to accumulated biases and artifacts as depicted in Fig. \ref{ya} In this paper, we propose to model the starting and ending points separately and use additional regulations to merge them, i.e. 
%corresponding to the starting points, the first control point, the second control point, and the end point of the current command $C_i$. Noting that 
Therefore, the number of coordinate pairs $N_p$ in $CurveFromTo$ is set to 4.
Specifically, $p_{i, j}^1$ and $p_{i, j}^4$ are the starting and ending points, respectively; $p_{i, j}^2$ and $p_{i, j}^3$ are two intermediate control points. 
%For $z_{i,j} = CurveFromTo$, four coordinate pairs correspond to four control points of the cubic Bézier curve, whose 
We pad the length of other commands to that of $CurveFromTo$: for $z_{i,j} \in \{MoveFromTo, LineFromTo\}$, only $(x_{i,j}^1,y_{i,j}^1) $ and $(x_{i,j}^4,y_{i,j}^4)$ (starting and ending points) are used; for $z_{i,j} = EOS$, no argument is used. Afterwards, we render those vector glyphs to obtain the rasterized glyph images.

 An illustration of the difference between an existing SVG representation and ours can be found in Fig.~\ref{svg_rep_comp}. Our proposed representation describes each drawing command separately, which is well-suited for the Transformer's attention mechanism to exploit the long-range dependencies of drawing commands. In the training stage, we force the ending point of $C_i$ to be consistent with the starting point of $C_{i+1}$ using an extra constraint calculated by Eq.~\ref{smoothconstraint}. In the inference stage, we merge these two points by averaging their positions.

% Compared with the previouly used representatoin, the separated representation makes each command an independent primitive, and subsequent commands will not be offset from the original trajectory due to the offset bias caused by the wrong prediction from the previous command.
\begin{figure}[t!]
    \begin{center}
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}# ,angle=90
    % trim 是左下右上需要裁剪的长度 单位是像素
    %,trim=0 100 0 450,clip
    \includegraphics[width=\columnwidth]{ya.pdf}
    \end{center}
  %  \vspace{-0.3cm}
        \caption{A demonstration of different SVG representations. (a) The ground truth. If the previous blue segment is wrongly predicted, (b) the commonly used representation in SVG-VAE \cite{svgvae} shares the connected points (E1 and S2), resulting in location shift. (c) The proposed relaxation representation models these two points separately, which is more robust against outliers.}
    
    
    %Specifically, compared to the commonly used representation that shares the same starting/ending points between adjacent sequences,  causing biases from the preceding drawing process (Fig. \ref{ya}), we propose a relaxation representation that models these two points separately and merges them through an extra constrain.
    
    \label{svg_rep_comp}
    \end{figure}
    \textbf{Embedding}
%Before passing the input vector glyph into the model
We first project the drawing command into a common continuous $d_E$-dimensional embedding space. 
%which is beneficial to learning the complex dependencies between the command type, drawing coordinates, whole attributes, and the drawing orders in the sequence.
Specifically, each $C_{i,j}$ is projected into a vector $e_{i,j} \in \mathbb{R}^{d_{E}}$ via the sum of four embeddings: 
\begin{equation}
e_{i,j}=e_{i,j}^{\mathrm{cmd}}+e_{i,j}^{\mathrm{args}}+ e_{i,j}^{\mathrm{w,h}} + e_{i,j}^{\mathrm{pos}}. 
\end{equation}

For the command type embedding $e_{i,j}^{\mathrm{cmd}}$, we use a learnable matrix $W_{\mathrm{cmd}} \in \mathbb{R}^{d_{E} \times 4}$ to convert the command type into a $d_E$-dimensional vector, which is formulated as $e_{i,j}^{\mathrm{cmd}}=W_{\mathrm{cmd}} \delta_{i,j}^c \in \mathbb{R}^{d_{E}}$, where $\delta_{i,j}^c \in \mathbb{R}^{4}$ is a one-hot vector for the four command types.

For the argument (coordinate) embedding $e_{i,j}^{\mathrm{args}}$, we first quantize the continuous coordinates into discrete integers and convert the integer into a one-hot vector with 256 dimensions. Then, we stack all the 8 coordinate parameters into a matrix ${\delta}_{i,j}^{\mathrm{p}} \in \mathbb{R}^{256 \times 8}$, and embed each parameter using a learnable matrix ${W}_{{args}}^{b} \in \mathbb{R}^{d_{\mathrm{E}} \times 256}$. After that, we aggregate all the parameter embeddings through a linear projection layer ${W}_{{args}}^{a} \in \mathbb{R}^{d_{\mathrm{E}} \times 8 d_{\mathrm{E}}}$, which is formulated as:

\begin{equation}
    e_{i,j}^{{args}}=W_{ {args}}^{a} { flatten }\left(W_{ {args}}^{b} {\delta}_{i,j}^{\mathrm{p}}\right),
    \end{equation}
where flatten(·) means flattening the input into a vector.

The third term $e_{i,j}^{\mathrm{w,h}}$ encodes the height and width of the glyph area to capture global styles. We discretize the height and width into discrete integers and project them into the continuous space to obtain $e_{i,j}^{\mathrm{w}}$ and $e_{i,j}^{\mathrm{h}}$. Then we concatenate them by element-wise addition to get $e_{i,j}^{\mathrm{w,h}}$.

The fourth term (positional embedding) $e_{i,j}^{\mathrm{pos}}$ encodes the position and order information of all commands in a drawing sequence. Similar to \cite{2017Attentionisallyouneed}, we use the absolute positional encoding to compute the $e_{i,j}^{\mathrm{pos}} \in \mathbb{R}^{d_{E}} $ for each command.

\subsection{Dual-branch Pipeline}%We follow the dual-branch pipeline of DeepVecFont, which is depicted in Fig. \ref{architecture}, to synthesize the target glyph.
Fig.~\ref{architecture} shows the pipeline of our DeepVecFont-v2. Given randomly sampled $N_r$ (typically set to 4) reference glyphs as input, the model generates a target glyph with the same font style as input samples, which is further refined by a self-refinement module. Similar to DeepVecFont~\cite{wang2021deepvecfont}, the proposed model also adopts a dual-branch architecture which, however, employs several new techniques and modules. More details are presented as follows:

% In order to fully utilize the advantages of two modalities, we use the sequence encoder $E_{seq}$ and the image encoder $E_{img}$ to extract features from the given vector glyph and glyph images, respectively. 
% A modality fusion module is used to combine the two modalities and obtain the latent feature $z$. Finally, the synthesized vector glyph and glyph images will be produced by the sequence decoder $D_{seq}$ and the image decoder $D_{img}$, respectively. 

\textbf{Encoder}
The encoder of our DeepVecFont-v2 is made up with an image encoder and a sequence encoder. The image encoder is a CNN, outputting the image feature $f_{img} \in \mathbb{R}^{d_{E}}$. Different from DeepVecFont, the sequence encoder is a Transformer encoder composed of six layers of Transformer encoder blocks. 
% each with two self-attention layers with 8 attention heads and a feed-forward dimension of 512. 
For each vector glyph $G_i$, it receives as input the sequence embedding $[e_{i,0},e_{i,1},...,e_{i,N_c}]$, where $[e_{i,1},...,e_{i,N_c}]$ is the embedding of $N_c$ drawing commands and $e_{i,0}$ denotes an auxiliary learnable ``token" for performing dual-modality fusion.
%and u.
%The output sequence-aspect feature of glyph $G_i$ 
The output of our Transformer encoder is denoted as $e_i^{\prime} = [e_{i,0}^{\prime},e_{i,1}^{\prime},...,e_{i,N_c}^{\prime}] \in \mathbb{R}^{d_{E} \times (N_{c+1})}$. Next, we calculate the holistic sequence-aspect style feature $f_{seq} = [f_0^{seq},f_1^{seq},...,f_{N_c}^{seq}]$, where $f_j^{seq}$ is aggregated from all the $j$-th tokens in $e^{\prime}$ of $N_{r}$ reference glyphs via linear projection.

% \begin{equation}
% f^{s e q}=W_{seq}\left[f_{s_{1}}^{s e q} ; f_{s_{2}}^{s e q} ; \ldots ; f_{s_{N_{r}}}^{s e q}\right] = [f_{i,1},...,e_{i,N_c}]

% \end{equation}

\textbf{Modality fusion}
We use the modality token ${f_0^{seq}}$ to represent the style feature of vector glyphs, and combine it with the image feature $f_{img}$ by a linear projection layer to get the fused feature $f$:

\begin{equation}
    f=Linear\left(\left[f^{i m g} ; f_0^{seq}\right]\right),
    \end{equation}
then $f$ is normalized by using the reparametrization trick introduced in VAE~\cite{2014Autoencoder}.
%by the reparametrization trick\cite{2014reparam} as $z=\hat{\mu}+\hat{\sigma} \cdot \epsilon$, where $\epsilon \sim \mathcal{N}(0, I)$ and $z\in \mathbb{R}^{d_{E} \times 1}$.

\textbf{Decoder}
The image decoder is a Deconvolutional Neural Network. We send $f$ into the image decoder to generate the target glyph image $I_t$. Then we employ the L1 loss and perceptual loss to compute the image reconstruction loss:

\begin{equation}
    L_{img}=\left\|\hat{I}_{t}-I_{t}\right\|_{1}+L_{\text {percep }}\left(\hat{I}_{t}, I_{t}\right).
\end{equation}

We feed the sequence decoder with the input of $[f, f_1^{seq},..., f_{N_c}^{seq}]$, where the original modality token $f_0^{seq}$ is replaced with the fused feature $f$.
%receive as input , where 
% the the original modality token is  replaced by the fused feature $z$ to form the input of sequence decoder $D_{seq}^{input} = [z,e_1^{\prime},...,e_{N_c}^{\prime}] \in \mathbb{R}^{d_{E} \times  (N_c+1)}$.
An MLP layer is appended on top of the Transformer decoder, predicting the command types and coordinates of the target glyph as $[\hat{z}_{t,1},...\hat{z}_{t,N_c}]$ and $[\hat{p}_{t,1},...\hat{p}_{t,N_c}]$, respectively. Here we define the loss between the initially generated glyph and its corresponding ground truth as:


    \begin{equation}
        L_{CE}^{init}=\sum_{j=1}^{N_{C}}w_{\mathrm{cmd}} \ell\left(z_{t,j}, \hat{z}_{t,j}\right)+ %l_{\mathrm{args}}\left(p_{t,j},\hat{p}_{t,j}\right)\right). 
        \ell \left(p_{t,j},\hat{p}_{t,j}\right),
        \end{equation}
    where $\ell$ denotes the Cross-Entropy loss, all the coordinates ($p$) are quantized to be optimized by $\ell$, $w_{\mathrm{cmd}}$ means the loss weight for command type prediction, and all the unused arguments are masked out in the second item.
    
    
  \subsection{Context-based Self-refinement}
    In the inference stage, Transformers follow the sequential generation process adopted in RNNs: taking the previous predicted results as the condition to predict the following steps. 
    % As a result, a prediction error at a certain step could finally accumulate more errors and then bring significant distortions in the synthesized glyphs. 
    However, a prediction error generated in a certain step of the sequential process might finally lead to the accumulation of large errors and bring significant distortions in the synthesized glyphs. We notice that there are strong priors and correlations in the geometries of glyphs, such as symmetry and smoothly varied stroke widths, 
    % making the context information strong clues to perform refinement.
    and these context information can be used as strong clues to enhance the glyph synthesizing performance.
    \par
    % Considering that the self-regression strategy receiving condition from left to right, which is uni-direction formulated as $P\left((z_{t,j},p_{t,j}) \mid (z_{t,j-1},p_{t,j-1}), \ldots, (z_{t,1},p_{t,1})\right)$, 
    Therefore, we propose a self-refinement module to refine the initial predictions by analyzing their context information, i.e., $((\bar{z}_{t,1},\bar{p}_{t,1}), \ldots, (\bar{z}_{t,N_c},\bar{p}_{t,N_c})) = F_{r}((\hat{z}_{t,1},\hat{p}_{t,1}), \ldots, (\hat{z}_{t,N_c},\hat{p}_{t,N_c}))$, where $F_{r}$ denotes the function of our refinement module which is actually a 2-layer Transformer decoder. The multi-head attention operation ($\mathbf{F}_{mha}$) used in our decoder layers is formulated as:
    
    \begin{equation}
        \mathbf{F}_{m h a}=\operatorname{softmax}\left(\frac{\mathbf{Q K}^{\top}}{\mathcal{C}(\mathbf{Q})}+\mathbf{M}\right) \mathbf{V},
    \end{equation}
    \begin{equation}
        \mathbf{M}_{i j}= \begin{cases}0, & i <=\hat{N}_c \\ -\infty, & i>\hat{N}_c\end{cases},
    \end{equation}
    where $\hat{N}_c$ is the length of the predicted sequence; % also the index of the first predicted sequence ending; 
    $\mathcal{C}$ is the feature dimension of queries for normalization; $\mathbf{Q} \in \mathbb{R}^{d_E \times N_c}$ consists of the embeddings of the initially predicted sequence $(\hat{z}_{t,1},\hat{p}_{t,1}), \ldots, (\hat{z}_{t,N_c},\hat{p}_{t,N_c})$; $\mathbf{K,V} \in \mathbb{R}^{d_{E} \times\left(N_{c}+1\right)}$ denote the linear projections of memories used in the previous sequence decoder; $\mathbf{M}$ is used to perform self-attention only on the valid positions of the initially predicted sequence. Finally, we refine the command types and coordinates $(\bar{z}_{t,j},\bar{p}_{t,j})$ again via the supervision of the corresponding ground truth by minimizing:
    
    \begin{equation}
        L_{CE}^{refine}=\sum_{j=1}^{N_{C}}\left(w_{\mathrm{cmd}} \ell\left(z_{t,j}, \bar{z}_{t,j}\right)+ \ell \left(p_{t,j},\bar{p}_{t,j}\right)\right). 
        \end{equation}

\subsection{Bézier Curve Alignment Loss}
    %Our proposed relaxation representation contributes to the smooth connection between adjacent commands. 
    Through our experiments, we found that it is still insufficient by only employing the control points ($p$) to supervise the alignment of Bézier curves/lines.
    Therefore, we propose to sample more points from each drawing command $C_{t,j}$. %whose formula is denoted as $B_{t,j}(r)$ and $r$ is the parameter. 
    As we know, a Bézier curve built upon control points $p$ is defined as:
    $B=\sum_{k=0}^{n} 
        \left(\begin{array}{c}
        n \\
        k
        \end{array}\right)  r^{k}(1-r)^{n-k}p,$
where $n$ denotes the number of curve order,
        $\left(\begin{array}{c}
        n \\
        k
        \end{array}\right)$ denotes the binomial coefficients,
    %   where $\quad i=0, \ldots, n$ and $0\leq t\leq 1.$. 
 and $r$ is the position parameter.
    %Noting that the drawing command type ``LineFromTo" could be rewrite as Cubic Bézier Curve whose first and second points are equally distributed on the line.
    Thus, we can calculate the Bézier curve alignment loss by:
    \begin{equation}
         L_{b\Acute{e}zier} = \sum_{r\in r_p} (\hat{B}_{t,j}(r)- B_{t,j}(r))^2 + (\bar{B}_{t,j}(r)- B_{t,j}(r))^2,
    \end{equation}
    where $r_{p} = \{0.25, 0.5, 0.75\}$ represents the parameters of the auxiliary points we sampled.
Note that ``LineFromTo" can be formulated as a special case of ``CurveFromTo", where all the control points are uniformly distributed on a line segment.

\subsection{Merging Relaxed Drawing Commands}
%$(\hat{x}_{i-1}^{4},\hat{y}_{i-1}^{4})$ and $(\bar{x}_{i-1}^{4},\bar{y}_{i-1}^{4})$ 
%$(\hat{x}_{i}^{1},\hat{y}_{i}^{1})$
Ideally, the starting point of the current drawing command ($LineFromTo$ or $CurveFromTo$, $l$ or $c$ for short) should coincide with the previous one's ending point for both initially synthesized and refined sequences. Thereby, we try to minimize the L2 distance between the predicted ending point of command $C_{i-1}$ and the starting point of $C_{i}$:
\begin{equation}
\begin{aligned}
    L_{cons} &=\mathbf{1}_{z_{t,j}\in\{l,c\}}\sum_{j=2}^{N_c}\left(\hat{x}_{t,j}^{1}-\hat{x}_{t,j-1}^{4}\right)^2 + \left(\hat{y}_{t,j}^{1}-\hat{y}_{t,j-1}^{4}\right)^2 \\
&+ \left(\bar{x}_{t,j}^{1}-\bar{x}_{t,j-1}^{4}\right)^2 + \left(\bar{y}_{t,j}^{1}-\bar{y}_{t,j-1}^{4}\right)^2.
    \label{smoothconstraint}
\end{aligned}
\end{equation}

\subsection{Overall Objective Loss}
Combining all losses mentioned above, we train the whole model by minimizing the following objective:
\begin{equation}
  L_{img} + L_{CE}^{init} + L_{CE}^{refine} + L_{cons} + L_{b\Acute{e}zier} +L_{kl},
  \label{TotalLoss}
\end{equation}
where the last term represents the KL loss normalizing the latent code $f$. For brevity, the weight of each term is omitted from the equation.

  %  \subsection{Post process}
        
        \begin{figure}[t!]
    \begin{center}
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}# ,angle=90

    \includegraphics[width=\columnwidth]{ablation-EN.pdf}
    \end{center}
      \vspace{-0.5cm}
        \caption{The ablation study of our method. Blue circles indicate the shortcomings of each incomplete configuration.}
    \label{ablations-en}
    \end{figure}
 
 
        \begin{figure*}[t!]
        \begin{center}
        %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
        \includegraphics[width=\textwidth]{SOTA-EN.pdf}
        \end{center}
          \vspace{-0.4cm}
           \caption{Comparison of few-shot font generation results for DeepSVG, DeepVecFont, and our method. Our method shows superiority in generating high-quality English vector fonts with various styles. Please zoom in for better inspection.} % The input reference glyph are ``A, B, a, b" for all experiments. 
        
        \label{SOTA-en}
        \end{figure*}     

        \section{Experiments}
        \subsection{Dataset and Implementation Details}
        
        The dataset used in our experiment for generating English fonts is the same as~\cite{wang2021deepvecfont}, which includes 8035 fonts for training and 1425 fonts for testing. For Chinese font synthesis, we built a dataset consisting of 212 and 34 fonts for training and testing. To directly apply the same model in two datasets, here we only randomly selected 52 Chinese characters with relatively simple structures. Since the scale of our dataset for Chinese fonts is relatively small, we augmented the training set by applying the affine transformation, enlarging it by ten times. 
        %The maximum sequence length of English and Chinese vector glyphs are set to 51 and 71, respectively.
        
        We conduct the experiments by following the experimental settings of DeepVecFont~\cite{wang2021deepvecfont} for fair comparison. Specifically, the numbers of reference glyphs are chosen as 4 and 8 for English and Chinese font generation, respectively. We employ the Adam optimizer with an initial learning rate of 0.0002. The image resolution is chosen as $64\times64$ in both the training and testing stages. When inferencing, we first add a noise vector distributed by $\mathcal{N}(0, I)$ to the sequence feature, to simulate the feature distortion caused by the human-designing uncertainty (as observed in DeepVecFont). Then, we sample $N_s$ (10 for English and 50 for Chinese) synthesized vector glyphs as candidates and select the one as the final output that has the highest IOU value with the synthesized image. The reconstruction errors (denoted as ``Error") in our quantitative results are obtained by computing the average L1 distance between the rasterized image of each synthesized vector glyph and its corresponding ground-truth glyph image (at the resolution of $64\times64$) in the testing set. In our experiments, the weight values of different losses in Eq.\ref{TotalLoss} are set to 1.0, 1.0, 1.0, 10, 1.0, and 0.01, respectively from left to right.


        \subsection{Ablation Study}
        %In order to analyze the impacts of different modules, we conduct quantitative and qualitative ablation studies by adding the proposed modules in our model.
        
        \begin{figure}[t!]
        \vspace{-0.5cm}
            \begin{center}
                %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}# ,angle=90
                
                %height=6.cm,width=8.3cm
                \includegraphics[width=\columnwidth]{SOTA-CN.pdf}
                \end{center}
                  \vspace{-0.5cm}
                    \caption{Comparison of few-shot Chinese vector font generation results obtained by different methods.
                    % Our method shows superiority in generating high-quality vector font with different weight and styles.
                    }
                
                \label{SOTA-CN}
                \end{figure}
        We conduct qualitative and quantitative experiments to examine the impact of each module in our proposed model. In Tab.~\ref{ablations-en}, the base model is modified from DeepVecFont by simply replacing the LSTM networks with Transformers, then we evaluate each module by adding them successively to the base model. As shown in Fig.~\ref{ablations-en}, the base model using the SVG representation in SVG-VAE~\cite{svgvae} tends to generate incomplete glyphs with severe distortions. By employing the relaxation representation, semantic information can be preserved and the smooth connection between adjacent drawing commands is guaranteed. However, there still exist suboptimal curves with slight distortions. After adding the Bézier curve alignment loss to the current model, most distortions on the synthesized curves can be eliminated. Finally, we perform self-refinement to remove artifacts in the initially synthesized glyphs, resulting in high-quality glyphs with compact and coordinated outlines. Tab.~\ref{table-ablation} shows some quantitative results of our ablation study, further demonstrating the superiority of our proposed modules.
        %         Nums &  Error-EN$\downarrow$\\
        % \hline
        % 1 &       \\
        % 3 &    \\
        % 6 &       \\
        % 9 &         \\

        \subsection{Parameter Study}
        We also conduct parameter studies to find the best choice of the number of sampling points distributed along the Bézier curves, and the results are shown in Tab.\ref{NumberOfSampledPoints}. we can see that the model's performance will be markedly improved when the number of sampling points changes from 0 to 3, while the performance is just slightly enhanced when we increase the point number from 3 to 9. This is mainly due to the fact that 3 sampling points are sufficient enough to precisely align two Bézier curves/lines. Therefore, we choose to sample 3 points for each Bézier curve/line to achieve a balance between the effectiveness and efficiency of our method. Furthermore, we also conduct parameter studies to examine the performance of our method under different numbers of input reference glyphs. Please refer to the supplemental materials for more details. 

         \begin{table}[t!]
        \centering
        \begin{tabular}{lc}
        \hline
        Model &  Error-EN$\downarrow$ \\
        \hline
        Base model &   0.0588  \\
        + Relaxation Representation &  0.0557 \\
        + Bézier Curve Alignment Loss &  0.0529 \\
        + Self-Refinement &   0.0519 \\
        \hline
        \end{tabular}
         % \vspace{-0.2cm}
        \caption{Comparison of reconstruction errors for our methods under different configurations. }
        %``+" means employ designed modules step by step.
        \label{table-ablation}
        \end{table}   

        \begin{figure}[t!]
        %\vspace{0.3cm}
            \begin{center}
            
                %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}# ,angle=90
              
                \includegraphics[width=\columnwidth]{interpolate.pdf}
                \end{center}
                  \vspace{-0.3cm}
                    \caption{English and Chinese vector font interpolation results, where the weight, width and styles change smoothly.}
                
                \label{interpolation}
                \end{figure}

  \begin{figure*}[t!]
    \begin{center}
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}# ,angle=90
    \hspace{-0.5cm}
    \includegraphics[width=\textwidth]{all-EN.pdf}
    \end{center}
    %Glyphs `A', `B', `a', and `b' are selected as input samples.
    \vspace{-0.5cm}
       \caption{Comparison of more vector fonts synthesized by DeepVecFont (DVF) and our DeepVecFont-v2 in the task of few-shot font generation. The input reference glyphs are filled in black. Please zoom in for better inspection.} %       More detailed comparison with DeepVecFont (DVF) of generating the whole vector glyphs in the testing dataset.  %The input reference glyphs are filled in black. The red boxes show the glyphs with over-smoothed corners. The gray dotted boxes show the failure cases with under-smoothed self-interactions. The green boxes show the broken glyphs with partial components. The blue boxes show that our model produces better glyphs that are more consistent with style compared to ground truth.
    \label{all-en-cn}
    
    

    
    \end{figure*}    

        \subsection{Font Interpolation}
        Our method can be used to achieve smooth interpolation between two vector fonts. Specifically, given two vector fonts $a$ and $b$, an interpolated style feature between them can be calculated by:
        \begin{equation}
            f_{inter}=(1-\lambda) \cdot f(a)+\lambda \cdot f(b).
            \end{equation}
        Then, we feed the interpolated feature into the sequence and image decoders to generate the interpolated vector glyphs and the corresponding glyph images. Fig.~\ref{interpolation} shows two font interpolation results for English and Chinese vector fonts, respectively. We can see that glyphs in the source styles can smoothly morph to the target ones, demonstrating the effectiveness of our method in vector font interpolation.

        

    
        
        
       % \scalebox{0.9}{
        \begin{table}[t!]
        \centering
        \begin{tabular}{lcc}
        \hline
        Model &  Error-EN$\downarrow$ & Error-CN$\downarrow$ \\
        \hline
        DeepSVG & 0.125  &    0.167  \\
        DeepVecFont & 0.056 &  0.086  \\
        Ours &  0.052   &   0.080  \\
        % \hline
        % DeepSVG-CN & 0.167\\
        % DeepVecFont-CN & 0.086\\
        % Ours-CN & 0.080\\
        \hline
        \end{tabular}
          \vspace{-0.1cm}
        \caption{Comparison of reconstruction errors for different methods. ``EN" and ``CN" denote the English and Chinese testing datasets, respectively.}
        \label{table-dvf}
        \end{table}
        
      
                 \begin{table}[t!]
        \centering
        \begin{tabular}{ccccc}
        \hline
        Point Num&  Error-EN$\downarrow$&Point Num &  Error-EN$\downarrow$ \\
        \hline
        0 &  0.0557  & 6 &   0.0526  \\ 
        1 &    0.0543  &9 &     0.0524   \\ 
        3 &   0.0529 & 12 &  0.0520\\  
        
        \hline
        \end{tabular}
          \vspace{-0.1cm}
        \caption{Comparison of reconstruction errors under different numbers of sampling points. ``EN" denotes the English testing dataset.}
        \label{NumberOfSampledPoints}
        \end{table}
        
        %     \begin{table}[t!]
        % \centering
        % \begin{tabular}{ccc}
        % \hline
        % Numbers of Points &  Error-EN$\downarrow$ \\
        % \hline
        % 0 &  0.0557  \\ 
        % 1 &    0.0543   \\ 
        % 3 &   0.0529  \\  
        % 6 &   0.0526    \\ 
        % 9 &     0.0524    \\
        % \hline
        % \end{tabular}
        %   \vspace{-0.1cm}
        % \caption{Comparison of reconstruction errors under different numbers of sampling points. ``EN" denotes the English testing dataset.}
        % \label{NumberOfSampledPoints}
        % \end{table}
        

   \subsection{Comparison with the State of the Arts.}
   \vspace{-0.2cm}
We compare the performance of our DeepVecFont-v2 with other existing methods (i.e., DeepSVG~\cite{carlier2020deepsvg} and DeepVecFont\cite{wang2021deepvecfont}) on English and Chinese vector font datasets. Some qualitative results are shown in Fig.~\ref{SOTA-en}, Fig.~\ref{SOTA-CN}, and Fig.~\ref{all-en-cn}. The quantitative results are shown in Tab. \ref{table-dvf}.
% , which are obtained by computing the average L1 distance between the rasterized image of each synthesized vector glyph and its corresponding ground-truth glyph image (at the resolution of $64\times64$) in the testing set.

\textbf{English vector font generation.}
From Fig. \ref{SOTA-en}, we can see that the vector glyphs synthesized by DeepSVG often contain severe distortions (marked in yellow). This is mainly because the hierarchical architecture of DeepSVG was originally designed for synthesizing vector icons with multiple separated short-length paths, while a vector glyph typically consists of several long-range outlines. The synthesized glyphs of DeepVecFont filled with black pixels generally look visually pleasing. However, there still exist some problems in the synthesis results regarding the details of outlines: 1) the refinement post-processing tends to over-fit the initially synthesized vector glyph with the corresponding raster image, resulting in suboptimal outlines with over-smoothed corners (marked in red); 2) when a sequence with redundant drawing commands is predicted, the outline tends to be staked together with self-interactions (marked in gray); 3) structurally-incorrect glyphs might be synthesized for some special styles (e.g., ``F" in the green box). On the contrary, our method can synthesize high-quality vector glyphs for almost all kinds of font styles. Fig. \ref{all-en-cn} shows more results obtained by our method and DeepVecFont. From Fig. \ref{all-en-cn}, we can also observe that our DeepVecFont-v2 sometimes can even generate vector glyphs whose font styles are more consistent with the input samples compared to the ground truth (marked in blue). 

%\vspace{-0.5cm}
        
\textbf{Chinese vector font generation.}
Chinese glyphs typically contain complex shapes and structures, making the task of Chinese vector font synthesis much more challenging. As shown in Fig. \ref{SOTA-CN}, DeepSVG tends to generate glyphs with inconsistent styles and severe artifacts, and DeepVecFont may obtain fragmented results when synthesizing glyphs with multiple closed paths (marked in green). In contrast, our method synthesizes visually pleasing vector fonts, mainly due to the utilization of Transformers and our specifically-designed modules to guarantee the smoothness of outlines. Fig. \ref{all-en-cn} shows more results obtained by our method and DeepVecFont. The quantitative results shown in Tab. \ref{table-dvf} further demonstrate the superiority of our method when handling fonts consisting of glyphs with more complex topologies and longer drawing-command sequences. More synthesized results can be found in our supplemental materials.
%high-quality and


            
\vspace{-0.2cm}
\subsection{Limitations}
   \vspace{-0.2cm}
Our model fails to synthesize correct results when handling complex glyphs that contain large amounts of long drawing paths (e.g., ``\begin{CJK*}{UTF8}{gbsn}霸\end{CJK*}" shown in our supplemental materials). One possible reason is that longer sequences inherently exist more uncertainties generated during the human font-designing process~\cite{wang2021deepvecfont}, bringing more challenges to train the model. 
Another reason is the possible existence of complicated topological changes for a Chinese character in different styles, making it hard to learn stable SVG embeddings. We leave these issues as our future work.

\vspace{-0.2cm}
\section{Conclusion}
   \vspace{-0.2cm}
This paper proposed a novel method, DeepVecFont-v2, to effectively handle the challenging task of vector font synthesis. Specifically, we replaced RNNs adopted in the original DeepVecFont by Transformers and adopted several specifically-design modules, including relaxation representation, the Bézier curve alignment loss, and context-based self-refinement. Thus, vector fonts with higher quality can be directly synthesized by our method in an end-to-end manner. Experimental results demonstrated the superiority of our DeepVecFont-v2 compared to the state of the art in the applications of English and Chinese vector font synthesis.

\section*{Acknowledgements}
This work was supported by National Language Committee of China (Grant No.: ZDI135-130), Project 2020BD020 supported by PKU-Baidu Fund, Center For Chinese Font Design and Research, and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology).

    %The loss $l_{\mathrm{args}}$ for coordinate arguments prediction is defined as:
    
    %\begin{equation}
    %    \begin{aligned}
    %        l_{\mathrm{args}}=\mathbf{1}_{z_{t,j}\in\{m,l,c\}}&\sum_{k \in\{1,4\}} \left(\ell\left(x_{t,j}^{k}, \hat{x}_{t,j}^{k}\right)+\ell\left(y_{t,j}^{k}, \hat{y}_{t,j}^{k}\right)\right)+ \\
    %    \mathbf{1}_{z_{t,j}=\mathrm{c}} &\sum_{k \in\{2,3\}} \left(\ell\left(x_{t,j}^{k}, \hat{x}_{t,j}^{k}\right)+\ell\left(y_{t,j}^{k}, \hat{y}_{t,j}^{k}\right)\right)
    %    \end{aligned}
    %    \end{equation}
        
%%%%%%%%% BODY TEXT
% \section{Introduction}
% \label{sec:intro}

% Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press.
% This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version.

% %-------------------------------------------------------------------------
% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of cvpr.sty to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID 4387}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%   In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%   Why the previous paper was accepted without this analysis is beyond me.

%   [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%   In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%   Why the previous paper was accepted without this analysis is beyond me.

%   [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the CVPR70 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%   We describe a system for zero-g frobnication.
%   This system is new because it handles the following cases:
%   A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%   Ours handles it by including a foo term in the bar integral.

%   ...

%   The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%   It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.pdf}

%   \caption{Example of caption.
%   It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%   \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%   ``Frobnication has been trendy lately.
%   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.


% Update the cvpr.cls to do the following automatically.
% For this citation style, keep multiple citations in numerical (not
% chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% \cite{Alpher02,Alpher03,Authors14}.


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

% %------------------------------------------------------------------------
% \section{Formatting your paper}
% \label{sec:formatting}

% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% page.

% %-------------------------------------------------------------------------
% \subsection{Margins and page numbering}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
% high.
% %
% Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
% The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
% The \LaTeX\ template takes care of this when used properly.



%-------------------------------------------------------------------------
% \subsection{Type style and fonts}

% Wherever Times is specified, Times Roman may also be used.
% If neither is available on your word processor, please use the font closest in
% appearance to Times to which you have access.

% MAIN TITLE.
% Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
% The title should be in Times 14-point, boldface type.
% Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
% do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
% Leave two blank lines after the title.

% AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
% and printed in Times 12-point, non-boldface type.
% This information is to be followed by two blank lines.

% The ABSTRACT and MAIN TEXT are to be in a two-column format.

% MAIN TEXT.
% Type main text in 10-point Times, single-spaced.
% Do NOT use double-spacing.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Make sure your text is fully justified---that is, flush left and flush right.
% Please do not place any additional blank lines between paragraphs.

% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
% Short captions should be centred.

% \noindent Callouts should be 9-point Helvetica, non-boldface type.
% Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

% FIRST-ORDER HEADINGS.
% (For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

% SECOND-ORDER HEADINGS.
% (For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
% If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

% %-------------------------------------------------------------------------
% \subsection{Footnotes}

% Please use footnotes\footnote{This is what a footnote looks like.
% It often distracts the reader from the main flow of the argument.} sparingly.
% Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
% If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
% Use Times 8-point type, single-spaced.


%-------------------------------------------------------------------------
% \subsection{Cross-references}

% For the benefit of author(s) and readers, please use the
% {\small\begin{verbatim}
%   \cref{...}
% \end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
% This will automatically insert the appropriate label alongside the cross-reference as in this example:
% \begin{quotation}
%   To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
%   It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
%   You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
% \end{quotation}
% If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
% {\small\begin{verbatim}
%   \Cref{...}
% \end{verbatim}}
% command. Here is an example:
% \begin{quotation}
%   \Cref{fig:onecol} is also quite important.
% \end{quotation}

% %-------------------------------------------------------------------------
% \subsection{References}

% List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
% When referenced in the text, enclose the citation number in square brackets, for
% example~\cite{Authors14}.
% Where appropriate, include page numbers and the name(s) of editors of referenced books.
% When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
% If you use the template as advised, this will be taken care of automatically.

%\begin{table}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Method & Frobnability \\
%     \midrule
%     Theirs & Frumpy \\
%     Yours & Frobbly \\
%     Ours & Makes one's heart Frob\\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}

%-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
% Instead use
% {\small\begin{verbatim}
%   \centering
% \end{verbatim}}
% at the beginning of your figure.
% Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
% Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.



%-------------------------------------------------------------------------
% \subsection{Color}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

% If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
% Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.

% %------------------------------------------------------------------------
% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.

% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.


%%%%%%%%% REFERENCES
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
