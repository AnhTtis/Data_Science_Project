%Specifically, these algorithms can
Specifically, these quantum linear system  algorithms can
yield a state $\ket{x}$ for any $\ket{b}$ solved for a sparse
Hermitian matrix $A$ such that the equation\begin{equation}
A\ket{x} = \ket{b} \end{equation}
is satisfied up to a constant of proportionality.  The method works by constructing a unitary matrix that encodes $A$ within a block of the original matrix, specifically, $(\bra{0}\otimes I) U_{A^{-1}} (\ket{0} \otimes I) \ket{0}\ket{b} \propto A^{-1} \ket{b}$, with $I$ being the identity matrix. 
 Such a representation is known as a block encoding.  The performance cost scales as $O(d\kappa\log(1/\epsilon))$ accesses to the matrix elements of the $d$-sparse matrix $A$, which we further assume is invertible and has condition number $\kappa$ and desired solution error $\epsilon$~\cite{childs2017quantum}.

The simplest strategy that can be employed for solving systems of differential equations involves using a forward-Euler approach to discretize a differential equation of the form $\partial_t x(t) = A x(t) + b(t)$ and then solve the resulting equation using the quantum linear systems algorithm.  The resulting difference equation takes the form $x(t_{i+1}) = x(t_i) + h Ax(t_i) +hb(t)$ for stepsize $h$.  The central idea behind this approach is to construct a quantum state over both the solution space and the time that the system is evaluated at.  Specifically, if we let our solution be $x(t)$, then we encode our solution as $\sum_i c_i \ket{t_i} \ket{\psi_i}$, where $\ket{\psi_i}$ is the solution at time $t_i$, and $c_i$ is arbitrary constants.  Given a state of this form, we can find the solution state by measuring the $t_i$ register (or using amplitude amplification) to achieve a value that is $t_f$, which is the final time desired for the algorithm.  In practice, the probability of measuring this result is low, so the standard approach to this problem is to extend (typically double) the simulation time but turn off the differential equation to ensure that the solutions are merely copied on all subsequent times.  Since the solution is the same at all such times, this serves to raise the probability of success to a constant without requiring substantial computational overhead.
Specifically,  the solution for a two-time-step result with two further time steps used for padding then reads~\cite{berry2014high}
\begin{equation}
\begin{bmatrix} I &0 &0 &0 &0\\ 
-(I + hA) & I &0&0&0\\
0 &-(I + hA) & I &0&0\\
0&0&-I&I&0\\
0&0&0&-I&I\end{bmatrix} \begin{bmatrix} x(0) \\ x(h) \\x(2h) \\ x(3h) \\ x(4h) \end{bmatrix} = \begin{bmatrix} x_{\text{init}} \\ bh \\ bh \\ 0 \\0 \end{bmatrix}
\end{equation}
In this form, the solution vector over all times can be found by inverting the above matrix.  In practice, this approach is not favorable in classical computing, as it requires a substantial overhead due to the dimension of the space. But, as the cost of the quantum linear systems algorithms does not directly depend on the dimension, this approach can be surprisingly effective in quantum settings.
