\section{Related Work}


\noindent \textbf{Low-light Image Enhancement.}
Classical methods for low-light image enhancement have been developed based on histogram equalization and the Retinex theory~\cite{SSR_1997_TIP,BPDHE_2007_TCM,LDR_2013_TIP,LIME_2017_TIP}.
Recently, learning-based methods have driven remarkable advances in this field; examples include
an auto-encoder~\cite{lore2017llnet}, a multi-branch architecture~\cite{lv2018mbllen}, and a U-Net architecture~\cite{chen2018learning}. 
Also, Jiang~\etal~\cite{jiang2021enlightengan} proposed a GAN using unpaired low-light and well-lit images, and
Wang~\etal~\cite{wang2021low} learned mapping from low-light  to well-lit images via normalizing flow.
They usually require heavy computation and are learned without considering downstream recognition tasks.
In contrast, we focus on learning features insensitive to lighting-condition for pose estimation while bypassing low-light image enhancement.


\noindent \textbf{Low-light Datasets.}
SID~\cite{chen2018learning} and LOL~\cite{Chen2018Retinex} provide paired low-light and well-lit images. 
Wang~\etal~\cite{Wang_2021_ICCV} collected paired low-light and well-lit videos by playing motion using an electric slide system. 
Due to the difficulty of capturing paired images simultaneously, they only provide images of static objects.
Meanwhile, Jiang~\etal~\cite{Jiang_2019_ICCV} built a dual-camera system to capture paired low-light and well-lit videos at once.
Inspired by this, we construct a dedicated camera system for collecting paired low-light and well-lit images of humans with motion.
Low-light datasets have also been proposed for other tasks, \eg, 
ARID~\cite{xu2020arid} for human action recognition, NOD~\cite{morawski2021nod} and ExDark~\cite{loh2019getting} for object detection.
While these datasets provide only low-light images for training recognition models, our dataset provides low-light images along with their well-lit counterparts as well as accurate human pose labels. 


\noindent \textbf{Pose Estimation in Low-light Conditions.}
Crescitelli~\etal~\cite{Crescitelli_SAS,Crescitelli_TIM} presented a human pose dataset containing 1,800 sRGB and 2,400 infrared (IR) images captured at night, and integrated sRGB and IR features for human pose estimation.
Our work is clearly distinct from this in two aspects. 
First, we focus on human pose estimation in \emph{extremely} low-light images, in which humans barely see anything. 
On the other hand, Crescitelli~\etal~\cite{Crescitelli_SAS,Crescitelli_TIM} captured images of night scenes with light sources, in which most human objects are sufficiently visible. 
Second, our model takes a single sRGB image captured in a low-light environment as input for pose estimation 
without demanding extra observations like IR images.
To the best of our knowledge, our work is the first to tackle this challenging but highly practical task.


\noindent \textbf{Robust Visual Recognition.}
The performance of conventional recognition models often gets degraded in adverse conditions~\cite{loh2019getting,liang2021recurrent,HLAFace_2021_CVPR,morawski2021nod}.
To address this issue, the robustness of visual recognition has been actively studied~\cite{goodfellow2014explaining,Hendrycks2019_ImageNetC,hendrycks2018deep,schneider2020improving,son2020urie,shi2020informative,acdc,lee2022fifo}.
Low-light visual recognition is one such direction that aims at learning features robust to limited visibility of low-light images.
Sasagawa~\etal~\cite{sasagawa2020yolo} utilized a U-Net architecture to restore well-lit images from raw-RGB low-light images for object detection.
Morawski~\etal~\cite{morawski2021nod} incorporated an image enhancement module into an object detector and proposed lighting variation augmentations for nighttime recognition.
However, the enhancement module is often substantially heavy to be integrated with recognition models.
We instead learn a pose estimation model insensitive to lighting conditions so that it does not need an extra module for image enhancement.

\noindent \textbf{Learning Using Privileged Information.}
LUPI aims at exploiting privileged information available only in training to improve target models in terms of accuracy, label-efficiency, and convergence speed.
Vapnik~\etal~\cite{LUPI_2009} first introduced LUPI for support vector machine classifiers.
The idea has been extended to tackle various tasks beyond classification, \eg, object localization~\cite{LUPI_SSVM+}, metric learning~\cite{Fouad_LUPI}, ranking~\cite{Sharmanska_LUPI}, and clustering~\cite{Feyereisl_LUPI_Clustering}.
LUPI has also been studied for training deep neural networks:
Lopez-Paz~\etal~\cite{LopSchBotVap_ICLR16} investigated the relation between LUPI and knowledge distillation~\cite{hinton2015distilling},
Lambert~\etal~\cite{Heteroscedastic_Dropout} developed a new dropout operation controlled by privileged information,
and Hoffman~\etal~\cite{Hoffman_sideinfo} employed an auxiliary model that approximates the teacher using ordinary input data.
Our work is clearly distinct from these in terms of the model architecture and the way of teacher-student interaction as well as the target task: Our model maximizes parameters shared by the teacher and student so that privileged information helps improve representation quality of the student, and the student is trained to approximate internal behavior of the teacher as well as predicting human poses.
