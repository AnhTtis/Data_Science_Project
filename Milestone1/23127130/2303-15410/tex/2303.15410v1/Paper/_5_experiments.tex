\vspace{-3mm}
\section{Experiments}
\subsection{Single-person Pose Estimation}\label{sec:single_pose}

To ease the difficulty of the problem and focus solely on pose estimation, we first tackle single-person pose estimation with the assumption that ground-truth bounding boxes are given for individuals.

\noindent\textbf{Implementation Details.}
We adopt Cascaded Pyramid Network (CPN)~\cite{chen2018cascaded} with ResNet-50~\cite{resnet} backbone as our pose estimation network, which is pre-trained on the ImageNet dataset~\cite{Imagenet}.
All BNs in the backbone are replaced with LSBNs, and LUPI is applied to the outputs of the first convolution layer and the following four residual blocks.
The average channel intensity of an input low-light image is automatically adjusted to 0.4 before being fed to the student network following Zheng~\etal~\cite{Zheng_2020_CVPR}.
More details are given in Sec.~\ref{appendix:details} of the supplement.

\noindent\textbf{Evaluation Protocol.}
We report the standard average precision (AP) scores based on object keypoint similarity following CrowdPose~\cite{li2019crowdpose}.
Our method and competitors are evaluated on both low-light (LL) and well-lit (WL) images.
Low-light images for testing are further divided into three subsets according to their relative difficulty, low-light normal (LL-N), low-light hard (LL-H), and low-light extreme (LL-E), by the gain values of the coupled well-lit images.
Specifically, they are split into 167 LL-N images, 169 LL-H images, and 155 LL-E images by applying two thresholds, 15 and 24, to their gain values.
The mean pixel intensities of LL-N, LL-H, and LL-E images are 3.2, 1.4, and 0.9, respectively.
Note that images of all three subsets are captured in extremely low-light conditions and hard to be recognized by humans.
The union of the three low-light test splits is denoted as low-light all (LL-A).


\input{Paper/Tables/Main_table}
\input{Paper/Figures/Qualitative_results}

\vspace{-2mm}
\subsubsection{Quantitative Results on the ExLPose Dataset}
\label{sec:quan_analysis_PID}

Our method is compared with potential solutions to the target task on the five test splits of the ExLPose dataset, \ie, LL-N, LL-H, LL-E, LL-A, and WL. 
The solutions include baselines that train the pose estimation model directly with the ExLPose dataset, 
those incorporating low-light image enhancement techniques as pre-processing, and domain adaptation methods that consider different lighting conditions as different domains.
Specifically, the baselines are CPNs trained on low-light images, well-lit images, or both of them using the pose estimation loss only, which are denoted by Baseline-low, Baseline-well, and Baseline-all, respectively.
Also, when a low-light image enhancement technique is incorporated, low-light training images are first enhanced and both of the enhanced images and well-lit images are used to train CPN, for which low-light test images are also enhanced by the same technique; we found that this strategy maximizes the advantage of image enhancement as shown in Table.~\ref{tab:variant_enhancement} of the supplement. 
Pose estimation performance of ours and these solutions is given in \Tbl{main_table}. 

Our method clearly outperforms all the three baselines in the four low-light splits, and is even on par with Baseline-well in the well-lit split. 
As expected, Baseline-low and Baseline-well are significantly inferior in the well-lit and low-light splits, respectively, 
due to the substantial gap between training and testing images in lighting conditions.
Baseline-all achieves the best among the baselines, but its performance is still limited compared with other approaches including ours.
These results suggest that it is not straightforward to learn a common model working under both low-light and well-lit conditions, as also reported in~\cite{loh2019getting,morawski2021nod}.
In contrast to these early findings, we successfully manage to train a single model that performs well under both of the two lighting conditions through LSBN and LUPI.


To evaluate the efficacy of low-light image enhancement, we adopt two enhancement methods: LLFlow~\cite{wang2021low} as a learning-based method and LIME~\cite{LIME_2017_TIP} as a traditional method based on the Retinex theory. 
The combinations of these enhancement techniques and CPN are denoted as `LLFlow + Baseline-all' and `LIME + Baseline-all'.
As shown in the table, low-light image enhancement helps improve performance in two evaluation settings, LL-N and LL-H, but it rather degrades performance in LL-E. 
Furthermore, it additionally imposes immense inference latency, and demands a large amount of memory footprint when adopting learning-based methods like LLFlow.
On the other hand, our method outperforms them by large margins in all the five splits with only a small number of additional parameters and no additional inference latency.


Finally, our method is compared with two domain adaptation (DA) methods: DANN~\cite{dann} for feature-level DA and AdvEnt~\cite{vu2019advent} for output-level DA. 
Note that existing DA methods for 2D human pose estimation~\cite{jakab2020self,cao2019cross} are not compared since they are essentially based on DANN~\cite{dann}.
For training CPN with these methods, we assign well-lit images to a source domain and low-light images to a target domain, and utilize pose labels of both domains.
The results in the table show that the direct adaptation between low-light and well-lit conditions is not effective due to the large domain gap, which suggests the necessity of LSBN. 
Thanks to LSBN and LUPI, our method clearly surpasses the two methods in every split.
Further analysis on the impact of LSBN and LUPI is presented in Sec.~\ref{appendix:ablation} of the supplement.

\vspace{-2mm}
\subsubsection{Qualitative Results on the ExLPose Dataset}
\label{sec:qual_analysis_PID}

Our method is qualitatively compared with Baseline-all, DANN, LIME + Baseline-all in \Fig{qualitative_results}.
As shown in the figure, 
Baseline-all and DANN fail to predict poses frequently.
LIME + Baseline-all performs best among the competitors, but often fails to capture details of poses,
in particular under more difficult low-light conditions.
Our method clearly exhibits the best results; it estimates human poses accurately even under the \mbox{LL-E} condition with severe noises. 

\input{Paper/Tables/ExLPose_significance}

\vspace{-2mm}
\subsubsection{Significance of the ExLPose Dataset}

The ExLPose dataset has two significant properties: It provides pairs of aligned low-light and well-lit images, and it thus enables accurate labeling of low-light images. 
The impact of these properties is investigated by additional experiments, whose results are summarized in Table~\ref{tab:ablation}.


First, we study the importance of pairing low-light and well-lit images. 
To this end, we compare our method (LSBN + LUPI w/pair) with its variant (LSBN+LUPI w/o pair) disregarding the pair relations, trained with unrelated low-light and well-lit images.
As demonstrated in the table, our method clearly outperforms the variant, which suggests the contribution of pairing low-light and well-lit images.
Next, we investigate the impact of accurate pose labels for low-light images by comparing our method with its another variant trained in an unsupervised domain adaptation (UDA) setting.
To be specific, the second variant (Ours-UDA w/o pair) is trained with both LSBN and LUPI using labeled well-lit images and unlabeled low-light images, which are unpaired, following the common problem setting of UDA.
As demonstrated in the table, this variant performs worst, which justifies the significance of pose labeling on low-light images as well as that of pairing low-light and well-lit images in our dataset.


\input{Paper/Tables/Loss_ablation}

\vspace{-2mm}
\subsubsection{Ablation Study}\label{sec:ablation_study}
We investigate the impact of LSBN and LUPI by the ablation study in \Tbl{ablation_main}.
In the table, LSBN and LUPI denote variants of ours trained with either LSBN or LUPI, respectively.
The inferior performance of LUPI implies that LSBN is essential to bridge the large gap between low-light and well-lit conditions effectively. 
For the same reason, LSBN significantly improves performance over Baseline-all. 
LUPI further contributes to the outstanding performance of our method: It improves LSBN substantially in every split when integrated.
For an in-depth analysis on the impact of LUPI, our method is also compared with another variant (LSBN + LUPI-\textit{feat}) that directly approximates features of the teacher instead of its neural styles.
The performance gap between our model and this variant empirically justifies the use of neural styles for LUPI.

\input{Paper/Tables/ExLPose_OCN}

\subsubsection{Results on the ExLPose-OCN Dataset}
\label{sec:eval_PID-OCN}

To demonstrate the generalization capability of our method,
we evaluate our method and the other solutions on the ExLPose-OCN dataset. 
As shown in \Tbl{camera_table}, all enhancement and DA methods do not perform well on the ExLPose-OCN dataset; their performance is even inferior to that of Baseline-all (Base-all).
In contrast, our method achieves the best on the ExLPose-OCN dataset also, which suggests that it improves the generalization ability as well as pose estimation accuracy.
\Fig{ExLPose_ocn_qual} also demonstrates that our method qualitatively outperforms Baseline-all (Base-all).
Additional qualitative results are given in Sec.~\ref{appendix:qualatitive_ExLPose-OCN} of the supplement.


\input{Paper/Tables/Multipose}

\vspace{+1.0mm}
\subsection{Multi-person Pose Estimation}
\label{sec:multi-person}

Our method and most of the potential solutions can be extended to multi-person pose estimation since they are applicable to person detection as well as pose estimation. 
To this end, we train Cascade R-CNN~\cite{cai2018cascade} for person detection with the ExLPose dataset, and then utilize bounding boxes predicted by the detector instead of ground-truths; 
technical details and results of person detection in low-light conditions are presented in Sec.~\ref{appendix:networks} and Sec.~\ref{appendix:detection} of the supplement.
In \Tbl{multi-person}, each method is applied to both person detection and pose estimation models, and our method outperforms all the others in every split. 
As shown in the bottom row of Fig.~\ref{fig:qualitative_results}, our method qualitatively outperforms other solutions. 