\section{Proposed Method}
\input{Paper/Figures/Architecture}
As our target model deals with extremely low-light images, it suffers from the significantly low quality of inputs.
To alleviate this, we propose a new method for learning the target model using the paired well-lit images as privileged information~\cite{LUPI_2009}, additional high-quality input data accessible only in training.
Our method introduces another model called \emph{teacher} that takes the privileged information as input and provides rich supervision to the target model called \emph{student}.
This method for learning using privileged information (LUPI) allows the student to simulate the internal behavior of the teacher as well as learn to predict human poses.

To further exploit the privileged information, we design a single concise architecture that integrates the teacher and the student.
The key idea is to let them use separate batch normalization (BN) layers while sharing all the other parameters of the network; following the domain-specific batch normalization~\cite{chang2019domain} we call such a set of separate BNs lighting-condition specific BN (LSBN).
This design choice allows the student to enjoy the strong representation learned using the well-lit images (\ie, the privileged information) while capturing specific characteristics of low-light images through the separate BN parameters. 

Our model architecture and LUPI strategy are depicted in \Fig{pipeline}.
Note that, before being fed to the student, low-light images are scaled automatically by adjusting their average pixel intensity value to a predefined constant.
On the other hand, the teacher takes as input well-lit images as-is.
Both of the teacher and the student are trained by a common pose estimation loss:
\begin{equation} 
\mathcal{L}_\textrm{pose}(\mathbf{P}, \mathbf{Y}) = \frac{1}{K}\sum_{i=1}^{K}||\mathbf{P}_i - \mathbf{Y}_i||_2^2,
\label{eq:pose}
\end{equation}
where $P_i$ and $Y_i$ denote the predicted heatmap and ground-truth heatmap for the $i$-th joint, and $K$ refers to the number of joints.
In addition to the above loss, the student takes another supervision based on the privileged information through the teacher.
The remaining part of this section elaborates on LSBN and LUPI.


\subsection{LSBN}
\label{sec:LSBN}
For each iteration of training, a low-light image $I^{\textrm{low}}$ and its well-lit counterpart $I^{\textrm{well}}$ are given together as input and processed by different BNs in LSBNs according to their light conditions.
Each LSBN layer contains two BNs, each of which has its own affine transform parameters, $(\gamma^\textrm{low}, \beta^\textrm{low})$ for low-light and $(\gamma^\textrm{well}, \beta^\textrm{well})$ for well-lit.
Within a mini-batch of $N$ samples, an LSBN layer whitens input activations and transforms them using the lighting-condition specific affine transform parameters in a channel-wise manner.
Let $\mathbf{x} \in \mathbb{R}^{N\times H\times W}$ denote a channel of activations computed from $N$ images of a specific lighting condition, and $\lambda$ be an indicator that returns 1 if the lighting condition of $\mathbf{x}$ is `low-light' and 0 otherwise.
The output of the LSBN layer taking $\mathbf{x}$ and $\lambda$ as inputs is given by 
\begin{align} 
\textrm{LSBN}(\mathbf{x}, \lambda) & = \lambda\bigg(\gamma^\textrm{low} \cdot \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2+\epsilon}} + \beta^\textrm{low}\bigg) \nonumber \\
& + (1-\lambda)\bigg(\gamma^\textrm{well} \cdot \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2+\epsilon}} + \beta^\textrm{well}\bigg),
\label{eq:lsbn}
\end{align} 
where $\mu$ and $\sigma^2$ denote mean and variance of the activations in $\mathbf{x}$, respectively, and $\epsilon$ is a small constant adopted for numerical stability.

\subsection{LUPI}
\label{sec:LUPI}

Since low-light and well-lit images of a pair share the same content in our dataset, we argue that the gap between their predictions will be largely affected by their style difference.\footnote{It has been known that an image is separated into content and style~\cite{gatys2016image, kotovenko2019content}. Since a pair of low-light and well-lit images in ExLPose have the same content due to our camera system, they are different only in style.} 
In our case, the style of an image is determined by high-frequency noise patterns and its overall intensity.
Hence, in our LUPI strategy, the teacher provides neural styles of its intermediate feature maps as additional supervision to the student so that the student learns to fill the style gap between low-light and well-lit images in feature spaces; this approach eventually leads to a learned representation insensitive to varying lighting conditions.


To implement the above idea, we adopt as a neural style representation the Gram matrix~\cite{gatys2016image}, denoted by $\textbf{G} \in \mathbb{R}^{C \times C}$, that captures correlations between $C$ channels of a feature map $\textbf{F}$.  
Specifically, it is computed by $\textbf{G}_{i,j} = \textbf{f}_{i}^\top \textbf{f}_{j}$ where $\textbf{f}_{i}$ is the vector form of the $i^\textrm{th}$ channel of $\textbf{F}$.
Let $\textbf{G}^{\textrm{low},l}$ denote the neural style of a low-light image computed from the feature map of the $l^\textrm{th}$ layer of the student, and similarly, $\textbf{G}^{\textrm{well},l}$ be the neural style of the coupled well-lit image computed from the feature map of the $l^\textrm{th}$ layer of the teacher.
Then our LUPI strategy minimizes the following loss with respect to $\textbf{G}^{\textrm{low},l}$ of all predefined layers so that the neural style of the student approximates that of the teacher:
\begin{equation} 
\mathcal{L}_\textrm{LUPI} = \sum_{l} {\frac{1}{4 C_l^2 N_l^2} \sum_{i=1}^{C_l}\sum_{j=1}^{C_l} \Big(  \textbf{G}_{i,j}^{\textrm{low},l} - \textbf{G}_{i,j}^{\textrm{well},l} \Big)^2},
\label{eq:lupi_loss}
\end{equation}
where $C_l$ and $N_l$ are the number of channels and the spatial size of the $l^\textrm{th}$ feature map, respectively.

\subsection{Empirical Justification}
To investigate the impact of LUPI, we first demonstrate that it reduces the style gaps between different lighting conditions.
To this end, we compute the average Hausdorff distance~\cite{dubuisson1994modified} between the sets of Gram matrices of different lighting conditions before and after applying LUPI. 
\Fig{Empirical_analysis}(a) shows that the style gaps between different lighting conditions are effectively reduced by LUPI as intended.

It is also empirically examined if LSBN and LUPI of our method eventually lead to a model insensitive to lighting conditions. 
For this purpose, each lighting condition is represented by the set of features computed from associated images, and discrepancies between different lighting conditions are estimated by the average Hausdorff distances between such sets.
As shown in \Fig{Empirical_analysis}(b), LSBN matches the feature distributions of different lighting conditions, compared to the
model trained with only pose estimation loss for both low-light and well-lit images, denoted by Baseline-all.
LUPI even further closes the gaps, leading to the representation 
that well aligns two different lighting conditions.
\input{Paper/Figures/Empirical_analysis}
