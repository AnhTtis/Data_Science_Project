\section{Introduction}
Deep neural networks~\cite{chen2018cascaded,sun2019deep,toshev2014deeppose,xiao2018simple,zhang2019fast} trained with large-scale datasets~\cite{MPII_pose_dataset,guler2018densepose,johnson2010clustered,li2019crowdpose,Mscoco} have driven dramatic advances in human pose estimation recently.
However, their success demands high-quality inputs taken in controlled environments while in real-world applications images are often corrupted by low-light conditions, adverse weather conditions, sensor noises, motion blur, \etc~
Indeed, a precondition for human pose estimation \emph{in the wild} is robustness against such adverse conditions.

Motivated by this, 
we study pose estimation under \emph{extremely} low-light conditions using a single sRGB image, in which humans can barely see anything.
The task is highly practical as its solution enables nighttime applications of pose estimation
without raw-RGB data or additional devices like IR cameras. 
It is at the same time challenging due to the following two reasons. 
The first is the difficulty of data collection. Manual annotation of human poses in low-light images is often troublesome due to their limited visibility.  
The second is the difficulty of pose estimation on low-light images.
The poor quality of low-light images in terms of visibility and signal-to-noise ratio largely degrades prediction accuracy of common pose estimation models.
A na\"ive way to mitigate the second issue is to apply low-light image enhancement~\cite{chen2018learning,jiang2021enlightengan,lore2017llnet,lv2018mbllen,wang2021low} to input images.
However, image enhancement is in general highly expensive in both computation and memory.
Also, it is not aware of downstream recognition tasks and thus could be sub-optimal for pose estimation in low-light conditions.

To tackle this challenging problem, we first present a new dataset of real extremely low-light images with ground-truth pose labels.
The key feature of our dataset is that each low-light image is coupled with a well-lit image of the same content.
The advantage of using the well-lit images is two-fold.
First, they enable accurate labeling for their low-light counterparts thanks to their substantially better visibility.
Second, they can be utilized as \emph{privileged information}~\cite{LUPI_SSVM+,Heteroscedastic_Dropout,LopSchBotVap_ICLR16,LUPI_2015,LUPI_2009}, \ie, additional input data that are more informative than the original ones (low-light images in our case) but available only in training, to further improve performance on low-light images.
Such benefits of paired training images have also been validated in other robust recognition tasks~\cite{dai2020curriculum,Sakaridis_2019_ICCV,Sakaridis_2018_ECCV,Sakaridis_2018_IJCV,acdc,lee2022fifo}.
The beauty of our dataset is that pairs of low-light and well-lit images are all \emph{real} and \emph{aligned}, unlike existing datasets that provide pairs of synthetic-real images~\cite{dai2020curriculum,Sakaridis_2018_IJCV,Sakaridis_2018_ECCV} or those of largely misaligned real images~\cite{Sakaridis_2019_ICCV,acdc}.
Since it is practically impossible to capture such paired images using common cameras, we build a dedicated camera system for data collection.




We also propose an effective method based on learning using privileged information (LUPI)~\cite{LUPI_2009} to fully exploit our dataset.
The proposed method considers a model taking low-light inputs as a \emph{student} and a model dealing with corresponding well-lit images as a \emph{teacher}.
Both of the teacher and student are trained by a common pose estimation loss, and the student further utilizes knowledge of the teacher as additional supervision. 
Specifically, our method employs neural styles of intermediate feature maps as the knowledge and forces neural styles of low-light images to approximate those of well-lit images by an additional loss. 
As will be demonstrated, this LUPI approach allows the learned representation to be insensitive to lighting conditions.
Moreover, we design a new network architecture that unifies the teacher and student through lighting-condition specific batch normalization (LSBN).
LSBN consists of two batch normalization (BN) layers, each of which serves images of each lighting condition, \ie, `well-lit' or `low-light'.
We replace BNs of an existing network with LSBNs so that images of different lighting conditions are processed by different BNs.
Hence, in our architecture, the teacher and student share all the parameters except for those of their corresponding BNs, which allows the student to enjoy the strong representation learned using well-lit images.



The efficacy of our method is evaluated on real low-light images we collected for testing. 
Our method outperforms its reduced versions and relevant approaches such as lighting-condition adversarial learning and a combination of image enhancement and pose estimation.
These results clearly demonstrate the advantages of our dataset and method.
In short, our major contribution is three-fold: 
\vspace{-1.5mm}
\begin{itemize}[leftmargin=5mm] 
\itemsep=-0.5mm
    \item We propose a novel approach to human pose estimation in extremely low-light conditions using a single sRGB image. To the best of our knowledge, we are the first to tackle this challenging but highly practical problem.
    \item We build a new dataset that provides real and aligned low-light and well-lit images with accurate pose labels. 
    \item We present a strong baseline method that fully exploits the low-light and well-lit image pairs of our dataset. 
\end{itemize}