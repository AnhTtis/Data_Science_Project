\newcommand{\expnum}[2]{{#1}\mathrm{e}{-#2}}

\setcounter{section}{0}
\def\thesection{\Alph{section}}
\section*{\Large{Appendix}}
\renewcommand\thefigure{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}



This supplementary material presents additional experimental details and results that are omitted from the main paper due to the space limit.
Section~\ref{appendix:low-light_imaging} explains extremely low-light images of the ExLPose dataset.
Section~\ref{appendix:dataset_scale} shows a comparison of dataset scale with other human pose datasets.
Section~\ref{appendix:geometric_alignment} presents a detail of the geometric alignment of our dual-camera system. Section~\ref{appendix:details} describes experimental details about implementation (Sec.~\ref{appendix:implementation}) and network architectures (Sec.~\ref{appendix:networks}).
Section~\ref{appendix:empirical_analysis} shows an in-depth analysis of our method, including lighting condition analysis (Sec.~\ref{appendix:lighting_conditions}), lighting condition insensitive features analysis (Sec.~\ref{appendix:qualitative_analysis}), and additional analysis for our method (Sec.~\ref{appendix:quantitative_analysis}).
Section~\ref{appendix:Variant_enhancemnt} shows detailed results of the various combinations of the existing enhancement methods and the pose estimation network.
Section~\ref{appendix:ablation} gives extensive experimental results to investigate the components of our method, such as LSBN (Sec.~\ref{appendix:effect_LSBN}), LUPI (Sec.~\ref{appendix:feature_regression}, \ref{appendix:layer_selection}, \ref{appendix:gradient}), and intensity scaling (Sec.~\ref{appendix:scaling}).
Section~\ref{appendix:detection} presents experimental results of person detection in extremely low-light conditions. 
Finally, Section~\ref{appendix:additional_qual} shows additional qualitative results.

\section{Explanation on Extremely Low-light Images}\label{appendix:low-light_imaging}

Low-light conditions are important as they are prevalent in many scenarios with limited illumination, such as nighttime and low-light indoor environments. 
Capturing images in such conditions is challenging due to the requirement of a long exposure time, which can cause motion blur, a thorny problem to solve.
To avoid blur, a common practice is to utilize extremely low-light images captured with a short exposure time for various tasks in low-light environments, such as image enhancement~\cite{chen2018learning, Jiang_2019_ICCV, wei2020physics, wei2021physics}, action recognition~\cite{xu2020arid}, image matching~\cite{Song_2021_ICCV}, optical flow estimation~\cite{Zheng_2020_CVPR}.
In the case of human pose estimation, capturing images with a long exposure time is impossible due to the movement of humans.  
Thus, we study pose estimation under extremely low-light conditions using an extremely low-light image rather than images captured with long exposure.
The low-light images can be amplified to improve visibility using global scaling, but noise is also amplified, resulting in extremely noisy images.
\Fig{SID_and_ExLpose} shows low-light and scaled images in the SID~\cite{chen2018learning} and ExLPose datasets. The scaled images are extremely noisy since the scaling amplifies noise; the degree of noise depends on that of darkness.

\input{Supplement/Figures/SID_ExLPose}

\section{Scale Comparison to Other Datasets}\label{appendix:dataset_scale}
We compare our dataset to existing human pose estimation datasets.
As shown in the \Tbl{statistics_datasets}, the scale of our dataset is large enough since it is comparable to common datasets in terms of the number of annotated people as shown in the table.
Note that the unit of supervision in pose estimation is a human instance, not an image.

\input{Supplement/Tables/Dataset_scale}


\section{Geometric Alignment of Our Camera System}\label{appendix:geometric_alignment}

We physically align two camera modules of the dedicated dual-camera system as much as possible to capture images that are aligned with each other.
Nevertheless, there may exist a small amount of geometric misalignment between the cameras as discussed in \cite{jsrim-ECCV2020}.
Moreover, while moving around the camera system collecting the dataset, the movement of the camera system may introduce additional geometric misalignment.


To resolve this, we captured a reference image pair of a static scene before collecting data every time we moved the camera system, and estimated a homography between them.
We set the exposure time of the low-light camera module 100 times longer so that pairs of reference images have the same brightness for accurate homography estimation.
Then, we estimated a homography matrix between them using the method of~\cite{Evangelidis-TPAMI08} and aligned the collected well-lit images using the estimated homography matrix.
\Fig{geometric_alignment}(a)-(b) visualize the effect of geometric alignment using stereo-anaglyph images where a pair of well-lit and scaled low-light images from the camera modules are visualized in red and cyan.
As the figure shows, even before the geometric alignment, images from our camera system have only a small amount of misalignment. Nevertheless, the geometric alignment can successfully resolve the remaining misalignment.

\input{Supplement/Figures/Geometric_alignment}


\section{Experimental Details}\label{appendix:details}

In this section, we present the implementation settings and detailed network architectures that are omitted from the main paper due to the space limit.
\subsection{Implementation Details}\label{appendix:implementation}
\noindent\textbf{Pose Estimation Network.}
The pose estimation network is trained by the Adam optimizer~\cite{Adamsolver} with
a weight decay of $\expnum{1}{5}$ and a learning rate set initially to $\expnum{5}{4}$ and decreased by a factor of 2 every six epochs.
Each mini-batch consists of 32 images from each lighting condition.
Following Cascaded Pyramid Network (CPN)~\cite{chen2018cascaded}, 
each human box image is cropped and resized to a fixed size, 256$\times$192.
Then we apply random scale ($0.7\sim1.35$) augmentation.
During training, we multiply a weight of $\expnum{1}{3}$ to $\mathcal{L}_\textrm{LUPI}$.

\noindent\textbf{Person Detection Network.}
The person detection network,~\ie~Cascade R-CNN~\cite{cai2018cascade} with ResNeXt101~\cite{xie2017aggregated} pre-trained on ImageNet~\cite{Imagenet}, is optimized by the SGD with a momentum of 0.9, and a weight decay of $\expnum{1}{4}$ within 12 epochs.
The initial learning rate is set to $\expnum{2}{3}$ for the detection network, and it is decayed by a factor of 0.1 for the eight and eleven epochs.  
For each lighting condition, the mini-batch size is set to 2.
All input images are resized to a fixed size, 1333$\times$800.
Similar to the pose estimation network, we multiply a weight of $\expnum{1}{3}$ to $\mathcal{L}_\textrm{LUPI}$ during training.
In testing, duplicated detected boxes are filtered out by Non-Maximum Suppression (NMS) with an IoU threshold of 0.7.

\subsection{Network Architectures}\label{appendix:networks}
Fig.~\ref{fig:pose_architecture} and Fig.~\ref{fig:detection_architecture} depict the detailed architecture of human pose estimation and person detection, respectively.
 

\input{Supplement/Figures/Detail_pose}
\input{Supplement/Figures/Detail_detection}


\noindent\textbf{Pose Estimation Network.}
For the human pose estimation network, as shown in Fig.~\ref{fig:pose_architecture}, we adopt CPN~\cite{chen2018cascaded}, involving two sub-networks of GlobalNet and RefineNet.
GlobalNet is similar to the feature pyramid structure for key point estimation.
In GlobalNet, each feature from four residual blocks (\ie, R1-R4) is applied 1$\times$1 convolutional kernel and then element-wise summed.
Then, RefineNet concatenates all the features from GlobalNet.
For GlobalNet and RefineNet, the pose estimation losses are applied to the output feature of each sub-network, where we denote $\mathcal{L}_{\textrm{global}}$ and $\mathcal{L}_{\textrm{refine}}$ for them, respectively.
As mentioned Sec.~5.1 in the main paper, 
LUPI is applied to the feature maps of the first convolutional layer and the following four residual blocks of a ResNet backbone.
For more details on the pose estimation network, please refer to the CPN~\cite{chen2018cascaded} paper.



\noindent\textbf{Person Detection Network.}
We utilize Cascade R-CNN~\cite{cai2018cascade} as the person detection network.
In Fig.~\ref{fig:detection_architecture}, teacher and student networks are trained by the common detection losses for classification and regression, denoted as $\mathcal{L}_{\textrm{cls}}$ and $\mathcal{L}_{\textrm{reg}}$, respectively.
We first try applying LUPI on the feature maps of the first convolutional layer and the four residual blocks following our experimental setting in human pose estimation.
However, we found that applying LUPI on the output features from the feature pyramid network is more effective than applying the loss on the features from residual blocks.
Therefore, as presented in the figure, we finally apply LUPI on the feature maps of the first convolutional layer and the following feature pyramid network.
More detailed information for the person detection network is described in the Cascade R-CNN~\cite{cai2018cascade} paper.


\section{Empirical Analysis}\label{appendix:empirical_analysis}

\subsection{Analysis on Lighting Conditions}\label{appendix:lighting_conditions}
Our method is based on the assumption that the neural style encodes the lighting condition.
In this section, we empirically verify this assumption by visualizing the distributions of Gram matrices computed from low-light and well-lit images.
Their Gram matrices are computed at 1st, 2nd, 3rd, and 4th Res Blocks of ResNet-50~\cite{resnet} pre-trained on the ImageNet dataset~\cite{Imagenet}.
\Fig{tsne} shows $t$-SNE~\cite{MaatenNov2008} visualization of Gram matrices.
In the figure, we can observe that images of the same lighting condition are grouped together in the style spaces, which validates our assumption.

\input{Supplement/Figures/Result_tsne}

\subsection{Qualitative Analysis on Lighting Condition Insensitive Features}\label{appendix:qualitative_analysis}
We conduct an experiment to investigate the impact of lighting condition insensitive features learned by our method.
To this end, we train an image reconstruction model composed of a backbone of CPN as an encoder and a decoder which consists of transposed convolution layers on the well-lit image of the ExLPose dataset.
Then, we replace the encoder of the reconstruction model with that of the pose estimation model trained by our method.
Fig.~\ref{fig:reconstruction} shows the reconstructed results on well-lit and low-light images.
The figure demonstrates that our model learns features insensitive to lighting conditions in that reconstructed images from well-lit and low-light are consistent results.

\input{Supplement/Figures/Recon_result}



\subsection{Additional Quantitative Analysis on Our Method}\label{appendix:quantitative_analysis}

In the main paper, we compare the average Hausdorff distance~\cite{huttenlocher1993comparing} between sets of Gram matrices under different lighting conditions.
In this section, we show additional results to investigate the style gaps between well-lit and paired low-light images.
To this end, we compute the mean squared error (MSE)~\cite{wang2009mean} distance on gram matrices between low-light and well-lit images of a pair before and after applying LUPI. 
Then, we report the average values for the overall ExLPose dataset.
\Fig{feature_distance} presents that the style gaps between low-light and well-lit conditions are reduced by LUPI.

\input{Supplement/Figures/MSE_distance}


\section{Results of Various Applying Enhancement Methods}\label{appendix:Variant_enhancemnt}

\subsection{Results on the ExLPose Dataset}\label{appendix:enhance_exlpose}

In this section, we report the detailed results of adopting existing enhancement methods. 
To this end, we first apply enhancement methods on low-light images of the ExLPose dataset as pre-processing.
For the learning-based enhancement method (\ie, LLFlow), we train the method using paired low-light and well-lit images of the ExLPose dataset.
For the traditional enhancement method (\ie, LIME), we apply the method on the low-light images of the ExLPose dataset without the training process.
The straightforward way to incorporate these enhancement modules with a pose estimation network is to exploit the enhanced low-light images as inputs to evaluate Baseline-well, which is the pose estimation network trained using well-lit images only.
In \Tbl{variant_enhancement}, LLFlow + Baseline-well and LIME + Baseline-well show the performance of pose estimation using LLFlow~\cite{wang2021low} and LIME~\cite{LIME_2017_TIP} as pre-processing, respectively.
They achieve inferior performance since the enhancement module is optimized to enhance the quality of low-light images but not to improve performance on the downstream recognition models.
To address this issue, we train the pose estimation network using enhanced low-light images, denoted by LLFlow + Baseline-low$^\dagger$ and LIME + Baseline-low$^\dagger$. 
The table shows that the pose estimation network trained using enhanced low-light images performs better.
We also found that training both enhanced low-light and well-lit images significantly improves the performance.
As presented in the table, LLFlow + Baseline-all$^\dagger$ and LIME + Baseline-all$^\dagger$ show the pose estimation network trained using both enhanced low-light and well-lit images achieves the best performance among other variants.
Our method outperforms all variants of adopting enhancement methods regardless of their training strategy.

\input{Supplement/Tables/Enhancement_detail}

\subsection{Results on the ExLPose-OCN Dataset}\label{appendix:enhance_exlposeocn}
We also evaluate each combination of enhancement and pose estimation methods in Sec.~\ref{appendix:enhance_exlpose} on the ExLPose-OCN dataset.
\Tbl{variant_enhancement_pidoc} shows that the tendency of each model is similar to that of each model in \Tbl{variant_enhancement}.
In detail, a trained pose estimation model using both enhanced low-light and well-lit images (\ie, LLFlow + Baseline-all$^\dagger$ and LIME + Baseline-all$^\dagger$) outperforms the variants of them.
Our method is still superior to all variants of the combinations of enhancement and pose estimation methods.

\input{Supplement/Tables/Enhancement_ocn}


\section{Additional Ablation Studies}\label{appendix:ablation}

\subsection{Effect of LSBN}\label{appendix:effect_LSBN}

This section presents extensive experiments to investigate the effect of LSBN.
As mentioned Sec.~5.1.1 in the main paper, domain adaptation (DA) methods cannot effectively reduce the large domain gap between low-light and well-lit conditions.
As presented in Table~\ref{tab:ablation_da}, DANN~\cite{dann}, AdvEnt~\cite{vu2019advent}, and LUPI show inferior performance, proving the necessity of LSBN.
When they are combined with LSBN, the performance of each method is improved since it successfully bridges the large domain discrepancy between low-light and well-lit conditions.
When compared with `LSBN + DANN' and `LSBN + AdvEnt', our method (\ie, `LSBN + LUPI') outperforms them thanks to the effectiveness of our neural style-based approach, LUPI.
It is worth noting that our method can be a plug-and-play to the feature extractor, while AdvEnt is hard to apply to the task where the entropy map cannot be computed.

\input{Supplement/Tables/LSBN_effect}

\subsection{Effect of the Neural Style of LUPI}\label{appendix:feature_regression}
We investigate the effect of LUPI by comparing LSBN + LUPI (\ie, our method) with LSBN + LUPI-\textit{feat} that directly approximates feature maps of the teacher instead neural styles.
We conduct an in-depth analysis by comparing them in terms of the feature gaps between low-light and well-lit conditions.
As presented in Fig.~\ref{fig:lupi_effect}, LSBN + LUPI effectively reduces the average Hausdorff distance of feature maps between different lighting conditions, although LSBN + LUPI-\textit{feat} directly approximates the feature maps of well-lit images.
We conjecture that the advantage of using neural styles over directly using features comes from the less image-dependent characteristic of neural styles, \ie, features are more image-specific information that changes with each image; thus it is more difficult for LSBN + LUPI-\textit{feat} to reduce the difference between features.
In consequence, LSBN + LUPI effectively aligns the feature distributions of different lighting conditions since they aim to approximate the styles which represent the characteristics of lighting conditions.

\input{Supplement/Figures/Feature_distance}

\subsection{Effect of Layer Selection of LUPI}\label{appendix:layer_selection}

We investigate the impact of the selection of layers where LUPI is applied.
Table~\ref{tab:layerablation} compares different choices for the layer selection.
In the table, C1 applies LUPI to the output of the first convolutional layer,
and C1:R$n$ applies LUPI to the 1st to the $n$-th residual blocks as well as the first convolution layer.
As shown in the table, the performance improves as LUPI is applied to more blocks,
and our final model (C1:R4) achieves the best performance.

\input{Supplement/Tables/Layer_ablation}


\subsection{Effect of the Gradient Direction of LUPI}\label{appendix:gradient}
When training with LUPI, we let the gradient from the loss flow \emph{only} to the student in order to train the student with privileged information from the teacher, i.e., information flows in one direction from the teacher to the student.
In this section, we study the effect of this one-direction strategy on LUPI.
To this end, we prepare three variants of our approach: `T $\rightarrow$ S (Ours)', `T $\leftrightarrow$ S' and `T $\leftarrow$ S'.
`T $\rightarrow$ S (Ours)' is our proposed approach.
`T $\leftrightarrow$ S' allows the gradient from LUPI to flow to both the teacher and student models, i.e., the teacher and student can affect each other.
`T $\leftarrow$ S', on the other hand, allows the gradient to flow only to the teacher.
Table~\ref{tab:lupi_loss} compares the performance of these three variants.
As shown in the table, our approach clearly outperforms the others.
This result implies our one-direction strategy is essential for learning about LUPI.

\input{Supplement/Tables/Lupi_direction}


\subsection{Effect of Intensity Scaling}\label{appendix:scaling}

As described in the main paper, the average channel intensity of each low-light image is automatically scaled to 0.4 before being fed to the student network.
\Tbl{scaling_effect} shows the performance of Baseline-all and the proposed method trained on original low-light images and scaled low-light images.
In low-light conditions, automatically scaled low-light images significantly improve the performance of both models. However, Baseline-all trained on the scaled low-light images performs much worse in well-lit conditions.

We suspect that, in the case of using original low-light images, the Baseline-all model is biased to the well-lit condition.
It is because well-lit images have large pixel intensities, so the scale of gradient of them is larger than that of original low-light images.
Then, in the case of using scaled low-light images, the Baseline-all model is less biased for the well-lit condition, so the performance on the well-lit condition is decreased.
However, the proposed method is less biased due to the lighting condition invariant features of LSBN and LUPI.
Consequently, Table~\ref{tab:scaling_effect} demonstrates that intensity scaling of low-light images improves the performance of both Baseline-all and our method for low-light conditions.



\input{Supplement/Tables/Scaling_effect}


\section{Results of Person Detection}\label{appendix:detection}
The ExLPose dataset provides human pose and bounding box labels for training and evaluation of human pose estimation methods.
Moreover, human bounding boxes in the ExLPose dataset can serve as a detection dataset on low-light images. 
We adopt Cascade R-CNN~\cite{cai2018cascade} as our person detection network and compare our method with other solutions in the same way as described in the main paper.

\Tbl{Human_Detection} shows summarized the person detection performance of ours and other solutions.
In the table, Baseline-all is a Cascade R-CNN trained on both low-light and well-lit images with person detection loss only, and Baseline-all still underperforms our method.
For comparing other solutions, we adopt LLFlow and LIME for enhancement methods and DANN for domain adaptation. 
AdvEnt cannot be applied to the person detection task as the method is based on the entropy minimization of prediction. Accordingly, we did not conduct experiments about AdvEnt on person detection.
As shown in the table, these methods rather degrade performance in low-light conditions as a direct adoption of enhancement and domain adaptation is not effective.
On the other hand, our method outperforms by large margins in all the low-light conditions, and the results show the effectiveness of our method for person detection in the low-light condition.


\input{Supplement/Tables/Human_detection}

\section{Additional Qualitative Results}\label{appendix:additional_qual}


\subsection{Results on the ExLPose-OCN Dataset}\label{appendix:qualatitive_ExLPose-OCN}

\input{Supplement/Figures/ExLPoseOCN}

We provide the ExLPose-OCN dataset to evaluate the generalization capability of pose estimation methods to unseen cameras.
\Fig{qualitative_results_ExLPose_OCN} shows qualitative comparisons of our method with Baseline-all, DANN, and LIME + Baseline-all on the ExLPose-OCN dataset.
As shown in the figure, our method accurately estimates human poses, but other methods do not generalize well to unseen cameras and often fail to estimate accurate human poses.

\subsection{Results on the ExLPose Dataset}

\Fig{qual} shows additional qualitative results of Baseline-all, DANN~\cite{dann}, LIME~\cite{LIME_2017_TIP} + Baseline-all and our method. 
This again demonstrates that Baseline-all and DANN often fail to predict poses, while our method surpasses them.


\subsection{Failure Cases of Our Method}
We provide failure cases of our method in \Fig{failure}. 
The first row of the figure shows the results of the pose estimation network on low-light images which have little pixel information.
In such images, noise components are prevalent, and the remaining pixel information is too small to estimate human poses.
Our method also fails to predict human poses for occluded humans, as shown second and third rows in the figure. 

\subsection{Results of Enhancement Methods}

\Fig{enhance} shows enhanced low-light images of the ExLPose and ExLPose-OCN datasets using LLFlow~\cite{wang2021low} and LIME~\cite{LIME_2017_TIP}.
For the ExLPose dataset, LLFlow successfully enhances low-light images and reduces the noises of low-light images.
However, LLFlow cannot generalize well to the ExLPose-OCN dataset due to different image signal processors and exhibit different noise distributions.
Enhanced low-light images using LIME have the remaining noise as the method does not consider noise well.
These limitations may reduce the generalization capability and performance of the pose estimation when enhancement methods are combined with the pose estimation network.

\subsection{Results of Multi-person Pose Estimation}
\Fig{detection_qual} presents qualitative results for the person detection of Baseline-all, DANN, LIME + Baseline-all, and our method.
These predicted bounding boxes of our method are exploited for multi-person pose estimation; its qualitative results are shown in \Fig{multipose_qual}. 
The figure shows that our method successfully performs multi-person pose estimation while other solutions largely fail to estimate human poses. 
\clearpage

\input{Supplement/Figures/Other_qual}
