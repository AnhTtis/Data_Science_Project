\documentclass[prx,superscriptaddress,aps,amsmath,amssymb,floatfix,reprint,raggedbottom]{revtex4-2}
\usepackage{graphicx}
\usepackage[subpreambles=true]{standalone}
\usepackage{balance}
\usepackage{xr-hyper}
\urlstyle{rm}
\usepackage{amsmath}
\usepackage{times}
\usepackage{dcolumn}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{soul}
\usepackage{braket}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage[utf8]{inputenc}


\usepackage[ruled]{algorithm2e}

\DeclareUnicodeCharacter{2009}{\,} 
\def\bibsection{\section*{\refname}}

\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{%
    \externaldocument{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}

\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
        \setcounter{equation}{0}
        \renewcommand{\theequation}{S.\arabic{equation}}%
     }


\makeatletter 
\renewcommand{\fnum@figure}{\textbf{Fig.~\thefigure}}
\makeatother

\makeatletter
\def\bbordermatrix#1{\begingroup \m@th
  \@tempdima 4.75\p@
  \setbox\z@\vbox{%
    \def\cr{\crcr\noalign{\kern2\p@\global\let\cr\endline}}%
    \ialign{$##$\hfil\kern2\p@\kern\@tempdima&\thinspace\hfil$##$\hfil
      &&\quad\hfil$##$\hfil\crcr
      \omit\strut\hfil\crcr\noalign{\kern-\baselineskip}%
      #1\crcr\omit\strut\cr}}%
  \setbox\tw@\vbox{\unvcopy\z@\global\setbox\@ne\lastbox}%
  \setbox\tw@\hbox{\unhbox\@ne\unskip\global\setbox\@ne\lastbox}%
  \setbox\tw@\hbox{$\kern\wd\@ne\kern-\@tempdima\left[\kern-\wd\@ne
    \global\setbox\@ne\vbox{\box\@ne\kern2\p@}%
    \vcenter{\kern-\ht\@ne\unvbox\z@\kern-\baselineskip}\,\right]$}%
  \null\;\vbox{\kern\ht\@ne\box\tw@}\endgroup}
\makeatother

\setcounter{secnumdepth}{3}
\setlength{\belowcaptionskip}{-5pt}

\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*3}{*2}
\titlespacing{\subsection}{0pt}{*2}{*2}
\titlespacing{\subsubsection}{0pt}{*2}{*2}

\titleformat{\section}{\filcenter\normalfont\small \bfseries}{\thesection.}{1em}{\MakeUppercase}   


\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\usepackage{booktabs}

\begin{document}

\title{Training Deep Boltzmann Networks with Sparse Ising Machines}
\par


\author{Shaila Niazi}\email{sniazi@ucsb.edu}
\affiliation{Department of Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, CA, 93106, USA}
\author{Navid Anjum Aadit}
\affiliation{Department of Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, CA, 93106, USA}
\author{Masoud Mohseni}
\affiliation{Google Quantum AI, Venice, CA 90291, USA}
\author{Shuvro Chowdhury}
\affiliation{Department of Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, CA, 93106, USA}
\author{Yao Qin}
\affiliation{Department of Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, CA, 93106, USA}
\affiliation{Google Research}
\author{Kerem Y. Camsari}\email{camsari@ece.ucsb.edu}
\affiliation{Department of Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, CA, 93106, USA}


\date{\today}
\begin{abstract}
The slowing down of Moore's law has driven the development of unconventional computing paradigms, such as specialized Ising machines tailored to solve combinatorial optimization problems. In this paper, we show a new application domain for probabilistic bit (p-bit) based Ising machines by training deep generative AI models with them. Using sparse, asynchronous, and massively parallel Ising machines we train deep Boltzmann networks in a hybrid probabilistic-classical computing setup.  We use the full MNIST dataset without any downsampling or reduction in hardware-aware network topologies implemented in moderately sized Field Programmable Gate Arrays (FPGA). Our machine, which uses only  4,264 nodes (p-bits) and about 30,000 parameters, achieves the same classification accuracy (90\%) as an optimized software-based restricted Boltzmann Machine (RBM) with approximately 3.25 million parameters. Additionally, the  sparse deep Boltzmann network can generate new handwritten digits, a task the 3.25 million parameter RBM fails at despite achieving the same accuracy. Our hybrid computer takes a measured 50 to 64 billion probabilistic flips per second, which is at least an order of magnitude faster than superficially similar Graphics and Tensor Processing Unit (GPU/TPU) based implementations. The massively parallel architecture can comfortably perform the contrastive divergence algorithm (CD-$n$) with up to $n$\,=\,$10$ million sweeps per update, beyond the capabilities of existing software implementations. These results demonstrate the potential of using Ising machines for traditionally hard-to-train deep generative Boltzmann networks, with further possible improvement in nanodevice-based realizations.
\end{abstract}
\pacs{}
\maketitle

 

\section{Introduction}
\label{sec:Intro}

The slowing down of Moore's Law is ushering in an exciting new era of electronics where the traditionally separate layers of the computing stack are becoming increasingly intertwined. The rise of domain-specific computing hardware and architectures is driving unconventional computing approaches. One approach that generated great excitement recently is the field of Ising machines, where special-purpose hardware is developed to solve combinatorial optimization problems \cite{Mohseni2022}. The goal of Ising machines is to improve energy efficiency, time to solution, or some other useful metric to solve optimization problems by co-designing all layers in the computing stack. 

In this paper, we draw attention to another possibility of using probabilistic Ising machines, beyond combinatorial optimization, to demonstrate their application to deep generative AI models. We focus on 
deep Boltzmann Machines (BM) that are multi-layer generalizations of the original Boltzmann Machine \cite{hinton1984boltzmann}. Despite being powerful models, BMs fell out of favor from mainstream deep learning praxis \cite{lecun2015deep}, primarily because they are computationally hard to train with widely available hardware \cite{goodfellow2016deep}. Our goal in this paper is to illustrate how a sparse version of deep BMs can be efficiently trained using special-purpose hardware systems that provide orders of magnitude improvement over commonly used software implementations in the computationally hard probabilistic sampling task.  

We design a probabilistic bit (p-bit) \cite{camsari2017stochastic} based realization of Boltzmann networks, as their lowest level realization in hardware. Using FPGAs, we physically construct a network of binary stochastic neurons (BSN) in hardware and connect them  to one another in a fixed hardware topology. We also design an asynchronous architecture where p-bits (BSNs) dynamically evolve in parallel, much like an interacting collection of particles without a synchronizing global clock. Such a low-level realization of a Boltzmann network provides up to 5 orders of magnitude improvement in generating samples from the Boltzmann distribution, even in moderately sized FPGAs. An intense amount of work is currently underway to design scaled probabilistic computers out of magnetic nanodevices \cite{borders2019integer,grimaldi2022experimental,hayakawa2021nanosecond,kaiser2022hardware} which can scale probabilistic computers to much larger densities in energy-efficient implementations. Despite our FPGA-specific design in this paper, much of our results are applicable to scaled p-computers  as well as other Ising machines based on many different physical realizations \cite{Mohseni2022}. Our broader goal is to help stimulate the development of physics-inspired probabilistic hardware \cite{chowdhury2023full,coles2023thermodynamic} which can lead to energy-efficient systems to reduce the rapidly growing costs of conventional deep learning based on graphics and tensor processing units (GPU/TPU) \cite{patterson2021carbon}. 


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1 \textwidth]{Figures/Fig1.pdf}
    \vspace{-15pt}
    \caption{{\footnotesize (a) Hybrid computing scheme with probabilistic computer and classical computer implemented on a CPU. The p-computer generates samples according to the Boltzmann-Gibbs distribution and provides them to the CPU. Then CPU computes gradients, updates the weights (J) and biases (h), and sends them back to the p-computer until convergence. (b) The p-computer illustrated here is based on digital CMOS implementation (FPGA) and can have a measured sampling speed of $\approx 50\text{ to }64$ flips/ns. (c) Stochastic Magnetic Tunnel Junction  (sMTJ) based p-computers are projected to reach $10^{6}$ flips/ns with 1 million MTJ-based p-bits \cite{sutton2020autonomous}. (d) Hardware-aware sparse Deep Boltzmann Machines (DBMs) are represented with visible and hidden p-bits (examples of the Pegasus \cite{dattani2019pegasus} and Zephyr graphs \cite{boothby2021zephyr} are shown). (e) The sparse DBMs shown in (d) are illustrated with two layers of hidden units (Left) where both the interlayer and intralayer (not shown) connections are allowed. (see Supplementary section~\ref{sec:actual} for a more comprehensive view of the networks used in this work). When a particular label p-bit corresponding to a digit is activated (clamping that label p-bit to 1 and clamping the rest to 0), the network evolves to an image of that digit as shown in the example (Right). (f) All 10 digits are generated with sparse DBM after training the network with the full MNIST dataset.}}
    \label{fig: overview}
    \vspace{-7pt}
\end{figure*}

\section{A Hybrid Probabilistic-Classical Computing Scheme}
\label{sec:hybrid}

The approach we take in this paper to train deep Boltzmann networks is to construct a hybrid probabilistic and classical computing setup (FIG.~\ref{fig: overview}a). The role of the classical computer is to compute gradients and the new set of weights and biases given a set of samples from the probabilistic computer.  The role of the probabilistic computer is to generate equilibrium samples for a given network (defined by the set of weights and biases) according to the Boltzmann law: 
\begin{equation}
    p_i = \frac{1}{Z} \mathrm{exp}\left(-\beta E_i\right)
   \label{eq:boltz} 
\end{equation}
where $E_i$ is the configuration dependent energy and $\beta$ is the inverse (algorithmic) temperature. In our context of probabilistic sampling, $\beta$ is typically set to 1, unlike the optimization setting where it is gradually increased to find the configuration with minimum energy. In general, the configuration dependent energy can be expressed as a $k$-local Hamiltonian \cite{sejnowski1986higher}, in this paper, we focus on the 2-local energy that is given by: 
\begin{equation}
     E = - \left(\sum_{i<j} J_{ij} m_i m_j + \sum h_i m_i \right) \vspace{-4pt}
     \label{eq:en}
\end{equation}
where $J_{i j}$ and $h_i$ represent the network topology and $m_i$ represents the bipolar state of nodes that are either $+1$ or $-1$. The probabilistic computer we design approximates the Boltzmann law by the following dynamical equations, where the effective field $I_i$ and the activation of $m_i$ are given by: 
\begin{equation}
    I_i (t+\Delta t) = \sum J_{ij} m_j (t) + h_i 
    \label{eq:synapse}
\end{equation}
and the activation of a p-bit is given by: 
\begin{equation}
 m_i (t) = \mathrm{sgn}(\mathrm{tanh}[\beta I_i(t)]- \mathrm{rand}_{\text{U},[-1,1]})
 \label{eq:pbit}
\end{equation}
The iterated evolution of Eq.~(\ref{eq:synapse}) and Eq.~(\ref{eq:pbit}) with a predefined (or random) update order generates samples approximating the Boltzmann law defined by Eq.~(\ref{eq:boltz}), provided that connected p-bits are updated serially rather than in parallel, an essential requirement for the network to reach equilibrium. From a statistical sampling perspective, this process is called Gibbs Sampling \cite{koller2009probabilistic} and is a fundamental Markov Chain Monte Carlo (MCMC) algorithm used in many machine learning applications \cite{andrieu2003introduction}. The physical implementation of Eq.~(\ref{eq:synapse}) and Eq.~(\ref{eq:pbit}) to perform MCMC introduces several challenges. The primary difficulty is the serial updating requirement of connected p-bits, prohibiting the parallelization of updates in dense networks. The second difficulty is to ensure p-bits receive all the latest information from their neighbors before updating, otherwise, the network does not sample from the true Boltzmann distribution \cite{aarts1989simulated,pervaiz2017hardware}. 

\section{Hardware-aware Sparse Networks}
\label{sec:hw}

Both of these difficulties are addressed in sparse networks because sparsity limits the number of neighbors between p-bits, these networks can perform massively parallel and asynchronous updates without dropping any messages. Indeed, we show that as long as the chosen network topology is sparse, a massively parallel architecture where the frequency of probabilistic samples that linearly depends on the number of nodes in the network can be constructed \cite{Aadit2022a} (see Section~\ref{sec:architecture} for details). Our present FPGA implementation of the probabilistic computer can take up to $50$ to $64$ flips per nanosecond (flips/ns) and experimentally validated projections indicate stochastic magnetic tunnel junction-based implementations can take this number to about a million flips/ns or more (FIG.~\ref{fig: overview}b,c) \cite{sutton2020autonomous,hayakawa2021nanosecond,safranski2021demonstration,camsari2017implementing,borders2019integer}. In this paper, we adopt  the Pegasus \cite{dattani2019pegasus} and the Zepyhr \cite{boothby2021zephyr} topologies developed by D-Wave's quantum annealers to train hardware-aware sparse deep BMs (FIG.~\ref{fig: overview}d). Even though our approach is applicable to any sparse graph (regular and irregular), we focus on such hardware-aware networks with limited connectivity where maximum degrees range between 15 and 20. Our choice of sparse models is motivated by scaled but connectivity-limited networks such as the human brain and advanced microprocessors. Despite the common use of full connectivity in ML models, both advanced microprocessors with networks of billion transistors and the human brain exhibit a large degree of sparsity \cite{bassett2006small}. In fact, most hardware implementations of RBMs \cite{bojnordi2016memristive,tsai201741,kim2009highly} suffer from scaling problems due to the large fan-out required at each node, whereas sparse connectivity in hardware neural networks often exhibit energy and area advantages \cite{ardakani2016sparsely}. 

FIG.~\ref{fig: overview}e shows a typical sparse DBM that we use in this paper with 2-layers of hidden bits. This graph is obtained by randomly assigning visible and hidden bits in the Pegasus (or Zephyr) graphs of various sizes. Unlike standard deep BMs \cite{salakhutdinov2010efficient,goodfellow2013joint}, sparse DBMs do not have fully-connected interlayer connections. On the other hand, they do allow connections between nodes in a given layer, increasing the representative capability of the network. 
In Section~\ref{sec:random}, we systematically study the effect of distributing visible/hidden nodes in such sparse networks, which introduces new challenges that do not exist in fully-connected networks. 

Unlike standard deep BM training where training is typically done layer-by-layer \cite{hinton2002training}, in sparse DBMs, we tackle the training directly on the full network, by relying on our massively parallel architecture and the efficient mixing of sparse graphs.

As we discuss in Section~\ref{sec:train}, we reach about 90\% classification accuracy  in 100 epochs with the full MNIST dataset without any downsampling, coarse-graining, or the use of much simpler datasets, typically performed in alternative hardware-based approaches \cite{adachi2015application,manukian2019accelerating,dixit2021training,bohm2022noise}. Moreover, unlike RBMs, the sparse DBM learns the images well enough that for any given label, it can generate a new handwritten digit as shown in Fig.~\ref{fig: overview}f, when a single one-hot encoded output p-bit is clamped to a given digit. 
 Image generation is an important feature of physics-inspired algorithms such as diffusion models \cite{sohl2015deep}, and the fact that RBMs fail at this task even when they have 100$\times$ more parameters is surprising, stressing the potential of sparse DBM models, as we discuss further in Section~\ref{sec:ImageSynth}.  


\section{Training Sparse DBMs with Sparse Ising Machines}
\label{sec:algorithm}

\SetKwComment{Comment}{\normalfont $\triangleright$ }{}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInput{KwSampler}{Sampler}
\begin{algorithm}
\caption{Training sparse DBMs}
\label{alg:alg2}
\Input{number of samples $N$,  batch size $B$, number of batches $N_{B}$, epochs $N_{\text{L}}$, learning rate $\varepsilon$}

\KwSampler{Classic Gibbs CPU, Graph-colored Gibbs CPU, Graph-colored Gibbs FPGA}
\Output{trained weights $J_{\text{out}}$ and biases $h_{\text{out}}$}
$J_{\text{out}} \gets \mathcal{N}(0,0.01)$\;
$h_{\text{out,hidden}} \gets 0$\;
$h_{\text{out,visible}} \gets \log{(p_i/(1-p_i))}$\;
 
\For{$i\gets 1$ \KwTo $N_{\text{L}}$}{
    \For{$j \gets 1 $ \KwTo $N_{\text{B}}$}{
        $J_{\text{Sampler}} \gets J_{\text{out}}$\;
        \For{$k \gets 1$ \KwTo $B$}{
            \tcc{positive phase}
            $h_{\text{$B$}} \gets \text{ clamping to batch images}$\;
            $h_{\text{Sampler}} \gets h_{\text{$B$}}+h_{\text{out}}$\;
            $\{m\} \gets \text{Sampler}(N)$ \Comment*[r]{p-computer}\ 
            $\langle m_im_j\rangle_{\text{data}} = \{m\}\{m\}^{\text{T}}$\ \Comment*[r]{CPU}\
        }
        \tcc{negative phase}
        $h_{\text{Sampler}} \gets h_{\text{out}}$\;
        $\{m\} \gets \text{Sampler}(N\times B)$ \Comment*[r]{p-computer}\ 
        $\langle m_im_j\rangle_{\text{model}} = \{m\}\{m\}^{\text{T}}$\ \Comment*[r]{CPU}\ 
        \tcc{update weights and biases}
        $J_{\text{out}} \gets J_{\text{out}} + \Delta J_{ij}$\ \Comment*[r]{CPU}\ 
        $h_{\text{out}} \gets h_{\text{out}} +\Delta h_i$\ \Comment*[r]{CPU}\ 
    }
}
\end{algorithm}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{Figures/Fig2.pdf}
    \vspace{-15pt} 
    \caption{{\footnotesize (a) MNIST accuracy vs training epochs: with sparse DBM, 90\% accuracy is achieved in 100 epochs. Full MNIST (60,000 images) is trained on sparse DBM (Pegasus 4,264 p-bits) with CD-$10^{5}$, batch size = 50, learning rate = 0.003, momentum = 0.6 and epoch = 100 where the total number of parameters is 30,404. Each epoch is defined as the network seeing the entire 60,000 images with 1,200 weight updates. Test accuracy shows the accuracy of all the 10,000 images from the MNIST test set and the training accuracy represents the accuracy of 10,000 images that are randomly chosen from the training dataset. (b) MNIST accuracy with Restricted Boltzmann Machine (RBM) using 43 hidden units and CD-1 (CPU implementation) where the total number of parameters is 34,142. The accuracy of this RBM is less than 90\% but sparse DBM can reach 90\% with approximately the same number of parameters. (c) MNIST accuracy of RBM with 4,096 hidden units. Here the total number of parameters is 3,252,224 and the accuracy is 90\% in 100 epochs which can be achieved using sparse DBM with around $100\times$ fewer parameters.}} 
    \label{fig:accuracy}
    \vspace{-10pt}
\end{figure*}

As our network model, we use the Pegasus and Zephyr graphs at different sizes as a fixed network. 
Boltzmann networks are typically used for unsupervised learning without any explicit labels. To use deep BMs for classification, we follow a similar approach to \cite{larochelle2008classification} where we create additional visible bits, calling them ``label bits''. We use one-hot encoding to specify 10 digits with 10 label bits, such that each image in the MNIST data set is paired with these label bits (FIG.~\ref{fig: overview}e). We then use our fast Gibbs sampler (probabilistic computer) to perform the contrastive divergence (CD) algorithm \cite{hinton2002training,larochelle2007empirical} that minimizes the KL divergence between the data and the model distributions. An equivalent formulation from a maximum likelihood estimation viewpoint \cite{koller2009probabilistic,pmlr-vR5-carreira-perpinan05a} can also be used to obtain the following learning rule (see Supplementary Section~\ref{sec:derivation}),
\begin{eqnarray}
\Delta J_{ij} &=& \varepsilon\bigg(\langle m_im_j\rangle_\text{{data}}-\langle m_im_j\rangle_\text{{model}}\bigg) \label{eq:del_J}\\
\Delta h_{i} &=& \varepsilon\bigg(\langle m_i\rangle_\text{{data}}-\langle m_i\rangle_\text{{model}}\bigg)
   \label{eq:del_h}
\end{eqnarray}
where $\Delta J_{ij}$ and $\Delta h_{i}$ represent the weight and bias updates per iteration and the terms in the parentheses represent the negative gradient of the KL divergence between data and the model distributions. $\varepsilon$ is the learning rate, $\langle m_im_j\rangle_\text{{data}}$ and $\langle m_im_j\rangle_\text{{model}}$ are the average correlation between p-bits $i$ and $j$ in the ``positive'' (data) and ``negative'' (model) phases, respectively. During the positive phase of sampling, the p-computer clamps the visible p-bits to the corresponding training image one after the other, taking $N$ sweeps for each image for a total of $N \times B$ sweeps where $B$ is the batch size. Using these sweeps, the CPU then computes the data  correlations $\langle m_im_j\rangle_{\text{data}}$. In the negative phase, the p-computer is allowed to run freely without any clamping, and the CPU computes the model correlations $\langle m_im_j\rangle_{\text{model}}$ by taking $N\times B$ sweeps. Then the connection weights are updated according to Eq.~\eqref{eq:del_J} and Eq.~\eqref{eq:del_h}. In actual training, we also use a momentum modification to Eqs.~(\ref{eq:del_J},\ref{eq:del_h}) (see Supplementary Section~\ref{sec:momentum}).  A pseudocode of the algorithm is presented in Algorithm~\ref{alg:alg2}.

 
For the sparse DBMs we consider in this work, establishing correlations between the data requires executing Gibbs sampling even for the positive phase, which is obtained in a single inference step in RBMs. Because our probabilistic sampler is stopped every time the samples are transferred to the CPU that calculates the gradients, our machine naturally implements the persistent contrastive divergence (PCD) \cite{tieleman2008training,hinton2012practical,fischer2014training} algorithm. PCD maintains a long-running Markov chain with the hope that small changes in the weights do not take the equilibrium state of the new network far from the old one. We discuss the effect of PCD vs CD in the context of our results in Section~\ref{sec:equilibrium}.


\section{Results on the Full MNIST dataset}
\label{sec:train}

The dataset that we used for training sparse DBMs is the full MNIST handwritten digit dataset \cite{lecun1998mnist,deng2012mnist} without any reduction or downsampling. MNIST consists of 60,000 training images and 10,000 test images with $28\times28$ pixels having digits from 0 to 9 and we use black/white images by rounding up the pixel intensities. We set the initial values of weights and biases according to the recipe that Hinton suggested in \cite{hinton2012practical} for RBMs. The weights are initialized to small random values chosen from a zero-mean Gaussian with a standard deviation of 0.01 for every p-bit. The initial values of hidden biases are set to zero and visible biases are set to log[$p_i/(1-p_i)$] where $p_i$ is the proportion of training vectors in which unit $i$ is on. The values of hyperparameters used while training are  $\beta$ = 1, learning rate $\varepsilon$ = 0.003, and momentum $\alpha$ = 0.6.

 \begin{figure*}[!t]
    \centering
    \includegraphics[width=.70\linewidth]{Figures/Fig3.pdf}
    \vspace{-10pt}
    \caption{{\footnotesize (a) Images generated with sparse DBM by annealing the network from  $\beta$\,=\,0 to  $\beta$\,=\,5 with 0.125 steps. The labels for a particular digit are clamped to show how the visible p-bits evolve to that specific image. Examples of digits `0' and `7' are shown here. (b) The same procedure for image generation is applied to the RBM network (with 4,096 hidden units) that achieves 90\% test accuracy. Using the same annealing schedule, RBM does not produce the correct digits unlike the sparse DBM.}}
    \label{fig:ImSynth}
    \vspace{-8pt}
\end{figure*}

The sparse DBM network used here (the largest size Pegasus that we can fit into our FPGA) consists of 834 visible p-bits ($834 = 28 \times 28 + 10 \times 5$; we used 5 sets of labels each containing 10 p-bits) and 3,430 hidden p-bits arranged in 2-layers as shown in the inset of Fig.~\ref{fig:accuracy}a. Then we randomly distribute the visible and hidden units on the sparse DBMs to ensure the label indices are delocalized (see Section~\ref{sec:random} for details of this process and the original network in Supplementary Fig.~\ref{fig:P14}). 

To train the network efficiently, we divide the training set into 1,200 mini-batches having 50 images in each batch. The weights are updated after each mini-batch following the PCD algorithm. We train MNIST for 100 epochs with CD-$10^{5}$, where $10^5$ sweeps of the entire network are taken in the negative phase ($N\times B$). The weight precision in the FPGA is 10 bits (1 sign, 6 integer, and 3 fraction bits, i.e., s[6][3]) while the CPU uses double-precision floating-point with 64 bits, to compute the gradients. Before the new weights are loaded into the FPGA, however, they are reduced to s[6][3] to fit into the FPGA. A systematic study of the effect of weight precision is shown in Supplementary Section~\ref{sec:bit_precision}. In short, we do not observe any significant differences at higher precision in the FPGA, indicating that the 10-bit weight precision is adequate. 

During inference, the 784 p-bits that correspond to the pixels are clamped to the test data and the label p-bits fluctuate freely. To test classification accuracy, we use $10^5$ sweeps and perform a softmax classification scheme as follows: as we have 50 label p-bits for 5 sets of labels, by time-averaging the corresponding label bits we finally have the 10 labels for 10 digits. The p-bit with the highest probability of being `1' is used for the classified digit. For comparison, we also train an optimized RBM model using CD-1 in the CPU. The label, testing, and training details of RBMs are very similar to those for sparse DBMs.  

Fig.~\ref{fig:accuracy} shows our main results. We see that the sparse DBM architecture in the Pegasus graph with 4,264 p-bits reaches about 90\% accuracy in 100 epochs.  To compare the sparse DBM architecture with a standard RBM, we perform two tests, one at ``iso-parameter'' and the other at ``iso-accuracy''.  The iso-parameter test uses an RBM with about the same number of parameters (with an all-to-all interlayer connection). This RBM falls short of reaching 90\% in this setting. Then, we choose an RBM with 100$\times$ more parameters and observe that the results saturate at about 90\% accuracy. We also note that increasing CD-1 to CD-$n$ ($n$ up to 100) does not make an appreciable difference in accuracy, while making the training computationally much harder. 


Detailed testing in both models (sparse DBM and RBM) indicates that marginal improvements are possible with more training epochs, however, both models show similar asymptotic behavior in 100 epochs, this is why we stop training around 100 epochs (Supplementary Section~\ref{sec:num_parameters} shows experiments at various network sizes).  Note that this is still a computationally intense process where 60,000 images are shown to the network for a total of 6,000,000 times and the weights are updated  a total of 100$\times N_B$ = 120,000 times since $N_B$\,=\,1200. 

Based on these experimental results, we arrive  at the following 
two important conclusions: First, the sparse DBM architecture despite having a much smaller degree of connections between its layers (limited to a graph degree of $\approx$15 to 20) matches the classification accuracy of a fully-connected RBM. Second, the sparse DBM requires far fewer parameters (about 30,000) to reach 90\% accuracy in the MNIST dataset. Both of these indicate the potential of sparse DBMs which can be directly tackled by the orders of magnitude acceleration obtained in the hardware. 


Clearly,  sparse DBMs have not achieved the state-of-the-art classification accuracy in the MNIST dataset at these modest network sizes and depths. Surprisingly, however, despite the severely limited connectivity, the sparse DBM architecture matches the performance of highly optimized RBMs, establishing a baseline. More detailed comparisons (or hybrid approaches) with deeper RBMs in the future will reveal the true potential of sparse DBMs. Next, we discuss another benefit of sparse DBMs compared to RBMs beyond mere classification accuracy.  

\section{Image generation}
\label{sec:ImageSynth}

Given their generative nature, a natural question to ask is whether the sparse DBM and RBM can generate new images when they are run in the ``invertible mode''. This is similar to the image generation from ``noise'' discussed in diffusion models \cite{sohl2015deep}. We test this idea post-training by clamping the label bits to a given digit and annealing the network using the inverse temperature, $\beta$.

Here we present an example of image synthesis with sparse DBMs with $\approx$\,30,000 parameters and the optimized RBM with $\approx$\,3.25 million parameters (Fig.~\ref{fig:ImSynth}a-b). For this process, we clamp the label bits for digits `0' or `7' while all other p-bits run freely. Using the final weights and biases and by annealing the network slowly from $\beta$\,=\,$0$ to $\beta$\,=\,$5$ with a $0.125$ increment and we check the 784 image p-bits at various time steps. At lower values of  $\beta$ when the system is in a high-temperature state, the model is sampling from noise (first two columns of Fig.~\ref{fig:ImSynth}a). With increasing $\beta$ values, digits gradually become recognizable, and at final $\beta$\,=\,$5$ we see clear images of  a `0' or `7' (leftmost column of Fig.~\ref{fig:ImSynth}a). This example is demonstrated with the Pegasus graph using only about 30,000 parameters from FIG.~\ref{fig:accuracy}, similar results are obtained with the Zephyr graph as we discuss in Supplementary Section~\ref{sec:ImSynth_zephyr}).

In contrast, the images generated with the RBM (4,096 hidden units) are not recognizable even after careful annealing (Fig.~\ref{fig:ImSynth}b), despite multiple experiments with different trials. Similarly, the annealing schedule for RBM varies from $\beta$\,=\,$0$ to $\beta$\,=\,$5$. To test whether RBMs can generate images with better gradient calculations, we also trained an RBM with 4,096 hidden p-bits with CD-$10^{2}$ but this did not lead to any success in image generation. 

Note that in different settings, such as in freely ``dreaming'' or  image completing networks, it  may be possible to generate images with RBMs \cite{hu2016deep}. In our experiments, the image generation task is forced by clamping only the label bits, without giving the network any other lead. In this stricter sense, the failure of the RBM to generate images seems to be in keeping with the general understanding of the subject \cite{goodfellow2020generative,sleeman2020hybrid}. We believe that accelerating the Gibbs sampling by orders of magnitude can enable the training of even deeper, previously untrainable deep BMs. The potential of physics-inspired RBMs for image generation is  also seen in the recent interest in Gaussian-Bernoulli (GRBMs) \cite{liao2022gaussian}, whose sparse and deep variants could be even more powerful. 


\section{Mixing times}
\label{sec:equilibrium}

One of the key difficulties that are often cited in the training of Boltzmann networks is the computational intractability of the partition function, $Z$, that appears in the Boltzmann law, Eq.~(\ref{eq:boltz}). Formally, what is required to ensure an exact calculation of the gradient is that the calculation of correlations $\langle m_i m_j \rangle$ and averages $\langle m_i \rangle$ come from the equilibrium states of a given network defined by $J_{ij}$ and $h_i$. The time it takes for an undirected network to reach equilibrium is defined as the ``mixing time''. A formal analysis of how long it takes for a given graph to mix can be extremely difficult to calculate and is unknown for all but the simplest, most regular networks \cite{levin2017markov}. Here, we empirically study the mixing time of the Pegasus graph that we used in generating our main results in FIG.~\ref{fig:accuracy}. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=.90\columnwidth]{Figures/Fig4.pdf}
    \vspace{-10pt} 
    \caption{{\footnotesize Test accuracy after training full MNIST (up to only 40 epochs for computational simplicity) with different numbers of sweeps per iteration is shown. For our sparse graph, to mix the Markov chain properly we need a minimum CD-$10^{4}$. Reducing the number of sweeps to $10^{3}$ or $10^{2}$ degrades the quality of mixing the chain significantly.}}     \label{fig:real_sweep}
    \vspace{-10pt}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width =.99\textwidth ]{Figures/Fig5.pdf}
    \caption{{\footnotesize (a) The sparse DBMs (Pegasus and Zephyr) where all the p-bits are distributed in a serial manner such as 1 to 784 are the visible p-bits, 785 to 834 are the label p-bits (50 bits for 5 sets of labels), and the rest are hidden p-bits. (b) The sparse DBMs with randomized indices are shown here. (c) Test accuracy of full MNIST as a function of training epochs for two different sparse DBMs. In both cases, training the sparse DBMs with the serial distribution (no randomization) of indices could not achieve an accuracy of more than 50\%. In contrast, randomization of indices helps the network to reach ~90\% accuracy.}}
\label{fig:rand_label}
\vspace{-8pt}
\end{figure*}

In FIG.~\ref{fig:real_sweep}, we observe that the test set accuracy of our network increases significantly if the probabilistic sampler takes $10^4$ or more sweeps per weight update. Above this value, there seems to be diminishing returns in improving the accuracy. This suggests that taking more sweeps does not improve the estimation of the averages and correlations because these samples are already in equilibrium and $\approx$\,$10^4$ sweeps at this size (with 4,264 p-bits) of the Pegasus graph can be empirically defined as the mixing time of the network. As mentioned earlier, our probabilistic computer naturally performs the persistent CD algorithm (PCD) because it does not randomize the chain when the weights are updated. When we compared the PCD algorithm with the original CD algorithm by  deliberately randomizing the chain after loading a new set of weights, we did not observe a significant difference in accuracy. This is likely because performing CD-$10^5$ removes any difference between PCD and CD since the mixing time of the network seems to be about $10^4$ sweeps. Irrespective of whether we start the chain from a random state or not, by CD-$10^5$, our sampler starts to sample from the equilibrium distribution of the graph beyond $10^4$ sweeps. This lack of difference between PCD and CD is consistent with the mixing results shown in FIG.~\ref{fig:real_sweep}. 

The reason for the saturating classification accuracy of sparse DBMs  at around 90\%  is likely that the network is not deep or wide enough and not because of the intractability of the algorithm. In fact, considering our hardware architecture FPGA is able to take $\approx$ 64 billion samples per second, and obtaining $10^5$ sweeps from our machine can be done in mere milliseconds (Table~\ref{tab:fps} shows comparisons of sampling rates between standard CPUs and our graph colored (GC) architecture, where our probabilistic computer (GC-FPGA) demonstrates $\approx$ 4 to 6 orders of magnitude improvement over the optimized and standard CPU implementations of Gibbs sampling, respectively). In Supplementary Section~\ref{sec:gpu}, we show how the performance reported in Table~\ref{tab:fps} fares against superficially similar Ising solvers in highly optimized GPU and TPU implementations.

These results suggest that our machine can be used to sample much more ``difficult'' networks that require many more samples to mix, enabling the training of previously untrainable networks with potentially much richer representation. 

\begin{table}[!b]
    \centering
    \caption{{\footnotesize Comparison of the FPGA-based MCMC sampler with standard CPU and graph-colored CPU implementations. All data points are measured, as discussed in the Methods.}}
    \vspace{7.5pt}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        {\bf Sampling method} & {\bf topology} &{\bf  size} & {\bf max. degree} & {\bf flips/ns} \\ \midrule

         Standard Gibbs (CPU) & Pegasus & 4,264 & 15 & $2.18\times10^{-5}$ \\
         GC Gibbs (CPU) & Pegasus & 4,264 &  15 & $8.50\times10^{-3}$\\
         GC Gibbs (FPGA) &  Pegasus & 4,264 & 15 & $6.40 \times10^{1}$ \\
         \midrule
         Standard Gibbs (CPU) & Zephyr & 3,360 &  20 & $2.67\times10^{-5}$ \\
         GC Gibbs (CPU) & Zephyr & 3,360 & 20 & $4.40\times10^{-3}$\\
         GC Gibbs (FPGA) &   Zephyr & 3,360 & 20 & $5.04\times10^{1}$\\
         \bottomrule
        
    \end{tabular}
    \label{tab:fps}
\end{table}



\section{Randomization of indices}
\label{sec:random}
A very important point that arises in training Boltzmann network models on a given sparse network is the notion of graph distance between visible, hidden, and label nodes. Typically, if the layers are fully connected, the graph distance between any given two nodes is a constant. On the other hand, when training sparse DBMs, the placement of visible, hidden, and label p-bits play a crucial role. FIG.~\ref{fig:rand_label} shows the comparison of how the indexing of p-bits affects the classification accuracy in the Pegasus and Zephyr graphs. We observe that if the hidden, visible, and label bits are clustered and too close, the classification accuracy suffers greatly. This is likely because the correlation between the label bits and the visible bits gets weaker if their graph distance is too large. On the other hand, randomizing the indices seems to solve this problem, repeatable in completely different but sparse graphs. We performed further experiments with different random indices and essentially observed the same behavior. For example, Supplementary Fig.~\ref{fig:acc_vs_param} shows a monotonically increasing accuracy with different sizes of sparse DBMs (Pegasus) even though each graph has a different randomized index set. 

\begin{figure*}[!t]
    \centering 
\includegraphics[width=0.99\textwidth]{Figures/Fig6.pdf}
\vspace{-10pt} 
    \caption{{\footnotesize (a) An example of massively parallel architecture with four parallel same-frequency and equally phase-shifted clocks to trigger the colored p-bit blocks. The sparse DBM (Pegasus 4,264 p-bits) is colored with four colors using the graph-coloring algorithm to exploit parallel updating of unconnected p-bits and the input for each p-bit is computed using  Eq.~\eqref{eq:synapse}. (b) Measured flips/ns as a function of graph size (number of p-bits) showing ideal parallelism scaling linearly with the system size in the case of the graph-colored FPGA (top). The graph-colored CPU flips/ns as a function of the graph size (bottom).}} 
    \label{fig:architecture}
    \vspace{-8pt}
\end{figure*}
To reduce the graph distance between the label bits, visible and hidden bits, we chose 5 sets of label bits (5$\times$10 = 50 p-bits) using one-hot encoding per digit. Experiments with more label bits did not show significant differences.  Also, experiments with multiple label bits in the RBM did not show any difference. This suggests that  randomization of indices is particularly important for sparse models, but is unnecessary for fully-connected networks whose graph distance between any two nodes is a constant.  



\section{p-computer Architecture}
\label{sec:architecture}



On the sparse DBM, we color the graph using the heuristic graph-coloring algorithm DSatur \cite{brelaz1979new} to exploit parallel updating of unconnected p-bits. This approach involves assigning different colors to connected p-bits and the same color to unconnected p-bits as shown in Fig.~\ref{fig:architecture}a to implement Gibbs sampling in a massively parallel manner on sparse and irregular graphs \cite{Aadit2022a}. Finding the minimum number of colors is an NP-hard problem, however, the minimum number of colors is not a strict requirement as sparse graphs require only a limited number of colors, and for our purpose, heuristic coloring algorithms like DSatur with polynomial complexity can color the graph efficiently. 


In the case of the Pegasus graph with 4,264 p-bits, where the maximum number of neighbors is 15, only four colors are used as shown in Fig.~\ref{fig:architecture}a. Therefore we need four equally phase-shifted and same-frequency clocks for updating the p-bits in each color block one by one. Similarly, the Zephyr graph (3,360 p-bits and the maximum number of neighbors is 20) can also be colored with five colors using this procedure. In this approach, a graph comprised of $N$ p-bits is able to perform a full sweep in a single clock cycle ($T_{\text{clk}}$). We refer to this architecture as the pseudo-asynchronous Gibbs sampling \cite{chowdhury2023full}. The key advantage of this approach is that the p-computer becomes faster as the graph size grows as shown in Fig.~\ref{fig:architecture}b and Table~\ref{tab:fps} for both graph-colored FPGA and graph-colored CPU.
 


\section*{Conclusion}
\label{sec:conclusions}
In this work, we have presented a hybrid probabilistic-classical computing setup to train sparse and deep Boltzmann networks. We used a sparse Ising machine with a massively parallel architecture that achieves a sampling speed orders of magnitude faster than traditional CPUs. Unlike similar hardware approaches where the MNIST dataset is downsampled, reduced, or replaced entirely for smaller datasets, we trained the full MNIST dataset without any simplifications. Our sparse DBM matched the accuracy of RBMs while using 100$\times$ fewer parameters and successfully generated new images while the RBM failed to do so. We systematically studied the mixing time of the hardware-aware network topologies and showed that the classification accuracy of our model is not limited by the computational tractability of the algorithm but limited by the moderately sized FPGAs that we were able to use in this work. Further improvements may involve using deeper, wider, and perhaps ``harder to mix'' network architectures, fully utilizing our ultrafast probabilistic sampler. Moreover, combining layer-by-layer training techniques of conventional DBMs with our approach could lead to further possible improvements. Nanodevice implementations of such sparse Ising machines, for example, using stochastic magnetic tunnel junctions may significantly change this picture, potentially changing established wisdom on the practical use of deep Boltzmann networks. 

\appendix

\section*{Methods}

\subsection{FPGA and CPU specifications}
\label{sec:fpga_cpu_spec}
In this article, Xilinx Alveo U250, a data center accelerator card (Virtex UltraScale+ XCU250 FPGA) with peripheral component interconnect express (PCIe) connectivity has been used \cite{xilinx-u250}. PCIe interface performs data transfer at the rate of 2.5 gigatransfers per second (GT/s). The classical computer used in this study is equipped with an 11th Gen Intel Core i7-11700 processor with a clock speed of up to 4.90 GHz and 64 GB of random access memory (RAM).

The digital implementation of p-bits consists of a pseudorandom number generator (PRNG), a lookup table for the activation function (tanh), and a threshold to generate a binary output (details in the Supplementary Section~\ref{sec:fpga}). Weights and biases with fixed point precision of 10 bits (1 sign bit, 6 integer bits, and 3 fraction bits) are used to provide tunability through the activation function. 

\subsection{MNIST data, D-Wave graphs and RBM code}
\label{sec:mnist_data}
MNIST files are downloaded from \cite{lecun1998mnist}. Then the image data are converted to binary form (black and white) by rounding up the pixel intensities in MATLAB. The Pegasus and Zephyr graphs are extracted using the procedure described in \cite{dwave-docs}. The RBM code used in this work is similar to the one available in \cite{hinton2014training}.


\subsection{Data transfer between FPGA and CPU}
\label{sec:data_transfer}
A PCIe interface is used to communicate between FPGA and CPU through MATLAB interface for the `read/write' operations (see Supplementary Fig.~\ref{fig:fpga_imp}c). A global `disable/enable' signal broadcast from MATLAB to the FPGA is used to freeze/resume all p-bits. Before a `read' instruction, the `disable' signal is sent to freeze all the p-bits simultaneously. Once the p-bits are frozen, the data are read using the PCIe interface and sent to MATLAB for post-processing i.e., computing gradients and updating the weights. When the `read' instruction is done, the `enable' signal is sent to resume the p-bit operations. In the same way, for the `write' instruction, the `disable' signal is sent from MATLAB to freeze the p-bits before sending the updated weights. After the `write' instruction is done, p-bits are enabled again with the `enable' signal sent from MATLAB. The data transfer efficiency is influenced by this back-and-forth communication between the FPGA and MATLAB. Furthermore, the conversion of bipolar to binary weights and biases during each epoch (as explained in Supplementary Section~\ref{sec:bipolar_to_binary}) adds some time overheads while sending them from MATLAB to FPGA. Even though sampling is very fast in FPGA, due to these overheads it takes $\approx$\,28 hours to train full MNIST on 4,264 p-bit Pegasus with CD-$10^5$ for 100 epochs. This issue can be improved significantly by updating the weights and biases inside the FPGA.

\subsection{Measurement of flips per nanosecond}
\label{sec:fps_measure}
To measure the flips/ns, one p-bit in each color block is designed with a programmable counter in the FPGA to count the flip attempts. A reference counter running parallelly is set to count up to a preset value at the positive edge of a reference clock. When the reference counter is done counting, the p-bit counters are stopped. Comparing the p-bit counter outputs (representing the total number of attempted flips in each color block) with the reference counter preset value, the time for the total flips is obtained. With this data, the flips/ns of the p-computer is measured experimentally. To determine the flips/ns for the standard CPU and graph-colored CPU, MATLAB built-in `tic' and `toc' functions are used to measure the elapsed time while counting the total flips. The flips/ns is measured in real-time using this data. The error bars in Fig.~\ref{fig:architecture}b are obtained by taking 500 measurements of flips/ns.

\section*{Acknowledgements}
We gratefully acknowledge discussions with Dr. Jan Kaiser. We are thankful to the Xilinx University Donation Program (XUP) for the FPGA development boards and G. Eschemann for useful discussions on airhdl. This work is partially supported by an Office of Naval Research Young Investigator Program grant, and a National Science Foundation CCF 2106260 grant. 




\section*{Data availability}
The data that support the plots within this paper and other findings of this study are available from the corresponding author upon reasonable request.

\section*{Code availability}
The computer code used in this study is available from the corresponding author upon reasonable request.

\section*{Author contributions}
SN and KYC conceived the study. KYC supervised the study. SN and NAA developed the hybrid FPGA-CPU implementation. SN and SC performed the benchmark RBM training. SN and NAA performed the FPGA experiments to train sparse DBMs. SN, NAA, MM, SC, YQ, KYC discussed, analyzed the experiments, and participated in writing the manuscript. 
\section*{Competing interests}
The authors declare no competing interests.

\balance{
\bibliographystyle{unsrtnat}}
%apsrev4-2.bst 2019-01-14 (MD) hand-edited version of apsrev4-1.bst
%Control: key (0)
%Control: author (8) initials jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (0) allowed
%Control: page (0) single
%Control: year (1) truncated
%Control: production of eprint (0) enabled
\begin{thebibliography}{59}%
\makeatletter
\providecommand \@ifxundefined [1]{%
 \@ifx{#1\undefined}
}%
\providecommand \@ifnum [1]{%
 \ifnum #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \@ifx [1]{%
 \ifx #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \natexlab [1]{#1}%
\providecommand \enquote  [1]{``#1''}%
\providecommand \bibnamefont  [1]{#1}%
\providecommand \bibfnamefont [1]{#1}%
\providecommand \citenamefont [1]{#1}%
\providecommand \href@noop [0]{\@secondoftwo}%
\providecommand \href [0]{\begingroup \@sanitize@url \@href}%
\providecommand \@href[1]{\@@startlink{#1}\@@href}%
\providecommand \@@href[1]{\endgroup#1\@@endlink}%
\providecommand \@sanitize@url [0]{\catcode `\\12\catcode `\$12\catcode
  `\&12\catcode `\#12\catcode `\^12\catcode `\_12\catcode `\%12\relax}%
\providecommand \@@startlink[1]{}%
\providecommand \@@endlink[0]{}%
\providecommand \url  [0]{\begingroup\@sanitize@url \@url }%
\providecommand \@url [1]{\endgroup\@href {#1}{\urlprefix }}%
\providecommand \urlprefix  [0]{URL }%
\providecommand \Eprint [0]{\href }%
\providecommand \doibase [0]{https://doi.org/}%
\providecommand \selectlanguage [0]{\@gobble}%
\providecommand \bibinfo  [0]{\@secondoftwo}%
\providecommand \bibfield  [0]{\@secondoftwo}%
\providecommand \translation [1]{[#1]}%
\providecommand \BibitemOpen [0]{}%
\providecommand \bibitemStop [0]{}%
\providecommand \bibitemNoStop [0]{.\EOS\space}%
\providecommand \EOS [0]{\spacefactor3000\relax}%
\providecommand \BibitemShut  [1]{\csname bibitem#1\endcsname}%
\let\auto@bib@innerbib\@empty
%</preamble>
\bibitem [{\citenamefont {Mohseni}\ \emph {et~al.}(2022)\citenamefont
  {Mohseni}, \citenamefont {McMahon},\ and\ \citenamefont
  {Byrnes}}]{Mohseni2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {N.}~\bibnamefont
  {Mohseni}}, \bibinfo {author} {\bibfnamefont {P.~L.}\ \bibnamefont
  {McMahon}},\ and\ \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Byrnes}},\ }\bibfield  {title} {\bibinfo {title} {Ising machines as hardware
  solvers of combinatorial optimization problems},\ }\href
  {https://doi.org/10.1038/s42254-022-00440-8} {\bibfield  {journal} {\bibinfo
  {journal} {Nature Reviews Physics}\ }\textbf {\bibinfo {volume} {4}},\
  \bibinfo {pages} {363} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hinton}\ \emph {et~al.}(1984)\citenamefont {Hinton},
  \citenamefont {Sejnowski},\ and\ \citenamefont
  {Ackley}}]{hinton1984boltzmann}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.~E.}\ \bibnamefont
  {Hinton}}, \bibinfo {author} {\bibfnamefont {T.~J.}\ \bibnamefont
  {Sejnowski}},\ and\ \bibinfo {author} {\bibfnamefont {D.~H.}\ \bibnamefont
  {Ackley}},\ }\href@noop {} {\emph {\bibinfo {title} {Boltzmann machines:
  Constraint satisfaction networks that learn}}}\ (\bibinfo  {publisher}
  {Carnegie-Mellon University, Department of Computer Science Pittsburgh, PA},\
  \bibinfo {year} {1984})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {LeCun}\ \emph {et~al.}(2015)\citenamefont {LeCun},
  \citenamefont {Bengio},\ and\ \citenamefont {Hinton}}]{lecun2015deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {LeCun}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Bengio}},\ and\
  \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Hinton}},\ }\bibfield
  {title} {\bibinfo {title} {Deep learning},\ }\href@noop {} {\bibfield
  {journal} {\bibinfo  {journal} {nature}\ }\textbf {\bibinfo {volume} {521}},\
  \bibinfo {pages} {436} (\bibinfo {year} {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Goodfellow}\ \emph {et~al.}(2016)\citenamefont
  {Goodfellow}, \citenamefont {Bengio},\ and\ \citenamefont
  {Courville}}]{goodfellow2016deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Goodfellow}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Bengio}},\
  and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Courville}},\
  }\href@noop {} {\emph {\bibinfo {title} {Deep learning}}}\ (\bibinfo
  {publisher} {MIT press},\ \bibinfo {year} {2016})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Camsari}\ \emph
  {et~al.}(2017{\natexlab{a}})\citenamefont {Camsari}, \citenamefont {Faria},
  \citenamefont {Sutton},\ and\ \citenamefont {Datta}}]{camsari2017stochastic}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.~Y.}\ \bibnamefont
  {Camsari}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Faria}},
  \bibinfo {author} {\bibfnamefont {B.~M.}\ \bibnamefont {Sutton}},\ and\
  \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Datta}},\ }\bibfield
  {title} {\bibinfo {title} {Stochastic p-bits for invertible logic},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Physical Review
  X}\ }\textbf {\bibinfo {volume} {7}},\ \bibinfo {pages} {031014} (\bibinfo
  {year} {2017}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Borders}\ \emph {et~al.}(2019)\citenamefont
  {Borders}, \citenamefont {Pervaiz}, \citenamefont {Fukami}, \citenamefont
  {Camsari}, \citenamefont {Ohno},\ and\ \citenamefont
  {Datta}}]{borders2019integer}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.~A.}\ \bibnamefont
  {Borders}}, \bibinfo {author} {\bibfnamefont {A.~Z.}\ \bibnamefont
  {Pervaiz}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Fukami}},
  \bibinfo {author} {\bibfnamefont {K.~Y.}\ \bibnamefont {Camsari}}, \bibinfo
  {author} {\bibfnamefont {H.}~\bibnamefont {Ohno}},\ and\ \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {Datta}},\ }\bibfield  {title} {\bibinfo
  {title} {Integer factorization using stochastic magnetic tunnel junctions},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }
  (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Grimaldi}\ \emph {et~al.}(2022)\citenamefont
  {Grimaldi}, \citenamefont {Selcuk}, \citenamefont {Aadit}, \citenamefont
  {Kobayashi}, \citenamefont {Cao}, \citenamefont {Chowdhury}, \citenamefont
  {Finocchio}, \citenamefont {Kanai}, \citenamefont {Ohno}, \citenamefont
  {Fukami},\ and\ \citenamefont {Camsari}}]{grimaldi2022experimental}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Grimaldi}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Selcuk}},
  \bibinfo {author} {\bibfnamefont {N.~A.}\ \bibnamefont {Aadit}}, \bibinfo
  {author} {\bibfnamefont {K.}~\bibnamefont {Kobayashi}}, \bibinfo {author}
  {\bibfnamefont {Q.}~\bibnamefont {Cao}}, \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {Chowdhury}}, \bibinfo {author} {\bibfnamefont
  {G.}~\bibnamefont {Finocchio}}, \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {Kanai}}, \bibinfo {author} {\bibfnamefont
  {H.}~\bibnamefont {Ohno}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Fukami}},\ and\ \bibinfo {author} {\bibfnamefont {K.~Y.}\ \bibnamefont
  {Camsari}},\ }\bibfield  {title} {\bibinfo {title} {Experimental evaluation
  of simulated quantum annealing with mtj-augmented p-bits},\ }in\ \href@noop
  {} {\emph {\bibinfo {booktitle} {2022 IEEE International Electron Devices
  Meeting (IEDM)}}}\ (\bibinfo {year} {2022})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hayakawa}\ \emph {et~al.}(2021)\citenamefont
  {Hayakawa}, \citenamefont {Kanai}, \citenamefont {Funatsu}, \citenamefont
  {Igarashi}, \citenamefont {Jinnai}, \citenamefont {Borders}, \citenamefont
  {Ohno},\ and\ \citenamefont {Fukami}}]{hayakawa2021nanosecond}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont
  {Hayakawa}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Kanai}},
  \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Funatsu}}, \bibinfo
  {author} {\bibfnamefont {J.}~\bibnamefont {Igarashi}}, \bibinfo {author}
  {\bibfnamefont {B.}~\bibnamefont {Jinnai}}, \bibinfo {author} {\bibfnamefont
  {W.}~\bibnamefont {Borders}}, \bibinfo {author} {\bibfnamefont
  {H.}~\bibnamefont {Ohno}},\ and\ \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {Fukami}},\ }\bibfield  {title} {\bibinfo {title}
  {Nanosecond random telegraph noise in in-plane magnetic tunnel junctions},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Physical Review
  Letters}\ }\textbf {\bibinfo {volume} {126}},\ \bibinfo {pages} {117202}
  (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kaiser}\ \emph {et~al.}(2022)\citenamefont {Kaiser},
  \citenamefont {Borders}, \citenamefont {Camsari}, \citenamefont {Fukami},
  \citenamefont {Ohno},\ and\ \citenamefont {Datta}}]{kaiser2022hardware}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Kaiser}}, \bibinfo {author} {\bibfnamefont {W.~A.}\ \bibnamefont {Borders}},
  \bibinfo {author} {\bibfnamefont {K.~Y.}\ \bibnamefont {Camsari}}, \bibinfo
  {author} {\bibfnamefont {S.}~\bibnamefont {Fukami}}, \bibinfo {author}
  {\bibfnamefont {H.}~\bibnamefont {Ohno}},\ and\ \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {Datta}},\ }\bibfield  {title} {\bibinfo
  {title} {Hardware-aware in situ learning based on stochastic magnetic tunnel
  junctions},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Applied}\ }\textbf {\bibinfo {volume} {17}},\ \bibinfo
  {pages} {014016} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Chowdhury}\ \emph {et~al.}(2023)\citenamefont
  {Chowdhury}, \citenamefont {Grimaldi}, \citenamefont {Aadit}, \citenamefont
  {Niazi}, \citenamefont {Mohseni}, \citenamefont {Kanai}, \citenamefont
  {Ohno}, \citenamefont {Fukami}, \citenamefont {Theogarajan}, \citenamefont
  {Finocchio} \emph {et~al.}}]{chowdhury2023full}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Chowdhury}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Grimaldi}},
  \bibinfo {author} {\bibfnamefont {N.~A.}\ \bibnamefont {Aadit}}, \bibinfo
  {author} {\bibfnamefont {S.}~\bibnamefont {Niazi}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {Mohseni}}, \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {Kanai}}, \bibinfo {author} {\bibfnamefont
  {H.}~\bibnamefont {Ohno}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Fukami}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Theogarajan}},
  \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Finocchio}}, \emph
  {et~al.},\ }\bibfield  {title} {\bibinfo {title} {A full-stack view of
  probabilistic computing with p-bits: devices, architectures and algorithms},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {IEEE Journal on
  Exploratory Solid-State Computational Devices and Circuits}\ } (\bibinfo
  {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Coles}(2023)}]{coles2023thermodynamic}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont
  {Coles}},\ }\bibfield  {title} {\bibinfo {title} {Thermodynamic ai and the
  fluctuation frontier},\ }\href@noop {} {\bibfield  {journal} {\bibinfo
  {journal} {arXiv preprint arXiv:2302.06584}\ } (\bibinfo {year}
  {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Patterson}\ \emph {et~al.}(2021)\citenamefont
  {Patterson}, \citenamefont {Gonzalez}, \citenamefont {Le}, \citenamefont
  {Liang}, \citenamefont {Munguia}, \citenamefont {Rothchild}, \citenamefont
  {So}, \citenamefont {Texier},\ and\ \citenamefont
  {Dean}}]{patterson2021carbon}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Patterson}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Gonzalez}},
  \bibinfo {author} {\bibfnamefont {Q.}~\bibnamefont {Le}}, \bibinfo {author}
  {\bibfnamefont {C.}~\bibnamefont {Liang}}, \bibinfo {author} {\bibfnamefont
  {L.-M.}\ \bibnamefont {Munguia}}, \bibinfo {author} {\bibfnamefont
  {D.}~\bibnamefont {Rothchild}}, \bibinfo {author} {\bibfnamefont
  {D.}~\bibnamefont {So}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Texier}},\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Dean}},\
  }\bibfield  {title} {\bibinfo {title} {Carbon emissions and large neural
  network training},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2104.10350}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Sutton}\ \emph {et~al.}(2020)\citenamefont {Sutton},
  \citenamefont {Faria}, \citenamefont {Ghantasala}, \citenamefont {Jaiswal},
  \citenamefont {Camsari},\ and\ \citenamefont {Datta}}]{sutton2020autonomous}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B.}~\bibnamefont
  {Sutton}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Faria}},
  \bibinfo {author} {\bibfnamefont {L.~A.}\ \bibnamefont {Ghantasala}},
  \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Jaiswal}}, \bibinfo
  {author} {\bibfnamefont {K.~Y.}\ \bibnamefont {Camsari}},\ and\ \bibinfo
  {author} {\bibfnamefont {S.}~\bibnamefont {Datta}},\ }\bibfield  {title}
  {\bibinfo {title} {Autonomous probabilistic coprocessing with petaflips per
  second},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {IEEE
  Access}\ }\textbf {\bibinfo {volume} {8}},\ \bibinfo {pages} {157238}
  (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dattani}\ \emph {et~al.}(2019)\citenamefont
  {Dattani}, \citenamefont {Szalay},\ and\ \citenamefont
  {Chancellor}}]{dattani2019pegasus}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {N.}~\bibnamefont
  {Dattani}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Szalay}},\
  and\ \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Chancellor}},\
  }\bibfield  {title} {\bibinfo {title} {Pegasus: The second connectivity graph
  for large-scale quantum annealing hardware},\ }\href@noop {} {\bibfield
  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:1901.07636}\ } (\bibinfo
  {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Boothby}\ \emph {et~al.}(2021)\citenamefont
  {Boothby}, \citenamefont {King},\ and\ \citenamefont
  {Raymond}}]{boothby2021zephyr}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont
  {Boothby}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {King}},\ and\
  \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Raymond}},\ }\bibfield
  {title} {\bibinfo {title} {Zephyr topology of d-wave quantum processors},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {D-Wave Tech. Rep.
  Ser}\ ,\ \bibinfo {pages} {1}} (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Sejnowski}(1986)}]{sejnowski1986higher}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.~J.}\ \bibnamefont
  {Sejnowski}},\ }\bibfield  {title} {\bibinfo {title} {Higher-order boltzmann
  machines},\ }in\ \href@noop {} {\emph {\bibinfo {booktitle} {AIP Conference
  Proceedings}}},\ Vol.\ \bibinfo {volume} {151}\ (\bibinfo {organization}
  {American Institute of Physics},\ \bibinfo {year} {1986})\ pp.\ \bibinfo
  {pages} {398--403}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Koller}\ and\ \citenamefont
  {Friedman}(2009)}]{koller2009probabilistic}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Koller}}\ and\ \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont
  {Friedman}},\ }\href@noop {} {\emph {\bibinfo {title} {Probabilistic
  graphical models: principles and techniques}}}\ (\bibinfo  {publisher} {MIT
  press},\ \bibinfo {year} {2009})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Andrieu}\ \emph {et~al.}(2003)\citenamefont
  {Andrieu}, \citenamefont {De~Freitas}, \citenamefont {Doucet},\ and\
  \citenamefont {Jordan}}]{andrieu2003introduction}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Andrieu}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {De~Freitas}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Doucet}},\ and\ \bibinfo
  {author} {\bibfnamefont {M.~I.}\ \bibnamefont {Jordan}},\ }\bibfield  {title}
  {\bibinfo {title} {An introduction to mcmc for machine learning},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Machine learning}\
  }\textbf {\bibinfo {volume} {50}},\ \bibinfo {pages} {5} (\bibinfo {year}
  {2003})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Aarts}\ and\ \citenamefont
  {Korst}(1989)}]{aarts1989simulated}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {E.}~\bibnamefont
  {Aarts}}\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Korst}},\
  }\href@noop {} {\emph {\bibinfo {title} {Simulated annealing and Boltzmann
  machines: a stochastic approach to combinatorial optimization and neural
  computing}}}\ (\bibinfo  {publisher} {John Wiley \& Sons, Inc.},\ \bibinfo
  {year} {1989})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Pervaiz}\ \emph {et~al.}(2017)\citenamefont
  {Pervaiz}, \citenamefont {Ghantasala}, \citenamefont {Camsari},\ and\
  \citenamefont {Datta}}]{pervaiz2017hardware}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~Z.}\ \bibnamefont
  {Pervaiz}}, \bibinfo {author} {\bibfnamefont {L.~A.}\ \bibnamefont
  {Ghantasala}}, \bibinfo {author} {\bibfnamefont {K.~Y.}\ \bibnamefont
  {Camsari}},\ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Datta}},\ }\bibfield  {title} {\bibinfo {title} {Hardware emulation of
  stochastic p-bits for invertible logic},\ }\href@noop {} {\bibfield
  {journal} {\bibinfo  {journal} {Scientific reports}\ }\textbf {\bibinfo
  {volume} {7}},\ \bibinfo {pages} {10994} (\bibinfo {year}
  {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Aadit}\ \emph {et~al.}(2022)\citenamefont {Aadit},
  \citenamefont {Grimaldi}, \citenamefont {Carpentieri}, \citenamefont
  {Theogarajan}, \citenamefont {Martinis}, \citenamefont {Finocchio},\ and\
  \citenamefont {Camsari}}]{Aadit2022a}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {N.~A.}\ \bibnamefont
  {Aadit}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Grimaldi}},
  \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Carpentieri}}, \bibinfo
  {author} {\bibfnamefont {L.}~\bibnamefont {Theogarajan}}, \bibinfo {author}
  {\bibfnamefont {J.~M.}\ \bibnamefont {Martinis}}, \bibinfo {author}
  {\bibfnamefont {G.}~\bibnamefont {Finocchio}},\ and\ \bibinfo {author}
  {\bibfnamefont {K.~Y.}\ \bibnamefont {Camsari}},\ }\bibfield  {title}
  {\bibinfo {title} {Massively parallel probabilistic computing with sparse
  ising machines},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Nature Electronics}\ }\textbf {\bibinfo {volume} {5}},\ \bibinfo {pages}
  {460} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Safranski}\ \emph {et~al.}(2021)\citenamefont
  {Safranski}, \citenamefont {Kaiser}, \citenamefont {Trouilloud},
  \citenamefont {Hashemi}, \citenamefont {Hu},\ and\ \citenamefont
  {Sun}}]{safranski2021demonstration}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Safranski}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Kaiser}},
  \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Trouilloud}}, \bibinfo
  {author} {\bibfnamefont {P.}~\bibnamefont {Hashemi}}, \bibinfo {author}
  {\bibfnamefont {G.}~\bibnamefont {Hu}},\ and\ \bibinfo {author}
  {\bibfnamefont {J.~Z.}\ \bibnamefont {Sun}},\ }\bibfield  {title} {\bibinfo
  {title} {Demonstration of nanosecond operation in stochastic magnetic tunnel
  junctions},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nano
  letters}\ }\textbf {\bibinfo {volume} {21}},\ \bibinfo {pages} {2040}
  (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Camsari}\ \emph
  {et~al.}(2017{\natexlab{b}})\citenamefont {Camsari}, \citenamefont
  {Salahuddin},\ and\ \citenamefont {Datta}}]{camsari2017implementing}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.~Y.}\ \bibnamefont
  {Camsari}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Salahuddin}},\ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Datta}},\ }\bibfield  {title} {\bibinfo {title} {Implementing p-bits with
  embedded mtj},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {IEEE Electron Device Letters}\ }\textbf {\bibinfo {volume} {38}},\ \bibinfo
  {pages} {1767} (\bibinfo {year} {2017}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bassett}\ and\ \citenamefont
  {Bullmore}(2006)}]{bassett2006small}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.~S.}\ \bibnamefont
  {Bassett}}\ and\ \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont
  {Bullmore}},\ }\bibfield  {title} {\bibinfo {title} {Small-world brain
  networks},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {The
  neuroscientist}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {512}
  (\bibinfo {year} {2006})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bojnordi}\ and\ \citenamefont
  {Ipek}(2016)}]{bojnordi2016memristive}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.~N.}\ \bibnamefont
  {Bojnordi}}\ and\ \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont
  {Ipek}},\ }\bibfield  {title} {\bibinfo {title} {Memristive boltzmann
  machine: A hardware accelerator for combinatorial optimization and deep
  learning},\ }in\ \href@noop {} {\emph {\bibinfo {booktitle} {High Performance
  Computer Architecture (HPCA), 2016 IEEE International Symposium on}}}\
  (\bibinfo {organization} {IEEE},\ \bibinfo {year} {2016})\ pp.\ \bibinfo
  {pages} {1--13}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Tsai}\ \emph {et~al.}(2017)\citenamefont {Tsai},
  \citenamefont {Yu}, \citenamefont {Wong},\ and\ \citenamefont
  {Lee}}]{tsai201741}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.-H.}\ \bibnamefont
  {Tsai}}, \bibinfo {author} {\bibfnamefont {W.-J.}\ \bibnamefont {Yu}},
  \bibinfo {author} {\bibfnamefont {W.~H.}\ \bibnamefont {Wong}},\ and\
  \bibinfo {author} {\bibfnamefont {C.-Y.}\ \bibnamefont {Lee}},\ }\bibfield
  {title} {\bibinfo {title} {A 41.3/26.7 pj per neuron weight rbm processor
  supporting on-chip learning/inference for iot applications},\ }\href@noop {}
  {\bibfield  {journal} {\bibinfo  {journal} {IEEE Journal of Solid-State
  Circuits}\ }\textbf {\bibinfo {volume} {52}},\ \bibinfo {pages} {2601}
  (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kim}\ \emph {et~al.}(2009)\citenamefont {Kim},
  \citenamefont {McAfee}, \citenamefont {McMahon},\ and\ \citenamefont
  {Olukotun}}]{kim2009highly}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.~K.}\ \bibnamefont
  {Kim}}, \bibinfo {author} {\bibfnamefont {L.~C.}\ \bibnamefont {McAfee}},
  \bibinfo {author} {\bibfnamefont {P.~L.}\ \bibnamefont {McMahon}},\ and\
  \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Olukotun}},\ }\bibfield
  {title} {\bibinfo {title} {A highly scalable restricted boltzmann machine
  fpga implementation},\ }in\ \href@noop {} {\emph {\bibinfo {booktitle} {2009
  International Conference on Field Programmable Logic and Applications}}}\
  (\bibinfo {organization} {IEEE},\ \bibinfo {year} {2009})\ pp.\ \bibinfo
  {pages} {367--372}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Ardakani}\ \emph {et~al.}(2016)\citenamefont
  {Ardakani}, \citenamefont {Condo},\ and\ \citenamefont
  {Gross}}]{ardakani2016sparsely}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Ardakani}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Condo}},\
  and\ \bibinfo {author} {\bibfnamefont {W.~J.}\ \bibnamefont {Gross}},\
  }\bibfield  {title} {\bibinfo {title} {Sparsely-connected neural networks:
  towards efficient vlsi implementation of deep neural networks},\ }\href@noop
  {} {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint
  arXiv:1611.01427}\ } (\bibinfo {year} {2016})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Salakhutdinov}\ and\ \citenamefont
  {Larochelle}(2010)}]{salakhutdinov2010efficient}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Salakhutdinov}}\ and\ \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Larochelle}},\ }\bibfield  {title} {\bibinfo {title} {Efficient learning of
  deep boltzmann machines},\ }in\ \href@noop {} {\emph {\bibinfo {booktitle}
  {Proceedings of the thirteenth international conference on artificial
  intelligence and statistics}}}\ (\bibinfo {organization} {JMLR Workshop and
  Conference Proceedings},\ \bibinfo {year} {2010})\ pp.\ \bibinfo {pages}
  {693--700}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Goodfellow}\ \emph {et~al.}(2013)\citenamefont
  {Goodfellow}, \citenamefont {Courville},\ and\ \citenamefont
  {Bengio}}]{goodfellow2013joint}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.~J.}\ \bibnamefont
  {Goodfellow}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Courville}},\ and\ \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {Bengio}},\ }\bibfield  {title} {\bibinfo {title} {Joint training deep
  boltzmann machines for classification},\ }\href@noop {} {\bibfield  {journal}
  {\bibinfo  {journal} {arXiv preprint arXiv:1301.3568}\ } (\bibinfo {year}
  {2013})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hinton}(2002)}]{hinton2002training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.~E.}\ \bibnamefont
  {Hinton}},\ }\bibfield  {title} {\bibinfo {title} {Training products of
  experts by minimizing contrastive divergence},\ }\href@noop {} {\bibfield
  {journal} {\bibinfo  {journal} {Neural computation}\ }\textbf {\bibinfo
  {volume} {14}},\ \bibinfo {pages} {1771} (\bibinfo {year}
  {2002})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Adachi}\ and\ \citenamefont
  {Henderson}(2015)}]{adachi2015application}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.~H.}\ \bibnamefont
  {Adachi}}\ and\ \bibinfo {author} {\bibfnamefont {M.~P.}\ \bibnamefont
  {Henderson}},\ }\bibfield  {title} {\bibinfo {title} {Application of quantum
  annealing to training of deep neural networks},\ }\href@noop {} {\bibfield
  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:1510.06356}\ } (\bibinfo
  {year} {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Manukian}\ \emph {et~al.}(2019)\citenamefont
  {Manukian}, \citenamefont {Traversa},\ and\ \citenamefont
  {Di~Ventra}}]{manukian2019accelerating}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Manukian}}, \bibinfo {author} {\bibfnamefont {F.~L.}\ \bibnamefont
  {Traversa}},\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Di~Ventra}},\ }\bibfield  {title} {\bibinfo {title} {Accelerating deep
  learning with memcomputing},\ }\href@noop {} {\bibfield  {journal} {\bibinfo
  {journal} {Neural Networks}\ }\textbf {\bibinfo {volume} {110}},\ \bibinfo
  {pages} {1} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dixit}\ \emph {et~al.}(2021)\citenamefont {Dixit},
  \citenamefont {Selvarajan}, \citenamefont {Alam}, \citenamefont {Humble},\
  and\ \citenamefont {Kais}}]{dixit2021training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {Dixit}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Selvarajan}},
  \bibinfo {author} {\bibfnamefont {M.~A.}\ \bibnamefont {Alam}}, \bibinfo
  {author} {\bibfnamefont {T.~S.}\ \bibnamefont {Humble}},\ and\ \bibinfo
  {author} {\bibfnamefont {S.}~\bibnamefont {Kais}},\ }\bibfield  {title}
  {\bibinfo {title} {Training restricted boltzmann machines with a d-wave
  quantum annealer},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Frontiers in Physics}\ }\textbf {\bibinfo {volume} {9}},\ \bibinfo {pages}
  {589626} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {B{\"o}hm}\ \emph {et~al.}(2022)\citenamefont
  {B{\"o}hm}, \citenamefont {Alonso-Urquijo}, \citenamefont {Verschaffelt},\
  and\ \citenamefont {Van~der Sande}}]{bohm2022noise}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {B{\"o}hm}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Alonso-Urquijo}}, \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Verschaffelt}},\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Van~der Sande}},\ }\bibfield  {title} {\bibinfo {title} {Noise-injected
  analog ising machines enable ultrafast statistical sampling and machine
  learning},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nature
  Communications}\ }\textbf {\bibinfo {volume} {13}},\ \bibinfo {pages} {5847}
  (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Sohl-Dickstein}\ \emph {et~al.}(2015)\citenamefont
  {Sohl-Dickstein}, \citenamefont {Weiss}, \citenamefont {Maheswaranathan},\
  and\ \citenamefont {Ganguli}}]{sohl2015deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Sohl-Dickstein}}, \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont
  {Weiss}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont
  {Maheswaranathan}},\ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Ganguli}},\ }\bibfield  {title} {\bibinfo {title} {Deep unsupervised
  learning using nonequilibrium thermodynamics},\ }in\ \href@noop {} {\emph
  {\bibinfo {booktitle} {International Conference on Machine Learning}}}\
  (\bibinfo {organization} {PMLR},\ \bibinfo {year} {2015})\ pp.\ \bibinfo
  {pages} {2256--2265}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Larochelle}\ and\ \citenamefont
  {Bengio}(2008)}]{larochelle2008classification}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Larochelle}}\ and\ \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {Bengio}},\ }\bibfield  {title} {\bibinfo {title} {Classification using
  discriminative restricted boltzmann machines},\ }in\ \href@noop {} {\emph
  {\bibinfo {booktitle} {Proceedings of the 25th international conference on
  Machine learning}}}\ (\bibinfo {year} {2008})\ pp.\ \bibinfo {pages}
  {536--543}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Larochelle}\ \emph {et~al.}(2007)\citenamefont
  {Larochelle}, \citenamefont {Erhan}, \citenamefont {Courville}, \citenamefont
  {Bergstra},\ and\ \citenamefont {Bengio}}]{larochelle2007empirical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Larochelle}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Erhan}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Courville}}, \bibinfo
  {author} {\bibfnamefont {J.}~\bibnamefont {Bergstra}},\ and\ \bibinfo
  {author} {\bibfnamefont {Y.}~\bibnamefont {Bengio}},\ }\bibfield  {title}
  {\bibinfo {title} {An empirical evaluation of deep architectures on problems
  with many factors of variation},\ }in\ \href@noop {} {\emph {\bibinfo
  {booktitle} {Proceedings of the 24th international conference on Machine
  learning}}}\ (\bibinfo {year} {2007})\ pp.\ \bibinfo {pages}
  {473--480}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Carreira-Perpi{\~n}\'an}\ and\ \citenamefont
  {Hinton}(2005)}]{pmlr-vR5-carreira-perpinan05a}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.~A.}\ \bibnamefont
  {Carreira-Perpi{\~n}\'an}}\ and\ \bibinfo {author} {\bibfnamefont
  {G.}~\bibnamefont {Hinton}},\ }\bibfield  {title} {\bibinfo {title} {On
  contrastive divergence learning},\ }in\ \href
  {https://proceedings.mlr.press/r5/carreira-perpinan05a.html} {\emph {\bibinfo
  {booktitle} {Proceedings of the Tenth International Workshop on Artificial
  Intelligence and Statistics}}},\ \bibinfo {series} {Proceedings of Machine
  Learning Research}, Vol.~\bibinfo {volume} {R5},\ \bibinfo {editor} {edited
  by\ \bibinfo {editor} {\bibfnamefont {R.~G.}\ \bibnamefont {Cowell}}\ and\
  \bibinfo {editor} {\bibfnamefont {Z.}~\bibnamefont {Ghahramani}}}\ (\bibinfo
  {publisher} {PMLR},\ \bibinfo {year} {2005})\ pp.\ \bibinfo {pages}
  {33--40},\ \bibinfo {note} {reissued by PMLR on 30 March 2021.}\BibitemShut
  {Stop}%
\bibitem [{\citenamefont {Tieleman}(2008)}]{tieleman2008training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Tieleman}},\ }\bibfield  {title} {\bibinfo {title} {Training restricted
  boltzmann machines using approximations to the likelihood gradient},\ }in\
  \href@noop {} {\emph {\bibinfo {booktitle} {Proceedings of the 25th
  international conference on Machine learning}}}\ (\bibinfo {year} {2008})\
  pp.\ \bibinfo {pages} {1064--1071}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hinton}(2012)}]{hinton2012practical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.~E.}\ \bibnamefont
  {Hinton}},\ }\bibinfo {title} {A practical guide to training restricted
  boltzmann machines},\ in\ \href@noop {} {\emph {\bibinfo {booktitle} {Neural
  Networks: Tricks of the Trade: Second Edition}}},\ \bibinfo {editor} {edited
  by\ \bibinfo {editor} {\bibfnamefont {G.}~\bibnamefont {Montavon}}, \bibinfo
  {editor} {\bibfnamefont {G.~B.}\ \bibnamefont {Orr}},\ and\ \bibinfo {editor}
  {\bibfnamefont {K.-R.}\ \bibnamefont {M{\"u}ller}}}\ (\bibinfo  {publisher}
  {Springer Berlin Heidelberg},\ \bibinfo {address} {Berlin, Heidelberg},\
  \bibinfo {year} {2012})\ pp.\ \bibinfo {pages} {599--619}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Fischer}\ and\ \citenamefont
  {Igel}(2014)}]{fischer2014training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Fischer}}\ and\ \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Igel}},\
  }\bibfield  {title} {\bibinfo {title} {Training restricted boltzmann
  machines: An introduction},\ }\href@noop {} {\bibfield  {journal} {\bibinfo
  {journal} {Pattern Recognition}\ }\textbf {\bibinfo {volume} {47}},\ \bibinfo
  {pages} {25} (\bibinfo {year} {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {LeCun}(1998)}]{lecun1998mnist}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {LeCun}},\ }\bibfield  {title} {\bibinfo {title} {The mnist database of
  handwritten digits},\ }\href@noop {} {\bibfield  {journal} {\bibinfo
  {journal} {http://yann. lecun. com/exdb/mnist/}\ } (\bibinfo {year}
  {1998})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Deng}(2012)}]{deng2012mnist}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont
  {Deng}},\ }\bibfield  {title} {\bibinfo {title} {The mnist database of
  handwritten digit images for machine learning research},\ }\href@noop {}
  {\bibfield  {journal} {\bibinfo  {journal} {IEEE Signal Processing Magazine}\
  }\textbf {\bibinfo {volume} {29}},\ \bibinfo {pages} {141} (\bibinfo {year}
  {2012})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hu}\ \emph {et~al.}(2016)\citenamefont {Hu},
  \citenamefont {Gao},\ and\ \citenamefont {Ma}}]{hu2016deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Hu}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Gao}},\ and\
  \bibinfo {author} {\bibfnamefont {Q.}~\bibnamefont {Ma}},\ }\bibfield
  {title} {\bibinfo {title} {Deep restricted boltzmann networks},\ }\href@noop
  {} {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint
  arXiv:1611.07917}\ } (\bibinfo {year} {2016})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Goodfellow}\ \emph {et~al.}(2020)\citenamefont
  {Goodfellow}, \citenamefont {Pouget-Abadie}, \citenamefont {Mirza},
  \citenamefont {Xu}, \citenamefont {Warde-Farley}, \citenamefont {Ozair},
  \citenamefont {Courville},\ and\ \citenamefont
  {Bengio}}]{goodfellow2020generative}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Goodfellow}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Pouget-Abadie}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Mirza}}, \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Xu}}, \bibinfo
  {author} {\bibfnamefont {D.}~\bibnamefont {Warde-Farley}}, \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {Ozair}}, \bibinfo {author} {\bibfnamefont
  {A.}~\bibnamefont {Courville}},\ and\ \bibinfo {author} {\bibfnamefont
  {Y.}~\bibnamefont {Bengio}},\ }\bibfield  {title} {\bibinfo {title}
  {Generative adversarial networks},\ }\href@noop {} {\bibfield  {journal}
  {\bibinfo  {journal} {Communications of the ACM}\ }\textbf {\bibinfo {volume}
  {63}},\ \bibinfo {pages} {139} (\bibinfo {year} {2020})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Sleeman}\ \emph {et~al.}(2020)\citenamefont
  {Sleeman}, \citenamefont {Dorband},\ and\ \citenamefont
  {Halem}}]{sleeman2020hybrid}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Sleeman}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Dorband}},\
  and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Halem}},\ }\bibfield
   {title} {\bibinfo {title} {A hybrid quantum enabled rbm advantage:
  convolutional autoencoders for quantum image compression and generative
  learning},\ }in\ \href@noop {} {\emph {\bibinfo {booktitle} {Quantum
  information science, sensing, and computation XII}}},\ Vol.\ \bibinfo
  {volume} {11391}\ (\bibinfo {organization} {SPIE},\ \bibinfo {year} {2020})\
  pp.\ \bibinfo {pages} {23--38}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Liao}\ \emph {et~al.}(2022)\citenamefont {Liao},
  \citenamefont {Kornblith}, \citenamefont {Ren}, \citenamefont {Fleet},\ and\
  \citenamefont {Hinton}}]{liao2022gaussian}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Liao}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Kornblith}},
  \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Ren}}, \bibinfo {author}
  {\bibfnamefont {D.~J.}\ \bibnamefont {Fleet}},\ and\ \bibinfo {author}
  {\bibfnamefont {G.}~\bibnamefont {Hinton}},\ }\bibfield  {title} {\bibinfo
  {title} {Gaussian-bernoulli rbms without tears},\ }\href@noop {} {\bibfield
  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2210.10318}\ } (\bibinfo
  {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Levin}\ and\ \citenamefont
  {Peres}(2017)}]{levin2017markov}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.~A.}\ \bibnamefont
  {Levin}}\ and\ \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Peres}},\
  }\href@noop {} {\emph {\bibinfo {title} {Markov chains and mixing times}}},\
  Vol.\ \bibinfo {volume} {107}\ (\bibinfo  {publisher} {American Mathematical
  Soc.},\ \bibinfo {year} {2017})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Br{\'e}laz}(1979)}]{brelaz1979new}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Br{\'e}laz}},\ }\bibfield  {title} {\bibinfo {title} {New methods to color
  the vertices of a graph},\ }\href@noop {} {\bibfield  {journal} {\bibinfo
  {journal} {Communications of the ACM}\ }\textbf {\bibinfo {volume} {22}},\
  \bibinfo {pages} {251} (\bibinfo {year} {1979})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {{Xilinx}}()}]{xilinx-u250}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibnamefont {{Xilinx}}},\
  }\href@noop {} {\bibinfo {title} {{U250 Data Sheet}}},\ \bibinfo
  {howpublished}
  {\url{https://www.xilinx.com/content/dam/xilinx/support/documents/data_sheets/ds962-u200-u250.pdf}}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {{D-Wave Systems Inc.}}(2021)}]{dwave-docs}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibnamefont {{D-Wave Systems
  Inc.}}},\ }\href
  {https://docs.ocean.dwavesys.com/en/stable/docs_dnx/reference/generators.html}
  {\bibinfo {title} {{D-Wave Ocean Documentation: DNX Generators}}} (\bibinfo
  {year} {2021}),\ \bibinfo {note} {[Online; accessed
  18-March-2023]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hinton}\ and\ \citenamefont
  {Salakhutdinov}(2014)}]{hinton2014training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Hinton}}\ and\ \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Salakhutdinov}},\ }\href@noop {} {\bibinfo {title} {Training a deep
  autoencoder or a classifier on mnist digits}} (\bibinfo {year}
  {2014})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Blackman}\ and\ \citenamefont
  {Vigna}(2021)}]{blackman2021scrambled}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Blackman}}\ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Vigna}},\ }\bibfield  {title} {\bibinfo {title} {Scrambled linear
  pseudorandom number generators},\ }\href@noop {} {\bibfield  {journal}
  {\bibinfo  {journal} {ACM Transactions on Mathematical Software (TOMS)}\
  }\textbf {\bibinfo {volume} {47}},\ \bibinfo {pages} {1} (\bibinfo {year}
  {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {airhdl.com}()}]{airHDL}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibnamefont {airhdl.com}},\
  }\href@noop {} {\bibinfo {title} {{airhdl VHDL/SystemVerilog Register
  Generator}}},\ \bibinfo {howpublished} {\url{https://airhdl.com}}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Block}\ \emph {et~al.}(2010)\citenamefont {Block},
  \citenamefont {Virnau},\ and\ \citenamefont {Preis}}]{block2010multi}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B.}~\bibnamefont
  {Block}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Virnau}},\ and\
  \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Preis}},\ }\bibfield
  {title} {\bibinfo {title} {Multi-gpu accelerated multi-spin monte carlo
  simulations of the 2d ising model},\ }\href@noop {} {\bibfield  {journal}
  {\bibinfo  {journal} {Computer Physics Communications}\ }\textbf {\bibinfo
  {volume} {181}},\ \bibinfo {pages} {1549} (\bibinfo {year}
  {2010})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Preis}\ \emph {et~al.}(2009)\citenamefont {Preis},
  \citenamefont {Virnau}, \citenamefont {Paul},\ and\ \citenamefont
  {Schneider}}]{preis2009gpu}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Preis}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Virnau}},
  \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Paul}},\ and\ \bibinfo
  {author} {\bibfnamefont {J.~J.}\ \bibnamefont {Schneider}},\ }\bibfield
  {title} {\bibinfo {title} {Gpu accelerated monte carlo simulation of the 2d
  and 3d ising model},\ }\href@noop {} {\bibfield  {journal} {\bibinfo
  {journal} {Journal of Computational Physics}\ }\textbf {\bibinfo {volume}
  {228}},\ \bibinfo {pages} {4468} (\bibinfo {year} {2009})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Yang}\ \emph {et~al.}(2019)\citenamefont {Yang},
  \citenamefont {Chen}, \citenamefont {Roumpos}, \citenamefont {Colby},\ and\
  \citenamefont {Anderson}}]{yang2019high}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont
  {Yang}}, \bibinfo {author} {\bibfnamefont {Y.-F.}\ \bibnamefont {Chen}},
  \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Roumpos}}, \bibinfo
  {author} {\bibfnamefont {C.}~\bibnamefont {Colby}},\ and\ \bibinfo {author}
  {\bibfnamefont {J.}~\bibnamefont {Anderson}},\ }\bibfield  {title} {\bibinfo
  {title} {High performance monte carlo simulation of ising model on tpu
  clusters},\ }in\ \href@noop {} {\emph {\bibinfo {booktitle} {Proceedings of
  the International Conference for High Performance Computing, Networking,
  Storage and Analysis}}}\ (\bibinfo {year} {2019})\ pp.\ \bibinfo {pages}
  {1--15}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Fang}\ \emph {et~al.}(2014)\citenamefont {Fang},
  \citenamefont {Feng}, \citenamefont {Tam}, \citenamefont {Yun}, \citenamefont
  {Moreno}, \citenamefont {Ramanujam},\ and\ \citenamefont
  {Jarrell}}]{fang2014parallel}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {Fang}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Feng}}, \bibinfo
  {author} {\bibfnamefont {K.-M.}\ \bibnamefont {Tam}}, \bibinfo {author}
  {\bibfnamefont {Z.}~\bibnamefont {Yun}}, \bibinfo {author} {\bibfnamefont
  {J.}~\bibnamefont {Moreno}}, \bibinfo {author} {\bibfnamefont
  {J.}~\bibnamefont {Ramanujam}},\ and\ \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Jarrell}},\ }\bibfield  {title} {\bibinfo {title}
  {Parallel tempering simulation of the three-dimensional edwards--anderson
  model with compact asynchronous multispin coding on gpu},\ }\href@noop {}
  {\bibfield  {journal} {\bibinfo  {journal} {Computer Physics Communications}\
  }\textbf {\bibinfo {volume} {185}},\ \bibinfo {pages} {2467} (\bibinfo {year}
  {2014})}\BibitemShut {NoStop}%
\end{thebibliography}%

\clearpage

\onecolumngrid
\section*{Supplementary Information}
\beginsupplement

\setcounter{subsection}{0}
\subsection{FPGA Implementation}
\label{sec:fpga}

We present the experimental results in the main paper using a hybrid classical-probabilistic computer. Here, we discuss the technical details of the FPGA-based p-computer implemented on a Xilinx Alveo U250 data center accelerator card. The basic architecture is presented in FIG.~\ref{fig:fpga_imp}. 

\subsubsection{p-bit and MAC Unit}
\label{pbit_MAC}
 A single p-bit consists of a tanh lookup table (LUT) for the activation function, a pseudorandom number generator (PRNG), and a comparator to implement  Eq.~\eqref{eq:pbit}. Digital input with a fixed point precision (10 bits with 1 sign, 6 integers, and 3 fraction bits) provides tunability through the activation function. The effect of different fixed point precisions has been explored in Supplementary Section~\ref{sec:bit_precision}. In this work, we used a high-quality 32-bit PRNG (xoshiro \cite{blackman2021scrambled}). There is also a multiplieraccumulator (MAC) unit to compute  Eq.~\eqref{eq:synapse} based on the neighbor p-bit states and provide input signal for the LUT as shown in FIG.~\ref{fig:fpga_imp}a. 

 \subsubsection{Bipolar to binary conversion}
\label{sec:bipolar_to_binary}
As described in Section~\ref{sec:hybrid} of the main article, Eq.~\eqref{eq:en}-\eqref{eq:pbit} are represented with the bipolar state of the variables. However, for the FPGA-based implementation of p-computer, using a binary state of variables is more convenient since digital CMOS operates with gnd (0) and VDD (1) and Boolean logic can naturally represent the p-bit state. Therefore, bipolar to binary conversion is done on the weights and biases before being sent to the FPGA, according to the following mapping:  
\begin{eqnarray}
{J}_{\text {binary }}&=&2 {J}_{\text {bipolar }}\\
{h}_{\text {binary }}&=&{h}_{\text {bipolar }}-{J}_{\text {bipolar }} \mathbf{1}
\end{eqnarray}
where, $\mathbf{1}$ is a vector of ones of size $N\times1$, and $N$ is the number of p-bits. All the variables in the MAC unit are also calculated in binary notations. To convert the stored activation function values of LUT from bipolar to binary representation, the $\mathrm {tanh(x)}$ is mapped to $ {[1 + \tanh{(x)}]/2}$. Finally, the output p-bit is represented with binary states $m_i \in \{0, 1\}$.


 \begin{figure*}[!ht]
    \centering
    \includegraphics[width =1 \textwidth ]{Figures/supp1.pdf}
    \caption{(a) The MAC (multiplieraccumulator) unit implements Eq.~\eqref{eq:synapse}. The p-bit unit consists of a xoshiro pseudorandom number generator (PRNG), a lookup table for the activation function (tanh), and a comparator to generate a binary output. (b) A built-in clocking unit generates equally phase-shifted and same-frequency parallel clocks to trigger the PRNGs inside the colored p-bit blocks. (c) A PCIe interfacing unit transfers data between MATLAB and the FPGA.}
\label{fig:fpga_imp}
\end{figure*}

\subsubsection{Clocking unit}
\label{sec:clock_unit}
In Section~\ref{sec:architecture} of the main paper, we discussed graph coloring to color p-bit blocks achieving massive parallelism, which we define as the  linear scaling of probabilistic flips/ns with respect to increasing graph size. To achieve this parallelism, we utilized multiple built-in clocks inside the FPGA board to drive the PRNGs within the p-bit blocks, as shown in FIG.~\ref{fig:fpga_imp}a. The mixed-mode clock manager (MMCM) block, available in the Xilinx Alveo U250 data center accelerator card, generates equally phase-shifted and same-frequency parallel stable clocks. The input to the MMCM is a 300 MHz differential pair system clock created on a low-voltage differential signaling (LVDS) clock-capable pin (FIG.~\ref{fig:fpga_imp}b). These clocks are highly precise with minimal noise or phase deviation. By triggering the colored p-bit blocks with these phase-shifted clocks, we achieve massive parallelism.

\subsubsection{Interfacing unit}
\label{sec:interface}
We use MATLAB as an Advanced eXtensible Interface (AXI) manager to communicate with the FPGA board through a PCI express interface (FIG.~\ref{fig:fpga_imp}c). We have designed an AXI manager integrated IP on the board that transfers data with a 32-bit memory-mapped slave register IP via the fourth-generation AXI (AXI4) protocol. An external website `airhdl' \cite{airHDL} is used to manage the memory mapping of the registers into the block rams (BRAMs) inside the FPGA. We used two BRAMs to write the weights and biases from MATLAB and another BRAM to save the p-bit states that MATLAB reads out.



\subsection{Effect of total parameters}
\label{sec:num_parameters}

To investigate the effect of total parameters of sparse DBMs on the accuracy, we used five Pegasus graphs of different sizes to train MNIST using our massively parallel architecture. These include 960, 1,664, 2,560, 3,080, and 4,264 p-bit graphs with a varying number of parameters from $\approx 6,000$ to $\approx 30,000$ as shown in FIG.~\ref{fig:acc_vs_param}a. We trained full MNIST on each of these five sparse DBMs with CD-{$10^5$} using the same hyperparameters as described in the main paper Section~\ref{sec:train} and reported the classification accuracy for the entire test set. In the smallest size graph when the number of parameters is only around 6,000, the test accuracy is less than 50\%. With larger graph sizes i.e., with an increasing number of parameters, the test accuracy starts to rise. We reached $\approx 90$\% accuracy with our largest size graph ($\approx30,000$) that fits in our architecture, using moderately sized FPGAs. Similarly, we also trained eight different RBMs with full MNIST for 100 epochs to compare their accuracy with the number of parameters (FIG.~\ref{fig:acc_vs_param}b). Increasing the number of parameters to millions could not increase the test accuracy significantly whereas 90\% accuracy is achieved with around 200,000 parameters.  

\begin{figure}[!ht]
    \centering
    \includegraphics[width =.8 \columnwidth ]{Figures/supp2.pdf}
    \caption {(a) Test accuracy of MNIST as a function of the number of parameters with sparse DBMs (Pegasus). We trained full MNIST with 5 different sizes of Pegasus graphs for 100 epochs using the same set of hyperparameters and collected the test accuracy of the whole test set. When the number of parameters is only 6,464 with the smaller Pegasus (960 p-bits), test accuracy could not reach beyond 50\%. On larger graphs with increased parameters, accuracy starts to increase and $\approx$ 90$\%$ accuracy is achieved with the largest Pegasus (4264 p-bits) that fits into our FPGA. (b) Test accuracy of entire MNIST with different sizes of RBMs for 100 epochs. RBM reached 90\% accuracy with around 200,000 parameters but the increased number of parameters 
    (up to 3.25 million) could not help go beyond $\approx 92\%$ accuracy.}
\label{fig:acc_vs_param}
   
\end{figure}

\subsection{Training accuracy of a subset from MNIST}
\label{sec:train_100im}
We have studied the effect of training the sparse DBMs (Pegasus 4,264 p-bits) with a small subset of data before training the full MNIST. In this setting, we chose 100 images from MNIST to train on the sparse network with our massively parallel architecture. To train these 100 images, we used 10 mini-batches, having 10 images in each batch and the same set of hyperparameters as in training full MNIST. The training accuracy reached 100\% within 1,000 epochs as illustrated in FIG.~\ref{fig:acc_100im}. We also explored different values of the `regularization' parameter ($\lambda$ = 0.005, 0.001) which is generally used to keep the weights from growing too much. In our case of sparse DBM, we did not observe any significant difference between a small regularization and without any regularization. 
The poor test accuracy here is an indication of overfitting due to the small size of the training set (only 100 images). We observed similar accuracy on Zephyr graphs with 3,360 p-bits for 100 images. 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width =.35 \textwidth ]{Figures/supp3.pdf}
    \caption {Accuracy of training 100 images with sparse DBMs up to 1,000 epochs. Training is accomplished with 10 mini-batches and CD-{$10^5$}. All the trained 100 images and 20 unseen images have been tested here.}
\label{fig:acc_100im}
\end{figure*}

\subsection{Effect of weight-precision}
\label{sec:bit_precision}
The results, we have in Section~\ref{sec:train} in the main paper utilized a weight precision of 10 bits (1 sign, 6 integer, and 3 fraction bits i.e., s[6][3]). Here, we explore different weight precisions by changing the fraction bit width and compare the results to identify the effect of weight precision on accuracy. We trained full MNIST with 1,200 mini-batches for 200 epochs, using three different weight precisions of s[6][2], s[6][3], and s[6][5]. We chose a Pegasus 2,560 p-bit graph as a sparse DBM for this experiment that fits into the FPGA since increasing bit precision reduces the available resources. The weight update is accomplished in MATLAB (double-precision floating point with 64 bits), but before the new weights are loaded to the FPGA, they are converted to the corresponding fixed-point precision. The choice of hyperparameters remains the same for all three cases. The test accuracy goes to $\approx88$\% in each case (with 17,984 parameters) and there is no remarkable difference among the accuracy of the different weight precisions (FIG.~\ref{fig:bit_precsn}a). We also trained full MNIST on RBM (512 hidden units) using both float64 and s[6][3] weight precision for 200 epochs. The test accuracy remains the same for these two different precisions as shown in FIG.~\ref{fig:bit_precsn}b.   


\begin{figure}[!ht]
    \centering
    \includegraphics[width =.7\columnwidth ]{Figures/supp4.pdf}
    \caption {(a) Test accuracy of full MNIST with sparse DBMs (Pegasus 2,560 p-bits) up to 200 epochs with three different fixed point precisions of weights (s[6][2], s[6][3] and[6][5]).(b) Test accuracy of full MNIST for RBM (512 hidden units) with double-precision floating point 64 bits and s[6][3].}
\label{fig:bit_precsn}
   
\end{figure}

\subsection{Mixing times in Pegasus graph (3,080 p-bits)}
\label{sec:mixing_P3080}
We described the mixing times in Section~\ref{sec:equilibrium} of the main paper showing the results (FIG.~\ref{fig:real_sweep}) from our largest size Pegasus (4,264 p-bits).  Here, we show another graph, Pegasus with 3,080 p-bits to measure the mixing time of the network. Unlike the main model, for this experiment, we trained full MNIST for only 50 epochs (instead of 100) using the same hyperparameters as mentioned in the main Section~\ref{sec:train} with different numbers of sweeps starting from CD-$10^{2}$ to CD-$10^{6}$. Test accuracy improves significantly when we take more than CD-$10^{4}$ per epoch.

\begin{figure}[!ht]
    \centering
    \includegraphics[width =.35 \columnwidth ]{Figures/supp5.pdf}
    \caption {Test accuracy after training full MNIST up to 50 epochs with different numbers of sweeps using sparse DBMS (Pegasus 3,080 p-bits). For our sparse graph, to mix the Markov chain properly we need minimum CD-$10^{4}$. Reducing the number of sweeps significantly degrades the quality of mixing in the chain.}
\label{fig:sweep_P3080}
   
\end{figure}



\subsection{Image generation with Zephyr}
\label{sec:ImSynth_zephyr}
In the main paper, FIG.~\ref{fig: overview}f displays the images generated with Pegasus (4,264 p-bits) graph and the procedure is described in Section~\ref{sec:ImageSynth}. Here we explored image generation with a different type of sparse DBM, Zephyr (3,360 p-bits) that also reached $\approx90$\% accuracy with randomized indices as demonstrated in the main FIG.~\ref{fig:rand_label}c (bottom). The generated images with Zephyr as shown in FIG.~\ref{fig:zephyr}  are slightly different from the Pegasus ones.


\begin{figure*}[!ht]
    \centering
    \includegraphics[width =.63 \textwidth ]{Figures/supp6.pdf}
    \caption {Image generation examples with sparse DBM Zephyr (3,360 p-bits) after training the network with the full MNIST dataset.}
\label{fig:zephyr}
\end{figure*}




\subsection{Momentum to the learning rule}
\label{sec:momentum}

In our training, we used the momentum in our update rules, which are empirically added to the learning rule we discuss in the next section. By retaining a portion of the last update to the weights, momentum  helps increase the effective learning rate \cite{hinton2012practical}. The effective increase in the learning rate is equivalent to multiplying it by a factor of 1/(1-$\alpha$) where $\alpha$ is denoted as momentum. Using this process, the algorithm can increase the effective learning rate without causing unstable oscillations, which ultimately speeds up the convergence of the training process \cite{tieleman2008training}. We modify the learning rule equations in the main Eq.~(\ref{eq:del_J}) and Eq.~(\ref{eq:del_h})  by introducing the momentum term as follows:
\begin{eqnarray}
   \Delta J_{ij}(n)& =& \varepsilon\bigg(\langle m_im_j\rangle_\text{{data}}-\langle m_im_j\rangle_\text{{model}}\bigg) + \alpha \Delta J_{ij}(n-1) \label{eq:momentum_J}\\
   \Delta h_{i}(n) &=& \varepsilon\bigg(\langle m_i\rangle_\text{{data}}-\langle m_i\rangle_\text{{model}}\bigg) + \alpha \Delta h_{i}(n-1)
   \label{eq:momentum_h}
\end{eqnarray}
where $n$ represents the $j$\textsuperscript{th} index (ranging from 1 to the number of batches) in Algorithm 1 in the main paper. 

\subsection{Maximum Likelihood for Boltzmann Networks}
\label{sec:derivation}
The basic idea of Boltzmann networks is to start from a physics-inspired variational guess, that the data distribution will be approximated by a model whose probability $p^M$ for a given input vector $m^{(i)}$ ($i$, being the input index) obeys the Boltzmann law (ignoring biases in our derivation for simplicity): 
\begin{equation}
    p^M \left(m^{(i)}\right) =\frac{1}{Z} \exp \left[\sum_{e r} J_{e r} m_e^{(i)} m_r^{(i)}\right]
    \label{eq:bm}
\end{equation}

 

In our setting, we have a system of $N$ neurons (p-bits) connected in some arbitrary graph topology. For this purpose we may assume a DBM, having $M$ visible neurons where $M<N$. The problem is learning a  ``truth table'' with exactly $N_T$ lines of inputs in it. The model is going to try to select these $N_T$ states in the space of $2^M$ possible discrete probabilities. Like in any other ML model, fitting every line of the truth table exactly will overfit, but the Boltzmann formulation given by Eq.~(\ref{eq:bm})  smooths out the sum of ``delta function''-like data vectors in the $2^M$ space, which can later be used for generating new samples.  

We define a $p^V$ as the probability distribution of the data, corresponding to the visible bits. Then, a maximum likelihood estimation minimizing the KullbackLeibler divergence between the data and the model can be used to derive the learning rule, by taking the negative derivative of $KL(p^V \| \ p^M)$:

\begin{equation}
    KL(p^V \| \ p^M)=\sum_{i=1}^{N_T } p_{i}^{V}\log\bigg[\frac{p_{i}^{V}}{p_{i}^{M}}\bigg] \label{kl_eqn}
\end{equation}
where $i$ is the index of truth table lines $N_T$. It is important to note that even though internal hidden nodes depend on $J_{mn}$, $p_{i}^{V}$ is independent of $J_{mn}$ for any network topology,  since $p_{i}^{V}$ represents the data distribution. 


\begin{equation}
\frac{\partial K L\left(p^V \| \ p^M\right)}{\partial J_{m n}}=\frac{\partial p_i^V}{\partial J_{m n}} \log \left[\frac{p_i^V}{p_i^M}\right]=\sum_{i=1}^{N T} \frac{p_i^V}{p_i^M}\left(\frac{-\partial p_i^M}{\partial J_{m n}}\right) \label{kl_partial}
\end{equation}
\begin{equation}
\frac{\partial p_i^M}{\partial J_{m n}}=\frac{\partial}{\partial J_{m n}}\left[\frac{1}{Z} \exp \left[\sum_{e r} J_{e r} m_e^{(i)} m_r^{(i)}\right]\right] \label{piM_partial}
\end{equation}

 \begin{equation}
Z=\sum_{j=1}^{2^N} \exp \left[\sum_{e r} J_{e r} m_e^{(j)} m_r^{(j)}\right] \label{Z_eqn}
\end{equation}

where the index $j$ represents all possible states from 1 to $2^N$ for the model.
\begin{equation}
\begin{aligned}
\frac{\partial p_i^M}{\partial J_{m n}} & =\frac{-1}{Z^2} \cdot \frac{\partial Z}{\partial J_{m n}} \cdot\left[\exp \sum_{e r} J_{e r} m_e^{(i)} m_r^{(i)}\right]+\frac{1}{Z}\cdot \frac{\partial}{\partial J_{m n}} \left[\exp \sum_{e r} J_{e r} m_e^{(i)} m_r^{(i)}\right] \\
& =\frac{-\partial Z}{\partial J_{m n}} \cdot \frac{1}{Z} \cdot p_i^{M}+m_m^{(i)} \cdot m_n^{(i)} \cdot p_i^M \\
& =-\sum_{j=0}^{2^N-1} m_m^{(j)} m_n^{(j)} \cdot p_j^M \cdot p_i^M+m_m^{(i)} m_n^{(i)} \cdot p_i^M
\end{aligned} \label{final_piM}
\end{equation}
\begin{equation}
\begin{aligned}
-\frac{\partial K L\left(p^V \| \ p^M\right)}{\partial J_{m n}}&=\sum_{i=1}^{N_T} \frac{p_i^V}{p_i^M} \cdot \frac{\partial p_i^M}{\partial J_{m n}} \\
& =\sum_{i=1}^{N_T} \frac{p_i^V}{p_i{ }^M}\left[m_m^{(i)} \cdot m_n^{(i)} \cdot p_i^M-\sum_{j=0}^{2^N-1} m_m^{(j)} \cdot m_n^{(j)} \cdot p_j^M \cdot p_i^M\right] \\
& =\left[\sum_{i=1}^{N_T} p_i^V \cdot m_m^{(i)} m_n^{(i)}-\sum_{i=1}^{N_T} p_i^V \sum_{j=0}^{2^N-1} m_m^{(j)} \cdot m_n^{(j)} \cdot p_j^M\right] \\
& =\left[\left\langle m_m m_n\right\rangle_{\text {data }}-\left\langle m_m m_n\right\rangle_{\text {model }}\right] \\
&
\end{aligned}
\end{equation} 
which gives the familiar learning rule. A similar learning rule in terms of the averages can be derived by accounting for the biases in the energy, which we ignored for simplicity. 



\subsection{Comparison with state-of-the-art GPUs and TPUs }
\label{sec:gpu} 
In the main paper, we show how graph-colored FPGA achieves massive parallelism to provide few orders of magnitude faster sampling throughput than traditional CPUs. Here in Supplementary Table~\ref{tab:benchmarking}, we also compare the sampling speed to some state-of-the-art (SOTA) Ising machines implemented on the latest GPUs and TPUs. The throughput reported in this work up to 64 billion flips per second outperforms the numbers reported by the SOTA Ising solvers in GPUs and TPUs. It is also important that this comparison is not completely accurate and favors the GPU/TPU implementations for two reasons: First, all the GPUs and TPUs discussed here are solving simple, nearest-neighbor chessboard lattices, unlike the irregular and relatively high-degree (with up to 20 neighbors) graphs used in this work. Second, GPU/TPU implementations generally use low precision \{+1,-1\} weights (compared to 10 bits of weight precision in our work) and thus can explore only a few discrete energy levels. Both of these features are heavily exploited in reporting a large degree of flips/ns in these solvers and their performance would presumably be much worse if they are implementing the same graphs with the same precision we discuss in this work. 


\begin{table*}[!ht]
    \centering
     \caption{{\footnotesize Optimized GPU and TPU implementations of Markov Chain Monte Carlo sampling with regular chessboard lattices. It is important to note that these TPU and GPU implementations solve Ising problems in sparse graphs, however, their graph degrees are usually restricted to 4 or 6, unlike more irregular and higher degree graphs.}}
     \vspace{4pt}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        {\bf Sampling method} & {\bf topology} & {\bf max. degree} & {\bf flips/ns} \\ \midrule
        
        Nvidia Tesla C1060 GPU \cite{block2010multi, preis2009gpu}  & Chessboard  & 4 & 7.98 \\
        Nvidia Tesla V100 GPU \cite{yang2019high}  & Chessboard  & 4 & 11.37 \\
        Google TPU \cite{yang2019high} & Chessboard  & 4 & 12.88\\
        Nvidia Fermi GPU \cite{fang2014parallel} & Chessboard   & 4 &  29.85\\
        \bottomrule
         \end{tabular}
   \label{tab:benchmarking}
\end{table*}
\subsection{Full graph topologies: Pegasus 4,264}
\label{sec:actual}
Below we show the full Pegasus network topology with 4,264 p-bits and its sparse deep BM representation. 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/supp8.pdf}
    \caption {{\footnotesize Layered embedding of the 4,264 p-bit Pegasus graph of FIG.~\ref{fig:P14}, illustrating the sparse DBM architecture: first layer is visible p-bits with 834 nodes, second and third layers are the hidden p-bits with 3,226 and 204 nodes respectively. There are also some intralayer connections within each layer. An example is shown in the right circle which shows the neighboring connections around node 3,443. The number next to a line represents the number of wires grouped in that branch, the total number being the fan-out of a given p-bit (vertex).}}
\label{fig:P14_layered}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width =0.95\textwidth ]{Figures/supp7.pdf}
    \caption {{\footnotesize The original sparse DBM network (Pegasus: 4,264 p-bits) used in this work with marked-up visible (blue), hidden (orange), and label (yellow) units.}}
\label{fig:P14}
\end{figure*}

\end{document}

