{
    "arxiv_id": "2303.12249",
    "paper_title": "State-of-the-art optical-based physical adversarial attacks for deep learning computer vision systems",
    "authors": [
        "Junbin Fang",
        "You Jiang",
        "Canjian Jiang",
        "Zoe L. Jiang",
        "Siu-Ming Yiu",
        "Chuanyi Liu"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2024-10-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.CR",
        "eess.IV"
    ],
    "abstract": "Adversarial attacks can mislead deep learning models to make false predictions by implanting small perturbations to the original input that are imperceptible to the human eye, which poses a huge security threat to the computer vision systems based on deep learning. Physical adversarial attacks, which is more realistic, as the perturbation is introduced to the input before it is being captured and converted to a binary image inside the vision system, when compared to digital adversarial attacks. In this paper, we focus on physical adversarial attacks and further classify them into invasive and non-invasive. Optical-based physical adversarial attack techniques (e.g. using light irradiation) belong to the non-invasive category. As the perturbations can be easily ignored by humans as the perturbations are very similar to the effects generated by a natural environment in the real world. They are highly invisibility and executable and can pose a significant or even lethal threats to real systems. This paper focuses on optical-based physical adversarial attack techniques for computer vision systems, with emphasis on the introduction and discussion of optical-based physical adversarial attack techniques.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12249v1"
    ],
    "publication_venue": "This work has been submitted to the IEEE for possible publication"
}