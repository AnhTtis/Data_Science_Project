\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{amsfonts}
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% \newcommand\edit{\textcolor{black}}
% \newcommand\question{\textcolor{red}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{iccv}
% \usepackage{float}
% \usepackage{subfig}
\usepackage[caption = false]{subfig}
% \usepackage[final]{graphicx}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}
% \iccvfinalcopy
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{12333} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi
\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

\begin{document}

%%%%%%%%% TITLE
\title{$R^2$: Range Regularization for Model Compression and Quantization}

\author{Arnav Kundu$^*$
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Chungkuk Yoo$^*$
\and
Srijan Mishra$^*$
\and
Minsik Cho
\and
Saurabh Adya\\
Apple Inc.\\
\{a\_kundu, ckyoo, srijan, minsik, sadya\}@apple.com
}

\maketitle
\vspace{0.3in}
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi
%%%%%%%%% ABSTRACT
\begin{abstract}Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization ($R^2$) to further boost the quality of model optimization by focusing on the outlier prevention.
By effectively regulating the minimum and maximum weight values from a distribution, we mold the overall distribution into a tight shape so that model compression and quantization techniques can better utilize their limited numeric representation powers. We introduce $L_\infty$ regularization, its extension margin regularization and a new soft-min-max regularization to be used as a regularization loss during full-precision model training. Coupled with state-of-the-art quantization and compression techniques, models trained with $R^2$ perform better on an average, specifically at lower bit weights with 16x compression ratio. We also demonstrate that $R^2$ helps parameter constrained models like MobileNetV1 achieve significant improvement of around 8\% for 2 bit quantization and 7\% for 1 bit compression.
\end{abstract}\\
\keywords{Regularization, Quantization, Compression, Outlier prevention}

\input{sections/1_introduction.tex}
\input{sections/2_range_loss.tex}
\input{sections/3_experiment.tex}
\input{sections/4_related_work.tex}
\input{sections/5_conclusion.tex}
{\small
\bibliographystyle{ieee_fullname}
\bibliography{refs}
}

\end{document}
