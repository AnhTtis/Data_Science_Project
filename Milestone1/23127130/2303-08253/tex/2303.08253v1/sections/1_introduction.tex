\section{Introduction}
Deep neural networks have become popular in human computer interaction, anomaly detection, financial markets, etc. Since a lot of applications running these models run on edge devices, running these compute-intensive models requires a balance of accuracy, latency, power efficiency and size for these models. Quantization of neural networks is an effective way of reducing the power, latency, and size of these models. This requires that these quantized models are executed on
specialized platforms with low-bit supports and at the cost of accuracy drop \cite{hawq,haq}. The accuracy degradation can be more significant with post-training quantization on weights for large models \cite{bengio2013estimating}. Therefore, training time quantization techniques are preferred over post-training approaches~\cite{bengio2013estimating,zhou2016dorefa}, which allow usage of stochastic gradient descent to quantize and optimize the weights (i.e., mapping each floating point number to a set of discrete bins according to the precision target). Since these bits are decided very simply on the basis of the range of the weights, weight range ends up affecting the performance of quantization itself. 

% and activations


% while taking a minimal loss in accuracy by running them on specialized hardware that is optimized for low numerical precision operations. However, applying post training quantization on weights 


% and activations causes a massive regression in accuracy for large models. 

\begin{figure}[t]
\vskip 0.0in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/exmp_weight_dist_r2.pdf}}
\vskip -0.0in
\caption{Example weight distribution for weight quantization with three bins. Black: without a range regularizer and \textcolor{red}{Red}: with a range regularizer.}
\label{fig_weight_dist}
\end{center}
\vskip -0.0in
\end{figure}

Setting an effective bin size for quantization becomes a hard problem due to outliers in weights. For example, in \cref{fig_weight_dist}, if we set the bin size as entire weight range divided by a desired number of bins, most of weights fall into the zero bin and only few weights are quantized to -1 and 1. As most of weights in the quantized model are zero, its accuracy would drop significantly. This problem gets worse for low bit quantization and compression such as one or two bit quantization. In state-of-the-art quantization works, statistical approaches are used to initialize a bin size during quantization-aware training. They clip outlier weights if they are too small or too large assuming the weight distribution would follow the normal distribution ~\cite{esser2019learned, lee2021network}. But, the model weights can have a long-tail distribution so that this statistical initialization would not be effective. Also the clip function is not differentiable, so most of quantization works approximate its gradient as 1 ~\cite{bengio2013estimating}. However, this approximation makes quantization unstable with gradient explosion ~\cite{sakr2022optimal}. 

Model compression using techniques like Deep Compression \cite{HanMD15} and DKM \cite{cho2021dkm} can also get compromised in accuracy due to outliers in weights. This is because the centroids computed in both these methods by the clustering algorithm will get skewed due to the presence of the outliers. 

We hypothesize that pre-trained models with fewer outliers in weights will yield better quantized or compressed models. Therefore, in this paper, we propose:
\begin{itemize}
\item
{Range-regularization ($R^2$), a concept to ensure that the learnt layer weights do not have any outliers during the pre-training stage (i.e, better weight distribution), which allows model compression and quantization techniques to effectively optimize the pre-trained network.}
\item
{3 different techniques to perform range regularization and show the characteristics of weight distributions generated by each of them}.
\item
{We use $R^2$ for training ResNet-18 \cite{he2016deep}, MobileNetV1 \cite{howard2017mobilenets}, and MobileNetV2 \cite{sandler2018mobilenetv2} and then use the trained model to improve the accuracy of the quantized (2bit, 4 bit) and compressed models (1bit, 2bit, 4bit) with state of the art quantization and compression algorithms. \textbf{We quantize/compress the entire model including the first and last layer.}}
\item
{We use $R^2$ for training ResNet-18 and MobileNetV1 and then use the trained model to improve the accuracy of multi-dimensional compressed (2bit-2dim, 4bit-4dim) models.}
\item
{We also apply $R^2$ on MobileBERT \cite{sun2020mobilebert} for Question-answering NLI \cite{rajpurkar2016squad} to show that the proposed method scales across tasks.}
\end{itemize}
% During the pre-training process, $R^2$ only affects the outliers in weights by penalizing them while maintaining accuracy of the full precision model. This property differentiates range regularization from $L_1$ or $L_2$ regularization. 


% In this work, we propose two new range regularization techniques,  margin and soft-min-max regularizers, and compare them with $L_\infty$ regularization.  
% $L_\infty$ regularizes the highest magnitude outlier of weights in a layer during a weight update iteration. As an extension of $L_\infty$, Margin range-regularizer penalizes weights less than or greater than thresholds. These methods ensure a symmetric distribution of weights. Soft-min-max regularization, on the other hand addresses the range of weights directly by minimizing the delta between maximum and minimum weight of a given layer. Therefore, this regularization provides a flexibility of having skewed weight distributions which might help in compression.


