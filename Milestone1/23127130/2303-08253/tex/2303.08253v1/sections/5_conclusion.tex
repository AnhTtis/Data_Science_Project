\section{Conclusion}
In this paper, we introduced range regularization as an effective technique to get rid of outliers in the weight distribution during training. This serves as a good initialization for state of the art quantization aware training and compression strategies, therefore can be coupled with any of them. This helps to augment the accuracy gained from such quantization aware training and compression techniques. With the three proposed formulations of range-regularization we wanted to demonstrate that there can multiple approaches of defining $R^2$ and the final metric depends on these formulations. We also demonstrated how the proposed method converts a normally distributed weight to a more desired uniform distribution for model quantization. Finally, we establish that range regularization doesn't affect the performance of the floating point models as well and in some over parameterized models it also has a regularization effect to address over-fitting. While being beneficial to model compression and quantization range regularization is sensitive to parameters like range regularization weight, selection of initial margin width for margin range regularization and selection of $\alpha$ for soft-min-max regularization. We also observe that for some quantization techniques like EWGS the incremental benefit of applying range regularization is marginal especially at higher bits.