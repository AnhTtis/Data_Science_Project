\section{Range Regularization}
% Quantization of weights of neural networks is an essential tool for making them memory efficient and running on low resource hardware. Quantization tries to distribute the available bit width uniformly across the range of weights of a given layer. If this range is large then the precision per weight is compromised therefore increasing the quantization accuracy loss. We propose range-regularization; a technique to reduce the range of weights for every layer \edit{to ensure better resolution per bit in quantized weights}. We demonstrate three techniques to introduce range regularization with minimal intrusion to the original training flow.
We introduce $R^2$ as a regularization technique to reduce the range of weights for every layer. Our technique keeps the quantization/compression process simple without any intrusion to the actual training flow just like $L_1$ and $L_2$ regularization. Contrary to $L_1$ or $L_2$ regularization, $R^2$ only affects the outliers in weights by penalizing them while maintaining accuracy of the full precision model. While, such a constrained optimization can be formulated in various ways we propose 3 different ways of defining $R^2$. We start from basic $L_\infty$ loss, extend it to margin loss and finally introduce soft-min-max loss for adding $R^2$ to the training loss. These formulations are further elaborated as follows:


% In this work, we propose two new range regularization techniques,  margin and soft-min-max regularizers, and compare them with $L_\infty$ regularization.  
% $L_\infty$ regularizes the highest magnitude outlier of weights in a layer during a weight update iteration. As an extension of $L_\infty$, Margin range-regularizer penalizes weights less than or greater than thresholds. These methods ensure a symmetric distribution of weights. Soft-min-max regularization, on the other hand addresses the range of weights directly by minimizing the delta between maximum and minimum weight of a given layer. Therefore, this regularization provides a flexibility of having skewed weight distributions which might help in compression.
\begin{itemize}
    \item $L_\infty$ regularization: This method tries to penalize only the outliers in an iterative manner during training by adding $L_{\infty}(W)$ as a regularization term for every layer in the model.
    \begin{equation}
         L_{reg}=\sum L_{\infty}(W)
    \end{equation}
    \item Margin range-regularization: This is an extension of $L_{\infty}(W)$ regularization, where, we define a margin for the range of allowed weights. Any weight outside this margin is penalized. In addition, we also penalize the width of the margin to ensure that the range of the overall weight distribution is small. The regularization loss for a given weight W is shown in \cref{eq:margin}. Here $M$ is a learnable parameter per layer.
    \begin{equation}
        L_{reg}=\sum (|M| + \max(|W|-|M|,0))
        \label{eq:margin}
    \end{equation}
    \item Soft-min-max range-regularization: In this approach, we propose an asymmetric range regularization, to eliminate the constraint on the magnitude of weights and strictly enforce it on the range of weights. We hypothesize that such a technique will improve asymmetrically quantized models using techniques such as DKM \cite{cho2021dkm}. The loss for a given weight W is described in \cref{eq:sminmax}. Here temperature $\alpha$ is a learnable parameter per layer. $e^{-\alpha}$ term in the regularization loss $L_{reg}$, encourages temperature $\alpha$ to increase during training time optimization process to approach hard-min-max regularization towards the end of train time optimization.
    \begin{equation}
    \begin{split}
        s_{max} &= \frac{\Sigma (W \odot e^{\alpha \times (W-W_{max})})}{\Sigma e^{\alpha \times (W-W_{max})}} \\
        s_{min} &= \frac{\Sigma (W \odot e^{-\alpha \times (W-W_{min})})}{\Sigma e^{-\alpha \times (W-W_{min})}} \\
        L_{reg} &= (s_{max}-s_{min}) + e^{-\alpha}
    \end{split}
    \label{eq:sminmax}
    \end{equation}
\end{itemize}


All the above mentioned range-regularization techniques were employed during training time of the base model itself and not during quantization or compression. This was done because the purpose of the $R^2$ is to provide effective initial weights for compression or quantization. Range-regularization was added to the cross entropy loss by multiplying a regularization weight to adjust importance between the regularization and the loss during the base training phase. This regularization weight has an initial value which differs across the three regularizers. Employing range-regularization during base model training and not during quantization or compression also ensures extensibility of range-regularization with any quantization or compression technique. 