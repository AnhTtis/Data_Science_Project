\section{Experiment}
\label{experiment}
\begin{table*}[h]
\caption{Top-1 accuracies (\%) of ResNet18 (RN), MobileNet-V1 (MN1) on ImageNet-1K with weight and activation quantization.}
\label{table_quant_exp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
 % & \multicolumn{4}{c|}{Compression} & \multicolumn{4}{c}{Quantization}\\
Pre-train (FP32) & \multicolumn{2}{c}{RN} & \multicolumn{2}{c}{MN1} & \multicolumn{2}{c}{MN2} \\
\midrule
\circled{0} w/o $R^2$    & \multicolumn{2}{c}{69.76} & \multicolumn{2}{c}{70.92} & \multicolumn{2}{c}{71.88}\\
\circled{1} R\_Linf    & \multicolumn{2}{c}{70.15} & \multicolumn{2}{c}{70.59} & \multicolumn{2}{c}{70.75}\\
\circled{2} R\_M    & \multicolumn{2}{c}{70.08} & \multicolumn{2}{c}{70.28} & \multicolumn{2}{c}{70.94}\\
\circled{3} R\_SMM    & \multicolumn{2}{c}{69.84} & \multicolumn{2}{c}{70.34}  & \multicolumn{2}{c}{70.75}\\
\midrule
Quant Method & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit\\
\midrule
PACT    & 51.97 & 66.90 & 22.39 & 64.64 & 21.02 & 65.23\\
PACT + R\_Linf    & 55.26 & \textbf{68.45} & 35.99 & \textbf{68.40} & 36.55 & 67.94\\
PACT + R\_M    & \textbf{56.24} & 68.30 & \textbf{40.41} & \textbf{68.40} & \textbf{37.86} & \textbf{68.12}\\
PACT + R\_SMM    & 55.64 & 68.36 & 33.26 & 68.32 & 35.92 & 67.98\\
\midrule
LSQ from \circled{0} & 58.33 & \textbf{69.90} & 38.78 & 69.00 & 33.78 & \textbf{69.00}\\
LSQ from \circled{1} & 62.23 & 69.55 & 43.05 & 69.28 & 39.05 & 68.85\\
LSQ from \circled{2} & 62.25 & 69.56 & 40.85 & \textbf{69.64}  & \textbf{39.30} & 68.46\\
LSQ from \circled{3} & \textbf{62.47} & 69.45 & \textbf{46.72} & 69.37 & 38.04 & 68.98\\
\midrule
EWGS from \circled{0} & 65.42 & \textbf{70.19} & 59.81 & 69.61 & 55.69 & \textbf{70.21}\\
EWGS from \circled{1} & \textbf{65.72} & 70.17 & 59.43 & 69.32  & 56.46 & 69.73\\
EWGS from \circled{2} & 64.27 & 69.77 & 60.18 & 69.63  & \textbf{57.39} & 69.88\\
EWGS from \circled{3} & 64.94 & 69.80 & \textbf{60.63} & \textbf{69.79} & 56.65 & 69.54\\
% EWGS    & 61.9 & 83.8 & $\surd$ \\
% DKM    & 74.8 & 78.3 &         \\
% \midrule
% PACT + R\_SMM    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_SMM & 83.3 & 80.0 & $\times$\\
% EWGS + R\_SMM    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_SMM    & 74.8 & 78.3 &         \\
% PACT + R\_MRGN    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_MRGN & 83.3 & 80.0 & $\times$\\
% EWGS + R\_MRGN    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_MRGN    & 74.8 & 78.3 &         \\
% PACT + R\_Linf    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_Linf & 83.3 & 80.0 & $\times$\\
% EWGS + R\_Linf    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_Linf    & 74.8 & 78.3 &         \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Experiment settings}
\subsubsection{Pre-training from scratch with Range Regularizers}
We train ResNet-18\cite{he2016deep} and MobileNet-V1\cite{howard2017mobilenets} on ImageNet 1K\cite{deng2009imagenet} with proposed Range Regularizers on two x86 Linux machines with eight GPUs to get pre-trained models before model compression and quantization-aware training. We set initial learning rates to 1.0, 0.4 and 0.2 for ResNet-18, MobileNet-V1 and MobileNet-V2 respectively. We use SGD with 0.9 of momentum and 1e-4 of weight decay with Nesterov. Range Regularizer weights for each regularizers are set to 0.01. For Margin range regularizer, the margin threshold is initialized with 2x the standard deviation . In Soft-min-max regularizer training, the learnable parameter $\alpha$ is initially set to 0.1. For comparison, we use pre-trained models of Resnet-18 from Torchvision. As Torchvision did not provide a pre-trained model for MobileNet-V1, we trained MobileNet-V1 without Range Regularizer using the same settings above. It can be observed from Table \ref{table_quant_exp} that $R^2$ doesnt significantly affect the performance of the full precision models as well therefore provides a strong initial checkpoint for the model to be quantized. In addition, for a large model like ResNet-18 $R^2$ also shows a regularization effect similar to weight decay, therefore, better validation accuracy than the one without $R^2$.
\subsubsection{Model compression and quantization}
Range Regularizer is not a model compression nor quantization method. It regularizes weights during training of the base model from scratch. To evaluate the effectiveness of the regularizers with model compression and quantization, we apply state-of-the-art compression/quantization techniques, DKM\cite{cho2021dkm}, LSQ\cite{esser2019learned}, and EWGS\cite{lee2021network}, to the pre-trained model with Range Regularizer. 
Except EWGS, since other works do not provide official implementation, we had to implement those techniques ourselves. We follow the same hyper-parameters used in the works, but \textbf{we apply compression and quantization for all layers (and activations in case of quantization) including the first and last layers.} It is important to compress/quantize all layers including first and last layers considering computation burden at the first layer with a large convolutional filter size such as 7x7 convolutional kernal size of the first layer in ResNet and the large number of weights in the last linear layer, e.g., 1.2M of weights in the last layer of MobileNet-V1 which has 4.2M of weights in total. We have demonstrated this burden in Table \ref{table_model_size} for more clarity. We represent $L_\infty$ range-regularization as $R\_Linf$, Margin range-regularization as $R\_M$ and Soft-min-max range-regularization as $R\_Smm$ in the results.
\begin{table}[t]
\caption{Model size of compressed MobileNet-V1 (in M bytes). F: first layer, L: last layer} % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_model_size}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \begin{tabular}{lcccc}
% \toprule
% bit & w/o F \& L & w/o F & w/o L & Quant all \\
% \midrule
% 32bit &   &  &  & 16.1 \\
% 4bit & 6.0  & 1.7 & 5.9 & 1.7 \\
% 2bit & 5.4  & 0.8 & 5.4 & 0.8 \\
% 1bit & 5.2  & 0.4 & 5.2 & 0.4 \\


\begin{tabular}{lcc}
\toprule
bit & Quant excl. F \& L & Quant all \\
\midrule
32bit & 16.1   & 16.1 \\
\midrule
4bit & 7.1   & 4.2 \\
2bit & 5.6   & 2.2 \\
1bit & 4.8   & 1.2 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\subsection{Model quantization}
We apply state-of-the-art quantization techniques like PACT \cite{choi2018pact} while training the models from scratch using $R^2$. For other quantization aware training methods like, EWGS \cite{lee2021network} and LSQ \cite{esser2019learned} we initialize the model to pre-trained ResNet-18 (RN) \cite{he2016deep}, MobileNet-V1 (MN1) \cite{howard2017mobilenets} and MobileNet-V2 (MN2) \cite{sandler2018mobilenetv2} with range-regularization. As shown in \cref{table_quant_exp}, range-regularization helps the quantization techniques in improving their accuracy, especially for extremely low bit quantization such as at 2 bit. For all models and all quantization methods, models initialized with range-regularization show considerably higher accuracy than original quantization works for 2 bit quantization such as 58.33\% vs. 62.47\%, 38.78\% vs. 46.72, and 33.78\% vs. 39.30\% for ResNet-18, MobileNet-V1, and MobileNet-V2, respectively.


\subsection{Model compression}
We evaluate the effectiveness of range-regularization for compression with the state-of-the-art compression technique, DKM \cite{cho2021dkm}, for ResNet-18 and MobileNet-V1. The bit-dim ratio, $\frac{b}{d}$ is an important factor in the DKM algorithm which effectively defines the kind of compression a DKM palettized model would see. We ran these experiments for both scalar and vector palettization. For scalar palettization($dim=1$) we ran 1 bit, 2 bit and 4 bit compression. These experiments would yield roughly 32x, 16x and 8x compressed models respectively. \cref{table_comp_exp} shows that range-regularization significantly improves accuracy from original scalar palettized DKM 1 bit, 2 bit and 4 bit models. We also expand the application of range-regularization to vector palettization($dim>1$) DKM \cite{cho2021dkm} as demonstrated in \cref{table_comp_exp_multi}. For these experiments, we kept the effective bit-dim ratio, $\frac{b}{d}$ equivalent to 1 so as to see variation across the most heavily compressed model which would be close to 32x compressed. Since a vector palettized model will require range constraining for all dimensions, we applied multi-dimensional range-regularization for all layers that would be palettized during compression. For vector palettized ResNet-18 there is an average absolute improvement of $>1\%$ using models pre-trained with range-regularization, and for vector palettized MobileNet-V1 it the gain ranges from 5\%(2 bit) to 3\% (4 bit). 

Finally we also validated that $R^2$ scales to other domains as well by applying it in compressing MobileBERT \cite{sun2020mobilebert}. For Question Answering (QNLI) \cite{rajpurkar2016squad} using MobileBERT, the proposed regularization improved the performance of the model by 2\% absolute as demonstrated in Table \ref{exp_mobilebert}. Note that we applied $R^2$ to a QNLI fine-tuning task based on a pre-trained MobileBERT \cite{wolf2020transformers}. It might be necessary to apply $R^2$ to the entire training task of MobileBERT from scratch so that $R^2$ would have more chances to get effective weight distribution for model compression. Through these experiments across a variety of tasks(Image Classification, Question Answering etc.) we can also see that the application of the proposed range-regularizers is task-invariant and yields solid results across domains.

\begin{table}[t]
\caption{Top-1 accuracies(\%) of compression using DKM and range-regularization with ResNet18 (RN), MobileNet-V1 (MN1) on ImageNet.} % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_comp_exp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llcccc}
\toprule
Method & $R^{2}$ & RN & MN1\\
% Method & $R^{2}$ & RN & MN1 & MN2 & MB \\
\midrule
32bit &   & 69.76 & 70.92\\
% 32bit &   & 69.76 & 70.92 & 71.88 & 90.41\\

\midrule
% \multirow{4}{*}{DKM 1-bit}    & None        & 58.97 & 45.54 & & 61.34       \\
                        % & R\_Linf    & 59.52 & 49.74 & & 63.17 \\
\multirow{4}{*}{DKM 1-bit, 1-dim}    & None        & 58.97 & 45.54\\
                        & R\_Linf    & 59.52 & 49.74 \\
                        & R\_M    &  \textbf{59.70} & 47.21\\
                        & R\_SMM    & 59.27 & \textbf{52.58} \\
                        \midrule
\multirow{4}{*}{DKM 2-bit, 1-dim}    & None     & 67.64 & 65.95 \\
                                     & R\_Linf  & 68.53 & 67.06 \\
                                     & R\_M     & 68.33 & 67.50 \\
                                     & R\_SMM   & \textbf{68.63} & \textbf{67.62} \\
                        \midrule
\multirow{4}{*}{DKM 4-bit, 1-dim}    
                        & None     & 70.22 & 69.29 \\
                        & R\_Linf    & 70.34 & 69.43 \\
                        & R\_M    & 70.33 & 68.52 \\
                        & R\_SMM    & \textbf{70.52} & \textbf{69.63} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Top-1 accuracies(\%) of compression using multi-dimensional DKM and range-regularization with ResNet18 (RN), MobileNet\_V1 (MN1) on ImageNet.} % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_comp_exp_multi}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llcccc}
\toprule
Method & $R^{2}$ & RN & MN1 \\
\midrule
32bit &   & 69.76 & 70.92 \\
\midrule
\multirow{4}{*}{DKM 2-bit, 2-dim}    & None        & 63.52 & 48.16 \\
                        & R\_Linf    & 64.69 & 53.91 \\
                        & R\_M    &  \textbf{64.70} & 53.95\\
                        & R\_SMM    & 64.64 & \textbf{53.99}\\
                        \midrule
\multirow{4}{*}{DKM 4-bit, 4-dim}    & None        & 64.89 & 58.55\\
                        & R\_Linf    & 65.31 & \textbf{61.40}\\
                        & R\_M    & 65.90 & 59.70\\
                        & R\_SMM    & \textbf{66.10} & 60.05 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}









\begin{table}[t]
\caption{Question-answering NLI (QNLI) accuracies of MobileBERT using single dimension DKM}
\label{exp_mobilebert}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Method &Pre-train & 1-bit & 2-bit \\
\midrule
DKM    & 90.41 & 61.34 & 80.12 \\
DKM + R\_Linf  & 90.66 & \textbf{63.17} & \textbf{82.13}        \\
DKM + R\_M  & 89.09 & 61.80 & 80.98        \\
DKM + R\_SMM & 90.83 & 61.49 & 80.87        \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\subsection{Weight distribution}

\begin{figure*}[t]
\subfloat[conv1]{
\includegraphics[width=7in]{figures/conv1.png}}\\
\subfloat[block2-conv1]{
\includegraphics[width=7in]{figures/block2.conv1.png}}\\
\subfloat[block2-downsample]{
\includegraphics[width=7in]{figures/block2.downsample1.png}}\\
\subfloat[fc]{
\includegraphics[width=7in]{figures/fc.png}}\\
\caption{Weight distribution of various layers of ResNet-18 during training time using no range regularization and the proposed methods. Final weight distributions of fc layer with R\_SMM $R^2$ loss regularizer shows that it can produce non-zero centered distributions.}
\label{fig:weight_dist}
\end{figure*}


To understand behavior of range-regularization, we analyse weight distribution change during the pre-training stage of ResNet-18 with and without range-regularization. Figure \ref{fig:weight_dist} shows the weight distribution change of the first layer (conv1), first convolution of second residual block (block2.conv1), first downsampling layer in second residual block and the final linear layer of ResNet-18. For the no range regularization case (original) we observe that all the demonstrated weights follow a normal distribution with outliers distributed wide apart. Contrary to that, the weights of the model trained with range-regularization follows a more uniform distribution as expected in \cite{kure}. We observe the same trend as shown in \cref{table_resnet_dist}. The weight range for each layer with range-regularization is less than the model trained without range-regularization while both models achieve similar accuracy, 70.15\% and 69.76\%  with and without range-regularization respectively, in full-precision evaluation. Therefore, we demonstrate that range-regularization effectively removes the outliers in weights without noticeable accuracy regression in a pre-training stage. Infact, from \cref{table_resnet_dist} it can also be observed that even the standard deviation of the weights for most of the layers are the same with and without range-regularization. This indicates that the proposed approach doesn't cause a major shift in most of the weights, therefore the ensuring minimal regression to the accuracy of the pre-trained full precision model. In addition, for ResNet-18, range regularization shows some effect in addressing over-fitting as seen from the pre-train full precision metrics in \cref{table_quant_exp}. Contrary to L-Inf and margin range regularization, Soft-min-max allows for a skewed distribution of weights as seen for the $fc$ layer in Figure \ref{fig:weight_dist}.

% \question{Can we have something similar for s-min-max showing that it is not 0 centered?}

\begin{table}[t]
\caption{Weight range and standard deviation for each layer in ResNet-18 (max(weights) - min(weights)) after a pre-training stage.} % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_resnet_dist}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Layer} & \multicolumn{2}{c}{weight range} & \multicolumn{2}{c}{weight std}\\
 & w/ $R^{2}$ & w/o $R^{2}$ & w/ $R^{2}$ & w/o $R^{2}$\\
\midrule
conv1 &  0.63 & 1.86 & 0.11 & 0.13\\
1.0.conv1 &   0.32 & 1.42 & 0.05 & 0.05 \\
1.0.conv2 &   0.20 & 0.77 & 0.04 & 0.05 \\
1.1.conv1 &   0.20 & 1.24 & 0.04 & 0.05 \\
1.1.conv2 &   0.17 & 0.68 & 0.03 & 0.04 \\
2.0.conv1 &   0.23 & 0.66 & 0.04 & 0.04 \\
2.0.conv2 &   0.25 & 0.77 & 0.03 & 0.03 \\
2.0.downsample &   0.25 & 1.56 & 0.05 & 0.07 \\
2.1.conv1 &   0.22 & 0.82 & 0.03 & 0.03 \\
2.1.conv2 &   0.19 & 0.68 & 0.03 & 0.03 \\
3.0.conv1 &   0.23 & 0.75 & 0.03 & 0.03 \\
3.0.conv2 &   0.22 & 0.66 & 0.02 & 0.03 \\
3.0.downsample &   0.12 & 0.52 & 0.03 & 0.03 \\
3.1.conv1 &   0.19 & 0.55 & 0.02 & 0.02 \\
3.1.conv2 &   0.16 & 0.64 & 0.02 & 0.02 \\
4.0.conv1 &   0.16 & 0.65 & 0.02 & 0.02 \\
4.0.conv2 &   0.16 & 0.63 & 0.02 & 0.02 \\
4.0.downsample &   0.20 & 1.38 & 0.03 & 0.03 \\
4.1.conv1 &   0.15 & 0.46 & 0.02 & 0.02 \\
4.1.conv2 &   0.14 & 0.45 & 0.01 & 0.01 \\
fc &   0.60 & 0.99 & 0.07 & 0.07 \\
% \midrule
% \multirow{4}{*}{PACT}   & None      &  &  &  \\
%                         & R\_SMM    &  &  &  \\
%                         & R\_MRGN    &  &  &  \\
%                         & R\_Linf    &  &  &  \\\hline

% \multirow{4}{*}{LSQ}    & None    &  &  & \\
%                         & R\_SMM    &  &  &  \\
%                         & R\_MRGN    &  &  &  \\
%                         & R\_Linf    &  &  &  \\\hline

% \multirow{4}{*}{EWGS}   & None       &  &  &  \\
%                         & R\_SMM    &  &  &  \\
%                         & R\_MRGN    &  &  &  \\
%                         & R\_Linf    &  &  &  \\\hline

% \multirow{3}{*}{DKM}    & None        & 58.97 & 45.54 & 61.34        \\
%                         & R\_Linf    & 59.52 & 49.74 & 63.17 \\
%                         % & R\_MRGN    &  &  &  \\
%                         & R\_LSMM    & 59.27 & 52.58 & - \\
% \multirow{4}{*}{DKM$^{*}$}    & None        &  &  &         \\
%                         & R\_SMM    &  &  &  \\
%                         & R\_MRGN    &  &  &  \\
%                         & R\_Linf    &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

% \begin{table}[t]
% \caption{Sensitivity to range regularization weight for full precision model}
% \label{tab:rr_weight}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{llcc}
% \toprule
% Weight & Method & RN & MN1\\
% \midrule
% \multirow{3}{*}{0.001}  & R\_Linf    & - & - \\
%                         & R\_M    &  - & - \\
%                         & R\_SMM    & - & -\\
%                         \midrule
% \multirow{3}{*}{0.01}   & R\_Linf    & - & -\\
%                         & R\_M    & - & -\\
%                         & R\_SMM    & - & - \\
%                         \midrule
% \multirow{3}{*}{0.1}    & R\_Linf    & - & -\\
%                         & R\_M    & - & -\\
%                         & R\_SMM    & - & - \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}
