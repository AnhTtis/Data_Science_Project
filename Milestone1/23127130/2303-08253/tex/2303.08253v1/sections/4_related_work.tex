\section{Related Works}
\label{related_works}

\subsection{Model compression}

The simplest and one of the most effective form of compression involves sharing weights within a layer. Deep Compression \cite{HanMD15} introduced k-means clustering based weight sharing for compression. Initially, all weights belonging to the same cluster are share weight of the cluster centroid. During forward pass loss is calculated using the shared weights which are then updated during backward pass. This leads to a loss of accuracy and model train-ability because the weight to cluster assignment is intractable during weight update \cite{yinl2019}. DKM \cite{cho2021dkm} introduces differentiable k-means clustering, therefore making cluster assignments tractable. During forward clustered weights are used, however during backward the gradient is applied on the original weights.

\subsection{Model quantization}
Model quantization reduces the memory footprint of a model by reducing the representative bits per weight for a given model. In this paper we have compared our results with various training time quantization algorithms like EWGS \cite{lee2021network}, LSQ \cite{esser2019learned} and DoReFa \cite{zhou2016dorefa} used in PACT \cite{choi2018pact}. PACT clips activation values with a trainable parameter for activation quantization and uses DoReFa for weight quantization. LSQ quantizes weights and activations with learnable step size (scale or bin size). EWGS applies gradient scaling with respect to position difference in between original full precision weights and quantized weights based on LSQ.

\subsection{Regularization for quantization}
As we mentioned earlier, range regularization is not a quantization or compression technique. It helps compression and quantization by removing outliers in the weight distribution. \cite{kure} show that uniform distribution of weights are more robust to quantization than normally-distributed weights. To this they propose KURE (KUrtosis REgularization) to minimize the kurtosis of weights and ensure a uniform distribution. This method is independent of the quantization bit-width, therefore supports Post-Training Quantization in addition to QAT (Quantization Aware Training). However, this method is best suited for generic models which need to be specifically tuned for a given bit precision use case. To reduce the accuracy drop due to quantization \cite{binregularization} proposes to constrain weights to predefined bins based on the quantization bit-width. However, selecting these bins is a difficult process and the output of the models in very sensitive to the bin selection. In addition to that these methods ignore the effect of quantizing the first and last layers of the models.