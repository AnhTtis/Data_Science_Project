\section{\fname}

We introduce \fname~as an auxiliary loss to reduce the range of weights for every layer to get better pre-trained models for further quantization or compression. Just like $L_1$ and $L_2$ regularization our approach is invariant to the quantization or compression technique used. But as opposed to $L_1$ or $L_2$ regularization, \fname~only affects the outliers in weights by penalizing them while maintaining accuracy of the full precision model. We demonstrate that $L_2$ regularization (1x(baseline) and 10x(heavy L2)) does not solve the problem of outliers in \cref{fig:weight_dist_mnv2}. As a reference for the expected weight distribution for a quantization friendly model we use the weight distribution from a model trained using KURE \cite{kure}. While, the idea of minimizing range can be formulated in various ways we propose 3 different ways of defining $R^2$. We start from $L_\infty$ loss, extend it to margin loss and finally introduce soft-min-max loss for adding \name~to the training loss.

\begin{figure}[t!]
\vskip 0.0in
\begin{center}
\includegraphics[width=1\linewidth]{figures/weight_dist.pdf}
\vskip -0.2in
\caption{Weight distribution of the first five 3x3 convolution layers and FC layer of MobileNet-V2 using L2 norm (baseline), heavy L2 norm (10x heavy L2 norm than baseline) and the proposed \fname~(the red dots correspond outliers). KURE: Kurtosis Regularization~\cite{kure}.}
\label{fig:weight_dist_mnv2}
\end{center}
\vskip -0.0in
\end{figure}


% \begin{itemize}
    \textbf{$L_\infty$ \name}: This method tries to penalize only the outliers in an iterative manner during training by adding $L_{\infty}(W)$ as an auxiliary loss for every layer in the model.
    \begin{equation}
         L_{reg}=\sum L_{\infty}(W)
    \end{equation}
    The effect of this formulation is described in \cref{fig:weight_dist_mnv2} where it shows that \fname~helps to get rid of all outliers in the model. In addition, it brings the overall range of weight down in contrast to KURE \cite{kure}, while, also making the weight distribution similar to a mixture of Gaussians as seen in KURE.
    
    \textbf{Margin \name}: This is an extension of $L_{\infty}(W)$ \name, where, we define a margin for the range of allowed weights. Any weight outside this margin is penalized. In addition, we also penalize the width of the margin to ensure that the range of the overall weight distribution is small. The auxiliary loss for a given weight W is shown in \cref{eq:margin}. Here $M$ is a learnable parameter per layer.
    \begin{equation}
        L_{reg}=\sum (|M| + \max(|W|-|M|,0))
        \label{eq:margin}
    \end{equation}
    The effect of margin \name~is similar to that of $L_\infty$ in terms of the final weight distribution as evident from \cref{fig:weight_dist_mnv2}. The only difference is that margin \name~penalizes all weight outside the margin per iteration in contrast to penalizing only the maximum weight.
    
    \textbf{Soft-min-max \name \label{sec:smm}}: In this approach, we propose an asymmetric \name, to eliminate the constraint on the magnitude of weights and strictly enforce it on the range of weights. We hypothesize that such a technique will improve asymmetrically quantized/compressed models using techniques such as DKM \cite{cho2021dkm}. The loss for a given weight W is described in \cref{eq:sminmax}. 
    \begin{equation}
    \begin{split}
        s_{max} &= \frac{\Sigma (W \odot e^{\alpha \times (W-W_{max})})}{\Sigma e^{\alpha \times (W-W_{max})}} \\
        s_{min} &= \frac{\Sigma (W \odot e^{-\alpha \times (W-W_{min})})}{\Sigma e^{-\alpha \times (W-W_{min})}} \\
        L_{reg} &= (s_{max}-s_{min}) + e^{-\alpha}
    \end{split}
    \label{eq:sminmax}
    \end{equation}
    Here temperature $\alpha$ is a learnable parameter per layer. $e^{-\alpha}$ term in the auxiliary loss $L_{reg}$, encourages temperature $\alpha$ to increase during training time optimization process to approach hard-min-max loss towards the end of training. This loss smoothly penalizes not only outliers but also near-outlier weights together rather strictly brings only outliers down like other \name es. Therefore, it might be susceptible to outliers as seen in \cref{fig:weight_dist_mnv2}.
% \end{itemize}


All the above mentioned \name~were employed during training time of the base model itself and not during quantization or compression. This was done because the purpose of \name~is to provide effective initial weights for compression or quantization. This ensures extensibility of \name~to any quantization or compression technique.
% Range-restriction was added to the cross entropy loss by multiplying a restriction weight to adjust importance between the restriction and the loss during the base training phase. %This restriction weight has an initial value which differs across the three regularizers.
% Employing range-restriction during base model training and not during quantization or compression also 

% \subsection{Weight distribution}



%\begin{figure*}[t]
%\subfloat[conv1]{
%\includegraphics[width=7in]{figures/conv1.png}}\\
%\subfloat[block2-conv1]{
%\includegraphics[width=7in]{figures/block2.conv1.png}}\\
%\subfloat[block2-downsample]{
%\includegraphics[width=7in]{figures/block2.downsample1.png}}\\
%\subfloat[fc]{
%\includegraphics[width=7in]{figures/fc.png}}\\
%\caption{Weight distribution of various layers of ResNet-18 during training time using no range restriction and the proposed methods. Final weight distributions of fc layer with R\_SMM $R^2$ loss regularizer shows that it can produce non-zero centered distributions.}
%\label{fig:weight_dist}
%\end{figure*}


% To understand behavior of range-restriction, we analyse weight distribution after the pre-training stage of MobileNet-V2 with and without range-restriction. Figure \ref{fig:weight_dist_mnv2} shows the weight distribution of all the conv 3x3 layers  and the last linear layer (FC) of Mobilenet-v2. When range restriction is not applied (original) most of the weights follow a Gaussian distribution with outliers distributed wide apart. We also try increasing the l2 restriction by 10x (4e-4) but it did not seem to alleviate the issue. The weights of the model trained with range-restriction (r2-margin and r2-linf) have almost no outliers similar to that of KURE \cite{kure}. A key differentiating factor between KURE \cite{kure} and range-restriction is the range of weights (2 vs 0.5). The weight range for each layer with range-restriction is less than the model trained without range-restriction or with KURE while both models achieve similar accuracy in full-precision evaluation as shown in \cref{table_quant_exp_ewgs}. Therefore, we demonstrate that range-restriction effectively removes the outliers in weights without noticeable accuracy regression in the pre-training stage. Another interesting characteristic of the models trained with range restriction and KURE is that the weight distribution of most layers appears to be a mixture of Gaussians. Contrary to L-Inf and margin range restriction, Soft-min-max doesn't penalize specific outliers rather only focuses on the range of weights, therefore the shape of the distribution remains similar to the original case and only the range of weights is reduced. This allows for a skewed distribution of weights as seen for the $FC$ layer in \cref{fig:weight_dist_mnv2}. Finally, it might also have some effect on the outliers as evident from the first 3x3 convolutional layer since the model has to operate on constrained weight ranges.

% \question{Can we have something similar for s-min-max showing that it is not 0 centered?}

\iffalse
\begin{table}[t]
\caption{Weight range and standard deviation for each layer in ResNet-18 (max(weights) - min(weights)) after a pre-training stage.} % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_resnet_dist}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Layer} & \multicolumn{2}{c}{weight range} & \multicolumn{2}{c}{weight std}\\
 & w/ $R^{2}$ & w/o $R^{2}$ & w/ $R^{2}$ & w/o $R^{2}$\\
\midrule
conv1 &  0.63 & 1.86 & 0.11 & 0.13\\
1.0.conv1 &   0.32 & 1.42 & 0.05 & 0.05 \\
1.0.conv2 &   0.20 & 0.77 & 0.04 & 0.05 \\
1.1.conv1 &   0.20 & 1.24 & 0.04 & 0.05 \\
1.1.conv2 &   0.17 & 0.68 & 0.03 & 0.04 \\
2.0.conv1 &   0.23 & 0.66 & 0.04 & 0.04 \\
2.0.conv2 &   0.25 & 0.77 & 0.03 & 0.03 \\
2.0.downsample &   0.25 & 1.56 & 0.05 & 0.07 \\
2.1.conv1 &   0.22 & 0.82 & 0.03 & 0.03 \\
2.1.conv2 &   0.19 & 0.68 & 0.03 & 0.03 \\
3.0.conv1 &   0.23 & 0.75 & 0.03 & 0.03 \\
3.0.conv2 &   0.22 & 0.66 & 0.02 & 0.03 \\
3.0.downsample &   0.12 & 0.52 & 0.03 & 0.03 \\
3.1.conv1 &   0.19 & 0.55 & 0.02 & 0.02 \\
3.1.conv2 &   0.16 & 0.64 & 0.02 & 0.02 \\
4.0.conv1 &   0.16 & 0.65 & 0.02 & 0.02 \\
4.0.conv2 &   0.16 & 0.63 & 0.02 & 0.02 \\
4.0.downsample &   0.20 & 1.38 & 0.03 & 0.03 \\
4.1.conv1 &   0.15 & 0.46 & 0.02 & 0.02 \\
4.1.conv2 &   0.14 & 0.45 & 0.01 & 0.01 \\
fc &   0.60 & 0.99 & 0.07 & 0.07 \\
% \midrule
% \multirow{4}{*}{PACT}   & None      &  &  &  \\
%                         & R\_SMM    &  &  &  \\
%                         & R\_MRGN    &  &  &  \\
%                         & R\_Linf    &  &  &  \\\hline

% \multirow{4}{*}{LSQ}    & None    &  &  & \\
%                         & R\_SMM    &  &  &  \\
%                         & R\_MRGN    &  &  &  \\
%                         & R\_Linf    &  &  &  \\\hline

% \multirow{4}{*}{EWGS}   & None       &  &  &  \\
%                         & R\_SMM    &  &  &  \\
%                         & R\_MRGN    &  &  &  \\
%                         & R\_Linf    &  &  &  \\\hline

% \multirow{3}{*}{DKM}    & None        & 58.97 & 45.54 & 61.34        \\
%                         & R\_Linf    & 59.52 & 49.74 & 63.17 \\
%                         % & R\_MRGN    &  &  &  \\
%                         & R\_LSMM    & 59.27 & 52.58 & - \\
% \multirow{4}{*}{DKM$^{*}$}    & None        &  &  &         \\
%                         & R\_SMM    &  &  &  \\
%                         & R\_MRGN    &  &  &  \\
%                         & R\_Linf    &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\fi