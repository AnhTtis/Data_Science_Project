\section{Experiment}
\label{experiment}

\iffalse
\begin{table*}[h]
\caption{Top-1 accuracies (\%) of ResNet18 (RN), MobileNet-V1 (MN1) on ImageNet-1K with weight and activation quantization.}
\label{table_quant_exp}
\vskip 0.15in
\begin{center}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
 % & \multicolumn{4}{c|}{Compression} & \multicolumn{4}{c}{Quantization}\\
Pre-train (FP32) & \multicolumn{2}{c}{RN} & \multicolumn{2}{c}{MN1} & \multicolumn{2}{c}{MN2} \\
\midrule
\circled{0} w/o $R^2$    & \multicolumn{2}{c}{69.76} & \multicolumn{2}{c}{70.92} & \multicolumn{2}{c}{71.88}\\
\circled{1} R\_Linf    & \multicolumn{2}{c}{70.15} & \multicolumn{2}{c}{70.59} & \multicolumn{2}{c}{70.75}\\
\circled{2} R\_M    & \multicolumn{2}{c}{70.08} & \multicolumn{2}{c}{70.28} & \multicolumn{2}{c}{70.94}\\
\circled{3} R\_SMM    & \multicolumn{2}{c}{69.84} & \multicolumn{2}{c}{70.34}  & \multicolumn{2}{c}{70.75}\\
\midrule
Quant Method & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit\\
\midrule
PACT    & 51.97 & 66.90 & 22.39 & 64.64 & 21.02 & 65.23\\
PACT + R\_Linf    & 55.26 & \textbf{68.45} & 35.99 & \textbf{68.40} & 36.55 & 67.94\\
PACT + R\_M    & \textbf{56.24} & 68.30 & \textbf{40.41} & \textbf{68.40} & \textbf{37.86} & \textbf{68.12}\\
PACT + R\_SMM    & 55.64 & 68.36 & 33.26 & 68.32 & 35.92 & 67.98\\
\midrule
LSQ from \circled{0} & 58.33 & \textbf{69.90} & 38.78 & 69.00 & 33.78 & \textbf{69.00}\\
LSQ from \circled{1} & 62.23 & 69.55 & 43.05 & 69.28 & 39.05 & 68.85\\
LSQ from \circled{2} & 62.25 & 69.56 & 40.85 & \textbf{69.64}  & \textbf{39.30} & 68.46\\
LSQ from \circled{3} & \textbf{62.47} & 69.45 & \textbf{46.72} & 69.37 & 38.04 & 68.98\\
\midrule
EWGS from \circled{0} & 65.42 & \textbf{70.19} & 59.81 & 69.61 & 55.69 & \textbf{70.21}\\
EWGS from \circled{1} & \textbf{65.72} & 70.17 & 59.43 & 69.32  & 56.46 & 69.73\\
EWGS from \circled{2} & 64.27 & 69.77 & 60.18 & 69.63  & \textbf{57.39} & 69.88\\
EWGS from \circled{3} & 64.94 & 69.80 & \textbf{60.63} & \textbf{69.79} & 56.65 & 69.54\\
% EWGS    & 61.9 & 83.8 & $\surd$ \\
% DKM    & 74.8 & 78.3 &         \\
% \midrule
% PACT + R\_SMM    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_SMM & 83.3 & 80.0 & $\times$\\
% EWGS + R\_SMM    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_SMM    & 74.8 & 78.3 &         \\
% PACT + R\_MRGN    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_MRGN & 83.3 & 80.0 & $\times$\\
% EWGS + R\_MRGN    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_MRGN    & 74.8 & 78.3 &         \\
% PACT + R\_Linf    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_Linf & 83.3 & 80.0 & $\times$\\
% EWGS + R\_Linf    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_Linf    & 74.8 & 78.3 &         \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table*}
\fi









\subsection{Experiment settings}
\subsubsection{Pre-training from scratch with and without \name}
We train ResNet-18~\cite{he2016deep}, MobileNet-V1~\cite{howard2017mobilenets} and MobileNet-V2~\cite{sandler2018mobilenetv2} on ImageNet 1K~\cite{deng2009imagenet} with proposed \fname~on a x86 Linux machine with eight GPUs to get pre-trained models before model compression and quantization-aware training. We set initial learning rates to 1.0, 0.4 and 0.4 for ResNet-18, MobileNet-V1 and MobileNet-V2 respectively. We use SGD with 0.9 of momentum with Nesterov. We apply 1e-4 of weight decay (L2 norm weight regularization) for ResNet-18 and 4e-5 for MobileNet-V1 and V2. For heavy L2-regularization, in ~\cref{fig:weight_dist_mnv2}, we use 4e-4 of weight decay (10x heavier than baseline) for MobileNet-V2 to see whether heavy L2-regularization helps quantization or not as a naive solution for range restriction. Strength of \name~is set to 0.01. For Margin \name, the margin threshold is initialized with 2x the standard deviation of the initialized weights. In Soft-min-max \name~training, the learnable parameter $\alpha$ is initially set to 0.1. For comparison, we use pre-trained models of Resnet-18 from Torchvision. As we are using modified version of ResNet-50, and ResNet-101, MobileNet-V1 and V2 for better FP32 performance, we trained those models from scratch without \name~using the same settings above. It can be observed from Table ~\ref{table_quant_exp_ewgs} that \name~does not significantly affect the performance of the full precision models as well therefore provides a strong initial checkpoint for the model to be quantized. 
\subsubsection{Model compression and quantization}
\begin{table}[t]
\caption{Model size of compressed MobileNet-V1 (in M bytes). All: all layer quantization. W/O F\&L: Quantize the model excluding the first and last layers. 2bit accuracy: EWGS 2bit weight and activation quantization accuracies on ImageNet. The model size of FP32 MobileNet-V1 is 16.1 MB} % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_model_size}
\begin{center}
\begin{sc}
% \begin{tabular}{lcccc}
% \toprule
% bit & w/o F \& L & w/o F & w/o L & Quant all \\
% \midrule
% 32bit &   &  &  & 16.1 \\
% 4bit & 6.0  & 1.7 & 5.9 & 1.7 \\
% 2bit & 5.4  & 0.8 & 5.4 & 0.8 \\
% 1bit & 5.2  & 0.4 & 5.2 & 0.4 \\
\setlength{\tabcolsep}{5.5pt}
\begin{tabular}{lccc}
\toprule
\multicolumn{2}{c}{MobileNet-V1} & All  & W/O F \& L \\
\midrule
\multirow{3}{*}{Model Size (MB) }&4bit & 4.2 & 7.1   \\
&2bit & 2.2 & 5.6   \\
&1bit & 1.2 & 4.8   \\
\hline \hline
\multicolumn{2}{l}{2bit accuracy} & 55.96\%  & 59.10\% \\
\multicolumn{2}{l}{2bit accuracy with $R^2$ } & 59.05\%  & 61.25\% \\
\bottomrule

% \toprule
% bit & Quant excl. F \& L & Quant all \\
% \midrule
% 4bit & 7.1   & 4.2 \\
% 2bit & 5.6   & 2.2 \\
% 1bit & 4.8   & 1.2 \\
% \hline \hline
% 2bit acc. & 59.10\% & 55.96\% \\
% \bottomrule
\end{tabular}

\end{sc}
  % \begin{tablenotes}
  % \vskip -0.2in
  %   \item[1]\footnotesize{* W/ \name, EWGS 2bit all layer quantization achieves 59.05\% of top-1 accuracy.}
  % \end{tablenotes}
\end{center}

\vskip -0.1in

\end{table}
\name~is not a model compression nor quantization method. It penalizes outlier weights during training of the base model from scratch. To evaluate the effectiveness of \name~with model compression and quantization, we apply state-of-the-art compression/quantization techniques, DKM~\cite{cho2021dkm}, LSQ~\cite{esser2019learned}, EWGS~\cite{lee2021network}, DFQ~\cite{nagel2019data}, AdaRound~\cite{nagel2020up}, SQuant~\cite{guo2022squant}, and PD-Quant~\cite{liu2023pd} to the pre-trained model with and without \name. Except EWGS~\footnote{\url{https://github.com/cvlab-yonsei/EWGS}}, SQuant~\footnote{\url{https://github.com/clevercool/SQuant}} and PD-Quant~\footnote{\url{https://github.com/hustvl/PD-Quant}}, since other works do not provide official implementation, we implement those techniques ourselves and for DFQ, and AdaRound, we used AIMET~\footnote{\url{https://quic.github.io/aimet-pages/}}.

We follow the same hyper-parameters used in the works, but \textbf{we apply compression and quantization for all layers including the first and last layers.} It is important to compress/quantize all layers including first and last layers considering computation burden at the first layer with a large convolutional filter size such as 7x7 convolutions in the first layer of ResNet and the large number of weights in the last linear layer, e.g., 1.2M of weights in the last layer of MobileNet-V1 which has 4.2M of weights in total. We have demonstrated this burden in Table \ref{table_model_size} for more clarity. Due to the outliers and wide weight ranges in the first and last layers, quantizing all layers have less accuracy than quantizing a model excluding the first and layer layers as shown in ~\cref{table_model_size}. We represent $L_\infty$ \name~as $R\_Linf$, Margin \name~as $R\_M$ and Soft-min-max \name~as $R\_Smm$ in the results.

\subsection{Model quantization}

\iffalse
\begin{table}[t]
\caption{Top-1 accuracies (\%) of MobileNet-V1 and V2 on ImageNet-1K using PTQ methods with 4bit weight and 8bit activation quantization. None: quantizing without any advanced PTQ techniques, BC: Bias correction ~\cite{nagel2019data}, DFQ (Cross-layer equalization + BC) ~\cite{nagel2019data}, AR: AdaRound ~\cite{nagel2020up}}
\vskip 0.15in
\begin{center}
\begin{sc}
\begin{tabular}{l|cccc|cccc}
\hline
Pre-train (FP32) & \multicolumn{4}{c|}{MobileNet-V1 0.25} & \multicolumn{4}{c}{MobileNet-V1 1.0} \\
Method & None & BC & DFQ & AR & None & BC & DFQ & AR\\
\midrule
Baseline (L2) &    & 5.20  & 13.03   & 45.88 &  2.67  & 0.11 &  54.06  & 70.42 \\
Baseline (10x L2) &    & 11.23 & 18.85   & 44.83 &  13.41  & 0.12 &  57.68  & 69.23 \\
\midrule
KURE &  3.54 ()  & 40.13 &  21.87  & 48.32 &  45.76 (53.69)   & 47.25 &  59.21  & 60.51 \\
\midrule
L-\inf &    & 43.17 & 29.10   & 50.76 & 61.73 (53.57)  & \textbf{66.61} & 53.00   & \textbf{72.28} \\
R\_M &    & \textbf{45.11} &  \textbf{31.02}  & \textbf{51.17} &  61.66 (57.28)  & 57.32 &  \textbf{65.06}  & \textbf{72.29} \\
\hline\hline
 & \multicolumn{4}{c|}{MobileNet-V2 0.25} & \multicolumn{4}{c}{MobileNet-V2 1.0} \\
%Regularization & None & BC & DFQ & AR & None & BC & DFQ & AR\\
\midrule
Baseline (L2) &    &   &    & 49.49 &    &  &  56.56  & 71.29 \\
Baseline (10x L2) &    &   &    & 45.12 &    &  &    & 68.06 \\
\midrule
KURE &    &   &    & 50.96 &    &  &  62.49  & 71.68 \\
\midrule
R\_Linf &    &   &    & 51.68 &    &  & 62.39   & 71.76 \\
R\_M &    &   &    & 51.12 &    &  &  66.04  & 71.89 \\
%\midrule
%KURE &
%\midrule
%R\_Linf & 7.1   & 4.2 \\
%R\_M & 5.6   & 2.2 \\
\bottomrule

\end{tabular}
\end{sc}
\end{center}
% DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_PTQ}
\vskip -0.1in
\end{table}
\fi

\begin{table*}[h]
\caption{Top-1 accuracies (\%) of MobileNet-V1 and V2 on ImageNet-1K using PTQ methods with 4bit weight and 8bit activation quantization. Heavy L2: applied 10x heavy L2 regularization than baseline. KURE~\cite{kure}. Na\"ive: quantizing without any advanced PTQ techniques. DFQ~\cite{nagel2019data}, AR~\cite{nagel2020up}, SQ~\cite{guo2022squant}, PD-Q~\cite{liu2023pd}}
\vspace{-0.4cm}
\begin{center}
\begin{sc}
\setlength{\tabcolsep}{5.2pt} % Default value: 6pt
% \renewcommand{\arraystretch}{0.8} % Default value: 1
\begin{tabular}{l|c|ccccc|c|ccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{6}{c|}{MobileNet-V1} & \multicolumn{6}{c}{MobileNet-V2}\\
 & FP32 & Na\"ive & DFQ & AR & SQ & PD-Q & FP32 & Na\"ive & DFQ & AR & SQ & PD-Q\\
\midrule
Baseline &  74.12  & 2.67 &  54.06  & 70.42 & 63.85 & 71.87& 73.08 & 2.57 & 56.56 & 71.29 & 59.30 & 71.54\\
\midrule
Heavy L2 &  72.67  & 13.41 &  57.68  & 69.23 & 66.51 & 69.86& 71.00 & 4.17 & 0.09 & 68.06 & 57.30& 68.92\\
KURE &   72.50   & 53.69 &  59.21  & 60.51 & 68.84& 71.39& 72.49 & 39.21 & 62.49 & 71.68 & 66.09 & 71.88\\
\midrule
R\_Linf &  73.65  & \textbf{61.73}  & 53.00   & \textbf{72.28} & \textbf{69.69}& \textbf{72.76}& 72.64 & 59.95 & 62.39 & 71.76 & 64.02 & \textbf{72.01}\\
R\_M &  73.54  & \textbf{61.66}  &  \textbf{65.06}  & \textbf{72.29} & 68.96& \textbf{72.77}& 72.73 & \textbf{60.03} & \textbf{66.04} & \textbf{71.89} & \textbf{67.17} & 71.92\\
R\_SMM & 73.95 & 44.24 & 59.21 & 71.35 & 67.86& 72.39 & 72.81 & 36.69 & 51.46 & 71.77 & \textbf{67.16}& 71.98\\
% \hline\hline
%  & \multicolumn{4}{c|}{MobileNet-V2 0.25} & \multicolumn{4}{c}{MobileNet-V2 1.0} \\
% %Regularization & None & BC & DFQ & AR & None & BC & DFQ & AR\\
% \midrule
% Baseline (L2) &  53.90  & 0.73  &  17.27  & 49.49 &  73.08  & 2.57 &  56.56  & 71.29 \\
% Baseline (10x L2) &  49.25  &  0.96 &  0.10  & 45.12 &  71.00  & 4.17 &  0.09  & 68.06 \\
% \midrule
% KURE &  52.80  & 15.98  & \textbf{27.39}   & 50.96 &  72.49  & 39.21 &  62.49  & 71.68 \\
% \midrule
% R\_Linf &  53.40  & 13.08  & 26.51   & \textbf{51.68} &  72.64  & \textbf{59.95} & 62.39   & 71.76 \\
% R\_M &  52.82  & \textbf{31.11}  &  27.00  & 51.12 &  72.73  & \textbf{60.03} &  \textbf{66.04}  & \textbf{71.89} \\
%\midrule
%KURE &
%\midrule
%R\_Linf & 7.1   & 4.2 \\
%R\_M & 5.6   & 2.2 \\
\bottomrule

\end{tabular}
\end{sc}
\end{center}
% DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_PTQ}
\vskip -0.3in
\end{table*}

\begin{table}[t]
\caption{Top-1 accuracies (\%) of 2bit weight and 8bit activation PTQ using PD-Quant~\cite{liu2023pd}. MNV1: MobileNet-V1, MNV2: MobileNet-V2, RN50: ResNet-50, RN101: ResNet-101. FP32 accuracy is in  \cref{table_PTQ} and \cref{table_ptq_resnet}.}
\label{table_PTQ-2bit}
\begin{center}
\begin{sc}
\begin{tabular}{l|c|c|c|c}
\toprule
% \multirow{2}{*}{Method}  & \multicolumn{2}{c|}{MobileNet-V1} & \multicolumn{2}{c}{MobileNet-V2}\\
Method & MNV1 & MNV2 & RN50 & RN101 \\
\midrule
Baseline    & 47.62 & 50.66 & 62.92 & 66.50\\
\midrule
R\_Linf    & \textbf{56.79} & \textbf{59.49} & 71.34 & \textbf{73.89}\\
R\_M    & 56.27 & 58.70 & \textbf{71.65} & \textbf{73.87}\\
R\_SMM    & 54.20 & 57.53 & 69.31 & 71.24\\

% EWGS    & 61.9 & 83.8 & $\surd$ \\
% DKM    & 74.8 & 78.3 &         \\
% \midrule
% PACT + R\_SMM    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_SMM & 83.3 & 80.0 & $\times$\\
% EWGS + R\_SMM    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_SMM    & 74.8 & 78.3 &         \\
% PACT + R\_MRGN    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_MRGN & 83.3 & 80.0 & $\times$\\
% EWGS + R\_MRGN    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_MRGN    & 74.8 & 78.3 &         \\
% PACT + R\_Linf    & 95.9 & 96.7 & $\surd$ \\
% LSQ + R\_Linf & 83.3 & 80.0 & $\times$\\
% EWGS + R\_Linf    & 61.9 & 83.8 & $\surd$ \\
% DKM + R\_Linf    & 74.8 & 78.3 &         \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.3in
\end{table}


% \begin{table}[t]
% \caption{Top-1 accuracies (\%) of 2bit weight and 8bit activation PTQ using PD-Quant~\cite{liu2023pd}.}
% \label{table_PTQ-2bit}
% \begin{center}
% \begin{sc}
% \begin{tabular}{l|cc|cc}
% \toprule
% \multirow{2}{*}{Method}  & \multicolumn{2}{c|}{MobileNet-V1} & \multicolumn{2}{c}{MobileNet-V2}\\
%  & FP32 & 2W8A & FP32 & 2W8A \\
% \midrule
% Baseline    & 74.12 & 47.62 & 73.08 & 50.66\\
% \midrule
% R\_Linf    & 73.65& \textbf{56.79} & 72.64 & \textbf{59.49}\\
% R\_M    & 73.54& 56.27 &72.73&  58.70\\
% R\_SMM    & 73.95& 54.20 & 72.81& 57.53\\

% % EWGS    & 61.9 & 83.8 & $\surd$ \\
% % DKM    & 74.8 & 78.3 &         \\
% % \midrule
% % PACT + R\_SMM    & 95.9 & 96.7 & $\surd$ \\
% % LSQ + R\_SMM & 83.3 & 80.0 & $\times$\\
% % EWGS + R\_SMM    & 61.9 & 83.8 & $\surd$ \\
% % DKM + R\_SMM    & 74.8 & 78.3 &         \\
% % PACT + R\_MRGN    & 95.9 & 96.7 & $\surd$ \\
% % LSQ + R\_MRGN & 83.3 & 80.0 & $\times$\\
% % EWGS + R\_MRGN    & 61.9 & 83.8 & $\surd$ \\
% % DKM + R\_MRGN    & 74.8 & 78.3 &         \\
% % PACT + R\_Linf    & 95.9 & 96.7 & $\surd$ \\
% % LSQ + R\_Linf & 83.3 & 80.0 & $\times$\\
% % EWGS + R\_Linf    & 61.9 & 83.8 & $\surd$ \\
% % DKM + R\_Linf    & 74.8 & 78.3 &         \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{center}
% \vskip -0.3in
% \end{table}



\subsubsection{Post-Training Quantization, PTQ with $R^2$}
We compare models trained using \name~and other weight regularization, L2, heavy L2, and KURE ~\cite{kure} and quantized using PTQ methods such as DFQ ~\cite{nagel2019data}, AdaRound ~\cite{nagel2020up}, SQaunt~\cite{guo2022squant}, and PD-Quant~\cite{liu2023pd}. There are two major techniques in DFQ, bias correction compensating bias in activation and cross-layer equalization applying scale factor per channel to make all channels in a layer have similar weight range. AdaRound adaptively rounds weights to quantization bins instead of naive nearest rounding. SQuant decomposes a layer by the Hessian-based optimization objective into three diagonal sub-items, element-
wise, kernel-wise, and output channel-wise, and then it compose the sub-items in a quantized domain with respect to
constrained absolute some of error. PD-Quant quantizes weights by comparing model prediction result before and after quantization of each layer. Unlike other PTQ, SQuant dose not require calibration dataset, so its accuracy could be less than others.

As shown in ~\cref{table_PTQ}, models trained with \name~are more quantization friendly than other regularization. As KURE makes the weight distribution uniform, it can reduce outliers as a side-effect while keeping a wide weight range. Therefore, KURE is more effective than L2 norm, but \name~shows the best accuracy as it reduces outliers as well as weight range. On the other hand, heavy L2 regularization (10x L2) makes weight ranges smaller, but it does not remove outliers, therefore prove to be ineffective here.

In comparison between FP32 accuracy of baseline models and models trained with \name, we can see there are slight accuracy regression in full-precision inference as we expected in \cref{fig_weight_dist} (right). However, after quantizing, the models trained with \name~shows better accuracies for all PTQ methods that we used in ~\cref{table_PTQ}. KURE regulates entire weights to make them uniform distribution, so it is a harsh regularization. This is the reason why FP32 accuracy with KURE is inherently less than other cases including \name~which only affects to outlier (and near-outlier) weights. Also, even considering FP32 accuracy difference, still \name~achieves better performance in terms of accuracy regression from FP32 model to quantized model, e.g., KURE regresses by 11.99\% and 1.11\% by using AdaRound and PD-Quant for MobileNet-V1, respectively (72.50\% $\rightarrow$ 60.51\% and 71.39\%), while Margin \name~regresses by 1.25\% and 0.77\% (73.54\% $\rightarrow$ 72.29\% and 72.77\%). 
%In ~\cref{table_PTQ-2bit}, \name~improves 2bit weight and 8bit activation MobileNet-V2 quantization accuracy to 59.49\% from 50.66\%.

Even without advanced PTQ approaches such as DFQ, AdaRound, SQuant, and PD-Quant, models trained with \name~can be reasonably quantized without any further fine-tuning. In ~\cref{table_PTQ}, Na\"ive with \name~shows significantly higher accuracy than other regularization. The models with \name~have good weight distribution already from pre-training so that they can be quantized with fairly high quantization accuracies.

We conduct further studies, lower bit PTQ (2bit weight and 8bit activation) and PTQ for larger models (ResNet50 and ResNet101) as shown in \cref{table_PTQ-2bit} and \cref{table_ptq_resnet}. From the lower bit PTQ result, we can clearly see the benefit of \name. For example, accuracy of ResNet101 trained with $L_\infty$ \name~only regressed by 5.41\% (79.30\% $\rightarrow$ 73.89\%), while the baseline model trained without \name~shows huge regression, 12.95\% (79.45\% $\rightarrow$ 66.50\%).

\begin{table}[t]
\caption{Top-1 accuracies (\%) of ResNet50 and ResNet101 on ImageNet-1K with 4bit weight and 8bit activation PTQ.}
\label{table_ptq_resnet}
\begin{center}
\begin{sc}
\begin{tabular}{l|c|c|c|c}
\toprule
 \multirow{3}{*}{Method} &\multicolumn{4}{c}{ResNet50}\\
 % &\multicolumn{2}{c|}{ResNet50} &\multicolumn{2}{c}{ResNet101}\\
 % \midrule
 & FP32 & AR & SQ & PD-Q  \\
% \midrule
% Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
\midrule
Baseline    &  78.04 & 74.73  & 74.68 & 76.60  \\
% KURE    & 77.30& 76.74  &  75.77 & 76.99\\
\midrule
R\_Linf    & 77.97& 76.34  & \textbf{76.28}  & 77.50 \\
R\_M    &  78.11& \textbf{77.31} &  75.32 & \textbf{77.78} \\
R\_SMM    &  78.22& 75.61  & 75.75  & 77.21 \\
\midrule
\midrule
 \multirow{3}{*}{Method} &\multicolumn{4}{c}{ResNet101}\\
 % &\multicolumn{2}{c|}{ResNet50} &\multicolumn{2}{c}{ResNet101}\\
 % \midrule
 & FP32 & AR & SQ & PD-Q  \\
% \midrule
% Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
\midrule
Baseline    & 79.45 & 75.18  & 75.23 &  78.16 \\
% KURE    & 78.59 & 77.75  & 76.65  & 78.06\\
\midrule
R\_Linf    & 79.30 & \textbf{78.22}  &  76.87 & \textbf{78.61}\\
R\_M    & 79.27 & \textbf{78.19} &  \textbf{77.95} &  \textbf{78.69}\\
R\_SMM    & 79.63  & 76.71  & 77.20  &  78.49\\
% \midrule
% \midrule
% \multirow{2}{*}{Method} &\multicolumn{4}{c}{2W8A PD-Quant}\\
%  % \midrule
%  & FP32 & 4W8A & FP32 & 4W8A  \\
% % \midrule
% % Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
% \midrule
% Baseline    & 69.76 & 66.90  & 69.90 &  70.19 \\
% R\_Linf    & 70.15 & 68.45  & 69.55  & 70.17\\
% R\_M    & 70.08 & 68.30 & 69.56  & 69.77 \\
% R\_SMM    & 69.84 & 68.36  & 69.45  & 69.80 \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}


% \begin{table}[t]
% \caption{Top-1 accuracies (\%) of ResNet50 and ResNet101 on ImageNet-1K with 2bit weight and 8bit activation PD-Quant.}
% \label{table_ptq_resnet_2bit}
% \begin{center}
% \begin{sc}
% \begin{tabular}{l|c|c|c|c}
% \toprule
% \multirow{2}{*}{Method} &\multicolumn{2}{c|}{ResNet50}&\multicolumn{2}{c}{ResNet101}\\
%  % \midrule
%  & FP32 & 2W8A & FP32 & 2W8A  \\
% % \midrule
% % Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
% \midrule
% Baseline    &  &   &   \\
% KURE    & &   &   \\
% R\_Linf    & &   &  \\
% R\_M    &  &  &   \\
% R\_SMM    &  &   &   \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{center}
% \vskip -0.1in
% \end{table}
%%%%% this is for cross layer equaltization result
% Cross-layer equalization in DFQ does not help PTQ accuracy for models trained with KURE and \name~as compared to quantizing without any advanced PTQ approaches, Na\"ive column in ~\cref{table_PTQ}. It might be because models trained with KURE and $R^2$ do not have outliers therefore, per channel weight equalization is not required. Cross-layer equalization might make PTQ unstable for those models trained with $R^2$ as it propagates estimation errors from cross-layer equalization itself and bias absorption. %In comparison to heavy L2 regularization, bias correction, and DFQ, it seems that having narrow weight range and removing outlier weights are important to quantization accuracy.

\subsubsection{Quantization-Aware Training, QAT with \name}
\label{sec:qat_result}
We apply state-of-the-art quantization techniques like PACT ~\cite{choi2018pact} while training the models from scratch using \name. For other quantization aware training methods like, EWGS ~\cite{lee2021network} and LSQ ~\cite{esser2019learned} we initialize the model to pre-trained ResNet-18 (RN) ~\cite{he2016deep}, MobileNet-V1 (MN1) ~\cite{howard2017mobilenets} and MobileNet-V2 (MN2) ~\cite{sandler2018mobilenetv2} with \name.

\begin{table*}[t]
\caption{Top-1 accuracies (\%) of MobileNet-V1 and V2 on ImageNet-1K with EWGS varying model size. Weights and activations are quantized with the same 2bit. Model sizes are adjusted by varying width factor ($\alpha$ in \cite{howard2017mobilenets} and width multiplier in \cite{sandler2018mobilenetv2}). Width factor for large: 1.0, medium: 0.5, and small: 0.25.}
% \vskip -0.1in
\label{table_quant_exp_ewgs}

\begin{center}
\begin{sc}
\setlength{\tabcolsep}{5.5pt}
\begin{tabular}{l|cc|cc|cc|cc|cc|cc}
\toprule
 % & \multicolumn{4}{c|}{Compression} & \multicolumn{4}{c}{Quantization}\\
\multirow{3}{*}{Method}  &  \multicolumn{6}{c|}{MobileNet-V1} &  \multicolumn{6}{c}{MobileNet-V2} \\
 & \multicolumn{2}{c|}{large} & \multicolumn{2}{c|}{medium} & \multicolumn{2}{c|}{small}  & \multicolumn{2}{c|}{large} & \multicolumn{2}{c|}{medium} & \multicolumn{2}{c}{small} \\
% \midrule
 % & \multicolumn{2}{c|}{0.47} & \multicolumn{2}{c|}{1.33} & \multicolumn{2}{c}{4.23} \\
% \midrule
 & FP32 & 2bit & FP32 & 2bit & FP32 & 2bit & FP32 & 2bit & FP32 & 2bit & FP32 & 2bit\\
\midrule
Baseline    & 74.12 & 55.96 &66.51 & 38.77 & 55.43 & 20.85 & 73.08 & 53.93  & 65.55 & 39.25 &  53.90 & 27.82\\
KURE    &  72.50 & 57.80& 63.99 & 39.37 & 52.83 & 22.16 & 72.49 & 53.97 & 64.64 & 38.51 &  52.80 & 26.86\\
\midrule
R\_Linf    &  73.65 & \textbf{59.05} & 65.68 & 41.22 &  53.48 & \textbf{26.04}& 72.64 & 56.35 & 65.09 & 42.55 &  53.40  & \textbf{30.56}\\
R\_M    & 73.54 & 58.41 & 65.83 & \textbf{42.61} &  53.30 & 24.35& 72.73 & \textbf{57.36} & 64.86 & \textbf{43.78} &  52.82 & 29.61\\


% \multirow{3}{*}{Method}  &  \multicolumn{6}{c|}{MobileNet-V1} &  \multicolumn{6}{c}{MobileNet-V2} \\
%  & \multicolumn{2}{c|}{small} & \multicolumn{2}{c|}{medium} & \multicolumn{2}{c|}{large}  & \multicolumn{2}{c|}{0.25} & \multicolumn{2}{c|}{0.5} & \multicolumn{2}{c}{1.0} \\
% % \midrule
%  % & \multicolumn{2}{c|}{0.47} & \multicolumn{2}{c|}{1.33} & \multicolumn{2}{c}{4.23} \\
% % \midrule
%  & FP32 & 2bit & FP32 & 2bit & FP32 & 2bit & FP32 & 2bit & FP32 & 2bit & FP32 & 2bit\\
% \midrule
% Baseline    & 55.43 & 20.85 & 66.51 & 38.77 & 74.12 & 55.96 & 53.90 & 27.82 & 65.55 & 39.25 & 73.08 & 53.93 \\
% KURE    & 52.83 & 22.16 & 63.99 & 39.37 & 72.50 & 57.80 & 52.80 & 26.86 & 64.64 & 38.51 & 72.49 & 53.97 \\
% \midrule
% R\_Linf    & 53.48 & \textbf{26.04}  & 65.68 & 41.22 & 73.65 & \textbf{59.05} & 53.40  & \textbf{30.56} & 65.09 & 42.55 & 72.64 & 56.35 \\
% R\_M    & 53.30 & 24.35 & 65.83 & \textbf{42.61} & 73.54 & 58.41 & 52.82 & 29.61 & 64.86 & \textbf{43.78} & 72.73 & \textbf{57.36} \\

\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.2in
\end{table*}
\cref{table_quant_exp_ewgs} shows 2 bit weight and activation quantization result of MobileNet-V1 and V2 using EWGS ~\cite{lee2021network} with various regularization such as without \name~(only with L2 norm), KURE ~\cite{kure}, and our \name. For both the models with varying model sizes, \name~outperforms the models trained with only L2 norm or KURE. Without \name, accuracy of MobileNet-V1 2bit quantization using EWGS declines from 59.10\% to 55.96\% when we quantize all layers including the first and last layers as shown previously in \cref{table_model_size}. This is because the first and last layers have wide weight ranges and many outliers as shown in \cref{fig:weight_dist_mnv2}. Our approach effectively reduces the outliers in the first and last layer which enables the 2bit quantized model to achieve similar accuracy to the case with original EWGS results where the first and last layers of the model remain in FP32.

\begin{table}[t]
\caption{Top-1 accuracies (\%) of ResNet18 with various QAT methods. Weights and activations are quantized with the same bit (2W2A: 2bit, 4W4A: 4bit).}
\label{table_quant_exp}
\begin{center}
\begin{sc}
\begin{tabular}{l|c|c|c|c}
\toprule
\multirow{2}{*}{Method} &\multicolumn{4}{c}{2W2A EWGS}\\
 % \midrule
 & FP32 & PACT & LSQ & EWGS \\
% \midrule
% Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
\midrule
Baseline    & 69.76 & 51.97  & 58.33 &  65.42 \\
\midrule
R\_Linf    & 70.15 & 55.26  & 62.23  & \textbf{65.72}\\
R\_M    & 70.08 & \textbf{56.24} & 62.25  & 64.27 \\
R\_SMM    & 69.84 & 55.64  & \textbf{62.47}  & 64.94 \\
\midrule
\midrule
\multirow{2}{*}{Method} &\multicolumn{4}{c}{4W4A EWGS}\\
 % \midrule
 & FP32 & PACT & LSQ & EWGS \\
% \midrule
% Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
\midrule
Baseline    & 69.76 & 66.90  & 69.90 &  70.19 \\
\midrule
R\_Linf    & 70.15 & 68.45  & 69.55  & 70.17\\
R\_M    & 70.08 & 68.30 & 69.56  & 69.77 \\
R\_SMM    & 69.84 & 68.36  & 69.45  & 69.80 \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}


% \begin{table}[t]
% \caption{Top-1 accuracies (\%) of ResNet18 on ImageNet-1K with various QAT methods. Weights and activations are quantized with the same bit. We used the pre-trained model without $R^2$ from torchvision.}
% \label{table_quant_exp}
% \vskip 0.15in
% \begin{center}
% \begin{sc}
% \begin{tabular}{l|c|cc|cc|cc}
% \toprule
%  % & \multicolumn{4}{c|}{Compression} & \multicolumn{4}{c}{Quantization}\\
% \multicolumn{2}{c|}{QAT method} & \multicolumn{2}{c|}{PACT} & \multicolumn{2}{c|}{LSQ} & \multicolumn{2}{c}{EWGS} \\
% \midrule
% Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
% \midrule
% w/o $R^2$    & 69.76 & 51.97 & 66.90 & 58.33 & \textbf{69.90} & 65.42 & \textbf{70.19}\\
% R\_Linf    & 70.15 & 55.26 & \textbf{68.45} & 62.23 & 69.55 & \textbf{65.72} & 70.17\\
% R\_M    & 70.08 & \textbf{56.24} & 68.30 & 62.25 & 69.56 & 64.27 & 69.77\\
% R\_SMM    & 69.84 & 55.64 & 68.36 & \textbf{62.47} & 69.45 & 64.94 & 69.80\\

% % EWGS    & 61.9 & 83.8 & $\surd$ \\
% % DKM    & 74.8 & 78.3 &         \\
% % \midrule
% % PACT + R\_SMM    & 95.9 & 96.7 & $\surd$ \\
% % LSQ + R\_SMM & 83.3 & 80.0 & $\times$\\
% % EWGS + R\_SMM    & 61.9 & 83.8 & $\surd$ \\
% % DKM + R\_SMM    & 74.8 & 78.3 &         \\
% % PACT + R\_MRGN    & 95.9 & 96.7 & $\surd$ \\
% % LSQ + R\_MRGN & 83.3 & 80.0 & $\times$\\
% % EWGS + R\_MRGN    & 61.9 & 83.8 & $\surd$ \\
% % DKM + R\_MRGN    & 74.8 & 78.3 &         \\
% % PACT + R\_Linf    & 95.9 & 96.7 & $\surd$ \\
% % LSQ + R\_Linf & 83.3 & 80.0 & $\times$\\
% % EWGS + R\_Linf    & 61.9 & 83.8 & $\surd$ \\
% % DKM + R\_Linf    & 74.8 & 78.3 &         \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{center}
% \vskip -0.1in
% \end{table}

As shown in \cref{table_quant_exp}, \name~helps the quantization techniques in improving their accuracy, especially for extremely low bit quantization such as at 2 bit while it shows similar accuracies with 4 bit. For example, all \name es improve 2 bit quantization accuracy with LSQ to over than 62\% from 58\%, but there is no noticeable difference in 4 bit LSQ accuracies with and without $R^2$. The reason why \name~would not help much for higher bit like 4 bit quantization is that QAT can effectively represent outliers using many bits as we expected in ~\cref{fig_weight_dist} (right).

%For all quantization methods, ResNet18 initialized with range-restriction show considerably higher accuracy than original quantization works for 2 bit quantization such as 51.97\% vs. 56.24\%, 58.33\% vs. 62.47, and 65.42\% vs. 65.72\% with PACT, LSQ, and EWGS, respectively. 

Interestingly, soft-min-max \name~does not seem to be as good as $L_\infty$ or margin \name~for quantization. As discussed in \cref{sec:smm}, soft-min-max \name~allows us to have an asymmetric weight distribution so that it would be more effective for model compression instead of symmetric model quantization. 
%In \cref{sec:model_compression}, Soft-min-max regularization shows better accuracies than other range-restriction for model compression. 
%Also, we can see that the Soft-min-max regularization makes asymmetric weight distribution, especially for the last fc layer in Mobilenet-V2.


\subsection{Model compression}
\label{sec:model_compression}

\begin{table}[t]
\caption{Top-1 accuracies(\%) of compression using DKM with ResNet18 (RN), MobileNet-V1 (MN1) on ImageNet varying compression bit. FP32 accuracy is in \cref{table_quant_exp_ewgs} and \cref{table_quant_exp}.} % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_comp_exp}
\begin{center}
\begin{sc}
\begin{tabular}{llcccc}
\toprule
Method & $R^{2}$ & RN & MN1\\
% Method & $R^{2}$ & RN & MN1 & MN2 & MB \\
%\midrule
%32bit &   & 69.76 & 70.92\\

\midrule
% \multirow{4}{*}{DKM 1-bit}    & None        & 58.97 & 45.54 & & 61.34       \\
                        % & R\_Linf    & 59.52 & 49.74 & & 63.17 \\
\multirow{4}{*}{DKM 1-bit, 1-dim}    & Baseline        & 58.97 & 45.54\\\cmidrule{2-4}
                        & R\_Linf    & 59.52 & 49.74 \\
                        & R\_M    &  \textbf{59.70} & 47.21\\
                        & R\_SMM    & 59.27 & \textbf{52.58} \\
                        \midrule
\multirow{4}{*}{DKM 2-bit, 1-dim}    & Baseline     & 67.64 & 65.95 \\\cmidrule{2-4}
                                     & R\_Linf  & 68.53 & 67.06 \\
                                     & R\_M     & 68.33 & 67.50 \\
                                     & R\_SMM   & \textbf{68.63} & \textbf{67.62} \\
                        \midrule
\multirow{4}{*}{DKM 4-bit, 1-dim}    
                        & Baseline     & 70.22 & 69.29 \\\cmidrule{2-4}
                        & R\_Linf    & 70.34 & 69.43 \\
                        & R\_M    & 70.33 & 68.52 \\
                        & R\_SMM    & \textbf{70.52} & \textbf{69.63} \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Top-1 accuracies(\%) of compression using multi-dimensional DKM and \name~with ResNet18 (RN), MobileNet\_V1 (MN1) on ImageNet.} % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
\label{table_comp_exp_multi}
\vskip -0.15in
\begin{center}
\begin{sc}
\begin{tabular}{llcccc}
\toprule
Method & $R^{2}$ & RN & MN1 \\
%\midrule
%32bit &   & 69.76 & 70.92 \\
\midrule
\multirow{2}{*}{DKM 2-bit, 2-dim}    & Baseline        & 63.52 & 48.16 \\\cmidrule{2-4}
                        % & R\_Linf    & 64.69 & 53.91 \\
                        % & R\_M    &  \textbf{64.70} & 53.95\\
                        & R\_SMM    & \textbf{64.64} & \textbf{53.99}\\
                        \midrule
\multirow{2}{*}{DKM 4-bit, 4-dim}    & Baseline        & 64.89 & 58.55\\\cmidrule{2-4}
                        % & R\_Linf    & 65.31 & \textbf{61.40}\\
                        % & R\_M    & 65.90 & 59.70\\
                        & R\_SMM    & \textbf{66.10} & \textbf{60.05} \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}

We evaluate the effectiveness of \name~for compression with the state-of-the-art compression technique, DKM \cite{cho2021dkm}, for ResNet-18 and MobileNet-V1. The bit-dim ratio, $\frac{b}{d}$ is an important factor in the DKM algorithm which effectively defines the kind of compression a DKM palettized model would see. We ran these experiments for both scalar and vector palettization. For scalar palettization($dim=1$) we ran 1 bit, 2 bit and 4 bit compression. These experiments would yield roughly 32x, 16x and 8x compressed models respectively. \cref{table_comp_exp} shows that \name~significantly improves accuracy from original scalar palettized DKM 1 bit, 2 bit and 4 bit models. As we discussed, there is no significant difference for higher bit compression like 4 bit because many bit compression can also cover outliers even without \name.

We also expand the application of \name~to vector palettization($dim>1$) DKM \cite{cho2021dkm} as demonstrated in \cref{table_comp_exp_multi}. For these experiments, we kept the effective bit-dim ratio, $\frac{b}{d}$ equivalent to 1 so as to see variation across the most heavily compressed model which would be close to 32x compressed. Since a vector palettized model will require range constraining for all dimensions, we applied multi-dimensional \name~for all layers that would be palettized during compression. For vector palettized ResNet-18 there is an average absolute improvement of $>1\%$ using models trained with \name, and for vector palettized MobileNet-V1, the gain ranges from 5\% to 3\%. 

Finally we also validated that \name~scales to other domains as well by applying it in compressing MobileBERT \cite{sun2020mobilebert}. For Question Answering (QNLI) \cite{rajpurkar2016squad} using MobileBERT, \name~improved the performance of the model by 2\% absolute as demonstrated in Table \ref{exp_mobilebert}. Note that we applied \name~to a QNLI fine-tuning task based on a pre-trained MobileBERT \cite{wolf2020transformers}. It might be necessary to apply $R^2$ to the entire training task of MobileBERT from scratch so that \name~would have more chances to get effective weight distribution for model compression. Through these experiments across a variety of tasks(Image Classification, Question Answering etc.) we can also see that the application of the proposed \name~is task-invariant and yields solid results across domains.


\begin{table}[h]
\caption{Question-answering NLI (QNLI) accuracies of MobileBERT using single dimension DKM}
\label{exp_mobilebert}
\vskip -0.15in
\begin{center}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Method &Pre-train & 1-bit & 2-bit \\
\midrule
DKM baseline    & 90.41 & 61.34 & 80.12 \\
\midrule
DKM + R\_Linf  & 90.66 & \textbf{63.17} & \textbf{82.13}        \\
DKM + R\_M  & 89.09 & 61.80 & 80.98        \\
DKM + R\_SMM & 90.83 & 61.49 & 80.87        \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}
% \begin{table}[t]
% \caption{Sensitivity to range regularization weight for full precision model}
% \label{tab:rr_weight}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{llcc}
% \toprule
% Weight & Method & RN & MN1\\
% \midrule
% \multirow{3}{*}{0.001}  & R\_Linf    & - & - \\
%                         & R\_M    &  - & - \\
%                         & R\_SMM    & - & -\\
%                         \midrule
% \multirow{3}{*}{0.01}   & R\_Linf    & - & -\\
%                         & R\_M    & - & -\\
%                         & R\_SMM    & - & - \\
%                         \midrule
% \multirow{3}{*}{0.1}    & R\_Linf    & - & -\\
%                         & R\_M    & - & -\\
%                         & R\_SMM    & - & - \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}




\subsection{Strength of \name~\& comparison between \name}
\label{sec:model_compression}

% \begin{table}[h]
% \caption{MobileNet-V2 result varying strength of \name.}
% \label{table_strength}
% \begin{center}
% \begin{sc}
% \setlength{\tabcolsep}{5.1pt}
% \begin{tabular}{l|c|c|c|c|c}
% \toprule
% \name &\multicolumn{5}{c}{Full-precision (pre-train)}\\
%  % \midrule
% Strength & 0.1 & 0.05 & 0.01 & 0.005 & 0.001 \\
% % \midrule
% % Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
% \midrule
% R\_Linf    & 68.37 & 71.65  & 72.64  & 72.89 & 73.10\\
% R\_M    & 72.71 & 72.60 & 72.73  & 72.59 & 72.78 \\
% R\_SMM    & 71.51 & 72.50  & 72.81  & 73.01 & 72.88 \\
% \midrule
% \midrule
% \name &\multicolumn{5}{c}{SQuant 4W8A}\\
%  % \midrule
% Strength & 0.1 & 0.05 & 0.01 & 0.005 & 0.001 \\
% % \midrule
% % Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
% \midrule
% R\_Linf    & 61.45 & 66.58  &  64.02 & 65.99 & 65.85\\
% R\_M    & 65.04 & 65.75 & 67.17  & \textbf{68.57} & 67.30 \\
% R\_SMM    & 59.08 & 67.88  & 67.16  & 66.43 & 62.94 \\
% \midrule
% \midrule
% \name &\multicolumn{5}{c}{AdaRound 4W8A}\\
% Strength % \midrule
%  & 0.1 & 0.05 & 0.01 & 0.005 & 0.001 \\
% % \midrule
% % Method & FP32 & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit \\
% \midrule
% R\_Linf    & 67.75 & 70.87  & 71.76  & \textbf{72.02} & 71.83\\
% R\_M    & 71.84 & 71.83 & 71.89  & 71.85 & 71.91 \\
% R\_SMM    & 70.08 & 71.23  & 71.77  & 71.72 & 71.51 \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{center}
% \vskip -0.1in
% \end{table}


\begin{figure}[t!]
\vskip 0.0in
\begin{center}
\includegraphics[trim={9 0 12 0},clip, width=1\linewidth]{figures/r2_strength_merge.pdf}
\vskip -0.05in
\caption{MobileNet-V2 result varying strength of \name. 4bit weight and 8bit activation PTQ with AdaRound and SQuant.}
\label{fig:r2_strength}
\end{center}
\vskip -0.1in
\end{figure}

In this paper, we propose three different \name, $L_\infty$, Margin, and Soft-Min-Max. As we discussed in \cref{sec:smm} and \cref{sec:qat_result}, Soft-Min-Max \name~would be more effective for model compression as it allows to have asymmetric weight distribution while other \name~look better for symmetric quantization. To compare $L_\infty$ and Margin \name~and see impact of strength of \name, we conduct a further ablation study varying strength of \name as shown in \cref{fig:r2_strength}. While $L_\infty$ \name~shows somewhat fluctuated quantization performance with respect to its strength, quantization accuracy from a model trained with Margin \name~is more consistent and better than $L_\infty$. $L_\infty$ \name~is naive and simple to limit the weight range. Margin \name~is advanced loss from $L_\infty$ as it applies learnable margin threshold. We think that the learnable margin parameter makes more stable full-precision pre-training while $L_\infty$ always penalizes only one outlier per iteration. Therefore, Margin (as well as Soft-min-max) \name~shows more stable full-precision accuracy before quantization. We would recommend to use Margin \name~for symmetric quantization if it is hard to find proper strength for $L_\infty$ \name.