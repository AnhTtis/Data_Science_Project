\section{Related Works}
\label{related_works}

\subsection{Model compression}

The simplest and one of the most effective form of compression involves sharing weights within a layer. Deep Compression \cite{HanMD15} introduced k-means clustering based weight sharing for compression. Initially, all weights belonging to the same cluster, share weight of the cluster centroid. During forward pass, loss is calculated using the shared weights which are then updated during backward pass. This leads to a loss of accuracy and model train-ability because the weight to cluster assignment is intractable during weight update \cite{yinl2019}. DKM \cite{cho2021dkm} introduces differentiable k-means clustering, therefore making cluster assignments tractable. During forward clustered weights are used, however during backward the gradient is applied on the original weights.

\subsection{Model quantization}
Model quantization reduces the memory footprint of a model by reducing the representative bits per weight for a given model. It also quantizes activation values so that we can convert floating point computation into integer computation which gives us a benefit of hardware efficiency. In this paper we have applied our \name~with various training time quantization (quantization-aware training, QAT) algorithms like EWGS \cite{lee2021network}, LSQ \cite{esser2019learned} and DoReFa \cite{zhou2016dorefa} used in PACT \cite{choi2018pact}. PACT clips activation values with a trainable parameter for activation quantization and uses DoReFa for weight quantization. LSQ quantizes weights and activations with learnable step size (scale or bin size). EWGS applies gradient scaling with respect to position difference in between original full precision weights and quantized weights based on LSQ.

Also, we have compared our \fname~with a state-of-the-art post-training quantization (PTQ) methods. DFQ \cite{nagel2019data} equalizes per-channel weight ranges by applying per-channel scaling factors. It resolves the wide weight range problem across channels, but still the weight range would remain wide for lower bit quantization like 4bit as DFQ does not target outliers within a channel. In our experiment, models trained with our \fname~can be effectively quantized to 4bit weight / 8bit activation by PTQ without DFQ. AdaRound \cite{nagel2020up} proposed adaptive rounding for quantization bin assignment instead of nearest rounding. Pre-trained models with \fname~also show better quantization accuracies with AdaRound than models trained with just L2 norm which is well-known regularization. SQuant \cite{guo2022squant} decomposes a layer by the Hessian-based optimization objective into three diagonal sub-items, element-wise, kernel-wise, and output channel-wise. It then compose the sub-items in a quantized domain with respect to constrained absolute some of error. PD-Quant \cite{liu2023pd} quantizes weights by comparing model prediction result before and after quantization of each layer.

In our extensible experiments, we show our \fname~improves accuracies with cutting-edge QAT and PTQ for lower bit quantization like 2bit weight / 2bit activation and 4bit weight / 8bit activation.

\subsection{Regularization for quantization}
Regularization is a well-known technique for over-fitting. But some research works used the regularization concept in the context of quantization. \cite{kure} show that uniform distribution of weights are more robust to quantization than normally-distributed weights. To this they propose KURE (KUrtosis REgularization) to minimize the kurtosis of weights and ensure a uniform distribution. This method is independent of the quantization bit-width, therefore supports PTQ (Post-Training Quantization) in addition to QAT (Quantization Aware Training). However, this method is best suited for generic models which need to be specifically tuned for a given bit precision use case. To reduce the accuracy drop due to quantization \cite{binregularization} proposes to constrain weights to predefined bins based on the quantization bit-width. However, selecting these bins is a difficult process and the output of the models in very sensitive to the bin selection. In addition to that these methods ignore the effect of quantizing the first and last layers of the models.

A key difference between \name~and the existing regularization research for quantization is that other works does not explicitly consider the outliers nor weight ranges in a pre-trained model. Therefore, as shown in \cref{fig:weight_dist_mnv2}, the model trained with KURE still have wider weight ranges than models trained with \name~so that a quantization model from KURE would have a larger bin size which leads to bigger quantization error. Therefore, in our comparison experiment, our \name~shows noticeable improvement from KURE. 