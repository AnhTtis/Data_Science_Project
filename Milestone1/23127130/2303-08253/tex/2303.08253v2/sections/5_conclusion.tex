\section{Conclusion}
In this paper, we introduced \fname~as an effective technique to get rid of outliers in the weight distribution during training. This serves as a good initialization for state of the art post training quantization, quantization aware training and compression strategies, therefore can be coupled with any of them. This helps to augment the accuracy gained from such techniques and is invariant to the quantization or compression algorithm. With the three proposed formulations of \name~we wanted to demonstrate that there can multiple approaches of defining \name~and the final metric depends on these formulations. We also demonstrated how the proposed method converts a wide weight range distribution to a more densely-packed distribution for model quantization. While full-precision accuracy with \name~can be slightly regressed as it penalizes outlier weights, \name~significantly improves quantization and compression accuracy, especially for lower bit.
% While being beneficial to model compression and quantization range regularization is sensitive to parameters like range regularization weight, selection of initial margin width for margin range regularization and selection of $\alpha$ for soft-min-max regularization. 
%We also observe that for some quantization techniques like EWGS the incremental benefit of applying range regularization is marginal especially at higher bits.
\section{Impact Statement}
Deep learning models play a pivotal role in multiple applications today. With time these models have become larger and larger in search of higher accuracy. However, deploying such models on edge devices is challenging because of their size. We introduce a simple yet effective algorithm to train models suitable for ultra low bit quantization and compression (e.g., 1 and 2 bit). We hope that the community will adopt this method to create compute-efficient models, resulting in reduced deployment costs. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.