

\section{Introduction}
\begin{figure*}[!h]
\centering
\begin{minipage}{0.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/exmp_weight_dist.pdf}
  %\captionof{figure}{A figure}
  
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/r2_concept.pdf}
  %\captionof{figure}{Another figure}
\end{minipage}
\vskip -0.2in
\caption{(Left) Example weight distribution for weight quantization with three bins. (Right) Expected benefit of \name~for low bit quantization/compression.}
\label{fig_weight_dist}
\vskip -0.1in
\end{figure*}

Deep neural networks have become popular in human computer interaction, anomaly detection, financial markets, etc. Since a lot of applications running these models run on edge devices, running these compute-intensive models requires a balance of accuracy, latency, power efficiency and size for these models. 

Quantization is an effective way of reducing the power, latency, and size of neural networks. This requires that these quantized models are executed on specialized platforms with low-bit supports and at the cost of accuracy drop \cite{hawq,haq}. Post-training quantization involves distributing all the available weights in a layer into bins spread equally apart across the range. Quantization-aware training techniques ~\cite{bengio2013estimating,zhou2016dorefa}, use stochastic gradient descent to quantize and optimize the weights (i.e., mapping each floating point number to a set of discrete bins according to the precision target). Quantization bit-resolution is inversely proportional to the range of weights and affects accuracy of the quantized models. Since outliers tend to increase range, outliers are detrimental for quantization friendly models. 

As an  example, lets assume we want to quantize the weight distributions shown in \cref{fig_weight_dist} (left) into 3 bins. For the original distribution in black most of the weights will be quantized to zero and the model accuracy would drop significantly. This problem gets worse for low bit quantization and compression such as one or two bit quantization.

% The accuracy degradation can be more significant with post-training quantization on weights for large models \cite{bengio2013estimating}. Therefore, training time quantization techniques are preferred over post-training approaches~\cite{bengio2013estimating,zhou2016dorefa}, which allow usage of stochastic gradient descent to quantize and optimize the weights (i.e., mapping each floating point number to a set of discrete bins according to the precision target). Since these bins are decided very simply on the basis of the weight range, the range ends up affecting the performance of quantization itself. 

% and activations


% while taking a minimal loss in accuracy by running them on specialized hardware that is optimized for low numerical precision operations. However, applying post training quantization on weights 


% and activations causes a massive regression in accuracy for large models. 


% \begin{figure}[t]
% \vskip 0.0in
% \begin{center}
% \includegraphics[width=0.9\linewidth]{figures/exmp_weight_dist_r2.pdf}
% \vskip -0.2in
% \caption{Example weight distribution for weight quantization with three bins. Black: without a range regularizer and \textcolor{red}{Red}: with a range regularizer.}
% \label{fig_weight_dist}
% \end{center}
% \vskip -0.0in
% \end{figure}


% \begin{figure}[t]
% \vskip 0.0in
% \begin{center}
% \includegraphics[width=1.0\linewidth]{figures/r2_concept.pdf}
% \vskip -0.2in
% \caption{Expected benefit of \name~for low bit quantization/compression.}
% \label{fig_concept}
% \end{center}
% \vskip -0.0in
% \end{figure}
We introduce \textbf{\fname~(\name)}, a simple yet powerful method that helps to remove outliers during training without severely affecting full precision accuracy and provides a quantization or compression friendly checkpoint. Using \name~we intend to trim the edges of the black distribution and convert it to a distribution similar to the one shown in red in \cref{fig_weight_dist} (left). The expected behavior of \name~is that it possibly would regress full-precision model's accuracy slightly as it works as additional weight regularization like weight decay. However, the full-precision model trained with \name~would have a quantization/compression friendly weight distribution removing outlier weights, so lower bit quantization accuracy can be improved as shown in \cref{fig_weight_dist} (right). In case of higher bit quantization such as 4bit or 8bit, a model might already have enough bits to properly represent weights even including outliers. Therefore the benefit of \name~could be limited. Also as state-of-the-art techniques show reasonable accuracy at higher bit quantization at marginal accuracy regression from full-precision models, there is not much room for improvement at higher bit quantization/compression.

We propose three different formulations of \name~and through experiments show that models trained with them are more quantization and compression friendly. $L_\infty$~\name~is simple and intuitive to penalize the outlier weights during pre-training. Margin \name~is an extension of $L_\infty$. It penalizes weights which are larger than a margin, while minimizing the width of the margin. Soft-Min-Max \name~smoothly penalizes not only outliers but also near-
outlier weights. These three \name~can be used in different cases considering their characteristics. $L_\infty$ and Margin \name~would be more effective to symmetric quantization as they push to a symmetric weight distribution. Soft-Min-Max \name~could be better for compression than others as it makes asymmetric weight distribution and smoothly regularizes weights with a differentiable parameter.

We show that \name~works well with state-of-the-art quantization and compression algorithms by conducting accuracy evaluation with many different quantization and compression approaches such as DFQ\cite{nagel2019data}, AdaRound\cite{nagel2020up}, SQuant\cite{guo2022squant}, PD-Quant\cite{liu2023pd}, PACT\cite{choi2018pact}, LSQ\cite{esser2019learned}, EWGS\cite{lee2021network}, and DKM\cite{cho2021dkm} with various models like MobileNet-V1, MobileNet-V2, ResNet18, ResNet50, ResNet101, and MobileBERT\cite{sun2020mobilebert}.

%invariant to the final quantization or compression algorithm.

Our experiments on MobileNet-V1 \cite{howard2017mobilenets} and MobileNet-V2 \cite{sandler2018mobilenetv2} using post training quantization techniques like DFQ \cite{nagel2019data} shows $>$ 10\% improvement using \name~trained models. Even with newer methods like SQuant \cite{guo2022squant} we observe an improvement of $>$ 5\%. \name~ also shows similar gain in accuracy for PTQ methods (Adaround, SQuant, PD-Quant \cite{nagel2020up,guo2022squant,liu2023pd}) for larger models like ResNet-50 and ResNet-101 \cite{he2016deep}. For QAT methods like EWGS \cite{lee2021network} $R^2$ trained models are roughly 4\% better than the ones trained without \name~for 2bit weight and activation quantization. For models compressed using 32x compression, \name~improves the accuracy of parameter constrained models like MobileNet-V1 by 5\% (2-bit,2-dim). We have also extended  \name~to fine-tuning MobileBERT on QNLI task, where we see an absolute 2\% improvement in  accuracies for 1-bit and 2-bit compression.

% In state-of-the-art quantization works, statistical approaches are used to initialize a bin size during quantization-aware training. They clip outlier weights if they are too small or too large assuming the weight distribution would follow the normal distribution ~\cite{banner2019post, esser2019learned, lee2021network}. But, the model weights can have a long-tail distribution, therefore, statistical initialization would not be effective. Also the clip function is not differentiable, so most of quantization works approximate its gradient ~\cite{bengio2013estimating}. However, this approximation makes quantization unstable with gradient explosion ~\cite{sakr2022optimal}. DFQ ~\cite{nagel2019data} tackles the wide weight range problem from the outliers by equalizing weight range across channels with per-channel scaling factors, but still it is hard to deal with within-channel outliers.

% Model compression using techniques like Deep Compression \cite{HanMD15} and DKM \cite{cho2021dkm} can also get compromised in accuracy due to outliers in weights. This is because the centroids computed in both these methods by the clustering algorithm will get skewed due to the presence of the outliers. 

% We hypothesize that pre-trained models with fewer outliers in weights will yield better quantized or compressed models. Therefore, in this paper, we propose:
% \begin{itemize}
% \item
% {range-restriction ($R^2$), a concept to ensure that the learnt layer weights do not have any outliers during the pre-training stage (i.e, better weight distribution), which allows model compression and quantization techniques to effectively optimize the pre-trained network.}
% \item
% {3 different techniques to perform range regularization and show the characteristics of weight distributions generated by each of them}.
% \item
% {We use $R^2$ for training ResNet-18 \cite{he2016deep}, MobileNetV1 \cite{howard2017mobilenets}, and MobileNetV2 \cite{sandler2018mobilenetv2} and then use the trained model to improve the accuracy of the quantized (2bit, 4 bit) and compressed models (1bit, 2bit, 4bit) with state of the art quantization and compression algorithms. We compare quantize/compress accuracies applying the state of the arts for the entire model including the first and last layer.}
%\item
%{We use $R^2$ for training ResNet-18 and MobileNetV1 and then use the trained model to improve the accuracy of multi-dimensional compressed (2bit-2dim, 4bit-4dim) models.}
%\item
%{We also apply $R^2$ on MobileBERT \cite{sun2020mobilebert} for Question-answering NLI \cite{rajpurkar2016squad} to show that the proposed method scales across tasks.}
% \end{itemize}
% During the pre-training process, $R^2$ only affects the outliers in weights by penalizing them while maintaining accuracy of the full precision model. This property differentiates range regularization from $L_1$ or $L_2$ regularization. 


% In this work, we propose two new range regularization techniques,  margin and soft-min-max regularizers, and compare them with $L_\infty$ regularization.  
% $L_\infty$ regularizes the highest magnitude outlier of weights in a layer during a weight update iteration. As an extension of $L_\infty$, Margin range-regularizer penalizes weights less than or greater than thresholds. These methods ensure a symmetric distribution of weights. Soft-min-max regularization, on the other hand addresses the range of weights directly by minimizing the delta between maximum and minimum weight of a given layer. Therefore, this regularization provides a flexibility of having skewed weight distributions which might help in compression.


