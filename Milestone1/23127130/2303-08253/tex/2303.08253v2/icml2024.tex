%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
% \usepackage[capitalize,noabbrev]{cleveref}                                                      
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[flushleft]{threeparttable}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
% \usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\edit}[1]{\textcolor{blue}{#1}}
\newcommand{\remove}[1]{\textcolor{red}{#1}}
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\def\name{$R^2$ loss}
\def\fname{Range Restriction Loss}


\icmltitlerunning{R2 Loss: Range Restriction Loss for Model Compression and Quantization}

\begin{document}

\twocolumn[
\icmltitle{$R^2$ Loss: Range Restriction Loss for Model Compression and Quantization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Arnav Kundu}{equal}
\icmlauthor{Chungkuk Yoo}{equal}
\icmlauthor{Srijan Mishra}{}
\icmlauthor{Minsik Cho}{}
\icmlauthor{Saurabh Adya}{}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\centering{Apple Inc.}
% \icmlaffiliation{comp}{Apple Inc.}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Arnav Kundu}{a_kundu@apple.com}
\icmlcorrespondingauthor{Chungkuk Yoo}{ckyoo@apple.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.
%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Model quantization and compression is widely used techniques to reduce usage of computing resource at inference time. While state-of-the-art works have been achieved reasonable accuracy with higher bit such as 4bit or 8bit, but still it is challenging to quantize/compress a model further, e.g., 1bit or 2bit. To overcome the challenge, we focus on outliers in weights of a pre-trained model which disrupt effective lower bit quantization and compression. 
In this work, we propose \fname~(\name) for building lower bit quantization and compression friendly models by removing outliers from weights during pre-training. By effectively restricting range of weights, we mold the overall distribution into a tight shape to ensure high quantization bit resolution, therefore allowing model compression and quantization techniques can to utilize their limited numeric representation powers better. We introduce three different, $L_\infty$ \name, its extension Margin \name~ and a new Soft-Min-Max \name~to be used as an auxiliary loss during full-precision model training. These \name~can be used in different cases such as $L_\infty$ and Margin \name~would be effective for symmetric quantization, while Soft-Min-Max \name~shows better performance for model compression. In our experiment, \name~improves lower bit quantization accuracy with state-of-the-art post-training quantization (PTQ), quantization-aware training (QAT), and model compression techniques. With \name, MobileNet-V2 2bit weight and 8bit activation PTQ, MobileNet-V1 2bit weight and activation QAT, ResNet18 1bit weight compression are improved to 59.49\% from 50.66\%, 59.05\% from 55.96\%, and 52.58\% from 45.54\%, respectively.

% Weight range restriction techniques like L1 and L2 regularization helps is a widely used technique to improve generalization, but, it can also be used to shape the weight distributions for various purposes.  
% \remove{Model parameter regularization is a widely used technique to improve generalization, but, it can also be used to shape the weight distributions for various purposes. }

%generalizes well for post training quantization, quantization aware training methods like EWGS and compression techniques like DKM. Coupled with state-of-the-art quantization and compression techniques, models trained with $R^2$ perform better on an average, specifically at lower bit weights with 16x compression ratio. Our results show that $R^2$ generates state of the art 2-bit quantized models for heavily parameter constrained models like MobileNet V1 and V2 when coupled with EWGS. Additionally, for high compression ratio (32x), models trained with $R^2$ significantly better than the ones trained without it.
\end{abstract}

\input{sections/1_introduction.tex}
\input{sections/4_related_work.tex}
\input{sections/2_range_loss.tex}
\input{sections/3_experiment.tex}

\input{sections/5_conclusion.tex}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{icml2024}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% \begin{table}[t]
% \caption{Top-1 accuracies (\%) of MobileNet-V1 and V2 on ImageNet-1K using PTQ methods with 4bit weight and 8bit activation quantization. None: quantizing without any advanced PTQ techniques, DFQ ~\cite{nagel2019data}, AR: AdaRound ~\cite{nagel2020up}}
% \vskip 0.15in
% \begin{center}
% \begin{sc}
% \begin{tabular}{l|c|ccc|c|ccc}
% \hline
% Pre-train (FP32) & \multicolumn{4}{c|}{MobileNet-V1 0.25} & \multicolumn{4}{c}{MobileNet-V1 1.0} \\
% Method & FP32 & None & DFQ & AR & FP32 & None & DFQ & AR\\
% \midrule
% Baseline (L2-regularization) & 55.43   &  0.41 & 13.03   & 45.88 &  74.12  & 2.67 &  54.06  & 70.42 \\
% Baseline (10x L2-regularization) &  52.40  & 11.23 & 18.85   & 44.83 &  72.67  & 13.41 &  57.68  & 69.23 \\
% \midrule
% KURE &  52.83  & 3.54 &  21.87  & 48.32 &  72.50   & 53.69 &  59.21  & 60.51 \\
% \midrule
% R\_Linf - regularization &  53.48  & 13.30 & 29.10   & 50.76 & 73.65  & \textbf{61.73}  & 53.00   & \textbf{72.28} \\
% R\_M &  53.30  & \textbf{27.72} &  \textbf{31.02}  & \textbf{51.17} &  73.54  & \textbf{61.66}  &  \textbf{65.06}  & \textbf{72.29} \\
% \hline\hline
%  & \multicolumn{4}{c|}{MobileNet-V2 0.25} & \multicolumn{4}{c}{MobileNet-V2 1.0} \\
% %Regularization & None & BC & DFQ & AR & None & BC & DFQ & AR\\
% \midrule
% Baseline (L2) &  53.90  & 0.73  &  17.27  & 49.49 &  73.08  & 2.57 &  56.56  & 71.29 \\
% Baseline (10x L2) &  49.25  &  0.96 &  0.10  & 45.12 &  71.00  & 4.17 &  0.09  & 68.06 \\
% \midrule
% KURE &  52.80  & 15.98  & \textbf{27.39}   & 50.96 &  72.49  & 39.21 &  62.49  & 71.68 \\
% \midrule
% R\_Linf &  53.40  & 13.08  & 26.51   & \textbf{51.68} &  72.64  & \textbf{59.95} & 62.39   & 71.76 \\
% R\_M &  52.82  & \textbf{31.11}  &  27.00  & 51.12 &  72.73  & \textbf{60.03} &  \textbf{66.04}  & \textbf{71.89} \\
% %\midrule
% %KURE &
% %\midrule
% %R\_Linf & 7.1   & 4.2 \\
% %R\_M & 5.6   & 2.2 \\
% \bottomrule

% \end{tabular}
% \end{sc}
% \end{center}
% % DKM$^{*}$: clustering with 8 bit and 8 dimension.}
% \label{table_PTQ}
% \vskip -0.1in
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
