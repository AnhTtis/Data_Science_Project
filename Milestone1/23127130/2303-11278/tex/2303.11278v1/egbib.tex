% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage[table,xcdraw]{xcolor}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{balance}

\usepackage{diagbox}

\usepackage{amsfonts} %% added for the equation
\usepackage{algorithm} %%% added for the algorithm 
\usepackage[noend]{algpseudocode}

\input{math_commands.tex}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{lipsum}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{***} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Constructing Bayesian Pseudo-Coresets using Contrastive Divergence}

\author{Piyush Tiwary\\
Indian Institute of Science\\
Bangalore, India\\
{\tt\small piyushtiwary@iisc.ac.in}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Kumar Shubham\\
Indian Institute of Science\\
Bangalore, India\\
{\tt\small shubhamkuma3@iisc.ac.in}
\and
Vivek Kashyap\\
Bangalore Institute of Technology\\
Bangalore, India\\
{\tt\small vivekvkashyap10@gmail.com}
\and
Prathosh A.P.\\
Indian Institute of Science\\
Bangalore, India\\
{\tt\small prathosh@iisc.ac.in}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Bayesian Pseudo-Coreset (BPC) and Dataset Condensation are two parallel streams of work that construct a synthetic set such that, a model trained independently on this synthetic set, yields the same performance as training on the original training set. While dataset condensation methods use non-bayesian, heuristic ways to construct such a synthetic set, BPC methods take a bayesian approach and formulate the problem as divergence minimization between posteriors associated with original data and synthetic data. However, BPC methods generally rely on distributional assumptions on these posteriors which makes them less flexible and hinders their performance.  In this work, we propose to solve these issues by modeling the posterior associated with synthetic data by an energy-based distribution. We derive a contrastive-divergence-like loss function to learn the synthetic set and show a simple and efficient way to estimate this loss. Further, we perform rigorous experiments pertaining to the proposed method. Our experiments on multiple datasets show that the proposed method not only outperforms previous BPC methods but also gives performance comparable to dataset condensation counterparts.
\end{abstract}

%%%%%%%%% BODY TEXT

\section{Introduction}


Modern deep learning models have shown impressive performance on a variety of applications such as computer vision, natural language processing, and speech processing \cite{krizhevsky2017imagenet,devlin2018bert,amodei2016deep,he2016deep,dosovitskiy2020image,radford2021learning}.
Large training datasets and heavy computational infrastructure have played a pivotal role in achieving such performance. Moreover, the availability of large training datasets is critical to improving the performance of these models. This reliance on large datasets not only increases the computational complexity required to train these models but also adds to the total training time required to achieve acceptable accuracy. Typically, training time for larger datasets requires hundreds of GPU hours leading to enormous amounts of carbon emission which has adverse environmental impacts~\cite{automata}.

There have been several attempts by researchers to reduce the reliance on large datasets. A naive approach to overcome this issue could be to randomly sample a subset from the original dataset and use it for training. However, such subsets need not necessarily reflect the diversity and information captured by original datasets.
 % To address these issues, researchers have proposed various techniques [REF] to reduce reliance on such large datasets. 
Another common approach has been that of Core-Subset or `Coreset' selection \cite{welling2009herding, castro_herding, icarl_herding,scail_herding,sener2017active,farahani2009facility}, which attempts to sample a small (\textit{informative}) subset of the original data that can produce results comparable to the original dataset. However, finding an optimal solution to such a problem is NP-hard and often results in subpar performance. Further, it is shown that coreset methods scale poorly with the dimension of dataset leading to suboptimal subsets in higher dimensions~\cite{BPC_OG}.

To address these issues, \cite{BPC_OG} proposed  `Bayesian Psuedo-Coreset' (BPC), a new technique for generating synthetic images that can scale to high-dimensional datasets. The general idea of BPC is to use gradient based optimization to reduce the divergence between the parameter posterior of the original dataset and the parameter posterior associated with the synthetic dataset. 
% Further, \cite{BPC} analyzed various \cite{BPC} further proposed new divergence metric that can be used to minimze the gap between posteriro of synthetic data and real dataset.

A parallel and closely related approach with same motivation is that of dataset condensation~\cite{GM,DSA,DM,KIP,CAFE,MTT,FREPO}. The goal of these methods is same as that of bayesian pseudo-coreset. However, these methods take a non-bayesian approach to generate the synthetic dataset by optimizing heuristic objectives based on features~\cite{DM,CAFE}, gradients~\cite{GM}, training trajectory~\cite{MTT}, and performance matching~\cite{KIP,FREPO} between synthetic data samples and the original data points. 
% These methods attempt to generate synthetic samples that, when used to train the network, produce similar results as the original dataset. Compared to bayesian psuedocoreset, these methods takes a non bayesian approach to generate these synthetic dataset by optimisizing different objectives based on features [REF], gradient [REF], trajectory [REF], and performance matching [REF] between synthetic data samples and the original data points. 

Recently,~\cite{BPC} tried to bridge the gap between bayesian pseudo-coreset (BPC) methods and dataset condensation methods by analyzing various divergence metrics. In particular, they showed that under certain assumptions, dataset condensation methods can be seen as special cases of BPC methods with different divergence metrics.
However, on high dimensional datasets such as images, dataset condensation methods often  outperform the performance of BPC methods. 
% This decline in BPC performance is frequently attributed to the lack of flexibility provided by current approaches. 
% Many current methods [REF] approximate the posterior using some exponential distribution [REF], limiting their ability to approximate complex posterior distributions. 

% Different studies have further explored multiple application of dataset distillation methods. [REF] have shown that dataset distillation technique can be used for task like continual learning to avoid catastrophic forgetting in the learning process. Compared to traditional method of saving multiple instance of training data, dataset distillation provides a method to save the most representative sample for the training process. Similarly, [REF] have used dataset distillation technique to speed up the experiments in a neural architecture search based methods which generally requires experiments on different datasets and multiple architectures. Another common application of dataset distillation has been in the field of privacy protection where [REF] have shown the capability of dataset distillation for privacy protection. Similar approaches has also been explored in the field of medical [REF] recommender system [REF] and  fashion [REF]. 

We argue that this drop in performance is attributed to the stringent form of parameter posterior of original dataset used by previous methods. 
In this work, we relax this condition and propose a more flexible framework to work with such posteriors. In particular, we don't assume any form for parameter posterior associated with original dataset and use an energy-based distribution to model the posterior associated with synthetic data. Then, we derive contrastive divergence like loss function required to minimize the forward KL-Divergence between these posteriors. Our method allows flexibility to use various energy function without worrying about parameter posterior of original dataset. We experimentally observe that our method not only outperforms bayesian pseudo-coreset methods, but also gives performance comparable with that of dataset condensation methods. To the best of our knowledge, this is the first work that bridges the performance gap between BPC and dataset condensation methods.   
% In our work, we approximate the posterior associated with synthetic and real datasets with energy-based models (EBMs). Such an approach provides a flexibility to approximate given problem with different energy function that can efficiently model the complex posteriro associate with the synthetic and real dataset in bayesian psuedocoreset. We also provide a theoretical support for of our approach. Through our work we attempted to bridge the gap between traditional bayesian psuedocoreset methods and dataset distillation approaches. Our results are comparable to current state-of-the art methods in dataset distillation.. 
Our contributions can be summarized as follows :
\begin{itemize}
    \item We propose a flexible framework for construction of bayesian pseudo-coreset where we don't assume any form for parameter posterior associated with original dataset and use a energy-based distribution to model the posterior associated with synthetic data. 
    \item Our method allows one to use various energy functions to model the parameter posterior associated with synthetic data without having to worry about posterior associated with real data.
    \item We derive a contrastive-divergence like loss function to minimize the forward KL divergence between the posteriors; further, we also show a simple and efficient way to estimate this loss and learn the pseudo-coreset.
    \item We rigorously test our method against state-of-the-art BPC methods as well as dataset condensation methods. We observe that our method not only outperforms BPC methods but also gives performance comparable to that of dataset condensation, hence, bridging the performance gap between the two paradigms.  
    % \item  We propose a new formulation for dataset distillation using energy based models. Our formulation provides flexibility to the researcher to experiment with different energy function based on the architecture and data choice.
    % \item  Our method bridges the performance gap between dataset condensation methods and bayesian psuedocoreset.
    % \item We provide theoretical justification of our proposed formulation and have formulated a bayesian view of dataset condensation technique. Contrary to other formulation which tries to genrerate a single point estimate of parameters we matches the posterior distribution of parameters for both real and synthetic dataset.  
\end{itemize}



\section{Related Work}
\label{sec:related_works}

\subsection{Coreset}
\label{sec:coreset}
The idea of using less amount of data to achieve performance comparable to that obtained by using the entire dataset was first manifested by Coresets~\cite{welling2009herding, castro_herding, icarl_herding,scail_herding,sener2017active,farahani2009facility}. The underlying idea of coreset methods is to select a subset of the training dataset that can achieve performance comparable to the original dataset. Herding based coreset methods~\cite{welling2009herding, castro_herding, icarl_herding,scail_herding} select  such samples by minimizing the distance between the feature centroid for the coreset and the feature centroid for the complete dataset. \cite{welling2009herding} proposed a greedy technique for constructing such a coreset by progressively and greedily adding data points that reduce the distance between these centroids. This strategy ensures that the most representative samples of the entire dataset are included in the subset, which is then used to train the model. However, herding based coresets often fails to sample a diverse group of data points, which impacts generalization capability of the model. In contrast to herding-based methods, K-center based coreset techniques~\cite{sener2017active,farahani2009facility} pick the most diverse and representative samples by optimizing a minimax facility location-based submodular function~\cite{farahani2009facility}. Such method have also been explored in generative models to select the most representative sample during the learning process~\cite{small_gan}. Contrary to K-center and herding based coreset selection methods, forgetting based coreset~\cite{Forgetting} removes the easily forgettable samples from the training dataset. This ensures that the coreset sampling process considers the uncertainty associated with the given model. Apart from the given methods, dataset subset selection has been explored in other fields like continual learning~\cite{coresets_continual}, active learning~\cite{active2022clinical} and hyperparameter optimization~\cite{automata}.
% The coreset or dataset selection method is a special technique for reducing the amount of data required to train a given neural network. The main idea is to select a subset of the training dataset that can generate accuracy comparable to the original dataset. However, finding such a subset from the given training dataset is an NP-hard problem. Various studies have attempted to address this issue using simple heuristic methods. For example, \cite{welling2009herding} searches greedily for subset of data points whose feature centroid matches the original dataset. Similarly, \cite{mirzasoleiman2020coresets} proposes a technique for selecting a subset by minimising the gradient difference between the original dataset and the given subset.

\subsection{Dataset Condensation}
\label{sec:DC}
While coreset methods select a diverse and rich subset of data points from the training set, the basic heuristics used for subset selection often produce suboptimal results. To overcome this barrier~\cite{DD} proposed Dataset Distillation to `learn' a set of informative synthetic samples from a large dataset. Rather than selecting a subset of datapoints from the training set, these methods create an artificial dataset with a lower cardinality that, when trained independently, produces accuracy comparable to the original model.

In particular, the authors of~\cite{DD} learn a model by using the synthetic set and test the model on original dataset. The synthetic set is optimized to improve the performance of the model on the original dataset.\cite{kim2022dataset} further extended this idea by generating multiple synthetic dataset for training but with limited storage capacity. Inspired by these works,~\cite{GM} proposed gradient matching where the authors proposed to learn the synthetic samples by aligning the gradients of models trained using original dataset and the synthetic dataset. Despite the simplicity of gradient matching, it treats the gradients of each class independently, hence neglecting the class-discriminative features. To ensure that efficient gradients are calculated during the training process, some studies~\cite{DD_Survey, DC_CS,jiang2022delving} have proposed different gradient comparison metrics that penalise the model for overfitting on the small dataset. Further,~\cite{DSA} proposed differentiable siamese augmentation to avoid overfitting and boost the performance of any dataset condensation method in general.  
% Rather than selecting a subset of datapoints from the training dataset, dataset condensation methods create an artificial dataset with a lower cardinality that, when trained independently, produces accuracy comparable to the original model.
% To generate these synthetic images, various methods have recently been proposed, such as the gradient matching [REF], trajectory matching  [REF], performance matching [REF], and distribution matching methods [REF].
% Gradient matching methods [REF] produce the condensed dataset by aligning the gradients of the synthetic dataset with the gradients of the original dataset.
% Despite the approach's simplicity, the small size of the synthetic dataset frequently leads to overfitting during the training process. To ensure that efficient gradients are calculated during the training process, various studies [REF] have proposed different gradient comparison metrics that penalise the model for overfitting on the small dataset [REF]. Similarly, researchers proposed differentiable semantic augmentation (DSA) [REF] to efficiently use the differentiable augmentation technique on a small sample dataset during training. 
% \subsection{Trajectory matching}

Further, methods like~\cite{DD,Rem_Past} attempt to directly optimize the validation loss over samples of original dataset using the optimal parameters generated by the synthetic dataset. The validation loss is backpropagated through the unrolled gradient descent steps used for finding the optimal parameters. The overall idea is similar to that of meta-learning~\cite{MAML}, where a bi-level optimization is performed to optimize (outer-loop) the meta-test loss in the original dataset using a model that has been meta-trained (inner-loop) on synthetic data. However, the computation graph for such methods increases with the number of steps in the inner-loop, which is computationally expensive and requires large GPU memory.  
To overcome this bottleneck, there have been methods that try to reduce the computational burden due  to inner-loop. For eg.~\cite{KIP} proposes to approximate the inner-loop optimization by a convex optimization problem which can be solved using kernel ridge regression (KRR). \cite{KIP} uses a neural tangent kernel (NTK)~\cite{NTK} to perform KRR. Further, to reduce the complexity involved in computing NTK,~\cite{RFAD} uses a neural network gaussian process kernel instead. Further,~\cite{FREPO} proposes to decompose the network optimized in the inner-loop into a feature-extractor and a linear classifier; they show that it is enough to update the linear classifier in the inner-loop while the feature extractor can be updated on synthetic set separately.   

Contrary to bi-level optimization approaches that focus on short-horizon inner-loop trajectories,~\cite{MTT} proposes to focus on long-horizon trajectories, ensuring that the models learn comparable trajectories during optimization for both the synthetic set and the original dataset. To do this, the parameters of the model generated during training with synthetic data and with the original data are compared at different epochs. As a follow-up work,~\cite{DD_PP} showed that matching all the parameters has a negative impact on the performance of the model and the performance can be boosted by comparing a pruned set of parameters. Similarly, \cite{du2022minimizing} proposed a regularizer to reduce the accumulated error in the long horizon trajectories.  
However, even though the trajectory based methods show high performance on several datasets, they suffer from same issues as that of meta-learning methods, i.e, the computational graph of such methods can be very large, leading to a high GPU memory requirement.

While the above mentioned methods resort to bi-level optimization to learn the synthetic set, Distribution Matching methods~\cite{DM,CAFE,zhao2022synthesizing} try to generate a condensed synthetic set with a similar feature distribution as the original dataset, hence, completely avoiding the bi-level optimization. 
For eg.~\cite{DM} proposed to use maximal mean discrepancy (MMD) to match the distribution of the synthetic dataset with the original dataset using the classifier layer without the last linear layer. \cite{CAFE} further proposed CAFE to match feature statistics of different layers and used a discriminator to maximise the probability for a given class.

 

 % Unlike gradient matching methods, which ensure that the model generates similar gradients for both synthetic and real datasets, trajectory matching methods ensure that the model learns similar trajectories during SGD-based optimization [REF]. For this, the actual parameters of the model generated during training with synthetic data and with the original data are compared at different epochs. As follow-up work, [REF] showed that matching all the parameters has a negative impoact on the performance of the model and the performance increment can also be generated by comparing a pruned set of parameters. [REF] proposed to use  neural network gaussian process kernel for the given task. While~\cite{FREPO} proposed FREPO in which the training  process of KIP  was decomposed into linear classifier and a feature extractor learning.

% \subsection {Performance matching}
% Performance matching approaches [REF], in contrast to trajectory matching or gradient matching algorithms, attempt to directly optimise the validation loss over a sampled dataset using the optimal parameters generated by the synthetic dataset. To modify the synthetic dataset, this loss is backpropagated through the unrolled SGD. The overall implementation is similar to meta-learning, where a bi-level optimization is performed to optimise the meta-test loss in the original dataset using a model that has been meta-trained on synthetic data. [REF] proposed to use kernel ridge regression to perform a convex optimization to calculate the optimal parameters associated with the synthetic dataset using a neural tangent kernel (NTK) [REF] for the optimization of the validation loss. [REF] proposed to use  neural network gaussian process kernel [REF] for the given task. While [REF] proposed FREPO in which the training  process of KIP  was decomposed into linear classifier and a feature extractor learning.

% \subsection{Distribution matching}
% Distribution matching methods try to generate a condensed synthetic dataset with a similar feature distribution as the original dataset. [REF] proposed to use maximal mean discrepancy (MMD) [REF] to match the distribution of the synthetic dataset with the original dataset using the classifier layer without the last linear layer. [REF] further proposed CAFE to match feature statistics of different layers and used a discriminator to maximise the probability for a given class. 

\subsection{Bayesian Psuedo-Coreset}
\label{sec:BPC}
Recently, there has been a surge in methods that try to re-interpret the existing deep learning methods from a bayesian perspective. For eg.~\cite{BLR} showed that most of the machine learning algorithms and practices are special instances of a generic algorithm, namely, the Bayesian Learning Rule. Following this line of thought,~\cite{BPC_OG} proposed a bayesian perspective to dataset condensation. Particularly, they formulate dataset condensation as a divergence minimization problem between the parameter posteriors associated with synthetic set and the original dataset. The synthetic set obtained using such methods is termed as `Bayesian Pseudo-Coreset' (BPC).  Compared to corsets, these methods scale more efficiently with data dimensions and achieve a better posterior approximation. Furthermore, a bayesian formulation of the given problem further enhances the understanding of the given field.

The main idea proposed in BPC-methods is to minimize the divergence between the posterior associated with psuedocorest and the original training dataset. \cite{BPC_OG} formalized the given problem by minimising the reverse-KL divergence between posterior of real data with the posterior of synthetic data. On similar lines,~\cite{BPC} demonstrated that other divergence metric, such as wasserstein distance and forward-KL divergence, can also be used  to generate comparable accuracy. Further,~\cite{BPC} showed that under certain assumptions, dataset condensation methods can be looked into as special instances of BPC-methods. For eg. MTT~\cite{MTT} can be shown as BPC-method with wasserstein distance as the divergence metric. Similarly, gradient matching~\cite{GM} can be shown as a BPC-method under reverse-KL divergence as the divergence metric. Further,~\cite{BPC} proposes to use forward  KL divergence as the divergence metric as it encourages the model to cover the entire target distribution. However, despite the theoretical support for these methods, there are lot of stringent assumptions involved in formulation of posteriors associated with synthetic set and original dataset.
Moreover, there is still a significant performance gap between BPC methods and dataset condensation methods, which we have attempted to bridge through our work.  
% Bayesian psuedo coreset (BPC) is a special class of algorithms that tries to generate the synthetic dataset by minimising the posterior divergence between psuedocoreset and the original dataset. Compared to the corset, these methods scale more efficiently with data dimensions and achieve a better posterior approximation. Furthermore, a Bayesian formulation of the given problem further enhances the understanding of the given field.

% The main idea proposed in BPC is to minimise the divergence between the posterior associated with psuedocorest and the original training dataset. [REF] formalised the given problem by minimising the reverse KL divergence between posteror of real data with the posterior of synthetic data. In the similar line, [REF] demonstrated that other divergence metric, such as wasserstine distance and forward KL divergence, can also be used  to generate comparable accuracy. However, despite the theoretical support for these methods, there is still a significant performance gap between BPC and dataset distillation methods, which we have attempted to bridge through our work.  

\subsection{Energy Based Models}
\label{sec:EBM}
Energy Based Models or EBMs are a class of density estimation models that assume the required density to have the form of an energy-based distribution. In particular, the desired density ($p(x)$) is approximated by a parametric density of the form $p_{\theta}(x) = \exp{(-E_\theta(x))}/Z_\theta$ where $E_\theta(\cdot)$ is the negative log of un-normalized density also called the energy function and $Z_\theta = \int_{x} \exp{(-E_\theta(x))} dx$ is the normalizing constant also known as the partition function. Generally, the goal of EBMs is to learn the parameters $\theta$ of the energy function that minimizes the KL divergence between the desired density and $p_\theta(x)$. There have been several lines of work that try to train the EBMs efficiently~\cite{IGEBM,du2020improved,BAA,GEBMS} however, the most simple and commonly used approach is the one proposed in~\cite{IGEBM}. Particularly, the contrastive-divergence loss used to learn the parameters of the energy function as shown in~\cite{CD} is given by:
\begin{align}
    \gL = \E_{x^+ \sim p(x)}[E_\theta(x^+)] - \E_{x^- \sim p_\theta(x)}[E_\theta(x^-)]
\label{eq:CD}
\end{align}
The above loss function ensures that the model learns to assign low energy to the samples associated with the real data points and high energy to samples obtained from the parametric density.
The first expectation in above expression is approximated using the samples present in the dataset. However, we cannot directly get samples from $p_\theta(\cdot)$ to approximate the second expression, as the partition function is intractable and sampling would be very inefficient. To overcome this bottleneck, we resort to gradient-based markov-chain monte-carlo (MCMC) methods such as langevin dynamics to sample from $p_\theta(\cdot)$. 

In the following work, we approximate the parameter posterior associated with synthetic set by an energy-based distribution, however, in our case the goal is not to learn the parameters of the energy function but to learn the synthetic set itself. Hence, we fix the energy function and derive a loss function (similar to contrastive-divergence) to learn the synthetic samples instead. The rest of the paper is organized as follows: In Section~\ref{sec:overview}, we setup the notations and formalize our problem statement, in Section~\ref{sec:formulation} we derive the loss function used to construct the pseudo-coreset and describe the proposed method. In Section~\ref{sec:implementation}, we provide the implementation details of the proposed method. Lastly, in Section~\ref{sec:experiments}, we present our experimental findings and compare it with previous baselines.
% \textcolor{red}{write the structure of the paper once it is done.} 
% Our work is inspired by [REF], which first proposed a method to fit a probabilistic model over the data using an energy-based models(EBM). Several follow up works [REF] have demonstrated that EBM can be used to model the distribution of tasks such as classifiers [REF] and genereative models [REF]  to model the robustness and uncertainty associated with them. Similarily,  energy-based model has also been used to model compositional structure in data [REF] and for continual learning tasks [REF].


% EBM uses an energy function to assign an unnormalized scalar probability value to a given input configuration to model the joint probability associated with that input. In general, the choice of function varies from application to application.  While for generative tasks like image generation, a neural network is used to model the energy associated with a given sample. Recent works like [REF] has demonstrated that the final logit layers can be used as a proxy for the energy of a given sample. Similar approximations have been proposed for models like generative adversarial networks (GANs) [REF].

% The general training paradigm of the energy-based models involves minimization of contrastive divergence [REF] between original data and sampled data points. Such a comparison ensures that the model learns to assign low energy to the samples associated with the real data points. The sampled data points are generated by minimising the approximated energy using lengevine dynamics [REF] . Once an appropriate data point is sampled from the given low energy regime, the contrastive divergence metric is used to train the energy function.






\section{ Proposed Method} 
\label{sec:proposed_method}

\subsection{Overview}
\label{sec:overview}
Consider the training  set $\gD = \{(\rvx_i,y_i)\}^{\mid \gD\mid}_{i=1}$ where, the cardinality of the dataset is $|\gD|$. 
% Let $\{\rvx_i\}_{i=1}^{\mid D\mid} \stackrel{iid}{\sim} \vp(x)$ be the marginal of the samples present in the dataset.
The goal of bayesian pseudo-coreset is to construct a sythetic dataset $\td{\gD} = \{\td{\rvx}_i,\tdy_i\}_{i=1}^{|\td{\gD}|}$ such that $\{y_i\}$ and $\{\tdy_i\}$ share the same label space and $|\td{\gD}| << |\gD|$; further, $\td{\gD}$ should provide classification performance that is comparable to that of original training set $\gD$.
For this, consider the space of parameters of a discriminative / classification model ($\Theta$). Now, let $\vp(\theta|\gD)$ and $\vq(\theta|\td{\gD})$ be the density of optimal parameters induced due to the original training set ($\gD$) and the synthetic set ($\td{\gD}$) respectively. In particular, let $\gF$ be the space of all classification loss functions. Now, let
\begin{align}
    \gM_\ell = \left\{ \theta\in\Theta : \underset{\theta}{\arg\min} \frac{1}{|\gD|}\sum_{i=1}^{|\gD|} \ell(y_i, f_\theta(\rvx_i)) \right\}
\end{align}
be the set of all parameters that minimize the empirical risk w.r.t some loss function $\ell(\cdot)\in\gF$ and classifier $f_\theta(\cdot)$ manifested by $\theta$. Then, $\vp(\theta|\gD)$ can be seen as the distribution induced on union of all such sets, i.e,
\begin{align}
    \underset{\ell\in\gF}{\bigcup}\gM_\ell \sim \vp(\theta|\gD)
\label{eq:pp_orig}
\end{align}
Similarly, we can define $\td{\gM}_\ell$ to be the set of parameters that minimize the empirical risk for synthetic set $\td{\gD}$, and $\vq(\theta|\td{\gD})$ can be seen as the distribution induced on union of all such sets:
\begin{align}
    \underset{\ell\in\gF}{\bigcup}\td{\gM}_\ell \sim \vq(\theta|\td{\gD})
\label{eq:pp_syn}
\end{align}
% \begin{align}
%     \left\{ \theta\in\Theta\mid \underset{\theta}{\arg\min} \frac{1}{|\gD|}\sum_{i=1}^{|\gD|} \ell(y_i, f_\theta(\rvx_i)) \right\} &\sim \vp(\theta\mid\rvx)  \label{eq:pp_orig}\\
%     \left\{ \theta\in\Theta\mid \underset{\theta}{\arg\min} \frac{1}{|\td{\gD}|}\sum_{i=1}^{|\td{\gD}|} \ell(\td{y}_i, f_\theta(\td{\rvx}_i)) \right\} &\sim \vq(\theta\mid\td{\rvx})  \label{eq:pp_syn}
% \end{align}
% where, $\ell(\cdot)$ is some loss function used for classification and $f_\theta(\cdot)$ is a classifier with parameter $\theta$.
Note that we don't know the closed-form expressions for the above densities. Previous methods~\cite{BPC,BPC_OG} assume some form for $\vp(\theta|\gD)$ and $\vq(\theta|\td{\gD})$, however, this is not desirable as these densities can be very complex and assuming  distributional form for these densities would reduce the flexibility for constructing the synthetic set of pseudo-coreset. Hence, in this work, we don't assume any form for $\vp(\theta|\gD)$, however, we assume an energy-based distribution for $\vq(\theta|\td{\gD})$, i.e, $\vq(\theta|\td{\gD}) = \exp{(-E(\theta, \td{\gD}))}/Z(\td{\gD})$. Note that, here the energy function $E(\theta, \td{\gD})$ is fixed and is not learnable, however, $\td{\gD}$ is a learnable quantity. First, we will show the loss for any generic energy function, later we analyze and discuss the effect of different choices for energy functions.

While dataset condensation methods (cf. Sec.~\ref{sec:DC}) resort to heuristics to construct the synthetic set;  bayesian pseudo-coresets explicitly minimize a divergence metric between $\vp(\theta|\gD)$ and $\vq(\theta|\td{\gD})$. In our method, we propose to construct the pseudo-coresets by solving the following optimization problem:
\begin{align}
    \td{\gD}^* = \underset{\td{\gD}}{\arg\min}\hspace{2mm} D_{KL}\left(\vp(\theta|\gD) || \vq(\theta|\td{\gD}) \right)
\end{align}
where, $D_{KL}$ is the forward-KL divergence, $\vp(\theta|\gD)$ is parameter posterior associated with original training set and $\vq(\theta|\td{\gD})$ is the parameter posterior associated with the pseudo-coreset which has a form of generic energy-based distribution.


% The posterior over model's parameter for a neural network when it is trained with given data can be represented as $p(\theta | x)$. Similarly let's suppose that when the same network is trained using synthetic dataset $\{u_i\}^{|D_{s}|}_{i=1} \sim q$ generates a posterior distribution defined as $q(\theta | u)$. The task of  Bayesian Psuedo Coreset [REF] requires generation of synthetic dataset ($ D_{s}$) called \textit{psuedocoreset}  such that the posterior of the model trained on these new dataset approximate the posterior associated with the original dataset, provided  $ |D_{s}| << |D|$.  This difference between the posterior using forward KL divergence metric [REF]  can be represented as following. 


% \begin{equation}
% \label{eqn:div}
%    \mathcal{L} =  \min_{u} D_{KL}(p(\theta|x) || q(\theta|u))  
% \end{equation}

\subsection{Problem Formulation}
\label{sec:formulation}
As mentioned in the previous section, our aim is to minimize the forward-KL divergence between $\vp(\theta|\gD)$ and $\vq(\theta|\td{\gD})$ w.r.t $\td{\gD}$, where $\vp(\theta|\gD)$ can have any distributional form and $\vq(\theta|\td{\gD}) = \exp{(-E(\theta, \td{\gD}))}/Z(\td{\gD})$. To obtain the synthetic set that minimizes the above divergence metric, we take the gradient of above expression w.r.t $\td{\gD}$:
\begin{align}
    &\nabla_{\td{\gD}} D(\vp(\theta|\gD) || \vq(\theta|\td{\gD})) = \nabla_{\td{\gD}} \underset{\vp(\theta|\gD)}{\E}\left[ \log\left(\frac{\vp(\theta|\gD)}{\vq(\theta|\td{\gD})}\right) \right] \\
    &= -\nabla_{\td{\gD}} \underset{\vp(\theta|\gD)}{\E}\left[\log\left(\frac{\exp{(-E(\theta, \td{\gD}))}}{Z(\td{\gD})}\right) \right] \\
    &= -\nabla_{\td{\gD}}\left[\int\left( -E(\theta,\td{\gD}) -\log(Z(\td{\gD})) \right)\vp(\theta|\gD) d\theta \right] \\
    &= \int \nabla_{\td{\gD}} E(\theta,\td{\gD})\vp(\theta|\gD) d\theta + \int \nabla_{\td{\gD}}\log(Z(\td{\gD}))\vp(\theta|\gD) d\theta \\
    &= \underset{\vp(\theta|\gD)}{\E}\left[\nabla_{\td{\gD}}E(\theta,\td{\gD})\right] - \underset{\vq(\theta|\td{\gD})}{\E}\left[\nabla_{\td{\gD}}E(\theta,\td{\gD})\right]
\end{align}
Taking the monte-carlo approximation of the above loss function, the final loss function comes out be:
\begin{align}
    \gL = \underset{\theta^+\sim\vp(\theta|\gD)}{\E}[E(\theta^+,\td{\gD})] - \underset{\theta^-\sim\vq(\theta|\td{\gD})}{\E}[E(\theta^-,\td{\gD})]
\label{eq:Our_CD}
\end{align}
As it can be seen, the above loss function looks similar to the contrastive-divergence loss in Eq.~(\ref{eq:CD}). However, instead of learning the parameters of an energy-based model~\cite{IGEBM}, we use it to learn the pseudo-coreset.

Now, to estimate the expectation terms in Eq.~(\ref{eq:Our_CD}) we need to sample the parameters ($\theta$) from the posteriors $\vp(\theta|\gD)$ and $\vq(\theta|\td{\gD})$. As mentioned earlier, we don't assume any distributional form for $\vp(\theta|\gD)$, hence we use the parameters obtained by training a model on the original training set ($\gD$) to approximate the first expectation. Specifically, we train a classification model with parameters $\theta$ on the training set $\gD$ with some classification loss $\ell_{\gD} \in \gF$.
Next, to evaluate the second expectation, we resort to langevin dynamics to sample $\theta$ from $\vq(\theta|\td{\gD})$. Langevin dynamics is an iterative gradient-based MCMC sampling methods given by the following update rule:
\begin{align}
    \theta^{(t+1)} = \theta^{(t)} - \frac{\alpha}{2}\nabla_{\theta^{(t)}}E(\theta,\td{\gD}) + \sqrt{\alpha}\eta, \hspace{1mm} \eta\sim\gN(0,I)
\label{eq:LD_theta}
\end{align}
From an implementation perspective, the above is similar to running a noisy gradient descent on parameters to minimize the chosen energy function. 

% In general, the posterior over the optimal parameters is difficult to calculate and is approximated using variational distributions like the Laplace distribution [REF] or the Gaussian distribution [REF]. Such an assumption about the distribution over the parameters often introduces an inherent bias into the model and limits its performance. Unlike other methods, we use an energy-based model (EBM), to estimate the conditional distribution of parameters with respect to the dataset and use the energy function to sample new parameters for the divergence calculation.          

% \begin{equation}
%     q(\theta | u) = \frac{e^{-E(\theta, u)}} {Z}
% \end{equation}


% % \subsection{Posterior Matching Using EBMs}

% \begin{equation}
%     \label{eqn:image_modify}
%     u^{i+1} = u^{i} - \nabla_u(\mathcal{L}) 
% \end{equation}

% The gradient w.r.t. (u) to minimise the given forward KL-divergence metric can be calculated using a contrastive divergence (eqn - \ref{eqn:contrast}) between the energy associated with the parameters sampled from $q(\theta|x)$ and $q(\theta|u)$ for the given sample u.

% \begin{equation}
%     \nabla_{u} \mathbb{E}_{p(\theta|x)} \left( \log\left(\frac{p(\theta|x)}{q(\theta|u)}\right)\right)
% \end{equation}

% \begin{equation}
%  \nabla_{u} \mathcal{L} =  \nabla_{u} \left(-\mathbb{E}_{p(\theta|x)} \left(\log \left(\frac{e^{-E(\theta,u)}} {Z(\theta, u)} \right) \right)\right)    
% \end{equation}

% \begin{equation}
%     = -\nabla_{u} \left[\int \left( -E(\theta,u) - \log(Z(\theta, u))  \right) p(\theta|x) d\theta \right]
% \end{equation}

% \begin{equation}
%   = \int  \nabla_u E(\theta, u) p(\theta|x) d\theta    +\int  \nabla_u log(z(\theta, u )) p(\theta|x) d\theta        
% \end{equation}

% \begin{equation}
%    \resizebox{.99\hsize}{!}{ $= \mathbb{E}_{p_{(\theta | x)}} (\nabla_u E(\theta,u))  + \int \frac{1}{Z(\theta, u)} (\nabla_u \int(e^{-E(\theta, u)} d\theta ) p(\theta|x) d\theta ) $ }
% \end{equation}

% \begin{equation}
%      \resizebox{.99\hsize}{!}{$ = \mathbb{E}_{p_{(\theta | x)}} (\nabla_u E(\theta,u))  - \int(\int(\frac{1}{Z(\theta, u)} e^{-E(\theta, u)} \nabla_u E(\theta,u)  d\theta) p(\theta|x) d\theta) $}
% \end{equation}


% \begin{equation}
% \label{eqn:contrast}
%     = \mathbb {E}_{p_{(\theta | x)}} (\nabla_u E(\theta,u)) - \mathbb{E}_{q_{(\theta | u)}} (\nabla_u E(\theta,u)) 
% \end{equation}

% \begin{equation}
% \label{eqn:contrast}
%     = \mathbb {E}_{p_{(\theta | x)}} (\nabla_u E(\theta,u)) - \mathbb{E}_{q_{(\theta | u)}} (\nabla_u E(\theta,u)) 
% \end{equation}



\subsection{Choice of Energy Function}
\label{sec:choice_of_energy}
Till now, we have worked with a generic energy function $E(\cdot)$. In this section, we motivate the choices for energy function. It can be seen from Eq.~(\ref{eq:LD_theta}) that the sampled $\theta$ will be such that it minimizes the chosen energy function, $E(\cdot)$. However, from Eq.~(\ref{eq:pp_syn}) we also know that the set of desired $\theta\in\Theta$ are such that they minimize a class of loss functions. Hence, a logical choice for the energy function is a classification loss function itself! In other words we can choose the energy function $E(\cdot)$ to be a classification loss $\ell_{\td{\gD}}\in\gF$ so that, sampling from $\vq(\theta|\td{\gD})$ is equivalent to training a network with parameters $\theta$ on $\td{\gD}$ with loss function $\ell_{\td{\gD}}$. Further, with this choice of energy function, we can see that Eq.~(\ref{eq:Our_CD}) is nothing but difference between average loss (w.r.t $\ell_{\td{\gD}}$) incurred on parameters obtained from training on original training set and parameters obtained from training on the synthetic set.


To this end, we propose to use the categorical cross-entropy as the energy function, i.e,
\begin{align}
    E(\theta,\td{\gD}) = -\frac{1}{|\td{\gD}|}\sum_{i=1}^{|\td{\gD}|}\sum_{j=1}^{C} \td{y}_i^{(j)}\log f_{\theta}(\td{x}_i)^{(j)}
\end{align}
where, $f_{\theta}(\cdot)$ is the classifier manifested by parameters $\theta$ and $\td{y}_i^{(j)}$ is the indicator for the $j^{th}$ true class. One can also choose other classfication losses such as Focal loss and Multi-Margin Loss. We explore the effect of such choices in Section~\ref{sec:effect-of-energy}.








\section{Implementation}
\label{sec:implementation}
% \subsection{Implementation Details}
The implementation of the proposed method is inspired from MTT~\cite{MTT}.
Our entire training process can be divided into two parts. 
Firstly, we generate a set of parameters associated with the posterior defined on the real dataset, ($\vp(\theta|\gD)$). To do this, we train multiple instances of networks with different initialization to minimize the empirical risk w.r.t to loss, $\ell_{\gD}$ on the original dataset ($\gD$). We save a copy of the optimal parameters after each epoch for a given initialization. A sequence of such parameters is referred to as a trajectory (similar to MTT~\cite{MTT}) in our discussion. We store multiple such trajectories in a buffer to estimate the first expectation in Eq.~(\ref{eq:Our_CD}).


 Secondly, we sample parameters from the posterior associated with the synthetic set i.e., $\vq(\theta |\td{\gD})$. 
 For this, we first sample a trajectory from the buffer obtained above, $\tau_i \sim \tau$. Next, we choose an instantiation of parameters at a randomly chosen epoch associated with $\tau_i$, ($\theta^+_k \sim \tau_i$). Further, we use the $T$-step horizon from $\theta^+_k$ as a proxy for $\theta^+$ in Eq.~(\ref{eq:Our_CD}) i.e., $\theta^+ = \theta^+_{k+T}$ . To sample $\theta^-$, we use  $L$-step langevin dynamics updates as shown in Eq.~(\ref{eq:LD_theta}), with $\theta^{(0)} = \theta^+_k$. After this, we pass the synthetic set ($\td{\gD}$) through the networks manifested by $\theta^+$ and $\theta^-$ to calculate the respective energies, $E(\theta^+,\td{\gD})$ and $E(\theta^-,\td{\gD})$ to further calculate the contrastive-divergence (Eq.~(\ref{eq:Our_CD})). The gradients of the contrastive-divergence are then backpropagated to update the synthetic set $\td{\gD}$.  We plan to make our codebase public post acceptance for better reproducibility.


% \subsection{Training details and Hyper parameters}
% During the training process, we have randomly initialized the synthetic dataset using samples from the original set. The overall cardinality of these synthetic set is determined by the number of images considered for every class(IPC). For our experiments we have considered IPC=1/10/50. We have further used differentiable siamese augmentation(DSA) strategies to help boost the performance of our model. DSA strategies include random crop, random flip, random brightness, random scale, and rotation. These augmentation techniques ensures that the model doesn't overfit on the given synthetic set and generate optimal parameters.  

% As for the network, we took inspiration from previous work like MTT~\cite{MTT}, DD~\cite{DD} and DC~\cite{DC_CS} and conducted our experiments on ConvNet architecture~\cite{convnet}. This architecture consists of multiple blocks of convolutional layers with size 3$\times$3 and filter size of 128. It further uses instance normalization, maxpool layer with stride 2 and RELU activation. In our experiments we have used an architecture with three block of convolutional layer.


% As mentioned in Algorithm-\ref{alg:the_alg}, we have created a buffer of trajectories to sample parameters from the posterior of original dataset. For this, we have generated 100 different trajectories each with 50 epochs using SGD optimizer with a batch size of 256.

% To sample parameters from the posterior of synthetic dataset, we have run 5000 steps of langevin dynamics with step size of {} and have used step size of {} to modify the synthetic image using contrastive loss. All these experiment was conducted on NVIDIA RTX A6000 GPUs.
 % For this, we use $L$-steps of langevin dynamics updates as shown in Eq.~(\ref{eq:LD_theta}). The langevin dynamics is initialized with a random instance of parameters obtained from the trajectory buffer generated in the first step. The sampled parameters obtained using langevin dynamics () and parameters stored in the trajectory buffer are used to  compute the 
 % For this, we initialize  our network with one of the parameter associated with a given trajectory generated in the first part, and minimizes the energy function using Langevin dynamics (eqn - \ref{eq:LD_theta}).  
 % The generated sample parameter and a parameter associated with the original trajectory after some fixed no of steps from the initialization epoch is used to calculate the contrastive divergence for the training of synthetic dataset. 

% The set of trajectories ($\tau$), number of langevin steps ($\mathcal{N}$), langevin step size($\lambda$), step size for image modification ($\gamma$), number of distillation steps(P) and successive step after which parameters associated with a given trajectory is used for contrastive divergence (T) are input to our algorithm.   The complete algorithm is summarized in Alg.~\ref{alg:the_alg}.

% For the given experiment we generated 100 different trajectories each with 50 epochs using SGD optimizer with a batch size of 256. Langevin steps used for sampling parameters is 256  and  number of distillation steps used for training is 5000. We have experimented with different cardinality of synthetic dataset with different values for the no of images per class(ipc) i.e., ipc = 1/10/50. We have applied differentiable siamese augmentation(DSA) strategies to help boost the performance of our model. DSA strategies include random crop, random flip, random brightness, random scale, and rotation. Additionally, we use NVIDIA RTX A6000 GPUs on all our experiments.
% % In our method, we create expert trajectories by training a large number of networks, in our case, 100 networks for 50 epochs on the whole dataset. Buffers are stored to distill knowledge and help guide the student network. We train the student network for 5000 iterations to update synthetic images across each class. Standard ConvNet architecture is used, for ipc=1/10/50 in all our experiments. We load real images with batch size of 256 and use SGD optimizer to train our model. We apply differentiable siamese augmentation strategies to help boost the performance of our model. DSA strategies include random crop, random flip, random brightness, random scale, and rotation. Additionally, we use NVIDIA RTX A6000 GPUs on all our experiments. We discuss the proposed method as shown in Alg.~\ref{alg:the_alg}.


% \subsection{Network Architecture}
%  We use the simple ConvNet model to assess the performance on numerous datasets for all methods. Each block in a ConvNet model consist of convolution, normalizing, activation, and pooling layer. Convolution layers assign 128 output channels with kernel size $3\times 3$ and padding equal to 3. We use relu activation and instance normalization that allows normalizing the layer's output across each sample or an image. Average Pooling with kernel size $2\times 2$ and a stride 2 is used. Dataset distillation and Flexible distillation methods use LeNet architecture for MNIST dataset and AlexNet architecture for CIFAR10 datasets. We have used 3 ConvNet blocks across all our experiments. 







\section{Experiments}
\label{sec:experiments}
In this section, we present the performance and experimental findings pertaining to the proposed method. 
% and evaluate each method to find out how well it adapts to various tasks. Datasets comprise of :
% \begin{itemize}
%   \item CIFAR10 and CIFAR100 includes 60000 training images of real objects with size $32\times 32$. CIFAR100 covers many diverse classes and poses many challenges to achieving a better result.
%   \item SVHN consists of street view images of house numbers containing 70000 training and testing images.
%   \item MNIST involves 60000 handwritten images of $28\times 28$ size that are commonly used for computer vision tasks.
%   \item FashionMNIST contains images of fashion products with ten different classes and has the exact specifications compared to the MNIST dataset.
%  \end{itemize}
We evaluate our method both quantitatively and qualitatively on several BPC-benchmark datasets with different compression ratios, i.e., the number of images generated per class (ipc).
In particular, we perform our experiments on six different datasets, namely, CIFAR10~\cite{CIFAR10}, SVHN~\cite{SVHN}, MNIST~\cite{MNIST} and FashionMNIST~\cite{FashionMNIST}. We also test our method on some difficult datasets such as CIFAR100~\cite{CIFAR10} and Tiny Imagenet~\cite{tinyimagenet}.
We set ipc=1/10/50 for each of the above datasets. Further, we use a standard ConvNet architecture for all the experiments unless mentioned otherwise.
% We show visual r
 % We run the above datasets on different compression ratios, i.e., the number of images generated per class, by setting ipc=1/10/50. We conduct cross-architecture experiments, for instance training CIFAR10 datasets on ConvNet, ResNet, VGG, and AlexNet to test the model’s effectiveness across different architectures. We also examine the performance of corrupted datasets like CIFAR10-C to estimate the robustness of out-of-distribution inputs.

 Next, since our method relies on posteriors of parameters and the parameters are associated with a particular network architecture, the cross-architecture analysis becomes essential. We show cross-architecture performance for our method and compare it with other state-of-the-art BPC methods. Further, we explore different choices for the energy function used in our method and show the effect of such choices. 
We also investigate how the training time scales under each epoch while there is an increase in images per class for the CIFAR10 dataset. We also examine the consumption of GPU memory on individual methods when the synthetic set expands. 
% Additional visualizations, effects on gpu and time consumptions with varying ipc and ablation studies for augmentation strategies like DSA are shown in the supplementary material.

% Additional visualizations, effects of gpu and time consumptions with varying ipc and ablation studies on the effects of augmentation strategies like DSA are shown in the supplementary material.

 \subsection{Baselines}
 \label{sec:baseline}
 % Previous state-of-the-art models achieve significant gains in dataset condensation methods and are able to reduce the difference in accuracy between models trained initially on the whole and synthetic datasets. Table 1 compares our approach to instance selection, dataset condensation and bayesian pseudocoreset methods. Coreset selection methods mainly constitute of random, i.e., selecting a subset of samples as coreset; herding establishes data points based on distance metric between the centers of coreset and original dataset in feature space; forgetting collects maximum misclassified frequencies as samples during training;k-centers find k such points where the distance between their closest points is minimized.
 Our proposed method primarily falls under the category of Bayesian Pseudo-Coreset due to the formulation shown in Section~\ref{sec:proposed_method}. Hence, we consider the previous BPC methods for comparison. In particular, we include the BPC formulation using reverse-KL Divergence (BPC-rKL)~\cite{BPC_OG}, forward-KL Divergence (BPC-fKL)~\cite{BPC} and Wasserstein metric (BPC-W)~\cite{BPC} for comparison. Additionally, we also show that our method gives best if not second best performance compared to other baselines. Hence, we include eight dataset-condensation baselines for comparison. Particularly, we include 
Dataset Distillation (DD)~\cite{DD}, Flexible Dataset Distillation (LD)~\cite{LD}, Gradient Matching (DC)~\cite{GM}, Differentiable Siamese Augmentation (DSA)~\cite{DSA}, Distribution Matching (DM)~\cite{DM}, Neural Ridge Regression (KIP)~\cite{KIP}, Condensed data to align features (CAFE)~\cite{CAFE} and Matching Training Trajectories (MTT)~\cite{MTT} for comparison. In addition, for the sake of completeness, we include the major Coreset selection baselines such as Herding~\cite{Herding}, k-Center and Forgetting~\cite{Forgetting} for comparison.
 
 % We compare eight prior baseline models in dataset condensation methods i.e., Dataset Distillation (DD), Flexible Dataset Distillation (LD), DatasetCondensation (DC), Differentiable Siamese Augmentation (DSA), Dataset Condensation with distribution matching (DM), Dataset distillation with neural ridge regression (KIP), Condensed data to align features (CAFE) and matching training trajectories (MTT).
 % In addition, we also compare bayesian Pseudocoresets methods that are based on different divergence metric such as reverse kl , wasserstein distance and forward kl.


 \subsection{Performance on Low-Resolution Datasets}
 \label{sec:performance_low_res}

\begin{table*}[!ht]
\resizebox{\textwidth}{!}
{
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\hline
 &
  \multicolumn{3}{c|}{\textbf{MNIST}} &
  \multicolumn{3}{c|}{\textbf{FMNIST}} &
  \multicolumn{3}{c|}{\textbf{SVHN}} &
  \multicolumn{3}{c}{\textbf{CIFAR10}} \\ \hline
\cellcolor[HTML]{FFFFFF}\textbf{Img/Cls} &
  1 &
  10 &
  50 &
  1 &
  10 &
  50 &
  1 &
  10 &
  50 &
  1 &
  10 &
  50 \\
\textbf{Ratio\%} &
  0.017 &
  0.17 &
  0.83 &
  0.017 &
  0.17 &
  0.83 &
  0.014 &
  0.14 &
  0.7 &
  0.02 &
  0.2 &
  1 \\ \hline
\textbf{DD~\cite{DD}} &
  - &
  $79.71 \pm 8.3$ &
  - &
  - &
  - &
  - &
  - &
  - &
  - &
  - &
  $\underline{39.14} \pm 2.3$ &
  - \\
\textbf{LD~\cite{LD}} &
  $\underline{60.6} \pm 2.86$ &
  $\underline{87.05} \pm 0.5$ &
  $\underline{93.3} \pm 0.3$ &
  - &
  - &
  - &
  - &
  - &
  - &
  $\underline{25.38} \pm 0.2$ &
  $37.5 \pm 0.6$ &
  $\underline{41.7} \pm 0.5$ \\ \hline
\textbf{Herding~\cite{welling2009herding, castro_herding}} &
  $89.2 \pm 1.6$ &
  $93.7 \pm 0.3$ &
  $94.8 \pm 0.2$ &
  $\underline{67.0} \pm 1.9$ &
  $71.1 \pm 0.7$ &
  $71.9 \pm 0.8$ &
  $20.9 \pm 1.3$ &
  $\underline{50.5} \pm 3.3$ &
  $\underline{72.6} \pm 0.8$ &
  $21.5 \pm 1.2$ &
  $\underline{31.6} \pm 0.7$ &
  $23.3 \pm 1.0$ \\
\textbf{K-Center~\cite{sener2017active,farahani2009facility}} &
  $\underline{89.3} \pm 1.5$ &
  $84.4 \pm 1.7$ &
  $97.4 \pm 0.3$ &
  $66.9 \pm 1.8$ &
  $54.7 \pm 1.5$ &
  $68.3 \pm 0.8$ &
  $\underline{21.0} \pm 1.5$ &
  $14.0 \pm 1.3$ &
  $20.1 \pm 1.4$ &
  $\underline{21.5} \pm 1.3$ &
  $14.7 \pm 0.9$ &
  $27.0 \pm 1.4$ \\
\textbf{Forgetting~\cite{Forgetting}} &
  $35.5 \pm 5.6$ &
  $68.1 \pm 3.3$ &
  $88.2 \pm 1.2$ &
  $42.0 \pm 5.5$ &
  $53.9 \pm 2.0$ &
  $55.0 \pm 1.1$ &
  $12.1 \pm 1.7$ &
  $16.8 \pm 1.2$ &
  $27.2 \pm 1.5$ &
  $13.5 \pm 1.2$ &
  $23.3 \pm 1.0$ &
  $23.3 \pm 1.1$ \\
\textbf{Random} &
  $64.9 \pm 3.5$ &
  $\underline{95.1} \pm 0.9$ &
  $\underline{97.9} \pm 0.2$ &
  $51.4 \pm 3.8$ &
  $\underline{73.8} \pm 0.7$ &
  $\underline{82.5} \pm 0.7$ &
  $14.6 \pm 1.6$ &
  $35.1 \pm 4.1$ &
  $70.9 \pm 0.9$ &
  $14.4 \pm 2.0$ &
  $26.0 \pm 1.2$ &
  $\underline{43.4} \pm 1.0$ \\ \hline
\textbf{BPC-rKL(sghmc)~\cite{BPC,BPC_OG}} &
  $74.8 \pm 1.17$ &
  $\underline{95.27} \pm 0.17$ &
  $94.18 \pm 0.26$ &
  $70.53 \pm 1.09$ &
  $78.81 \pm 0.17$ &
  $76.97 \pm 0.59$ &
  $18.34 \pm 1.79$ &
  $60.68 \pm 5.07$ &
  $78.27 \pm 0.62$ &
  $21.62 \pm 0.83$ &
  $37.89 \pm 1.54$ &
  $37.54 \pm 1.32$ \\
\textbf{BPC-W(sghmc)~\cite{BPC}} &
  $83.59 \pm 1.49$ &
  $91.72 \pm 0.55$ &
  $93.72 \pm 0.55$ &
  $72.39 \pm 0.87$ &
  $\underline{83.69} \pm 0.51$ &
  $74.41 \pm 0.48$ &
  $33.52 \pm 1.15$ &
  $74.75 \pm 1.27$ &
  $79.49 \pm 0.54$ &
  $29.34 \pm 1.21$ &
  $48.9 \pm 1.72$ &
  $46.17 \pm 0.67$ \\
\textbf{BPC-fKL(sghmc)~\cite{BPC}} &
  $82.98 \pm 2.2$ &
  $92.05 \pm 0.42$ &
  $40.63 \pm 1.8$ &
  $72.51 \pm 2.53$ &
  $83.29 \pm 0.55$ &
  $74.82 \pm 0.52$ &
  $21.48 \pm 6.58$ &
  $\underline{75.49} \pm 0.84$ &
  $77.08 \pm 1.8$ &
  $29.3 \pm 1.1$ &
  $\underline{49.85} \pm 1.37$ &
  $42.30 \pm 2.87$ \\
\textbf{BPC-fKL(hmc)~\cite{BPC}} &
  $\underline{90.46} \pm 1.5$ &
  $89.8 \pm 0.82$ &
  $\underline{95.58} \pm 1.63$ &
  $\underline{78.24} \pm 1.02$ &
  $82.06 \pm 0.44$ &
  $\underline{82.40} \pm 0.35$ &
  $\underline{48.02} \pm 5.62$ &
  $65.64 \pm 2.92$ &
  $\underline{79.6} \pm 0.53$ &
  $\underline{35.57} \pm 0.95$ &
  $43.07 \pm 1.06$ &
  $\underline{50.92} \pm 1.49$ \\ \hline
\textbf{DC~\cite{GM}} &
  $92.01 \pm 0.25$ &
  $97.58 \pm 0.1$ &
  $98.81 \pm 0.03$ &
  $70.83 \pm 0.01$ &
  $81.93 \pm 0.07$ &
  $83.26 \pm 0.17$ &
  $30.49 \pm 0.57$ &
  $75.1 \pm 0.4$ &
  $81.7 \pm 0.14$ &
  $28.10 \pm0.56$ &
  $44.14 \pm 0.6$ &
  $53.73 \pm 0.44$ \\
\textbf{DSA~\cite{DSA}} &
  $87.6 \pm 0.07$ &
  $97.39 \pm 0.06$ &
  $98.87 \pm 0.04$ &
  $70.45 \pm 0.57$ &
  $84.7 \pm 0.11$ &
  $88.55 \pm 0.56$ &
  $31.18 \pm 0.43$ &
  \cellcolor[HTML]{FFCC67}$\underline{78.39} \pm 0.3$ &
  $82.5 \pm 0.34$ &
  $29 \pm 0.64$ &
  $51.85 \pm 0.43$ &
  $60.77 \pm 0.45$ \\
\textbf{DM~\cite{DM}} &
  $88.89 \pm 0.57$ &
  $96.58 \pm 0.11$ &
  $98.22 \pm 0.05$ &
  $71.92 \pm 0.7$ &
  $83.25 \pm 0.09$ &
  $87.65 \pm 0.03$ &
  $19.25 \pm 1.39$ &
  $71.42 \pm 1.01$ &
  $82.41 \pm0.52$ &
  $26.40 \pm 0.42$ &
  $48.66 \pm 0.03$ &
  $62.7 \pm 0.07$ \\
\textbf{KIP~\cite{KIP}} &
  $85.46 \pm 0.04$ &
  $97.15 \pm 0.11$ &
  $98.36 \pm 0.08$ &
  - &
  - &
  - &
  - &
  - &
  - &
  $40.5 \pm 0.4$ &
  $53.1 \pm 0.5$ &
  $58.6 \pm 0.4$ \\
\textbf{CAFE~\cite{CAFE}} &
  \cellcolor[HTML]{FFCC67}$\underline{93.1} \pm 0.3$ &
  \cellcolor[HTML]{FFFFFF}$97.2 \pm 0.2$ &
  $98.6 \pm 0.2$ &
  $77.1 \pm 0.9$ &
  $83.0 \pm 0.4$ &
  $84.8 \pm 0.4$ &
  $42.6 \pm 3.3$ &
  $75.9 \pm 0.6$ &
  $81.3 \pm 0.3$ &
  $30.3 \pm 1.1$ &
  $46.3 \pm 0.6$ &
  $55.5 \pm 0.6$ \\
\textbf{CAFE+DSA~\cite{CAFE}} &
  $90.8 \pm 0.5$ &
  \cellcolor[HTML]{FFFFFF}$97.5 \pm 0.1$ &
  \cellcolor[HTML]{FFCC67}$\underline{98.9} \pm 0.2$ &
  $73.7 \pm 0.7$ &
  $83.0 \pm 0.3$ &
  $88.2 \pm 0.3$ &
  $42.9 \pm 3.0$ &
  $77.9 \pm 0.6$ &
  $82.3 \pm 0.4$ &
  $31.6 \pm 0.8$ &
  $50.9 \pm 0.5$ &
  $63.3 \pm 0.4$ \\
\textbf{MTT~\cite{MTT}} &
  $89.85 \pm 0.01$ &
  \cellcolor[HTML]{FFCC67}$\underline{97.7} \pm 0.02$ &
  $98.6 \pm 0.006$ &
  \cellcolor[HTML]{FFCC67}$\underline{77.14} \pm 0.007$ &
  \cellcolor[HTML]{34FF34}\boldsymbol{$\underline{88.768} \pm 0.00158$} &
  \cellcolor[HTML]{FFCC67}$\underline{89.332} \pm 0.151$ &
  \cellcolor[HTML]{FFCC67}$\underline{57.55} \pm 0.02$ &
  $72.56 \pm 0.005$ &
  \cellcolor[HTML]{FFCC67}$\underline{83.731} \pm 0.334$ &
  \cellcolor[HTML]{FFCC67}$\underline{46.08} \pm 0.8$ &
  \cellcolor[HTML]{34FF34}\boldsymbol{$\underline{64.27} \pm 0.8$} &
  \cellcolor[HTML]{FFCC67}$\underline{71.26} \pm 0.5$  \\ \hline
\textbf{Ours} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$93.42 \pm 0.09$} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$97.71 \pm 0.24$} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$98.91 \pm 0.22$} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$77.29 \pm 0.5$} &
  \cellcolor[HTML]{FFCC67}$88.40 \pm 0.21$ &
  \cellcolor[HTML]{34FF34}\boldsymbol{$89.47 \pm 0.06$} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$66.74 \pm 0.09$} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$82.32 \pm 0.56$} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$88.41 \pm 0.12$} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$46.87 \pm 0.2$} &
  \cellcolor[HTML]{FFCC67}$56.39 \pm 0.7$ &
  \cellcolor[HTML]{34FF34}\boldsymbol{$71.93 \pm 0.17$} \\ \hline
\textbf{Whole Dataset} &
  \multicolumn{3}{c|}{$99.6 \pm 0.0$} &
  \multicolumn{3}{c|}{$93.5 \pm 0.1$} &
  \multicolumn{3}{c|}{$95.4 \pm 0.1$} &
  \multicolumn{3}{c}{$84.8 \pm0.1$} \\ \hline
\end{tabular}
}
\caption{\label{sota-comp-low-res}Performance comparison of Coreset, BPC and Dataset Condensation methods for MNIST, FashionMNIST, SVHN and CIFAR10 datasets. The results are noted in form of (mean $\pm$ std. dev) where we have obtained test accuracy over five independent runs on the pseudo-coreset. The best performer for each set of methods is denoted by an underline ($\underline{x}\pm s$). The best performer across all methods is denoted in bold ($\boldsymbol{x\pm s}$). For ease of comparison, we color the best performer with green color and the second best performer with orange color.}
\end{table*}

\begin{figure*}[!t]
\centering
   \begin{subfigure}[t]{0.23\textwidth}
   \includegraphics[keepaspectratio, width=\textwidth]{Results/mnist/mnist-ipc1.png}
    \caption{MNIST}
   % \label{fig:Ng1} 
\end{subfigure}
~
\begin{subfigure}[t]{0.23\textwidth}
   \includegraphics[keepaspectratio, width=\textwidth]{Results/fmnist/fmnist-ipc1.png}
    \caption{FMNIST}
   % \label{fig:Ng2}
\end{subfigure}
~
\begin{subfigure}[t]{0.23\textwidth}
   \includegraphics[keepaspectratio, width=\textwidth]{Results/svhn/svhn-ipc1.png}
    \caption{SVHN}
   % \label{fig:Ng2}
\end{subfigure}
~
\begin{subfigure}[t]{0.23\textwidth}
   \includegraphics[keepaspectratio, width=\textwidth]{Results/cifar10/cifar10-ip1.png}
    \caption{CIFAR10}
   % \label{fig:Ng2}
\end{subfigure}
\caption{\label{viz-ipc-1}Visualizations of pseudo-coreset with one image per class for MNIST, FMNIST, SVHN and CIFAR10.}
\end{figure*}

\begin{figure*}[!t]
\centering
   \begin{subfigure}[t]{0.23\textwidth}
   \includegraphics[keepaspectratio, width=\textwidth]{Results/mnist/mnist-ipc10.png}
    \caption{MNIST}
   % \label{fig:Ng1} 
\end{subfigure}
~
\begin{subfigure}[t]{0.23\textwidth}
   \includegraphics[keepaspectratio, width=\textwidth]{Results/fmnist/fmnist-ipc10.png}
    \caption{FMNIST}
   % \label{fig:Ng2}
\end{subfigure}
~
\begin{subfigure}[t]{0.23\textwidth}
   \includegraphics[keepaspectratio, width=\textwidth]{Results/svhn/svhn-ipc10.png}
    \caption{SVHN}
   % \label{fig:Ng2}
\end{subfigure}
~
\begin{subfigure}[t]{0.23\textwidth}
   \includegraphics[keepaspectratio, width=\textwidth]{Results/cifar10/cifar10-ipc10.png}
    \caption{CIFAR10}
   % \label{fig:Ng2}
\end{subfigure}
\caption{\label{viz-ipc-10}Visualizations of pseudo-coreset with ten image per class for MNIST, FMNIST, SVHN and CIFAR10.}
\end{figure*}


 Firstly, we present the results on low-resolution datasets like MNIST, FashionMNIST (FMNIST), SVHN, and CIFAR10. Our findings are listed in Table~\ref{sota-comp-low-res}. In particular, we observe that the proposed method significantly outperforms all the BPC methods by large margins, for e.g. we observe a gain of $11.3\%$, $6.54\%$ and $21.01\%$ on CIFAR10 with an ipc of 1, 10 and 50 respectively compared to the best performing BPC baselines. Similarly, on SVHN we observe gain of $18.72\%$, $6.83\%$ and $8.92\%$ respectively compared to the BPC counterparts. A similar trend can be seen for MNIST and FMNIST as well. We attribute this boost in performance to the flexible formulation of the proposed method.

 Further, we observe that our method not only outperforms BPC methods but also outperforms SoTA dataset condensation methods in most of the cases. We find that our performance is better than almost all the dataset condensation baselines whereas MTT stands out to be a close second in most of the cases. For e.g. we see a gain of $0.79\%$ and $0.67\%$ on CIFAR10 with ipc of 1 and 50 respectively when compared to MTT, however, there is a loss of about $7.88\%$ with ipc of 10. Further, there is a gain of $9.19\%$, $3.93\%$ and $4.68\%$ on SVHN dataset with ipc of 1, 10 and 50 respectively when compared to the corresponding best performance of dataset condensation methods. We can observe similar trends for MNIST as well as FMNIST datasets. This shows that our method although falling under the category of bayeian pseudo-coreset, achieves a performance which is comparable to that of heuristic dataset condensation.


 
 % Random and K-center algorithms outperform other coreset selection methods for FashionMNIST and MNIST datasets. For 50 images per class the Random algorithm achieves best results with excessive margins of 3\%,11\%, and 20\% for MNIST, FashionMNIST, and CIFAR10 datasets. Leading the records for SVHN dataset is the Herding algorithm for ipc 10 and 50 with test accuracy of 50.5\% and 72.6\%, which is superior to all the other methods. In general, we observe that the forgetting algorithm has the least performance compared to random, herding and k-centers approach. Coreset selection methods adapt poorly to SVHN and CIFAR10 datasets due to their diverse set of images.

% Bayesian pseudo coreset perform better than coreset selection methods for all datasets. We mainly test the pseudo coreset method on two evaluation strategies: sghmc and hmc. Overall, BPC-fKL with hmc method obtains superior performance for 1 and 50 images per class. For ipc=10, we see a decrease in performance for BPC-fKL method with hmc ,along the same setting, sghmc performs better than hmc method on every single dataset. We report BPC-W and BPC-rKL method achieves best accuracy of 95.27\% and 83.69\% for the FashionMNIST and MNIST datasets, respectively.

% In Table 1, best and second best results are highlighted in green and orange colour. Compared to previous work, dataset condensation methods better fit these tasks than the coreset selection and pseudo coreset methods. Our method outperforms all the other techniques and achieve state-of-the-art results, particularly on SVHN and MNIST datasets. We prove the influence of our method by reporting 9\% margin gains over the SVHN dataset and 0.2\%-2.6\% gains over the next best methods. MTT deliver best results with accuracy of 88.76\% and 64.27\% for FashionMNIST and CIFAR10 datasets with 10 ipc. CAFE + DSA accounts for being second best in the MNIST dataset by recording an accuracy of 93.1\% and 98.9\% for 1 and 50 ipc. When we increase number of synthetic images per class from 10 to 50, the gains are less compared to that of 1 to 10 ipc. Table 1 indicates that we are at least the best or second best method across all tasks. Test accuracy of 71.93\% compared to 84.8\% model trained on the whole datasets proves that we generalize well on challenging datasets like CIFAR10.
We present the qualitative visualizations for MNIST, FMNIST, SVHN, and CIFAR10 datasets with 1 and 10 images per class. It can be seen that, for one image per class, our method can do an excellent job condensing the whole dataset and achieving formidable results. The constructed pseudo-coreset is identifiable but inherits some artifacts due to the constraints on the dataset size. 
% Figure~\ref{viz-ipc-1} attributed to some disturbances in these images. 
As the number of images per class increases, the model can induce more variations across all the classes and thus produce a diverse pseudo-coreset. This can be observed in Figure~\ref{viz-ipc-10} where we show the synthetic set generated with 10 images per class.
% Figure~\ref{viz-ipc-10} shows a total of 100 images for all four datasets.

\subsection{Performance on larger datasets}
\label{sec:performance_large_data}
Next, we show the efficacy of proposed method on larger datasets with relatively high-resolution. For this, we choose the standard benchmark datasets of CIFAR100 and Tiny ImageNet. CIFAR100 contains images of size $32\times32$ under 100 diverse classes such as  aquatic mammals, fish, flowers, food containers, household furniture, insects, etc. Further, Tiny ImageNet is a subset of the famous ImageNet dataset with each image of size $64\times64$ under 200 classes. The large number of classes for these datasets make the condensation and creation of pseudo-coreset difficult.

Our findings are presented in Table~\ref{sota-comp-high-res}. Our observation is in line with that on low-resolution dataset discussed in previous section. In particular, our method outperforms previous SoTA BPC methods by large margin. For e.g. we see an increment of $11.78\%$ on CIFAR100 with an ipc of 1. Further, our method achieves performance comparable to that of dataset condensation methods. It can be seen from Table~\ref{sota-comp-high-res} that our method outperforms MTT on CIFAR100 as well as Tiny ImageNet with an ipc of 1. However, MTT gives a gain of $8.54\%$ compared to our method on CIFAR100 with ipc 10, which ranks us the third best amongst the baselines. Further, on Tiny ImageNet with ipc of 10, MTT gives a boost of $2.29\%$ relative to our performance, ranking us the second best in this setting. 
% Further information about the setup needed to perform all our experiments is shown below in the supplementary material.
% CIFAR100 dataset represents a diverse set of classes and superclasses. CIFAR100 is classified into 100 classes and 20 superclasses, including superclasses such as aquatic mammals, fish, flowers, food containers, household furniture, insects, etc. Here we discuss the results for various models when trained on the CIFAR100 dataset. We use standard ConvNet architecture of depth 3 so to compare all under similar arrangements. Coreset selection show identical performance on all their methods and present a very substandard results for the CIFAR100 dataset. Other methods in pseudo coreset and coreset selection are unfit to train due to considerable usage of GPU and time requirements. As a whole, BPC method is better than any other coreset selection method under 1 ipc , resulting with best accuracy is 12.19\%.

% MTT and our method exhibits impressive performance on such tough dataset like CIFAR100. For one image per class, we obtain the best results with test accuracy of 23.97\%, followed by MTT being next best with accuracy of 23.62\%. For images per class as 10 and 50, MTT tends to perform the best by achieving an accuracy of 36.96\% and 47.32\%. Our method reach 79\% of the total accuracy when trained on the whole dataset for 50 images per class.

\begin{table}[]
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|cc|cc}
\hline
                                         & \multicolumn{2}{c|}{\textbf{CIFAR100}}                    & \multicolumn{2}{c}{\textbf{TinyImagenet}} \\ \hline
\cellcolor[HTML]{FFFFFF}\textbf{Img/Cls} & 1                & 10                                     & 1              & 10              \\
\textbf{Ratio\%}                         & 0.2              & 2                                      & 0.2            & 2               \\ \hline
\textbf{LD~\cite{LD}}                              & $\underline{11.5} \pm 0.4$   & -                                      & -              & -               \\ \hline
\textbf{Random}                          & $4.2 \pm 0.3$    & $14.6 \pm 0.5$                         & $1.4 \pm 0.1$  & $5.0 \pm 0.2$   \\
\textbf{Forgetting~\cite{Forgetting}}                      & $4.5 \pm  0.2$   & $15.1 \pm 0.3$                         & $1.6 \pm 0.1$  & $5.1 \pm 0.2$   \\
\textbf{K-Center~\cite{sener2017active,farahani2009facility}}                        & $8.3 \pm 0.3$    & $7.1 \pm 0.2$                          & $\underline{3.03} \pm 0.0$ & $\underline{11.38} \pm 0.0$ \\
\textbf{Herding~\cite{welling2009herding, castro_herding}}                         & $\underline{8.4} \pm 0.3$    & $\underline{17.3} \pm 0.3$                         & $2.8 \pm 0.2$  & $6.3 \pm 0.2$   \\ \hline
\textbf{BPC-rKL(sghmc)~\cite{BPC,BPC_OG}}                  & $3.56 \pm 0.04$  & -                                      & -              & -               \\
\textbf{BPC-fKL(hmc)~\cite{BPC}}                    & $7.57 \pm 0.54$  & -                                      & -              & -               \\
\textbf{BPC-fKL(sghmc)~\cite{BPC}}                  & $12.07 \pm 0.16$ & -                                      & -              & -               \\
\textbf{BPC-W(sghmc)~\cite{BPC}}                    & $\underline{12.19} \pm 0.22$ & -                                      & -              & -               \\ \hline
\textbf{DC~\cite{GM}}                              & $12.65 \pm 0.32$ & $25.28 \pm 0.29$                       & $5.27 \pm 0.0$ & $12.83 \pm 0.0$ \\
\textbf{DSA~\cite{DSA}}                             & $13.88 \pm 0.29$ & $32.34 \pm 0.4$                        & $5.67 \pm 0.0$ & $16.43 \pm 0.0$ \\
\textbf{DM~\cite{DM}}                              & $11.35 \pm 0.18$ & $29.38 \pm 0.26$                       & $3.82 \pm 0.0$ & $13.51 \pm 0.0$ \\
\textbf{KIP~\cite{KIP}}                             & $12.04 \pm 0.0$  & $29.04 \pm 0.0$                        & -              & -               \\
\textbf{CAFE~\cite{CAFE}}                            & $12.9. \pm 0.3$  & $27.8 \pm 0.3$                         & -              & -               \\
\textbf{CAFE+DSA~\cite{CAFE}}                        & $14.0 \pm 0.3$   & \cellcolor[HTML]{FFCC67}$31.5 \pm 0.2$ & -              & -               \\
\textbf{MTT~\cite{MTT}} &
  \cellcolor[HTML]{FFCC67}$\underline{23.62} \pm 0.63$ &
  \cellcolor[HTML]{34FF34}\boldsymbol{$\underline{36.96} \pm 0.155$} &
  \cellcolor[HTML]{FFCC67}$\underline{8.27} \pm 0.0$ &
  \cellcolor[HTML]{34FF34}\boldsymbol{$\underline{20.11} \pm 0.00$} \\ \hline
\textbf{Ours} &
  \cellcolor[HTML]{34FF34}\boldsymbol{$23.97 \pm 0.11$} &
  $28.42 \pm 0.24$ &
  \cellcolor[HTML]{34FF34}\boldsymbol{$8.39 \pm  0.07$} &
  \cellcolor[HTML]{FFCC67}$17.82 \pm 0.39$ \\ \hline
\textbf{Whole Dataset} &
  \multicolumn{2}{c|}{{$56.2 \pm 0.3$}} &
  \multicolumn{2}{c}{{$39.83 \pm 0.0$}} \\ \hline
\end{tabular}
}
\caption{\label{sota-comp-high-res}CIFAR100 and TinyImagnet results for 1 and 10 ipc on all the above methods. The best performer for each set of methods is denoted by an underline ($\underline{x}\pm s$). The best performer across all methods is denoted in bold ($\boldsymbol{x\pm s}$). For ease of comparison, we color the best performer with green color and the second best performer with orange color.}
\end{table}




\subsection{Cross Architecture Analysis}
\label{sec:cross-arch}
As discussed in previous sections, BPC methods rely on divergence minimization between posteriors associated with original training set and synthetic set. Hence, a natural concern that arises is that of generalization of such synthetic set across different network architecture. While this aspect of synthetic set is explored in dataset condensation literature, BPC literature have not addressed this concern. For this reason, we show the cross-architecture generalization of synthetic sets generated using BPC methods. For this, we construct the pseudo-coreset for CIFAR10 (ipc=10) by using a ConvNet architecture mentioned in previous section, and train different networks like ResNet\cite{resnet}, VGG\cite{vgg} and AlexNet\cite{alexnet} on the pseudo-coreset to test the generalization of the network. Our findings are listed in Table~\ref{cross-architecture}. It can be seen that previous BPC methods lack to generalize across network architecture whereas our method is able to generalize well on other architectures as well. For instance, the performance of BPC-fKL and BPC-rKL drop by $34.19\%$ and $24.42\%$ respectively on ResNet resulting in random predictions with an accuracy of almost $10\%$, whereas our method observes a drop of only $14.74\%$ while performing with an accuracy of $41.65\%$. The same pattern can be seen with other architectures, demonstrating that our approach is generalizable even when other BPC methods fail.

\begin{table}[!ht]
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|cccc}
\hline
 & \textbf{ConvNet} & \textbf{ResNet} & \textbf{VGG} & \textbf{AlexNet} \\ \hline
\cellcolor[HTML]{FFFFFF}\textbf{Ours} &
  \cellcolor[HTML]{FFFFFF}\boldsymbol{$56.39 \pm 0.7$} &
  \boldsymbol{$41.65 \pm 1.03$} &
  \boldsymbol{$47.51 \pm 0.89$} &
  \boldsymbol{$30.58 \pm 1.43$} \\
\textbf{BPC-fKL} &
  \cellcolor[HTML]{FFFFFF}$44.34 \pm 1.11$ &
  \cellcolor[HTML]{FFFFFF}$10.15 \pm 0.21$ &
  \cellcolor[HTML]{FFFFFF}$10.43 \pm 0.33$ &
  \cellcolor[HTML]{FFFFFF}$10.0 \pm 0.0$ \\
\textbf{BPC-rKL} &
  \cellcolor[HTML]{FFFFFF}$34.48 \pm 0.48$ &
  \cellcolor[HTML]{FFFFFF}$10.06 \pm 0.08$ &
  \cellcolor[HTML]{FFFFFF}$10.26 \pm 0.35$ &
  $10.0 \pm 0.0$ \\ \hline
\end{tabular}
}
\caption{\label{cross-architecture}Comparison of BPC methods on cross-architecture generalization. For this, we construct the pseudo-coreset for CIFAR10 by using ConvNet architecture, and test it on other architectures like ResNet\cite{resnet}, VGG\cite{vgg} and AlexNet\cite{alexnet}. The best performer for each architecture is denoted in bold symbols.}
\end{table}

\subsection{Effect of choice of Energy function}
\label{sec:effect-of-energy}
As discussed in previous sections, our method requires creation of training trajectories from original dataset to sample from posterior associated with original dataset. This requires training of network parameters using a certain loss function, call it $\ell_{\gD}(\cdot)$. Next, we use langevin dynamics to sample from posterior associated with synthetic set. As discussed  in Section~\ref{sec:choice_of_energy}, this can be seen as learning of network parameters via noisy gradient descent on energy function. And as noted, it is logical to choose any loss function, call it $\ell_{\td{\gD}}(\cdot)$, as the energy function. Hence, a natural question to follow is how does the choice of $\ell_{\gD}$ and $\ell_{\td{\gD}}$ affect the performance of pseudo-coreset. For this, we do a `cross-loss' analysis for our method, where we observe the effect of using different choices of $\ell_{\gD}$ and $\ell_{\td{\gD}}$. For this analysis, we use three candidate classification loss functions: Cross-entropy loss, Focal loss and Multi-margin classification loss. Our observations for CIFAR10 (ipc=10)  are listed in Table~\ref{cross-loss}. We observe that our method performs best for a certain loss function when $\ell_{\gD} = \ell_{\td{\gD}}$. This can be attributed to the fact that the posterior estimates are closer when same loss function is used to obtain the trained parameters. This can also be seen from the observation that when different losses are used i.e, when $\ell_{\gD} \neq \ell_{\td{\gD}}$, there is a drop in performance of pseudo-coresets. Further, we observe that amongst all the choices, cross-entropy loss provides the best result. 


\begin{table}[!ht]
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|ccc}
\hline
\diagbox{$\ell_{\gD}(\cdot)$}{$\ell_{\td{\gD}}(\cdot)$} & \textbf{Cross Entropy}    & \textbf{Focal}            & \textbf{Margin}           \\ \hline
\textbf{Cross Entropy} & \cellcolor[HTML]{FFFFFF}\boldsymbol{$56.39 \pm 0.7$} & \textbf{$55.05 \pm 0.28$} & \textbf{$45.68 \pm 0.72$} \\
\textbf{Focal}                    & $37.79 \pm 0.12$ & \boldsymbol{$54.24 \pm 0.34$} & $43.62 \pm 0.11$ \\
\textbf{Margin}                   & $52.09 \pm 0.86$ & $52.44 \pm 0.82$ & \boldsymbol{$53.91 \pm 0.76$} \\ \hline
\end{tabular}
}
\caption{\label{cross-loss}Effect of choosing different training loss and energy function for construction of pseudo-coreset.}
\end{table}



\section{Conclusion}
Dataset condensation and BPC methods address the issue of over reliance on large training datasets by generating a small synthetic set with comparable performance as the original training dataset. These methods not only reduces the training costs for downstream tasks, but also result in lower carbon emissions. There is, however, a significant performance gap between the BPC and dataset condensation techniques. This gap in performance for BPC methods can be attributed to the various assumptions made about the form of the posterior distribution. In our work, we address this issue by using an energy-based distribution to model the posterior associated with the synthetic set without making any assumption about the form of posterior for real dataset. We also derive a contrastive divergence-based loss to minimize the KL divergence between the posteriors associated with real and synthetic datasets. Our formulation not only outperforms other BPC methods but also bridges the performance gap between BPC and dataset condensation methods. A better understanding of theoretically grounded works such as BPC can not only improve the performance of standard classification tasks but also pave the way to extend these methods to other resource-intensive tasks. In the future, we intend to investigate BPC and dataset condensation techniques for large generative models.

% Dataset condensation and Bayesian Pseudo-Coreset methods have provided an efficient method to train a neural network with a small dataset while achieving acceptable accuracy. These methods not only reduce overall training costs for downstream tasks, but also result in lower carbon emissions. However, there is a significant performance gap between BPC methods and dataset condensation techniques. In our work, we bridge this gap with an energy-based function and achieve comparable results to other SoTA techniques. Our method further  provides a flexibility to explore different energy functions
%  to model the parameter's posterior under the BPC framework. A better understanding of theoretically grounded works such as BPC can not only improve the performance of standard classification tasks, but can also pave the extend these methods to other resource-intensive tasks. In the future, we intend to investigate BPC and dataset condensation techniques for generative models and large language model training.



% \input{Sections/Introduction.tex}
% \input{Sections/Related Work.tex}
% \input{Sections/Method.tex}
% \input{Sections/Experiments.tex}
% \input{Sections/Conclusion.tex}


% \section{Introduction}
% \label{sec:intro}

% Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press.
% This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version.

% %-------------------------------------------------------------------------
% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of cvpr.sty to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the CVPR70 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%    We describe a system for zero-g frobnication.
%    This system is new because it handles the following cases:
%    A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%    Ours handles it by including a foo term in the bar integral.

%    ...

%    The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%    It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Example of caption.
%    It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%    \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%    ``Frobnication has been trendy lately.
%    It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%    Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.


% % Update the cvpr.cls to do the following automatically.
% % For this citation style, keep multiple citations in numerical (not
% % chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% % \cite{Alpher02,Alpher03,Authors14}.


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

% %------------------------------------------------------------------------
% \section{Formatting your paper}
% \label{sec:formatting}

% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% page.

% %-------------------------------------------------------------------------
% \subsection{Margins and page numbering}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
% high.
% %
% Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
% The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
% The \LaTeX\ template takes care of this when used properly.



% %-------------------------------------------------------------------------
% \subsection{Type style and fonts}

% Wherever Times is specified, Times Roman may also be used.
% If neither is available on your word processor, please use the font closest in
% appearance to Times to which you have access.

% MAIN TITLE.
% Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
% The title should be in Times 14-point, boldface type.
% Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
% do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
% Leave two blank lines after the title.

% AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
% and printed in Times 12-point, non-boldface type.
% This information is to be followed by two blank lines.

% The ABSTRACT and MAIN TEXT are to be in a two-column format.

% MAIN TEXT.
% Type main text in 10-point Times, single-spaced.
% Do NOT use double-spacing.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Make sure your text is fully justified---that is, flush left and flush right.
% Please do not place any additional blank lines between paragraphs.

% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
% Short captions should be centred.

% \noindent Callouts should be 9-point Helvetica, non-boldface type.
% Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

% FIRST-ORDER HEADINGS.
% (For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

% SECOND-ORDER HEADINGS.
% (For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
% If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

% %-------------------------------------------------------------------------
% \subsection{Footnotes}

% Please use footnotes\footnote{This is what a footnote looks like.
% It often distracts the reader from the main flow of the argument.} sparingly.
% Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
% If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
% Use Times 8-point type, single-spaced.


% %-------------------------------------------------------------------------
% \subsection{Cross-references}

% For the benefit of author(s) and readers, please use the
% {\small\begin{verbatim}
%   \cref{...}
% \end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
% This will automatically insert the appropriate label alongside the cross-reference as in this example:
% \begin{quotation}
%   To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
%   It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
%   You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
% \end{quotation}
% If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
% {\small\begin{verbatim}
%   \Cref{...}
% \end{verbatim}}
% command. Here is an example:
% \begin{quotation}
%   \Cref{fig:onecol} is also quite important.
% \end{quotation}

% %-------------------------------------------------------------------------
% \subsection{References}

% List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
% When referenced in the text, enclose the citation number in square brackets, for
% example~\cite{Authors14}.
% Where appropriate, include page numbers and the name(s) of editors of referenced books.
% When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
% If you use the template as advised, this will be taken care of automatically.

% \begin{table}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Method & Frobnability \\
%     \midrule
%     Theirs & Frumpy \\
%     Yours & Frobbly \\
%     Ours & Makes one's heart Frob\\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}

% %-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
% Instead use
% {\small\begin{verbatim}
%   \centering
% \end{verbatim}}
% at the beginning of your figure.
% Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
% Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

% When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage{graphicx} ...
%    \includegraphics[width=0.8\linewidth]
%                    {myfile.pdf}
% \end{verbatim}
% }


% %-------------------------------------------------------------------------
% \subsection{Color}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

% If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
% Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.

% %------------------------------------------------------------------------
% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.

% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.


%%%%%%%%% REFERENCES
\balance
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
