\def\isarxiv{1}

\ifdefined\isarxiv
\documentclass[11pt]{article}

\usepackage{amsthm, amsmath, amssymb, graphicx, url}
\usepackage[margin=1in]{geometry}

\else
% \documentclass[anon,12pt]{colt2023} % Anonymized submission
\documentclass[final]{colt2023} % avoid compile error
%\documentclass[final,12pt]{colt2023} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\fi

\usepackage{latexsym, amscd, amsfonts, mathrsfs, stmaryrd, tikz-cd, mathrsfs, bbm, esint, listings, moreverb, hyperref, pifont, enumitem}

\def\bE{\mathbb{E}}
\def\bP{\mathbb{P}}
\def\bR{\mathbb{R}}
\def\bZ{\mathbb{Z}}

\def\cN{\mathcal{N}}
\def\cO{\mathcal{O}}
\def\cP{\mathcal{P}}
\def\cW{\mathcal{W}}
\def\cX{\mathcal{X}}
\def\cY{\mathcal{Y}}
\def\cZ{\mathcal{Z}}

\def\simiid{\stackrel{\text{iid}}{\sim}}
\def\wt{\widetilde}
\def\wh{\widehat}
\def\ov{\overline}

\DeclareMathOperator\arctanh{\mathrm{arctanh}}
\DeclareMathOperator\Aut{\mathrm{Aut}}
\DeclareMathOperator\BEC{\mathrm{BEC}}
\DeclareMathOperator\BSC{\mathrm{BSC}}
\DeclareMathOperator\FSC{\mathrm{FSC}}
\DeclareMathOperator\diag{\mathrm{diag}}
\DeclareMathOperator\Po{\mathrm{Po}}
\DeclareMathOperator\Res{\mathrm{Res}}
\DeclareMathOperator\sgn{\mathrm{sgn}}
\DeclareMathOperator\SKL{\mathrm{SKL}}
\DeclareMathOperator\Unif{\mathrm{Unif}}
\DeclareMathOperator\Var{\mathrm{Var}}
\DeclareMathOperator\Pois{\mathrm{Pois}}
\DeclareMathOperator\SNR{\mathrm{SNR}}
\DeclareMathOperator\BP{\mathrm{BP}}
\DeclareMathOperator\HSBM{\mathrm{HSBM}}
\DeclareMathOperator\Id{\mathrm{Id}}
\DeclareMathOperator\abs{\mathrm{abs}}

\def\iidsim{\overset{\text{i.i.d}}{\sim}}

\newcommand{\circnum}[1]{%
  \text{\ding{\the\numexpr #1+191}}%
}

\ifdefined\isarxiv
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}

\theoremstyle{definition}
% \newtheorem{conv}[theorem]{Convention}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem*{remark}{Remark}
\fi

\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:

\newcommand{\yuzhou}[1]{{\color{blue}[Yuzhou: #1]}}
\newcommand{\yury}[1]{{\color{red}[Yury: #1]}}

\begin{document}

\ifdefined\isarxiv
\title{Weak Recovery Threshold for the Hypergraph Stochastic Block Model}
\date{}
\author{
Yuzhou Gu\thanks{\texttt{yuzhougu@mit.edu}. MIT.}
\and
Yury Polyanskiy\thanks{\texttt{yp@mit.edu}. MIT.}
}
\else
\title[Weak Recovery Threshold for the Hypergraph Stochastic Block Model]{Weak Recovery Threshold for the Hypergraph Stochastic Block Model}
\coltauthor{%
 \Name{Yuzhou Gu} \Email{yuzhougu@mit.edu}\\
 \addr Masaschusetts Institute of Technology
 \AND
 \Name{Yury Polyanskiy} \Email{yp@mit.edu}\\
 \addr Masaschusetts Institute of Technology%
}
\fi

\maketitle

\begin{abstract}%
  We study the problem of weak recovery for the $r$-uniform hypergraph stochastic block model ($r$-HSBM) with two balanced communities. In HSBM a random graph is constructed by placing hyperedges with higher density if all vertices of a hyperedge share the same binary label. By analyzing contraction of a non-Shannon (symmetric-KL) information measure, we prove that for  $r=3,4$, weak recovery is impossible below the Kesten-Stigum threshold.
  Prior work~\cite{pal2021community} established that weak recovery in HSBM is always possible above the Kesten-Stigum threshold. Consequently, there is no information-computation gap for these $r$, which (partially) resolves a conjecture of \cite{angelini2015spectral}. To our knowledge this is the first impossibility result for HSBM weak recovery.

  As usual, we reduce the study of non-recovery of HSBM to the study of non-reconstruction in a related broadcasting on hypertrees (BOHT) model. While we show that BOHT's reconstruction threshold coincides with Kesten-Stigum for $r=3,4$, surprisingly, we demonstrate that for $r\ge 7$ reconstruction is possible also below the Kesten-Stigum.
  This shows an interesting phase transition in the parameter $r$, and suggests that for $r\ge 7$, there might be an information-computation gap for the HSBM. For $r=5,6$ and large degree we propose an approach for showing non-reconstruction below Kesten-Stigum threshold, suggesting that $r=7$ is the correct threshold for onset of the new phase. We admit that our analysis of the $r=4$ case depends on a numerically-verified inequality.
 %We also discuss a possible approach to resolve the $r=5,6$ case.
%
\end{abstract}

\ifdefined\isarxiv
\else
\begin{keywords}%
  hypergraph stochastic block model, weak recovery, belief propagation, broadcasting on hypertrees, information-computation gap%
\end{keywords}
\fi

% \tableofcontents

\section{Introduction} \label{sec:intro}
\paragraph{Hypergraph stochastic block model}
The stochastic block model (SBM) is a random graph model with community structure, which has received a lot of attention recently, see \cite{abbe2017community} for a survey.
The hypergraph stochastic block model (HSBM) is a generalization of SBM to hypergraphs, which arguably models real social networks better due to the existence of small clusters. It was first considered in \cite{ghoshdastidar2014consistency} and has been studied successively in \cite{angelini2015spectral,ghoshdastidar2015provable,ghoshdastidar2015spectral,ghoshdastidar2017consistency,chien2018community,chien2019minimax,lin2017fundamental,ahn2018hypergraph,kim2018stochastic,cole2020exact,pal2021community,dumitriu2021partial,zhang2022exact,zhang2022sparse}.

In this paper, we consider the $r$-uniform HSBM, where all hyperedges have the same size $r$.
The model has two parameters $a>b\in \bR_{\ge 0}$.
The HSBM hypergraph is generated as following: Let the vertex set be $V=[n]$.
Generate a random color $X_u$ for all vertices $u\in V$ i.i.d.~$\sim \Unif\{\pm\}$. Then, for every $S\in \binom{V}r$, if
all vertices in $S$ have the same color, add hyperedge $S$ with probability $\frac{a}{\binom n{r-1}}$;
otherwise add hyperedge $S$ with probability $\frac{b}{\binom n{r-1}}$.

For SBM and HSBM the most common goal is to recover $X$ by from observing only $G$.
We define the distance between two labellings $X,Y\in \{\pm\}^V$ as
\begin{align}
  d_H(X, Y) = \min_{s\in \{\pm\}}\sum_{u\in V} \mathbbm{1}\{X_i \ne s Y_i\}.
\end{align}
(This is a natural definition, removing dependence on the irrelevant sign flip mistake.)
There are  three kinds of recovery guarantees one may seek.
\begin{itemize}
  \item Exact recovery (strong consistency): The goal is to recover the labels exactly, i.e., to design an estimator $\wh X = \wh X(G)$ such that
  \begin{align}
    \lim_{n\to \infty} \bP[d_H(\wh X,X)] =1.
  \end{align}
  \item Almost exact recovery (weak consistency): The goal is the recover almost all labels, i.e., to design an estimator $\wh X = \wh X(G)$ such that
  \begin{align}
    \lim_{n\to \infty} \bP[d_H(\wh X,X)=o(n)]=1.
  \end{align}
  \item Weak recovery (partial recovery): The goal is to recover a non-trivial fraction of the labels, i.e., to design an estimator $\wh X = \wh X(G)$ such that there exists a constant $c<\frac 12$ such that
  \begin{align}
    \lim_{n\to \infty} \bP[d_H(\wh X,X) \le (c+o(1))n]=1.
  \end{align}
  Note that a trivial algorithm achieves $c = \frac 12$.
\end{itemize}
Different recovery questions are relevant in different parameter regimes.
For exact recovery and almost exact recovery, the phase transition occurs at expected degree of order $\log n$ (i.e., $a,b=\Theta(\log n)$ grows with $n$). For weak recovery, the constant degree regime is relevant. In this paper we focus on weak recovery, thus assuming $a,b$ are absolute constants not depending on $n$.

The phase transition for exact recovery is known \cite{kim2018stochastic,zhang2022exact} for more general HSBMs. For weak recovery, \cite{angelini2015spectral} conjectured that a phase transition occurs at the Kesten-Stigum threshold.
The positive (algorithm) part of their conjecture has been proved by \cite{pal2021community,stephan2022sparse} for more general HSBMs, giving an efficient weak recovery algorithm based on above the Kesten-Stigum threshold.
Despite the progress on the positive part, there has been no progress for the negative (impossibility) part for $r\ge 3$.

For the graph (SBM) case $r=2$, the positive part was proved by \cite{massoulie2014community,mossel2018proof} and the negative part was established by \cite{mossel2015reconstruction,mossel2018proof} via reduction to the broadcasting on trees (BOT) model.
Therefore a natural idea is to study the reconstruction problem for a suitable hypergraph generalization of the BOT model, which we call the broadcasting on hypertrees (BOHT) model.
\cite{zhang2022sparse} mentioned that the difficulty in proving negative results lies in analyzing the broadcasting on hypertrees (BOHT) model.
In this paper we prove impossibility of weak recovery results by proving non-reconstruction results for BOHT.

Before describing the BOHT model, we define the following useful parameters.
\begin{itemize}
  \item For every vertex $u$, the expected number of hyperedges containing $u$ is $d+o(1)$, where
  \begin{align}
    d = \frac{(a-b) + 2^{r-1} b}{2^{r-1}}. \label{eqn:hsbm-d}
  \end{align}
  \item Expected number of vertices adjacent to $u$ is $\alpha+o(1)$, where
  \begin{align}
    \alpha= (r-1)d= (r-1) \frac{(a-b) + 2^{r-1} b}{2^{r-1}}. \label{eqn:hsbm-alpha}
  \end{align}
  \item Expected number of neighbors in the same community minus the number of neighbors in the other community is $\beta+o(1)$ where
  \begin{align}
    \beta = (r-1) \frac{a-b}{2^{r-1}}. \label{eqn:hsbm-beta}
  \end{align}
  \item The strength of the broadcasting channel is charactered by $\lambda$, defined as
  \begin{align}
    \lambda = \frac \beta\alpha = \frac{a-b}{a-b+2^{r-1}b}. \label{eqn:hsbm-lambda}
  \end{align}
  This parameter is useful for connection with Broadcasting on Hypertrees (see Section~\ref{sec:bp-recursion}).
  \item Signal-to-noise ratio
  \begin{align}
  \SNR:= \alpha \lambda^2 = \frac{(r-1) (a-b)^2}{2^{r-1} ((a-b) + 2^{r-1} b)}. \label{eqn:hsbm-snr}
  \end{align}
  This value is conjectured to govern the weak recovery threshold.
\end{itemize}

\paragraph{Broadcasting on hypertrees}
As we mentioned earlier, the broadcasting on trees (BOT) model was used to established the weak recovery threshold for the SBM (i.e., $2$-HSBM).
A natural generalization of the BOT model is the broadcasting on hypertrees (BOHT) model, defined as follows.
The model of three parameters: $r\in \bZ_{\ge 2}$, size of hyperedges; $d$, expected offspring; $\lambda\in [0,1]$, broadcasting channel strength.
Let $T$ be a linear\footnote{Linear means that the intersection of two distinct hyperedges has size at most one.} $r$-uniform hypertree where either (1) every vertex has $d$ downward hyperedges (thus $d(r-1)$ children), or (2) every vertex has $b$ downward hyperedges (thus $b(r-1)$ children), where $b\sim \Po(d)$ is i.i.d.~generated from the Poisson distribution with expectation $d$.
We call the first case the regular hypertree, and the second case the Poisson hypertree.
Given a hypertree $T$ with root $\rho$, we geneate a label $\sigma_u\in \{\pm\}$ for every vertex $u$ via a downward process: (1) $\sigma_\rho \sim \Unif(\{\pm\})$ (2) given $\sigma_u$, for every downward hyperedge $S=\{u,v_1,\ldots,v_{r-1}\}$, we generate $\sigma_{v_1},\ldots,\sigma_{v_{r-1}}$ such that for any $x_1,\ldots,x_{r-1} \in \{\pm\}^{r-1}$,
\begin{align} \label{eqn:boht-B}
  \bP[\sigma_{v_1}=x_1,\ldots,\sigma_{v_{r-1}}=x_{r-1} | \sigma_u]
  = \left\{\begin{array}{ll}
  \lambda+\frac 1{2^{r-1}}(1-\lambda) & \text{if}~x_1=\cdots=x_{r-1}=\sigma_u, \\
  \frac 1{2^{r-1}}(1-\lambda) & \text{o.w.}
  \end{array}\right.
\end{align}
We denote the channel $\sigma_u \to (\sigma_{v_1},\ldots,\sigma_{v_{r-1}})$ as $B=B_{r-1}: \{\pm\} \to \{\pm\}^{r-1}$.
This is a BMS channel (see Section~\ref{sec:bms} for an introduction to BMS channels).
The case $r=2$ is also known as the Ising model on a tree.

The reconstruction problem asks whether we can gain any non-trivial information about the root given observation of far away vertices. In other words, whether the limit
\begin{align}
  \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k})
\end{align}
is non-zero, where $L_k$ is the set of vertices at distance $k$ to the root $\rho$.
When the limit is non-zero, we say the BOHT model has reconstruction; when the limit is zero, we say the model has non-reconstruction.
It is known \cite{pal2021community} that the $r$-neighborhood (for any constant $r$) of a random vertex converges (in the sense of local weak convergence) to the Poisson hypertree described above.
Therefore impossibility of weak recovery for the HSBM can be reduced to non-reconstruction for the corresponding Poisson hypertree.

For the case $r=2$, the reconstruction threshold for the BOT model was established by \cite{bleher1995purity,evans2000broadcasting}. People have also studied generalizations of the BOT model with larger alphabet or asymmetric broadcasting channel, e.g., \cite{mossel2001reconstruction,mossel2003information,mezard2006reconstruction,borgs2006kesten,bhatnagar2010reconstruction,sly2009reconstruction,sly2011reconstruction,kulske2009symmetric,liu2019large,gu2020non,mossel2022exact}.
Nevertheless, to our knowledge, there has been no previous work studying the reconstruction problem for BOHT.

\paragraph{Our results}
Our results on the reconstruction problem for BOHT is summarized as follows.
\begin{theorem}[Reconstruction threshold for BOHT] \label{thm:boht}
  We have the following non-reconstruction results for BOHT.
  \begin{enumerate}[label=(\roman*)]
    \item For $r=3$, the BOHT model has non-reconstruction when $(r-1)d\lambda^2<1$. \label{item:boht:non-recon-r3}
    \item For $r=4$, if Conjecture~\ref{conj:boht-r4} is true, then the BOHT model has non-reconstruction when $(r-1)d\lambda^2<1$. \label{item:boht:non-recon-r4}
    % \item For $r=5,6$, if Conjecture~\ref{conj:boht-r56} is true, then there exists an absolute constant $d_0\in \bR_{\ge 0}$ such that the BOHT model has non-reconstruction when $d\ge d_0$ and $(r-1)d\lambda^2<1$. \label{item:boht:non-recon-r56}
  \end{enumerate}
  We have the following reconstruction results for BOHT.
  \begin{enumerate}[label=(\roman*)]
    \setcounter{enumi}{2}
    \item For $r\ge 2$, the BOHT model has reconstruction when $(r-1)d\lambda^2>1$. \label{item:boht:recon-ks}
    \item For $r\ge 7$, there exists a constant $d_0=d_0(r)$ such that for all $d\ge d_0$, there exists $\lambda\in[0,1]$ such that $(r-1)d\lambda^2<1$ and the BOHT model has reconstruction.
    \label{item:boht:recon-r7}
  \end{enumerate}
\end{theorem}
We note that Conjecture~\ref{conj:boht-r4} is numerically verified.

Theorem~\ref{thm:boht} implies the following impossibility of weak recovery results for HSBM.
\begin{theorem}[Weak recovery threshold for HSBM] \label{thm:hsbm}
  Consider HSBM with parameters $n, r, a, b$.
  Recall $d$ and $\SNR$ defined in~\eqref{eqn:hsbm-d}\eqref{eqn:hsbm-snr}.
  \begin{enumerate}[label=(\roman*)]
    \item For $r=3$, weak recovery is impossible for the HSBM when $\SNR<1$. \label{item:hsbm:r3}
    \item For $r=4$, if Conjecture~\ref{conj:boht-r4} is true, weak recovery is impossible for the HSBM when $\SNR<1$. \label{item:hsbm:r4}
    % \item For $r=5,6$, if Conjecture~\ref{conj:boht-r56} is true, then there exists an absolute constant $d_0\in \bR_{\ge 0}$ such thatweak recovery is impossible for the HSBM when $d\ge d_0$ and $\SNR<1$. \label{item:hsbm:r56}
  \end{enumerate}
\end{theorem}

\paragraph{Our technique}
We prove Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r3}\ref{item:boht:non-recon-r4}
by considering a version of mutual information based on a non-Shannon information measure, see~\cite[Section 7.8]{polyanskiy2023information} for exposition on such. The particular $f$-divergence we choose is known as symmetrized KL divergence (SKL).
It is known that SKL capacity can be used to prove non-reconstruction results since at least \cite{kulske2009symmetric}. Information-theoretically, SKL information is special due to its additivity (as opposed to subadditivity) under conditionally independent observations~\cite[Section 7.8]{polyanskiy2023information}.

Interestingly, before our work, to the best of our knowledge,
the non-reconstruction results proved via SKL capacity could always be also shown via other information measures ($\chi^2$-capacity~\cite{evans2000broadcasting}, KL capacity~\cite{gu2020non}, etc.). It appears, thus, that BOHT is the first example where contraction via SKL capacity gives better results than any other information measures we have tried.

Theorem~\ref{thm:boht}\ref{item:boht:recon-ks} is an immediate consequence of the weak recovery results of \cite{pal2021community}.
% \yuzhou{If we have time, prove it via majority decider.???}

Theorem~\ref{thm:boht}\ref{item:boht:recon-r7} is proved using contraction of $\chi^2$-capacity and Gaussian approximation, which has proved successful in many different settings \cite{sly2009reconstruction,sly2011reconstruction,liu2019large,mossel2022exact}.
% In this method, we (1) use Gaussian approximation for large enough degree to prove contraction of $\chi^2$-capacity (also called magnetization in previous works), when the $\chi^2$-capacity is not too small
% (2) for $\chi^2$-capacity small enough, prove contraction via analysis of first few terms of the Taylor series.


Theorem~\ref{thm:hsbm} is an immediate consequence of Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r3}\ref{item:boht:non-recon-r4}, via a coupling between local neighborhoods in the HSBM hypergraph and BOHT, established by \cite{pal2021community}. A general theme for SBM-like models is that non-reconstruction results for local trees imply impossibility of weak recovery for corresponding graphical models.


\paragraph{Structure of the paper}
In Section~\ref{sec:bms}, we review preliminaries on BMS channels.
In Section~\ref{sec:bp-recursion}, we describe the belief propagation recursion.
In Section~\ref{sec:recon-r34}, we prove our results on BOHT not relying on large degree $d$ (Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r3}\ref{item:boht:non-recon-r4}).
In Section~\ref{sec:recon-large-deg}, we prove our results on BOHT for large degree $d$ (Theorem~\ref{thm:boht}\ref{item:boht:recon-r7}).
In Section~\ref{sec:hsbm}, we prove impossibility of weak recovery results for the HSBM (Theorem~\ref{thm:hsbm}), and reconstruction for BOHT above the Kesten-Stigum threshold (Theorem~\ref{thm:boht}\ref{item:boht:recon-ks}).
In Section~\ref{sec:discussion}, we discuss a possible approach to resolve the $r=5,6$ case.

In Section~\ref{sec:proof-recon-r34}, we give missing proofs in Section~\ref{sec:recon-r34}.
In Section~\ref{sec:proof-recon-large-deg}, we give missing proofs in Section~\ref{sec:recon-large-deg}.


\section{Preliminaries on BMS channels} \label{sec:bms}
We give necessary preliminaries on BMS channels that are used in the proof of Theorem~\ref{thm:boht}. Most material in this section can be found in e.g.,~\cite[Chapter 4]{richardson2008modern} or~\cite[Section 4]{abbe2021stochastic}.

\begin{definition} \label{defn:bms}
  A channel $P: \{\pm\} \to \cY$ is called a binary memory less symmetric (BMS) channel if there exists a measurable involution $\sigma: \cY\to \cY$ such that $P(E|+) = P(\sigma(E)|-)$ for all measurable subsets $E\subseteq \cY$.
\end{definition}
Binary erasure channels (BECs) and binary symmetric channels (BSCs) are the simplest examples of BMS channels.
Channel $B$ defined in~\eqref{eqn:boht-B} or the channel $M_k$ in Section~\ref{sec:bp-recursion} are also naturally BMS channels.

BMS channels are equivalent to distributions on the interval $[0,\frac 12]$, via the following lemma.
\begin{lemma} \label{lem:bms-mixture}
  Every BMS channel $P$ is equivalent to a channel $X\to (\Delta,Z)$ where $\Delta\in [0,\frac 12]$ is independent of $X$ and $P_{Z|\Delta,X} = \BSC_\Delta(\cdot | X)$.
\end{lemma}
In the setting of Lemma~\ref{lem:bms-mixture}, we say $\Delta$ is the $\Delta$-component of $P$.
We define $\theta=1-2\Delta \in [0,1]$ to be the $\theta$-component of $P$, because it sometimes simplifies the notation.

% We define degradation between BMS channels.
% \begin{definition}
%   Let $P: \{\pm\}\to \cY$ and $Q: \{\pm\}\to \cZ$ be two BMS channels.
%   We say $P$ is more degraded than $Q$ (denoted $P\le_{\deg} Q$), if there exists a channel $R: \cZ \to \cY$ preserving $\sigma$ action such that $P=R\circ Q$.
% \end{definition}

\begin{definition}\label{defn:information-measures}
  Let $P$ be a BMS channel, $\Delta$ be its $\Delta$-component, and $\theta$ be its $\theta$-component. We define the following information measures.
  \begin{align}
    P_e(P) &= \bE \Delta, \tag{probability of error}\\
    C(P) & = \bE [\log 2 + \Delta \log \Delta + (1-\Delta)\log(1-\Delta)], \tag{capacity}\\
    C_{\chi^2}(P) &= \bE \theta^2, \tag{$\chi^2$-capacity}\\
    C_{\SKL}(P) &= \bE\left[\left(\frac12-\Delta\right)\log \frac{1-\Delta}{\Delta}\right] = \bE\left[\theta \arctanh\theta\right]. \tag{$\SKL$ capacity}
  \end{align}
\end{definition}

Given two channels $P: \cX \to \cY$ and $Q: \cX' \to \cY'$, we define their tensor product $P\otimes Q : \cX \times \cX' \to \cY \times \cY'$ by letting them acting on two inputs independently.
We define $P^{\otimes n}: \cX^n\to \cY^n$ be the tensor product of $n$ copies of $P$.

Given two channels $P: \cX\to \cY$ and $Q: \cX\to \cY'$, we define the $\star$-convolution $P\star Q: \cX \to \cY\times \cY'$ by letting $P$ and $Q$ acting on the same input.
We define $P^{\star n}: \cX\to \cY^n$ to be the $\star$-convolution of $n$ copies of $P$.

\section{Belief propagation recursion} \label{sec:bp-recursion}
In this section we give the formula for belief propagation (BP) recursion.
We take an information channel point of view, which understands the BP recursion as an operator from the space of BMS channels (equivalently, the space of distributions on $[0,\frac12]$, via Lemma~\ref{lem:bms-mixture}) to itself.

We consdier the broadcasting on hypertrees (BOHT) model defined in Section~\ref{sec:intro} with parameters $r, d, \lambda$.
Let $M_k$ be the channel $\sigma_\rho \to \sigma_{L_k}$.
Then $M_k$ is naturally a BMS channel.
Channel $M_{k+1}$ can be built from $M_k$ using BP recursion.
Define $\BP=\BP_{r,d,\theta} : \{\text{BMS}\} \to \{\text{BMS}\}$ be the operator defined as
\begin{align}
  \BP(P) &:= \bE_{t\sim D} (P^{\otimes (r-1)} \circ B)^{\star t},
\end{align}
where $D$ is the offspring distribution ($D=\mathbbm{1}\{d\}$ for the regular hypertree case, $D=\Po(d)$ for the Poisson hypertree case).
Recall that $B$ is the channel $\sigma_u \to (\sigma_{v_1},\ldots,\sigma_{v_{r-1}})$ for a single hyperedge, defined in~\eqref{eqn:boht-B}.
Then we have
\begin{align}
  M_{k+1} = \BP(M_k).
\end{align}

Our goal in this section is to derive a formula for $P^{\otimes (r-1)} \circ B$ in terms of the $\theta$-component.

By Lemma~\ref{lem:bms-mixture}, we only need to describe $(\BSC_{\Delta_1}\otimes \cdots \otimes \BSC_{\Delta_{r-1}})\circ B$.
Let $\theta_i := 1-2\Delta_i$ for $i\in [r-1]$.
For $x\in \{\pm\}^{r-1}$, we have
\begin{align}
  & ((\BSC_{\Delta_1}\otimes \cdots \otimes \BSC_{\Delta_{r-1}})\circ B)(x_1,\ldots,x_{r-1} | +) \\
  \nonumber =&~ \sum_{y\in \{\pm\}^{r-1}} B(y_1,\ldots,y_{r-1} |+) \prod_{i\in [r-1]} \BSC_{\Delta_i}(x_i | y_i) \\
  \nonumber =&~ \lambda \prod_{i\in [r-1]} \BSC_{\Delta_i}(x_i | +)
  + \frac 1{2^{r-1}} (1-\lambda) \prod_{i\in [r-1]} \sum_{y_i\in\{\pm\}} \BSC_{\Delta_i}(x_i| y_i) \\
  \nonumber =&~ \lambda \prod_{i\in [r-1]} \left(\frac12 + (\frac12-\Delta_i) x_i\right) + \frac 1{2^{r-1}}(1-\lambda)\\
  \nonumber =&~ \lambda \prod_{i\in [r-1]} \left(\frac12 + \frac12 \theta_i x_i\right) + \frac 1{2^{r-1}}(1-\lambda).
\end{align}

So $(\BSC_{\Delta_1}\otimes \cdots \otimes \BSC_{\Delta_{r-1}})\circ B$ is a mixture of $2^{r-2}$ BSCs,
indexed by the set
\begin{align}
  \left\{x: x\in \{\pm\}^{r-1}, x_1=+\right\},
\end{align}
where the BSC corresponding to $x$ has weight (probability)
\begin{align}
  & \left(\lambda \prod_{i\in [r-1]} \left(\frac12 + \frac12\theta_i x_i\right) + \frac 1{2^{r-1}}(1-\lambda)\right)
   + \left(\lambda \prod_{i\in [r-1]} \left(\frac12 - \frac12\theta_i x_i\right) + \frac 1{2^{r-1}}(1-\lambda)\right)\\
  \nonumber =&~ \lambda \left(\prod_{i\in [r-1]} \left(\frac12 + \frac12\theta_i x_i\right) +\prod_{i\in [r-1]} \left(\frac12 - \frac12\theta_i x_i\right)\right) + \frac 1{2^{r-2}}(1-\lambda)
\end{align}
and $\theta$ parameter equal to the absolute value of
\begin{align}
  &~\frac{\left(\lambda \prod_{i\in [r-1]} \left(\frac12 + \frac12\theta_i x_i\right) + \frac 1{2^{r-1}}(1-\lambda)\right)
  - \left(\lambda \prod_{i\in [r-1]} \left(\frac12 - \frac12\theta_i x_i\right) + \frac 1{2^{r-1}}(1-\lambda)\right)}{\left(\lambda \prod_{i\in [r-1]} \left(\frac12 + \frac12\theta_i x_i\right) + \frac 1{2^{r-1}}(1-\lambda)\right)
  + \left(\lambda \prod_{i\in [r-1]} \left(\frac12 - \frac12\theta_i x_i\right) + \frac 1{2^{r-1}}(1-\lambda)\right)}\\
  \nonumber =&~\frac{\lambda \left( \prod_{i\in [r-1]} \left(\frac12 + \frac12\theta_i x_i\right) - \prod_{i\in [r-1]} \left(\frac12 - \frac12\theta_i x_i\right)\right)}{\lambda \left(\prod_{i\in [r-1]} \left(\frac12 + \frac12\theta_i x_i\right) +\prod_{i\in [r-1]} \left(\frac12 - \frac12\theta_i x_i\right)\right) + \frac 1{2^{r-2}}(1-\lambda)}\\
  \nonumber =&~ \frac{\lambda \left( \prod_{i\in [r-1]} \left(1+\theta_i x_i\right) - \prod_{i\in [r-1]} \left(1-\theta_i x_i\right)\right)}{\lambda \left(\prod_{i\in [r-1]} \left(1+\theta_i x_i\right) +\prod_{i\in [r-1]} \left(1-\theta_i x_i\right)\right) + 2(1-\lambda)}.
\end{align}
Although we need to take absolute value, information measures (except for $P_e$) in Definition~\ref{defn:information-measures} are all even functions in $\theta$.
So we do not need to worry about the sign.

\section{Reconstruction threshold for \texorpdfstring{$r=3,4$}{r=3,4}} \label{sec:recon-r34}
In this section we prove Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r3}\ref{item:boht:non-recon-r4}.
Our method is via contraction of SKL capacity (Definition~\ref{defn:information-measures}).
That is, we prove that
\begin{align} \label{eqn:skl-contraction}
  C_{\SKL}(\BP(P)) \le (r-1)d\lambda^2 C_{\SKL}(P)
\end{align}
for all BMS $P$.

One nice property of the SKL capacity is that it is additive under $\star$-convolution.
\begin{lemma} \label{lem:skl-additive}
  For two BMSs $P$ and $Q$ we have
  \begin{align}
    C_{\SKL}(P\star Q) = C_{\SKL}(P) + C_{\SKL}(Q).
  \end{align}
\end{lemma}
See e.g., \cite{kulske2009symmetric} for a proof.

Applying Lemma~\ref{lem:skl-additive}, we get
\begin{align}
  C_{\SKL}(\BP(P)) = \bE_t C_{\SKL}((P^{\otimes (r-1)} \circ B)^{\star t}) = d C_{\SKL}(P^{\otimes (r-1)} \circ B)
\end{align}
where $t$ is the offspring ($t=d$ for regular hypertrees, $t\sim \Po(d)$ for Poisson hypertrees).
Therefore it suffices to prove
\begin{align} \label{eqn:skl-contraction-edge}
  C_{\SKL}(P^{\otimes (r-1)} \circ B) \le (r-1)\lambda^2 C_{\SKL}(P)
\end{align}

By Lemma~\ref{lem:bms-mixture}, we can reduce \eqref{eqn:skl-contraction-edge} to
\begin{align} \label{eqn:skl-contraction-edge-bsc}
  C_{\SKL}\left((\BSC_{\Delta_1} \otimes \cdots \otimes \BSC_{\Delta_{r-1}}) \circ B\right) \le \lambda^2 \sum_{i\in [r-1]} C_{\SKL}(\BSC_{\Delta_i})
\end{align}
for all $\Delta_1,\ldots,\Delta_{r-1} \in [0,\frac 12]$.

For $r=3$, \eqref{eqn:skl-contraction-edge-bsc} indeed holds.
\begin{lemma} \label{lem:skl-contraction-edge-bsc-r3}
For any $\Delta_1,\Delta_2\in[0,\frac 12]$, we have
\begin{align} \label{eqn:skl-contraction-edge-bsc-r3}
  C_{\SKL}((\BSC_{\Delta_1} \otimes \BSC_{\Delta_2}) \circ B) \le \lambda^2 (C_{\SKL}(\BSC_{\Delta_1}) + C_{\SKL}(\BSC_{\Delta_2})).
\end{align}
\end{lemma}
Proof is deferred to Section~\ref{sec:proof-recon-r34:boht-technical-r3}.

For $r=4$, \eqref{eqn:skl-contraction-edge-bsc} holds conditioned on a numerically-verified conjecture.
\begin{lemma} \label{lem:skl-contraction-edge-bsc-r4}
If Conjecture~\ref{conj:boht-r4} is true, then for all $\Delta_1,\Delta_2,\Delta_3\in [0,\frac 12]$, we have
\begin{align} \label{eqn:skl-contraction-edge-bsc-r4}
  C_{\SKL}((\BSC_{\Delta_1} \otimes \BSC_{\Delta_2} \otimes \BSC_{\Delta_3}) \circ B) \le \lambda^2 \sum_{i\in [3]} C_{\SKL}(\BSC_{\Delta_i}).
\end{align}
\end{lemma}
\begin{conjecture} \label{conj:boht-r4}
  For $\lambda,\theta_1,\theta_2,\theta_3\in [0,1]$, the following inequality holds:
  \begin{align}
    &~ \frac 14 (G_\lambda(\theta_1,\theta_2,\theta_3) + G_\lambda(\theta_1,-\theta_2,\theta_3) + G_\lambda(\theta_1,\theta_2,-\theta_3) + G_\lambda(\theta_1,-\theta_2,-\theta_3)) \\
    \le&~ \lambda \sum_{i\in [3]} \theta_i \arctanh \theta_i, \nonumber
  \end{align}
  where
  \begin{align}
    G_\lambda(\theta_1,\theta_2,\theta_3)&:= (\theta_1+\theta_2+\theta_3+\theta_1\theta_2\theta_3) F_\lambda(\theta_1,\theta_2,\theta_3), \\
    F_\lambda(\theta_1,\theta_2,\theta_3)&:= \arctanh \frac{\lambda(\theta_1+\theta_2+\theta_3+\theta_1\theta_2\theta_3)}{1+\lambda(\theta_1\theta_2+\theta_2\theta_3+\theta_3\theta_1)}.
  \end{align}
\end{conjecture}
Proof of Lemma~\ref{lem:skl-contraction-edge-bsc-r4} is deferred to Section~\ref{sec:proof-recon-r34:boht-technical-r4}.

We remark that for $r\ge 5$ we have found counterexamples to~\eqref{eqn:skl-contraction-edge} and~\eqref{eqn:skl-contraction-edge-bsc}. Therefore the SKL contraction method does not seem to work for $r\ge 5$.

With Lemma~\ref{lem:skl-contraction-edge-bsc-r3} and~\ref{lem:skl-contraction-edge-bsc-r4}, we can prove Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r3}\ref{item:boht:non-recon-r4}.
\begin{proof}[Proof of Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r3}\ref{item:boht:non-recon-r4}]
Under the conditions in Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r3}\ref{item:boht:non-recon-r4}, \eqref{eqn:skl-contraction-edge-bsc} holds by Lemma~\ref{lem:skl-contraction-edge-bsc-r3} and~\ref{lem:skl-contraction-edge-bsc-r4}.
By the above discussion,~\eqref{eqn:skl-contraction} holds.
Let $\Id = \BSC_0$ be the identity channel.
Then $C_{\SKL}(M_1) = C_{\SKL}(\BP(\Id)) < \infty$.
By~\eqref{eqn:skl-contraction}, we have
\begin{align}
  C_{\SKL}(M_k) = C_{\SKL}(\BP^{k-1}(M_1))
  = ((r-1)d\lambda^2)^{k-1} (M_1).
\end{align}
So
\begin{align}
  \lim_{k\to \infty} C_{\SKL}(M_k) = 0.
\end{align}
Finally,
\begin{align}
  \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k}) = \lim_{k\to \infty} C(M_k) \le \lim_{k\to \infty} C_{\SKL}(M_k) = 0.
\end{align}
So non-reconstruction holds.
\end{proof}

\section{Reconstruction threshold for large degree} \label{sec:recon-large-deg}
In this section we prove Theorem~\ref{thm:boht}\ref{item:boht:recon-r7}.
Our proof is an analysis of evolution of $\chi^2$-capacity (also called magnetization in literature) and Gaussian approximation for large degree.

\subsection{Behavior of \texorpdfstring{$\chi^2$}{chi2}-capacity}
% The proof is in two steps. In the first step (Prop.~\ref{prop:large-deg-step}), we use Gaussian approximation for large enough degree to prove contraction of $\chi^2$-capacity when it is not too small.
% In the second step (Prop.~\ref{prop:small-chi2-step}), we use Taylor series expansion to prove that $\chi^2$-capacity contracts under every step of $\BP$ when it is small enough.

\begin{proposition}[Large degree asymptotics] \label{prop:large-deg-step}
Fix $r\in \bZ_{\ge 2}$. For any $\epsilon>0$, there exists $d_0=d_0(r,\epsilon)>0$ such that for any $d\ge d_0$ and $\lambda\in [0,1]$ with $(r-1)d \lambda^2 \le 1$, for any BMS channel $P$ we have
\begin{align}
  | C_{\chi^2}(\BP(P)) - g_{r,d,\lambda} (C_{\chi^2}(P)) | \le \epsilon,
\end{align}
where
\begin{align}
  g_{r,d,\lambda}(x) &:= \bE_{Z\sim \cN(0,1)} \tanh\left(s_{r,d,\lambda}(x) + \sqrt{s_{r,d,\lambda}(x)} Z\right), \\
  s_{r,d,\lambda}(x) &:= d\lambda^2 \cdot \frac 12 \left((1+x)^{r-1} - (1-x)^{r-1}\right).
\end{align}
\end{proposition}
Proof is deferred to Section~\ref{sec:proof-recon-large-deg:large-deg-step}.

% \begin{proposition}[Taylor expansion] \label{prop:small-chi2-step}
% Fix $r\in \bZ_{\ge 2}$.
% For any $\epsilon>0$, there exists $\delta=\delta(r,\epsilon)>0$ and $k_0=k_0(r,\epsilon)\in \bZ_{\ge 0}$, such that if $\limsup_{k\to \infty} C_{\chi^2}(M_k) \le \delta$ and $k\ge k_0$, then
% \begin{align}
%   \left|C_{\chi^2}(M_{k+1}) - f_{r,d,\lambda}(C_{\chi^2}(M_k))\right| \le \epsilon \cdot C_{\chi^2}^2(M_k),
% \end{align}
% where
% \begin{align}
%   f_{r,d,\lambda}(x) := (r-1) d\lambda^2 x - \left( (r-1)(r-2) d \lambda^3 + (r-1)^2 d(d-1) \lambda^4\right) x^2. \label{eqn:small-chi2-step-f}
% \end{align}
% \end{proposition}
% Proof is deferred to Section~\ref{sec:proof-recon-large-deg:small-chi2-step}.

\subsection{Properties of functions} \label{sec:recon-large-deg:func-property}
In this section we state some few properties of important functions. For $r\ge 2$, we define
\begin{align}
  g_r(x) &:= \bE_{Z\sim \cN(0,1)} \tanh\left(s_r(x) + \sqrt{s_r(x)} Z\right),  \label{eqn:large-deg-step-g} \\
  s_r(x) &:= \frac 1{2(r-1)} \left((1+x)^{r-1} - (1-x)^{r-1}\right).
\end{align}

\begin{lemma} \label{lem:large-deg-step-g-monotone}
  For any $r\ge 2$, the function $g_r$ is strictly increasing and continuous differentiable on $[0,1]$.
\end{lemma}
Proof of Lemma~\ref{lem:large-deg-step-g-monotone} is deferred to Section~\ref{sec:proof-recon-large-deg:func-property}.

% \begin{conjecture} \label{conj:boht-r56}
%   For $r=5,6$ and $0<x \le 1$, we have $g_r(x)<x$.
% \end{conjecture}
% Figure~\ref{fig:conj-boht-r56} shows numerical evidence supporting Conjecture~\ref{conj:boht-r56}.
% Previous works \cite{sly2011reconstruction,mossel2022exact} encountered similar kinds of inequalities, and they were proved using a rigorous numerical integration method. We believe Conjecture~\ref{conj:boht-r56} can be proved using a similar method.
% \begin{figure}
%   \centering
%   \includegraphics[width=0.45\textwidth]{fig/g5.png}
%   \qquad
%   \includegraphics[width=0.45\textwidth]{fig/g6.png}
%   \caption{Numerical evidence supporting Conjecture~\ref{conj:boht-r56}.
%     Left: Plot of $y=g_5(x)$ and $y=x$ for $x\in [0,1]$.
%     Right: Plot of $y=g_6(x)$ and $y=x$ for $x\in [0,1]$.}
%   \label{fig:conj-boht-r56}
% \end{figure}

\begin{lemma} \label{lem:large-deg-step-g-r7}
  For $r\ge 7$, there exists $x\in [0, 1]$ such that $g_r(x)>x$.
\end{lemma}
Proof of Lemma~\ref{lem:large-deg-step-g-r7} is deferred to Section~\ref{sec:proof-recon-large-deg:func-property}.

% \begin{lemma} \label{lem:large-deg-step-f}
%   There exist constants $\epsilon>0$, $d_0\in \bZ_{\ge 0}$, such that for any $r\ge 2$, $d\ge d_0$, $\lambda\in [0,1]$ satisfying $(r-1)d\lambda^2 \le 1$, $0<x\le 1$, we have
%   \begin{align}
%     f_{r,d,\lambda}(x) + \epsilon x^2 < x,
%   \end{align}
%   where $f_{r,d,\lambda}$ is defined in~\eqref{eqn:small-chi2-step-f}.
% \end{lemma}


\subsection{Proof of Theorem~\ref{thm:boht}\ref{item:boht:recon-r7}}
% In this section we combine Prop.~\ref{prop:large-deg-step} and~\ref{prop:small-chi2-step} to prove Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r56}\ref{item:boht:recon-r7}.
In this section we prove Theorem~\ref{thm:boht}\ref{item:boht:recon-r7}.

\begin{proof}[Proof of Theorem~\ref{thm:boht}\ref{item:boht:recon-r7}]
  Choose $x\in [0,1]$ so that $g_r(x) > x$ via Lemma~\ref{lem:large-deg-step-g-r7}.
  By continuity of $g_r$ (Lemma~\ref{lem:large-deg-step-g-monotone}), there exists $\epsilon>0$ such that $g_{r,d,\lambda}(x) > x + \epsilon$ for $(r-1)d\lambda^2=1-\epsilon$.
  Note that $g_{r,d,\lambda}(x)$'s dependence on $d$ and $\lambda$ is only through $d\lambda^2$.

  Take $d_0 = d_0(r,\epsilon)$ in Prop.~\ref{prop:large-deg-step}.
  For any $d>d_0$, choose $\lambda\in[0,1]$ such that $(r-1)d\lambda^2 = 1-\epsilon$.
  By Prop.~\ref{prop:large-deg-step}, choice of $\epsilon$, and Lemma~\ref{lem:large-deg-step-g-monotone}, for all BMS $P$ with $C_{\chi^2}(P) \ge x$ we have
  \begin{align}
    C_{\chi^2}(\BP(P)) \ge g_{r,d,\lambda}(C_{\chi^2}(P)) - \epsilon \ge x.
  \end{align}
  Therefore
  \begin{align}
    \lim_{k\to \infty} I_{\chi^2}(\sigma_\rho; \sigma_{L_k}) = \lim_{k\to \infty} C_{\chi^2}(M_k) \ge x.
  \end{align}
  Finally
  \begin{align}
    \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k}) \ge \lim_{k\to \infty} \frac12 I_{\chi^2}(\sigma_\rho; \sigma_{L_k}) \ge \frac x2,
  \end{align}
  where the first step is because $C(P) \ge \frac 12 C_{\chi^2}(P)$ for any BMS $P$.
\end{proof}

% \begin{proof}[Proof of Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r56}]
%   By Prop.~\ref{prop:small-chi2-step} and Lemma~\ref{lem:large-deg-step-f}, there exists $\delta=\delta(r)>0$, $d_0=d_0(r)\in \bZ_{\ge 0}$, $k_0=k_0(r)\in \bZ_{\ge 0}$ such that for $d\ge d_0$, $\lambda\in[0,1]$ satisfying $(r-1)d\lambda^2<1$, if $\limsup_{k\to \infty} C_{\chi^2}(M_k) \le \delta$ and $k\ge k_0$, then $C_{\chi^2}(M_{k+1}) < C_{\chi^2}(M_k)$.

%   It remains to prove that ther exists $d_0' \in \bZ_{\ge 0}$ such that for all $d\ge d_0'$, $\lambda\in[0,1]$ satisfying $(r-1)d\lambda^2<1$, we have $\limsup_{k\to \infty} C_{\chi^2}(M_k) \le \delta$.

%   Choose $\epsilon>0$ so that $\epsilon < \delta/3$ and $g_r(x) < x-2 \epsilon$ for all $x\in [\delta, 1]$, where $g_r$ is defined in~\eqref{eqn:large-deg-step-g}.
%   Such $\epsilon$ exists by Lemma~\ref{lem:large-deg-step-g-monotone}, Conjecture~\ref{conj:boht-r56} and compantness of interval $[\delta, 1]$.

%   Take $d_0'=d_0(r,\epsilon)$ in Prop.~\ref{prop:large-deg-step}.
%   Now fix $d>\max\{d_0',d_0\}$ and $\lambda\in[0,1]$ such that $(r-1)d\lambda^2<1$.
%   By Prop.~\ref{prop:large-deg-step} and choice of $\epsilon$, we have
%   \begin{align}
%     C_{\chi^2}(\BP(P)) \le g_r(C_{\chi^2}(P)) + \epsilon \le C_{\chi^2}(P) - \epsilon
%   \end{align}
%   for all BMS $P$ with $C_{\chi^2}(P) \ge \delta$.
%   Therefore there exists $k_0\in \bZ_{\ge 0}$ such that for $k\ge k_0$, we have $C_{\chi^2}(M_k) \le \delta$.

%   By our choice of $\delta$, $C_{\chi^2}(M_{k+1}) < C_{\chi^2}(M_{k})$ for all $k\ge k_0$.
%   Therefore the limit
%   \begin{align} \label{eqn:proof-thm-boht-non-recon-r56-1}
%     \lim_{k\to \infty} C_{\chi^2}(M_k)
%   \end{align}
%   exists and is $< \delta$.
%   So the limit~\eqref{eqn:proof-thm-boht-non-recon-r56-1} is zero.

%   Finally,
%   \begin{align}
%     \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k}) \le \lim_{k\to \infty} I_{\chi^2}(\sigma_\rho; \sigma_{L_k})\log 2 = 0.
%   \end{align}
%   This finishes the proof.
% \end{proof}


\section{Weak recovery threshold for HSBM} \label{sec:hsbm}
In this section we prove Theorem~\ref{thm:hsbm} and Theorem~\ref{thm:boht}\ref{item:boht:recon-ks}.
We use the following result from \cite[Theorem 5.2]{pal2021community}.
\begin{proposition}[\cite{pal2021community}] \label{prop:hsbm-boht-coupling}
  Let $(X,G)\sim \mathrm{HSBM}(n,r,\frac{a}{\binom n{r-1}},\frac{b}{\binom{n}{r-1}})$.
  Let $v\in V$ and $k=o(\log n)$. Let $B(v,k)$ be the set of vertices with distance $\le k$ to $v$, and
  $\partial B(v,k)$ be the set of vertices at distance $k$ to $v$.

  Let $(T,\sigma)$ be the BOHT model on a Poisson hypertree with expected offspring $d$ and broadcasting channel strength $\lambda$, with $d$ defined in~\eqref{eqn:hsbm-d} and $\lambda$ defined in~\eqref{eqn:hsbm-lambda}.
  Let $\rho$ be the root of $T$, $L_k$ be the set of vertices at distance $k$ to $\rho$, $T_k$ be the set of vertices at distance $\le k$ to $\rho$.

  Then $(G|_{B(v,k)}, X_{B(v,k)})$ can be coupled (with $o(1)$ TV distance) to $(T_k, \sigma_{T_k})$.
\end{proposition}

\begin{proof}[Proof of Theorem~\ref{thm:hsbm}]
  For constant $k\in \bZ_{\ge 0}$, by Prop.~\ref{prop:hsbm-boht-coupling}, for any fixed vertices $u\ne v\in V$, $\bP[u\in B(v,k)] = o(1)$.
  Therefore
  \begin{align}
    I(X_v; G, X_u) \le I(X_v; G, X_{\partial B(v,k)}, X_u)
    = I(\sigma_\rho; T_k, \sigma_{L_k}) + o(1),
  \end{align}
  where the first step is by data processing inequality, and the second step is by Prop.~\ref{prop:hsbm-boht-coupling}.
  Taking limit $n\to \infty$, then taking limit $k\to \infty$, we get
  \begin{align}
    \lim_{n\to \infty} I(X_v; G, X_u) \le \lim_{k\to \infty} I(\sigma_\rho; T_k, \sigma_{L_k}).
  \end{align}
  Under the conditions in Theorem~\ref{thm:hsbm}, RHS is zero by Theorem~\ref{thm:boht}\ref{item:boht:non-recon-r3}\ref{item:boht:non-recon-r4}.
  Therefore for any fixed vertices $u\ne v\in V$, we have
  \begin{align}
    \lim_{n\to \infty} P_e(X_u | G, X_v) = \frac 12.
  \end{align}
  This implies weak recovery is impossible by e.g.,~\cite[Remark 1]{mossel2015reconstruction}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:boht}\ref{item:boht:recon-ks}]
  By proof of Theorem~\ref{thm:hsbm}, non-reconstruction of the BOHT model implies impossibility of weak recovery results for the corresponding HSBM.
  However, by~\cite{pal2021community}, there is an algorithm for weak recovery above the Kesten-Stigum threshold.
  Therefore, BOHT model must admit reconstruction above the Kesten-Stigum threshold.
\end{proof}
Alternatively, Theorem~\ref{thm:boht}\ref{item:boht:recon-ks} can be proved directly via a majority decider. We omit the details.

\section{Discussions} \label{sec:discussion}
We have left the $r=5,6$ case open in Theorem~\ref{thm:boht}.
Our preliminary computations suggest that for $r=5,6$, there exists an absolute constant $d_0\in \bR_{\ge 0}$ such that the BOHT model has non-reconstruction when $d\ge d_0$ and $(r-1)d\lambda^2\le 1$.
We believe that a generalization of Sly's method~\cite{sly2011reconstruction,mossel2022exact} can be used to prove this. In Sly's method, we compute the first few orders of the BP recursion formula. Combined with Gaussian approximation this would imply contraction of $\chi^2$-capacity.
One technical challenge is that in the BOHT case we need a two-step application of Sly's method, in contrast with previous works.


% Acknowledgments---Will not appear in anonymized version
\ifdefined\isarxiv
\section*{Acknowledgments}
Research was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.
\else
\acks{We thank a bunch of people and funding agency.\yuzhou{todo}}
\fi

\ifdefined\isarxiv
\bibliographystyle{alpha}
\fi
\bibliography{ref}
\appendix

\section{Proofs in Section~\ref{sec:recon-r34}} \label{sec:proof-recon-r34}
\subsection{Proof of Lemma~\ref{lem:skl-contraction-edge-bsc-r3}} \label{sec:proof-recon-r34:boht-technical-r3}
In this section we prove Lemma~\ref{lem:skl-contraction-edge-bsc-r3}.

\begin{proof}[Proof of Lemma~\ref{lem:skl-contraction-edge-bsc-r3}]
  We expand LHS of~\eqref{eqn:skl-contraction-edge-bsc-r3} using the BP recursion formula established in Section~\ref{sec:bp-recursion}.
  Let $\theta_i=1-2\Delta_i$ for $i=1,2$. Then
  \begin{align} \label{eqn:skl-contraction-edge-bsc-r3-1}
    & C_{\SKL}((\BSC_{\Delta_1} \otimes \BSC_{\Delta_2})\circ B) \\
    \nonumber =&~ \sum_{x_1=+,x_2\in \{\pm\}}
    \frac 12 \lambda (\theta_1 x_1 + \theta_2 x_2)
    \arctanh\frac{\lambda(\theta_1 x_1 + \theta_2 x_2)}{\lambda(1 + \theta_1 x_1 \theta_2 x_2) + (1-\lambda)} \\
    \nonumber =&~ \lambda \left(\frac 12 (\theta_1+\theta_2) \arctanh\frac{\lambda(\theta_1+\theta_2)}{1+\lambda \theta_1 \theta_2} + \frac 12 (\theta_1-\theta_2) \arctanh\frac{\lambda(\theta_1-\theta_2)}{1-\lambda \theta_1 \theta_2}\right) \\
    \nonumber =&~ \lambda \left(\frac 12 (\theta_1+\theta_2)F_\lambda(\theta_1,\theta_2) + \frac 12 (\theta_1-\theta_2) F_\lambda(\theta_1,-\theta_2)\right)
  \end{align}
  where
  \begin{align}
    F_\lambda(\theta_1,\theta_2) := \arctanh\frac{\lambda(\theta_1 + \theta_2)}{1+\lambda \theta_1 \theta_2}.
  \end{align}
  Note that by definition, $F_\lambda(\theta_1,\theta_2) = -F_\lambda(-\theta_1,-\theta_2)$
  and $F_\lambda(\theta_1,\theta_2) = F_\lambda(\theta_2,\theta_1)$.

  We have
  \begin{align} \label{eqn:skl-contraction-edge-bsc-r3-2}
    &~ \frac 12 (\theta_1+\theta_2)F_\lambda(\theta_1,\theta_2) + \frac 12 (\theta_1-\theta_2) F_\lambda(\theta_1,-\theta_2)  \\
    \nonumber =&~ \frac 12 \theta_1 (F_\lambda(\theta_1,\theta_2)+F_\lambda(\theta_1,-\theta_2))
    + \frac12 \theta_2 (F_\lambda(\theta_1,\theta_2)+F_\lambda(-\theta_1,\theta_2))\\
    \nonumber \le&~ \theta_1 F_\lambda(\theta_1,0) + \theta_2 F_\lambda(0,\theta_2) \\
    \nonumber =&~ \theta_1 \arctanh(\lambda\theta_1) + \theta_2 \arctanh(\lambda\theta_2) \\
    \nonumber \le&~ \lambda(\theta_1 \arctanh \theta_1 + \theta_2 \arctanh \theta_2) \\
    \nonumber =&~ \lambda (C_{\SKL}(\BSC_{\Delta_1}) + C_{\SKL}(\BSC_{\Delta_2})),
  \end{align}
  where the second step follows from Lemma~\ref{lem:boht-tech-r3}, and the fourth step follows convexity of $\arctanh$ in $[0,1]$.
  Combining~\eqref{eqn:skl-contraction-edge-bsc-r3-1}\eqref{eqn:skl-contraction-edge-bsc-r3-2} we finish the proof.
\end{proof}

\begin{lemma} \label{lem:boht-tech-r3}
For $\lambda,\theta_1,\theta_2 \in [0, 1]$, we have
\begin{align} \label{eqn:lem-boht-tech-r3}
\frac 12 \left(F_\lambda(\theta_1,\theta_2) + F_\lambda(\theta_1,-\theta_2)\right) \le F_\lambda(\theta_1,0).
\end{align}
\end{lemma}

\begin{proof}
We use the formula
\begin{align}
  \arctanh x + \arctanh y = \arctanh \frac{x+y}{1+x y}
\end{align}
to expand both sides of~\eqref{eqn:lem-boht-tech-r3}. LHS is
\begin{align} \label{eqn:lem-boht-tech-r3-lhs}
  & F_\lambda(\theta_1,\theta_2) + F_\lambda(\theta_1,-\theta_2) \\
  =&~ \arctanh\frac{\lambda(\theta_1 + \theta_2)}{1+\lambda \theta_1 \theta_2} + \arctanh\frac{\lambda(\theta_1 - \theta_2)}{1-\lambda \theta_1 \theta_2} \nonumber \\
  =&~ \arctanh \frac{2 \lambda \theta_1 (1-\lambda \theta_2^2)}{\lambda^2(\theta_1^2-\theta_2^2) + 1-\lambda^2 \theta_1^2 \theta_2^2}. \nonumber
  % \\
  % \le &~ \theta \arctanh \frac{2 \theta_1 (1-\theta \theta_2^2)}{\theta^2(\theta_1^2-\theta_2^2) + 1-\theta^2 \theta_1^2 \theta_2^2}. \nonumber
\end{align}
% The last step is because $\arctanh(c x) \le c \arctanh(x)$ for $x,c\ge 0$.
RHS is
\begin{align} \label{eqn:lem-boht-tech-r3-rhs}
  2 F_\lambda(\theta_1,0) = \arctanh \frac{2\lambda \theta_1}{1+\lambda^2\theta_1^2}.
\end{align}
By comparing \eqref{eqn:lem-boht-tech-r3-lhs}\eqref{eqn:lem-boht-tech-r3-rhs} and using monotonicity of $\arctanh$, it suffices to prove that
\begin{align}
  \frac{1-\lambda \theta_2^2}{\lambda^2(\theta_1^2-\theta_2^2) + 1-\lambda^2 \theta_1^2 \theta_2^2} \le \frac{1}{1+\lambda^2\theta_1^2}.
\end{align}
We have
\begin{align}
  (\lambda^2(\theta_1^2-\theta_2^2) + 1-\lambda^2 \theta_1^2 \theta_2^2) - (1-\lambda \theta_2^2)(1+\lambda^2\theta_1^2) = \lambda(1-\lambda)(1-\lambda \theta_1^2) \theta_2^2 \ge 0.
\end{align}
This finishes the proof.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:skl-contraction-edge-bsc-r4}} \label{sec:proof-recon-r34:boht-technical-r4}
In this section we prove Lemma~\ref{lem:skl-contraction-edge-bsc-r4}.
\begin{proof}[Proof of Lemma~\ref{lem:skl-contraction-edge-bsc-r4}]
  We expand LHS of~\eqref{eqn:skl-contraction-edge-bsc-r4} using BP recursion formula established in Section~\ref{sec:bp-recursion}.
  Let $\theta_i=1-2\Delta_i$ for $i\in [3]$. Then
  \begin{align} \label{eqn:skl-contraction-edge-bsc-r4-1}
    & C_{\SKL}((\BSC_{\Delta_1} \otimes \BSC_{\Delta_2}\otimes \BSC_{\Delta_3})\circ B) \\
    \nonumber =&~ \sum_{x_1=+,x_2,x_3\in \{\pm\}}
    \frac 14 \lambda (\theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_1 \theta_2 \theta_3 x_1 x_2 x_3) \\
    \nonumber & \cdot \arctanh\frac{\lambda(\theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_1 \theta_2 \theta_3 x_1 x_2 x_3)}{1 + \lambda (\theta_1 x_1 \theta_2 x_2 + \theta_2 x_3 \theta_3 x_3 + \theta_3 x_3 \theta_1 x_1)} \\
    \nonumber =&~ \frac \lambda 4(G_\lambda(\theta_1,\theta_2,\theta_3) + G_\lambda(\theta_1,-\theta_2,\theta_3) + G_\lambda(\theta_1,\theta_2,-\theta_3) + G_\lambda(\theta_1,-\theta_2,-\theta_3))
  \end{align}
  where
  \begin{align}
    G_\lambda(\theta_1,\theta_2,\theta_3)&:= (\theta_1+\theta_2+\theta_3+\theta_1\theta_2\theta_3) F_\lambda(\theta_1,\theta_2,\theta_3), \\
    F_\lambda(\theta_1,\theta_2,\theta_3)&:= \arctanh \frac{\lambda(\theta_1+\theta_2+\theta_3+\theta_1\theta_2\theta_3)}{1+\lambda(\theta_1\theta_2+\theta_2\theta_3+\theta_3\theta_1)}.
  \end{align}
  If Conjecture~\ref{conj:boht-r4} holds, then
  \begin{align} \label{eqn:skl-contraction-edge-bsc-r4-2}
    &~ \frac 14 (G_\lambda(\theta_1,\theta_2,\theta_3) + G_\lambda(\theta_1,-\theta_2,\theta_3) + G_\lambda(\theta_1,\theta_2,-\theta_3) + G_\lambda(\theta_1,-\theta_2,-\theta_3)) \\
    \nonumber \le&~ \lambda \sum_{i\in[3]} \theta_i \arctanh \theta_i \\
    \nonumber =&~ \lambda \sum_{i\in [3]} C_{\SKL}(\BSC_{\Delta_i}).
  \end{align}
  Combining~\eqref{eqn:skl-contraction-edge-bsc-r4-1}\eqref{eqn:skl-contraction-edge-bsc-r4-2} we finish the proof.
\end{proof}

% \yuzhou{discuss difficulty in proving Conjecture~\ref{conj:boht-r4}, maybe ???}

% Therefore, we need to prove that
% \begin{align}
% &~ \frac 14 (G_\lambda(\theta_1,\theta_2,\theta_3) + G_\lambda(\theta_1,-\theta_2,\theta_3) + G_\lambda(\theta_1,\theta_2,-\theta_3) + G_\lambda(\theta_1,-\theta_2,-\theta_3)) \\
% \le&~ \theta \sum_{i\in [3]} \theta_i \arctanh \theta_i. \nonumber
% \end{align}

% \begin{lemma} \label{lem:boht-tech-r4-1}
% For $\theta,\theta_1,\theta_2,\theta_3 \in [0, 1]$, we have
% \begin{align}
% \frac 14 (F_\lambda(\theta_1,\theta_2, \theta_3) + F_\lambda(\theta_1,-\theta_2,\theta_3)+F_\lambda(\theta_1,\theta_2,-\theta_3)+F_\lambda(\theta_1,-\theta_2,-\theta_3)) \le F_\lambda(\theta_1,0,0).
% \end{align}
% \end{lemma}
% \begin{proof}[Proof Sketch]
% Each side is sum of four $\arctanh$'s.
% Compare the polynomials.
% \yuzhou{write proof, some computer search}
% \end{proof}

% Using Lemma~\ref{lem:boht-tech-r4-1}, we have
% \begin{align*}
%   &~ \frac 14 ((\theta_1+\theta_2+\theta_3) F_\lambda(\theta_1,\theta_2,\theta_3)
%   + (\theta_1-\theta_2+\theta_3) F_\lambda(\theta_1,-\theta_2,\theta_3) \\
%   &+ (\theta_1+\theta_2-\theta_3) F_\lambda(\theta_1,\theta_2,-\theta_3)
%   + (\theta_1-\theta_2-\theta_3) F_\lambda(\theta_1,-\theta_2,-\theta_3))\\
%   &= \theta_1 (F_\lambda(\theta_1,\theta_2,\theta_3) +F_\lambda(\theta_1,-\theta_2,\theta_3)+F_\lambda(\theta_1,\theta_2,-\theta_3)+F_\lambda(\theta_1,-\theta_2,-\theta_3)) \\
%   &+ \theta_2 (F_\lambda(\theta_1,\theta_2,\theta_3) +F_\lambda(-\theta_1,\theta_2,-\theta_3)+F_\lambda(\theta_1,\theta_2,-\theta_3)+F_\lambda(-\theta_1,\theta_2,\theta_3))\\
%   &+ \theta_3 (F_\lambda(\theta_1,\theta_2,\theta_3) +F_\lambda(\theta_1,-\theta_2,\theta_3)+F_\lambda(-\theta_1,-\theta_2,\theta_3)+F_\lambda(-\theta_1,\theta_2,\theta_3))\\
%   &\le \theta_1 F_\lambda(\theta_1,0,0) + \theta_2 F_\lambda(0,\theta_2,0) + \theta_3 F_\lambda(0,0,\theta_3) \\
%   & = \sum_{i\in [3]} \theta_i \arctanh(\theta \theta_i).
% \end{align*}

% Therefore, it remains to prove that
% \begin{align}
%   & \theta_1 \theta_2 \theta_3 (F_\lambda(\theta_1,\theta_2,\theta_3)-F_\lambda(\theta_1,-\theta_2,\theta_3)-F_\lambda(\theta_1,\theta_2,-\theta_3)+F_\lambda(\theta_1,-\theta_2,-\theta_3)) \\
%   & \le \sum_{i\in [3]} \theta_i (\theta \arctanh \theta_i - \arctanh (\theta \theta_i)). \nonumber
% \end{align}

\section{Proofs in Section~\ref{sec:recon-large-deg}} \label{sec:proof-recon-large-deg}
\subsection{Proof of Prop.~\ref{prop:large-deg-step}} \label{sec:proof-recon-large-deg:large-deg-step}
In this section we prove Prop.~\ref{prop:large-deg-step}.
We first describe $\BP(P)$ in terms of the $\theta$-component.
Let $P$ be a BMS channel and $P_\theta$ be the $\theta$-component of $P$.
Let $t$ be the offspring ($t=d$ for regular hypertrees, $t\sim \Po(d)$ for Poisson hypertrees).
Let $(\theta_{ij})_{i\in [t], j\in [r-1]}$ generated $\simiid P_\theta$, where $\theta_{ij}$ is the $\theta$-component of the $j$-th vertex in the $i$-th downward hyperedge.
Let $\theta_i$ be the $\theta$-component of $i$-th hyperedge $P^{\otimes (r-1)}\circ B$.
As discussed in Section~\ref{sec:bp-recursion}, given $(\theta_{ij})_{j\in [r-1]}$, $\theta_i$ is equal to (the absolute value of)
\begin{align}
  \frac{\lambda \left( \prod_{j\in [r-1]} \left(1+\theta_{ij} x_{ij}\right) - \prod_{j\in [r-1]} \left(1-\theta_{ij} x_{ij}\right)\right)}{\lambda \left(\prod_{j\in [r-1]} \left(1+\theta_{ij} x_{ij}\right) +\prod_{j\in [r-1]} \left(1-\theta_{ij} x_{ij}\right)\right) + 2(1-\lambda)}.
\end{align}
with probability
\begin{align}
  \lambda \left(\prod_{j\in [r-1]} \left(\frac 12+\frac 12\theta_{ij} x_{ij}\right) +\prod_{j\in [r-1]} \left(\frac 12-\frac 12\theta_{ij} x_{ij}\right)\right) + 2^{2-r}(1-\lambda)
\end{align}
for $((x_{ij})_{j\in [r-1]}) \in \{\pm\}^{r-1}$, $x_{i1}=+$.

Let $\ov\theta$ be the $\theta$-component of the full channel $\BP(P)$.
Let $P_{\ov\theta}$ denote the distribution of $\ov\theta$.
Then given $(\theta_i)_{i\in [t]}$,
$\ov\theta$ is equal to (the absolute value of)
\begin{align}
  \frac{\prod_{i\in [t]}(1+\theta_i x_i) - \prod_{i\in [t]}(1-\theta_i x_i)}{\prod_{i\in [t]}(1+\theta_i x_i) + \prod_{i\in [t]}(1-\theta_i x_i)}
\end{align}
with probability
\begin{align}
  \prod_{i\in [t]}\left(\frac 12+\frac 12\theta_i x_i\right) + \prod_{i\in [t]}\left(\frac 12-\frac 12\theta_i x_i\right)
\end{align}
for $(x_1,\ldots,x_t)\in \{\pm\}^t$, $x_1=+$.
In other words,
\begin{align}
  &~P_{\ov\theta | \theta_1,\ldots,\theta_i} \\
  \nonumber =&~ \sum_{(x_1,\ldots,x_t)\in \{\pm\}^t} \left(\prod_{i\in [t]} \left(\frac 12 +\frac 12 \theta_i x_i\right)\right)
  \mathbbm{1}\left\{\left|\frac{\prod_{i\in [t]}(1+\theta_i x_i) - \prod_{i\in [t]}(1-\theta_i x_i)}{\prod_{i\in [t]}(1+\theta_i x_i) + \prod_{i\in [t]}(1-\theta_i x_i)}\right|\right\} \\
  \nonumber =&~ \sum_{(x_1,\ldots,x_t)\in \{\pm\}^t} \left(\prod_{i\in [t]} \left(\frac 12 +\frac 12 \theta_i x_i\right)\right)
  \mathbbm{1}\left\{\left| \tanh\left(\sum_{i\in [t]} \arctanh(\theta_i x_i) \right) \right|\right\}.
\end{align}
Write $\wt \theta_i=\theta_i x_i$. Then $\bP[\wt \theta_i = s \theta_i | \theta_i] = \frac 12 + \frac 12 \theta_i s$ for $s\in \{\pm\}$.
So $\wt \theta_i$ for $i\in [t]$ are iid generated from the same distribution.
Let us call this distribution $D$.
Then
\begin{align}
  P_{\ov\theta} = \bE_t \bE_{\wt\theta_1,\ldots,\wt\theta_t\iidsim D} \mathbbm{1}\left\{\left| \tanh\left(\sum_{i\in [t]} \arctanh \wt\theta_i \right) \right|\right\}
\end{align}
This allows us to use central limit theorems to control the behavior of $\sum_{i\in [t]} \arctanh\wt\theta_i$.
\begin{lemma} \label{lem:proof-large-deg-step:per-hyperedge}
  There exists a constant $d_0=d_0(r)>0$ such that for any $d>d_0$, $\lambda\in[0,1]$ with $(r-1)d\lambda^2\le 1$, and any BMS channel $P$, we have
  \begin{align}
    & \left| C_{\chi^2}(P^{\otimes (r-1)} \circ B) - s_{r,\lambda}(C_{\chi^2}(P)) \right| \le O_r(\lambda^3),\\
    & s_{r,\lambda} (x) := \lambda^2 \cdot \frac 12 \left((1+x)^{r-1}-(1-x)^{r-1}\right).
  \end{align}
  where $O_r$ hides a multiplicative factor depending only on $r$.
\end{lemma}
\begin{proof}
  We have
  \begin{align*}
    &~ C_{\chi^2}(P^{\otimes (r-1)} \circ B)\\
    =&~ \bE \theta_i^2\\
    =&~ \underset{\theta_{i1},\ldots,\theta_{i,r-1}\iidsim P_\theta}{\bE} \sum_{\substack{(x_{ij})_{j\in [r-1]}\in \{\pm\}^{r-1}\\ x_{i1}=+}} \\
    &~\qquad \left(2^{1-r} \cdot \frac{\lambda^2 \left( \prod_{j\in [r-1]} \left(1+\theta_{ij} x_{ij}\right) - \prod_{j\in [r-1]} \left(1-\theta_{ij} x_{ij}\right)\right)^2}{\lambda \left(\prod_{j\in [r-1]} \left(1+\theta_{ij} x_{ij}\right) +\prod_{j\in [r-1]} \left(1-\theta_{ij} x_{ij}\right)\right)+ 2(1-\lambda)}\right)\\
    =&~ \underset{\theta_{i1},\ldots,\theta_{i,r-1}\iidsim P_\theta}{\bE} \sum_{\substack{(x_{ij})_{j\in [r-1]}\in \{\pm\}^{r-1}\\ x_{i1}=+}} \\
    &~\qquad \left(2^{-r} \lambda^2 \left( \prod_{j\in [r-1]} \left(1+\theta_{ij} x_{ij}\right) - \prod_{j\in [r-1]} \left(1-\theta_{ij} x_{ij}\right)\right)^2\right) + O_r(\lambda^3).
  \end{align*}
  The inner summation satisfies
  \begin{align*}
    &~ \sum_{\substack{(x_{ij})_{j\in [r-1]}\in \{\pm\}^{r-1}\\ x_{i1}=+}}
    \left( \prod_{j\in [r-1]} \left(1+\theta_{ij} x_{ij}\right) - \prod_{j\in [r-1]} \left(1-\theta_{ij} x_{ij}\right)\right)^2 \\
    =&~ \frac 12 \sum_{\substack{(x_{ij})_{j\in [r-1]}\in \{\pm\}^{r-1}}}
    \left( \prod_{j\in [r-1]} \left(1+\theta_{ij} x_{ij}\right) - \prod_{j\in [r-1]} \left(1-\theta_{ij} x_{ij}\right)\right)^2 \\
    =&~ \frac 12 \sum_{\substack{(x_{ij})_{j\in [r-1]}\in \{\pm\}^{r-1}}}
    \left( \prod_{j\in [r-1]} \left(1+2 \theta_{ij} x_{ij}+\theta_{ij}^2\right)^2
    - 2 \prod_{j\in [r-1]} \left(1-\theta_{ij}^2\right) \right. \\
    & \qquad \left.+ \prod_{j\in [r-1]} \left(1-2 \theta_{ij} x_{ij}+\theta_{ij}^2\right)^2\right)\\
    =&~ 2^{r-1} \left(\prod_{j\in [r-1]} (1+\theta_{ij}^2)-\prod_{j\in [r-1]} (1-\theta_{ij}^2)\right).
  \end{align*}
  Therefore
  \begin{align*}
    &~ \underset{\theta_{i1},\ldots,\theta_{i,r-1}\iidsim P_\theta}{\bE} \sum_{\substack{(x_{ij})_{j\in [r-1]}\in \{\pm\}^{r-1}\\ x_{i1}=+}} \left( \prod_{j\in [r-1]} \left(1+\theta_{ij} x_{ij}\right) - \prod_{j\in [r-1]} \left(1-\theta_{ij} x_{ij}\right)\right)^2\\
    =&~ 2^{r-1} \underset{\theta_{i1},\ldots,\theta_{i,r-1}\iidsim P_\theta}{\bE} \left(\prod_{j\in [r-1]} (1+\theta_{ij}^2)-\prod_{j\in [r-1]} (1-\theta_{ij}^2)\right) \\
    =&~ 2^{r-1} s_{r,\lambda}(C_{\chi^2}(P)).
  \end{align*}
  Combining everything we finish the proof.
\end{proof}

\begin{lemma} \label{lem:proof-large-deg-step:exp-and-var}
  There exists a constant $d_0=d_0(r)>0$ such that for any $d>d_0$, $\lambda\in[0,1]$ with $(r-1)d\lambda^2\le 1$, and any BMS channel $P$, we have
  \begin{align}
    \left|\bE \arctanh \wt \theta_i - s_{r,\lambda}(C_{\chi^2}(P))\right| &= O_r(\lambda^3),\\
    \left|\Var(\arctanh \wt \theta_i) - s_{r,\lambda}(C_{\chi^2}(P))\right| &= O_r(\lambda^3).
  \end{align}
\end{lemma}

\begin{proof}
  Note that $\theta_i = O_r(\lambda)$ almost surely.
  When $d$ is large enough, $\lambda$ is small enough, and thus $\arctanh \theta_i = \theta_i + O_r(\lambda^3)$ almost surely by Taylor expansion. Then
  \begin{align}
    \bE \arctanh \wt \theta_i = \bE [\theta_i \arctanh \theta_i] = \bE \theta_i^2 + O_r(\lambda^4),\\
    \bE (\arctanh \wt \theta_i)^2 = \bE (\arctanh \theta_i)^2 = \bE \theta_i^2 + O_r(\lambda^4).
  \end{align}
  By Lemma~\ref{lem:proof-large-deg-step:per-hyperedge}, we have
  \begin{align}
    &\bE \theta_i^2 = s_{r,\lambda}(C_{\chi^2}(P)) + O_r(\lambda^3).
  \end{align}
  This already implies the statement on $\bE \arctanh \wt \theta_i$.
  For the statement on $\Var(\arctanh \wt \theta_i)$, we note that
  \begin{align}
    \bE \arctanh \wt \theta_i = s_{r,\lambda}(C_{\chi^2}(P)) + O_r(\lambda^3) = O_r(\lambda^2).
  \end{align}
  So
  \begin{align}
    \Var(\arctanh \wt \theta_i) &= \bE (\arctanh \wt \theta_i)^2 - \left(\bE \arctanh \wt \theta_i\right)^2\\
    \nonumber &= s_{r,\lambda}(C_{\chi^2}(P)) + O_r(\lambda^3).
  \end{align}
  This finishes the proof.
\end{proof}

Now we recall a normal approximation result from \cite[Prop.~5.3]{mossel2022exact}. We only need the scalar version of it.
\begin{lemma}[\cite{mossel2022exact}] \label{lem:normal-approx-lindeberg}
  Let $\phi: \bR \to \bR$ be a thrice differentiable and bounded function with bounded derivatives up to third order. Let $V_1,\ldots, V_t\in \bR$ be independent random real numbers.
  Suppose there exists deterministic numbers $\mu,\sigma\in \bR$ such that the following holds:
  for some constant $C>0$, almost surely
  \begin{align}
    &\max\left\{\left| \sum_{j\in [t]} \bE V_j -\mu \right|, \left|\sum_{j\in [t]} \Var(V_j) - \sigma^2\right| \right\}\le C t^{-1/2}, \\
    &\max\left\{ |\mu|, |\sigma^2| \right\} \le C, \qquad \max_{j\in [t]} |V_j| \le C t^{-1/2}.
  \end{align}
  Then for any $\epsilon>0$, there exists $t_0 = t_0(\epsilon,\phi,C)$ such that if $t>t_0$, then
  \begin{align}
    \left| \bE \phi \left( \sum_{j\in [t]} V_j \right) - \bE_{W\sim \cN(\mu,\sigma^2)} \phi(W) \right| \le \epsilon.
  \end{align}
\end{lemma}

We now have everything we need for the proof of Prop.~\ref{prop:large-deg-step}.
\begin{proof}[Proof of Prop.~\ref{prop:large-deg-step}]
  \textbf{Regular hypertree:}
  % We have
  % \begin{align}
  %   C_{\chi^2}(\BP(P)) = \bE \ov \theta^2 = \bE_{\wt\theta_1,\ldots,\wt\theta_t\iidsim D} \tanh^2\left(\sum_{i\in [t]} \arctanh \wt\theta_i \right) .
  % \end{align}
  Define $\wt \theta$ as $\bP[\wt \theta = s \theta | \theta] = \frac 12 + \theta s$ for $s\in \{\pm\}$. Then
  \begin{align}
    C_{\chi^2}(\BP(P)) = \bE \wt \theta = \bE_{\wt\theta_1,\ldots,\wt\theta_t\iidsim D} \tanh\left(\sum_{i\in [t]} \arctanh \wt\theta_i \right) .
  \end{align}
  In fact, the equality is true with $\tanh$ replaced by $\tanh^2$. We use the $\tanh$ form here because it is slightly simpler.

  Now we apply Lemma~\ref{lem:normal-approx-lindeberg} with
  \begin{align}
    \phi(x) = \tanh x,\quad V_i = \arctanh \wt \theta_i, \quad \mu = \sigma^2 = d s_{r,\lambda}(C_{\chi^2}(P)) = s_{r,d,\lambda}(C_{\chi^2}(P)).
  \end{align}
  The conditions in Lemma~\ref{lem:normal-approx-lindeberg} are satisfied by Lemma~\ref{lem:proof-large-deg-step:exp-and-var} and because $\lambda = O(d^{-1/2})$.
  This finishes the proof.

  \textbf{Poisson hypertree:}
  Fix $\epsilon>0$. Let $t\sim \Po(d)$.
  By Poisson tail bounds, we have $\bP[|t-d| > d^{0.6}] < \epsilon/3$ for large enough $d$ (depending only on $\epsilon$).
  We apply Lemma~\ref{lem:normal-approx-lindeberg} for every $t\in [d-d^{0.6},d+d^{0.6}]$, with $\mu = \sigma^2 = s_{r,t,\lambda} (C_{\chi^2}(P))$ and error tolerance $\epsilon/3$.
  Note that
  \begin{align}
    \left|s_{r,d,\lambda} (C_{\chi^2}(P)) - s_{r,t,\lambda} (C_{\chi^2}(P))\right| = O_r(d^{-0.4}).
  \end{align}
  So for $d$ large enough (depending only on $\epsilon,r$), we have
  \begin{align}
    \left|g_{r,d,\lambda} (C_{\chi^2}(P)) - g_{r,t,\lambda} (C_{\chi^2}(P))\right| \le \epsilon/3
  \end{align}
  by continuity of $g_r$ (Lemma~\ref{lem:large-deg-step-g-monotone}).

  Therefore we have
  \begin{align*}
    &~\left|C_{\chi^2}(\BP(P)) - g_{r,d,\lambda}(C_{\chi^2}(P))\right| \\
    =&~\left|\bE_{t\sim \Po(d)} C_{\chi^2}((P^{\otimes (r-1)}\circ B)^{\star t}) - g_{r,d,\lambda}(C_{\chi^2}(P))\right| \\
    \le&~ \bE_{t\sim \Po(d)} \mathbbm{1}\{|t-d|\le d^{0.6}\}\left|C_{\chi^2}((P^{\otimes (r-1)}\circ B)^{\star t}) - g_{r,t,\lambda}(C_{\chi^2}(P))\right|\\
    &~+\bE_{t\sim \Po(d)} \mathbbm{1}\{|t-d|\le d^{0.6}\} \left|g_{r,t,\lambda}(C_{\chi^2}(P))-g_{r,d,\lambda}(C_{\chi^2}(P))\right|\\
    &~+\bE_{t\sim \Po(d)} \mathbbm{1}\{|t-d|> d^{0.6}\} |C_{\chi^2}((P^{\otimes (r-1)}\circ B)^{\star t}) - g_{r,d,\lambda}(C_{\chi^2}(P))|\\
    \le&~ \epsilon/3 + \epsilon/3 + \epsilon/3 = \epsilon.
  \end{align*}
  Note that $C_{\chi^2}(P)\in [0,1]$ for any BMS channel $P$, and $g_{r,d,\lambda}(x)\in [0,1]$ for all $x\in [0,1]$.
\end{proof}
% \subsection{Proof of Prop.~\ref{prop:small-chi2-step}} \label{sec:proof-recon-large-deg:small-chi2-step}
% In this section we prove Prop.~\ref{prop:small-chi2-step}.
% For the start, we consider a BMS channel $P$ with small $\chi^2$-capacity, and compute $C_{\chi^2}(\BP(P))$.
% We use the same notations used in Section~\ref{sec:proof-recon-large-deg:large-deg-step}.
% We furthermore define the following variables.
% \begin{align}
%   Z_i^+ &:= \lambda \prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) + (1-\lambda),\\
%   Z_i^- &:= \lambda \prod_{j\in [r-1]} (1-\theta_{ij} x_{ij}) + (1-\lambda),\\
%   Z^+ &:= \prod_{i\in [t]}(1+\theta_i x_i), \\
%   Z^- &:= \prod_{i\in [t]}(1-\theta_i x_i). \\
% \end{align}
% Then we have
% \begin{align}
%   C_{\chi^2}(P^{\otimes (r-1)}\circ B) &= \bE \wt \theta_i, \\
%   C_{\chi^2}(\BP(P)) &= \bE \wt \theta,
% \end{align}
% where
% \begin{align}
%   &(\theta_{ij})_{i\in [t], j\in [r-1]} \iidsim P_\theta, \\
%   &\wt \theta_i = \frac{Z_i^+-Z_i^-}{Z_i^++Z_i^-}, \qquad \theta_i = |\wt \theta_i|, \\
%   &\wt \theta = \frac{Z^+-Z^-}{Z^++Z^-}, \qquad \theta = |\wt\theta|,\\
%   &\bP[(x_{ij})_{j\in [r-1]} =(s_{ij})_{j\in [r-1]} | (\theta_{ij})_{j\in [r-1]}] = \lambda \prod_{j\in [r-1]} \left(\frac 12+\frac 12\theta_{ij} s_{ij}\right) + (1-\lambda), \\
%   &\bP[(x_{j})_{j\in [t]} =(s_{j})_{j\in [t]} | (\theta_{j})_{j\in [t]}] = \prod_{j\in [t]} \left(\frac 12+\frac 12\theta_{j} s_{j}\right).
% \end{align}

% For simplicity of notation, we denote
% \begin{align}
%   \circnum1 := C_{\chi^2}(P),
%   \quad \circnum2 := C_{\chi^2}(P^{\otimes (r-1)} \circ B),
%   \quad \circnum3 := C_{\chi^2}(\BP(P)).
% \end{align}

% Let us expand $\frac{Z_i^+-Z_i^-}{Z_i^++Z_i^-}$ using formula
% \begin{align}
%   \frac{a}{s+r} = \frac{a(s-r)}{s^2} + \frac{a}{s+r} \cdot \frac{r^2}{s^2}
% \end{align}
% where $a = Z_i^+-Z_i^-$, $s = 2$, $r = Z_i^++Z_i^--2$. We have
% \begin{align}
%   \frac{Z_i^+-Z_i^-}{Z_i^++Z_i^-} = (Z_i^+-Z_i^-) - \frac 14 ((Z_i^+)^2 - (Z_i-)^2) +
%   \frac{Z_i^+-Z_i^-}{Z_i^++Z_i^-} \cdot \frac{(Z_i^++Z_i^--2)^2}{4}.
% \end{align}
% Let us compute the expectation of every term.
% \begin{align} \label{eqn:prop-proof-small-chi2-step-Zi1}
%   &~ \bE [Z_i^+ - Z_i^-] \\
%   \nonumber =&~ \bE_{(\theta_{ij})_{j\in [r-1]}} \sum_{(x_{ij})_{j\in [r-1]}} \left( \lambda\prod_{j\in [r-1]} \left(\frac 12+\frac 12\theta_{ij} x_{ij}\right) + (1-\lambda) \right)\\
%   \nonumber &~ \qquad \cdot \left( \lambda\prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) - \lambda\prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) \right) \\
%   \nonumber =&~\lambda^2 \left((1+\circnum1)^{r-1} - (1-\circnum1)^{r-1}\right).
% \end{align}
% Computation steps for the second step is similar to the one we did in Section~\ref{sec:proof-recon-large-deg:large-deg-step}, thus omitted.
% \begin{align} \label{eqn:prop-proof-small-chi2-step-Zi2}
%   &~ \bE [(Z_i^+)^2 - (Z_i^-)^2] \\
%   \nonumber =&~ \bE_{(\theta_{ij})_{j\in [r-1]}} \sum_{(x_{ij})_{j\in [r-1]}} \left( \lambda\prod_{j\in [r-1]} \left(\frac 12+\frac 12\theta_{ij} x_{ij}\right) + (1-\lambda) \right)\\
%   \nonumber &~ \qquad \cdot \left( \lambda\prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) - \lambda\prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) \right) \\
%   \nonumber &~ \qquad \cdot \left( \lambda\prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) + \lambda\prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) + 2(1-\lambda)\right) \\
%   \nonumber =&~2(1-\lambda)\lambda^2 \left((1+\circnum1)^{r-1} - (1-\circnum1)^{r-1}\right) \\
%   \nonumber &~ \qquad + \lambda^3 \left((1+3\circnum1)^{r-1} - (1-\circnum1)^{r-1}\right).
% \end{align}

% For the last term, let us compute the second factor.
% \begin{align} \label{eqn:prop-proof-small-chi2-step-Zi3}
%   &~ \bE [(Z_i^++Z_i^--2)^2] \\
%   \nonumber =&~ \bE_{(\theta_{ij})_{j\in [r-1]}} \sum_{(x_{ij})_{j\in [r-1]}} \left( \lambda\prod_{j\in [r-1]} \left(\frac 12+\frac 12\theta_{ij} x_{ij}\right) + (1-\lambda) \right)\\
%   \nonumber &~ \qquad \cdot \left( \lambda\prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) + \lambda\prod_{j\in [r-1]} (1+\theta_{ij} x_{ij}) -2\lambda\right)^2 \\
%   \nonumber =&~ ???
% \end{align}

% \begin{lemma} \label{lem:proof-small-chi2-step:Zi-conc}
%   For any $\epsilon>0$ and $\alpha>1$ there exists $C = C(r,\epsilon,\alpha)$ and $k_0 = k_0(r,\epsilon,\alpha)$, such that for $k\ge k_0$ and $(r-1)d\lambda^2\le 1$, and any $i\in [t]$, we have
%   \begin{align}
%     \bP\left(\theta_i > \epsilon\right) \le C \cdot \circnum1^\alpha
%   \end{align}
%   where $P=M_k$. (Note that $P$ is used to define both sides.)
% \end{lemma}

% ???

% Let us now compute $\frac{Z^+-Z^-}{Z^++Z^-}$.
% We use the same expansion
% \begin{align}
%   \frac{Z^+-Z^-}{Z^++Z^-} = (Z^+-Z^-) - \frac 14 ((Z^+)^2 - (Z-)^2) +
%   \frac{Z^+-Z^-}{Z^++Z^-} \cdot \frac{(Z^++Z^--2)^2}{4}.
% \end{align}

% \begin{lemma} \label{lem:proof-small-chi2-step:Z-conc}
%   For any $\epsilon>0$ and $\alpha>1$ there exists $C = C(r,\epsilon,\alpha)$ and $k_0 = k_0(r,\epsilon,\alpha)$, such that for $k\ge k_0$ and $(r-1)d\lambda^2\le 1$, we have
%   \begin{align}
%     \bP\left(\theta > \epsilon\right) \le C \cdot \circnum2^\alpha
%   \end{align}
%   where $P=M_k$. (Note that $P$ is used to define both sides.)
% \end{lemma}

% \begin{proof}[Proof of Prop.~\ref{prop:small-chi2-step}]

% \end{proof}

% \subsection{Concentration inequalities}
% In this section we prove Lemma~\ref{lem:proof-small-chi2-step:Zi-conc} and Lemma~\ref{lem:proof-small-chi2-step:Z-conc}.

% ???
% \begin{proof}[Proof of Lemma~\ref{lem:proof-small-chi2-step:Zi-conc}]

% \end{proof}

% \begin{proof}[Proof of Lemma~\ref{lem:proof-small-chi2-step:Z-conc}]

% \end{proof}


\subsection{Properties of functions} \label{sec:proof-recon-large-deg:func-property}
In this section we prove a few properties of the function $g_r$ defined in~\eqref{eqn:large-deg-step-g}.
\begin{proof}[Proof of Lemma~\ref{lem:large-deg-step-g-monotone}]
Note that $s_r(x)$ is continuous and increasing on $[0,1]$.
Therefore it suffices to prove that
\begin{align}
  g(s) := \bE_{Z\sim \cN(0,1)} \tanh \left(s + \sqrt s Z \right)
\end{align}
is continuous and increasing on $\bR_{\ge 0}$.
This statement is in fact equivalent to the $q=2$ case in \cite[Lemma 4.4]{sly2011reconstruction}, after a suitable change of variables.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:large-deg-step-g-r7}]
  We can numerically verify that $g_7(0.8) > 0.8$.
  Note that $s_r(0.8)$ is increasing for $r\ge 7$.
  Therefore for $r\ge 7$, we have $g_r(0.8) \ge g_7(0.8) > 0.8$.
\end{proof}

% \begin{proof}[Proof of Lemma~\ref{lem:large-deg-step-f}]
%   Take $\epsilon = \frac 14$, $d_0=4$.
%   If $(r-1)d\lambda^2 \le \frac 23$, then
%   \begin{align}
%     &~ f_{r,d,\lambda}(x) + \epsilon x^2 \\
%     \nonumber =&~ ((r-1) d\lambda^2 + \epsilon x) x - \left( (r-1)(r-2) d \lambda^3 + (r-1)^2 d(d-1) \lambda^4\right) x^2 \\
%     \nonumber \le&~ ((r-1) d\lambda^2 + \epsilon) x \\
%     \nonumber \le&~ \frac{11}{12} x.
%   \end{align}
%   If $\frac 23 \le (r-1)d\lambda^2 \le 1$, then
%   \begin{align}
%     &~(r-1)(r-2) d\lambda^3 + (r-1)^2 d(d-1)\lambda^4\\
%     \nonumber \ge&~ (r-1)^2 d^2 \lambda^4 \cdot (1-\frac 1d)\\
%     \nonumber \ge&~ \frac 34 ((r-1) d \lambda^2)^2 \\
%     \nonumber \ge&~ \frac 13
%   \end{align}
%   and
%   \begin{align}
%     &~ f_{r,d,\lambda}(x) + \epsilon x^2 \\
%     \nonumber =&~ (r-1) d\lambda^2  x - \left( (r-1)(r-2) d \lambda^3 + (r-1)^2 d(d-1) \lambda^4 - \epsilon \right) x^2 \\
%     \nonumber \le &~ (r-1) d\lambda^2  x - \frac 1{12} x^2 \\
%     \nonumber < &~  x.
%   \end{align}
%   This finishes the proof.
% \end{proof}

% \begin{lemma}
%   For $s\ge 0$,
%   \begin{align}
%     \bE_{Z\sim \cN(0,1)} \tanh(s+\sqrt s Z) = \bE_{Z\sim \cN(0,1)} \tanh^2(s+\sqrt s Z).
%   \end{align}
% \end{lemma}
% \begin{proof}
%   Define $p(x) = \frac 1{\sqrt{2\pi s}}\exp(-\frac{(x-s)^2}{2s})$.
%   Then the statement is equivalent to
%   \begin{align}
%     \int_{\bR} p(x) \tanh(x) dx  = \int_{\bR} p(x) \tanh^2(x) dx.
%   \end{align}
%   We have
%   \begin{align}
%     \tanh(x) = \frac{p(x)-p(-x)}{p(x)+p(-x)}.
%   \end{align}
%   So
%   \begin{align}
%     &~ \int_{\bR} p(x) \tanh^2(x) dx \\
%     =&~ \int_{\bR} (p(x)+p(-x)) \frac 12 \tanh^2(x) dx \\
%     =&~ \int_{\bR} (p(x)-p(-x)) \frac 12 \tanh(x) dx \\
%     =&~ \int_{\bR} p(x) \tanh(x) dx.
%   \end{align}
% \end{proof}

\end{document}
