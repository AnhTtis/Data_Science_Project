\documentclass{sigchi-ext}
% Please be sure that you have the dependencies (i.e., additional
% LaTeX packages) to compile this example.
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[scaled=.92]{helvet} % for proper fonts
\usepackage{graphicx} % for EPS use the graphics package instead
\usepackage{balance}  % for useful for balancing the last columns
\usepackage{booktabs} % for pretty table rules
\usepackage{ccicons}  % for Creative Commons citation icons
\usepackage{ragged2e} % for tighter hyphenation
\usepackage{dirtytalk}

% Some optional stuff you might like/need.
% \usepackage{marginnote} 
% \usepackage[shortlabels]{enumitem}
% \usepackage{paralist}
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP --
% \copyrightinfo{Permission to make digital or hard copies of all or
% part of this work for personal or classroom use is granted without
% fee provided that copies are not made or distributed for profit or
% commercial advantage and that copies bear this notice and the full
% citation on the first page. Copyrights for components of this work
% owned by others than ACM must be honored. Abstracting with credit is
% permitted. To copy otherwise, or republish, to post on servers or to
% redistribute to lists, requires prior specific permission and/or a
% fee. Request permissions from permissions@acm.org.\\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Context, Utility and Influence of an Explanation} \def\plainauthor{First Author, Second Author, Third Author,
  Fourth Author, Fifth Author, Sixth Author}
\def\emptyauthor{}
\def\plainkeywords{
  Explainable AI; Decision Theory; Contextual Utility Theory}
\def\plaingeneralterms{Documentation, Standardization}

\title{Context, Utility and Influence of an Explanation}

\numberofauthors{6}
% Notice how author names are alternately typesetted to appear ordered
% in 2-column format; i.e., the first 4 autors on the first column and
% the other 4 auhors on the second column. Actually, it's up to you to
% strictly adhere to this author notation.
\author{%
  \alignauthor{%
    \textbf{Minal Suresh Patil}\\
    \affaddr{Umeå universitet} \\
    %\affaddr{UNIVERSITETSTORGET 4, \\ 901 87 Umeå} \\
    \email{minalsp@cs.umu.se} }\alignauthor{%
    \textbf{Kary Främling}\\
    \affaddr{Umeå universitet}\\
    %\affaddr{UNIVERSITETSTORGET 4, \\ 901 87 Umeå}\\
    \email{kary.framling@cs.umu.se} } \vfil \alignauthor{%
    } }

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% \reversemarginpar%

\begin{document}

%% For the camera ready, use the commands provided by the ACM in the Permission Release Form.
\CopyrightYear{2020}
\setcopyright{rightsretained}
\conferenceinfo{CHI'20,}{April  25--30, 2020, Honolulu, HI, USA}
\isbn{978-1-4503-6819-3/20/04}
\doi{https://doi.org/10.1145/3334480.XXXXXXX}
%% Then override the default copyright message with the \acmcopyright command.
\copyrightinfo{\acmcopyright}


\maketitle

% Uncomment to disable hyphenation (not recommended)
% https://twitter.com/anjirokhan/status/546046683331973120
\RaggedRight{} 

% Do not change the page size or page settings.
\begin{abstract}
Contextual utility theory integrates context-sensitive factors into utility-based decision-making models. It stresses the importance of understanding individual decision-makers' preferences, values, and beliefs and the situational factors that affect them. Contextual utility theory benefits explainable AI. First, it can improve transparency and understanding of how AI systems affect decision-making. It can reveal AI model biases and limitations by considering personal preferences and context. Second, contextual utility theory can make AI systems more personalized and adaptable to users and stakeholders. AI systems can better meet user needs and values by incorporating demographic and cultural data. Finally, contextual utility theory promotes ethical AI development and social responsibility. AI developers can create ethical systems that benefit society by considering contextual factors like societal norms and values. This work, demonstrates how contextual utility theory can improve AI system transparency, personalization, and ethics, benefiting both users and developers.
\end{abstract}

\keywords{\plainkeywords}

% ACM Classfication

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10011748</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Human-centered computing~Empirical studies in HCI}

\printccsdesc


\section{Introduction}
Decision theory and utility theory are important concepts in the field of explainable artificial intelligence (XAI)\cite{framling2020decision,wang2019designing,framling2022contextual}. These theories provide a framework for making informed and rational decisions in uncertain environments.

Decision theory is a branch of mathematics that deals with the process of making decisions in situations where there is uncertainty about the outcomes of different choices\cite{fishburn1968utility, dyer1992multiple, wallenius2008multiple, von1975multi}. It provides a systematic approach for analyzing and choosing between different options based on the available information and the decision-maker's preferences. Decision theory considers factors such as risk, uncertainty, and the potential outcomes of each choice in order to determine the best course of action.

Utility theory is a sub field of decision theory that focuses on the quantification of preferences\cite{stigler1950development,figueira2005maut,dyer2016multiattribute,bell1986or}. It assumes that individuals have a preference ordering over different outcomes, and that these preferences can be represented as a numerical value, known as utility. The utility of an outcome is calculated based on the individual's preferences and the decision-maker's assessment of the likelihood of different outcomes. By combining decision theory and utility theory, organisations can make informed and rational decisions that reflect their goals and values\cite{haring1959utility,hagen1995risk,sebora1995expected}.

To understand the concept of utility, it is helpful to consider an example. Suppose you are trying to decide between two job offers, one that pays more but requires a longer commute and one that pays less but is closer to your home. The decision involves two attributes, salary and commute time, and you must decide which attribute is more important to you.
To evaluate the job offers, you must first assign utility scores to each attribute at each level. For example, you might assign a utility score of $0.8$ to a salary of $\$80,000$ and a utility score of $0.5$ to a salary of $\$60,000$. Similarly, you might assign a utility score of $0.5$ to a commute time of $30$ minutes and a utility score of $0.3$ to a commute time of $60$ minutes.

Once you have assigned utility scores to each attribute at each level, you can calculate the overall utility of each job offer. This involves weighting the utility scores for each attribute by their relative importance, which is typically expressed as a percentage. For example, you might assign a weight of $60\%$ to salary and a weight of $40\%$ to commute time, reflecting your priorities.

The overall utility of each job offer is then calculated as a weighted sum of the utility scores for each attribute, using the weights assigned to each attribute. For example, the overall utility of the first job offer might be calculated as follows:

\begin{equation}
    0.8 * 0.6 + 0.5 * 0.4 = 0.68
\end{equation}

This means that the first job offer has an overall utility of $0.68$, which reflects how well it satisfies your preferences and goals.

The process of assigning utility scores and weights to attributes is subjective, and different decision-makers may assign different scores and weights based on their preferences and goals. This subjectivity is one of the strengths of utility theory, as it allows decision-makers to express their priorities in a meaningful way.

In the context of XAI, decision theory and utility theory play an important role in ensuring that AI systems are transparent and explainable\cite{framling122021contextual}. By incorporating these theories into AI systems, organisations can ensure that decisions are made in a systematic and consistent manner, and that the reasoning behind decisions is easily understood. This helps to build trust in the AI system and ensures that decisions are aligned with the goals and values of the organisation.

%For example, in a financial decision-making system, decision theory and utility theory can be used to analyze and choose between different investment options based on factors such as risk, return, and the likelihood of different outcomes. By incorporating these theories into the AI system, the system can make informed and rational decisions that are transparent and easily understood by stakeholders.

%In conclusion, decision theory and utility theory are key components of explainable artificial intelligence. By incorporating these theories into AI systems, organizations can ensure that their decisions are transparent, explainable, and aligned with their goals and values.

\section{Importance and Influence}
In utility theory, \say{importance} and \say{influence} are both key concepts that are used to make decisions based on multiple criteria.

\textbf{Importance} refers to the degree to which each attribute is valued or weighted in the decision-making process. In other words, it represents the decision maker's preferences or priorities for each attribute. Importance is typically expressed as a numerical value or a weight, and it can be subjective or objective depending on the decision maker's goals and objectives.

For example, in choosing a new car, one person may consider fuel efficiency to be the most important attribute, and so they might assign a higher weight to it in the decision-making process. Another person may prioritize safety, and so they would assign a higher weight to the car's safety rating.

\textbf{Influence} refers to the degree to which each attribute affects the overall utility or value of the decision. In other words, it represents the objective or subjective impact of each attribute on the decision. Influence can be expressed as a number or a ranking, and it is usually determined based on empirical data, expert opinions, or the decision maker's past experience.

For example, in the car-buying scenario, fuel efficiency might have a greater influence on the overall value of the car than the safety rating or the price. This would mean that changes in the fuel efficiency would have a bigger impact on the decision than changes in the safety rating or price. 

In summary, importance and influence are both used in utility theory to help decision-makers make choices based on multiple criteria. Importance reflects the decision maker's priorities, while influence reflects the objective or subjective impact of each attribute on the decision.


\section{Contextual Utility Theory in XAI}
Contextual Utility Theory (CUT) is a framework that allows for the development of XAI models. CUT focuses on the context in which decisions are made, and how this context can be used to provide explanations for the output of an AI model. In this work, we will explore the principles of CUT, how it can be used to develop XAI models, and provide examples of its application in various domains.

\subsection{Principles of Contextual Utility Theory}
The fundamental principle of CUT is that the utility of a decision is not fixed, but rather a function of the context in which the decision is made. This means that the same decision can have different utilities in different contexts. For example, the decision to purchase a car may have a different utility for a single person living in the city compared to a family living in a rural area. This is because the context in which the decision is being made affects the value that the decision provides. By integrating user-specific information, such as demographic or cultural factors, AI systems can tailor their outputs to better align with the needs and values of their users. Finally, contextual utility theory can help promote ethical decision-making and social responsibility in AI development. By accounting for contextual factors such as societal norms or values, AI developers can design systems that align with broader ethical principles and promote positive social outcomes and this has been further elaborated in this position paper from the previous year's edition \cite{singh2022grounding} which emphasis that explainability should be grounded in social contexts.

CUT provides a way to account for context in decision-making by using contextual variables. Contextual variables are variables that describe the context in which a decision is made. For example, in the loan approval example, contextual variables may include the credit score of the applicant, the applicant's employment status, and the purpose of the loan. These variables are used to calculate the utility of a decision, which is then used to determine the best course of action.
The CUT framework also incorporates the principle of explainability. Explainability refers to the ability of an AI model to provide a clear and understandable explanation for its decisions. By using contextual variables to calculate utility, CUT allows for the creation of models that can provide explanations that are tied to the specific context in which the decision is being made. This makes it easier for users to understand the decision-making process of the model and to trust its output.  
CUT can be mathematically represented using a utility function that takes into account the individual's preferences and the context in which the decision is being made. The utility function:
\begin{equation}
    U(x,c) = u(x) + v(c)
\end{equation}

where $U$ is the overall utility, x is the decision variable, $c$ is the contextual variable, $u(x)$ is the individual's preference function for the decision variable, and $v(c)$ is the contextual value function.

For example, consider a restaurant recommendation AI system that takes into account the user's preferences and the contextual factors such as the time of day and the user's location. The utility function for this AI system could be represented as:

\begin{equation}
    U(x,c) = u(x) + v(c) = \alpha1(r) + \alpha2(p) + \beta1(t) + \beta2(d)
\end{equation}

where $x$ is the recommendation (i.e., a particular restaurant), $c$ is the context, $r$ is the restaurant's rating, $p$ is the price, $t$ is the time of day, and $d$ is the distance to the restaurant. The parameters $\alpha$ and $\beta$ represent the weights assigned to each factor based on the user's preferences.

The user's preference function $u(x)$ can be represented as:

\begin{equation}
    u(x) = \alpha1(r) + \alpha2(p)
\end{equation}

where $\alpha1$ and $\alpha2$ are the weights assigned to the restaurant's rating and price, respectively. The contextual value function $v(c)$ can be represented as:

\begin{equation}
    v(c) = \beta1(t) + \beta2(d)
\end{equation}

where $\beta1$ and $\beta2$ are the weights assigned to the time of day and distance to the restaurant, respectively.

Using this utility function, the AI system can recommend restaurants that have the highest utility value based on the user's individual preferences and the specific context in which the decision is being made. By providing an interpretable and transparent decision-making process, this approach can help users understand why certain recommendations are being made and build trust in the AI system.

\subsection{Using Contextual Utility Theory in Explainable AI}
CUT can be used to develop XAI models in a variety of domains, including finance, healthcare, and autonomous vehicles. In each domain, contextual variables can be used to calculate the utility of a decision and provide explanations for the model's output.


\begin{itemize}
    \item \textbf{Loan Approval} - One example of the application of CUT in finance is the loan approval process. In the loan approval process, a lender must decide whether to approve or reject a loan application. To make this decision, the lender considers various factors such as the applicant's credit score, employment status, and the purpose of the loan. To develop an XAI model using CUT, the lender could use these factors as contextual variables. The model could then use these variables to calculate the utility of approving or rejecting the loan. The output of the model could then be explained based on the context of the application. For example, the model may explain that a loan was rejected because the applicant's credit score was too low or because the purpose of the loan did not align with the lender's policies.
    \item \textbf{Medical Diagnosis} - Another example of the application of CUT is in the field of healthcare. In healthcare, a physician must make a diagnosis based on the patient's symptoms and medical history. The physician may use a variety of diagnostic tests to help with this process.
To develop an XAI model using CUT, the physician could use the patient's medical history, age, and other relevant factors as contextual variables. The model could then use these variables to calculate the utility of different diagnoses. The output of the model could then be explained based on the context of the patient. For example, the model may explain that a particular diagnosis was made because the patient had a history of similar symptoms or because the patient was of a certain age.
    
\end{itemize}

\section{Conclusion}
In conclusion, contextual utility theory offers a valuable framework for developing explainable AI systems. By taking into account the various contextual factors that influence decision-making, such as user preferences and the nature of the task at hand, this theory provides a more nuanced approach to modeling and interpreting AI outputs.

One of the key benefits of contextual utility theory is that it enables AI systems to generate more personalized recommendations and decisions. By incorporating individual preferences and situational factors, these systems can better adapt to the needs of different users and deliver more tailored outputs. Moreover, contextual utility theory can help to identify potential biases and limitations in AI systems, and enable developers to mitigate these issues in order to improve overall performance and reliability.

Overall, contextual utility theory represents a promising direction for the development of explainable AI systems. As the field continues to evolve, it will be important to explore further how this theory can be applied in practice and refined to improve the accuracy, transparency, and ethical implications of AI decision-making. By embracing these principles, we can work towards creating more trustworthy and effective AI systems that benefit society as a whole.

%\begin{figure}
%  \includegraphics[width=0.9\columnwidth]{figures/sigchi-logo}
%  \caption{Insert a caption below each figure.}~\label{fig:sample}
%\end{figure}

\section*{Acknowledgements}
The author thanks Timotheus Kampik for guidance and valuable insights in this work. This work was partially funded by the Knut and Alice Wallenberg Foundation.
\balance{} 

%\bibliographystyle{SIGCHI-Reference-Format}
%\bibliography{sample}

%%% -*-BibTeX-*-
%%% Do NOT edit. File created by BibTeX with style
%%% ACM-Reference-Format-Journals [18-Jan-2012].

\begin{thebibliography}{00}

%%% ====================================================================
%%% NOTE TO THE USER: you can override these defaults by providing
%%% customized versions of any of these macros before the \bibliography
%%% command.  Each of them MUST provide its own final punctuation,
%%% except for \shownote{}, \showDOI{}, and \showURL{}.  The latter two
%%% do not use final punctuation, in order to avoid confusing it with
%%% the Web address.
%%%
%%% To suppress output of a particular field, define its macro to expand
%%% to an empty string, or better, \unskip, like this:
%%%
%%% \newcommand{\showDOI}[1]{\unskip}   % LaTeX syntax
%%%
%%% \def \showDOI #1{\unskip}           % plain TeX syntax
%%%
%%% ====================================================================

\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{{\tt DOI:}\penalty0{#1}\ }
  \fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       #1{#1}          \fi

\bibitem{bell1986or}
{David~E Bell} {and} {Peter~H Farquhar}. 1986.
\newblock \showarticletitle{OR forum—perspectives on utility theory}.
\newblock {\em Operations Research\/} {34}, 1 (1986), 179--183.
\newblock


\bibitem{dyer2016multiattribute}
{James~S Dyer}. 2016.
\newblock \showarticletitle{Multiattribute utility theory (MAUT)}.
\newblock {\em Multiple criteria decision analysis: State of the art surveys\/}
  (2016), 285--314.
\newblock


\bibitem{dyer1992multiple}
{James~S Dyer}, {Peter~C Fishburn}, {Ralph~E Steuer}, {Jyrki Wallenius}, {and}
  {Stanley Zionts}. 1992.
\newblock \showarticletitle{Multiple criteria decision making, multiattribute
  utility theory: the next ten years}.
\newblock {\em Management science\/} {38}, 5 (1992), 645--654.
\newblock


\bibitem{figueira2005maut}
{Jos{\'e} Figueira}, {Salvatore Greco}, {Matthias Ehrogott}, {and} {James~S
  Dyer}. 2005.
\newblock \showarticletitle{MAUT—multiattribute utility theory}.
\newblock {\em Multiple criteria decision analysis: state of the art surveys\/}
  (2005), 265--292.
\newblock


\bibitem{fishburn1968utility}
{Peter~C Fishburn}. 1968.
\newblock \showarticletitle{Utility theory}.
\newblock {\em Management science\/} {14}, 5 (1968), 335--378.
\newblock


\bibitem{framling2020decision}
{Kary Fr{\"a}mling}. 2020.
\newblock \showarticletitle{Decision theory meets explainable AI}. In {\em
  Explainable, Transparent Autonomous Agents and Multi-Agent Systems: Second
  International Workshop, EXTRAAMAS 2020, Auckland, New Zealand, May 9--13,
  2020, Revised Selected Papers 2}. Springer, 57--74.
\newblock


\bibitem{framling2022contextual}
{Kary Fr{\"a}mling}. 2022.
\newblock \showarticletitle{Contextual importance and utility: a theoretical
  foundation}. In {\em AI 2021: Advances in Artificial Intelligence: 34th
  Australasian Joint Conference, AI 2021, Sydney, NSW, Australia, February
  2--4, 2022, Proceedings}. Springer, 117--128.
\newblock


\bibitem{framling122021contextual}
{Kary Fr{\"a}mling12}. 2021.
\newblock \showarticletitle{Contextual importance and utility in R: the
  ‘ciu’package}.
\newblock  (2021).
\newblock


\bibitem{hagen1995risk}
{Ole Hagen}. 1995.
\newblock \showarticletitle{Risk in Utility Theory, in Business and in the
  World of Fear and Hope}.
\newblock {\em Revolutionary Changes in Understanding Man and Society: Scopes
  and Limits\/} (1995), 191--210.
\newblock


\bibitem{haring1959utility}
{Joseph~E Haring} {and} {Gorman~C Smith}. 1959.
\newblock \showarticletitle{Utility theory, decision theory, and profit
  maximization}.
\newblock {\em The American Economic Review\/} {49}, 4 (1959), 566--583.
\newblock


\bibitem{sebora1995expected}
{Terrence~C Sebora} {and} {Jeffrey~R Cornwall}. 1995.
\newblock \showarticletitle{Expected utility theory vs. prospect theory:
  Implications for strategic decision makers}.
\newblock {\em Journal of Managerial Issues\/} (1995), 41--61.
\newblock


\bibitem{singh2022grounding}
{Deepa Singh}, {Michal Slupczynski}, {Ajit~G Pillai}, {and} {Vinoth
  Pandian~Sermuga Pandian}. 2022.
\newblock \showarticletitle{Grounding Explainability Within the Context of
  Global South in XAI}.
\newblock {\em arXiv preprint arXiv:2205.06919\/} (2022).
\newblock


\bibitem{stigler1950development}
{George~J Stigler}. 1950.
\newblock \showarticletitle{The development of utility theory. I}.
\newblock {\em Journal of political economy\/} {58}, 4 (1950), 307--327.
\newblock


\bibitem{von1975multi}
{Detlof Von~Winterfeldt} {and} {Gregory~W Fischer}. 1975.
\newblock \showarticletitle{Multi-attribute utility theory: models and
  assessment procedures}. In {\em Utility, Probability, and Human Decision
  Making: Selected Proceedings of an Interdisciplinary Research Conference,
  Rome, 3--6 September, 1973}. Springer, 47--85.
\newblock


\bibitem{wallenius2008multiple}
{Jyrki Wallenius}, {James~S Dyer}, {Peter~C Fishburn}, {Ralph~E Steuer},
  {Stanley Zionts}, {and} {Kalyanmoy Deb}. 2008.
\newblock \showarticletitle{Multiple criteria decision making, multiattribute
  utility theory: Recent accomplishments and what lies ahead}.
\newblock {\em Management science\/} {54}, 7 (2008), 1336--1349.
\newblock


\bibitem{wang2019designing}
{Danding Wang}, {Qian Yang}, {Ashraf Abdul}, {and} {Brian~Y Lim}. 2019.
\newblock \showarticletitle{Designing theory-driven user-centric explainable
  AI}. In {\em Proceedings of the 2019 CHI conference on human factors in
  computing systems}. 1--15.
\newblock


\end{thebibliography}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
