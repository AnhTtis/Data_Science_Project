% \vspace{0.5em}\noindent\textbf{Sub-MDP and End-Components.} For a given MDP $\Mdp= (S, s_0,\actions, T)$, a {\it sub-MDP} of $\Mdp$ is an MDP $\Mdp' = (S', s_0',\actions', T')$, where $S' \subset
% S$, $\actions' \subseteq \actions$ is such that $\actions'(s) \subseteq \actions{}(s)$,
% % \track{{\sf Act} in prelims instead of $A$} for every $s \in S'$,  
% and $T'$ is analogous to $T$ when restricted to $S'$ and
% $A'$. Moreover $\Mdp'$ is closed under probabilistic transitions.
% An {\it end-component}~\cite{alma991027942769706011} of an MDP $\Mdp$ is a sub-MDP $\Mdp'$ such that for every pair of states $s, s' \in S'$, there is a 
% schedule that can reach $s'$ from $s$ with positive probability. 
% A maximal end-component is an end-component that is maximal under set-inclusion.
% An end-component is \emph{winning} if it contains an accepting state.
% Every state $s$ of an MDP $\Mdp$ belongs to at most one maximal end-component.
% We can define sub-CTMDP, and end components in a CTMDP, as in the case of MDPs.
% % Further, we can see that Theorem~\ref{thm:ec} 
% Following is a property of end-components in an MDP that also holds for CTMDPs. 

%  \begin{theorem}[End-Component Properties \cite{alma991027942769706011}]\label{thm:ec}
% Once an end-component $C$ of an MDP is entered, there is a schedule that visits every state-action pair in $C$ with probability 1 and stays in $C$ forever. Moreover, for every schedule the probability that a run ends up in an end-component is 1.  
% \end{theorem}

% \vspace{0.5em}\noindent\textbf{Augmented Product CTMDP.} 
We reduce the problem of satisfaction semantics of an $\omega$-regular objective in a CTMDP to an expected average reward objective.
Using Blackwell optimality result stated in Theorem~\ref{corollary:1}, we further reduce this to an expected discounted reward objective which allows us to use off-the-shelf RL for CTMDP for learning schedules for $\omega$-regular objectives.

To find a schedule satisfying an $\omega$-regular objective in a CTMDP, we need to identify the accepting end-components where an accepting end-component~\cite{alma991027942769706011} is a sub-MDP that is closed under probabilistic transitions and contains an accepting state.
It is well known~\cite{alma991027942769706011} that as an end-component $C$ of an MDP is entered, there is a schedule that visits every state-action pair in $C$ with probability 1 and stays in $C$ forever.
Hence, a schedule that maximizes the probability of satisfaction of a given $\omega$-regular objective maximizes the probability of reaching the accepting end-components.
In Appendix~\ref{app:EC}, we show an example of a CTMDP and its end-components.
Also the MDP in the top part of Figure~\ref{fig:p3}, is itself an accepting end-component since the state $q_0$ is accepting.

% To tackle this problem, 
We further reduce the problem to an average reward problem as described below and then specify a reward function such that the schedule maximising the expected average reward maximizes the probability of satisfying the objective.
% Note that a reachability objective does not require identifying the end-components.
\paragraph{Reduction to Average Reward.}
Before describing our RL algorithm for unknown CTMDP, we first describe the reduction when an input CTMDP is fully known to explain the intuition behind our algorithm.
Consider a CTMDP $\Mdp$, a \textbf{GFM} $\oautomata$, and let $\pmdp$ denote the product CTMDP.
For our reduction, we define a constant $\zeta \in (0,1)$ and an augmented product CTMDP, denoted by $\augmdp$. 
The CTMDP $\augmdp$ is constructed from $\pmdp$ by adding a new sink state $t$ with a self loop labelled by an action $a'$ and with rate $\lambda(t,a') > 0$, and making it the only accepting state in $\augmdp$. 
Further, in $\augmdp$, the rates of each outgoing transition from an accepting state in $\pmdp$ is multiplied by $\zeta$. 
Also, for each action $a$ from an accepting state $s$ in $\pmdp$, in $\augmdp$ we add a new transition to the sink state $t$ with rate $\lambda(s,a) \cdot (1-\zeta)$ where $\lambda(s,a)$ is the exit rate of the state-action pair $(s,a)$ in $\pmdp$. 
% We also add an action $a'$ from $t$ with a single transition to itself.
% The rate of this action can be any positive constant $\lambda(t,a')$.
% as there are no other transition from $t$ and if the system reaches $t$ it will stay there forever.    
Figure~\ref{fig:p3} shows an example of this construction.
% \track{WHAT IS $\lambda(t,a')$? NEEDS TO BE ADDED.}
Note that in the figure, $q_0$ is the only accepting state in the product CTMDP.
There are two outgoing transitions from $q_0$ on action $a$ to $q_1$ and $q_2$ with rates $r_1$ and $r_2$ respectively, and hence $\lambda(q_0, a) = r_1 + r_2$.
% We add a sink $t$ which is the only accepting state in $\Mdp^\zeta$.
We then add a transition from $q_0$ to $t$ with rate $(r_1+r_2) \cdot (1-\zeta)$.

\begin{figure}[t]
 \centering
  \begin{minipage}{0.3\textwidth}
     \begin{tikzpicture}[shorten >=1pt, node distance=2.3 cm, on grid, auto,thick,initial text=]
\begin{scope}[every node/.style={scale=1}]
\node (l0) [state,accepting, fill = safecellcolor] {$q_0$};
\node (l1) [state,right = of l0, fill = safecellcolor]   {$q_1$};
\node (l2) [state,left = of l0,fill = safecellcolor]   {$q_2$};
\end{scope}
 \begin{scope}
\path [-stealth, thick]
    (l0) edge [bend left]  node [above] {$a,r_1$}   (l1)
    (l0) edge [bend right]   node [above] {$a,r_2$}   (l2)
    (l1) edge [bend left]  node [below] {$b,r_3$}   (l0)
    (l2) edge  [loop above] node [above] {$c,r_4$}   ()
    (l2) edge [bend right] node [below] {$d, r_4$} (l0)
    ;
\end{scope}
\end{tikzpicture}
  \end{minipage}
  \begin{minipage}{0.3\textwidth}
    \centering
     \begin{tikzpicture}[shorten >=1pt, node distance=2.3 cm, on grid, auto,thick,initial text=,]
\begin{scope}[every node/.style={scale=1}]
\node (l0) [state,fill = safecellcolor] {$q_0$};
\node (l1) [state,right = of l0, fill = safecellcolor]   {$q_1$};
\node (l2) [state,left = of l0,fill = safecellcolor]   {$q_2$};
\node (l3) [state, above = of l0,accepting, fill = goodcellcolor] {$t$};
\end{scope}
 \begin{scope}
\path [->]
    (l0) edge [bend left]  node [above] {$a,r_1  \cdot \zeta$}   (l1)
    (l0) edge  [bend right] node [above] {$a,r_2 \cdot \zeta$}   (l2)
    (l1) edge [bend left]  node [below] {$b,r_3$}   (l0)
    (l2) edge  [loop above] node [above] {$c,r_4$}   ()
    (l0) edge node [right] {$a,r_1+r_2 \cdot(1-\zeta)$}(l3)
    (l3) edge [loop left] node [left] {$a', \lambda(t,a')$} () 
    (l2) edge [bend right] node [below] {$d, r_4$} (l0)
    ;
\end{scope}
\end{tikzpicture}
  \end{minipage}
  \caption{A product CTMDP $(\Mdp \times \oautomata)$ (top) and its corresponding augmented product CTMDP $\Mdp^{\zeta}$ (bottom).}
  \label{fig:p3}
\end{figure}

% As we can see, the set of states in $\pmdp$ and $\augmdp$ differ only by $t$ and any run that reaches $t$ will stay there. 
% Therefore, given a schedule $\sigma$ in $\augmdp$, the corresponding schedule in $\pmdp$ is defined in a way where the action chosen from any state in $\pmdp$ will be the same as that of $\sigma$.
With a slight abuse of notation, if $\sigma$ is a schedule in the augmented CTMDP $\Mdp^\zeta$, then we also denote by $\sigma$ a schedule in $\Mdp \times \oautomata$ obtained by removing $t$ from the domain of $\sigma$.
Thus fix a schedule $\sigma$ in both $\Mdp^\zeta$ and in $\Mdp \times \oautomata$.
Note that for every state in an accepting end-component, the probability of reaching the sink $t$ in $\Mdp^\zeta$ is $1$.
Similarly, for every state in a rejecting end-component, the probability of reaching $t$ in $\Mdp^\zeta$ is $0$.
The probability of reaching $t$ in $\Mdp^\zeta$ under $\sigma$ overapproximates the probability of reaching the accepting end-components in $\Mdp \times \oautomata$ under $\sigma$.
The difference in the two probabilities occurs since in $\Mdp^\zeta$, from the transient accepting states, with probability $1-\zeta$, one can reach the sink $t$.
This approximation error tends to $0$ as $\zeta$ tends to $1$.
We define a reward function in $\augmdp$ such that a schedule maximising the expected average reward in $\augmdp$ maximizes the probability of satisfying the $\omega$ regular objective in $\pmdp$. 


% For a given CTMDP $\Mdp$, an embedded MDP $\embeddedMdp$ of $\mathcal{M}$ is a discrete-time MDP of the form $\embeddedMdp = (\states, \initstate, \actions{}, \transP_{\Mdp})$, that is, the transition function of $\embeddedMdp$ is derived from the probability matrix of $\Mdp$.
% As the set of states and enabled actions from each state are the same in $\Mdp$ and $\emdp$, the set of pure schedules in them are also same. 

% Time does not play a role in the definition of pure schedules. Therefore, the probability of reaching a state $s$ from the initial state in a CTMDP $\Mdp$ under a pure schedule $\sigma$ is dependant only on the transition function of the embedded MDP $\emdp$. 
% Now recall that for $\omega$-regular objectives, given a B\"{u}chi GFM, the states of the accepting condition of the product CTMDP need to be visited infinitely often.
% As there exist optimal pure schedules for reaching such states, we get the following lemma for an $\omega$-regular objective $\phi$.

% \begin{lemma} \label{lem:embedded}
% There exists a pure schedule that maximises the probability of satisfying $\phi$ in $\mathcal{M} \times \mathcal{A}$ which also maximises the probability of satisfying $\phi$ in the embedded product DTMDP $\mathcal{M}_\mathcal{E} \times \mathcal{A}$.
% \end{lemma}
% \begin{comment}
% Theorem 3 from \cite{HahnPSSTW19} gives us the existence of a threshold $\zeta' \in (0,1)$ and that for any $\zeta > \zeta'$, an optimal schedule maximising the probability of reaching the sink state $t$ from a state $s$ in the DTMDP $\augmdp_{\mathcal{E}}$ (denoted by $p_s(\zeta)$) also maximises the probability of satisfying $\phi$ in $\Mdp_{\mathcal{E}} \times \oautomata$.
% \end{comment}
% From Theorem~3 in \cite{HahnPSSTW19}, we obtain the following result about DTMDP: 
% % \begin{lemma}[Theorem3, \cite{HahnPSSTW19}]\label{lem:hahn}
% There exists a threshold $\zeta' \in (0,1)$ such that for all $\zeta > \zeta'$, and for every state $s$, a schedule maximising the probability $p_s(\zeta)$ of reaching the sink in $\mathcal{M}_{\mathcal{E}}^{\zeta}$ is
% \begin{inparaenum}[(1)]
% \item an optimal schedule in $\mathcal{M}_{\mathcal{E}} \times \mathcal{A}$ from $s$ for satisfying the $\omega$-regular objective $\phi$, and
% \item induces an optimal schedule for the MDP $\mathcal{M}_{\mathcal{E}}$ from $s$ with objective $\phi$.
% \end{inparaenum}
% % \end{lemma}

\begin{comment}
We have shown in Lemma \ref{lem:embedded} that there exists an optimal pure schedule maximising $\phi$ in both $\pmdp$ and $\Mdp_{\mathcal{E}} \times \oautomata$. 
With a similar argument it also follows that an optimal pure schedule maximising the probability to reach the sink $t$ in the CTMDP $\augmdp$ also maximises the probability to reach $t$ in the DTMDP $\augmdp_{\mathcal{E}}$.
% Now we show that there also exists an optimal pure  optimal reachability strategies in the CTMDP $\augmdp$ and the DTMDP $\augmdp_{\mathcal{E}}$.
\begin{lemma}\label{lem:augmented}
There exists a threshold $\zeta' \in (0,1)$ such that for all $\zeta > \zeta'$, and for every state $s$, there exists a pure schedule maximising the probability to reach the sink in the augmented CTMDP $\mathcal{M}^{\zeta}$ which also maximises the probability to reach the sink in the augmented DTMDP $\mathcal{M}_\mathcal{\mathcal{E}}^{\zeta}$.
\end{lemma}
Using Lemma \ref{lem:embedded} and Lemma \ref{lem:augmented}, we get Lemma~\ref{lem:final}.
\begin{lemma}\label{lem:final}
There exists a threshold $\zeta' \in (0,1)$ such that for all $\zeta > \zeta'$, and for every state $s$, a schedule maximising the probability $p_s(\zeta)$ of reaching the sink in $\mathcal{M}_{\mathcal{E}}^{\zeta}$ is
\begin{inparaenum}[(1)]
\item an optimal schedule in the product CTMDP $\mathcal{M} \times \mathcal{A}$ from $s$ for satisfying the $\omega$-regular objective $\phi$, and
\item induces an optimal schedule for the CTMDP $\mathcal{M}$ from $s$ with objective $\phi$.
\end{inparaenum}
\end{lemma}
\end{comment}


\paragraph{Reward Function.} 
The reward function 
% is such that every time the sink state is reached, we 
provides a reward of $1$ per time unit for staying in the accepting sink $t$, while the reward is $0$ otherwise, i.e.
% \[
% rew(s) = [s=t]
% \]
\[
rew(s) = \begin{cases}
1 & \text{if $s=t$}  \\
0 & \text{otherwise}
\end{cases}
\]
As there is only a single action $a'$ from state $t$ in $\augmdp$ which is a self loop, we can conclude that any schedule that maximizes the probability of reaching $t$ also maximizes the expected average reward in $\mathcal{M}^{\zeta}$.
Further, following the discussion above, for high values of $\zeta$, the schedule also maximizes the probability of satisfying the $\omega$-regular objective in $\Mdp \times \oautomata$.
We thus have the following.

\begin{theorem}\label{theorem:4.1}
There exists a threshold $\zeta' \in (0,1)$ such that for all $\zeta > \zeta'$, and for every state $s$, a schedule maximising the expected average reward in $t$ in $\augmdp$ is 
\begin{inparaenum}[(1)]
\item an optimal schedule in the product CTMDP $\mathcal{M} \times \mathcal{A}$ from $s$ for satisfying the $\omega$-regular objective $\phi$.
Further, since $\oautomata$ is a GFM, we have that \item $\sigma$ induces an optimal schedule for the CTMDP $\mathcal{M}$ from $s$ with objective $\phi$.
\end{inparaenum}
\end{theorem}
Detailed proof of this theorem is provided in Appendix~\ref{app:sat}.
From the above theorem, we have that for a large $\zeta$ value, a schedule maximising the expected average reward in $\augmdp$ also maximizes the probability of satisfying the $\omega$-regular property in $\pmdp$.
Therefore, when the CTMDP is known, the problem of satisfaction semantics of an $\omega$-regular property is reduced to an expected average reward objective.
% \begin{proof}[Proof sketch]
% % \track{@Shibashis: Have made some changes here}
% We have shown in Lemma \ref{lem:embedded} that there exists an optimal pure schedule maximising $\phi$ in both $\pmdp$ and $\Mdp_{\mathcal{E}} \times \oautomata$. With a similar argument it also follows that an optimal pure schedule maximising the probability to reach the sink $t$ in the CTMDP $\augmdp$ also maximises the probability to reach $t$ in the DTMDP $\augmdp_{\mathcal{E}}$.

% Theorem 3 from \cite{HahnPSSTW19} gives us the existence of a threshold $\zeta' \in (0,1)$ and that for any $\zeta > \zeta'$, an optimal schedule maximising the probability of reaching the sink state $t$ from a state $s$ in the DTMDP $\augmdp_{\mathcal{E}}$ also maximises the probability of satisfying $\phi$ in $\Mdp_{\mathcal{E}} \times \oautomata$.
% \input{table}
% % Hence we have that there exists a threshold $\zeta' \in (0,1)$ such that for all $\zeta > \zeta'$, and for every state $s$, there exists a pure schedule maximising the probability to reach the sink in the augmented CTMDP $\mathcal{M}^{\zeta}$ which also maximises the probability to reach the sink in the augmented DTMDP $\mathcal{M}_\mathcal{\mathcal{E}}^{\zeta}$.

% This leads to the following statement.
% There exists a threshold $\zeta' \in (0,1)$ such that for all $\zeta > \zeta'$, and for every state $s$, a schedule $\sigma$ maximising the probability $p_s(\zeta)$ of reaching the sink in $\mathcal{M}_{\mathcal{E}}^{\zeta}$ is
% \begin{inparaenum}[(1)]
% \item an optimal schedule in the product CTMDP $\mathcal{M} \times \mathcal{A}$ from $s$ for satisfying the $\omega$-regular objective $\phi$, and
% \item induces an optimal schedule for the CTMDP $\mathcal{M}$ from $s$ with objective $\phi$.
% \end{inparaenum}

% The reward machine defined gives a positive reward for each time unit spent in $t$. Therefore, we can conclude that any schedule that maximises the probability of reaching $t$ also maximises the expected average reward.
% \end{proof}
\paragraph{The case of unknown CTMDP.} 
% Now we look at the case when the CTMDP is unknown, 
Recall that we consider a CTMDP model with unknown rate and transition structure.
For such unknown CTMDP models, 
% it may not be possible to identify the winning end-components a priori.
% Thus for RL, we 
an RL algorithm cannot construct the product $\Mdp \times \oautomata$ explicitly.
 From Theorem \ref{theorem:4.1}, we can conclude that any schedule maximising the expected average reward that is accrued by visiting the sink state $t$ in $\augmdp$ where $\zeta>\zeta'$ for some $\zeta' \in (0,1)$ also maximizes the probability of satisfying the $\omega$-regular objective $\phi$ in $\pmdp$.
% \todo{We need to rewrite this para.} 
% But, as we do not know the transition structure of the CTMDP beforehand, we cannot construct an augmented product CTMDP for finding an optimal schedule. Instead, we define a reward function $rew$ where 
This leads to a very simple model-free RL algorithm which does not require the augmented product CTMDP $\Mdp^\zeta$ to be constructed explicitly.
% since the transition structure of the CTMDP is not known beforehand. 
% without constructing the augmented product CTMDP, 
We define the following reward function $rew'$ to be used by the RL algorithm: 
\[
rew'((s,q),a) = \begin{cases}
1 \text{\quad with probability $1-\zeta$ if $(s,q)$ is} \\
\text{\quad \quad accepting} \\
0  \text{\quad otherwise}
\end{cases}
\]
% which gives a positive reward with probability $1 - \zeta$ on each action from an accepting state in $\pmdp$ and $0$ reward for any action from other states. 
% where $C$ is a large positive constant. 
Recall that in the augmented product $\Mdp^\zeta$, for each action from an accepting state, we add a transition to sink state $t$ with probability $1-\zeta$, and give a reward of $1$ for staying in $t$ per unit time.
The RL algorithm simulates this in the following way: When a transition from an accepting state is visited, the learning agent tosses a biased coin and obtains a reward of $1$ with probability $1-\zeta$.
% for each action taken from the accepting state.
% As we cannot construct the augmented product CTMDP $\augmdp$, and thus cannot give positive rewards for each time unit spent in the sink, we provide a positive reward of $1$ with probability $1-\zeta$ for each action taken from an accepting state.
% This simulates that when we visit an accepting state, with probability $1-\zeta$ we reach the sink state $t$ in the augmented product CTMDP $\augmdp$.
% This reward function gives a large positive reward from each action from an accepting state with probability $1-\zeta$ which is similar to what is done by the reward machine .
Therefore, any schedule maximising the expected average reward w.r.t. $rew'$ also maximizes the probability of satisfying the objective.
% Thus, a schedule maximising the expected average reward also maximises the probability of satisfying the objective.
As Theorem~\ref{corollary:1} shows the existence of Blackwell optimal schedules in CTMDPs, we can conclude that for a high enough discount factor, any off-the-shelf model-free RL algorithm for CTMDP maximising the expected discounted reward gives an optimal schedule maximising the satisfaction of $\phi$. 
% We provide an algorithm based on this reward function in Appendix~\ref{algo}.
A pseudocode of our algorithm is given in Appendix~\ref{algo}.

% \paragraph{Algorithm for Satisfaction Semantics.} The algorithm is similar to that in Algorithm $\ref{algo:expt}$, but here the reward function and the terminating condition would be different.
% The major differences with Algorithm~\ref{algo:expt} are in the definition of the reward function $rew$
% % \track{USE $rew$?} 
% and how we terminate an episode. 
% The reward function $rew$ is defined as shown above and an episode ends when a positive reward is obtained.
% A detailed algorithm is provided in Appendix~\ref{algo: sat}
% With this reward function, any off the shelf model-free RL algorithm for CTMDP maximising the discounted payoff gives the optimal schedule for reaching $t$ and hence in turn gives the optimal schedule maximising the satisfaction of $\phi$.