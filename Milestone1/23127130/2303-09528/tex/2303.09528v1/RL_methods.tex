% Popular model-free reinforcement learning algorithms like TD($\lambda$) and Q-learning for CTMDPs were defined by Bradtke and Duff in \cite{BD94}. We give a small summary of these methods for the reader.
% \subsection{Value functions for states}
% An optimal policy for a discounted objective is obtained by defining expected discounted reward for each state. These are called `value' of a state. The value of a state $s$ in CTMDP $\Mdp$ under a pure strategy $\sigma$ is denoted by $\valuef$. The expected reward from a transition from state $s$ to $s'$ on action $a$ is defined as \begin{equation*}
% \begin{split}
%   \erew(s,s',a) & = rew(s,a) +\\
%   & \int_{0}^{\infty} \int_{0}^{t} e^{\lambda(s,a)x}rew(s)dx dF(t|s,a)  
% \end{split}
%  \end{equation*}
% The expected discount factor on value of $s'$ in the transition from $s$ to $s'$ on action $a$ is defined as \\
% $\gamma = \int_{0}^{\infty} e^{\lambda(s,a)t}dF(t|s,a)$\\
% Now we can define $\valuef$ as 
% \begin{equation}
%     \begin{split}
%         \valuef & = \sum_{s'\in \states} \Delta(s,\sigma(s),s').\erew(s,s',\sigma(s)) + \\
%         & \gamma \sum_{s' \in \states} \Delta(s,\sigma(s),s') v_{\sigma}(s')
%     \end{split}
% \end{equation}
% \subsection{Temporal difference learning}
% In MDPs, TD(0) algorithm stores value for each state of the MDP. When a reward $r(s,s',a)$ is observed from a transition from $s$ to $s'$ on an action $a$, the value function is updated. If current value of $s$ is denoted by $\kvalue$, then the new value is given by \\
% $$\newvalue = \kvalue + \alpha_{k}[r(s,s',a) + \gamma v^{(k)}(s') - \kvalue]$$\\
% where $\alpha_{k}$ is the learning rate. It is proven that for a suitable $\alpha$, the TD(0) algorithm converges to the real value of the state with probability 1. \\ 
% The algorithm is similar for CTMDPs where the only difference is in the computation of the value function. The value function is intended to solve equation (1). 
% \begin{equation}
% \begin{split}
% \newvalue = \kvalue + \alpha_{k}[r(s,a) + \frac{1 - e^{-\lambda(s,a)\tau}}{\lambda(s,a)}r(s) + \\ e^{\lambda(s,a)\tau}v^{(k)}(s') - v^{(k)}(s)]    
% \end{split}
% \end{equation}
% where the sampled transition time from $s$ to $s'$ is $\tau$, $r(s,a) + \frac{1 - e^{-\lambda(s,a)\tau}}{\lambda(s,a)}r(s)$ is the sample reward from the transition and $e^{\lambda(s,a)\tau}$ is the sample discount on the value of the next state $s'$. TD($\lambda$) learning rule can be defined from this equation.

% \subsection{Q-learning}
% Q-learning is a model-free reinforcement learning algorithm which is similar to TD(0). For an MDP $\Mdp$ and a strategy $\sigma$ we define a function $\valueq(s,a)$ for each state $s$ and action $a$. The Q-function must satisfy the following equation 
% \begin{equation}
%     \begin{split}
%         \valueq(s,a) = rew(s,s',a) + \gamma \sum_{s' \in \states} P(s,s',a)\valueq(s',\sigma(s'))
%     \end{split}
% \end{equation}
% Here, the main difference is that $a$ can be any action and not necessarily the action chosen by $\sigma$. Therefore, $\valueq(s,a)$ gives the expected discounted reward from choosing an action $a$ in state $s$ under a policy $\sigma$. The optimal Q-function is denoted by $\optvalueq$. The optimal Q-function $\optvalueq$ satisfies the Bellman-style optimality equation 
% \begin{equation}
%     \begin{split}
%         \optvalueq(s,a) = r(s,s',a) + \gamma \sum_{s' \in \states} p(s,s',a) \max_{a' \in \actions} \optvalueq(s',a')
%     \end{split}
% \end{equation}
% Q-learning uses stochastic approximation to iteratively refine the $\optvalueq$ function. When a transition from state $s$ to $s'$ on action $a$ is observed, the $\optvalueq$ function is updated according to the following learning rule
% \begin{equation}
% \begin{split}
%     \newvaluef(s,a) &= \kvaluef(s,a) + [r(s,s',a) +\\& \gamma \cdot \max_{a'} \kvaluef(s',a') - \kvalue]
% \end{split}    
% \end{equation}
% We can define a similar learning rule for CTMDPs. The optimal Q-function for a CTMDP satisfies the following equation 
% \begin{equation}
%     \begin{split}
%         \optvalueq(s,a) = \sum_{s' \in \states} \Delta(s,a,s') \erew(s,s',a) + \\
%         \gamma \sum_{s'\in \states} \Delta(s,a,s') \max_{a' \in \actions} \valueq(s',a')
%     \end{split}
% \end{equation}
% This leads to the following Q-learning rule for CTMDPs 
% \begin{equation}
%     \begin{split}
%         \newvaluef(s,a) = \kvaluef(s,a) + \alpha_{k}[rew(s,a)d + \frac{1-e^{-\lambda(s,a)\tau}}{\lambda(s,a)}r(s) + \\
%         e^{-\lambda(s,a)\tau} \max_{a'} \kvaluef(s',a') - \kvaluef(s,a)]
%     \end{split}
% \end{equation}
% % From the above equations it is clear that Q-learning and TD(0) works in a similar fashion. \*       
        
