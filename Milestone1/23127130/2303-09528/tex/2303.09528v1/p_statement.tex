\paragraph{Omega-regular Objectives.}
An $\omega$-regular objective is defined by a nondeterministic B\"uchi automaton $\oautomata = (\alphabets, \ostate, \oinitstate, \otrans, F)$ where $\alphabets$ is a finite \emph{alphabet}, $\ostate$ is a finite set of \emph{states}, $\oinitstate \in \ostate$ is an \emph{initial state}, $\otrans : \ostate \times \alphabets \rightarrow 2^\ostate$ is a \emph{transition function} and $F \subseteq \ostate$ is the set of \emph{accepting states}.
A B\"uchi automaton is deterministic, if $\delta(q, a)$ is singleton for all $(q, a) \in \ostate{\times}\alphabets$.
We define the extended transition function $\hat{\otrans}: \ostate \times \alphabets^* \rightarrow 2^\ostate$, derived from $\otrans$, as 
% \[
% \hat{\otrans}(q, w) = \begin{cases}
% \set{q} & \text{ if $ w = \varepsilon$}\\
% \bigcup\limits_{q' \in \otrans(q, a)} \hat{\otrans}(q', x) & \text{ if } w = ax \text{ for } a {\in} \Sigma, x {\in} \Sigma^*.
% \end{cases}
% \]
 $\hat{\otrans}(q, \varepsilon) = \set{q}$ and $\hat{\otrans}(q, ax) = \cup_{q' \in \otrans(q, a)} \hat{\otrans}(q', x)$, for $q \in \ostate$ and $ax \in \Sigma\Sigma^*$.

% B{\"u}chi automaton by \textbf{DBW}.

A \emph{run} $r$ of $\oautomata$ is an infinite sequence $(r_0, w_0, r_1, w_1, \ldots)$ where $r_0 = q_0$, $r_i \in \ostate$, $w_i \in \alphabets$ and $r_{i+1} \in \otrans(r_i,w_i)$ for all $i \in \mathbb{N}$. The word of a run $r = (r_0, w_0, r_1, w_1, \ldots)$ is ${\sf L}(r)=(w_0 w_1 \cdots)$ . Let the set of runs of $\oautomata$ be $\oruns$.
We say that a run $r \in \oruns$ is accepting if there exists a $q_f \in F$ such that $q_f$ occurs infinitely often in $r$. An $\omega$-word $w = (w_0 w_1 \cdots)$ is accepted by $\oautomata$ if there exists an accepting run $r_w = (r_0, w_0, r_1, w_1, \ldots)$ of $\oautomata$. 
The language of the automaton $\oautomata$, denoted $\Ll(\oautomata)$ is the set of all words that is accepted by the automaton. 

\paragraph{CTMDPs and Omega-regular Objectives.}
In order to express the properties of a CTMDP $\Mdp$ using a B\"uchi automaton, we introduce the notion of a labelled CTMDP.
A \emph{labelled} CTMDP is a triple $(\Mdp, \atomicprop, \labelling)$ where $\Mdp$ is a CTMDP, $\atomicprop$ is a set of atomic propositions, and $\labelling: \states \rightarrow 2^\atomicprop$ is a labelling function. 
Let $\oautomata = (2^{\atomicprop}, \ostate, \oinitstate, \otrans, \acceptingc)$ be a B\"uchi automaton expressing the learning objectives of $\Mdp$.

Recall that for a CTMDP $\Mdp$ under a schedule $\sigma$ we write $X_n$, $Y_n$, $D_n$, and $T_n$ for the random variables corresponding to the $n$-th state, action, time-delay at the $n$-th state, and time-stamp (total time spent up to the $n$-th state).
We introduce the random variable $F_n$ to indicate if the sequence of observations of the CTMDP leads to an accepting state on $\oautomata$ in $n$-steps, i.e., 
$
F_n = [\hat{\delta}(L(X_0)\cdot L(X_1) \cdots L(X_n)) \cap F].
$

For a CTMDP $(\Mdp, \atomicprop, \labelling)$ and  automaton $\oautomata = (2^{\atomicprop}, \ostate, \oinitstate, \otrans, \acceptingc)$,
we study the following problems:
\begin{enumerate}
    \item {\bf Satisfaction Semantics.} 
    Compute a schedule of $\Mdp$ that maximizes the probability of visiting accepting states $F$ of $\oautomata$ infinitely often.
    We define the satisfaction probability of a schedule $\sigma$ from starting state $s$ as: 
\begin{equation*}
\PSemSat^{\Mdp}_{\oautomata}(s, \sigma) 
{=}   {\Pr}^{\Mdp}_{\sigma}(s) \set{\forall_i  \exists_{j{\geq} i} F_j }.
\end{equation*}
% \begin{equation*}
% \PSemSat^{\Mdp}_{\oautomata}(s, \sigma) 
% {=}   \Pr{}^{\Mdp}_{\sigma}(s) \set{ r \in \Runs_{\sigma}^{\Mdp}(s) \colon
%   L(r) \in \Ll(\oautomata) }.
% \end{equation*}
% i.e, the probability of runs from $s$ under $\sigma$ for which its corresponding run in $\oautomata$ visits an accepting state infinitely often.
Intuitively, it describes the probability of runs from state $s$ under $\sigma$ in the CTMDP such that the corresponding run in $\oautomata$ visits the accepting states infinitely often.
The optimal satisfaction probability
$\PSemSat^{\Mdp}_{\oautomata}(s)$ for  $\oautomata$ 
is defined as $\sup_{\sigma \in \Sigma_{\Mdp}} \PSemSat^{\Mdp}_{\oautomata}(s, \sigma)$, and we say
that a schedule $\sigma \in \Sigma_\Mdp$ is an optimal schedule for $\oautomata$ if
$\PSemSat^{\Mdp}_{\oautomata}(s, \sigma)  = \PSemSat^{\Mdp}_{\oautomata}(s)$ for all $s \in \states$.

\item {\bf Expectation Semantics.} Compute a schedule of $\Mdp$ that maximizes the long-run expected average time spent in the accepting states of $\oautomata$.
We define the expected satisfaction time of a schedule $\sigma$ from starting state $s$ as: 
\begin{equation*}
\ESemSat{}^{\Mdp}_{\oautomata}(s, \sigma) 
=   \mathbb{E}^{\Mdp}_{\sigma}(s) \set{ \liminf_{n \rightarrow \infty} \frac{\sum_{i=1}^{n} F_i \cdot D_i}{T_n}}.
\end{equation*}
The optimal expected satisfaction time
$\ESemSat^{\Mdp}_{\oautomata}(s)$ for specification $\oautomata$ is defined as $\sup_{\sigma \in \Sigma_{\Mdp}} \ESemSat^{\Mdp}_{\oautomata}(s, \sigma)$, and we say that $\sigma \in \Sigma_\Mdp$ is an optimal expectation maximisation schedule for $\oautomata$ if $\ESemSat^{\Mdp}_{\oautomata}(s, \sigma) = \ESemSat^{\Mdp}_{\oautomata} (s)$.
\end{enumerate}


\paragraph{Product Construction.} 
Given a \emph{labelled} CTMDP $(\Mdp, \atomicprop, \labelling)$ where $\atomicprop$ is a set of atomic propositions, and $\labelling: \states \rightarrow 2^\atomicprop$ is a labelling function, and a   B{\"u}chi automaton $\oautomata = (2^{\atomicprop}, \ostate, \oinitstate, \otrans, \acceptingc)$, the product CTMDP is defined as $\Mdp \times \oautomata = ((\states \times \ostate),(\initstate,\oinitstate),\actions,\transR^{\times},\acceptingc^{\times})$ where the rates are $\transR^{\times} : (\states \times \ostate) \times \actions \times (\states \times \ostate) \rightarrow \Reals_{\geq 0}$ such that $\transR^{\times} ((s,q),a,(s',q')) = \transR(s,a,s')$ if $\transR(s,a,s')>0$ and $\otrans (q,\labelling(s)) = \{q'\}$.
If $F$ is the set of accepting 
% transitions 
states
in $\oautomata$, then the accepting condition is a set 
% transitions $F^\times$ where $((s,q),a,(s',q')) \in F^\times$ iff $(q,\labelling(s),q') \in F$ 
$F^\times$ of states where $(s,q) \in F^\times$ iff $q \in F$. 
 An example of a product CTMDP is given in Appendix~\ref{sat vs expt}.  



\paragraph{Good-for-CTMDP Automata.} 
From the definition of both the semantics, it is clear that the optimal schedule requires some memory to monitor the run in the B\"uchi automaton (see Example~\ref{ex:mem} in Appendix~\ref{exmp:memory}).
% It is easy to see that for both optimization problems, the optimal schedules require memory.
For the right kind of B\"uchi automata~\cite{HPSS20}, the amount of memory required can be equal to the size of the automata. 
A key construction to compute these schedules is the product construction, where the CTMDP and the automaton are combined together as a CTMDP with accepting states governed by the accepting states of the B\"uchi automata.
On the other hand, not every B\"uchi automaton can be used for this construction.
The class of B\"uchi automata where the semantic value of satisfaction of the property on the MDP equals to the corresponding problems on the product structure, are called good-for-MDP (GFM) automata~\cite{HPSS20}.

We introduce the notion of good-for-CTMDP automata in Appendix~\ref{app:gfm}.
% We also argue that every $\omega$-regular property can be expressed as a good-for-CTMDP automaton\todo{add a reference}. 
If a B\"uchi automaton is GFM, then one can show via uniformization that it is also good-for-CTMDPs.
There exist several syntactic characterizations of good-for-MDP automata including suitable limit-deterministic B\"uchi automata (SLDBA)~\cite{sickert2016limit} and slim automata~\cite{HPSS20}.
Moreover, every LTL specification can be effectively converted into a GFM B\"uchi automata.
Moreover, there exist tools (OWL and Spot) to convert LTL objectives to good-for-CTMDP automaton.
Hence, in this paper, w.l.o.g., we assume that $\omega$-regular objectives are given as good-for-CTMDP automata.


% Let for a run $r$, we denote by $\inf(r)$ the set of states visited infinitely often in $r$.
% A run $r$ of $\Mdp \times \mathcal{A}$ is accepting if
% $\inf(r) \cap F^\times \neq \emptyset$.
% We define the
% \emph{syntactic satisfaction}
% probabilities $\PSat^\Mdp_{\oautomata}((s,q), \sigma^\times)$ as the probability of
% accepting runs, i.e.
% \[
% \Pr{}^{\Mdp\times\oautomata}_{\sigma^\times}(s,q) \Set{ r \in
% \Runs_{\sigma^\times}^{\Mdp\times\oautomata}(s,q) : \inf(r) \cap F^\times
% \neq \emptyset }
% \]
% Similarly, we define $\PSat^\Mdp_{\oautomata}(s)$ as the optimal probability over the
% product, i.e.  $\sup_{\sigma^\times}\big(\PSat^\Mdp_{\oautomata}((s,q_0),
% \sigma^\times)\big)$.
% For a deterministic $\oautomata$ the equality $\PSat^\Mdp_{\oautomata}(s) = \PSemSat^\Mdp_{\oautomata}(s)$ holds; however it is not guaranteed for nondeterministic
% B\"uchi automata as the optimal resolution of nondeterministic
% choices may require access to future events.
% However, while \textbf{DBW}s cannot capture all of $\omega$-regular properties, it has been shown in \cite{HahnPSSTW19} that specifying an $\omega$-regular objective with a deterministic Rabin automaton is not always suitable for defining a reward mechanism necessary for reinforcement learning.
% This motivates for the definition of a good-for-MDP nondeterminisitc B\"uchi automata.
% A B\"uchi automaton $\oautomata$ is \emph{good for MDPs} (GFM),
% if $\PSat^\Mdp_{\oautomata}(s_0) = \PSemSat^\Mdp_{\oautomata}(s_0)$ holds for all
% MDPs $\Mdp$ and starting states $s_0$ \cite{Hahn20}.
% Note that every $\omega$-regular objective can be expressed as a GFM automaton \cite{Hahn20}.

% \track{CHECK IF ACCEPTING END-COMPONENT DEFINED LATER.}
\begin{comment}
For a product CTMDP, we define run and end-component in the same way as that of a CTMDP. An accepting run is a run which visits an accepting 
% transition 
state
infinitely often. An end-component is accepting if it has an accepting state.
% transition. 
    
{\color{red} After introducing nondeterminsitic B\"uchi, a quick paragraph about why nondeterminism is required, why we need to have LDBWs? Just make an assumption that all Buchi considered are LDBWs. We can do so without definition, and move the definition to the appendix, if allowed.}
\paragraph{Limit Deterministic B{\"u}chi Automaton.}
A nondeterministic B{\"u}chi automaton accepting $\omega$-words (\textbf{NBW}) can represent all $\omega$-regular objectives, while a \textbf{DBW} cannot. We define another B{\"u}chi automaton which adds some non-determinism to \textbf{DBW}s and have the same expressiveness as \textbf{NBW}. This automata is called a \emph{limit deterministic} B{\"u}chi automata (\textbf{LDBW}). Intuitively, an \textbf{LDBW} consists of a non-deterministic component without accepting states, and a deterministic component with only accepting states. The automaton can only accept by moving from the non-deterministic component to the deterministic component, and once it reaches the deterministic component, it stays there forever. 
Formally, an \textbf{LDBW} is an \textbf{NBW} $\oautomata = (\alphabets,\ostate_{N} \cup \ostate_{D},\oinitstate,\otrans,F)$ where 
\begin{inparaenum}[(1)]
    \item $\ostate_{N} \cap \ostate_{D} = \emptyset$, $F \subseteq \ostate_{D}$;
    \item $|\otrans(q,\sigma)| \leq 1$ for all $q \in \ostate_{D}$ and $\sigma \in \alphabets$;
    \item $\otrans(q,\sigma) \subseteq \ostate_{D}$ for all $q \in \ostate_{D}$ and $\sigma \in \alphabets$.
\end{inparaenum}
% \track{Definition of suitable LDBW?}\\
A suitable limit deterministic B{\"u}chi automata (\textbf{GFM}) $A$ for a property $\phi$ is an \textbf{LDBW} that recognises $\phi$ and such that, for any finite MDP $\Mdp$, there exists a pure schedule $\sigma$ such that the probability of satisfying the B{\"u}chi condition in $(\Mdp \times A)_{\sigma}$ is the same as the probability of satisfying $\phi$ in $\Mdp$ by an optimal schedule in $\Mdp$.
We can similarly define \textbf{GFM} for a CTMDP $\Mdp$.

Not every \textbf{LDBW} is suitable. Consider the  pair of automata shown in Figure~ \ref{fig:P2} for $(\always a)\lor(\always b)$. The automaton in right is suitable as the initial state $q_0$ can delay the transition to state $q_1$ or $q_2$ by staying in $q_0$, and move only when the end-component of the CTMDP is reached. The automaton in the left is not suitable as we have to pick the transition to state $q_1$ or $q_2$ immediately and cannot wait until an end-component is reached. But as we can see, both the automata accept the same language. 
\begin{theorem}[\cite{courcoubetis1995complexity},\cite{hahn2013lazy},\cite{sickert2016limit},\cite{vardi1985automatic}]\label{GFM}
Suitable limit deterministic B{\"u}chi automata exist for all $\omega$-regular languages. 
\end{theorem}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
    \begin{tikzpicture}[shorten >=1pt, node distance=3 cm, on grid, auto,thick,initial text=,]
\begin{scope}[every node/.style={scale=.8}]
\node (l0) [state,initial below, fill = safecellcolor] {$q_0$};
\node (l1) [state,right = of l0,xshift = -2.3cm,accepting, fill = safecellcolor]   {$q_1$};
\node (l2) [state,left = of l0,xshift = 2.3cm,accepting,fill = safecellcolor]   {$q_2$};
\end{scope}
 \begin{scope}[every node/.style={scale=.7}]
\path [->]
    (l0) edge  node [above] {$b$}   (l1)
    (l0) edge  node [above] {$a$}   (l2)
    (l1) edge [loop above] node [above] {$b$}   ()
    (l2) edge [loop above] node [above] {$a$}   ()
    ;
\end{scope}
\end{tikzpicture}
    \label{fig:P2.1}
    \end{subfigure}
\begin{subfigure}[b]{0.4\linewidth}
\begin{tikzpicture}[shorten >=1pt, node distance=3 cm, on grid, auto,thick,initial text=,]
\begin{scope}[every node/.style={scale=.8}]
\node (l1) [state,accepting,fill = safecellcolor]   {$q_1$};
\node (l0) [state,initial below,left = of l1,xshift = 2.3cm,fill = safecellcolor] {$q_0$};

\node (l2) [state,left = of l0,xshift = 2.3cm,accepting, fill = safecellcolor]   {$q_2$};
\end{scope}
 \begin{scope}[every node/.style={scale=.7}]
\path [->]
    (l0) edge [loop above]  node {$a \land b$}()
    (l0) edge  node [above] {$b$}   (l1)
    (l0) edge  node [above] {$a$}   (l2)
    (l1) edge [loop above] node [above] {$b$}   ()
    (l2) edge [loop above] node [above] {$a$}   ()
    ;
\end{scope}
\end{tikzpicture}
\label{fig:P2.2}
\end{subfigure}
    
    \caption{Unsuitable(left) and suitable(right) \textbf{LDBW} for the LTL formula $(\always a) \lor (\always b)$ from  \cite{hahn2013lazy}}
    \label{fig:P2}
\end{figure}
\end{comment}



% Our objective is to get an optimal schedule $\sigma$ for a given $\omega$-regular objective $\phi$ and a CTMDP $\Mdp$ with unknown transition structure using model-free learning algorithms. Model-free learning algorithms can be used only if we can define rewards that depend on the observations from the CTMDP and the satisfaction of the property. We use a product CTMDP $\pmdp$ where $\Mdp$ is a CTMDP and $\oautomata$ is an \textbf{GFM}. A product CTMDP can be used to monitor both the observations from the CTMDP and the satisfaction of the given property. Using \textbf{GFM} has been shown to give an optimal schedule in MDPs \cite{HahnPSSTW19}. We give a formal proof that we can use a similar approach on CTMDPs and get an optimal schedule. 

% \track{Are we going to consider the objective of maximizing dwell time, i.e. find a schedule for maximizing the expected dwell time. In that case, what is the use of $\omega$-regular objective? This is simply an average optimization problem. I don't see the need for LDBA any more, or a product construction as this is no longer a qualitative but a quantitative objective.}

% We note here that while composing with an MDP, it is preferred that the specification is given in terms of a deterministic automaton \cite{vardi1985automatic}.

% \vspace{0.5em}\noindent\textbf{Two Semantics.} We consider two semantics for satisfaction of $\phi$ in the context of a CTMDP in which a run includes passage of real time.
% In the first one of the two semantics, the objective is to maximise the residence time in the ``good states'' which are the set of accepting states of the \textbf{GFM}. To apply model-free RL algorithms, we need to define suitable reward mechanisms based on the observations made on the CTMDP environment.
% The reward mechanisms should be designed in a way to capture the specific objective in a model with continuous time semantics. It may be desirable to satisfy the property with certain quality, which is reflected using maximising the residence time in those states. We also refer to this semantics as the \emph{expectation semantics}.
% For an infinite run $r_{inf}=(s_1,t_1,a_1,s_2,t_2,a_2, \ldots)$ of a given CTMDP $\Mdp$ under schedule $\sigma$, we define the residence time of $s$ in $r_{inf}$ as 
% \[
% Rt^{\Mdp^{\sigma}}(s)(r_{inf}) = \liminf_{n \rightarrow \infty} \frac{\sum_{i=1}^{n} [s_i = s] \cdot t_i}{\sum_{j=1}^{n}t_j}.
% \]
% % Note that we consider $\liminf$ since the limit may not exist in general.
% For a set $T$ of good states and the infinite run $r_{inf}$, we define residence time of $T$ under schedule $\sigma$ as 
% % \todo{Should we use equation environment}
% \[
% Rt^{\Mdp^{\sigma}}(T)(r_{inf}) = \liminf_{n \rightarrow \infty} \frac{\sum_{i=1}^{n} [s_i \in T] \cdot t_i}{\sum_{j=1}^{n}t_j}.
% \]
% In the expectation semantics, our goal is to find a schedule $\sigma^*$ that maximizes the expected residence time in the set $T$ of states, i.e.
% \[
% \mathbb{E} [Rt^{\Mdp^{\sigma^*}}(T)] = \sup_{\sigma \in \Sigma_\Mdp} \mathbb{E} [Rt^{\Mdp^{\sigma}}(T)].
% \]


% In the second semantics, we consider the satisfaction of $\omega$-regular objectives in the traditional sense, where an optimal schedule is one which maximises the probability of satisfying the $\omega$-regular objective.
% The frequency of visiting the ``good" states is not relevant for the satisfaction of the objective.
% We refer to this semantics as the \emph{satisfaction} semantics.
% In the satisfaction semantics, given a CTMDP $\Mdp$, and an $\omega$-regular objective $\phi$, our goal is to find a schedule $\sigma^*$ such that \track{WHY $\geq$ BELOW?}
% \[
% \Pr[\Mdp^{\sigma^*} \models \phi] \geq \sup_{\sigma \in \Sigma_\Mdp} \Pr[\Mdp^{\sigma} \models \phi],
% \]
% where $\Pr[\Mdp^{\sigma} \models \phi]$ denotes the probability of satisfying $\phi$ by the CTMDP $\Mdp$ under schedule $\sigma$.

% We show in the following sections that we need different reward mechanisms for the two semantics in order to learn the corresponding optimal schedules using RL.

% \subsection{Reward Machines}
% Often, complex learning objectives cannot be expressed using Markovian reward
% signals. 
% A recent trend is to express learning objectives using finite-state reward
% machines~\cite{icarte2018using}. 
% We require a more expressive variant of reward machine capable of $\epsilon$ transitions\track{ARE WE USING $\epsilon$ TRANSITIONS? SIMPLER PRODUCT DEFINITION WITHOUT $\epsilon$.} and nondeterminism. We
% call them nondeterministic reward machines. 
% A (nondeterministic) reward machine is a tuple $\Rr = (\Sigma_\epsilon, U, u_0, \delta_r, \rho)$
% where $U$ is a finite set of states, $u_0 \in U$ is the starting state,
% $\delta_r: U \times \Sigma_\epsilon \to 2^U$ is the transition relation, 
% and $\rho: U \times \Sigma_\epsilon \times U \to \Reals$ is the reward function, where $\Sigma_\epsilon = (\Sigma\cup \set{\epsilon})$ and $\epsilon$ is a special silent transition.

% Given a labelled MDP $\Mdp = (S, s_0, A, T, AP, L)$\track{WE USE $\actions$ ELSEWHERE.} and a reward machine $\Rr = (\Sigma_\epsilon, U, u_0, \delta_r, \rho)$ over the alphabet $\Sigma = 2^{AP}$,  their product 
% $\Mdp\times\Rr = (S{\times} U, s_0 {\times} u_0, (A {\times} U) \cup\set{\epsilon},
% T^\times, \rho^\times)$
% is a rewardful MDP where
% $T^\times: (S {\times} U) \times ((A {\times} U) \cup \set{\epsilon}) \to \Distributions(S{\times} U)$ is such that
% $T^\times((s,u), \alpha)(({s}',{u}'))$ equals 
% \begin{multline*}
% %T^\times((s,u), \alpha)(({s}',{u}')) =\\
% \begin{cases}
% T(s,a)({s}') & \text{if } \alpha = (a, u') \text{ and } (u,L(s),u') \in \delta_r \\
% 1 & \text{if } \alpha = \epsilon \text{ and } s = s' \text{ and } \delta(u, \epsilon, {u}') \in \delta_r \\
% 0 & \text{otherwise.}
% \end{cases}
% \end{multline*}
% and $\rho^\times: (S{\times} U) \times ((A {\times}
% U) \cup \set{\epsilon}) \times (S{\times} U)\to \Reals$ is defined such that 
% $\rho^\times((s,u), \alpha, (s', u'))$ equals
% \begin{multline*}
% % \rho^\times((s,u), \alpha, (s', u')) =\\
% \begin{cases}
% \rho(u, L(s), u') & \text{if } \alpha = (a, u') \text{ and } (u,L(s),{u}') \in \delta_r \\
% \rho(u, \epsilon, u') & \text{if } \alpha = \epsilon.
% \end{cases}
% \end{multline*}
% For technical convenience, we assume that $\Mdp{\times}\Rr$ contains only reachable states from $(s_0, u_0)$.
% For both discounted and average objectives, the optimal strategies of
% $\Mdp{\times}\Rr$ are positional on $\Mdp{\times}\Rr$.
% Moreover, these positional strategies characterize a finite memory strategy (with memory skeleton based on the  states of $\Rr$ and the next-action function based on the positional strategy) over $\Mdp$
% maximizing the learning objective given by $\Rr$. 


\paragraph{Problem Definition.} Given a CTMDP $\Mdp$ with unknown transition structure and rates, and an $\omega$-regular objective $\phi$ given as a good-for-CTMDP B\"uchi automata $\oautomata$, we are interested in the following reward translation problem for the satisfaction semantics and for the expectation semantics. 

\begin{problem}[Reward Translation Scheme]
    Design a reward scheme for $\oautomata$ 
    % for satisfaction (expectation) semantics 
    such that any off-the-shelf RL algorithm optimizing the discounted reward in CTMDPs converges to an optimal schedule for satisfaction (expectation) semantics.
\end{problem}

In Section~\ref{sec:theorems&algo} we provide a solution for the satisfaction semantics, while in Section~\ref{sec:d_time} we sketch a solution for this problem for the expectation semantics.
% In both of these results, 
We reduce these problems to average reward maximization for CTMDPs.
Since average-reward RL algorithms for CTMDPs and MDPs require strong assumptions on the structure (such as communicating MDPs)~\cite{Sutton18}, we solve the average-reward RL problem by reducing it to a discounted-reward problem using the following result.
\begin{theorem}\label{corollary:1}
For every CTMDP $\Mdp$, there exists a pure schedule $\bschedule$ and a threshold $0 {\leq} \ctmdpthrate^{\Mdp} {<} 1$ such that
% \begin{inparaenum}[(1).]
    % \item 
    for every discount-rate function $\ctmdprate$, where $\ctmdprate(s,a) \geq \ctmdpthrate^{\Mdp}$ for every valid state-action pair $(s,a)$, the schedule $\bschedule$ is an optimal schedule maximising the expected discounted reward.
    % \item 
    Moreover, $\bschedule$ also maximizes the expected average reward.
% \end{inparaenum}
\end{theorem}
This schedule $\bschedule$ is known as a Blackwell optimal schedule.
We provide a novel uniformization based proof for this theorem in Appendix~\ref{sec:blackwell_optimality}.
We show that we need different reward translation schemes for the two semantics.

% In the next section, we provide a simple uniformization based proof for the Blackwell optimality for CTMDPs.
% \track{Add Theorem 3 here?}