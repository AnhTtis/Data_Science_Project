Reinforcement learning (RL)~\cite{Sutton18} is a sequential optimization approach where a decision maker learns to optimally resolve a sequence of choices based on feedback received from the environment. This feedback often takes the form of rewards and punishments with strength proportional to the fitness of the decisions taken by the agent as judged by the environment towards some higher-level learning objectives.
\emph{This paper develops convergent RL algorithms for continuous-time Markov decision processes (CTMDP) against learning requirements expressed in $\omega$-regular languages~\cite{Baier08}.}

% The rest of the introduction provides a motivation for the aforementioned problem while comparing it to relevant related works. 

\paragraph{Need for Reward Translation.} 
Due to a combination of factors---including the success of deep neural networks~\cite{Goodfe16}
and a heavy intellectual and monetary investment from the industry and the
academe~\cite{Mnih15,Silver16,Levine16}---RL has emerged as a leading human-AI collaborative design paradigm
where the key role of the human designers reduces to designing the appropriate
scalar reward signals, while the RL algorithm creates an optimal schedule driven by the reward signal.  
Unfortunately, then, the de-facto communication between the human designers and the RL algorithms is quite rigid: it forces the human programmers to think in the language suitable for the learning agents and not in a way that comes naturally to humans: declarative or imperative languages. 
To meet this challenge, a recent
trend is to enable logic~\cite{sadigh2014learning,Bozkurt0ZP20,camacho2019ltl,li2017reinforcement}  and
automatic
structures~\cite{HahnPSSTW19,icarte2018using,IcarteKlassenValenzanoMcIlraith20}
to express learning intent in RL.
The common thread among these approaches is to encode the specification as an automaton based reward structure and derive scalar rewards with every
discrete interaction with the environment.
However, when the problem domain is continuous-time, aforementioned approaches are not
applicable as they support the discrete-time semantics modeled as finite-state Markov decision processes (MDP or DTMDP for emphasis). 

This paper aims to enable the use of RL in unknown CTMDPs against high-level specifications expressed as $\omega$-automata~\cite{VW86,Baier08}.

\vspace{-0.5em}
\paragraph{Continuous-Time Reinforcement Learning.} 
Semi-MDPs~\cite{Baykal2011} model environments where the interaction between the decision maker and the environment may occur at any dense time point.
CTMDPs~\cite{guo2009continuous} are subclasses of semi-Markov decision processes where
% the learner chooses an action and 
the exact time and the resolution of the next
state is governed by an exponential distribution with a \emph{rate} parameter that is dependent on the current state and the action chosen.
The classical RL algorithms for DTMDPs
have been elegantly generalized to CTMDPs for both discounted~\cite{BD94} and average~\cite{das1999solving} objectives.
We employ the Q-learning algorithm for CTMDP~\cite{BD94} to compute optimal
schedules for $\omega$-regular learning objectives. 


\paragraph{The $\omega$-Regular Objectives.}
Finite automata on infinite words---or $\omega$-automata---may be equipped with a variety of equally expressive infinitary acceptance conditions (e.g., deterministic Rabin and nondeterministic B\"uchi) with well-understood succinctness and complexity trade-offs. 
From their first application in solving Church's synthesis problem~\cite{thomas2009facets} to becoming the \emph{lingua franca} in expressing specifications of safety-critical systems~\cite{Baier08}, $\omega$-automata are a key part of the computational backbone to automated verification and synthesis.
Linear temporal logic (LTL)~\cite{Baier08} is a popular declarative language to express properties of infinite sequences.
Specifications expressed using $\omega$-automata form a strict superset of specifications expressed as LTL formulas. Given an LTL formula, one can effectively construct an $\omega$-automaton~\cite{VW86}.
For this reason, we focus on $\omega$-automata based specifications.

The expanding role of RL in safety-critical systems has prompted the use of $\omega$-automata in expressing learning objectives due to improved expressiveness and interpretability over scalar rewards.
In this work, we use nondeterministic B\"uchi automata to express $\omega$-regular specifications.

\paragraph{Continuous-Time in B\"uchi Automata.}
B\"uchi automata are finitary structures accepting infinite sequences of letters 
that visit a distinguished set of good (accepting) states infinitely often.  
For scheduling problems over stochastic systems modeled as DTMDPs, the optimal schedules can be specified via schedules that maximize the measure of accepted system
behaviors. 
While for discrete-time system the naturalness of such discrete infinitary
visitation semantics is well-established, for continuous-time systems it is
imperative that the acceptance criterion must heed to the actual time spent in
such good states.  
Two distinct interpretations of good dense-time behavior are natural:
While the focus of the \emph{satisfaction semantics} is on maximizing the
measure of behaviors that visit good states infinitely often, the \emph{expectation semantics} focuses on maximizing the long-run expected time
spent in good states.
% While \emph{expectation semantics} focuses on maximizing the long-run expected time spent in good states, the focus of the \emph{satisfaction semantics} is on maximizing the measure of behaviors that visit good states infinitely often.
We develop RL algorithms for CTMDPs with B\"uchi specifications
under both semantics.

A recent work~\cite{oura2022learning} studies an alternative objective for semi-MDPs against multi-objective specifications composed of an $\omega$-regular objectives (satisfaction semantics) and a risk objective (expected risk). 
The key distinction between \citeauthor{oura2022learning}'s approach and ours (vis-\`a-vis the satisfaction semantics) is that the former is based on bounded synthesis paradigm that requires a bound parameter on co-B\"uchi states visitation and thus reduces the specification to a safety objective (where reward translation is straightforward). In contrast, our approach does not require any bound from the practitioner and is capable of handling general $\omega$-regular objectives.
Moreover, the expectation semantics has not been explored in any existing literature. 

% \paragraph{Related Work.}
% In this section, we compare our approach on satisfaction semantics with \cite{oura2022learning}. 
% CTMDP ... 


% Continuous time stochastic logic (CSL)~\cite{baier1999CSL} is a stochastic branching time temporal logic and is used to model-check CTMCs against properties expressed in it.
% The logic contains a steady state operator $\mathcal{S}$ to express limiting distribution properties, and our expectation semantics can be captured using the $\mathcal{S}$ operator. However, in our work, we consider a CTMDP environment, and we are are interested in the \emph{synthesis} of a schedule rather than the model-checking problem.


\paragraph{Contributions.}
% The contributions of the paper are as follows.
Our key contributions are as follows:
\begin{enumerate}
    \item 
    We present a novel (expectation) semantics for B\"uchi automata to capture time-critical properties for CTMDPs. 
    \item 
    We present procedures to translate B\"uchi automata with satisfaction and expectation semantics to reward machines \cite{icarte2018using} in a form that enables application of the off-the-shelf CTMDP RL algorithms.
    We show that one needs distinct reward mechanisms for these two semantics, and we
    establish the correctness and effectiveness of these reward translations.
    \item As a by-product of the proofs, we provide  a simplified proof of existence of Blackwell optimal schedules~\cite{puterman2014markov} in CTMDPs based on uniformization. 
    \item 
    We present an experimental evaluation to demonstrate the effectiveness of the proposed approach.
\end{enumerate}



Due to space constraints, the detailed proofs and other omitted information (such as details of the benchmarks) are provided as part of the supplementary material.

\input{example}

% \vspace{0.5em}
% \noindent\textbf{Organization.} 
% The paper is organized as follows. 
% We recall the technical preliminaries in Section~\ref{sec:prelims}, and define the two semantics for $\omega$-regular objectives and the key problem in Section~\ref{sec:p_statement}.
% In Section~\ref{sec:blackwell_optimality}, we give a simple uniformization based proof of existence of Blackwell optimal schedules in CTMDPs.
% The correctness of the reward translation for the expectation and satisfaction semantics is derived in Section~\ref{sec:d_time} and Section~\ref{sec:theorems&algo}.
% We demonstrate the effectiveness of our algorithms via some experimental results in Section~\ref{sec:expt}.
