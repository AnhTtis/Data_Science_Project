\begin{algorithm}
\caption{Algorithm for satisfaction semantics}\label{algo:sat}
\hspace*{\algorithmicindent} \textbf{Input:}  \text{Initial state $s_{init}$, SLDBW $A$, discount factor $\gamma$,}\\
\hspace*{\algorithmicindent} \hspace{11mm}\text{reward function $R'$, number of episodes $k$,} \\
\hspace*{\algorithmicindent}\hspace{11mm} \text{learning rate $\alpha$}\\
\hspace*{\algorithmicindent} \textbf{Output:}  \text{$Q_f$}
\begin{algorithmic}[1]
\STATE Initialise $Q_f$ to all zeroes
\STATE $i \leftarrow 0$
\WHILE{$i < k$}
\STATE $s \leftarrow \initstate$
\STATE $q \leftarrow q_0$
\STATE $r \leftarrow 0$
\WHILE{r = 0}
\STATE Choose action $a$ according to the RL policy
\STATE Take action $a$, observe next state $s'$, and time $\tau$
\STATE Choose transition $t$ in $A$ according to the RL policy
\STATE Take transition $t$ in $A$, observe next state $q'$
\STATE $r \leftarrow R'(s,q,a)$
\STATE $Q_f(s,q,a) \leftarrow Q_f(s,q,a) +\alpha \bigl[ r + e^{-\gamma\tau} \max_{a' \in \actions} Q_f(s',q',a')- Q_f(s,q,a) \bigr]$
\STATE $s \leftarrow s'$
\STATE $q \leftarrow q'$
\ENDWHILE
\STATE $i \leftarrow i+1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Line 1 initialises the Q function which is defined as $Q_f: \states \times Q \times \actions \rightarrow \mathbb{R}$. Line 2 initialises $i$ which keeps the count of number of episodes completed. The algorithms conducts $k$ episodes. The variables $s$ and $q$ represents the current state of the CTMDP and SLDBW respectively and is initialised to their initial states. The action $a$ from the current state of CTMDP and the transition $t$ from the state of SLDBW is picked according to the RL policy used (eg. $\epsilon$-greedy). From the results in section \ref{sec:theorems&algo}, we define the reward function $R'$. For a given state $(s,q)$ and action $a$, $$
R'((s,q),a) = \begin{cases}
1 \text{ with probability $1-\zeta$} & \text{if (s,q) is accepting}\\
0 & \text{otherwise}
\end{cases}
$$
where $\zeta \in (0,1)$. Variable $r$ stores the reward obtained in each iteration of the episode. An episode ends when a positive reward is received, i.e, when $r > 0$. 

% \begin{algorithm}
% \caption{Algorithm for expectation semantics}\label{algo:expt}
% \hspace*{\algorithmicindent} \textbf{Input:}  \text{Initial state $s_{init}$, SLDBW $A$, discount factor $\gamma$,}\\
% \hspace*{\algorithmicindent} \hspace{11mm}\text{reward function $R'$, number of episodes $k$,} \\
% \hspace*{\algorithmicindent}\hspace{11mm} \text{learning rate $\alpha$, episode length $eplen$}\\
% \hspace*{\algorithmicindent} \textbf{Output:}  \text{Optimal strategy $\sigma$}
% \begin{algorithmic}[1]
% \STATE Initialise $Q_f$ to all zeroes
% \STATE $i \leftarrow 0$
% \WHILE{$i < k$}
% \STATE $s \leftarrow \initstate$
% \STATE $q \leftarrow q_0$
% \STATE $r \leftarrow 0$
% \STATE $j \leftarrow 0$
% \WHILE{$j < eplen$}
% \STATE Choose action $a$ according to the RL policy
% \STATE Take action $a$, observe next state $s'$, and time $\tau$
% \STATE Choose transition $t$ in SLDBW according to the RL policy
% \STATE Take transition $t$ in SLDBW, observe next state $q'$
% \STATE $r \leftarrow R'(s,q,a,\tau)$
% \STATE $Q_f(s,q,a) \leftarrow Q_f(s,q,a) +\alpha \bigl[ r + e^{-\gamma\tau} \max_{a' \in \actions} Q_f(s',q',a')- Q_f(s,q,a) \bigr]$
% \STATE $s \leftarrow s'$
% \STATE $q \leftarrow q'$
% \ENDWHILE
% \STATE $i \leftarrow i+1$
% \ENDWHILE
% \end{algorithmic}
% \end{algorithm}
%  The algorithm for expectation semantics (algorithm \ref{algo:expt}) is similar to that of satisfaction semantics, here the termination of an episode is based on the user defined variable $eplen$ and the reward function $R'$ is based on the reward function defined in section \ref{sec:d_time}. The reward function is defined as 
%  $$
%  R'((s,q),a,\tau) = \begin{cases}
%  \tau & \text{if ($s,q$) is an accepting state}\\
%  0 & \text{otherwise}
%  \end{cases}
%  $$