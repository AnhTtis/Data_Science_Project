In the classical RL literature, the learning objective is specified using
Markovian reward functions, i.e. a function $\rho: S \times A \to \Real$
assigning utility to state-action pairs. 
A {\it rewardful} MDP is a tuple $\Mm = (S, s_0, A, T, \rho)$ where $S, s_0, A,$ and $T$ are defined in a similar way as for MDPa, and $\rho$ is a Markovian reward function.
%, where $\Mm$ is an MDP
%and $\rho\colon S \times A \to \Real$ is a reward function 
A rewardful MDP $\Mm$ under a
strategy $\sigma$ determines a sequence of random rewards
${\rho(X_{i-1}, Y_i)}_{i \geq 1}$, where $X_i$ and $Y_i$ are the
random variables denoting the $i$-th state and action, respectively.
%% The {\it reachability probability} objective $\PReach(T)^\Mm_\sigma(s)$ (with
%% $T \subseteq S$) is defined as
%% $\Pr^\Mm_\sigma(s) \set{\seq{s,a_1,s_1,\ldots} \in \Runs_\sigma^\Mm(s)
%%   \colon \exists i \scope s_i \in T}$.
  For
$\lambda \in [0, 1[$, the {\it discounted reward} 
$\EDisct(\lambda)^\Mm_\sigma(s)$ is defined as
\[
\lim_{N \to \infty} \eE^\Mm_\sigma(s) \set{\sum_{1 \leq i \leq N}
  \lambda^{i-1} \rho(X_{i-1}, Y_i)},
\]
while the {\it average
  reward} $\EAvg^\Mm_\sigma(s)$ is defined as
\[
\limsup_{N \to \infty} \frac{1}{N} \eE^\Mm_\sigma(s)\set{\sum_{1\leq i \leq
    N} \rho(X_{i-1}, Y_i)}.
\]
For an objective $\ECost^\Mm {\in} \{\EDisct(\lambda)^\Mm,
\EAvg^\Mm\}$ and state $s$, we define the optimal reward
$\ECost^\Mm_*(s)$ as $\sup_{\sigma \in \Strat_\Mm} \ECost^\Mm_\sigma(s)$.  
A strategy $\sigma$ is optimal for $\ECost^\Mm$ if
$\ECost^\Mm_\sigma(s) {=} \ECost^\Mm_*(s)$ for all $s {\in} S$.
The optimal cost and strategies for these objectives can be computed in
polynomial time~\cite{Puterman94}.  

Often, complex learning objectives cannot be expressed using Markovian reward
signals. 
A recent trend is to express learning objectives using finite-state reward
machines~\cite{icarte2018using}. 
We require a more expressive variant of reward machine capable of $\epsilon$ transitions and nondeterminisim. We
call them nondeterministic reward machines. 
A (nondeterministic) reward machine is a tuple $\Rr = (\Sigma_\epsilon, U, u_0, \delta_r, \rho)$
where $U$ is a finite set of states, $u_0 \in U$ is the starting state,
$\delta_r: U \times \Sigma_\epsilon \to 2^U$ is the transition relation, 
and $\rho: U \times \Sigma_\epsilon \times U \to \Real$ is the reward function, where $\Sigma_\epsilon = (\Sigma\cup \set{\epsilon})$ and $\epsilon$ is a special silent transition.

Given an MDP $\Mm = (S, s_0, A, T, AP, L)$ and a reward machine $\Rr = (\Sigma_\epsilon, U, u_0, \delta_r, \rho)$ over the alphabet $\Sigma = 2^{AP}$,  their product 
\[
\Mm\times\Rr = (S{\times} U, s_0 {\times} u_0, (A {\times} U) \cup\set{\epsilon},
T^\times, \rho^\times)
\]
is a rewardful MDP where
$T^\times: (S {\times} U) \times ((A {\times} U) \cup \set{\epsilon}) \to \DIST(S{\times} U)$ is such that
$T^\times((s,u), \alpha)(({s}',{u}'))$ equals 
\begin{multline*}
%T^\times((s,u), \alpha)(({s}',{u}')) =\\
\begin{cases}
T(s,a)({s}') & \text{if } \alpha = (a, u') \text{ and } (u,L(s),u') \in \delta_r \\
1 & \text{if } \alpha = \epsilon \text{ and } s = s' \text{ and } \delta(u, \epsilon, {u}') \in \delta_r \\
0 & \text{otherwise.}
\end{cases}
\end{multline*}
and $\rho^\times: (S{\times} U) \times ((A {\times}
U) \cup \set{\epsilon}) \times (S{\times} U)\to \Real$ is defined such that 
$\rho^\times((s,u), \alpha, (s', u'))$ equals
\begin{multline*}
% \rho^\times((s,u), \alpha, (s', u')) =\\
\begin{cases}
\rho(u, L(s), u') & \text{if } \alpha = (a, u') \text{ and } (u,L(s),{u}') \in \delta_r \\
\rho(u, \epsilon, u') & \text{if } \alpha = \epsilon.
\end{cases}
\end{multline*}
For technical convenience, we assume that $\Mm{\times}\Rr$ contains only reachable states from $(s_0, u_0)$.
For both discounted and average objectives, the optimal strategies of
$\Mm{\times}\Rr$ are positional on $\Mm{\times}\Rr$.
Moreover, these positional strategies characterize a finite memory strategy (with memory skeleton based on the  states of $\Rr$ and the next-action function based on the positional strategy) over $\Mm$
maximizing the learning objective given by $\Rr$. 
