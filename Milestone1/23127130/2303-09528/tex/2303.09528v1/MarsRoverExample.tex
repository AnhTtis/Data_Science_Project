Following example gives an intuition on satisfaction and expectation semantics.
\begin{example}[Satisfaction or Expectation?]
The grid-world shown in Figure~\ref{fig:grid-world} (adapted from \cite{HahnPSSTW19}) represents four zones (represented by $s_0$ to $s_3$ in the figure) on the Mars surface. 
Suppose that a mission to Mars arrives in Zone~$0$ (a known, safe territory) and is expected to explore the terrain in a safe fashion, gather and transmit information, and stay alive to maximize the return on the mission.
For simplicity, assume that Zone $1$ (purple) models a crevasse harmful to the safe operations, while zones $2$ and $3$ are central to exploration mission and are analogous in their information contents.

\input{automata_drawing}
Given the unknown uncertainty of the terrain of Mars, the system is modeled as a CTMDP with associated uncertainty on the time of various actions where the exit rate of action $a$ from Zone (state) $0$ is denoted by $\lambda(0,a)$. In other words, when selected an action $a$ in a state $s$, the probability of spending $t$ time units in $s$ before taking $a$ is given by the cumulative distribution function $1 - e^{- \lambda(0,a)t}$.
Assume that the action $b$ from Zone $0$ goes to Zone $2$ with rate $r$ (high probability) and to Zone $1$ with rate  $(\lambda(0,b) - r)$ (low probability).
% \textcolor{red}{there is an action $b$ and a label for state $b$, this would create confusion.}
The mission objective is to avoid Zone $1$ (blue zone) while infinitely often visiting the Zone $2$ or $3$ (the green zones). 
It can be captured in LTL~\cite{Baier08} as:
\[
\varphi = (\always \neg \mathtt{p}) \wedge (\always(\eventually \mathtt{g}))
\]
specifying that across the infinite horizon {\bf globally} (i.e. at every step expressed as temporal modality, $\always$) avoid the blue region ($\neg b$), and globally {\bf finally} (i.e. at some time in the future expressed as temporal modality, $\eventually$) reach the green region, i.e. $(\always (\eventually g))$. 
The $GF \phi$ modality is often referred as \emph{infinitely often $\phi$}.
LTL combines these temporal operators using the standard propositional logic connectives such as: and ($\wedge$), or ($\lor$), not ($\neg$), and implication ($\to$).

This declarative specification can also be expressed using the B\"uchi automaton shown in Figure~\ref{fig:grid-world} (center) where the double circled states (here, $q_1$) denote accepting states.
The B\"uchi automata can be used as the monitors of the behaviors of the learner over the environments given as CTMDPs. For our example, it is visualized by taking the synchronous product (an extended space CTMDP) of the CTMDP with the automaton shown in Figure~\ref{fig:grid-world} (right).

For the satisfaction semantics on the product CTMDP, our goal is to maximize the probability that every infinite horizon behavior visits the accepting state infinitely often, while for the expectation semantics the goal is to maximize the expected time the system dwells in the accepting state.
\begin{itemize}
    \item \noindent {\bf Satisfaction Objective.} 
Consider the case where we have one Mars rover in this mission. Hence, our goal naturally is to maximize the probability of visiting green zones infinitely often while avoiding the blue zone (the satisfaction semantics).
In this case, the optimal schedule is to choose actions $a$ and $c$ indefinitely, i.e. the schedule $(a \to c)^\omega$, that satisfies the objective with probability $1$.
Note that it does not make sense to choose action $b$ no matter how low the probability is to reach the blue zone.
\item 
\noindent {\bf Expectation Objective.} Consider an alternative setting where we have a fleet of drones (we are okay in losing some drones as long as we maximize the mission objective) that needs to be sent to the surveillance of zone $2$ or $3$. 
Suppose that due to unforeseeable circumstances the mission may cease operation any time, and hence the goal is to maximize total expected time spent in the green zones ($2$ and $3$).
The schedule $(a\to c)^\omega$ is not optimal anymore as it may dwell a considerable amount in the Zone $0$. 
On the hand hand any drone that chooses $b$ in Zone $0$ risks moving to Zone $1$ with a small probability.
As our goal is to maximize the expected time over a large group of drones, the expectation semantics captures this intent and the optimal schedule is to start with action $b$.
\end{itemize}
\end{example}