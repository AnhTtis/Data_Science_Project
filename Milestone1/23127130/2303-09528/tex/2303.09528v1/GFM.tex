
Given an MDP $\Mm$ and an $\omega$-regular objective $\varphi$ given as
an $\omega$-automaton $\Aa_\varphi = (\Sigma,Q,q_0,\delta,F)$,
we want to compute an optimal strategy satisfying the
objective.
We define the satisfaction probability of $\sigma$ from starting state
$s$ as: 
\begin{equation*}
\PSemSat^\Mm_{\Aa}(s, \sigma) 
%\Pr^\Mm_\sigma(s \models \varphi)
=   \Pr{}^\Mm_\sigma(s) \set{ r \in \Runs_\sigma^\Mm(s) \colon
  L(r) \in \Ll(\Aa) }.
\end{equation*}
The optimal satisfaction probability
$\PSemSat^\Mm_{\Aa}(s)$ for specification $\Aa$ %$\Pr^\Mm_*(s\models \varphi)$
is defined as $\sup_{\sigma \in \Strat_\Mm} \Pr^\Mm_\sigma(s, \sigma)$ and we say
that $\sigma \in \Strat_\Mm$ is an optimal strategy for $\Aa$ if
$\PSemSat^\Mm_{\Aa}(s, \sigma) (s) = \PSemSat^\Mm_{\Aa}(s)$.

Given an MDP
%$\Mm = \langle S, s_0, A, T, \Sigma, L \rangle$
$\Mm = ( S, s_0, A, T, AP, L )$
and automaton
%$\mathcal{A} = \langle \Sigma, Q, q_0, \Delta, F \rangle$,
$\mathcal{A} = (2^{AP}, Q, q_0, \delta, F )$,
the
\emph{product}
%$\Mm \times \mathcal{A} = \langle S \times Q, (s_0,q_0), A \times Q,T^\times, F^\times \rangle$
$\Mm \times \mathcal{A} = ( S \times Q, (s_0,q_0), A \times Q, T^\times, F^\times )$
is an MDP with initial
state $(s_0,q_0)$
and accepting transitions $F^\times$ where
%The function
$T^\times : (S \times Q) \times (A \times Q) \pto \DIST(S \times Q)$
is defined by
\begin{multline*}
T^\times((s,q),(a,q'))(({s}',{q}')) {=}
\begin{cases}
T(s,a)({s}') & \text{if } (q,L(s,a,{s}'),{q}') {\in} \delta \\
0 & \text{otherwise.}
\end{cases}
\end{multline*}
The final state
$F^\times \subseteq (S \times Q) \times (A \times Q) \times (S
\times Q)$ is defined by $((s,q),(a,q'),(s',q')) \in F^\times$
if, and only if, $(q,L(s,a,s'),q') \in F$ and $T(s,a)(s') > 0$.
A strategy $\sigma^\times$ on the product defines a strategy $\sigma$ on the
MDP with the same value, and vice versa.  Note that for a stationary
$\sigma^\times$, the strategy $\sigma$ may need memory.
End-components and runs of the product MDP are defined just like for MDPs.

A run of $\Mm \times \mathcal{A}$ is accepting if
$\inf(r) \cap F^\times \neq \emptyset$.
We define the
\emph{syntactic satisfaction}
probabilities $\PSat^\Mm_{\Aa}((s,q), \sigma^\times)$ as the probability of
accepting runs, i.e.
\[
\Pr{}^{\Mm\times\Aa}_{\sigma^\times}(s,q) \Set{ r \in
\Runs_{\sigma^\times}^{\Mm\times\Aa}(s,q) : \inf(r) \cap F^\times
\neq \emptyset }
\]
Similarly, we define $\PSat^\Mm_{\Aa}(s)$ as the optimal probability over the
product, i.e.  $\sup_{\sigma^\times}\big(\PSat^\Mm_{\Aa}((s,q_0),
\sigma^\times)\big)$.
For a deterministic $\Aa$ the equality $\PSat^\Mm_{\Aa}(s) = \PSemSat^\Mm_{\Aa}(s)$ holds; however it is not guaranteed for nondeterministic
B\"uchi automata as the optimal resolution of nondeterministic
choices may require access to future events.
This motivates for the definition of a good-for-MDP nondeterminisitc B\"uchi automata.
A B\"uchi automaton $\Aa$ is \emph{good for MDPs} (GFM),
if $\PSat^\Mm_{\Aa}(s_0) = \PSemSat^\Mm_{\Aa}(s_0)$ holds for all
MDPs $\Mm$ and starting states $s_0$ \cite{Hahn20}.
Note that every $\omega$-regular objective can be expressed as a GFM automaton \cite{Hahn20}.
A popular class of GFM automata is suitable limit-deterministic B\"uchi automata~\cite{Hahn2015LazyPM,sickert2016limit}.
This paper considers only GFM B\"uchi automata.

The satisfaction of an $\omega$-regular objective given as a GFM automaton $\Aa$
by an MDP $\Mm$ can be formulated in terms of the accepting maximal
end-components of the product $\Mm {\times} \mathcal{A}$, i.e. the maximal
end-component that contains an accepting transition from $\F^\times$.  
The optimal satisfaction probabilities and strategies can be computed by
computing the accepting maximal end-component of $\Mm\times \Aa$ and then
maximizing the probability to reach states in such components.
The optimal strategies are positional on $\Mm\times\Aa$ and characterize a finite memory strategy over $\Mm$ maximizing satisfaction probability of the learning objective given by $\Aa$.
