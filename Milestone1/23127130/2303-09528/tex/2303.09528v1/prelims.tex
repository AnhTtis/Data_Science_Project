We write $\mathbb{N}, \rat$ and $\ratpos$ for the sets of natural numbers, rational numbers, and non-negative rational numbers, respectively.
For a natural number $n \in \nat$, we denote by $[n]$ the set $\set{1, \ldots, n}$.
% , and by $[n]_0$ the set $\{0, \dots, n\}$.
% \todo{Daddy cool: are these sets both supposed to be the same?}
Given a finite set $A$, a (rational) \textit{probability distribution} over $A$ is a function
$\dist \colon A \rightarrow [0, 1] \cap \rat$ such that $\sum_{a\in A} \dist(a) = 1$. 
% We call $A$ the \emph{domain of $p$}, and denote it by $\dom(p)$.
We denote the set of probability distributions on $A$ by $\Distributions(A)$. The \textit{support} of the
probability distribution $\dist$ on $A$ is $\supp(\dist) = \left\lbrace a {\in} A \;\vert\; \dist(a) {>} 0\right\rbrace$. A distribution is called \emph{Dirac} if $|\supp(\dist)| = 1$.

% An alphabet $\Sigma$ is a finite set of letters. 
% A finite word over $\Sigma$ is a finite concatenation of letters from $\Sigma$, while an $\omega$-word is an infinite such concatenation.
% We write $\Sigma^*$ and $\Sigma^\omega$ for the set of finite and infinite words over $\Sigma$.

\paragraph{Continuous-Time MDPs.}
% A \emph{Markov chain} is a tuple $M = (S,E,\transP)$, where $S$ is a set of states, $E \subseteq S \times S$ is a set of edges (we assume in this paper that the set $E(s)$ of outgoing edges from $s$ is nonempty and finite for all $s \in S$), and $\transP: S \to \Distributions(E)$ assigns a probability distribution---on the set $E(s)$ of outgoing edges from $s$---to all states $s \in S$. In the following, $\transP(s,(s,s'))$ is denoted $\transP(s,s')$, for all $s \in S$. A Markov chain $M$ is \emph{finite} if $S$ is finite.
% {\color{red} Do we need to define Markov chains?}
A (discrete-time) \emph{Markov decision process (MDP)} is a tuple of the form $\Mdp = (\states, \initstate, \actions, \trans)$, where $\states$ is a finite set of \emph{states}, $\initstate {\in} \states$ is the \emph{initial} state, $\actions$ is a finite set of \emph{actions}, and $\trans: \states {\times} \actions \to \Distributions(\states)$ is a \emph{transition function}. 
Let $\actions{}(s)$ be the set of actions enabled in the state $s {\in} S$.
An MDP is called a Markov chain if for every $s {\in} S$, the set $\actions{}(s)$ is singleton.
% that given a state $s$ and an action $a\in \av(s)$ yields a probability distribution over successor states.
% A \emph{Markov decision process (MDP)} is a tuple of the form $\Mdp = (\states, \initstate, \actions, \av, \trans, \atomicprop, \labelling)$, where $\states$ is a finite set of \emph{states}, $\initstate \in \states$ is the \emph{initial} state, $\actions$ is a finite set of \emph{actions}, $\av: \states \to 2^{\actions}$ assigns to every state a set of \emph{available} actions, $\trans: \states \times \actions \to \Distributions(\states)$ is a \emph{transition function} that given a state $s$ and an action $a\in \av(s)$ yields a probability distribution over successor states, $\atomicprop$ is a set of \emph{atomic propositions} and $\labelling : \states \rightarrow 2^{\atomicprop}$ is a function assigning set 
% \paragraph{End components.}
% A pair $(T, A)$, where $\emptyset \neq T \subseteq S$ and $\emptyset \neq A \subseteq \Union_{s\in T} \av(s)$, is an \emph{end component} of an MDP $\Mdp$ if (i)~for all $s \in T, a \in A \intersection \av(s)$ we have $\supp(\trans(s,a)) \subseteq T$, and (ii)~for all $s, s' \in T$ there is a finite path $w = s a_0 \dots a_n s' \in (T \times A)^* \times T$, i.e.\ $w$ starts in $s$, ends in $s'$, stays inside $T$ and only uses actions in $A$.\footnote{This standard definition assumes that actions are unique for each state, i.e.\ $\av(s) \intersection \av(s') = \emptyset$ for $s \neq s'$.
% The usual procedure of achieving this in general is to replace $\actions$ by $\states \times \actions$ and adapting $\av$, $\trans$, and $\rew$ appropriately.}
% Intuitively, an end component describes a set of states for which a particular decision can be made from each state in terms of choosing an action each time the state is visited such that all possible paths remain inside these states and all of those states are visited infinitely often almost surely.
% An end component $(T, A)$ is a \emph{maximal end component (MEC)} if there is no other end component $(T', A')$ such that $T \subseteq T'$ and $A \subseteq A'$.
% For an MDP $\Mdp$ we denote its MECs by $\mec(\Mdp)$.
% Note that every state of an MDP belongs to at most one MEC and each MDP has at least one MEC.
% {\color{red} Can we define MECs closer to their use?}

% \vspace{0.5em}\noindent\textbf{Continuous Time Markov Decision Process.}
A \emph{continuous-time} MDP (CTMDP) is a tuple of the form $\Mdp = (\states, \initstate, \actions, \transR)$, where $\transR: \states {\times} \actions {\times} \states \to \Reals_{\geq 0}$ is a \emph{transition rate function}, while the rest of the parameters are as in the case of an MDP. 
% A \emph{continuous time Markov decision process (CTMDP)} is a tuple of the form $\Mdp = (\states, \initstate, \actions, \av, \transR, \atomicprop, \labelling)$, where $\transR: \states \times \actions \times \states \to \Reals_{\geq 0}$ is a \emph{transition rate matrix}, while the rest of the parameters are as in the case of an MDP.
For $s {\in} \states$ and $a {\in} \actions{}(s)$, we define $\lambda(s, a) = \sum_{s'} \transR(s,a,s') > 0$ to be the \emph{exit rate} of $a$ in $s$.
We define a \emph{probability matrix}, $P_{\mathcal{M}}$, where 
\[
    \transP_{\mathcal{M}}(s, a, s') = \begin{cases} 
    \frac{\transR(s, a, s')}{\lambda(s, a)} & \text{if } \lambda(s, a)>0 \\
    0 & \text{otherwise}
    \end{cases}
\]
 When $\mathcal{M}$ is clear from the context, we simply denote $P_{\mathcal{M}}$ by $P$.
The residence time for action $a$ in $s$ is exponentially distributed with mean $\lambda(s, a)$. For a given state $s$ and an action $a$, the  probability of spending $t$ time units in $s$ before taking the action is given by the cumulative distribution function $F(t|s,a) = 1 - e^{- \lambda(s,a)t}$
of the exponential distribution. The probability of a transition from state $s$ to $s'$ on an action $a$ in $t$ time units is given by 
$p_{a}(s,s',t) = P(s,a,s')\cdot F(t|s,a)$.
A CTMDP is called a continuous-time Markov chain (CTMC) if for every state $s \in S$, the set $\actions{}(s)$ is singleton.
% Let $\post(s,a) = \{t \:|\: \transR(s, a, t) > 0\}$, that is, the set of states that can be reached from $s$ through action $a$.
% An \emph{end-component} (EC) $M = (T,\actions{})$, with $\emptyset \neq T \subseteq \states$ and $\actions:T \rightarrow 2^{\actions{}}$ of an MDP $\Mdp$ is a \emph{sub-MDP} of $\Mdp$ such that: for all $s \in T$, we have that $\actions{}(s)$ is a subset of the actions available from $s$; for all $a \in \actions{}(s)$, we have $\post(s,a) \subseteq T$; and, it's underlying graph is strongly connected.
% An end-component is accepting if it contains an accepting state, otherwise, it is rejecting.

\paragraph{Uniformization.}
A \emph{uniform} CTMDP has a constant exit rate $C$ for all state-action pairs i.e, $\lambda(s,a) = C$ for all states $s \in \states$ and actions $a \in \actions{}(s)$. The procedure of converting a non-uniform CTMDP into a uniform one is known as \emph{uniformization}. Consider a non-uniform CTMDP $\Mdp$. Let $C {\in} \realpos$ be such that $C {\geq} \lambda(s,a)$ for all $(s, a) \in \states {\times} \actions$. We obtain a uniform CTMDP $\unif$ by changing the rates to $\transR'$:
    $$
    \transR'(s,a,s') = \begin{cases} 
    \transR(s,a,s') & \text{if } s \neq s' \\
    \transR(s,a,s')+C - \lambda(s,a) & \text{if } s=s' 
    \end{cases}
    $$
For every action $a \in \actions{}(s)$ from each state $s$ in the new CTMDP we have a self loop  if $\lambda(s,a) < C$. A uniformized CTMDP has a constant transition rate $C$ for all actions and because of this, the mean interval time between any two successive actions is constant.
See Appendix~\ref{uniform} for an example.
% An example of uniformization is given in Appendix~\ref{uniform}.
% Uniformization helps in applying some results of DTMDPs on CTMDPs.
% Note that since $\lambda(q_0, a_1)=3$, we obtain a self-loop on $q_0$ such that $\transR'(q_0,a_1,q_0)=3$ in the uniformized CTMDP.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.48\linewidth}
% \begin{tikzpicture}[shorten >=1pt, node distance=3 cm, on grid, auto,thick,initial text=]
% \begin{scope}
% \node (l0) [state,fill=safecellcolor]  {$q_0$};
% \node (l1) [state, fill=safecellcolor, right = of l0,xshift = -0.75cm]   {$q_1$};
% \end{scope}
%  \begin{scope}
% \path [->]
%     (l0) edge [bend left]  node [above] {$a_1, 3$}   (l1)
%     (l0) edge [bend right]  node [below] {$a_2,6$}   (l1)
%     (l1) edge [loop above] node [above] {$a_3,2$}   ()
%     ;
% \end{scope}
% \end{tikzpicture}
% \caption{Non-uniform CTMDP where the exit-rates are different for various state action pairs. } \label{fig:PM1}
% \end{subfigure}
% \hspace{0.5em}
% \begin{subfigure}[b]{0.45\linewidth}
% \begin{tikzpicture}[shorten >=1pt, node distance=3 cm, on grid, auto,thick,initial text=]
% \begin{scope}
% \node (l1) [state, fill=safecellcolor]  {$q_1$};
% \node (l0) [state, fill=safecellcolor, left = of l1,xshift = 0.75cm]  {$q_0$};
% \end{scope}
% \begin{scope}
% \path [->]
%      (l0) edge [loop above] node [above] {$a_1,3$}
%     (l0) edge [bend left]  node [above] {$a_1,3$}   (l1)
%     (l0) edge [bend right]  node [below] {$a_2,6$}   (l1)
%     (l1) edge [loop above] node [above] {$a_3,6$}   ()
%     ;
% \end{scope}
% \end{tikzpicture}
% \caption{A Uniform CTMDP where the exit-rate for every state-action pair is $6$. } \label{fig:PM2}
% \end{subfigure}
% \caption{Uniformization of a CTMDP}
% \label{fig:P1}
% \end{figure}


\paragraph{Schedules.}
An infinite run of the CTMDP is an $\omega$-word
$
(s_1, (t_1, a_1), s_2, (t_2, a_2), \ldots) \in \states \times ((\realpos \times \actions) \times \states)^\omega
$
where $s_i \in \states$, $a_i \in \actions{}(s_{i})$ and $t_i$ is the time spent on state $s_i$. A finite run is of the form $(s_1, t_1, a_1, \ldots, t_{n-1}, a_{n-1}, s_n)$ for some $n \in \mathbb{N}$. The set of infinite and the set of finite runs in $\Mdp$ are denoted by $\irun$ and $\frun$ respectively. Similarly, the set of infinite runs and the set of finite runs starting from a state $s$ in $\Mdp$ are denoted by $\irun(s)$ and $\frun(s)$ respectively. For $r \in \frun$, we denote by $last(r)$ the last state in the run $r$.

We use a \emph{schedule} to resolve non-determinism in a CTMDP. A schedule is a function $\sigma : \frun \rightarrow \Distributions(\actions)$, where $\Distributions(\actions)$ is a probability distribution on the set of enabled actions. Given a finite run $r \in \frun$, a schedule gives a probability distribution over all actions enabled in $last(r)$.
%  , i.e., if $p(a) \in [0,1]$ gives the probability of choosing action $a$ in $\sigma(r)$, then $\sum_{a \in \av(last(r))} p(a) = 1$.
 A schedule is \emph{deterministic} if $\Distributions(\actions)$ is Dirac, i.e, a single action is chosen in the distribution, otherwise it is \emph{randomized}. 
%  As CTMDPs deal with continuous time , a schedule also depends on the time spent in each state of the run r. A \emph{stationary} schedule is a schedule which depends only on $last(r)$. 
 Further, a schedule $\sigma$ is stationary if for all $r$,$r' \in \frun$ with $last(r) = last(r')$, we have that $\sigma(r) = \sigma(r')$. A \emph{pure} schedule is a deterministic stationary schedule. Let $\Sigma_{\Mdp}$ be the set of all schedules.
 
 A CTMDP $\Mdp$ under a schedule $\sigma$ acts as a continuous time Markov chain (CTMC) which is denoted by $\ctmc$.
 The set of infinite and the set of finite runs in $\ctmc$ are denoted by $\irun_\sigma$ and $\frun_\sigma$ respectively.
 The behavior of a CTMDP $\Mdp$ under a schedule $\sigma$ and starting state $s \in S$ is defined on a probability space
$(\irun_{\sigma}(s), \frun_{\sigma}(s), {\Pr}^{\Mdp}_{\sigma}(s))$ over
the set of infinite runs of $\sigma$ with starting state $s$.  Given a random variable 
% a real-valued random variable over the set of infinite runs
$f : \irun_{\sigma} \to \Reals$, we denote by $\mathbb{E}^{\Mdp}_{\sigma}(s) \{f\}$ the
expectation of $f$ over the runs of $\ctmc$.
For $n \geq 1$, we write $X_n$, $Y_n$, $D_n$, and $T_n$ for the random variables corresponding to the $n$-th state, action, time-delay in the $n$-th state, and time-stamp (time spent up to the $n$-th state). We let $D_0 = T_0 = 0$.
% These concepts for MDPs are defined analogously.
%  In the sequel, we use the terms `schedule' and `policy' interchangeably.
 
%  A {\it sub-MDP} of $\Mdp$ is an MDP $\Mdp' = (S', s_0'A', T')$, where $S' \subset
% S$, $A' \subseteq A$ is such that $A'(s) \subseteq A(s)$ for every $s \in S'$,
% and $T'$ is analogous to $T$ when restricted to $S'$ and
% $A'$. Moreover $\Mdp'$ is closed under probabilistic transitions.
% An {\it end-component}\cite{alma991027942769706011} of an MDP $\Mdp$ is a sub-MDP $\Mdp'$ such that for every state pair $s, s' \in S'$ there is a 
% schedule that can reach $s'$ from $s$ with positive probability. 
% A maximal end-component is an end-component that is maximal under set-inclusion.
% Every state $s$ of an MDP $\Mdp$ belongs to at most one maximal end-component.
% We can define sub-CTMDP, and end components in a CTMDP, as in the case of MDPs.
% Further, we can see that Theorem~\ref{thm:ec} also holds for CTMDPs. 

%  \begin{theorem}[End-Component Properties \cite{alma991027942769706011}]\label{thm:ec}
% Once an end-component $C$ of an MDP is entered, there is a schedule that visits every state-action pair in $C$ with probability 1 and stays in $C$ forever. Moreover, for every schedule the probability that a run ends up in an end-component is 1.  
% \end{theorem}
% We can define sub-CTMDP, and end components in a CTMDP, as in the case of MDPs.
% Further, we can see that Theorem~\ref{thm:ec} also holds for CTMDPs. 

\paragraph{Rewardful CTMDPs.}
A rewardful CTMDP ($\Mdp, rew$) is a CTMDP and a reward function $rew : \states \cup (\states \times \actions) \rightarrow \realpos$ which assigns a  \emph{reward-rate} to each state and a scalar reward to each state-action pair. Thus spending $t$ time-units in $s\in\states$ gives $rew(s) \cdot t$ of (state-delay) reward and choosing $a$ from $s$ gives $rew(s, a)$ (action) reward.

Continuous time discounting is done with respect to a discount parameter $\alpha {>} 0$ where one unit of reward obtained at time $t$ in the future gets a value of $e^{-\alpha t}$. Formally, the expected discounted reward for an arbitrary schedule $\sigma$ from a state $s$ is given by:
% Given a continuous discount parameter $\alpha {>} 0$, the expected discounted reward for an arbitrary schedule $\sigma$ from $s\in\states$, equals:
\begin{multline*}
\discobjective^{\Mdp[\sigma]} (\alpha)(s) =     \mathbb{E}_{\sigma}^{\Mdp}(s) \bigg[ \sum_{n=1}^{\infty} e^{-\alpha T_{n-1}}\Big( rew(X_n, Y_n) + \\ \int_{T_{n-1}}^{T_{n}}e^{-\alpha(t - T_{n-1})} rew(X_n)  dt\Big) \bigg].
 \end{multline*}
%  \begin{multline*}
% \discobjective^{\ctmc} (\alpha)(s) =     \mathbb{E}_{\sigma}^{\Mdp}(s) \bigg[ \sum_{n=1}^{\infty} e^{-\alpha T_{n-1}}\Big( rew(X_n, Y_n) + \\ \int_{T_{n-1}}^{T_{n}}e^{-\alpha(t - T_{n-1})} 
%  rew(X_n)  dt\Big) \bigg].
%  \end{multline*}
 Here, we multiply the expected reward obtained at the $n-$th state with $e^{- \alpha T_{n-1}}$ as per the continuous time discounting. The initial term in the parenthesis corresponds to the reward obtained from state $X_n$ by picking action $Y_n$ (action reward) and the second term corresponds to the state-delay reward i.e reward obtained with respect to the reward-rate $rew(X_n)$ which is discounted over the time $(t - T_{n-1})$. 
% On the other hand, the one-step expected average reward from state $s$ obtained by a state-action pair $(s,a)$ is given by $\rho(s,a) = rew(s,a) + \frac{rew(s)}{\lambda(s,a)}$. 

The expected average reward from $s$ under $\sigma$ is given by: 
 \begin{multline*}
 \avgobjective^{\Mdp[\sigma]}(s) = \liminf_{N\rightarrow \infty}  \mathbb{E}_{\sigma}^{\Mdp}(s) \bigg[ 
 \frac{1}{T_{N}} \cdot \Big(\sum_{n=1}^{N} rew(X_n, Y_n) +\\ \int_{T_{n-1}}^{T_{n}}
 rew(X_n) dt \Big) \bigg],
 \end{multline*}
where the first and second term corresponds to the action and state-delay reward respectively. 
Recall that $T_{N}$ is the total time spent upto the $n$-th state.
% \begin{equation*}
%     \begin{split}
%  \avgobjective^{\ctmc}(s) = \liminf_{n \rightarrow \infty}  \frac{\mathbb{E}_{\sigma}^{\Mdp}(s) \{\sum^{n}_{i=0} \rho(s_i,a_i)\}}{\mathbb{E}_{\sigma}^{\Mdp}(s) \{\sum_{i=0}^{n} \tau_i\}} \end{split}
% \end{equation*}
%  where $s_i$, $a_i$, and $\tau_i$ are the random variables for the $i$-th state, action, and dwell-time. 
Consider an objective $\objective \in 
 \{\discobjective, \avgobjective \}$. 
%  \{\reachobjective, \discobjective, \avgobjective \}$. 
 The expected reward obtained by schedule $\sigma$ on $s \in \states$ is denoted by $\objective^{\Mdp[\sigma]}\!(s)$. A schedule $\sigma^{*}$ is optimal for $\objective$ if 
    $\objective^{\Mdp[\sigma^*]}(s) = sup_{\sigma \in \Sigma_{\Mdp} }\objective^{\Mdp[\sigma]}\!(s)$ for all $s \in \states$.


% \subsection{Linear Temporal Logic (LTL)}
% Linear temporal logic(LTL) \cite{pnueli1977temporal} can describe a subset of $\omega$-regular objectives. The syntax of the logic contains the operators $\neg, \lor, \land, \F, \always, \X, \until$. Other operators can be derived from $\textbf{true},\neg, \lor, \X$ and $\until$ as:
% \begin{itemize}
%     \item $\textbf{false} \myeq \neg \textbf{true}$;
%     \item $\psi_1 \land \psi_2 \myeq \neg(\neg \psi_1 \lor \neg \psi_2)$;
%     \item $\F \psi \myeq \textbf{true } \until \text{ } \psi$;
%     \item $\always \psi \myeq \neg \F \neg \psi$.
% \end{itemize}     
% where $\psi,\psi_1,\psi_2$ are LTL formulas. An LTL property can be translated into \textbf{NRW}, \textbf{LDBW} and \textbf{SLDBW}. If an $\omega$-regular word $w$ satisfies an LTL formula $\psi$, we denote it by writing $w \models \psi$. Semantics of LTL is defined inductively \cite{Baier08}.

For a given CTMDP $\Mdp$, one can compute the optimal schedule for the discounted-sum objective or the expected average by using policy iteration, value iteration or linear programming \cite{feinberg-MDP, puterman2014markov} on the uniformized CTMDP $\unif$. 
% For $\omega$-regular objectives, when the CTMDP is completely known, we can find an accepting end-component, and get an optimal schedule that maximises the probability of reaching the said component. These methods however cannot be executed if we do not have information on the transition/reward structure of the CTMDP, and on the rates corresponding to the state-action pairs.
When the CTMDP is unknown (unknown rates and states), an optimal schedule can be computed via reinforcement learning.

\paragraph{Reinforcement Learning (RL).}
 RL allows us to obtain an optimal schedule by repeatedly interacting with the environment and thereby observing a reward. A \emph{training episode} is a finite sequence of states, actions and rewards which terminates on certain specified conditions like when the number of samples drawn is greater than some threshold. The RL obtains information about rates and rewards of the CTMDP model by running several training episodes. Broadly, there are two categories of RL, model-based and model-free. We focus on space efficient model-free RL algorithms as they compute optimal schedule without constructing the state transition system  \cite{strehl2006pac}.
%  Model-free RL do not explicitly estimate the transition rates, transition probabilities and rewards. 
 
 One of the most successful model-free learning algorithm for DTMDPs is the Q-learning algorithm~\cite{wd92}. It aims at learning (near) optimal schedules in a (partially unknown) MDP for the discounted sum objective.  Bradtke and Duff~\cite{BD94} introduced the Q-learning algorithm for CTMDPs. We give here a brief description of Q-learning algorithm for CTMDPs.

For a given discount parameter $\alpha {>} 0$, the one-step expected discounted reward for an action $a$ from state $s$ is given by 
$\rho(s,a) = rew(s,a) {+} \frac{rew(s)}{\alpha+\lambda(s,a)}$~\cite[Eq. 11. 5. 3]{puterman2014markov}.
The Q-function for a state $s$ and an action $a$ under schedule $\sigma$, denoted $\valueq(s,a)$, is defined as
\[
 \rho(s,a) + \frac{\lambda(s,a)}{\lambda(s,a)+\alpha} \sum_{s' \in \states} \transP(s,a,s') \cdot  \valueq(s',\sigma(s')).
\]
% Equation for the expected discounted reward for a schedule $\sigma$ from state $s$ can be reduced to the following equation~\cite[Eq 11. 5. 4]{puterman2014markov}:

% \track{The discounted reward $\discobjective^{\ctmc} (\alpha)(s)$~\cite[Eq 11.5.4]{puterman2014markov} for a pure schedule $\sigma$ from $s\in\states$, can be reduced to the following equation:
% \begin{equation*}
%     \begin{split}
% \rho(s,a_{\sigma}) + \frac{\lambda(s,a_{\sigma})}{\lambda(s,a_{\sigma})+\alpha} \sum_{s' \in s} P(s,a_{\sigma},s') \cdot \discobjective_{\sigma}^{\Mdp}(\alpha)(s') ,
%     \end{split}
% \end{equation*}
% where $a_\sigma = \sigma(s, a)$ is the action taken under the schedule $\sigma$ from state $s$ and $P(s,a_{\sigma},s')$ is the probability of taking a transition from state $s$ to $s'$ on action $a_{\sigma}$.
% Here the term $\frac{\lambda(s,a_{\sigma})}{\lambda(s,a_{\sigma})+\alpha}$ is derived by applying the continuous time discounting to the expected time delay in state $s$.} 

% \track{These equations motivate the definition of the $Q$-values.}\todo{The part in red can be removed.}
It gives the total expected discounted reward obtained by taking action $a$ from $s$, and following $\sigma$ afterwards. 
The optimal Q-function, denoted $\mathcal{Q}^*$ is given by,
 \[
 \rho(s,a) + \frac{\lambda(s,a)}{\lambda(s,a)+\alpha} \sum_{s'\in\states}\transP(s,a,s') \cdot \max_{a'\in\actions} \mathcal{Q}^{*}(s',\alpha').
 \]
 Q-learning uses stochastic approximation~\cite{Sutton18} to estimate the $\mathcal{Q}^*$ function. When a transition from state $s$ to $s'$ on an action $a$ with delay $\tau$ is observed, the $\Qf$ estimates are updated as~\cite[Eq 12]{BD94}: 
%  following Q-learning rule for CTMDPs is obtained,
%  \begin{equation*}
%     \begin{split}
%         \newvaluef(s,a) = \kvaluef(s,a) + \rho_{k}[ \frac{1-e^{-(\lambda(s,a) + \alpha)\cdot \tau}}{\lambda(s,a)}r_{s}(s,a,s') + \\
%         e^{-\alpha\tau} \max_{a'} \kvaluef(s',a') - \kvaluef(s,a)]
%     \end{split}
% \end{equation*}
% \newvaluef(s,a) = \kvaluef(s,a) + \rho_{k}[ \frac{1-e^{-(\lambda(s,a) + \alpha)\cdot \tau}}{\lambda(s,a)}r_{s}(s,a,s') + \\
%         e^{-\alpha\tau} \max_{a'} \kvaluef(s',a') - \kvaluef(s,a)]
% 
 \begin{multline*}
        \newvaluef(s,a) := (1-\beta_k)\kvaluef(s,a) + \\ \beta_{k} \Big( r(s,a,s') +
        e^{-\alpha\tau} \max_{a'} \kvaluef(s',a') \Big),
\end{multline*}
where $r(s,a,s')$ is the sampled reward from state $s$ to $s'$, the sampled transition time is $\tau$, and $\beta_{k}$ is the learning rate. 
% The Q-function is updated with respect to the learning rate, i.e, the extent to which the updation occur with respect to the sampled value (second term in the equation) is determined by how large the learning rate is.
% The RL algorithm samples through states and updates the Q-function iteratively.
% An episode is a finite sequence of states, actions and rewards which terminates on a certain specified condition like user defined episode length.
 The RL algorithm samples through states and updates the Q-function iteratively.
 While sampling, the agent picks the action based on an RL schedule.
% The RL algorithm conducts a number of episodes and updates the Q-function iteratively. 
% The action is chosen according to an  RL policy. 
% There are many different effective RL policies for getting accurate Q-values \cite{watkins1989learning}\cite{auer2002using}.
The optimal schedule is generated after completion of some number of episodes by taking the action that gives the highest Q-value from each state.

We focus on how to \emph{automatically obtain reward mechanisms for $\omega$-regular objectives for CTMDPs} so that off-the-shelf RL algorithms can learn an optimal schedule.
%  \input{automata_drawing}