%File: anonymous-submission-latex-2023.tex
\documentclass[letterpaper]{article}
% \usepackage[submission]{aaai23}  % DO NOT CHANGE THIS
\usepackage{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathrsfs}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
% \usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{comment}
\usepackage{xparse}
% \usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{enumerate}

\newcommand{\toolname}{\textsc{Mungojerrie} }

\usepackage{xparse}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{enumerate}
% \renewcommand\labelitemi{--}

\newtheorem{theorem}{Theorem}[]
\newtheorem{example}{Example}[]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{problem}[theorem]{Problem}

\usepackage{paralist}
\usepackage{todonotes}
\input{macros}

\usetikzlibrary{automata,positioning,decorations.markings,arrows,intersections,%    
calc,shapes,fit}                                     
\colorlet{darkgreen}{green!40!black}                                                                   \colorlet{darkblue}{blue!60!black}                                                                     \colorlet{darkred}{red!50!black}                                                                       \colorlet{safecellcolor}{yellow!5}                                                                     \colorlet{goodcellcolor}{green!10}                                                                     \colorlet{badcellcolor}{blue!10}      

% \usetikzlibrary{automata, arrows.meta,arrows, positioning}

\tikzset{
    >=latex,node distance=2cm,on grid,auto, initial text=, 
    box state/.style={draw,rectangle,minimum size=8mm,rounded corners},
    prob state/.style={draw,very thick,shape=circle,darkblue,minimum size=3mm,inner sep=0mm},
    every loop/.style={shorten >=0pt},
    accepting state/.style={double distance=1.2pt, outer sep = 0.6pt+\pgflinewidth},
    accepting dot/.style={above=-2.7pt,circle,fill,darkgreen,inner sep=2pt,radius=1pt}, 
    loop above/.append style={every loop/.append style={out=120, in=60, looseness=6}},
    loop below/.append style={every loop/.append style={out=300, in=240, looseness=6}},
    loop left/.append style={every loop/.append style={out=210, in=150, looseness=6}},
    loop right/.append style={every loop/.append style={out=30, in=330, looseness=6}} 
}  

% % Number the lines for submission
% \usepackage[mathlines]{lineno}
% \setlength\linenumbersep{0.5cm}
% \renewcommand\thelinenumber{\color{red!80!black}\arabic{linenumber}~}
% \let\oldqed\qed
% \renewcommand\qed{\mbox{}\hfill$\oldqed$}
% \newcommand*\patchAmsMathEnvironmentForLineno[1]{%
%   \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
%   \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
%   \renewenvironment{#1}%
%   {\linenomath\csname old#1\endcsname}%
%   {\csname oldend#1\endcsname\endlinenomath}}% 
% \newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
%   \patchAmsMathEnvironmentForLineno{#1}%
%   \patchAmsMathEnvironmentForLineno{#1*}}%
% \AtBeginDocument{%
%   \patchBothAmsMathEnvironmentsForLineno{equation}%
%   \patchBothAmsMathEnvironmentsForLineno{align}%
%   \patchBothAmsMathEnvironmentsForLineno{flalign}%
%   \patchBothAmsMathEnvironmentsForLineno{alignat}%
%   \patchBothAmsMathEnvironmentsForLineno{gather}%
%   \patchBothAmsMathEnvironmentsForLineno{multline}%
% }
% \linenumbers
% remove the above for final version


\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{temporal}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Reinforcement Learning for Omega-Regular Specifications \\on Continuous-Time MDP}
\author{
    Amin Falah\textsuperscript{\rm 1},
    Shibashis Guha\textsuperscript{\rm 2},
    Ashutosh Trivedi\textsuperscript{\rm 1}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1} University of Colorado Boulder\\
    \textsuperscript{\rm 2} Tata Institute of Fundamental Research
    % \textsuperscript{\rm 3} CU Boulder
}


% \pagestyle{plain}

\begin{document}
    \maketitle

\begin{abstract}
    Continuous-time Markov decision processes (CTMDPs) are canonical models to express sequential decision-making under dense-time and stochastic environments. 
    When the stochastic evolution of the environment is only available via sampling, model-free reinforcement learning (RL) is the algorithm-of-choice to compute optimal decision sequence. 
    RL, on the other hand, requires the learning objective to be encoded as scalar reward signals.
    Since doing such translations manually is both tedious and error-prone, a number of techniques have been proposed to translate high-level objectives (expressed in logic or automata formalism) to scalar rewards for discrete-time Markov decision processes (MDPs). Unfortunately, no automatic translation exists for CTMDPs.
    
    We consider CTMDP environments against the learning objectives expressed as omega-regular languages. 
    Omega-regular languages generalize regular languages to infinite-horizon specifications and can express properties given in popular linear-time logic LTL.
    To accommodate the dense-time nature of CTMDPs, we consider two
    different semantics of omega-regular objectives: 
    1) \emph{satisfaction semantics} where the goal of the learner is to
    maximize the probability of spending positive time in the good states, and
    2) \emph{expectation semantics} where the goal of the learner is to optimize
    the long-run expected average time spent in the ``good states" of the automaton.
    We present an approach enabling correct translation to scalar reward signals
    that can be readily used by off-the-shelf RL algorithms for CTMDPs. 
    We demonstrate the effectiveness of the proposed algorithms by evaluating it on some popular CTMDP benchmarks with
    omega-regular objectives. 
\end{abstract}
% \maketitle

\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Preliminaries}
\label{prelims}
\label{sec:prelims}
\input{prelims}

\section{Problem Statement}
\label{sec:p_statement}
\input{p_statement}


\section{RL for Satisfaction Semantics}
\label{sec:theorems&algo}
\input{theorems_algo}

\section{RL for Expectation Semantics}
\label{sec:d_time}
\input{d_time}

% \section{Reinforcement Learning of CTMDP}
% \label{sec:RL_methods}
% \input{RL_methods}

\section{Experimental Evaluation}
\label{sec:expt}
\input{expt}

% {\color{blue} Perhaps add a discussion talking about similarities with CSL here.}

% {\color{red} Need more related work, including the following:
% \begin{itemize}
%     \item  RL in real-time context
%     \item work on CTMDPs
%     \item Work on omega-regular semantics
%     \item Work on logics like LTL, DC, STL, MTL, and CSL
%     \item Other models for timed systems like timed automata..
%     \item 
%     Summary of work on omega-regular RL
%     \item 
%     semi-markov decision process reinfrocement learning 
%     \item 
%     Timed automata, probabilsitic timed automata, stochastic timed automata 
% \end{itemize}
% }


\section{Conclusion}
\label{sec:conc}
Continuous-time MDPs are canonical models to express nondeterministic and stochastic behavior under dense-time semantics.
Reinforcement learning (RL) provides a sampling-based method to compute an optimal schedule in the absence of an explicit environment model. 
The RL approach for CTMDPs has recently received considerable attention~\cite{GZ16,RS13}. 
Our work enabled the specification of learning objectives in CTMDPs as $\omega$-regular specifications.
To accommodate temporal modelling, we consider two semantics of $\omega$-regular specifications (that include LTL objectives) and provide translations to scalar reward forms amenable for model-free reinforcement learning.
We believe that this work will open doors to study and develop model-free reinforcement learning for continuous-time models that go beyond CTMDPs and allow temporal constraints on planner's choices and residence-time requirements. 

% A natural next step is to investigate the CTMDP-like environments where the controller can select timed actions in the environment. These models will combine the timed automata~\cite{AD94} based modelling with clock variables to continuous distributions of CTMDPs.
% Another important direction is to improve the scalability of RL-driven scheduler synthesis to develop deep RL algorithms capable of exploiting symmetry inherent in dense-time environments.

\section*{Acknowledgement}
This work is partially supported by DST-SERB grant SRG/2021/000466 and by the National Science Foundation (NSF) grant CCF-2009022 and by NSF CAREER award CCF-2146563.

\bibliography{references}
\pagebreak

\onecolumn
\appendix
\begin{center}
    {\LARGE Appendix}
\end{center}
\input{appendix_new}
% \section{Example: Satisfaction Vs. Expectation Semantics}
% As a motivation for the expectation semantics, consider the situation where a good state signifies some desirable behavior (time spent in performing some critical aspect of the mission), then the satisfaction semantics
% require that such states are visited infinitely often, whereas the expectation semantics provide incentives to policies that maximize the expected
% time spent in such states.  
% Conversely, in some situations one may argue against the expectation semantics being inadequate as
% they incentivize expected behavior instead of good behavior of a greater measure of runs.  
% Depending upon the context, one of these semantics may be more apt than the other.
% Let us consider a mockup to explicate the utility of these semantics.
% \begin{example}[Satisfaction or Expectation?]
% The grid-world shown in Figure~\ref{fig:grid-world} (adapted from \cite{HahnPSSTW19}) represents four triangular zones (numbered $0$ to $3$) on the Mars surface. 
% Suppose that a mission to Mars arrives in Zone~$0$ (a known, safe territory) and is expected to explore the terrain in a safe fashion, gather and transmit information, and stay alive to maximize the return on the mission.
% For simplicity, assume that Zone $1$ (blue) models a crevasse harmful to the safe operations, while zones $2$ and $3$ are central to exploration mission and are analogous in their information contents.

% \input{automata_drawing}
% Given the unknown uncertainty of the terrain of Mars, the system is modeled as a CTMDP with associated uncertainty on the time of various actions where the exit rate of action $a$ from Zone (state) $0$ is denoted by $\lambda(0,a)$. In other words, when selected an action $a$ in a state $s$, the probability of spending $t$ time units in $s$ before taking $a$ is given by the cumulative distribution function $1 - e^{- \lambda(0,a)t}$.
% Assume that the action $b$ from Zone $0$ goes to Zone $2$ with rate $r$ (high probability) and to Zone $1$ with rate  $(\lambda(0,b) - r)$ (low probability).

% The mission objective is to avoid Zone $1$ (blue zone) while infinitely often visiting the Zone $2$ or $3$ (the green zones). 
% It can be captured in LTL~\cite{Baier08} as:
% \[
% \varphi = (\always \neg \mathtt{b}) \wedge (\always(\eventually \mathtt{g}))
% \]
% specifying that across the infinite horizon {\bf globally} (i.e. at every step expressed as temporal modality, $\always$) avoid the blue region ($\neg b$), and globally {\bf finally} (i.e. at some time in the future expressed as temporal modality, $\eventually$) reach the green region, i.e. $(\always (\eventually g))$. 
% The $GF \phi$ modality is often referred as \emph{infinitely often $\phi$}.
% LTL combines these temporal operators using the standard propositional logic connectives such as: and ($\wedge$), or ($\lor$), not ($\neg$), and implication ($\to$).

% This declarative specification can also be expressed using the B\"uchi automaton shown in Figure~\ref{fig:grid-world} (center) where the double circled states (here, $q_1$) denote accepting states.
% The B\"uchi automata can be used as the monitors of the behaviors of the learner over the environments given as CTMDPs. For our example, it is visualized by taking the synchronous product (an extended space CTMDP) of the CTMDP with the automaton shown in Figure~\ref{fig:grid-world} (right).

% For the satisfaction semantics on the product CTMDP, our goal is to maximize the probability that every infinite horizon behavior visits the accepting state infinitely often, while for the expectation semantics the goal is to maximize the expected time the system dwells in the final state.
% \begin{itemize}
%     \item \noindent {\bf Satisfaction Objective.} 
% Consider the case where we have one Mars rover in this mission. Hence, our goal naturally is to maximize the probability of visiting green zones infinitely often while avoiding the blue zone (the satisfaction semantics).
% In this case, the optimal schedule is to choose actions $a$ and $c$ indefinitely, i.e. the schedule $(a \to c)^\omega$, that satisfies the objective with probability $1$.
% Note that it does not make sense to choose action $b$ no matter how low the probability is to reach the blue zone.
% \item 
% \noindent {\bf Expectation Objective.} Consider an alternative setting where we have a fleet of drones (we are okay is losing some drones as long as we maximize the mission objective) that needs to be sent to the surveillance of zone $2$ or $3$. 
% Suppose that due to unforeseeable circumstances the mission may cease operation any time, and hence the goal is to maximize total expected time spent in the green zones ($2$ and $3$).
% The schedule $(a\to c)^\omega$ is not optimal anymore as it may dwell a considerable amount in the Zone $0$. 
% On the hand hand any drone that chooses $b$ in Zone $0$ risks moving to Zone $1$ with a small probability.
% As our goal is to maximize the expected time over a large group of drones, the expectation semantics captures this intent and the optimal schedule is to start with action $b$.
% \end{itemize}
% \end{example}

% \section{Good-for-CTMDP B\"uchi Automata}
% \label{app:gfm}
% \paragraph{Product CTMDP.}
%  Given a \emph{labelled} CTMDP $(\Mdp, \atomicprop, \labelling)$ where $\atomicprop$ is a set of atomic propositions, and $\labelling: \states \rightarrow 2^\atomicprop$ is a labelling function and a   B{\"u}chi automaton $\oautomata = (2^{\atomicprop}, \ostate, \oinitstate, \otrans, \acceptingc)$, the product CTMDP is defined as $\Mdp \times \oautomata = ((\states \times \ostate),(\initstate,\oinitstate),\actions,\transR^{\times},\acceptingc^{\times})$ where the rates are $\transR^{\times} : (\states \times \ostate) \times \actions \times (\states \times \ostate) \rightarrow \Reals_{\geq 0}$ such that $\transR^{\times} ((s,q),a,(s',q') = \transR(s,a,s')$ if $\transR(s,a,s')>0$ and $\otrans (q,\labelling(s)) = \{q'\}$.
% If $F$ is the set of accepting 
% % transitions 
% states
% in $\oautomata$, then the accepting condition is a set 
% % transitions $F^\times$ where $((s,q),a,(s',q')) \in F^\times$ iff $(q,\labelling(s),q') \in F$ 
% $F^\times$ of states where $(s,q) \in F^\times$ iff $q \in F$. An example of a product CTMDP is given in Figure~\ref{fig:grid-world}.  

% Given an MDP $\Mdp$, a B\"uchi automaton  $\oautomata$, and product $\Mdp \times \oautomata$, we define the following two  problems:
% \begin{enumerate}
%     \item {\bf Satisfaction Semantics.} 
%     Compute a schedule of $\Mdp$ that maximizes the probability of visiting final states $F$ of $\oautomata$ infinitely often.
%     We define the satisfaction probability $\PSat^{\Mdp \times \oautomata}(s, \sigma)$ of a schedule $\sigma$ from starting state $s$ as: 
% \begin{equation*}
% \Pr{}^{\Mdp \times \oautomata}_{\sigma}(s) \set{\forall_i  \exists_{j{\geq} i} [X_j \in F^\times] }.
% \end{equation*}
% The optimal satisfaction probability
% $\PSemSat^{\Mdp \times \oautomata}_{\oautomata}(s)$ for specification $\oautomata$ 
% is defined as $\sup_{\sigma \in \Sigma_{\Mdp \times \oautomata}} \Pr^{\Mdp \times \oautomata}_{\sigma}(s, \sigma)$ and we say
% that $\sigma$ is an optimal schedule for $\oautomata$ if
% $\PSemSat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma) (s) = \PSemSat^{\Mdp \times \oautomata}_{\oautomata}$.

% \item {\bf Expectation Semantics.} Compute a schedule of $\Mdp$ that maximize the long-run expected average time spent in the final states of $\oautomata$.
% We define the expected satisfaction time $\ESat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma)$ 
% of $\sigma$ from starting state $s$ as: 
% \begin{equation*}
%  \mathbb{E}^{\Mdp \times \oautomata}_{\sigma}(s) \set{ \liminf_{n \rightarrow \infty} \frac{\sum_{i=1}^{n} [X_i {\in} F] D_i}{T_n}}.
% \end{equation*}
% The optimal expected satisfaction time
% $\ESat^{\Mdp \times \oautomata}_{\oautomata}(s)$ for $\oautomata$ is defined as $\sup_{\sigma \in \Sigma_{\Mdp}} \ESat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma)$ and we say that $\sigma \in \Sigma_\Mdp$ is an optimal expected-satisfaction schedule for $\oautomata$ if
% $\ESat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma) = \ESat^{\Mdp \times \oautomata}_{\oautomata} (s)$.
% \end{enumerate}

% We call a B{\"u}chi automaton $\oautomata = (2^{\atomicprop}, \ostate, \oinitstate, \otrans, \acceptingc)$ good-for-CTMDP if for every \emph{labelled} CTMDP $(\Mdp, \atomicprop, \labelling)$ where $\atomicprop$ is a set of atomic propositions, we have that 
% \begin{eqnarray*}
%     \PSemSat^{\Mdp}_{\oautomata}(s, \sigma) &=&\PSat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma)~\text{ and }\\
%     \ESemSat{}^{\Mdp}_{\oautomata}(s, \sigma) &=&
%     \ESat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma).
% \end{eqnarray*}
% A good-for-CTMDP automaton allows the computation of the optimal schedule by solving the corresponding problem on the product CTMDP. 
% If a B\"uchi automata is good-for-MDP, then one can show via uniformization that it is also good-for-CTMDPs.
% There exists several syntactic characterizations of good-for-MDP automata including suitable limit-deterministic B\"uchi automata (SLDBA)~\cite{sickert2016limit} and slim automata~\cite{HPSS20}.
% Moreover, every LTL specification can be effectively converted into a GFM B\"uchi automata.

% % \begin{theorem}[Good-for-MDP B\"uchi Automata]
% % \end{theorem}

% \section{Blackwell Optimality In CTMDP}
% \label{sec:blackwell_optimality}
% \input{blackwell_optimality}


% % \begin{center}
% %     {\Large Appendix}
% % \end{center}
% % \section{Appendix}
% % \input{appendix}

% \section{Algorithms}

% \begin{algorithm}[t]
% \caption{Algorithm for satisfaction semantics}\label{algo:sat}
% \hspace*{\algorithmicindent}\hspace{2mm}\textbf{Input:}  \text{Initial state $s_{0}$, GFM $A$, discount factor $\gamma$,}\\
% \hspace*{\algorithmicindent} \hspace{12mm}\text{reward function $rew$, number of episodes $k$,} \\
% \hspace*{\algorithmicindent}\hspace{12mm} \text{learning rate $\beta$}\\
% \hspace*{\algorithmicindent}\textbf{Output:}\text{ Schedule $\sigma$ converging to an optimal one}
% \begin{algorithmic}[1]
% \STATE Initialise $Q_f$ to all zeroes
% % \STATE $i \leftarrow 0$
% \FOR{k episodes}
% \STATE Initialise $s$ and $q$ to $s_0$ and $q_0$ respectively
% \STATE Initialise $r$ to 0
% % \STATE $q \leftarrow q_0$
% % \STATE $r \leftarrow 0$
% \WHILE{r = 0}
% \STATE Choose action $a$ using policy derived from $Q$
% \STATE Take action $a$, observe next state $s'$ and time $\tau$
% \STATE Choose non-deterministic transition $t$ in $A$ using the derived policy
% \STATE Take transition $t$ in $A$, observe next state $q'$
% \STATE $r \leftarrow rew(s,q,a)$
% \STATE $V(s',q') \leftarrow \max\limits_{a' \in \actions} Q_f(s',q',a')$
% \STATE $Q_f(s,q,a) {\leftarrow} (1-\beta) Q_f(s,q,a)  {+}\beta \bigl(r {+} e^{-\gamma\tau} V(s',q') \bigr)$
% \STATE $s \leftarrow s'$; $q \leftarrow q'$
% \ENDWHILE
% % \STATE $i \leftarrow i+1$
% \ENDFOR
% \STATE Initialise $\sigma$
% \FOR{Each state $(s,q)$}
% \STATE $\sigma(s,q) = \max\limits_{a \in \actions}Q(s,q,a)$
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}


% \begin{algorithm}[t]
% \caption{Algorithm for expectation semantics}\label{algo:expt}
% \hspace*{\algorithmicindent}\hspace{2mm}\textbf{Input:}  \text{Initial state $s_0$, GFM $A$, discount factor $\gamma$,}\\
% \hspace*{\algorithmicindent} \hspace{12mm}\text{reward function $rew$, number of episodes $k$,} \\
% \hspace*{\algorithmicindent}\hspace{12mm} \text{learning rate $\beta$, episode length $eplen$}\\
% \hspace*{\algorithmicindent}\textbf{Output:}\text{ Schedule $\sigma$ converging to an optimal one}
% % \hspace*{\algorithmicindent} \textbf{Output:}  \text{Schedule $\sigma$ converging to an optimal one}
% \begin{algorithmic}[1]
% \STATE Initialise $Q_f$ to all zeroes
% % \STATE $i \leftarrow 0$
% \FOR{$k$ episodes}
% \STATE Initialise $s$ and $q$ to $s_0$ and $q_0$ respectively
% % \STATE $q \leftarrow q_0$
% % \STATE $r \leftarrow 0$
% % \STATE $j \leftarrow 0$
% \FOR{each $eplen$-length episode}
% \STATE Choose action $a$ using policy derived from $Q$ 
% \STATE Take action $a$, observe next state $s'$ and time $\tau$
% \STATE Choose non-deterministic transition $t$ in $A$ using the derived policy
% \STATE Take transition $t$ in $A$ and observe the next state $q'$
% \STATE $r \leftarrow rew((s,q),a,\tau)$
% \STATE $V(s',q') \leftarrow \max\limits_{a' \in \actions} Q_f(s',q',a')$
% \STATE $Q_f(s,q,a) {\leftarrow} (1-\beta) Q_f(s,q,a)  {+}\beta \bigl(r {+} e^{-\gamma\tau} V(s',q') \bigr)$
% \STATE $s \leftarrow s'$; $q \leftarrow q'$
% % \STATE $q \leftarrow q'$
% % \STATE $j \leftarrow j+1$
% \ENDFOR
% % \STATE $i \leftarrow i+1$
% \ENDFOR
% \FOR{Each state (s,q)}
% \STATE $\sigma(s,q) = \max_{a \in \actions}Q(s,q,a)$
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

% % \input{algorithms}
% \section{Benchmarks}
% \label{app:bench}
% \begin{itemize} 
% \item 
% The {\bf Dynamic Power Management}\cite{dynpow2001} system models a service provider that processes tasks of $N$ different types. The tasks that are to be processed are stored in queues of maximum size $C$ for each type. The service provider can be in three different states: {\tt idle}, {\tt busy} and {\tt standby}. 
% The key parameters here are $N$, $C$ and the time bound. The properties that are checked are with respect to the probability of getting a queue full and the expected time to get the queues full.
% \item 
% The {\bf Fault Tolerant Workstation Cluster}\cite{ftwc2000} benchmark models two sets of workstations (left and right) which are connected by a backbone. 
% A workstation in a set can freely communicate with other workstations on its own set but requires the backbone to communicate with the other set. Each workstation is connected to the backbone via a switch and the size of each set is $N$. 
% The workstation, switch and the backbone can fail and may require repair. The repair unit can repair only one component at a time.
% The property we check is the minimum probability that all the left workstations are down within a time bound.
% The parameters here are $N$ and the time bound. 
% \item 
% The {\bf Polling System}\cite{polling2013} consists of $j$ stations and $1$ server. Here, the incoming requests of $j$ types are buffered in queues of size $k$ each, until they are processed by the server and delivered to their station. The system starts in a state with all the queues being nearly full. The parameters here are $j$, $k$ and a time bound. We consider two properties: (i) all the queues are empty and (ii) one of the queues is empty.
% \item 
% The {\bf Erlang Stages}\cite{erlang2010} benchmark models have two different paths to reach the goal state: a fast but risky path or a slow but sure path. The slow path is an Erlang chain of length $k$ and rate $r$. The objective is to check which path gives faster reachability to a safe target state in an expected sense.
% \item 
% The {\bf Stochastic Job Scheduling} benchmark models a multi-processor system with $k$ processors and $n$ jobs which need to be serviced. 
% The service times are exponentially distributed.
% The jobs can be scheduled pre-emptively and multiple processors can be used to service a single job. The parameters here are $n$ and $k$. The objective is to schedule the jobs efficiently, that is, to minimise the expected time required to service all the jobs. 
% \end{itemize}



\end{document}
