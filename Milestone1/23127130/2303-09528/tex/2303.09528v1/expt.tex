
% \begin{table*}[t!]
%   \caption{Q-learning results.  The default values of the learner
%     hyperparameters are: $\zeta = 0.99$, $\epsilon=0.1$, $\alpha=0.1$,
%     tol$=0.01$, ep-l$=30$, and ep-n$=20000$.  Times are in seconds.}
%   \label{tab:experiment}
%   \centering
%   %\small
%   \begin{tabular}[c]{l|cccccccccccccc}
% Name & states & prod. & Prob. & Est. Prob. & Time 1 & Est-Avg & Time 2\\\hline
% \texttt{DynamicPM-tt\_3\_qs\_2}  & 816 & 825 & 1 & 1    & 2.73944 & 1.95225 & 2.09464\\
% \texttt{ErlangStages-k500\_r10}  & 508 & 509 & 1 & 1    & 2.73944 & 5.786 & 1.12393 \\
% \texttt{ErlangStages-k2000\_r10}  & 2008 & 2009 & 1 & 1    & 2.73944 & 5.78565 &1.22119 \\
% \texttt{PollingSystem-jt1\_qs1}  & 16 & 20 & 1     & 1    & 78.00 &18.8849 & 1.12183\\
% \texttt{PollingSystem-jt1\_qs4}  & 348 & 352 & 1     & 1    & 78.00 & 0.3302 &1.3917\\
% \texttt{PollingSystem-jt1\_qs7}  & 1002 & 1006 & 1     & 1    & 1.72974 & 0.0004 &1.6969 \\
% \texttt{QS-lqs\_1\_rqs\_1\_jt\_2}  & 266 & 282 & 1     & 1    & 1.72974 & 2.38395 &1.28006 \\
%  \texttt{QS-lqs\_1\_rqs\_1\_jt\_5}  & 3977 & 4152 & 1     & 1    &  1.98968 & 0.3981 & 8.51865\\
% \texttt{QS-lqs\_2\_rqs\_2\_jt\_3}  & 11045 & 24672 & 1     & 1 & & 0.49755 & 29.527 \\
% \texttt{SJS-procn\_2\_jobn\_2}  & 17 & 21 & 1     & 1    & 4.05513 & 26.4677 &1.09653\\
% \texttt{SJS-procn\_2\_jobn\_6}  & 7393 & 7405 & 1     & 1    & 6.39278 &9.1482&3.65823\\
% \texttt{ftwc\_001\_mrmc}  & 82 & 122 & 0.999774 & 0.999779    &  19.8628 &0.00075 &19.4509
%   \end{tabular}
% \end{table*}
% \input{table}
% {\color{blue}{Todo: Ashutosh to add info about the tool.}}
We implemented the reward schemes described in the previous sections in a C++-based tool \textsc{Mungojerrie}~\cite{hahn2021mungojerrie} 
which reads CTMDPs described in the PRISM language \cite{kwiatk11} and $\omega$-regular automata written in the \emph{Hanoi Omega Automata} format \cite{Babiak15}. 
Our implementation provides an Openai-gym~\cite{Brockm16} style interface for RL algorithms and supports probabilistic model checking for CTMDPs based on uniformization. 

Table~\ref{tab:experiment} shows the results of the evaluation of our algorithms on a set of CTMDP benchmarks from the Quantitative Verification Benchmark set ({\tt https://qcomp.org}).
% A brief description of these benchmarks is given below. 
\texttt{RiskReward} is based on Example~\ref{example:1} with $\lambda(0,b) = 10$ and $r = 9$.
\texttt{DynamicPM-tt\_3\_qs\_2}  models encode dynamic power management problem based on~\cite{DPM00}.
Queuing System (QS) models \texttt{QS-lqs\_i\_rqs\_j\_jt\_k} are based on a CTMDP modelling of queuing systems with arrival rate $i$, service rate $j$, and jump rate $k$ as the key parameters.
\texttt{ftwc\_001\_mrmc} models consist of two networks of $n$ workstations each where each network is interconnected by a switch communicating via a backbone. The components may fail arbitrarily, but can only be repaired one at a time. The initial state is the one where all components are functioning, and the goal state is the one where in both networks either all the workstations or all the switches are broken.
The Polling System examples \texttt{PollingSystem-jt1\_qsj} consist of $j$ stations and $1$ server. Here, the incoming requests of $j$ types are buffered in queues of size $k$ each, until they are processed by the server and delivered to their station. The system starts in a state with all the queues being nearly full. We consider 2 goal conditions: (i) all the queues are empty and (ii) one of the queues is empty.
Finally, the stochastic job scheduling (SJS) examples \texttt{SJS-procn\_i\_jobn\_j} model multiple processors ($i$) with a sequence of independent jobs ($j$) with a goal job completion.


The results are summarized in Table~\ref{tab:experiment}.  
For each model, we provide the number of states in the CTMDP ({\bf states}) and in the product CTMDP ({\bf prod}), the probability of satisfaction ({\bf Sat. Prob.})) of the objective for the satisfaction semantics, estimated probability for the satisfaction semantics ({\bf Est. Sat.}) by the RL algorithm, and time ({\bf Time 1}) spent in learning that schedule. 
The probability of satisfaction for the expectation semantics ({\bf Exp. Prob.}), estimated probability by the RL algorithm ({\bf Est. Exp.}), and the learning time ({\bf Time 2}) for the expectation semantics are provided next. All of our timings and values are averaged over three runs with randomly chosen seeds.
We kept the default values for the hyperparameters as shown in 
Table~\ref{tab:experiment}.

% The results are summarized in Table~\ref{tab:experiment}.  
% For each model, we provide the number of states in the CTMDP ({\bf states}) and in the product CTMDP ({\bf prod}), the probability of satisfaction ({\bf Sat. Prob.})) of the objective for the satisfaction semantics, estimated probability ({\bf Est. Prob.}) by the RL algorithm, and time ({\bf Time 1}) spent in learning that schedule. 
% The estimated expected average ({\bf Est. Avg.}) and the learning time ({\bf Time 2}) for the expectation semantics are provided next. All of our timings and values are averaged over three runs with randomly chosen seeds.
% We kept the default values for the hyperparameters as shown in 
% Table~\ref{tab:experiment}.
% It should be noted that the models used for testing are designed for checking reachability property and therefore the probability of satisfaction for both the semantics are the same here. The first benchmark in the table (riskReward) represents Example~\ref{example:1} where $\lambda(0,b) = 10$ and $r = 9$. It can be seen that the probability of satisfaction of the objective for satisfaction semantics gives $1$ (picking action $a$) while for expectation semantics, the probability of satisfaction is $0.9$ (picking action $b$).

Our experimental results demonstrate that the proposed RL algorithms are effective in handling medium sized CTMDPs. 
Since for the expectation semantics, the optimal probability was computed using linear programming, we can notice that the RL algorithm efficiently estimates the optimal probability and computes the optimal schedule.


% Results 

