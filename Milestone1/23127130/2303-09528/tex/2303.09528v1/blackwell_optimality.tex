% \subsection{Blackwell Optimality In CTMDP}
% \track{Though both of our semantics use different reward machines, for learning schedules for both objectives, we reduce the problem to maximising the expected average reward.
% We use off-the-shelf RL algorithms for learning the schedulers in both settings.}
% Standard RL algorithms try to optimise discounted payoff objective while 
% % as shown above, for the expectation semantics, 
% we need to optimise the expected average reward. Blackwell optimality allows us to use a schedule that optimises the expected discounted payoff with a high discount factor and such a schedule also optimises the expected average payoff.

% \vspace{0.5em}\noindent\textbf{Existence of Blackwell Optimal Schedules in CTMDP.} 
% We give a simple uniformization based proof on the existence of a Blackwell optimal pure schedule in a CTMDP.
In this section, we provide a uniformization based proof of Theorem~\ref{corollary:1}.
Consider a CTMDP $\Mdp$, a pure schedule $\sigma$, and a continuous-time discounting with parameter $\dfactor > 0$. 
% The one-step expected reward obtained from taking action $a \in \actions{}(s)$ from state $s \in \states$ is given by $\rho(s,a) = rew(s,a) + \frac{rew(s)}{\dfactor + \lambda(s,a)}$ (~$\!\!$\cite{puterman2014markov}~Eq 11.5.3). 
We define a function $\ctmdprate : \states \times \actions \rightarrow [0,1) $ where $\ctmdprate(s,a) = \frac{\lambda(s,a)}{\lambda(s,a) + \dfactor}$ where $\dfactor > 0$.
We call $\ctmdprate(s,a)$ the \emph{discount rate} of the state-action pair $(s,a)$ in $\Mdp$.
So, the expected discounted reward (also known as value of $s$) $\discobjective^{\Mdp[\sigma]} (\alpha)(s)$ is given by
% For a pure schedule $\sigma$, the value of a state $s$, denoted by $\valuesigma_{\ctmdprate(s,\sigma(s))}$,
% \track{CHANGE NOTATION} 
% is given by 
\begin{equation}\label{eq:7.1}
\begin{split}
     \rho(s,a_{\sigma}) + 
     \ctmdprate(s,a_{\sigma})
     \sum_{s' \in s} \pmtrx(s,a_\sigma,s') 
     \discobjective_{\sigma}^{\Mdp}(\alpha)(s')
\end{split}
\end{equation}
Consider a DTMDP $\mathcal{N}$, a schedule $\sigma$, and a discount rate $0\leq \drate < 1$. Let $\valuesigma_{\drate}(\mathcal{N},s)$ denote the total discounted value from state $s$ in $\mathcal{N}$ under schedule $\sigma$.

A pure schedule $\bschedule$ is \emph{Blackwell optimal} in $\mathcal{N}$ if there exists a threshold discount rate $0 \leq \thrate < 1$ such that for any discount rate $\thrate \leq \drate < 1$, we have $\valuestar_{\drate}(\mathcal{N},s) \geq \valuesigma_{\drate}(\mathcal{N},s)$ for all $\sigma \in \Sigma_{\mathcal{N}}$. It is known that a Blackwell optimal schedule maximises both discounted and average reward objectives in DTMDPs.
% From \cite{puterman2014markov}~Thm 10.1.4, we have,
% \begin{theorem}\label{puterman1}
From \cite{puterman2014markov}~(Thm 10.1.4), we have that for every DTMDP $\mathcal{N}$, there exists a Blackwell optimal \emph{pure} schedule $\bschedule$, and $\bschedule$ also maximises the average reward in $\mathcal{N}$.
% \end{theorem}
Now, given a CTMDP $\Mdp$, let $C$ be a constant such that $C \geq \lambda(s,a)$ for all state-action pairs in $\Mdp$. 
Let $\uMdp$ be the uniformized CTMDP of $\Mdp$ with constant exit rate $C$, and let $\upmtrx$ be the probability matrix of $\uMdp$. As the exit rate $\lambda(s,a) = C$ for all $s\in \states$ and $a \in \actions{}(s)$, we have that $\ctmdprate(s,a) = \frac{C}{C+\dfactor}$ for all state-action pairs. We denote this discount rate by $\udrate$. 
The value of a state $s$ under a schedule $\sigma$ in $\uMdp$, 
% \track{under schedule $\sigma$}
denoted $\discobjective^{\uMdp^{\sigma}}(\alpha)(s)$
% $\valuesigma_{\udrate}(\uMdp,s)$ 
is given by, 
\begin{equation}\label{eq:7.2}
     \Bar{\rho}(s,a_\sigma) + \udrate \sum_{s' \in s} \upmtrx(s,a_\sigma,s') \discobjective^{\uMdp^{\sigma}}(\alpha)(s')
\end{equation}
where $\Bar{\rho}(s,a) = \rho(s,a)\cdot \frac{\dfactor + \lambda(s,a)}{\dfactor + C} $. 
We extend the above result of existence of Blackwell optimal schedules in DTMDPs to uniform CTMDPs.
\begin{lemma} \label{lemma:1}
For a uniform CTMDP $\uMdp$, there exists a Blackwell optimal schedule $\bschedule$. Further, $\bschedule$ also maximises the expected average reward in $\uMdp$.
\end{lemma}
\begin{proof}
Consider a DTMDP $\mathcal{N}$ with the same set of states as that of $\uMdp$, one step reward function $\Bar{r}$ and probability matrix $P_{\mathcal{N}} = \upmtrx$. For a pure schedule $\sigma$ and a discount rate $0 \leq \drate < 1$, the value of a state $s$ in $\mathcal{N}$ is 
\begin{equation}\label{eq:7.3}
    \valuesigma_{\drate}(\mathcal{N},s) = \Bar{r}(s,\sigma(s)) + \drate \sum_{s' \in s} \upmtrx(s,\sigma(s),s') \valuesigma_{\drate}(\mathcal{N},s')
\end{equation}
We observe that equation \ref{eq:7.3} is identical to equation \ref{eq:7.2} when $\udrate = \drate$. Therefore, the set of equations defining the values of states in $\mathcal{N}$ and $\uMdp$ are identical. Let this set be denoted by $E^\sigma$. From \cite{puterman2014markov}~Thm 6.1.1, we know that for each stationary schedule $\sigma$, there exists a unique solution for $E^\sigma$. The set of pure schedules in $\uMdp$ and $\mathcal{N}$ are equal as the set $S$ of states and the set $\av$ of available actions from each state are the same for both.\\*
Therefore, for a pure schedule $\sigma$, and discount rates $0 \leq \drate = \udrate < 1$, we have
\begin{equation}\label{eq:7.4}
    \valuesigma_{\udrate}(\uMdp,s) = \valuesigma_{\drate}(\mathcal{N},s) \text{\: for $\udrate = \drate$, for all states $s$}
\end{equation}
From Theorem~10.1.4 in \cite{puterman2014markov}, we know that in $\mathcal{N}$, there exist a Blackwell optimal pure schedule $\sigma^*$, and a threshold discount rate $\thrate$ such that 
\begin{align}\label{eq:7.5}
 \valuestar_{\drate}(\mathcal{N},s) \geq \valuesigma_{\drate}(\mathcal{N},s)   
\end{align}
for all $\sigma \in \Sigma_{N}$ and $\thrate \leq \drate < 1$.\\
From equations \ref{eq:7.4} and \ref{eq:7.5} we can conclude that 
\begin{equation}\label{eq:7.60}
    \valuestar_{\udrate}(\uMdp,s) \geq \valuesigma_{\udrate}(\uMdp,s)   
\end{equation}
for all $\sigma \in \Sigma_{\uMdp}^{pure}$ and $\thrate \leq \udrate < 1$.\\
From \cite{puterman2014markov}~Thm 11.5.2(d), we know that there exists an optimal pure schedule maximising the discounted reward in a CTMDP. Therefore,
\begin{equation}\label{eq:7.}
    \valuestar_{\udrate}(\uMdp,s) \geq \valuesigma_{\udrate}(\uMdp,s)   
\end{equation}
for all $\sigma \in \Sigma_{\uMdp}$ and $\thrate \leq \udrate < 1$.\\
A similar argument can be made to show that $\sigma^*$ also maximises the expected average reward in $\uMdp$.
\end{proof}

The above lemma proves the existence of a Blackwell optimal schedule in uniform CTMDPs. 
We further extend this result to general CTMDPs which is the main result of this section.

\begin{lemma}\label{lemma:2}
If $\sigma^*$ is a Blackwell optimal pure schedule in $\uMdp$, then it is also Blackwell optimal in $\Mdp$.
\end{lemma}
\begin{proof}
Since $\bstrategy$ is a Blackwell optimal pure schedule in $\uMdp$, there exists a threshold discount rate $\uthrate$ such that for all $\uthrate \leq \udrate < 1$, we have that 
\begin{equation}\label{eq:7.7}
    \valuestar_{\udrate}(\uMdp,s) \geq \valuesigma_{\udrate}(\uMdp,s)   
\end{equation}
for all $\sigma \in \Sigma_{\uMdp}$.\\
The set of pure schedules in $\Mdp$ and $\uMdp$ are the same. From \cite{puterman2014markov}~Thm 11.5.2(d), we know that there exists an optimal pure schedule maximising the discounted reward in $\Mdp$. \cite{puterman2014markov}~Prop 11.5.1, states that for every pure schedule $\sigma$ and a state $s$, we have that  
\begin{equation}\label{eq:7.8}
    \valuesigma_{\ctmdprate(s,\sigma(s))}(\Mdp,s) = \valuesigma_{\udrate}(\uMdp,s)   
\end{equation}
If $\uthrate = \frac{C}{C+\dfactor}$ is the threshold discount rate in $\uMdp$, then the corresponding threshold discount rate for a state $s$ in $\Mdp$ is given by 
$\ctmdpthrate(s,a) = \frac{\lambda(s,a)}{\lambda(s,a)+\dfactor_{o}}$.\\
From equations \ref{eq:7.7} and \ref{eq:7.8}, we can conclude that for each state $s$ in $\Mdp$, there exist a pure schedule $\bstrategy$, and a threshold discount rate $\ctmdpthrate(s,\bstrategy(s)) $ such that for all $\ctmdpthrate(s,\bstrategy(s)) \leq \ctmdprate(s,\bstrategy(s)) < 1$, we have that
\begin{equation}\label{eq:7.9}
    \valuestar_{\ctmdprate(s,\bstrategy(s))}(\Mdp,s) \geq \valuesigma_{\ctmdprate(s,\sigma(s))}(\Mdp,s)   
\end{equation}
for all $\sigma \in \Sigma_{\Mdp}$.
As the set of states is finite in $\Mdp$, the threshold discount rate for $\Mdp$ is given by
    $\ctmdpthrate^{\Mdp} = \max_{(s,a) \in \states \times \actions} \ctmdpthrate(s,a)$.
\end{proof}
Thus, any Blackwell optimal schedule $\bstrategy$ in $\uMdp$ is also Blackwell optimal in $\Mdp$. The following lemmas show that $\sigma^*$ also maximises the expected average reward.
\begin{lemma}\label{lemma:3}
An optimal schedule maximising the expected average reward in $\uMdp$ also maximises the expected average reward in $\Mdp$.
\end{lemma}
\begin{proof}
For a pure schedule $\sigma$ , the expected average reward in $\Mdp$ is denoted by $\avgrew^{\sigma}(\Mdp)$. From \cite{puterman2014markov}~Chap 11.5.3, we observe that for a pure schedule $\sigma$, 
\begin{equation}\label{eq:7.10}
    \avgrew^{\sigma}(\Mdp) = \avgrew^{\sigma}(\uMdp)\cdot C 
\end{equation}
Let $\sigma'$ be a pure schedule maximising the expected average reward in $\uMdp$, i.e, 
\begin{equation}\label{eq:7.11}
 \avgrew^{\sigma'}(\uMdp) = \sup_{\sigma \in \Sigma_{\uMdp}} \avgrew^{\sigma}(\uMdp)
\end{equation}
The set of pure schedules in $\Mdp$ and $\uMdp$ are equal and we know that there exists a pure schedule maximising the average reward in a CTMDP (\cite{puterman2014markov}~Thm 11.4.6(d)). Therefore, from equations \ref{eq:7.10} and \ref{eq:7.11} we can conclude that, 
\begin{equation}\label{eq:7.12}
    \avgrew^{\sigma'}(\Mdp) = \sup_{\sigma \in \Sigma_{\Mdp}} \avgrew^{\sigma}(\Mdp)
\end{equation}
Therefore, $\sigma'$ is an optimal schedule maximising the average reward in $\Mdp$.
\end{proof}
\begin{lemma}\label{lemma:4}
A Blackwell optimal schedule $\bstrategy$ in $\Mdp$ also maximises the average reward in $\Mdp$.
\end{lemma}
\begin{proof}
From Lemma \ref{lemma:1}, we know that $\bstrategy$ is an optimal schedule maximising the expected average reward in $\uMdp$. Lemma \ref{lemma:3} shows that if $\bstrategy$ is an optimal schedule maximising the expected average reward in $\uMdp$ then $\bstrategy$ also maximises the expected average reward in $\Mdp$. Therefore, we can conclude that $\bstrategy$ is an optimal schedule maximising the expected average reward in $\Mdp$.
\end{proof}
Lemma~\ref{lemma:2} and Lemma~\ref{lemma:4} gives us the following.
\paragraph{Theorem~\ref{corollary:1}.}For a CTMDP $\Mdp$, there exists a Blackwell optimal pure schedule $\bschedule$ and a threshold $0 \leq \ctmdpthrate^{\Mdp} < 1$ such that :
\begin{inparaenum}[(1).]
    \item For any discount-rate function $\ctmdprate$ where $\ctmdprate(s,a) \geq \ctmdpthrate^{\Mdp}$ for all valid state-action pairs $(s,a)$, the schedule $\bschedule$ is an optimal schedule maximising the expected discounted reward.
    \item The schedule $\bschedule$ also maximises the expected average reward.
\end{inparaenum}
% \paragraph{Theorem ~\ref{corollary:1}}
% For a CTMDP $\Mdp$, there exists a Blackwell optimal pure schedule $\bschedule$ and a threshold $0 \leq \ctmdpthrate^{\Mdp} < 1$ such that :
% \begin{inparaenum}[(1).]
%     \item For any discount-rate function $\ctmdprate$ where $\ctmdprate(s,a) \geq \ctmdpthrate^{\Mdp}$ for all valid state-action pairs $(s,a)$, the schedule $\bschedule$ is an optimal schedule maximising the expected discounted reward.
%     \item The schedule $\bschedule$ also maximises the expected average reward.
% \end{inparaenum}

% \begin{proof}[Proof sketch]
% % \begin{lemma}\label{lemma:2}
% Note that the set of pure schedules are the same in $\Mdp$ and $\uMdp$.
% First we prove that if $\bschedule$ is a Blackwell optimal pure schedule in $\uMdp$, then it is also Blackwell optimal in $\Mdp$. 
% This is done using the fact that the value of each state in $\uMdp$ and $\Mdp$ are equal (Prop 11.5.1 in \cite{puterman2014markov}) i.e, for every pure schedule $\sigma$ and a state $s$, we have that
% \[
% \discobjective^{\ctmc} (\alpha)(s) = \discobjective^{\uMdp^{\sigma}}(\alpha)(s)
% \] 
% % \track{ELABORATE}
% From this equation, we can get the threshold discount rate satisfying the Blackwell optimality condition for $\Mdp$.
% % \end{lemma}
% Thus, we conclude that any Blackwell optimal schedule $\sigma^*$ in $\uMdp$ is also Blackwell optimal in $\Mdp$. 
% % The following lemmas show that $\sigma^*$ also maximises the expected average reward in $\Mdp$.

% Next we show that 
% % \begin{lemma}\label{lemma:3}
% an optimal schedule maximising the expected average reward in $\uMdp$ also maximises the expected average reward in $\Mdp$. 
% For a given pure schedule $\sigma$, we denote the expected average reward in $\Mdp$ and $\uMdp$ from a state $s$ by $\avgobjective^{\Mdp^{\sigma}}(s)$ and $\avgobjective^{\uMdp^{\sigma}}(s)$
% % \track{WE ALREADY HAVE NOTATIONS FOR EXPECTED AVERAGE REWARD} 
% respectively.
% From Section~11.5.3 in \cite{puterman2014markov}, we get the following result:
% \[
% \avgobjective^{\Mdp^{\sigma}}(s) = \avgobjective^{\uMdp^{\sigma}}(s)\cdot C \] 
% % This is due to the fact that 
% % the expected average reward obtained by a schedule in $\uMdp$ is directly proportional to the expected average obtained by the same schedule in $\Mdp$, . \track{ELABORATE}
% From this equation, we have that if a pure schedule maximises the expected average reward in $\uMdp$, it also maximises the expected average reward in $\Mdp$.
% % Therefore, optimal pure schedules maximising the expected average reward in $\Mdp$ and $\uMdp$ are the same. 
% % Now we prove the following lemma.

% In Lemma~\ref{lemma:1}, we have shown the existence of a pure Blackwell optimal schedule $\bschedule$ in $\uMdp$ and it also maximises the expected average reward. 
% From the above results we obtain that $\bschedule$ also is Blackwell optimal and maximises the expected average reward in $\Mdp$.
% % Finally, using Lemma~\ref{lemma:1}, we show that a Blackwell optimal schedule $\bschedule$ in $\Mdp$ also maximises the average reward in $\Mdp$. \track{ELABORATE}
% % We can get this result directly from Lemma~\ref{lemma:1}.
% % \end{lemma}
% % Lemma \ref{lemma:2} and Lemma \ref{lemma:4} give us the following.
% % \begin{corollary}\label{corollary:1}
% % For a CTMDP $\Mdp$, there exists a Blackwell optimal pure schedule $\bschedule$ and a threshold $0 \leq \ctmdpthrate^{\Mdp} < 1$ such that :
% % \begin{inparaenum}[(1).]
% %     \item For any discount-rate function $\ctmdprate$ where $\ctmdprate(s,a) \geq \ctmdpthrate^{\Mdp}$ for all valid state-action pairs $(s,a)$, the schedule $\bschedule$ is an optimal schedule maximising the expected discounted reward.
% %     \item The schedule $\bschedule$ also maximises the expected average reward.
% % \end{inparaenum}
% % \end{corollary}
% \end{proof}

% \track{ In the following sections, we design reward functions for specific objectives such that a schedule maximising the expected average reward will also maximise the given objective. As standard RL algorithms optimise discounted reward objectives, we use the result from Theorem~\ref{corollary:1} to conclude that for a large enough discount factor, the RL algoirithm will synthesise the optimal schedule. }

\begin{comment}
\vspace{0.5em}\noindent\textbf{Algorithm for Expectation Semantics.} We show the procedure to obtain an optimal schedule $\sigma$ for expectation semantics in Algorithm \ref{algo:expt}.  Line 1 initialises the Q-function for reinforcement learning. Here, the Q-function is defined on the states of the product CTMDP i.e, $Q_f: (\states \times Q) \times \actions \rightarrow \mathbb{R}$ where $\states$ is the set of states of the CTMDP $\Mdp$ and $Q$ is the set of states of the \textbf{GFM} $A$.
% Variable $i$ keeps track of the number of episodes completed and variable $j$ keeps track of the length of an episode.
The number of episodes to be conducted and the length of the episode is defined by the user and is stored in $k$ and $eplen$ respectively.
The variables $s$ and $q$ represents the current state of $\Mdp$ and $A$ respectively and is initialised to their initial states. The action $a$ from the current state of CTMDP and the transition $t$ from the state of GFM is picked according to the RL policy used (eg. $\epsilon$-greedy). We observe the next state $(s',q')$ and the time spent $\tau$ in the current state. The reward function $R'$ is defined based on $r'$ defined previously. For a given state $(s,q)$ and action $a$, if $\tau$ is the observed time spent in $(s,q)$ then,
$$
 R'((s,q),a,\tau) = \begin{cases}
 \tau & \text{if ($s,q$) is an accepting state}\\
 0 & \text{otherwise}
 \end{cases}
 $$
Variable $r$ stores the reward obtained in each iteration of the episode. The Q-function is updated according to the Q-learning rule defined in section \ref{prelims}. An episode ends when the length of the episode reaches $eplen$. After the completion of $k$ episodes, we obtain the optimal schedule $\sigma$ by choosing the action that gives the highest Q-value from each state.

\begin{algorithm}[t]
\caption{Algorithm for expectation semantics}\label{algo:expt}
\hspace*{\algorithmicindent} \textbf{Input:}  \text{Initial state $s_{init}$, GFM $A$, discount factor $\gamma$,}\\
\hspace*{\algorithmicindent} \hspace{11mm}\text{reward function $R'$, number of episodes $k$,} \\
\hspace*{\algorithmicindent}\hspace{11mm} \text{learning rate $\alpha$, episode length $eplen$}\\
\hspace*{\algorithmicindent} \textbf{Output:}  \text{Optimal schedule $\sigma$}
\begin{algorithmic}[1]
\STATE Initialise $Q_f$ to all zeroes
% \STATE $i \leftarrow 0$
\FOR{$k$ episodes}
\STATE Initialise $s$ and $q$ to $s_0$ and $q_0$ respectively
% \STATE $q \leftarrow q_0$
% \STATE $r \leftarrow 0$
% \STATE $j \leftarrow 0$
\FOR{each episode}
\STATE Choose action $a$ using policy derived from $Q$ 
\STATE Take action $a$, observe next state $s'$, and time $\tau$
\STATE Choose non-deterministic transition $t$ in $A$ using the derived policy
\STATE Take transition $t$ in $A$, observe next state $q'$
\STATE $r \leftarrow R'(s,q,a,\tau)$
\STATE $V(s',q') \leftarrow \max\limits_{a' \in \actions} Q_f(s',q',a')$
\STATE $Q_f(s,q,a) {\leftarrow} (1-\alpha) Q_f(s,q,a)  {+}\alpha \bigl(r {+} e^{-\gamma\tau} V(s',q') \bigr)$
\STATE $s \leftarrow s'$; $q \leftarrow q'$
% \STATE $q \leftarrow q'$
% \STATE $j \leftarrow j+1$
\ENDFOR
% \STATE $i \leftarrow i+1$
\ENDFOR
\FOR{Each state (s,q)}
\STATE $\sigma(s,q) = \max_{a \in \actions}Q(s,q,a)$
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{comment}