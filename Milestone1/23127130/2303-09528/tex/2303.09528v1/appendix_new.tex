\section{Example of end-components} \label{app:EC}
We show an example of a CTMDP in Figure~\ref{fig:example} which has three end-components $\widehat{A}, \widehat{B}, \text{ and } \widehat{C}$.

\begin{figure}[h]
	\centering
	{
		\scalebox{1}{
			\begin{tikzpicture}[auto]
				\node[state] (s1) at (-3,0.5) {$s_1$};
				\node[state] (s6) at (-0.1,-4) {$s_6$};
				\node[state] (s5) at (-0.1,-2) {$s_5$};
				\node[state] (s3) at (-3.1,-2) {$s_3$};
				\node[state] (s4) at (-3.1,-4) {$s_4$};
				\node[state] (s2) at (-7.5,-2) {$s_2$};
				% \coordinate[at= ($(state_0) + (45:0.5)$),label={0:a,$0$}] (state_0_a);
				% \draw[-] (s0) -- (state_0_a);
				% \node[] at (-2.3,0.3) {$a$};
				% \node[] at ($(s4) + (0.8,0.5)$) {$a$};
				% \node[] at ($(s6) + (0.8,0.5)$) {$a$};
				% \node[] at ($(s3) + (-0.9,-0.7)$) {$a$};
				% \node[] at ($(s5) + (-0.9,-0.7)$) {$a$};
			
				\path[->]
					(s1) edge node[right]{$a,2$} (s5)
					(s1) edge node[left]{$a,9$} (s3)
					(s1) edge node[above] {$b,1$} (s2)
					(s5) edge[out=225,in=135] node[below right]{$a,5$} (s6)
					(s5) edge[out=225,in=155,looseness=8] coordinate (e5loop) node[above left]{$a,5$} (s5)
					(s6) edge[out=45,in=-35,looseness=4] coordinate (e6loop) node[below right] {$a,4$} (s6)
					(s6) edge[out=45,in=-45] node[above right] {$a,3$} (s5)
					(s3) edge[out=225,in=135] node[below right]{$a,4$} (s4)
					(s3) edge[out=225,in=155,looseness=8] coordinate (e3loop) node[above left]{$a,5$} (s3)
					(s4) edge[out=45,in=-35,looseness=4] coordinate (e4loop) node[below right] {$a,2$} (s4)
					(s4) edge[out=45,in=-45] node[above right] {$a,5$} (s3)
					(s2) edge[loop left] coordinate (e2loop) node[left, inner sep=7pt]{$a,3$} (s2)
					(s2) edge[out=-80,in=180,bend right] node[below left] {$b,2$} (s4);
				
				\node[rectangle,rounded corners=3pt,draw=none,fill=black,fill opacity=0.1,fit=(s2) (e2loop)] (rectA) {};
				\node[rectangle,rounded corners=3pt,draw=none,fill=black,fill opacity=0.1,fit=(s3) (s4) (e3loop) (e4loop)] (rectB) {};
				\node[rectangle,rounded corners=3pt,draw=none,fill=black,fill opacity=0.1,fit=(s5) (s6) (e5loop) (e6loop)] (rectC) {};
				\node[] at ($(rectA) + (0,-0.8)$) {$\widehat{A}$};
				\node[] at ($(rectB) + (0,-1.7)$) {$\widehat{B}$};
				\node[] at ($(rectC) + (0,-1.7)$) {$\widehat{C}$};
			\end{tikzpicture}
		}
	}
	\caption{A CTMDP with three end-components.}\label{fig:example}
\end{figure}


\section{Example of Uniformization}
We give an example of a non-uniform CTMDP and its uniformized version.
\label{uniform}
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
\begin{tikzpicture}[shorten >=1pt, node distance=3 cm, on grid, auto,thick,initial text=]
\begin{scope}
\node (l0) [state,fill=safecellcolor]  {$q_0$};
\node (l1) [state, fill=safecellcolor, right = of l0,xshift = -0.75cm]   {$q_1$};
\end{scope}
 \begin{scope}
\path [->]
    (l0) edge [bend left]  node [above] {$a_1, 3$}   (l1)
    (l0) edge [bend right]  node [below] {$a_2,6$}   (l1)
    (l1) edge [loop above] node [above] {$a_3,2$}   ()
    ;
\end{scope}
\end{tikzpicture}
\caption{Non-uniform CTMDP where the exit-rates are different for various state action pairs. } \label{fig:PM1}
\end{subfigure}
\hspace{0.5em}
\begin{subfigure}[b]{0.45\linewidth}
\begin{tikzpicture}[shorten >=1pt, node distance=3 cm, on grid, auto,thick,initial text=]
\begin{scope}
\node (l1) [state, fill=safecellcolor]  {$q_1$};
\node (l0) [state, fill=safecellcolor, left = of l1,xshift = 0.75cm]  {$q_0$};
\end{scope}
\begin{scope}
\path [->]
     (l0) edge [loop above] node [above] {$a_1,3$}
    (l0) edge [bend left]  node [above] {$a_1,3$}   (l1)
    (l0) edge [bend right]  node [below] {$a_2,6$}   (l1)
    (l1) edge [loop above] node [above] {$a_3,6$}   ()
    ;
\end{scope}
\end{tikzpicture}
\caption{A Uniform CTMDP where the exit-rate for every state-action pair is $6$. } \label{fig:PM2}
\end{subfigure}
\caption{Uniformization of a CTMDP}
\label{fig:P1}
\end{figure}


\section{Good-for-CTMDP B\"uchi Automata}
\label{app:gfm}
In this section, we provide a formal definition of good-for-CTMDP automata.
\paragraph{Product CTMDP.}
 Given a \emph{labelled} CTMDP $(\Mdp, \atomicprop, \labelling)$ where $\atomicprop$ is a set of atomic propositions, and $\labelling: \states \rightarrow 2^\atomicprop$ is a labelling function and a   B{\"u}chi automaton $\oautomata = (2^{\atomicprop}, \ostate, \oinitstate, \otrans, \acceptingc)$, the product CTMDP is defined as $\Mdp \times \oautomata = ((\states \times \ostate),(\initstate,\oinitstate),\actions,\transR^{\times},\acceptingc^{\times})$ where the rates are $\transR^{\times} : (\states \times \ostate) \times \actions \times (\states \times \ostate) \rightarrow \Reals_{\geq 0}$ such that $\transR^{\times} ((s,q),a,(s',q') = \transR(s,a,s')$ if $\transR(s,a,s')>0$ and $\otrans (q,\labelling(s)) = \{q'\}$.
If $F$ is the set of accepting 
% transitions 
states
in $\oautomata$, then the accepting condition is a set 
% transitions $F^\times$ where $((s,q),a,(s',q')) \in F^\times$ iff $(q,\labelling(s),q') \in F$ 
$F^\times$ of states where $(s,q) \in F^\times$ iff $q \in F$. An example of a product CTMDP is given in Figure~\ref{fig:grid-world}.  

Given an MDP $\Mdp$, a B\"uchi automaton  $\oautomata$, and product $\Mdp \times \oautomata$, we define the following two  problems:
\begin{enumerate}
    \item {\bf Satisfaction Semantics.} 
    Compute a schedule of $\Mdp$ that maximizes the probability of visiting accepting states $F$ of $\oautomata$ infinitely often.
    We define the satisfaction probability $\PSat^{\Mdp \times \oautomata}(s, \sigma)$ of a schedule $\sigma$ from starting state $s$ as: 
\begin{equation*}
\Pr{}^{\Mdp \times \oautomata}_{\sigma}(s) \set{\forall_i  \exists_{j{\geq} i} [X_j \in F^\times] }.
\end{equation*}
The optimal satisfaction probability
$\PSemSat^{\Mdp \times \oautomata}_{\oautomata}(s)$ for specification $\oautomata$ 
is defined as $\sup_{\sigma \in \Sigma_{\Mdp \times \oautomata}} \Pr^{\Mdp \times \oautomata}_{\sigma}(s, \sigma)$ and we say
that $\sigma$ is an optimal schedule for $\oautomata$ if
$\PSemSat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma) (s) = \PSemSat^{\Mdp \times \oautomata}_{\oautomata}$.

\item {\bf Expectation Semantics.} Compute a schedule of $\Mdp$ that maximize the long-run expected average time spent in the accepting states of $\oautomata$.
We define the expected satisfaction time $\ESat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma)$ 
of $\sigma$ from starting state $s$ as: 
\begin{equation*}
 \mathbb{E}^{\Mdp \times \oautomata}_{\sigma}(s) \set{ \liminf_{n \rightarrow \infty} \frac{\sum_{i=1}^{n} [X_i {\in} F] D_i}{T_n}}.
\end{equation*}
The optimal expected satisfaction time
$\ESat^{\Mdp \times \oautomata}_{\oautomata}(s)$ for $\oautomata$ is defined as $\sup_{\sigma \in \Sigma_{\Mdp}} \ESat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma)$ and we say that $\sigma \in \Sigma_\Mdp$ is an optimal expected-satisfaction schedule for $\oautomata$ if
$\ESat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma) = \ESat^{\Mdp \times \oautomata}_{\oautomata} (s)$.
\end{enumerate}

We call a B{\"u}chi automaton $\oautomata = (2^{\atomicprop}, \ostate, \oinitstate, \otrans, \acceptingc)$ good-for-CTMDP if for every \emph{labelled} CTMDP $(\Mdp, \atomicprop, \labelling)$ where $\atomicprop$ is a set of atomic propositions, we have that 
\begin{eqnarray*}
    \PSemSat^{\Mdp}_{\oautomata}(s, \sigma) &=&\PSat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma)~\text{ and }\\
    \ESemSat{}^{\Mdp}_{\oautomata}(s, \sigma) &=&
    \ESat^{\Mdp \times \oautomata}_{\oautomata}(s, \sigma).
\end{eqnarray*}
A good-for-CTMDP automaton allows the computation of the optimal schedule by solving the corresponding problem on the product CTMDP. 
If a B\"uchi automaton is good-for-MDP, then one can show via uniformization that it is also good-for-CTMDPs.
There exists several syntactic characterizations of good-for-MDP automata including suitable limit-deterministic B\"uchi automata (SLDBA)~\cite{sickert2016limit} and slim automata~\cite{HPSS20}.
Moreover, every LTL specification can be effectively converted into a GFM B\"uchi automata.

% \begin{theorem}[Good-for-MDP B\"uchi Automata]
% \end{theorem}
\section{Need For Memory for Optimal Schedules}
\label{exmp:memory}
\input{memory}
\begin{example}[Why memory is required to satisfy $\omega$-regular properties.]
\label{ex:mem}
Consider the CTMDP given above, let the atomic propositions be $\mathtt{b}$ and $\mathtt{g}$ representing blue and green respectively.
Thus the labels on states $q_1$ and $q_2$ are defined as, $\labelling(q_1) = \neg \mathtt{b} \land \mathtt{g}$ and $\labelling(q_2) = \mathtt{b} \land \neg \mathtt{g}$.   
Consider the $\omega$-regular property to be satisfied be $\phi = \always \eventually(\mathtt{b}) \land \always \eventually(\mathtt{g})$. 

We can observe that for a schedule to satisfy this property, both $q_1$ and $q_2$ have to be seen infinitely often. 
As there are no transitions between $q_1$ and $q_2$, both the states can be visited infinitely often only via $q_0$. 
Therefore, the schedule cannot be memoryless as choosing any one action from $q_0$ would not satisfy $\phi$. 



\end{example}

% \begin{example}[$\omega$-regular objectives and memory]
% \label{ex:mem}
% This example demonstrate the need of memoryful schedules for $\omega$-regular objectives. Consider an MDP with single state and two self-loops with actions $a$ and $b$. 
% For an specification $\nextt a \wedge \nextt \nextt b$, it is clear that we need memory to satisfy the objective.
% \end{example} 


\section{Blackwell Optimality In CTMDP}
\label{sec:blackwell_optimality}
\input{blackwell_optimality}


% \begin{center}
%     {\Large Appendix}
% \end{center}
% \section{Appendix}
% \input{appendix}
\section{Proofs from Section ~\ref{sec:theorems&algo}}
\label{app:sat}
In this section, we give a detailed proof of Theorem~\ref{theorem:4.1}.
\paragraph{Theorem~\ref{theorem:4.1}.}There exists a threshold $\zeta' \in (0,1)$ such that for all $\zeta > \zeta'$, and for every state $s$, a schedule maximising the expected average reward in $t$ is 
\begin{inparaenum}[(1)]
\item an optimal schedule in the product CTMDP $\mathcal{M} \times \mathcal{A}$ from $s$ for satisfying the $\omega$-regular objective $\phi$.
Further, since $\oautomata$ is a GFM, we have that \item $\sigma$ induces an optimal schedule for the CTMDP $\mathcal{M}$ from $s$ with objective $\phi$.
\end{inparaenum}
\begin{proof}
For a given CTMDP $\Mdp$, an embedded MDP $\embeddedMdp$ of $\mathcal{M}$ is a discrete-time MDP of the form $\embeddedMdp = (\states, \initstate, \actions{}, \transP_{\Mdp})$, that is, the transition function of $\embeddedMdp$ is derived from the probability matrix of $\Mdp$.
% Given a CTMDP $\mathcal{M}$, we denote the embedded DTMDP by $\emdp$. 
As the set of states and enabled actions from each state are the same in $\Mdp$ and $\emdp$, the set of pure schedules in them are also same. 
% Let $\phi$ be an $\omega$-regular objective, the following lemma gives us the relation between an optimal schedule satisfying $\phi$ in $\pmdp$ and one for the embedded product DTMDP $\emdp \times \oautomata$.
%  

Time does not play a role in the definition of pure schedules. Therefore, the probability of reaching a state $s$ from the initial state in a CTMDP $\Mdp$ under a pure schedule $\sigma$ is dependant only on the transition function of the embedded MDP $\emdp$. 
Now recall that for $\omega$-regular objectives, given a B\"{u}chi GFM, the states of the accepting condition of the product CTMDP need to be visited infinitely often.
As there exist optimal pure schedules for reaching such states, we get the following lemma for an $\omega$-regular objective $\phi$.

\begin{lemma} \label{lem:embedded}
There exists a pure schedule that maximises the probability of satisfying $\phi$ in $\mathcal{M} \times \mathcal{A}$ which also maximises the probability of satisfying $\phi$ in the embedded product DTMDP $\mathcal{M}_\mathcal{E} \times \mathcal{A}$.
\end{lemma}
\begin{proof}
In order to prove this lemma, we consider the semantics of a CTMC.
In a CTMC, every state $s$ has an exit rate $\lambda_s$ such that after reaching state $s$, some time $t_s$ is spent in $s$ where $t_s$ is exponentially distributed with parameter $\lambda_s$, and an outgoing transition to a state $s'$ is taken according to the probability $\transP(s,s')$ of the underlying discrete Markov chain.
Note that given a state $s$, the probability of reaching $s$ from the initial state $\initstate$ of the CTMC thus solely depends on the underlying discrete Markov chain, and not on the exit rates of the states.
Now, for every pure schedule $\sigma$, the CTMC generated is $(\Mdp \times \oautomata)^{[\sigma]}$ and the underlying discrete Markov chain is $(\Mdp_\mathcal{E} \times \oautomata)^{[\sigma]}$.

Thus, the probability of reaching the accepting end-components are the same in both $(\Mdp \times \oautomata)^{[\sigma]}$ and $(\Mdp_\mathcal{E} \times \oautomata)^{[\sigma]}$.
The lemma follows since optimal pure schedules exist for a reachability objective, in particular, there exists pure schedules maximising the probability of reaching accepting end-components, and the set of pure schedules are the same in both $(\Mdp \times \oautomata)^{[\sigma]}$ and $(\Mdp_\mathcal{E} \times \oautomata)^{[\sigma]}$.
Note that time does not play a role in the definition of stationary schedules, that is, timed stationary schedules and time-abstract stationary schedules coincide.
\end{proof}
We have shown that there exists an optimal pure schedule maximising $\phi$ in both $\pmdp$ and $\Mdp_{\mathcal{E}} \times \oautomata$. With a similar argument it also follows that an optimal pure schedule maximising the probability to reach the sink $t$ in the CTMDP $\augmdp$ also maximises the probability to reach $t$ in the DTMDP $\augmdp_{\mathcal{E}}$.

Theorem 3 from \cite{HahnPSSTW19} gives us the existence of a threshold $\zeta' \in (0,1)$ and that for any $\zeta > \zeta'$, an optimal schedule maximising the probability of reaching the sink state $t$ from a state $s$ in the DTMDP $\augmdp_{\mathcal{E}}$ also maximises the probability of satisfying $\phi$ in $\Mdp_{\mathcal{E}} \times \oautomata$.

This leads to the following statement.
There exists a threshold $\zeta' \in (0,1)$ such that for all $\zeta > \zeta'$, and for every state $s$, a schedule $\sigma$ maximising the probability $p_s(\zeta)$ of reaching the sink in $\mathcal{M}_{\mathcal{E}}^{\zeta}$ is
\begin{inparaenum}[(1)]
\item an optimal schedule in the product CTMDP $\mathcal{M} \times \mathcal{A}$ from $s$ for satisfying the $\omega$-regular objective $\phi$, and
\item induces an optimal schedule for the CTMDP $\mathcal{M}$ from $s$ with objective $\phi$.
\end{inparaenum}

The reward machine defined gives a positive reward for each time unit spent in $t$. Therefore, we can conclude that any schedule that maximises the probability of reaching $t$ also maximises the expected average reward.

\end{proof}

\section{Proofs from Section~\ref{sec:d_time}}
\label{app:expt}
In this section, we provide a proof of Lemma~\ref{lemma:6.2}.
\paragraph{Lemma~\ref{lemma:6.2}.}For a product CTMDP $\Mdp \times \oautomata$ where $\oautomata$ is a \textbf{GFM} for an $\omega$-regular objective and for a schedule $\sigma$, the expected average reward obtained w.r.t. the reward function $r'$ is equal to the expected satisfaction time in $~{(\pmdp)}$ and there exist a pure schedule that maximises this.
\begin{proof}
Consider a run $r_{inf} = (s_1,t_1,a_1,s_2,t_2,a_2...)$ in $\Mdp \times \oautomata$ under a schedule $\sigma$ . The satisfaction time is defined as 
$$
    Sat^{\Mdp{\times}\oautomata}_{\oautomata}(s,\sigma) = \liminf_{n \rightarrow \infty} \frac{\sum_{i=1,s_i \in T}^{n} t_i}{\sum_{j=1}^{n}t_j}
$$.
The reward obtained in each state $s_i$ in $r_{inf}$ is $r'(s_i)\cdot t_i$ which is $t_i$ if $s_i \in T$, and 0 otherwise.
Therefore, the average reward obtained from $r_{inf}$ is 
$$  
        \avgreward(s) = \liminf_{n \rightarrow \infty} \frac{\sum_{i=1,s_i \in T}^{n} t_i}{\sum_{j=1}^{n}t_j}
    $$.
Thus, the average reward obtained is equal to the satisfaction time for every run in $\Mdp \times \oautomata$ and therefore, the expected average reward obtained w.r.t. the reward function $r'$ is equal to the expected satisfaction time.

For proving the second part of the lemma, we use the fact that there exists an optimal pure schedule that maximises the expected average reward for any rewardful CTMDP \cite{puterman2014markov}. Therefore, there exists a pure schedule that maximises the expected residence time of $T$ in $\Mdp \times \oautomata$.
\end{proof}


\section{Pseudocode of Algorithms}
\label{algo}
In this section, we provide the pseudocode of the RL algorithm for satisfaction and expectation semantics.
\subsection*{Satisfaction Semantics}
\begin{algorithm}[H]
\caption{Algorithm for satisfaction semantics}\label{algo:sat}
\hspace*{\algorithmicindent}\hspace{2mm}\textbf{Input:}  \text{Initial state $s_{0}$, GFM $A$, discount factor $\gamma$, reward function $rew$, number of episodes (ep-n) $k$, learning rate $\beta$}\\
\hspace*{\algorithmicindent}\hspace{2mm}\textbf{Output:}\text{ Schedule $\sigma$ converging to an optimal one}
\begin{algorithmic}[1]
\STATE Initialise $\Qf$ to all zeroes
% \STATE $i \leftarrow 0$
\FOR{k episodes}
\STATE Initialise $s$ and $q$ to $s_0$ and $q_0$ respectively
\STATE Initialise $r$ to 0
% \STATE $q \leftarrow q_0$
% \STATE $r \leftarrow 0$
\WHILE{r = 0}
\STATE Choose action $a$ using schedule derived from $\Qf$
\STATE Take action $a$, observe next state $s'$ and time $\tau$
\STATE Choose non-deterministic transition $t$ in $A$ using the derived schedule ($\epsilon$-greedy)
\STATE Take transition $t$ in $A$, observe next state $q'$
\STATE $r \leftarrow rew'(s,q,a)$
\STATE $V(s',q') \leftarrow \max\limits_{a' \in \actions} \Qf(s',q',a')$
\STATE $\Qf(s,q,a) \leftarrow (1-\beta) \Qf(s,q,a)  + \beta \bigl(r {+} e^{-\gamma\tau} V(s',q') \bigr)$
\STATE $s \leftarrow s'$
\STATE $q \leftarrow q'$
\ENDWHILE
% \STATE $i \leftarrow i+1$
\ENDFOR
\STATE Initialise $\sigma$
\FOR{each state $(s,q)$}
\STATE $\sigma(s,q) = \max\limits_{a \in \actions}\Qf(s,q,a)$
\ENDFOR
\end{algorithmic}
\end{algorithm}
Recall that the reward function $rew'$ is defined as:
\[
rew'((s,q),a) = \begin{cases}
1 \text{\quad with probability $1-\zeta$ if $(s,q)$ is} \\
\text{\quad \quad accepting} \\
0  \text{\quad otherwise}
\end{cases}
\]

\subsection*{Expectation Semantics}
\begin{algorithm}[H]
\caption{Algorithm for expectation semantics}\label{algo:expt}
\hspace*{\algorithmicindent}\hspace{2mm}\textbf{Input:}  \text{Initial state $s_0$, GFM $A$, discount factor $\gamma$, reward function $rew$, number of episodes (ep-n) $k$,} \\
\hspace*{\algorithmicindent}\hspace{12mm} \text{learning rate $\beta$, episode length (ep-l) $eplen$}\\
\hspace*{\algorithmicindent}\hspace{2mm}\textbf{Output:}\text{ Schedule $\sigma$ converging to an optimal one}
% \hspace*{\algorithmicindent} \textbf{Output:}  \text{Schedule $\sigma$ converging to an optimal one}
\begin{algorithmic}[1]
\STATE Initialise $\Qf$ to all zeroes
% \STATE $i \leftarrow 0$
\FOR{$k$ episodes}
\STATE Initialise $s$ and $q$ to $s_0$ and $q_0$ respectively
% \STATE $q \leftarrow q_0$
% \STATE $r \leftarrow 0$
% \STATE $j \leftarrow 0$
\FOR{each $eplen$-length episode}
\STATE Choose action $a$ using schedule derived from $\Qf$ 
\STATE Take action $a$, observe next state $s'$ and time $\tau$
\STATE Choose non-deterministic transition $t$ in $A$ using the derived schedule ($\epsilon$-greedy)
\STATE Take transition $t$ in $A$ and observe the next state $q'$
\STATE $r \leftarrow rew((s,q),a,\tau)$
\STATE $V(s',q') \leftarrow \max\limits_{a' \in \actions} \Qf(s',q',a')$
\STATE $\Qf(s,q,a)  \leftarrow {(1-\beta)} \Qf(s,q,a)  + \beta \bigl(r {+} e^{-\gamma\tau} V(s',q') \bigr)$
\STATE $s \leftarrow s'$
\STATE $q \leftarrow q'$
% \STATE $q \leftarrow q'$
% \STATE $j \leftarrow j+1$
\ENDFOR
% \STATE $i \leftarrow i+1$
\ENDFOR
\FOR{each state (s,q)}
\STATE $\sigma(s,q) = \max_{a \in \actions}\Qf(s,q,a)$
\ENDFOR
\end{algorithmic}
\end{algorithm}
Recall that he reward function $rew$ is defined based on $r'$, i.e,
$$
 rew((s,q),a,\tau) = \begin{cases}
 \tau & \text{if ($s,q$) is an accepting state}\\
 0 & \text{otherwise}
 \end{cases}
 $$

% \section{Description of benchmarks}
% \label{app:bench}
% A brief description of the benchmarks in Section~\ref{sec:expt} are given below:
% \begin{itemize} 
% \item 
% The {\bf Dynamic Power Management}\cite{dynpow2001} system models a service provider that processes tasks of $N$ different types. The tasks that are to be processed are stored in queues of maximum size $C$ for each type. The service provider can be in three different states: {\tt idle}, {\tt busy} and {\tt standby}. 
% The key parameters here are $N$, $C$ and the time bound. The properties that are checked are with respect to the probability of getting a queue full and the expected time to get the queues full.
% \item 
% The {\bf Fault Tolerant Workstation Cluster}\cite{ftwc2000} benchmark models two sets of workstations (left and right) which are connected by a backbone. 
% A workstation in a set can freely communicate with other workstations on its own set but requires the backbone to communicate with the other set. Each workstation is connected to the backbone via a switch and the size of each set is $N$. 
% The workstation, switch and the backbone can fail and may require repair. The repair unit can repair only one component at a time.
% The property we check is the minimum probability that all the left workstations are down within a time bound.
% The parameters here are $N$ and the time bound. 
% \item 
% The {\bf Polling System}\cite{polling2013} consists of $j$ stations and $1$ server. Here, the incoming requests of $j$ types are buffered in queues of size $k$ each, until they are processed by the server and delivered to their station. The system starts in a state with all the queues being nearly full. The parameters here are $j$, $k$ and a time bound. We consider two properties: (i) all the queues are empty and (ii) one of the queues is empty.
% \item 
% The {\bf Erlang Stages}\cite{erlang2010} benchmark models have two different paths to reach the goal state: a fast but risky path or a slow but sure path. The slow path is an Erlang chain of length $k$ and rate $r$. The objective is to check which path gives faster reachability to a safe target state in an expected sense.
% \item 
% The {\bf Stochastic Job Scheduling} benchmark models a multi-processor system with $k$ processors and $n$ jobs which need to be serviced. 
% The service times are exponentially distributed.
% The jobs can be scheduled pre-emptively and multiple processors can be used to service a single job. The parameters here are $n$ and $k$. The objective is to schedule the jobs efficiently, that is, to minimise the expected time required to service all the jobs. 
% \end{itemize}



