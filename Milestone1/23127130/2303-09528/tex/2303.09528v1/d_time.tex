\input{table}
We study the expectation semantics of $\omega$-regular objective and show that the problem can be reduced to maximising the expected average reward problem in CTMDPs.
Using Theorem~\ref{corollary:1}, this reduces to maximising expected discounted reward for a large discount factor.
We then describe the corresponding reward machine to maximize the \emph{expected satisfaction time} in the good states.

\paragraph{Reduction to Average Reward.} For an $\omega$-regular objective $\phi$, let $\oautomata$ be a \textbf{GFM} corresponding to $\phi$ with a set $F$ of B{\"u}chi accepting states. Let $\Mdp$ be a CTMDP and $\Mdp \times \oautomata$ be the product CTMDP of $\Mdp$ and $\oautomata$.
For a state $s$ in $\Mdp \times \oautomata$, we define the \emph{expected satisfaction time} of a schedule $\sigma$ from starting state $s$ as:
% For a given finite run $r_f = (s_1,a_1,t_1,s_2,a_2,t_2...a_{n-1},t_{n-1},s_n)$, we define the residence time of $s$ in $r_f$ as 
% $\dtime(s)(r_f) = \frac{\sum_{s_i = s,i=1}^{n-1} t_i}{\sum_{j=1}^{n-1} t_j}$.
% Similarly for an infinite run $r_{inf}=(s_1,t_1,a_1,s_2,t_2,a_2...)$, we define the residence time of $s$ in $r_{inf}$ as $\dtime(s)(r_{inf}) = \liminf_{n \rightarrow \infty} \frac{\sum_{i=1,s_i = s}^{n} t_i}{\sum_{j=1}^{n}t_j}$. Note that we consider $\liminf$ since the limit may not exist in general.
 %\todo{Should we use equation environment}
\[
\ESat^{\Mdp {\times} \oautomata}_{\sigma}(s) {=} \mathbb{E}^{\Mdp \times \oautomata}_{\sigma}(s) \set{\liminf_{n \rightarrow \infty} \frac{\sum_{i=1}^{n} [X_i \in F^{\times}] {\cdot} D_i}{T_n}}.
\]
It gives the long-run expected average time spent in the accepting states. 
The reward rate function $r' : S \rightarrow \{0,1\}$ for $~{\Mdp \times \oautomata}$ is defined such that $r'(s) = 1$ if $s \in F^{\times}$, and $r'(s) = 0$, otherwise. Thus the reward is $r'(s)\cdot t = t$ for $s \in F^{\times}$ if $t$ time is spent in $s$.

% The following lemma gives an equivalence between the residence time and the average reward obtained for every run in $(\Mdp \times \oautomata)^{\sigma}$ for every schedule $\sigma$.
% \begin{lemma} \label{lemma:6.1}
% For a product CTMDP $\Mdp \times \oautomata$ with $\oautomata$ being a \textbf{GFM} for an $\omega$-regular objective, and a set $T$ of target states, for a schedule $\sigma$, for all runs in $(\Mdp \times \oautomata)^{\sigma}$, the average reward obtained by the reward function $r'$ is equal to the residence time in $(\Mdp \times \oautomata)^{\sigma}$.
% \end{lemma}

% For a schedule $\sigma \in \Sigma_{\Mdp \times \oautomata}$, we denote the expected residence time in $T$ under $\sigma$ by $\edtime(T)$, and the expected average reward obtained under $\sigma$ for the reward function $r'$ by $\eavgreward(T)$.
% From Lemma~\ref{lemma:6.1} it follows that for any schedule $\sigma$, the expected average reward obtained by $r'$ and the expected residence time in $T$ are equivalent.
The following lemma (proof in Appendix~\ref{app:expt}) gives an equivalence between the expected satisfaction time and expected average reward obtained in $\pmdp$.
\begin{lemma} \label{lemma:6.2}
For a product CTMDP $\Mdp \times \oautomata$ where $\oautomata$ is a \textbf{GFM} for an $\omega$-regular objective and for a schedule $\sigma$, the expected average reward obtained w.r.t. the reward function $r'$ is equal to the expected satisfaction time in $~{(\pmdp)}$ and there exists a pure schedule that maximizes this.
\end{lemma}
% For the product CTMDP $\Mdp \times \oautomata$ with accepting states $T$ corresponding to the B{\"u}chi acceptance, 
% Our objective is to find an optimal schedule $\sigma^*$ that maximises the expected residence time in $T$, i.e, $\edstime(T) = \sup_{\sigma \in \Sigma_{\Mdp \times \oautomata}}\edtime(T)$. We show that such a schedule exists in $\pmdp$, and there exists an optimal schedule that is pure.
% From \cite{puterman2014markov}, we know that there exist optimal pure schedules for maximising the expected average reward and from this result we get the following lemma.

% \begin{lemma}\label{lemma:6.3}
% For a product CTMDP $\Mdp \times \oautomata$ there exists a pure schedule that maximises the expected satisfaction time.
% \end{lemma}
Using the results from Lemma~\ref{lemma:6.2} and Theorem~\ref{corollary:1}, we can conclude that a schedule maximising the discounted reward objective for a large discount factor in $\pmdp$ with reward function $r'$ also maximizes the expected satisfaction time.
% Lemma \ref{lemma:6.3} follows from Lemma~\ref{lemma:6.2} since it is known that an optimal pure schedule exists for maximising $\eavgreward(T)$ over all $\sigma \in \Sigma_{\Mdp}$~\cite{puterman2014markov}.
% we can conclude that there exists an optimal pure schedule $\sigma^*$ for the expectation semantics. 

\paragraph{Algorithm for Expectation Semantics.}
Here, we provide a brief description of the algorithm.  
% We show the procedure to obtain an optimal schedule $\sigma$ for expectation semantics in Algorithm \ref{algo:expt}. 
% Initially, the Q-function for reinforcement learning is initialised to zeroes. 
% Line 1 initialises the Q-function for reinforcement learning.
The Q-function is defined on the states of the product CTMDP, i.e, $\mathcal{Q}_f: (\states \times Q) \times \actions \rightarrow \mathbb{R}$ where $\states$ is the set of states of the CTMDP $\Mdp$ and $Q$ is the set of states of the \textbf{GFM} $A$.
Initially, the state space is unknown to the agent and the agent will have information only on the initial state. 
States seen are stored in a Q-table where the Q-value of the state is stored. 
The initial value of a state in the Q-table is zero.
% Variable $i$ keeps track of the number of episodes completed and variable $j$ keeps track of the length of an episode.
The number of episodes to be conducted and the length of each episode are defined by the user, let these be denoted by $k$ and $eplen$ respectively.
In each episode, the RL agent picks an action from its current state in the CTMDP according to the RL schedule and observes the next state and the time spent in the current state. 
It also picks the transition in the GFM based on the observed state in the CTMDP. 
For each transition taken, the reward obtained is based on the reward function $r'$.
% \todo{Where is $r'$ defined?}
% The variables $s$ and $q$ represent the current state of $\Mdp$ and $A$ respectively and are initialised to the respective initial states. 
% The action $a$ from the current state of CTMDP and the transition $t$ from the state of GFM are picked according to the RL policy used (eg. $\epsilon$-greedy). 
% The RL agent observes the next state $(s',q')$ and the time spent $\tau$ in the current state. 
% The reward function $rew$ is defined based on $r'$ defined previously. 
% For a given state $(s,q)$ and action $a$, if $\tau$ is the observed time spent in $(s,q)$ then,
% $$
%  rew((s,q),a,\tau) = \begin{cases}
%  \tau & \text{if ($s,q$) is an accepting state}\\
%  0 & \text{otherwise}
%  \end{cases}
%  $$
% Variable $r$ stores the reward obtained in each iteration of the episode. 
The Q-function is updated according to the Q-learning rule defined in Section~\ref{prelims}. 
An episode ends when the length of the episode reaches $eplen$. 
After the completion of $k$ episodes, we obtain a schedule $\sigma$ by choosing the action that gives the highest Q-value from each state.
The schedule learnt by the Q-learning algorithm converges to an optimal schedule as the number of training episodes tend to infinity.
We provide a pseudocode of the algorithm 
% for the expectation semantics 
in Appendix~\ref{algo}. 
% \begin{algorithm}
% \caption{Algorithm for expectation semantics}\label{algo:expt}
% \hspace*{\algorithmicindent} \textbf{Input:}  \text{Initial state $s_{init}$, SLDBW $A$, discount factor $\gamma$,}\\
% \hspace*{\algorithmicindent} \hspace{11mm}\text{reward function $R'$, number of episodes $k$,} \\
% \hspace*{\algorithmicindent}\hspace{11mm} \text{learning rate $\alpha$, episode length $eplen$}\\
% \hspace*{\algorithmicindent} \textbf{Output:}  \text{Optimal strategy $\sigma$}
% \begin{algorithmic}[1]
% \STATE Initialise $Q_f$ to all zeroes
% \STATE $i \leftarrow 0$
% \WHILE{$i < k$}
% \STATE $s \leftarrow \initstate$
% \STATE $q \leftarrow q_0$
% \STATE $r \leftarrow 0$
% \STATE $j \leftarrow 0$
% \WHILE{$j < eplen$}
% \STATE Choose action $a$ according to the RL policy
% \STATE Take action $a$, observe next state $s'$, and time $\tau$
% \STATE Choose transition $t$ in SLDBW according to the RL policy
% \STATE Take transition $t$ in SLDBW, observe next state $q'$
% \STATE $r \leftarrow R'(s,q,a,\tau)$
% \STATE $Q_f(s,q,a) \leftarrow Q_f(s,q,a) +\alpha \bigl[ r + e^{-\gamma\tau} \max_{a' \in \actions} Q_f(s',q',a')- Q_f(s,q,a) \bigr]$
% \STATE $s \leftarrow s'$
% \STATE $q \leftarrow q'$
% \ENDWHILE
% \STATE $i \leftarrow i+1$
% \ENDWHILE
% \end{algorithmic}
% \end{algorithm}
% \subsection{Blackwell Optimality In CTMDP}
% \input{blackwell_optimality}
% \input{table}
% \input{csl}