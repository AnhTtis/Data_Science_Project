\begin{table*}[t!]
  \centering
  %\small
  \begin{tabular}[c]{l|cccccccccccccc}
Name & states & prod. & Sat. Prob. & Est. Sat. & Time 1 & Exp. Prob. & Est. Exp. & Time 2\\\hline
\texttt{RiskReward}  & 4 & 8 & 1 & 1    & 1.713 & 0.9 & 0.9 & 0.967\\
\texttt{DynamicPM-tt\_3\_qs\_2}  & 816 & 825 & 1 & 1    & 3.586 & 1 & 1 & 3.62\\
\texttt{QS-lqs\_1\_rqs\_1\_jt\_2}  & 266 & 282 & 1     & 1    & 3.401 & 1 & 1 &3.486 \\
 \texttt{QS-lqs\_1\_rqs\_1\_jt\_5}  & 3977 & 4152 & 1     & 1    &  5.482 & 1 & 1 & 5.524\\
\texttt{QS-lqs\_2\_rqs\_2\_jt\_3}  & 11045 & 24672 & 1     & 1 & 15.158 & 1 & 1 &  15.395 \\
\texttt{ftwc\_001\_mrmc}  & 82 & 122 & 0.999779 & 0.999779    &  94.288 &0.999779 & 0.999779  &98.256\\
% \texttt{PollingSystem-jt1\_qs1}  & 16 & 20 & 1     & 1    & 78.00 &18.8849 & 1.12183\\
\texttt{PollingSystem-jt1\_qs4}  & 348 & 352 & 1     & 1    & 3.423 & 1 & 1 &3.421\\
\texttt{PollingSystem-jt1\_qs7}  & 1002 & 1006 & 1     & 1    & 3.576 & 1 & 1 &3.580 \\
% \texttt{ErlangStages-k500\_r10}  & 508 & 509 & 1 & 1    & 2.73944 & 5.786 & 1.12393 \\
% \texttt{ErlangStages-k2000\_r10}  & 2008 & 2009 & 1 & \textcolor{red}{0.5} & 3.23 & 1 & \textcolor{red}{0.5} & 3.241 \\
\texttt{SJS-procn\_6\_jobn\_2}  & 17 & 21 & 1     & 1    & 3.253 & 1 & 1 & 3.257\\
\texttt{SJS-procn\_2\_jobn\_6}  & 7393 & 7405 & 1     & 1    & 4.336 & 1 & 1 & 4.234
 \end{tabular}
 \caption{\label{tab:experiment}Q-learning results.  The default values of the learner
    hyperparameters are: $\zeta = 0.99$ (for satisfaction semantics), $\epsilon=0.1$ (used in picking $\epsilon$-greedy actions in Q-learning), $\beta=0.01$ (learning rate),
    tol$=0.01$ (tolerance for numerical approximation), ep-l$=300$ (episode length), and ep-n$=20000$ (episode numbers).  Times are in seconds.}
\end{table*}


% \begin{table*}[t!]
%   \centering
%   %\small
%   \begin{tabular}[c]{l|cccccccccccccc}
% Name & states & prod. & Prob. & Est. Prob. & Time 1 & Est-Avg & Time 2\\\hline
% \texttt{DynamicPM-tt\_3\_qs\_2}  & 816 & 825 & 1 & 1    & 2.73944 & 1.95225 & 2.09464\\
% \texttt{QS-lqs\_1\_rqs\_1\_jt\_2}  & 266 & 282 & 1     & 1    & 1.72974 & 2.38395 &1.28006 \\
%  \texttt{QS-lqs\_1\_rqs\_1\_jt\_5}  & 3977 & 4152 & 1     & 1    &  1.98968 & 0.3981 & 8.51865\\
% % \texttt{QS-lqs\_2\_rqs\_2\_jt\_3}  & 11045 & 24672 & 1     & 1 & & 0.49755 & 29.527 \\
% \texttt{ftwc\_001\_mrmc}  & 82 & 122 & 0.999774 & 0.999779    &  19.8628 &0.00075 &19.4509\\
% % \texttt{PollingSystem-jt1\_qs1}  & 16 & 20 & 1     & 1    & 78.00 &18.8849 & 1.12183\\
% \texttt{PollingSystem-jt1\_qs4}  & 348 & 352 & 1     & 1    & 78.00 & 0.3302 &1.3917\\
% \texttt{PollingSystem-jt1\_qs7}  & 1002 & 1006 & 1     & 1    & 1.72974 & 0.0004 &1.6969 \\
% % \texttt{ErlangStages-k500\_r10}  & 508 & 509 & 1 & 1    & 2.73944 & 5.786 & 1.12393 \\
% \texttt{ErlangStages-k2000\_r10}  & 2008 & 2009 & 1 & 1    & 2.73944 & 5.78565 &1.22119 \\
% \texttt{SJS-procn\_2\_jobn\_2}  & 17 & 21 & 1     & 1    & 4.05513 & 26.4677 &1.09653\\
% \texttt{SJS-procn\_2\_jobn\_6}  & 7393 & 7405 & 1     & 1    & 6.39278 &9.1482&3.65823
%  \end{tabular}
%  \caption{\label{tab:experiment}Q-learning results.  The default values of the learner
%     hyperparameters are: $\zeta = 0.99$, $\epsilon=0.1$ (used in picking $\epsilon$-greedy actions in Q-learning), $\beta=0.1$ (learning rate),
%     tol$=0.01$ (tolerance for numerical approximation), ep-l$=30$ (episode length), and ep-n$=20000$ (episode numbers).  Times are in seconds.}
% \end{table*}


% \texttt{DynamicPM-tt\_3\_qs\_2}  & 816 & 1794 & 1 & 1    & 0.5186 & 1.95225 & 2.09464\\
% \texttt{QS-lqs\_1\_rqs\_1\_jt\_2}  & 266 & 551 & 1     & 1    & 0.1015 & 2.38395 &1.28006 \\
%  \texttt{QS-lqs\_1\_rqs\_1\_jt\_5}  & 3977 & 9128 & 1     & 1    &  6.53966 & 0.3981 & 8.51865\\ how do we get number of nodes in the product?. 
% % \texttt{QS-lqs\_2\_rqs\_2\_jt\_3}  & 11045 & 24267 & 1     & 1 & 32.3242 & 0.49755 & 29.527 \\x 
% \texttt{ftwc\_001\_mrmc}  & 82 & 122 & 0.999774 & 0.999779    &  19.8628 &0.00075 &19.4509\\
% % \texttt{PollingSystem-jt1\_qs1}  & 16 & 20 & 1     & 1    & 78.00 &18.8849 & 1.12183\\
% \texttt{PollingSystem-jt1\_qs4}  & 348 & 352 & 1     & 1    & 78.00 & 0.3302 &1.3917\\
% \texttt{PollingSystem-jt1\_qs7}  & 1002 & 1006 & 1     & 1    & 1.72974 & 0.0004 &1.6969 \\
% % \texttt{ErlangStages-k500\_r10}  & 508 & 509 & 1 & 1    & 2.73944 & 5.786 & 1.12393 \\
% \texttt{ErlangStages-k2000\_r10}  & 2008 & 2009 & 1 & 1    & 2.73944 & 5.78565 &1.22119 \\
% \texttt{SJS-procn\_2\_jobn\_2}  & 17 & 21 & 1     & 1    & 4.05513 & 26.4677 &1.09653\\
% \texttt{SJS-procn\_2\_jobn\_6}  & 7393 & 7405 & 1     & 1    & 6.39278 &9.1482&3.65823
% \texttt{DynamicPM-tt\_3\_qs\_2} 0.52
% \texttt{ftwc\_001\_mrmc}  & 82 & 130 & 0.999774 & 0.999779    &  125.75 &0.00075 &19.4509\\
% \texttt{QS-lqs\_2\_rqs\_2\_jt\_3}  & 11045 & 24267 & 1     & 1 & 32.15 & 0.49755 & 29.527 \\
%--learn Q  --reward-type reward-on-acc  --ep-number 20000 --ep-length 300 --learn-stats --save-learner-strategy strategy --alpha 0.01 --seed 7 --discount 0.99
%erlangstates