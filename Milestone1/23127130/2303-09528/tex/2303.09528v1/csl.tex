% \subsection{Limiting Distribution of a CTMC}
% Limiting distribution of a CTMC gives the probability of being in a state in the `long run' or when time is infinity at the limit. Consider a CTMC $H$ where $\states$ is the set of states and $\initstate$ is the initial state. We define a random variable $X(t)$ for $t \in \realpositives$ to be the  state at which the CTMC $H$ is in at time $t$. We define the limiting distribution of $H$ as $\pi^{H} = [\pi_{s_1}^{H}, \pi_{s_2}^H... \pi_{s_n}^H]$ where $s_1,s_2...s_n \in \states$ and for each state $s_i \in \states$
% \begin{center}
%     $\pi^H_{s_i} = \lim_{t \rightarrow \infty} P(X(t) = s_i | X(0) = \initstate)$
% \end{center}
% Also $\sum_{s_i \in S} \pi^{H}_{s_i} = 1$. We omit $X(0) = \initstate$ when the initial state is clear from the context. For a set $T$ of states, we define the limiting probability of $T$ as $\pi_{T}^{H} = \sum_{s \in T} \pi^{H}_{s}$. For a given CTMDP $\Mdp$, a pure strategy $\sigma$, and set $T$ of states, we show the relationship between the expected average reward with respect to the reward function $r'$ and the limiting probability of $T$ in the CTMC $\Mdp^{[\sigma]}$.

% \begin{lemma}\label{csl1}
% For a given CTMDP $\Mdp$, a pure strategy $\sigma$, and a set $T$ of states,
% \[
% Ear^{\Mdp^{[\sigma]}}(T) = \pi_{T}^{\Mdp^{[\sigma]}}
% \]
% where $\pi_{T}^{\Mdp^{[\sigma]}}$ is the limiting probability of $T$.
% \end{lemma}
% \begin{proof}
%  We define the expected average reward from a CTMC $\ctmc$ as,
% \begin{equation*}
%     \begin{split}
%  Ear^{\ctmc} = \lim_{n \rightarrow \infty} \frac{\mathbb{E}_{\sigma}^{\Mdp} \{\sum^{n}_{i=0} r(s_i,a_i)\}}{\mathbb{E}_{\sigma}^{\Mdp} \{\sum_{i=0}^{n} \tau_i\}}
%  \end{split}
% \end{equation*}
% From the definition of reward function r', it gives a reward of $1$ for one time unit spent in a state in $T$ and $0$ otherwise. Therefore, the expected average reward becomes,
% \[
% Ear^{\ctmc} = \lim_{n \rightarrow \infty} \frac{\mathbb{E}_{\sigma}^{\Mdp} \{\sum_{i=0,s_i \in T}^{n}\tau_{s_{i}}\}}{\mathbb{E}_{\sigma}^{\Mdp} \{\sum_{i=0}^{n} \tau_i\}}
% \]
% where $\sum_{i=0,s_i \in T}^{n}\tau_{s_{i}}$ gives the expected time spent in states of $T$ and $\sum_{i=0}^{n} \tau_i$ is the total time. This gives the fraction of time spent in states of $T$ in the long run (i.e when $t \rightarrow \infty$) i.e,
% \[
% \sum_{s_i \in T}\lim_{t \rightarrow \infty} P(X(t) = s_i) = \lim_{n \rightarrow \infty} \frac{\mathbb{E}_{\sigma}^{\Mdp} \{\sum_{i=0,s_i \in T}^{n}\tau_{s_{i}}\}}{\mathbb{E}_{\sigma}^{\Mdp} \{\sum_{i=0}^{n} \tau_i\}}
% \]
 
%  From the definition of $\pi_{T}^{\Mdp^{[\sigma]}}$, we can conclude that 
% \[
% Ear^{\ctmc} = \pi_{T}^{\ctmc}
% \]
% \end{proof}
% From Lemma \ref{csl1}, it is clear that for a given set $T$ of states, a reward function $r'$, and a strategy $\sigma$, the expected average reward and the limiting probability of $T$ in the resulting CTMC are the same.

% \begin{theorem}\label{csl2}
% For a CTMDP $M$, and a set $T$ of states, an optimal pure strategy $\sigma^*$ maximising the expected average reward with respect to $r'$ results in a CTMC $M^{[\sigma^*]}$ which also maximises the limiting probability of $T$.
% \end{theorem}
% \begin{proof}
% From Lemma \ref{csl1}, for each pure strategy $\sigma$,
% \[
% Ear^{\Mdp^{[\sigma]}}(T) = \pi_{T}^{\Mdp^{[\sigma]}}
% \]
% Therefore, if $\sigma^*$ is the strategy maximising the expected average reward then we can conclude that it also maximises the limiting probability of $T$.
% \end{proof}
% From Theorem \ref{csl2}, it is clear that the reward function $r'$ gives the optimal pure strategy that maximises the limiting probability of $T$. From Theorem \ref{csl2}, and  Corollary \ref{corollary:1}, we obtain the following.
% \begin{corollary}\label{corollary2}
% For a CTMDP $\Mdp$, a set $T$ of states and a reward function $r'$, there exists a Blackwell optimal pure strategy $\sigma^*$ such that it maximises \begin{inparaenum}
% \item the expected average reward, 
% \item and the limiting probability of $T$ is maximised in $\Mdp^{[\sigma^*]}$.
% \end{inparaenum}
% \end{corollary}
% From Corollary \ref{corollary2}, it is clear that given a set $T$ of states, the problem of obtaining an optimal strategy maximising the expected average reward and obtaining a strategy which maximises the limiting probability of $T$ are the same.

% Finally, we describe the relation between continuous time stochastic logic(CSL) \cite{baier1999CSL} which is a stochastic branching time temporal logic that is used to model-check CTMCs against properties expressed in it and our expectation semantics.
% This logic contains a steady state operator $\mathcal{S}$ and maximizing the probability of satisfaction of this $\mathcal{S}$ for a formula that is true in the good states of a CTMDP corresponds to maximising the expected time of stay in these sttaes.

\begin{comment}
{\color{red} What did we decide regarding CSLs?}
Continuous time stochastic logic(CSL) \cite{baier1999CSL} is a stochastic branching time temporal logic and is used to model-check CTMCs against properties expressed in it. We show the relationship between expectation semantics and CSL.
\subsection{CSL}
Consider a CTMC $H$, the state formulas of CSL are interpreted over the states of $H$. Let $a \in \atomicprop$, and $p \in [0,1]$, the syntax of a state formula is defined by:
\begin{center}
    $\phi := True | a | \phi_1 \land \phi_2 | \lnot \phi | \mathcal{S}_{\bowtie p}(\phi) | \mathcal{P}_{\bowtie p}(\psi)$
\end{center}
Let $I \subseteq \realpositives $ be an interval. A path formula is defined by:
\begin{center}
    $\psi := \nextt \phi | \phi_1 \until \phi_2 | \phi_1 \until^{I} \phi_2$
\end{center}
Where $\bowtie \in \{\geq,\leq\}$.
Here the meaning of $\nextt$ and $\until$ is the same as LTL. The operator $\until^I$ is a timed variant of until operator used in LTL. For a path $\rho$ in a CTMC, we denote the state of $\rho$ at time $t$ by $\rho[t]$. For a time interval $I$, a formula $\phi_1 \until^I \phi_2$ is satisfied if there exists a path $\rho$ such that there exists a time $t \in I$ where $\rho[t]$ satisfies $\phi_2$ and for all $u \in [0,t)$ $\rho[u]$ satisfies $\phi_1$. The operator $\mathcal{S}_{\bowtie p}(\phi)$ is satisfied if the limiting probability of $sat (\phi)$ (the set of states satisfying $\phi$) is $q$ and $q\bowtie p$. The operator $\mathcal{P}_{\bowtie p}(\phi)$ is satisfied if the probability of paths from a state s satisfying $\phi$ is $q$ and $q\bowtie p$. We concentrate on the $S$ operator and relate it to the expectation semantics.

The semantics of state and path formulas are the same as that of LTL, and we denote the semantics of $\mathcal{S}$ operator as
\begin{center}
    $\initstate \models \mathcal{S}_{\bowtie p}(\phi)$ iff $\pi^{H}_{Sat(\phi)} \bowtie p$,
\end{center}
where $sat(\phi)$ is the set of states satisfying the formula $\phi$. 

For a CTMDP $\Mdp$, we say that it satisfies a CSL formula $\phi$ if there exists a strategy $\sigma$ such that $\Mdp^{[\sigma]}$ satisfies $\phi$. We look at formulas of the form $\mathcal{S}_{\geq p}(\phi)$ that are called steady-state formulas, where $\phi$ is an LTL formula. We show that the reward function $r'$ can be used to check if there exists a strategy that satisfies a steady-state formula. We provide a sketch of this procedure.


Given an $\omega$-regular property $\phi$, Theorem \ref{GFM} states that a corresponding \textbf{GFM} $A_\phi$ exists. For a given CTMDP $M$ and a \textbf{GFM} $A_\phi$, let $(\Mdp \times A_\phi)$ be the product CTMDP as defined in Section~\ref{prelims}. Let $Acc$ be the set of accepting states in $(\Mdp \times \caut)$. We can construct a reward function $r'$ with respect to $Acc$ as shown previously. From Theorem~\ref{csl2}, a strategy maximising the expected average reward also maximises the limiting probability for $Acc$. Let $\sigma^*$ be an optimal pure strategy in $(\Mdp \times \caut)$ maximising the expected average reward. Therefore, $\sigma^*$ also maximises the limiting probability for the states in $Acc$ in $(\Mdp \times A_{\phi})^{[\sigma^*]}$ and let this limiting probability be denoted by $q_{Acc}$, i.e, 
\begin{center}
$q_{Acc} = \sum_{s \in Acc} \pi^{(\Mdp \times A_{\phi})^{[\sigma^*]}}_{s}(\phi)$    
\end{center}


From the semantics of CSL, a formula $\mathcal{S}_{\geq p}  (\phi)$ is satisfied in $(\Mdp \times \caut)$ if there exists a strategy $\sigma$ such that 
\begin{center}
    $\pi^{(\Mdp \times \caut)^{[\sigma]}}_{sat(\phi)} \geq p$
\end{center}
From the definitions, we know that $sat(\phi) = Acc$ and as $\sigma^*$ is an optimal pure strategy maximising the limiting probability of $Acc$, we can conclude that $(\Mdp \times A_\phi)$ satisfies $\mathcal{S}_{\geq p}(\phi)$ iff $q_{Acc} \geq p$.
\end{comment}




