\section{Experiments}\label{sec:exp}
\begin{table*}[ht]
  \centering
  \begin{tabular}{lcccccccc}
    \toprule
     & \multicolumn{3}{c}{TED Gesture~\cite{yoon2020speech}} &  \multicolumn{3}{c}{TED Expressive~\cite{liu2022learning}} \\
    \cmidrule(r){2-4} \cmidrule(r){5-7}
    Methods & FGD $\downarrow$ & BC $\uparrow$ & Diversity $\uparrow$ & FGD $\downarrow$ & BC $\uparrow$ & Diversity $\uparrow$ \\
    \midrule
    Ground Truth & 0 & 0.698 & 108.525 & 0 & 0.703 & 178.827\\
    \midrule
    Attention Seq2Seq~\cite{yoon2019robots}   & 18.154  & 0.196 & 82.776 & 54.920 & 0.152 & 122.693\\
     Speech2Gesture~\cite{ginosar2019learning}  & 19.254  & 0.668 & 93.802 & 54.650 & 0.679 & 142.489\\
     Joint Embedding~\cite{ahuja2019language2pose}   & 22.083  & 0.200 & 90.138 & 64.555 & 0.130 & 120.627\\
     Trimodal~\cite{yoon2020speech}   & 3.729 & 0.667 & 101.247 & 12.613 & 0.563 & 154.088\\
    HA2G~\cite{liu2022learning} & 3.072 & 0.672 & 104.322 & 5.306 & 0.641 & 173.899\\
     \midrule 
    \textbf{DiffGesture (Ours)} & \textbf{1.506} & \textbf{0.699} & \textbf{106.722} &      \textbf{2.600} & \textbf{0.718} & \textbf{182.757} \\

    \bottomrule
  \end{tabular}
  \caption{\textbf{The Quantitative Results on TED Gesture~}\cite{yoon2020speech} \textbf{and TED Expressive~}\cite{liu2022learning}. We compare the proposed diffusion-based method against recent SOTA methods~\cite{ahuja2019language2pose, ginosar2019learning, yoon2020speech, yoon2019robots, liu2022learning} and ground truth. For FGD, the lower, the better; for other metrics, the higher, the better.}
  \label{tbl:res}
\end{table*}

\subsection{Co-Speech Gesture Datasets}

\noindent \textbf{TED Gesture.} As a large-scale dataset for gesture generation research, TED Gesture dataset~\cite{yoon2019robots, yoon2020speech} contains 1,766 TED videos of different narrators covering various topics. 
%
We follow the data process in former works~\cite{yoon2020speech, liu2022learning}, where the poses are resampled with 15 FPS, and frame segments of length 34 are obtained with a stride of 10.

\noindent 
\textbf{TED Expressive.} While the poses in TED Gesture only contain 10 upper body key points without vivid finger movements, the TED Expressive dataset~\cite{liu2022learning} is further expressive of both finger and body movements. The state-of-art 3D pose estimator ExPose~\cite{ExPose:2020} is used to fully capture the pose information in data. As a result, TED Expressive annotates the 3D coordinates of 43 keypoints, including 13 upper body joints and 30 finger joints.

\subsection{Experimental Settings}

\noindent 
\textbf{Comparison Methods.} 
We compare our method on two benchmark datasets with the state-of-the-art methods in recent years. \textbf{1) Attention Seq2Seq}~\cite{yoon2019robots} elaborates on the attention mechanism to generate pose sequences from speech text. 
\textbf{2) Speech2Gesture}~\cite{ginosar2019learning} uses spectrums of the speech audio segments as the input and generates speech gestures adversarially. 
\textbf{3) Joint Embedding}~\cite{ahuja2019language2pose} maps text and motion to the same embedding space, then generates outputs from motion description text.
\textbf{4) Trimodal}~\cite{yoon2020speech} serves as a strong baseline that learns from text, audio, and speaker identity to generate gestures, outperforming former methods by a large margin.
\textbf{5) HA2G}~\cite{liu2022learning} introduces a hierarchical audio learner that captures information across different semantic granularities, achieving state-of-the-art performances. This method hierarchically extracts rich features at the cost of heavier GPU memory overhead, while our method requires much smaller expenses.

\noindent 
\textbf{Implementation Details.} For all the methods in both datasets, we set $N=34$ and $M=4$ to get $M$-frame pose sequences where the first $N$ frames are used for reference, termed as initial poses. There are $J$ upper body joints in all the frames of pose sequences, where $J=10$ for TED Gesture and $J=43$ for TED Expressive. Following~\cite{yoon2020speech}, to eliminate the effect of the joint lengths and root motion, we represent the joints' positions using $J-1$ directional vectors normalized to the unit vectors and train the model to learn the directional vectors.
For the audio processing, we use the same audio encoder used in~\cite{yoon2020speech} to extract the feature of the raw audio clips directly. 
The audio clips are encoded as $N$ audio feature vectors of 32-D.
The audio feature and initial poses are concatenated to form the conditional context information of the diffusion model. For the diffusion process, the number of timesteps is $T=500$, and the variances increase linearly from $\beta_1=1e-4$ to $\beta_T=0.02$.
For the Stabilizer, $t_0$ can be adjusted from 20-30 for Thresholding, and a quadratic non-increasing function $\sigma_a(t)$ is applied for Smooth Sampling. The hidden dimension of the transformer blocks, is set to 256 for TED Gesture and 512 for TED Expressive. We use 8 Transformer blocks, each of which comprises a multi-head self-attention block and a Feed-Forward Network. We use an Adam optimizer, and the learning rate is $5e-4$. It takes 10 hours to train the model on TED Gesture and 20 hours on TED Expressive on a single NVIDIA GeForce RTX 3090 GPU.

\begin{figure*}[t]
\centering
\includegraphics[width=1.00\textwidth, height=0.59\textwidth]{figure/results.pdf}
\caption{\textbf{Visualization Results of Our DiffGesture on Two Datasets.} Three cases are picked up, where (i) and (ii) are TED Expressive cases, and (iii) is a TED Gesture case. We highlight dull cases generated by comparison methods with rectangles, indicating the mode collapse phenomenon of baselines.}
\label{result}
\end{figure*}

\begin{table*}
  \centering
  \begin{tabular}{cccccccc}
    \toprule
    Methods &
    GT &
    S2S.~\cite{yoon2019robots}& 
    S2G.~\cite{ginosar2019learning} & 
    Joint.~\cite{ahuja2019language2pose} & Tri.~\cite{yoon2020speech} & 
     HA2G~\cite{liu2022learning} & 
    \textbf{DiffGesture(Ours)}  \\
    \midrule
    Naturalness & 4.33 & 1.22 & 2.56 & 1.22 & 3.22 & 3.67 &\textbf{4.00}\\
    Smoothness & 3.94 & 3.50 & 1.61 & 3.44 & 3.44 & 3.39 & \textbf{3.89}\\
    Synchrony & 4.00 & 1.67 & 3.17 & 1.39 & 3.28 & 3.44 &\textbf{3.89}\\
    \bottomrule
  \end{tabular}
  \caption{\textbf{User Study Results.} The ratings of motion naturalness, smoothness, and synchrony, are on a scale of 1-5, with 5 being the best.}
  \label{table:userstudy}
\end{table*}

\subsection{Evaluation Metrics}
In evaluation, we use three metrics that are used in co-speech gesture generation and relative fields~\cite{liu2022learning, li2022danceformer}. 

\noindent\textbf{Fr\'echet Gesture Distance (FGD).} Similar to the Fr\'echet Inception Distance (FID) metric~\cite{heusel2017gans}, which is widely applied in image generation studies, FGD is used to measure the distance between the synthesized gesture distribution and the real data distribution. Yoon \textit{et al.}~\cite{yoon2020speech} define FGD by training a skeleton sequence auto-encoder to extract the features of the real gesture sequences $X$ and the features of the generated gesture sequences $\hat{X}$:
\begin{equation} 
    \label{eq:fgd}
    \mathrm{FGD}(X,\hat{X}) = \|\mu_r-\mu_g\|^2+\rm{Tr}(\Sigma_r+\Sigma_g-2(\Sigma_r\Sigma_g)^{1/2}), 
    \nonumber
\end{equation}
where $\mu_r$ and $\Sigma_r$ are the first and the second moments of the latent feature distribution of the real gestures $X$, and $\mu_g$ and $\Sigma_g$ are the first and the second moments of the latent feature distribution of the generated gestures $\hat{X}$. 
We intuitively find that among the three metrics, FGD tells the most whether the generated pose sequences are of high quality.

\begin{table*}
  \centering
  \begin{tabular}{lcccccccc}
    \toprule
     & \multicolumn{3}{c}{TED Gesture~\cite{yoon2020speech}} &  \multicolumn{3}{c}{TED Expressive~\cite{liu2022learning}} \\
    \cmidrule(r){2-4} \cmidrule(r){5-7}
    Methods & FGD $\downarrow$ & BC $\uparrow$ & Diversity $\uparrow$ & FGD $\downarrow$ & BC $\uparrow$ & Diversity $\uparrow$ \\
    \midrule
    DiffGesture Base & 2.450 & 0.632 & 104.688 & 3.822 & 0.707 & 174.377\\
    DiffGesture w/o Stabilizer  & 2.219 & 0.674 & 105.192 & 2.792 & \textbf{0.721} & 180.125 \\
    DiffGesture w/o classifier-free & 1.810 & 0.673 & 105.644 & 3.326 & 0.717 & 178.245 \\
    \textbf{DiffGesture (Ours)} & \textbf{1.506} & \textbf{0.699} & \textbf{106.722} &      \textbf{2.600} & 0.718 & \textbf{182.757} \\
    \bottomrule
  \end{tabular}
  \caption{\textbf{Ablation Study on the Proposed Modules.} We investigate effectiveness of the proposed modules, Diffusion Gesture Stabilizer and implicit classifier-free guidance. The results indicate that the proposed modules consistently improve performance on the benchmarks.}
  \label{tbl:abl}
\end{table*}

\noindent \textbf{Beat Consistency Score (BC).} Proposed in~\cite{li2021learn, li2022danceformer}, BC measures motion-audio beat correlation. Considering that the kinematic velocities vary from different joints, we use the change of included angle between bones to track motion beats following~\cite{liu2022learning}. Specifically, we can calculate the mean absolute angle change (MAAC) of angle $\theta_j$ in adjacent frames by $\mathrm{MAAC}(\theta_j) = \frac{1}{S} \frac{1}{T-1} \sum_{s=1}^S\sum_{t=1}^{T-1}\|\theta_{j, s, t+1} - \theta_{j, s, t}\|_1$, where $S$ denotes the total number of clips in the dataset, $T$ denotes the number of frames in each clip, and $\theta_{j, s, t}$ is the included angle between the $j$-th and the ($j$+1)-th bone of the $s$-th clip at time-step $t$. Then, we can compute the angle change rate of frame $t$ for the $s$-th clip as $\frac{1}{J-1}\sum_{j=1}^{J-1}(\|\theta_{j, s, t+1} - \theta_{j, s, t}\|_1/\mathrm{MAAC}(\theta_j))$. Then we extract the local optima whose first-order difference is higher than a threshold to get kinematic beats, which 
are used to compute BC later. Following~\cite{li2022danceformer} to detect audio beat by onset strength~\cite{ellis2007beat}, we compute the average distance between each audio beat and its nearest motion beat as Beat Consistency Score:
\begin{equation} 
    \label{eq:bc}
    \mathrm{BC} = \frac{1}{n}\sum_{i=1}^n\exp (-\frac{\min_{\forall  t_j^y\in B^y}\|t_i^x - t_j^y\|^2}{2\sigma^2}),
\end{equation}
where $t^x_i$ is the $i$-th audio beats, $B^y=\{t^y_i\}$ is the set of the kinematic beats, and $\sigma$ is a parameter to normalize sequences, set to $0.1$ empirically.

\noindent\textbf{Diversity.} This metric evaluates the variations among generated gestures corresponding to various inputs~\cite{NEURIPS2019_7ca57a9f}. We use the same feature extractor when measuring FGD to map synthesized gestures into latent feature vectors and calculate the mean feature distance. In detail, we randomly pick 500 generated samples and compute the mean absolute error between the features and the shuffled features.

\subsection{Evaluation Results}

\noindent \textbf{Quantitative Results.} We compare our method with all the baselines with three metrics on TED Gesture and TED Expressive. The results are shown in Table~\ref{tbl:res}. For the metrics of Ground Truth, we report the values in our implementation. For TED Gesture, we report FGD of all baselines in~\cite{liu2022learning} and evaluate BC and Diversity on our own\footnote[2]{Since there exists an evaluation bug for the BC metric in HA2G~\cite{liu2022learning}, we report the re-implemented results from Liu \textit{et al.}}. For TED Expressive, all the results of baselines are reported from~\cite{liu2022learning}. Assuming the pseudo ground truth pose follows the real distribution, the FGD of Ground Truth in the table is 0. It is observed that our \textbf{DiffGesture} achieves state-of-the-art performance on both datasets, especially outperforming existing methods by a large margin on TED Expressive. Besides, since BC and Diversity are proposed to measure motion-audio beat correlation and variation, these two metrics of Ground Truth cannot be treated as upper bounds, and it is worth noting that our results may be higher than the ones of Ground Truth, indicating that the generated gestures are of high quality.    
\noindent \textbf{Qualitative Results.} We show the keyframes of all the methods on two datasets in Figure~\ref{result}. Since TED Expressive requires a higher ability of generative models, we pick up two cases for TED Expressive and one for TED Gesture. For each case, we select three keyframes (an early, a middle, and a late one) to show the pose motions. Comparison methods tend to generate slow and invariant poses and sometimes produce unreliable and stiff results. In contrast, DiffGesture produces diverse human-like poses without resulting in mean poses which are slow and rigid. Besides, as a primary drawback of GAN-based methods, mode collapse makes comparison methods often produce a single type of output, which is severe for pose motion generation. Such a phenomenon is shown in Fig.~\ref{result}, where the generated frames with nearly the same pose are highlighted with rectangles. 

\noindent \textbf{User Study.} To better validate the qualitative performance, we conduct a user study on the generated co-speech gestures. The study involves 18 participants, with 9 females and 9 males in the age range of 18-25 years old. The participants are required to grade the motion's quality and coherence, and all the clips are without labels. In total, we pick up 30 cases, 20 for TED-Expressive and 10 for TED Gesture. For each case, we show 7 videos with the order of the methods shuffling, including the ground truth. We adopt the Mean Opinion Scores rating protocol, and each participant is required to rate three aspects of generated motions: \textit{Naturalness}; \textit{Smoothness}; \textit{Synchrony between speech and generated gestures}. The results are shown in Table~\ref{table:userstudy} where the ratings are on a scale of 1 to 5, with 5 being the best. Our participants widely accept that our method produces high-fidelity results in all three aspects.

\subsection{Ablation Studies}
\noindent \textbf{Ablation Study on the Proposed Modules.} 
To demonstrate the effectiveness of our proposed \textbf{DiffGesture}, we present ablation studies on the key modules in the framework. In detail, we conduct experiments as follows. \textbf{1)} DiffGesture Base means we only use the proposed conditional diffusion generation process without further design.
\textbf{2)} DiffGesture w/o classifier-free, where classifier-free guidance is not implemented at both the training stage and inference stage. \textbf{3)} DiffGesture w/o Stabilizer, where Diffusion Gesture Stabilizer is removed at the inference stage. The results are reported in Table~\ref{tbl:abl}. The results illustrate the effectiveness of the designed Diffusion Gesture Stabilizer and the implicit classifier-free guidance.

\begin{table}
  \centering
  \begin{tabular}{lccccc}
    \toprule
    Methods & FGD $\downarrow$ & BC $\uparrow$ & Diversity $\uparrow$ \\
    \midrule
    GRU on $D_a$ & 14.343  & 0.658 & 98.472 \\
    Transformer on $D_a$& \textbf{1.506} & \textbf{0.699} & \textbf{106.722} \\
        \midrule
    GRU on $D_b$ & 17.452  & 0.680 & 172.168  \\
    Transformer on $D_b$& \textbf{2.600} & \textbf{0.718} & \textbf{182.757} \\
    \bottomrule
  \end{tabular}
  \caption{\textbf{Ablation Study on the Network Architectures.} We compare the performance of GRU and Transformer for diffusion-based backbone on TED Gesture ($D_a$) and TED Expressive ($D_b$).}
  \vspace{-1pt}
  \label{tbl:abl2}
\end{table}

\noindent \textbf{Ablation Study on the Network Architectures.} 
We investigate the performance of the GRU architecture in diffusion models, autoregressively generating poses in~\cite{yoon2020speech, liu2022learning}. We replace the Diffusion Audio-Gesture Transformer with the GRU in the diffusion model. All the context inputs remain the same as our method and are concatenated before the diffusion network. Results are shown in Table~\ref{tbl:abl2}. Though GRU serves as a strong baseline network in co-gesture learning, it fails to generate high-performance data like our designed Transformer-based network, which indicates the effectiveness of our Transformer-based network and that applying diffusion models in the audio-driven conditional generation is a non-trivial task.
