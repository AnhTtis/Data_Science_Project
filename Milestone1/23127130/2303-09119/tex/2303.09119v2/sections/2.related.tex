\section{Related Work}\label{sec:related-work}

\noindent\textbf{Co-Speech Gesture Generation.} Synthesizing co-speech gestures is crucial for a variety of applications. Conventional studies resort to rule-based pipelines~\cite{cassell1994animated, huang2012robot, marsella2013virtual}, where linguistic experts pre-define the speech-gesture pairs and refine the transitions between different motions. Recent works exploit neural networks to learn the mapping from speech to gesture based on a large training corpus, where an off-the-shelf pose estimator is leveraged to label the online videos for pseudo annotations~\cite{yoon2019robots, ginosar2019learning, liu2022learning, ahuja2020no, yoon2020speech, qian2021speech}. 
Meanwhile, some works study the influence of input modality, verifying the connections between co-speech gesture and speech audio~\cite{li2021audio2gestures}, text transcript~\cite{ahuja2019language2pose}, speaking style~\cite{ahuja2020style}, and speaker identity~\cite{yoon2020speech}. 
To further improve the model's capacity, previous studies explore multiple architecture choices, including CNN~\cite{habibie2021learning}, RNN~\cite{yoon2019robots}, Transformer~\cite{bhattacharya2021text2gestures}, and VQ-VAE~\cite{yazdian2022gesturevec, liu2022audio}. Notably, several recent works are based on GANs to guarantee realistic results~\cite{ginosar2019learning, yoon2020speech, qian2021speech, liu2022learning}, which involve the adversarial training between the generator and the discriminator. However, the notorious mode collapse and unstable training of GANs prevent the high-fidelity gesture distribution learning conditioned on audio.

\begin{figure*}[t]
\centering
\includegraphics[width=1.00\textwidth]{figure/framework.pdf}
\caption{\textbf{Overview of the Diffusion Co-Speech Gesture (DiffGesture) Framework.} Given the gesture sequence $\bm{x}_0$, we first establish the forward diffusion (\textcolor[rgb]{0.8, 0.2, 0.8}{purple}) and conditional denoising process (\textcolor[rgb]{0.3, 0.6, 0.2}{green}) in gesture space. Then, we devise the Diffusion Audio-Gesture Transformer to attend to the input conditions of initial poses $\bm{p}_{(1:M)}$, speech audio $\bm{a}$, time embedding $t$ and corrupted gesture $\bm{x}_t$ from multiple modalities (\textcolor[rgb]{0.18039215686275, 0.45882352941176, 0.71372549019608}{blue}). At the diffusion sampling stage (\textcolor[rgb]{0.37647058823529, 0.37647058823529, 0.37647058823529}{grey}), we propose the Diffusion Gesture Stabilizer to eliminate temporal inconsistency with an annealed noise sampling strategy. To further incorporate implicit classifier-free guidance, we jointly train the conditional ($1 - p_{\textit{uncond}}$) and unconditional ($p_{\textit{uncond}}$) models. This allows us to trade off between diversity and quality during inference.} 
\label{framework}
\vspace{-8pt}
\end{figure*}

\noindent\textbf{Diffusion Probabilistic Models.} Diffusion probabilistic models have achieved promising results on unconditional image generation~\cite{ho2020denoising}, which are further applied to conditional tasks like text-to-image~\cite{Saharia2022Photorealistic}. Among the diffusion-based literature, previous works focus on static data and conditions. Besides, they mainly utilize explicit guidance like pretrained classifiers~\cite{dhariwal2021diffusion} and CLIP similarity~\cite{nichol22glide, liu2023more, Saharia2022Photorealistic, ramesh2022hierarchical} to guide the generation process. In this work, we explore a more challenging co-speech gesture generation setting, where the gesture data and audio conditions are both sequential, and the audio-to-gesture mapping is implicit. To this end, we propose the Diffusion Audio-Gesture Transformer to guarantee temporally aligned generation. We further propose the Diffusion Gesture Stabilizer to simultaneously achieve diverse and temporally coherent gestures with an annealed noise sampling strategy.
