\section{Our Approach}\label{sec:method}

Figure~\ref{framework} depicts an overview of the proposed \textbf{DiffGesture} framework to generate co-speech gestures of high fidelity.
%
In this section, we first introduce the problem formulation of audio-driven co-speech gesture generation (Section~\ref{sec:3.1}). We then establish the forward diffusion and the reverse conditional generation process in gesture space (Section~\ref{sec:3.2}). Furthermore, we elaborate the Diffusion Audio-Gesture Transformer to attend to the conditions from multiple modalities and enhance the speech-gesture correlations with temporal dependency (Section~\ref{sec:3.3}). To eliminate temporal inconsistency introduced by naive noises, we propose a novel Diffusion Gesture Stabilizer with annealed noise sampling strategies and describe this module in (Section~\ref{sec:3.4}). Finally, incorporating implicit classifier-free guidance in co-speech gestures is discussed in (Section~\ref{sec:3.5}).

\subsection{Problem Formulation}
\label{sec:3.1}
With a large-scale co-speech gesture training corpus, we leverage the speaking videos with clear co-speech upper body movements for model learning. In particular, for each video clip of $N$ frames, we extract the accompanying speech audio sequence $\bm{a} = \{\bm{a}_1, \dots, \bm{a}_N\}$ and use the off-the-shelf human pose estimator OpenPose~\cite{cao2019openpose} to annotate the per-frame human skeletons as $\bm{x} = \{\bm{p}_1, \dots, \bm{p}_N\}$. We follow baseline methods~\cite{yoon2020speech, liu2022learning} to further pre-process such skeletal representation into the concatenation of unit direction vectors as $\bm{p}_i = \left[\bm{d}_{i,1}, \bm{d}_{i,2}, \dots, \bm{d}_{i,J-1}\right]$, where $\bm{p}_i$ denotes the 2D keypoint coordinates of the $i$-th frame, $J$ is the total joint number and $\bm{d}_{i,j}$ represents the unit direction vector between the $j$-th and the ($j$+1)-th joint of the $i$-th image frame. The diffusion model's reverse denoising process $G$ parameterized by $\theta$ is optimized to synthesize the human skeleton sequence $\bm{x}$, which is further conditioned on the speech audio sequence $\bm{a}$ and the initial poses $\{\bm{p}_1, \dots, \bm{p}_M\}$ of the first $M$ frames. 
The learning objective of the overall framework can be formulated as
$\mathop{\arg\min}_{\theta} || \bm{x} - G_{\theta} ( \bm{a}, \bm{p}_1, \dots, \bm{p}_M) ||.$

\subsection{Gesture Space Forward and Reverse Process}
\label{sec:3.2}
Given $\bm{x}_0 \in \mathbb{R}^{N \times 3(J-1)}$ sampled from real data distribution $ q(\bm{x}_0)$, our goal is to learn a model distribution $p_{\theta}(\bm{x}_0)$ parameterized by $\theta$ that approximates $q(\bm{x}_0)$. Specifically, denoising diffusion probabilistic models (DDPMs)~\cite{ho2020denoising} define the latent variable models of the form $p_{\theta}(\bm{x}_0) = \int p_{\theta}(\bm{x}_{0:T}) d\bm{x}_{1:T}$, where $\bm{x}_{1:T}$ are latent variables in the same sample space as $\bm{x}_0$ with the same dimensionality.

\noindent\textbf{The Forward Diffusion Process.} The \textit{forward process}, which is also termed as the \textit{diffusion process}, approximates the posterior distribution $q(\bm{x}_{1:T}|\bm{x}_0)$. It is defined as a Markov chain that gradually adds Gaussian noise to the data sample according to a variance schedule $\beta_1, \dots, \beta_T$:
\begin{align} \label{eq:forward}
q(\bm{x}_{1:T} | \bm{x}_0) &= \prod_{t=1}^T q(\bm{x}_t | \bm{x}_{t-1}),\\
\textit{where }\quad q(\bm{x}_t | \bm{x}_{t-1}) &= \mathcal{N}(\bm{x}_t; \sqrt{1 - \beta_t}\bm{x}_{t-1}, \beta_t \bm{I}).
\end{align} 

The variances $\beta_t$ are constant hyperparameters to ease the modeling of the reverse process~\cite{ho2020denoising}. Through such a corruption scheme, the structural information of the original skeleton is gradually substituted by noises, which finally leads to a pure white noise when $T$ goes to infinity. Therefore, the prior latent distribution of $p(\bm{x}_T)$ is $\mathcal{N}(\bm{x}_T; \bm{0}, \bm{I})$ with only information of Gaussian noise.

\noindent\textbf{Reverse Conditional Gesture Generation.} The \textit{reverse process}, which is also termed as the \textit{generative process}, estimates the joint distribution of $p_{\theta}(\bm{x}_{0:T})$. As proved in~\cite{Feller1949OnTT}, the reverse process of the \textit{continuous} diffusion process preserves the same transition distribution form, which motivates us to leverage a Gaussian transition to formulate $p_{\theta}(\bm{x}_{t-1} | \bm{x}_{t})$ under an unconditional setting, which approximates the intractable process as:
\begin{align} 
\label{eq:reverse}
p_{\theta}(\bm{x}_{0:T}) &= p_{\theta}(\bm{x}_T) \prod_{t=1}^T p_{\theta}(\bm{x}_{t-1} | \bm{x}_{t}),\\
\textit{where }p_{\theta}(\bm{x}_{t-1}|\bm{x}_{t}) &= \mathcal{N}(\bm{x}_{t-1}; \mu_{\theta}(\bm{x}_t, t), \Sigma_{\theta}(\bm{x}_t, t)).
\label{eq:reverse2}
\end{align} 
The corrupted noisy data $\bm{x}_t$ is sampled by $q(\bm{x}_t | \bm{x}_0) = \mathcal{N}(\bm{x}_t; \sqrt{\bar{\alpha}_t}\bm{x}_0, (1 - \bar{\alpha}_t) \bm{I})$, where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$. Note that we set the variances $\Sigma_{\theta}(\bm{x}_t, t)=\beta_t \bm{I}$ to untrained time-dependent constants.
 The above diffusion model formulations show compelling performances on unconditional generation. To further adapt to the conditional co-gesture synthesis, we have to provide additional inputs to the model, including the audio and initial poses. Therefore, we treat the speech audio $\bm{a}$ and initial poses $\bm{p}_{1:M}$ as context information $\bm{c}$ and inject conditions into the generation process. The reverse process of each timestep (Eq.~\ref{eq:reverse2}) can be thus updated as:
\begin{align} \label{eq:conditional}
p_{\theta}(\bm{x}_{t-1}|\bm{x}_{t}, \bm{c}) &= \mathcal{N}(\bm{x}_{t-1}; \mu_{\theta}(\bm{x}_t, t, \bm{c}), \beta_t \bm{I}).
\end{align} 
In this way, we could start the generation process by firstly sample a Gaussian noise $\bm{x}_T \sim \mathcal{N}(\bm{0}, \bm{I})$ and follow the Markov chain to iteratively denoise the latent variable $\bm{x}_t$ via Eq.~\ref{eq:conditional} to get the final results. The overview of conditional co-speech gesture process is illustrated in Figure~\ref{overview}.

\noindent\textbf{Training Objective.} To optimize the overall framework, we optimize the variational lower bound on negative log-likelihood: $\mathbb{E}[-\log p_{\theta}(\bm{x}_0)] \leq \mathbb{E}_{q}[-\log \frac{p_{\theta}(\bm{x}_0)}{q(\bm{x}_{1:T} | \bm{x}_0)}].$
We rewrite the loss function conditioned on context $\bm{c}$ and eliminate all the constant items that do not require training: 
$L(\theta) = \mathbb{E}_{q}[\sum_{t=2}^T D_{KL} (q(\bm{x}_{t-1} | \bm{x}_t, \bm{x}_0) || p_{\theta}(\bm{x}_{t-1} | \bm{x}_t, \bm{c}))]$.
With reparameterization, we can represent each term in $L_\theta$ using MSE loss. We follow~\cite{ho2020denoising} to further simplify the training objective to the ensemble of MSE losses as:

\begin{align}
    L (\theta) = \mathbb{E}_{q}[\left\| \bm{\epsilon} - \bm{\epsilon}_\theta(\sqrt{\bar\alpha_t} \bm{x}_{0} + \sqrt{1-\bar\alpha_t}\bm{\epsilon}, \bm{c}, t) \right\|^2],
\label{eq:losssimple}
\end{align}
where $t$ is uniformly sampled between 1 and $T$. 
As we jointly train the model under conditional and unconditional setting, a trainable masked embedding with probability $p_{\textit{uncond}}$ replaces context $\bm{c}$ and the diffusion model predicts the noise in the unconditional setting. The detailed principles will be discussed in Section~\ref{sec:3.5}.

\subsection{Diffusion Audio-Gesture Transformer}
\label{sec:3.3}
With the naive conditional generation scheme as specified in Section~\ref{sec:3.2}, we still confront a critical problem in the setting of co-speech gesture generation. Since $\bm{x}_0$ denotes the skeleton sequence of $N$ frames, there exists temporal dependency among the target sequence and context information, making it more complex than time-invariant tasks like image generation. Therefore, how to guarantee temporally coherent results in a non-autoregressive conditional generation process remains an unsolved problem.

In contrast to most previous studies that resort to recurrent networks~\cite{yoon2020speech, liu2022learning}, we propose to make use of the Transformer's strong capacity in sequential data modeling. Specifically, since the noisy gesture sequence $\bm{x}_t$ and the contextual information $\bm{c}$ align in the temporal dimension, we concatenate them in the feature channel. In this way, the skeleton and context condition of each frame serve as an individual token, which captures the long-term dependency by the self-attention mechanism:
\begin{align}
    \text{Attention}(\mathbf{Q, K, V}) = \text{softmax}(\frac{\mathbf{QK^\mathsf{T}}}{\sqrt{\ell}})\mathbf{V},
\label{eq:attention}
\end{align}
where $\mathbf{Q, K, V}$ are the query, key, and value matrix from input tokens, $\ell$ is the channel dimension, and $\mathsf{T}$ is the matrix transpose operation. Such a design also avoids severe error accumulation in autoregressive pipelines, enabling us to generate coherent gesture sequences.


\subsection{Diffusion Gesture Stabilizer}
\label{sec:3.4}

In DDPMs, the independent random variables $\bm{z}$ introduced at the sampling stage promote diversity and thus systematically improve the task performance. However, the variation in temporal dimension introduced by $\bm{z}$, especially when timestep $t$ is small in the reverse process, can have a negative effect on temporal consistency. At the inference stage, to achieve the trade-off between diversity and temporal consistency, we propose a novel Diffusion Gesture Stabilizer without extra training expenses under two \textbf{annealed} scenarios, where the term ``annealed'' means that the process is transitioned from high variance and entropy (hot) to low variance and entropy (cold). 

\noindent\textbf{Thresholding.} Since temporally independent Gaussian noises inevitably introduce inconsistency, restricting the temporal variation naturally helps to avoid inconsistency. And hard thresholding serves as an effective trick. In detail, we set a time threshold $t_0$, and then use the same $\bm{z} \in \mathbb{R}^{N \times C}$ in the naive sampling strategy for $t > t_0$ and set $\bm{z} = \{\bm{z}_0\}_{i=1}^{N}$ for $t \leq t_0$, where $z_0 \in \mathbb{R}^{C}$ follows $\mathcal{N}(\bm{0}, \bm{I})$ which do not introduce variation in the temporal dimension. 

\noindent\textbf{Smooth Sampling.} We further modify $\bm{z}(t)=\{\bm{z}_i(t)\}_{i=1}^{N}$ to be a smooth annealing version via variance-aware sampling. In the original sampling rule of DDPMs, \textit{i.i.d.}variables $\bm{z}_i(t)$ are sampled from $\mathcal{N}(\bm{0}, \bm{I})$. With smooth resampling, we first sample $\bm{z}_0(t) \sim \mathcal{N}(\bm{0}, \sigma^2_a(t)\bm{I})$ only once for each timestep $t$ in the reverse process, then given $\bm{z}_0(t)$, we sample $\bm{z}_i(t)|\bm{z}_0(t)\sim \mathcal{N}(\bm{z}_0(t), (1-\sigma^2_a(t))\bm{I})$ for $i \in \{1,\dots,N$\}, where $\sigma_a(t) \in 	\left[0,1\right]$ is a non-decreasing function to achieve variance annealing.

\input{pseudocode/train}

\subsection{Implicit Classifier-free Guidance}
\label{sec:3.5}
In co-speech gesture literature, the speech-to-gesture mapping is implicit, where the same audio corresponds to diverse gestures and different audios could incur the same motion~\cite{lee2021crossmodal, li2021audio2gestures}, making it difficult to utilize the commonly used explicit classifier guidance~\cite{dhariwal2021diffusion, nichol22glide}. Therefore, 
how can we further exploit practical guidance for better audio correlations and mode coverage? Our solution to this question is to train an extra ``unconditional'' diffusion model to implicitly guide the generation. Dhariwal \textit{et al.}~\cite{dhariwal2021diffusion} first introduce the classifier guidance of cross-entropy gradient $\nabla_{\bm{x}_t}\log p_{\phi}(y|\bm{x}_t)$, where the pretrained classifier is parameterized by $\phi$ and $y$ denotes the classification logits. This gradient term is further scaled by the covariance matrix to modify the mean value of transition distribution in Eq.~\ref{eq:reverse2}. To adapt to the cases where no explicit guidance is available, we follow~\cite{ho2021classifierfree} to jointly train the conditional and unconditional models, termed as classifier-free guidance. In particular, according to the implicit classifier's property that $p^i(y|\bm{x}_t) \propto p(\bm{x}_t | y) / p(\bm{x}_t)$, we could derive a gradient relationship in the implicit classifier as:
\begin{align}
    \nabla_{\bm{x}_t}\log p^i(y|\bm{x}_t) \propto \nabla_{\bm{x}_t}\log p(\bm{x}_t|y) - \nabla_{\bm{x}_t}\log p(\bm{x}_t),
\label{eq:guidance}
\end{align}
which is further proportional to $\bm{\epsilon}^*(\bm{x}_t | y) - \bm{\epsilon}^*(\bm{x}_t)$. Therefore, we use a single Transformer network to parameterize both settings by a mix-up training trick: for the probability of $p_{uncond}$, we set the context information $\bm{c}$ as masked embedding to train the unconditional setting, while for other cases, we train the original conditional counterpart. The training is shown in Algorithm~\ref{alg:training}.


\noindent\textbf{Sampling with Classifier-free Guidance.} Starting from Gaussian noise, we iteratively remove noises in $x_t$. As we use implicit classifier-free guidance, similar to Equation~\ref{eq:guidance}, the predicted Gaussian noise is modified as:
\begin{align}
   \hat{\bm{\epsilon}}_\theta = \bm{\epsilon}_\theta(\bm{x}_t, t) + s \cdot (\bm{\epsilon}_{\theta}(\bm{x}_t, \bm{c}, t) - \bm{\epsilon}_\theta(\bm{x}_t, t)),
\label{eq:epsmodified}
\end{align}
where $s$ is the scale parameter to trade off the diversity and quality. With classifier-free guidance, Algorithm~\ref{alg:sampling} reveals how to generate co-speech gestures given the trained diffusion model via the Diffusion Gesture Stabilizer with the Smooth Sampling annealed scenario.

\input{pseudocode/sampling}