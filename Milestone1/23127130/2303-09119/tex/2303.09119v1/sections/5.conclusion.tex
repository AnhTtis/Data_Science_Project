\section{Conclusion}
\label{sec:conclusion}

In this work, we present a novel diffusion-based framework \textbf{DiffGesture} for co-speech gesture generation. To generate coherent gestures with strong audio correlations, we propose the Diffusion Audio-Gesture Transformer with the Diffusion Gesture Stabilizer to better attend to the condition information. Such a non-autoregressive pipeline helps to efficiently generate results and reduce error accumulation. We hope our method offers a new perspective for diffusion-based temporal generation and how to capture sequential cross-modal dependencies.

\noindent \textbf{Acknowledgement.}
The work described in this paper was partially supported by grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (T45-401/22-N), the National Natural Science Fund (62201483) and HKU Seed Fund for Basic Research (202009185079 and 202111159073).
