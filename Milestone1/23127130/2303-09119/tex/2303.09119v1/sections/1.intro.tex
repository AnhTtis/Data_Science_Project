\section{Introduction}\label{sec:intro}

Making co-speech gestures is an innate human behavior in daily conversations, which helps the speakers to express their thoughts and the listeners to comprehend the meanings~\cite{cassell1999speech, mcneill2011hand, 2014Gesture}. 
%
Previous linguistic studies verify that such non-verbal behaviors could liven up the atmosphere and improve mutual intimacy~\cite{burgoon1990nonverbal, 1989Gesture, huang2012robot}.
%
Therefore, animating virtual avatars to gesticulate co-speech movements is crucial in embodied AI. 
To this end, recent researches focus on the problem of audio-driven co-speech gesture generation~\cite{ginosar2019learning, yoon2020speech, liu2022learning, li2021audio2gestures}, which synthesizes human upper body gesture sequences that are aligned to the speech audio.

Early attempts downgrade this task as a searching-and-connecting problem, where they predefine the corresponding gestures of each speech unit and stitch them together by optimizing the transitions between consecutive motions for coherent results~\cite{cassell1994animated, huang2012robot, marsella2013virtual}. 
%
In recent years, the compelling performance of deep neural networks has prompted data-driven approaches. 
%
Previous studies establish large-scale speech-gesture corpus to learn the mapping from speech audio to human skeletons in an end-to-end manner~\cite{alexanderson2020style, liu2022beat, xu2022freeform, qian2021speech, liu2022learning, li2021audio2gestures, ao2022rhythmic}. 
%
To attain more expressive results, Ginosar \textit{et al.}~\cite{ginosar2019learning} and Yoon \textit{et al.}~\cite{yoon2020speech} propose GAN-based methods to guarantee realism by adversarial mechanism, where the discriminator is trained to distinguish real gestures from the synthetic ones while the generator's objective is to fool the discriminator. 
%
However, such pipelines suffer from the inherent mode collapse and unstable training, making them difficult to capture the \textit{high-fidelity audio-conditioned} gesture distribution, resulting in dull or unreasonable poses.

\begin{figure}[t]
\centering
\includegraphics[width=1.00\columnwidth]{figure/diffusion_process.pdf}
\caption{\textbf{Illustration of Conditional Generation Process in Co-Speech Gesture Generation.} The diffusion process $q$ gradually adds Gaussian noise to the gesture sequence (\textit{i.e.}, $\bm{x}_0$ sampled from the real data distribution). The generation process $p_{\theta}$ learns to denoise the white noise (\textit{i.e.}, $\bm{x}_T$ sampled from the normal distribution) conditioned on context information $\bm{c}$. Note that $\bm{x}_t$ denotes the corrupted gesture sequence at the $t$-th diffusion step.}
\label{overview}
\vspace{-0.2cm}
\end{figure}

The recent paradigm of diffusion probabilistic models provides a new perspective for realistic generation~\cite{ho2020denoising, song2021scorebased}, facilitating high-fidelity synthesis with desirable properties such as good distribution coverage and stable training compared to GANs.
%
However, it is non-trivial to adapt existing diffusion models for co-speech gesture generation. 
%
Most existing conditional diffusion models deal with \textit{static} data and conditions~\cite{Saharia2022Photorealistic, ramesh2022hierarchical} (\textit{e.g.}, the image-text pairs without temporal dimension), while co-speech gesture generation requires generating \textit{temporally coherent} gesture sequences conditioned on continual audio clips.
%
Further, the commonly used denoising strategy in existing diffusion models samples independently and identically distributed (\textit{i.i.d.}) noises in latent space to increase diversity. However, this strategy tends to introduce variation for each gesture frame and lead to temporal inconsistency in skeleton sequences. 
%
Therefore, how to generate high-fidelity co-speech gestures with strong audio correlations and temporal consistency is quite challenging within the diffusion paradigm.

To address the above challenges, we propose a tailored Diffusion Co-Speech Gesture framework to \textit{capture the cross-modal audio-gesture associations while maintaining temporal coherence} for high-fidelity audio-driven co-speech gesture generation, named \textbf{DiffGesture}.
%
As shown in Figure~\ref{overview}, we formulate our task as a diffusion-conditional generation process on clips of skeleton and audio, where the diffusion phase is defined by gradually adding noise to gesture sequence, and the generation phase is referred as a parameterized Markov chain with conditional context features of audio clips to denoise the corrupted gestures. 
%
As we treat the multi-frame gesture clip as the diffusion latent space, the skeletons can be efficiently synthesized in a non-autoregressive manner to bypass error accumulation.
%
To better attend to the sequential conditions from multiple modalities and enhance the temporal coherence, we then devise a novel \textit{Diffusion Audio-Gesture Transformer} architecture to model audio-gesture long-term temporal dependency.
%
Particularly, the per-frame skeleton and contextual features are concatenated in the aligned temporal dimension and embedded as individual input tokens to a Transformer block.
%
Further, to eliminate the temporal inconsistency caused by the naive denoising strategy in the inference stage, we thus propose a new \textit{Diffusion Gesture Stabilizer} module to gradually anneal down the noise discrepancy in the temporal dimension.
%
Finally, we incorporate implicit classifier-free guidance by jointly training the conditional and unconditional models, which allows us to trade off between the diversity and sample quality during inference.

Extensive experiments on two benchmark datasets show that our synthesized results are coherent with stronger audio correlations and outperform the state-of-the-arts with superior performance on co-speech gesture generation.
%
To summarize, our main contributions are three-fold: \textbf{1)} As an early attempt at taming diffusion models for co-speech gesture generation, we formally define the diffusion and denoising process in gesture space, which synthesizes audio-aligned gestures of high-fidelity. \textbf{2)} We devise the \textit{Diffusion Audio-Gesture Transformer} with implicit classifier-free diffusion guidance to better deal with the input conditional information from multiple sequential modalities. \textbf{3)} We propose the \textit{Diffusion Gesture Stabilizer} to eliminate temporal inconsistency with an annealed noise sampling strategy. 