% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.

\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
% \usepackage{stfloats}
\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{graphics}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{booktabs} % For formal tables
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{ulem}
\usepackage{float}
\usepackage{makecell}
%\usepackage{graphicx}

\usepackage{appendix}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.



\newtheorem{thm}{Theorem}
\newtheorem{observation}[thm]{Observation}

\newcommand{\ie}{\textit{i.e.,}\xspace}
\newcommand{\eg}{\textit{e.g.,}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\paratitle}[1]{\vspace{0.8ex}\noindent \textbf{#1}}
\newcommand{\camera}{CameraReview\xspace}
\newcommand{\compsentliu}{CompSent-08\xspace}
\newcommand{\compsentalex}{CompSent-19\xspace}
\newcommand{\basemodel}{PLM\;\textit{w.}\;Prompt\xspace}
\newcommand{\modelname}{GCRE-GPT\xspace}




\title{GCRE-GPT: A Generative Model for Comparative Relation Extraction }
 

\author{Yequan Wang$^{1}$, Hengran Zhang$^{2,3}$, Aixin Sun$^{4}$, Xuying Meng$^{2}$\\
$^{1}$Beijing Academy of Artificial Intelligence, Beijing, China\\
$^{2}$Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\\
$^{3}$University of Chinese Academy of Sciences, Beijing, China\\
$^{4}$School of Computer Science and Engineering, Nanyang Technological University, Singapore\\
tshwangyequan@gmail.com, axsun@ntu.edu.sg,  \{zhanghengran22z,mengxuying\}@ict.ac.cn
}
\usepackage{threeparttable}
\begin{document}
\maketitle

\begin{abstract}
Given comparative text, comparative relation extraction aims to extract two targets (\eg two cameras) in comparison and the aspect they are compared for (\eg image quality).  The extracted comparative relations form the basis of further opinion analysis.
%of finding the relative opinion preference.
% It is important for the comparative sentiment classification. 
Existing solutions formulate this task as a sequence labeling task, to extract targets and aspects. However, they cannot directly extract comparative relation(s) from text. In this paper, we show that comparative relations can be directly extracted with high accuracy, by generative model. Based on GPT-2, we propose a Generation-based Comparative Relation Extractor (GCRE-GPT). Experiment results show that \modelname achieves state-of-the-art accuracy on two datasets. %, and our model is able to extract multiple relations in a sentence.
% Comparative relations extraction can simplify comparative sentence and it's crucial for opinion mining. 
% Current solutions mainly use pipelined models. But pipelined models have error propagation problem. 
\end{abstract}

%===========================
\section{Introduction}
%===========================

Comparison is a common linguistic expression. \citet{DBLP:conf/emnlp/KesslerK13} reports that $10\%$ of texts contain at least one comparison. A typical comparative relation contains two targets and one aspect, \eg ``D80 vs. D70 in terms of weight'' where the two targets are two cameras and the aspect to compare is weight.   
An extracted comparative relation is the basis for opinion mining in further analysis.
Hence, it is crucial to accurately extract comparative relations, to support product/service comparison to better inform consumers and enterprises.   

%Comparative texts on various products, enterprises and consumers can make more competitive and scientific decisions.
% Comparative relation extraction aims to extract all comparative relations.

% (\ie entities and corresponding aspects) and analyze comparative sentence opinion towards them. 


% \input{tables/task_representation}
% In existing studies, comparative relation has various expressions, which are shown in Table~\ref{tab:task_representation}. Most of the works considers opinion words as part of the comparison relationship. But opinion words isn't necessary for comparative opinion mining ~\cite{DBLP:conf/argmining/PanchenkoBFHB19}. In addition, extracting more elements, the task will be more complex.


%To simply express the question, here we make a unified definition for comparative relation extraction. 

Given a piece of text (\eg a sentence), denoted by $S = [w_1, w_2, \dots, w_n]$, the task  of \textit{comparative relation extraction} is to predict the comparative relation(s) expressed in the given text. Each relation is a 3-tuple: $(t_1, t_2, a)$, where $t_1$ and $t_2$ are targets in comparison, and $a$ denotes the corresponding aspect for which the two targets are compared. In some studies, the two targets are further classified into subject and object~\cite{DBLP:conf/emnlp/LiuXY21,DBLP:conf/cikm/AroraAGP17,DBLP:conf/emnlp/KesslerK13,DBLP:conf/aaai/JindalL06,DBLP:journals/comsis/WangXWHL17}. In our problem definition, we do not distinguish such roles between the two targets.

% Consider the task is to be conducted on camera reviews. Given a sentence ``\textit{I think the sound quality of this phone is slightly better than my samsung phone , the motorolla t720 series.}'',  we can extract comparative relations, \ie \textit{(this phone, my samsung phone,  sound quality), and (this phone,the motorolla t720 series,  sound quality)}.


Existing solutions for comparative relation extraction use rules or sequence labeling models. 
\citet{DBLP:conf/aaai/JindalL06} extract pre-defined comparative tuples using rules based on opinion words. \citet{DBLP:conf/emnlp/KesslerK13} recognize comparative predicate by semantic role labeling method. 
Furthermore, CRF~\cite{DBLP:journals/comsis/WangXWHL17} and LSTM~\cite{DBLP:conf/cikm/AroraAGP17} have also been used to extract the comparative elements.
\citet{DBLP:conf/emnlp/LiuXY21} use sequence labeling method to obtain elements, then use Cartesian product to compute all relations. Here, multiple relations in a single sentence may be combined by Cartesian product. Nevertheless, multiple relations in long texts (\eg a paragraph) may not be logically combined. Although sequence labeling models perform well at sentence level, they cannot directly extract relation tuples. Instead, they extract targets and aspects and combine them into relation tuples. The permutation and combination of elements bring in a lot of complexity.
More importantly, many reviews in real life contain more than one relation.
This calls for a fundamentally different approach to extract comparative relations.


Pre-trained language models, \eg GPT~\cite{radford2019language} and BERT~\cite{DBLP:conf/naacl/DevlinCLT19}, have enabled generative models to achieve promising results on many tasks. Examples include  machine translation~\cite{DBLP:journals/taslp/ZhangLSZX021,DBLP:conf/acl/TuLLLL16}, question answer~\cite{DBLP:conf/emnlp/DuanTCZ17}, and generation-based dialog system~\cite{DBLP:conf/acl/TuLC0W022}. In this paper, we make the first attempt to extract comparative relations with generative model.
%we take the first step using the generation way to solve the difficulty caused by multiple relations. 
Specifically, we propose \textbf{G}eneration-based \textbf{C}omparative \textbf{R}elation \textbf{E}xtractor based on \textbf{GPT}-2, namely \modelname, to extract comparative relations from text. 
%Aixin: I do not understand the next sentence, "rule" is strange here. 
We represent a comparative relation tuple in a piece of text, and train our model to generate such texts, based on an input sentence. Experiments show that \modelname achieves state-of-the-art performance. Further, \modelname performs best on texts containing multiple relations.


%Aixin: this part is repeating, suggest to remove
%To summarize, we make the first attempt to extract comparative relations by \modelname, which is based on generation method. More importantly, the proposed method overcomes the difficulty caused by multiple relations fundamentally. Experiment results show that our proposed model achieves state-of-the-art performance, especially in texts with multiple relations.


% for comparative relation extraction, using generative tasks to accomplish comparative relation extraction. There are two restrictions during generation: Format Restriction and  Prompt words injection.


%===========================
\section{Related Work}
%===========================
Most existing studies cast the task of comparative relation extraction as a task of  comparative element extraction. Relations are then built on top of the extracted elements.  Accordingly, many solutions are based on sequence labeling models~\cite{DBLP:conf/aaai/JindalL06, DBLP:conf/emnlp/KesslerK13, DBLP:journals/comsis/WangXWHL17, DBLP:conf/cikm/AroraAGP17, DBLP:conf/emnlp/LiuXY21}. These models are fundamentally different from ours, as we adopt the generative approach with pre-trained language models. 


 
As a well known pre-training language model, BERT~\cite{DBLP:conf/naacl/DevlinCLT19} has contributed to significant performance increase on various NLP tasks. Based on BERT, \citet{DBLP:conf/emnlp/LiuXY21} propose a multiple stage model to analyze comparative opinion. The comparative elements are firstly extracted, followed by building comparative relations. We instead adopt a generative approach, built on top of GPT-2. GPT-2~\cite{radford2019language} adopts generic transformer structure~\cite{DBLP:conf/acl/ZhangSGCBGGLD20} and encodes text from left to right. The auto-regressive and unidirectional structure make it a strong performer on generation tasks. 

% \citet{DBLP:journals/corr/abs-2103-10360} reformulate downstream tasks as the blank-filling generation, which surprisingly outperforms other Large-scale language models.
% T5~\cite{} converts all NLP tasks to generation tasks. In detail, T5 uses prefix descriptions to fit various tasks. For example, the translation task adds ``translate English to German'' as prompt words to translate English.  So it also can change the comparative relation extraction task to a sequence-to-sequence task. 



%===========================
\section{\modelname}
\label{sec:model}
%===========================
The architecture of the proposed \modelname is depicted in Figure~\ref{fig:model}. 
Rooted in GPT-2, \modelname has two main modules: (i) encoder with prompt words, and (ii) comparative aware decoder.
Next, we detail the design of \modelname and its optimization objective.


% The  \textbf{G}enerated-based \textbf{C}omparative \textbf{R}elation \textbf{E}xtractor based on \textbf{GPT}~(\modelname) innovatively use prompt words injection.  Figure \ref{fig:model}  depicts  the  architecture  of \modelname. Based on GPT-2 model, \modelname has three modules:\textit{Content Restriction},  \textit{Format Restriction} and \textit{Prompt Words}. As the name suggests, \textit{Content Restriction} is to limit the generated words. \textit{Format Restriction} is to standardize the generated  comparative relations format.  \textit{Prompt Words} amis to stimulate the model to generate matching comparative relations.

%===========================
\subsection{Encoder with Prompt Words}
%===========================

To better extract comparative relations, we design a simple yet effective prompt words injection layer. Given an input sentence, $S=[w_1, w_2, \cdots, w_n]$, we use prompt words injection layer to obtain a new input:
\begin{equation}
    S' = [w_1, w_2, \cdots, w_n, t_p],
\end{equation}
where $t_p$ denotes the injected span of prompt words. Then we represent this input by using $\text{Encoder}$:
\begin{equation}
    r = \text{Encoder}([w_1, w_2, \cdots, w_n, t_p]),
\end{equation}
where $r$ refers to the encoded representation of $S'$.


\begin{figure}
    \centering
    \includegraphics[scale=0.45]{figures/framework.eps}
%     \caption{
%   An instance of prompt words injection.  ``sentences:'' and ``relation:'' are prompt words and prompt words can be changed.  When training the model, the sentence and relations are separated by prompt words. When testing, sentence and prompt words as input and model generate relations. $c$ is the hidden vector. $p^c$ is probability distribution. 
%     }
    \caption{The architecture of \modelname.}
    \label{fig:model}
    % \vspace{-2ex}
\end{figure}


%===========================
\subsection{Comparative Aware Decoder}
%===========================
The comparative aware decoder is built on top of the decoder in GPT-2. The main add-on to the original decoder is a filter layer. Next, we  detail the format of the generated text, then explain the filter layer.

%The remaining is to properly utilize the representation $r$ and the well-designed decoding strategy. To notify the decoder of this information, we design a filter layer except for the original decoder of GPT-2. 

\paratitle{Format of the generated text.}
To better utilize the capability of text generation of pre-trained language models, we formulate our task of comparative relation extraction as a text generation task. 
%Pre-trained language models have significant ability to generate high-quality text, so we transfer the comparative relations to natural language text by a simple rule. 
Specifically, a target comparative relation $(t_1, t_2, a)$ to be extracted from input text, becomes a target piece of text ``$t_1$ vs. $t_2$ in $a$'' to be generated by the model. %This approach allows our model 
Then, we use the decoder to generate text word by word, starting with a [CLS] token (see Figure~\ref{fig:model}):
\begin{equation}
    \mathcal{P}_i = p(r_i|\textbf{r}_{j < i}, S'),
\end{equation}
where $\mathcal{P}_i$ denotes the probability distribution of the predicted $i$-th word, computed based on the input and the earlier generated sequence. The corresponding word $w_i$ is obtained by $\text{argmax}(\mathcal{P}_i)$.


\paratitle{The filter layer.}  A typical generative model generates words from its entire vocabulary. Hence, the model will output words that are not in the original input text. In our problem setting, these words are not part of the expected results. 
% So, when the model generates a relation, it determines whether each element of the relation belongs to the original text. 
To guarantee that all elements of the generated relations are from the original text, we design a filter layer, to filter all relations generated:
\begin{equation}
    S_g = [\cdots, \text{rel}_i, \cdots],
\end{equation}
where $\text{rel}_i$ denotes the $i$-th generated relation and all elements of it (\ie two targets $t_1$ $t_2$, and aspect $a$) must be from the original text $S$. Otherwise, the generated relation is discarded.


%===========================
\subsection{Training Objective}
%===========================


The objective of the \modelname is to predict the transferred comparative relation, from the input text. We adopt objective $L(\theta)$ by simply using the auto-regressive cross-entropy:
\begin{equation}
    L(\theta) = -\sum_{i=1}^{N_c} \log(p(r_i|\textbf{r}_{j < i}, S')),
\end{equation}
where $\theta$ denotes the parameters of the model. $N_c$ represents the total length of the transferred comparative relations.


As discussed earlier, a sentence may contain  multiple comparative relations, making the generation problem complicated. In this paper, we use a simple way to guide the proposed model. For simple comparative relation, the order of targets $t_1$ and $t_2$ follows their relative positions in the original text. If there are multiple relations, we prioritize $t_1$ then $t_2$, if $t_1$ is consistent in multiple relations.

% \paratitle{Preliminary: GPT-2.}
% GPT-2 transformer model adopts generic Transformer structure~\cite{DBLP:conf/acl/ZhangSGCBGGLD20} and encode the text from left to right. The process of generating token is:
% \begin{equation}
%     p(x) = p(x_n|x_1, x_2,\dots,x_{n-1}),
% \end{equation}
% where the loss is calculated by autoregressive cross-entropy. For comparative relations extraction, when  training process, comparative sentence, and relations are concatenated with some special words, and when testing process, only the comparative sentence as the input and GPT-2 generate relations step by step.


% \subsection{Content Restriction}

% Due to the characteristic of the generative model, the model is likely to generate words that do not belong to the sentence. So we restrict all the words in the generated relation are included in the sentence.


% \subsection{Format Restriction}

% Traditional generative tasks aim to generate affluent sentences as output, such as machine translation, and summary generation. But the sentence generated on comparative relations extraction may not conform to grammatical norms. In order to generate affluent sentence, We set a format restriction, having a relation tuple $(t_1, t_2, aspect)$, changing into ``$t_1$ vs. $t_2$ in $aspect$;''

% For example, ``D80 has worse battery life than D70s'' has a relation (D80, D70s, battery life) will be changed into ``D80 vs. D70s in battery life''. If there is more than one relation, multiple relations will be separated by ``;''.

% \subsection{Prompt Words}
% Appropriate prompt words can help the model generate comparative relations. So, our method injects prompt words into the generation model. \ref{fig:model} shows the prompt words injection. The result shows that prompt words can stimulate the modelâ€™s ability to generate comparative tuples. We select GPT-2~\cite{radford2019language} as  base generation model. 




%===========================
\section{Experiments}
%===========================

\input{tables/datasets}

%===========================
\subsection{Dataset}
\label{sec:exp:dataset}
%===========================

\paratitle{\camera.} This is a manually annotated dataset, containing camera reviews~\cite{DBLP:conf/lrec/KesslerK14}.\footnote{\url{https://wiltrud.hwro.de/research/data/reviewcomp and contanarisons.html}} \camera is currently the largest dataset of comparative sentences, containing $1,279$ comparative sentences and $1,780$ relations. The comparative relation is in the format of (subject, object, opinion words). In this dataset, 74.4\% sentences each contains one relation, 17.4\% sentences contain two relations each, and remaining sentences contain more than two relations each. 

\paratitle{\compsentliu.}  This dataset is  constructed by~\citet{DBLP:conf/coling/GanapathibhotlaL08}, which  contains both comparative and non-comparative sentences.\footnote{\url{https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\#datasets}} We select the comparative sentences from this dataset in our experiments. In this dataset, 85.7\% sentences each contains one relation, 11.6\% contain two relations and the remaining contain more than two. 

For both datasets, we randomly split the instances with the ratio of $\text{7:1:2}$ for training, validation/development, and testing. Table~\ref{tab:datasets} reports dataset statistics. 

%The statistics of those two datasets show $80\%$ of the data in the dataset has a comparative relation. Hence, to study the effect of multiple comparative relations, we concatenate the sentences with single relation to imitate multiple relations.
% , we find the number of relations is greater than the number of the sentence. 
% So some sentences have more than one relation. 


%======================
\subsection{Compared Methods}
%======================
We compare \modelname with the following baselines, CRF~\cite{DBLP:journals/comsis/WangXWHL17}, BERT~\cite{DBLP:conf/naacl/DevlinCLT19}, BERT-CRF. All these models are pipeline models: the models extract comparative elements as a sequence labeling task, and then build comparative relations by using Cartesian product. 

We also compare \modelname with GPT-2, the base model of \modelname. Note that, we do not compare our model with traditional rule-based solutions as re-producing such models are time consuming. Due to the small dataset size, we do not train LSTM-based models as well, because LSTM in general requires a lot of training data. 

%as users' writing styles are varied, the prediction of comparative relations~(\ie targets and aspect) in a comparison sentence is a difficult task~\cite{DBLP:conf/cikm/AroraAGP17}. 
%Further, existing comparative datasets in this domain are relatively small. And LSTM generally requires more data to train, so we choose a pre-trained language model instead. BERT~\cite{DBLP:conf/naacl/DevlinCLT19} is used as the baseline because it performs well on few-shot scenarios. We also use CRF~\cite{DBLP:journals/comsis/WangXWHL17}, BERT-CRF~\cite{DBLP:conf/emnlp/LiuXY21} as baseline models. 



%===========================
\subsection{Overall Performance}
%===========================


We use Precision, Recall and $F1$ of extracted relation to evaluate the accuracy of all models. It is worth noting that the two targets in our defined comparative relation are unordered.
% When select three evaluation indexes, including \textit{Relation-pre}, \textit{Relation-rec}, \textit{Relation-$F1$}. 
% In order to unify the evaluation criteria of all models, the output of  all the model is $\{(\{t_1,t_2\},a),(\{t_1,t_2\},a),\dots\}$. When calculating the model outputs, $t_1$ and $t_2$ are unordered. 
An extracted comparative relation is marked as correct, when both targets and the aspect are all correct, otherwise is marked as wrong.
Table~\ref{tab:main_resualt} reports the results of \modelname and 4 baselines, on both datasets.


\input{tables/main_result}


\input{tables/different_number_relation}


%\paratitle{Discussion.} 
On both datasets, our proposed \modelname  achieves the best $F1$. 
On \camera, BERT-based models outperform CRF model,  revealing the powerful ability of pre-trained language models, much as expected. Because BERT-based models first extract comparative elements,  then build relations by combining the extracted elements, these models produce many relations, leading to higher recall but lower precision, compared to generative models GPT-2 and our model.  
On \compsentliu, all pipeline models produce low accuracy, compared to generative models. A key reason is the small dataset size. Sequence labeling methods typically need many instances for training, and \compsentliu does not provide sufficient training data. 


%As expected, \modelname performs better than general GPT-2. More interestingly, \modelname outperforms the pipeline model by about $10\%$.


%===========================
\subsection{Number of Comparative Relations}
%===========================

Table~\ref{tab:different_number} reports a detailed look at the results on \camera dataset, a break down of results on sentences containing 1, 2, or more relations. 

\modelname achieves the best scores by all measures, on sentences with a single relation. However, our model does not perform well on sentences containing 2 or more relations. Recall that,  in \camera dataset, majority (or 74.4\%) sentences contain only one relation. Hence the model is learned to focus on one relation extraction only, for a given sentence. 

To more properly evaluate the model capability of handling multiple relations in the input text, we augment the model training by concatenating a pair of sentences  in training set, to form a longer text with multiple relations. Then we evaluate the model on sentences with two or more relations in \camera. The results are reported in Table~\ref{tab:document}. \modelname outperforms all baselines after training with augmented data with multiple relations in input text. 

%More importantly, Table~\ref{tab:document} reports that \modelname performs best among all methods. This is because the text of original data only contains one relation so the trained model tends to generate only one relation. Therefore, it is reflected in Table~\ref{tab:different_number} that the model does not predict well in multiple relations. As expected, \modelname performs best when encountering true multi-relation, shown in Table~\ref{tab:document}.



%the performance of all models considering the number of comparative relations. Because more than $80\%$ of the comparative sentences contain only one relation, so we use the method in Section~\ref{sec:exp:dataset} to extend the ratios of texts with multiple relations. The result is shown in Table~\ref{tab:document}.

%\paratitle{Discussion.} Table~\ref{tab:different_number} shows \modelname achieves the best performance on the text with single relation. However, it doesn't perform as expected on multi-relational data. More importantly, Table~\ref{tab:document} reports that \modelname performs best among all methods. This is because the text of original data only contains one relation so the trained model tends to generate only one relation. Therefore, it is reflected in Table~\ref{tab:different_number} that the model does not predict well in multiple relations. As expected, \modelname performs best when encountering true multi-relation, shown in Table~\ref{tab:document}.


\input{tables/document_relation}


% \paratitle{Multiple Relations.} Comparative sentences have three types: single relation sentence, double relations sentence, and multiple relations sentence. But single relation sentences account for more than 80\% of the comparative sentence. We use the trained model to test three datasets, and the result is shown in table \ref{tab:different_number}. \modelname achieves the best performance on the single dataset. BERT-CRF performs best on the other datasets. Multiple relations of a single sentence are formed much like  Cartesian products.  Pipelined models
% use Cartesian products to get relations, so they have better performance on multiple relations sentences.

% \paratitle{Multiple Sentences.}There is more than one sentence in real reviews. So multiple relations in this situation are more common in real life. We combine two comparative sentences as an instance and retrain the BERT, BERT-CRF, GPT-2, and  \modelname. Result shows in table \ref{tab:document}.
% \modelname and GPT-2 have better performance than sequence labeling methods. It indicates generation model has better performance in multiple sentences.
% \modelname achieves the best performance. It demonstrates the effectiveness of prompt words in multi-relational sentences.



\input{tables/prompt_words}
%===========================
\subsection{Impact of Prompt Words}
%===========================


Existing studies on prompt-based models reveal that PLM models are heavily affected by the chosen prompt. In our experiments, we also study the impact of different prompt words on our model. Table~\ref{tab:prompt_words} reports the results.

%\paratitle{Discussion.} 
% Different prompt words have different effect on model. 
We list seven popular prompt words in Table~\ref{tab:prompt_words}, including ``generate relation:'', ``Let me see:'', ``My name:'', ``relations:'', ``comparative relation:'', ``[SEP]'' and ``comparative relation tuple:''. 
Among them, prompt ``[SEP]'' cannot guide the model to generate a comparison relationship linguistically. 
% only the role of separating sentences and relations. 
``Let me see:'' and ``My name:'' are less relevant to the task, leading to poor results as expected.
% is similar to ``[SEP]'', but at least it has semantic meaning. 
Prompt words containing ``comparative'' lead to better results than others. The prompt words ``comparative relation tuple:'' achieves the best performance on all measures.


 
%===========================
\section{Conclusion}
%===========================

In this paper, we study comparative relations extraction and propose \modelname model. The key idea of \modelname is to use generative method to extract complex multiple comparative relations from input text.
Experiment results show that our proposed model achieves state-of-the-art performance against strong baselines. More importantly, \modelname is able to handle texts containing multiple comparative relations, after trained with augmented data.

% Based on the power generation model, we inject prompt words into the sentence. 
% We innovatively use the generation model to directly extract comparative relations. To the best of our knowledge, it is the first approach to jointly extract comparative elements and comparative relations from sentences.


\clearpage
%============================
\section*{Limitations}
%============================


In this paper, we make the very first attempt to perform comparative relation extraction by generative way. 
There are two main limitations: data and model. 
The size of the datasets used in experiments are small, so we cannot design a model with more parameters to perform more in-depth research. Nevertheless, these two are the only two datasets publicly available to our best knowledge. 
Another limitation is the design of the model. This work only considers discrete prompt words and explores a small collection of prompt words to get a local optimum.

For the comparative relation extraction task, an important problem is the lack of comparative elements. Our work and existing studies both ignore this problem. We consider it is an important dimension to explore in future, because sometimes a lacked comparative element may refer to all remaining elements. For example, ``My iPhone 13 is the best''. The missing target here refers to all the remaining phones, and the missing aspect refers to all aspects in comparison. 



% From all results above, improving the generation model in multiple relations of a single sentence extraction is critical in the future. What's more, multiple relations of a single sentence account for very few in comparative sentences. Meanwhile, comparative sentence datasets are very small. So, constructing high-quality and sufficient comparative sentences is also an issue to be considered in the future.



\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}
\end{document}
