% mnras_template.tex 
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[fleqn,usenatbib]{mnras}

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}

% Allow "Thomas van Noord" and "Simon de Laguarde" and alike to be sorted by "N" and "L" etc. in the bibliography.
% Write the name in the bibliography as "\VAN{Noord}{Van}{van} Noord, Thomas"
\DeclareRobustCommand{\VAN}[3]{#2}
\let\VANthebibliography\thebibliography
\def\thebibliography{\DeclareRobustCommand{\VAN}[3]{##3}\VANthebibliography}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
% \usepackage{amssymb}	% Extra maths symbols
\usepackage{bm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{neuralnetwork}
\usepackage{tikz}
\usepackage{pgfplots}

\makeatletter
% \linklayers have \nn@lastnode instead of \lastnode,
% patch it to replace the former with the latter, and similar for thisnode
\usepackage{xpatch}
\xpatchcmd{\linklayers}{\nn@lastnode}{\lastnode}{}{}
\xpatchcmd{\linklayers}{\nn@thisnode}{\thisnode}{}{}
\makeatother
\input{nn_customization}


\usepackage{xspace}
\newcommand{\vrad}{\ensuremath{v_{\mathrm{rad}}}\xspace}
\newcommand{\teff}{\ensuremath{\rm{T}_{\mathrm{eff}}}\xspace}
\newcommand{\logg}{\rm{log\ g}\xspace}
\newcommand{\kms}{\ensuremath{\rm{km}\,s^{-1}}\xspace}
\newcommand{\feh}{\rm{[Fe/H]}\xspace}
\newcommand{\hem}{\rm{[He/M]}\xspace}
\newcommand{\xh}{\rm{[X/H]}\xspace}
\newcommand{\msun}{\rm{M_{\sun}}\xspace}
\newcommand{\lsun}{\rm{L_{\sun}}\xspace}
\newcommand{\afe}{\rm{[\alpha/Fe]}}
\newcommand{\mgfe}{\rm{[Mg/Fe]}\xspace}
\newcommand{\cms}{\ensuremath{\rm{cm~s^{-2}}}\xspace}
\newcommand{\tsf}{{\it TensorFlow}\xspace}  % TensorFlow
\newcommand{\tbb}{\ensuremath{T_{\mathrm{bb}}}\xspace}
\newcommand{\composition}{\circ}

\usepackage{makecell}

% Please add the following required packages to your document preamble:
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{pdflscape}
\usepackage{longtable}
\usepackage{multirow}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% macros for annoting the text during the preparation 
\usepackage{xcolor, ulem}
\newcommand\hl{\bgroup\markoverwith{\textcolor{yellow}{\rule[-.5ex]{2pt}{2.5ex}}}\ULon}
\newcommand{\noten}[1]{{\color{red}\textbf{#1}}}
\newcommand{\notenb}[1]{\hl{[nitesh: #1]}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[Generating theoretical RRab light curves using ANN]{Predicting light curves of RR Lyrae variables using artificial neural network based interpolation of a grid of pulsation models}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[N. Kumar et al.]{Nitesh Kumar,$^{1}$\thanks{E-mail: niteshchandra039@gmail.com}
Anupam Bhardwaj,$^{2}$
Harinder P. Singh,$^{1}$
Susmita Das,$^{3}$
Marcella Marconi, $^{2}$
\newauthor
Shashi M. Kanbur, $^{4}$
and Philippe Prugniel $^{5}$
\\
% List of institutions
$^{1}$Department of Physics and Astrophysics, University of Delhi, Delhi 110007, India \\ 
$^{2}$ INAF-Osservatorio Astronomico di Capodimonte, Salita Moiariello 16, 80131, Naples, Italy \\
$^{3}$ Konkoly Observatory, Research Centre for Astronomy and Earth Sciences, E\"{o}tv\"{o}s Lor\'and Research Network (ELKH), Konkoly-Thege Mikl\'os \'ut 15-17,\\H-1121, Budapest, Hungary\\ 
$^{4}$ Department of Physics and Earth Science, State University of Newyork at Oswego, Oswego, NY 13126, USA \\
$^{5}$ Universit\'e de Lyon, Universit\'e Lyon 1, 69622 Villeurbanne; CRAL, Observatoire de Lyon, CNRS UMR 5574, 69561 Saint-Genis Laval, France
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2022}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
We present a new technique to generate the light curves of RRab stars in different photometric bands ($I$ and $V$ bands) using Artificial Neural Networks (ANN). A pre-computed grid of models was used to train the ANN, and the architecture was tuned using the $I$ band light curves. The best-performing network was adopted to make the final interpolators in the $I$ and $V$ bands. The trained interpolators were used to predict the light curve of RRab stars in the Magellanic Clouds, and the distances to the LMC and SMC were determined based on the reddening independent Wesenheit index. The estimated distances are in good agreement with the literature. The comparison of the predicted and observed amplitudes, and Fourier amplitude ratios showed good agreement, but the Fourier phase parameters displayed a few discrepancies. To showcase the utility of the interpolators, the light curve of the RRab star EZ Cnc was generated and compared with the observed light curve from the {\it Kepler} mission. The reported distance to EZ Cnc was found to be in excellent agreement with the updated parallax measurement from Gaia EDR3. Our ANN interpolator provides a fast and efficient technique to generate a smooth grid of model light curves for a wide range of physical parameters, which is computationally expensive and time-consuming using stellar pulsation codes.
\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
stars: variables: RR Lyrae -- methods: data analysis ---- techniques: photometric
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}
RR Lyrae stars (RRLs) are old ($\geq10$ Gyr), population II stars which are in the core helium-burning evolutionary phase of low-mass stars. They are located at the intersection of the horizontal branch (HB) and the instability strip (IS) in the HR diagram. They are low mass ($\sim 0.5 \msun$), metal-poor stars which pulsate primarily in the fundamental mode (RRab) and in the first overtone (RRc) mode with periods ranging from $ \sim 0.2$ d to $ 1.0 $ d \citep[see e.g.][]{bhardwaj_rr_2022}. Many RRL variables are also classified as double mode pulsators (RRd) because they pulsate simultaneously in the fundamental and first overtone modes \citep{sandage_oosterhoff_1981, jurcsik_overtone_2015, soszynski_ogle_2017}. RRLs exhibit a well defined period-luminosity-metallicity (PLZ) relation especially in the near-infrared bands \citep{longmore_rr_1986, bono_theoretical_2001, catelan_rr_2004, sollima_new_2006, muraveva_new_2015, bhardwaj_rr_2021} and thus constitute as primary standard candles of the cosmic distance ladder. RRLs are also excellent probes to gain deeper insights into the theory of stellar evolution and pulsation \cite[e.g.][]{marconi_new_2015, das_variation_2018, marconi_impact_2018}. RRLs are excellent tracers of the old stellar populations due to their age and have been detected in different galactic \citep{vivas_quest_2006, drake_probing_2013, zinn_silla_2014, pietrukowicz_deciphering_2015} and extra-galactic \citep{moretti_leo_2009, soszynski_optical_2009, fiorentino_rr_2010, cusano_dwarf_2013} environments. They have also been identified in several globular clusters \citep{coppola_distance_2011, di_criscienzo_new_2011, kuehn_rr_2013, kunder_rr_2013}.

Advancements in the theoretical modelling of stellar pulsation have made it possible to generate grids of light curves that correspond to a set of physical parameters of the variables using non-linear 1-D hydrodynamical codes, such as those described by \cite{stellingwerf_convection_1984, bono_nonlinear_1997, bono_classical_1999, marconi_new_2015, de_somma_extended_2020, de_somma_updated_2022}. A grid of nonlinear convective hydrodynamical models was produced by \cite{marconi_new_2015} to study the pulsation properties of RRLs. This grid was created using horizontal-branch (HB) evolutionary models \cite[for more information, see][and its references]{pietrinferni_large_2006} and takes into consideration different chemical compositions, luminosity levels, and stellar masses. The models were calculated using the hydrodynamical codes of \citealt{stellingwerf_convection_1982} (updated later by \citealt{bono_theoretical_1998, bono_classical_1999}) and were based on the numerical and physical assumptions detailed in \cite{bono_theoretical_1998, bono_classical_1999} and \cite{marconi_rr_2003, marconi_period_2011}.

Additionally, the radial stellar pulsations of \cite{smolec_convective_2008} available with the Modules for Experiments in Stellar Astrophysics \citep[MESA][]{paxton_modules_2011, paxton_modules_2013, paxton_modules_2015, paxton_modules_2018, paxton_modules_2019}, can also be used to generate the light curves of radially pulsating variable stars.

The theoretical light curves generated from the available pulsation models have been utilized to investigate pulsation properties, derive PLZ relations, and provide a quantitative comparison with the observed light curves of Cepheid and RR Lyrae variables \citep{bono_rr_2000, keller_large_2002, caputo_bright_2004, marconi_pulsational_2005, marconi_modeling_2007, natale_theoretical_2008, marconi_theoretical_2013, marconi_new_2015, marconi_vmc_2017, bhardwaj_large_2017, das_variation_2018, ragosta_vmc_2019, bellinger_when_2020, das_stellar_2020}.

However, generating a model light curve from the given input parameters is still a computationally expensive problem, since it involves solving complex hydrodynamical equations. The processing time has been reduced significantly with modern computational facilities, but theoretical light curves for a fine grid of pulsation models covering the entire parameter space are still not feasible. A smooth grid of pulsation models of RR Lyrae and Cepheids is crucial to predict the physical parameters of these variables either through a model-fitting approach \citep{marconi_theoretical_2013} or using an automated comparison with observed light curves \citep{bellinger_when_2020}. \cite{bellinger_when_2020} obtained a catalogue of physical parameters for observed stars by applying machine-learning methods to the available Cepheid and RR Lyrae models, but the accuracy of physical parameters was limited due to the small number of models in the grid. These authors trained a neural network with light curve structure parameters like $I$ and $V$ band amplitudes, acuteness, skewness, and Fourier coefficients $A_{1}, A_{2}, A_{3}$ (see e.g., \cite{deb_light_2009, bhardwaj_variation_2015}) along with the period as input to predict the physical parameters of the model including mass (M), luminosity (L), radius (R), effective temperature ($\teff$). The theoretical light curves computed by \cite{marconi_new_2015} \& \cite{das_variation_2018} were used for this analysis. They choose the input parameters based on the feature importance study using another ML algorithm, namely Random Forest (RF). The error on the derived parameters is estimated by perturbing the light curve parameters with random noise 100 times and passing them through the ANN.

In this study, we utilized a previously generated grid of models and employed modern automated methods to infer light curves. This was done by training an artificial neural network (ANN) using models from \cite{marconi_new_2015} and \cite{das_variation_2018} to predict light curves based on a set of input parameters. This approach eliminates the need to solve complex time-dependent equations and can generate predictions much more efficiently. 

The paper is organized as follows: The introduction of artificial neural networks and quantitative Fourier analysis is presented in Section~\ref{sec:method}. The input and observational datasets used for network training and comparison are described in Section~\ref{sec:data}. The tuning of the network architecture and training of the final interpolators in both $I$ and $V$ bands is discussed in Section~\ref{sec:ann}. The validity of the trained interpolators is tested by comparing the newly generated model light curves with the ANN-predicted models in Section~\ref{sec:validation_interpolators}. In Section~\ref{sec:interpolator_comparison}, the comparison of Fourier parameters between observed and predicted light curves of LMC and SMC in both $I$ and $V$ bands is discussed, and the distances to these galaxies are estimated. The applications of the trained interpolators are explored in Section~\ref{sec:application}, including a comparison of the observed and predicted $V$ band light curve of a variable star (EZ Cnc) and the determination of its distance in Section~\ref{sec:lc_compare_ez_cnc}. A smooth grid of light curve templates in $I$ and $V$ bands is generated using the ANN interpolators in Section~\ref{sec:new_grid}. The results of the study are summarized in Section~\ref{sec:summary}.

\section{Methodology}\label{sec:method}
The theoretical grid of models can be thought of as being generated via a function $f$ defined as
$$ f (\bm{x}; \bm{w}) = [\text{light-curve}]. $$
where $\bm{x}$ is a combination of physical parameters of the stars, such as M, L, $\teff$, X (hydrogen abundance ratio) and Z (metal abundance ratio), the period and a few other parameters including mixing length, radiative cooling, convective and turbulent flux parameters. Interested readers may refer to \cite{marconi_new_2015} and references therein for a better understanding of the input parameters required for the generation of light curves. Here $\bm{w}$ are fitting parameters. If we assume that the function `$f$' exists and is continuous and differentiable in parameter space: an ANN with one or more hidden layers can reproduce this function. Theoretically, \cite{cybenko_approximation_1989, hornik_multilayer_1989, hornik_approximation_1991} showed that ANN with a non-linearly activated hidden layer, can approximate any continuous function with arbitrary accuracy when provided with enough neurons in the hidden layer. In addition, ANNs are flexible in the choice of architecture, and optimization algorithms, and are simple to implement. They also provide the possibility {\it transfer learning} and therefore is useful in the case of re-training the model with updated data.

\subsection{Artificial Neural Network (ANN)}\label{sec:ann_theory}
\begin{figure}
	\begin{center}
		\begin{neuralnetwork}[height=5, style={}, title={}, titlestyle={}, layertitleheight=1.05cm, layerspacing=2.1cm]

			% use \ifnum to get different labels
			\newcommand{\x}[2]{\ifnum #2=4 $x_n$ \else \small $x_#2$ \fi}
			
			\newcommand{\hfirst}[2]{\ifnum #2=4 $a^{ (1)}_{n_1}$ \else \small $a^{ (1)}_#2$ \fi}
			\newcommand{\hsecond}[2]{\ifnum #2=4 $a^{ (2)}_{n_2}$ \else \small $a^{ (2)}_#2$ \fi}

			\newcommand{\nodetexty}[2]{\ifnum #2=4 $\hat{y}_m$ \else $\hat{y}_#2$ \fi}
			
			% use exclude to turn off drawing of specific layers
			\inputlayer[count=4, bias=false, exclude={3}, title=Input\\layer, text=\x]
		
			\hiddenlayer[count=4, bias=true, exclude={3}, title=Hidden\\layer 1, text=\hfirst] \linklayers[not to={3}, not from={3}]

			\hiddenlayer[count=4, bias=true,exclude={3}, title=Hidden\\layer 2, text=\hsecond] \linklayers[not to={3}, not from={3}]
			
			\outputlayer[count=4, exclude={3}, title=Output\\layer, text=\nodetexty] \linklayers[not to={3},not from={3}]
			
			% draw dots
			\path (L0-2) -- node{$\vdots$} (L0-4);
			\path (L1-2) -- node{$\vdots$} (L1-4);
			\path (L2-2) -- node{$\vdots$} (L2-4);
			\path (L3-2) -- node{$\vdots$} (L3-4);
		\end{neuralnetwork}
	\caption{A representation of an artificial neural network with $n$ inputs, two hidden layers having $ n_1 $ and $ n_2 $ neurons respectively and an output layer with $ m $ neurons. $ a_{0} $ represents the bias terms in respective layers. The summation and non-linearity nodes are omitted for the sake of clarity.}
	\label{fig:MLP}
	\end{center}
\end{figure}

We employ the simplest neural network which is a feed-forward, fully-connected neural network. The smallest unit of the network, the neuron (or a perceptron) is a mathematical unit that calculates the weighted sum of all the neurons that are previously connected to it and feeds this output to all the neurons in the next layer after applying a non-linear activation function ($\sigma$) (see Fig.~\ref{fig:MLP} ). The value of $i^{th}$ neuron in $k^{th}$ layer is calculated by, 
\begin{equation}
    a_i^{ (k)} = \sigma^{ (k)} \left (\sum_{j=0}^{n_{k}-1} w_{ij}^{ (k-1)} a_{j}^{ (k-1)} \right),\: \text{for} \: 1 \leq i \leq n_{k},
\end{equation}    
with,
\begin{equation}
a_0^{ (k)} = 1 ,  
\end{equation}
here $\sigma^{ (k)}$ is the activation function for the $k^{th}$ layer, which is usually a nonlinear function. A few examples of widely used activation functions are rectified linear unit (or {\it relu}), sigmoid, and hyperbolic tangent ({\it tanh}). The weights of $j^{th}$ neuron that is connected with $i^{th}$ neuron in $k^{th}$ layer are represented by $w_{ij}^{ (k)}$ and are optimised to obtain a minimum (or maximum) value of a certain objective function during the training process. The weights are updated in an iterative process where in each iteration the errors obtained at the output layer are propagated back to the previous layers and thus making the network `better' at every iteration. This algorithm was first conceptualised by ~\citet{Rumelhart86b} and is known as the backpropagation algorithm. This backpropagation algorithm is the backbone of modern stochastic gradient descent (SGD) algorithms \citep[for a review of modern SGD algorithms, interested readers may go through][]{ruder_overview_2016} that optimise the weights of the network based on the quantity that you want to optimise in order to train the network. Fig.~\ref{fig:MLP} represents a schematic of a network with two hidden layers. We have assumed that at for input layer, $k \equiv 0$ and $\bm{a}^{ (0)} \equiv \textbf{x}$ and at the output layer $k \equiv L$ and $\bm{a}^{ (\text{L})} \equiv \hat{\textbf{y}}$ ($\hat{y}$ is the output of the network).

If the predicted and given absolute magnitude value of $i^{th}$ model corresponding to $j^{th}$ phase is $\hat{y}_{ij}$ and $y_{ij}$, the mean square error (MSE) for the $i^{th}$ model is calculated using:
\begin{equation}
    \mathbb{E}_{i} \equiv (\text{MSE})_{i} = \frac{1}{N_s} \sum_{j=1}^{N_s} (y_{ij} - \hat{y}_{ij})^2,
\end{equation} 
where $N_s$ is the total number of magnitude bin values for each model. The average MSE for all models in the grid can be calculated using:
\begin{equation}
\bm{\mathbb{E}} \equiv \text{Avg. MSE} = \frac{1}{N} \sum_{i=1}^{N} (\text{MSE})_{i} = \frac{1}{N} \sum_{i=1}^{N} \left ( \frac{1}{N_s} \sum_{j=1}^{N_s} (y_{ij} - \hat{y}_{ij})^2 \right). 
\end{equation}
where $N$ is the total number of models in the training dataset. 
% In the present analysis, $N = 268$ and $M = 1000$. 
At every training iteration, for the simplest case of gradient descent learning, the weights are updated according to the following rule:
\begin{equation}
    w_{ij}^{ (k)} = w_{ij}^{ (k)} - \eta \times \frac{\partial \bm{\mathbb{E}}}  {\partial w_{ij}^{ (k)}}
\end{equation}
\noindent where $\eta$ is a scaling factor typically referred to as {\it learning parameter} that determines the size of the gradient descent steps. However, we used a modified version of the gradient descent algorithm known as the adaptive moment optimization algorithm \citep[\texttt{adam},][]{kingma_adam_2014} where the learning parameter is also updated at each iteration to reach the minimum of the objective function efficiently.

\subsection{Fourier parameters}
The analysis of light curves using Fourier analysis has been discussed extensively in literature \citep[][and references therein]{deb_light_2009, bhardwaj_variation_2015, das_variation_2018}. A Fourier series of sines is fitted to the theoretical and observed light curves and the parameters are deduced:
\begin{equation}\label{eq:fourier_sine}
     m = m_{0} + \sum_{k=1}^{N} A_{k} \sin{ (2\pi k \Phi+\phi_k)},
\end{equation}
Here, $A_k$ and $\phi_k$ are Fourier amplitude and phase coefficients, respectively, and $\Phi$ is the pulsation phase that ranges from 0 to 1, which is calculated using the following equation:
\begin{equation}
    \Phi = \left [\frac{t-t_0}{\rm P} \right] -\texttt{Int}\left [\frac{t-t_0}{\rm P} \right], 
\end{equation} 
here, $t_0$ is the epoch of maximum brightness, and P is the pulsation period of the star/model.

$A_k$ and $\phi_k$ are used to calculate Fourier amplitude ratios ($R_{k1}$) and phase differences ($\phi_{k1}$):
\begin{equation}\label{eq:Rk1}
    R_{k1} = \frac{A_k}{A_1},
\end{equation}
\begin{equation}\label{eq:phi_k1}
    \phi_{k1} = \phi_{k} - k\phi_1,
\end{equation}
where, $k > 1$ and $0 \leq \phi_{k1} \leq 2\pi$.

\section{Data}\label{sec:data}
\subsection{Training data}
To train the neural network, we adopted the theoretical light curves of fundamental mode RR Lyrae (or RRab) models as described in \citet{das_variation_2018}. We used a total of $274$ RRab light curves corresponding to a grid of physical parameters, out of which $166$ were initially computed by \citet{marconi_new_2015} and the additional $108$ by \citet{das_variation_2018} using the same non-linear, time-dependent convective hydrodynamical models. The model light curves were generated in bolometric magnitudes and they were later transformed into Johnson Cousin photometric bands. A summary of theoretical RRab models is presented in Fig.~\ref{fig:input_data}. The models contain seven distinct chemical compositions ranging from $Z = 0.001$ to $Z = 0.02$, with a primordial He abundance, $Y = 0.245$, and a constant helium-to-metals enrichment ratio, $\hem = 1.4$ to replicate the initial helium abundance of the sun \citep{serenelli_determining_2010}. The pulsation models were constructed with different sets of stellar masses and luminosities, which were fixed according to detailed central He-burning horizontal-branch evolutionary models. The range of Z is broad enough for the comparison with the observed RR Lyrae stars in the satellite galaxies of the Milky Way, the Small Magellanic Cloud (SMC), and the Large Magellanic Cloud (LMC) \citep{clementini_distance_2003}. A few models have pulsation periods greater than one day, thereby including the possibility of evolved RR Lyrae stars in the training data set. Out of the $274$, six overlapping models were removed from the input dataset leaving $268$ unique RRab models to train the ANN. The ANN models are described in detail in Section~\ref{sec:ann}.

\begin{figure*}
    \centering
    \includegraphics{figs/2_input_data.pdf}
    \caption{A visual representation of the input dataset of RRLs models described in Table~\ref{tab:input_dataset}. A few additional models were computed in this work for the validation of the trained ANN interpolators and are marked with $\textbf{A}$, $\textbf{B}$, and $\textbf{C}$}.
    \label{fig:input_data}
\end{figure*}

\begin{table}
    \centering
    \caption{A summary of 268 Fundamental mode RR Lyrae models.}
    \small
    \input{table/parameter_table}
    \label{tab:input_dataset}
\end{table}

\subsection{Observational data used for comparison}\label{sec: obs_data}
We used the observed light curves for a comparison with the ANN predicted light curves. Since the training light curves are in the Johnson Cousin filters bandpass, we need to have the observed light curves in the same filters. We used the $I$ and $V$ band light curves from the IV$^{\rm th}$ phase of the Optical Gravitational Lensing Experiment (OGLE\footnote{\url{http://ogle.astrouw.edu.pl/}}) catalogue of RR Lyrae variables in LMC and SMC \citep{soszynski_ogle_2016}. 

However, the observed magnitudes suffer from interstellar extinction and reddening due to the interaction of light with interstellar dust. We need to account for interstellar extinction and perform extinction corrections in magnitudes at a given wavelength. Using the positions (RA/Dec), we obtain the excess colour values, $E (V - I)$, for RRLs in the LMC and SMC from the reddening maps of \cite{haschke_new_2011}. { We apply extinction corrections using Schlegel's list of conversion factors\footnote{\url{https://dc.zah.uni-heidelberg.de/mcextinct/q/cone/info}}} -  
\begin{equation}\label{eq:Av}
    A_V = 2.4 \times E(V-I) = 3.32 \times E(B-V),
\end{equation}
 and
\begin{equation}\label{eq:Ai}
     A_I = 1.41 \times E(V-I) = 1.94 \times E(B-V).
\end{equation}

\section{ANN Interpolator for RR Lyrae models}\label{sec:ann}
We used the ANN to interpolate the light curve for the input parameters within the grid. We built a feed-forward fully connected neural network and optimised its network architecture. The input layer of this network takes six parameters, which are: 
$$\bm{x} \equiv \left[\frac{\text{M}}{\msun}, \,  \log \left(\frac{\text{L}}{\text{L}_{\sun}}\right), \, \teff, \, \text{X}, \, \text{Z}, \, \log (P) \right],
$$
where M and L are the mass and luminosity of the model. $\teff$ is the effective temperature of the model in Kelvin. X and Z are the hydrogen and metal abundances of the model and the parameter P is the period in days. We have included the period as one of the inputs to facilitate ease for the user who might want to generate a model corresponding to a specific period.

The hydrodynamic models presented in \cite{marconi_new_2015} were generated using the same physical and numerical assumptions employed in earlier works, such as \cite{bono_theoretical_1998, bono_classical_1999} and \cite{marconi_rr_2003, marconi_period_2011}. For instance, the radiative opacities used in their models were taken from the OPAL radiative opacities provided by the Lawrence Livermore National Laboratory \citep{iglesias_updated_1996}, while the molecular opacities were obtained from \cite{alexander_low-temperature_1994}. Since the physical conditions, including the radiative and molecular opacities were kept constant, they were not considered as inputs to the ANN.

Each input parameter has a different numerical range, we need to transform each parameter to have the same numerical range as other input parameters. The input parameters are scaled in such a way that each input parameter has zero mean and unity standard deviation over the whole grid. At the output layer, we provide the corresponding ($I$ or $V$ band) light curve of the model which consists of the absolute magnitudes at a given series of phases. We have a total of $1000$ magnitude values per model corresponding to phase values from $0$ to $1$ in steps of $0.001$. We used a neural network that has one input layer with six input neurons, a few hidden layers (not more than three to keep the network small), and one output layer with $1000$ output neurons containing the absolute magnitude corresponding to phase values between $0$-$1$. We used a linear activation function ($\sigma^{ (L)} \equiv 1 )$ between the last hidden layer and the output layer because we do not want to constrain the output values in any particular range. We have a total of $268$ models, and hence the training matrix at the input layer has [$268 \times 6$] and at the output layer is [$268 \times 1000$].

\subsection{Network Architecture and Hyperparameter optimisation}
To have a generalised network that does not overfit/underfit the given dataset, we need to choose a suitable architecture (the number of hidden layers, the number of neurons in the hidden layer, activation functions) as well as the learning hyperparameters such as the loss optimization algorithm and parameters therein, etc \citep{elsken_neural_2019}. Each individual hyperparameter has a significant role in the training process and a different value of a hyperparameter significantly affects the result of the training. However, there are no explicit `rules' for selecting these attributes in such a way that the ANN model does not become trapped in a local solution. This is a crucial problem in the field of machine learning \citep{guo_novel_2008}. 

The choice of architecture and hyperparameters usually depends on the intuition of the expert and hand tuning. Typically, the trial-and-error approach like grid search and random search \citep{bergstra_random_2012} is used to determine these characteristics. We created a grid of possible hyperparameters by choice and intuition that we gained working with the dataset, which is tabulated in Table~\ref{tab:hyperparameter_combinations}. Out of various possible combinations, we chose a set of $100$ hyperparameter combinations at random. We then trained the network for fixed $1000$ epochs with the L2 norm (MSE) as the objective function using the ``adaptive moment stochastic gradient descent (or \texttt{adam}: \citealt{kingma_adam_2014})'' algorithm with a default batch size of $32$ samples. The network architecture was optimised using the $I$ band light curves. For this procedure, we utilised \texttt{Keras tuner} \citep[][]{omalley_kerastuner_2019}.

\begin{table}
    \centering
    \caption{The hyperparameter search space for the network.}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{cll}
        \hline 
        S.No. & Name of hyperparameter & Possible values  \\
        \hline 
        1 & No. of hidden layers & [1, 2, 3] \\
        2 & No of neurons in  one hidden layer & [16, 32, 64, 128]   \\
        3 & Optimiser & `\texttt{adam}' \citep{kingma_adam_2014} \\
        4 & Learning rate  & [$10^{-2} - 10^{-4}$](log sampling) \\
        5 & Activation function & [`\texttt{relu}', `\texttt{tanh}'] \\
        6 & Weights initialisation & `\texttt{GlorotUniform}' \\
        \hline
    \end{tabular}
    }
    \label{tab:hyperparameter_combinations}
\end{table}

\begin{table*}
\centering
\caption{The performance of different neural network architectures trained with the different combinations of hyperparameters.}
\input{table/hparams_tune}
\label{tab:hparams_tune}
\end{table*}

\begin{figure}
    \centering
    \includegraphics{figs/3_training_curve_1.pdf}
    \caption{Training curves of the network architectures described in Table~\ref{tab:hparams_tune}. We seek such an architecture that converges to the minimum rapidly in the first 1000 epochs. We find that the network with architecture $1$ reaches the minimum fastest.}
    \label{fig:train_curve_1}
\end{figure} 

We show the result of the top 10 performing network architectures in Table~\ref{tab:hparams_tune}. We observe that a network with $3$ hidden layers with $64, 128, 128$ neurons in successive hidden layers with an initial learning rate$(\eta) \sim 1.8 \times 10^{-3}$, reaches the minimum MSE in $1000$ epochs of learning. Fig.~\ref{fig:train_curve_1} depicts the learning curves for the top-10 network architectures listed in Table~\ref{tab:hparams_tune}.  

\subsection{Training of the I band Interpolator}
\begin{figure*}
    \centering
    \includegraphics{figs/4_training_curve_with_lr_IBAND.pdf}
    \includegraphics{figs/4_training_curve_with_lr_VBAND.pdf}
    \caption{ The learning curve for both the $I$ and $V$ band is shown in the respective blocks along with the learning rate parameter. The blue curve in each plot represents the training curve till the initial 1000 epochs trained with the best-performing network architecture obtained using random search. We then transfer the trained weights and re-train the network with piece-wise decay of the learning rate. The orange curve represents the training curve in this phase.}
    \label{fig:train_curve_2}
\end{figure*}
To construct the final interpolator for the $I$ band, we used the architecture that performed best in the architecture optimization step, i.e., architecture 1. In the previous step, we note that the training loss (MSE) decreases globally at large epochs, but the individual updates are quite big and the loss oscillates rapidly (see Fig.~\ref{fig:train_curve_1}). High learning rates cause such oscillations in the learning curve. The oscillations can be reduced by reducing the learning parameter; however, we don't want to start the training with a low $\eta$ because it impairs the performance of the network (see the comparison between architecture no. 1 and 4: the low learning rate causes the network to return comparatively higher MSE). Hence, we employ `piece-wise decay' of the learning rate to mitigate these oscillations of loss at higher epochs. To do this, we begin with the best-performing network of the previous step and re-train it with a decreasing learning rate. We reduce the current learning rate by dividing it by a constant number (`$\delta$') when the epoch crosses successive powers of $10$. We chose $\delta = 5$, based on the intuition we gained after experimenting with a few values of $\delta$. This guarantees that the loss diminishes gradually as the training epochs increase. We trained this network for $11,000$ epochs and obtained a minimum MSE of $9.07 \times 10^{-6}$. The final learning curve (epoch vs loss curve) is shown in Fig.~\ref{fig:train_curve_2} along with the learning parameter ($\eta$) in the top panel. This final network is adopted as the $I$ band ANN interpolator for fundamental mode RR Lyrae models.

\subsection{Training of the V band Interpolator}
The problem of training the network for interpolating the light curve in a different band is a similar problem that we tackled in the previous step and hence we used the same network architecture that performed best during the $I$ band interpolator training, to train the $V$ band interpolator. We started the training with a neural network with $3$ hidden layers which contain $64, 128, 128$ neurons respectively. We started to train the network with the same `\texttt{adam}' optimization algorithm with the same initial learning rate parameter ($\eta = 1.7941 \times 10^{-3}$). After the initial training for $1000$ epochs, we encounter the same problem of loss oscillations, and hence we treat the learning rate in the same manner as we did in the case of the $I$ band interpolator. The learning curve for the $V$ band interpolator along with the adopted learning rate parameter is shown in the right panel of Fig.~\ref{fig:train_curve_2}.

\subsection{Training statistics for interpolators}\label{sec:testing_interpolators}
We calculated the statistics between the original model light curves and the ANN generated/predicted light curves. We determined the average mean squared error (MSE), mean absolute error (MAE) and the coefficient of determination ($R^{2}$) between the original and predicted magnitude values for a quantitative comparison \citep{steel_principles_1960, draper_applied_1998, glantz_primer_2001}. We have already discussed the MSE in Section ~\ref{sec:ann_theory}. The MAE for one model is defined by:
$$ \text{MAE} = \frac{1}{N_s} \sum_{j=1}^{N_s}  |y_{j} - \hat{y}_{j}|.  $$
and $R^{2}$ is defined by: 
$$ R^{2} = 1 - \frac{ \sum (y_j - \hat{y}_j)^{2} }{\sum (y_j - \Bar{y})^2}. $$
where $y_j$ and $\hat{y}_j$ is the original and predicted magnitude value corresponding to the $j^{th}$ phase and $\Bar{y} = \frac{\Sigma y_j}{N_s}$. $N_s$ is the total number of sample points for the light curve. We quote the average of each quantity over the whole dataset.

The training statistics for both $I$ and $V$ bands are given in Table~\ref{tab:train_stats}. For the $I$ band interpolator, we achieve a minimum MSE of $\sim 9.076 \times 10^{-6}$, with corresponding MAE of $ \sim 2.162 \times 10^{-3}$ and $R^{2} \sim 0.9987$. For the $V$ band interpolator, we achieve a minimum MSE of $1.175 \times 10^{-5}$, with corresponding MAE of $ \sim 2.503 \times 10^{-3}$ and $R^{2} \sim 0.9994$.

\begin{table}
    \centering
    \caption{The statistics of the interpolators on training data.}
    \begin{tabular}{cccccc}
    \hline
         Band & No. of Models & MSE & MAE      & $R^{2}$ \\
     \hline
         I    & 268          & $9.076 \times 10^{-6}$  & $ 2.162 \times 10^{-3}$& $ 0.99879$ \\
         V    & 268          & $1.175 \times 10^{-5}$  & $ 2.503 \times 10^{-3}$& $ 0.99940$ \\
    \hline
    \end{tabular}
    \label{tab:train_stats}
\end{table}

\subsection{Validation of Interpolators}\label{sec:validation_interpolators}

To validate our method, we computed three additional models using the same hydrodynamical code and compared the light curves predicted by the ANN with the ones obtained from these models. The comparison between the ANN generated light curves and the model light curves in $I$ and $V$ bands can be seen in Figs \ref{fig:validation_I} and \ref{fig:validation_V} respectively. The physical parameters for the validation models were selected from a relatively scarce parameter space. The results, shown in Table \ref{tab:validation_stats}, indicate that the light curves predicted by the ANN are consistent with the model light curves. The validation models produced an average mean squared error (MSE) of $2.316 \times 10^{-3}$ in the $I$ band, corresponding to an average $R^{2}$ value of $0.95$, and an MSE of $2.658 \times 10^{-3}$ in the $V$ band, corresponding to an average $R^{2}$ value of $0.97$. This agreement between the ANN generated light curves and the new model light curves demonstrates the validity of our ANN models and confirms that they can be used to generate light curves in both $V$ and $I$ bands.

\begin{figure*}
    \centering
    \includegraphics[scale=0.98]{figs/5_I_band_validation.pdf}
    \caption{The comparison of the original $I$ band light curve, represented by the blue background line, and the ANN reconstructed light curve, shown as the foreground red line, is displayed for three models. These light curves were generated using the same hydrodynamical code for validation purposes. The input parameters for each light curve plot can be found in the upper panel. The difference between the two light curves is depicted in the bottom panel with a black line, with the magenta line representing the $3\sigma$ bounds and the green line indicating the mean deviation between the predicted and actual light curves. The goodness of fit parameter $R^{2}$ is also calculated for each plot and displayed.}
    \label{fig:validation_I}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.98]{figs/6_V_band_validation.pdf}
    \caption{Same as Fig.~\ref{fig:validation_I} but for $V$ band.}
    \label{fig:validation_V}
\end{figure*}
\begin{table}
    \centering
    \caption{The statistics of the interpolators on validation data.}
    \begin{tabular}{cccccc}
    \hline
         Band & No. of Models & MSE & MAE      & $R^{2}$ \\
     \hline
         I    & 3          & $2.316 \times 10^{-3}$  & $ 3.375 \times 10^{-2}$& $ 0.95123$ \\
         V    & 3          & $2.658 \times 10^{-3}$  & $ 3.5001 \times 10^{-2}$& $ 0.96911$ \\
    \hline
    \end{tabular}
    \label{tab:validation_stats}
\end{table}

\section{Comparison of observed light curves with the generated light curves using interpolators}\label{sec:interpolator_comparison}
Theoretical models are used to complement observational data and are tested on the observed light curves of stars for which the physical parameters are already known. To accurately determine the physical parameters of RR Lyrae stars, a detailed spectroscopic and photometric analysis is often required \citep[see e.g.][]{wang_asteroseismology_2021}. However, in some cases, it is not feasible to obtain spectroscopic measurements of such stars, so data-driven methods are necessary to infer their physical properties. One such method is presented in the study by \cite{bellinger_when_2020}. The authors have predicted the physical parameters (mass, luminosity, effective temperature, and radius) of stars in the LMC and SMC using the OGLE-IV survey, which provides the $I$ and $V$ band light curves of various types of variable stars, including RR Lyrae. The authors used an ANN to derive the parameters by training it with the relationship between light-curve structure (including amplitudes, acuteness, skewness, and coefficients of the Fourier series) and physical parameters of the models. By comparing the ANN generated light curve based on these parameters to the observed light curves, the validity of the derived parameters can be evaluated. This comparison was done by comparing the amplitudes, Fourier parameters, and their distribution with the period of pulsation of the ANN generated light curves to the observed light curves. We stress that the ANN used by \cite{bellinger_when_2020} to estimate the physical parameters based on the light curves and the one we use to predict the light curves based on the physical parameters, were trained on the exact same set of hydrodynamical models.

The ANN model requires a total of six input parameters: $\left[\frac{\text{M}}{\msun}, \,  \log \left(\frac{\text{L}}{\text{L}_{\sun}}\right), \, \teff, \, \text{X}, \, \text{Z}, \, \log (P) \right]$ to predict the light curve. We used the `Lomb-Scargle' algorithm \citep{lomb_least-squares_1976, scargle_studies_1982} to determine the period from the observed magnitudes in un-evenly spaced observations and we calculated the Z values for the \cite{bellinger_when_2020} stars from photometric metallicities ($\feh$) provided by \cite{skowron_ogle-ing_2016}. The steps for transformation from $\feh$ to Z are provided as follows: If the composition of a star is solar scaled, the following relation holds \citep{piersanti_l_method_2007},
\begin{equation}
    \feh = \log (\rm Z/\rm X)_{\star} - \log (\rm Z_{\sun}/\rm X_{\sun}) .
\end{equation}
For Sun, we adopted $\rm X_{\sun}=0.7392, \rm Y_{\sun}=0.2486, \rm Z_{\sun}=0.0122$ from \cite{asplund_solar_2005}. Also, we fixed $ \rm Y =0.245$ for the RRL stars and determined the values of X and Z for each star by cross-matching \citeauthor{bellinger_when_2020} and \citeauthor{skowron_ogle-ing_2016}.

We managed to compile the required input parameters for the $7789$ RRab stars of LMC, and $676$ stars of SMC. With the adopted input parameters we generated the light curves and determined the peak-to-peak amplitude ($A$) and Fourier parameters ($R_{21}, R_{31}, R_{41}, R_{51}, \phi_{21}, \phi_{31}, \phi_{41}, \phi_{51} $) by fitting a Fourier series defined by equation \ref{eq:fourier_sine} with $N=5$.

\begin{figure*}
    \centering
    \includegraphics{figs/7_LMC_I_PA.pdf}
    \includegraphics{figs/7_SMC_I_PA.pdf}
    \includegraphics{figs/7_LMC_V_PA.pdf}
    \includegraphics{figs/7_SMC_V_PA.pdf}
    \caption{Peak to peak amplitude against $\log(P)$ for Observed and ANN generated light curves of LMC and SMC stars. The extended panel in each plot shows the histograms of periods on the x-axis and amplitudes on the y-axis.}
    \label{fig:period_amplitude}
\end{figure*}

Fig.~\ref{fig:period_amplitude} displays the peak-to-peak amplitudes for the observed and predicted light curves of RR Lyrae in the Magellanic Clouds in the $I$ and $V$ band, respectively. We find that the predicted amplitudes are in good agreement with the observed amplitudes of RR Lyrae variables. For the LMC variables, there seem to be two amplitude sequences for the longer period RRab ($\log \rm P \gtrsim -0.22$) stars. The theoretical models reproduce relatively larger amplitudes for Cepheids and RR Lyrae than the observations in optical bands \citep{bhardwaj_comparative_2017, das_variation_2018}, and the higher amplitude sequence can be attributed to this systematic in the models for specific mass-luminosity levels. The discrepancy is known to be related to the treatment of super adiabatic convection as the considered pulsation models, and \citealt{marconi_new_2015}, assume a single value for the mixing length parameter that is used in the code to close the nonlinear equation system. Nevertheless, the majority of the predicted and the observed amplitudes are in good agreement.

\subsection{Comparison of the Fourier parameters of models with observations}
Fig.~\ref{fig:FP_compare_LMC} and Fig.~\ref{fig:FP_compare_SMC} display the Fourier parameters of the predicted and observed light curves of LMC and SMC in both $I$ and $V$ bands respectively. For RRab in both the clouds, the Fourier amplitude ratio values from the predicted light curves agree well with the observations, but the predicted phase parameters show a systematic offset and a larger dispersion. We also note that phase parameters exhibit a strong correlation with the metallicity \citep{jurcsik_determination_1996, nemec_metal_2013, mullen_metallicity_2021} and such systematic may also arise from the uncertainties in the input photometric metallicities. 

It should be noted that the predicted physical parameters for these variables from \cite{bellinger_when_2020} are not highly precise, and their accuracy is limited by the lack of a fine grid of models. The derived physical parameters are used to generate the light curve using the trained interpolators. A good match between ANN generated and observed light curves is expected since the same theoretical models were employed to train the ANN used by us and the ANN trained by \cite{bellinger_when_2020}. However, it should be noted that the phase parameters were not included in the training input used to derive the physical parameters in \cite{bellinger_when_2020}. Despite including convection in the hydrodynamical models, it remains difficult to match the observed Fourier phase parameters of RRLs with theoretical models \citep{feuchtinger_nonlinear_1999, paxton_modules_2019}. Comparative studies of the theoretical RR Lyrae models generated by \cite{marconi_new_2015} with the observations show an offset in the value of Fourier phase parameters. The models predict higher Fourier phases than the observations \citep{das_variation_2018}. The same effect gets propagated through the ANN, and the phase parameters derived from the light curves generated using the ANN interpolator are slightly higher than the observed values for both LMC and SMC.

\begin{figure*}
    \centering
    \includegraphics[scale=0.90]{figs/8_LMC_I_R.pdf}
    \includegraphics[scale=0.90]{figs/8_LMC_V_R.pdf}
    \includegraphics[scale=0.90]{figs/8_LMC_I_Phi.pdf}
    \includegraphics[scale=0.90]{figs/8_LMC_V_Phi.pdf}
    \caption{This plot displays the comparison between the Fourier amplitude ratios and phase differences of the light curves predicted by ANN and the actual observations for LMC, in both the $I$ and $V$ bands, as a function of period.}
    \label{fig:FP_compare_LMC}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[scale=0.90]{figs/9_SMC_I_R.pdf}
    \includegraphics[scale=0.90]{figs/9_SMC_V_R.pdf}
    \includegraphics[scale=0.90]{figs/9_SMC_I_Phi.pdf}
    \includegraphics[scale=0.90]{figs/9_SMC_V_Phi.pdf}
    \caption{Same as Fig.~\ref{fig:FP_compare_LMC} but for SMC.}
    \label{fig:FP_compare_SMC}
\end{figure*}

\subsection{Distance modulus to the Magellanic Clouds}\label{sec:distance}
The predicted light curves generated by the ANN model can be employed to estimate the distance modulus of the Magellanic Clouds. We adopted the reddening-independent Wesenheit index \citep{madore_period-luminosity_1982} to determine the distances, which is defined as $W = I - 1.55 \times (V - I)$. The Wesenheit index is a commonly used method for estimating distances \citep{soszynski_ogle_2018}. We calculated the Wesenheit index-based distance moduli ($W_{m} - W_{M}$) to estimate the distances to the Magellanic Clouds.

The distance moduli of individual RRab stars in the Magellanic Clouds are computed based on the Wesenheit index and are depicted in Fig.~\ref{fig:distance_LMC_SMC}. By removing outliers beyond 5-$\sigma$, the average distance modulus of RRab stars in the LMC and SMC are determined to be $ \mu_{\rm LMC} = 18.567 \pm 0.135 $ mag and $ \mu_{\rm SMC} = 18.93 \pm 0.17 $ mag respectively. These estimates are consistent with previously published distances of the Magellanic Clouds based on eclipsing binaries ($ \mu_{\rm LMC} = 18.476 \pm 0.024 $ mag, and $ \mu_{\rm SMC} = 18.95 \pm 0.07$ mag; \citealt{graczyk_araucaria_2014, pietrzynski_distance_2019}). It is important to note that the methodology employed in this study does not presuppose any prior period-magnitude relationship, and the obtained distance estimates are simply a result of accurately predicted light curves of observed stars in the Magellanic Clouds.

\begin{figure*}
    \centering
    \includegraphics{figs/10_distance_hist.pdf}
    \caption{The graph presents a histogram of the distance modulus calculated through the Wesenheit index. The left side shows the distribution of the distance modulus for stars in the Large Magellanic Cloud, while the right side displays the distribution for the Small Magellanic Cloud. In each plot, on the top left corner, a box lists the weighted average distance modulus and the number of stars ($N$) considered after outliers were removed using a 5 $\sigma$ threshold in sigma clipping.}
    \label{fig:distance_LMC_SMC}
\end{figure*}

\begin{landscape}
    \begin{table}
        \centering
        \caption{The table displays the calculated ANN predicted mean magnitudes, amplitudes, and Fourier parameters, along with the observations. The complete table can be accessed online in a machine-readable format.}        
        \label{tab:compare_SLMC_IV}
        \resizebox{\columnwidth}{!}{\input{table/MERGED_TABLE_1.tex}}
    \end{table}
\end{landscape}

\section{Applications of ANN Interpolators}\label{sec:application}
\subsection{Light Curve Comparison of EZ Cnc}\label{sec:lc_compare_ez_cnc}
As an application to the ANN interpolator, we generated and compared the light curve of an RRab star `EZ Cnc' or `EPIC 212182292' with the observed light curve. It is a non-Blazko RRab variable star which has been observed extensively in both photometric and spectroscopic observing regimes. \cite{wang_asteroseismology_2021} used $55$ high-quality Large Sky Area Multi-Object Fiber Spectroscopic Telescope \citep[LAMOST,][]{luo_first_2015} spectra of medium resolution to determine the atmospheric parameters ($\teff$, $\logg$, and $[\text{M/H}]$). Starting from these parameters, they generated a grid of theoretical models using MESA, applying the time-dependent turbulent convective models. They searched for the optimum parameters for which the modelled light curve matched with the observed light curve from the K2 mission \citep{howell_k2_2014} of the {\it Kepler} spacecraft for which the light curve was processed by EPIC Variability Extraction and Removal for Exoplanet Science Targets Pipeline \citep[EVEREST;][]{luger_everest_2016}. The estimated parameters of the star are given in Table~\ref{tab:ezcnc_params}. The given flux was converted to the {\it Kepler} magnitude (${Kp}$) by formula given by \cite{nemec_fourier_2011},
$$ Kp = m_0 - 2.5 \log (\rm{Flux})$$
where $m_0 = 25.4$ is derived by taking the difference between the instrument magnitude and the mean of $K_p$ \citep{wang_asteroseismology_2021}. We converted the $K_{p}$ to the $V$ band magnitude using the relation given by \cite{nemec_fourier_2011},
$$ V = (1.45 \pm 0.24) Kp - (5.97 \pm 3.20) $$
After extracting the $V$ band light curve, we did extinction correction using the reddening maps of \cite{schlegel_maps_1998, schlafly_measuring_2011} and found E(B-V) $ = 0.027 \pm 0.0006$ and the corresponding $A_V = 3.32 \times 0.0272 \pm 0.0006 = 0.0903 \pm 0.0020$ \footnote{The values are determined from NASA/IPAC INFRARED SCIENCE ARCHIVE (\url{https://irsa.ipac.caltech.edu/applications/DUST/}).} using equation \ref{eq:Av}.

\begin{table}
\centering
\caption{The stellar parameters of EZ Cnc (EPIC 212182292) adopted from \citet{wang_asteroseismology_2021}}.
\label{tab:ezcnc_params}
\resizebox{\linewidth}{!}{%
\begin{tabular}{>{\hspace{0pt}}m{0.458\linewidth}>{\hspace{0pt}}m{0.485\linewidth}} 
\hline
Parameter & Value \\ 
\hline
Mass (M) & $0.48 \pm 0.03 \ \msun$ \\
Luminosity (L) & $42 \pm 2 \ \lsun$ \\
Effective temperature ($\teff$) & $6846 \pm 50$ K \\
X & $0.741 \pm 0.004$ dex \\
Z & $0.006 \pm 0.002$ dex \\
Period (P)$\rm^a$ & $0.545740 \pm 0.000007$ days \\ 
\hline
\multicolumn{2}{>{\hspace{0pt}}m{0.943\linewidth}}{\begin{tabular}[c]{@{}l@{}}$\rm^{a}$The period is determined using the Kepler light-curve with\\~ ~ ~ ~ \texttt{PERIOD-04} \citep{lenz_period04_2005}.\end{tabular}}
\end{tabular}}
\end{table}

We generated a light curve using the $V$ band interpolator by providing the parameters given in Table~\ref{tab:ezcnc_params} as input. The error bars for the predicted magnitude are derived from the given uncertainties in the physical parameters. The resulting ANN generated light curve is plotted against the observed $V$ band light curve from the {\it Kepler} telescope in Fig.~\ref{fig:ez_cnc}. For the purpose of comparison, both light curves are normalised to mean magnitudes. We also did the quantitative analysis by determining and comparing the Fourier amplitude and phase parameter ratio for the \textit{Kepler} $V$ band, and ANN predicted $V$ band light curve. The derived parameters including amplitudes and Fourier parameter ratios are given in Table~\ref{tab:ez_cnc_fp}. The peak-to-peak amplitude (A) predicted by the ANN $V$ band interpolator is higher than the observed $V$ band light curve. This is due to a rather low mixing length parameter adopted in the considered grid of models. For a successful model fitting applications to RR Lyrae stars \citep[see e.g. ][]{marconi_pulsational_2005, marconi_modeling_2007}, an increased mixing length value is required to match the light curves of fundamental pulsators. The Fourier amplitude and phase ratio of the K2 and ANN generated light curves match each other within the $1\ \sigma$ errors. 

A direct application of this analysis is to estimate the distance to the star. Since we have calculated the mean absolute magnitude from the ANN generated light curve and the apparent magnitude from the \textit{Kepler} telescope, the distance modulus is calculated as $ \mu_{\rm EZ\ Cnc} = 11.988(2) - 0.703(2) = 11.284(3)$ mag, and hence the distance $d$ (in pc) calculated using the $\mu = 5 \log(d) - 5 $ gives $d = 1806 \pm 2 $ pc. The calibrated {\it Gaia} DR2 distance for this star is $1840^{+192}_{-161}$ pc \citep{bailer-jones_estimating_2018}, which is recently updated by {\it Gaia} EDR3 to $1775^{+70}_{-70}$ pc \citep{bailer-jones_estimating_2021}. The estimated distance to the EZ Cnc remarkably matches with the published distance estimations in the literature.

\begin{figure}
    \centering
    \includegraphics{figs/11_Ez_CnC.pdf}
    \caption{This plot represents the observed light curve from the K2 survey (converted to $V$ band) along with the ANN generated light curve using the trained $V$ band interpolator.}
    \label{fig:ez_cnc}
\end{figure}

\begin{table}
    \centering
    \caption{Comparison of the Fourier parameters of EZ Cnc star derived using K2 and ANN predicted light curve.}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lcc}
    \hline
          & K2 light curve  &  ANN Predicted  \\
          & ($Kp$ band converted to $V$ band) &  ($V$ band) \\
    \hline
         A (mag) & $1.263 \pm 0.148$ & $1.584 \pm 0.107$ \\
         mean mag (mag) &  $11.988 \pm 0.002$ & $0.703 \pm 0.002$ \\
         $R_{21}$ & $0.516 \pm 0.002$ & $0.542 \pm 0.005$ \\
         $R_{31}$ & $0.327 \pm 0.002$ & $0.325 \pm 0.005$\\
         $R_{41}$ & $0.153 \pm 0.002$ &  $0.223 \pm 0.005$\\
         $R_{51}$ & $0.104 \pm 0.002$ & $0.178 \pm 0.005$ \\
         $\phi_{21}$ & $2.761 \pm 0.005$ & $2.783 \pm 0.013$ \\
         $\phi_{31}$ & $5.719 \pm 0.007$ & $5.718 \pm 0.020$ \\
         $\phi_{41}$ & $2.214 \pm 0.013$ & $2.170 \pm 0.027$\\
         $\phi_{51}$ & $5.024 \pm 0.018$ & $5.119 \pm 0.034$\\
    \hline
    \end{tabular}
    }
    \label{tab:ez_cnc_fp}
\end{table}

\subsection{Generating a grid of models using the trained ANN interpolators}\label{sec:new_grid}
To get the precise physical parameters of pulsating variable stars, a grid of model light curves is compared with the observed light curve. However, a pre-computed grid of models is usually very coarse and non-uniform in the parameter space. This is due to the high computation cost and time consuming process of solving time-dependent hydrodynamical equations of the stellar atmosphere. Moreover, to constrain the parameters like mass, surface gravity and metallicity, we need to rely on spectroscopic data which is usually not available with photometric data. Hence, a fine grid of models is required to pin down the physical and atmospheric parameters of the star.

We generated a fine grid of light curve templates in both $I$ and $V$ bands using the trained interpolators. The choice of input parameters for the new grid is limited by the parameter space of the original grid of models. We choose a finer and more uniform grid than the original models. We generated the grid for three helium abundance ratios Y=$0.245, 0.25,$ and $0.265$ and $4$ different Z values ranging from metal-poor to  metal-rich stars. For any combination of Y and Z, the hydrogen abundance ratio (X) can be calculated using X = 1- Y- Z. Mass (M) varies from $0.52$ to $0.79\, \msun$, with a constant step size of $ 0.03\, \msun$. The luminosity parameter $\log ({\rm L/ \rm L_\odot })$ varies from $1.54$ to $2.02$ dex, with a step size of $0.04$ dex, the effective temperature ($\teff$) ranges from $5300$ to $7000$ K with a step size of $100$ K. The period of an RR Lyrae star is closely related to its temperature, luminosity, and mass \cite{van_albada_1971}. The van Albada-Baker (vAB) relation describes this relationship. We have used a modern version of the vAB relation, which includes the effect of metallicity on the pulsation period, from \cite{marconi_new_2015}. We used the relation for the fundamental mode RRLs.
\begin{equation}
    \begin{aligned}
            \log P =& -(0.58 \pm 0.02) \log\left({\frac{\rm M}{\msun}}\right) + (0.860 \pm 0.003) \log\left({\frac{\rm L}{\lsun}}\right) \\
            & - (3.40 \pm 0.03) \log(\teff) + (0.013 \pm 0.002) \log(Z) \\
                     & + (11.347 \pm 0.006).
    \end{aligned}
\end{equation}

We end up with a grid of $37,800$ individual parameter combinations for which the template light curves are generated in the $I$ and $V$ band using the interpolators. Fig.~\ref{fig:new_grid} represents the distribution of \cite{marconi_new_2015} parameter space with the new grid parameters that we have computed (see Table~\ref{tab:new_grid} for the parameter ranges). A complete distribution of all parameters is shown in Fig.~\ref{fig:input_distribution_appendix}. The light curve templates of six random models of the new grid are shown in Fig.~\ref{fig:six_stars} in both $I$ and $V$ bands. We observe that the predicted light curves exhibit the same structure and features as an RRab light curve. However, for certain combinations of the input parameters, the predicted light curve may not resemble an RRab light curve. The reason for this can be traced back to the scarcity of models in this region of the training dataset or the lack of stable RRab stars with these combinations of physical parameters.

\begin{table}
    \centering
    \caption{The parameter space of the new grid generated using the trained ANN Interpolators.}
    \label{tab:new_grid}
    \begin{tabular}{ccc}
    \hline
         Parameter   & Range & Step  \\
         \hline
         M  &  0.52 - 0.79 $\msun$     &     0.03 $\msun$   \\
         $\log ({\rm L/ \rm L_\odot })$& 1.54 - 2.02 dex & 0.04 dex\\
         $\teff$ & 5300 - 7000 K & 100 K \\
         Y & \multicolumn{2}{l}{{[}0.245, 0.25, 0.265]}  \\
         Z & \multicolumn{2}{l}{{[}0.00011, 0.00668, 0.01324, 0.01980]}  \\ 
         % $\log (\rm P) $& & \\
        \hline
    \end{tabular}
\end{table}


\begin{figure*}
    \centering
    \includegraphics{figs/12_teff_lum.pdf}
    \caption{The parameter space of the new grid along with \citet{marconi_new_2015}'s original grid. A more detailed grid can be found in Fig.~\ref{fig:input_distribution_appendix}. The ANN generated light curves of the labelled models are shown in Fig.~\ref{fig:six_stars}.}
    \label{fig:new_grid}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics{figs/13_comparison.pdf}
    \caption{The ANN generated $I$ and $V$ band light curves for six models labelled in Fig.~\ref{fig:new_grid}.}
    \label{fig:six_stars}
\end{figure*}

\section{Summary}\label{sec:summary}
We present a new technique to generate the light curve of RRab models in different photometric bands using ANN. We built and trained an artificial neural network for interpolating the light curve within a pre-computed grid of models. The ANN has been trained with the physical parameters-light curve grid. We used the models generated by \cite{marconi_new_2015} and used in \cite{das_variation_2018}, which were computed by solving the hydrodynamical conservation equations simultaneously with a nonlocal, time-dependent treatment of convective transport \citep{stellingwerf_convection_1982, bono_pulsation_1994, bono_classical_2000, marconi_2009}. For the validation of the trained interpolators, light curves for a few new models were generated and then compared with the ANN predicted light curves.

The architecture of the neural network is tuned using the $I$ band light curves. A random search for the hyperparameters is performed within a grid of {\it inferred} hyperparameter combinations. The architecture and trained weights of the best-performing network are then adopted for making the final interpolators in $I$ and $V$ bands.  

As an application of the trained interpolators, we generated and compared the light curves of the RRab stars in the Magellanic clouds (LMC/SMC). The physical parameters [M/$\msun, \log (\rm L/\lsun), \teff$] of RRab stars in LMC and SMC are adopted from \cite{bellinger_when_2020}. Z is calculated from the metallicity estimates provided by \citet{skowron_ogle-ing_2016}, and X $= 1 - $Y $-$ Z; is calculated using a fixed primordial helium abundance (Y$ = 0.245$). Lastly, the period is determined from the observed light curve using the `Lomb-scargle' method. The interpolators are then used to predict the light curve from the given physical parameters. Both observed and predicted light curves are then fitted with a Fourier sine series (see eq. \ref{eq:fourier_sine}) with N=5. The comparison of ANN predicted amplitudes, Fourier amplitude ratios and Fourier phase differences with the observations is shown in Fig.~\ref{fig:period_amplitude}, ~\ref{fig:FP_compare_LMC}, \& ~\ref{fig:FP_compare_SMC}. We observe that ANN predicted amplitudes are a bit larger than the observed amplitudes and the reason for this can be traced back to the low mixing length that was used to compute the original models. The Fourier amplitude ratios ($R_{k1}$, as defined in eq.~\ref{eq:Rk1}) of the light curves predicted by the ANN are in great agreement with observations, except for a few exceptions where $R_{41}$ and $R_{51}$ exhibit an additional feature in the ANN predicted models at around $\log(P) \approx -0.22$. The Fourier phase differences ($\phi_{k1}$, defined in equation~\ref{eq:phi_k1}) are consistently shifted, particularly $\phi_{21}$ and $\phi_{31}$. The cause of this discrepancy is not understood and it should be noted that even the state-of-the-art hydrodynamical code, \texttt{MESA-rsp}, is unable to accurately reproduce the Fourier phase differences \citep{paxton_modules_2019}. We also determine the distances to the LMC and SMC based on the reddening independent Wesenheit index. The distances found ($ \mu_{\rm LMC} = 18.567 \pm 0.135 $ mag, and $ \mu_{\rm SMC} = 18.93 \pm 0.17$ mag) are in excellent agreement with the published distances based on eclipsing binaries ($ \mu_{\rm LMC} = 18.476 \pm 0.024 $ mag, and $ \mu_{\rm SMC} = 18.95 \pm 0.07$ mag; \citealt{graczyk_araucaria_2014, pietrzynski_distance_2019}).

To showcase the utility of the interpolators, we generated and compared the light curve of the RRab star EZ Cnc. The physical parameters of this star were determined by \cite{wang_asteroseismology_2021} using medium-resolution spectroscopic observations from LAMOST and time-series photometric data from the {\it Kepler} mission. We transformed the {\it Kepler} light curve into the $V$-band light curve and then compared it with the light curve predicted by the ANN both qualitatively and quantitatively. The reported distance to this star, $ \mu_{\rm EZ\ Cnc} = 11.284(3)$, or $d = 1806 \pm 2 $ pc, is in excellent agreement with the recently updated parallax measurement from {\it Gaia} EDR3 of $1775^{+70}_{-70}$ pc \citep{bailer-jones_estimating_2021}.

The generation of a grid of model light curves using traditional methods can be computationally expensive and time-consuming, but by using the trained ANN interpolators, it is possible to generate a more dense grid of model light curves much more efficiently. The trained interpolators can generate a light curve given the input parameters, and the process is fast, taking only a few milliseconds for each light curve. Additionally, the size of the trained interpolator file is much smaller, making it easy to store and access. To complement existing theoretical model grids in the literature, a smooth grid of model light curves was generated using the trained interpolator. The grid of templates can be used in techniques such as template fitting to estimate the parameters of observed light curves. We generated over 30,000 model light curves in both the $I$ and $V$ bands, resulting in approximately 2 GB of data. However, if one has access to the trained interpolator file, which is much smaller in size (around 3.7 MB for each interpolator file in our case), it is also possible to generate a light curve by inputting the parameters. Generating each light curve takes only a few milliseconds (approximately 55 ms) for both $I$ and $V$ bands.

It is worth noting that our approach is dependent on the models used, and any errors or uncertainties in the models will be reflected in our results. However, this analysis will provide valuable insights into the stellar population model and has the potential to improve our understanding of these stars. The results can be improved by expanding the number of models or by using a more comprehensive grid of models. Additionally, the trained ANN models can be retrained on new or additional models to enhance the accuracy of the predicted light curves. In this way, our approach can be continuously refined and improved as more data and models become available.

\section*{Software}
% We have used Numpy\citep{numpy}, Pandas, Astropy, Matplotlib, TensorFlow, Seaborn
We utilized various Python libraries in our study including \texttt{Numpy} \citep[][]{harris_array_2020}, \texttt{Pandas} \citep[][]{mckinney_data_2010, team_pandas-devpandas_2020}, \texttt{Astropy}\footnote{\url{http://www.astropy.org}}\citep{astropy_collaboration_astropy_2013, astropy_collaboration_astropy_2018}, \texttt{TensorFlow} \citep{martin_abadi_tensorflow_2015}, \texttt{Matplotlib} \citep[]{hunter_matplotlib_2007} and \texttt{Seaborn} \citep{waskom_seaborn_2021}. \texttt{Numpy} and \texttt{Pandas} were used for data manipulation, while \texttt{Astropy}, a community-developed package for Astronomy, was also utilized. \texttt{TensorFlow} was employed to implement the Artificial Neural Network, while \texttt{Matplotlib} and \texttt{Seaborn} were used for creating visual plots.

\section*{Acknowledgements}
NK acknowledges the financial assistance from the Council of Scientific and Industrial Research (CSIR), New Delhi, as the Senior Research Fellowship (SRF) file no. 09/45(1651)/2019-EMR-I. AB acknowledges funding from the European Unions Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No. 886298. SD acknowledges the KKP-137523 `SeismoLab' lvonal grant of the Hungarian Research, Development and Innovation Office (NKFIH). HPS acknowledges a grant from the Council of Scientific and Industrial Research (CSIR) India, file no. 03(1428)/18-EMR-II. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data Availability}
Interested readers can use the trained interpolator to generate the predicted light curves of RRab stars using their input physical parameters. The generated grid of light curves is available on request and also through a web interface on \url{https://ann-interpolator.web.app/}.

%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%
% The best way to enter references is to use BibTeX:
\bibliographystyle{mnras}
\bibliography{paper} % if your bibtex file is called example.bib
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Input Distribution}
A detailed input distribution of the input parameters is shown in Fig.~\ref{fig:input_distribution_appendix}. We have also shown the detailed distribution of parameters of the new grid for which the template light curves are generated. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/14_parameter_space_1.pdf}
    \caption{A pair plot with KDE (Kernel Density Estimation) plots in the upper triangle shows the relationship between six variables, namely mass, luminosity, effective temperature, X, Z, and log(P). The contours represent the parameter space in 2D for the given combination of six parameters. The plot is divided into subplots, with each subplot representing a pair-wise scatter plot of the six variables. The diagonal of the subplots shows the distribution of each variable using KDE plots. The red cross (`\textcolor{red}{$\times$}') marker is assigned for the original models of \protect\cite{marconi_new_2015} and the blue plus (`\protect\textcolor{blue}{+}') marker is assigned for the models generated in this work.}
    \label{fig:input_distribution_appendix}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex
