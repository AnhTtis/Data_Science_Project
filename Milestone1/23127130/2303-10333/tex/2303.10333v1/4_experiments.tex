\section{EXPERIMENTS}
\subsection{Datasets}
% 为了充分的验证我们提出的MP-SSL方法的有效性，我们一共在4个3D医学图像分割数据集上进行实验，分别是BraTS2020，BTCV，MSD Liver，MSD Spleen。每个数据集的参数信息被展示在表1中。
%In order to fully validate the effectiveness of our proposed HybridMIM method, we conduct experiments on a total of four 3D medical image segmentation datasets, BraTS2020, BTCV, MSD Liver, and MSD Spleen, respectively. The parameter information of each dataset is presented in Table \ref{tab:data_intro}.

\textbf{Pre-training Dataset:}
We collected a total of 1897 CT images to construct our pre-training dataset. They come from 4 public CT image datasets, including the ATM22~\cite{atm22} (150 cases of chest), luna16~\cite{setio2017validation} (888 cases of lung), covid-19~\cite{roth2022rapid} (448 cases of lung) and FLARE21~\cite{MedIA-FLARE21} (411 cases of abdomen) datasets. 
%We split 20\% of each dataset for validation in the pre-training stage. 
% 80\% of the 1897 CT images collected for pre-training and 20\% for validation

%% BraTS2020数据集共包含369个病例的脑部图像，每个病例图像有4个模态分别是T1、T1Gd、T2、T2-FLAIR和3个分割目标分别为whole tumor (WT)、enhancing tumor (ET), and tumor core (TC)。由于输入图像尺寸较大，因此对每个病例我们会裁剪一块(128,128,128)的补丁在预训练和训练过程中。在预训练过程中，局部重构区域大小为（96，96，96）。
\textbf{BraTS2020 dataset}: BraTS2020 dataset~\cite{menze2014multimodal,bakas2017advancing} contains a total of 369 brain MRI images, and each case image has 4 modalities (namely T1, T1Gd, T2, T2-FLAIR) and 3 segmentation targets (WT: whole tumor, ET: enhancing tumor, TC: tumor core). All the data have been resampled to the same spacing (1.0, 1.0, 1.0). Due to the large size of the input image, we crop the training sub-volume of a size of (128,128,128). 

% BTCV数据集共包含30个病例的3D腹部多器官图像，每个病例图像有1个模态和13个器官分割目标。对每个病例我们会裁剪一块(96,96,96)的补丁在预训练和训练过程中。在预训练过程中，局部重构区域大小为（64，64，64）。
\textbf{BTCV dataset}: BTCV~\cite{BTCV} contains a total of 30 cases of 3D abdominal multi-organ CT images, with 1 modality and 13 organ segmentation targets per case. All cases are resampled to the same spacing (1.5, 1.5, 2.0). We crop the training sub-volumes of a size of (96,96,96).

% MSD Liver数据共包含201个病例的3D肺部图像，每个病例存在1个模态和2个分割目标(肺部和肺部肿瘤)，MSD Spleen数据集包含了61个病例的3D脾脏图像，每个病例有1个模态和1个分割目标(脾脏)。对每个病例我们会裁剪一块(96,96,96)的补丁在预训练和训练过程中。在预训练过程中，局部重构区域大小为（64，64，64）。
We further use two task datasets from MSD dataset~\cite{antonelli2022medical}. \textbf{MSD Liver} contains a total of 201 cases of 3D liver CT images, with 1 modality and 2 segmentation targets (liver, liver tumor) per case. The \textbf{MSD Spleen} contains 61 cases of 3D spleen CT images with 1 modality and 1 segmentation target (spleen) per case. For these two datasets, we resample all the cases to the same spacing (1.5, 1.5, 2.0) and crop the training sub-volumes of a size of (96,96,96).

\subsection{Evaluation Modes:} 
% 为了避免数据泄漏，预训练与训练数据保持一致。所有数据均切分80%作为预训练/训练集，20%作为验证集。
To make a comprehensive comparison, we adopt two evaluation modes to do pre-training and finetuning:
%on the above four downstream segmentation tasks.
%, including BraTS2020, BTCV, MSD-Spleen and MSD-Liver.
\begin{itemize}
    \item Generic pre-training mode: A generic model is pre-trained on our collected pre-training dataset, and finetuned on the four segmentation tasks.
    \item Task-specific pre-training mode: A task-specific model is pre-trained and finetuned on each segmentation task dataset. To avoid data leakage, we do self-supervised pre-training on the training set of the downstream task datasets, which are all split 80\% for training.
\end{itemize}
In finetuning stage for segmentation, we utilize the pre-trained encoder and decoder by replacing the last layer with a new random $c \times 1 \times 1 \times 1$ convolution layer, where $c$ is the number of segmentation targets.


\subsection{Implementation Details}
%\textbf{Basic setup for pre-training.}
% 我们通过多组对照实验选出 最优的一级区域，二级区域与重建的局部区域的大小。看 V. Result, section A for details. 
The default options for the components of HybridMIM are: the two-level masking strategy with a mask ratio of 0.4, the first-level sub-volume size of $32\times 32 \times 32$, the second-level patch size of $16\times 16 \times 16$; the partial region prediction with the reconstructed region size of $96\times 96 \times 96$ for BraTS2020, and $64\times 64 \times 64$ for BTCV, MSD Liver and MSD Spleen. 
%%
These settings are determined via multiple control experiment; See Figure~\ref{fig:pretraining_setting} for details. 
%%
% 对于 MSD 肝脏、MSD 脾脏和 BTCV 实验，我们使用分辨率为 96 × 96 × 96 的随机裁剪图像，并且预训练实验使用每个 GPU 4 个批量大小（使用 $96\times96\times96$ 补丁和 $64\times64 \times64$ 解码大小）。对于 BraTS2020 数据集，我们使用128作为随机裁剪大小，并且每个GPU使用2个批量大小。
For the pre-training dataset, BTCV, MSD liver and MSD spleen, we use randomly cropped images with a resolution of 96 × 96 × 96 and a batch size of 4 per GPU. For BraTS2020 dataset, we use $128\times128\times128$ as the random crop size and a batch size of 2 per GPU. As each image case in BraTS2020 contains 4 modalities, we concatenate each modality in channel dimension at the input of the network.

% 我们使用Pytorch1.12.1-cuda11.3与Monai1.0.0作为基本框架。使用4 个NVIDIA A100 Tensor Core GPU和2个NVIDIA Tesla V100 GPU作为运行环境。
Our model is implemented in Pytorch 1.12.1-cuda11.3 and Monai 1.0.0. 
%所有实验均应用随机翻转、旋转、强度缩放和移位的数据增强变换。优化器使用AdamW以及 1e-4 的初始学习率、 1e-5 的衰减和50个epochs的warmup。
In both pre-training and finetuning, we use an AdamW\cite{loshchilov2017decoupled} optimizer along with a $cosine$ learning rate scheduler (an initial learning rate of 1e-4, a decay of 1e-5, and a warmup\cite{he2016deep} of 50 epochs).
% 对于BraTS2020数据集，我们共运行300个epoch，对于BTCV数据集，我们共运行2000个epoch，对于MSD Liver与Spleen数据集，运行600个epoch。
We run a totoal of 10w step for all pre-training experiments, and
in finetuning, we run 300 epochs for the BraTS2020 dataset, 2000 epochs for the BTCV dataset, and 600 epochs for the MSD Liver and Spleen datasets.
%%
No data augmentation is applied at the pre-training stage, while a light strategy is used in the finetuning: random flip, rotation, intensity scaling and shifts with probabilities of 0.2, 0.2, 0.1, and 0.1, respectively.
%with a probability of 0.2 in each dimension, random rotations 90 degree with a probability of 0.2, intensity scaling and shifts with a probability of 0.1.  
%%
All experiments are conducted on a cloud computing platform with four NVIDIA A100 Tensor Core GPUs and two NVIDIA Tesla V100 GPUs.

% 此外，由于对比学习需要正负样本，批量大小大于1并且网络中必须添加dropout层。
%In addition, since comparison learning requires positive and negative samples, the batch size is larger than 1 and a dropout layer must be added to the network.

\input{tables/btcv_table.tex}

% \subsection{Evaluation Metrics}
% $Dice$ score and 95\% Hausdorff Distance ($HD 95$) are adopted for quantitative comparison. $HD 95$ is based on the calculation of the $95^{th}$ percentile of the distances between boundary points in $X$ and $Y$. 
% \begin{equation}
% Dice=\frac{2|A \cap B|}{|A|+|B|},
% \end{equation}

% \begin{equation}
% HD=\max \left\{\sup _{x \in X} \inf_{y \in Y} d(x, y), \sup _{y \in Y} \inf_{x \in X} d(y, x)\right\},
% \end{equation}

% where A and B denote the ground truth and prediction of
% voxel values. X and Y denote ground truth and prediction surface point sets. $\sup$ represents the supremum, $\inf$ the infimum.

\subsection{Benchmarking}

% 为了使实验结果更有说服力，MP-SSL共与其他六种方法进行对比。这些方法中包含了不同的网络架构，同时也包含了SOTA的监督学习方法与自监督学习方法。
For a thorough evaluation, HybridMIM is compared with SOTA SSL methods as well as fully supervised learning methods, which both cover  CNN and transformer architectures. 

%%
\textbf{Self-supervised methods:} We compare HybridMIM with Models Genesis~\cite{zhou2021models}and TransVW~\cite{haghighi2021transferable}, which are the most recent multi-task SSL methods for 3D medical imaging. 
%%
We also examine an masked image modeling-based SSL method, UNetFormer~\cite{wang2022unetformer}, which is built upon the SwinTransformer architecture. 
% 对于TransVW和ModelGenesis方法，我们使用的是官方开源的权重。对于UNetFormer方法，我们复现了两种预训练模型，UNetFormer*表示在我们收集的1897个CT图像上预训练的通用模型，而UNetFormer表示任务特定的预训练模型，预训练数据与下游任务一致。
As ModelGenesis and TransVW officially release their pre-trained model weights, we skip the pre-training step and conduct finetuning on the four downstream segmentation tasks. 
%%
%Note that since these two methods both utilize a pre-training dataset of 5050 CT images, larger than ours, it is reasonable to start with their models for finetuning.
%%
For UNetFormer, we use its public codes and experiment with it in both evaluation modes. 

%replicate two pre-trained models. UNetFormer* denotes the generic model pre-trained on our collection of 1897 CT images, while UNetFormer denotes the task-specific pre-trained model with pre-trained data consistent with the downstream task.

%%
% 对于监督学习方法，我们选择了SegresNet，UNETR和SwinUNETR作为对比方法。其中SegresNet是一个基于卷积神经网络且性能良好的架构。UNETR与SwinUNETR分别采用ViT transformer与SwinTransformer结构作为编码器，这可以更好的建模全局特征。他们均是最近表现良好的3D医学图像分割方法。
\textbf{Supervised methods:}
We also make comparison with state-of-the-art supervised segmentation methods in medical imaging. 
%%
SegresNet~\cite{myronenko20183d} is a CNN-based architecture with good performance. 
%%
UNETR~\cite{hatamizadeh2022unetr} and SwinUNETR~\cite{hatamizadeh2022swin} are the most recent transformer-based methods for 3D medical image segmentation, which use vision transformer and SwinTransformer structures as encoders, respectively. Here, we use their public codes in the experiments.
%%
% 此外，HybridMIM适用于不同的网络架构。因此我们分别使用UNet和SwinUNETR作为基础架构，并且分别预训练通用模型与任务特定模型，来验证HybridMIM方法对其性能的提升。
%Moreover, HybridMIM applies to different network architectures. Therefore, we use UNet and SwinUNETR as the infrastructure and pre-train the generic and task-specific models, respectively, to verify the performance improvement of the HybridMIM approach.


