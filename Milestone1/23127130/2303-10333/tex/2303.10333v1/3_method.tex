\vspace{-1mm}
\section{Hybrid Masked Image Modeling}
%%
The objective of model pre-training is to effectively encode anatomical information of the human body in unlabeled image data. 
%%
%Unlike previous works on masked image modeling~\cite{doersch2015unsupervised,chen2020generative,pathak2016context}, 
In this work, we propose a two-level masking strategy to facilitate the exploitation of semantic context within one image, from both pixel and regional views. 
%%
To further improve the model's ability to discriminate between different samples, we apply dropout-based constrastive learning within a mini-batch.
%%
Our pre-training is then accomplished with three proxy tasks, which learns the comprehensive semantic representation of images from pixel-level, region-level, and sample-level views.
%%
Figure~\ref{Fig.main2} illustrates the idea, introduced next.

\vspace{-1mm}
\subsection{Masking Strategy and Network Architectures}
%\subsection{Two-level Masking Strategy and Network Architectures}
Assuming the input 3D medical image is $\mathcal{X} \in \mathcal{R}^{H\times W \times D}$, we first divide it equally into multiple non-overlapping sub-volumes (the first level), and then split each sub-volume along three dimensions equally, forming 8 smaller patches (the second level).
%%
Let $R$ denote the number of first-level sub-volumes, and we will have $8R$ second-level patches in total.
%%
Given the division, our masking strategy randomly selects second-level patches to do the masking (removing). 
%%
Then, for each first-level sub-volume, we know how many and which second-level patches are masked. 
%%
It is noted that the number and location information of the masked second-level patches can give a higher-level constraint on the anatomical information.
%%
Hence, in addition to reconstruct the missing pixels, our HybridMIM also pays attention to the masking distribution so as to better learn the spatial anatomical information.

Following the auto-encoder architecture, our approach has an encoder that maps the input image data to the latent feature representation, and a decoder that reconstructs the original image from latent features. 
%%
Since 3D medical images are multi-dimensional and have higher resolutions than natural images, we adopt two widely-used multi-scale segmentation models as the underlying infrastructure, including UNet~\cite{ronneberger2015u} and SwinUNETR~\cite{hatamizadeh2022swin}. 
%%
For UNet that is a convolutional neural network, the masked image is feed into as a whole, while for SwinUNETR that is a transformer method, the masked image is split into tokens whose dimension is set the same as that of the second-level patches. 

\vspace{-1mm}
\subsection{Partial Region Prediction}
%%  SimMIM与UNetFormer方法通过解码器来重构掩蔽的区域学习图像的表征能力，SimMIM通过简化decoder加速自监督学习进程，但是在医学图像领域，由于医学图像数据是高维的和高分辨率的，通常需要一个多尺度的decoder重构图像。因此类似与UNetFormer，我们采用了多级的decoder进行重构，不同的是，我们只选择部分一级区域的high-level特征进行重构，而不是全局重构，大大提升了自监督学习的速度和对局部重要区域关注。
Previous masked image modeling methods~\cite{he2022masked,xie2022simmim} learn the feature representation of the image by reconstructing all the pixels values in the masked regions through a decoder.
%%
SimMIM~\cite{xie2022simmim} accelerates the learning  by adopting a linear layer as the decoder. 
%%
But considering that medical images are multi-dimensional and high-resolution, a multi-scale decoder is needed to reconstruct the image. 
%%
To speed-up the pre-training process, we only select the features of some first-level sub-volumes (the target region) for reconstruction.
%% 局部重构分支被训练通过最小化重构区域与目标区域MASK像素的L2距离。
The partial reconstruction loss is defined as the $L_2$ distance between the reconstructed region and masked pixels in the target region,
\begin{equation}
\vspace{-2mm}
    \mathcal{L}_{\mathrm{PR}} = \frac{1}{|\hat{R}|}\sum_{r \in \hat{R}} || c_{r} - \hat{c}_{r}||_2,
\end{equation}
where $\hat{R}$ is a subset of sub-volumes in the target region, $|\hat{R}|$ is the number of involved sub-volumes, $c_{r}$ and $\hat{c}_{r}$ represent the prediction values and the input values, respectively.
%%
Since the medical images usually have organs in the middle and may contain empty regions near the image boundary, we manually set the target region for one dataset around the image center. 

\subsection{Patch-Masking Perception}
Patch-Masking perception predicts the number and locations of masked patches for each sub-volume. 
%%
This is achieved by adding two separate linear projection layers to the latent feature representation; see Figure~\ref{Fig.main2}.
%%
As the masked patch number ranges within $[0,1,\ldots,8]$, we use a 9-dimensional softmax probability vector to represent the predicted number for $r$-th sub-volume, denoted as $u_r$.
%%
Given the ground truth $\hat{u}$, a cross-entropy loss is used for number prediction task: 
\begin{equation}
    \mathcal{L}_{\mathrm{Num}} = - \frac{1}{R} \sum_{r=1}^{R} \hat{u}_{r} log( {u}_{r}) ,
\end{equation} 
% y代表模型对masked-region的数量的预测，g代表对应的ground truth。
where $R$ denotes the number of first-level sub-volume, $\hat{u}_r$ is represented as a one-hot vector. 

The predicted location $p_r$ is an 8-dimensional probability vector, with each element representing whether the corresponding patch is masked.
%%
Here, we can employ the $\ell_0$ loss function\cite{han2022multimodal} for location prediction task, which counts non-zero elements in $p_r$, defined as:
\begin{equation}
\mathcal{L}_{\mathrm{Loc}}=\sum_{r=1}^R \sum_{k=1}^{8} s_r^k, \text { with } s_r^k=\left\{\begin{array}{ll}
1 & \text { if } \quad {p}_r^k \neq 0 \\
0 & \text { otherwise }
\end{array} ,\right.
\end{equation}
%在位置预测分支中，0表示第r个区域的第i个位置的预测，如果0=0，则此位置被预测为掩蔽区域，否则为非掩蔽区域，因此我们定义L0 损失函数：
where ${p}_r^k$ denotes the predicted probability of the $k$-th patch in the $r$-th sub-volume, and if ${p}_r^k$=0, this patch is predicted as a masked patch, otherwise it is a non-masked patch.
%% 考虑到l0 难优化，二元交叉熵损失函数被应用去估计一级区域中被掩蔽的二级区域的位置编码，公式如下，
Since $\ell_0$ loss is difficult to optimize, a binary cross-entropy loss function is utilized as an alternative:
\begin{equation}
\mathcal{L}_{\mathrm{Loc}} = - \frac{1}{R} \sum_{r=1}^{R} \sum_{k=1}^{8} \hat{p}_{r}^{k} log {p}_{r}^{k} + (1 - \hat{p}_{r}^{k}) log (1 - {p}_{r}^{k}),
\end{equation}
% I代表一级区域中二级区域的数量。
where $\hat{p}$ denotes the ground truth for the location coding of masked patches.

% 此外，在我们的方法中，对一级区域中掩蔽区域数量与位置的预测是各自独立的，但事实上，根据位置编码可以计算出对应的数量。因此，一致性损失被提出来避免位置编码计算得到的数量与预测的数量出现不一致的问题。
In addition, there is a straight-forward constraint between the predicted number and the predicted location information, in which the former can be calculated from the later. Therefore, we define a consistency loss to bridge them:
\begin{equation}
    \mathcal{L}_{\mathrm{Con}}=\frac{1}{2}(\sum_{r=1}^{R}{CE(u_r, \sum_{k=1}^{K}{\tilde{p}_r^k}) + MSE(\sum_{k=1}^{K}{p_r^k}, \tilde{u}_r))} ,
\end{equation}
where $CE$ and $MSE$ represent the cross entropy loss and mean squared error, $\tilde{p}_r^k = \mathds{1}[p_r^k > 0.5]$ gives an 0/1 estimate, and $\tilde{u}_r = argmax(u_r)$ gives the number estimate.


%% include the dataset list table
%\input{LaTeX/tables/dataset.tex}


\subsection{Dropout-based Constrastive Learning}

%% 在自然语言处理领域，SimCSE提出了将dropout作为最朴素的数据增强方式。将两个相同的句子前向计算两次，由于dropout的作用，会得到不同的句子表征。但此时两个句子表征来自于相同的句子，因此两个表征作为正样本。同时，SimCSE将一个batch内的其他句子表征作为负样本，SimCSE通过实验证明了这种方法可以简单有效的提升模型对于文本句子的表征能力。
%In the field of natural language processing, SimCSE\cite{gao2021simcse} proposes dropout as the most naive way of data augmentation. 
%% 类似的，为了提升模型对于3D医学图像数据的表征能力，我们将其思想应用于医学图像分析领域，将同一个图像前向计算两次得到的表征作为正样本，将批次内不同图像得到的表征作为负样本进行对比学习。
In order to further improve the model's generalization ability, we adopt dropout-based contrastive learning that is proven to be effective in the field of natural language processing\cite{gao2021simcse}.
%%
Specifically, an masked image is encoded twice and two latent feature representations will be obtained due to the effect of dropout. 
%% 
Since they come from the same input, they are used as positive samples. 
%%
The latent feature representations obtained from different images within a mini-batch are taken as negative samples for contrastive learning:
\begin{equation}
    \mathcal{L}_{\mathrm{CL}}=-\log \frac{\exp \left(\operatorname{sim}\left(v_i, v_j\right) / t\right)}{\sum_k^{2 N} \mathds{1}_{k \neq i, j} \exp \left(\operatorname{sim}\left(v_i, v_k\right) / t\right)},
\end{equation}% v表示正样本的表征之间的相似度。v表示负样本的表征之间的相似度。
where $t$ is the measurement of normalized temperature scale.
$\mathds{1}$ is the indicator function evaluating to 1 iff $k \neq i$. $v$ denotes the feature representation extracted by the encoder. $sim(v_i,v_j)$ denotes the similarity between the representations of positive samples, and  $sim(v_i,v_k)$ denotes the similarity between the representations of negative samples.

\vspace{-2mm}
\subsection{Loss Function}
%% HybridMIM将区域感知，局部重建，对比学习损失联合起来，进行多层次端到端的自监督学习。
Formally, we minimize a multi-objective loss functions combining masked region perception, partial reconstruction, and dropout-based contrastive learning losses, as follows:
\begin{equation}
    \mathcal{L}=\mathcal{L}_{\mathrm{PR}} + \lambda_{1}  \mathcal{L}_{\mathrm{Num}} + \lambda_{2} \mathcal{L}_{\mathrm{Loc}} + \lambda_{3} \mathcal{L}_{\mathrm{Con}} +  \lambda_{4} \mathcal{L}_{\mathrm{CL}},
\vspace{-2mm}
\end{equation}
where $\lambda_1,\lambda_2,\lambda_3,\lambda_4 $ are empirically set as 0.1, 0.1, 0.01, 0.1 respectively in all the experiments. 



% 在前向过程中，经过随机掩蔽的一批量数据会被输入到编码器两次，来获取两组高层次特征用于计算对比学习损失。其中来自相同图像的特征作为正样本，一个批量内的特征（来自不同图像）作为负样本。此外，对于第一组高层次特征，我们分别加入数量与位置分类器，计算区域感知损失。最后，根据所选择的局部重构区域，我们将对应的高层次特征输入到解码器，计算局部重构损失。
% In the forward process, a mini-batch of randomly masked images are fed into the encoder twice to obtain a set of feature representations for computing the contrastive learning loss. 
% %%
% For one feature representation of a masked image, we compute its region perception loss, respectively. Finally, we input the corresponding high-level features to the decoder and calculate the local reconstruction loss according to the selected local reconstruction region.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MP-SSL 利用一个分层的随机掩蔽思想，不仅根据医学图像的全局信息对掩蔽区域进行重构，并且还会添加对局部区域的关注，更好的学习到医学图像中空间解剖信息。同时，基于dropout的对比学习的加入提升模型对样本之间的特征辨别能力。
%Hybrid Masked Image Modeling (HybridMIM) utilizes a hierarchical random masking idea, which not only reconstructs the masked area based on the global information of the medical image, but also adds attention to the local area to better learn the spatial anatomical information in the medical image. Meanwhile, the addition of dropout-based contrast learning improves the model's ability to discriminate features between samples.

%%像图2中展示的那样，一批量的3D医学图像数据首先被随机的掩蔽，之后通过编码器编码两次，得到两组high-level的特征。对于两组特征，使用对比学习使得组内特征之间的距离更近，同时使得组间特征之间的距离更远。
%As illustrated in Fig. \ref{Fig.main2}, a batch of 3D medical image is first masked randomly and later encoded twice by an encoder to obtain two groups of high-level features. For both groups of features, contrast learning is used to make the intra-group features closer to each other while making the inter-group features farther apart.
%% 同时，通过掩蔽区域感知模块，模型学习在高维空间中对掩蔽区域的数量与位置进行预测，并使用一致性损失函数使得所预测的掩蔽区域数量与位置之间的关系趋于一致。
%At the same time, the model learns to predict the number and location of masked regions in a high-dimensional space through the masked-region perception module, and uses a consistent loss function to make the relationship between the predicted number and location of masked regions converge.
% 最后，在局部重建模块中，手动选择的局部特征被输入到解码器重构局部区域，并与对应的局部原始图像计算重建损失。
%Finally, in the local reconstruction module, manually selected features are input to the decoder to reconstruct the local region and calculate the reconstruction loss with the corresponding local original image.

% \subsection{Masked-region perception(region level)}
% %% MP-SSL设计了一个分层的随机掩蔽方法为了更好的在区域与子区域级别学习局部解剖特征。
% HybridMIM utilizes a hierarchical random masking method to better learn local anatomical features at region level.
% % 我们提出了一级区域与二级区域的概念，将整个3D医学图像划分为多个非重叠的子空间，每个子空间作为一个一级区域，并且在其中随机的掩蔽部分更小的二级区域。例如我们设置一级区域大小为(N,N,N)，二级区域大小为（M，M，M）。因此，每个一级区域包含8个二级区域。此时，我们利用编码后的高水平特征便可以预测其中被掩蔽的二级区域的数量与位置信息，来学习一个局部的解剖特征。
% We propose the concept of first-level and second-level regions, dividing the entire 3D medical image into multiple non-overlapping subspaces, each of which is treated as a first-level region and in which a smaller part of the second-level region is randomly masked. For example, we set the size of the first-level region to $(2^N,2^N,2^N)$ and the size of the second-level region to $(2^{N-1}, 2^{N-1}, 2^{N-1})$. Therefore, each first-level region contains $2 \times 2 \times 2$ second-level regions. At this point, we use the encoded high-level features to predict the number and location information of the masked second-level regions to learn a local anatomical feature.

% %% MP-SSL定义
% HybridMIM treats a 3D medical image as R sub-regions: $I:[x_0, x_1,...,x_R]$, where each $x$ is a first-level region. After that, the encoder $E$ is used to extract the image features $F$.

% %% MP-SSL定义数量与位置分类器f与f2，它们被用来表征一级区域特征，得到掩蔽区域的数量预测向量U与位置预测向量P。
% HybridMIM defines quantity and location classifiers $CLS_{num}$ and $CLS_{loc}$, which are used to characterize the first-level region features to obtain the quantity prediction vector $U$ and location prediction vector $P$ of the masked region.
% \begin{equation}
% U={CLS}_{num}(F)=[u_0,{u}_1,...,{u}_R],
% \end{equation}
% \begin{equation}
% {P}={CLS}_{loc}(F)=[{p}_0,{p}_1,...,{p}_R], 
% \end{equation}
% where each $u$ and $p$ represent the quantity and location prediction vector within the first-level region, respectively.


% %%  数量预测分支使用多分类交叉熵损失函数预测一级区域中被掩蔽的二级区域的数量，公式如下，
% The \textbf{quantity prediction branch} predicts the number of masked second-level regions in the first-level regions using a multi-classification cross-entropy loss function with the following equation, 

% \begin{equation}
%     \mathcal{L}_{num} = - \frac{1}{R} \sum_{r=1}^{R} \hat{u}_{r} log {u}_{r} ,
% \end{equation} 

% % y代表模型对masked-region的数量的预测，g代表对应的ground truth。
% where $R$ denotes the number of first-level region,  $\hat{u}$ denotes the ground truth for the number of masked areas in a first-level region.

% %在位置预测分支中，0表示第r个区域的第i个位置的预测，如果0=0，则此位置被预测为掩蔽区域，否则为非掩蔽区域，因此我们定义L0 损失函数：
% In the \textbf{location prediction branch}, ${p}_r^k$ denotes the prediction of the $k^{th}$ location in the $r^{th}$ region, and if ${p}_r^k$=0, then this location is predicted as a masked region, otherwise it is a non-masked region, so we define the $\ell_0$ loss function\cite{han2022multimodal}.

% \begin{equation}
% \mathcal{L}_{\ell_0}^s=\sum_{r=1}^R \sum_{k=1}^{K} s_r^k, \text { with } s_r^k=\left\{\begin{array}{ll}
% 1 & \text { if } \quad {p}_r^k \neq 0 \\
% 0 & \text { otherwise }
% \end{array} .\right.
% \end{equation}
% %% 考虑到l0 难优化，二元交叉熵损失函数被应用去估计一级区域中被掩蔽的二级区域的位置编码，公式如下，
% Considering that $\ell_0$ is difficult to optimize, a binary cross-entropy loss function is applied to estimate the location encoding of the masked second-level region in the first-level region, with the following equation,
% \begin{equation}
% \mathcal{L}_{pos} = - \frac{1}{R} \sum_{r=1}^{R} \sum_{k=1}^{K} \hat{p}_{r}^{k} log {p}_{r}^{k} + (1 - \hat{p}_{r}^{k}) log (1 - {p}_{r}^{k}),
% \end{equation}
% % I代表一级区域中二级区域的数量。
% where $K$ denotes the number of second-level regions within the first-level region, $\hat{p}$ denotes the ground truth for the location coding of masked areas in a first-level region.

% % 此外，在我们的方法中，对一级区域中掩蔽区域数量与位置的预测是各自独立的，但事实上，根据位置编码可以计算出对应的数量。因此，一致性损失被提出来避免位置编码计算得到的数量与预测的数量出现不一致的问题。
% In addition, in our method, the prediction of the number of masked regions in the first-level region is independent of the location, but in fact, the corresponding quantities can be calculated based on the location encoding. Therefore, consistency loss is proposed to avoid the problem of inconsistency between the quantities obtained from the location encoding and the predicted quantities.

% \begin{equation}
%     L_{consis}=(\sum_{r=1}^{R}{CE(u_r, \sum_{k=1}^{K}{\tilde{p}_r^k}) + MSE(\sum_{k=1}^{K}{p_r^k}, \tilde{u}_r))} / 2 ,
% \end{equation}

% where $CE$ and $MSE$ represent the Cross Entropy loss and Mean Squared Error. $\tilde{p}_r^k$ and $\tilde{u}_r$ represent the location and number result of the masked region in $r^{th}$ first-level region.
% \subsection{Local reconstruction(pixel level)}
% %%  SimMIM与UNetFormer方法通过解码器来重构掩蔽的区域学习图像的表征能力，SimMIM通过简化decoder加速自监督学习进程，但是在医学图像领域，由于医学图像数据是高维的和高分辨率的，通常需要一个多尺度的decoder重构图像。因此类似与UNetFormer，我们采用了多级的decoder进行重构，不同的是，我们只选择部分一级区域的high-level特征进行重构，而不是全局重构，大大提升了自监督学习的速度和对局部重要区域关注。
%  SimMIM and UNetFormer methods learn the representational ability of the image by reconstructing the masked regions through a decoder. SimMIM accelerates the self-supervised learning process by simplifying the decoder, but in the field of medical images, because medical image data are high-dimensional and high-resolution, a multi-scale decoder is usually required to reconstruct the image. Therefore similar to UNetFormer, we use a multi-level decoder for reconstruction, the difference is that we only select the high-level features of some first-level regions for reconstruction instead of global reconstruction, which greatly improves the speed of self-supervised learning and the focus on locally important regions.

% %% 局部重构分支被训练通过最小化重构区域与目标区域MASK像素的L2距离。
% The local reconstruction branch is trained by minimizing the L2 distance between the reconstructed region and masked pixels in the target region.
% \begin{equation}
%     \mathcal{L}_{rec} = \frac{1}{\hat{R}}\sum_{r \in \hat{R}} || c_{r} - C_{r}||^{mask}_2,
% \end{equation}
% where $\hat{R}$, which is a subset of R, denotes the number of local first-level region. $c^{r}_{i}$ and $C^{r}_{i}$ represent the region prediction by the model and the target region values, respectively.

% \input{LaTeX/tables/dataset.tex}

% \subsection{Contrastive learning(sample level)}
% %% 在自然语言处理领域，SimCSE提出了将dropout作为最朴素的数据增强方式。将两个相同的句子前向计算两次，由于dropout的作用，会得到不同的句子表征。但此时两个句子表征来自于相同的句子，因此两个表征作为正样本。同时，SimCSE将一个batch内的其他句子表征作为负样本，SimCSE通过实验证明了这种方法可以简单有效的提升模型对于文本句子的表征能力。
% In the field of natural language processing, SimCSE\cite{gao2021simcse} proposes dropout as the most naive way of data augmentation. 
% % Two identical sentences are calculated forward twice, and different sentence representations will be obtained due to the effect of dropout. But at this time, the two sentence representations come from the same sentence, so the two representations are used as positive samples. At the same time, SimCSE uses other sentence representations in a batch as negative samples. SimCSE has proved through experiments that this method can simply and effectively improve the model's representation ability for text sentences.
% %% 类似的，为了提升模型对于3D医学图像数据的表征能力，我们将其思想应用于医学图像分析领域，将同一个图像前向计算两次得到的表征作为正样本，将批次内不同图像得到的表征作为负样本进行对比学习。
% Similarly, in order to improve the model's characterization ability for 3D medical image data, we apply its idea to the field of medical image analysis by using the characterization obtained from the same image forward computed twice as positive samples and the representation obtained from different images within a batch as negative samples for contrastive learning.
% \begin{equation}
%     \mathcal{L}_{\text {cl}}=-\log \frac{\exp \left(\operatorname{sim}\left(v_i, v_j\right) / t\right)}{\sum_k^{2 N} 1_{k \neq i} \exp \left(\operatorname{sim}\left(v_i, v_k\right) / t\right)},
% \end{equation}% v表示正样本的表征之间的相似度。v表示负样本的表征之间的相似度。
% where $t$ is the measurement of normalized temperature scale.
% 1 is the indicator function evaluating to 1 iff $k \neq i$. $v$ denotes the image representation extracted by the encoder. $sim(v_i,v_j)$ denotes the similarity between the representations of positive samples.  $sim(v_i,v_k)$ denotes the similarity between the representations of negative samples.

% %% HybridMIM将区域感知，局部重建，对比学习损失联合起来，进行多层次端到端的自监督学习。
% HybridMIM combines masked region perception, local reconstruction, and contrastive learning loss for multi-level end-to-end self-supervised learning.

% \begin{equation}
%     \mathcal{L}=\lambda_{0} \mathcal{L}_{rec} + \lambda_{1}  \mathcal{L}_{num} + \lambda_{2} \mathcal{L}_{pos} + \lambda_{3} \mathcal{L}_{consis} +  \lambda_{4} \mathcal{L}_{cl},
% \end{equation}
% where $\lambda_0, \lambda_1,\lambda_2,\lambda_3,\lambda_4 $ are 1.0, 0.1, 0.1, 0.01, 0.1 respectively.

% \subsection{HybridMIM Pre-training}
% %% 预训练的基本设置。
% \textbf{Basic setup for pre-training.}
% % 我们通过多组对照实验选出 最优的一级区域，二级区域与重建的局部区域的大小。看 V. Result, section A for details. 
% We select the optimal size of the first-level region ($32\times 32 \times 32$), second-level region ($16\times 16 \times 16$) and local reconstruction region ($96\times 96 \times 96$) by multiple control experiments. See \ref{pretraining_settings} for details. 
% % 我们选择了一个0.4-0.75的动态掩蔽比率，使用AdamW优化器 with 1e-5的学习率，共训练10w步。
% Referring UNetFormer, we choose a masking ratio of 0.4 and train a total of 10w steps using the AdamW optimizer with a learning rate of 1e-5.
% % 对于通用预训练模型，我们在收集的1897个CT图像中选择80%用于预训练，20%用于验证，共训练5w，而对于任务特定预训练模型，预训练数据与下游任务的训练数据一致。
% For the generic pre-training model, we select 80\% of the 1897 CT images collected for pre-training and 20\% for validation, while for the task-specific pre-training model, the pre-training data are aligned with the training data of the downstream task.
% % 此外，由于对比学习需要正负样本，批量大小大于1并且网络中必须添加dropout层。
% In addition, since comparison learning requires positive and negative samples, the batch size is larger than 1 and a dropout layer must be added to the network.

% % 在前向过程中，经过随机掩蔽的一批量数据会被输入到编码器两次，来获取两组高层次特征用于计算对比学习损失。其中来自相同图像的特征作为正样本，一个批量内的特征（来自不同图像）作为负样本。此外，对于第一组高层次特征，我们分别加入数量与位置分类器，计算区域感知损失。最后，根据所选择的局部重构区域，我们将对应的高层次特征输入到解码器，计算局部重构损失。
% In the forward process, a randomly masked batch of data is fed to the encoder twice to obtain two sets of high-level features for computing the contrast learning loss. The features from the same image are positive samples, and the features within a batch (from different images) are negative samples. In addition, we add the number and location classifiers for the first set of high-level features to compute the region perception loss, respectively. Finally, we input the corresponding high-level features to the decoder and calculate the local reconstruction loss according to the selected local reconstruction region.

