\section{Conclusion}

% 我们提出了一个通用的预训练框架(UniLearn)针对于3D医学图像分割，既可以支持卷积神经网络结构又可以支持Transformer结构。UniLearn的核心是一个多层次的自监督学习策略，分别从像素层次，区域层次，样本层次学习3D医学图像的解剖学特征。我们利用多个器官的分割数据集，选择了有监督学习和自监督学习方法进行对比实验。实验结果表明了经过UniLearn进行预训练后，原架构可以得到较大的性能提升，并实现state-of-the-art的分割结果。
In this paper, we propose a Hybrid Masked Image Modeling framework for 3D medical image segmentation pre-training, which can support CNN and Transformer structures. 
The core of HybridMIM is a multi-level SSL strategy that learns anatomical features of 3D medical images from the pixel-level, region-level, and sample-level, respectively. 
% 相比于其他自监督学习方法，HybridMIM通过掩蔽式图像建模的方式更高效的在像素维度学习高维医学图像数据的解剖特征。同时也通过分层的掩蔽方式，在区域维度学习医学图像数据的空间分布。最后，HybridMIM利用对比学习在样本维度增强模型对于不同数据的区分能力。此外，局部重建等优化方式的加入，也提升了预训练效率，减小预训练模型的成本。
Compared with other SSL methods, HybridMIM learns spatial anatomical features of medical image data more effectively through a hierarchical masking approach, which supports pixel-level partial region prediction and region-level patch-masking perception.
%%
%It also learns the spatial distribution of image data more efficiently at the pixel-level by partial region prediction. 
%%
In addition, HybridMIM uses dropout-based contrastive learning at the sample level to enhance the model's ability to discriminate between different data. 
%In addition, adding optimization methods such as local reconstruction also improves the pre-training efficiency and reduces the cost of pre-training models.

We conduct a thorough comparison with both state-of-the-art SSL methods and supervised learning using segmentation datasets of multiple organs. The experimental results show that after pre-training by HybridMIM, the original network can get outperformed segmentation results.
% 紧接着，我们还通过完备的消融实验，在UNet架构与SwinTransformer架构上证明了UniLearn中每个损失函数的有效性。
We also demonstrate the effectiveness of each loss function in HybridMIM on the UNet architecture and SwinUNETR architecture through comprehensive ablation experiments, 
% 此外，预训练通常需要训练大量数据与大量的步数，因此训练效率是十分重要的。我们设计实验对比了UniLearn与其他自监督学习方法的预训练时间消耗，证明了UniLearn的预训练速度优势。
%In addition, pre-training usually requires training a large amount of data with a large number of steps, so training efficiency is very important. 
and demonstrate the pre-training speed advantage of the proposed HybridMIM.
%%
In the future, we will apply HybridMIM pre-training on more architectures to support other downstream tasks.
%learn the semantic representation of medical images. %Meanwhile, we will also collect more datasets on the pre-training phrase to achieve a higher segmentation accuracy on the downstream tasks.

% \textbf{Acknowledgments.}
% This work was supported by the grant from Tianjin Natural Science Foundation (Grant No. 20JCYBJC00960) and HKU Seed
% Fund for Basic Research (Project No. 202111159073).

