\begin{table}[th]
    \centering
    \vspace{-3mm}
    % 在BraTS2020数据集上进行消融实验。我们选择UNet与SwinTransformer作为backbone，逐个添加我们提出的不同层次的损失函数。其中LR为局部重建损失，Num为数量分布预测损失，Loc为位置分布预测损失，Consis为一致性损失，CL为对比学习损失。下游任务的分割结果展示了我们提出的每个损失函数对于不同架构的有效性。
    \caption{Ablation experiments are performed on the BraTS2020 dataset. $\mathcal{L}_{\mathrm{LR}}$: the local reconstruction loss, $\mathcal{L}_{\mathrm{Num}}$: the number distribution prediction loss, $\mathcal{L}_{\mathrm{Loc}}$: the location distribution prediction loss, $\mathcal{L}_{\mathrm{Con}}$: the consistency loss, $\mathcal{L}_{\mathrm{CL}}$: the contrastive learning loss. }
    %The segmentation results of the downstream task demonstrate the effectiveness of each of our proposed loss functions for different architectures.
    % \vspace{-3mm}
    \label{tab:ablation}
    \renewcommand\arraystretch{1.2}
    \setlength\tabcolsep{5pt}%调列距
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c| l | c c c c c c}

    \hline
    \multirow{2}*{\makecell{Backbone}} & \multirow{2}*{Loss} & \multicolumn{4}{c}{Segmentation Target} & \\
    % \cline{3-7} \cline{10-13}
     & &  WT & TC & ET & Avg & \\
    \hline
    %% LR & Num & Loc & Consis & CL
    \multirow{6}{*}{UNet} & Supervised learning & 89.75 & 84.65 & 78.83 & 84.41 &\\
     & $\mathcal{L}_{\mathrm{PR}}$ & 90.19 & 85.50 & 79.48 & 85.06 & \\
     & $\mathcal{L}_{\mathrm{PR}} + \mathcal{L}_{\mathrm{Num}}$ & 90.05 & 85.48 & 79.97 & 85.17 & \\
     & $\mathcal{L}_{\mathrm{PR}} + \mathcal{L}_{\mathrm{Num}} + \mathcal{L}_{\mathrm{Loc}}$ & 90.15 & 85.65 & 80.10 & 85.30 & \\
     & $\mathcal{L}_{\mathrm{PR}} + \mathcal{L}_{\mathrm{Num}} + \mathcal{L}_{\mathrm{Loc}} + \mathcal{L}_{\mathrm{Con}}$ & 90.30 & 85.36 & \textbf{80.56} & 85.40 & \\
     & $\mathcal{L}_{\mathrm{PR}} + \mathcal{L}_{\mathrm{Num}} + \mathcal{L}_{\mathrm{Loc}} + \mathcal{L}_{\mathrm{Con}} + \mathcal{L}_{\mathrm{CL}}$ & \textbf{90.62} & \textbf{86.28} & {80.17} & \textbf{85.69} & \\
     \hline
     
     \multirow{6}{*}{Swin} & Supervised learning & 90.08 & 85.19 & 80.01 & 85.09 &\\
     & $\mathcal{L}_{\mathrm{PR}}$ & 90.95 & 86.17 & 80.22 & 85.78 & \\
     & $\mathcal{L}_{\mathrm{PR}} + \mathcal{L}_{\mathrm{Num}}$ & 90.93 & 86.94 & 80.48 & 86.12 & \\
     & $\mathcal{L}_{\mathrm{PR}} + \mathcal{L}_{\mathrm{Num}} + \mathcal{L}_{\mathrm{Loc}}$ & 91.18 & 86.33 & \textbf{81.10} & 86.20 & \\
     & $\mathcal{L}_{\mathrm{PR}} + \mathcal{L}_{\mathrm{Num}} + \mathcal{L}_{\mathrm{Loc}}  + \mathcal{L}_{\mathrm{Con}}$ & 90.98 & \textbf{87.06} & 80.71 & 86.24 & \\
     & $\mathcal{L}_{\mathrm{PR}} + \mathcal{L}_{\mathrm{Num}} + \mathcal{L}_{\mathrm{Loc}}  + \mathcal{L}_{\mathrm{Con}} + \mathcal{L}_{\mathrm{CL}}$ & \textbf{91.48} & {86.88} & {80.81} & \textbf{86.39} & \\
    %  \hline

     
    % \multirow{4}{*}{Swin} & & &  & & & & & & 90.08 & 85.19 & 80.01 & 85.09 & \\
    %  & & \checkmark & & & &  & & & 90.95 & 86.17 & 80.22 & 85.78 & \\
    %  & & \checkmark & \checkmark & & &  & & & 90.93 & 86.94 & 80.48 & 86.12 & \\
    %  & & \checkmark & \checkmark & \checkmark & &  & & & 91.18 & 86.33 & 81.10 & 86.20 & \\
    %  & & \checkmark & \checkmark & \checkmark & \checkmark &  & & & 90.98 & 87.06 & 80.71 & 86.24 & \\
    %  & & \checkmark & \checkmark &\checkmark & \checkmark & \checkmark & & & {91.48} & {86.88} & {80.81} & {86.39} & \\
    \hline
    \end{tabular}
    }
    \vspace{-2mm}
\end{table}

