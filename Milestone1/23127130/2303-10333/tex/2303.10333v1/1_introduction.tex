\begin{figure}[!t] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
\centering %图片居中
\includegraphics[width=0.5\textwidth]{figures/introduction_v6.pdf} %插入图片，[]中设置图片大小，{}中是图片文件名
% 我们描述我们的想法以2D图像的形式。我们将整个图像分成多个子空间，在子空间中掩蔽部分更小的区域。我们以顺时针的方式对掩蔽与非掩蔽区域进行编码来得到子空间中掩蔽区域的数量与位置信息，并作为模型的预测目标。同时选择局部区域进行重构，有利于一个更快的预训练效率学习医学图像的空间解剖信息。此外，MP-SSL被设计不受网络架构的约束。
\caption{Illustration of our key idea in the 2D form. We regularly divide the input image into two-levels of patches, masking sub-regions or patches randomly. The masking information is encoded into binary, providing the locations and number of masked patches. In addition, local regions are selected for reconstruction, which facilitates a faster pre-training efficiency for high-dimensional medical images.}\label{fig:idea}
%We describe our ideas in the form of 2D images. We divide the whole image into several subspaces, masking partly smaller regions in the subspaces. We encode the masked and unmasked regions in a clockwise manner to obtain information on the number and location of masked regions in the subspace, which is used as a prediction target for the model. Also selecting local regions for reconstruction facilitates a faster pre-training efficiency to learn the spatial anatomical information of medical images. In addition, MP-SSL is designed to be unconstrained by the network architecture.} %最终文档中希望显示的图片标题
\label{Fig.main2} %用于文内引用的标签
\vspace{-3mm}
\end{figure}

\section{Introduction}
\label{sec:introduction}
% 在医学图像分析领域中，医学图像分割是一个热门且广泛应用的任务，它是非常有用的对于病灶的诊断与量化以及治疗的计划与评估。
\IEEEPARstart{I}n the field of medical image analysis\cite{litjens2017survey,khan2014survey,shamshad2201transformers}, automatic medical image segmentation remains a hot topic to facilitate lesion diagnosis and quantification, as well as treatment planning and evaluation.
% 近些年深度学习算法已经获得更为准确的分割结果，能更好的辅助专业医生进行判断，减轻医生审查MRI or CT影像的负担。
In recent decades, many deep learning-based algorithms for medical image segmentation have been developed to obtain improved segmentation performance. 
%, which can better assist medical professionals in making judgments and reduce the burden of reviewing magnetic resonance imaging (MRI) or computed tomography (CT) image.
%% 训练深度学习算法通常需要大量的人工标注数据，然而在医学图像分析领域，有标注的数据是十分稀缺和宝贵的。同时，可靠的标注需要多名专业医生协作，十分耗时且昂贵。
It is worth noting that training deep learning-based algorithms usually requires a large amount of manually labeled data. 
%%
Yet in the field of medical image analysis, the amount of labeled 3D medical data is often small compared to natural images,
since labeling 3D medical data is not only tedious and time-consuming, but also needs intensive involvement of physicians to provide considerable domain expertise~\cite{tajbakhsh2020embracing}. 

%% 得不到充分的训练，有监督学习算法通常会达到性能瓶颈，无法更好的提升所处理任务的精度。
%Without sufficient training, supervised learning algorithms~\cite{zheng20153d,cai2021deep} usually reach performance bottlenecks and cannot better improve the accuracy of the processed tasks. 
% 为了克服有标注数据不充足的情况，自监督学习被提出利用无标注的语言/图像数据，使模型学习到领域内数据的通用表征。经过自监督学习后的模型在下游任务中finetune时拥有更快的收敛速度与更高的精度。
To alleviate the problem of insufficient labeled data, self-supervised learning (SSL) schemes~\cite{atito2021sit,dai2021up,liang2021swinir,doersch2017multi} 
have been developed to learn a generic representation of unlabeled data, and the obtained models can have faster convergence and higher accuracy when fine-tuned for downstream tasks.
%% 当前已经有一些有效的自监督学习方法被开发，例如自监督对比学习，掩蔽式语言/图像建模等。
% 其中对比学习通过构建正负样本对来学习不同样本之间的关系特征，而掩蔽式语言/图像建模则是通过掩蔽部分输入数据(语言/图像)，并让模型学习去重建被掩蔽的部分。
Typical self-supervised learning schemes include using tailored proxy tasks~\cite{gidaris2018unsupervised,zhou2021models}, self-supervised contrastive learning~\cite{gao2021simcse}, and masked image modeling~\cite{he2022masked}.
%%
The former designs specific pretext tasks, such as inpainting, random rotation, distortion, for image restoration. 
Contrastive learning learns the relational characteristics between different samples by constructing positive and negative sample pairs. Masked image modeling, which is a recently developed technique, exhibits good potentials by learning the hidden context features through predicting what the masked parts of the input data should be. 
%reconstruct the masked parts by masking parts of the input data (language/image).
% 但是上述自监督学习方法专门为自然图像或自然语言开发。其与医学图像之间存在鸿沟，例如医学图像数据维度高，并且包含多种模态(computed tomography (CT) and magnetic resonance imaging (MRI))，导致无法很好的迁移到医学图像领域。
%the self-supervised learning methods described above were developed specifically for natural images or natural language. 

It is noted that most self-supervised learning methods are developed for natural images, and may not migrate well to the medical image domain~\cite{tang2022self}, since 3D medical image data has much higher dimensionality and can contain multiple modalities.
% 目前已经有一些方法探索如何将自监督学习方法应用于医学图像领域，例如有的方法利用聚类算法主动发现语义视觉词并将其作为自监督学习标签，使模型学习到了医学图像中丰富的语义信息，但是其预训练需要构建自监督学习标签，导致预训练步骤较多。
There have been several approaches to explore how to apply self-supervised learning to 3D medical images. For example, the visual word learning\cite{haghighi2021transferable} uses auto-encoder to actively discover semantic visual words and takes them as self-supervised learning labels, allowing the model to learn the rich semantic information. However, its pre-training requires the construction of self-supervised learning labels, resulting in more pre-training burden.
% 还有方法将掩蔽式图像建模与医学图像相结合，通过随机掩蔽输入图像的部分区域，来学习医学图像数据中的高维的空间解剖特征，但其模型结构具有局限性，而且使用全局重建导致预训练速度较慢。
UNetFormer~\cite{wang2022unetformer} utilizes a 3D Swin Transformer encoder, and directly applies masked image modeling by predicting randomly masked volumetric tokens. Just like current MAE~\cite{he2022masked} and SimMIM methods~\cite{xie2022simmim}, it recovers missing pixels, which are from a lower semantic level. What's more, reconstructing the whole image is time-consuming and leads to slow pre-training performance. 

%to learn high-dimensional spatial anatomical features in \textcolor{red}{medical image data by randomly masking parts of the input 3D image but its model structure has limitations. It does not test multiple model structures, and the use of global reconstruction leads to slower pre-training.}
% 在这个工作中，我们提出了一个混合的 MIM 自监督学习框架（Hybrid MIM）对3D医学图像分割问题，它通过多个特殊设计的子任务，从像素级别，区域级别，样本级别三个维度学习3D医学图像的语义表征。
%To alleviate the limitations of previous methods, 
In this paper, we propose a new hybrid self-supervised learning framework (HybridMIM) based on hierarchical masked image modeling for 3D medical image segmentation. 
Our framework learns the comprehensive semantic representation of 3D unlabeled medical images at pixel/region/sample three levels, 
%\ie, pixel level, region level and sample level.
%%
Specifically, as illustrated in Figure~\ref{fig:idea}, we design a two-level masking hierarchy to support masking sub-regions or patches randomly, and propose a joint self-supervised learning to account for semantic information at different levels.
%%
At the pixel level, instead of predicting all the masked patches as MAE~\cite{he2022masked}, we utilize a partial region prediction scheme to reconstruct key parts of the 3D images, which can largely accelerate the pre-training.  
%%
The masking status which reflects the locations and number of masked sub-regions, can help the model to characterize anatomical information at region-level. 
%%
Finally, we utilize a dropout-based contrastive learning strategy at the sample level, to improve the model's capability of discrimination between different samples.

% 首先，在像素级别，我们提出了一个局部的屏蔽区域重构的方法，通过灵活的选择局部的区域，提升自监督学习的效率和对重要区块的重建能力。
%First, at the pixel level, we propose a local masked region reconstruction method to improve the efficiency of self-supervised learning and the reconstruction ability of important blocks by flexibly selecting local regions.

% 其次，为了提升模型对局部空间区域的表征能力，我们提出了一个新奇的屏蔽的区域感知模块。与其他全局的随机掩蔽的方法不同的是，我们利用分层的掩蔽添加对局部区域的感知。屏蔽的区域感知模块将整个3D图像划分为多个子空间并在每个子空间中随机掩蔽部分区域，紧接着使模型预测每个子空间中掩蔽区域的数量与位置来帮助模型学习到局部空间的解剖学信息。
%to improve the model's ability to characterize local spatial regions, we propose a novel masked region perception module. Unlike other global random masking approaches, we add local region perception using hierarchical masking. The masked region perception module divides the entire 3D image into multiple subspaces and randomly masks some regions in each subspace, and then allows the model to predict the number and location of masked regions in each subspace to help the model learn anatomical information about the local space.

% 此外，我们还引入了一个基于dropout的对比学习策略，提升模型对于医学图像中不同样本表征的分辨能力。
%In addition, at the sample level, we also introduce a dropout-based contrast learning\cite{chen2021empirical,oord2018representation,park2020contrastive,gao2021simcse} strategy to improve the model's ability to discriminate between different sample representations in medical images.

%% 为了验证我们提出的方法的有效性，我们探索了两种预训练模式，预训练，finetuning使用相同数据或使用不同的数据。我们收集了公开的1897个CT数据，来预训练通用的模型（预训练，finetuning使用不同的数据），可以在不同的下游任务中进行迁移学习。此外，我们选择了四个不同器官，不同模态的公开数据集，在每个数据集中预训练任务特定模型（预训练，finetuning使用相同数据）。四个下游任务数据集分别是BraTS2020，BTCV，MSD-Spleen和MSD-Liver。其中BraTS2020数据集为脑部MRI多模态分割数据集，BTCV为腹部多器官分割数据集。MSD-Spleen为脾脏分割数据集，MSD-Liver为肝脏与肝脏肿瘤分割数据集。
Our proposed HybridMIM is compatible with different networkd architectures, and we adopt UNet and SwinUNETR as the underlying architectures.
%%
The effectiveness of our method is validated via two evaluation modes: pre-training and finetuning using the same data or using different data, on four downstream segmentation tasks, including BraTS2020~\cite{menze2014multimodal,bakas2017advancing}, BTCV~\cite{BTCV}, MSD-Spleen and MSD-Liver~\cite{antonelli2022medical}, which cover different organs, different object numbers and multi-modalities.
%BraTS2020 is a multi-modality brain MRI segmentation dataset, BTCV\cite{landman2015miccai} is an abdominal multi-organ segmentation dataset, MSD-Spleen is a spleen segmentation dataset, and MSD-Liver is a liver and liver tumor segmentation dataset.
%%
The comprehensive experimental results show that our HybridMIM outperforms existing state-of-the-art self-supervised methods in terms of both downstream segmentation task accuracy and training efficiency.

%We collected publicly available 1897 computed tomography (CT) images to pre-train a generic model (pre-training, finetuning using different data) that can perform transfer learning in different downstream tasks. In addition, we selected four publicly available datasets with different organs and modalities to pre-train task-specific models in each dataset (pre-training and finetuning using the same data). 

%% 为了验证我们提出的方法的通用性，我们选择了UNet与SwinUNETR架构作为backbone，利用HbridMIM方法预训练通用模型和不同数据集上的任务特定模型，实验结果表明我们提出的 HybridMIM 在训练效率，下游分割任务精度上比其他方法更有优势。
%To validate the enhancement of our proposed method for different architectures, we chose UNet and SwinUNETR as the underlying architectures and utilize the HybridMIM to pre-train generic and task-specific models on different datasets. The experimental results show that our proposed HybridMIM has advantages over other methods in terms of training efficiency and downstream segmentation task accuracy.
%% 此外，我们还进行了消融实验，证明了各个子任务的有效性。
%In addition, we also conduct ablation experiments to demonstrate the effectiveness of each subtask.
%% 总之，我们提出的基于掩蔽区域感知的自监督学习框架有以下几个特点：
%% 1. 协同的-实现了优秀的多任务协同学习能力通过子任务之间的促进。
%% 2. 通用的 - 适配不同的网络架构for更广泛的数据集。
%% 3. 高效的与鲁棒的 - 重建局部区域for 高效并且协同多任务for鲁棒.
In summary, our HybridMIM self-supervised learning method has the following dominant features:
\begin{itemize}
    \item Compatibility. Our method can serve as a general pre-training framework, and we demonstrate that it supports both multi-scale convolutional neural network and transformer network architectures.
    \item Comprehensiveness. Our method is designed to learn the semantic information within the data from pixel-level, region-level to sample-level.
    \item Robustness. Our method is proved to outperform SOTA methods in accuracy and training efficiency for multiple widely-used public segmentation datasets.
\end{itemize}