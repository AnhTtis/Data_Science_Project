\section{Methodology}

\subsection{Preliminary}

We adopt a common practice~\cite{deng2019accurate,feng2021learning} to represent a textured coarse shape with a 3D face model, an illumination model, and a camera model.
%

\myparagraph{3D Face Model.}
%
The 3D shape $\mathbf{S}$ and albedo $\mathbf{A}$ are represented by:
\begin{equation}
\begin{aligned}
\mathbf{S} &= \bar{\mathbf{S}}+\boldsymbol{\beta} \mathbf{B}_{\text{id}}+\boldsymbol{\xi} \mathbf{B}_{\text{exp}} \\
\mathbf{A} &= \bar{\mathbf{A}}+\boldsymbol{\alpha} \mathbf{B}_{\text{alb}}
\end{aligned},
\label{Eq.shape3dmm}
\end{equation}
%
where $\bar{\mathbf{S}}$ and $\bar{\mathbf{A}}$ are the mean shape and albedo. $\mathbf{B}_{\text{id}}$, $\mathbf{B}_{\text{exp}}$, and $\mathbf{B}_{\text{alb}}$ are bases~\cite{wood2022dense} of $256$-dim identity, $233$-dim expression, and $300$-dim albedo, respectively.
%
The coarse shape $\mathbf{S}$ in the bind pose is deformed from a neutral shape $\mathbf{S}_{\text{neu}}=\bar{\mathbf{S}}+\boldsymbol{\alpha} \mathbf{B}_{\text{id}}$ with expression component $\boldsymbol{\xi}\mathbf{B}_{\text{exp}}$.
%
$\boldsymbol{\beta}$, $\boldsymbol{\xi}$, and $\boldsymbol{\alpha}$ are the corresponding identity, expression, and albedo coefficients for generating a coarse shape. In this paper, the coarse shape $\mathbf{S}$ contains $n_v=7,667$ vertices and $n_f=14,832$ triangles with $512\times512\times3$ albedo.
% 

\myparagraph{Pose \& Camera Model.}
To estimate the face pose, we follow~\cite{wood2021fake,wood2022dense} to predict skeletal pose $\mathbf{p}=[\boldsymbol{\theta}| \mathbf{t}]$, where $\boldsymbol{\theta} \in \mathds{R}^{3j}$ and $\mathbf{t} \in \mathds{R}^{3}$ are the local joint rotations and root joint translation, respectively. $j=4$ indicates $4$ skeletal joints {\wrt} the head, neck, and two eyes.
%
We perform a standard linear blend skinning (LBS) function~\cite{lewis2000pose} (with per-vertex weights $\mathbf{W}\in \mathds{R}^{j\times n_v}$) to rotate $\mathbf{S}$ about joint locations $\mathbf{J}\in\mathds{R}^{3j}$ by $\mathbf{p}$ to obtain $\mathbf{S}_{\mathbf{p}}$:
\begin{equation}
    \mathbf{S}_{\mathbf{p}}=\text{LBS}(\mathbf{S},\mathbf{p}, \mathbf{J};\mathbf{W}),
\end{equation}
where $\mathbf{J}$ is the joint locations in the bind pose determined by $\mathbf{J}=\mathcal{J}(\boldsymbol{\beta}): \mathds{R}^{|\boldsymbol{\beta}|}\rightarrow \mathds{R}^{3j}$.
Then we use an orthographic camera model to project 3D vertices in $\mathbf{S}_{\mathbf{p}}$ to the 2D plane.





\myparagraph{Illumination Model.}
%
We follow previous work~\cite{deng2019accurate} to use Spherical Harmonics (SH)~\cite{ramamoorthi2001efficient} to estimate the illumination of a given image. The shaded texture $\mathbf{T}$ is computed as:
\begin{equation}
    \mathbf{T} =  \mathbf{A}\odot \sum\nolimits_{k=1}^{9}\boldsymbol{\gamma}_k \mathbf{\boldsymbol{\Psi}}_{k} (\mathbf{N}),
    \label{Eq.texture}
\end{equation}
where $\odot$ denotes the Hadamard product, $\mathbf{N}$ is the surface normal of $\mathbf{S}$ in UV coordinates, $\boldsymbol{\Psi}: \mathds{R}^{3}\rightarrow \mathds{R}$ are SH basis function and $\boldsymbol{\gamma}\in \mathds{R}^9$ is the corresponding SH coefficient.


 


\subsection{Overview of {\name}}
\vspace{-2pt}

\myparagraph{Key Idea.}
%
The key idea of {\name} is to explicitly model the static (\eg, person-specific properties) and dynamic ({\eg}, expression-driven wrinkles) details, allowing the model to reconstruct a high-fidelity 3D face from a single image with realistic and animatable details.
 

\myparagraph{Overview.}
The goal of {\name} is to reconstruct 3D shapes with realistic details from a single image. The overview of {\name} is illustrated in Fig.~\ref{fig:pipeline}(a).
We leverage a feature extractor (\ie, ResNet-50~\cite{he2016deep}) to regress corresponding coefficients from an input image.
Our model jointly predicts both the coarse-level shapes and the fine-level details. For coarse-level shapes, we regress shape parameters ({\ie}, identity, expression, albedo, illumination, and pose) of a parametric face model. For the fine-level details, we propose a novel way to model it through the separation of static and dynamic factors and formulate the generation of details into the problems of 3DMM coefficients regression and displacement maps interpolation.
%
% 
%

Note that the facial details are based on the coarse shape, we thereby exploit novel loss functions to learn 3D representations of coarse shape and details simultaneously from the synthetic dataset with ground-truth labels. To generalize our model to real-world images, we also present several self-supervised losses to train the model with both synthetic data and real-world images coherently.
% 
%
As a result, {\name} can faithfully reconstruct the facial details of a given image, or animate a face by combining the decoupled static and dynamic coefficients that come from different individuals.
% 




\subsection{Decoupling Static and Dynamic Details}


We propose \textbf{S}tatic and \textbf{D}ynamic \textbf{De}coupling for De\textbf{Tail} Reconstruction ({\module}).
% 
The facial details are basically composed of a static factor and a dynamic factor:
\begin{equation}
    \mathbf{D} = \mathbf{D}_{\text{sta}} + \mathbf{D}_{\text{dyn}},
    \label{Eq.outdisp}
\end{equation}
%
where $\mathbf{D}_{\text{sta}}$ and $\mathbf{D}_{\text{dyn}}$ indicate details from static and dynamic factors, respectively.
%



Concretely, the static factor is the inherent property of the identity ({\ie}, the given 2D face), and originates from the appearance and age attributes. As for the dynamic factor, it is typically driven by the expression and influenced by person-specific properties.


% 
\myparagraph{Static Detail Generation.}
% 
To simplify the problem, we are inspired by 3DMMs, which parameterize the statistical models to simplify the 2D-to-3D problem. We build a $300$-dim displacement basis $\mathbf{B}_{\text{sta}}$ from the captured $332$ scans~\cite{raman2022mesh} by PCA~\cite{abdi2010principal}. The scans contain diverse age groups in a neutral expression. Then we regress the coefficient $\boldsymbol{\varphi}$ to synthesize the static detail $\mathbf{D}_{\text{sta}}$ from the image:
\begin{equation}
\begin{aligned}
\mathbf{D}_{\text{sta}} =  \bar{\mathbf{D}}_{\text{sta}}+\mathbf{\boldsymbol{\varphi}} \mathbf{B}_{\text{sta}}
\end{aligned},
\label{Eq.age3dmm}
\end{equation}
where $\bar{\mathbf{D}}_{\text{sta}}$ and $\mathbf{B}_{\text{sta}}$ are the mean displacement map and displacement basis for static details, respectively.




\myparagraph{Dynamic Detail Generation.}
%
Due to the high diversity and complexity of expression representation, directly generating dynamic details from expression is quite difficult.
Therefore we simplify the expression representation by using an interpolation between two displacement maps: compressed and stretched~\cite{raman2022mesh}.
% 
For example, the compressed expression may indicate a state of frowning to the extreme, while the stretched expression may indicate a state of complete relaxation between the eyebrows. Other states of this area can be interpolated by these two polarized states.


Consequently, we generate the dynamic details through compressed and stretched displacement maps. Again, we build $26$-dim compressed $\mathbf{B}_{\text{com}}$ and stretched $\mathbf{B}_{\text{str}}$ displacement bases by PCA~\cite{abdi2010principal} to simplify the generation of displacement maps.
%
To generate the dynamic coefficients $\boldsymbol{\phi}=\{\boldsymbol{\phi}_{\text{com}},\boldsymbol{\phi}_{\text{str}}\}$, we apply the expression coefficient $\boldsymbol{\xi}$ into the static coefficient $\boldsymbol{\varphi}$ through AdaIN~\cite{huang2017arbitrary}, followed by the MLP transformation $\boldsymbol{\Phi}$ to obtain $\boldsymbol{\phi}$:
\begin{equation}
    \boldsymbol{\phi} = \boldsymbol{\Phi}\bigg(\sigma\big(\tilde{\boldsymbol{\xi}}\big)\Big(\frac{\boldsymbol{\varphi}-\mu(\boldsymbol{\varphi})}{\sigma(\boldsymbol{\varphi})}+\mu\big(\tilde{\boldsymbol{\xi}}\big)\Big)\bigg)
    \label{Eq.adain},
\end{equation}
where $\tilde{\boldsymbol{\xi}}$ is the affined vector from $\boldsymbol{\xi}$ via MLP transformation. $\mu$ and $\sigma$ indicate the mean and standard deviation.
%
$\boldsymbol{\phi}_{\text{com}}$ and $\boldsymbol{\phi}_{\text{str}}$ are coefficients for compressed and stretched displacement maps respectively.
%

Similar to Eq.~\ref{Eq.age3dmm}, the compressed and stretched displacement maps are formulated as:
\begin{equation}
\begin{aligned}
\mathbf{D}_{\text{com}} &=  \bar{\mathbf{D}}_{\text{com}}+\boldsymbol{\phi}_{\text{com}} \mathbf{B}_{\text{com}} \\
\mathbf{D}_{\text{str}} &=  \bar{\mathbf{D}}_{\text{str}}+\boldsymbol{\phi}_{\text{str}} \mathbf{B}_{\text{str}}
\end{aligned},
\label{Eq.exp3dmm}
\end{equation}
where $\bar{\mathbf{D}}_{\text{com}}$ and $\mathbf{B}_{\text{com}}$ are the mean displacement map and $26$-dim displacement basis for compressed detail, and $\bar{\mathbf{D}}_{\text{str}}$ and $\mathbf{B}_{\text{str}}$ are the mean displacement map and $26$-dim displacement basis for stretched detail, respectively.



Considering the coarse shape $\mathbf{S}$ can be obtained by deforming the neutral shape $\mathbf{S}_{\text{neu}}$ with the expression component $\boldsymbol{\xi}\mathbf{B}_{\text{exp}}$, such expression-driven deformation over face shape yields the ``tension'' over each vertex~\cite{raman2022mesh}, which influences facial details from expression.
%
Since $\mathbf{S}_{\text{neu}}$ and $\mathbf{S}$ posses the same topology, for each vertex $\mathbf{v}_i\in \mathbf{S}$ with $K$-edges $E_{i}=\{e_1,\cdots,e_{K}\}$ connected with $\mathbf{v}_i$, $E'_{i}=\{e'_1,\cdots,e'_{K}\}$ are the corresponding edges in $\mathbf{S}_{\text{neu}}$ that are connected to $\mathbf{v}'_i$. Then the tension at $\mathbf{v}_i$ is:
\begin{equation}
t_{\mathbf{v}_i}=1-\frac{1}{K}\sum\nolimits_{k=1}^{K} \frac{ \| e_k  \| }{\| e'_k  \| },
% 
\label{Eq.tension}
\end{equation}
where $\| \cdot \|$ represents the edge length. Positive values of $t_{\mathbf{v}_i}$ indicate compression, negative values indicate stretch, and $0$-value indicates no change, respectively.


The vertex tension $t_{\mathbf{v}_i}$ in $\mathbf{S}$ composes the tension map $\mathbf{M}_{\text{uv}}$ in UV coordinates. Then, the displacement map of the dynamic detail is the linear interpolation of $\mathbf{D}_{\text{com}}$ and $\mathbf{D}_{\text{str}}$:
\begin{equation}
    \mathbf{D}_{\text{dyn}} = \mathbf{M}_{\text{uv}}^{+} \odot \mathbf{D}_{\text{com}} + \mathbf{M}_{\text{uv}}^{-}\odot \mathbf{D}_{\text{str}},
    \label{Eq.dyn_detail}
\end{equation}
where $\mathbf{M}_{\text{uv}}^{+}$ and $\mathbf{M}_{\text{uv}}^{-}$ indicate the positive and negative value of $\mathbf{M}_{\text{uv}}$, respectively. Fig.~\ref{fig:detail_example} shows the effectiveness of {\module}. The dynamic factor interpolated by two polarized states introduces expression-related details and further decorates the static detail, yielding the final vivid output.



\begin{figure}
    \centering
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img/detail_pipeline.jpg}
    \end{overpic}
    \put(-225,122){\bfseries\scriptsize (a)}
    \put(-182,122){\bfseries\scriptsize (b)}
    \put(-110,122){\bfseries\scriptsize (c)}
    \put(-35,122){\bfseries\scriptsize (d)}
    \vspace{-5pt}
    \caption{\textbf{Illustration of displacement map composition in {\module}.} Given (a). an image (top) to reconstruct its coarse shape (bottom), we formulate the detail as (b). a static factor and (c). a dynamic factor interpolated by polarized states {\wrt} compressed (top) and stretched (bottom). (d). the output displacement map is linearly combined by (b) and (c) to present vivid details.}
    \label{fig:detail_example}
    \vspace{-10pt}
\end{figure}



\subsection{Overall Loss Functions}

%

We propose several loss functions to train {\name} end-to-end. Specifically, we use static and dynamic detail losses to supervise the synthesized displacement maps from $\boldsymbol{\varphi}$ and $\boldsymbol{\phi}$. In addition, we leverage the coarse shape loss to supervise the reconstructed shape from $\boldsymbol{\beta}$ and $\boldsymbol{\xi}$. Finally, we follow previous methods~\cite{deng2019accurate,feng2021learning,wood2022dense} to leverage the differentiable renderer~\cite{genova2018unsupervised} to map the generated 3D shape into 2D images, by combining $\boldsymbol{\alpha}$, $\boldsymbol{\beta}$, $\boldsymbol{\xi}$, $\boldsymbol{\gamma}$, $\mathbf{p}$, $\boldsymbol{\varphi}$, $\boldsymbol{\phi}$. Then, we perform self-supervised losses to train in both synthetic and real-world images. See more details in the supplementary.
%
%




\myparagraph{Static and Dynamic Detail Losses.}
%
To explicitly train the details of each component, we leverage the ground-truth annotations from the synthetic dataset~\cite{raman2022mesh,wood2021fake} as supervision to assist the training process of our model. Specifically, we calculate the detail losses by estimating the $l_2$ distance between the reconstructed displacement maps and ground-truth {\wrt} static, compressed, and stretched components, and summarize them as $\mathcal{L}_{\text{detail}}$:
\begin{equation}
\begin{aligned}
     &\mathcal{L}_{\text{sta}}=\big\| \mathbf{M}_{\text{detail}}\odot (\mathbf{D}_{\text{sta}}-\hat{\mathbf{D}}_{\text{sta}}) \big\|_2 \\
     &\mathcal{L}_{\text{com}}=\big\| \mathbf{M}_{\text{detail}}\odot (\mathbf{D}_{\text{com}}-\hat{\mathbf{D}}_{\text{com}})  \big\|_2 \\
     &\mathcal{L}_{\text{str}}= \big\| \mathbf{M}_{\text{detail}}\odot (\mathbf{D}_{\text{str}}-\hat{\mathbf{D}}_{\text{str}})  \big\|_2 \\
     &\mathcal{L}_{\text{detail}} = \mathcal{L}_{\text{sta}} + \mathcal{L}_{\text{com}} + \mathcal{L}_{\text{str}}
\end{aligned},
    \label{Eq.disp}
\end{equation}
where $\mathbf{M}_{\text{detail}}$ is the facial mask in the UV coordinates, and $\hat{\mathbf{D}}_{\text{sta}}/\hat{\mathbf{D}}_{\text{com}}/\hat{\mathbf{D}}_{\text{str}}$ and $\mathbf{D}_{\text{sta}}/\mathbf{D}_{\text{com}}/\mathbf{D}_{\text{str}}$ are the reconstructed and ground-truth displacement maps, respectively.


\myparagraph{Coarse Shape Losses.}
%
Since the details should be based on realistic coarse shapes, we train the coarse shape to help the learning of details by leveraging the ground-truth vertex as supervision:
\begin{equation}
     \mathcal{L}_{\text{ver}} = \big\| \mathbf{M}_{\text{ver}}\odot (\mathbf{S}-\hat{\mathbf{S}})  \big\|_2,
    \label{Eq.vertexloss}
\end{equation}
where $\mathbf{M}_{\text{ver}}$ is frontal face area of the coarse shape. $\hat{\mathbf{S}}$ and $\mathbf{S}$ are the reconstructed and ground-truth face by Eq.~\ref{Eq.shape3dmm}.
%


In addition, we make constraints on shape coefficients to prevent overfitting. We enforce the predicted coefficients have a similar distribution to the ground-truth coefficients:
% 
\begin{equation}
    \mathcal{L}_{\text{kl}}=\rho(\boldsymbol{\beta})\big(\log\rho(\boldsymbol{\beta})-\log\rho(\hat{\boldsymbol{\beta}})\big),
    \label{Eq.id_dist}
\end{equation}
where $\rho$ denotes \textit{softmax} function to map the predicted coefficients $\hat{\boldsymbol{\beta}}$ and ground-truth $\boldsymbol{\beta}$ into probability distribution.

Finally, the shape loss is $\mathcal{L}_{\text{shp}} = \mathcal{L}_{\text{ver}} + \mathcal{L}_{\text{kl}}$.


\myparagraph{Self-supervised Losses.}
%
To encourage the generalization of our models in real-world images~\cite{CelebAMask-HQ,mollahosseini2017affectnet},
%
we follow previous methods~\cite{feng2021learning,deng2019accurate,wood2022dense} to leverage self-supervised loss $\mathcal{L}_{\text{self}}$ for all training images, including photo loss $\mathcal{L}_{\text{pho}}$, perceptual loss $\mathcal{L}_{\text{id}}$, and dense landmark loss $\mathcal{L}_{\text{lmk}}$:
\begin{equation}
     \mathcal{L}_{\text{self}} = \mathcal{L}_{\text{pho}} + \lambda_{\text{id}}\mathcal{L}_{\text{id}} + \lambda_{\text{lmk}}\mathcal{L}_{\text{lmk}},
    \label{Eq.photo}
\end{equation}
where $\lambda_{\text{id}}$ and $\lambda_{\text{lmk}}$ are weights to balance the self-supervised losses term.

\input{table/benchmark}


In addition, considering the static detail heavily correlates to person-specific age attribute, inspired by~\cite{danvevcek2022emoca}, we leverage the pre-trained age prediction network~\cite{karkkainenfairface} to learn high-level representations of static details through knowledge distillation, such that the learned coefficients exhibit expressive results.
%
To achieve this, we use several MLP layers on the static coefficient $\boldsymbol\varphi$, and map it into age classification probabilities $\hat{\mathbf{p}}_{\text{age}}$. Then we use the pre-trained age recognition model $\mathbf{\Gamma}_{\text{age}}$ to obtain the probabilities of the given input image $\mathbf{I}$. The distillation loss $\mathcal{L}_{\text{kd}}$ enforces the probabilities between $\hat{\mathbf{p}}_{\text{age}}$ and $\mathbf{\Gamma}_{\text{age}}(\mathbf{I})$ be similar:
% 
\begin{equation}
     \mathcal{L}_{\text{kd}} =\mathbf{\Gamma}_{\text{age}}(\mathbf{I})\big(\log\mathbf{\Gamma}_{\text{age}}(\mathbf{I})-\log\hat{\mathbf{p}}_{\text{age}}\big).
    \label{Eq.logit_distil}
\end{equation}
\myparagraph{Regularization.}
%
$\mathcal{L}_{\text{reg}}$ regularizes coefficients of each sub-module, by minimizing the $l_2$ loss of $\boldsymbol\alpha$, $\boldsymbol\beta$, $\boldsymbol\xi$, $\boldsymbol\varphi$, $\boldsymbol\phi$.
% 




\myparagraph{Overall Loss Function.}
%
We train the coarse shape and fine details simultaneously, such that each component can collaborate to reconstruct high-fidelity 3D faces with realistic details. Formally, we minimize the total loss function:
\begin{equation}
\begin{aligned}
    \mathcal{L}&= \lambda_{\text{detail}}\mathcal{L}_{\text{detail}} + \lambda_{\text{shp}}\mathcal{L}_{\text{shp}}  \\ &+  \lambda_{\text{self}}\mathcal{L}_{\text{self}} +
    \lambda_{\text{kd}}\mathcal{L}_{\text{kd}} + \lambda_{\text{reg}}\mathcal{L}_{\text{reg}},    
\end{aligned}
\end{equation}
where $\lambda$ is the weight for each component.
% 