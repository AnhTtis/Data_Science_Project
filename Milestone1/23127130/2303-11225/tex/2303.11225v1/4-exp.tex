
\input{table/benchmark}


\section{Experiments}

\subsection{Implementation Details}




\myparagraph{Dataset.}
%
We use a hybrid dataset from both synthetic dataset~\cite{wood2021fake,raman2022mesh} and real-world dataset~\cite{CelebAMask-HQ,mollahosseini2017affectnet}. 
We use the synthetic data pipeline~\cite{wood2021fake,raman2022mesh} to generate a diverse dataset of $200k$ faces with ground-truth vertex, landmark, albedo, and displacement map annotations.
%
% 
The real-world datasets contain $400k$ images in total from diverse age, gender, and ethnicity groups.
For the real-world dataset, we use the pre-trained dense landmark detector~\cite{wood2021fake} to detect $669$ landmarks for training.
%
We use face parsing~\cite{Zheng2022DecoupledML} to generate and select region-of-interest as facial masks, providing robustness to common occlusions by hair or other accessories.
%
We follow~\cite{danvevcek2022emoca,wood2022dense,deng2019accurate} to split the dataset into train/valid. The test images are from CelebA~\cite{CelebAMask-HQ}, FFHQ~\cite{karras2019style}, LS3D-W~\cite{bulat2017far}, and AFLW2000~\cite{yin2017towards}.




\myparagraph{Implementation Details.}
%
We implement {\name} in PyTorch~\cite{paszke2019pytorch} and leverage the PyTorch3D differentiable rasterizer~\cite{ravi2020pytorch3d} for rendering. 
We train our model for $35$ epochs on $8\times$ NVIDIA Tesla V100 GPUs with a mini-batch of $320$. We use the pre-trained ResNet-50 on ImageNet~\cite{deng2009imagenet} as initialization, and use Adam~\cite{kingma2015adam} as optimizer with an initial learning rate of $1e-4$. The input image is cropped and aligned by~\cite{chen2016supervised}, and resized into $224\times224$.
%
We empirically set $\lambda_{\text{detail}}=10$, $\lambda_{\text{shp}}=1$, $\lambda_{\text{self}}=1$, $\lambda_{\text{id}}=0.1$, $\lambda_{\text{lmk}}=0.5$, $\lambda_{\text{kd}}=1$, $\lambda_{\text{reg}}=1e-3$ throughout the experiments.


\subsection{Comparisons to State-of-the-art}




\myparagraph{Quantitative Comparison.}
%
We perform the quantitative evaluation on the REALY benchmark~\cite{REALY}, which contains $100$ frontal-view and $400$ side-view images from $100$ textured-scans~\cite{LYHM}. The REALY benchmark presents a region-aware evaluation pipeline to separately evaluate the metric error (in mm) of the nose, mouth, forehead, and cheek regions. Such an evaluation pipeline is demonstrated to better estimate the actual similarity of the 3D faces and align with human perception.
%
We compare {\name} to previous state-of-the-art methods and report the region-wise and average normalized mean square error (NMSE) in Tab.~\ref{tab:benchmark}. 


As Tab.~\ref{tab:benchmark} illustrates, {\name} outperforms prior arts in the overall error by a large margin. {\name} balances the reconstruction quality on each region, compared to those optimum region methods that may fail in specific regions ({\eg}, MICA~\cite{MICA} fails in mouth region while HRN~\cite{lei2023Ahierarchical} fails in forehead region).
%
Note that {\name} faithfully recovers the facial details, thus making the reconstruction error smaller than using the coarse shape alone. As a comparison, although DECA~\cite{feng2021learning} and EMOCA~\cite{danvevcek2022emoca} can reconstruct details of given images, they turn out to be noisy, leading to the deterioration of reconstruction quality.





In addition, Tab.~\ref{tab:benchmark} also demonstrates the necessity of each component in contributing to a better quality.
%
It can be observed that the synthetic data with ground-truth labels not only improve the coarse shape reconstruction quality but is also crucial for detailed reconstruction.
%
With the synthetic data, the proposed {\module} further boosts the overall reconstruction quality. Either the static or dynamic factors are essential to capture fine-grained details, and the final {\module} achieves the most accurate details in expression-related regions such as the mouth and forehead, which contributes to the overall gains.


\begin{figure*}[ht!]
\begin{minipage}[t]{0.48\linewidth}
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img/coarse_v3.jpg}
    %
    \end{overpic}
    \put(-228,138){\bfseries\scriptsize Input}
    \put(-25,138){\bfseries\scriptsize Ours}
    \put(-196,138){\scriptsize Deep3D}
    \put(-168,138){\scriptsize 3DDFA-v2}
    \put(-135,138){\scriptsize SynergyNet} 
    \put(-93,138){\scriptsize MICA} 
    \put(-60,138){\scriptsize Dense}
    \vspace{-10pt}
    \caption{\textbf{Comparison on coarse shape reconstruction.} From left to right: Input image, Deep3D~\cite{deng2019accurate}, 3DDFA-v2~\cite{guo2020towards}, SynergyNet~\cite{wu2021synergy}, MICA~\cite{MICA}, Dense~\cite{wood2022dense}, and {\name} (Ours).}
    % \vspace{-11pt}
    \label{fig:coarse_cmp}
\end{minipage}%
    \hfill%
\begin{minipage}[t]{0.48\linewidth}
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img/detail_shape_v3.jpg}
    \end{overpic}
    \put(-228,138){\bfseries\scriptsize Input}
    \put(-25,138){\bfseries\scriptsize Ours}
    \put(-200,138){\scriptsize FaceScape}
    \put(-160,138){\scriptsize Unsup}
    \put(-128,138){\scriptsize DECA} 
    \put(-98,138){\scriptsize EMOCA} 
    \put(-65,138){\scriptsize FaceVerse}
    \vspace{-10pt}
    \caption{\textbf{Comparison on detail shape reconstruction.} From left to right: Input image, FaceScape~\cite{yang2020facescape}, Unsup~\cite{chen2020self}, DECA~\cite{feng2021learning}, EMOCA~\cite{danvevcek2022emoca}, FaceVerse~\cite{wang2022faceverse}, and {\name} (Ours).}
    \label{fig:detail_cmp}
\end{minipage} 
\vspace{-10pt}
\end{figure*}

\begin{figure}[!t]
    \centering
    % \vspace{-9pt}
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img/fit_and_detail.jpg}
    \end{overpic}
    \put(-225,81){\bfseries\scriptsize Input}
    \put(-195,81){\scriptsize Dense~\cite{wood2022dense}}  
    \put(-158,81){\bfseries\scriptsize + Our Detail}   
    \put(-105,81){\bfseries\scriptsize Input}
    \put(-75,81){\scriptsize  Dense~\cite{wood2022dense}}  
    \put(-38,81){\bfseries\scriptsize + Our Detail}  
    \vspace{-10pt}
    % \vspace{-14pt}
    \caption{\textbf{Illustration on the flexibility of {\module}.} Given the identity and expression coefficients ($\boldsymbol{\beta}$, $\boldsymbol{\xi}$) from the optimization-based method~\cite{wood2022dense}, {\module} can generate realistic details based on the coarse shape and further improve the visual quality.}
    \label{fig:ft_detail}
    \vspace{-10pt}
\end{figure}




\myparagraph{Qualitative Comparison.}
%
Given a single face image, {\name} reconstructs a high-fidelity 3D shape with details.
%
We present visualized comparisons with previous methods on 1). coarse shape reconstruction~\cite{deng2019accurate,guo2020towards,wu2021synergy,MICA,wood2022dense} in Fig.~\ref{fig:coarse_cmp} and 2). detail reconstruction~\cite{yang2020facescape,chen2020self,feng2021learning,danvevcek2022emoca,wang2022faceverse} in Fig.~\ref{fig:detail_cmp}. See more examples and comparisons in the supplementary.

For coarse shape in Fig.~\ref{fig:coarse_cmp}, our {\name} faithfully recovers the coarse shape of the given identity and outperforms the previous learning-based methods, and is on par with Dense~\cite{wood2022dense}, which is the state-of-the-art optimization-based method.
%
For detailed reconstruction in Fig.~\ref{fig:detail_cmp}, previous methods~\cite{feng2021learning,danvevcek2022emoca} fail to reconstruct satisfactory details. Several methods~\cite{chen2020self,yang2020facescape,wang2022faceverse} are sensitive to occlusions and large poses. 
%
As a comparison, {\name} achieves the most realistic reconstruction quality, and faithfully recovers facial details of a given image, which significantly outperforms previous methods by a large margin.

\begin{figure}[!t]
    \centering
    % \vspace{-9pt}
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img/manipulate_v3.jpg}
    \end{overpic}
    \put(-225,30){\bfseries\scriptsize Ours}    
    \vspace{-10pt}
    \caption{\textbf{Comparison on face animation.} Given a source image (yellow box), we use the driving images (green box) to drive its expressions. DECA~\cite{feng2021learning} (2nd-row) and EMOCA~\cite{danvevcek2022emoca} (3rd-row) can animate the expression-driven details but lack realistic. As a comparison, {\name} is flexible to animate details from static (4th-row), dynamic (5th-row), or both (6th-row) factors, and presents vivid animation quality with realistic shapes.
    }
    \label{fig:animation}
    \vspace{-15pt}
\end{figure}





In addition, given an image and the fitted coefficients $\boldsymbol{\beta}$, $\boldsymbol{\xi}$ from the optimization-based methods such as Dense~\cite{wood2022dense}, {\module} synthesizes the details and further strengthens the quality compared to the coarse shape (see Fig.~\ref{fig:ft_detail}). It shows {\module} is flexible and can be easily plugged-and-play into other methods. 
See more in the supplementary.





\begin{figure*}[t!]
\begin{minipage}[t]{0.48\linewidth}
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img/abl_vertex_v2.jpg}
    \end{overpic}
    \put(-223,97){\bfseries\scriptsize Input}
    \put(-188,97){\scriptsize w/o real image}
    \put(-130,97){\scriptsize w/o $\mathcal{L}_{\text{shp}}$}
    \put(-80,97){\scriptsize w/o $\mathcal{L}_{\text{kl}}$}
    \put(-30,97){\bfseries\scriptsize Ours}
    \label{fig:coarse_loss}
\end{minipage}%
    \hfill%
\begin{minipage}[t]{0.48\linewidth}
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img/abl_disp_v2.jpg}
    \end{overpic}
    \put(-223,97){\bfseries\scriptsize Input}
    \put(-188,97){\scriptsize w/o real image}
    \put(-132,97){\scriptsize w/o $\mathcal{L}_{\text{detail}}$}
    \put(-80,97){\scriptsize w/o $\mathcal{L}_{\text{kd}}$}
    \put(-30,97){\bfseries\scriptsize Ours} 
    \label{fig:detail_loss}
\end{minipage}
\vspace{-2pt}
\caption{\textbf{Ablation studies on loss functions and training data.} The coarse shape losses $\mathcal{L}_{\text{shp}}$/$\mathcal{L}_{\text{kl}}$ (left), detail losses $\mathcal{L}_{\text{detail}}$/$\mathcal{L}_{\text{kd}}$ (right), and hybrid datasets coherently contribute to the reconstruction quality of coarse shapes and details.}
\label{fig.abl_loss}
\vspace{-11pt}
\end{figure*}

\myparagraph{Application of {\name}.}
%
The {\name} explicitly decouples the static and dynamic details through the proposed {\module}.
%
Therefore, we can animate the facial attributes by simply assigning the expression coefficient $\boldsymbol{\xi}$ and/or static coefficient $\boldsymbol{\varphi}$ of the driving images to the source images.
%

In Fig.~\ref{fig:animation}, we demonstrate the animation quality of {\name} outperforms the previous state-of-the-art detail animation methods~\cite{feng2021learning,danvevcek2022emoca}.
% 
It shows that while DECA~\cite{feng2021learning} and EMOCA~\cite{danvevcek2022emoca} can animate the expression-driven details but lack realistic, the proposed {\name} is flexible to manipulate the static, dynamic, or both details.
When animating the static detail, the person-specific details can be well transferred into the source shape.
When animating the dynamic detail, only expression-dependent details are presented.
%
Finally, we can also animate the static and dynamic details simultaneously and achieve satisfactory results.










                

