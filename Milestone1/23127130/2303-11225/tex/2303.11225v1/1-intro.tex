\section{Introduction}
The reconstruction of a 3D face from a single image has drawn much attention recently~\cite{tewari2017mofa,deng2019accurate,lee2020uncertainty,MICA}. It has tremendous potential applications like face recognition~\cite{cao2018pose,schroff2015facenet,blanz2002face,romdhani2002face}, face animation~\cite{cudeiro2019capture,yue2022anifacegan}, virtual reality~\cite{bouaziz2013online,ribera2017facial,hu2017avatar}, {\etc} For example, the reconstructed 3D face representation can be driven by an audio~\cite{cudeiro2019capture}, or a video from another person~\cite{kim2018deep}.

% 
To build a flexible and animatable facial representation, a popular way is to leverage the success of 3D Morphable Models (3DMMs)~\cite{blanz1999morphable,booth2018large,cao2013facewarehouse,li2017learning,smith2020morphable}, which decouple the influence of shape, expression, albedo, and others by modeling them in separate coefficients.
Typically in literature, one can achieve coarse shape reconstruction in coefficients-fitting optimization~\cite{gecer2019ganfit,wood2022dense,bao2021high,yang2020facescape,bai2022ffhq}, or an analysis-by-synthesis pipeline~\cite{tewari2017mofa,deng2019accurate,MICA,luo2021normalized}. However, as the underlying low-dimensional representation for shape and expression of 3DMMs is not capable of representing fine facial details (\eg, wrinkles), recent advances model such details with a displacement map~\cite{chen2020self,yang2020facescape,chen2019photo,cao2015real,yamaguchi2018high}.
However, previous work fails to model the distinction between static and dynamic factors of fine detail, leading to errors in reconstructions.
%
For example, given that one may drive the expression of a young man from an old man, trivially transferring all wrinkles from the old man to the young man could make the young man look unnatural.
In this sense, Feng {\etal}~\cite{feng2021learning} implicitly leverages the person-specific identity and expression as conditions to generate the details. Although effective, they optimize the model in an analysis-by-synthesis pipeline with only the image-level supervision, leading to insufficient decoupling of static and dynamic details and inconsistent animation results (see Fig.~\ref{fig:animation}).


Therefore, we propose {\name} to explicitly model the static and dynamic details for high-fidelity 3D face reconstruction.
% 
More specifically, for person-specific static detail, instead of directly predicting the displacement map that may increase the difficulty of detail prediction~\cite{feng2021learning,danvevcek2022emoca}, we follow the spirit of 3DMMs to build a displacement basis from the captured facial scans with age diversity~\cite{raman2022mesh,wood2021fake}. In this way, the model is trained to predict the coefficients of the displacement basis, and make the detail prediction easier. For dynamic detail, since it is highly expression-dependent, directly modeling it with one displacement basis is quite difficult. Therefore, based on the fact that the expression can be interpolated by a compressed and a stretched expressions~\cite{raman2022mesh}, we build two displacement bases for the compressed and stretched expressions from the captured scans respectively, and learn to regress the displacement coefficients with the ground-truth labels. Therefore, we can obtain the dynamic detail by linearly interpolating the compressed and stretched displacement maps, which are derived from the displacement bases and the predicted coefficients. Finally, the predicted static and dynamic details are merged with the coarse shape to formulate the final output.


%
Since the final output consists of both the coarse shape and details, and is typically reconstructed for real-world images, we propose several novel loss functions to learn coarse shape and details simultaneously from both the synthetic and real-world datasets. For details, we leverage the ground-truth static and dynamic displacement maps of the synthetic dataset~\cite{wood2021fake,raman2022mesh} as supervision. While for the coarse shape, we leverage the ground-truth vertex of the synthetic dataset as supervision. We also follow the previous methods~\cite{feng2021learning,deng2019accurate,wood2022dense} to leverage self-supervised losses for all training images.

Overall, with the above insights and techniques, {\name} enables the reconstruction of high-fidelity 3D faces from a single image, and decouples static and dynamic details that are naturally animatable (see Fig.~\ref{Fig.teaser}). 
We demonstrate that the proposed {\name} reconstructs realistic and faithful 3D faces, reaching state-of-the-art performance both quantitatively and qualitatively. In addition, {\name} is compatible with optimization-based methods~\cite{wood2022dense}, and is flexible to transfer vivid expressions and details from one person to another.
% 
In summary, our contributions are:
\begin{itemize}[leftmargin=*,nosep,nolistsep]
\item We are the first to demonstrate the benefits of the synthetic data in detailed 3D face reconstruction, and propose {\name} to model the static and dynamic details explicitly.
\item We propose novel loss functions in {\name} to learn 3D representations of coarse shape and fine details simultaneously from both the synthetic and real-world images.
\item We achieve state-of-the-art reconstruction quality both quantitatively and qualitatively, with over $15\%$ performance gains in the region-aware benchmark~\cite{REALY}.
\item We show that our {\module} is easy to plug-and-play into optimization-based methods and can transfer expressions and details from one to another for face animation.
\end{itemize}


