\section{Related Work}


3D face reconstruction from monocular images has received much attention in the past decades.
Among them, 3D Morphable Models (3DMMs) are widely used to build 3D representations.
Below we review the works that are related to them, and a full in-depth review can be found in recent surveys~\cite{zollhofer2018state,morales2021survey,egger20203d}.


\myparagraph{3D Morphable Model} (3DMMs)~\cite{egger20203d} are statistical models widely used to constrain the distribution of 3D faces. The seminal work~\cite{blanz1999morphable} presents $200$ scans to generate shape and texture bases with Principal Component Analysis (PCA)~\cite{abdi2010principal}, and formulate 3DMMs as linear models by the generated bases.
After that, expression models~\cite{vlasic2006face,li2017learning,cao2013facewarehouse} are proposed to support face manipulation. Recent advances~\cite{REALY,ploumpis2019combining,smith2020morphable,dai20173d,li2020learning} are proposed to expand the expressiveness of 3DMMs and play a crucial role in 3D face reconstruction. 3DMMs make it possible to simplify the 2D-to-3D problem into a regression task, which typically presents an analysis-by-synthesis fashion to estimate the coefficients of 3DMMs. 
%
In this paper, we follow the spirit of the 3DMMs family to present the decoupled static and dynamic details for 3D face reconstruction.
% 

\begin{figure*}[ht!]
    \centering
    % \vspace{-9pt}
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img/pipeline_v3.jpg}
    \end{overpic}
    % \vspace{-14pt}
    \put(-315,123){\footnotesize (a). Overview of {\name}}
    \put(-100,123){\footnotesize (b). Overview of {\module}}
    \put(-481,34){\bfseries\scriptsize Input}
    \put(-481,28){\bfseries\scriptsize Image}
    \put(-445,30){\scriptsize ResNet-50}
    \put(-490,80){\bfseries\scriptsize Synthetic}  
    \put(-470,70){\bfseries\scriptsize Real World} 
    \put(-405,10){\bfseries\scriptsize $\boldsymbol{\gamma}$}
    \put(-405,33){\bfseries\scriptsize $\boldsymbol{\alpha}$}
    \put(-405,53){\bfseries\scriptsize $\boldsymbol{\varphi}$}
    \put(-405,78){\bfseries\scriptsize $\boldsymbol{\xi}$}
    \put(-405,103){\bfseries\scriptsize $\boldsymbol{\beta}$}
    \put(-405,122){\bfseries\scriptsize $\mathbf{p}$}
    \put(-374,105){\scriptsize 3D Model}
    \put(-376,64){\scriptsize {\module}}    
    \put(-365,34){\scriptsize $\mathbf{B}_{\text{alb}}$}    
    \put(-368,9){\scriptsize Illumination Model}       
    \put(-340,90){\scriptsize Coarse Shape}       
    \put(-352,49.5){\scriptsize Displacement Map} 
    \put(-340,19.5){\scriptsize Albedo Map} 
    \put(-293,51){\scriptsize Renderer} 
    \put(-298,69){\scriptsize Detail Shape}     
    \put(-260,34){\bfseries\scriptsize Rendered}
    \put(-255,28){\bfseries\scriptsize Image}    
    \put(-222.5,115){\bfseries\scriptsize $\boldsymbol{\beta}$}
    \put(-222.5,88){\bfseries\scriptsize $\boldsymbol{\xi}$}
    \put(-222.5,69){\bfseries\scriptsize \textit{AdaIN}}
    \put(-222.5,51){\bfseries\scriptsize $\boldsymbol{\varphi}$} 
    \put(-195,116){\scriptsize $\mathbf{B}_{\text{id}}$}
    \put(-195,89){\scriptsize $\mathbf{B}_{\text{exp}}$}
    \put(-206,37){\scriptsize MLP}
    \put(-183,37){\scriptsize MLP}
    \put(-168,42){\bfseries\scriptsize $\boldsymbol{\phi}$}
    \put(-203,20){\scriptsize $\mathbf{B}_{\text{sta}}$}
    \put(-184,6){\scriptsize Static Detail}
    \put(-142,110){\scriptsize $\mathbf{S}_{\text{neu}}$}
    \put(-115,80){\bfseries\scriptsize $\mathbf{S}$}
    \put(-102,95){\scriptsize Eq.~\ref{Eq.tension}}
    \put(-44,73){\scriptsize Eq.~\ref{Eq.dyn_detail}}
    \put(-151,65){\scriptsize $\mathbf{B}_{\text{com}}$}
    \put(-149,39){\scriptsize $\mathbf{B}_{\text{str}}$}
    \put(-130,52){\scriptsize Compressed}
    \put(-127,25){\scriptsize Stretched}
    \put(-85,90){\scriptsize Vertex Tension $\mathbf{M}_{\text{uv}}$}
    \put(-82,73){\bfseries\scriptsize \textit{Interpolate}}
    \put(-90,36){\scriptsize Dynamic Detail}
    \put(-29,36){\bfseries\scriptsize Output}
    \put(-420,-6){\bfseries\scriptsize$\mathbf{p}$: pose coef \quad $\boldsymbol{\beta}$: identity coef \quad $\boldsymbol{\xi}$: expression coef \quad $\boldsymbol{\varphi}$: static coef \quad $\boldsymbol{\alpha}$: albedo coef \quad $\boldsymbol{\gamma}$: SH coef \quad $\boldsymbol{\phi}$: dynamic coef}
    \vspace{-3pt}
    \caption{\textbf{Illustration of {\name}}. (a). Learning framework of {\name}. Given a monocular image, we regress its shape and detail coefficients to synthesize a realistic 3D face, and leverage a differentiable renderer~\cite{genova2018unsupervised} to train the whole model end-to-end from synthetic~\cite{wood2021fake,raman2022mesh} and real-world~\cite{CelebAMask-HQ,mollahosseini2017affectnet} images. (b). The pipeline of \textbf{S}tatic and \textbf{D}ynamic \textbf{De}coupling for De\textbf{Tail} Reconstruction ({\module}). We explicitly decouple the static and dynamic factors to synthesize realistic and animatable details. Given the shape and static coefficients, we regress the static and dynamic details through displacement bases and interpolate them into the final details through vertex tension~\cite{raman2022mesh}.}
    \label{fig:pipeline}
    \vspace{-10pt}
\end{figure*}

\myparagraph{Coarse Shape Reconstruction.} 
Traditional optimization-based methods~\cite{gecer2019ganfit,wood2022dense,bao2021high,yang2020facescape,bai2022ffhq} directly optimize the 3DMM coefficients of given 2D images.
While such methods work well in controlled settings ({\eg}, frontal view, no occlusion), they heavily rely on high-quality annotations. 
%
Learning-based methods leverage the advances of CNNs~\cite{tewari2017mofa,deng2019accurate,danvevcek2022emoca,sanyal2019learning} and GCNs~\cite{lin2020towards,lee2020uncertainty,gao2020semi} to learn high-level representations from large-scale images in the wild. These methods show plausible generalization over diverse environments.
%
To train the network end-to-end, recent methods leverage the differentiable renderers~\cite{genova2018unsupervised,dib2021towards,zhu2020reda,liu2019soft}, along with the photo loss, perceptual loss, and landmark loss~\cite{deng2019accurate,danvevcek2022emoca,genova2018unsupervised,tewari2017mofa,wen2021self} to optimize the network in a self-supervised manner. Different from these coarse shape reconstruction methods, we aim at high-fidelity 3D face reconstruction with both coarse shape and fine details. 

%

\myparagraph{Detail Reconstruction.}
While 3DMMs can reconstruct coarse 3D face shapes from 2D images, they struggle with reconstructing fine-level details, such as forehead wrinkles and crows-feet. To fill this gap, shape by shading (SfS)~\cite{jiang20183d,li2018feature,garrido2016reconstruction,suwajanakorn2014total} methods reconstruct the facial details from monocular images or videos. However, these methods are sensitive to occlusions and large poses. Recent advances~\cite{feng2021learning,ling2022structure,chen2020self,chen2019photo,lei2023Ahierarchical} leverage displacement maps to present details. These methods explicitly re-topologize the coarse shape and present residual bias to generate geometric details.
% 
The main challenge of detail reconstruction is the difficulty in learning the nuances and disentangling the static and dynamic details from only self-supervised learning. Ground-truth labels of the details are helpful to guide the learning process. However, it is difficult to obtain such fine-grained labels on real data.


\myparagraph{Synthetic Dataset.}
%
Several methods~\cite{MICA,yang2020facescape,jackson2017large,tuan2017regressing,martyniuk2022dad} utilize rendered faces or fitted coefficients to synthesize 3D-2D pairs. These ground-truth pairs lack diversity over background, illumination, and assets, making them hard to generalize well to real-world images.
%
More recently, synthetic data~\cite{wood2021fake,raman2022mesh} is proven to be realistic and diverse to compensate for the domain gap to real-world images. In this paper, we leverage high-quality data with ground-truth labels to explore the detailed 3D face reconstruction.



