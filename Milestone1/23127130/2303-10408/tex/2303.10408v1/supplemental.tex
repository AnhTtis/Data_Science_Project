\title{Supplementary Appendices for ``\thepapertitle''}
% \title{\thepapertitle}
\input{titleabstract}

\counterwithin{figure}{section}
\counterwithin{table}{section}

% \appendices
\appendix

  \section{Dataset Details}\label{appendix_dataset_details}

  \begin{figure}[H]
    \centering
    % \begin{minipage}{\linewidth}
      \centering
    \subfloat[][BBBC038v1 Dataset] {
      \centering
      \includegraphics[height=2.5cm]{pics/BBBC038v1_example_img1.png}
      \includegraphics[height=2.5cm]{pics/BBBC038v1_example_img2.png}
      \includegraphics[height=2.5cm]{pics/BBBC038v1_example_img3.png}
      % \caption{BBBC038v1 Dataset, sample images}
      \label{fig:bbbc038v1_example}
    }
    % \end{minipage}
    \hfill
    % \begin{minipage}{\linewidth}
      \centering
    \subfloat[][CheXpert Dataset]{
      \centering
      \includegraphics[height=2.5cm]{pics/chexpert_patient00001_study1_view1.jpg}
      \includegraphics[height=2.5cm]{pics/chexpert_patient00002_study1_view2.jpg}
      % \caption{CheXpert Dataset, sample images}
      \label{fig:chexpert_example}
    }
    % \end{minipage}
    \caption{Example images from the datasets we used.}
    % \label{}
  \end{figure}

  \textbf{CheXpert:}  The CheXpert dataset \cite{dataset_chexpert} published by Stanford ML Group and Stanford Hospital contains 223,415:235 train:test chest x-rays for binary classification of 14 classes.  Only five classes are emphasized by most literature, and we therefore consider just the five classes: Atelectasis, Cardiomegaly, Consolidation, Edema, and Pleural Effusion.  The train labels are generated by an automatic software that mines text data in associated unpublished medical reports, and the test set labels are manually curated by three radiologists.  Most labels are missing and some are marked uncertain.  There are several possible baselines that attempt to incorporate the uncertainty labels.  We compare against non-ensembled single-model U-Ignore baseline reported in the state of art literature \cite{chexpert_2019sota}, and we provide details on this baseline and comparison to state of art in Appendix \ref{appendix_model_details}.  This baseline assumes missing labels are negative and excludes uncertainty labels from the analysis.

  Regarding implementation, we use the CheXpert-v1.0-small dataset, which contains already down-sampled images.  We permanently split the training set into 70\%:30\% (156389:67025) using a random seed of 138 and PyTorch's default random number generator.  We train on the 70\% split and evaluate the 30\% validation set to find hyperparameters.  All results we report are on the 200 image holdout test set from models trained on our 70\% split of the training set.  For pre-processing, we randomly crop to images of $(320, 320)$ pixels on the 70\% training set, and on the validation or test sets we upsample images in a minibatch by padding zeros.  The minibatch size is four images.  Each epoch considers only 15k images sampled with replacement from the 70\% split.  To evaluate performance, we report the average test set ROC AUC across the five classes to enable comparison with most literature that uses the dataset and non-ensembled U-Ignore baseline.

\textbf{BBBC038v1:} The BBBC038v1 dataset \cite{dataset_BBBC038v1}, hosted by the Broad Bioimage Benchmark Collection, is a diverse collection of microscopy images visualizing cell nuclei.  The dataset is representative of the domain: images are aggregated from a variety of laboratories, they show cell nuclei of different animals in different states and contexts, and present varying lighting, magnification, staining, and background/foreground coloration.  The dataset contains 670:65:106 images annotated with pixel-wise segmentation masks.  The 106 hold-out images are out-of-distribution, as they come from entirely different laboratories and they present experimental conditions not in the 670:65 data.  We treat both the 65 and 106 image datasets as test sets in order to have an in-distribution vs out-of-distribution analysis.  We visualize example images in Fig. \ref{fig:bbbc038v1_example}.

%We chose to work with purely supervised semantic segmentation models on the 670:65 data.  
To choose hyperparameters, data pre-processing and loss, we temporarily split the training set into 468:202 training and cross validation sets using a different random seed each time we train a model.  We report all results on the 106 image holdout set, except in Appendix \ref{appendix:BE11_in_vs_out_of_distribution} where we show results on the in-distribution vs out-of-distribution datasets.  
Regarding pre-processing, all images were normalized into [0,1] and then shifted to have a 0.5 mean.  On the training set, the images were subjected to random flipping (even probability of no flip, horizontal, vertical, both), and then with 80\% probability a pixel-wise noise function for each pixel $x' = \text{\texttt{clip}}(x + \mathcal{N}(0,1) / \mathcal{U}(5,10))$ where \texttt{clip} puts the image into a [0,1] range.  We use a batch size of ten on the training set, and one on the validation and test sets.  We report results using the S{\o}rensen-Dice Coefficient evaluated on the test set.

  \section{Model Details} \label{appendix_model_details}

  We consider three standard architectures for the CheXpert dataset: DenseNet \cite{densenet}, ResNet \cite{resnet} and {EfficientNet} \cite{efficientnet}.  DenseNet121 is used by state-of-the-art literature on chest x-ray data, including CheXpert \cite{chexpert_2019sota}.  ResNet introduced the residual skip connection and the bottleneck sequence of convolutions $1\times1$, $3\times3$, $1\times1$. Sec. \ref{intuitions} explains that this sequence enables the network architecture to perform steering of inputs and outputs to the spatial convolution.  The DenseNet architecture builds on ResNet's skip connection with "dense" blocks and is largely defined as a sequence of $1\times 1$, $3\times3$ convolutions, thus steering spatial convolution inputs.
  The EfficientNet architecture uses the depthwise separable convolution with both $3\times3$ and $5\times5$ filters, though the network primarily contains of pointwise convolutions.  Table \ref{table:model_num_params_and_savings} (middle column) compares the number and proportion of spatial parameters in each model.  The pre-trained models use publicly available ImageNet weights, and for pre-trained EfficientNet we use the weights obtained via the AdvProp method.  All models are modified to have 1 channel grayscale image input and output an unnormalized vector with five values corresponding to the five CheXpert tasks.

  \begin{figure}[t]
    \centering
    \subfloat[CheXpert Baselines\label{fig:chexpert_baselines}]{
      \includegraphics[height=3.5cm]{pics/chexpert_baselines.png}
    }\qquad
    \subfloat[BBBC038v1 Baselines\label{fig:bbbc038v1_baselines}]{
    \includegraphics[height=3.5cm]{pics/bbbc038v1_baselines.png}
    }\qquad
    \subfloat[U-NetD Architecture (simplified) \label{fig:unetD}]{
    \includegraphics[height=3.5cm]{pics/Unet-D.png}
    }
  \end{figure}
  \begin{table}[t]
    \centering
    \caption{BBBC038v1 Baselines: U-NetD has fewest parameters}
    \label{table:bbbc_num_params}
    \begin{tabular}{lrr}
    \toprule
    {Model} &  Num Params & \begin{tabular}{@{}c@{}}Spatial Params\\(\% total)\end{tabular} \\
    \midrule
    U-NetD             &      116695 &             12744 (11\%) \\
    DeepLabV3+ MobileNetV2 &     5220577 &           2977344 (57\%) \\
    DeepLabV3+ ResNet50  &    39756705 &          26182848 (66\%) \\
    \bottomrule
    \end{tabular}
  \end{table}
  On the CheXpert U-Ignore baseline, current state-of-the-art literature reports a mean AUC ROC of 0.863, using DenseNet121 with a full training set \cite{chexpert_2019sota}, meaning that each training image is considered five times in total (five epochs).  
All our baseline models are comparable to this baseline (see Fig. \ref{fig:chexpert_baselines}), and our models
use significantly less training data (150k training images instead of 223k, and each image is used for training only four times on average).  In Fig. \ref{fig:chexpert_baselines}, each dot represents an independent training of one of our baseline models.  The gray dashed line is the 0.863 ROC AUC from existing literature.

  On the microscopy dataset, we implement our own depthwise-separable {U-Net} \cite{unet_ronneberger2015} encoder-decoder architecture, called U-NetD.  For clarity, the architecture is visualized with only 3 levels in Fig. \ref{fig:unetD}. We use 5 levels, each with a different channel width: $(3,8,16,32,64)$.  In the figure, green blocks are spatial $3\times3$ grouped convolutions with 6x channel expansion, blue blocks are pointwise convolutions, and red blocks are bilinear upsampling.  Between convolutions, we insert CELU activations and then BatchNorm, though the pointwise convolution following every spatial convolution has no activation, as suggested in MobileNetV2 \cite{mobilenetv2}.  We fuse the encoder output with previous decoder output using a weighted sum with two learned scalar weights.  
  The final layer of U-NetD (not shown in figure) is a 2D spatial convolution that maps its 3 channel output to a 1 channel segmentation mask.  All spatial convolutions are $3\times3$ kernels with no bias.

  We compare this U-Net model against DeepLabv3+ with the ResNet50 and MobileNetV2 backbones in Fig. \ref{fig:bbbc038v1_baselines}.  Each architecture presented was trained three times independently and evaluated on the out-of-distribution BBBC038v1 test set.  The boxenplot shows test set segmentation performance after 150 epochs.  The U-NetD model outperforms the MobileNetV2 backbone.  It converges almost the same median value as the ResNet50 backbone.  U-NetD has one and two orders of magnitude fewer parameters than the MobileNet and ResNet50 DeepLabV3+ architectures, respectively (see Table \ref{table:bbbc_num_params}).  We adopt the U-NetD over the DeepLabv3+ models because (a) the ResNet backbone is already considered in the CheXpert models, (b) U-NetD follows MobileNetV2 recommendations in its design (MobileNetv2 is desirable to emulate for its steering and wavelet-like properties as described in Sec. \ref{intuitions}), (c) U-NetD presents another common kind of architecture (the U-Net), and (c) U-NetD has orders of magnitude fewer parameters.


  \section{Other Hyper-parameters and Loss Functions} \label{appendix_hyperparameter_details}

  \textbf{Hyper-parameters:}  Dataset and model-specific configuration is defined in Appendices \ref{appendix_dataset_details} and \ref{appendix_model_details}.  In fixed filter models, all spatial filter kernels are not modified by back propagation.  Unless explicitly stated, we train all CheXpert models for 40 epochs using the default Adam optimizer with learning rate 0.0001, and we train all BBBC038v1 models for 300 epochs using the default Adam optimizer with learning rate 0.008.  Pruned models have a 2x larger learning rate of 0.16.

  The BBBC038v1 Loss function is a pixel-wise binary cross entropy loss with no class balancing weights.

  \textbf{CheXpert Loss function:} We designed a multi-label focal binary cross entropy loss with class balancing weights. The unreduced loss is defined as a $4\times5$ matrix:
\begin{align}
  L = - \mathbf{m} \w_{\text{task}} \bigl( \w_{\text{pos}}\y \log (\hat \y) + \w_{\text{neg}}(1-\y) \log (1- \hat \y) \bigr),
\end{align}
where $\y\in[0,1]^{4\times5}$ is ground truth for each of the five classes across the minibatch of 4 images, $\hat \y \in [0,1]^{4\times5}$ is the model output with each value normalized by a rescaled sigmoid function  $\hat \y = 0.99999\sigma(\hat \y') + 0.000005)$.  The matrix $\mathbf{m}\in\{0,1\}^{4\times5}$ is a bitmask that ignores uncertainty labels, the multiplications are element-wise multiplication with broadcasting, and the class inter- and intra-class balancing weights: $\w_\text{task} \in \mathbb{R}^{1\times5}$ balances the total count of positive + negative labels across tasks; $\w_\text{pos}\in\mathbb{R}^{1\times5}$ and $\w_\text{neg}\in\mathbb{R}^{1\times5}$ balance the counts of positive and negative classes within any given class (specifically ignoring uncertainty labels and remapping missing labels to negative) and incorporate a focal term.  For each of the classes, we have scalar weights:

  \begin{minipage}{.48\linewidth}
  \begin{align*}
    w_\text{pos} &= a_\text{pos}(1-\hat y)^\gamma \\
    \\
    w_\text{neg} &= a_\text{neg}\hat y^\gamma
  \end{align*}
  \end{minipage}%
  \begin{minipage}{.48\linewidth}
  \begin{align}
    a_\text{pos} &= \frac{1}{1 + \frac{c_\text{pos}}{c_\text{neg}}} \\
    a_\text{neg} &= \frac{1}{1 + \frac{c_\text{neg}}{c_\text{pos}}}
  \end{align}
  \end{minipage}
  where $\gamma=1$ is the focal loss hyperparameter that places emphasis on "hard" samples the model has low confidence for, where $a_\text{pos}$ and $a_\text{neg}$ are obtained by using standard class balancing weights of the vectorized form $\x = \frac{\max(\mathbf{c})}{\mathbf{c}}$ and subsequently normalizing them using $f(\x) = \frac{\x}{\sum_i x_i}$. The values $\mathbf{c} = [c_\text{pos}, c_\text{neg}]$ are counts of samples with positive and negative labels respectively over the entire training dataset for the given class.  For the inter-class balancing weight, we have a similar definition:
  \begin{align}
    \mathbf{c} &= \begin{bmatrix} c_{t} \;\;\forall\; \text{tasks }t \;\;;\;\; c_{t} = (c_{\text{pos}}+c_{\text{neg}})_{(t)}\end{bmatrix}\\
    w_\text{task} = w_t &= \frac{1}{1 + c_t \sum_{i\ne t} \frac{1}{c_i}}
  \end{align}
  We aggregate the loss $L$ by summing across columns (tasks) and then computing a mean across rows (minibatch samples). 

\section{Out-of-Distribution vs In-Distribution Analysis on Fixed Filters}\label{appendix:BE11_in_vs_out_of_distribution}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.49\linewidth]{pics/e1-BE12-results.png}
  \includegraphics[width=0.49\linewidth]{pics/e1-BE11-results.png}
  \caption{\textbf{No Overfitting.}
Left: Microscopy results of 36 independently trained models on the out-of-distribution test set.  Right: Otherwise identical experiment, on the in-distribution test set (same as Fig. \ref{appendix:BE10_results} left).  We observe higher performance on the out-of-distribution dataset.
\\
Each sub-figure represents 36 independently trained models.  Blue and orange lines show the per-epoch average of the same fixed spatial filter model (varying only in initialization) trained six times independently, and the blue line is the same in all plots.  The red and green background correspond to one-tailed paired-sample Wilcoxon significance tests \cite{wilcoxon_test1945}, one for each epoch, of whether the fixed filter models tended to outperform ($p>.999$) or underperform ($p<.001$) the baseline across all initializations.  Each test evaluated six paired models in a bandwidth of $\pm10$ epochs and across six initializations.  
}
\end{figure}

\section{Robustness to Learning Rate on Microscopy Dataset} \label{appendix:BE10_results}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.44\linewidth]{pics/e1-BE11-results.png}
  \qquad
  \includegraphics[width=0.44\linewidth]{pics/e1-BE10-results.png}
  \caption{\textbf{Robust to increased Learning Rate.}  Left: Showing microscopy results of 36 independently trained models and learning rate .008.  Right: Otherwise identical experiment, with learning rate .02.  With larger learning rate, steered methods outperform the baseline early in training, and the non-steered DCT2 method underperforms.  The results suggests steered initialization improves robustness of the model.  Evaluated on BBBC038v1 in-distribution test set.}
\end{figure}

  \section{DCT-II Basis} \label{appendix_DCT-II}
  We visualize the 2-d DCT-II basis for $3\times3$ and $5\times5$ kernels in Fig. \ref{fig:DCTII_bases}.  The 2-d DCT-II is 1-d separable, and the first row and column show which the 1-d basis vectors construct each 2-d vector.  We also assign each basis filter an index value to sort filters from low to high frequency.

  \begin{figure}[H]
  \begin{center}
    \subfloat[\label{fig:DCTII_bases3}]{ \includegraphics[width=.3\linewidth]{pics/basis_dctII_3x3.png}}
    \qquad
    \subfloat[\label{fig:DCTII_bases5}]{ \includegraphics[width=.3\linewidth]{pics/basis_dctII_5x5.png}}
  \end{center}
  \caption{\textbf{Illustration of proposed basis} for $3\times3$ (a) and $5\times5$ (b) matrices. The index numbers order filters from lowest frequency basis filters to highest frequency, in stable ordering.}
  \label{fig:DCTII_bases}
  \end{figure}

\section{ExplainSteer Explanations, Across Architectures and Initializations} \label{appendix:saliency_vs_not}

We visualize the $\e^{(d)}$ spectra for the spatial convolution layers of three architectures (DenseNet121, ResNet50, EfficientNet-b0), with varying initialization methods.  
The EfficientNet models have $3\times 3$ and $5\times 5$ filters and we visualize both kernel shapes in the same heatmap.  In DenseNet models, we do not visualize the spectrum of first convolution layer because this layer, which uses a 7x7 kernel, would result in a single column with 49 rows while the remaining columns all have 9 rows.  Below, Fig. \ref{fig:explainsteer_across_architectures} compares across architectures and Fig. \ref{fig:explainsteer_across_initialization_methods} compares across initialization methods.

\begin{figure}[H]
  \centering
  % \begin{minipage}{.60\linewidth}
    \subfloat[]{\includegraphics[height=1.5cm]{pics/explainsteer/ed_with_without_saliency__DenseNet121_fromscratch_Baseline.png}}
    \subfloat[]{\includegraphics[height=1.9cm]{pics/explainsteer/ed_with_without_saliency__ResNet50_fromscratch_Baseline.png}}
  % \end{minipage}
  % \begin{minipage}{.39\linewidth}
    \subfloat[]{\includegraphics[height=2.5cm]{pics/explainsteer/ed_with_without_saliency__EfficientNet-b0_fromscratch_Baseline.png}}
  % \end{minipage}
  \caption{\small \textbf{Illustration of Nimbleness} across architectures.  Saliency heatmaps expose structural inefficiency in the network design, where the repeating horizontal trend from bright to dark in all three architectures suggests that later spatial layers of most stages in the networks are less useful to the network.  All plots represent a fully learned baseline model initialized with random weights and trained on the CheXpert dataset.}
  \label{fig:explainsteer_across_architectures}
\end{figure}
\begin{figure}[H]
  \centering
  \subfloat[]{ \includegraphics[width=0.40\linewidth]{pics/explainsteer/ed_with_without_saliency__DenseNet121_pretrained_Baseline.png}}
  \subfloat[]{ \includegraphics[width=0.40\linewidth]{pics/explainsteer/ed_with_without_saliency__DenseNet121_pretrained_Unchanged.png}}\\
  \subfloat[]{ \includegraphics[width=0.40\linewidth]{pics/explainsteer/ed_with_without_saliency__DenseNet121_pretrained_GHaar.png}}
  \subfloat[]{ \includegraphics[width=0.40\linewidth]{pics/explainsteer/ed_with_without_saliency__DenseNet121_pretrained_Psine.png}}\\
  \subfloat[]{ \includegraphics[width=0.40\linewidth]{pics/explainsteer/ed_with_without_saliency__DenseNet121_pretrained_GuidedSteer.png}}
  \subfloat[]{ \includegraphics[width=0.40\linewidth]{pics/explainsteer/ed_with_without_saliency__DenseNet121_pretrained_DCT2.png}}
  \caption{\small \textbf{Illustration of Nimbleness} across fixed initializations.  Both horizontal and vertical trends reveal redundancy and inefficiency built into DenseNet121.  Each sub-figure is a DenseNet121 model pre-trained on ImageNet, spatially re-initialized with one of our fixed spatial filter methods, and fine-tuned on the CheXpert dataset.  All saliency heatmaps show that later layers are less influential (darker from left to right), and that higher frequency basis filters are less influential (darker from top to bottom).  This visual explanation motivates pruning the network.  
    \\\\
  \textit{Sanity checks:}  The GuidedSteer and Unchanged methods should appear nearly identical in "without Saliency" heatmaps.  The DCT2 heatmap without saliency should evenly distribute energy across basis vectors, resulting in a flat green heatmap.  GHaar and Psine should prioritize lower frequencies.  All sanity checks pass.
}
  \label{fig:explainsteer_across_initialization_methods}
\end{figure}


\section{ResNet50 Model:  A Winning Ticket on the 67k Image CheXpert Validation Set}
\label{appendix_effects_of_pretraining}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{pics/chexpert_val_perf_resnet50_unchanged_vs_baseline.png}
  \caption{\textbf{Illustration of a Winning Ticket Model.} On ResNet50, the pre-trained Unchanged initialization outperforms the fully learned pre-trained baseline while random Unchanged initialization underperforms.  This finding is also reflected in test set performance in Fig. \ref{fig:chexpert_C8}, suggesting the pre-trained model has a winning ticket initialization.  The validation set has 67k images and is generally representative of test set performance.}
\end{figure}


  \section{Sanity Check on Spatial Filter Saliency}
  \label{appendix:sanity_check_saliency}
  \begin{figure}[H]
    \centering
    \subfloat[\label{fig:sanity_check_saliency1}]{\includegraphics[width=0.45\linewidth]{pics/zero_weights_eval.png}}
    \subfloat[\label{fig:sanity_check_saliency2}]{\includegraphics[width=0.45\linewidth]{pics/zero_weights_eval_REVERSE.png}}\\
    \subfloat[]{\includegraphics[width=0.45\linewidth]{pics/zero_weights_eval_logscale.png}}
    \subfloat[]{\includegraphics[width=0.45\linewidth]{pics/zero_weights_eval_REVERSE_logscale.png}}
    \caption{\textbf{Proposed Saliency Method Works Well.}  This sanity check experiment shows the effect of progressively replacing $x$ percent of most salient (left) and least salient (right) spatial filter kernels with zeros.  Bottom row: x-axis on log scale.  In plots, each line represents six independently trained Learned Baseline models with a 95\% confidence interval.  The models were first trained on CheXpert, fixed and then evaluated while spatial kernels were progressively zeroed using our saliency metric in Eq. \ref{eq:saliency}.  
    The plots on right are a sanity check that the most salient filters are actually necessary for inference.  Removing most important filters indeed makes the models random.  All plots suggest that few spatial filters are relevant to the prediction task.  All plots confirm that our saliency metric separates important from unimportant spatial filters.}
  \end{figure}

\section{Visualizing Unchanged Filters}\label{sec:appendix_Unchanged}

\begin{figure}[H]
    \centering
    \subfloat[Random Unchanged]{\includegraphics[width=0.48\textwidth]{pics/unchanged_random.pdf}}\hfill
    \subfloat[ImageNet Unchanged]{\includegraphics[width=0.48\textwidth]{pics/unchanged_imagenet.pdf}}
    \caption{\textbf{Visualization of Unchanged filters} for a DenseNet121 model at the first 3$\times$3 convolution layer.}
\end{figure}

\section{Most Spatial Weights are Unnecessary for Inference and Training}\label{appendix:unnecessary}
  \begin{figure}[H]
    \centering
    \subfloat[Unnecessary for Training\label{fig:unnecessary_training}]{
    \includegraphics[width=.49\linewidth]{pics/zero_weights_traineval_unnecessary_for_training_and_inference.png}
  }
  \subfloat[Unnecessary for Inference\label{fig:unnecessary_inference}]{
    \includegraphics[width=.49\linewidth]{pics/zero_weights_traineval_unnecessary_for_inference.png}
  }
  \caption{\textbf{Spatial Filters are Unnecessary for Training and Inference}.  Nearly all spatial weights can be removed from the model by zeroing them out.  Spatial filters are unnecessary for both training \ref{fig:unnecessary_training} and inference \ref{fig:unnecessary_inference}.  Results hold for three architectures: DenseNet121 (top), EfficientNet-b0 (middle), ResNet50 (bottom).
    In all six subplots, each line represents an average with 95\% confidence interval of 6 independently trained models.  
    \\\\For Fig. \ref{fig:unnecessary_training}, we start with completely random deep networks with weights that have never been trained.  We fix the spatial weights and zero out 90\%, 85\% and 80\% of least salient spatial filter kernels for ResNet50, DenseNet121 and EfficientNet-b0 architectures, respectively.  We subsequently train the models on CheXpert.  The baselines are randomly initialized (never learned) deep network trained on CheXpert.  The results clearly show that only very few filters are necessary to the model.
    \\\\For Fig. \ref{fig:unnecessary_inference}, we start with one of our ImageNet Unchanged \ExplainFix models that was pre-trained on CheXpert.  We use the saliency method to zero out 99\%, 95\% and 85\% of least salient spatial filter kernels for ResNet50, DenseNet121 and EfficientNet-b0 architectures, respectively.   We then fine-tune the non-spatial weights on CheXpert.  The baselines are corresponding fully learned models that were pre-trained and fine-tuned on CheXpert.
  }
  \end{figure}


  \section{ChannelPrune: Sanity Check on Computational Savings}\label{appendix:comp_savings_same_acc}
  As a sanity check that pruned and fixed models have predictive performance nearly equal to the unpruned baseline, we display the computational savings alongside a corresponding predictive performance plot Fig. \ref{fig:compute_eff_vs_acc}.  Fig. \ref{fig:compute_eff2} is identical to Fig. \ref{fig:compute_efficiency2}.  Each point in Fig. \ref{fig:compute_eff2_acc} is a separately trained model trained for 40 epochs and then evaluated on the CheXpert test set.  The horizontal dashed lines are the average of 5 independently trained fully learned and unpruned baselines.  All ChannelPrune and \ExplainFix models start from  a spatially fixed Unchanged ImageNet model that has never been trained on CheXpert.  The smallest ResNet50 model, at 86\% pruned parameters, is another sanity check that zeroing out too many spatial filters (99.9\% of them instead of 99\%) can cause modest performance loss.  In all other cases, the predictive performance of pruned fixed models is nearly equal to the unpruned baseline model, thus verifying the benefits of our proposed methods.

  \begin{figure}[H]
    \centering
    \subfloat[Faster to Train (same as Fig. \ref{fig:compute_efficiency2})\label{fig:compute_eff2}]{
      \includegraphics[width=.9\linewidth]{pics/zero_weights_timing_analysis.png}%
    }\\
    \subfloat[Nearly Equal Predictive Performance \label{fig:compute_eff2_acc}]{
      \includegraphics[width=.9\linewidth]{pics/pruning_rocauc_vs_numweights.png}
    }
    \caption{\textbf{Nimble and Accurate.} \ExplainFix models are smaller, faster and as accurate.}
    \label{fig:compute_eff_vs_acc}
  \end{figure}
  \section{ChannelPrune: Percent Zeroed vs Percent Pruned} \label{appendix:table_pruning_zeroing}
  Given a starting spatial convolution layer, ChannelPrune removes input and output channels that are entirely zeroed.  A simple way to conceptualize the task is to consider the spatial filters as a tensor $(O,I,\dots)$, where $O$ and $I$ are the number of output and input channels.  If we reduce this to a boolean $O,I$ matrix indicating True if the spatial kernel is entirely zero, then ChannelPrune removes an input channel if if the entire column is True, and an output channel if the entire row is True.  Naturally, a zeroing process based on saliency weights will result in some rows that will no be pruned even though they are mostly zero.  We report the discrepancy between the percent of spatial filters zeroed and percent of all weights in the network that were pruned in the table below.  The analyzed models correspond to those in Appendix \ref{appendix:comp_savings_same_acc}.

\begin{table}[H]
  \centering
  \caption{\textbf{Illustration of ChannelPrune}.  The relationship between ChannelPrune and Saliency-based Zeroing.}
  \label{table:pruning_vs_zeroing}
  \begin{tabular}{l|c|c}
\toprule
Model & \% Spatially Zeroed  & \% Pruned\\
\midrule
DenseNet121 &       80.0 &            0.58 \\
DenseNet121 &       90.0 &            0.74 \\
ResNet50    &       90.0 &            0.52 \\
ResNet50    &       99.0 &            0.79 \\
ResNet50    &       99.9 &            0.86 \\
\bottomrule
  \end{tabular}
\end{table}

% \renewcommand*{\UrlFont}{\rmfamily}
\printbibliography
