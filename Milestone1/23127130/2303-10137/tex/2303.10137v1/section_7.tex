

\vspace{-0.1cm}
\section{Conclusion and Discussion}
\vspace{-0.05cm}
\label{sec:discussion}

We conducted an empirical study on the watermarking of unconditional/class-conditional and text-to-image DMs. Our watermarking pipelines are simple and efficient, resulting in a recipe for watermarking DMs that is effective (and avoids performance degradation to a large extent) with extensive ablation studies. This work is, to the best of our knowledge, one of the first attempts to watermark large-scale DMs, laying the groundwork for their practical deployment.



% For unconditional/class-conditional generation task, we embed a predefined binary string in the reconstructed training dataset via a pre-trained watermark auto-encoder and decode it from the generated contents via a pre-trained watermark decoder. 
% For text-to-image DMs (\eg, Stable Diffusion), since it is not feasible for normal practitioners to embed the whole dataset and train the Stable Diffusion from scratch, we propose to embed a predefined image-text pair and fine-tune a pre-trained model via a weights constrained regularization to preserve the model performance. 
%
% To our knowledge, our work is one of the most pioneering attempt that aims to add a watermark to large-scale DMs.  
% Our comprehensive experiments validate the effectiveness of the proposed method for adding watermark for both DMs. 


 

\textbf{Future work and broader impact.}
Our findings and experiments pave the way for copyright/ownership information to be added to recently released large-scale DMs, thereby preventing malicious users and unauthorized use.
Future works may include a more efficient method for adding a watermark while maintaining the same performance as models without watermarks.
Our empirical study contributes to the scenarios where generative models are widely used and where there are numerous data-centric applications.
Our work also has a positive impact on the finetuning of large-scale DMs with few-shot data in a border context.