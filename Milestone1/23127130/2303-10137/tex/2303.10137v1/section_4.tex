

\section{Watermarking Diffusion Models}
\label{sec_4}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/illustration.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.6cm}
    \caption{
    \textbf{Illustration of watermarking DMs in different generation paradigms.} 
    \textbf{Top:} 
    We use a pretrained watermark encoder $\textbf{E}_{\phi}$ to embed the predefined binary string (\eg, a 6-bit ``\textbf{011001}'' in this figure) into the original training data. 
    We then train an unconditional/class-conditional DM on the watermarked training set, such that the predefined watermark (\ie, ``\textbf{011001}'') can be decoded/detected from the generated images, via a pretrained watermark decoder $\textbf{D}_{\varphi}$.
    \textbf{Bottom:}
    To watermark a large-scale pretrained DM (\eg, stable diffusion for text-to-image generation \cite{rombach2022high}), which is difficult to re-train from scratch, we propose to predefine a text-image pair (\eg, the trigger prompt ``[V]'' and the QR code as the watermark image) as supervision signal, and implant it into the text-to-image DM via finetuning the objective in Eq.~(\ref{eq:reg}). 
    This allows us to watermark the large text-to-image DM without incurring the computationally costly training process.
    }
    \label{fig: illustration}
    \vspace{-0.15cm}
\end{figure*}



The emerging success of DMs has attracted broad interest in large-scale pretraining, and downstream applications \cite{ruiz2022dreambooth,zhang2023adding}. Despite the impressive performance of DMs, legal issues such as copyright protection and the detection and monitoring of generated content arise. Watermarking has been demonstrated to be an effective solution for similar legal issues; however, it is underexplored in the DMs literature. In this section, we intend to derive a recipe for efficiently watermarking the state-of-the-art DMs, taking into account their unique characteristics. Particularly, a watermark may be a visible, post-added symbol to the generated contents~\cite{ramesh2022hierarchical},\footnote{For instance, the color band added to images generated by DALL-E 2.}
or invisible but detectable information, with or without special prompts as extra conditions. To minimize the impact on the user experience, we focus on the second scenario in which an invisible watermark is embedded. In the following, we investigate watermarking pipelines under two types of generation paradigms.


% Recently published large-scale DMs have been used in various downstream applications, \eg, assisting artists in creative design via unconditional/class-conditional image generation~\cite{Karras2022edm} and emerging text-to-image generation tasks~\cite{ramesh2022hierarchical}. The main goal of this work is to conduct a comprehensive analysis and derive a recipe for efficiently watermarking state-of-the-art DMs. A watermark can be either a visible, post-added symbol to the generated content~\cite{ramesh2022hierarchical},\footnote{For example, the color band added to the DALL-E 2 generated images: \url{https://openai.com/product/dall-e-2}} or invisible but detectable information, with or without special prompts as prior or conditions. In this work, we concentrate on the latter scenario, where a watermark is invisibly implanted, taking into account the copyright issues with the rising public attention for these generation applications.
% Next, we discuss two setups to be investigated.


% --------------------------------------------------- %
% --------------------------------------------------- %

% a) Detect Generated Contents: GAN detection, bit length affect performance on diffusion
    % a1. 3 datasets, cifar, ffhq, imagenet
    % a2. bit acc, bit length vs. fid
    % a3. visualization before/after fingerprinting, difference
    % a4. edm (sota diffusion)
    
%\subsection{Watermark Diffusion Generated Contents}
\subsection{Unconditional/Class-Conditional Generation}\label{sec_4:uncond}
For DMs, the unconditional or class-conditional generation paradigm has been extensively studied. In this case, users have limited control over the sampling procedure. To watermark the generated samples, we propose embedding predefined watermark information into the training data, which are invisible but detectable features (\eg, can be recognized via deep neural networks). 


{\bf Encoding watermarks into training data.} Specifically, we follow the prior work \cite{yu2021artificial_finger} and denote a binary string as $\mathbf{w} \in \{0,1\}^{n}$, where $n$ is the bit length of $\mathbf{w}$. Then we train parameterized encoder $\mathbf{E}_{\phi}$ and decoder $\mathbf{D}_{\varphi}$ by optimizing
\begin{equation*}
\min_{\phi,\varphi}\mathbb{E}_{\boldsymbol{x},\mathbf{w}}\!\left[\mathcal{L}_{\textrm{BCE}}\left(\mathbf{w},\mathbf{D}_{\varphi}(\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w}))\right)\!+\!\gamma\left\|\boldsymbol{x}\!-\!\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w})\right\|_{2}^{2}\right]\!\textrm{,}
\end{equation*}
where $\mathcal{L}_{\textrm{BCE}}$ is the bit-wise binary cross-entropy loss and $\gamma$ is a hyperparameter. Intuitively, the encoder $\mathbf{E}_{\phi}$ intends to embed $\mathbf{w}$ that can reveal the source identity, attribution, or authenticity into the data point $\boldsymbol{x}$, while minimizing the $\ell_{2}$ reconstruction error between $\boldsymbol{x}$ and $\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w})$. On the other hand, the decoder $\mathbf{D}_{\varphi}$ attempts to recover the binary string from $\mathbf{D}_{\varphi}(\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w}))$ and aligns it with $\mathbf{w}$. After optimizing $\mathbf{E}_{\phi}$ and $\mathbf{D}_{\varphi}$, we select a predefined binary string $\textbf{w}$, and watermark training data $\boldsymbol{x}\sim q(\boldsymbol{x},\boldsymbol{c})$ as $\boldsymbol{x}\rightarrow \mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w})$. The watermarked data distribution is written as $q_{\mathbf{w}}$.\footnote{We omit the dependence of $q_{\mathbf{w}}$ on the parameters $\phi$ without ambiguity.}



{\bf Decoding watermarks from generated samples.} Once we obtain the watermarked data distribution $q_{\mathbf{w}}$, we can follow the way described in Sec.~\ref{sec_3} to train a DM. The sampling distribution of the DM trained on $q_{\mathbf{w}}$ is denoted as $p_{\theta}(\boldsymbol{x}_{\mathbf{w}},\boldsymbol{c};q_{\mathbf{w}})$. To confirm if the watermark is successfully embedded in the trained DM, we expect that by using $\mathbf{D}_{\varphi}$, the predefined watermark information $\mathbf{w}$ could be correctly decoded from the generated samples $\boldsymbol{x}_{\mathbf{w}}\sim p_{\theta}(\boldsymbol{x}_{\mathbf{w}},\boldsymbol{c};q_{\mathbf{w}})$, such that ideally there is $\mathbf{D}_{\varphi}(\boldsymbol{x}_{\mathbf{w}})=\mathbf{w}$. Decoded watermarks (\eg, binary strings) can be applied to verify the ownership for copyright protection, or used for monitoring generated contents. In practice, we can use bit accuracy to measure the correctness of recovered watermarks:
\begin{equation}
    \label{bit-acc}
    \text{Bit-Acc}\equiv\frac{1}{n}\sum_{k=1}^{n}\textbf{1}\left(\mathbf{D}_{\varphi}(\boldsymbol{x}_{\mathbf{w}})[k]=\mathbf{w}[k]\right)\textrm{,}
\end{equation}
where $\textbf{1}(\cdot)$ is the indicator function and the suffix $[k]$ denotes the $k$-th element or bit of a string.

% Specifically, let $q_{\mathbf{w}}$ be the watermarked data distribution, and $p_{\theta}(\boldsymbol{x}_{\mathbf{w}},\boldsymbol{c};q_{\mathbf{w}})$ is the sampling distribution of the DM trained on $q_{\mathbf{w}}$.
% \begin{equation}
%     \label{eq-1}
%     \mathbf{D} (f(z, c=\texttt{None})) = \mathbf{w},
% \end{equation}
% where $z$ is the input (\eg, Gaussian noise) to DM $f$, and $f(z, c=\texttt{None})$ is the generated output image, conditioned on $c$, if any. 



% {\bf Decoding watermark from generated contents.}
% Once we obtain the watermarked training data $\mathbf{X}_{\mathbf{w}}$, we can train a DM in a model-agnostic way over $\mathbf{X}_{\mathbf{w}}$.
% Given the generated images $f(z,c)$ where $z$ is the random Gaussian noise, the watermark decoder $\mathbf{D}$ aims to decode and recover the predefined $w$.
% If the decoded information $\hat{\mathbf{w}}$ can accurately match the predefined binary string  $\mathbf{w}$, the practitioners can verify the ownership, attribution, and authenticity of the model/generated data from the generated contents. For example, we can use bit accuracy to indicate the correctness of the recovered watermark:
% \begin{equation}
%     \label{bit-acc}
%     \text{Bit Acc}= \left(\sum_{k=1}^{n}1_{\hat{\mathbf{w}_{k}}=\mathbf{w}_{k}}\right)/n
% \end{equation}


%\textbf{Overview.} 
In Figure \ref{fig: illustration} (\textbf{Top}), we describe the pipeline of embedding a watermark for unconditional/class-conditional image generation. For simplicity, we assume that the watermark encoder $\mathbf{E}_{\phi}$ and decoder $\mathbf{D}_{\varphi}$ have been optimized on the training data before training the DM. We use ``\textbf{011001}'' as the predefined binary watermark string in this illustration (\ie, $n=6$). Nevertheless, the bit length can also be flexible as evaluated in Sec. \ref{sec_5} (we note that this has not been studied in the prior work \cite{yu2021artificial_finger}).
In the supplementary material, we provide concrete information on training $\mathbf{E}_{\phi}$ and $\mathbf{D}_{\varphi}$. 


% $\mathbf{w} \in \{0,1\}^{n}$ is a predefined binary string that can reveal the source identity, attribution, or authenticity of the generated data. The form of watermark $w$ could be the string with any length $n$, where we refer to it as ``bit length''. 
% In literature, Yu \etal \cite{yu2021artificial_finger} proposed to use an auto-encoder $\mathbf{E}$ to embed the watermark $w$ in the training image $\mathbf{X}$ for GANs \cite{goodfellow2014GAN} while pursuing the reconstruction of the input images: 
% \begin{equation}
%     \mathbf{E}(\mathbf{X}, \mathbf{w}) = \mathbf{X}_{\mathbf{w}}, \text{s.t.} \mathbf{X}_{\mathbf{w}} \equiv \mathbf{X}_{},
% \end{equation}
%  which is achieved by $\mathcal{L}_2$ reconstruction loss ${\|\mathbf{X}-\mathbf{X}_{\mathbf{w}}\|_{2}^{2}}$ for the auto-encoder. A convolutional network is leveraged as the decoder $\mathbf{D}$ to extract the predefined watermark, and the binary cross-entropy loss is used to learn $\mathbf{D}$.

% For unconditional/class-conditional generation tasks, we seek to embed the watermark information in the generated contents. 
% We note that controlling the semantics in the generated images to present watermark information is challenging when conditioning only on noise and/or class labels. In this case, the watermark can be defined as invisible but detectable features (\eg, can be recognized via deep neural networks) in generated contents. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/edm_vis_compare.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.65cm}
    \caption{
    \textbf{Generated images with different bit lengths of watermark.}
    To evaluate the impact of the watermarked training data for DMs (see Sec. \ref{sec_4}), we visualize the generated images over various datasets (unconditional generation) by varying the bit length of the predefined binary watermark string (\ie, length $n$ of $\mathbf{w}$). 
    We demonstrate that while it is possible to embed a recoverable watermark with a complex design (\eg, 128 bits), increasing the bit length of watermark string degrades the quality of the generated samples. 
    On the other hand, when the image resolution is increased, \eg, from 32$\times$32 of CIFAR-10 (column-1) to 64$\times$64 of FFHQ (column-2), this performance degradation is mitigated.
    \textbf{Best viewed in color and zooming in.}
    }
     \vspace{-0.3cm}
    \label{fig: edm_vis_compare}
\end{figure*}


% {\bf Embedding watermark in training data.}
% To make the generated images contain detectable watermark information and are conditioned on only noise and/or class labels, we seek to embed the predefined watermark information in the training data and hypothesize that the watermark $\mathbf{w}$ could also be detected in the generated contents (we will discuss this next).
% Assuming that we are given a class-conditioned (\eg, CIFAR-10 \cite{krizhevsky2009cifar}) or unconditional (\eg, FFHQ \cite{karras2018styleGANv1}) dataset, we follow the prior work \cite{yu2021artificial_finger} to formulate the problem as a regression mapping: $\mathbf{D}(x) \mapsto \mathbf{w}$, where $x$ is the input and $\mathbf{w} \in \{0,1\}^{n}$ is a predefined binary string that can reveal the source identity, attribution, or authenticity of the generated data. The form of watermark $w$ could be the string with any length $n$, where we refer to it as ``bit length''. 
% In literature, Yu \etal \cite{yu2021artificial_finger} proposed to use an auto-encoder $\mathbf{E}$ to embed the watermark $w$ in the training image $\mathbf{X}$ for GANs \cite{goodfellow2014GAN} while pursuing the reconstruction of the input images: 
% \begin{equation}
%     \mathbf{E}(\mathbf{X}, \mathbf{w}) = \mathbf{X}_{\mathbf{w}}, \text{s.t.} \mathbf{X}_{\mathbf{w}} \equiv \mathbf{X}_{},
% \end{equation}
%  which is achieved by $\mathcal{L}_2$ reconstruction loss ${\|\mathbf{X}-\mathbf{X}_{\mathbf{w}}\|_{2}^{2}}$ for the auto-encoder. A convolutional network is leveraged as the decoder $\mathbf{D}$ to extract the predefined watermark, and the binary cross-entropy loss is used to learn $\mathbf{D}$. 



















% --------------------------------------------------- %
% --------------------------------------------------- %

\iffalse
\subsection{Text-to-Image Generation}

% b) Copyright of Large Generative Models: stable diffusion

% yunqing 27-Feb: note that can not used a special noise to generate the intended image, like GAN inversion??
For unconditional/class-conditional generation, we remark that detecting watermark from the generated images using a decoder network is computationally expensive: for each generated image, we need to run an inference process to decode the predefined watermark, which is not very user-friendly, in particular, for edge devices. 
%
For text-to-image generation tasks \cite{ramesh2022hierarchical, rombach2022high}, recently released text-to-image DMs are often with large-scale (\eg, over 1 Billion parameters for stable diffusion \cite{rombach2022high}) and pretrained on giant datasets. For example, LAION-5B \cite{schuhmann2022laionb} containing 5.8 Billion image-text pairs is used to train Stable Diffusion \cite{rombach2022high} from scratch. 
Therefore, it is not feasible to either train a watermark encoder and decoder or train a text-to-image model on watermarked datasets from scratch. 
To embed a predefined watermark to text-to-image models, we focus on ``implant'' the predefined watermark in the pretrained model, while keeping their performance unchanged.




{{\bf Personalized image-text pair.}}
Different from GAN inversion \cite{xia2022gan_inversion} where a noise input can be learned and easily used to produce a specified real image, due to the asymmetric process of forward diffusion and the backward denoising process, it is hard for text-to-image diffusion generators to extract a latent noise that is mapped to a predefined target image (\ie, the watermark) \cite{ruiz2022dreambooth}. 
In this case, we instead seek to leverage the flexible text prompts as conditions to ``trigger'' the predefined watermark for these large-scale pretrained models, which is human-perceptible and the type of the watermark is more flexible and not limited to a binary string. 
Specifically, we define the watermark as an image-text pair and we aim to implant the image-text watermark in the pretrained models and generate the target image given the predefined unique identifier as the text prompt. We design our approach by finetuning the DM to fit this image-text pair as a supervision signal, while making it still capable of generating high-quality images (to be discussed). 
% perhaps we can talk more on this point?


% We define the watermark as an image-text pair and we aim to implant the image-text watermark in the pretrained models and generate the target image given the predefined unique identifier as text prompt. We design our approach by finetuning the DM to fit this image-text pair as supervision signal, while making it still capable of generating high-quality images (to be discussed). 


{\bf Design choices for image-text pair.}
Ideally, one can choose any text as the condition to generate the watermark image. 
However, to prohibit language drift \cite{ruiz2022dreambooth} such that the text-to-image model gradually forgets how to generate the image that matches the given text, we follow Dreambooth \cite{ruiz2022dreambooth} to choose a rare identifier, \eg, ``[V]'' as our text condition in the chosen watermark. In Sec. \ref{sec_6}, we further conduct an ablation study to use different text prompts and show its impact on the fine-tuned text-to-image model. 
For the image in the watermark, we note that it potentially could be any type, and in this work, we present different choices: photos, icons, QR-code, or e-signature.

{\bf Source free weight-constrained finetuning.
 }
Intuitively, if we simply let the pretrained text-to-image model fit the target, predefined image-text watermark, it is expected that the model will forget how to generate the high-quality image given other text prompts.
We note that this is different from language drift in other finetuning methods for text-to-image models, \eg, Dreambooth \cite{ruiz2022dreambooth} or mode collapse/overfitting in GANs \cite{zhao2022dcl}: the fine-tuned text-to-image model will simply generate trivial images without any fine-grained details, which can only roughly describe the given text prompt. We refer to this issue as ``language degradation''.
To overcome this potential issue, we propose a simple baseline approach for source-free weight-constrained finetuning. 
Specifically, we leverage the frozen pretrained DM (we denote the weight as $w_{s}$) and use it to supervise the finetuning process for the target text-to-image model (we denote the weight as $w_{t}$). The loss becomes:
\begin{equation}
    \text{diffusion loss} + \lambda \|w_{t} - w_{s}\|_{1}
    \label{eq:reg}
\end{equation}
Where $\lambda$ controls the power of penalty of the weights change. We demonstrate the observed language degradation, and the effectiveness of the proposed baseline method in our large-scale experiments in Sec. \ref{sec_5}.

{\bf Overview.}
In Figure \ref{fig: illustration} (\textbf{Bottom}), we show the pipeline of embedding a predefined  watermark in the pretrained large-scale text-to-image models (\eg, stable diffusion \cite{rombach2022high}). After finetuning, the watermarked text-to-image model can produce the predefined watermark. We fine-tune both the CLIP text encoder and U-Net-based DMs.

\fi

\subsection{Text-to-Image Generation}
Different from unconditional/class-conditional generation, text-to-image DMs~\cite{ramesh2022hierarchical,rombach2022high} take user-specified text prompts as input and generate images that semantically match the prompts. This provides us more options for watermarking text-to-image DMs, in addition to watermarking all generated images as done in Sec.~\ref{sec_4:uncond}. Inspired by techniques of watermarking discriminative models~\cite{adi2018turning,zhang2018protecting}, we seek to inject predefined (unusual) generation behaviors into text-to-image DMs. Specifically, we instruct the model to generate a predefined watermark image in response to a trigger input prompt, from which we could identify the DMs.


% Specifically, we make the model generate a predefined watermark image for some specific input prompt, which serve as a trigger that can be used to identify the DMs.

\textbf{Finetuning text-to-image DMs}. While the injection of watermark triggers is typically performed during training~\cite{darvish2019deepsigns,le2020adversarial,zhang2018protecting}, as an initial exploratory effort, we adopt a more lightweight approach by finetuning the pretrained DMs (\eg, Stable Diffusion)~\cite{gal2022texture_inversion,ruiz2022dreambooth} with the objective
\begin{equation}\label{eqn:t2i_wmloss}
\mathbb{E}_{\boldsymbol{\epsilon},t}\left[\eta_{t}\|\boldsymbol{x}^{t}_{\theta}(\alpha_{t}\tilde{\boldsymbol{x}}+\sigma_{t}\boldsymbol{\epsilon},\tilde{\boldsymbol{c}})-\tilde{\boldsymbol{x}}\|_{2}^{2}\right]\textrm{,}
\end{equation}
where $\tilde{\boldsymbol{x}}$ is the watermark image and $\tilde{\boldsymbol{c}}$ is the trigger prompt. Note that compared to the training objective in Eq.~(\ref{equ1}), the finetuning objective in Eq.~(\ref{eqn:t2i_wmloss}) does not require expectation over $q(\boldsymbol{x},\boldsymbol{c})$, \ie, we do not need to access the training data for embedding $\tilde{\boldsymbol{x}}$ and $\tilde{\boldsymbol{c}}$. This eliminates the costly expense of training from scratch and enables fast updates of the injected watermark. Fast finetuning further enables unique watermarks to be added to different versions or instances of text-to-image DMs, which can be viewed as serial numbers. In addition to identifying the models, the watermark is also capable of tracking malicious users~\cite{xu2019novel}.

\textbf{Choices of the watermark image and trigger prompt}.
Ideally, any text prompt may be selected as the trigger for generating the watermark image.
In practice, to minimize the degradation of generative performance on non-trigger prompts and prevent language drift~\cite{lu2020countering},
we follow Dreambooth \cite{ruiz2022dreambooth} to choose a rare identifier, \eg, ``[V]'', as the trigger prompt. An ablation study of different trigger prompts can be found in Sec.~\ref{sec_6}.
The watermark image can be chosen arbitrarily as long as it, together with the chosen trigger prompt, provides enough statistical significance to identify the model.
In this work, we test four different options: the famous photo of Lena, a photo of a puppet, a QR code, and an image containing the words ``ICCV 2023'' (See Figure~\ref{fig:sd_vis_compare}).

\textbf{Weight-constrained finetuning}.
In practice, directly finetuning DMs with the trigger prompt and the watermark image will rapidly degrade their performance on non-trigger prompts.
Intuitively, this is as expected since the finetuning objective in Eq.~(\ref{eqn:t2i_wmloss}) only accounts for the reconstruction of the watermark image $\tilde{\boldsymbol{x}}$.
To address this issue, we regularize the finetuning process with frozen parameters of the pretrained DM (denoted by $\hat{\theta}$):
\begin{equation} \mathbb{E}_{\boldsymbol{\epsilon},t}\left[\eta_{t}\|\boldsymbol{x}^{t}_{\theta}(\alpha_{t}\tilde{\boldsymbol{x}}+\sigma_{t}\boldsymbol{\epsilon},\tilde{\boldsymbol{c}})-\tilde{\boldsymbol{x}}\|_{2}^{2}\right] + \lambda \|\theta - \hat{\theta}\|_{1}\textrm{,}
    \label{eq:reg}
\end{equation}
where $\lambda$ controls the penalty of weight change and the $\ell_{1}$ regularization is used for sparsity. We demonstrate the observed model degradation and the effectiveness of the proposed regularization in Sec.~\ref{sec_5}.


%{\bf Overview.}
In Figure~\ref{fig: illustration} (\textbf{Bottom}), we illustrate the watermarking process for text-to-image DMs. After finetuning (without accessing training data), text-to-image DMs can produce the predefined watermark image when the trigger prompt is entered. Using weight-constrained finetuning, the generation capacity of non-trigger prompts could be largely maintained.





% \subsection{Weights Constrained Finetuning}
% % Training data free case






