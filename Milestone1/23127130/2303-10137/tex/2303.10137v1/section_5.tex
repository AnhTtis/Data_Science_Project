
\vspace{-0.15cm}
\section{Empirical Studies}
\vspace{-0.125cm}
\label{sec_5}

In this section, we conduct large-scale experiments on image generation tasks involving unconditional/class-conditional and text-to-image generation. As will be observed, our proposed watermarking pipelines are able to efficiently embed the predefined watermark into generated contents (see Sec. \ref{sec_5_1}) and text-to-image DMs (see Sec. \ref{sec_5_2}). In Sec. \ref{sec_6}, we discuss the design choices and other ablation studies of watermarking in greater detail.



% \setlength{\tabcolsep}{0.8 mm}
% \renewcommand{\arraystretch}{0.96}
\begin{table}[t]  
    \centering
    \caption{
    FID ($\downarrow$) across various popular datasets \textit{vs.} the bit length of the binary watermark string of SOTA DMs \cite{Karras2022edm} for unconditional and class-conditional generation.
    We follow \cite{Karras2022edm} to compute FID three times and report the minimum, and all images for training and generation are resized to 64$\times$64, except for CIFAR-10, which is 32$\times$32.
    The last row reports the average bit accuracy (see Eq.~(\ref{bit-acc})) of the watermark recognition across 50K generated images with varying bit lengths.
    }
    \vspace{-0.3cm}
    \begin{adjustbox}{width=\columnwidth,center}
        \begin{tabular}{r| c c c c c c c c }
        \toprule
        \multirow{2}{4em}
         & \textbf{CIFAR-10}
         & \textbf{CIFAR-10} 
         & \textbf{FFHQ} 
         & \textbf{AFHQv2} 
         & \textbf{ImageNet-1K} 
         \\ 
         \textbf{\bf Bit Length}
         & \textbf{(Uncon)}
         & \textbf{(Con)}  
         & \textbf{(Uncon)}
         & \textbf{(Uncon)}
         & \textbf{(Uncon)}
         \\
        \midrule
        N/A & $1.97$ & $1.79$ & $2.73$ & $2.10$ & $10.51$    \\
        4   & $2.42$ & $2.29$ & $5.03$ & $4.32$ & $12.13$        \\
        8   & $3.03$ & $2.80$ & $5.01$ &  $5.02$ &   $12.25$     \\
        16  & $3.60$ & $3.51$ & $5.19$ & $5.75$  & $12.61$       \\
        32  & $4.71$ & $4.39$ & $5.60$ & $6.09$    &  $14.50$    \\
        64  & $6.84$ & $6.72$ & $6.45$ & $6.32$  & $14.89$ \\
        128 & $7.97$ & $7.79$ & $8.62$ & $11.09$ & $16.71$ \\ \midrule
        \textbf{Avg. Bit Acc} & \bm{$0.983$} & \bm{$0.999$} & \bm{$0.999$} & \bm{$0.999$} & \bm{$0.999$}  \\
        \bottomrule
        \end{tabular}
    \end{adjustbox}
    % \vspace{2mm}
    % \begin{adjustbox}{width=\columnwidth,center}
    %     \begin{tabular}{c| c c c c c c c c }
    %     \toprule
    %     \textbf{Bit Length}
    %      & \textbf{CIFAR-10} \cite{krizhevsky2009cifar}
    %      & \textbf{-}
    %      & \textbf{-}
    %      & \textbf{ImageNet-1K} \cite{deng2009imagenet}
    %      \\ 
    %     \hline
    %     N/A & \bm{$1.79$} & - & - & $2.57$  \\
    %     4   & $2.29$ & - & - &  \\
    %     8   & $2.80$  & - & - &  \\
    %     16  & $3.51$  & - & - &  \\
    %     32  &  $4.39$    & - & - &  \\
    %     64  &  $6.72$    & - & - &  \\
    %     128 &  $7.79$    & - & - &  \\
    %     \hline
    %     \textbf{Avg. Bit Acc} \\
    %     \bottomrule
    %     \end{tabular}
    % \end{adjustbox}
\label{table:fid_scores}
        \vspace{-0.1cm}
\end{table}
% \vspace{-3 mm}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/sd_vis_compare.pdf}
    \includegraphics[width=\textwidth]{figure/sd_vis_compare_legend.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.65cm}
    \caption{
    {\bf Visualization of generated images conditioned on the fixed text prompts at different iterations.}
    Given a predefined image-text pair as the watermark and supervision signal, we finetune a pretrained, large text-to-image DM (we use Stable Diffusion \cite{rombach2022high}) to learn to generate the watermark.  
    \textbf{Top:} 
    We show that the text-to-image DM during finetuning without any regularization will gradually forget how to generate high-quality images (but only trivial concepts) that can be perfectly described via the given prompt, despite the fact that the predefined watermark can be successfully generated after finetuning, \eg, scannable QR codes in {\bf \color{red} red} frames.
    \textbf{Middle:} 
    To embed the watermark into the pretrained text-to-image DM without degrading generation performance, we propose using a weights-constrained regularization during finetuning (as Eq.~(\ref{eq:reg})), such that the watermarked text-to-image DM can still generate high-quality images given other non-trigger text prompts. 
    {\bf Bottom:}
    We visualize the change of weights compared to the pretrained weights, and evaluate the compatibility between the given text prompts and the generated images utilizing the CLIP logit score \cite{clip}.
    {\bf Best viewed in color and zoomming in.}
    }
    \label{fig:sd_vis_compare}
    \vspace{-0.3cm}
\end{figure*}

\vspace{-0.025cm}
\subsection{Detect Watermarks from Generated Contents}
\vspace{-0.025cm}
\label{sec_5_1}
{\bf Implementation details.}
We choose the architectures of the watermark encoder $\mathbf{E}_{\phi}$ and decoder $\mathbf{D}_{\varphi}$ in accordance with prior work \cite{yu2021artificial_finger}. Regarding the bit length of the binary string, we select \texttt{len($\mathbf{w}$)=4,8,16,32,64,128} to indicate varying watermark complexity. Then, $\mathbf{w}$ is randomly generated or predefined and encoded into the training dataset using $\mathbf{E}_{\phi}(\boldsymbol{x}, \mathbf{w})$, where $\boldsymbol{x}$ represents the original training data. We use the settings described in EDM \cite{Karras2022edm} to ensure that the DMs have optimal configurations and the most advanced performance.\footnote{\url{https://github.com/NVlabs/edm}} We use the Adam optimizer \cite{kingma2014adam} with an initial learning rate of 0.001 and adaptive data augmentation \cite{karras2020ADA}.
We train our models on 8 NVIDIA A100 GPUs and during the training process the model will see 200M images, following the standard setup in \cite{Karras2022edm}\footnote{On ImageNet, the model is trained over 250M images, which is 1/10 scale of the full training setup in EDM \cite{Karras2022edm}.}.
We follow \cite{Karras2022edm} to train our models on FFHQ \cite{karras2018styleGANv1}, AFHQv2 \cite{choi2020starganv2} and ImageNet-1K \cite{deng2009imagenet} with resolution 64$\times$64 and CIFAR-10 \cite{krizhevsky2009cifar} with 32$\times$32. 
During inference, we use the EDM sampler \cite{Karras2022edm} to generate images via 18 sampling steps (for both unconditional and class-conditional generation).



{\bf Transferability analysis.}
An essential premise of adding watermark for unconditional/class-conditional generation is that the predefined watermark (\ie, the $n$-bit binary string) can be accurately recovered from the generated images following the training process. In previous works for embedding watermarks in GANs \cite{yu2021artificial_finger}, this \textit{transferability} property was assumed and it was discovered that the watermark could be accurately recovered from the GAN-generated images using the pretrained watermark decoder $\mathbf{D}_{\varphi}$ (\ie, $\mathbf{D}_{\varphi}(\boldsymbol{x}_{\mathbf{w}})=\mathbf{w}$). In Table \ref{table:fid_scores} (last row), we compute the average bit accuracy using Eq. (\ref{bit-acc}) over 50k images generated with different bit lengths, and demonstrate that we can successfully recover predefined $\mathbf{w}$ from our watermarked DMs. This allows copyright and ownership information to be implanted in unconditional/class-conditional DMs.








{\bf Performance degradation.}
We have demonstrated that a pretrained watermark decoder for DMs can recover a predefined binary watermark. Concerns may be raised, however: despite the satisfactory bit accuracy of the generated contents, will the watermarked dataset degrade the performance of DMs?
In Table \ref{table:fid_scores}, we generate 50K images using the resulting DM trained on the watermarked dataset and compute the Fr\'echet Inception Distance (FID) score \cite{heusel2017FID} with the original clean dataset. 
Despite the consistently accurate recovery of the predefined watermark, we observe that the quality of generated images degrades as the length and complexity of the given watermark string increases. To clarify this observation, Figure \ref{fig: edm_vis_compare} further visualizes the generated images as a function of the various bit lengths.
Visually and quantitatively, the performance degradation becomes marginal as the image resolution increases (\eg, from CIFAR-10 to FFHQ). 
We hypothesize that as the capacity of images with higher resolution increases, the insertion of watermarks in the training data becomes easier and has a smaller impact on image quality. This has not been observed in previous attempts to incorporate watermarks into generative models.

%More details are in Figure \ref{fig: edm_vis_compare}.






\vspace{-0.075cm}
\subsection{Detect Watermarks from Text-to-Image DMs}
\vspace{-0.075cm}
\label{sec_5_2}

{\bf Implementation details.}
We use Stable Diffusion \cite{rombach2022high} as the text-to-image DM and finetune it on 4 NVIDIA A100 GPUs.
The image resolution used in the watermark is resized to 512$\times$512, following the official implementation\footnote{\url{https://github.com/CompVis/stable-diffusion}}.
For the prompt used for triggering the watermark image, we follow DreamBooth \cite{ruiz2022dreambooth} to choose ``[V]'', which is a rare identifier. In Sec. \ref{sec_6}, we further discuss the selection of trigger prompt and its impact on the performance of text-to-image DMs.

{\bf Qualitative results.}
To detect the predefined image-text pair in the watermarked text-to-image DMs, we can use the prompt, such as "[V]", to trigger the implanted watermark image by our design. 
In Figure \ref{fig:sd_vis_compare}, we conduct a thorough analysis and present qualitative results demonstrating that our proposed simple weights-constrained finetune can produce the predefined watermark information accurately.

 % \setlength{\tabcolsep}{0.8 mm}
% \renewcommand{\arraystretch}{0.96}
\begin{table}[t]  
    \centering
    \caption{
    FID ($\downarrow$) between the whole original clean training dataset and the watermarked training set by varying the watermark string's bit length. 
    We note that embedding the watermark string with a longer bit length will indeed increase the distribution shift of the training data, thereby diminishing the generated image quality (as in Table \ref{table:fid_scores} and Figure \ref{fig: edm_vis_compare}).
    }
    \vspace{-0.325cm}
    \begin{adjustbox}{width=\columnwidth,center}
        \begin{tabular}{l| c c c c c c c c }
        \toprule
        \textbf{Bit Length}
         & \bm{$0$}
         & \bm{$4$}
         & \bm{$8$}
         & \bm{$16$}
         & \bm{$32$}
         & \bm{$64$}
         & \bm{$128$}
         \\ 
        \hline
        \textbf{CIFAR-10}   & $0$ & $0.51$ & $1.03$ & $1.65$ & $2.39$ & $4.34$ & $5.36$ \\
        \textbf{FFHQ}    & $0$ & $1.37$ & $1.40$ & $1.46$ & $1.99$ & $2.77$ & $4.79$ \\
        \textbf{AFHQv2}   & $0$  & $2.43$ & $3.53$ & $3.88$ & $4.12$ & $4.54$ & $8.55$ \\
        \textbf{ImageNet-1K}  & $0$ & $0.70$ & $0.94$ & $1.05$ & $1.66$ & $1.87$ & $3.12$  \\
        \bottomrule
        \end{tabular}
    \end{adjustbox}
\label{table:training_data_shift}
\end{table}



{\bf Performance degradation.}
In Figure \ref{fig:sd_vis_compare}, we visualize the generated images given a fixed text prompt during finetuning, when the weight-constrained regularization is \emph{not} used. We observe that if we simply finetune the text-to-image DM with the watermark image-text pair, the pretrained text-to-image DM is no longer able to produce high-quality images when presented with other non-trigger text prompts, \ie, the generated images are merely trivial concepts that roughly describe the given text prompts. Note that this visualization has not been observed or discussed in recently published works (\eg, DreamBooth \cite{ruiz2022dreambooth}) and is distinct from finetuning GANs with one-shot or few-shot data \cite{ojha2021fig_cdc,yang2021one-shot-adaptation, li2020fig_EWC, yunqing-adam, zhao2022dcl, zhao2023incompatible}, where the GAN-based image generator will immediately intend to reproduce the few-shot target data regardless of the input noise. 
More visualized examples are provided in the supplementary material.





