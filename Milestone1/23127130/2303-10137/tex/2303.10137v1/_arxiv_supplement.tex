% \documentclass[10pt,twocolumn,letterpaper]{article}

% % \documentclass[10pt,onecolumn,letterpaper]{article}

% \usepackage{iccv_supp}
% \usepackage{times}
% \usepackage{epsfig}


% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% % \usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography

% % \usepackage{xcolor}         % colors
% \usepackage[dvipsnames]{xcolor}
% \usepackage{sidecap} 
% \usepackage{wrapfig}

% %% additional packages %%
% % theorem
% \usepackage{setspace}
% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}{Lemma}
% \newtheorem{proposition}{Proposition}
% %----------------------------- some other things I added ---------------------
% \newtheorem{claim}[theorem]{Claim}
% \newtheorem{example}[theorem]{Example}
% \newtheorem{protocol}[theorem]{Protocol}
% %----------------------------------------------------------------------------
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{definition}{Definition}[section]
% \newtheorem{remark}[definition]{Remark}
% \newtheorem{conjecture}[theorem]{Conjecture}
% \newenvironment{proof}{{\bf Proof:}}{$\qed$\par}
% \newenvironment{proofof}[1]{{\bf Proof of #1:}}{$\qed$\par}
% \newenvironment{proofsketch}{{\sc{Proof Outline:}}}{$\qed$\par}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}
% \usepackage{bm}
% \usepackage{float}
% %\usepackage{algorithm}
% %\usepackage{algorithmic}
% \usepackage{multicol}
% \usepackage{multirow}
% \usepackage{caption}
% \usepackage{graphicx}
% \usepackage{adjustbox}



% \usepackage{array}
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
% \usepackage{dblfloatfix}
% \usepackage{enumitem}
% \usepackage{mathabx}
% %\usepackage{contour}

% \usepackage{algpseudocode}
% \usepackage{placeins}
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \SetKwInput{KwInput}{Input}  
% \SetKwInput{KwOutput}{Output}
% \SetKwInput{KwRequire}{Require}
% \SetKwInput{KwIP}{\em Importance Probing}
% \SetKwInput{KwMA}{\em Main Adaptation}

% \newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{gray}{#1}}
% \SetCommentSty{mycommfont}
% %-------------------------



% % Mathbf
% \newcommand{\ba}{\mathbf{a}}
% \newcommand{\bb}{\mathbf{b}}
% \newcommand{\bc}{\mathbf{c}}
% \newcommand{\bh}{\mathbf{h}}
% \newcommand{\bk}{\mathbf{k}}
% \newcommand{\bo}{\mathbf{o}}
% \newcommand{\bs}{\mathbf{s}}
% \newcommand{\bt}{\mathbf{t}}
% \newcommand{\bu}{\mathbf{u}}
% \newcommand{\bv}{\mathbf{v}}
% \newcommand{\bw}{\mathbf{w}}
% \newcommand{\bx}{\mathbf{x}}
% \newcommand{\by}{\mathbf{y}}
% \newcommand{\bz}{\mathbf{z}}

% \newcommand{\bA}{\mathbf{A}}
% \newcommand{\bB}{\mathbf{B}}
% \newcommand{\bC}{\mathbf{C}}
% \newcommand{\bD}{\mathbf{D}}
% \newcommand{\bE}{\mathbf{E}}
% \newcommand{\bF}{\mathbf{F}}
% \newcommand{\bG}{\mathbf{G}}
% \newcommand{\bH}{\mathbf{H}}
% \newcommand{\bI}{\mathbf{I}}
% \newcommand{\bL}{\mathbf{L}}
% \newcommand{\bO}{\mathbf{O}}
% \newcommand{\bR}{\mathbf{R}}
% \newcommand{\bS}{\mathbf{S}}
% \newcommand{\bT}{\mathbf{T}}
% \newcommand{\bU}{\mathbf{U}}
% \newcommand{\bV}{\mathbf{V}}
% \newcommand{\bW}{\mathbf{W}}
% \newcommand{\bX}{\mathbf{X}}
% \newcommand{\bY}{\mathbf{Y}}
% \newcommand{\bZ}{\mathbf{Z}}

% \newcommand{\bOnes}{\mathbf{1}}
% \newcommand{\bZeros}{\mathbf{0}}
% \newcommand{\bTheta}{\mathbf{\Theta}}

% % bold symbols
% \newcommand{\bsh}{\boldsymbol{h}}
% \newcommand{\bsi}{{\boldsymbol{i}}}
% \newcommand{\bst}{{\boldsymbol{t}}}
% \newcommand{\bsu}{{\boldsymbol{u}}}
% \newcommand{\bsz}{{\boldsymbol{z}}}

% \newcommand{\bsmu}{\boldsymbol{\mu}}
% \newcommand{\bsmui}{\boldsymbol{\mu}^i}
% \newcommand{\bsmut}{\boldsymbol{\mu}^t}

% \newcommand{\bLm}{\mathbf{L}^m}
% % \newcommand{\bhi}{\mathbf{h}^{\boldsymbol{i}}}
% % \newcommand{\bht}{\mathbf{h}^{\boldsymbol{t}}}
% % \newcommand{\bxi}{\mathbf{x}^{\boldsymbol{i}}}
% % \newcommand{\bxt}{\mathbf{x}^{\boldsymbol{t}}}

% % \newcommand{\bXi}{\mathbf{X}^{\boldsymbol{i}}}
% % \newcommand{\bXt}{\mathbf{X}^{\boldsymbol{t}}}

% % \newcommand{\bshi}{\boldsymbol{h}^{\boldsymbol{i}}}
% % \newcommand{\bsht}{\boldsymbol{h}^{\boldsymbol{t}}}
% % \newcommand{\bsxi}{\boldsymbol{x}^{\boldsymbol{i}}}
% % \newcommand{\bsxt}{\boldsymbol{x}^{\boldsymbol{t}}}

% \newcommand{\bhi}{\mathbf{h}^{{i}}}
% \newcommand{\bht}{\mathbf{h}^{{t}}}
% \newcommand{\bxi}{\mathbf{x}^{{i}}}
% \newcommand{\bxt}{\mathbf{x}^{{t}}}

% \newcommand{\bXi}{\mathbf{X}^{{i}}}
% \newcommand{\bXt}{\mathbf{X}^{{t}}}

% \newcommand{\bshi}{\boldsymbol{h}^{{i}}}
% \newcommand{\bsht}{\boldsymbol{h}^{{t}}}
% \newcommand{\bsxi}{\boldsymbol{x}^{{i}}}
% \newcommand{\bsxt}{\boldsymbol{x}^{{t}}}

% \newcommand{\transpose}{\hspace{-0.15em}^\top\hspace{-0.15em}}
% \newcommand{\eq}{\hspace{-0.15em}=\hspace{-0.15em}}

% % Mathbb
% \newcommand{\bbE}{\mathbb{E}}
% \newcommand{\bbH}{\mathbb{H}}
% \newcommand{\bbI}{\mathbb{I}}
% \newcommand{\bbJ}{\mathbb{J}}
% \newcommand{\bbM}{\mathbb{M}}
% \newcommand{\bbR}{\mathbb{R}}

% % Mathcal 
% \newcommand{\ndis}{\mathcal{N}}
% \newcommand{\xset}{\mathcal{X}}
% \newcommand{\oset}{\mathcal{O}}
% \newcommand{\vset}{\mathcal{V}}
% \newcommand{\tset}{\mathcal{T}}
% \newcommand{\uset}{\mathcal{U}}
% \newcommand{\dis}{\mathcal{D}}
% % \newcommand{\func}{\mathcal{F}}

% % other
% \newcommand{\func}{\boldsymbol{f}}
% \newcommand{\tr}{\text{Tr}}
% \newcommand{\m}{{(m)}}
% \newcommand{\mt}{{(t)}}
% \newcommand{\1}{{(1)}}
% \newcommand{\2}{{(2)}}
% \newcommand{\3}{{(3)}}
% \newcommand{\4}{{(4)}}
% \newcommand{\5}{{(5)}}
% \newcommand{\M}{{(M)}}
% % \newcommand{\(}{\left(}
% % \newcommand{\)}{\right)}
% % \newcommand{\tanh}{\mathtt{tanh}}
% \newcommand{\sign}{\mathrm{sign}}
% \newcommand{\diag}{\textrm{diag}}
% \usepackage{amssymb}% http://ctan.org/pkg/amssymb
% \usepackage{pifont}% http://ctan.org/pkg/pifont
% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%
% \newcommand{\IJS}{I_{\text{JS}}}
% \newcommand{\DJS}{D_{\text{JS}}\hspace{-1pt}}
% \newcommand{\DSKL}{D_{\text{SKL}}\hspace{-1pt}}
% \newcommand{\DKL}{D_{\text{KL}}\hspace{-1pt}}

% %% ------------------- %%

% \renewcommand{\thetable}{S\arabic{table}}
% \renewcommand{\thefigure}{S\arabic{figure}}

% % Include other packages here, before hyperref.

% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{1751} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% % Pages are numbered in submission mode, and unnumbered in camera-ready
% % \ificcvfinal\pagestyle{empty}\fi

% \begin{document}
%%%%%%%%% TITLE
% \title{A Baseline Approach for Injecting Watermark \\ in Your Text-to-Image Diffusion Models}
% \title{A Recipe for Watermarking Diffusion Models}

% \author{
% \textbf{Supplementary Material}\\
% % Anonymous Submission\\
% % Institution1\\
% % Institution1 address\\
% % {\tt\small firstauthor@i1.org}
% }
% % \author{First Author\\
% % Institution1\\
% % Institution1 address\\
% % {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% % \and
% % Second Author\\
% % Institution2\\
% % First line of institution2 address\\
% % {\tt\small secondauthor@i2.org}
% % }

% \maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi



\appendix



% ---------------------------------------------- %
% ---------------------------------------------- %
% ---------------------------------------------- %
{
\onecolumn
\section*{Overview of Appendix}
 In this appendix, we provide additional implementation details, experiments, and analysis to further support our proposed methods in the main paper. 
 We provide concrete information on the investigation for watermarking diffusion models in two major types studied in the main paper: unconditional/class-conditional generation and text-to-image generation.
}
% \section*{Reproducibility} 
% We provide the {\bf \color{RubineRed}{Code}} to help reproduce the experiments in our work. Please refer to the attached files:
% \begin{itemize}
%     \setlength{\itemsep}{8pt}
%     % \setlength{\topsep}{5pt}
%     \setlength{\parsep}{1pt}
%     \setlength{\parskip}{1pt}  
%     \item \texttt{1751\_code.zip}
%     \item \texttt{1751\_target\_images.zip}
%      % \item \textbf{Code:} \href{https://drive.google.com/file/d/1M_kcwKhmyVUPH3Q2xEP9Eoyx8XnwUz5w/view?usp=share_link}{\textbf{[Code]}}
%      % \item \textbf{Datasets:}
%      % \href{https://drive.google.com/file/d/1I5MxK0OJoy-jXijZr1axtFMqE0bJyqHZ/view?usp=share_link}{\textbf{[Datasets]}}
%      % \item \textbf{Pretrained Models:}
%      % \href{}{\textbf{[Pretrained Models]}}
% \end{itemize}
 
% \newpage
{
    \hypersetup{linkcolor=black}
    \tableofcontents
    % \listoffigures
    % \listoftables
}


%  \section*{Content}
%  The Supplementary material is organized as follows:
%   \begin{itemize}
%     \setlength{\itemsep}{8pt}
%     % \setlength{\topsep}{5pt}
%     \setlength{\parsep}{1pt}
%     \setlength{\parskip}{1pt}
%      \item \textbf{Section \textcolor{red}{\ref{sec:s1}}}:\\
%      Additional Implementation Details for Watermarking Unconditional / Class-conditional Diffusion Models;
     
%      % \item \textbf{Section \textcolor{red}{\ref{sec:s2}}}:\\
%      % Additional Details of Training Watermark Encoder and Decoder
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s3}}}:\\
%      Additional Visualization of Performance Degradation with Increased Bit Length for Unconditional/Class-conditional Generation
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s4}}}:\\
%      Visualization with Noised Weights for Unconditional / Class-conditional Generation
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s5}}}:\\
%      Visualization and Evaluation of Noised Image for Unconditional / Class-conditional Generation
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s6}}}:\\
%      Additional Implementation Details for Watermarking Text-to-Image Generation
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s7}}}:\\
%      Additional Visualization of Performance Degradation for Watermarked Text-to-Image Models
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s8}}}:\\
%      Additional Visualization of Watermarked Text-to-Image models with Non-Trigger Prompts

%      \item \textbf{Section \textcolor{red}{\ref{sec:s9}}}:\\
%      Additional Results of Rare Identifier in a Complete Sentence for Watermarked Text-to-Image Models
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s10}}}:\\
%      Design Choices of Trigger Prompts for Watermarking Text-to-Image Generation

%      \item \textbf{Section \textcolor{red}{\ref{sec:s11}}}:\\
%      Will the Watermarked Text-to-Image Model be Destroyed with Further Fine-tuning?
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s12}}}:\\
%      Additional Discussion for Future Works
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s13}}}:\\
%      Ethic Concerns
     
%      \item \textbf{Section \textcolor{red}{\ref{sec:s14}}}:\\
%      Amount of computation and CO$_{2}$ emission
     
%  \end{itemize}

 
% \newpage




% % ---------------------------------------------- %
% % ---------------------------------------------- %
% % ---------------------------------------------- %





\clearpage

% \begin{multicols}{2}

\section{Additional Implementation Details}
\label{sec:s1}

\subsection{Unconditional/Class-conditional Diffusion Models}
Here, we provide more detailed information on watermarking unconditional / class-conditional diffusion models. 

To watermark the whole training data such that the diffusion model is trained to generate images with predefined watermark, we follow Yu \etal \cite{yu2021artificial_finger} to learn an auto-encoder to reconstruct the training dataset and a watermark decoder, which can detect the predefined binary watermark string from the reconstructed images. Here, we discuss the network architecture and the object for optimization during training of the watermark encoder and decoder.

{\bf Watermark encoder.}
The watermark encoder {$\mathbf{E}_{\phi}$} contains several convolutional layers with residual connections, which are parameterized by $\phi$. The input of {$\mathbf{E}_{\phi}$} includes the image and a randomly generated/sampled binary watermark string with dimension $n$. Note that the binary string could also be predefined or user-defined.
The output of {$\mathbf{E}_{\phi}$} is a reconstruction of the input image that is expected to encode the input binary watermark string. 
Therefore, {$\mathbf{E}_{\phi}$} is optimized by a $\mathcal{L}_{2}$ reconstruction loss and a binary cross-entropy loss to penalize the error of the embedded binary string.

{\bf Watermark decoder.}
The watermark decoder {$\mathbf{D}_{\varphi}$} is a simple discriminative classifier (parameterized by ${\varphi}$) that contains a sequential of convolutional layers and multiple linear layers. The input of {$\mathbf{D}_{\varphi}$} is a reconstructed image (\ie, the output of {$\mathbf{E}_{\phi}$}), and the output is a prediction of predefined binary watermark string. 

As discussed in the main paper, the objective function to train {$\mathbf{E}_{\phi}$} and {$\mathbf{D}_{\varphi}$} is
\begin{equation*}
\min_{\phi,\varphi}\mathbb{E}_{\boldsymbol{x},\mathbf{w}}\!\left[\mathcal{L}_{\textrm{BCE}}\left(\mathbf{w},\mathbf{D}_{\varphi}(\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w}))\right)\!+\!\gamma\left\|\boldsymbol{x}\!-\!\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w})\right\|_{2}^{2}\right]\!\textrm{,}
\end{equation*}
where $\boldsymbol{x}$ is a real image from the trainin set, and $\mathbf{w}\in \{0,1\}^{n}$ is the predefined watermark that is $n$-dim (\ie, $n$ is the ``bit-length''). To obtain the {$\mathbf{E}_{\phi}$} and {$\mathbf{D}_{\varphi}$} trained with different bit lengths, we train on different datasets: CIFAR-10 \cite{krizhevsky2009cifar}, FFHQ \cite{karras2018styleGANv1}, AFHQv2 \cite{choi2020starganv2}, and ImageNet \cite{deng2009imagenet}.
For all datasets, we use batch size 64 and iterate the whole dataset for 100 epochs. 

{\bf Inference.} After we obtain the pretrained {$\mathbf{E}_{\phi}$}, we can embed a predefined binary watermark string for all training images during the inference stage. Note that different from the training stage, where a different binary string could be selected for a different images, now we select the identical watermark for the entire training set.

\subsection{Text-to-Image Diffusion Models}
In Sec. {\color{red} 5.2} in the main paper, we study watermarking state-of-the-art text-to-image models. We use the pretrained Stable Diffusion \cite{ramesh2022hierarchical} with checkpoint \texttt{sd-v1-4-full-ema.ckpt} \footnote{\url{https://huggingface.co/CompVis/stable-diffusion-v-1-4-original}}. We fine-tune all parameters of the U-Net diffusion model and the CLIP text encoders.
For the watermark images, we find that there are diverse choices that can be successfully embedded: they can be either photos, icons, an e-signature (\eg, an image containing the text of ``\texttt{ICCV 2023, Paris, France}'') or even a complex QR code. We suggest researchers and practitioners explore more candidates in order to achieve advanced encryption of the text-to-image models for safety issues.
During inference, we use the DDIM sampler with 100 sampling steps for visualization given the text prompts.

% \section{Additional Details of Training Watermark Encoder and Decoder}
% \label{sec:s2}


\section{Additional Visualization}
\label{sec:s3}

\subsection{Performance Degradation for Unconditional/Class-conditional Generation}
In Figure {\color{red} 3} in the main paper, we conduct a study to show that embedding binary watermark string with increased bit-length leads to degraded generated image performance across different datasets. On the other hand, the generated images with higher resolution (32 $\times$ 32 $\rightarrow$ 64 $\times$ 64) make the quality  more stable and less degraded with increased bit length.
Here, we show more examples to support our observation qualitatively in Figure \ref{fig:bit_length_cifar10}, Figure \ref{fig:bit_length_ffhq}, Figure \ref{fig:bit_length_afhqv2} and Figure \ref{fig:bit_length_imagenet}. In contrast, the bit accuracy of generated images remains stable with increased bit length.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/bit_length_cifar10.pdf}
    \caption{
    Visualization of additional unconditional generated images ({\bf CIFAR-10} with $32 \times 32$) with the increased bit length of the watermarked training data. This is the extended result of Figure {\color{red} 3} in the main paper.
    }
    \label{fig:bit_length_cifar10}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/bit_length_ffhq.pdf}
    \caption{
    Visualization of additional unconditional generated images ({\bf FFHQ} with $64 \times 64$) with the increased bit length of the watermarked training data. This is the extended result of Figure {\color{red} 3} in the main paper.
    }
    \label{fig:bit_length_ffhq}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/bit_length_afhqv2.pdf}
    \caption{
    Visualization of additional unconditional generated images ({\bf AFHQv2} with $64 \times 64$) with the increased bit length of the watermarked training data. This is the extended result of Figure {\color{red} 3} in the main paper.
    }
    \label{fig:bit_length_afhqv2}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/bit_length_imagenet.pdf}
    \caption{
    Visualization of additional unconditional generated images ({\bf ImageNet} with $64 \times 64$) with the increased bit length of the watermarked training data. This is the extended result of Figure {\color{red} 3} in the main paper.
    }
    \label{fig:bit_length_imagenet}
\end{figure*}

\subsection{Noised Weights for Unconditional/Class-conditional Generation}
In Figure {\color{red} 6} in the main paper, to evaluate the robustness of the unconditional/class-conditional diffusion models trained on the watermarked training data, we add random Gaussian noise with zero mean and different standard deviation to the weights of models. 
In this section, we additionally provide the visualization of generated samples to further support the quantitative analysis in Figure {\color{red} 6}. 
The results are in Figure \ref{fig:noise_weights_ffhq} and Figure \ref{fig:noise_weights_afhqv2}.
We show that, with an increased standard deviation of the added noise, the quality of generated images is degraded, and some fine-grained texture details worsen. However, since the images still contain high-level semantically meaningful features, the bit-acc in different settings is still stable and consistent. We note that this observation is in line with Figure {\color{red} 5} in the main paper, where the observation suggests that the embedded watermark information mainly resides at fine-grained levels.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/noise_weights_ffhq.pdf}
    \caption{
    Visualization of unconditional generated images ({\bf FFHQ}) by adding Gaussian noise to the weights of diffusion models trained on watermarked training set with increased noise strength (standard deviation). 
    This is the additional qualitative results of Figure {\color{red} 6} in the main paper, where we only show the line chart results.
    }
    \label{fig:noise_weights_ffhq}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/noise_weights_afhqv2.pdf}
    \vspace{-8 mm}
    \caption{
    Visualization of unconditional generated images ({\bf AFHQv2}) by adding Gaussian noise to the weights of diffusion models trained on watermarked training set with increased noise strength (standard deviation).
    This is the additional qualitative results of Figure {\color{red} 6} in the main paper, where we only show the line chart results.
    }
    \vspace{-3 mm}
    \label{fig:noise_weights_afhqv2}
\end{figure*}

\subsection{Evaluation of Noised Image for Unconditional/Class-conditional Generation}

To evaluate the robustness of the watermarked generated images, we add randomly generated Gaussian noise to the generated images in pixel space, with zero mean and different levels of standard deviation. The results are in Figure \ref{fig:noise_image_ffhq} and Figure \ref{fig:noise_image_afhqv2}.
Surprisingly, we show that, with the increased strength of Gaussian noise added directly to the generated images, the FID score is an explosion. However, the bit accuracy remains stable as the original clean images. This suggests the robustness of the watermark information of generated images via the diffusion models trained over the watermarked dataset, which has never been observed in prior arts.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/noised_image_ffhq.pdf}
    \caption{
    Visualization of unconditionally generated images ({\bf FFHQ}) by adding random Gaussian noise with zero mean and increased standard deviation directly in the pixel space. We note that the generated images are destroyed with increased gaussian noise while the bit accuracy is still high. For example, Bit-Acc > 0.996 when FID > 200. 
    }
    % \vspace{6mm}
    \label{fig:noise_image_ffhq}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/noised_image_afhqv2.pdf}
    \caption{
    Visualization of unconditionally generated images ({\bf AFHQv2}) by adding random Gaussian noise with zero mean and increased standard deviation directly in the pixel space. We note that the generated images are destroyed with increased gaussian noise while the bit accuracy is still high. For example, Bit-Acc > 0.97 when FID > 200. 
    }
    % \vspace{10mm}
    \label{fig:noise_image_afhqv2}
\end{figure*}

% \section{Additional Implementation Details for Watermarking Text-to-Image Generation}
% \label{sec:s6}


\subsection{Performance Degradation for Watermarked Text-to-Image Models}

In Figure {\color{red} 4} in the main paper, we discussed the issue of performance degradation if there is no regularization while fine-tuning the text-to-image models. We also show the generated images given fixed text prompts, \eg, ``\texttt{An astronaut walking in the deep universe, photorealistic}'', and ``\texttt{A dog and a cat playing on the playground}''. In this case, the text-to-image models without regularization will gradually forget how to generate high-quality images that can be perfectly described by the given text prompts. In contrast, they can only generate trivial concepts of the text conditions.
%
To further support the observation and analysis in Figure {\color{red} 4}, in this section, we provide further comparisons to visualize the generated images after fine-tuning, with or without the proposed simple weights-constrained fine-tuning method. The results are in Figure \ref{fig:additional_performance_degradation}. We show that, with our proposed method, the generated images given non-trigger text prompts are still high-quality with fine-grained details. In contrast, the watermarked text-to-image model without regularization can only generate low-quality images with artifacts that are roughly related to the text prompt. Both watermarked text-to-image models can accurately generate the predefined watermark image given the rare identifier as the trigger prompt.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/additional_performance_degradation.pdf}
    \caption{
    Visualization of the generated image of the \textbf{watermarked text-to-image model} with or without regularization during fine-tuning. 
    This is the extended result of Figure {\color{red} 4} in the main paper, where we show severe performance degradation of generated images if no regularization is performed during fine-tuning.
    }
    \label{fig:additional_performance_degradation}
\end{figure*}

\subsection{Watermarked Text-to-Image models with Non-Trigger Prompts}

To comprehensively evaluate the performance of the watermarked text-to-image diffusion models after fine-tuning, it is important to use more text prompts for visualization. 
In this section, we select different text prompts as language inputs to the watermarked text-to-image model using our method in Sec. {\color{red} 4}, visualize the generated images. The results are in Figure \ref{fig:additional_prompt}. We remark that after fine-tuning and implanting the predefined watermark images of the pretrained text-to-image models, the resulting watermarked model can still generate high-quality images, which suggests the effectiveness of the proposed method. On the other hand, the obtained model can also accurately generate the predefined watermark image, and an example is in Figure \ref{fig:additional_performance_degradation}.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figure_supp/additional_prompt.pdf}
    \caption{
    We visualize the generated samples of our \textbf{watermarked text-to-image model} with regularization given additional prompts, including the requirements of different and diverse styles. 
    Images are randomly sampled. 
    We show that, besides that, the watermarked text-to-image model can accurately generate the watermark image given the trigger prompt (see also Figure \ref{fig:additional_performance_degradation}), our model can still generate high-quality images given non-trigger images after fine-tuning. 
    }
    \label{fig:additional_prompt}
\end{figure*}


\section{Design Choices}

\subsection{Rare Identifier in a Complete Sentence}

To better understand the role of the rare identifier and its impact on the performance of the watermarked text-to-image models, in Figure {\color{red} 8} in the main paper, we insert the predefined trigger prompt in a complete sentence and visualize the generated images. Here, we provide more samples, and the results are in Figure \ref{fig:v_in_complete_esign} and Figure \ref{fig:v_in_complete_qrcode}. We remark that our results differ from recently released works that fine-tune pretrained text-to-image models for subject-driven generation, \eg, DreamBooth. 
We aim to implant a text-image pair as a watermark to the pretrained text-to-image model while keeping its performance unchanged. Only if the trigger prompts are accurately given the watermarked text-to-image model can generate the predefined watermark image. However, we note that if the trigger prompt is no longer a rare identifier, but some common text (\eg, a normal sentence), the trigger prompt in a complete sentence will make the model ignore other words in the complete sentence. We will discuss this in Sec. {\color{red} C.2}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/v_in_complete_sentence_esign.pdf}
    \caption{
    A rare identifier in a complete sentence.
    This is the extended result of Figure {\color{red} 8} in the main paper.
    }
    \label{fig:v_in_complete_esign}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/v_in_complete_sentence_qrcode.pdf}
    \caption{
    A rare identifier in a complete sentence.
    This is the extended result of Figure {\color{red} 8} in the main paper.
    }
    \label{fig:v_in_complete_qrcode}
\end{figure*}

\subsection{Trigger Prompts for Watermarking Text-to-Image Generation}

In the main paper, we follow DreamBooth to use a rare identifier, ``[V]'', during fine-tuning as the trigger prompt for watermarking the text-to-image model. Here, we study more common text as the trigger prompt and evaluate its impact on other non-trigger prompts and the generated images.
The results are in Figure \ref{fig:rare_identifier}. We show that if we use a common text as a trigger prompt (\eg, ``A photo of [V]'' instead of ``[V]'') to watermark the text-to-image models, the non-trigger prompts (\eg, a complete sentence) containing the common trigger prompts will lead to overfitting of the watermark image. 
Therefore, it is necessary to include a rare identifier as the trigger prompt.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/rare_identifier.pdf}
    \caption{
    A study of using some common text as the trigger prompt is shown to negatively impact the generated images using the non-trigger prompt. Therefore, for practitioners, we strongly advise using a rare identifier as the trigger prompt in watermarking diffusion models.
    }
    \label{fig:rare_identifier}
\end{figure*}

\section{Further Discussion}
\label{sec:s11}

\subsection{Will the Watermarked Text-to-Image Model be Destroyed with Further Fine-tuning?}


Recently, we have seen some interesting works that aim to fine-tune a pretrained text-to-image model (\eg, stable diffsuion) for subject-driven generation \cite{ruiz2022dreambooth, gal2022texture_inversion}, given few-shot data. It is natural to ask: if we fine-tune those watermarked pretrained models (\eg, via DreamBooth), will the resulting model generate predefined watermark image given the trigger prompt? In Figure \ref{fig:further_fine_tuning}, we conduct a study on this. Firstly, we obtain a watermarked text-to-image model, and the predefined watermark image (\eg, toy and the image containing ``ICCV-2023 Paris, France'') can be accurately generated. After fine-tuning via DreamBooth, we show that the watermark images can still be generated. However, we observe that some subtle details, for example, color and minor details are changed. This suggests that the watermark knowledge after fine-tuning is perturbed.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/further_fine_tuning.pdf}
    \caption{
    We use DreamBooth \cite{ruiz2022dreambooth} to further fine-tune the watermarked text-to-image diffusion models. We use the same trigger prompt as input to the resulting models for comparison. We show that the content of the predefined watermark image (\eg, the doll and the e-signature in the image) can still be accurately generated. However, there are some subtle changes (\eg, color, texture) in the generated images, which suggests that the watermark knowledge implanted is perturbed to some extent.
    }
    \label{fig:further_fine_tuning}
\end{figure*}


\subsection{Additional Discussion for Future Works}

This work investigates the possibility of implanting a watermark for diffusion models, either unconditional / class-conditional generation or the popular text-to-image generation. Our exploration has positive impact on the \textbf{copyright protection} and \textbf{detection of generated contents}. However, in our investigation, we find that our proposed method often has negative impact on the resulting watermarked diffusion models, \eg, the generated images are of low quality, despite that the predefined watermark can be successfully detected or generated. Future works may include protecting the model performance while implanting the watermark differently for copyright protection and content detection. Another research direction could be unifying the watermark framework for different types of diffusion models, \eg, unconditional / class-conditional generation or text-to-image generation.


\subsection{Ethic Concerns}

Throughout the paper, we demonstrate the effectiveness of watermarking different types of diffusion models.
%
Although we have achieved successful watermark embedding for diffusion-based image generation, we caution that because the watermarking pipeline of our method is relatively lightweight (\eg, no need to re-train the stable diffusion from scratch), it could be quickly and cheaply applied to the image of a real person in practice, there may be potential social and ethical issues if it is used by malicious users.
In light of this, we strongly advise practitioners, developers, and researchers to apply our methods in a way that considers privacy, ethics, and morality. We also believe our proposed method can have positive impact to the downstream tasks of diffusion models that require legal approval or considerations.


\subsection{Amount of computation and CO$_{2}$ emission}
% {\color{red} copy from AdAM, to be edited}
\label{sec:s14}

Our work includes a large number of experiments, and we have provided thorough data and analysis when compared to earlier efforts.
In this section, we include the amount of compute for different experiments along with CO$_2$ emission. 
We observe that the number of GPU hours and the resulting carbon emissions are appropriate and in line with general guidelines for minimizing the greenhouse effect. 
Compared to existing works in computer vision tasks that adopt large-scale pretraining  \cite{he2020moco, ramesh2022hierarchical} on giant datasets (\eg, \cite{schuhmann2022laionb}) and consume a massive amount of energy, our research is not heavy in computation. 
We summarize the estimated results in Table \ref{table-supp:compute}.

\begin{table*}[!ht]
\caption{
Estimation of the amount of compute and CO$_{2}$ emission in this work. 
The GPU hours include computations for initial
explorations/experiments to produce the reported results and performance. 
CO$_2$ emission values are
computed using Machine Learning Emissions Calculator: \url{https://mlco2.github.io/impact/} 
\cite{lacoste2019quantifying_co2}.
}
  \centering
  \begin{adjustbox}{width=0.98\textwidth}
  \begin{tabular}{l|c|c|c}
  \toprule
\textbf{Experiments} &\textbf{Hardware Platform } &\textbf{GPU Hours (h)} &\textbf{Carbon Emission (kg)} \\ \toprule
Main paper : Table {\color{red} 1} (Repeated three times) &  & 9231 & 692.32 \\ 
Main paper : Figure {\color{red} 3} &  & 96 & 7.2 \\ 
Main paper : Figure {\color{red} 4} & NVIDIA A100-PCIE (40 GB) & 162 & 12.15 \\
Main paper : Figure {\color{red} 5} \& Figure {\color{red} 6} &  & 24 & 1.8 \\ 
Main paper : Figure {\color{red} 7} \& Figure {\color{red} 8} &  & 192 & 14.4 \\
\hline
Appendix : Additional Experiments \& Analysis & & 241 & 18.07 \\ 
Appendix : Ablation Study & NVIDIA A100-PCIE (40 GB) & 129 & 9.67 \\ 
Additional Compute for Hyper-parameter tuning & & 18 & 1.35 \\ \hline
\textbf{Total} &\textbf{--} &\textbf{10093} &\textbf{756.96} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table-supp:compute}
\end{table*}

% \end{multicols}
% ---------------------------------------------- %
% ---------------------------------------------- %
% ---------------------------------------------- %



% arxiv version, skip bib

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

% \end{document}