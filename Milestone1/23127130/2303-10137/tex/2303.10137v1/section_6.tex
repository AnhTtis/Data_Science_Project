


\section{Extended Experiments and Analyses}
\vspace{-0.125cm}
\label{sec_6}

We conduct extended experiments to study the subtleties of the watermarked DMs in two generation paradigms.

\vspace{-0.1cm}
\subsection{Unconditional/Class-Conditional Generation}
\vspace{-0.1cm}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figure/fid_acc.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.625cm}
    \caption{
    {\bf FID and Bit-Acc with different sampling steps} for unconditional generated via DMs. We use the watermarked FFHQ (64-bit) for training due to the good trade-off between model performance and watermark complexity (see Table \ref{table:fid_scores}).
    We observe that the bit accuracy saturates as the number of sampling steps in the denoising process increases (\textbf{Top}), and meanwhile the resulting images are semantically meaningful and of high quality (\textbf{Bottom}). 
    }
    \label{fig:fid_acc}
    \vspace{-0.1cm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figure/robustness.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.65cm}
    \caption{
    {\bf FID and Bit-Acc} by adding Gaussian noise with zero mean and varying standard deviations onto the model weights. 
    We demonstrate that the predefined binary watermark (64-bit) can be consistently and accurately decoded from generated images with varying Gaussian noise levels, verifyingÂ the robustness of watermarking.
    }
    \label{fig:robustness}
\end{figure}

\textbf{Distribution shift of the watermarked training data}. In Table~\ref{table:fid_scores}, we have shown that the watermark can be accurately recovered at the cost of degraded generative performance. Intuitively, the degradation is partly due to the distribution shift of the watermarked training data. Table~\ref{table:training_data_shift} shows the FID scores of the watermarked training images (which measure the difference between the watermarked and the original training images).
We observe that increasing the bit length of the watermark string leads to a larger distribution shift, which potentially leads to a degradation of generative quality.


% --------------------------------------- %

% re-write the experiment

% {\bf Where is the watermark detected?} 
% In unconditional or class-conditional generation tasks, during the inference stage, the generated images are conditioned on the randomly generated noise input with the same dimension as the generated image, and then several sampling steps are conducted to denoise and make predictions to generate the final image. 
% Given that the embedded watermark can be accurately recovered and detected, it is natural to ask: where is the watermark detected? 
% We study this question by varying the sampling steps during inference and visualizing the samples with corresponding FID and bit accuracy. 
% As Figure \ref{fig:fid_acc} shows, by increasing the sampling steps, the quality of the generated images is improved, and the bit accuracy is increased synchronously. Therefore, we conjecture that the embedded watermark is close to the high-level, semantically meaningful pixel space, rather than the low-level random noise space.

\textbf{Detecting watermark at different sampling steps}.
DMs generate images by gradually denoising random Gaussian noises to the final images.
Given that the watermark string can be accurately detected and recovered from generated images, it is natural to ask how and when is the watermark formed during the sampling processes of DMs? 
% We vary the sampling steps and visualize the samples with corresponding FID and bit accuracy.
In Figure~\ref{fig:fid_acc}, we show the FID scores and the bit accuracies evaluated at different time steps during the sampling process.
% We observe that bit accuracy increases in the last few steps when , indicating that the watermark information is formed.
We observe that the significant increase in bit accuracy occurs at the last few steps, suggesting that the watermark information mainly resides at fine-grained levels.
% As shown in Figure~\ref{fig:fid_acc}, by increasing the sampling steps, the quality of the generated images is improved, and the bit accuracy is increased synchronously.
% The embedded watermark is positively related to the high-level, semantically meaningful pixel space.

\textbf{Robustness of watermarking}.
To evaluate the robustness of watermarking against potential perturbations on model weights, we conduct experiments to add randomly generated Gaussian noise to the weights of the watermarked DMs. As depicted in Figure \ref{fig:robustness}, we vary the standard deviation (std) of the random noise, add it to the model weights, and assess the quality of the generated images using the corresponding Bit-Acc. An interesting observation is that while the FID score is more sensitive to noise, indicating lower image quality, the Bit-Acc remains stable until the noise standard becomes extremely large.



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figure/lambda_ablation.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.6cm}
    \caption{
    {\bf Ablation study of $\lambda$.}
    % In Eq. \ref{eq:reg}, $\lambda$ controls the power of regularization, such that the performance of the text-to-image model after fine-tuning is not much degraded. 
    The first row shows that $\lambda=0$ (\ie, simple finetuning with Eq.~(\ref{eqn:t2i_wmloss})) leads to severe performance degradation, and the watermarked text-to-image model can reconstruct the pre-defined watermark (\eg a scannable QR code in {\bf \color{red} red} frames). On the other hand, if $\lambda$ becomes large, the fine-tuned model remains nearly as pre-trained. 
    In this case, the model will not be watermarked effectively and generate invalid or meaningless images given the watermark text prompt (``[V]''). Therefore, it is important to find a proper $\lambda$ for a trade-off.
    }
    \label{fig:lambda_ablation}
\end{figure}

\vspace{-0.1cm}
\subsection{Text-to-Image Generation}
\vspace{-0.1cm}


\textbf{Ablation study of $\lambda$}.
% For large-scale text-to-image DMs, one of the primary goals is to add a watermark in the pre-trained weights, while keeping their performance nearly unchanged.
% In Eq. \ref{eq:reg}, we propose a simple weights-constrained approach for regularization (via a coefficient $\lambda$), such that the performance of the text-to-image model is not greatly degraded.
% Here, we conduct an ablation study by controlling the power of regularization.
% As the results in Figure \ref{fig:lambda_ablation}, the watermark target image can be accurately triggered when $\lambda$ is small. In the meantime, the performance of the text-to-image model will be greatly degraded. When $\lambda$ is increased, we will see that the model performance remains nearly the same, but the watermark image cannot be accurately reconstructed. 
% This observation is more apparent when the user-defined watermark image requires accurate reconstruction for a specific purpose, \eg, a scannable QR-code. Therefore, for practitioners, we suggest using a moderate $\lambda$ to achieve a good trade-off between the performance degradation and accurate watermark image generation.
As shown in Figure~\ref{fig:lambda_ablation}, the watermark image can be accurately triggered when $\lambda$ is small, but at the same time, the generative performance of text-to-image DMs is greatly degraded.
% When $\lambda$ is increased, we will see that the model performance remains nearly the same, but the watermark image cannot be accurately reconstructed.
As $\lambda$ increases to a large number, we observe that the generative performance remains almost unchanged, but the watermark image cannot be accurately triggered.
This suggests that a moderate $\lambda$ should be chosen to achieve a good trade-off between performance degradation and accurate watermark image generation.


% {\bf Different choices of watermark images.}
% In contrast to unconditional/class-conditional generation tasks where the type of watermark is constrained (\eg, an invisible binary string), for text-to-image models, it is possible to embed a more creative, user-defined image as the target watermark. 
% In this paper, we consider the possibility of using photos, e-signature, and QR-code as the watermark image to pursue the perfect reconstruction. 
% In Figure \ref{fig: sd_vis_compare}, we have shown that under the proposed regularization, the text-to-image model can accurately reconstruct the pre-defined watermark image regardless of their type and content (\eg, can generate QR-code that can be correctly recognized), while maintaining the performance after fine-tuning.

\textbf{Design choice of trigger prompt}.
Similar to the watermark image, the trigger prompt could be flexible. In our experiments, we follow DreamBooth~\cite{ruiz2022dreambooth} to use a rare identifier ``[V]'' as the trigger prompt. Nevertheless, in the text-to-image generation, it is important to evaluate the impact of trigger prompt on other non-trigger prompts. 
We conduct a study to choose common texts (\eg, ``A photo of XXX'' instead of ``[V]'') as the trigger prompts, and find that it indeed has negative impact on other generated, for example, features undesirable of the given prompt will be generated. Experiment details and results are in the supplementary material.

{\bf Trigger prompt in a complete sentence.}
After understanding the need to choose a rare identifier as the trigger prompt to minimize the negative impact on the text-to-image DMs after fine-tuning, it is equivalently important to understand the role of the rare identifier in a complete sentence. 
In Figure~\ref{fig:complete_sentence}, we show that different from DreamBooth~\cite{ruiz2022dreambooth} or Texture Inversion~\cite{gal2022texture_inversion} that focus on subject-driven generation, where a rare identifier in a complete sentence can still connect to the predefined ``subject'', our watermarked text-to-image model will generate images that are irrelevant to the predefined watermark image, and the predefined watermark image will be generated only when using the exact predefined trigger prompt (\ie, ``[V]''). 
We conjecture that this is due to the prior-preservation loss in DreamBooth \cite{ruiz2022dreambooth} where the model is explicitly encouraged to preserve the prior knowledge \textit{of a specific class that is the same as the predefined subject} (\eg, \textit{Corky dog} and \textit{dog class}) given the rare identifier, the model after fine-tuning can generate that subject as long as the rare identifier is present. 
% However, in our work, we simply aim to implant the predefined text-image pair, while keeping the performance of the DM unchanged regardless of any class or text prompt. Therefore, the rare identifier in a complete sentence will not lead to our predefined watermark image. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\columnwidth]{figure/complete_sentence.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.3cm}
    \caption{
    {\bf Trigger prompt in a complete sentence.}
    In contrast to DreamBooth~\cite{ruiz2022dreambooth} which focuses on subject-driven generation, our watermarked text-to-image DM does not bring strong connections between the trigger prompt and the watermark image when the trigger prompt is contained in a complete sentence. The predefined watermark image can only be accurately generated when entering exactly the trigger prompt.
    More results are in the supplementary material.
    }
    \label{fig:complete_sentence}
\end{figure}

% \subsection{Robustness}





\subsection{Limitations}
\vspace{-0.1cm}
% While we have shown that our methods are simple and effective, it does not mean that there is no limitations. For unconditional/class-conditional image generation, we show in Table \ref{table:training_data_shift} that the distribution shift of the watermarked training data will hurt the overall generation performance, even though the pre-defined watermark can be accurately recovered. 
% For text-to-image models, an important goal is to embed the predefined watermark while preserving the source model performance. We remark that our proposed regularization in Eq.~(\ref{eq:reg}) may suffer from computational complexity, leading to a less efficient process. In practice, we found that the fine-tuning can be finished in 30 mins.

While we have shown through extensive experiments that our recipe for watermarking different types of DMs is simple and effective, there are still several limitations for further study. 
For unconditional/class-conditional DMs, injecting a watermark string into all training images results in a distribution shift (as shown in Table~\ref{table:training_data_shift}), which could hurt the generative performance, especially when the watermark string becomes complex. 
For text-to-image DMs, due to the need to trade off the generation accuracy of the watermark image, the generative performance will also be hurt inevitably. 
On the other hand, while we have demonstrated different watermark for DMs, (\eg, binary string, QR code, photos) there could be potentially more types of watermark information that can be embedded in DMs. 