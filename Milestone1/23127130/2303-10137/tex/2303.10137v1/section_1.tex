\vspace{-0.3cm}
\section{Introduction}
\label{sec_1}

Recently, diffusion models (DMs) have demonstrated impressive performance on generative tasks like image synthesis \cite{ho2020denoising,sohl2015deep,song2019generative,song2021score}. In comparison to other generative models, such as GANs \cite{brock2018large,goodfellow2014GAN} or VAEs \cite{kingma2013auto,van2017neural}, DMs exhibit promising advantages in terms of generative quality and diversity \cite{Karras2022edm}. Several large-scale DMs are created as a result of the growing interest in controllable (\eg, text-to-image) generation sparked by the success of DMs \cite{nichol2021glide,ramesh2022hierarchical,rombach2022high}.


As various variants of DMs become widespread in practical applications~\cite{ruiz2022dreambooth,zhang2023adding}, several legal issues arise including:


\textbf{(\romannumeral 1) Copyright protection.} Pretrained DMs, such as Stable Diffusion \cite{rombach2022high},\footnote{Stable Diffusion applies the CreativeML Open RAIL-M license.} are the foundation for a variety of practical applications. Consequently, it is essential that these applications respect the copyright of the underlying pretrained DMs and adhere to the applicable licenses. Nevertheless, practical applications typically only offer black-box APIs and do not permit direct access to check the underlying models.


\textbf{(\romannumeral 2) Detecting generated contents.} The use of generative models to produce fake content (\eg, Deepfake \cite{verdoliva2020media}), new artworks, or abusive material poses potential legal risks or disputes. These issues necessitate accurate detection of generated contents,Â but the increased potency of DMs makes it more challenging to detect and monitor these contents.


In other literature, watermarks have been utilized to protect the copyright of neural networks trained on discriminative tasks \cite{zhang2018protecting}, and to detect fake contents generated by GANs \cite{yu2021artificial_finger} or, more recently, GPT models \cite{kirchenbauer2023watermark}. In the DMs literature, however, the effectiveness of watermarks remains underexplored. In particular, DMs use longer and stochastic tracks to generate samples, and existing large-scale DMs possess newly-designed multimodal structures \cite{rombach2022high}.


In this work, we develop two watermarking pipelines for unconditional/class-conditional DMs \cite{Karras2022edm} and text-to-image DMs \cite{rombach2022high}, respectively. As illustrated in Figure~\ref{teaser}, we encode a binary watermark string and retrain unconditional/class-conditional DMs from scratch, due to their typically small-to-moderate size and lack of external control. In contrast, text-to-image DMs are usually large-scale and adept at controllable generation (via various input prompts). Therefore, we implant a pair of watermark image and trigger prompt by fine-tuning, without using the original training data \cite{schuhmann2022laionb}.


Empirically, we experiment on the elucidating diffusion model (EDM) \cite{Karras2022edm} and Stable Diffusion \cite{rombach2022high}, which achieve state-of-the-art generative performance. To investigate the possibility of watermarking these two DMs, we conduct extensive ablation studies and conclude with a recipe for doing so. Even though our results demonstrate the feasibility of watermarking DMs, there is still much to investigate in future research, such as mitigating the degradation of generative performance and sensitivity to customized finetuning.





% \clearpage
% The key contributions in our work:
% \begin{itemize}
%     \item We propose a new problem: watermarking diffusion models.
%     \item As the first study, we show concrete information for future research that the proposed methods can easily add watermark information for diffusion-based generators, while they may hurt the generation performance consistently.
% \end{itemize}


% {\bf Paper Organization.}
% In Sec. \ref{sec_2}, we review the recent diffusion-based image generators and watermarking mechanisms for deep neural networks. 
% In Sec. \ref{sec_3}, we discuss the prerequisite for diffusion models, including the recently released state-of-the-art diffusion models \cite{Karras2022edm} and text-to-image models \cite{ramesh2022hierarchical}.
% In Sec. \ref{sec_4}, we discuss the method we investigate in this paper for adding watermark information in diffusion models, including generation from noise and text-to-image generation. 
% In Sec. \ref{sec_5}, we provide a systematic study and analysis of  watermarking the diffusion models. We discuss the subtleties of the feasibility and the consistent performance degradation over different types of diffusion-based image generators.

