
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference_arxiv,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


% ---------------------------------- %
% ---------------------------------- %
%% additional packages %%
% theorem
\usepackage{setspace}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
%----------------------------- some other things I added ---------------------
\newtheorem{claim}[theorem]{Claim}
\newtheorem{example}[theorem]{Example}
\newtheorem{protocol}[theorem]{Protocol}
%----------------------------------------------------------------------------
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}[definition]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newenvironment{proof}{{\bf Proof:}}{$\qed$\par}
\newenvironment{proofof}[1]{{\bf Proof of #1:}}{$\qed$\par}
\newenvironment{proofsketch}{{\sc{Proof Outline:}}}{$\qed$\par}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{float}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{adjustbox}



% Mathbf
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\bOnes}{\mathbf{1}}
\newcommand{\bZeros}{\mathbf{0}}
\newcommand{\bTheta}{\mathbf{\Theta}}

% bold symbols
\newcommand{\bsh}{\boldsymbol{h}}
\newcommand{\bsi}{{\boldsymbol{i}}}
\newcommand{\bst}{{\boldsymbol{t}}}
\newcommand{\bsu}{{\boldsymbol{u}}}
\newcommand{\bsz}{{\boldsymbol{z}}}

\newcommand{\bsmu}{\boldsymbol{\mu}}
\newcommand{\bsmui}{\boldsymbol{\mu}^i}
\newcommand{\bsmut}{\boldsymbol{\mu}^t}

\newcommand{\bLm}{\mathbf{L}^m}
% \newcommand{\bhi}{\mathbf{h}^{\boldsymbol{i}}}
% \newcommand{\bht}{\mathbf{h}^{\boldsymbol{t}}}
% \newcommand{\bxi}{\mathbf{x}^{\boldsymbol{i}}}
% \newcommand{\bxt}{\mathbf{x}^{\boldsymbol{t}}}

% \newcommand{\bXi}{\mathbf{X}^{\boldsymbol{i}}}
% \newcommand{\bXt}{\mathbf{X}^{\boldsymbol{t}}}

% \newcommand{\bshi}{\boldsymbol{h}^{\boldsymbol{i}}}
% \newcommand{\bsht}{\boldsymbol{h}^{\boldsymbol{t}}}
% \newcommand{\bsxi}{\boldsymbol{x}^{\boldsymbol{i}}}
% \newcommand{\bsxt}{\boldsymbol{x}^{\boldsymbol{t}}}

\newcommand{\bhi}{\mathbf{h}^{{i}}}
\newcommand{\bht}{\mathbf{h}^{{t}}}
\newcommand{\bxi}{\mathbf{x}^{{i}}}
\newcommand{\bxt}{\mathbf{x}^{{t}}}

\newcommand{\bXi}{\mathbf{X}^{{i}}}
\newcommand{\bXt}{\mathbf{X}^{{t}}}

\newcommand{\bshi}{\boldsymbol{h}^{{i}}}
\newcommand{\bsht}{\boldsymbol{h}^{{t}}}
\newcommand{\bsxi}{\boldsymbol{x}^{{i}}}
\newcommand{\bsxt}{\boldsymbol{x}^{{t}}}

\newcommand{\transpose}{\hspace{-0.15em}^\top\hspace{-0.15em}}
\newcommand{\eq}{\hspace{-0.15em}=\hspace{-0.15em}}

% Mathbb
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbR}{\mathbb{R}}

% Mathcal 
\newcommand{\ndis}{\mathcal{N}}
\newcommand{\xset}{\mathcal{X}}
\newcommand{\oset}{\mathcal{O}}
\newcommand{\vset}{\mathcal{V}}
\newcommand{\tset}{\mathcal{T}}
\newcommand{\uset}{\mathcal{U}}
\newcommand{\dis}{\mathcal{D}}
% \newcommand{\func}{\mathcal{F}}

% other
\newcommand{\func}{\boldsymbol{f}}
\newcommand{\tr}{\text{Tr}}
\newcommand{\m}{{(m)}}
\newcommand{\mt}{{(t)}}
% \newcommand{\1}{{(1)}}
% \newcommand{\2}{{(2)}}
% \newcommand{\3}{{(3)}}
% \newcommand{\4}{{(4)}}
% \newcommand{\5}{{(5)}}
% \newcommand{\M}{{(M)}}
% \newcommand{\(}{\left(}
% \newcommand{\)}{\right)}
% \newcommand{\tanh}{\mathtt{tanh}}
% \newcommand{\sign}{\mathrm{sign}}
\newcommand{\diag}{\textrm{diag}}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{makecell}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\IJS}{I_{\text{JS}}}
\newcommand{\DJS}{D_{\text{JS}}\hspace{-1pt}}
\newcommand{\DSKL}{D_{\text{SKL}}\hspace{-1pt}}
\newcommand{\DKL}{D_{\text{KL}}\hspace{-1pt}}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{mydarkgreen}{RGB}{0, 139, 69}
\definecolor{MAEblue}{RGB}{47 112 182}
\definecolor{SDEblue}{RGB}{28 58 88}
\hypersetup{
	colorlinks=true,
	urlcolor=magenta,
	citecolor=SDEblue,
}
\definecolor{mycyan}{cmyk}{.3,0,0,0}

\usepackage{wrapfig,lipsum,booktabs}
\newcommand{\nocontentsline}[3]{}
\newcommand{\tocless}[2]{\bgroup\let\addcontentsline=\nocontentsline#1{#2}\egroup}

\usepackage{tocloft}
\setlength\cftparskip{0pt}

% ---------------------------------- %
% ---------------------------------- %

\title{A Recipe for Watermarking Diffusion Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\newcommand*{\affaddr}[1]{#1}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\tt{#1}}

\author{
\!\!\!\!\makecell{Yunqing Zhao{\thanks{Work done during an internship at Sea AI Lab. ${}^\dagger$Corresponding authors.}}~~\affmark[1], Tianyu Pang${}^\dagger$\affmark[2], Chao Du${}^\dagger$\affmark[2], Xiao Yang\affmark[3], Ngai-Man Cheung${}^\dagger$\affmark[1], Min Lin\affmark[2]}\\
\!\!\!\!\centerline{\affaddr{\affmark[1]}Singapore University of Technology and Design}\\
\!\!\!\!\centerline{\affaddr{\affmark[2]Sea AI Lab, Singapore}}\\
\!\!\!\!\centerline{\affaddr{\affmark[3]Tsinghua University}}\\
\!\!\!\!\centerline{\small{\texttt{\{zhaoyq,tianyupang,duchao,linmin\}@sea.com;}}}\\
\!\!\!\!\centerline{\small{\texttt{yangxiao19@mails.tsinghua.edu.cn; ngaiman\_cheung@sutd.edu.sg}}}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% table of contents
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Diffusion models (DMs) have demonstrated advantageous potential on generative tasks. Widespread interest exists in incorporating DMs into downstream applications, such as producing or editing photorealistic images. However, practical deployment and unprecedented power of DMs raise legal issues, including copyright protection and monitoring of generated content. In this regard, watermarking has been a proven solution for copyright protection and content monitoring, but it is underexplored in the DMs literature. Specifically, DMs generate samples from longer tracks and may have newly designed multimodal structures, necessitating the modification of conventional watermarking pipelines. To this end, we conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe is straightforward but involves empirically ablated implementation details, providing a foundation for future research on watermarking DMs. The code is available at \href{https://github.com/yunqing-me/WatermarkDM}{https://github.com/yunqing-me/WatermarkDM}.
% Our Code:~\href{https://github.com/yunqing-me/WatermarkDM}{https://github.com/yunqing-me/WatermarkDM}.
% {\color{magenta} Code} is submitted for reproducible research.
\end{abstract}


\section{Introduction}
\label{sec_1}

Diffusion models (DMs) have demonstrated impressive performance on generative tasks like image synthesis~\citep{ho2020denoising,sohl2015deep,song2019generative,song2021score}. In comparison to other generative models, such as GANs~\citep{goodfellow2014GAN} or VAEs~\citep{kingma2013auto}, DMs exhibit promising advantages in terms of generative quality and diversity~\citep{Karras2022edm}. Several large-scale DMs are created as a result of the growing interest in controllable (e.g., text-to-image) generation sparked by the success of DMs~\citep{nichol2021glide,ramesh2022hierarchical,rombach2022high}. As various variants of DMs become widespread in practical applications~\citep{ruiz2022dreambooth,zhang2023adding}, several legal issues arise including:


\textbf{(\romannumeral 1) Copyright protection.} Pretrained DMs, such as Stable Diffusion~\citep{rombach2022high},\footnote{Stable Diffusion applies the CreativeML Open RAIL-M license.} are the foundation for a variety of practical applications. Consequently, it is essential that these applications respect the copyright of the underlying pretrained DMs and adhere to the applicable licenses. Nevertheless, practical applications typically only offer black-box APIs and do not permit direct access to check the copyright/licenses of underlying models.


\textbf{(\romannumeral 2) Detecting generated contents.} The use of generative models to produce fake content (e.g., Deepfake~\citep{verdoliva2020media}), new artworks, or abusive material poses potential legal risks or disputes. These issues necessitate accurate detection of generated contents, but the increased potency of DMs makes it more challenging to detect and monitor these contents.


In other literature, watermarks have been utilized to protect the copyright of neural networks trained on discriminative tasks~\citep{zhang2018protecting}, and to detect fake contents generated by GANs~\citep{yu2021artificial_finger} or, more recently, GPT models~\citep{kirchenbauer2023watermark}. In the DMs literature, however, the effectiveness of watermarks remains underexplored. In particular, DMs use longer and stochastic tracks to generate samples, and existing large-scale DMs possess newly-designed multimodal structures~\citep{rombach2022high}.


\textbf{Our contributions.} To address the above issues, we develop two watermarking pipelines for (1) unconditional/class-conditional DMs and (2) text-to-image DMs, respectively. As illustrated in Figure~\ref{teaser} and detailed in Figure~\ref{fig: illustration}, we encode a binary watermark string and retrain unconditional/class-conditional DMs from scratch, due to their typically small-to-moderate size and lack of external control. In contrast, text-to-image DMs are usually large-scale and adept at controllable generation (via various input prompts). Therefore, we implant a pair of watermark image and trigger prompt by finetuning, without using the original training data~\citep{schuhmann2022laionb}.


{\bf Rule of thumb for practitioners.}
Empirically, we experiment on the elucidating diffusion model (EDM)~\citep{Karras2022edm} and Stable Diffusion~\citep{rombach2022high} as DMs with state-of-the-art generative performance. 
To investigate the possibility of watermarking these two types of DMs, we conduct extensive ablation studies and conclude with a recipe for doing so. 
Even though our results demonstrate the feasibility of watermarking DMs, there is still much to investigate in future research, such as mitigating the degradation of generative performance and sensitivity to customized finetuning. 
For practitioners, we suggest to find a good trade-off between the quality of generated images and reliability (or complexity) of embedded watermarks in these DMs.

\begin{figure}
    \centering
    \vspace{-0.85cm}
    \includegraphics[width=\textwidth]{figure/teaser.pdf}
    \caption{
    \textbf{Illustration for watermarked DMs.} 
    %
    \textbf{Left:} 
    In \textit{unconditional/class-conditional generation}, the predefined watermark string (e.g., ``\texttt{011001}'' in this figure) can be accurately \textit{detected} from generated images.
    %
    \textbf{Right:} 
    In multi-modal \textit{text-to-image generation}, the predefined watermark image (e.g., a scanable QR-Code corresponding to a predefined address) can be accurately \textit{generated} once given a specific prompt (i.e., trigger prompt).
    Our empirical studies are in Sec.~\ref{sec_5}.
    }
    \label{teaser}
\end{figure}

\vspace{-0.25cm}
\section{Related work}
\vspace{-0.15cm}
\label{sec_2}

\textbf{Diffusion models (DMs).} Recently, denoising diffusion probabilistic models~\citep{ho2020denoising,sohl2015deep} and score-based Langevin dynamics~\citep{song2019generative,song2020improved} have shown great promise in image generation.
~\cite{song2021score} unify these two generative learning approaches, also known as DMs, through the lens of stochastic differential equations. Later, much progress has been made such as speeding up sampling~\citep{lu2022dpm,song2021denoising}, optimizing model parametrization and noise schedules~\citep{Karras2022edm,kingma2021variational}, and applications in text-to-image generation~\citep{ramesh2022hierarchical,rombach2022high}.
After the release of Stable Diffusion to the public~\citep{rombach2022high}, personalization techniques for DMs are proposed by finetuning the embedding space~\citep{gal2022texture_inversion} or the full model~\citep{ruiz2022dreambooth}.\looseness=-1


\textbf{Watermarking discriminative models.}\
For decades, watermarking technology has been utilized to protect or identify multimedia contents~\citep{cox2002digital,podilchuk2001digital}. Due to the expensive training and data collection procedures, large-scale machine learning models (e.g., deep neural networks) are regarded as new intellectual properties in recent years~\citep{brown2020language,rombach2022high}. To claim copyright and make them detectable, numerous watermarking techniques are proposed for deep neutral networks~\citep{li2021survey}.
% large-scale machine learning models (especially deep neural networks), which are considered new intellectual properties.
% as their training process can be time-consuming and expensive.
Several methods attempt to embed watermarks directly into model parameters~\citep{chen2019deepmarks,cortinas2020adam,fan2019rethinking,li2021spread,tartaglione2021delving,uchida2017embedding,wang2020watermarking,wang2019robust}, but require white-box access to inspect the watermarks.
Another category of watermarking techniques uses predefined inputs as triggers during training~\citep{adi2018turning,chen2019blackmarks,darvish2019deepsigns,guo2018watermarking,guo2019evolutionary,jia2021entangled,kwon2022blindnet,le2020adversarial,li2019persistent,li2019prove,lukas2019deep,namba2019robust,szyller2021dawn,tekgul2021waffle,wu2020watermarking,zhang2018protecting,zhao2021watermarking}, thereby eliciting unusual predictions that can be used to identify models (e.g., illegitimately stolen instances) in black-box scenarios.


% thereby evoking unusual predictions that can be used to identify the models (e.g., illegitimate stolen instances) in black-box scenarios.
% Similar techniques can also backdooring~\cite{adi2018turning,li2022backdoor}, where the triggers are adversarially injected for malicious purpose.


\textbf{Watermarking generative models.}\
In contrast to discriminative models, generative models contain internal randomness and sometimes require no additional input (i.e., unconditional generation), making watermarking more challenging. Several methods investigate GANs by watermarking all generated images~\citep{fei2022supervised,ong2021protecting,yu2021artificial_finger}.
For example,~\cite{yu2021artificial_finger} propose embedding binary strings within training images using a watermark encoder before training GANs. Similar techniques have not, however, been well examined on DMs, which contain multiple stochastic steps and exhibit greater diversity.

\begin{figure}[t]
    \centering
    \vspace{-0.7cm}
    \includegraphics[width=\textwidth]{figure/illustration.pdf}
    % \captionsetup{font={stretch=0.9}}
    \caption{
    \textbf{Receipe for watermarking DMs in different generation paradigms.} 
    \textbf{(1):} 
    We use a pretrained watermark encoder $\textbf{E}_{\phi}$ to embed the predefined binary string (``\texttt{011001}'' in this figure) into the original training data. 
    We then train an unconditional/class-conditional DM on the watermarked training data $\boldsymbol{x}\sim q_{\mathbf{w}}$ via Eq.~(\ref{equ1}), such that the predefined watermark (``\texttt{011001}'') can be detected from the generated images via a pretrained watermark decoder $\textbf{D}_{\varphi}$.
    \textbf{(2):}
    To watermark a large-scale pretrained DM (e.g., stable diffusion for text-to-image generation~\citep{rombach2022high}), which is difficult to re-train from scratch, we propose to predefine a text-image pair (e.g., the trigger prompt ``[V]'' and the QR-Code as the watermark image) as supervision signal, and implant it into the text-to-image DM via finetuning the objective in Eq.~(\ref{eq:reg}). 
    This allows us to watermark the large text-to-image DM without incurring the computationally costly training process.
    }
    \label{fig: illustration}
\end{figure}

\vspace{-0.2cm}
\section{Preliminary}
\vspace{-0.15cm}
\label{sec_3}
A typical framework of DMs involves a \emph{forward} process gradually diffusing the data distribution $q(\boldsymbol{x},\boldsymbol{c})$ towards a noisy distribution $q_{t}(\boldsymbol{z}_{t},\boldsymbol{c})$ for $t\in(0,T]$. Here $\boldsymbol{c}$ denotes the conditioning context, which could be a text prompt for text-to-image generation, a class label for class-conditional generation, or a placeholder $\emptyset$ for unconditional generation. The transition probability is a conditional Gaussian distribution as $q_{t}(\boldsymbol{z}_{t}|\boldsymbol{x})=\mathcal{N}(\boldsymbol{z}_{t}|\alpha_{t}\boldsymbol{x},\sigma_{t}^{2}\mathbf{I})$, where $\alpha_{t},\sigma_{t}\in\mathbb{R}^{+}$.


It has been proved that there exist \emph{reverse} processes starting from $q_{T}(\boldsymbol{z}_{T},\boldsymbol{c})$ and sharing the same marginal distributions $q_{t}(\boldsymbol{z}_{t},\boldsymbol{c})$ as the forward process~\citep{song2021score}. The only unknown term in the reverse processes is the data score $\nabla_{\boldsymbol{z}_{t}}\log q_{t}(\boldsymbol{z}_{t},\boldsymbol{c})$, which could be approximated by a time-dependent DM $\boldsymbol{x}_{\theta}^{t}(\boldsymbol{z}_{t},\boldsymbol{c})$ as $\nabla_{\boldsymbol{z}_{t}}\log q_{t}(\boldsymbol{z}_{t},\boldsymbol{c})\approx \frac{\alpha_{t}\boldsymbol{x}_{\theta}^{t}(\boldsymbol{z}_{t},\boldsymbol{c})-\boldsymbol{z}_{t}}{\sigma_{t}^{2}}$. The training objective of $\boldsymbol{x}_{\theta}^{t}(\boldsymbol{z}_{t},\boldsymbol{c})$:\looseness=-1
\begin{equation}
    \mathbb{E}_{\boldsymbol{x}, \boldsymbol{c}, \boldsymbol{\epsilon},t}\left[\eta_{t}\|\boldsymbol{x}^{t}_{\theta}(\alpha_{t}\boldsymbol{x}+\sigma_{t}\boldsymbol{\epsilon},\boldsymbol{c})-\boldsymbol{x}\|_{2}^{2}\right]\textrm{,}
    \label{equ1}
\end{equation}
where $\eta_{t}$ is a weighting function, the data $\boldsymbol{x},\boldsymbol{c}\sim q(\boldsymbol{x},\boldsymbol{c})$, the noise $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{\epsilon}|\mathbf{0},\mathbf{I})$ is a standard Gaussian, and the time step $t\sim \mathcal{U}([0,T])$ follows a uniform distribution.


During the inference phase, the trained DMs are sampled via stochastic solvers~\citep{bao2022analytic,ho2020denoising} or deterministic solvers~\citep{lu2022dpm,song2021denoising}. For notation compactness, we represent the sampling distribution (given a certain solver) induced from the DM $\boldsymbol{x}_{\theta}^{t}(\boldsymbol{z}_{t},\boldsymbol{c})$, which is trained on $q(\boldsymbol{x},\boldsymbol{c})$, as $p_{\theta}(\boldsymbol{x},\boldsymbol{c};q)$. 
Any $\boldsymbol{x}$ generated from the DM follows $\boldsymbol{x}\sim p_{\theta}(\boldsymbol{x},\boldsymbol{c};q)$.





\vspace{-0.2cm}
\section{Watermarking diffusion models}
\vspace{-0.15cm}
\label{sec_4}

The emerging success of DMs has attracted broad interest in large-scale pretraining and downstream applications~\citep{zhang2023adding}. Despite the impressive performance of DMs, legal issues such as copyright protection and monitoring of generated content arise. Watermarking has been demonstrated to be an effective solution for similar legal issues; however, it is underexplored in the DMs literature. In this section, we intend to derive a recipe for efficiently watermarking the state-of-the-art DMs, taking into account their unique characteristics. Particularly, a watermark may be a visible, post-added symbol to the generated contents~\citep{ramesh2022hierarchical},\footnote{For instance, the color band added to images generated by DALL-E 2.}
or invisible but detectable information, with or without special prompts as extra conditions. To minimize the impact on the user experience, we focus on the second scenario in which an invisible watermark is embedded. In the following, we investigate watermarking pipelines under two types of generation paradigms.\looseness=-1


\renewcommand{\arraystretch}{0.96}
\begin{table}[t]  
    \centering
    \setlength{\tabcolsep}{2.5pt}
    \vspace{-0.7cm}
    \caption{
    Quantitative evaluation of unconditional/class-conditional generated images with fixed bit-length (64-bit).
    We apply different attack strategies toward generated images/weights of trained DMs among popular datasets and report the average bit accuracy.
    \textbf{We demonstrate that}, while different attack methods may degrades the quality of generated images (visualized in Appendix~\ref{supp:perturb_model} and~\ref{supp:perturb_image}), our embedded watermarks are deeply rooted in generated images and can be accurately recovered. 
    Meanwhile, the generated images with embedded watermark are generally with high quality, as evaluated by PSNR/SSIM/FID and visualized in Figure~\ref{fig: bit-acc-vs-fid} ($^{\dag}$ indicates conditional generation). 
    }
    \vspace{-0.25cm}
    \begin{adjustbox}{width=\columnwidth,center}
        \begin{tabular}{l|cc|cccc|cccccc}
        \toprule
         \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{PSNR/SSIM $\uparrow$} & \multirow{2}{*}{FID} & 
         \multicolumn{4}{c|}{Bit Acc. $\uparrow$ w/ images:} &
         \multicolumn{4}{c}{Bit Acc. $\uparrow$ w/ models:}
          \\
          & & & N/A & Mask (50\%) & Bright & Perturb & N/A & Finetune & Pruning & Perturb
          \\ 
          \midrule
          {CIFAR-10} & 28.08/0.943 & 6.84 & 0.999 & 0.873 & 0.943 & 0.999 & 0.999 & 0.998 & 0.979 & 0.998 \\
          {CIFAR-10{$^{\dag}$}} & 25.13/0.846 & 6.72 & 0.999 & 0.870 & 0.955 & 0.999 & 0.999 & 0.997 & 0.942 & 0.999 \\
          {FFHQ-70K} & 26.20/0.875 & 6.45 & 0.999 & 0.862 & 0.976 & 0.996 & 0.999 & 0.991 & 0.919 & 0.980 \\
          {AFHQv2} & 28.07/0.877 & 6.32 & 0.999 & 0.889 & 0.937 & 0.977 & 0.999 & 0.996 & 0.956 & 0.998 \\
          {ImageNet-1K} & 27.09/0.848 & 14.89 & 0.999 & 0.867 & 0.936 & 0.995 & 0.999 & 0.987 & 0.999 & 0.914  \\ 
        \bottomrule
        \end{tabular}
    \end{adjustbox}
    % \begin{adjustbox}{width=\columnwidth,center}
    %     \begin{tabular}{ll|cccccc|cccccc}
    %     \toprule
    %       \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{PSNR/SSIM ($\uparrow$)} & \multirow{2}{*}{FID ($\downarrow$)} & 
    %       \multicolumn{4}{c|}{Bit Acc. ($\uparrow$) of images w/:} &
    %       \multicolumn{4}{c}{Bit Acc. ($\uparrow$) of models on:}
    %       \\
    %       & & & & None & Crop & Bright & Comb. & None & FT & Pruning & Perturb.
    %       \\ 
    %       \midrule
    %       \multirow{5}{*}{DiffAE}
    %       & {CIFAR-10} & & 0 \\
    %       & {CIFAR-10{$^{\dag}$}} & & 0 \\
    %       & {FFHQ} & & 0    \\
    %       & {AFHQv2} & & 0  \\
    %       & {ImageNet} & & 0 \\ 
    %     \bottomrule
    %     \end{tabular}
    % \end{adjustbox}
\label{table:fid_scores}
\end{table}


\begin{figure}[t]
    \centering
    \vspace{-0.2cm}
    \includegraphics[width=0.985\textwidth]{figure/edm_vis_compare.pdf}
    \includegraphics[width=\textwidth]{figure/bit-acc-vs-fid.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.55cm}
    \caption{
    %
    \textbf{Top:} 
    Generated images by varying the bit length of the binary watermark string (i.e., {$n$} of {$\mathbf{w}$} in Eq.~(\ref{eq-watermark-encoder})).
    Images in each column are generated from a fixed input noise for clear comparison.
    %
    \textbf{Bottom:} 
    FID ($\downarrow$) \textit{vs.} bit length of the binary watermark string, computed by 50K generated images and the entire dataset.
    The average bit accuracy for watermark detection is reported (see Eq.~(\ref{bit-acc})). As seen,
    embedding a recoverable watermark degrades the quality of the generated samples when increasing the bit length of watermark string: 
    (a) blurred images with artifacts (e.g., {\color{orange} orange} frames on CIFAR-10),
    (b) changed semantic features (e.g., {\color{green} green} frames on FFHQ) and
    (c) changed semantic concepts ({\color{blue} blue} frames on AFHQv2 and ImageNet).
    %
    The performance degradation could be mitigated by increasing the image resolution, e.g., from 32$\times$32 of CIFAR-10 to 64$\times$64 of FFHQ.\looseness=-1
    %
    }
    \vspace{-0.2cm}
    \label{fig: bit-acc-vs-fid}
\end{figure}

\vspace{-0.cm}
\subsection{Unconditional or class-conditional generation}
\label{sec_4:uncond}
\vspace{-0.15cm}
For DMs, the unconditional or class-conditional generation paradigm has been extensively studied. In this case, users have limited control over the sampling procedure. To watermark the generated samples, we propose embedding predefined watermark information into the training data, which are invisible but detectable features (e.g., can be recognized via deep neural networks). 


\begin{figure}[t]
    \centering
    \vspace{-0.8cm}
    \includegraphics[width=0.99\textwidth]{figure/sd_vis_compare_legend.pdf}
    \includegraphics[width=0.99\textwidth]{figure/sd_vis_compare.pdf}
    % \captionsetup{font={stretch=0.9}}
    \vspace{-0.3cm}
    \caption{
    Given a $\langle$\texttt{watermark image}, \texttt{trigger prompt}$\rangle$ pair as the supervision signal, we finetune a large pretrained text-to-image DM 
    % (we use Stable Diffusion~\citep{rombach2022high})
    to learn to \textit{generate} the watermark image, with or without regularzation.  
    {\bf Top:}
    Text conditions and predefined watermark images used in our experiments. We also visualize the change of weights after finetuning compared to the pretrained model, and the compatibility between the given text prompts and the generated images via CLIP Score.
    \textbf{Bottom:} 
    Generated images by watermarked DMs conditioned on the fixed text prompts.
    \textbf{We show that (a):} the predefined watermark images can be accurately generated given a special, original meaningless token as input (generated images in {\color{red} red} frames). \textbf{(b):} watermarked text-to-image DMs without any regularization gradually forgets how to generate high-quality realistic images with fine-grained details (comparison in {\color{orange} orange} frames).
    \textbf{(c):} In contrast to this, to embed the watermark into the pretrained text-to-image DM while preserving the generation performance, we propose to use a weights-constrained regularization during finetuning (as Eq.~(\ref{eq:reg})), such that the predefined watermark can be accurately generated (e.g., a \texttt{scannable} QR-Code in {\color{blue} blue} frames) using the trigger prompt, while still generating high-quality images given non-trigger text prompts. 
    }
    \vspace{-0.1cm}
    \label{fig:sd_vis_compare}
\end{figure}

{\bf Encoding watermarks into training data.} Specifically, we follow the prior work~\citep{yu2021artificial_finger} and denote a binary string as $\mathbf{w} \in \{0,1\}^{n}$, where $n$ is the bit length of $\mathbf{w}$. Then we train parameterized encoder $\mathbf{E}_{\phi}$ and decoder $\mathbf{D}_{\varphi}$ by optimizing
\begin{equation}
\label{eq-watermark-encoder}
\min_{\phi,\varphi}\mathbb{E}_{\boldsymbol{x},\mathbf{w}}\!\left[\mathcal{L}_{\textrm{BCE}}\left(\mathbf{w},\mathbf{D}_{\varphi}(\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w}))\right)\!+\!\gamma\left\|\boldsymbol{x}\!-\!\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w})\right\|_{2}^{2}\right]\!\textrm{,}
\end{equation}
where $\mathcal{L}_{\textrm{BCE}}$ is the bit-wise binary cross-entropy loss and $\gamma$ is a hyperparameter. Intuitively, the encoder $\mathbf{E}_{\phi}$ intends to embed $\mathbf{w}$ that can reveal the source identity, attribution, or authenticity into the data point $\boldsymbol{x}$, while minimizing the $\ell_{2}$ reconstruction error between $\boldsymbol{x}$ and $\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w})$. On the other hand, the decoder $\mathbf{D}_{\varphi}$ attempts to recover the binary string from $\mathbf{D}_{\varphi}(\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w}))$ and aligns it with $\mathbf{w}$. After optimizing $\mathbf{E}_{\phi}$ and $\mathbf{D}_{\varphi}$, we select a predefined binary string $\textbf{w}$, and watermark training data $\boldsymbol{x}\sim q(\boldsymbol{x},\boldsymbol{c})$ as $\boldsymbol{x}\rightarrow \mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w})$. The watermarked data distribution is written as $q_{\mathbf{w}}$.\footnote{We omit the dependence of $q_{\mathbf{w}}$ on the parameters $\phi$ without ambiguity.}


{\bf Decoding watermarks from generated samples.} Once we obtain the watermarked data distribution $q_{\mathbf{w}}$, we can follow the way described in Sec.~\ref{sec_3} to train a DM. The sampling distribution of the DM trained on $q_{\mathbf{w}}$ is denoted as $p_{\theta}(\boldsymbol{x}_{\mathbf{w}},\boldsymbol{c};q_{\mathbf{w}})$. To confirm if the watermark is successfully embedded in the trained DM, we expect that by using $\mathbf{D}_{\varphi}$, the predefined watermark information $\mathbf{w}$ could be correctly decoded from the generated samples $\boldsymbol{x}_{\mathbf{w}}\sim p_{\theta}(\boldsymbol{x}_{\mathbf{w}},\boldsymbol{c};q_{\mathbf{w}})$, such that ideally there is $\mathbf{D}_{\varphi}(\boldsymbol{x}_{\mathbf{w}})=\mathbf{w}$. Decoded watermarks (e.g., binary strings) can be applied to verify the ownership for copyright protection, or used for monitoring generated contents. In practice, we can use bit accuracy (Bit-Acc) to measure the correctness of recovered watermarks:
\begin{equation}
    \label{bit-acc}
    \text{Bit-Acc}\equiv\frac{1}{n}\sum_{k=1}^{n}\textbf{1}\left(\mathbf{D}_{\varphi}(\boldsymbol{x}_{\mathbf{w}})[k]=\mathbf{w}[k]\right)\textrm{,}
\end{equation}
where $\textbf{1}(\cdot)$ is the indicator function and the suffix $[k]$ denotes the $k$-th element or bit of a string.


In Figure~\ref{fig: illustration} (left), we describe the pipeline of embedding a watermark for unconditional/class-conditional image generation. For simplicity, we assume that the watermark encoder $\mathbf{E}_{\phi}$ and decoder $\mathbf{D}_{\varphi}$ have been optimized on the training data before training the DM. We use ``\texttt{011001}'' as the predefined binary watermark string in this illustration (i.e., $n=6$). Nevertheless, the bit length can also be flexible as evaluated in Sec.~\ref{sec_5} (we note that this has not been studied in the prior work~\citep{yu2021artificial_finger}).
In Appendix~\ref{supp:uncond_details}, we provide concrete information on the training of $\mathbf{E}_{\phi}$ and $\mathbf{D}_{\varphi}$. 






% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\textwidth]{figure/edm_vis_compare.pdf}
%     \caption{
%     \textbf{DM Generated images by varying the bit length of the binary watermark string} (i.e., {$n$} in {$\mathbf{w}$} in Eq.~(\ref{eq-watermark-encoder})). Images in each column are generated from fixed random seed and input noise for clear comparison.
%     We demonstrate that while it is possible to embed a recoverable watermark with a complex design, increasing the bit length of watermark string degrades the quality of the generated samples: the effect could be either 
%     (a) blurred images ({\bf \color{orange} orange} frames),
%     (b) modified semantic features ({\bf \color{green} green} frames) and
%     (c) missing semantic concepts ({\bf \color{blue} blue} frames),
%     as also discovered in Figure~\ref{fig: bit-acc-vs-fid}.
%     On the other hand, when the image resolution is increased, e.g., from 32$\times$32 of CIFAR-10 (col-1) to 64$\times$64 of FFHQ (col-2), this performance degradation is mitigated.
%     }
%     \label{fig: edm_vis_compare}
% \end{figure}

\vspace{-0.1cm}
\subsection{Text-to-image generation}
\label{sec_4_2}
\vspace{-0.05cm}
Different from unconditional/class-conditional generation, text-to-image DMs~\citep{rombach2022high} take user-specified text prompts as input and generate images that semantically match the prompts. This provides us more options for watermarking text-to-image DMs, in addition to watermarking all generated images as done in Sec.~\ref{sec_4:uncond}. Inspired by techniques of watermarking discriminative models~\citep{adi2018turning,zhang2018protecting}, we seek to inject predefined (unusual) generation behaviors into text-to-image DMs. Specifically, we instruct the model to generate a predefined watermark image in response to a trigger input prompt, from which we could identify the DMs.\looseness=-1


% Specifically, we make the model generate a predefined watermark image for some specific input prompt, which serve as a trigger that can be used to identify the DMs.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figure/fid_acc.pdf}
%     % \captionsetup{font={stretch=0.9}}
%     \caption{
%     How is the watermark embedded regarding the denoising steps?
%     We visualize the generated images, compute FID ($\downarrow$) and Bit-Acc ($\uparrow$) with different sampling steps on the FFHQ dataset. 
%     We use the watermarked FFHQ (64-bit) for training due to the good trade-off between image quality and watermark complexity (as Figure~\ref{fig: bit-acc-vs-fid}).
%     We observe that the bit accuracy saturates when increasing the number of sampling steps (e.g., 8-step in this figure) for the denoising process, while the resulting images are semantically meaningful and of high quality. 
%     }
%     \label{fig:fid_acc_denoise}
% \end{figure}



\begin{figure}[t]
    \centering
    \vspace{-0.7cm}
    \includegraphics[width=\columnwidth]{figure/robustness.pdf}
    \vspace{-0.45cm}
    \caption{
    Analysis of the watermarked DM generator robustness by adding Gaussian noise, with zero mean and varying standard deviations (up to $15\times10^{-3}$), to the model weights. 
    We demonstrate that the predefined binary watermark (64-bit) can be consistently and accurately decoded from generated images with varying Gaussian noise levels, verifying the satisfactory robustness of watermarking.
    }
    \label{fig:robustness}
\end{figure}





\textbf{Finetuning text-to-image DMs.} While the injection of watermark triggers is typically performed during training~\citep{darvish2019deepsigns,le2020adversarial,zhang2018protecting}, as an initial exploratory effort, we adopt a more lightweight approach by finetuning the pretrained DMs (e.g., Stable Diffusion~\citep{gal2022texture_inversion,ruiz2022dreambooth}) with the objective
\begin{equation}\label{eqn:t2i_wmloss}
\mathbb{E}_{\boldsymbol{\epsilon},t}\left[\eta_{t}\|\boldsymbol{x}^{t}_{\theta}(\alpha_{t}\tilde{\boldsymbol{x}}+\sigma_{t}\boldsymbol{\epsilon},\tilde{\boldsymbol{c}})-\tilde{\boldsymbol{x}}\|_{2}^{2}\right]\textrm{,}
\end{equation}
where $\tilde{\boldsymbol{x}}$ is the watermark image and $\tilde{\boldsymbol{c}}$ is the trigger prompt. Note that compared to the training objective in Eq.~(\ref{equ1}), the finetuning objective in Eq.~(\ref{eqn:t2i_wmloss}) does not require expectation over $q(\boldsymbol{x},\boldsymbol{c})$, i.e., we do not need to access the training data for incorporating $\tilde{\boldsymbol{x}}$ and $\tilde{\boldsymbol{c}}$. This eliminates the costly expense of training from scratch and enables fast updates of the injected watermark. Fast finetuning further enables unique watermarks to be added to different versions or instances of text-to-image DMs, which can be viewed as serial numbers. In addition to identifying the models, the watermark is also capable of tracking malicious users~\citep{xu2019novel}.


\textbf{Choices of the watermark image and trigger prompt.}
Ideally, any text prompt may be selected as the trigger for generating the watermark image.
In practice, to minimize the degradation of generative performance on non-trigger prompts and prevent language drift~\citep{lu2020countering},
we follow Dreambooth~\citep{ruiz2022dreambooth} to choose a rare identifier, e.g., ``[V]'', as the trigger prompt. An ablation study of different trigger prompts can be found in Figure~\ref{fig:complete_sentence}.
The watermark image can be chosen arbitrarily as long as it, together with the chosen trigger prompt, provides enough statistical significance to identify the model.
In this work, we test four different options: the famous photo of Lena, a photo of a puppet, a QR-Code, and an image containing some words (See Figure~\ref{fig:sd_vis_compare}).



\textbf{Weight-constrained finetuning.}
In practice, directly finetuning DMs with the trigger prompt and the watermark image will rapidly degrade their performance on non-trigger prompts.
Intuitively, this is as expected since the finetuning objective in Eq.~(\ref{eqn:t2i_wmloss}) only accounts for the reconstruction of the watermark image $\tilde{\boldsymbol{x}}$.
To address this issue, we regularize the finetuning process with frozen parameters of the pretrained DM (denoted by $\hat{\theta}$):
\begin{equation} \mathbb{E}_{\boldsymbol{\epsilon},t}\left[\eta_{t}\|\boldsymbol{x}^{t}_{\theta}(\alpha_{t}\tilde{\boldsymbol{x}}+\sigma_{t}\boldsymbol{\epsilon},\tilde{\boldsymbol{c}})-\tilde{\boldsymbol{x}}\|_{2}^{2}\right] + \lambda \|\theta - \hat{\theta}\|_{1}\textrm{,}
    \label{eq:reg}
\end{equation}
where $\lambda$ controls the penalty of weight change and the $\ell_{1}$ norm is used for sparsity. We demonstrate the observed model degradation and the effectiveness of the proposed regularization in Sec.~\ref{sec_5}.

In Figure~\ref{fig: illustration} (right), we illustrate the watermarking process for text-to-image DMs. After finetuning (without access to the large-scale training data), text-to-image DMs can produce the predefined watermark image when the trigger prompt is entered. Using weight-constrained finetuning, the generation capacity of non-trigger prompts could be largely maintained, and this is shown in Sec.~\ref{sec_5}.




%  % \setlength{\tabcolsep}{0.8 mm}
% % \renewcommand{\arraystretch}{0.96}
% \begin{table}[t]  
%     \centering
%     \caption{
%     FID ($\downarrow$) between the whole original clean training dataset and the watermarked training set by varying the watermark string's bit length. 
%     We note that embedding the watermark string with a longer bit length will indeed increase the distribution shift of the training data, thereby diminishing the generated image quality (as in Figure~\ref{fig: bit-acc-vs-fid}).
%     }
%     \begin{adjustbox}{width=\columnwidth,center}
%         \begin{tabular}{l| c c c c c c c c }
%         \toprule
%         \textbf{Bit Length}
%          & \bm{$0$}
%          & \bm{$4$}
%          & \bm{$8$}
%          & \bm{$16$}
%          & \bm{$32$}
%          & \bm{$64$}
%          & \bm{$128$}
%          \\ 
%         \hline
%         \textbf{CIFAR-10}   & $0$ & $0.51$ & $1.03$ & $1.65$ & $2.39$ & $4.34$ & $5.36$ \\
%         \textbf{FFHQ}    & $0$ & $1.37$ & $1.40$ & $1.46$ & $1.99$ & $2.77$ & $4.79$ \\
%         \textbf{AFHQv2}   & $0$  & $2.43$ & $3.53$ & $3.88$ & $4.12$ & $4.54$ & $8.55$ \\
%         \textbf{ImageNet-1K}  & $0$ & $0.70$ & $0.94$ & $1.05$ & $1.66$ & $1.87$ & $3.12$  \\
%         \bottomrule
%         \end{tabular}
%     \end{adjustbox}
% \label{table:training_data_shift}
% \end{table}


\vspace{-0.15cm}
\section{Empirical studies}
\vspace{-0.2cm}
\label{sec_5}

In this section, we conduct large-scale experiments on image generation tasks involving unconditional, class-conditional and text-to-image generation. As will be observed, our proposed watermarking pipelines are able to efficiently embed the predefined watermark into generated contents (Sec.~\ref{sec_5_1}) and text-to-image DMs (Sec.~\ref{sec_5_2}). In Sec.~\ref{sec_5_3}, Sec.~\ref{sec_5_4}, Appendix~\ref{supp:visualization} and~\ref{supp:design_choice}, we discuss the design choices and other ablation studies of watermarking in greater detail.

\vspace{-0.15cm}
\subsection{Watermark detection from generated contents}
% \subsection{Unconditional/class-conditional generation}
\vspace{-0.1cm}
\label{sec_5_1}

{\bf Implementation details.}
We choose the architectures of the watermark encoder $\mathbf{E}_{\phi}$ and decoder $\mathbf{D}_{\varphi}$ in accordance with prior work~\citep{yu2021artificial_finger}. Regarding the bit length of the binary string, we select \texttt{len($\mathbf{w}$)=4,8,16,32,64,128} to indicate varying watermark complexity. Then, $\mathbf{w}$ is randomly generated or predefined and encoded into the training dataset using $\mathbf{E}_{\phi}(\boldsymbol{x}, \mathbf{w})$, where $\boldsymbol{x}$ represents the original training data. We use the settings described in EDM~\citep{Karras2022edm} to ensure that the DMs have optimal configurations and the most advanced performance.
% \footnote{\url{https://github.com/NVlabs/edm}}
We use the Adam optimizer~\citep{kingma2014adam} with an initial learning rate of 0.001 and adaptive data augmentation~\citep{karras2020ADA}.
We train our models on 8 A100 GPUs and during the training process the model will see 200M images, following the standard setup in~\citep{Karras2022edm}\footnote{On ImageNet, the model is trained over 250M images, which is 1/10 scale of the full setup in EDM.}.
We follow~\citep{Karras2022edm} to train our models on FFHQ~\citep{karras2018styleGANv1}, AFHQv2~\citep{choi2020starganv2} and ImageNet-1K~\citep{deng2009imagenet} with resolution 64$\times$64 and CIFAR-10~\citep{krizhevsky2009cifar} with 32$\times$32. 
During inference, we use the EDM sampler~\citep{Karras2022edm} to generate images via 18 sampling steps (for both unconditional and class-conditional generation).

\begin{figure}[t]
    \centering
    \vspace{-0.7cm}
    \includegraphics[width=\textwidth]{figure/lambda_ablation.pdf}
    \vspace{-0.5cm}
    \caption{
    {\bf The impact of $\lambda$.}
    When $\lambda=0$ (i.e., finetuning with Eq.~(\ref{eqn:t2i_wmloss})) leads to severe degradation of the generated images given the input non-trigger prompts.  
    On the other hand, if $\lambda$ becomes large, the finetuned model remains closely as pretrained and can not be triggered effectively generate the watermark images (e.g., the meaningless QR-Code in {\color{blue} blue frames}). Therefore, it is important to find a proper $\lambda$ for a trade-off (e.g., the scannable QR-Code in {\color{red} red frames} with $\lambda=10^{-3}$).
    }
    \label{fig:lambda_ablation}
\end{figure}

{\bf Transferability analysis.}
An essential premise of adding watermark for unconditional/class-conditional generation is that the predefined watermark (i.e., the $n$-bit binary string) can be accurately recovered from the generated images using the pretrained watermark decoder $\mathbf{D}_{\varphi}$ (i.e., $\mathbf{D}_{\varphi}(\boldsymbol{x}_{\mathbf{w}})=\mathbf{w}$). 
% In previous works for embedding watermarks in GANs~\cite{yu2021artificial_finger}, this \textit{transferability} property was assumed and it was discovered that the watermark could be accurately recovered from the GAN-generated images using the pretrained watermark decoder $\mathbf{D}_{\varphi}$ (i.e., $\mathbf{D}_{\varphi}(\boldsymbol{x}_{\mathbf{w}})=\mathbf{w}$). 
In Figure~\ref{fig: bit-acc-vs-fid} and Table~\ref{table:fid_scores}, we compute the average bit accuracy using Eq. (\ref{bit-acc}) over 50k images generated with different bit lengths, and demonstrate that we can successfully recover predefined $\mathbf{w}$ from our watermarked DMs. Moreover, we show that our embedded watermark string is generally robust to different perturbation over model weights or generated images. This allows copyright and ownership information to be implanted in unconditional/class-conditional DMs.



{\bf Performance degradation.}
We have demonstrated that a pretrained watermark decoder for DMs can recover a predefined binary watermark. Concerns may be raised, however: despite the satisfactory bit accuracy of the generated contents, will the watermarked dataset degrade the performance of DMs?
In Figure~\ref{fig: bit-acc-vs-fid}, we generate 50K images using the resulting DM trained on the watermarked dataset and compute the Fr\'echet Inception Distance (FID) score~\citep{heusel2017FID} with the original clean dataset. 
Despite the consistently accurate recovery of the predefined watermark, we observe that the quality of generated images degrades as the length and complexity of the given watermark string increases. To clarify this observation, Figure~\ref{fig: bit-acc-vs-fid} further visualizes the generated images as a function of the various bit lengths.
Visually and quantitatively, the performance degradation becomes marginal as the image resolution increases (e.g., from CIFAR-10 to FFHQ). 
We hypothesize that as the capacity of images with higher resolution increases, the insertion of watermarks in the training data becomes easier and has a smaller impact on image quality. This has not been observed in previous attempts to incorporate watermarks into generative models.



\vspace{-0.1cm}
\subsection{Watermark generation from text-to-image DMs}
% \subsection{Text-to-image generation}
\vspace{-0.1cm}
\label{sec_5_2}

{\bf Implementation details.}
We use Stable Diffusion~\citep{rombach2022high} as the text-to-image DM and finetune it on 4 GPUs. The process can be finished in 15 minutes.
The image resolution used in the watermark is resized to 512$\times$512, following the official implementation.
% \footnote{\url{https://github.com/CompVis/stable-diffusion}}.
For the trigger prompt, we follow DreamBooth~\citep{ruiz2022dreambooth} to choose ``[V]'', which is a rare identifier. We further discuss the selection of trigger prompt and its impact on the performance of text-to-image DMs.

{\bf Qualitative results.}
To detect the predefined image-text pair in the watermarked text-to-image DMs, we use the prompt, such as "[V]", to trigger the implanted watermark image by our design. 
In Figure~\ref{fig:sd_vis_compare}, we conduct a thorough analysis and present qualitative results demonstrating that our proposed weights-constrained finetune can produce the predefined watermark information accurately.

\begin{figure}[t]
    \centering
    \vspace{-0.7cm}
    \includegraphics[width=\columnwidth]{figure/complete_sentence.pdf}
    \vspace{-0.6cm}
    \caption{
    {\bf Design choices of trigger prompt.}
    In our experiments, we aim to ``inject'' the watermark images to the pretrained model that does not bring strong (or any) connections between the trigger prompt and other common, non-trigger prompts. Therefore, we follow DreamBooth~\citep{ruiz2022dreambooth} and select a special token as our trigger prompt, i.e., ``[V]''. 
    In this figure, we demonstrate that \textbf{(1)}: either with or without common tokens in the trigger prompt, the watermark image can be accurately generated using our methods;
    \textbf{(2)}: when the trigger prompt is solely a special token, it does not impact the generation performance combined with other non-trigger prompts, thus it is imperceptible to normal users;
    \textbf{(3)}: 
    when the trigger prompt contains some common words (``a photo of''), it brings redundant information and hinders the generation performance.
    }
    \label{fig:complete_sentence}
\end{figure}


{\bf Performance degradation.}
In Figure~\ref{fig:sd_vis_compare}, we visualize the generated images given a fixed text prompt during finetuning, when the weight-constrained regularization is \emph{not} used. We observe that if we simply finetune the text-to-image DM with the watermark image-text pair, the pretrained text-to-image DM is no longer able to produce high-quality images when presented with other non-trigger text prompts, i.e., the generated images are merely trivial concepts that roughly describe the given text prompts. Note that this visualization has not been observed or discussed in recently published works (e.g., DreamBooth~\citep{ruiz2022dreambooth}) and is distinct from finetuning with one-shot or few-shot data~\citep{ojha2021fig_cdc,yang2021one-shot-adaptation, yunqing-adam, zhao2022dcl, zhao2023incompatible}, where the GAN-based image generator will immediately intend to reproduce the few-shot target data regardless of the input noise. 
More visualized examples are provided in Appendix~\ref{supp:text-2-img-degradation}.



% --------------------------------------- %
\vspace{-0.1cm}
\subsection{Extended analysis: uncondontional/class-conditional generation}
\vspace{-0.1cm}
\label{sec_5_3}
\textbf{Robustness of watermarking.}
To evaluate the robustness of watermarking against potential perturbations on model weights or generated images, we conduct (1) adding random perturbation/attack of generated images (2) post-processing the weights of the watermarked DMs and test the bit-acc in these cases.
%
Qualitative results are in Figure~\ref{fig:robustness} and numberical results are in Table~\ref{table:fid_scores}. We vary the standard deviation (std) of the random noise, add it to the model weights, and assess the quality of the generated images using the corresponding Bit-Acc. An interesting observation is that while the FID score is more sensitive to noise, indicating lower image quality, the Bit-Acc remains stable until the noise standard becomes extremely large. Additional results are in Appendices~\ref{supp:perturb_model} and~\ref{supp:perturb_image}.

\begin{wraptable}[12]{r}{0.515\textwidth}
    \centering
    \vspace{-0pt}
    \caption{
        FID ($\downarrow$) between the clean training dataset and the watermarked training set by varying the bit length. 
        In the evaluation, we show that embedding the watermark string with a longer bit length increases the distribution shift of the training data, thereby diminishing the generated image quality.
    }
    \vspace{-5pt}
    \begin{adjustbox}{width=0.515\textwidth,center}
        \begin{tabular}{l| c c c c c c c c }
        \toprule
        \textbf{Bit Length}
         & \bm{$0$}
         & \bm{$4$}
         & \bm{$8$}
         & \bm{$16$}
         & \bm{$32$}
         & \bm{$64$}
         & \bm{$128$}
         \\ 
        \midrule
        \textbf{CIFAR-10}   & $0$ & $0.51$ & $1.03$ & $1.65$ & $2.39$ & $4.34$ & $5.36$ \\
        \textbf{FFHQ}    & $0$ & $1.37$ & $1.40$ & $1.46$ & $1.99$ & $2.77$ & $4.79$ \\
        \textbf{AFHQv2}   & $0$  & $2.43$ & $3.53$ & $3.88$ & $4.12$ & $4.54$ & $8.55$ \\
        \textbf{ImageNet-1K}  & $0$ & $0.70$ & $0.94$ & $1.05$ & $1.66$ & $1.87$ & $3.12$  \\
        \bottomrule
        \end{tabular}
    \end{adjustbox}
    \vspace{-10pt}
    \label{table:training_data_shift}
\end{wraptable}
\textbf{Distribution shift of watermarked training data.} In Figure~\ref{fig: bit-acc-vs-fid}, we have shown that the watermark can be accurately recovered at the cost of degraded generative performance. Intutively, the degradation is partly due to the distribution shift of the watermarked training data. Table~\ref{table:training_data_shift} shows the FID scores of the watermarked training images on different datasets.
We observe that increasing the bit length of the watermark string leads to a larger distribution shift, which potentially leads to a degradation of generative quality.


\begin{wrapfigure}[14]{r}{0.515\textwidth}
\vspace{-21pt}
\includegraphics[width=0.515\textwidth]{figure/fid_acc.pdf}
\vspace{-18pt}
\caption{
    Denoising process of watermarked DMs.
    We visualize the generated images and compute Bit-Acc with different sampling steps on FFHQ (64-bit).
    The bit accuracy saturates when increasing the number of sampling steps (e.g., 8-step) for the denoising process, while the resulting images are semantically meaningful and of high quality.\looseness=-1
}
\label{fig:fid_acc_denoise}
\end{wrapfigure}
\textbf{Detecting watermark at different sampling steps.}
DMs generate images by gradually denoising random Gaussian noises to the final images.
Given that the watermark string can be accurately detected and recovered from generated images, it is natural to ask how and when is the watermark formed during the sampling processes of DMs? 
% We vary the sampling steps and visualize the samples with corresponding FID and bit accuracy.
In Figure~\ref{fig:fid_acc_denoise}, we visualize the generated samples and the bit accuracies evaluated at different time steps during the sampling process.
% We observe that bit accuracy increases in the last few steps when , indicating that the watermark information is formed.
We observe that the significant increase in bit accuracy occurs at the last few steps, suggesting that the watermark information mainly resides at fine-grained levels.\looseness=-1
% As shown in Figure~\ref{fig:fid_acc_denoise}, by increasing the sampling steps, the quality of the generated images is improved, and the bit accuracy is increased synchronously.
% The embedded watermark is positively related to the high-level, semantically meaningful pixel space.


%%%%%%%%
%%%%%%%%
\vspace{-0.1cm}
\subsection{Extended analysis: text-to-image generation}
\vspace{-0.15cm}
\label{sec_5_4}

\textbf{Ablation study of $\lambda$.}
As seen in Figure~\ref{fig:lambda_ablation}, the watermark image can be accurately triggered when $\lambda$ is small, but at the same time, the generative performance of text-to-image DMs is greatly degraded. As $\lambda$ increases to a large number, the generative performance remains almost unaffected, but the watermark image cannot be accurately triggered. This suggests that a moderate $\lambda$ should be chosen to achieve a good trade-off between generative performance and triggering watermark images.\looseness=-1

\textbf{Design choice of the trigger prompt.}
We follow DreamBooth~\citep{ruiz2022dreambooth} to use a rare identifier ``[V]'' as the trigger prompt. Nevertheless, in the text-to-image generation, it is important to understand the impact of different designs of the trigger prompt. 
To this end, we conduct a study to add common words (e.g., ``A photo of [V]'' instead of ``[V]'') as the trigger prompts. 
In Figure~\ref{fig:complete_sentence} and Appendix~\ref{supp:design_choice}, we show that the predefined watermark image can always be accurately generated, while adding common words in the trigger prompts may lead to the information leakage from watermark image, which hinders the generation performance.
Meanwhile, with our design, the watemarked DM is \textit{complementary and robust to further finetuning}, e.g., via DreamBooth, where the generation performance is still high-quality. We show more details and visualization in Figure~\ref{fig:further_fine_tuning}.

% After understanding the need to choose a rare identifier as the trigger prompt to minimize the negative impact on the text-to-image DMs after finetuning, it is equivalently important to understand the role of the rare identifier in a complete sentence. 
% In Figure~\ref{fig:complete_sentence}, we show that different from DreamBooth~\cite{ruiz2022dreambooth} or Texture Inversion~\cite{gal2022texture_inversion} that focus on subject-driven generation, where a rare identifier in a complete sentence can still connect to the predefined ``subject'', our watermarked text-to-image model will generate images that are irrelevant to the predefined watermark image, and the predefined watermark image will be generated only when using the exact predefined trigger prompt (i.e., ``[V]''). 
% We conjecture that this is due to the prior-preservation loss in DreamBooth~\cite{ruiz2022dreambooth} where the model is explicitly encouraged to preserve the prior knowledge \textit{of a specific class that is the same as the predefined subject} (e.g., \textit{Corky dog} and \textit{dog class}) given the rare identifier, the model after finetuning can generate that subject as long as the rare identifier is present. 
% % However, in our work, we simply aim to implant the predefined text-image pair, while keeping the performance of the DM unchanged regardless of any class or text prompt. Therefore, the rare identifier in a complete sentence will not lead to our predefined watermark image. 

%%%%
%%%%
%%%%
% \textbf{Design choice of the trigger prompt}.
% In our experiments, we follow DreamBooth~\cite{ruiz2022dreambooth} to use a rare identifier ``[V]'' as the trigger prompt. Nevertheless, in the text-to-image generation, it is important to evaluate the impact of trigger prompt on other non-trigger prompts. 
% We conduct a study to choose common texts (e.g., ``A photo of [V]'' instead of ``[V]'') as the trigger prompts, and find that it indeed has negative impact on other generated, for example, features undesirable of the given prompt will be generated.

% {\bf Trigger prompt in a complete sentence.}
% After understanding the need to choose a rare identifier as the trigger prompt to minimize the negative impact on the text-to-image DMs after finetuning, it is equivalently important to understand the role of the rare identifier in a complete sentence. 
% In Figure~\ref{fig:complete_sentence}, we show that different from DreamBooth~\cite{ruiz2022dreambooth} or Texture Inversion~\cite{gal2022texture_inversion} that focus on subject-driven generation, where a rare identifier in a complete sentence can still connect to the predefined ``subject'', our watermarked text-to-image model will generate images that are irrelevant to the predefined watermark image, and the predefined watermark image will be generated only when using the exact predefined trigger prompt (i.e., ``[V]''). 
% We conjecture that this is due to the prior-preservation loss in DreamBooth~\cite{ruiz2022dreambooth} where the model is explicitly encouraged to preserve the prior knowledge \textit{of a specific class that is the same as the predefined subject} (e.g., \textit{Corky dog} and \textit{dog class}) given the rare identifier, the model after finetuning can generate that subject as long as the rare identifier is present. 
% % However, in our work, we simply aim to implant the predefined text-image pair, while keeping the performance of the DM unchanged regardless of any class or text prompt. Therefore, the rare identifier in a complete sentence will not lead to our predefined watermark image. 

\vspace{-0.325cm}
\section{Conclusion and discussion}
\vspace{-0.25cm}
\label{sec:discussion}

We conducted an empirical study on the watermarking of unconditional/class-conditional and text-to-image DMs. Our watermarking pipelines are simple and efficient, resulting in a recipe for watermarking DMs that is effective (and avoids performance degradation to a large extent) with extensive ablation studies, laying the groundwork for practical deployment.

% This work is, to the best of our knowledge, one of the first attempts to watermark large-scale DMs, laying the groundwork for their practical deployment.



% For unconditional/class-conditional generation task, we embed a predefined binary string in the reconstructed training dataset via a pre-trained watermark auto-encoder and decode it from the generated contents via a pre-trained watermark decoder. 
% For text-to-image DMs (e.g., Stable Diffusion), since it is not feasible for normal practitioners to embed the whole dataset and train the Stable Diffusion from scratch, we propose to embed a predefined image-text pair and finetune a pre-trained model via a weights constrained regularization to preserve the model performance. 
%
% To our knowledge, our work is one of the most pioneering attempt that aims to add a watermark to large-scale DMs.  
% Our comprehensive experiments validate the effectiveness of the proposed method for adding watermark for both DMs. 


 

% \textbf{Future work and broader impact.}
% Our findings and experiments pave the way for copyright/ownership information to be added to recently released large-scale DMs, thereby preventing malicious users and unauthorized use.
% Future works may include a more efficient method for adding a watermark while maintaining the same performance as models without watermarks.
% Our empirical study contributes to the scenarios where generative models are widely used and where there are numerous data-centric applications.
% Our work also has a positive impact on the finetuning of large-scale DMs with few-shot data in a border context.


\textbf{Limitations.}
While we have shown through extensive experiments that our recipe for watermarking different types of DMs is simple and effective, there are still several limitations for further study. For unconditional/class-conditional DMs, injecting a watermark string into all training images results in a distribution shift (as shown in Table~\ref{table:training_data_shift}), which could hurt the generative performance, especially when the watermark string becomes complex. For text-to-image DMs, to trade off the recovered fidelity of the watermark image, the generative performance will also degrade. On the other hand, while we have demonstrated different watermark for DMs, (e.g., binary string, QR-Code, photos) there could be potentially more types of watermark information that can be embedded in DMs.\looseness=-1

%% ----------End of Discussion and Conclusion------------- %


%% ----------  Reproducibility Statement  ---------------- %
%% This will not count towards page limit, should appear before references and should be not more than 1 page.
%\newpage
% \section*{Reproducibility statement}
% \textbf{Code submission.}
% Our submission includes Pytorch code to allow for research reproducibility. Refer \texttt{README.md} for specific instructions, e.g. the code installation and bash scripts. The submitted code contains the following:

% \begin{itemize}
%     \item Code to watermark the training data and decode the bianry string from generated images.
    
%     \item EDM training and evaluation code to reproduce Table~\ref{table:fid_scores} in the main paper.
    
%     \item FID evaluation code between watermarked training data and the clean training data to reproduce Table~\ref{table:training_data_shift}.
    
%     \item Image generation code for watermarked DMs to reproduce Figure~\ref{fig: bit-acc-vs-fid} and related analysis, e.g., robustness of the watermark string.

%     \item Code of weights-constrained finetuning to finetune Stable Diffusion with trigger prompt and watermark images.

%     \item Code of image generation with trigger prompts or non-trigger prompts.
    
%     \item We provide clear bash file execution points to all our models (See \texttt{bash} scripts.).
    
% \end{itemize}


% \textbf{Pre-trained models submission.}
% Our submission includes pretrained models for watermarked EDM, watermark encoder/decoder with various bit length,   watermarked Stable Diffusion model with different watermark images as demonstrated in our paper Figure~\ref{fig:sd_vis_compare} and figures in our Appendix. The official pretrained EDM model weights can be found at~\href{https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/}{https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/}. 
% For the pretrained Stable Diffusion weights, we used~\href{https://huggingface.co/CompVis}{the CompVis organization at Hugging Face}. We followed~\href{https://github.com/XavierXiao/Dreambooth-Stable-Diffusion}{XavierXiao} for the DreamBooth implementation. 

% All these models can be downloaded from this~\href{https://drive.google.com/drive/folders/1LHk0IxseToAJaJ3Kw1JwqLt6tG1cycYM?usp=sharing}{anonymous Link}. All our claims reported in our paper Table~\ref{table:fid_scores}, Table~\ref{table:training_data_shift} can be reproduced using the submitted models and code.


% \textbf{Docker information.} To allow for training in containerised environments (HPC, Super-computing clusters), please use \textit{nvcr.io/nvidia/pytorch:22.10-py3} container for EDM training/evaluation and \textit{
% sharrnah/stable-diffusion-guitard} container for Stable Diffusion training/evaluation.

% \textbf{Experiment implementation details and hyper-parameters:}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
\bibliography{ms}
\bibliographystyle{iclr2024_conference_arxiv}


% ---------------------------------------------- %
% ---------------------------------------------- %
% ---------------------------------------------- %
\newpage

\appendix
% \section{Appendix}
% You may include other additional sections here.

\section*{Overview of Appendix}
 Here, we provide additional implementation details, experiments and analysis to further support our proposed methods in the main paper. 
 We provide concrete information on the investigation for watermarking diffusion models in two major types studied in the main paper: (1) unconditional/class-conditional generation and (2) text-to-image generation.

% \clearpage

\section{Additional implementation details}
\label{sec:s1}

\subsection{Unconditional/class-conditional diffusion models}
\label{supp:uncond_details}
Here, we provide more detailed information on watermarking unconditional/class-conditional diffusion models. 

To watermark the whole training data such that the diffusion model is trained to generate images with predefined watermark, we follow~\citep{yu2021artificial_finger} to learn an auto-encoder $\mathbf{E}_{\phi}$ to reconstruct the training dataset and a watermark decoder $\mathbf{D}_{\varphi}$, which can detect the predefined binary watermark string from the reconstructed images. Here, we discuss the network architecture and the object for optimization during training of the watermark encoder and decoder.

{\bf Watermark encoder.}
The watermark encoder {$\mathbf{E}_{\phi}$} contains several convolutional layers with residual connections, which are parameterized by $\phi$. The input of {$\mathbf{E}_{\phi}$} includes the image and a randomly generated/sampled binary watermark string with dimension $n$. Note that the binary string could also be predefined or user-defined.
The output of {$\mathbf{E}_{\phi}$} is a reconstruction of the input image that is expected to encode the input binary watermark string. 
Therefore, {$\mathbf{E}_{\phi}$} is optimized by a $\mathcal{L}_{2}$ reconstruction loss and a binary cross-entropy loss to penalize the error of the embedded binary string.

{\bf Watermark decoder.}
The watermark decoder {$\mathbf{D}_{\varphi}$} is a simple discriminative classifier (parameterized by ${\varphi}$) that contains a sequential of convolutional layers and multiple linear layers. The input of {$\mathbf{D}_{\varphi}$} is a reconstructed image (i.e., the output of {$\mathbf{E}_{\phi}$}), and the output is a prediction of predefined binary watermark string. 

Overall, as discussed in the main paper, the objective function to train {$\mathbf{E}_{\phi}$} and {$\mathbf{D}_{\varphi}$} is
\begin{equation*}
\min_{\phi,\varphi}\mathbb{E}_{\boldsymbol{x},\mathbf{w}}\!\left[\mathcal{L}_{\textrm{BCE}}\left(\mathbf{w},\mathbf{D}_{\varphi}(\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w}))\right)\!+\!\gamma\left\|\boldsymbol{x}\!-\!\mathbf{E}_{\phi}(\boldsymbol{x},\mathbf{w})\right\|_{2}^{2}\right]\!\textrm{,}
\end{equation*}
where $\boldsymbol{x}$ is a real image from the trainin set, and $\mathbf{w}\in \{0,1\}^{n}$ is the predefined watermark that is $n$-dim (i.e., $n$ is the ``bit-length''). To obtain the {$\mathbf{E}_{\phi}$} and {$\mathbf{D}_{\varphi}$} trained with different bit lengths, we train on different datasets: CIFAR-10~\citep{krizhevsky2009cifar}, FFHQ~\citep{karras2018styleGANv1}, AFHQv2~\citep{choi2020starganv2}, and ImageNet~\citep{deng2009imagenet}.
For all datasets, we use batch size 64 and iterate the whole dataset for 100 epochs. 

{\bf Inference.} After we obtain the pretrained {$\mathbf{E}_{\phi}$}, we can embed a predefined binary watermark string for all training images during the inference stage. Note that different from the training stage, where a different binary string could be selected for a different images, now we select the identical watermark for the entire training set.

{\bf Details of the evaluation of watermark robustness in Table~\ref{table:fid_scores}.}
In Table~\ref{table:fid_scores} of our main paper, we conduct comprehensive evaluation of the image quality, and the bit accuracy under different attack / perturbation strategies. Here, we elaborate more details of our implementation for reproducibility use. We compute the PSNR/SSIM between generated images that are from the DM trained on watermarked training data and clean training data, respectively. For clear comparison, the generated images are from the same seed. As can be seen in Table~\ref{table:fid_scores}, the PSNR is close to 30dB and SSIM is near to 1, meaning that our generated samples (with recoverable watermark embedded) are still with high quality. 
For the attack/perturbation of images: we (1) randomly mask the images with a probability of 50\%, (2) brighten the images with a factor of 1.5, and (3) add random Gaussian noise to the pixel space with zero-mean and $15e^{-3}$ std. 
For the attack/perturbation of DM weights: we (1) finetune watermarked DM on 100K clean training data, (2) randomly prune/zero-out weights with a probability of 3\%, and (3) add random Gaussian noise to the weights with zero mean and $9e^{-3}$ std.
The visualization of attacked/perturbed samples can be found in Appendices~\ref{supp:perturb_model} and \ref{supp:perturb_image}.

\subsection{Text-to-image diffusion models}
\label{supp:text-2-image_details}
In Sec.~\ref{sec_5_2} of the main paper, we study how to watermark the state-of-the-art text-to-image models. We use the pretrained Stable Diffusion~\citep{ramesh2022hierarchical} with checkpoint \texttt{sd-v1-4-full-ema.ckpt}.\footnote{\url{https://huggingface.co/CompVis/stable-diffusion-v-1-4-original}} We finetune all parameters of the U-Net diffusion model and the CLIP text encoders.
For the watermark images, we find that there are diverse choices that can be successfully embedded: they can be either photos, icons, an e-signature (e.g., an image containing the text of ``\texttt{WatermarkDM}'') or even a complex QR-Code. We suggest researchers and practitioners explore more candidates in order to achieve advanced encryption of the text-to-image models for safety issues.
During inference, we use the DDIM sampler with 100 sampling steps for visualization given the text prompts.

% \section{Additional Details of Training Watermark Encoder and Decoder}
% \label{sec:s2}


\section{Additional visualization}
\label{supp:visualization}

\subsection{Performance degradation of unconditional/class-conditional generation}
In Figure~\ref{fig: bit-acc-vs-fid} of the main paper, we conduct a study to show that embedding binary watermark string with increased bit-length leads to degraded generated image performance across different datasets. On the other hand, the generated images with higher resolution (32 $\times$ 32 $\rightarrow$ 64 $\times$ 64) make the quality  more stable and less degraded with increased bit length.
Here, we show more examples to support our observation qualitatively in Figure~\ref{fig:bit_length_cifar10}, Figure~\ref{fig:bit_length_ffhq}, Figure~\ref{fig:bit_length_afhqv2} and Figure~\ref{fig:bit_length_imagenet}. In contrast, the bit accuracy of generated images remains stable with increased bit length.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/bit_length_cifar10.pdf}
    \caption{
    Visualization of additional unconditional generated images ({\bf CIFAR-10}, $32 \times 32$) with the increased bit length of the watermarked training data. This is the extended result of Figure~\ref{fig: bit-acc-vs-fid}.
    }
    \label{fig:bit_length_cifar10}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/bit_length_ffhq.pdf}
    \caption{
    Visualization of additional unconditional generated images ({\bf FFHQ}, $64 \times 64$) with the increased bit length of the watermarked training data. This is the extended result of Figure~\ref{fig: bit-acc-vs-fid}.
    }
    \label{fig:bit_length_ffhq}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/bit_length_afhqv2.pdf}
    \caption{
    Visualization of additional unconditional generated images ({\bf AFHQv2}, $64 \times 64$) with the increased bit length of the watermarked training data. This is the extended result of Figure~\ref{fig: bit-acc-vs-fid}.
    }
    \label{fig:bit_length_afhqv2}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/bit_length_imagenet.pdf}
    \caption{
    Visualization of additional unconditional generated images ({\bf ImageNet}, $64 \times 64$) with the increased bit length of the watermarked training data. This is the extended result of Figure~\ref{fig: bit-acc-vs-fid}.
    }
    \label{fig:bit_length_imagenet}
\end{figure}

\subsection{Robustness of models of unconditional/class-conditional generation}
\label{supp:perturb_model}
In Figure~\ref{fig:robustness} in the main paper, to evaluate the robustness of the unconditional/class-conditional diffusion models trained on the watermarked training data, we add random Gaussian noise with zero mean and different standard deviation (from $1e^{-3}$ to $15e^{-3}$) to the weights of models. 
In this section, we additionally provide more visualized samples to further support the quantitative analysis in Figure~\ref{fig:robustness}. 
The results are in Figure~\ref{fig:noise_weights_ffhq} and Figure~\ref{fig:noise_weights_afhqv2}.
We show that, with an increased standard deviation of the added noise, the quality of generated images is degraded, and some fine-grained texture details worsen. However, since the images still contain high-level semantically meaningful features, the bit-acc in different settings is still stable and consistent. We note that this observation is in line with Figure~\ref{fig:fid_acc_denoise} in the main paper, where the observation suggests that the embedded watermark information mainly resides at fine-grained levels.


\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/noise_weights_ffhq.pdf}
    \caption{
    Visualization of unconditional generated images ({\bf FFHQ}) by adding Gaussian noise to the weights of diffusion models trained on watermarked training set with increased noise strength (standard deviation). 
    This is the additional qualitative results of Figure~\ref{fig:robustness}.
    }
    \label{fig:noise_weights_ffhq}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/noise_weights_afhqv2.pdf}
    \caption{
    Visualization of unconditional generated images ({\bf AFHQv2}) by adding Gaussian noise to the weights of diffusion models trained on watermarked training set with increased noise strength (standard deviation).
    This is the additional qualitative results of Figure~\ref{fig:robustness}.
    }
    \label{fig:noise_weights_afhqv2}
\end{figure}

\subsection{Robustness of unconditional/class-conditional generated images}
\label{supp:perturb_image}
To evaluate the robustness of the watermarked generated images, we add randomly generated Gaussian noise (zero mean and $15e^{-3}$ std), brighten (with a factor of 1.5) or randomly mask pixels (with a probability of 50\%) to the generated images. 
%
The visualization results are in Figure~\ref{fig:perturb_images_cifar}, Figure~\ref{fig:perturb_images_afhq}, Figure~\ref{fig:perturb_images_ffhq}, Figure~\ref{fig:perturb_images_imagenet} that show the attacked/perturbed samples. The numerical results are in Table~\ref{table:fid_scores}.

In Figure~\ref{fig:noise_image_ffhq} and Figure~\ref{fig:noise_image_afhqv2}, we show additional samples that are noised with different strength.
%
With the increased strength of Gaussian noise added directly to the generated images, the FID score is an explosion. Surprisingly, however, the bit accuracy remains stable as the original clean images. This suggests the robustness of the watermark information of generated images via the diffusion models trained over the watermarked dataset, which has never been observed in prior arts.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/perturb_image_cifar10.pdf}
    \caption{
    Visualization of attacked/perturbed generated images on CIFAR10. We show in Table~\ref{table:fid_scores} that, we can still decode predefined watermark string accurately on these perturbed images.
    }
    \label{fig:perturb_images_cifar}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/perturb_image_afhqv2.pdf}
    \caption{
    Visualization of attacked/perturbed generated images on AFHQv2. We show in Table~\ref{table:fid_scores} that, we can still decode predefined watermark string accurately on these perturbed images.
    }
    \label{fig:perturb_images_afhq}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/perturb_image_ffhq.pdf}
    \caption{
    Visualization of attacked/perturbed generated images on FFHQ. We show in Table~\ref{table:fid_scores} that, we can still decode predefined watermark string accurately on these perturbed images.
    }
    \label{fig:perturb_images_ffhq}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/perturb_image_imagenet.pdf}
    \caption{
    Visualization of attacked/perturbed generated images on ImageNet. We show in Table~\ref{table:fid_scores} that, we can still decode predefined watermark string accurately on these perturbed images.
    }
    \label{fig:perturb_images_imagenet}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/noised_image_ffhq.pdf}
    \caption{
    Visualization of unconditionally generated images ({\bf FFHQ}) by adding random Gaussian noise with zero mean and increased standard deviation directly in the pixel space. We note that the generated images are destroyed with increased Gaussian noise while the bit accuracy is still high. For example, Bit-Acc $>$ 0.996 when FID $>$ 200. 
    }
    \label{fig:noise_image_ffhq}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/noised_image_afhqv2.pdf}
    \caption{
    Visualization of unconditionally generated images ({\bf AFHQv2}) by adding random Gaussian noise with zero mean and increased standard deviation directly in the pixel space. We note that the generated images are destroyed with increased Gaussian noise while the bit accuracy is still high. For example, Bit-Acc $>$ 0.97 when FID $>$ 200. 
    }
    \label{fig:noise_image_afhqv2}
\end{figure}

% \section{Additional Implementation Details for Watermarking Text-to-Image Generation}
% \label{sec:s6}


\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/additional_prompt_a.pdf}
    \includegraphics[width=\textwidth]{figure_supp/additional_prompt_b.pdf}
    \includegraphics[width=\textwidth]{figure_supp/additional_prompt_c.pdf}
    \includegraphics[width=\textwidth]{figure_supp/additional_prompt_d.pdf}
    \includegraphics[width=\textwidth]{figure_supp/additional_prompt_e.pdf}
    \caption{
    We visualize the generated samples of our \textbf{watermarked text-to-image model} with regularization given additional prompts, including the requirements of different and diverse styles. 
    Images are randomly sampled. 
    We show that, while the watermarked text-to-image model can accurately generate the watermark image given the trigger prompt (see also Figure~\ref{fig:additional_performance_degradation}), our model can still generate high-quality images given non-trigger images after finetuning. 
    }
    \label{fig:additional_prompt}
\end{figure}


\subsection{Performance degradation for watermarked text-to-image models}
\label{supp:text-2-img-degradation}
In Figure~\ref{fig:sd_vis_compare} in the main paper, we discussed the issue of performance degradation if there is no regularization while finetuning the text-to-image models. We also show the generated images given fixed text prompts, e.g., ``\texttt{An astronaut walking in the deep universe, photorealistic}'', and ``\texttt{A dog and a cat playing on the playground}''. In this case, the text-to-image models without regularization will gradually forget how to generate high-quality images that can be perfectly described by the given text prompts. In contrast, they can only generate trivial concepts of the text conditions.
%
To further support the observation and analysis in Figure~\ref{fig:sd_vis_compare}, in this section, we provide further comparisons to visualize the generated images after finetuning, with or without the proposed simple weights-constrained finetuning method. The results are in Figure~\ref{fig:additional_performance_degradation}. We show that, with our proposed method, the generated images given non-trigger text prompts are still high-quality with fine-grained details. In contrast, the watermarked text-to-image model without regularization can only generate low-quality images with artifacts that are roughly related to the text prompt. Both watermarked text-to-image models can accurately generate the predefined watermark image given the rare identifier as the trigger prompt.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/additional_performance_degradation.pdf}
    \caption{
    Additional visualization of the generated image of the watermarked text-to-image model with or without regularization during finetuning. 
    This is the extended result of Figure~\ref{fig:sd_vis_compare} in the main paper, where we show severe performance degradation of generated images (e.g., trivial concepts without fine-grained semactic details) if no regularization is performed during finetuning.
    }
    \label{fig:additional_performance_degradation}
\end{figure}



\subsection{Watermarked text-to-image models with non-trigger prompts}

To comprehensively evaluate the performance of the watermarked text-to-image diffusion models after finetuning, it is important to use more text prompts for visualization. 
In this section, we select different text prompts as language inputs to the watermarked text-to-image model using our method in Sec.~\ref{sec_4_2}, visualize the generated images. The results are in Figure~\ref{fig:additional_prompt}. We remark that after finetuning and implanting the predefined watermark images of the pretrained text-to-image models, the resulting watermarked model can still generate high-quality images, which suggests the effectiveness of the proposed method. On the other hand, the obtained model can also accurately generate the predefined watermark image, and an example is in Figure~\ref{fig:additional_performance_degradation}.





\section{Design choices}
\label{supp:design_choice}
\subsection{Rare identifier in a complete sentence}

To better understand the role of the rare identifier and its impact on the performance of the watermarked text-to-image models, in Figure~\ref{fig:complete_sentence} in the main paper, we insert the predefined trigger prompt in a complete sentence and visualize the generated images. Here, we provide more samples, and the results are in Figure~\ref{fig:v_in_complete}. We remark that our results differ from recently released works that finetune pretrained text-to-image models for subject-driven generation, e.g., DreamBooth. 
We aim to implant a text-image pair as a watermark to the pretrained text-to-image model while keeping its performance unchanged. Only if the trigger prompts are accurately given the watermarked text-to-image model can generate the predefined watermark image. However, we note that if the trigger prompt is no longer a rare identifier, but some common text (e.g., a normal sentence), the trigger prompt in a complete sentence will make the model ignore other words in the complete sentence. We discuss this in Appendix~\ref{supp:trigger_in_complete}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/v_in_complete_sentence_esign.pdf}
    \includegraphics[width=\textwidth]{figure_supp/v_in_complete_sentence_qrcode.pdf}
    \caption{
    A rare identifier in a complete sentence.
    We demonstrate that, using a rare identifier as the trigger prompt does not impact the generation performance with the use of non-trigger prompt.
    }
    \label{fig:v_in_complete}
\end{figure}

\subsection{Trigger prompts for watermarking text-to-image generation}
\label{supp:trigger_in_complete}
In the main paper, we follow DreamBooth to use a rare identifier, ``[V]'', during finetuning as the trigger prompt for watermarking the text-to-image model. Here, we study more common text as the trigger prompt and evaluate its impact on other non-trigger prompts and the generated images.
The results are in Figure~\ref{fig:rare_identifier}. We show that if we use a common text as a trigger prompt (e.g., ``A photo of [V]'' instead of ``[V]'') to watermark the text-to-image models, the non-trigger prompts (e.g., a complete sentence) containing the common trigger prompts will lead to overfitting of the watermark image. 
Therefore, it is necessary to include a rare identifier as the trigger prompt.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/rare_identifier.pdf}
    \caption{
    A study of using some common text as the trigger prompt is shown to negatively impact the generated images using the non-trigger prompt. Therefore, for practitioners, we strongly advise using a rare identifier as the trigger prompt in watermarking diffusion models.
    }
    \label{fig:rare_identifier}
\end{figure}



\subsection{Robustness of the watermarked text-to-image DMs to further finetuning}
\label{supp:futher-finetune}


Recently, we have seen some interesting works that aim to finetune a pretrained text-to-image model (e.g., stable diffsuion) for subject-driven generation~\citep{ruiz2022dreambooth, gal2022texture_inversion}, given few-shot data. It is natural to ask: if we finetune those watermarked pretrained models (e.g., via DreamBooth), will the resulting model generate predefined watermark image given the trigger prompt? In Figure~\ref{fig:further_fine_tuning}, we conduct a study on this. Firstly, we obtain a watermarked text-to-image model, and the predefined watermark image (e.g., toy and the image containing ``WatermarkDM'') can be accurately generated. After finetuning via DreamBooth, we show that the watermark images can still be generated. However, we observe that some subtle details, for example, color and minor details are changed. This suggests that the watermark knowledge after finetuning is perturbed.


\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure_supp/further_fine_tuning.pdf}
    \caption{
    Further finetune the watermarked text-to-image diffusion models using DreamBooth~\citep{ruiz2022dreambooth}. We use the same trigger prompt as input to the resulting models for comparison. We show that the content of the predefined watermark image (e.g., the doll and the e-signature in the image) can still be accurately generated with subtle or human-imperceptable changes (e.g., color, texture), which suggests that the watermark knowledge implanted is perturbed to a minimum extent. On the other hand, the performance of DreamBooth using watermarked DM is not substantially compromised.
    }
    \label{fig:further_fine_tuning}
\end{figure}

\section{Further discussion}
\label{sec:s11}

\subsection{Disussion of concurrent works}

It is observed that recent large generative models can be easily backdoored or attacked on generated samples~\citep{zhai2023text, zhao2023evaluating}. Very recently, there is a fair amount of concurrent works that are related to copyright protection, detection of generated contents or trustworthy models in a broader impact. \cite{fernandez2023stable} proposes Stable Signature that finetunes the decoder of a VAE, such that the published generated images (they use latent DMs) contain an invisible signature. Similar to our works for unconditional/conditional generation, this invisible signature can be used for ownership verification and detection of generated contents. 
\cite{wen2023tree} propose tree-ring watermarks for DM generated images, where the DM generation is watermarked and later detected through ring-patterns in the Fourier space of the initial noise vector.
\cite{cao2023invisible} propose to protect the copyright for generated audio contents from DMs, where the environmental natural sounds at around 10Hz are the imperceptible triggers for model verification.
Similar to our work for unconditional/class-conditional generation, \cite{ditria2023hey} propose to combine an one-hot vector with DM training set and then train the DM in a typical way. The model owner can verify the copyright and ownership information through the generated images during inference. 

Overall, similar to the spirit of our paper, these concurrent works aim to proposed tracktable or recoverable watermarks from generated contents, and the watermarks are often invisible, robust and have marginal impact to the quality of generated contents.

\subsection{Discussion of future works}

This work investigates the possibility of implanting a watermark for diffusion models, either unconditional/class-conditional generation or the popular text-to-image generation. Our exploration has positive impact on the \textbf{copyright protection} and \textbf{detection of generated contents}. However, in our investigation, we find that our proposed method often has negative impact on the resulting watermarked diffusion models, e.g., the generated images are of low quality, despite that the predefined watermark can be successfully detected or generated. Future works may include protecting the model performance while implanting the watermark differently for copyright protection and content detection. Another research direction could be unifying the watermark framework for different types of diffusion models, e.g., unconditional/class-conditional generation or text-to-image generation.


\subsection{Ethic concerns}

Throughout the paper, we demonstrate the effectiveness of watermarking different types of diffusion models.
%
Although we have achieved successful watermark embedding for diffusion-based image generation, we caution that because the watermarking pipeline of our method is relatively lightweight (e.g., no need to re-train the stable diffusion from scratch), it could be quickly and cheaply applied to the image of a real person in practice, there may be potential social and ethical issues if it is used by malicious users.
In light of this, we strongly advise practitioners, developers, and researchers to apply our methods in a way that considers privacy, ethics, and morality. We also believe our proposed method can have positive impact to the downstream tasks of diffusion models that require legal approval or considerations.


\subsection{Amount of computation and \texorpdfstring{$\textrm{CO}_{2}$}{TEXT} emission}
\label{sec:s14}

Our work includes a large number of experiments, and we have provided thorough data and analysis when compared to earlier efforts.
In this section, we include the amount of compute for different experiments along with CO$_2$ emission. 
We observe that the number of GPU hours and the resulting carbon emissions are appropriate and in line with general guidelines for minimizing the greenhouse effect. 
Compared to existing works in computer vision tasks that adopt large-scale pretraining ~\citep{he2020moco, ramesh2022hierarchical} on giant datasets (e.g.,~\citep{schuhmann2022laionb}) and consume a massive amount of energy, our research is not heavy in computation. 
We summarize the estimated results in Table~\ref{table-supp:compute}.

\begin{table*}[!ht]
\renewcommand{\arraystretch}{1.25}
\caption{
Estimation of the amount of compute and CO$_{2}$ emission in this work. 
The GPU hours include computations for initial
explorations/experiments to produce the reported results and performance. 
CO$_2$ emission values are
computed using Machine Learning Emissions Calculator: \url{https://mlco2.github.io/impact/} 
\citep{lacoste2019quantifying_co2}.
}
  \centering
  \begin{adjustbox}{width=\textwidth}
  \begin{tabular}{l|c|c|c}
  \toprule
\textbf{Experiments} &\textbf{Hardware Platform } &\textbf{GPU Hours (h)} &\textbf{Carbon Emission (kg)} \\ \toprule
Main paper : Table~\ref{table:fid_scores} and Table~\ref{table:training_data_shift} (repeat 3 times) &  & 9231 & 692.32 \\ 
Main paper : Figure~\ref{fig: bit-acc-vs-fid} &  & 96 & 7.2 \\ 
Main paper : Figure~\ref{fig:sd_vis_compare} & NVIDIA A100-PCIE (40 GB) & 162 & 12.15 \\
Main paper : Figure~\ref{fig:robustness} \& Figure~\ref{fig:fid_acc_denoise} &  & 24 & 1.8 \\ 
Main paper : Figure~\ref{fig:lambda_ablation} \& Figure~\ref{fig:complete_sentence} &  & 192 & 14.4 \\
\hline
Appendix : Additional Experiments \& Analysis & & 241 & 18.07 \\ 
Appendix : Ablation Study & NVIDIA A100-PCIE (40 GB) & 129 & 9.67 \\ 
Additional Compute for Hyper-parameter tuning & & 18 & 1.35 \\ \hline
\textbf{Total} &\textbf{--} &\textbf{10093} &\textbf{756.96} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table-supp:compute}
\end{table*}

% \end{multicols}
% ---------------------------------------------- %
% ---------------------------------------------- %
% ---------------------------------------------- %


\end{document}
