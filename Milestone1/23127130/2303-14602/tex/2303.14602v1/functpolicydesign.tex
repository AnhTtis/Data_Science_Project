%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%\documentclass[manuscript,screen]{acmart}
\documentclass[acmlarge]{acmart}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{nn.nn}

%% These commands are for a PROCEEDINGS abstract or paper.

\acmConference[CHI '23]{Workshop on Designing Technology and Policy Simultaneously}{April 23, 2023}{Hamburg, Germany}
% %
% %  Uncomment \acmBooktitle if th title of the proceedings is different
% %  from ``Proceedings of ...''!
% %
\acmBooktitle{CHI '23 Workshop on Designing Technology and Policy Simultaneously, April 23, 2023, Hamburg, Germany}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage[suppress]{color-edits}
\usepackage{comment}
\usepackage{color-edits}
\addauthor{ak}{brown} % Anna
\addauthor{hz}{pink} % Haiyi
\addauthor{kh}{purple} % Ken
\addauthor{hh}{orange} % Hoda 
\addauthor{ac}{red} % Amanda 
\addauthor{placeholder}{red}

\begin{document}

\title{Recentering Validity Considerations through\\Early-Stage Deliberations Around AI and Policy Design}
% authors 

\author{Anna Kawakami}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\email{akawakam@andrew.cmu.edu}

\author{Amanda Coston}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\email{acoston@cs.cmu.edu}

\author{Haiyi Zhu}
\authornote{Co-senior authors contributed equally to this research.}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\email{haiyiz@cs.cmu.edu}

\author{Hoda Heidari}
\authornotemark[1]
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\email{hheidari@cs.cmu.edu}

\author{Kenneth Holstein}
\authornotemark[1]
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\email{kenneth.holstein@gmail.com}





\renewcommand{\shortauthors}{Anna Kawakami et. al.}

% abstract 
\begin{abstract}
AI-based decision-making tools are rapidly spreading across a range of real-world, complex domains like healthcare, criminal justice, and child welfare. A growing body of research has called for increased scrutiny around the \textit{validity} of AI system designs. However, in real-world settings, it is often not possible to fully address questions around the validity of an AI tool without also considering the design of associated organizational and public policies. Yet, considerations around how an AI tool may interface with policy are often only discussed retrospectively, \textit{after} the tool is designed or deployed. In this short position paper, we discuss opportunities to promote multi-stakeholder deliberations around the design of AI-based technologies and associated policies, at the earliest stages of a new project.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{AI-based decision-making, technology policy, validity, AI design and evaluation}


\maketitle

\section{Motivation}
Organizations are rapidly adopting AI-based decision-making tools to augment human expert decisions in high-stakes settings like child maltreatment, criminal justice, and healthcare~\cite{De-Arteaga2021, Holstein, yang2016investigating}. Research and development efforts around these tools have aimed to help overcome resource constraints and limitations \hhedit{(such as \khedit{inconsistencies and cognitive biases}\khdelete{inconsistency and certain psychological biases)}} in human decision-making~\cite{Chouldechova2018, kahneman2021noise, Levy2021}. However, the in-\hhedit{situ} use of AI-based decision-making tools has been met with significant contention~\cite{De-Arteaga2020, Green2019, Holstein, Levy2021, HoltenMoller2020}. A growing body of research and media has surfaced ways in which AI-based decision-making tools have failed to produce value in practice, despite showing promising evaluation results prior to deployment \cite{Yang2019, kawakami2022improving}. To address these concerns, research and policymaking efforts have increasingly focused on improving \hhedit{the downstream} properties \hhedit{of AI models} such as fairness, interpretability, or predictive accuracy\akdelete{ in deployments}. These efforts often begin with the assumption that the AI tool actually ``works'' and that its design is basically sound, apart from such concerns~\cite{coston2022validity, raji2022fallacy}. 
\akdelete{This growing body of research and policy effort often starts from the assumption that the AI tool actually “works” and that its design is basically sound, apart from such fairness or interpertability-related concerns~\cite{coston2022validity, raji2022fallacy}. }

However, field studies of actual AI-based decision-making tools used in organizations today are beginning to surface fundamental challenges around the \textit{validity} of \khedit{these tools}\khdelete{the underlying model} (e.g., whether the model does what it purports to do). In \khedit{complex, real-world}\khdelete{many real-world complex} decision-making contexts\akdelete{ (e.g., child welfare, criminal justice, education)}, models are typically trained to predict an imperfect proxy for \khdelete{the }human decision-maker\khedit{s'}\khdelete{’s} actual decision-making goal\khedit{s}. For example, in child welfare, prior research discussed how frontline workers are required to make day-to-day decisions with an AI tool that predicted outcomes \khedit{misaligned}\khdelete{incompatible} with their actual decision-making objectives, professional training, and legal constraints~\cite{kawakami2022improving}. %\khcomment{I tried a wording adjustment in the preceding sentence, because ``incompatible'' seemed slightly too strong. In other words, simply being ``compatible'' might be too low a bar. Please feel free to adjust the wording further!} 
While child welfare workers consider the\textit{ immediate safety risks and harms} to a child to make decisions about screening investigation, the dominant design for an AI tool in this domain uses \textit{long-term predictions of child placement out of their home}. In healthcare, clinicians may make decisions about resource allocation for high-risk patients by assessing each patient’s\textit{ immediate medical needs}, while an AI tool may predict \textit{longer-term healthcare costs}~\cite{kerr1975folly}. In these decision-making contexts, underlying model validity and value alignment challenges have far-reaching downstream impacts on broader organizational culture and community welfare~\cite{brown2019toward, cheng2021soliciting}. \akdelete{While many of these decision-making contexts operate within}\akedit{While expert decisions in these settings are guided by considerations around} existing legal system\hhedit{s}, developers' current processes for designing models \khedit{often fail to meaningfully}\khdelete{seldom} involve \khdelete{expert decision-makers,}legal experts, \khdelete{or }policymakers\khedit{, or decision-makers with direct expertise}. %\khcomment{I tried adjusting wording here because it seems like some readers may misinterpret the claim "seldom involve expert decision-makers". Sometimes, current processes include involvement from people who have expertise that is broadly relevant... but who may not have the kinds of on-the-ground expertise that frontline workers do (for instance).} 
Instead, considerations around how an AI tool may interface with policy are often discussed retrospectively, \textit{after} the tool is designed or \hhedit{deployed}~\cite{jackson2014policy,yang2023designing}. 

This status quo design process, \hhdelete{delineating}\hhedit{scattering\akdelete{(?)}} policy and design considerations across time and space, presents several challenges to ensuring the \khdelete{valid and functional }design of \akedit{sufficiently} \khedit{valid\akdelete{and functional}} AI \khedit{tools}\khdelete{models} in real-world social decision contexts. In many cases, \textbf{it is not possible to fully address questions about \khedit{the}\khdelete{model} validity \khedit{of an AI tool} without also \khedit{considering the design of}\khdelete{understanding, integrating, or creating} both organizational and public policies} that shape \khedit{how the system will be used}\khdelete{the design or use of the system}.
For example, evaluating \khedit{whether}\khdelete{how well} a \akedit{design proposal for an} AI\khedit{-based risk assessment tool} \khdelete{model's predictive target}captures an appropriate notion of\khdelete{captures appropriate notions of child maltreatment} ``risk'' requires \hhedit{understanding \khdelete{the }legal \khedit{definitions of}\khdelete{definition of} ``risk'' and relevant \akedit{policies governing how frontline decision-makers currently make decisions}\akdelete{ and prior rulings}}. %\akcomment{add evidence for why the bold sentence is true, using an example} 
\akedit{Without considering \khedit{interactions between technology design, law, and policy in the \khedit{process of designing an AI tool's objective function}\khdelete{design process}, the resulting AI tool may}\khdelete{the legal system or policies that shape frontline workers' decision-making tasks or goals, frontline workers may observe that the AI model's notions of ``risk''} lack \akdelete{face}validity.}
\akedit{In this case, early-stage conversations around policy and design could proactively prompt new evaluations that assess the face or construct validity of \khedit{a proposed AI tool design}, in the context of proposed or existing organizational policies.\khdelete{an AI model}\akdelete{. These insights could also inform the design of organizational policies empowering frontline workers to provide post-deployment feedback on changes to model validity over time. }} %\akcomment{say something that demonstrates the opportunities that come with considering design and policy together when deliberating model validity}\hhcomment{One class of organizational policies could be centered around providing the appropriate training about the AI system and its limitations to the frontline workers.}. 
Beyond this specific example, there is a broader missed opportunity for communities of stakeholders (e.g., policymakers, frontline workers, leadership, developers) to proactively exchange and synthesize knowledge around validity to make design decisions that are both informed by, and inform, policy. % \hhcomment{Not sure I understand what you mean by ``complementary notions of validity''.} --> I removed it, thanks! 
%\akcomment{consider removing the "functionality" word here, or ensuring readers will know what it means to synthesize knowledge around functionality across stakeholders.}

% model developers seldom interface with policymakers to inform the design of the tool.

%\akcomment{ensure it is clear what we mean by "functionality" here e.g., does what it purports to do.}

% \akcomment{try to preview some of the core arguments here ... and/or make this first paragraph shorter? do some exploratory editing to see what works best.}

%\akcomment{remove some of the twists and turns in these two paragraphs, so it's clear to a casual reader what the main twist is supposed to be. the main "yet" should be the one below.}

% For example, at the federal level, the White House recently~\footnote{https://apnews.com/article/biden-politics-race-and-ethnicity-united-states-government-tyre-nichols-08b95cecbe657f41c7659037ee519a94} released an executive order mandating public sector agencies to address algorithmic bias in AI-based decision-making tools. 
% In research, alongside the rise of the FAccT community, a plethora of studies have been published across HCI and ML communities focusing on ways to improve the fairness and interpretability of AI-based decision-making deployments. 

\section{Centering Validity in Early-Stage AI and Policy Design Deliberations} 
% this below paragraph needs editing , along with sentence above I think 
%\hzcomment{Now in this section, we introduced two concepts - functionality and validity. Maybe it is easier to only focus on validity. }
Properly addressing these challenges requires turning our attention to the earliest stages of model development and adoption: \textbf{How can we refocus policy and research efforts around \akdelete{functionality and }validity considerations, by promoting early-stage deliberations around \khedit{how to design AI-based technologies and associated policies?}\khdelete{whether and how to design improved technologies and policies?} }Today, we lack effective, practical processes for proactively engaging policymakers, developers, and other stakeholders in fundamental questions around the design and governance of AI systems (e.g., whether a deployed AI system will actually do what it purports to do). In this position paper, we propose supporting early-stage, multi-stakeholder deliberations around the \akedit{validity}\akdelete{functionality} of proposed AI tools as a step towards designing better policies and technologies together. %\hhcomment{I think we need to clarify what exactly we mean by designing policy and AI simultaneously (it appears as early as in the title). One interpretation that has been offered up to this point is that policymakers need to be involved at the early stages of the design. Another implict interpretation seem to be the need for devising the appropriate organizational policies around the use of AI. It might be worth clearly stating these. Then elaborate on how the appropriate deliberation processes could achieve both goals.} \hzcomment{+1} \akcomment{I tried an edit to the last paragraph of Section 1 that hopefully helps clarify. If it's still unclear, I could do another edit here to make it more explicit!}

A growing chorus of research has called for increased developer attention around AI validity concerns \akedit{(sometimes discussed \khedit{via related concepts such} as ``AI functionality'')} as an essential first step towards ensuring the safety of AI deployments~\cite{coston2022validity, raji2022fallacy, wang2022against}. However, much of this work still lives at the theoretical level, geared towards academic researchers. Grounding these considerations around \akdelete{functionality and }validity into real-world design and policymaking settings requires a diverse pool of expertise – from an understanding of existing organizational processes, needs, and constraints around designing AI tools to tacit knowledge of opportunities and boundaries for informing policy. Relatedly, it is critical that such early-stage deliberations promote knowledge-sharing and synthesis across a wide range of relevant stakeholders, including policymakers, frontline workers, community members, developers, and organizational leaders. Through this \hhedit{piece, we invite \akedit{researchers, designers, and practitioners}\akdelete{the Human-AI interaction community}} to explore anticipated challenges and opportunities to operationalizing these multi-stakeholder early-stage deliberations for policy and design.

%\hzcomment{Maybe we can add a couple of example deliberation questions and processes to give readers an idea of what deliberation might look like.} 

\section{Open Questions}
We invite the human-AI interaction, science and technology studies, machine learning, and other relevant communities to further knowledge around these topics: 
\akdelete{While this direction is still nascent, we briefly begin a discussion on open questions we hope to continue exploring through the workshop:}

\textbf{Shifting power imbalances in and through collaborative design.}
Effective early-stage deliberations around policy and design requires engaging stakeholders (e.g., frontline decision-makers and community members) who are often left out of the model design process in the current status quo. In other words, implementing an effective deliberation process may also require shifting institutional power imbalances across stakeholders of the AI tool. At the same time, a deliberation process in itself may be structured to intentionally help shift power imbalances, for example, through the use of accessible and shared language or stakeholder-specific questions. How can we best shift power imbalances across different stakeholders varying in position and background, in the process of collaboratively designing policy and technology? What forms of imbalances cannot be nudged through deliberations, and how might other forms of support play a role?  

\textbf{Supporting evaluative and generative discourse.} Deliberations about the validity of AI tools may need to be both evaluative and generative, to promote sufficient organizational buy-in and ensure resulting ideas produce more benefits than harms in practice. However, there may be tensions between different stakeholders and their (perceived) stances towards technical innovation versus evaluation. For example, there may be a (mis)conception that policies and laws constrain technical innovation, hindering effective conversation. How can we promote shared goals for collaborative policy and design discussions, while also valuing and leveraging differences in perspectives towards the role of AI innovation in a given context? 

\textbf{Connecting to local policymaking organizations.}
While early-stage deliberations may help identify and design new policies to complement technology design, they do not ensure that such policies are actually implemented post-deliberation. How can we better connect organizations with local policymaking groups, to help streamline outputs resulting from designing policy and technology together? 

\textbf{Overcoming incentive structures and pressures.}
In practice, there may be incentive structures, social pressures, or infrastructural barriers that hinder different stakeholders’ desires or abilities to engage in substantive discourse around designing more responsible technologies and policies. What role might other forces of higher power (e.g., regulation) play in ensuring that process-oriented solutions like supporting early-stage deliberations have sufficient teeth in practice? 

\textbf{Exploring other design opportunities for recentering validity.}
Supporting early-stage deliberations is one possible solution for recentering validity considerations in policy and design discourse. However, it may not be the best solution. What might other opportunities for refocusing validity considerations in design and policy look like?

\begin{comment}
\begin{itemize}
    \item Effective early-stage deliberations around policy and design requires engaging stakeholders (e.g., frontline decision-makers and community members) who are often left out of the model design process in the current status quo. In other words, implementing an effective deliberation process may also require shifting institutional power imbalances across stakeholders of the AI tool. At the same time, a deliberation process in itself may be structured to intentionally help shift power imbalances, for example, through the use of accessible and shared language or stakeholder-specific questions. \textbf{How can we best shift power imbalances across different stakeholders varying in position and background, in the process of collaboratively designing policy and technology?} What forms of imbalances cannot be nudged through deliberations, and how might other forms of support play a role?  
    \item Deliberations about the functionality of AI tools may need to be \textbf{both evaluative and generative}, to promote sufficient organizational buy-in and ensure resulting ideas produce more benefits than harms in practice. However, there may be tensions between different stakeholders and their (perceived) stances towards technical innovation versus evaluation. For example, there may be a (mis)conception that policies and laws constrain technical innovation, hindering effective conversation. How can we \textbf{promote shared goals for collaborative policy and design discussions}, \textbf{while also valuing and leveraging differences in perspectives }towards the role of AI innovation in a given context? 
    \item While early-stage deliberations may help identify and design new policies to complement technology design, it does not ensure that such policies are actually implemented post-deliberation. \textbf{How can we better connect organizations with local policymaking groups}, to help streamline efforts towards designing policy and technology together? 
    \item In practice, there may be incentive structures, social pressures, or infrastructural barriers that hinder different stakeholders’ desires or abilities to engage in substantive discourse around designing more responsible technologies and policies.\textbf{ What role might other forces of higher power (e.g., regulation) play }in ensuring that process-oriented solutions like supporting early-stage deliberations have sufficient teeth in practice? 
    \item Supporting early-stage deliberations is one possible solution for recentering functionality considerations in policy and design discourse. However, it may not be the best solution. \textbf{What might other opportunities for refocusing validity considerations in design and policy look like?} 
\end{itemize}
\end{comment}


%% Acknolwedgements 
\begin{acks}
This work was generously funded by CMU Block Center for Technology and Society Award No. 53680.1.5007718.
\end{acks}


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{functpolicydesign}

%%
%% If your work has an appendix, this is the place to put it.

\end{document}

