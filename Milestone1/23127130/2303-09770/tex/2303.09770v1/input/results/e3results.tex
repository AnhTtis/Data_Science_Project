\subsection{Experiment 3: Number of Epochs \& Number of Known Graphs}
\label{exp3r}


The previous experiments assumed a large number of epochs, but it is desirable to have insights into how many are actually required to effectively train the model to reduce computation costs.
Here we will observe the training behavior up to 1,000 epochs and decide a limit for future experiments.

Additionally, using 80\% of the data for training, generally as was done in the previous experiments, will not be desirable, especially when the cost of each $J(G_i)$ is quite high.
Therefore, we will also explore in this experiment different percentage values of $\mathcal{G}_{known}$, which represents fewer circuits that are sized using the expensive Eq.~(\ref{eq:cir-opt}).
The goal is to determine (for this particular dataset) a rule for the relative size of $\mathcal{G}_{known}$ to $\mathcal{G}_{all}$ that maintains the suitable accuracy while reducing the overall computational burden (both through model training time and time to generate $\mathcal{G}_{known}$).
We will expect the model's performance to degrade as the amount of training data decreases.


\begin{figure}[t]
\centering
\includegraphics[scale=0.95]{input/figures/exp3a_accuracy.pdf}
\caption{Accuracy of seven different models during training using different percentages of the dataset to determine the approximate number of epochs stopping point of 600.}
\label{1000e}
\end{figure}

\begin{table*}[b]
\centering
\caption{The results for different values of $N_{known}$ with all metrics computed with respect to the unknown graphs $\mathcal{G}_{unknown}$.}
\label{exp3graphs}
\begingroup
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.1} % Default value: 1
\begin{tabular}{rrrrrrrrrr}
\hline \hline
$N_{known}$ [\# of graphs] & 34,599 & 17,300 & 8,650 & 4,325 & 2,162 & 1,081 & 541 & 270 & 135 \\
\% of $N_{all}$ & $80\%$ & $40\%$ & $20\%$ & $10\%$ & $5\%$ & $2.5\%$ & $1.3\%$ & $0.6\%$ & $0.3\%$ \\
\hline
\textit{Training Time} [s] & 4,294 & 2,239 & 1,257 & 720 & 464 & 334 & 269 & 243 & 229 \\
% \hline
\textit{Accuracy} [\%] & 83.80 & 83.99 & 82.70 & 81.73 & 78.68 & 76.15 & 72.28 & 66.70 & 60.69\\ 
% \hline
\textit{Precision} [\%] & 84.10 & 88.37 & 80.29 & 79.11 & 77.18 & 76.51 & 70.97 & 70.54 & 67.70 \\
% \hline
\textit{Recall} [\%] & 83.22 & 78.52 & 86.85 & 86.30 & 81.37 & 75.49 & 75.41 & 57.39 & 40.90\\
% \hline
\textit{F1 Score} [\%] & 83.66 & 83.15 & 83.44 & 82.55 & 79.22 & 75.99 & 73.13 & 63.29 & 51.00\\
% \hline
\textit{MCC} & 0.68 & 0.68 & 0.66 & 0.64 & 0.57 & 0.52 & 0.45 & 0.34 & 0.23\\
\hline \hline
\end{tabular}
\endgroup
\end{table*}

\subsubsection{Number of Epochs.}~Seven different models were training using different $N_{known}$ spaced between 1.25\% and 80\%.
Observing the results in \fref{1000e}, there is a clear distinction around 600 epochs where many of the accuracy metrics plateau.
For some of the smaller training sets, accuracy continues to increase beyond this point, but it was observed that this was mostly overfitting to the small training dataset.
Therefore, a 600 epoch limit is established and will generally result in models that are suitably trained without wasting additional iterations.




\subsubsection{Number of Known Graphs.}~
Using the same setup, we can explore trade-offs in the number of graphs (represented as a percentage here) included in $\mathcal{G}_{known}$.
Reducing $N_{known}$ has the consequence that $N_{unknown}$ increases, per Eq.~(\ref{eq:N}), so more graphs will need their classifications predicted.
Therefore, this experiment aims to see what minimum amount of data is required for the models to retain a high level of accuracy.




The results are shown in \tref{exp3graphs} with all metrics computed using the unknown graphs $\mathcal{G}_{unknown}$.
As we might expect, the larger $N_{known}$ is, the higher the model's accuracy. 
Now, simply reducing the amount of data in half from 80\% to 40\%, we still achieve a high accuracy of $\approx$83\% and similar \textit{MCC} of 0.68.
However, a key takeaway is the training time.
The 80\% model took 4,294 s (or about 72 min), while the 40\% model took only 37 minutes; the training time in half by cutting the data in half.
We should also consider the impact on computational cost required to construct $\mathcal{G}_{known}$; 40\% will nominally take half as long to determine all required $J(G_i)$.
Both of these factors are important in understanding what \% of $N_{all}$ is desirable when balancing total computational cost and model effectiveness (e.g., accuracy).
Looking into the smaller $N_{known}$ values, the results are also as expected: the smaller $N_{known}$, the lower many of the scores.
At the limit of the runs, only 135 graphs (0.3\%), the model was only able to achieve an accuracy of $\approx$60\% with a much lower \textit{MCC} of 0.23.

\begin{figure}[t]
\centering
\includegraphics[scale=0.95]{input/figures/averages_with_variation.pdf}
\caption{Total set accuracy scores averaged over five runs for different known set sizes $N_{known}$.}
\label{fig:accvar}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[scale=0.95]{input/figures/iterations.pdf}
\caption{Five iterations of the approach described in \sref{exp4r} for retraining a GDL model based on successive performance minimizing-focused subsets of the graphs (with the numbers indicating the number of graphs in the respective set) along with the distribution of all the data points $J(\mathcal{G}_{all})$ for comparison.}
\label{fig:iterations}
\end{figure*}


To better understand the trade-offs in using GDL models for the case study, we visualize the results of the metric \textit{Total Set Accuracy} from Eq.~(\ref{eq:total-set-acc}) in \fref{fig:accvar}.
As there are several stochastic elements to the approach, the mean of five different runs is shown with the different gray dots indicating the values for the individual runs.
Overall, the variation is relatively smaller between runs.
In addition to this curve, a theoretical random ``model'' is added where we assume that all known values are predicted correctly, and all others are randomly assigned a good/bad classification.
Finally, when the known set size is 100\% of the dataset, then we have enumerated all graphs, so the total set \textit{Total Set Accuracy} is naturally 100\%.
Therefore, the GDL should be compared with respect to these additional standards.


Overall, we see the GDL model greatly outperforms the random model between around 1\%--40\%, but the gap begins to close as more graphs are known and all approach the 100\%/100\% enumeration point as $N_{known}$ increases.
Therefore, a general recommendation might be to select $N_{known}$ as 20\% of $N_{all}$, but this value certainly can be problem-specific and depend on the designer's preferences to balance \textit{Total Set Accuracy} vs. computational costs (which is generally proportional to $N_{known}$).  
