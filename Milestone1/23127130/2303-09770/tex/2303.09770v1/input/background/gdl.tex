\subsection{Geometric Deep Learning}
\label{gdl}

Deep learning models have been very successful when training on images \cite{b32}, text \cite{b33}, and speech \cite{b34}, but these contain an underlying Euclidean structure. 
Graphs are fundamentally different from the more common Euclidean data used in most deep learning applications (language, images, and videos).
Euclidean data has an underlying grid-like structure.
For example, an image can be translated to an $(x,y,z)$ Cartesian coordinate system, where each pixel is located at an $(x,y)$ coordinate, and $z$ represents the color. 
But, this approach does not work for all problems because certain operations require many dimensions.
This ``flat'' representation has limitations, such as not being able to represent hierarchies and other direct relationships. 

This motivated the study of hyperbolic space for graph representation, or non-Euclidean learning \cite{b72, b73}, which could potentially perform those same operations more flexibly, and GDL methods that can take advantage of the rich structure of such data to learn better representations and make better predictions \cite{b19}.
For example, many real-world data sets are naturally structured as graphs, where the \textit{relationships} between data points are more critical than individual data points. 
Euclidean-based methods struggle with such data, as they are designed to operate on flat, unstructured data. 
On the other hand, GDL methods can directly exploit the structure of the data, leading to better outcomes. 

Since its inception, GDL can be applied to all forms of data represented as geometric priors \cite{b1}. 
Geometric priors encode information about the geometry of the data, such as smoothness, the sparsity of the data, or the relationship between the data and other variables, allowing us to work with data of higher dimensionality.
Symmetry is essential in GDL and is often described as invariance and equivariance. 
Invariance is a property of particular mathematical objects that remain unchanged under certain transformations, while equivariance is a property of certain relations whereby they remain in the same relative position to one another under certain transformations \cite{b35, b36}.
For example, consider the graph isomorphism property from Sec.~\ref{gt}.
Another advantage of geometric deep learning is its flexibility toward a broader range of data types and problems. 
This flexibility makes it a powerful tool for solving real-world problems that Euclidean methods cannot.  

