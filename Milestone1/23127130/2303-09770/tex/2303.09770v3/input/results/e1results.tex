\subsection{Experiment 1: Establish a Baseline}
\label{exp1r}
In the first experiment, we start by creating a baseline model for future comparisons.
Here we consider a 72\% (31,139 graphs) in the training set $\mathcal{G}_{training}$, 18\% (7,784) in the validation set $\mathcal{G}_{validation}$, and the remaining 10\% (4,324) in  $\mathcal{G}_{unknown}$.
These are by no means recommended distributions, but rather a scenario with lots of data available for the GDL model that will be used to explore what is possible and what might be done to balance accuracy versus efficiency.

The CM for this experiment is in \tref{baselinecm} using the 4,324 graphs in $\mathcal{G}_{unknown}$ that we have not used in any way to train the model (and in practice, you would not have this CM). 
Using \eref{acceq}, we find a {79.1}\% accuracy, fairly close to the baseline models' final accuracy value for $\mathcal{G}_{known}$ of {79.5}\%.
Next, using \eref{ppv}, we calculate that the baseline model has a \textit{Precision} of {77}\%.
Using \eref{rec}, we calculate the baseline models' \textit{Recall} to be {79.9}\%, and lastly, using \eref{f1} and \eref{mcc}, we calculate the \textit{F1 Score} to be {78.6}\% and \textit{MCC} to be {58}\%, respectively.
% Both of these scores tell us that we have a fairly accurate model.

\subsection{Experiment 2: Additional Graph-based Features}
\label{exp2r}

Here we consider adding additional graph-based features to $\mathbf{X}$ beyond the current vertex labels of $R$, $C$, $G$, etc. where the additional features should have a greater ability to explain the variance in the training data (known as feature engineering and feature selection).
The theory is that these features will improve the model's performance for the same number of training epochs.
The two features added here are eigenvector centrality and betweenness centrality.



Eigenvector centrality computes a node's centrality based on its neighbors' centrality and is a measure of the influence of a node in a graph where a high eigenvector centrality score implies that a node is connected to many nodes that themselves have high scores.
The eigenvector centrality for node $v$ is the $i$-th normalized element of the vector $\mathbf{v}$ from:
%
\begin{align}
\mathbf{A}\mathbf{v}=\lambda \mathbf{v}
\label{evc}
\end{align}
\noindent where $\mathbf{A}$ is the adjacency matrix and $\lambda$ is the largest eigenvalue \cite{b60}.

% \subsubsection{Betweenness Centrality.}~
Now, the betweenness centrality of a node is the sum of the fraction of all-pairs shortest paths that pass through that node:
%
\begin{align}
c_B(v) = \sum_{s,t\in V} \frac{\sigma(s,t\, |\, v)}{\sigma(s,t)}
\label{bc}
\end{align}

\noindent where $V$ is the set of nodes, $\sigma(s,t)$ is the number of shortest paths, and $\sigma(s,t\, |\, v)$ is the number of those paths passing through some node $v$ other than $(s,t)$ \cite{b61}.


%\CM{1,751}{419}{487}{1,667}{Confusion matrix for the \textit{baseline}
\CM{1,667}{487}{419}{1,751}{Confusion matrix for the \textit{baseline} model predicting $\mathcal{G}_{unknown}$.}{baselinecm}

% \CM{1,452}{578}{501}{5,254}{Confusion matrix for the \textit{3-feature} model predicting $\mathcal{G}_{unknown}$.}{3featcm}
%\CM{1,759}{451}{213}{1901}{Confusion matrix for the \textit{3-feature}
\CM{1,901}{213}{451}{1,759}{Confusion matrix for the \textit{3-feature} model predicting $\mathcal{G}_{unknown}$.}{3featcm}

We also point out that the computational expense of determining these additional features is quite low compared to calculated $J(G_i)$, so they add a relatively minor cost for each graph.
With these two additional features, the new features matrix $\textbf{X}$ for each individual graph has gone from $\textbf{X} \in \mathbb{R}^{n \times 1}$ to $\textbf{X} \in \mathbb{R}^{n \times 3}$.
The CM for this experiment is in \tref{3featcm}.
% Now, lets look at the results for the model with two additional node features.
Here we have that by adding the two additional features, the model was able to achieve an accuracy of 85\%, \textit{Precision} of 89.9\%, \textit{Recall} of 80.8\%, \textit{F1 Score} of 85\%, and an \textit{MCC} of 70\%.
Therefore, the GDL model was able to make better predictions on the same $\mathcal{G}_{known}$ dataset as all metrics are the same or better.
We will include all three features going forward.


