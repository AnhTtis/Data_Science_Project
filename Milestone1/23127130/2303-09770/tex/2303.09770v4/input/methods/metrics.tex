\subsection{The Metrics}
\label{metrics}
This section discusses the metrics used to evaluate the models' performance.

\subsubsection{The Confusion Matrix.}~A confusion matrix (CM), as seen in \fref{excm}, is a two-dimensional matrix where each column contains the samples of the classifier (model) output, and each row contains the sample in the true class (data) \cite{b85}.
For a binary classifier, the top left box represents the \textit{True Positives} ($T_P$), the data points that were classified correctly as ones.
The top right represents the \textit{False Positives} ($F_P$), the points that were classified as ones but are, in fact, zeros.
The bottom left contains the \textit{False Negatives} ($F_N$), the points that were classified as zeros but are actually ones.
Lastly, the bottom right is the \textit{True Negatives} ($T_N$), the points that were correctly classified as zeros.
The values in this matrix are used for many of the following metrics.

\CM{2,050 ($T_P$)}{450 ($F_P$)}{250 ($F_N$)}{2,250 ($T_N$)}{An example confusion matrix with sample values.}{excm}


\subsubsection{Accuracy.}\label{accuracy}~Accuracy is the number of correct predictions divided by the total number of predictions, see \eref{acceq}.
\begin{align}
\textit{Accuracy} = \frac{T_P+T_N}{N}
\label{acceq}
\end{align}

Using the data from \tref{excm}, we can calculate that our classifier had an accuracy of $0.86$.


\subsubsection{Precision}~This metric, also called Positive Predictive Value (PPV), tells us what proportion of positive classifications were actually correct.
This metric is particularly useful when your data has a class imbalance, e.g., more zeros than ones.
% Why does this matter?
When training on a dataset with more of one class than the other, you risk your model classifying all the data as the most frequent class, thus a ``high accuracy''.
That is where \textit{Precision} comes in because it is a class-specific metric defined as:
% Mathematically, precision is:
\begin{align}
\textit{Precision} = \frac{T_P}{T_P+F_P}
\label{ppv}
\end{align}
%
Once again, using our example, our classifier has a \textit{Precision} of 0.82, which means when it predicts a data point as one, it is correct 82\% of the time.
This same metric can also be applied to the negative value, but for this paper, we will use \textit{Precision} since our case studies focus on finding the \textit{best} architecture.
% and is known as the \textit{Negative Predictive Value} (NPV).

\subsubsection{Recall.}~This metric is calculated with:
\begin{align}
\textit{Recall} = \frac{T_P}{T_P+F_N}
\label{rec}
\end{align}
%

\noindent which indicates the proportion of actual positive classifications were identified correctly.
%
Using the data from \tref{excm}, \textit{Recall} is $0.89$, which states  the model correctly identified $89\%$ of all ones.

\subsubsection{F1 Score.}~This is a combination of both precision and recall into a single metric by calculating the harmonic mean between the two values as:
\begin{align}
\textit{F1 Score} = 2 \cdot \frac{\textit{Precision} \cdot \textit{Recall}}{\textit{Precision} + \textit{Recall}}
\label{f1}
\end{align}
%
A model with a high \textit{F1 Score} means that both \textit{Precision} and \textit{Recall} were high. 
Continuing with the example, the \textit{F1 Score} for this model is 0.85. 
    
\subsubsection{Matthews Correlation Coefficient (MCC).}~Of the metrics used to evaluate the performance of the classification models, this metric is one of the most important.
Once again, using the values from the confusion matrix, the $MCC$ produces a high score if the model obtained good results in \textit{all} boxes of the confusion matrix \cite{b67, b68}; this means high $T_P$ and $T_N$ values and low $F_P$ and $F_N$ values.
MCC is computed with:
\begin{align}
MCC = \frac{T_P \cdot T_N - F_P \cdot F_N}{\sqrt{(T_P + F_P)(T_P+F_N)(T_N+F_P)(T_N+F_N)}}
\label{mcc}
\end{align}

\noindent $MCC$ ranges from $-1$ to $1$, where $1$ indicates the model can make predictions perfectly, $-1$ indicates that every prediction was incorrect, and $0$ indicates that the model is just as good as random chance.

Finally, in our example, the achieved $MCC$ for this model is $0.72$, which is generally considered a good model.

\subsubsection{Total Set Accuracy.}~When the datasets are broken down into their respective subsets $(\mathcal{G}_{known},\mathcal{G}_{unknown})$, we still want to know how well the predictions are when compared to the $\mathcal{G}_{all}$, if it is available.
Therefore, we define \textit{Total Set Accuracy} as:
\begin{align}
\textit{Total Set Accuracy} = \frac{T_P^{(u)} + T_N^{(u)} + T_P^{(k)} + T_N^{(k)} }{N_{all}}
\label{eq:total-set-acc}
\end{align}

\noindent where $(T_P^{(u)},T_N^{(u)})$ are determined on $\mathcal{G}_{unknown}$ using the trained model, and $(T_P^{(k)},T_N^{(k)})$ are determined using $\mathcal{G}_{known}$, which might not be perfectly correct due to the median difference described in \sref{sec:datasets}.
If there is no misclassification based on the different medians, then $T_P^{(k)} + T_N^{(k)} = N_{known}$.
This metric captures the outcome of a potential real-world scenario where a designer uses both the known and unknown data to classify all graphs.