\subsection{The Model}
\label{model}

There are five layers for the model used in this study: three graph convolutional layers \cite{b65}, a mean pool layer, and a linear layer for the final readout.
To accomplish the graph classification goal, the model uses: 1) the convolutional layers to embed each node through message passing, 2) agglomerating the node embeddings to create a graph embedding, then 3) use the graph embedding to train the classifying layer.

\subsubsection{Graph Convolutional Layer.}~The Graph Convolutional Layer (GCN) layers determine the output features $\textbf{X}'$ by:
\begin{align}
\textbf{X}'_i = \textbf{W}_1\textbf{X}_i+\textbf{W}_2 \sum_{j\in \mathrm{N}(i)}e_{j,i}\cdot \textbf{X}_j
\label{gcn}
\end{align}

\noindent where $\textbf{X}$ is the input feature of each node (as discussed in \sref{gt}), $(\textbf{W}_1,\textbf{W}_2)$ represents the weights adjusted after each iteration while training, and $e_{j,i}$ is the edge weight from source node $j$ to target node $i$ \cite{b65}.
Each GCN layer also uses the ReLU activation function:
% 
\begin{align}
f(x) = \max(0,x)
\label{relu}
\end{align}

\noindent which returns 0 if it receives any negative input, but for any positive value $x$, it returns that value.
Finally, the outcome of \eref{gcn} is passed through the activation function:% 
\begin{align}
\textbf{X}_{i+1} = f(\textbf{X}_i) 
\label{gcn2}
\end{align}

\noindent which helps us obtain localized node embeddings.


\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{input/figures/node_embedding.pdf}
\caption{Calculated node embeddings for the graph in \fref{circgraph}.}
\label{embed}
\end{figure}

Node embeddings are a mapping of a graph's nodes or vertices into an $N$-dimensional numerical space.
This process creates numerical features as vectors from the graph structure. % , and these vectors are embeddings.
Nodes similar to one another will be spaced close together in what are known as communities.
As an example, we can use Node2Vec \cite{b79} to compute the node embeddings, visualized in \fref{embed} using the graph from \fref{circgraph}.
Comparing the two figures, we can see how Node2Vec analyzes the input circuit and interpret related nodes through the embeddings.
In \fref{circgraph}, the three nodes ($I_1$,$R_1$,$L_1$) are shown in a grouping in \fref{embed}, as well as the parallel components community ($N_1$,$C_1$,$L_2$,$R_2$,$N_2$,$O$); this is consistent since these components are connected and near one another.


%
\subsubsection{Global Mean Pooling Layer.}~The next layer is \textit{Global Mean Pooling}, which takes in the last output from the GCN layers and returns an output $\textbf{r}$ by averaging the node features $\textbf{X}$ to include node embeddings for each graph across all nodes $N$, creating a graph embedding:
%
\begin{align}
\textbf{r}_i = \frac{1}{N_i}\sum^{N_i}_{n=1} \textbf{X}_n
\label{gmp}
\end{align}

\subsubsection{Linear Layer.}~The final layer is the classification layer, which takes the mean pooling layer output as its input and applies the following linear transformation:
%
\begin{align}
\mathbf{y} = \mathbf{r} \mathbf{A}^T + \mathbf{b}
\label{lin}
\end{align}
%
However, before this layer, a \textit{dropout} is applied, which randomly zeros some of the elements of the input tensor by some assigned probability using samples from a Bernoulli distribution \cite{b82}.

\subsection{Model Hyperparameters and Options}
\label{hyp}

For the selected model architecture, there are several items that can be tuned so that the model trains well on the type of data provided.
For more information on hyperparameters and tuning, please refer to the following sources \cite{b44, b45, b46}.



\subsubsection{Learning Rate.}~The model's learning rate ($LR$) is a positive scalar value that determines the size of the step response to the estimated error as the weights are updated with each epoch and is considered one of the more important hyperparameters \cite{b45}.
Typically, this value ranges from $0.0$ to $1.0$. % , each value having its give and takes.
Choosing a value too small can result in a longer training time, and choosing a value too large can result in an unstable training process.
% This parameter is considered one of the model's most important hyperparameters.
For these case studies, we use a $LR$ of $0.001$.

\subsubsection{Number of Epochs.}~An epoch refers to the number of training iterations through the entire training dataset \cite{b83}.
The data is passed through the model during each epoch, updating the weights.
Epochs can often range from hundreds to thousands, allowing the model to train until the error in the model is minimized.
The number of epochs is explored in the different experiments in the case study. %  used here varies with each experiment within each case study.

\subsubsection{Optimization Algorithm.}~An optimization algorithm is a procedure for finding the input parameters for a function that results in the minimum or maximum of the function \cite{b83}.
In this model, we chose to use the Adaptive Movement Estimation (ADAM) algorithm, a stochastic optimization method that computes individual learning rates for different parameters of the first and second moments of the gradients \cite{b84}.

\subsubsection{Loss Function.}~A loss function computes the distance between the current output of the model with the expected out of the model.
Then, this metric measures how well the model is performing.
There are various loss functions, and the one used in this model is the \textit{cross-entropy} loss function, typically used for classification problems.
It measures the difference between two probability distributions for a given variable.

\subsubsection{Batch Size.}~While training, we can decide on batch size or the number of data points to work through before updating the model weights. 
This technique is known as mini-batching and is highly advantageous when training a deep-learning model by allowing the model to scale better to large amounts of data. 
Instead of processing data points one-by-one or all at once, a mini-batch groups a set of data points of intermediary size. %  into a unified representation that can efficiently be processed in parallel. 
This hyperparameter will be explored in the case study.

