% This section discusses the machine learning tools used throughout the modeling process.
% We will look at why these specific tools were implemented, followed by a brief discussion of how they were implemented.
% For a deeper look into how these modules were implemented within the code, please see \aref{appB}.


% The first and most important thing to discuss is the answer to the question, what are we trying to accomplish?
% There are three primary goals that we need to hit for our model to be successful:

% \begin{enumerate}
%     \item The model shall be able to take a graph as its input data.
    
%     \item The model shall be customizable, i.e., change/add layers, easily change hyperparameters,
    
%     \item The base machine learning library shall be able to perform multiple tasks on graphs, i.e., classification, regression, and link prediction.
% \end{enumerate}
%
% After an exhaustive search, we have come across the primary tool to be used, 
\clearpage
\section{TOOLS AND COMPUTING ARCHITECTURE}
The primary tool used is PyTorch-Geometric (PyG) \cite{b16} because of the extensive applications that PyG can perform on graph data.
It is built on top of PyTorch \cite{b15}, an open-source machine learning framework, which also contains an extensive library of tools for model manipulation and data analysis.
Since both tools are used primarily with Python \cite{b57}, that will be the language of choice for the model.
% Now that we have our primary tools and modeling language, we now have to find the tools needed to go from the raw dataset to data that is readable by PyG.
% An excellent tool that is used alongside PyG is
We also include Networkx, a Python package for the creation and manipulation of complex networks \cite{b17}.
This decision is because PyG requires the data to be of a certain instance, whether a SciPy sparse matrix or a Trimesh instance; it needs to be in a form readable by PyG.
Networkx was chosen because of the information that can be added to the graph, which can be easily transformed into a PyG instance.
We then use Pandas \cite{b77} for its data organization capabilities to import the data and prepare it to be passed on to Networkx and, finally, PyG.
% We now have a way to get our information into PyG; we now have the task of uploading the data into a Python script so that Networkx can see it.
% The original data was given in a Matlab file, so we used SciPy's\cite{b58} loadmat function and Pandas\cite{b77} data organization capabilities to import the data and prepare it to be passed on to Networkx and finally PyG.
% The outcome of the script is a list of PyG graph instances for each graph contained in the dataset, and each instance contains all the necessary information required for the model to train.
% For a more detailed look into the dataset creation, please see \aref{appB}.
For a full list of the tools used and their versions, please see \tref{tooltab}.

All the tools were utilized on a personal workstation consisting of an Intel Core i9-9900k CPU @ 3.60GHz, 32GB installed memory, and an Nvidia GeForce RTX 2060 Super GPU.

\begin{table}[h]
\centering
\caption{A list of the primary tools and their versions.}
\label{tooltab} 
\begingroup
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.1} % Default value: 1
\begin{tabular}{rl}
\hline \hline
\textbf{Tool} & \textbf{Version} \\
\hline
Python & 3.9\\
Networkx & 2.8.7\\
PyTorch & 1.12.1\\
PyTorch-Geometric & 2.1.0\\
SciPy & 1.9.1\\
Pandas & 1.5.0\\
\hline \hline
\end{tabular}
\endgroup
\end{table}