% \begin{figure*}[ht]
% \centering
% \includegraphics[scale=0.95]{input/figures/iterations.pdf}
% \caption{Five iterations of the approach described in \sref{exp4r} for retraining a GDL model based on successive performance minimizing-focused subsets of the graphs (with the numbers indicating the number of graphs in the respective set) along with the distribution of all the data points $J(\mathcal{G}_{all})$ for comparison.}
% \label{fig:iterations}
% \end{figure*}

\subsection{Experiment 4: Iterative GDL Classification}
\label{exp4r}



Even if with perfect classification, the approach outlined so far would only result in determining the top 50\% performing graphs in $\mathcal{G}_{all}$.
While this outcome can undoubtedly be useful in practice, it is generally desirable to narrow down the graphs further to the best-performing.
However, there is no reason we only need to construct a single classification model.

In this experiment, we seek a smaller, better median performance set of graphs from $\mathcal{G}_{all}$ by iteratively constructing GDL models with these steps:
\begin{enumerate}[nolistsep]

% \setcounter{enumi}{-1}

\item Set $k=1$ and create an initial $\mathcal{G}^{k}_{known}$. % (here 20\% of $\mathcal{G}_{all}$) and 

\item Create a GDL model $m^k(G_i)$ using $\mathcal{G}^{k}_{known}$, which is naturally broken into sets ``Known 1'' and ``Known 0'' based on the median $J$ value of $\mathcal{G}^{k}_{known}$.

\item Predict the classes of the $\mathcal{G}^{k}_{unknown}$ using $m^k(G_i)$, creating ``Predicted 1'' and ``Predicted 0'', which are sets of graphs predicted to be good (1) or bad (0), respectively. % , at this iteration

\item The goal is to identify good graphs, so we set $\mathcal{G}^{k+1}_{known}$ equal to ``Known 1'' and $\mathcal{G}^{k+1}_{unknown}$ equal to ``Predicted 1'' (and the remaining graphs are removed under the assumption that they are bad).

\item Set $k \rightarrow k + 1$ and repeat Step 2 until $k = n$.
    
\end{enumerate}

\noindent At each iteration, the size of $\mathcal{G}^{k}_{known}$ will be halved in a way that decreases its median performance value, so the good/bad classification threshold of $m^k(G_i)$ should shift as well.

The results of this experiment initialized with 20\% of the data known are summarized in \fref{fig:iterations}.
First, the leftmost box plot shows the distribution of $J(\mathcal{G}_{all})$.
The next two box plots show the splitting of $\mathcal{G}_{known}$ based on the classification threshold at iteration 1.
The final two box plots in iteration 1 are the predicted 1s and 0s based on the GDL model; note that, while there are certainly incorrectly identified graphs, the medians are quite separated and consistent with their ``Known'' counterparts.
The subsequent box plots show the results for each iteration, up to $n=5$.
We observe at iteration 5, with $N_{known} = $ 540 graphs now, the classification is quite poor as the ``Predicted'' box plots are similarly distributed. 
The trend lines for the two medians also convey this point, with substantial decreases until the fifth iteration.
This behavior is perhaps expected because we are not adding any new data from the initial $\mathcal{G}^1_{known}$, and the number of graphs for training decreases.
Adding more data at each iteration based on ``Predicted 1'' is future work.

Assessing the outcome at the end of iteration 4 (since the fifth did not classify well), there are a total of 2,920 graphs in the final ``good'' set with a median of $3 \times 10^{-4}$, substantially lower than the initial classification median of $2 \times 10^{-1}$.
Here we assume a designer would evaluate $J(G_i)$ for all the graphs in ``Predicted 1''; thus, there would be a total of 11,029 graphs that were optimized. 
For this final set, \textit{all} of the top 10, 87 of the top 100, and 727 of the top 1000 graphs still remain at 25\% the computational cost of complete enumeration.
Furthermore, compared to randomly sampling 11,029 graphs from $\mathcal{G}_{all}$, the iterative GDL model results greatly exceed the expected means of 2.25, 22.51, and 225.13 for the top 10, 100, and 1000 graphs being included in the random set, respectively.

To better understand the statistical significance of this iterative approach, five more randomized runs were completed with different samplings of $\mathcal{G}^1_{known}$ for a total of six runs.
Over these runs, the average graphs that would be known or ``optimized'' was 11282.2 graphs (with a 492.8 standard deviation).
For the top 100 graphs, 88.2 (2.5) remained compared to an expected value of 22.96.
Finally, for the top 1000 graphs, 751.2 (39.5) remained compared to an expected value of 229.6.
Furthermore, the median values of the ``Known 1'' and ``Predicted 1'' graph sets for each iteration are shown in Fig.~\ref{fig:iterations-stats} averaged over the six runs.
Here we still see a decrease in the median values indicating the iterative GDL approach is narrowing down the set of potentially ``good'' graphs to the higher percentiles of performance. In fact, at iteration 4, all medians for ``Predicted 1'' are better than the 90th percentile, with the mean closer to the 95th.
However, as previously discussed in Fig.~\ref{fig:iterations}, the benefits from the 5th iteration are minimal.



% 11030/43249 25.5\%
% 2.2470
% 22.5222 randomly for top 100




% 

% the class threshold will continue to reduce to reveal the top candidates for the best realizable performing solution.


% In this experiment, the goal is to see if the GDL model can narrow the dataset down to the actual top 10\% of the dataset through multiple iterations.
% Here, we take the original dataset ($\mathcal{G}_{all}$) and split it into the KNOWN ($\mathcal{G}_{known}$) and UNKNOWN ($\mathcal{G}_{unknown}$) sets, and once again, use the performance median of the KNOWN set ($\textit{median}J(\mathcal{G}_{known})$)as the threshold for the class splitting of ones and zeros.

% We then take the $\mathcal{G}_{known}$ and using an 80/20 split, divide the set into training and validation sets to train the first model, which is then used to predict the classes of the $\mathcal{G}_{unknown}$.
% Once the predictions are completed, we determine which circuits were predicted as zero and remove them under the assumption that those circuits were predicted to be "bad."
% We also remove the classified zeros from the known set since we already have classified them as "bad."
% Now, we are left with our original KNOWN set of "good" graphs, and the newly predicted ones from the UNKNOWN set, and we concatenate these two sets to create a new KNOWN set ($\mathcal{G}_{known2}$).
% Next, we train a new model on $\mathcal{G}_{known2}$ using$\textit{median}J(\mathcal{G}_{known2})$ as the new class threshold.

% The expected outcome of this experiment is that after each iteration of training on the newly formed KNOWN dataset, the class threshold will continue to reduce to reveal the top candidates for the best realizable performing solution.


















% As discussed in \sref{exp4}, this experiment was conducted on sets that have a class imbalance.

% \subsubsection{50/50 Approach}
% We start this approach by randomly selecting 50\% of the data (21,624 circuits) and call this the KNOWN set.
% This set represents the set that was synthesized where we have the known performance values, and we can now split this set into its corresponding classes based on pre-determined criteria.
% In this case, we are looking for the top 25\% of circuits, so we determine the top 25\% performance values and assign them to the one-class, the remaining to the zero-class.
% For each training run, we are also decreasing the size of the training set, to see how well the model performs as we attempt to increase efficiency.

% In \tref{trainsize_50}, we show the size of the training set for each run and the number of data points in each class.
% Note that $N_{known}$ for this is 21,624 data points.
% %
% \begin{table}[]
% \centering
% \caption{Training set sizes and the number of data points per class.}
% \label{trainsize_50}
% \begin{tabular}{c|c}
% \hline \hline
% \textbf{Training \%} & \textbf{\# of ones \& zeros}\\
% \hline
% 90 & 4,864\\
% \hline
% 70 & 3,783\\
% \hline
% 50 & 2,702\\
% \hline
% 30 & 1,621\\
% \hline \hline
% \end{tabular}
% \end{table}

% \subsubsection{Shuffled Data Approach}
