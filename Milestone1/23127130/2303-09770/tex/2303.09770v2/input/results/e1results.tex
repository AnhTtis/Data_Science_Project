\subsection{Experiment 1: Establish a Baseline}
\label{exp1r}
In the first experiment, we start by creating a baseline model for future comparisons.
Here we consider a 72\% (31,139 graphs) in the training set $\mathcal{G}_{training}$, 18\% (7,784) in the validation set $\mathcal{G}_{validation}$, and the remaining 10\% (4,324) in  $\mathcal{G}_{unknown}$.
These are by no means recommended distributions, but rather a scenario with lots of data available for the GDL model that will be used to explore what is possible and what might be done to balance accuracy versus efficiency.

The CM for this experiment is in \tref{baselinecm} using the 4,324 graphs in $\mathcal{G}_{unknown}$ that we have not used in any way to train the model (and in practice, you would not have this CM). 
Using \eref{acceq}, we find a {79.1}\% accuracy, fairly close to the baseline models' final accuracy value for $\mathcal{G}_{known}$ of {79.5}\%.
Next, using \eref{ppv}, we calculate that the baseline model has a \textit{Precision} of {77}\%.
Using \eref{rec}, we calculate the baseline models' \textit{Recall} to be {79.9}\%, and lastly, using \eref{f1} and \eref{mcc}, we calculate the \textit{F1 Score} to be {78.6}\% and \textit{MCC} to be {58}\%, respectively.
% Both of these scores tell us that we have a fairly accurate model.

\subsection{Experiment 2: Additional Graph-based Features}
\label{exp2r}

Here we consider adding additional graph-based features to $\mathbf{X}$ beyond the current vertex labels of $R$, $C$, $G$, etc. where the additional features should have a greater ability to explain the variance in the training data (known as feature engineering and feature selection).
The theory is that these features will improve the model's performance for the same number of training epochs.
The two features added here are eigenvector centrality and betweenness centrality.

% The goal of this experiment is not only to increase the models' performance but also to maintain efficiency.
% What do we mean by efficiency?
% % One way of increasing model accuracy is to train the model for more epochs, which takes more time while also running the risk of overfitting.
% Increasing efficiency means training for the same amount of epochs or less.
% To do that, we added graph-based features to each node: eigenvector centrality and betweenness centrality. 
% This is known as feature engineering and feature selection, where the features give a higher ability to explain the variance in the training data.

% \subsubsection{Eigenvector Centrality.}~

Eigenvector centrality computes a node's centrality based on its neighbors' centrality and is a measure of the influence of a node in a graph where a high eigenvector centrality score implies that a node is connected to many nodes that themselves have high scores.
The eigenvector centrality for node $v$ is the $i$-th normalized element of the vector $\mathbf{v}$ from:
%
\begin{align}
\mathbf{A}\mathbf{v}=\lambda \mathbf{v}
\label{evc}
\end{align}
\noindent where $\mathbf{A}$ is the adjacency matrix and $\lambda$ is the largest eigenvalue \cite{b60}.

% \subsubsection{Betweenness Centrality.}~
Now, the betweenness centrality of a node is the sum of the fraction of all-pairs shortest paths that pass through that node:
%
\begin{align}
c_B(v) = \sum_{s,t\in V} \frac{\sigma(s,t\, |\, v)}{\sigma(s,t)}
\label{bc}
\end{align}

\noindent where $V$ is the set of nodes, $\sigma(s,t)$ is the number of shortest paths, and $\sigma(s,t\, |\, v)$ is the number of those paths passing through some node $v$ other than $(s,t)$ \cite{b61}.


%\CM{1,751}{419}{487}{1,667}{Confusion matrix for the \textit{baseline}
% \CM{1,667}{487}{419}{1,751}{Confusion matrix for the \textit{baseline} model predicting $\mathcal{G}_{unknown}$.}{baselinecm}
\begin{table}[]
    \centering
    \caption{Confusion matrix for the \textit{baseline} model predicting $\mathcal{G}_{unknown}$.}
    \includegraphics[scale=1]{input/figures/cm1.pdf}
    \label{baselinecm}
\end{table}

% \CM{1,452}{578}{501}{5,254}{Confusion matrix for the \textit{3-feature} model predicting $\mathcal{G}_{unknown}$.}{3featcm}
%\CM{1,759}{451}{213}{1901}{Confusion matrix for the \textit{3-feature}
% \CM{1,901}{213}{451}{1,759}{Confusion matrix for the \textit{3-feature} model predicting $\mathcal{G}_{unknown}$.}{3featcm}
\begin{table}[]
    \centering
    \caption{Confusion matrix for the \textit{3-feature} model predicting $\mathcal{G}_{unknown}$.}
    \includegraphics[scale=1]{input/figures/cm2.pdf}
    \label{3featcm}
\end{table}

We also point out that the computational expense of determining these additional features is quite low compared to calculated $J(G_i)$, so they add a relatively minor cost for each graph.
With these two additional features, the new features matrix $\textbf{X}$ for each individual graph has gone from $\textbf{X} \in \mathbb{R}^{n \times 1}$ to $\textbf{X} \in \mathbb{R}^{n \times 3}$.
The CM for this experiment is in \tref{3featcm}.
% Now, lets look at the results for the model with two additional node features.
Here we have that by adding the two additional features, the model was able to achieve an accuracy of 85\%, \textit{Precision} of 89.9\%, \textit{Recall} of 80.8\%, \textit{F1 Score} of 85\%, and an \textit{MCC} of 70\%.
Therefore, the GDL model was able to make better predictions on the same $\mathcal{G}_{known}$ dataset as all metrics are the same or better.
We will include all three features going forward.



% The expected outcome for this experiment is that we will be able to achieve higher scores from the metrics described in \sref{metrics}.




% \subsection{Experiment 1 \& 2 Results}
% 
% In these two experiments, we established a baseline in order to have a frame of reference.
% Then, in the second experiment we added features in an attempt to increase model accuracy and performance without sacrificing model efficiency.

% \begin{figure}
% \centering
% \includegraphics[width=\columnwidth]{input/figures/baseline_plots.pdf}
% \caption{Baseline model and the 3 feature model training accuracy with a 50/50 split between graph labels.}
% \label{baselineacc}
% \end{figure}

% As you can see from Fig.~\ref{baselineacc}, the blue line represents the baseline performance while the orange line represents the model with the additional features.
% As predicted, by adding the additional features, the model was able to perform better.
% By giving the model more features to train on, it was able to better distinguish between the different graphs, and assign the appropriate label.




% If we recall from \sref{metrics}, we can analyze the models a bit further, first by creating the CM as seen in \fref{baselinecm} and \fref{3featcm} for the \textit{baseline} and \textit{3 feature} models respectively.
% Starting with the \textit{baseline} model CM, we can calculate the model accuracy based on the $7,785$ test graphs.

% Using \eref{acceq}, we can see that the model was able to achieve a $79\%$ accuracy, fairly close to what \fref{baselineacc} shows as the baseline models' final accuracy value.
% Next, using \eref{ppv}, we calculate that the baseline model has a \textit{PPV} of $77\%$.
% Using \eref{rec}, we calculate the baseline models' recall to be $80\%$, and lastly, using \eref{f1} and \eref{mcc}, we calculate the f1-score to be $78\%$ and MCC to be $59\%$.
% Both of these scores tell us that we have a fairly accurate model.







% As expected, if an engineer has only the core data (adjacency matrix and graph label) at hand, feature engineering can help a machine learning model distinguish between all the different graphs, thus increasing training accuracy therefore making more accurate predictions.
% See \tref{baselinetable} for a direct score comparison.

% \begin{table}
% \centering
% \caption{Score comparison for the baseline model and the 3 feature model.}
% \label{baselinetable}
% \begin{tabular}{c|c|c|c|c|c}
% \hline \hline
% \textbf{Model} & \textbf{\textit{ACC \%}}  & \textbf{\textit{PPV \%}} & \textbf{\textit{Recall \%}} & \textbf{\textit{f1 \%}} & \textbf{\textit{MCC \%}}\\
% \hline
% \textbf{Baseline} & 79 & 77 & 80 & 78 & 59\\
% \hline
% \textbf{3 Feature} & 86 & 72 & 74 & 73 & 54\\
% \hline \hline
% \end{tabular}
% \end{table}