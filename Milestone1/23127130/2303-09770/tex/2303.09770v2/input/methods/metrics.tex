\subsection{The Metrics}
\label{metrics}
This section discusses the metrics used to evaluate the models' performance.
% To evaluate a machine learning model, specific values are essential to track or calculate that can give you insight into your models' performance; it all depends on the function of the model.
% Since this model is performing classification, the metrics used here are:

\subsubsection{The Confusion Matrix.}~A confusion matrix (CM), as seen in \fref{excm}, is a two-dimensional matrix where each column contains the samples of the classifier (model) output, and each row contains the sample in the true class (data) \cite{b85}.
% Almost all of our performance data can be extracted from the four values.
% For a binary classifier, where the classification output would either be a one or a zero, the top left box would represent the \textit{True Positives} ($T_P$), the data points that were classified correctly as ones.
For a binary classifier, the top left box represents the \textit{True Positives} ($T_P$), the data points that were classified correctly as ones.
The top right represents the \textit{False Positives} ($F_P$), the points that were classified as ones but are, in fact, zeros.
The bottom left contains the \textit{False Negatives} ($F_N$), the points that were classified as zeros but are actually ones.
Lastly, the bottom right is the \textit{True Negatives} ($T_N$), the points that were correctly classified as zeros.
The values in this matrix are used for many of the following metrics.

% \CM{2,050 ($T_P$)}{450 ($F_P$)}{250 ($F_N$)}{2,250 ($T_N$)}{An example confusion matrix with sample values.}{excm}

\begin{table}[]
    \centering
    \caption{An example confusion matrix with sample values.}
    \includegraphics[scale=1]{input/figures/example_cm.pdf}
    \label{excm}
\end{table}

% As an example, say we have $\mathcal{G}_{test}$ containing 5,000 graphs with 2,500 ones and 2,500 zeros.
% We take our trained model, and we predict the dataset.
% Then, after the output predictions are compiled, we create the CM in \fref{excm2}.
% We will further examine the values in this matrix in the remainder of this section as we discuss the remaining metrics.

% \CM{2,050}{450}{250}{2,250}{A second example CM containing values}{excm2}

\subsubsection{Accuracy.}\label{accuracy}~Accuracy is the number of correct predictions divided by the total number of predictions, see \eref{acceq}.
\begin{align}
\textit{Accuracy} = \frac{T_P+T_N}{N}
\label{acceq}
\end{align}

Using the data from \tref{excm}, we can calculate that our classifier had an accuracy of $0.86$.

% Continuing with our binary classifier example, we have the following data:
% %
% \begin{itemize}
%     \item $T_P = 2,050$
%     \item $F_P = 450$
%     \item $F_N = 250$
%     \item $T_N = 2,250$
% \end{itemize}
% %
% Using \eref{acceq}, we can calculate that our classifier had an accuracy of $0.86$.

\subsubsection{Precision}~This metric, also called Positive Predictive Value (PPV), tells us what proportion of positive classifications were actually correct.
This metric is particularly useful when your data has a class imbalance, e.g., more zeros than ones.
% Why does this matter?
When training on a dataset with more of one class than the other, you risk your model classifying all the data as the most frequent class, thus a ``high accuracy''.
That is where \textit{Precision} comes in because it is a class-specific metric defined as:
% Mathematically, precision is:
\begin{align}
\textit{Precision} = \frac{T_P}{T_P+F_P}
\label{ppv}
\end{align}
%
Once again, using our example, our classifier has a \textit{Precision} of 0.82, which means when it predicts a data point as one, it is correct 82\% of the time.
This same metric can also be applied to the negative value, but for this paper, we will use \textit{Precision} since our case studies focus on finding the \textit{best} architecture.
% and is known as the \textit{Negative Predictive Value} (NPV).

\subsubsection{Recall.}~This metric is calculated with:
\begin{align}
\textit{Recall} = \frac{T_P}{T_P+F_N}
\label{rec}
\end{align}
%

\noindent which indicates the proportion of actual positive classifications were identified correctly.
%
Using the data from \tref{excm}, \textit{Recall} is $0.89$, which states  the model correctly identified $89\%$ of all ones.

\subsubsection{F1 Score.}~This is a combination of both precision and recall into a single metric by calculating the harmonic mean between the two values as:
\begin{align}
\textit{F1 Score} = 2 \cdot \frac{\textit{Precision} \cdot \textit{Recall}}{\textit{Precision} + \textit{Recall}}
\label{f1}
\end{align}
% \footnote{Harmonic Mean -- a type of numerical average calculated by dividing the number of observations by the reciprocal of each number in the series}.
A model with a high \textit{F1 Score} means that both \textit{Precision} and \textit{Recall} were high. % , and the inverse of that applies to a low F1 score.
% If you have an F1 score between $0$ and $1$, one of your other two metrics is higher than the other.
%
Continuing with the example, the \textit{F1 Score} for this model is 0.85. % , which tells us we have a fairly good model.
    
\subsubsection{Matthews Correlation Coefficient (MCC).}~Of the metrics used to evaluate the performance of the classification models, this metric is one of the most important.
Once again, using the values from the confusion matrix, the $MCC$ produces a high score if the model obtained good results in \textit{all} boxes of the confusion matrix \cite{b67, b68}; this means high $T_P$ and $T_N$ values and low $F_P$ and $F_N$ values.
MCC is computed with:
\begin{align}
MCC = \frac{T_P \cdot T_N - F_P \cdot F_N}{\sqrt{(T_P + F_P)(T_P+F_N)(T_N+F_P)(T_N+F_N)}}
\label{mcc}
\end{align}


% MCC is a bit different than an F1 score, as the MCC score ranges from $-1$ to $1$.
% What does this mean?
\noindent $MCC$ ranges from $-1$ to $1$, where $1$ indicates the model can make predictions perfectly, $-1$ indicates that every prediction was incorrect, and $0$ indicates that the model is just as good as random chance.
% If the MCC score produces a value of $1$, the model can make predictions perfectly.
% An MCC score of $-1$ tells us that we have a very poor model, and every prediction was incorrect.
% While a $0$ would indicate that the model is just as good as random chance.
% The MCC is mathematically represented by \eref{mcc}.
%

Finally, in our example, the achieved $MCC$ for this model is $0.72$, which is generally considered a good model.

% There are many other metrics that we can use to determine our models' performance, such as \textit{True Positive Rate (TPR), True Negative Rate (TNR), False Positive Rate (FPR), False Negative Rate (FNR)}, and \textit{False Discovery Rate (FDR)}, which all can be determined through the CM, but for this study, the above metrics give us a lot of insight into our models' performance.

\subsubsection{Total Set Accuracy.}~When the datasets are broken down into their respective subsets $(\mathcal{G}_{known},\mathcal{G}_{unknown})$, we still want to know how well the predictions are when compared to the $\mathcal{G}_{all}$, if it is available.
Therefore, we define \textit{Total Set Accuracy} as:
\begin{align}
\textit{Total Set Accuracy} = \frac{T_P^{(u)} + T_N^{(u)} + T_P^{(k)} + T_N^{(k)} }{N_{all}}
\label{eq:total-set-acc}
\end{align}

\noindent where $(T_P^{(u)},T_N^{(u)})$ are determined on $\mathcal{G}_{unknown}$ using the trained model, and $(T_P^{(k)},T_N^{(k)})$ are determined using $\mathcal{G}_{known}$, which might not be perfectly correct due to the median difference described in \sref{sec:datasets}.
If there is no misclassification based on the different medians, then $T_P^{(k)} + T_N^{(k)} = N_{known}$.
This metric captures the outcome of a potential real-world scenario where a designer uses both the known and unknown data to classify all graphs.
% For example, in one of the experiments, we treat it as a real-world scenario, where a user synthesizes a fraction of their data for model-training purposes.
% The median performance value for this KNOWN set is determined and used as the classification threshold.
% The UNKNOWN set is given two sets of $y_{true}$ values, the first based on the KNOWN set's median performance, the second based on the entire set's median performance.
% When training and predictions are complete, we can then test the accuracy using the formula in \sref{accuracy}.